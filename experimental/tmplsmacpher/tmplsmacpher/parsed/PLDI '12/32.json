{"article_publication_date": "06-11-2012", "fulltext": "\n Dynamic Trace-Based Analysis of Vectorization Potential of Applications Justin Holewinski RagavendarRamamurthi \nMaheshRavishankar NazninFauzia Louis-No\u00a8el Pouchet AtanasRountev P. Sadayappan Department ofComputerScience \nandEngineering TheOhioState University {holewins,ramamurr,ravishan,fauzia,pouchet,rountev,saday}@cse.ohio-state.edu \n Abstract Recent hardware trends with GPUs and the increasing vector lengthsofSSE-likeISAextensionsformulticoreCPUsimplythat \neffective exploitation ofSIMDparallelismis criticalfor achieving highperformanceon emergingandfuturearchitectures.Avastma\u00adjorityofexistingapplicationsweredevelopedwithout \nanyattention bytheirdevelopers towardseffectivevectorizabilityofthecodes. While developers of production \ncompilers such asGNU gcc,In\u00adtelicc,PGI pgcc, andIBMxlchaveinvested considerableeffort and made signi.cant \nadvances in enhancing automatic vectoriza\u00adtion capabilities, these compilers still cannot effectively \nvectorize many existing scienti.c and engineering codes. It is therefore of considerable interest to \nanalyze existing applications to assess the inherentlatentpotentialforSIMD parallelism,exploitablethrough \nfurther compiler advances and/or via manual code changes. Inthispaperwedevelopanapproachtoinferaprogram \nsSIMD parallelizationpotentialbyanalyzingthedynamicdata-dependence graph derived from a sequential execution \ntrace. By considering onlythe observed run-timedatadependences forthetrace, andby relaxingtheexecutionorderof \noperationstoallowanydependence\u00adpreserving reordering, we can detect potential SIMD parallelism that may \notherwisebe missedby moreconservativecompile-time analyses.Weshowthatfor severalbenchmarks ourtooldiscovers \nregionsof codewithin computationally-intensiveloopsthatexhibit highpotentialforSIMDparallelismbutare \nnotvectorizedbystate\u00adof-the-artcompilers.Wepresent several casestudiesoftheuseof thetool, bothinidentifying \nopportunitiestoenhancethe transfor\u00admation capabilities of vectorizing compilers, as well asinpoint\u00ading \nto code regions to manually modify in order to enable auto\u00advectorizationand performanceimprovementbyexistingcompilers. \nCategories and Subject Descriptors C.4[Performance of sys\u00adtems]: Measurement techniques, Performance \nattributes; D.1.3 [Programming techniques]: Concurrent programming Parallel programming; D.3.4 [Programming \nLanguages]: Processors Compilers,Optimization General Terms Performance, Measurement, Algorithms Keywords \nPerformance analysis, dynamic analysis, vectorization Permission to make digital or hard copies of all \nor part of this work for personal or classroom use is granted without fee provided that copies are not \nmade or distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. PLDI 12, June 11 16, 2012, Beijing, China. Copyright c &#38;#169; \n2012 ACM 978-1-4503-1205-9/12/06. . . $10.00 1. Introduction The SIMD vector units in modern multi-processors \nachieve very highperformancebyapplyingthesameinstructiontomultipledata elementsatonce.Asnewergenerationsof \nmulti-coreprocessors andGPUscontinuetoextendthewidth ofvector processors,the exploitation of vector instructions \nis ofincreasingimportance. Un\u00adfortunately,manyprogramsare writtenusing structures,pointers, andother \nnon-array constructsthatpreventmodern compilersfrom performing the analyses and transformations that \nare required to fullyexploitthese vector-processing resources.Evenfor programs that usearrays,vectorization \npotential thatexistsinthe computa\u00adtionisoften missedbycompilers.Sometypical reasonsforthisare (1)conservative \ndependenceanalysis,(2) conditionalbehaviorfor handling of boundary cases, which precludes the vectorization \nof thecommon case, and(3) datalayoutsthatdo notallowthe con\u00adtiguous memoryaccessesneededforef.cientvectorprocessing. \nGiventhe sustainedtrendofincreasingly-widevector units, one keyquestioniswhetherexistingprogramscan takeadvantageof \nthesehardwarecapabilities. Themaincontributionof ourwork is an automatic approach to characterize the \ninherent vectorizability potential of existing applications by analyzing information about run-timedependences \nandmemory accesspatterns.The approach instruments the program to monitor and record instructions and \ntheir data accesses, and then analyzes the resulting trace to con\u00adstructthe dynamicdependencegraphforthe \nobserved execution. Next, the graph is used to partition the dynamic instances of in\u00adstructionsinto setsthat \nare bothindependent and accessthe mem\u00adory witha.xed stride.Thesesetsrepresent instructioninstances that \ncanpotentiallyutilizevector resourceseffectively. Technical Challenges The identi.cation of potentially \nvector\u00adizable operations requires the discovery of .ne-grained concur\u00adrency among operations that access \ncontiguously located data el\u00adements.Althoughtherehas been considerablepriorwork(e.g., [2,3,8,11,12,14,16,17,19,21,23,25,28,29,33,35,39])on \nus\u00adingdynamicanalysisfor characterizingparallelisminapplications, previouslydevelopedapproaches havefundamentallimitationsfor \ndiscovering potentially vectorizable operations. Existing work on usingdynamicanalysistocharacterize \npotentialparallelisminse\u00adquentialprogramsfallsbroadly undertwo general categories:(1) generation ofaparallelismpro.le \nand critical-path analysis ofthe directed acyclicgraph(explicitlyconstructed orimplicitly mod\u00adeled) representing \nthe run-time dependences of the computation, and(2)loop-level or region-level characterization ofparallelism, \nwherecomputationswithintheloop/region areconstrainedtoex\u00adecuteintheoriginalsequential order.Anadvantage \noftheformer approach is that thegeneratedparallelismpro.leimplicitly mod\u00adelsallpossibledependence-preservingreorderingofthe \noperations sinceitperformscritical-path analysisof the computationDAG. However,asdiscussedinthenext section,adisadvantageisthatin\u00addependenceand \npotentialconcurrency atthelevel of speci.c state\u00admentsorexpressionsinaloop cannotbe deduced.Thesigni.cant \nadvantageofthe second approachisthat such speci.cloop/region level concurrencyinformationisextracted.But \nunliketheformer type of analysis, this characterization may be constrained by the orderofoperationswithinthemodeled \nregion/loopthatisimposed by the original program. Thus, the potential for increased paral\u00adlelism via \ndependence-preserving reordering of operations is left unexplored.Finally, none oftheprevious approachesto \ndynamic analysis for characterizingparallelismconsider thepatternsofrun\u00adtime memoryaccesses, which are \ncritical in the characterization of vectorizationpotential.  Approach Wedevelop a new approach to analysis \nof the dynamic data dependence graph to characterize maximal concurrency per statement/operator under \nall possible dependence-preserving re\u00adorderings of the computation, with further analysis of concurrent \noperations accessing contiguous or uniformly strided data. The analysisis usefulina numberof ways: 1. \nCharacterization of code bases: An automatedtoolthat can be runthroughlargeexisting code basestocharacterizetheinher\u00adentvectorizationpotential \nof thoseprograms canbevaluable to multiple groups.First,ISVs(IndependentSoftwareVendors) with largelegacy \nsoftwaresystemscanassess whichportions ofthecode may need completealgorithmicrewrite(ifthetool shows \nno vectorizability) versus code changes without algo\u00adrithmchange(ifthe toolshowshigh vectorizabilitypotential). \nThequantitative informationonaveragevector lengthscanbe usefulin assessingthe potentialbene.t of converting \nthe code touseGPUs(wheremuchhigherdegreeofSIMDparallelismis neededthanwithshort-vectorSIMDISAs).Second,CPU/GPU \ndesigners can assess the potential future bene.ts of widened SIMDstructuresforimportantmarketsegmentsby \ncharacteriz\u00adingtheinherent unexploited.ne-grained parallelismavailable inwidely-used softwareindifferentdomains.Inorderto \nillus\u00adtratethisuse ofthetool,weprovideacharacterization of the .oating-point benchmarksoftheSPEC2006 \nbenchmark suite. 2. Aid in performance optimization: Many existing applications contain hotloopswithsigni.cantvectorization \npotential.While sometimessimple scanningoftime-consumingloopsby an ap\u00adplicationdevelopermay revealthe \npotentialforvectorization, thisis non-trivialinmany real codesdueto multiplelevelsof functioncallsthat \nmustbeanalyzed.Theautomaticidenti.ca\u00adtionofcodeportions thatexhibit inherentvectorizabilitypoten\u00adtialcanaidanapplicationsexpertwhocanthen \nmanually trans\u00adformthecodetoenhance itsvectorizabilityby compilers.We illustrate this useofthe approach \nthrough several case studies. 3. Aid to compiler writers: Identi.cationofpotentialvectorization (forexistingprograms)thatis \nnotexploitedbycurrentvectoriz\u00adingcompilerscan leadtonewinsightsforcompilerwriters,and eventually tonewstaticanalysesandtransformationsforstate\u00adof-the-art \ncompilertechnology.Further development(beyond the scope of this paper) to characterize patterns of statically\u00adanalyzablevectorization \nopportunities(i.e.,nodata-dependent conditions)missedby avectorizing compiler canbehelpfulto compiler \nwriters in enhancing auto-vectorization capabilities. Weprovidean illustrationofthisuse casethrougha \ncase study.  2. Background and Overview Theproposed approachis basedonthefollowingkeyobservation: to \nidentifyandquantify thevectorizationpotential of agivenpro\u00adgram, the dynamic analysis needs to uncover \nindependent opera\u00ad 1 for(i =1;i <N;++i){ 2 A[i] = 2.0 * A[i-1]; // S1 3 } 4 for(i =0;i <N;++i){ 5 for(j=1; \nj<N;++j){ 6 B[j][i] = B[j-1][i] * A[i]; // S2 7 } 8 }  Listing1:Example1for dynamicparallelism analysis. \nTimestamps t=1 B[2,1] B[2,2] A[1] A[2] B[1,1] B[1,2] t=2 t=3   t=2(N-1) (a) (b) Figure1:DependencesforExample \n1. tions that couldbeexecutedconcurrently.Furthermore,theseinde\u00adpendent operations should exhibit a pattern \nof contiguous access to memorylocations.The restofthis sectionprovideshigh-level overviewand examplesto \nillustratethesetwokeyissues.Thespe\u00adci.c detailsoftheapproach areelaboratedlaterinthe paper. 2.1 Finding \nIndependent Operations Prior work on using dynamic analysis to characterize potential parallelisminsequentialprogramsfallsbroadly \nunder oneoftwo generalcategories.Oneapproach,exempli.edbythe earlyworkof Kumar[11], performstimestamp-based \nanalysis ofinstrumented statement-levelexecutionofthe sequentialprogram,using shadow variablestomaintainlast-modifytimes \nfor eachvariable.Each run\u00adtimeinstanceof a statementisassociatedwitha timestampthatis one greater than \nthe largest of the last-modify times of all input operandsofthestatement.Ahistogramofthenumberofoperations \nat each time value provides a .ne-grained parallelism pro.le of thecomputation,andthemaximaltimestamprepresentsthecritical \npathlengthforthe entire computation. Incontrasttotheabove.ne-grained approach,analternatetech\u00adniquebyLarus[14] \nperformsanalysisofloop-levelparallelismat differentlevelsofnestedloops.Loop-levelparallelismismeasured \nbyforcingasequentialorderofexecutionofstatementswithineach iterationof aloop being characterized,sothatthe \nonly available concurrencyis acrossdifferentiterationsofthatloop. To illustrateKumar sapproach,considerthecodeexamplein \nListing 1. For explanation purposes, this example is extremely simpleandisusedonlytohighlighttheparallelismcharacterization \n 1 for(i = 1; i < N; ++i) { 2 A[i] = 2.0 * B[i-1]; // S1 3 B[i] = 0.5 * C[i]; // S2 4 } Listing2:Example2for \ndynamicparallelism analysis. fromprior work.The run-timeinstances ofthe .rststatementform a chain of \ndependences of length N - 1. The second statement has N(N - 1) run-timeinstances,withdependences as shownin \nthe.gure.Therun-timestatementinstances andtheir dependences de.ne a dynamic data-dependence graph (DDG), \nas shown in Fig. 1(a). The top row represents instances of statement S1 and the other nodes represent \ninstances of statement S2. For ease of comprehension,a node maybelabeledwiththearray elementthat is writtenbythat \nnode. Theanalysisofpotentialparallelismcomputesa timestampfor each DDG node,representing the earliesttimethis \nnode couldbe executed;these timestampsarealsoshowninFig.1(a).Thelargest timestamp, compared to the number \nof nodes, provides a charac\u00adterization oftheinherent .ne-grainparallelisminthe program;this largesttimestampgivesthelength \nofthe critical path in the DDG. Inessence,the timestampsimplicitly model thebestparallel ex\u00adecution of \nall possible dependence-preserving reorderings of the operations performedbytheprogram.Intheexamplefromabove, \nthe criticalpathhas length 2(N - 1),andthe overallparallelismis characterizedby (N +1)/2,whichis theratiobetweenthenumber \n(N +1)(N - 1) of DDG nodes and thelengthofthe critical path. Allnodeswiththesametimestamp areindependent \nand canbe executedinparallel.However,this methodforpartitioningofDDG nodes cannotbe usedto uncoverthegroupsofindependent \nopera\u00adtions neededto characterize the vectorizability ofthe computation. Considertheexamplein Listing1.StatementS2haslargevector\u00adizationpotential:foraparticular \n.xed valueof j,all N run-time statementinstances forvariousvaluesof i areindependent(and, as discussedlater,they \nexhibit apatternof contiguous access).How\u00adever,ifone weretoconsidertheinstancesofS2thatarepartitioned \nbasedonthesame timestampinFig.1(a),thosepartitionsuncover less parallelism for S2 than what is truly \navailable in the DDG: rather than having N - 1 partitions ofsizeN,wehaveatotal of 2(N - 1) partitions.Further,theoperationsin \neachpartitiondo not access contiguous memory locations, i.e., are not potentially vectorizable. As describedlater,weproposea \nnewformoftimestamp com\u00adputationandcriticalpathanalysisthat focusesonallinstances of a speci.c statement \n(e.g., S2 in thisexample).Thisanalysisconsid\u00aderswhether twoinstancesofthestatementofinterestareconnected \nbyapathintheDDG(withanyinstancesof otherstatementsalong thepath).If suchapath exists, ouralgorithm guaranteesthatthe \ntimestamp of the .rst node is smaller than the timestamp of the second node(i.e.,thetwo nodeswillbeplacedindifferentparti\u00adtions). \nFurthermore, each node is guaranteed to have the earliest possible timestamp.Forthe DDG fromFig.1(a),our \ntimestamps areshowninFig.1(b).Notethatallinstances ofS2for j=1 are nowgiventimestamp1, becausetheydo \nnot depend on any other instancesofS2.Ingeneral,allinstancesofS2fora particularvalue of j havethesame \ntimestampandformapartition.Asdescribed later,thesepartitions(containingindependent operations) are then \nsubjectedto an analysisforcontiguous memory accesspatterns. Thekeyprobleminthisexampleisthatthe parallelismanaly\u00adsisinterleavestheinstancesofS1andS2.Analternative \napproach couldbetoseparately considertheloopsinListing1,and perform loop-levelparallelismanalysisusinganapproach \nbasedonworkby Larus[14].Thistechniquetracksinter-iterationdependences and A[1] A[2] A[3] A[4] A[5]  \nA[5]  A[1] B[1] B[1] B[2] B[3] B[4] B[5] (a) B[2] A[3]  A[4] B[1] B[2] B[3] B[4] B[5] B[4] B[5] A[1] \nA[2] A[3] A[4] A[5] (c) (b) Figure2:DependencesforExample 2. computes timestampsforall statementinstancesinallloop \nitera\u00adtions.Insidealoopiteration,thestatementinstancesareexecuted sequentially. When a statement instance \ns in loop iteration i de\u00adpends onastatementinstance s . in another iteration, the execution of i stops \nupon reaching s, until s . is executed.With this approach, thesecondloopinListing1willbe consideredindependently,and \ntheanalysiswilluncoverthatanytwoiterationsofthe i loopatline 4areindependent.Inessence,this willcreatetheparallelpartitions \nshowninFig.1(b). However,this approachfor uncoveringloop-levelparallelismis alsoinappropriatefor ourtarget \ngoaltodiscoverindependent oper\u00adationsthat canbevectorized.Thecodein Listing2illustratesthis point.Thereisaloop-carried \ndependence fromS2 toS1,as illus\u00adtratedinFig. 2(a).As a result,theloop-levelparallelismidenti.ed bytheanalysiswouldbeoftheform \nshowninFig.2(b).Theresult\u00adingpartitionsdo notexposethehighvectorization potentialofS1 (orS2).However,itis \neasytoseethatthe computation canbesplit intotwoseparateloops: .rst,aloopthatiteratesoverallinstances \nofS2,followedby anotherloopthat iteratesoverallinstancesof S1.Theloop-levelparallelismanalysiscan easilyuncoverthat \neach loop(inthis hypotheticaltransformedversion)isfullyparallel;in fact,eachloopisfullyvectorizable.However,sincetheunitofanal\u00adysisistheoriginalloop \ncode,the potentialfor parallelism/vector\u00adizationis notdiscovered.Ourapproach,whenanalyzingtheDDG inFig.2(a),will.rst \nconsiderallinstancesofS1,willdiscoverthat theyareallindependent,andwillformapartitioncontaining allof \nthem.Similarly,allinstancesofS2 willbeputinasinglepartition. Theresult of ourtechniqueis illustratedinFig.2(c).Comparing \nwithFig.2(b),itisclearthat weuncovermoreparallelism,which inturnleadsto.nding morepotentialvectorization. \nTo summarize, this technique characterizes the parallelism of an entire loop. However, this characterization \nis constrained by the order of operations within the loop body that is imposed by the sequential execution \nof the original program. Thus the poten\u00adtialforincreasedparallelismvia dependence-preservingreordering \nof operations maybemissed.Ourapproach considersallpossible dependence-preserving reorderings of all run-time \ninstances of a speci.c statementofinterest, whichexposesthenecessary paral\u00adlelismforthe purposesofvectorization. \n 2.2 Finding Operations that Access Contiguously Located Data Elements Consider againExample1from above, \nand speci.callythetimes\u00adtampsshowninFig.1(b).Eachtimestampde.nesapartitioncon\u00adtaining N instances ofS2that \nareindependent of each other.Fur\u00adthermore,alltheseinstances accesscontiguousregionsof memory.  For a \n.xed j and varying i, the triples of memory addresses cor\u00adresponding to the triple of expressions (B[j][i], \nB[j - 1][i], A[i]) exhibit apatternof contiguous memory accesswiththerow-major datalayout usedforarraysinC.Thismakesthestatementinstances \nwithin eachpartitionviable candidatesforvectorization. We propose the .rst analysis to analyze contiguous \nmemory accesses of independent operations in order to characterize vec\u00adtorization potential. As described \nlater, the analysis considers all statement instances withinasingleparallelpartition,andde.nes subpartitions \nsuch that within a subpartition, the tuples of mem\u00adory accesses follow the same pattern of contiguous \nmemory ac\u00adcesses. For example, tuple (B[j][i], B[j - 1][i], A[i]) for a .xed j and varying i will produce \ntuples of run-time addresses of the form (c1 + i \u00d7 d, c2 + i \u00d7 d, c3 + i \u00d7 d).Here d is the size an array \nelement and c1,2,3 arebase addresses.Thesetuples represent accessestocontiguous memory,andtogetherform \nonesinglesub\u00adpartition(whichcovers theentireparallelpartitionde.nedbythis particular value ofj). One \ncanimagine all statementinstancesin the subpartitionbeing combined intoasinglevector operation [c1 : \nc1 +(N -1)\u00d7d]=[c2 : c2 +(N -1)\u00d7d].[c3 : c3 +(N -1)\u00d7d] where . represents a vector operation on vectors \nof size N.Our analysis computes such subpartitions andusesthemto characterize thevectorizationpotentialofthe \nanalyzedstatement. 3. Analyzing Dynamic Data-Dependence Graphs for Vectorization Potential DDG Generation \nGeneratinga dynamicdata-dependencegraph (DDG) requires an execution trace of the program (or a con\u00adtiguous \nsubtrace),containingrun-timeinstancesofstaticinstruc\u00adtions, including any relevant run-time data such \nas memory ad\u00addresses for loads/stores,procedure calls,etc.Our implementation usesLLVM[15]toinstrument \narbitraryC/C++/Fortrancode.The Clang[4]front-endis usedto compileC/C++codeintoLLVM IR,andtheDragonEgg[5]GCCpluginis \nusedtocompile Fortran 77/90 codeintoLLVMIR.TheLLVMIRisinstrumentedto gener\u00adatea run-timetracetodisk,andtheinstrumented \ncodeiscompiled to native code. Once an execution trace is available, the construction of the DDG createsagraph \nnodefor each dynamicinstructioninstance. Edgesare created between pairsofdependent nodes(i.e., onein\u00adstruction \ninstance consumes a value produced by the other). In ourimplementation eachgraph noderepresentsa dynamicinstance \nof anLLVMIRinstruction, anddependences aretrackedthrough memoryandLLVMvirtualregisters.To constructthegraphedges, \nbookkeepinginformationfor each memory/registerremembersthe graph nodethat performedthelastwritetothislocation.Notethat \nthegraphrepresentsonly.owdependences.Anti-dependencesand output dependences are not considered, since \nthey do not repre\u00adsent essential features of the computation, and could potentially be eliminated via \ntransformations such as scalar/array expansion. Controldependences arealso not considered,since our goalisto \nfocus on thedata .ow and the optimizationpotential impliedby it.Itis straightforwardtoaugmenttheDDG withadditional \ncate\u00adgoriesofdependences, withouthavingto modify inany waythe subsequent graphanalyses(describedbelow). \nOneinterestingcasethatarisesinpracticeis duetoreductions, forexample,because ofastatement s+=a[i] in \nan i-loop.The instancesof suchastatementwouldformachainofdependencesin theDDG.However,sometimesitispossibletovectorizereductions \n(e.g., by updatinga vector sv instead of a scalars). Our analysis currentlydoes not considerthe potentialfor \nsuchvectorization.A possibleenhancementistoidentify andremove dependenceedges that are due to updates \nof reduction variables; detection of such Algorithm 1: Timestamp computation. Input:id:static instructionID, \ngraph:DDG 1 foreach Node node in TopologicalOrder(graph) do 2 TS . 0; 3 foreach pred in Predecessors(graph, \nnode) do 4 TS . Max(TS, GetTimeStamp(pred)); 5 end 6 if GetInstructionID(node) == id then 7 TS . TS \n+1; 8 end 9 AssignTimeStamp(node, TS);  10 end dependences hasalreadybeen usedbypriorwork[22]inadifferent \ncontext. Candidate Instructions The execution trace may contain many instructions that should not be \nanalyzed for SIMD parallelism, such asinteger operationsforloop book-keeping.Hence,theanal\u00adysis is restrictedto \ninstructions thatinvolve .oating-pointaddition, subtraction, multiplication,anddivision.Theseinstructionscorre\u00adspondtothesetof.oating-pointinstructionsthathavevector \ncoun\u00adterpartsinSIMD architectures.They are also of particularimpor\u00adtanceforoptimizationof certaincomputationally-intensive \nappli\u00adcations(asexempli.edbytheSPEC.oating-point benchmarks). Ofcourse,allotherinstructionsthatparticipateindependencesare \ntakeninto accountbytheanalysis,buttheirpotentialSIMDparal\u00adlelismis not characterized. 3.1 Generation \nof Parallel Partitions To bene.t from SIMD parallelism, an instruction must exhibit .ne-grained parallelism. \nFor a static instruction s that is being characterized,the potentialparallelismof {s1,s2,...} (where \nsk is the k-th run-timeinstance of s)can be uncoveredbyobserving the data dependences from any si to \nany sj for j>i.This is equivalent to identifying whether the DDG contains a path from the nodeforsi tothe \nnodeforsj.Such apathexistsifand only if thedataproduced by si isdirectlyorindirectlyusedby sj (i.e., \nsi and sj cannot be executed concurrently). Each candidate static instruction s (asdescribed earlier)is \nan\u00adalyzedindependently,usingAlgorithm1.AuniqueIDfor s is as\u00adsignedatinstrumentationtime.A topological \nsorttraversalof the DDGis performed andatimestampis assignedto each node.For eachvisitednode,thelargestpredecessortimestampisdetermined. \nIfthe nodeis aninstanceofthestaticinstructions being analyzed, the timestampisincrementedby one; otherwise,itisnot \naltered. The generated timestamps are then used to construct parti\u00adtions withinthegraph,byputting allinstancesof \ns with the same timestampintothesamepartition.Anexample illustratingthisap\u00adproachwasshown earlierinSection2.Considerthe \nDDG shown inFig.1(a).The nodesatthetop of theDDG representtherun\u00adtimeinstancesofS1,whiletherestoftheDDGnodesareinstances \nofS2.Supposewewantedtoevaluatethe potentialvectorizability ofS2.Forthis staticinstruction,theanalysiswillcompute \ntimes\u00adtampsforS2instancesasshowninFig.1(b).Eachtimestampvalue de.nes onepartition ofS2instances. PROPERTY \n3.1. Consider any DDG node si and a DDG path p ending at si.Let s(p) be the number of nodes on p (excluding \nsi) that are instances of the static instruction s being analyzed. The timestamp computed for si by Algorithm \n1 is the largest value of s(p) for all p leading to si. Theproofisbyinduction onthelengthofp.Thispropertyhas \ntwoimplications.First,consider twoinstances si and sj of s.If  there exists a run-timedependencefromsi \nto sj (eitherdirectly, or indirectlythroughinstancesofinstructions otherthans, orthrough other instances \nof s itself), then the timestamp of si is strictly smaller than the timestamp of sj. Thus, all instances \nof s with the same timestamp (i.e., in the same partition) are independent of each other. Second, each \nsi is assigned the smallest possible timestamp that is, it is scheduled at the earliest possible time. \nConsidering the average partition size as a metric of available parallelism for the instances of s, the \nfollowing property can be proven easily. PROPERTY 3.2. Algorithm 1 .nds the maximum available paral\u00ad \nlelism for each static instruction s.  3.2 Partitioning for Contiguous Access Algorithm1ensuresthattheinstancesofs \nwithinapartitionare in\u00addependent,butef.cientSIMD parallelismalsorequirescontiguous memory access. On \nmostSIMD architectures,the costofloading vectorelementsindividuallyandshuf.ingthemintoavectorregis\u00adter \noffsets thebene.t ofexploitingthe vectorhardware. Thus, the partitions must be further subdivided into \nunits that exhibitparallelism and contiguous memory access.In general, we want to ensure unit-stride \naccesses the distance in memory be\u00adtween consecutive memory accessesis equaltothesizeofthedata type.Wealsoallowzero-stride \n(i.e.,thedistanceinmemorybe\u00adtween consecutive accessesis zero),sincevectorsplats(copyinga scalarvalueintoallelementsofavector) \nare cheap formostSIMD architectures.Notethatthe zero-stride casealsocovers operations withconstant operands. \nToensureunit-stride,theinstructioninstances withinaparallel partition are sorted according to the memory \naddresses of their operands. For constants or values produced by other instructions but not savedto memory, \nan arti.cial addressofzeroisused.The sorted lististhenscanned, ending the current subpartition(and starting \nanewone)whenthestrideis(1) non-zeroand non-unit, or(2)differentfromthepreviouslyobserved stride.Theresultisa \n(now potentially larger) set of subpartitions such that the dynamic instruction instances within a subpartition \nare independent, and exhibit uniform zero-stride or unit-stride accessestomemory.The average size of \nthese subpartitions is a metric of the vectorization potentialofthestaticinstructionbeing characterized. \n 3.3 Non-Unit Constant Stride Access Thecontiguous accesscheckperformedinthepreviousstageisim\u00adportantfordiscoveringef.cientvectorizationpotential,butavaria\u00adtionofthe \ncheck canbeusedtoexplorethepotentialbene.tofdata layouttransformations ontheoriginalcode.Itis not uncommonto \n.nd computations where .ne-grained concurrency exists but the data accessedhasa non-unitbut constantstride.Inthe.rstloopin \nListing3thereisaloop-carried dependencealongtheinnerj loop but the i loop is parallel. If the loops were \npermuted, we would have .ne-grainedparallelismintheinnerloopfortheinstances of S1,butthe accessstridewouldbeN.Adatalayouttransformation \ntotransposethe array(i.e.,swapthe.rstandsecond arraydimen\u00adsions) would enable unit-stride access and \nef.cient vectorization. Listing4showshowthecode couldbetransformed. Thesecondloopin Listing3illustratesa \nscenario witharrays ofstructuresthatresultsin.ne-grainedconcurrent operationsbut non-unit accessstride.TheinstancesofS2 \nareallindependentbut they exhibitstride-2 access (e.g.,8bytesifx and y are single\u00adprecision .oating-point).Thesameistruefortheinstances \nofS3. Inthiscase, changingthedatastructure fromanarrayof structures toastructureof arrayswouldenableparallel,stride-1 \naccesswhich couldbe automatically vectorized. By relaxingtheunit/zero-strideconditiontoinstead checkfor \nany non-unit constant stride,we can detect cases such asthe ones 1 for( i=0; i<N; i++) 2 for(j=2 ;j<N \n;j++) 3 A[i][j] = 2*A[i][j-1] -A[i][j-2]; // S1 4 5 for ( i=0; i<N; i++) { 6 C[i].x = B[i].x + B[i].y; \n// S2 7 C[i].y = B[i].x -B[i].y; // S3 8 } Listing3:Vectorizationbene.tsof datalayouttransforma\u00adtions:stride-N \ncolumn access and array-of-structures access. 1 // transposed declarations for A, B, and C 2 for( j=2; \nj<N; j++) 3 for(i=0 ;i<N ;i++) 4 A[j][i] = 2*A[j-1][i] -A[j-2][i]; // S1 5 6 for ( i=0; i<N; i++) { 7 \nC.x[i] = B.x[i] + B.y[i]; // S2 8 C.y[i] = B.x[i] -B.y[i]; // S3 9 } Listing4:Looptransformations anddatalayouttransforma\u00adtionsappliedtoListing3. \nillustrated in Listing 3. Given the partitions produced by Algo\u00adrithm1,weapply the unit-stride analysisfromtheprevious \nsub\u00adsection.Attheend, anyinstructioninstancethatbelongstoasub\u00adpartitionofsizeoneisidenti.ed.All suchinstances(of \nthe same static instruction, and with the same timestamp) are then sorted and scanned. When the currently \nobserved stride does not match thepreviously observed one,the instructionisput on a waitlistfor futureprocessing,andthescanningbasedonthecurrentstride \ncon\u00adtinues untiltheendofthelistisreached;thisresultsin one subpar\u00adtition.Anywaitlistedinstructionsarethentraversedagain,in \nsorted order, so that the next subpartition can be formed. 4. Evaluation Inthissectionwepresent anumberof \nstudiesto illustratetheuse of the dynamic analysis tool. Although the studies are restricted to analysisof \nsequentialprograms,thetool canalsobe usedwith parallelprograms usingPthreads,OpenMP,MPI, etc. theinstru\u00admentation \nand trace generation would be applied to one or more sequential processes or threads of the parallel \nprogram to assess thepotentialfor SIMD vectorparallelismwithinaprocess/thread. Further, although we only \nconcentrate on characterizing .oating\u00adpoint operations (becausetheytendtobethefocusof mostSIMD optimizationefforts),suchanalysis \ncanbe carriedoutforanytype of operations, e.g., integer arithmetic. The experiments were performed on \na machine with an In\u00adtel Xeon E5630 processor and 12GB memory, running Linux 2.6.32. To obtain performance \nmeasurements, the Intel icc com\u00adpiler(12.1.3)wasusedtocompiletheprogram code,atthe O3 op\u00adtimizationlevel.Pro.lingdatawasobtained \nwith HPCToolkit[10] version5.2.1,at samplingperiod500thousandcycles.Theinstru\u00admentationinfrastructure \nwasimplementedinLLVM3.0. 4.1 Characterization of Applications We illustrate the use of the tool for \ncharacterizing software col\u00adlectionsby applyingittotheSPECCFP2006.oating-point bench\u00admarks, andkernels \nfromtheUTDSPbenchmark suite[32].Wealso includetwostand-alone computekernels:a2-DGauss-Seidelsten-cilcodeandakernelfroma2-DPDEgrid-basedsolver;they \nare elaboratedupon lateras casestudies illustrating potentialuseofthe analysis forperformance optimization \nand compiler enhancement.  Unit Stride Non-unit Stride Percent Percent Average Percent Average Percent \nAverage Benchmark Loop Cycles Packed Concur. Vec. Ops Vec. Size Vec. Ops Vec. Size 410.bwaves block \nsolver.f : 55 block solver.f : 176 79.2% 65.8% 53.0% 66.4% 39.9 8.3 97.5% 100.0% 11.1 5.0 0.0% 0.0% 433.milc \ngauge stuff.c : 258 path product.c : 49 quark stuff.c : 566 quark stuff.c : 960 quark stuff.c : 973 quark \nstuff.c : 1452 quark stuff.c : 1460 quark stuff.c : 1523 22.0% 17.9% 15.2% 44.9% 35.0% 14.2% 13.6% 15.4% \n0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 10453.4 73316.6 23687.7 11447.3 61566.7 20736.0 20736.0 2921.1 \n36.2% 36.4% 88.3% 65.1% 57.4% 36.4% 36.4% 55.0% 10427.4 69441.5 11.4 15.5 13.8 20736.0 20736.0 2000.0 \n49.7% 63.6% 7.5% 18.7% 32.9% 63.6% 63.6% 45.0% 3.3 3.2 4.2 2.3 2.4 502.3 20736.0 4.2 434.zeusmp advx3.f \n: 637 11.3% 35.0% 66613.9 74.3% 442.1 16.6% 16.0 435.gromacs innerf.f : 3960 ns.c : 1264 ns.c : 1461 \nns.c : 1503 60.4% 16.2% 35.3% 13.3% 4.4% 3.8% 3.3% 0.0% 4.0 4.9 40.3 5.0 60.3% 60.0% 64.1% 62.5% 12.0 \n42.0 31.0 5.0 21.5% 20.9% 35.1% 30.0% 2.0 2.1 2.0 2.0 436.cactusADM StaggeredLeapfrog2.F : StaggeredLeapfrog2.F \n: 342 366 18.4% 81.1% 100.0% 96.9% 80.0 78.0 100.0% 100.0% 80.0 78.0 0.0% 0.0% tml.f : 522 15.6% 98.5% \n8805.5 100.0% 158.3 0.0% 437.leslie3d tml.f : 889 tml.f : 1269 13.4% 12.4% 99.2% 99.2% 7434.2 438.3 99.9% \n100.0% 178.4 22.0 0.0% 0.0% tml.f : 3569 21.6% 98.6% 8100.0 100.0% 90.0 0.0% 444.namd ComputeList.C : \n71 ComputeList.C : 75 ComputeNonbondedBase.h : 321 33.2% 66.4% 12.9% 0.0% 0.0% 0.0% 130.2 313.3 15.6 \n86.0% 93.3% 85.9% 101.1 295.4 262.8 13.7% 6.6% 7.8% 11.4 7.8 5.4  mapping q1.cc : 514 step-14.cc : 715 \n447.dealII step-14.cc : 780 step-14.cc : 3198 10.4% 16.2% 10.9% 19.1% 0.0% 0.0% 0.0% 3.1% 1.0 130.9 \n27.0 335.6 0.0% 75.6% 66.7% 87.5% 0.0% 58.2 24.3% 24.9 27.0 33.3% 27.0 12.5 12.5% 18.8 18.5 56.2% 18.2 \n6.5 39.4% 2.0 25.6 3.5% 2.1 15.9 5.0% 2.1 40.0% 2.0 ssvector.cc : 983 slufactor.cc : 839 450.soplex \nspxsolve.cc : 126 spxsolve.cc : 200 svector.h : 293 10.6% 12.5% 37.7% 58.5% 12.8% 0.0% 0.0% 0.0% 0.0% \n0.0% 373.0 59.7 384.3 361.6 1.7 32.2% 43.2% 92.3% 88.2% 0.0% bbox.cpp : 894 53.3% 0.2% 11.2 62.6% 14.8 \n27.3% 2.7 csg.cpp : 248 58.6% 0.0% 4.3 35.5% 4.8 41.1% 2.0 csg.cpp : 254 16.2% 0.2% 1.0 0.0% 0.0% lbuffer.cpp \n: 1373 23.0% 0.1% 14.8 63.8% 16.4 29.8% 2.9 453.povray lighting.cpp : 600 66.9% 1.0% 13.1 65.4% 13.9 \n28.1% 2.0 lighting.cpp : 938 27.1% 0.1% 13.7 63.3% 15.7 29.7% 2.8 lighting.cpp : 2298 31.5% 1.0% 7.7 \n59.3% 16.2 26.5% 2.9 lighting.cpp : 4120 41.7% 0.8% 11.4 64.8% 13.1 25.0% 2.1 Chv update.c : 736 e c3d.f \n: 675 FrontMtx update.c : 207 454.calculix FrontMtx update.c : 38 mafillsm.f : 144 Utilities DV.c : \n1241   13.6% 69.7% 16.4% 14.0% 74.7% 11.4% 91.5% 0.1% 91.3% 91.2% 0.4% 96.6% 27.4 35.6 774.0 1116.3 \n6064.8 2.0 48.4% 100.0% 96.4% 96.7% 99.2% 50.0% 15.0 51.6% 11.4 12.3 0.0% 28.6 3.1% 9.4 12.9 2.6% 4.7 \n136.9 0.8% 3.1 49.0 0.0%  NFT.F90 : 1068 17.4% 0.0% 24.2 69.9% 9.9 19.3% 2.1 459.GemsFDTD update.F90 \n: 108 17.3% 97.4% 201.0 100.0% 201.0 0.0% update.F90 : 242 17.1% 97.3% 200.0 100.0% 200.0 0.0% mol.F90 \n: 5565 15.7% 80.4% 50779.4 99.2% 150.7 0.3% 2.4 465.tonto mol.F90 : 11659 59.0% 19.5% 266.6 97.2% 31.6 \n1.0% 4.4 470.lbm lbm.c : 186 99.6% 100.0% 137487.0 61.6% 137487.0 38.4% 72.1 481.wrf solve em.F90 : 179 \nsolve em.F90 : 884 solve em.F90 : 1258 87.9% 14.4% 14.8% 79.1% 89.3% 89.6% 1198.6 54721.8 9887.1 97.4% \n99.8% 93.6% 39.7 117.0 89.1 1.2% 0.2% 6.4% 15.1 29.1 28.5 solve em.F90 : 1538 12.8% 87.4% 95531.4 95.4% \n27.6 4.6% 7.6 approx cont mgau.c : 279 cont mgau.c : 652 482.sphinx3 subvq.c : 456 vector.c : 521  \n 39.8% 17.0% 30.8% 25.9% 68.1% 72.8% 75.0% 86.1% 8949.0 3.7 19154.8 3.3 75.2% 75.0% 75.5% 75.0% 6886.1 \n24.8% 2.3 39.0 0.0% 15360.0 24.5% 2048.0 13.0 0.0%  Table1:Analysisresultsforanalyzed benchmarkloops. \nThestand-alonekernelsandUTDSPbenchmarksweredirectly benchmarks,the outputproducedbythe dynamicanalysiswillbe \nanalyzedbythetoolandtheresultsareshowninTables2and3.For veryextensive.Therefore, we only analyze andreport \ncharacter\u00adfullapplicationcodes, suchastheSPECCFP2006.oating-point isticsfortime-consumingloopsidenti.edviapro.lingbyHPC\u00ad \n Unit Stride Non-unit Stride Benchmark Percent Packed Average Concur. Percent Vec. Ops Average Vec. Size \nPercent Vec. Ops Average Vec. Size 2-DGauss-Seidel Stencil 0.0% 226 22.2% 46.1 77.4% 9.3 2-D PDE GridSolver \n0.0% 231426 100.0% 820.8 0.0% Table 2:Analysis results for computation kernels. Benchmark FFT FIR IIR \nType Array Pointer Array Pointer Array Pointer Percent Packed 49.9% 0.0% 99.8% 0.0% 0.00% 0.00% Average \nConcur. 568.9 568.9 99.9 99.9 43.6 43.6 Unit Stride Percent Average Vec. Ops Vec. Size 79.3% 24.1 79.3% \n24.1 100.0% 57.4 100.0% 57.4 64.8% 14.3 64.8% 14.3 Non-unit Stride Percent Average Vec. Ops Vec. Size \n12.2% 2.0 12.2% 2.0 0.0% 0.0% 15.6% 8.9 15.6% 8.9 Array 7.8% 7.4 74.6% 23.9 0.0% LATNRM Pointer 8.2% \n7.4 74.6% 23.9 0.0% Array 0.0% 2.7 48.3% 22.1 16.5% 21.8 LMSFIR Pointer 0.0% 2.8 49.4% 28.0 16.2% 21.9 \nArray 50.4% 181.9 100.0% 18.2 0.0% MULT Pointer 0.00% 181.9 100.0% 18.2 0.0% Table3:AnalysisresultsforUTDSPbenchmarksuite. \nToolkit,analyzingonlyloopsthat accountforatleast10%oftotal execution cycles during a run of the benchmark \nusing the SPEC referencedataset(the 10%threshold was selectedtoreducethe amount of data presented; we \nalso collected data at a threshold of 5%).To performtheDDG analysisfora particularloop,we col\u00adlectedseveralsubtraces \ncorrespondingto separateinstancesofthe loop(usingtheSPECtrain data set, andforafewlargeloopsthe test \ndataset).Asubtracewasstarted uponloop entryandterminated uponloopexitanditsDDGwasconstructed and analyzed.Weran\u00addomly \nchose several instances of the loop, analyzed each corre\u00adsponding subtracetoobtainthevariousmetricsdescribedlater,and \nchose one representative subtrace to be included in the measure\u00admentspresentedinthepaper. The results \nof the analysis for SPEC CFP2006 are shown in Table1.For each benchmark, we onlyshowloopsthat accountfor \natleast10% of total executioncycles.We start withallinnermost loops, and only include a parent loop if \nthe total percentage of executioncyclesspentinitis atleast10percentage pointsgreater thanthesum ofthepercentagesforitsinnerloops.The \ngamess benchmark couldnotbe compiled withLLVMandwas not usedin the experiments. The Percent Packed metric \nshows the percentage of .oating\u00adpointrun-time operations thatwereexecuted using packed(i.e., vector)SSEinstructions,asreportedbyHPCToolkit.Thiscolumn \nprovidesinformationontheeffectivenessofcurrent compilers(we usedInteliccsincewehavefoundittobe superiortootherpro\u00adductioncompilersinvectorization \ncapability)invectorizing each oftheidenti.ed hotloops.Ahighvalueindicatesthatasigni.cant fractionofthe.oating-point \noperationsinthatloopwereexecuted viapackedSIMDinstructions.Azerovalueindicatesthatthecom\u00adpilerwas unableto \nachieve any vectorizationat allfortheloop. The Average Concurrency metric was computed by determin\u00adingtheaveragepartitionsizeacross \nthecollectionofpartitionsfor all .oating-point instructions in thegraph, wherepartitions were formedby \nconsidering onlyinstructionindependence(Sect.3.1). For this metric, both singleton and non-singleton \npartitions were considered. From the non-singleton parallel partitions, subparti\u00adtions containing unit-stride \noperationswereformed(asdescribed inSect.3.2).ThePercent Vec. Ops metric (i.e.,potentiallyvector\u00adizablerun-timeinstructions)showsthe \nnumberof operationsthat belong to non-singleton unit-stride subpartitions,aspercentofthe totalnumberofoperationsinthegraph.TheAverage \nVec. Size met\u00adric represents the average size of these non-singleton vectorizable subpartitions.The generaltrendis \nthat for mostofthe loops, many run-time instructions belong to partitions that exhibit both inde\u00adpendence \nand contiguous memory access. Furthermore, the sizes ofthesepartitionsare large inmany cases,muchlarger \nthan the vectorsizesinexisting and emerging architectures.Thus,the anal\u00adysisindicatesthatthemajorityoftheanalyzedloops \nmayhavehigh vectorizationpotential. Therun-timeinstructionswithina non-singleton parallelparti\u00adtionthatdidnotbelonginanyunit-stride \nsubpartitionwerefurther analyzedwiththe non-unit strideanalysisdescribedinSection3.3. The analysis reported \nthe number of such instructions that could be placed in subpartitions accessing data at some .xed non-unit \nstride.This number, as percentof thetotalnumberof all run-time instructions in thegraph, is shownincolumn \nPercent Vec. Ops.The averagesizeofsuchsubpartitionsisgiveninthe last column.There areseveralexampleswhereasigni.cant \nnumberofindependentin\u00adstructions canbe combinedtogetherusing non-unit stride,andthe sizesofthepartitionsarelargeas \nwell.Thisindicatesthatdatalay\u00adouttransformations maybebene.cialinthese cases. There may be cases where \nthe percentage of packed instruc\u00adtions observed via pro.ling exceeds the sum of the values in columns \nPercent Vec. Ops (e.g., Utilities DV.c:1241 in 454.calculix and vector.c:521 in 482.sphinx3).This happens \ninthepresenceof a reduction(e.g., s+=expr):our analysis con\u00adsidersthechainof dependences andtreatsthe \ncomputationas non\u00advectorizable.However,thereexist approachestovectorizereduc\u00adtions,andiccemploys someofthem.Infuturework, \nour approach couldbeextendedtoignoredependences duetoreductions,which would uncoverthese additional vectorization \nopportunities. Asistypicalofotherworkon dynamicanalysisof.ne-grained dependences(e.g.,[14, 36]),theinstrumentationincursanover\u00adheadoftwotothreeordersof \nmagnitude,relativetotheexecution time of the original unmodi.ed code. The cost of DDG analysis depends \non graph size and memory access patterns, and is typi\u00adcally of the order of tens to hundreds of microseconds \nper DDG node.Although wehave not focusedontool optimization, theutil\u00adityofthecurrent unoptimizedimplementationofourprototypeis \nnot hamperedfortworeasons.First,the analysisisintendedtobe performedof.ine,e.g., duringperformancetuning. \nManypro.ling analyses have been successfully used in this setting, and various existingtechniques canbereadilyappliedtoreducetheir \ncost(e.g., [36, 38]). Second, the instrumentedcode can be run with much smallerproblemsizesthanthe production \nsize:inourexperience, although metricssuchasaveragevectorsize canvarywithproblem size, the qualitative \ninsights about potential vectorizability do not change.   4.2 Assisting Vectorization Experts Many \ninstitutions possess large code bases that were largely de\u00adveloped before the recent emergence of SIMD \nparallelism in all CPUs/GPUs. When the original developers of the code are not availabletoadaptitforimproved \nvectorization, an automatedtool canbevery valuable.For someloops,aquick scanofthecodeofa hotloopbyavectorizationexpert \nwillimmediately revealthe op\u00adportunitiesfor enhancingvectorizability through code changes. But thisiscertainlynotthenorm,especially \nwithC++codesorCcodes that make heavyuseofpointers.An automatedtoolallowsthevec\u00adtorizationexperttoquickly \neliminateloopswith littletonovector\u00adizationpotential, andconcentrateontheloopswithhighpotential. With \nthese, some ofthe code structures involve multiple levels of function calls andthe outputfromthetoolisvaluableinputtothe \nexpert,indicatingthattheeffortto unravelthecodeislikely worth it.As anexample,the hotloopsin444.namd \naregenerated using Cpreprocessormacros anditisverydif.culttoget an understand\u00adingofthe codejustby scanningit.If \nweexaminetheHPCToolkit pro.le data,we knowtheloops are hot,but notwhetheror notwe have any hopeofvectorizing \nthem.However, our analysis shows thatthereisahigh potentialforvectorizationinthispart of code, soitmaybeworththe \ntimeinvestment of avectorizationexpertto carefullyanalyzetheseloops. Anotheruse caseisfor identifying \nmissed opportunitiesincom\u00adpilertestsuites.Vendor compilers aretypicallytested againstlarge amountsofcodetogaugetheperformanceofthecompiler \nsvector-izer.Itis easytoautomatethistestingtoseehowmuchofthe code isvectorized,butfortheremainingcode,itis \nnotclearwhetherthe codeisjust notvectorizable,orifthe compilerismissing an oppor\u00adtunity.Itwould takeconsiderableeffort \nforavectorizationexpert to manuallyanalyzeallofthe non-vectorized code.The analysistool can help to automatetheprocess \nand focustheexpert seffort on identifyingwhy code thathasbeen identi.ed asbeingpotentially vectorizableis \nnot actuallybeing vectorizedbythecompiler. 4.3 Array-Based vs. Pointer-Based Code Auto-vectorizingcompilersarebecomingincreasinglygoodatvec\u00adtorizingarray-basedcode,butpointer-based \ncodeisoftennotvec\u00adtorizeddue totheadded complexitieswithpointeraliasing andthe veri.cationof contiguous \naccess duringthe compiler sstatic anal\u00adysis.Aprimarybene.toftheproposed dynamicanalysistechnique istheability \ntoanalyzepointerbased codejustaseasilyasarray basedcode.Bothversions ofthesamecomputationwillprovidethe \nsame analysisresults,sincethedynamicanalysisconsidersIR-level arithmeticoperations,anddoesnotmakeadistinctionbetweendata \nthatisreadfromarrays orpointer dereferencing. Totestthisfacet of our analysis,weusedthe UTDSP[32] benchmarksuite,which \ncontainsbotharray-andpointer-basedver\u00adsions of several computationkernelsfordigital signalprocessors \n(DSP).The suitewascreated toevaluatethequalityof codegen\u00aderatedbyahigh-levellanguage compiler(e.g.,aC \ncompiler) tar\u00adgetingaprogrammableDSP.Thus, eachkernelwas writtenindif\u00adferentstyles,including an array-basedversion \nandapointer-based version. Both versions provide identical functionality, except for theuseof arrays \norpointerstotraversethe datastructures. Table3showstheresultsofthisexperiment.The measurements includethepercentageofvectorizable \noperationsfoundinthepro\u00adgram,theaveragevectorsize,andthepercentageof operationsthat are actually vectorizedbytheIntelicccompiler.We \nseethat our analysisisinvarianttotheformofthecode1,buticcfails tovec\u00adtorize someofthepointer-based code.Such \nknowledgewouldbe very usefulin optimizing certain applications, where a conversion frompointer-based \ncodetoarray-basedcodemaybeworthwhileif thepotentialbene.tsarehigh.The dynamicanalysisfrom ourtool couldbeavaluable \n.rst stepintheprocess.  4.4 Case Studies Basedontheresultsfromtheprevious subsections,somebench\u00admarksweremanually \ntransformedto enablevectorizationbyicc. Thetargetedbenchmarks werethe Gauss-Seidelstencil,thePDE gridsolver,andthe \n410.bwaves, 433.milc,and 435.gromacs bench\u00admarksfromSPECCFP2006.A comparisonofthe performanceof the original \nand modi.ed versions is shown in Table 4. In addi\u00adtiontotheIntelXeonE5630 machineusedforthemeasurements \npresented earlier, two other machines were used in these experi\u00adments: anIntelCorei72600K andanAMDPhenomII1100T, \nboth with the same icc con.guration. For each benchmark, we show thetotalexecutiontimefor both versions,aswell \nasthe achieved speedup.Whenthetargetof theoptimizationisa particularloop (e.g., bwaves and gromacs), \nthe measurements in the table are basedonthetotal timespentintheloop.Thereference data sets were usedwhenrunningtheSPECbenchmarks. \nGauss-Seidel Inthis casestudy we analyzethevectorization po\u00adtential of a9-pointGauss-Seidelstencil code.Thiscode \nhasbeen identi.edby our analysis as beingnot auto-vectorizedbytheven\u00addorcompiler,butpossessing non-trivialvectorizationpotential(see \nTable2).Listing5showsthe originalkernel.Ithasaloop-carried dependence in the innermost j loop, since \nthe fourth operand A[i][j-1] is produced in the previous iteration of this loop. Similarly, the outer \ni loopalso hasloop-carried dependences.Due tothedependences,iccwas unabletovectorizethe code.Thiswas \nnot unexpected.However,whatsurprised uswasthatthe dynamic analysisrevealedvectorizationpotentialforthis \ncode. The analysis classi.ed two out of the eight addition opera\u00adtions(A[i-1][j-1]+A[i-1][j]+A[i-1][j+1])asvector\u00adizable.Thisisbecausethe \noperands wereallproducedintheprevi\u00adous iteration ofthe i loop.The onlytruedependenceintheloopis 1ThediscrepancyinLMSFIRis \nduetoadifferenceinthewaythetwover\u00adsionsarewritten,resultinginslightlydifferentdistributionsof operations. \n 1 /* Original */ 2 cnst = 1/9.0; 3 for(t=0; t<T ; t++) 4 for(i=1; i<N-1; i++) 5 for(j=1; j<N-1; j++) \n6 A[i][j] = (A[i-1][j-1] + A[i-1][j] + 7 A[i-1][j+1] + A[i][j-1] + 8 A[i][j] + A[i][j+1] + 9 A[i+1][j-1] \n+ A[i+1][j] + 10 A[i+1][j+1]) * cnst; 11 /* Transformed */ 12 cnst = 1/9.0; 13 for(t=0; t<T; t++) 14 \nfor(i=1; i<N-1; i++) { 15 for(j=1; j<N-1; j++) 16 temp[j] = A[i-1][j-1] + A[i-1][j] + 17 A[i-1][j+1] \n+ A[i][j] + 18 A[i][j+1] + A[i+1][j-1] + 19 A[i+1][j] + A[i+1][j+1]; 20 21 for(j=1;j<N-1;j++) 22 A[i][j] \n= cnst * (A[i][j-1] + temp[j]); 23 } Listing5:OriginalandtransformedGauss-Seidel code. dueto A[i][j-1].The \noperations involving elements from row i+1, and even the addition of A[i][j] and A[i][j+1], could beperformedinvectorized \nmodebysplitting theloopintoase\u00adquence oftwoloops, as showninListing5.The .rst j loopin the transformed \ncodeisnowcompletely vectorizedbyicc, resultingin signi.cant performance improvement (see the results \nin Table 4, obtainedfor N = 1000 and T =20). The extent of vectorizability of the Gauss-Seidel code was \na surprise to us since our initial expectation was thatthe code would notexhibitvectorizationpotentialduetoloop-carried \ndependences. Closerexaminationofthe dependences showsthatalltheinforma\u00adtionneededtotransformthe codeis \nactuallyderivablefrom purely static analysis.However,to our knowledge,noresearch orproduc\u00adtioncompiler \ncanperformthetransformation weperformed manu\u00adally.Thisexample illustrateshow thedeveloped dynamicanalysis \ncan be valuable for compiler writers to identify scenarios where enhancementsto staticanalysisandtransformationcapabilitiescan \nenableimproved codetransformationsfor vectorization. 2-D PDE Solver Inthis casestudy we analyzethe core \ncomputa\u00adtionfroma2-DPDEgrid-based solver.The codeisfromtheexam\u00adplesincludedwithPETSc[20]3.1-p7andsolvesthe \nsolidfueligni\u00adtionproblem,modeled asa partialdifferential equation.Theorigi\u00adnalsource canbefound under \n/src/snes/examples/tutorials/ex5.c in thePETScdistribution.We seethatthiscodeis not auto-vectorized byicc,but \nouranalysisshowsveryhigh vectorizationpotential.In thiskernel,the2-Dcomputationgridisdistributed ontoa2-Dgrid \nof blocks, where the computation is performed by iterating over everycell withineveryblock.For our purposes,weconsider \nonly sequential execution ofthe program. Theper-blockkernel code is showninListing6.The if condi\u00adtioninthe \ninnermostloopisaboundary condition check that forces gridpointsonthe boundarytofollowadifferentpath of \nexecution as comparedtotheotherinteriorpoints.Theloop boundsforthis loop nest, along with two of the \nfour conditions that can trigger the if statement, are data dependent. As a result, compilers are forcedtobe \nconservative and assumethat for eachiterationofthe loop,itis unknownwhetherthethen or else clause willbe \nex\u00adecuted. Due to thisconstraint, thevectorizabilityofthisparticular loop nest,aswritten,isverylow.Further, \nwithout more constraints on the values within the info structure, static analysis cannot de\u00adterminetransformationsthat \nwouldenable vectorization. 1 /* Original */ 2 for (j=info->ys; j<info->ys+info->ym; j++) { 3 for (i=info->xs; \ni<info->xs+info->xm; i++) { 4 if (i == 0 || j == 0 || 5 i == info->mx-1 || j == info->my-1) { 6 f[j][i] \n= x[j][i]; 7 } else { 8 u = x[j][i]; 9 uxx = (2.0*u-x[j][i-1]-x[j][i+1])*hydhx; 10 uyy = (2.0*u-x[j-1][i]-x[j+1][i])*hxdhy; \n11 f[j][i] = uxx+uyy-sc*PetscExpScalar(u); 12 } 13 } 14 } 15 /* Transformed */ 16 if(info->ys==0 || info->xs==0 \n|| 17 (info->ys+info->ym)==my || 18 (info->xs+info->xm)==mx) { 19 /* Same as lines 2-14 */ 20 } 21 else \n{ 22 for(j=info->ys; j<info->ys+info->ym; j++) { 23 for(i=info->xs; i<info->xs+info->xm; i++) { 24 u \n= x[j][i]; 25 uxx = (2.0*u-x[j][i-1]-x[j][i+1])*hydhx; 26 uyy = (2.0*u-x[j-1][i]-x[j+1][i])*hxdhy; 27 \nf[j][i] = uxx+uyy-sc*PetscExpScalar(u); 28 } 29 } 30 }  Listing6:Original andtransformed2-DPDESolver. \nHowever, the results of the dynamic analysis show a great po\u00adtential for vectorizability within this \ncode (see Table 2). Speci.\u00adcally, the else clause exhibitsperfect vectorizability.To allow a compilertovectorizethisloop,we \ncanrewritethecodetoextract the if/then/else construct and then hoist an if toprovide a vectorizableloop.Themodi.edcodeisshowninListing6.Thekey \nto the vectorization-enablingtransformationisthe observationthat cells which correspond to the boundary \ncondition can only occur on boundaryblocks.We observethat i and j cannot be zero ex\u00adceptwithinblocksalongthetoporleftedgeofthegrid,and \ncannot be equaltothe maximumindexvalue(mx-1 and my-1)except within blocks along the right or bottom edge \nof the grid. There\u00adfore,thekernel code canbesplitintotwoseparateversions; one for boundaryblocks and \noneforinteriorblocks, as showninList\u00ading 6. While this code should provide no speed-up for boundary blocks,it \nenablesvectorizationforinteriorblocks andprovidesan advantagewhentheblocks areinatleasta 3 \u00d7 3 grid. \nTable 4 shows the total execution time for the original and modi.ed codefora casewheretheblocksizeis \n512 \u00d7 512 and blocks are in a grid of size 16 \u00d7 16. As shown in the table, the performance improvement \nis substantial. 410.bwaves In the 410.bwaves benchmark, one of the loops we analyzedin ourextendedstudy(at5% \nthresholdfor hotloops)is at jacobi_lam.f:30.Thisloopexhibitsaverylow percentage ofpackedinstructions,whilethe \ndynamicanalysisshowsthatthere are signi.cant numbers of vectorizable operations at unit and non\u00adunit \nstride. The result from the non-unit stride analysis suggests possibleimprovementsthrough datalayouttransformation. \nListing7isrepresentative ofthecomputationwithinthisloop. Arrays je and jv are of size (5,5,nx,ny,nz), \nandarray q is of size (5,nx,ny,nz) where nx, ny and nz areprogram con\u00adstants.Whilethereisnodependencebetweenthe \niterationsofthe innermost loop i,there aretwofactorsthathindervectorization. First,thereisno unit-stride \ndata access patternsince i is used to accessthethirddimension ofthearrays.Second,theuse of mod Listing7:Original \nandtransformedbwaves code.  1 ish = 1 2 ksh = 1 3 jsh = 1 4 !! Original 5 do k = 1,nz 6 kp1 = mod(k,nz+1-ksh)+ksh \n7 do j=1,ny 8 jp1=mod(j,ny+1-jsh)+jsh 9 do i=1,nx 10 ip1=mod(i,nx+1-ish)+ish 11 !! Some computation 12 \nje(1,1,i,j,k) = ... 13 je(1,2,i,j,k) = ... 14 ... 15 je(4,5,i,j,k) = ... 16 je(5,5,i,j,k) = ... 17 !! \nSome computation 18 ros = q(1,ip1,jp1,kp1) 19 !! Similar computation for jv as for je 20 enddo 21 enddo \n22 enddo 23 !! Transformed 24 do k = 1,nz 25 kp1 = mod(k,nz+1-ksh)+ksh 26 do j=1,ny 27 jp1=mod(j,ny+1-jsh)+jsh \n28 do i=1,nx-1 29 ip1=i+1 30 !! Some computation 31 je(i,1,1,j,k) = ... 32 je(i,1,2,j,k) = ... 33 ... \n34 !! Some computation 35 ros = q(ip1,1,jp1,kp1) 36 !! Similar computation for jv as for je 37 enddo \n38 i=nx 39 ip1=1 40 !! Inner loop computation (lines 11-19) 41 enddo 42 enddo  operations to calculate \nthe neighbor with wrap-around boundary conditions hampersthevectorizationof accessesto array q. To address \nthese problems, a data layout transformation was performed on arrays je, jv,and q: the dimension which \nwas originally accessed by i (and ip1)was moved to become the fastest varying dimension of the arrays. \nThis transformation is showninListing7.The modi.ed code hasa stride-1 data access pattern. The mod operations \nwere removed by peeling the last iteration ofthe i loop, andintroducingip1=i+1 withintheloop. The value \nof ip1 for the peeled iteration was set to 1.Table 4 showsthe performanceoftheoriginaland modi.ed versions. \n433.milc Inthis casestudyweexplorethe bene.tsofdatalay\u00adout transformations on the 433.milc benchmark. \nSpeci.cally, we focus on one of the loops from Table 1. The loop starting at quark_stuff.c:1452 showsnoautomaticvectorizationbythe \ncompiler.Thereisalso limitedpotentialforvectorizationatunit stride.However,the non-unit strideanalysisshowssigni.cantpo\u00adtentialfor \nvectorization,implyingthat adatalayouttransformation may speed upthe computation. Thisloopiteratesovereverypointinalattice, \napplyingamatrix\u00advector multiplicationoperationat eachpoint.The matrices areof size 3 \u00d7 3 and the vectors \nare of size 3, both containing complex numbers. The lattice itself holds a matrix at each point, and \nex\u00adternalarraysofvectorsare usedforthevectorinputsand outputs. Thematrix-vector multiplicationisnot vectorizedbythecompiler \ndue to non-unit stride access (distributionofreal/imaginary com\u00adponents) anda smallinnerlooptripcount(3). \n1 /* Original data layout */ 2 typedef struct { double r, i; } complex; 3 typedef struct { complex c[3]; \n} su3_vector; 4 typedef struct { complex e[3][3]; } su3_matrix; 5 su3_matrix lattice[NUM_SITES]; 6 su3_vector \nvec[NUM_SITES], out_vec[NUM_SITES]; 7 /* Original computation */ 8 for(s = 0; s < NUM_SITES; ++s) { 9 \nfor(i=0; i<3;++i){ 10 complex x = { 0.0, 0.0 }; 11 for(j = 0; j < 3; ++j) { 12 complex y; 13 y.r = lattice[s].e[i][j].r \n* vec[s].c[j].r \u00ad 14 lattice[s].e[i][j].i * vec[s].c[j].i; 15 y.i = lattice[s].e[i][j].r * vec[s].c[j].i \n+ 16 lattice[s].e[i][j].i * vec[s].c[j].r; 17 x.r += y.r; x.i += y.i; 18 } 19 out_vec[s].c[i] = x; 20 \n} 21 } 22 /* Transformed data layout */ 23 typedef struct { 24 double r[3][3][NUM_SITES]; 25 double i[3][3][NUM_SITES]; \n26 } lattice_dlt; 27 typedef struct { 28 double r[3][NUM_SITES]; 29 double i[3][NUM_SITES]; 30 } vec_dlt; \n31 lattice_dlt lattice; 32 vec_dlt vec, out_vec; 33 /* Transformed computation */ 34 /* Initialize the \nelements of out_vec to 0.0 */ 35 for(i =0;i <3;++i){ 36 for(j=0; j<3;++j){ 37 for(s = 0; s < NUM_SITES; \n++s) { 38 double x_r, x_i; 39 x_r = lattice.r[i][j][s] * vec.r[j][s] \u00ad 40 lattice.i[i][j][s] * vec.i[j][s]; \n41 x_i = lattice.r[i][j][s] * vec.i[j][s] + 42 lattice.i[i][j][s] * vec.r[j][s]; 43 out_vec.r[i][s] += \nx_r; 44 out_vec.i[i][s] += x_i; 45 } 46 } 47 } Listing8:Originalandtransformedmilc code. To help us \nisolate the required changes, we created a version ofthebenchmarkthat onlycontainsthecomputationweidenti.ed. \nThe full benchmark was too large to optimize manually without in-depth understanding of the entire application, \nand the smaller, kernel-izedversionallowedustoshowproof-of-conceptbene.tsof adatalayouttransformation.To \noptimizethis operation,thetrans\u00adformationwasappliedtothelatticedatastructure,where thelattice of matriceswas \nconvertedtoamatrixoflattices.Thismodi.cation exposes unit-stride operationswithintheinnerloop.Theoriginal \nandtransformed code areshowninListing8.Table4demonstrates asigni.cantspeedupforthe modi.edkernel. 435.gromacs \nFor this case study we focus on the loop at line 3960 in innerf.f from 435.gromacs. While icc is not \nable to vectorizethisloopin any signi.cantmanner,the dynamicanalysis resultsindicatetheexistenceofunit-stride \noperations.Lines 1 14 ofListing9representthe computationwithintheloop. Thevalue ofj3 isdata dependent(based \nontherun-timevalues in indirectionarray jjnr)andisusedtoindexintoarrayspos and faction.Thecompilerhas \ntoassume thattheloopisnotparallel, in case two or more elements of jjnr have the same value and thus \ncreate a dependence through faction(j3).Furthermore, the access patternsfor pos and faction are not regular \ndue to the arbitraryvalues of j3.Thus,theloopis notvectorizedbyicc.  1 !! Original 2 do k = nj0,nj1 \n3 jnr = jjnr(k)+1 4 j3 = 3*jnr-2 5 jx1 = pos(j3) 6 ... 7 tx11 = ... 8 fjx1 = faction(j3) -tx11 9 tx21 \n= ... 10 fjx1 = fjx1 -tx21 11 tx31 = ... 12 faction(j3) = fjx1 -tx31 13 ... 14 enddo 15 !! Transformed \n16 do k = nj0,nj1,4 17 do k_vect = 1,4 18 jnr = jjnr(k+k_vect-1)+1 19 vect_j3(k_vect) = 3*jnr-2 20 vect_jx1(k_vect) \n= pos(vect_j3(k_vect)) 21 ... 22 vect_fjx1(k_vect) = faction(vect_j3(k_vect)) 23 ... 24 enddo 25 do k_vect \n= 1,4 26 tx11 = ... 27 vect_fjx1(k_vect) = vect_fjx1(k_vect) -tx11 28 tx21 = ... 29 vect_fjx1(k_vect) \n= vect_fjx1(k_vect) -tx21 30 tx31 = ... 31 vect_fjx1(k_vect) = vect_fjx1(k_vect) -tx31 32 ... 33 enddo \n34 do k_vect = 1,4 35 faction(vect_j3(k_vect)) = vect_fjx1(k_vect) 36 ... 37 enddo 38 enddo Listing9:Originalandtransformedgromacs \ncode. Whenthe dynamic analysisresultswereexamined atthelevel ofindividualstatements,itbecameclearthattheloopisinfactpar\u00adallel: \nthevalues injjnr ensurethat distinct elementsof faction are accessedbyeachiteration.(The relativelylowaverage \nconcur\u00adrencyinTable1is duetothesmallnumberofloopiterations and toafew chainsofreductions.)Furthermore,althoughthe \naccesses to pos and faction do not exhibit any patterns, the rest of the computationintheloop bodyis \ndonethrough scalarsand canbe easily vectorized. Forbettervectorization, theloop wasstrip-mined as shownin \nLines 15 38ofListing9.(Thecleanuploopis not shown.)Loop distributionoftheinner k_vect loopwasthenappliedtomove \nall readsfrom pos and faction abovethecomputation, andtomove all writes to faction afterthe computation.Array \nexpansionof temporary j3 was also performed to hold the necessary indices of faction.The middle k_vect \nloopis nowvectorizedbyicc. Table4showsthereductioninthetotaltimespentintheloop due to thetransformation. \nUnlike in the earlier case studies, here the analysis results do not necessarilygeneralizetoarbitraryinput \ndata.Speci.cally,the fact that all iterations of the loop were independent affects both thevectorizationpotential \nandthecode modi.cations.Aswritten, the transformed code in Listing 9 is not correct for all possible \ninputs.Itbecomesnecessaryto assertthis correctnesswiththehelp of additionalinformation e.g.,anexpert \ns knowledgeof certain propertiesoftheproblem domain, or some compileranalysisofthe intra-andinter-procedural \ncode contextsurroundingtheloop. Limitations Althoughthese casestudies demonstratethe useful\u00adness of the \nanalysis reports, the proposed technique has a num\u00adber of limitations. For example, for the loop from \n435.gromacs, the conclusions about vectorizability are dependent on proper\u00adties of the input data. As \nanother example, we investigated loop bbox.cpp:894 from 453.povray in greater depth. This bench\u00admarkisaray-tracer, \nandtheloopin questionimplementsawork\u00adlistalgorithmthatintersectsaray with atreeof bounding boxes. Thecomputationisdrivenby \napriorityqueue.Each iterationof theloopremovesa bounding boxfromthe queue and,ifnecessary, adds other \nbounding boxestothe queue.Insideaniteration,what processingis performed onthecurrent bounding box depends \non whetheritisaninner node oraleafofthetree, andwhetherthe rayintersectsthe boxesofits children.Theoverallstructureofthe \ncomputation is very irregular and heavily depends on the actual run-time data. As part of the intersection \ntests for boxes and the scene objectscontainedinthem, somelow-level operations(e.g., computing the anglebetweentwovectors) \noccur repeatedly,with high concurrency andwith somepotential vectorizabilityforcer\u00adtain subcomputations. \nHowever, the highly-irregular structure of thecontrol.ow makesitextremelychallengingtoexploitthevec\u00adtorization \npotential without signi.cant changes to the code by a domainexpert withadeep understandingofthealgorithm. \nAninterestingdirectionforfutureworkistore.nethe dynamic analysistodistinguish computationswithirregular \ndata-dependent control .ow from ones where the control .ow is more structured andvectorization potentialismorelikelytobe \nactuallyrealizable through codetransformations. 5. Related Work Anumberoftechniqueshavebeenproposedto \nmeasuretheavail\u00adableparallelismatthe statement orinstructionlevel(e.g.,[2,8,9, 11,12, 16,17,21,23 25,28,33]).Severalapproachesaimtochar\u00adacterize \ninstruction-level parallelism (ILP) and how it is affected by hardwarefeatures andcompileroptimizations(e.g.,[12,33]). \nAustinandSohi[2]constructa dynamicdependencegraphanduse itto measureILPforseveral con.gurations.Typically,inthese \nand similarstudies,aprogramexecutiontraceis.rst created;next, pos\u00adsibleparallel schedules arede.nedby \ntakinginto accountthede\u00adpendences betweenbinaryinstructionsinthetrace, undervarious assumptions(e.g., \nantiand output dependences maybeignored).A notableexceptionistheworkbyKumar[11],whichdoes not create \natraceandinsteadinstrumentstheprogramto computetheparallel schedule online.Some generalizationsofthis \napproachhave been proposedin recent work[8]. Researchers have also considered dynamic analysis of loop\u00adlevel \nparallelism, where all iterations of a loop may run concur\u00adrentlywith eachother.The approachbyLarus[14]generates \nan executiontrace andanalyzesitto modeltheeffectsofloop-level parallelism.A relatedtechniqueis appliedinthe \ncontext of spec\u00adulativeparallelizationofloops,where shadowlocations areused totrack dynamicdependences \nacrossloopiterations[22].Several other approachesofsimilarnaturehavebeeninvestigatedin more recentwork(e.g.,[3,19,30,31,35, \n39]). The ef.cient collection of dynamic data-dependence informa\u00adtionhasalso beenexploredbypreviouswork.Tallametal. \nuserun\u00adtime control .ow information to reconstruct signi.cant portions ofthe dynamicdata-dependencegraphoftheprogramrun[26], \nandhaveproposed atechniqueforproducing lightweighttracing of multi-threaded code[27].Zhang et al.useanapproach \ntocol\u00adlect compressedpro.lesfromprogramexecutions,including data dependenceinformation[37]; theyalso \nproposetechniquestode\u00adcreasethe costofmaintaining a dynamicdependencegraphinthe context ofdynamicslicing[36,38]. \nAutomaticparallelizationisalsoclosely related tothecharac\u00adterization of vectorization in programs. Techniques \nto automati\u00adcally exploit.ne-grained parallelismmust.rstdeterminethede\u00adpendences between instructions. \nThere is a body of work on the characterization of dynamic data dependences in the context of speculative \nexecution(e.g.,[22,30, 40]).  Automaticvectorizationhasbeen the subject ofextensive study (e.g.,[1,6,7,13,13,18,34]).Theseapproaches \nusestaticde\u00adpendenceanalysisandthen convert scalarinstructionstovectorin\u00adstructions,in cases whenthe \nconditions foref.cientvectorization aremet.Tothe bestof our knowledge,nopriorworkhasaddressed the topic \nof this paper the development of an approach for dy\u00adnamic analysis of vectorizability of computations. \n6. Conclusions This paperpresentsa new dynamicanalysisforthecharacteriza\u00adtionofSIMDparallelism potentialinprograms.Existing \nmethods forcharacterizingconcurrencyhavefundamentallimitationsindis\u00adcoveringpotentiallyvectorizableoperations,buttheseproblemsare \novercomebythe newlydevelopedapproach.The useofthe analysis is illustrated by characterizing several computationally-intensive \nloops in benchmarks. The results demonstrate that the approach can detect potential opportunitiesforvectorizationthatare \nmissed bya state-of-the-art vectorizing compiler. In addition to its use in characterizinglarge software \nsuitesforvectorization potential,the proposedtechnique can assistvectorizationexpertsinidentifying potentiallypro.tablecoderegions \nonwhichattentionshouldbefo\u00adcused, as well as aid compiler experts by identifying potentially vectorizable \ncodethatthe compiler svectorizermisses. Acknowledgments We thank the PLDI reviewers for their valuable \ncomments. This material is based upon work supported by the National Sci\u00adence Foundation under grantsCCF-0811781,CCF-0926127,OCI\u00ad0904549,CCF-1017204, \nandbytheDepartmentofEnergy sOf.ce ofAdvancedScienti.cComputingundergrantDE-SC0005033. References [1]R.Allen \nand K.Kennedy. Optimizing Compilers for Modern Archi\u00adtectures: A Dependence-based Approach.MorganKaufmann, \n2001. [2] T. Austin and G. Sohi. Dynamic dependency analysis of ordinary programs. In ISCA,pages 342 \n351, 1992. [3]M. Bridges,N.Vachharajani,Y.Zhang,T.Jablin,andD.August. Re\u00advisitingthe sequentialprogramming \nmodelformulti-core.In MICRO, pages 69 84, 2007. [4] Clang. clang.llvm.org. [5] DragonEgg. dragonegg.llvm.org. \n[6] A. Eichenberger, P. Wu, and K. O Brien. Vectorization for SIMD architectureswith alignmentconstraints.In \nPLDI,pages 82 93, 2004. [7] L. Fireman, E. Petrank, and A. Zaks. New algorithms for SIMD alignment. In \nCC,pages 1 15, 2007. [8] S. Garcia, D. Jeon, C. M. Louie, and M. B. Taylor. Kremlin: Re\u00adthinking andrebooting \ngprofforthemulticoreage. In PLDI,pages 458 469, 2011. [9] C. Hammacher, K. Streit, S. Hack, and A. Zeller. \nPro.ling Java programsfor parallelism. In IWMSE,pages 49 55, 2009. [10] HPCToolkit. www.hpctoolkit.org. \n[11]M.Kumar. Measuringparallelismincomputation-intensivescien\u00adti.c/engineering applications. IEEE TC, \n37(9):1088 1098, 1988. [12] M.LamandR.Wilson.Limitsof control.owon parallelism.In ISCA, pages 46 57, \n1992. [13]S.LarsenandS.Amarasinghe.Exploitingsuperwordlevel parallelism with multimedia instruction sets. \nIn PLDI,pages 145 156, 2000. [14]J.Larus. Loop-levelparallelismin numeric andsymbolic programs. IEEE \nTPDS, 4(1):812 826, 1993. [15] C.Lattner andV.Adve.LLVM:Acompilationframeworkfor lifelong program analysis&#38;transformation. \nIn CGO, page 75, 2004. [16]J. MakandA. Mycroft. Limitsof parallelismusing dynamicdepen\u00addencygraphs. In \nWODA,pages 42 48, 2009. [17]A.NicolauandJ.Fisher.Measuringthe parallelismavailableforvery long instruction \nword architectures. IEEE TC, 33(11):968 976, 1984. [18]D. Nuzman,I.Rosen,andA.Zaks.Auto-vectorization \nofinterleaved data forSIMD. In PLDI,pages 132 143, 2006. [19] C. Oancea and A. Mycroft. Set-congruence \ndynamic analysis for thread-level speculation(TLS). In LCPC,pages 156 171, 2008. [20] PETSc. www.mcs.anl.gov/petsc. \n[21]M.Postiff,D. Greene,G.Tyson,andT.Mudge.Thelimitsofinstruc\u00adtionlevel parallelisminSPEC95applications. \nSIGARCH Computer Architecture News, 27(1):31 34, 1999. [22]L. RauchwergerandD.Padua.TheLRPD test:Speculative \nrun-time parallelization of loops with privatization and reduction paralleliza\u00adtion. In PLDI,pages 218 \n232, 1995. [23] L. Rauchwerger, P. Dubey, and R. Nair. Measuring limits of paral\u00adlelism and characterizing \nits vulnerability to resource constraints. In MICRO,pages 105 117, 1993. [24] A. Rountev, K. Van Valkenburgh, \nD. Yan, and P. Sadayappan. Un\u00adderstanding parallelism-inhibitingdependencesinsequentialJavapro\u00adgrams. \nIn ICSM,page9, 2010. [25]D.Stefanovi\u00b4candM. Martonosi.Limitsandgraphstructureof avail\u00adable instruction-level \nparallelism. In Euro-Par, pages 1018 1022, 2000. [26] S. Tallam and R. Gupta. Uni.ed control .ow and \ndata dependence traces. ACM TACO, 4(3):19, 2007. [27]S.Tallam,C.Tian,R.Gupta,andX.Zhang.Enablingtracingof \nlong\u00adrunningmultithreaded programsvia dynamicexecutionreduction. In ISSTA,pages 207 218, 2007. [28] K. \nTheobald, G. Gao, and L. Hendren. On the limits of program parallelism and its smoothability. In MICRO,pages \n10 19, 1992. [29] C. Tian, M. Feng, V. Nagarajan, and R. Gupta. Copy or discard execution model for speculative \nparallelization on multicores. In MICRO,pages 330 341, 2008. [30] C.Tian,M.Feng,V. Nagarajan, andR.Gupta.Speculative \nparalleliza\u00adtionof sequentialloops onmulticores. JPP, 37(5):508 535, 2009. [31] G.Tournavitis,Z.Wang,Zheng, \nB.Franke, andM.O Boyle. Towards a holistic approach to auto-parallelization. InPLDI,pages 177 187, 2009. \n[32] UTDSPBenchmarks. www.eecg.toronto.edu/ corinna. [33]D.Wall. Limitsof instruction-levelparallelism. \nIn ASPLOS,pages 176 188, 1991. [34] M. Wolfe. High Performance Compilers For Parallel Computing. Addison-Wesley, \n1996. [35]P.Wu,A.Kejariwal,andC. Cas\u00b8caval. Compiler-driven dependence pro.ling to guide program parallelization. \nIn LCPC,pages 232 248, 2008. [36] X.ZhangandR. Gupta. Costeffective dynamicprogramslicing. In PLDI,pages \n94 106, 2004. [37]X.ZhangandR.Gupta.Wholeexecutiontraces andtheir applications. ACM TACO, 2(3):301 334, \n2005. [38] X. Zhang, R. Gupta, and Y. Zhang. Cost and precision tradeoffs of dynamic data slicing algorithms. \nACM TOPLAS, 27(4):631 661, 2005. [39] H.Zhong, M. Mehrara, S.Lieberman, andS.Mahlke. Uncovering hidden \nloop level parallelism in sequential applications. In HPCA, pages 290 301, 2008. [40] X.Zhuang, A.E.Eichenberger,Y.Luo, \nK. O Brien, andK.O Brien. Exploitingparallelismwith dependence-awarescheduling. InPACT, pages 193 202, \n2009.   \n\t\t\t", "proc_id": "2254064", "abstract": "<p>Recent hardware trends with GPUs and the increasing vector lengths of SSE-like ISA extensions for multicore CPUs imply that effective exploitation of SIMD parallelism is critical for achieving high performance on emerging and future architectures. A vast majority of existing applications were developed without any attention by their developers towards effective vectorizability of the codes. While developers of production compilers such as GNU gcc, Intel icc, PGI pgcc, and IBM xlc have invested considerable effort and made significant advances in enhancing automatic vectorization capabilities, these compilers still cannot effectively vectorize many existing scientific and engineering codes. It is therefore of considerable interest to analyze existing applications to assess the inherent latent potential for SIMD parallelism, exploitable through further compiler advances and/or via manual code changes.</p> <p>In this paper we develop an approach to infer a program's SIMD parallelization potential by analyzing the dynamic data-dependence graph derived from a sequential execution trace. By considering only the observed run-time data dependences for the trace, and by relaxing the execution order of operations to allow any dependence-preserving reordering, we can detect potential SIMD parallelism that may otherwise be missed by more conservative compile-time analyses. We show that for several benchmarks our tool discovers regions of code within computationally-intensive loops that exhibit high potential for SIMD parallelism but are not vectorized by state-of-the-art compilers. We present several case studies of the use of the tool, both in identifying opportunities to enhance the transformation capabilities of vectorizing compilers, as well as in pointing to code regions to manually modify in order to enable auto-vectorization and performance improvement by existing compilers.</p>", "authors": [{"name": "Justin Holewinski", "author_profile_id": "81500660090", "affiliation": "Ohio State University, Columbus, OH, USA", "person_id": "P3471259", "email_address": "holewins@cse.ohio-state.edu", "orcid_id": ""}, {"name": "Ragavendar Ramamurthi", "author_profile_id": "81502742673", "affiliation": "Ohio State University, Columbus, OH, USA", "person_id": "P3471260", "email_address": "ramamurr@cse.ohio-state.edu", "orcid_id": ""}, {"name": "Mahesh Ravishankar", "author_profile_id": "81502682598", "affiliation": "Ohio State University, Columbus, OH, USA", "person_id": "P3471261", "email_address": "ravishan@cse.ohio-state.edu", "orcid_id": ""}, {"name": "Naznin Fauzia", "author_profile_id": "81502806714", "affiliation": "Ohio State University, Columbus, OH, USA", "person_id": "P3471262", "email_address": "fauzia@cse.ohio-state.edu", "orcid_id": ""}, {"name": "Louis-No&#235;l Pouchet", "author_profile_id": "81330496337", "affiliation": "Ohio State University, Columbus, OH, USA", "person_id": "P3471263", "email_address": "pouchet@cse.ohio-state.edu", "orcid_id": ""}, {"name": "Atanas Rountev", "author_profile_id": "81100162864", "affiliation": "Ohio State University, Columbus, OH, USA", "person_id": "P3471264", "email_address": "rountev@cse.ohio-state.edu", "orcid_id": ""}, {"name": "P. Sadayappan", "author_profile_id": "81453642049", "affiliation": "Ohio State University, Columbus, OH, USA", "person_id": "P3471265", "email_address": "saday@cse.ohio-state.edu", "orcid_id": ""}], "doi_number": "10.1145/2254064.2254108", "year": "2012", "article_id": "2254108", "conference": "PLDI", "title": "Dynamic trace-based analysis of vectorization potential of applications", "url": "http://dl.acm.org/citation.cfm?id=2254108"}