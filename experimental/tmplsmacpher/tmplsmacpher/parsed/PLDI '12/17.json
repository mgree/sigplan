{"article_publication_date": "06-11-2012", "fulltext": "\n Ef.cient State Merging in Symbolic Execution Volodymyr Kuznetsov Johannes Kinder Stefan Bucur George \nCandea School of Computer and Communication Sciences \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL), \nSwitzerland {vova.kuznetsov,johannes.kinder,stefan.bucur,george.candea}@ep..ch Abstract Symbolic execution \nhas proven to be a practical technique for building automated test case generation and bug .nding tools. \nNevertheless, due to state explosion, these tools still struggle to achieve scalability. Given a program, \none way to reduce the number of states that the tools need to explore is to merge states obtained on \ndifferent paths. Alas, doing so increases the size of symbolic path conditions (thereby stressing the \nunderlying constraint solver) and interferes with optimizations of the exploration process (also referred \nto as search strategies). The net effect is that state merging may actually lower performance rather \nthan increase it. We present a way to automatically choose when and how to merge states such that the \nperformance of symbolic execution is signi.cantly increased. First, we present query count estimation, \na method for statically estimating the impact that each symbolic variable has on solver queries that \nfollow a potential merge point; states are then merged only when doing so promises to be advan\u00adtageous. \nSecond, we present dynamic state merging, a technique for merging states that interacts favorably with \nsearch strategies in automated test case generation and bug .nding tools. Experiments on the 96 GNU COREUTILS \nshow that our ap\u00adproach consistently achieves several orders of magnitude speedup over previously published \nresults. Our code and experimental data are publicly available at http://cloud9.epfl.ch. Categories and \nSubject Descriptors D.2.4 [Software Engineer\u00ading]: Software/Program Veri.cation; D.2.5 [Software Engineer\u00ading]: \nTesting and Debugging Symbolic Execution, Testing Tools; F.3.1 [Logics and Meanings of Programs]: Specifying \nand Verify\u00ading and Reasoning about Programs Keywords Testing, Symbolic Execution, Veri.cation, Bounded \nSoftware Model Checking, State Merging 1. Introduction Recent tools [5 7, 18, 19] have applied symbolic \nexecution to automated test case generation and bug .nding with impressive results they demonstrate that \nsymbolic execution brings unique practical advantages. First, such tools perform dynamic analysis, in \nthat they actually execute a target program and can directly execute any calls to external libraries \nor the operating system by concretiz\u00ading arguments; this broadens their applicability to many real-world \nprograms. Second, these tools share with static analysis the ability Permission to make digital or hard \ncopies of all or part of this work for personal or classroom use is granted without fee provided that \ncopies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. PLDI 12, June 11 16, Beijing, China. Copyright \n&#38;#169; 2012 ACM 978-1-4503-1205-9/12/04. . . $10.00 to simultaneously reason about multiple program \nbehaviors, which improves the degree of completeness they achieve. Third, symbolic execution does not \nuse abstraction but is fully precise with respect to predicate transformer semantics [11]; it generates \nper-path ver\u00ad i.cation conditions whose satis.ability implies the reachability of a particular statement, \nso it generally does not have false positives. Fourth, recent advances in SAT and SMT (SAT Modulo Theory) \nsolving [10, 12, 15] have made tools based on symbolic execution signi.cantly faster. Overall, symbolic \nexecution promises to help solve many important, practical program analysis problems. Nevertheless, today \ns symbolic execution engines still struggle to achieve scalability, because of path explosion: the number \nof possible paths in a program is generally exponential in its size. States in symbolic execution encode \nthe history of branch decisions (the path condition) and precisely characterize the value of each variable \nin terms of input values (the symbolic store), so path explosion becomes synonymous with state explosion. \nAlas, the bene.t of not having false positives in bug .nding (save for over\u00adapproximate environment assumptions) \ncomes at the cost of having to analyze an exponential number of states. One way to reduce the number \nof states is to merge states that correspond to different paths. This is standard in classic static analysis, \nwhere the resulting merged state over-approximates the individual states that were merged. Several techniques, \nsuch as ESP [9] and trace partitioning [27], reduce but do not eliminate the resulting imprecision (which \ncan be a source of false positives) by associating separate abstract domain elements to some sets of \nexecution paths. In symbolic execution, as a matter of principle, a merged state would have to precisely \nrepresent the information from all execution paths without any over-approximation. Con\u00adsider, for example, \nthe program if (x<0) {x=0;} else {x=5;} with input X assigned to x. We denote with (pc,s) a state that \nis reach\u00adable for inputs obeying path condition pc and in which the sym\u00adbolic store s =[v0 = e0,...,vn \n= en] maps variable vi to expression ei, respectively. In this case, the two states (X < 0,[x = 0]) and \n(X = 0, [x = 5]), which correspond to the two feasible paths, can be merged into one state (true,[x = \nite(X < 0,0,5)]). Here, ite(c, p,q) denotes the if-then-else operator that evaluates to p if c is true, \nand to q otherwise. If states were merged this way for every branch of a program, symbolic execution \nwould become similar to veri.ca\u00adtion condition generation or bounded model checking, where the entire \nproblem instance is encoded in one monolithic formula that is passed in full to a solver. State merging \neffectively decreases the number of paths that have to be explored [16, 20], but also increases the size \nof the sym\u00ad bolic expressions describing variables. Merging introduces disjunc\u00adtions, which are notoriously \ndif.cult for SMT solvers, particularly for those using eager translation to SAT [15]. Merging also converts \ndiffering concrete values into symbolic expressions, as in the ex\u00adample above: the value of x was concrete \nin the two separate states, but symbolic (ite(X < 0,0, 5)) in the merged state. If x were to ap\u00adpear \nin branch conditions or array indices later in the execution, the choice of merging the states may lead \nto more solver invoca\u00adtions than without merging. This combination of larger symbolic expressions and \nextra solver invocations can drown out the bene.t of having fewer states to analyze, leading to an actual \ndecrease in the overall performance of symbolic execution [20].  Furthermore, state merging con.icts \nwith important optimiza\u00adtions in symbolic execution: search-based symbolic execution en\u00adgines, like the \nones used in test case generators and bug .nding tools, employ search strategies to prioritize searching \nof interest\u00ading paths over less interesting ones, e.g., with respect to max\u00adimizing line coverage given \na .xed time budget. To maximize the opportunities for state merging, however, the engine would have to \ntraverse the control .ow graph in topological order, which typically contradicts the strategy s path \nprioritization policy. Contributions. In this paper, we describe a solution to these two challenges that \nyields a net bene.t in practice. We combine the state space reduction bene.ts of merged exploration with \nthe constraint solving bene.ts of individual exploration, while mitigating the en\u00adsuing drawbacks. Experiments \non the GNU COREUTILS show that employing our approach in a symbolic execution engine achieves speedups \nover the state of the art that are exponential in the size of symbolic input. Our contributions are: \n We present query count estimation, a way to statically approx\u00adimate the number of times each variable \nwill appear in future solver queries after a potential merge point. We then selectively merge two states \nonly when we expect differing variables to appear infrequently in later solver queries. Since this selective \nmerging merely groups paths instead of pruning them, inaccu\u00adracies in the estimation do not hurt soundness \nor completeness.  We present dynamic state merging, a merging algorithm specif\u00adically designed to interact \nfavorably with search strategies. The algorithm explores paths independently of each other and uses a \nsimilarity metric to identify on-the-.y opportunities for merg\u00ading, while preserving the search strategy \ns privilege of dictating exploration priorities.  Organization. In \u00a72, we characterize the design space \nof pre\u00ad cise symbolic analysis using a generic algorithm. With bounded model checking on one end of the \nspectrum and symbolic execu\u00adtion on the other, we analyze middle-ground approaches (such as function \nsummaries [16]) and argue for an opportunistic, dynamic approach to navigating this design space based \non cost estimates. In this context, we introduce query count estimation (\u00a73) and dy\u00ad namic state merging \n(\u00a74). We then describe our implementation in the KLEE symbolic execution engine and present a systematic \neval\u00aduation of the individual and combined effects of the two proposed methods (\u00a75). We review related \nwork in \u00a76 and conclude with \u00a77. 2. Trade-offs in Symbolic Program Analysis Test generation by symbolic \nexecution is just one of a multitude of precise symbolic program analyses that are facilitated by SAT \nor SMT solvers. Tools such as CBMC [8], Saturn [31], and Calysto [2] have shown that exact, abstraction-free \npath sensitive local reason\u00ading is feasible and can be fully outsourced to an external solver. With the \nhelp of a generic worklist algorithm (\u00a72.1), we illustrate the relationship among precise symbolic program \nanalyses and ex\u00adplain the trade-offs in the resulting solver queries (\u00a72.2). We con\u00ad clude that state \nmerging critically affects all types of precise sym\u00adbolic program analysis. Motivated by this insight, \nwe give a brief overview of our proposed approach to state merging (\u00a72.3). 2.1 General Symbolic Exploration \nPrecise symbolic program analyses essentially perform forward ex\u00adpression substitution starting from \na set of input variables. The re- Input: Choice function pickNext, similarity relation ~, branch checker \nfollow, and initial location e0. Data: Worklist w and set of successor states S. 1 w := {(e0,true,. v.v)}; \n2 while w= \u00d8do 3 (e,pc,s) := pickNext(w); S := \u00d8; // Symbolically execute the next instruction 4 switch \ninstr(e) do 5 case v := e // assignment 6 S := {(succ(e),pc, s[v . eval(s,e)])}; 7 case if(e) goto e' \n// conditional jump 8 if follow(pc . s . e) then 9 S := {(e',pc . e,s)}; 10 if follow(pc . s .\u00ace) then \n11 S := S .{(succ(e),pc .\u00ace, s)}; 12 case assert(e) // assertion 13 if isSatis.able(pc . s .\u00ace) then \nabort; 14 else S := {(succ(e),pc,s)}; 15 case halt // program halt 16 print pc; // Merge new states with \nmatching ones in w ' 17 forall (e'', pc,s') . S do '''') . w : (e'''''') ~ (e''' 18 if .(e'',pc, s,pc,s,pc,s') \nthen 19 w := w \\{(e'',pc'',s'')}; ''' 20 w := w.{(e'',pc'.pc,. v.ite(pc,s'[v],s''[v]))}; 21 else ' 22 \nw := w .{(e'', pc,s')}; 23 print \"no errors\"; Algorithm 1. Generic symbolic exploration. sulting formulae \nare then used to falsify assertions and .nd bugs, or to generate input assignments and generate test \ncases. Algorithm 1 is a generic algorithm for symbolic program analysis that can be used to implement \ndifferent analysis .avors. For illustration pur\u00adposes, we consider only a simple input language with \nassignments, conditional goto statements, assertions, and halt statements. The algorithm is parameterized \nby a function pickNext for choosing the next state in the worklist, a function follow that re\u00adturns a \ndecision on whether to follow a branch, and a relation ~ that controls whether states should be merged. \nWe now extend the notation for states used in \u00a71 to triples (e,pc,s), consisting of a program location \ne, the path condition pc, and the symbolic store s that maps each variable to either a concrete value \nor an expression over input variables. In line 1, the worklist w of the algorithm is initialized with \na state whose symbolic store maps each variable to itself (for simplicity, we exclude named constants). \nHere, . x.e denotes the function mapping parameter x to an expression e (we will use . (x1,...xn).e for \nmultiple parameters). In each iteration, the algorithm picks a new state from the worklist (line 3). \nOn encountering an assignment v := e (lines 5-6), the algo\u00adrithm creates a successor state at the fall-through \nsuccessor loca\u00adtion succ(e) of e by updating the symbolic store s with a mapping from v to a new symbolic \nexpression obtained by evaluating e in the context of s, and adds the new state to the set S. At every \nbranch (lines 7-11), the algorithm .rst checks whether to follow either path and, if so, adds the corresponding \ncondition to the suc\u00adcessor state, which in turn is added to S. Analyses can decide to not follow a branch \nif the branch is infeasible or would exceed a limit on loop unrolling. For assertions (line 12-14), the \npath condition, the symbolic store, and the negated assertion are put in conjunction and checked for \nsatis.ability. Since the algorithm does not over\u00adapproximate, this check has no false positives. Halt \nstatements ter\u00adminate the analyzed program, so the algorithm just outputs the path condition, a satisfying \nassignment of which can be used to generate a test case for the execution leading to the halt.  In lines \n17-22, the new states in S are then merged with any matching states in the worklist before being added \nto the worklist themselves. Two states match if they share the same location and are similar according \nto ~. Merging creates a disjunction of the two path conditions (which can be simpli.ed by factoring out \ncommon pre.xes) and builds the merged symbolic store from ite expressions that assert one or the other \noriginal value, depending on the path taken (line 20). The ite expressions that assert an identical value \nin both cases (because it was equal in both symbolic stores) can be simpli.ed to that value.  2.2 The \nDesign Space of Symbolic Program Analysis The differences between various implementations of precise \nsym\u00adbolic analysis lie in the following aspects: 1. the handling of loops and/or recursion; 2. whether \nand how the feasibility of individual branches is checked to avoid encoding infeasible paths; 3. whether \nand how states from different paths are merged; 4. compositionality, i.e., the use of function summaries. \n Loops affect soundness and completeness, while the other as\u00adpects are trade-offs that critically affect \nanalysis performance. We now illustrate these different aspects using Algorithm 1. Loops and Recursion. \nBounded model checkers [8] and ex\u00ad tended static checkers [2, 13, 31] unroll loops up to a certain bound, \nwhich can be iteratively increased if an injected unwinding asser\u00adtion fails. Such unrolling is usually \nperformed by statically rewrit\u00ading the CFG, but can be .t into Algorithm 1 by de.ning follow to re\u00adturn \nfalse for branches that would unroll a loop beyond the bound. Symbolic execution explores loops as long \nas it cannot prove the infeasibility of the loop condition. A search strategy, implemented in the function \npickNext, can bias the analysis against states that perform many repetitions of the same loop. For example, \na search strategy optimized for line coverage selects states close to unex\u00adplored code and avoids states \nin deep loop unrollings [6]. Dynamic test generation as implemented in DART [18] starts with an arbitrary \ninitial unrolling of the loop and explores different unrollings in subsequent tests. That is, DART implements \npickNext to follow concrete executions, postponing branch alternatives until they are covered by a subsequent \nconcrete execution. All these approaches essentially perform loop unrolling and are generally incomplete \nfor .nite analysis times. Loop invariants are rarely used (though [17] is an exception) since weak invariants \ncan introduce false positives, which these precise analyses are speci.\u00adcally designed to avoid. Weakest \nprecondition-based program veri\u00ad.cation engines such as Boogie [26] and Havoc [24], which also in\u00ad terface \nwith external solvers, rely on the user to supply suf.ciently strong invariants for proving all properties \nof interest. Feasibility Checking. While performing expression substitu\u00adtion along individual paths, \ncertain combinations of conditional branches can turn out to be infeasible. Not propagating states that \nrepresent infeasible paths helps to reduce path explosion by invest\u00ading solving time earlier in the execution. \nIntermediate feasibility checks are usually performed only by symbolic execution engines that follow \na single path at a time (when reasoning about groups of paths, branches are less likely to be infeasible), \nin which case follow simply invokes the constraint solver. State Merging. When states meet at the same \ncontrol location, there are two general possibilities for combining their information: either the states \nare maintained separately, or the states are merged into a single state. In precise symbolic analysis, \nmerging is not allowed to introduce abstraction. From a conceptual viewpoint, state merging therefore \nonly changes the shape of a formula that characterizes a set of execution paths: if states are kept separate, \na set of paths is described by their disjunction; if states are merged, there is only one formula with \ndisjunctions in the path condition and ite expressions in the symbolic store that guard the values of \nthe variables depending on the path taken. In general, we distinguish two extremes: (i) complete separation \nof paths, as implemented by search-based symbolic execution (e.g., [4 6, 18, 19, 23]), and (ii) complete \nstatic state merging, as imple\u00admented by veri.cation condition generators (e.g., [2, 8, 21, 31]). Static \nstate merging combines states at join points after completely encoding all subpaths, i.e., it de.nes \npickNext to explore all sub\u00adpaths leading to a join point before picking any states at the join point, \nand it de.nes ~ to contain all pairs of states. In search-based symbolic execution engines, pickNext \ncan be chosen freely accord\u00ading to the search goal, and ~ is empty. Thus, they can, for example, choose \nto explore just the successors of a speci.c state and delay exploration of additional loop iterations. \nSome approaches adopt intermediate merging strategies. In the context of bounded model checking (BMC), \nGanai and Gupta [14] investigate splitting the veri.cation condition along subsets of paths. This moves \nBMC a step into the direction of symbolic ex\u00adecution, and corresponds to partitioning the ~ relation. \nHansen et al. [20] describe an implementation of static state merging in which they modify the exploration \nstrategy to effectively traverse the CFG in topological order and merge all states that share the same \nprogram location. For two of their three tested examples, the total solving time increases with this \nstrategy thus showing this approach to be sub-optimal. Another prominent example of state merging is \nthe use of function summaries in symbolic execution, which we explain below. Compositionality. For precise \ninterprocedural symbolic execu\u00adtion, the simplest and most common approach is function inlining. This \ncauses functions to be re-analyzed at every call site, which could be avoided using function summaries. \nSummaries that do not introduce abstraction and are thus suitable for symbolic execution can be implemented \nby computing an intraprocedural path condi\u00adtion in terms of function inputs, and then merging all states \nat the function exit. Alas, applying such a function summary is essentially as ex\u00adpensive as re-analyzing \nthe function, if the translation effort from the programming logic into the representation logic is negligible. \nUsing a summary instead of inlining avoids only the feasibility checks for intraprocedural paths that \nare infeasible regardless of the function input. The cost of the other feasibility checks that a non-compositional \nsymbolic execution would perform is not elim\u00adinated by function summaries. Instead, the branch conditions \nare contained in the ite expressions of the summary and will increase the complexity of later SMT queries. \nFor dynamic test generation, Godefroid [16] suggests to collect summaries as disjunctions of pairs of \ninput and output constraints. In further work [1], this is extended to record summaries one path at a \ntime and to apply partial summaries whenever they match the input preconditions. Dynamic test generation \nre-executes the full program (with heavy instrumentation) for each branch of which the alternate case \nis to be analyzed. Due to re-execution, analyzing all branches in functions would come at an especially \nhigh cost, so the savings outweigh the additional solving costs for the merged summary states. For simplicity, \nAlgorithm 1 is just intraprocedural, supporting function calls by inlining. It can generate precise symbolic \nfunction summaries, if invoked per procedure and with a similarity relation that merges all states when \nthe function terminates.  2.3 Our Approach: Dynamically Navigate the Design Space Precise symbolic \nanalyses are roughly equivalent in their treatment of loops, but all other design choices (merging at \ncontrol points, feasibility checking, and function summaries) boil down to choos\u00ading which paths to analyze \nseparately vs. which ones to combine into common formulae. In other words, all analyses lie along a spectrum, \nwith search-based symbolic execution (no state merging) at one extreme and whole-program veri.cation \ncondition genera\u00adtion (static state merging) at the other extreme. Instead of making a static design \nchoice of where to be in this spectrum, our approach is to enable a symbolic analysis to choose dynamically \nthe most ad\u00advantageous point and merge states according to the expected bene.t of such a merge. On the \none hand, merging reduces the number of states, but on the other hand, the remaining states become more \nexpensive to ex\u00adplore. In the merged state, each variable that had distinct values in the original states \nmust be constrained by an input-dependent ite ex\u00adpression, which increases the time required to solve \nfuture queries involving such variables. If the distinct values were concrete, merg\u00ading would cause additional \nsolver invocations where expressions could have been evaluated concretely in separate states. Furthermore, \nthere is an inherent incompatibility between par\u00adtial searches using coverage-guided search strategies \n(as is often done in test generation) and static state merging at control .ow join points: merging can \nbe maximized by exploring the CFG in topological order, so that a combined state can be computed from \nits syntactic predecessors. A coverage-guided search strategy, how\u00adever, will dynamically deprioritize \nsome states (e.g., defer for later the exploration of additional iterations of a loop), which prevents \nusing a topological order. Therefore, to make state merging practical, we must solve two problems: (1) \nAutomatically identify an advantageous balance be\u00adtween exploring fewer complex states vs. more simpler \nstates, and merge states only when this promises to reduce exploration time; and (2) Ef.ciently combine \nstate merging with search strategies that deprioritize non-interesting execution paths. To solve the \n.rst problem, we developed query count estima\u00adtion (QCE), a way to estimate how variables that are different \nin two potentially mergeable states will be used in the future. We pre\u00adprocess the program using a lightweight \nstatic analysis to identify how often each variable is used in branch conditions past any given point \nin the CFG, and use this as a heuristic estimate of how many subsequent solver queries that variable \nis likely to be part of. Using this heuristic, we check whether two states are suf.ciently similar that \nmerging them would yield a net bene.t. That is, the additional cost of solving more and harder SMT queries \nis outweighed by the savings from exploring fewer paths. The results of this static analy\u00adsis affect \nonly the completion time of the symbolic analysis not its soundness or completeness. To solve the second \nproblem, we introduce dynamic state merg\u00ading (DSM), a way to dynamically identify opportunities for merging \nregardless of the exploration order imposed by the search strategy. Without any restrictions on the search \nstrategy, only states that meet at the same location by chance could ever be merged. To increase the \nopportunities for merging, we maintain a bounded history of the predecessors of the states in the worklist. \nWhen picking the next state to process from the worklist, we check whether some ' state a1 is similar \nto a predecessor a2 of another state a2 in the worklist. If yes, then state a1, which is in some sense \nlagging be\u00adhind a2, is prioritized over the others. This causes it to be tem\u00adporarily fast-forwarded, \nuntil its own successor matches up with the candidate-for-merging state a2. If the state diverges, i.e., \none of a1 s successors is no longer suf.ciently similar to a predecessor of a2, the merge attempt is \nabandoned. Thus, while the search strategy is still in control, DSM identi.es merge opportunities dynamically \n1 void main(int argc, char **argv) { 2 int r = 1, arg = 1; 3 if (arg < argc) 4 if (strcmp(argv[arg], \n\"-n\") == 0) { 5 r = 0; ++arg; 6 } 7 for (; arg < argc; ++arg) 8 for (int i = 0; argv[arg][i] != 0; ++i) \n9 putchar(argv[arg][i]); 10 if (r) 11 putchar( \\n ); 12 } Figure 1. Simpli.ed version of the echo program. \nwithin a .xed distance and only brie.y takes over control to attempt the merge. After the merge attempt, \nthe search strategy continues as before. Like QCE, DSM does not affect soundness or completeness of the \nsymbolic analysis. We combine the solutions to these two problems by using QCE (explained in detail in \n\u00a73) to compute the similarity relation used by DSM (\u00a74). Even though we initially developed these techniques \nto improve the performance of search-based symbolic execution engines for test generation, we believe \nour analysis and the insights into building ef.ciently solvable symbolic formulae from programs are applicable \nto other symbolic program analyses as well. 3. Query Count Estimation We now illustrate the need for \nestimating the expected bene.t of merging using an example (\u00a73.1), show how to compute the query count \nestimates (\u00a73.2), and then justify our decisions (\u00a73.3). 3.1 Motivating Example Consider the example \nprogram in Figure 1, a simpli.ed version of the UNIX echo utility that prints all its arguments to standard \nout\u00adput, except for argument 0, which holds the program name. If the .rst regular argument is \"-n\", no \nnewline character is appended. We analyze this program using Algorithm 1, assuming bounded in\u00ad put. Speci.cally, \nwe assume that argc = N + 1 for some constant N = 1, and that each of the N command-line arguments, pointed \nto by the corresponding element of argv, is a zero-terminated string of up to L characters. For simplicity, \nwe assume that strcmp and putchar do not split paths. Under these preconditions, the total number of \nfeasible program paths is LN + LN-1, and the branch condition at line 3 is always true. The execution \npaths .rst split at line 4 on the condition C that argv[1] points to the string -n . Line 6 is then reached \nby the two states (6,C,[r = 0, arg = 2]) and (6,\u00acC, [r = 1,arg = 1]). These two can be merged into the \nsingle (but fully precise) state (6, true, [r = ite(C,0,1), arg = ite(C,2,1)]). Consider now the loop \ncondition arg < argc in line 7. If the states were kept separate, this condition could be evaluated concretely \nin both states, as 1 < N + 1 and 2 < N + 1, respectively. In the merged state, however, the condition \nwould become the disjunctive expression ite(C,2, 1) < N +1, which now requires a solver invocation where \nit was not previously necessary. The consequences of having merged at line 6 become even worse later \nin the execution, for the condition at line 8. The array index is no longer concrete, so the SMT solver \nis required to reason about symbolic memory accesses in the theory of arrays on every iteration of the \nnested loop. In this example, merging reduces the total number of states, but the merged state is more \nexpensive to reason about. Our experiments con.rm that the total time required to fully explore all feasible \npaths in this program is signi.cantly shorter if the paths are not merged on line 6. Now consider the \nbranching point in the inner loop header at line 8. Since this loop may be executed up to L times, each \nstate that enters the loop creates L successor states, one for each loop exit .xed probability \u00df . Consider \nfunction q that descends recursively possibility. For example, a state exiting after the second iteration \ninto the control .ow graph counting the number of queries that are is (8,... . argv[1][0]= 0 . argv[1][1]= \n0,[..., i = 1]). On the selected by a function c: next iteration of the outer loop (line 7), each of \nthese L states again  ' q(e \u00df . .. ,c)= (3) spawns L successors. At the end of the N outer loop iterations, \nthere is a total of LN states. However, all of the states created in the loop at line 8 during the same \niteration of the outer loop differ only in ''' '' ' ),c)+ \u00df \u00b7q(e,c)+ c(e,e) instr(e ' )= if(e) goto e \n\u00b7q(succ(e instr(e ' )= halt ' ),c) otherwise 0 q(succ(e .. the value of the temporary variable i, which \nis never used again in the program. Therefore, merging these states does not increase the cost of subsequent \nfeasibility checks, yet it cuts the number of states after the outer loop down to the number of states \nbefore the loop (2 in our example). Note that, while the path condition of the merged state is created \nas a disjunction, here it can be simpli.ed to Then Qadd(e,v) and Qt(e) can be computed recursively as \nfollows: e,. (e, e). ite  ' ' Qadd(e,v)= q (e,v) < (e,e),1, 0 ' e,. (e Qt(e)= q , e). 1, (4)the common \npre.x of all path conditions. There is another, less obvious, opportunity for merging states. Looking \nback at the .rst feasible branch at line 4, consider the state (7,C,[r = 0,arg = 1]), which corresponds \nto the path through the then branch, and the state (7, \u00acC,[r = 1,arg = 1]), which cor\u00adresponds to the \npath through the else branch and one .rst itera\u00adtion over the outer loop. Merging these two states yields \nthe state (7,true, [r = ite(C,0,1), arg = 1]), which introduces a disjunction for the symbolic expression \nrepresenting the value of the variable r. Unlike the arg variable we discussed above, r is used only \nonce on line 10, just before the program terminates. Therefore, the time saved by exploring the loops \nat lines 7-9 with fewer states can out\u00adweigh the cost of testing the more complex branch condition on \nline 10 in the merged state. This example demonstrates that the net bene.t of merging two states depends \nheavily on how often variables whose values differ between two states affect later branch conditions. \nThis is the key insight behind QCE, which we explain next.  3.2 Computing the Heuristic To make an exact \nmerging decision, one would have to compute the cumulative solving times for both the merged and unmerged \ncases. But this is impractical, so the query count estimation heuristic (QCE) makes several simpli.cations \nthat allow it to be largely pre\u00adcomputed before symbolic execution begins. QCE can be calibrated using \na number of parameters, which we denote using the Greek letters a, \u00df , and .. At each program location \ne, QCE pre-computes a set H(e) of hot variables that are likely to cause many queries to the solver if \nthey were to contain symbolic values. The heuristic is to avoid introducing new symbolic values for these \nhot variables . Specif\u00adically, states should be merged only if every hot variable either has the same \nconcrete value in both states or is already symbolic in at least one of the states. Formally, QCE is \nimplemented by de.ning the similarity relation ~ of Algorithm 1 as (e, pc1,s1) ~qce (e, pc2,s2) .. .v \n. H(e) : s1[v]= s2[v] . I <s1[v] . I <s2[v], (1) where I < s[v] denotes that variable v has a symbolic \nvalue in the symbolic store s (i.e., it depends on the set of symbolic inputs I). In order to check whether \na variable v is hot at location e, QCE estimates the number of additional queries Qadd(e,v) that would \nbe executed after reaching e if variable v were to be made symbolic. Variable v is determined to be hot \nif this number is larger than a .xed fraction a of the total number of queries Qt(e) that will be executed \nafter reaching e: H(e)= {v . V | Qadd(e,v) > a \u00b7 Qt(e)} (2) To estimate these numbers of queries ef.ciently, \nwe assume that every executed conditional branch leads to a solver query with a .xed probability (which \ncould be taken into account by suitably adjusting the value of a), and that each branch is feasible with \na '' where (e,v)< (e,e) denotes the fact that expression e at location e may depend on the value of variable \nv at location e. For the sake of simplicity, we assume all program loops to be unrolled and all function \ncalls to be inlined. For loops (and recursive function calls) whose number of iterations cannot be determined \nstatically, QCE assumes a .xed maximum number of iterations .. Note that the implementation of QCE is \nlimited to estimating the number of additional queries without taking into account the fact that queries \nmay become more expensive due to ite expressions. Our evaluation shows that this suf.ces in most cases, \nbut we also found a few cases in which lifting this limitation would improve our results. The justi.cation \nof QCE in \u00a73.3 describes how to integrate the cost of ite expressions in the computation. Interprocedural \nQCE. In our implementation, we avoid the as\u00adsumption of inlined functions by computing Qadd(e,v) and \nQt(e) for all function entry points e as function summaries. We do this compositionally, by computing \nper-function local query counts in a bottom-up fashion. The local query counts for a function F in\u00adclude \nall queries issued inside F and all functions called by F. To compute these, we extend Equation (3) to \nhandle function calls. At every call site, the local query counts are incremented by the local query \ncounts at the entry point of the callee. Since the local query counts do not include queries issued after \nthe function returns to the caller (this would require context-sensitive local query counts), we perform \nthe last step of the computation dynamically during sym\u00adbolic execution. We obtain the global query counts \nby adding the local query counts at the location of the current state to the sum of the local query counts \nof all return locations in the call stack. Parameters. In our implementation, QCE is parametrized by \na, \u00df , and the loop bound .. Optimal values for these parameters are dif.cult to compute analytically. \nFor a given program, one can empirically .nd good parameter values using a simple hill-climbing method. \nIn our experiments, we determined the parameter values this way using four programs and then used these \nvalues for the other programs, with good results (see \u00a75.1). Illustrating Example. Consider again the \nprogram in Figure 1 with the same input constraints we described in \u00a73.1. We now illustrate how QCE can \nbe used to decide whether to merge states at lines 6 and 7. We use the heuristic parameters a = 0.5, \n\u00df = 0.6 and, to keep the example brief, we set . = 1. First, we pre-compute Qt(7) and Qadd(7,v) for v \n.{r, arg} using Equation (4). For brevity, we omit the computation of Qadd for argc, argv, and array \ncontents referenced by argv. For Qadd(7, arg), we get Qadd(7, arg)= q(7,c) = \u00df q(8,c)+ \u00df q(10,c)+ c(7,arg \n< argc)= \u00df q(8,c)+ 1 = \u00df (\u00df q(9,c)+ \u00df q(10, c)+ c(8,argv[arg][i]= 0)) + 1 = \u00df (\u00df q(9,c)+ 1)+ 1 = \u00df (\u00df \nq(10, c)+ 1)+ 1 = \u00df + 1 = 1.6,  '' where c = . (e,e).ite((7,arg) < (e,e),1,0). Similarly, we com\u00adpute \nthat Qadd(7, r)= \u00df +2\u00df 2 = 1.32 and Qt(7)= 1+2\u00df +2\u00df 2 = 2.92 and, according to Equation (2), H(7)= {arg}. \nAs there are no branches between lines 6 and 7, we have H(6)= H(7)= {arg}. Hence, in this example, the \nQCE similarity relation (1) allows the states at line 6 or 7 to be merged if the values of arg in the \ntwo states are either equal or symbolic. This is consistent with the re\u00adsults of our manual analysis \nin \u00a73.1. 3.3 Justi.cation  We now link our design of QCE to a cost model through the suc\u00adcessive application \nof .ve key simplifying assumptions. Since QCE is merely a heuristic and not a precise computation, the \nfollowing only provides a justi.cation for the reasoning behind it, but not a formal derivation. Here \nwe explain a full variant of QCE that in\u00adcludes an estimate of the cost for introducing ite expressions, \neven though it is not currently implemented in our prototype. As mentioned above, an optimal heuristic \nfor the similarity relation would compute whether the cumulative solving time Tm for 0 instr(e ' )= halt \nwhich simpli.es to (. - 1)Qite + Qadd < Qt. (5) The values for Qt, Qite, and Qadd must be computed over \nthe set of all feasible executions of the merged state. To statically estimate the feasibility of future \npaths, we add the following simpli.cation: Simplifying Assumption 3. Each branch of a conditional state\u00adment \nis feasible with probability 0.5 < \u00df < 1, independently of the other branch. We can now estimate the \nquery counts recursively. In the follow\u00ad ' ing de.nition, which is restated from (3), function c(e,e) \ncan be instantiated for Qt, Qite, and Qadd individually to return 1 if check\u00ad ' ing the feasibility of \na branch condition e at location e causes a query of the speci.c type (regular, involving ite expressions, \nor ad\u00additional), or 0 otherwise:1 ' q(e \u00df . .. (6) ,c)= ''' '' ' ),c)+ \u00df \u00b7q(e,c)+ c(e,e) instr(e ' )= \nif(e) goto e \u00b7q(succ(e all descendants of the merged state is guaranteed to be less than the combined \nrespective times T1 and T2 for the two individual states, i.e., whether Tm < T1 +T2. In the ideal case \nof merging two identical states, we would have Tm = T1 = T2. Thus, merging just two states could theoretically \ncut the remaining exploration time in half. This is why, in principle, repeated merging can reduce the \ncumulative solving time by an exponential factor. Precisely predicting the time required for solving \na formula without actually solving it is generally impossible, therefore we apply a .rst simpli.cation: \nSimplifying Assumption 1. A query takes one time unit to solve. Introducing new ite expressions into \nthe query increases the cost to . > 1 time units, where . is a parameter of the heuristic. Thus, we assume \nthe estimated solving time to be linear in the number of queries of each type. In a further simpli.cation, \nwe treat the number of queries that each one of two merge candidates would individually cause in the \nfuture as equal: Simplifying Assumption 2. Two states at the same program loca\u00adtion that are candidates \nfor merging will cause the same number Qt of queries if they are explored separately. This simpli.cation \nis a prerequisite for statically computing query counts for a location in a way that is independent of \nthe actual states during symbolic execution. The merged state then will also invoke these Qt queries, \nbut some queries will take longer to solve due to introduced ite expressions, and some additional queries \nbecome necessary. We denote the number of queries into which merging introduces ite expressions by Qite \n(with Qite = Qt). The total cumulative cost of solving these queries is . \u00b7 Qite, as per our .rst simpli.cation. \nAdditionally, the merged state can require extra solver invocations for queries corresponding to branch \nconditions that depend on constant but different values in the individual states (as in the loop conditions \non lines 9 and 10 of Figure 1). This number of additional queries is Qadd. Note that we ignore the possible \ncost of introducing disjunc\u00adtions into the path condition. In many common cases, the differ\u00adent conjuncts \nof the two path conditions are just negations of each other, and thus the disjunctive path condition \ncan be simpli.ed to the common pre.x of the two individual path conditions. With these simpli.cations, \nthe total cost of solver queries in the merged state is 1 \u00b7 (Qt - Qite) for the remaining regular queries \nplus . \u00b7 Qite for queries involving new ite expressions, plus 1 \u00b7 Qadd for the additional queries. We \ncan thus formulate the criterion for performing a single merge as Qt - Qite + . \u00b7 Qite + Qadd < 2 \u00b7 Qt, \n' ),c) otherwise q(succ(e .. For this de.nition, loop unrolling ensures that conditional state\u00adments \nin loops are counted as many times as the loop can execute. Loops and recursive calls with bounds that \nare not statically known are unrolled up to a .xed depth, given by the heuristic parameter .. The symbolic \nexecution engine issues a query whenever a state (e, pc,s) encounters a branch with a conditional expression \ne that depends on program input, i.e., e evaluates to an expression s[e] containing variables from the \nset of inputs I. We denote this by I < s[e]. To ease notation, we add the following shorthands: we use \ndef s1[v]=ss2[v] . (I < s1[v] . I < s2[v]) . s1[v]= s2[v] for the con\u00addition causing ite expressions, \ni.e., symbolic but non-equal vari\u00ad def ables in two states, and we use s1[v]=cs2[v] .\u00ac(I < s1[v] . I \n< s2[v]) . s1[v]= s2[v] for the condition causing additional queries, i.e., concrete and non-equal variables \nin two states. ' To de.ne a function c(e, e) for the different types of query counts, we need a method \nto check whether the branch condition e depends on inputs when reached from one of the individual states. \nWe approximate this statically using a path-insensitive data ' dependence analysis, and write (e,v) < \n(e,e) if expression e at ' location e may depend on the value of variable v at location e. Thus, we can \nde.ne the query counts as follows: ' Qt((e, pc1,s1),(e, pc2,s2)) = q e,. (e,e). ' ite(.v:(I <s1[v]. I \n<s2[v]).(e,v)<(e,e)),1, 0 ' Qite((e, pc1,s1), (e, pc2,s2)) = q e,. (e,e). ' ite(.v : s1[v]=ss2[v] . (e,v)< \n(e,e)),1,0 ' Qadd((e, pc1, s1),(e, pc2,s2)) = q e,. (e,e). ' ite(.v : s1[v]=cs2[v] . (e,v)< (e,e),1, \n0 Computing this recursive relation is expensive, and it cannot be pre-computed before symbolic execution \nbecause it requires determining which variables depend on program inputs in the states considered for \nmerging. We therefore assume a .xed probability of input dependence: Simplifying Assumption 4. The number \nof branches whose con\u00additions are dependent on inputs is a .xed fraction . of the total number of conditional \nbranches. 1 Note that, for simplicity of exposition, we only refer to branch conditions here. In practice, \nother instructions, such as assertion checks or memory accesses with input-dependent offsets, will also \ntrigger solver queries. Our implementation extends the de.nition of c to account for these queries. \n This enables us to eliminate all variable dependencies from Qt ' and simplify it to Qt(e)= . \u00b7q(e,. \n(e,e). 1). Now, Qt depends only on the program location and can thus be statically pre-computed. Qite \nand Qadd count queries for which speci.c variable pairs are not equal in the two merge candidates. Therefore, \nwe would need to statically pre-compute Qite and Qadd for each subset of variables that could be symbolic \nin either state during symbolic execution. To eliminate this dependency on the combination of speci.c \nvari\u00adables, we compute query counts for individual variables. The per\u00advariable query counts Qite(e,v) \nand Qadd(e,v) are de.ned as the value of Qite(e) and Qadd(e), respectively, computed as if v was the \nonly variable that differs between the merge candidates. The per\u00advariable query counts can be computed \nas Qite(e,v)= Qadd(e,v)= '' q (e,. (e,e).ite((e,v)< (e,e), 1,0)). Summing the per-variable query counts \nfor all variables that differ between the merge candidates will grossly over-estimate the actual values \nof Qite and Qadd, since conditional expressions often depend on more than just one variable, and many \nqueries would thus be counted multiple times. Similarly, using just the maximum per-variable query count \nwould cause an under-estimation. In fact, max Qadd(e,v) = Qadd(e) = . Qadd(e,v) {v.V |s1[v]=cs2[v]} {v.V \n|s1[v]=cs2[v]} and analogously for Qite. We therefore make a .nal simpli.cation: Simplifying Assumption \n5. Total query counts are equal to the maximum per-variable query counts for an individual variable times \nsome factor s, i.e., Qite(e) s \u00b7 max Qite(e, v) {v.V |s1[v]=ss2[v]} max Qadd(e) s \u00b7 Qadd(e,v). {v.V \n|s1[v]=cs2[v]} The intuition behind this assumption is that the number of independent variables correlates \nwith the input size and not with the total number of variables. Applying this substitution to Equation \n(5) we can now de.ne the similarity relation ~qce as def (e, pc1, s1) ~qce (e, pc2,s2) .. (7) Qt (. -1) \nmax Qite(e,v)+ max Qadd(e,v) < {v.V |s1[v]=ss2[v]}{v.V |s1[v]=cs2[v]} s with '' Qite(e,v)= Qadd(e,v)= \nq e,. (e, e).ite (e,v) < (e,e),1, 0 , ' Qt(e)= . \u00b7 q e,. (e,e). 1 , and the recursively descending q \nas de.ned in Equation (6). For convenience, we rename s. to the uni.ed parameter a. Thus, a, \u00df , . and \nthe unrolling bound . remain as the only parameters to QCE. The variant of QCE implemented in our prototype \nis derived from Equation (7) by removing Qite from the criterion, to arrive at max Qadd(e, v) < aQt, \n{v.V |s1[v]=cs2 [v]} which is equivalent to .v . V : s1[v]=cs2[v] . Qadd(e,v) < aQt. To facilitate an \nef.cient implementation in combination with dy\u00adnamic state merging, as discussed in the next section, \nwe col\u00adlect a set of variables that exceed the threshold Hadd(e)= {v . V | Qadd(v) > aQt} and can state \nthe similarity relation as (1). This motivates the use of QCE for estimating the similarity of states. \nWe show that QCE is effective in practice in \u00a75. 4. Dynamic State Merging We now explain the challenges \nfor applying state merging in fully automated, precise, but incomplete symbolic program analy\u00ad 1 if (logPacketHash) \n{ 2 hash = computeHash(pkt); 3 log(\"Packet: %s, hash: %s\", pkt->name, hash); 4 } else { 5 log(\"Packet: \n%s\", pkt->name); 6 } 7 handlePacket(pkt); Figure 2. Example code illustrating how static state merging \ncan interfere with search heuristics. sis (\u00a74.1). To overcome these problems, we motivate (\u00a74.2) and \nintroduce (\u00a74.3) the dynamic state merging algorithm. 4.1 Static Merging and Incomplete Exploration A \nsymbolic program analysis using static state merging traverses the CFG in topological order and attempts \nto merge states at ev\u00adery joint point. This allows to perform exhaustive exploration with state merging \nin the fewest possible steps. To ensure termination, the analysis has to stop loop unrolling at a certain \ndepth, unless loops can be summarized by loop invariants. This method is opti\u00admal for veri.cation condition \ngenerators that encode full programs with bounded or summarized loops. Search-based symbolic exe\u00adcution \nengines, however, which typically perform incomplete ex\u00adplorations, do not bound loops but are guided \nby search strategies that prioritize exploring new code over unrolling additional loop it\u00aderations. An \nexploration in strict topological order would override such strategies and stall the engine by requiring \nit to fully unroll the possibly in.nitely many iterations of a loop before proceeding. This is a problem \neven for loops that symbolic execution could, in principle, explore exhaustively. Coverage-oriented search \nstrate\u00adgies are designed to quickly maximize metrics such as statement coverage. The restriction to topological \norder interferes with such strategies, reducing their performance or even completely stopping them from \nachieving any progress towards their goal. We support this argument with experimental evidence in \u00a75.5. \nConsider the example code in Figure 2. Depending on the .ag logPacketHash, the program writes to a log \neither just the name of a packet or both the name and a hash value. The code then processes the packet. \nIn this example, exploring the else branch of the conditional statement on line 1 is fast, while ex\u00adploring \nthe then branch is expensive due to the computeHash function processing the entire input packet. A coverage-oriented \nsearch strategy will likely choose to explore the else branch .rst to quickly reach handlePacket, or \nswitch to the else branch after not making progress towards its coverage goal being stuck unrolling loops \ninside the computeHash function. With static state merging, the symbolic execution engine has to merge \nall states at line 7, which requires all execution paths to be ex\u00adplored exhaustively up to that location. \nTherefore, no code in the handlePacket function can be reached before exploring every path in the computeHash \nfunction, thus being in con.ict with the coverage-oriented search heuristic. This con.ict could be solved \nby allowing the merge of only those states that, according to the cho\u00adsen search strategy, would reach \nthe same point in a program at about the same time. In our example, the state that takes the else branch \nshould not be merged with any state that takes the then branch, allowing the search strategy to prioritize \nit independently. 4.2 Rationale Behind Dynamic State Merging To solve the problems of static state merging, \nwe propose dynamic state merging (DSM). DSM does not require states to share the same program location \nin order to be considered for merging. The rationale behind dynamic state merging is the following: consider \ntwo abstract states a1 =(e1,pc1,s1) and a2 =(e2, pc2,s2), with ' e1 = e2, that are both in the worklist. \nAssume that a1, one of the transitive successors of a1 (which have not been computed yet) Input: Worklist \nw, choice functions pickNextD and  pickNextF , similarity relation ~, trace function pred, threshold \nd Data: Forwarding set F. Result: The next state to execute. // Determine the forwarding set 1 F := {a \n. w |.a '. w : .a '' . pred(a ' ,d ) : a ~ a ''}; 2 if F = \u00d8then // Choose a state from the forwarding \nset 3 return pickNextF (F) 4 else // Choose a state using the driving heuristic 5 return pickNextD(w) \nAlgorithm 2. The pickNext method for dynamic state merging. will reach location e2. Provided that the \nnumber of steps required ' to reach e2 from a1 is small, and the expected similarity of a 1 and a2 is \nhigh, enough that merging them will be bene.cial, it is worth overriding a coverage-oriented search strategy \nto compute ' a1 next and to then merge it with a2. We refer to this override as fast-forwarding, because \na1 is forwarded to a2 s location with temporary priority before resuming the regular search strategy. \nTo check whether a1 can be expected to be similar to a2 in the near future, we check whether a1 could \nhave been merged '' with a predecessor a2 of a2, i.e., whether a1 ~ a2. The underlying expectation, which \nour experiments con.rm, is that if two states are similar, then their two respective successors after \na few steps of execution are also likely (but not guaranteed) to be similar. Note that fast-forwarding \ndeals with special cases automati\u00adcally: if a state forks while being fast-forwarded, all children that \nare still similar to a recent predecessor of a state in the worklist are fast-forwarded. If a state leaves \nthe path taken by the state it is sim\u00adilar to, i.e., fast-forwarding diverges, the state is no longer \nsimilar to any predecessor and is thus no longer prioritized. 4.3 The Dynamic State Merging Algorithm \nThe DSM algorithm is an instance of Algorithm 1 with a pickNext function as de.ned in Algorithm 2. DSM \nrelies on an external driv\u00ading heuristic (given as a function pickNextD), such as a traditional coverage-oriented \nheuristic, to select the next state. However, when the algorithm detects that some states, computed as \na set F, are likely to be mergeable after at most d steps of execution, DSM over\u00adrides the driving heuristic \nand picks the next state to execute from F according to another external search heuristic pickNextF . \nThe algorithm uses a function pred(a,d ) to compute the set of predecessors of a within a distance of \nd . The function can ' be de.ned as pred(a,d )= {a |.n = d : a . postn(a ' )}, where post(a ' ) denotes \nthe set of immediate successor states computed by Algorithm 1 for a state a '. Keeping a precise history \nof reached states can incur prohibitive space costs, but we reduce the space requirements as follows: \nstates from the history are only used for comparisons with respect to ~, hence it is only required to \nstore the parts of the state that are relevant to the relation. Moreover, if the ~ relation is only sensitive \nto equality, the implementation can store and compare hash values of the relevant information from past \nstates. In this case, checking whether the state belongs to F is implemented as a simple hash table lookup. \nHash collisions do not pose a problem, because a full check of the similarity relation is still performed \nwhen fast-forwarding .nishes and the states are about to be merged. Moreover, the set F is rebuilt after \neach execution step, so, if a state was added to F due to a collision, it is unlikely to be added again \nto F in the next step, as a second collision for two different hash values has low probability. The relation \n~qce, de.ned in Equation (1) in \u00a73.3, can be mod\u00ad i.ed to check for equality only, as required for using \nhashing in the implementation. We express the condition for variables to be either symbolic or equal \nin Equation (1) by h(s1[v]) = h(s2[v]), where h(v)= ite(I < v,*,v) .lters out symbolic variables by map\u00adping \nthem to a unique special value. The implementation can thus store just the hash value of v.H(e) h(v) \nfor a state. Then ~qce can be checked by comparing the hash values of the two states (modulo hash collisions). \nThe function pickNextF determines the execution order among the states selected for fast-forwarding. \nIn our implementation, we pick the .rst state from F according to the topological order of the CFG. Thus, \nstates that lie behind with respect to the topological order .rst catch up and are merged with later \nstates. 5. Experimental Evaluation We now show that, in practice, our approach attains exponen\u00adtial speedup \ncompared to base symbolic execution (Figure 5). We .rst introduce our prototype implementation (\u00a75.1) \nand present our evaluation metrics (\u00a75.2). We then evaluate how DSM com\u00adbined with QCE improves the thoroughness \nof program exploration in a time-bounded scenario (\u00a75.3). We then show that QCE lies at a sweet spot \nbetween single-path exploration and static merg\u00ading (\u00a75.4). Finally, we demonstrate that DSM is essential \nfor com\u00adbining state merging with coverage-oriented search strategies in in\u00adcomplete exploration (\u00a75.5). \n5.1 Prototype We built our prototype on top of the KLEE symbolic execution en\u00adgine [6]. It takes as input \na program in LLVM bitcode [25] and a speci.cation of which program inputs should be marked as sym\u00adbolic \nfor the analysis: command-line arguments or .le contents. KLEE implements precise non-compositional symbolic \nexecution with feasibility checks performed at every conditional branch. It uses search strategies to \nguide exploration; the stock strategies in\u00adclude random search and a strategy biased toward covering \nprevi\u00adously unexplored program statements. To support QCE, we extended KLEE to perform a static analy\u00adsis \nof the LLVM bitcode to compute local query count estimates as explained in \u00a73.2. The analysis is executed \nbefore the path explo\u00ad ration and annotates each program location with the corresponding query count \nestimates Qt(e) and Qadd(e,v) as de.ned in \u00a73.2. The pass is implemented as an LLVM per-function bottom-up \ncall graph traversal (with bounded recursion) and performs the analysis com\u00adpositionally. When analyzing \neach function, the pass attempts to statically determine trip counts (number of iterations) for loops. \nIf it cannot, it approximates them with the loop bound parameter .. The QCE analysis tracks the query \ncount for local variables, func\u00adtion arguments, and in-memory variables indexed by a constant offset \nand pointed to by either a local variable, a function argu\u00adment, or a global variable. We check data \ndependencies between variables by traversing the program in SSA form. As LLVM s SSA form handles only \nlocal variables, we do not track dependencies between in-memory variables except when loading them to \nlocals. We modi.ed KLEE to compute interprocedural query counts and sets of hot variables H(e) dynamically \nduring symbolic execution, following \u00a73.2. We implemented DSM as de.ned by Algorithm 2 in the form of \na search strategy layer in KLEE s stacked strategy system plus an execution tracking system that incrementally \ncomputes state hashes (see \u00a74.3). Each strategy uses its own logic to select a state from the worklist, \nbut can rely on an underlying strategy whenever it has to make a choice among a set of equally important \nstates. In our case, the DSM strategy returns a state from the fast\u00adforwarding set of states (pickNextF \n), or, if this set is empty, it resorts to the underlying driving heuristic to select a state from the \ngeneral worklist (pickNextD). Depending on the purpose of each experiment, we employ different driving \nheuristics: for complete Figure 3. The exact number of paths as a function of state multi\u00adplicity for \n3 COREUTILS tools. Both axes are logarithmic.  explorations, we used random search, while for partial \nexplorations aimed at obtaining statement coverage, we employed the coverage\u00adoriented search heuristics \nin [6]. Evaluation Targets. We performed all our experiments on the COREUTILS suite of widely used UNIX \ncommand-line utilities, ranging from .le manipulation (cp, mv, etc.) to text processing (cut, sort, etc.) \nand shell control .ow (e.g., test). The total size of the COREUTILS code is 72.1 KLOC, as measured by \nSLOC-COUNT [30]. We tested these tools using symbolic command line arguments and stdin as input. In some \ncases, the symbolic input size is small enough for KLEE to complete the exploration in less than 5 minutes. \nWe discarded these data points from our evaluation, since such a short period of time is dominated by \nthe constant over\u00adhead of our static analysis, whereas we are interested in evaluating the prototype \ns asymptotic behavior. 5.2 Evaluation Metrics We evaluate our prototype by comparing it to non-merging \nsearch\u00adbased symbolic execution as implemented in KLEE. We perform the comparison according to (1) the \namount of exploration performed given a .xed time budget, and (2) the time necessary to complete a .xed \nexploration task, i.e., the exhaustive search of a set of paths determined by a given symbolic input. \nEstimating Number of Paths in Merged States. A direct com\u00adparison of the amount of exploration between \nmerge-based and reg\u00adular symbolic execution is dif.cult, because counting the feasible paths that have \nbeen explored with state merging requires checking the feasibility of each path individually. This is \nas hard as repeating the exploration without merging, so it is impractical in the cases where symbolic \nexecution without merging times out. We therefore estimate the path count in a merged state with the \nhelp of state multiplicity: the multiplicity of a single-path state is 1; when two states merge, the \nmultiplicity of the resulting state is the sum of their multiplicities. State multiplicity over-estimates \nthe path count because, when the state is split at later branches, state multiplicity carries over to \nboth child states, effectively doubling the counted number of paths at each branch (assuming that, as \nlong as a branch is feasible for the merged state, it is also feasible for all the paths represented \nby it). For our estimation, we made the assumption that path explo\u00adsion can be modeled as an exponential \nfunction a \u00b7 2b\u00b7n, where a and b are program-speci.c constants, and the growth parameter n indicates \nthe progress of unrolling the program CFG along the paths of interest. Both the path count p and state \nmultiplicity m are then based on this formula, each with different values for the constants a and b. \nSince n depends on the behavior of the search strategy, it cannot be easily determined. However, we can \navoid computing n because, at any point in time, both the exact path count and the Figure 4. Relative \nincrease in explored paths for DSM + QCE vs. regular KLEE (1h time budget). Each bar represents a COREUTIL. \nstate multiplicity have the same growth parameter n correspond\u00ading to the current exploration progress. \nHence, we can take the two model equations m = am \u00b72bm \u00b7n and p = ap \u00b72bp \u00b7n and obtain n from the .rst \nequation and substitute it in the second. This yields a rela\u00adtion between p and m of the form log p \nc1 + c2 logm, where c1 and c2 are program-dependent coef.cients. To empirically validate this relation, \nwe extended our prototype to accurately track the number of feasible paths by maintaining all the original \nsingle-path states along with the merged states. We counted the exact number of paths and the state multiplicity \nfor 1 hour and con.rmed a linear relation between the logarithms of the two values. Figure 3 illustrates \nthe dependency between m and p with representative measurements of c2 for 3 COREUTILS. In the experiments \nreported in the rest of this section, we ap\u00adproximated the number of paths for state merging as follows. \nFirst, we ran the experiments for 1 hour, while accurately tracking the number of feasible paths as explained \nabove (if exploration with merging was 1000\u00d7 faster, this 1 hour would correspond to ~4 seconds of exploration). \nFrom this data, we computed the values of c1 and c2. Second, we ran the full experiments, while tracking \nonly the state multiplicity for merged states, which does not incur a signi.cant overhead. Using c1 and \nc2, we then converted the state multiplicity values into the estimation of the feasible path count. 5.3 \nFaster Path Exploration with DSM and QCE We were .rst concerned with how much DSM and QCE, when combined, \nspeed up symbolic execution. We let both our prototype and KLEE run for 1 hour on each of the COREUTILS, \nand we measured the number of paths explored by each tool. The size of the symbolic inputs passed to \neach utility was large enough to keep each tool busy for the duration of each run. Figure 4 shows, for \neach of the tested COREUTILS, a bar rep\u00adresenting the ratio between the number of program paths explored \nby our prototype and KLEE, respectively. The results indicate that our technique explores up to 11 orders \nof magnitude more paths than plain symbolic execution in the same amount of time. On 14 utilities, our \nprototype explored fewer paths, which we believe to be due to ignoring ite expressions (see \u00a73.3) and \nlimitations of our prototype (see \u00a75.1). Our prototype crashed on 5 utilities, which we do not include \nin our results. We show a single representative for every tool aliased by multiple names (e.g., ls, dir, \nand vdir). 5.4 Achieving Exponential Speedup with QCE We now answer the question of how much faster can \nour technique exhaustively explore a program for a .xed input size. In exhaustive exploration, a coverage-oriented \nsearch strategy is not necessary, therefore we focused on the effects of QCE alone. We used it in implementing \na selective form of static state merging instead of DSM. Static state merging (SSM) chooses states from \nthe worklist Figure 5. Speedup of QCE versus input size for exhaustive explo\u00adration of three representative \nCOREUTILS.  in topological order and attempts to merge them at control .ow join points. Selective SSM \nuses query count estimation to keep some states separate. Intuitively (and con.rmed experimentally in \n\u00a75.5), SSM performs better than DSM in exhaustive exploration, since it avoids unnecessary computation. \nHowever, for incomplete exploration, SSM performs worse than DSM. We evaluate QCE from two perspectives. \nFirst, we look at how much QCE speeds up state merging for complete path exploration tasks. Second, we \nlook at how the QCE heuristic parameters in\u00ad.uence performance. We collected our measurements by running \neach of the COREUTILS for 2 hours, using plain KLEE, and QCE with SSM. For each con.guration, we ran \nexperiments with multi\u00adple values of symbolic input size, and we measured the correspond\u00ading completion \ntime. Exhaustive Exploration. Figure 5 shows the evolution of the completion time ratio (speedup) between \nSSM using QCE and plain KLEE for three representative COREUTILS programs, as we in\u00adcrease the symbolic \ninput size. One of them achieved the highest speedup, another shows an average speedup, while the last \ndoes not show any improvement. This graph illustrates that our pro\u00adtotype completed the exploration goal \nexponentially faster as the symbolic input size increased. Figure 5 also shows that applying QCE does \nnot always lead to speedup. However, Figure 6 shows this is actually an infrequent case. The scatter \nplot illustrates how the execution time of SSM using QCE compares to KLEE when ag\u00adgregating over the \nentire set of experiments. The black dots corre\u00adspond to experiment instances where both our prototype \nand KLEE .nished on time, while the triangles on the right side correspond to situations where KLEE timed \nout after 2 hours (and thus indicate a lower bound on the actual speedup). The gray disks indicate the \nrel\u00adative size of the symbolic inputs used in each experiment instance. Since, in the majority of cases, \nKLEE times out without complet\u00ading the exploration, we only show on this graph the timeout points of \nthe smallest input size for each tool, which give a loose lower bound on the speedup. We notice that \nmost of the points in Figure 6 are located in the lower-right part of the graph, which correspond to \nhigher speedup values. Moreover, in the cases where both KLEE and our prototype .nish on time, large \nspeedups (the lower-right part of the graph) tend to correspond to larger sizes of the sym\u00adbolic input \n(larger gray circles). This re-con.rms the fact that our speedup is proportional to the size of program \ninput, i.e., the very source of the exponential growth of paths that bottlenecks KLEE. To get a better \nunderstanding of the behavior of QCE, we took a deeper look at the execution of several COREUTILS tools: \nsome that exhibit high speedup, and some for which the performance of our prototype is worse than KLEE \ns. We extended our prototype to explore states in both merged and unmerged forms. For every solver query \nin a merged state, we matched all the queries during the same execution step in all the corresponding \nunmerged states. We then Figure 6. QCE + SSM vs. plain KLEE with varying input sizes (shown as gray disk \nsize). Triangles denote that KLEE timed out after 2h and are thus lower bounds on the actual speedup. \n compared query times in the merged and unmerged versions and identi.ed those that became slower due \nto state merging. We discovered that, even for tools that exhibited high overall speedup, some queries \nare more expensive in the merged state than the corresponding queries for the unmerged states combined. \nIn these cases, however, the slowdown was amortized by the reduction in the total number of states to \nexplore, and hence the total number of queries to solve. A typical example is the sleep utility, which \nreads a list of integers from the command line and sums their value in the variable seconds. It then \nvalidates the resulting value and performs the actual sleep operation. Here, QCE does not identify seconds \nas a hot variable, and all states forked during parsing are merged into a single state, avoiding the \nexponential increase in paths for each additional integer parsed. Since the value of seconds depends \non the parsing result, it becomes a complex symbolic expression in the merged state, leading to several \ncomplex solver queries in the validation code. Nevertheless, these queries are amortized by the substantial \nreduction in the number of states to analyze. This example shows that QCE does allow merging of states \nthat differ in live variables, so it is strictly more general than methods based on live variable analysis \n[3]. In cases where our prototype performed worse than plain KLEE, we observed a large number of queries \nthat were more expensive in the merged state. These queries commonly contained ite expres\u00adsions and disjunctive \npath conditions introduced by state merging. The former case shows that our QCE prototype can be improved \nby including the estimation of ite expressions introduced by state merging, as described in \u00a73.3. The \nlatter suggests using a constraint solver that can handle disjunctions more ef.ciently. In.uence of Heuristic \nParameters. The values of the QCE pa\u00adrameters a, \u00df , and . affect the exploration time of each program. \nWe determined optimal values for a and \u00df experimentally, using hill-climbing over four COREUTILS chosen \nat random, and ob\u00adtained a = 10-12 and \u00df = 0.8. We then reused the same values for all other experiments \nin our evaluation and found that these values perform well in practice. Regarding the loop bound ., we \nnoticed that many of the loops with an input-dependent number of itera\u00adtions actually iterate over program \ninputs. Hence, we chose . = 10 corresponding to the average input size in our experiments. We observed \nthat, among these parameters, the value of a has the highest impact on the running time. In essence, \na controls how aggressively the engine tries to merge. When a is 8, no variables Figure 7. Impact on \nperformance of the threshold parameter a.   Figure 8. Change in statement coverage of DSM and SSM vs. \nregular KLEE for a coverage-oriented, incomplete exploration. are determined to be hot, and QCE allows \nall states to be merged. When a is 0, states that contain variables with different concrete values are \nnever merged. Due to this property, we call a the QCE threshold parameter. To illustrate this dependency, \nwe randomly chose four COREUTILS (link, nice, paste, pr) and ran them using SSM for up to 1 hour with \ndifferent values of a. Figure 7 shows, for each target program, the dependency of the completion time \non the threshold parameter. The special point (no merge) on the x-axis corresponds to executions with \nstate merging disabled. Note that here we did not cut execution times below 5 minutes. 5.5 Reaching an \nExploration Goal with DSM We now verify whether DSM allows the underlying driving heuris\u00adtic to reach \nthe goal while still merging states according to QCE. To isolate the effects of DSM, we compare DSM to \nour SSM implemen\u00adtation, with both using QCE in making merge decisions. First, we use the coverage-oriented \nsearch heuristic from [6] with DSM, and look at how much statement coverage it can achieve in an incomplete \nsetting (1 hour timeout and large search space). The value of the fast-forwarding distance d was chosen \nexperi\u00admentally to equal 8 basic blocks. Figure 8 compares the increase in statement coverage obtained \nby DSM and SSM over base KLEE on those COREUTILS for which the exploration remained incom\u00adplete after \n1 hour. SSM consistently obtains worse coverage values, con.rming its inability to adapt to the exploration \ngoal. However, DSM roughly matches the coverage values of the underlying driving heuristic. Thus, the \nexperiment con.rms that DSM s merging avoids interfering with the logic of the driving heuristic, while \ntraversing orders of magnitude more paths (\u00a75.3). Even though these addi\u00ad tional paths do not necessarily \nincrease statement coverage, they can increase other coverage metrics and ultimately offer higher con.dence \nin the resulting tests. We measured that, on average, 69% of the states selected for fast-forwarding \nwere successfully merged with another state. Hence, the DSM approach to predict state similarity (\u00a74.3) \nworks well in practice. Figure 9. Comparison between the time needed to achieve exhaus\u00adtive exploration \nfor SSM and DSM. Second, we evaluated the penalty of DSM compared to SSM in exhaustive exploration (see \n\u00a75.4). For this experiment, we ran both techniques with varying input sizes. Figure 9 aggregates the \nresults in a scatter plot. Most data points are grouped around the diagonal, indicating that the performance \nof both techniques is comparable, even though DSM is slower than SSM by 15% on average. We conclude that \nDSM, while being slightly less ef.cient than SSM in exhaustive exploration, meets its purpose of allowing \nthe driving heuristic to follow the exploration goal. Overall, the combination of DSM and QCE offers \nexponential speedups over KLEE, which suggests that these are two important steps towards improving the \nperformance of symbolic execution. 6. Related Work We discussed the most closely related work in \u00a72, \nwhere we fo\u00ad cused on what we called precise symbolic program analysis. In this section, we look a bit \nfurther into alternative approaches that are similar in that they build symbolic expressions and rely \non SAT or SMT solving, but use other techniques for improving scalability. A .rst class of techniques \nfocuses on pruning redundant states in symbolic execution. Boonstoppel et al. [3] dynamically deter\u00ad \nmine variable liveness during symbolic execution. Their analysis considers the already explored paths \nthrough the current statement and determines the variables that are dead on all paths. It then uses the \nrest of the variables to check whether a state is equivalent to a previously explored one and can be \nsafely pruned. In a sense, this is a special case of QCE, where no merging is performed unless the differing \nvariables never used again. In our approach, we do not actually prune paths but still represent them \nin the merged state. If the differing variables are never used, this is equivalent to pruning one of \nthe paths. This allows us to use imprecise static analysis and to merge in cases where variables are \nnot dead but just rarely used. McMillan [28] introduces lazy annotation in symbolic execu\u00ad tion to build \nsummaries on the .y and generalize them by Craig in\u00adterpolation. This generalization goes beyond regular \nsymbolic sum\u00admaries, but is also computationally more expensive; we would like to measure the net effectiveness \nof this technique in future work. A proven effective way to scale up symbolic program analysis is to \nforgo precision and introduce abstraction. Saturn [31] uses a symbolic exploration algorithm to build \nveri.cation conditions for speci.c properties. It is speci.cally designed to .nd bugs in large system \nsoftware and therefore sacri.ces precision at several points. Loops are unrolled just once, and functions \nare aggressively sum\u00admarized. Similarly, Calysto [2] relies on structural abstraction to initially represent \nfunction effects as fresh variables. False positives are iteratively eliminated by replacing these variables \nwith precise function summaries.  The bounded model checker in the Varvel/F-Soft veri.cation platform \nuses lightweight static analysis to infer over-approximate function summaries that are only applied below \na con.gurable depth in the call graph [21, 22]. Therefore, it introduces abstrac\u00ad tion only at deeper \nlevels, in an effort to reduce false positives. Sery et al. [29] describe the use of over-approximate \nsummaries in bounded model checking. Whenever assertion violations are found, their method falls back \nto inlining, to avoid false positives. There\u00adfore, speedups are only attainable for successful veri.cation \nruns. Abstraction-based analyses could scale signi.cantly better than symbolic execution. However, they \nare prone to false positives and, perhaps more importantly, are harder to deploy. Symbolic execu\u00adtion \nengines do not require hand-written stubs for external func\u00adtions or system calls, but can instead simply \nexecute the call by concretizing its parameters. This sacri.ces the theoretical guaran\u00adtee of eventually \nachieving complete path coverage, but is a signif\u00adicant advantage for test case generation and bug .nding. \n7. Conclusions In symbolic execution, state merging reduces the number of states that have to be explored, \nbut increases the burden on the constraint solver. We introduced two techniques for reaping practical \nbene\u00ad.ts from state merging: query count estimation and dynamic state merging. With this combination \nof techniques, state merging be\u00adcomes completely dynamic and bene.t-driven, unlike static strate\u00adgies \nsuch as static merging or precise function summaries. We ex\u00adperimentally con.rmed that our approach can \nsigni.cantly improve exploration time and coverage. This suggests that we have indeed come close to a \nsweet spot in balancing the simplicity of exploring single paths vs. the reduction of redundancy in exploring \nmultiple paths in a merged state. Other types of precise symbolic program analysis face simi\u00adlar design \nchoices for grouping paths, but approach the sweet spot from a different angle. Therefore, we believe \nthat our results gener\u00adalize beyond just symbolic execution and that, for example, query count estimation \ncan serve as a partitioning strategy for veri.cation conditions in bounded model checking. Acknowledgments \nWe would like to thank P\u00e9ter Bokor, Aarti Gupta, Rupak Majum\u00addar, Raimondas Sasnauskas, Jonas Wagner, \nand Cristian Zam.r for their valuable feedback on earlier drafts of this paper. We are grate\u00adful to Google \nand Microsoft for generously supporting our work. References [1] S. Anand, P. Godefroid, and N. Tillmann. \nDemand-driven composi\u00adtional symbolic execution. In Intl. Conf. on Tools and Algorithms for the Construction \nand Analysis of Systems (TACAS), 2008. [2] D. Babic and A. J. Hu. Calysto: scalable and precise extended \nstatic checking. In Intl. Conf. on Software Engineering (ICSE), 2008. [3] P. Boonstoppel, C. Cadar, and \nD. R. Engler. RWset: Attacking path explosion in constraint-based test generation. In Intl. Conf. on \nTools and Algorithms for the Construction and Analysis of Systems (TACAS), 2008. [4] R. S. Boyer, B. \nElspas, and K. N. Levitt. SELECT a formal system for testing and debugging programs by symbolic execution. \nIn Intl. Conf. on Reliable Software (ICRS), 1975. [5] C. Cadar, V. Ganesh, P. M. Pawlowski, D. L. Dill, \nand D. R. Engler. EXE: Automatically generating inputs of death. In Conf. on Computer and Communications \nSecurity (CCS), 2006. [6] C. Cadar, D. Dunbar, and D. R. Engler. KLEE: Unassisted and au\u00adtomatic generation \nof high-coverage tests for complex systems pro\u00ad grams. In Symp. on Operating Systems Design and Implementation \n(SOSP), 2008. [7] V. Chipounov, V. Kuznetsov, and G. Candea. S2E: A platform for in-vivo multi-path analysis \nof software systems. In Intl. Conf. on Architectural Support for Programming Languages and Operating \nSystems (ASPLOS), 2011. [8] E. Clarke, D. Kroening, and F. Lerda. A tool for checking ANSI-C programs. \nIn Intl. Conf. on Tools and Algorithms for the Construction and Analysis of Systems (TACAS), 2004. [9] \nM. Das, S. Lerner, and M. Seigle. ESP: Path-sensitive program veri\u00ad.cation in polynomial time. In Intl. \nConf. on Programming Language Design and Implem. (PLDI), 2002. [10] L. M. de Moura and N. Bj\u00f8rner. Z3: \nAn ef.cient SMT solver. In Intl. Conf. on Tools and Algorithms for the Construction and Analysis of Systems \n(TACAS), 2008. [11] E. W. Dijkstra. Guarded commands, nondeterminacy and formal derivation of programs. \nCommun. ACM, 18(8):453 457, 1975. [12] N. E\u00e9n and N. S\u00f6rensson. An extensible SAT-solver. In Intl. Conf. \non Theory and Applications of Satis.ability Testing (SAT), 2003. [13] C. Flanagan, K. R. M. Leino, M. \nLillibridge, G. Nelson, J. B. Saxe, and R. Stata. Extended static checking for Java. In Intl. Conf. on \nProgramming Language Design and Implem. (PLDI), 2002. [14] M. K. Ganai and A. Gupta. Tunneling and slicing: \ntowards scalable BMC. In Design Automation Conf. (DAC), 2008. [15] V. Ganesh and D. L. Dill. A decision \nprocedure for bit-vectors and arrays. In Intl. Conf. on Computer Aided Veri.cation (CAV), 2007. [16] \nP. Godefroid. Compositional dynamic test generation. In Symp. on Principles of Programming Languages \n(POPL), 2007. [17] P. Godefroid and D. Luchaup. Automatic partial loop summarization in dynamic test \ngeneration. In Intl. Symp. on Software Testing and Analysis (ISSTA), 2011. [18] P. Godefroid, N. Klarlund, \nand K. Sen. DART: Directed automated random testing. In Intl. Conf. on Programming Language Design and \nImplem. (PLDI), 2005. [19] P. Godefroid, M. Y. Levin, and D. Molnar. Automated whitebox fuzz testing. \nIn Network and Distributed System Security Symp. (NDSS), 2008. [20] T. Hansen, P. Schachte, and H. Sondergaard. \nState joining and splitting for the symbolic execution of binaries. In Intl. Conf. on Runtime Veri.cation \n(RV), 2009. [21] F. Ivancic, Z. Yang, M. K. Ganai, A. Gupta, I. Shlyakhter, and P. Ashar. F-soft: Software \nveri.cation platform. In Intl. Conf. on Computer Aided Veri.cation (CAV), 2005. [22] F. Ivancic, G. Balakrishnan, \nA. Gupta, S. Sankaranarayanan, N. Maeda, H. Tokuoka, T. Imoto, and Y. Miyazaki. DC2: A frame\u00adwork for \nscalable, scope-bounded software veri.cation. In Intl. Conf. on Automated Software Engineering (ASE), \n2011. [23] J. C. King. A new approach to program testing. In Intl. Conf. on Reliable Software (ICRS), \n1975. [24] S. K. Lahiri and S. Qadeer. Back to the future: revisiting precise program veri.cation using \nSMT solvers. In Symp. on Principles of Programming Languages (POPL), 2008. [25] C. Lattner and V. Adve. \nLLVM: A compilation framework for lifelong program analysis and transformation. In Intl. Symp. on Code \nGenera\u00adtion and Optimization (CGO), 2004. [26] K. R. M. Leino and P. R\u00fcmmer. A polymorphic intermediate \nveri.\u00adcation language: Design and logical encoding. In Intl. Conf. on Tools and Algorithms for the Construction \nand Analysis of Systems (TACAS), 2010. [27] L. Mauborgne and X. Rival. Trace partitioning in abstract \ninterpre\u00adtation based static analyzers. In European Symp. on Programming (ESOP), 2005. [28] K. L. McMillan. \nLazy annotation for program testing and veri.cation. In Intl. Conf. on Computer Aided Veri.cation (CAV), \n2010. [29] O. Sery, G. Fedyukovich, and N. Sharygina. Interpolation-based function summaries in bounded \nmodel checking. In Haifa Veri.cation Conf. (HVC), 2011. [30] D. Wheeler. SLOCCount. http://www.dwheeler.com/sloccount/, \n2010. [31] Y. Xie and A. Aiken. Scalable error detection using boolean satis.\u00adability. In Symp. on Principles \nof Programming Languages (POPL), 2005.   \n\t\t\t", "proc_id": "2254064", "abstract": "<p>Symbolic execution has proven to be a practical technique for building automated test case generation and bug finding tools. Nevertheless, due to state explosion, these tools still struggle to achieve scalability. Given a program, one way to reduce the number of states that the tools need to explore is to merge states obtained on different paths. Alas, doing so increases the size of symbolic path conditions (thereby stressing the underlying constraint solver) and interferes with optimizations of the exploration process (also referred to as search strategies). The net effect is that state merging may actually lower performance rather than increase it.</p> <p>We present a way to automatically choose when and how to merge states such that the performance of symbolic execution is significantly increased. First, we present <i>query count estimation</i>, a method for statically estimating the impact that each symbolic variable has on solver queries that follow a potential merge point; states are then merged only when doing so promises to be advantageous. Second, we present <i>dynamic state merging</i>, a technique for merging states that interacts favorably with search strategies in automated test case generation and bug finding tools.</p> <p>Experiments on the 96 GNU Coreutils show that our approach consistently achieves several orders of magnitude speedup over previously published results. Our code and experimental data are publicly available at http://cloud9.epfl.ch.</p>", "authors": [{"name": "Volodymyr Kuznetsov", "author_profile_id": "81538729856", "affiliation": "&#201;cole Polytechnique F&#233;d&#233;rale de Lausanne (EPFL), Lausanne, Switzerland", "person_id": "P3471192", "email_address": "vova.kuznetsov@epfl.ch", "orcid_id": ""}, {"name": "Johannes Kinder", "author_profile_id": "81413599694", "affiliation": "&#201;cole Polytechnique F&#233;d&#233;rale de Lausanne (EPFL), Lausanne, Switzerland", "person_id": "P3471193", "email_address": "johannes.kinder@epfl.ch", "orcid_id": ""}, {"name": "Stefan Bucur", "author_profile_id": "81453653551", "affiliation": "&#201;cole Polytechnique F&#233;d&#233;rale de Lausanne (EPFL), Lausanne, Switzerland", "person_id": "P3471194", "email_address": "stefan.bucur@epfl.ch", "orcid_id": ""}, {"name": "George Candea", "author_profile_id": "81100415229", "affiliation": "&#201;cole Polytechnique F&#233;d&#233;rale de Lausanne (EPFL), Lausanne, Switzerland", "person_id": "P3471195", "email_address": "george.candea@epfl.ch", "orcid_id": ""}], "doi_number": "10.1145/2254064.2254088", "year": "2012", "article_id": "2254088", "conference": "PLDI", "title": "Efficient state merging in symbolic execution", "url": "http://dl.acm.org/citation.cfm?id=2254088"}