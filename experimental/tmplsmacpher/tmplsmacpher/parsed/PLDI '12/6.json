{"article_publication_date": "06-11-2012", "fulltext": "\n Algorithmic Pro.ling Dmitrijs Zaparanuks Matthias Hauswirth University of Lugano University of Lugano \n Dmitrijs.Zaparanuks@usi.ch Matthias.Hauswirth@usi.ch Abstract Traditional pro.lers identify where a \nprogram spends most of its resources. They do not provide information about why the pro\u00adgram spends those \nresources or about how resource consumption would change for different program inputs. In this paper \nwe intro\u00adduce the idea of algorithmic pro.ling. While a traditional pro.ler determines a set of measured \ncost values, an algorithmic pro.ler determines a cost function. It does that by automatically determin\u00ading \nthe inputs of a program, by measuring the program s cost for any given input, and by inferring an empirical \ncost function. Categories and Subject Descriptors C.4 [Performance of Sys\u00adtems]: Measurement techniques; \nD.2.8 [Software Engineering]: Metrics Performance measures General Terms Performance, Measurement Keywords \nAlgorithmic Pro.ling, Algorithmic Complexity 1. Introduction When developers need to understand and \noptimize the performance of their code, they use traditional pro.lers. Since the introduction of gprof \n[5], pro.lers have been focused on .nding the locations in software that are responsible for excessive \nresource consump\u00adtion, be that execution time, memory allocation, usage, or leaks, cache performance, \nvarious forms of contention, or even energy consumption. One commonality of such pro.ling approaches \nis that they only provide information about a speci.c program run. Dif\u00adferent runs, with different inputs, \nmay lead to different pro.les. A pro.le created by a traditional pro.ler does not enable the devel\u00adoper \nto predict how the program might scale to larger inputs, and it provides only limited information for \nthe reasons of the observed resource consumption. The resource consumption of a software system is affected \nby three main factors, (1) the algorithm used, (2) the problem size(the program input), and (3) the implementation \nof that algorithm in a programming language running on a concrete execution platform. Traditional pro.lers \nprovide measures of resource consumption that con.ate all three factors: they only report the overall \ncost. They do not help in understanding how the cost was affected by the algorithm, the program input, \nand the underlying implementation. Our new pro.ling approach, Algorithmic Pro.ling, addresses this limitation. \nInstead of providing a single number representing Permission to make digital or hard copies of all or \npart of this work for personal or classroom use is granted without fee provided that copies are not made \nor distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. PLDI 12, June 11 16, 2012, Beijing, China. Copyright c &#38;#169; \n2012 ACM 978-1-4503-1205-9/12/06. . . $10.00 the cost of a given run, we provide a cost function that \nrelates program input to algorithmic steps. Algorithms researchers and advanced practitioners use cost \nfunctions when analyzing the complexity of their algorithms. They usually perform asymptotic analysis \nto bound cost. In contrast, our algorithmic pro.ling approach automatically determines approx\u00adimate cost \nfunctions based on multiple program runs. Our cost functions are approximations, not bounds. They thus \nprovide no guarantees, but they have the advantage that they represent the ex\u00adpected realistic cost, \nnot a possibly loose worst case bound or an idealized, potentially unrealistic, average case cost. The \nprogram in Listing 1 shows an implementation of the in\u00adsertion sort algorithm for a linked list. If we \nrun this program under a traditional hotness pro.ler, we learn that most of the execution time is spent \nin method sort. A .ner grained pro.le would tell us in which lines, statements, or bytecode instructions \nwe spent most time. However, the pro.le would not provide any information about the algorithmic complexity \nof the program. An algorithmic pro.ler, on the other hand, would automatically produce results such as \nthe ones shown in Figure 1. The .gure shows three examples. In all three examples the program was run \non a set of inputs that are rep\u00adresentative of the expected application usage. In the top plot (a), that \nusage corresponds to sorting random lists, in the middle plot (b), it corresponds to sorting lists that \nare already sorted, while in the bottom plot (c), it correspond to sorting lists that are already sorted \nin reverse order. The x-axis in each plot shows the input size (corresponding to the length of the list), \nand the y-axis shows the number of algorithmic steps (corresponding to overall iteration counts). Each \ndot represents a program run with a different input, and the curves represent the approximate cost functions \nof the pro\u00adgram expected for the given type of inputs. These plots may look simple, and indeed, they \nare what a pro\u00adgrammer would produce when manually determining the empirical cost function of an algorithm. \nThe programmer would study the code to (1) locate the algorithm, to (2) determine the algorithm s essential \noperations (e.g., the comparisons or the swaps in the inser\u00adtion sort), to (3) determine what the input \nof the algorithm could be (e.g., the linked list passed as an argument to the sort function), and to \n(4) determine how to quantify the input s size (e.g., by traversing the linked lists to count the number \nof nodes). He would then (5) instrument the program to count these operations and to measure the input \ns size. Our algorithmic pro.ling approach performs all the above steps automatically for arbitrary programs. \nIt only needs the original program and a set of representative program executions to produce graphs like \nthose in Figure 1. These graphs of cost functions provide the developer with much deeper insight into \nprogram performance than a simple hotness pro.le. A hotness pro.le provides a single number (or a number \nfor each code region). An algorithmic pro.le uncovers the relationship of execution cost and program \ninput. It provides the programmer with scalability information by pointing out algorithms with high complexity, \nand it uncovers which inputs are the causes of long  (a) Random Inputs size of Node-based recursive \nstructure (b) Sorted Inputs size of Node-based recursive structure (c) Reverse-Sorted Inputs size of \nNode-based recursive structure Figure1. Cost Function of Insertion Sort Figure2. Traditional Pro.le: \nCalling Context Tree Listing1. Insertion Sort 1 public c la s s L i s t { 2 private Node head , t a \ni l ; 3 public void s o r t ( ) { 4 if ( head == null || head . n e x t == null ) return ; 5 Node f i \nr s t U n s o r t e d = head . n e x t ; 6 while ( f i r s t U n s o r t e d != null ) { 7 Node t a r \ng e t = f i r s t U n s o r t e d ; 8 Node n e x t U n s o r t e d = f i r s t U n s o r t e d . n e \nx t ; 9 while ( t a r g e t . p r e v != null &#38;&#38; 10 t a r g e t . p r e v . v a l u e >t a r \ng e t . v a l u e ) { 11 fin a l Node c a n d i d a t e = t a r g e t . p r e v ; 12 fin a l Node p r \ned = c a n d i d a t e . p r e v ; 13 fin a l Node s u cc = t a r g e t . n e x t ; 14 if ( p r e d != \nnull ) { 15 p r ed . n e x t = t a r g e t ; 16 } e ls e { 17 head = t a r g e t ; 18 } 19 t a r g e \nt . p r e v = p r e d ; 20 if ( s u c c != null ) { 21 s u c c . p r e v = c a n d i d a t e ; 22 } e \nls e { 23 t a i l = c a n d i d a t e ; 24 } 25 c a n d i d a t e . n e x t = s u c c ; 26 t a r g e \nt . n e x t = c a n d i d a t e ; 27 c a n d i d a t e . p r e v = t a r g e t ; 28 } 29 f i r s t U \nn s o r t e d = n e x t U n s o r t e d ; 30 } 31 } 32 public void append ( in t v a l u e ) { 33 fin \na l Node node = new Node ( v a l u e ) ; 34 if ( t a i l == null ) { 35 t a i l = node ; 36 head = t \na i l ; 37 } e ls e { 38 t a i l . n e x t = node ; 39 node . p r e v = t a i l ; 40 t a i l = t a i \nl . n e x t ; 41 } 42 } 43 } running times. Moreover, by combining an algorithmic pro.le with a traditional \nhotness pro.le, the developer can understand all three factors (problem size, algorithmic complexity, \nand implementation cost) affecting the overall performance of the program. This paper makes the following \ncontributions: 1. We introduce Algorithmic Pro.ling, an approach to automat\u00ad ically infer approximations \nof the expected algorithmic cost functions of algorithm implementations. An algorithmic pro\u00ad .ler requires \nno human intervention or code annotations. It sim\u00ad ply analyzes program executions on a set of representative \nin\u00ad puts. 2. We discuss different approaches to automatically determine the input of an algorithm and \nits size (the domain of the cost func\u00ad tion). Traditionally, a human analyst has to de.ne the concrete \nmeaning of the abstract notion of input size (e.g., the size of this array, the number of nodes in this \ntree) for a given algorithm implementation. 3. We discuss different approaches to automatically determine \nthe cost of an algorithm (the range of the cost function). Tradition\u00ad ally, a human analyst has to specify \nhow to measure the cost of  Listing2. Example Program 1 public c la s s Main { 2 public s ta tic void \nmain ( S t r i n g [ ] a r g s ) { 3 measure ( ) ; 4 } 5 private s ta ti c void measure ( ) { 6 for ( \nin t s i z e = 0 ; s i z e <1000; s i z e ++) { 7 for ( in t i = 0 ; i <10; i ++) { 8 fin a l L i s t \nl i s t = new L i s t ( ) ; 9 c o n s t r u c t R a n d o m ( l i s t , s i z e ) ; 10 s o r t ( l i \ns t ) ; 11 } 12 } 13 } 14 private s ta ti c void c o n s t r u c t R a n d o m ( 15 L i s t l i s t , \nin t s i z e ) { 16 fin a l Random r = new Random ( ) ; 17 for ( in t i = 0 ; i<s i z e ; i ++) { 18 \nl i s t . append ( r . n e x t I n t ( s i z e ) ) ; 19 } 20 } 21 private s ta ti c void s o r t ( L \ni s t l i s t ) { 22 l i s t . s o r t ( ) ; 23 } 24 } 25 26 public c la s s Node { 27 public Node p \nr ev ; 28 public Node n e x t ; 29 public fin a l in t v a l u e ; 30 public Node ( in t v a l u e ) \n{ 31 this . v a l u e = v a l u e ; 32 } 33 } an algorithm (e.g., the number of comparisons, or the number \nof swaps). 4. We present an approach to automatically partition a program into multiple algorithms. Any \nrealistic program involves many different algorithms, and the boundaries between algorithms are dif.cult \nto de.ne. We introduce an intuitive heuristic to identify boundaries between different algorithms in \na program. 5. We describe AlgoProf, our prototype implementation of an Algorithmic Pro.ler for Java programs, \nand we demonstrate it in several small case studies. The remainder of this paper is structured as follows. \nSection 2 presents our approach, Section 3 describes our implementation, Section 4 demonstrates the approach, \nSection 5 discusses limita\u00ad tions and future work, Section 6 presents related work, and Sec\u00ad tion 7 concludes. \n2. Approach This section describes the idea of Algorithmic Pro.ling on a con\u00ad ceptual level. The next \nsection will describe our prototype, Algo- Prof, which implements some, but not all, of the key aspects \nof this approach. We explain our approach based on a running example, which consists of the full program \n(Listing2) containing the inser\u00ad tion sort algorithm shown in Listing 1. The program constructs an unsorted \nlinked list of Nodes in Main.constructRandom() and sorts that list in Main.sort(). This process is repeated \nfor lists of length 0 to 999, ten times for each length, in Main.measure(). Most traditional pro.lers \nattach execution cost to syntactic con\u00ad structs (e.g., methods or statements) of the program. Figure \n2 il\u00ad lustrates an example of such a pro.le, showing the calling context  Figure3. Algorithmic Pro.le: \nRepetition Tree tree of our running example. Each method (or, more precisely, each calling context) is \nannotated with the number of times it was called and the total time spent in it (its hotness). The pro.le \nshows that List.append and the Node constructor are the most frequently called methods, and that List.sort \nis the hottest method (in terms of exclusive time). The pro.le does not provide any information for the \ncause of the hotness (why the method takes so much time), and it does not allow the developer to predict \nhow different inputs would affect the hotness. These two aspects are important for un\u00adderstanding and \nimproving program performance. Our algorithmic pro.les provide this missing information. They focus on \nrepetitions, which are the essential ingredients of any algorithm [21]. We identify all loops in control-.ow \ngraphs and all recursions in the program s call graph. Instead of attributing execution cost to a calling \ncontext tree, we attribute execution cost and inputs to a repetition tree (a dynamic loop and recursion \nnesting tree). Figure 3 shows the repetition tree for our running example. It shows that our program \ncontains .ve loops, and it shows their dynamic nesting. Instead of using cost metrics such as invocation \ncounts, wall-clock time, or instruction counts, we use higher-level cost metrics such as repetition counts \nand data structure access counts. Moreover, unlike traditional pro.lers, we also determine the algorithm \ninput and the size of that input. Given a set of program runs, each providing an input size and a cost, \nwe then produce plots mapping input size to cost (like those in Figure 1) and we infer cost functions \n(like steps = 0.25*size^2 in Figure 3).  For our running example, the annotation in the bottom right \ngray box in Figure 3 shows that our pro.ler automatically identi.ed an algorithm that modi.es a Node-based \nrecursive structure (a data structure consisting of objects of class Node). This tells us that the algorithm \ndoes not just traverse the structure, but that it modi.es it, without creating new Nodes. It also shows \nthat, given a structure of size Nodes, this algorithm takes 0.25*size 2 algorithmic steps. This information \nrepresents a high-level summary of the algorithm s performance. A developer can retrieve the data from \nwhich that cost function was inferred (the plot in Figure 1 (a)), and get summary statistics (such as \nthe number of times the algorithm was called, or the range of input sizes it encountered). Given this \ninformation that the algorithm s expected complexity is quadratic, and that the sizes of the inputs \nare not negligible the developer can decide to constrain the size of the input, or to replace the algorithm \nwith a more ef.cient one. 2.1 ConstructRepetitionTree The repetition tree is our basic structure for \nrepresenting a set of program executions. Figure 3 shows a repetition tree for a pro\u00adgram without recursion. \nTo construct repetition trees of recursive programs, we collapse recursive call chains, and we represent \nthe header method of the recursive chain as a node in the repetition tree. The repetition tree thus consists \nof loop nodes and recursion header nodes. Each node represents a repetitive computation. It keeps track \nof each invocation of that computation (entrance to the loop, and entry call of the recursion header \nmethod). For each invocation of a repetitive computation, we gather the number of repetitions (count\u00ading \nloop back edge traversals, and counting subsequent calls to the recursion header method). An algorithm \ncorresponds to a connected subgraph of the rep\u00adetition tree. It has a root node, to which we attribute \ncost and in\u00adput size. It may also contain descendants of the root node, if they are deemed to be part \nof the same algorithm (see Section 2.5 for how we group repetition nodes into algorithms). Figure 3 repre\u00adsents \neach algorithm with a gray rectangle, annotated with infor\u00admation about its input and with an empirical \ncost function. 2.2 MeasureCost Computational complexity theory uses models of computation that de.ne \nthe set of primitive operations used in the computation and their respective costs. These models, and \ntheir primitive operations, are chosen by the person performing the analysis. A commonly used model is \nthe random access machine model, where the prim\u00aditive operations are memory reads and writes, and where \nthese op\u00aderations have unit cost. Our algorithmic pro.ling approach also admits multiple possi\u00adble cost \nmodels. Our models are de.ned by their primitive opera\u00adtions. We abstract away from the underlying platform \nand focus on operations that are meaningful to the developer at the source code level: AlgorithmicSteps. \nAn algorithmic step corresponds to one iter\u00adation of a loop or recursion. An algorithmic step is a generic \noperation that abstracts away from the speci.c operations per\u00adformed in each repetition. It thus allows \nthe comparison of the cost of arbitrary repetitions using a single measure. StructureReads. A structure \nread corresponds to a read of a ref\u00aderence in a recursive data type or a load of an element in an array. \nIt allows us to distinguish between read-only traversals of structures and modi.cations of structures. \nStructureWrites. Analog to a structure read, a structure write corresponds to an update of a reference \nin a recursive data type or a store of an element in an array. StructureElementCreations. A structure \nelement creation cor\u00adresponds to the allocation of a new object of a recursive data type. It allows us \nto distinguish between modi.cations and con\u00adstructions of a data structure. InputReads. An input read \ncorresponds to a read operation of data from outside the program (e.g., a read from a .le or network \nsocket). Some algorithms may not operate on in-memory struc\u00adtures, but they may consume input from outside \nthe program. OutputWrites. An output write corresponds to a write operation of data to the outside world \n(e.g., a write to a .le or network socket).  2.3 DetermineInputs To determine the inputs of an algorithm, \nan algorithmic pro.ler determines which data structures or external .les the algorithm accesses. RecursiveDataStructures. \nTo .nd the recursive data structures related to an algorithm, the pro.ler tracks accesses to .eld ref\u00aderences \nwithin all recursive data structures (Node.next and Node.prev in our running example), and whenever an \naccess occurs at runtime, it determines the current node in the repeti\u00adtion tree (the innermost loop \nor recursion), and it associates that data structure with the corresponding repetition node. Arrays. \nTo .nd arrays related to an algorithm, the pro.ler tracks all array loads and stores and associates the \ncorresponding arrays with the current repetition tree node. ProgramInputs/Outputs. To .nd program inputs \nand outputs re\u00adlated to an algorithm, the pro.ler tracks all reads and writes to the external world, \nand associates the corresponding streams or .le handles to the current repetition tree node. Given our \napproach, an algorithm can be related to multiple inputs. For example, an algorithm may traverse a graph-like \ndata structure and serialize it into a .le, it may traverse a given data structure and produce a translation \nin the form of a different structure (e.g., convert a linked list into an array), or it may process information \nfrom two independent data structures (e.g., compute the dot product of two vectors). An algorithmic pro.ler \nwill keep track of all the inputs it encounters. Some algorithms (for example mathematical algorithms) \ndo not operate on data structures and do not read or write external data, but they encode data in primitive \ntypes (e.g., in variables of type int). For such algorithms, determining the input (and input size) is \ndif.cult. For example, the input size n for multiplication (with a complexity of O(n 2 ) when using the \nschoolbook long multiplication algorithm) is de.ned by the number of digits in the factors, while the \ninput size m for the factorial algorithm (with a complexity of O(m 2 log m) when using bottom-up multiplication) \nis the value of the argument. Note that for arbitrary precision arithmetic, the algorithms will have \nto use either recursive types or arrays to represent their numbers, and thus our approach will be able \nto compute some measure of the input size.  2.4 MeasureInputSize To measure the input size, we traverse \nthe recursive data structure, we count the array s elements, or we measure the size of the ex\u00adternal \n.le. In an executing program, structures are dynamic: they shrink and grow. Many algorithms do not just \ntraverse a structure, but they modify it throughout their execution. There thus is no sin\u00adgle size of \na structure, and we have to measure the size of a struc\u00adture every time an algorithm accesses that structure. \nTo .nd a single number representing the size (needed to compute a cost function re\u00adlating structure size \nto execution cost) we use the maximum size of the structure throughout the algorithm s operation. Using \nthe max\u00adimum is reasonable as it produces intuitive results for the many common algorithms that create \na structure (size is 0 at the begin\u00adning and N at the end), or algorithms that destroy a structure. If \nwe repeatedly want to measure the size of a continuously changing structure, we need to address the issue \nof identity. How do we know whether two structures that are not identical should be considered the same? \nIs a structure with one less element still the same structure? We answer these questions as follows. \nEach time a structure is accessed, we take a snapshot of that structure by traversing it. A snapshot \nof a structure I corresponds to the set S of all elements in that structure at that time. We use one \nof the following equivalence criteria to determine whether two snapshots represent the same structure. \nAllElementsEquivalent. Two snapshots S1 and S2 are equiva\u00ad lent if all elements of the two snapshots \nare equivalent (S1 = S2 ). SomeElementsEquivalent. Two snapshots S1 and S2 are equiv\u00adalent if some of \nthe elements of the two snapshots are equiva\u00adlent (S1 nS2 = \u00d8). This less strict equivalence criterion \nis not only useful because data structures evolve, but also because the traversals capturing the two \nsnapshots may have started from different elements, and thus the two snapshots may not see all the elements \nof the entire structure (if the structure instance is not strongly connected). SameArray. This equivalence \ncriterion only works for arrays. Ar\u00adray elements are contained in an array object, and thus two snapshots \nthat correspond to identical arrays (I1 = I2 ) can be considered equivalent. SameType. Two snapshots \nare equivalent if they have the same type (t(S1 ) = t(S2 )). This criterion considers two entirely disconnected \nstructure instances equivalent. 2.5 GroupRepetitionNodesintoAlgorithms Given a realistic program, does \nthat program implement one big al\u00adgorithm, or does it implement a collection of multiple algorithms? \nWe argue that both answers are valid, and that algorithms can in\u00advoke other algorithms just like methods \ncan invoke other methods. While a program could be described as one large algorithm, the computational \nconstructs programmers usually call algorithm are more limited in focus. In our motivating example, sorting \nthe list represents a traditional algorithm, and constructing the list repre\u00adsents another algorithm. \nThe loop nest in the measure method is more dif.cult to classify: it could be considered yet another \nalgo\u00adrithm, each of its two loops could be considered an independent algorithm, or the two loops could \nbe considered too trivial for be\u00ading called an algorithm at all. To approximate an intuitive notion of \nalgorithm, we partition the repetition tree of an entire program run into connected subgraphs using one \nof several possible strategies. Moreover, given an interactive visualization tool for the repetition \ntree, developers could group nodes into algorithms according to their own intuition. Listing3. Combining \nCosts for ( int o=0; o <3; o++) { ... for ( int i = 0; i<o; i++) { ... } ... } Our pro.ling approach \nuses an automatic strategy for grouping repetition tree nodes into algorithms. The general idea is to \ngroup parent and child repetitions that access at least one common input into a single algorithm. We \nuse the equivalence criteria de.ned in the previous section to determine whether the two structure snapshots \nfrom the two repetitions represent the same structure. In our running example, the two loops in the sort \nmethod intuitively form a single algorithm. For this example, all of the applicable equivalence criteria \n(Same Array is not applicable for a recursive data structure) would lead to this intuitive grouping of \nrepetition nodes. One could envision other strategies, such as the grouping of loops located in the same \nmethod (which would work for the sort algorithm in the running example). 2.6 CombineCosts When two repetition \ntree nodes are grouped into an algorithm, the child s cost is added to the parent s cost. In particular, \nfor a given invocation of the parent, the parent s overall cost is equal to the parent s cost (e.g., \nalgorithmic steps) plus the sum of the child s costs across all invocations of the child inside that \nparent invocation. In the example of Listing 3, the cost of an invocation of the outer loop would be \n3 outer loop iterations plus (0+1+2) inner loop iterations, leading to a total of 6 algorithmic steps. \nBy also counting the outer loop s iterations, this approach accounts for outer loop iterations even when \nthe inner loop does not execute. 2.7 InferCostFunction Given a set of <input size, cost> tuples, an \nalgorithmic pro.ler should produce plots like those in Figure 1. Those plots show the raw data and a \ncost function. The step of automatically inferring a cost function or estimating an upper bound asymptotic \ncomplexity from the raw data is the subject of study in the area of empirical algorithmics [8, 9, 14], \nwhich investigates the use of regression approaches and heuristics for this purpose. In this paper we \ndo not discuss this step, and in our algorithmic pro.ling prototype we currently .t cost functions by \nhand.  2.8 ClassifyAlgorithms Besides inferring a cost function for an algorithm, an algorithmic pro.ler \ncan provide additional information that helps in under\u00adstanding the algorithm. In particular, given the \ninformation about the algorithm s input, the pro.ler can distinguish between several different kinds \nof algorithms: Traversal. A traversal algorithm performs a read-only traversal of a recursive data structure \nor an array. It performs Structure Reads, but does not perform any Structure Writes or Structure ElementCreations. \nModi.cation. A modi.cation algorithm modi.es the links of a re\u00adcursive data structure or the contents \nof an array. It performs Structure Writes, but does not perform any Structure Element Creations. As Figure \n3 shows, the loop nest in List.sort is considered a Modi.cation of a Node-based recursive struc\u00adture \n.  Construction. A construction algorithm performs Structure Ele\u00adment Creations, that is, it allocates \nobjects of recursive types. As Figure 3 shows, the loop in List.constructRandom is considered a Construction \nof a Node-based recursive struc\u00adture . Input. An input algorithm performs Input Reads, that is, it con\u00adsumes \nexternal input. Output. An output algorithm performs Output Writes, that is, it produces external output. \nData-structure-less algorithm. Any algorithm that does not fall into any of the above categories is considered \ndata-structure\u00adless. The absence of a measurable input also means that we are unable to infer the input \nsize and cost function. As Figure 3 shows, the two loops in Main.measure are considered Data structure-less \nalgorithms . Traversal, modi.cation, and construction are mutually exclusive with respect to a given \ndata structure. That is, if an algorithm con\u00adstructs a speci.c data structure (e.g., the constructRandom \nloop in the running example creates a Node-based structure), it is classi.ed as a construction of that \ndata structure, but not as a modi.cation or traversal of that data structure. This mutual exclusion is \nlimited to operations on the same data structure, e.g., the same algorithm may traverse one data structure \nand construct a different data structure.  3. Implementation To evaluate our algorithmic pro.ling approach \nwe built AlgoProf, an algorithmic pro.ler for Java. AlgoProf analyzes and instruments Java bytecode. \n3.1 Instrumentation AlgoProf uses dynamic binary instrumentation to instrument the following constructs \nin the application code: Loop entry andloop exit. These drive the construction of the loop nodes in the \nrepetition tree. Loop back edges. These support counting algorithmic steps. Method entries and exits. \nThese enable the construction of the recursion nodes in the repetition tree. Moreover, they enable the \ncounting of algorithmic steps in recursive algorithms. By using a static analysis to determine headers \nin recursive method cycles [21], AlgoProf can limit this instrumentation to only those methods that are \nrecursion headers. Array and referenceinstance .eld accesses. These *ALOAD and *ASTORE as well as PUTFIELD \nand GETFIELD bytecode instructions enable the detection and measurement of input data and the counting \nof read or write-based costs. By using a static analysis to determine recursive data structures [22], \nAl\u00adgoProf can limit .eld access instrumentation to accesses of .elds participating in a recursive cycle \n(e.g., Node.next and Node.prev, but not Node.payload). Object allocations. These NEW bytecode instructions \nenable the counting of allocation-based costs. Static analysis can limit this instrumentation to allocations \nof instances of classes that are part of a recursive type (e.g., Node). AlgoProf currently does not track \nexternal program input and output operations. Support for this could be added by instrumenting the Java \nI/O classes. 3.2 DynamicAnalysis At runtime, AlgoProf is called from the instrumented code. It collects \nall the information necessary for producing an algorithmic pro.le. Most importantly, it incrementally \nbuilds a repetition tree for each thread executing in the program. The tree already folds recursive calls, \nso on any given path to the root, a given method can only occur once. To support the unwinding of recursive \ncalls, AlgoProf also maintains an unfolded shadow stack (stack). Each stack element represents a loop \nor recursion and points to the corresponding repetition tree node. At any given time, AlgoProf maintains \na reference to the current repetition tree node (tn). The element at the top of the shadow stack (stack.top()) \nusually points to the same repetition tree node pointed to by tn. AlgoProf handles calls from the instrumented \ncode in the following ways: Loop entry. tn = tn . getOrCreateChild ( loop ) stack .push(tn)  Loop exit. \nremeasureInputs () finalizeRepetition (tn) tn = stack .pop()  Loopback edge. tn . cost {STEP}++ Methodentry. \nheader = tree.findOnPathToRoot(tn, method) if ( header != null ) { tn = header tn . cost {STEP}++ } \nelse { tn = tn.getOrCreateChild(method) } tn .recursionDepth++ stack .push(tn)  Methodexits. tn . recursionDepth \n-- if ( tn . recursionDepth ==0) { remeasureInputs () finalizeRepetition (tn) } tn = stack .pop() Array \naccess. tn . cost {type ( array ) , LOAD/STORE}++ identifyAndMeasureArray ( array , tn )  Referenceinstance \n.eldaccess. if (partOfRecursiveType(type(object ))) { tn . cost {id ( object ) , GET/PUT}++ tn . cost \n{id ( object ) , type ( object ) , GET/PUT}++ identifyAndMeasureStructure ( object , tn ) } Object allocation. \nif (partOfRecursiveType(type(object ))) { tn . cost {type ( object ) , NEW}++ } Note that AlgoProf correctly \nhandles exceptional control .ow, i.e., when exceptions cause control to exit a loop or a method, AlgoProf \nperforms the corresponding Loop exit or Method exit operation.  3.3 MeasuringCost The cost .eld in the \nrepetition tree nodes represents a map from speci.c primitive operations (on speci.c inputs) to their \nexecution counts. Algorithmic step. cost{STEP}. 15 means that the repetition performed 15 algorithmic \nsteps. Array access. cost{input#1, LOAD}. 10 means that the repetition performed 10 LOAD operations on \nan array known as input with ID 1. Recursive structure access. cost{input#3, PUT}. 99 means that the \nrepetition performed 99 PUT operations on the recur\u00adsive structure known as input with ID 3. Recursive \nstructure access(by element type). cost{input#3, Vertex, PUT}. 33 means that the repetition performed \n33 PUT operations on .elds of type Vertex in the recursive structure known as input with ID 3 (e.g., \na recursive structure modeling a graph as Edge and Vertex objects). Recursive structure element creation. \ncost{ListNode, NEW} . 9 means that the repetition allocated 9 ListNode instances (a class that is part \nof a recursive structure). The analysis calls finalizeRepetition whenever a repetition terminates (at \na loop exit or at a return from the outermost call of a recursive method). This .nalization of the repetition \nadds the con\u00adtents of the cost .eld to the history of invocations of this repetition tree node. This \nmeans that each node in the repetition tree contains historical information (data structure sizes and \noperation counts) for each and every invocation of that repetition. While this can lead to large memory \nrequirements for our pro.ler, keeping historic in\u00adput size and cost information is necessary to infer \ncost functions. To reduce this overhead, an optimized version of a pro.ler could try to infer the cost \nfunction online, and discard the individual data points, or it could try to sample a subset of invocations \nfor fre\u00adquently invoked repetitions. 3.4 MeasuringInputSize To identify an input and to determine its \nsize, we traverse the corresponding array or recursive structure. We measure input sizes in three possible \nsituations: IdentifyAndMeasureArray is called due to an array access. For array identi.cation, out of \nthe strategies described in Sec\u00adtion 2.5, AlgoProf implements the Some Elements Identical strategy, because \nit is effective for algorithms that resize ar\u00adrays by reallocation1. AlgoProf supports two strategies \nfor measuring the size of an array: the capacity strategy uses the array s capacity (the num\u00adber of elements \nthe array can store), while the unique element count strategy traverses all elements of the array (for \nreference arrays, all non-null elements), and computes the size of the set of unique elements. This second \nstrategy is useful to approxi\u00admate the amount of space used in an array by algorithms that allocate an \narray but only use a small fraction of its capacity. However, it has the drawback that it does not count \nduplicate elements. This is particularly problematic for arrays of small primitive types (e.g., booleans \nor bytes). AlgoProf treats multi-dimensional arrays analog to algorith\u00admic steps. That is, it counts \nall elements of the top-level ar\u00ad 1 Implementations of resizable arrays allocate a new, larger, array \nwhen the current backing-array gets full. Treating the original (small) and the newly allocated (grown) \narray as the same input is essential when reasoning about an algorithm operating on such a dynamically \nresizable array. ray and all elements of the lower-level arrays. For example, the 2-dimensional triangular \narray new int[][] {new int[0], new int[1], new int[2]}has a size of 3 + (0 + 1 + 2) el\u00adements, which \nis exactly the same as the number of algorithmic steps in the analog loop nest in Listing 3. IdentifyAndMeasureStructure \nis called due to a recursive ref\u00aderence .eld access. For recursive data structure identi.cation, out \nof the strategies described in Section 2.5, AlgoProf imple\u00adments the SomeElementsIdentical strategy. \nA recursive data structure can consist of multiple Java classes. For example, a graph can be modeled \nas a recursive structure involving a Vertex and an Edge class. AlgoProf provides a to\u00adtal object count \n(i.e., the total number of objects involved in a graph), and it provides separate counts of objects for \neach spe\u00adci.c type (i.e., the number of Vertex objects involved in the graph, and the number of Edge \nobjects involved in that graph). Many recursive structures also involve arrays. For example, a tree may \nconsist of a Node class that has a Node[] .eld con\u00adtaining references to all its children. When traversing \na recur\u00adsive structure, we also traverse these arrays. In addition to ob\u00adject counts, AlgoProf also provides \nthe counts of non-null array elements traversed in this way. Analog to object counts, Algo-Prof provides \na total reference count as well as separate refer\u00adence counts by types. For example, for a graph modeled \nwith a Vertex class that has a Vertex[] .eld containing references to all its incident vertices, the \ngraph s edges are implicit and correspond to the references in the Vertex[]. IdentifyAndMeasureStructure \nreceives an object refer\u00adence. It starts traversing the structure at that reference. If the structure \nis not strongly connected, then it will not reach all objects that might be deemed part of that structure \n(had the traversal started from a different object reference). This is one reason for why AlgoProf uses \nthe Some Elements Identical strategy to determine the equivalence of two recursive structure snapshots. \nRemeasureInputs is called when control exits the repetition (exit from a loop or recursive call chain). \nIt enables an optimization: instead of taking a structure snapshot at every access to a struc\u00adture, we \nonly take two snapshots: .rst at the repetition s .rst accesses of the structure (starting from the .rst \nreference ac\u00adcessed), and a second time when the repetition exits (starting from the last reference accessed \nin that repetition). This way, if a repetition is a Construction, such as in one of the examples in Listing \n4, we can still traverse and measure the completely constructed input in the end, but we do not need \nto traverse the structure at every access. We only need to memorize the one accessed reference at every \naccess, so we know where to start our traversal from at the exit of the repetition.  3.5 Using AlgoProf \nAlgoProf produces an algorithmic pro.le consisting of a repetition tree similar to the one shown in Figure \n3. For each algorithm, it pro\u00adduces multiple plots of its complexity, based on the combinations of their \ninputs (sizes of all their accessed structures) and cost mea\u00adsures (algorithmic steps, the various structure \naccess counts, and element allocation counts). Figure 1 (a) shows an example of such a plot for the sort \nalgorithm in Figure 3, with the number of Node objects in the Node-based recursive structure as input \nsize and al\u00adgorithmic steps as the cost measure. AlgoProf currently does not automatically .t a cost \nfunction onto the measured data points, but we manually .t those functions using a statistics package. \nGiven that we have several cost measures, and given that some algorithms access multiple inputs, the \nnumber of cost functions for each algo\u00adrithm can become large. We use simple heuristics to automatically \n Listing4. First access in Constructions cannot see whole structure Node constructListWithLoop ( int \nsize ) { Node list ; for ( int i = 0; i<size ; i++) { Node head = new Node (); // fi rst PUTFIELD: reachable \nstruct ure si ze 1 head.next = list ; list = head; } return head ; } Node constructListWithRecursion( \nint size ) { if ( size ==0) return null ; else { Node list = constructList(size -1); Node head = new \nNode (); // fi rst PUTFIELD: reachable struct ure si ze 1 head.next = list ; return head ; } } void \nconstructPartiallyUsedArray () { int [] values = new int [1000]; for ( int i = 0; i <10; i ++) { // fi \nrst IASTORE: array si ze 1 values[i] = i *2; } } highlight useful ones: we focus on algorithmic steps \nas a cost, and we exclude those cost functions of inputs that never change in size or that cause constant \ncost. Moreover, for realistic applications, we use traditional CCT hotness pro.les (gathered with Java \ns built-in hprof pro.ler) to focus algorithmic pro.ling on hot regions of code.   4. Demonstration \nWe now summarize the behavior of AlgoProf on several examples representing creations and traversals of \nvarious data structure im\u00adplementations, and we present two small, illustrative examples. 4.1 HandlingDifferentKindsofDataStructures \nTable 1 provides an overview over 18 illustrative example pro\u00adgrams we pro.led with AlgoProf. Those programs \nessentially im\u00adplement traversals of various data structure implementations. Each example focuses on \none kind of data structure but implements sev\u00aderal algorithms (e.g., building the structure, traversing \nthe structure iteratively, and traversing the structure recursively). The examples involve naked arrays, \nlists, trees, and graphs (col\u00adumn Struct ). Column Impl. shows how this structure is im\u00adplemented (as \nan array , or as a linked (recursive) data struc\u00adture). Column Linkage shows how the elements of the \nstructure are linked. Column T describes how one can de.ne the struc\u00adture s payload type (the type of \nthe information stored inside the elements): the type of the payload is hard-coded (B), the struc\u00adture \nuses inheritance to allow subclasses to de.ne the payload type (I), or the structure uses generics (G) \nto allow de.ning the payload type. Column Rem. contains further remarks: For arrays it spec\u00adi.es whether \nthey are one or multi-dimensional, for dynamically resized array-based lists it describes how they grow \n(grow by one, or double in size), and for trees it describes their arity (binary or N-ary). The remaining \nthree columns summarize the results. Column I states whether all the inputs we as programmers considered \ninputs were detected (x) or not (-). For all these examples, AlgoProf Listing5. Repetition nest not grouped \nby AlgoProf int [][] array = ...; for ( int i = 0; i<array . length ; i ++) // no access to array[ i] \nhere for ( int j = 0; j<array[i].length; j++) array[i][j] = ...; detected the inputs as expected. Column \nS shows that AlgoProf correctly measured the size of each input. Column G describes whether the loops \nwe consider to be part of one algorithm were indeed grouped together (x) or not (-). In this column, \na (*) means that we ended up with a correct grouping, but that a slight change in the algorithm s implementation \ncould lead to an incorrect grouping. The way AlgoProf groups repetitions into algorithms does not work \nwell for array-based systems. The reason is that in repetition nests that implement an algorithm, such \nas the one in Listing 5, sometimes only the innermost repetition actually accesses the array. That is, \noutside the innermost loop, the outer loop contains no *ALOAD or *ASTORE instructions that would access \nthe array. Thus, AlgoProf does not merge the two loops into one algorithm, but it creates two separate \nalgorithms instead: the innermost loop which is accessing the array, and the outer loop as a data-structure\u00adless \nalgorithm. We believe that this limitation could be overcome with a data\u00ad.ow analysis that determines \nwhich loops increment the indices used in the array accesses. For Listing 5, the outer loop increments \nvariable i, which is then used in the array access in the inner loop. Such an approach could be considered \nthe dual to the way we handle recursive structures: we look at the accesses of the recursive links (e.g., \nthe next .eld in a linked list node), not at the accesses of the payload (e.g., the value .eld in a linked \nlist node; or the element in an array). Struct Impl. Linkage T Rem. I S G array array NA B1d xx* array \narray NA B2d xx\u00adlist array NA B double x x * list array NA B growby1 x x * list array NA G growby1 x \nx * list array NA I growby1 x x * list linked directed B x x x list linked directed G x x x list linked \ndirected I x x x tree array NA B binary x x * tree linked directed B binary x x x tree linked bidi B \nbinary x x x tree linked directed B n-ary x x x tree linked bidi B n-ary x x x graph array directed B \n2d x x \u00adgraph linked directed B x x x graph linked bidi B x x x graph linked unidirected B x x x Table1. \nData Structure Examples  4.2 UncoveringAlgorithmicInef.ciencies Listing 6 shows an implementation of \na dynamically-growing array-backed list. Given that Java s arrays cannot grow, such a list needs to allocate \na new, larger array when it runs out of space in the current array. A naive developer will grow the array \nby one element (or by a constant number of elements) which will lead to quadratic cost. By changing one \nline in the code (grow the array by doubling its size), the cost can be made linear.  Listing6. Growing \nan array-backed list 1.2e+07 fin a l A r r a y L i s t l i s t = new A r r a y L i s t ( ) ; for ( in \nt i = 0 ; i<s i z e ; i ++) { l i s t . append ( n + i ) ; 1.0e+07 } ... public void append ( final \nString value ) { growIfFull (); array [ size ++] = value ; } private void grow () { if (size==array \n. length ) { final String [] newArray = new String [ array . length +1]; // naive 8.0e+06 6.0e+06 4.0e+06 \nCost of List.append 2.0e+06  new String [ array . length * 2]; // ideal for ( int i = 0; i <array . \nlength ; i++) newArray[i] = array[i]; 0.0e+00 array = newArray ; } } Size of Elements Figure5. Cost \nfunctions for growing array by 1 and doubling array 5. LimitationsandFutureWork AlgoProf is based on \nheuristics. Its grouping of repetitions into al\u00adgorithms and its notion of input (the way it identi.es \nthe data an algorithm is tied to) may not always match the developer s intu\u00adition. Moreover, AlgoProf \ncan only infer a cost function for algo\u00adrithms that operate on data structures, because it cannot infer \nany input size for algorithms that operate on primitive data types. Fi\u00adnally, its cost functions only \nrepresent approximations. However, we believe that the repetition tree, the (partial) grouping of repeti\u00adtion \ntree nodes into algorithms based on the data they access, the classi.cation of these algorithms (into \nconstruction, modi.cation, traversal, input, and output), and the approximations of their cost functions, \nprovide useful information to a developer. We are not aware of any other pro.ling approach that can produce \nsuch infor\u00admation automatically. The probably most severe limitation of AlgoProf is its over\u00adhead, both \nin terms of space and time. Realistic benchmarks exe\u00adcute several orders of magnitude slower under AlgoProf. \nHowever, AlgoProf only represents an initial prototype, without any signi.\u00adcant optimizations. There \nis a clear need and we believe a great potential for optimizations. For example, taking complete snap\u00adshots \nof structures at ever access, or even at every start and end of a repetition, and storing the complete \nsnapshots in memory, is wasteful, and incremental approaches, together with more power\u00adful static analyses, \ncould probably drastically reduce the overhead of our current prototype. Similar gains might be possible \nby piggy\u00adbacking data structure size measurements on the heap traversals performed by the garbage collector \nWhile AlgoProf correctly deals with multiple threads, our ap\u00adproach speci.cally targets sequential algorithms. \nAlgoProf pro\u00adduces a repetition tree for each thread, and it completely ignores any communication between \nthreads. Besides investigating possible optimizations for our approach, we also would like to perform \na more thorough evaluation. Algo-Prof cannot possibly be correct in what it deems an algorithm, that \nalgorithm s input, and a useful measure of cost. All it can do is match a human developer s intuition. \nThus, it would be interesting to explore how algorithmic pro.les help developers to detect and .x performance \nbugs, and which of its strategies and heuristics are most effective for this purpose. Figure 4 shows \nthe program s repetition tree, indicating three repetition nodes grouped into two algorithms. The top \nalgorithm consists of the harness running testForSize with various sizes. The lower algorithm consists \nof the loop calling list.append() and the inner loop that grows the list. Given that the two loops are \ngrouped together, we see the total cost of appending size elements. Figure 5 shows the corresponding \ncost function. For the naive developer s implementation, the cost grows quadratically in the size of \nthe list. For the ideal implementation, the cost grows linearly.  4.3 BeingAgnostictoProgrammingParadigm \nThe insertion sort implementation in Listing 1 is imperative, itera\u00adtive, and uses a mutable data structure. \nDoes AlgoProf produce the same pro.le for an insertion sort implementation that is functional, recursive, \nand uses an immutable data structure? We found that the pro.le was almost identical2. The repetition \ntree contains the same repetitions as the tree in Figure 1, and the repetitions are grouped into the \nsame algorithms with the same complexities. This demon\u00adstrates one of the key points about algorithmic \npro.ling: the im\u00adplementations may look entirely different, but their automatically generated algorithmic \npro.les agree. 2 http://sape.inf.usi.ch/algorithmic-profiling   6. RelatedWork Goldsmith et al. [4] \nhad a similar goal to ours: they ran a program given several inputs, measured cost and .tted a curve. \nHowever, ex\u00adcept for the cost, which was automatically determined by counting basic block invocations, \nthe other aspects (e.g., algorithm identi\u00ad.cation and input size determination) had to be performed manu\u00adally. \nLater work [3] proposed to switch the cost metric from basic block counts to loop iterations, but it \nignored recursions, and it still required all the former manual interventions. Related work in em\u00adpirical \nalgorithmics [8, 9, 14] discusses the dif.culties in inferring algorithmic cost functions and asymptotic \ncomplexity from exper\u00adimental data. And further research analyzes program executions to extract performance \nmodels other than cost functions, for example models in the form of layered queueing networks [6]. Jump \n[7] introduces dynamic shape analysis, focusing on the shapes and sizes of recursive structures in the \nheap. Prior re\u00adsearch [13, 15] performed shape analysis statically. We traverse recursive heap structures \nat runtime, when identifying inputs, and we measure their sizes. We do not, however, try to infer any \nshape properties. We possibly might bene.t from understanding their shape properties, for example to \nincrementalize our approach to measuring structure sizes. Further work on analyzing data structures at \nruntime includes approaches to .nd costly data structures [10], to measure object lifetimes [2], to track \ndata structures for post-mortem analysis [16], to pro.le the use of recursive data structures [11, 12], \nto perform copy pro.ling [19], to .nd low-utility data structures [20], to perform container pro.ling \n[17] and to .nd inef.ciently used containers [18]. It would be interesting to try to combine those approaches \nwith algorithmic pro.ling. Recently, Bergel et al. presented domain-speci.c pro.ling [1], an approach \nthat also produces higher-level pro.les. That work takes almost the opposite direction as ours: algorithmic \npro.ling is a domain-independent pro.ling approach that strives to achieve a level of abstraction without \nrequiring any developer involvement. 7. Conclusions Algorithmic pro.ling is an automatic approach to \ninfer the com\u00adputational cost of the algorithms embodied in a program. Given a program and a set of representative \nprogram executions, an algo\u00adrithmic pro.ler identi.es algorithms and inputs to those algorithms, measures \nmeaningful notions of cost, and infers a cost function for each algorithm that relates cost to input \nsize. The pro.ler presents the programmer with a repetition tree, in which it highlights the al\u00adgorithms. \nIt annotates each algorithm with the kind of operations it performs (e.g., construct, modify, traverse), \nthe kind of data struc\u00adtures it processes, and an estimated cost function. Developers can use algorithmic \npro.les to identify algorithms with high computational complexity, or to better understand pro\u00adgram regions \nwith high measured execution times. Given an algo\u00adrithmic pro.le, programmers can further estimate how \na program scales, that is, how a program s running time would be affected by further increases in input \nsize. We believe that algorithmicpro.ling relates to algorithmic com\u00adplexity analysis like software testing \nrelates to formal veri.cation: it does not provide any guarantees, but can be effective in .nding and \n.xing real-world problems.  References [1] A. Bergel, O. Nierstrasz, L. Renggli, and J. Ressia. Domain-speci.c \npro.ling. In J. Bishop and A. Vallecillo, editors, Objects, Models, Components, Patterns, volume 6705 \nof Lecture Notes in Computer Science, pages 68 82. Springer, 2011. [2] B. Dufour, B. G. Ryder, and G. \nSevitsky. Blended analysis for per\u00adformance understanding of framework-based applications. In Pro\u00adceedings \nof the2007international symposiumonSoftwaretesting and analysis, ISSTA 07, pages 118 128. ACM, 2007. \n[3] S. F. Goldsmith. Measuring Empirical Computational Complexity. PhD thesis, University of California, \nBerkeley, 2009. [4] S. F. Goldsmith, A. S. Aiken, and D. S. Wilkerson. Measuring em\u00adpirical computational \ncomplexity. In Proceedings of thethe6thjoint meeting of theFSE, ESEC-FSE 07, pages 395 404. ACM, 2007. \n[5] S. L. Graham, P. B. Kessler, and M. K. McKusick. gprof: a call graph execution pro.ler. SIGPLANNot., \n39(4):49 57, 2004. [6] T. Israr, M. Woodside, and G. Franks. Interaction tree algorithms to extract effective \narchitecture and layered performance models from traces. J.Syst.Softw., 80:474 492, April 2007. [7] M. \nJump and K. S. McKinley. Dynamic shape analysis via degree metrics. In Proceedings of the 2009 international \nsymposium on Memory management, ISMM 09, pages 119 128. ACM, 2009. [8] C. McGeoch, P. Sanders, R. Fleischer, \nP. R. Cohen, and D. Precup. Experimental algorithmics, chapter Using .nite experiments to study asymptotic \nperformance. Springer, 2002. [9] C. C. Mcgeoch. Experimental analysis of algorithms. PhD thesis, Carnegie \nMellon University, Pittsburgh, PA, USA, 1986. [10] N. Mitchell, E. Schonberg, and G. Sevitsky. Making \nsense of large heaps. In Proceedings of the 23rd European Conference on ECOOP 2009, Genoa, pages 77 97. \nSpringer, 2009. [11] S. Pheng and C. Verbrugge. Dynamic data structure analysis for java programs. In \nProgram Comprehension, 2006. ICPC2006. 14th IEEE International Conference on, pages 191 201, 2006. [12] \nE. Raman and D. I. August. Recursive data structure pro.ling. In Proceedings of the 2005 workshop on \nMemory system performance, MSP 05, pages 5 14. ACM, 2005. [13] M. Sagiv, T. Reps, and R. Wilhelm. Solving \nshape-analysis problems in languages with destructive updating. ACM Trans. Program. Lang. Syst., 20:1 \n50, January 1998. [14] P. Sanders and R. Fleischer. Asymptotic complexity from experi\u00adments? a case study \nfor randomized algorithms. In WAE 00: Pro\u00adceedings ofthe4thInternationalWorkshop onAlgorithmEngineering, \npages 135 146. Springer, 2001. [15] R. Wilhelm, S. Sagiv, and T. W. Reps. Shape analysis. In Proceedings \nof the 9th International Conference on Compiler Construction, CC 00, pages 1 17. Springer, 2000. [16] \nX. Xiao, J. Zhou, and C. Zhang. Tracking data structures for post\u00admortem analysis. In ICSE2011NIER Track, \n2011. [17] G. Xu and A. Rountev. Precise memory leak detection for java soft\u00adware using container pro.ling. \nIn Proceedings of the 30th interna\u00adtional conference onSoftwareengineering, ICSE 08, pages 151 160. ACM, \n2008. [18] G. Xu and A. Rountev. Detecting inef.ciently-used containers to avoid bloat. In Proceedings \nofthe2010ACMSIGPLANconference on Programming language design and implementation, PLDI 10, pages 160 173, \nNew York, NY, USA, 2010. ACM. [19] G. Xu, M. Arnold, N. Mitchell, A. Rountev, and G. Sevitsky. Go with \nthe .ow: pro.ling copies to .nd runtime bloat. In Proceedings of the 2009 ACM SIGPLAN conference on Programming \nlanguage design and implementation, PLDI 09, pages 419 430. ACM, 2009. [20] G. Xu, N. Mitchell, M. Arnold, \nA. Rountev, E. Schonberg, and G. Se\u00advitsky. Finding low-utility data structures. In Proceedings of the2010 \nACMSIGPLANconference onProgramminglanguagedesignandim\u00adplementation, PLDI 10, pages 174 186. ACM, 2010. \n[21] D. Zaparanuks and M. Hauswirth. The beauty and the beast: Separat\u00ading design from algorithm. In \nProceedings of the European Confer\u00adence onObject-OrientedProgramming(ECOOP 11), 2011. [22] D. Zaparanuks \nand M. Hauswirth. Vision paper: The essence of structural models. In J. Whittle, T. Clark, and T. K\u00a8uhne, \neditors, Model DrivenEngineering Languages andSystems, volume 6981 of Lecture Notes in Computer Science, \npages 470 479. Springer, 2011.  \n\t\t\t", "proc_id": "2254064", "abstract": "<p>Traditional profilers identify where a program spends most of its resources. They do not provide information about why the program spends those resources or about how resource consumption <i>would change</i> for different program inputs. In this paper we introduce the idea of <i>algorithmic profiling</i>. While a traditional profiler determines a set of measured cost <i>values</i>, an algorithmic profiler determines a cost <i>function</i>. It does that by automatically determining the \"inputs\" of a program, by measuring the program's \"cost\" for any given input, and by inferring an empirical cost function.</p>", "authors": [{"name": "Dmitrijs Zaparanuks", "author_profile_id": "81372591618", "affiliation": "University of Lugano, Lugano, Switzerland", "person_id": "P3471152", "email_address": "zaparand@usi.ch", "orcid_id": ""}, {"name": "Matthias Hauswirth", "author_profile_id": "81332503330", "affiliation": "University of Lugano, Luganp, Switzerland", "person_id": "P3471153", "email_address": "Matthias.Hauswirth@usi.ch", "orcid_id": ""}], "doi_number": "10.1145/2254064.2254074", "year": "2012", "article_id": "2254074", "conference": "PLDI", "title": "Algorithmic profiling", "url": "http://dl.acm.org/citation.cfm?id=2254074"}