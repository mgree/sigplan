{"article_publication_date": "06-11-2012", "fulltext": "\n Static Analysis and Compiler Design for Idempotent Processing Marc de Kruijf Karthikeyan Sankaralingam \nSomesh Jha University of Wisconsin Madison {dekruijf,karu,jha}@cs.wisc.edu Abstract Recovery functionality \nhas many applications in computing sys\u00adtems, from speculation recovery in modern microprocessors to fault \nrecovery in high-reliability systems. Modern systems commonly recover using checkpoints. However, checkpoints \nintroduce over\u00adheads, add complexity, and often save more state than necessary. This paper develops a \nnovel compiler technique to recover program state without the overheads of explicit checkpoints. The \ntechnique breaks programs into idempotent regions regions that can be freely re-executed which allows \nrecovery without check\u00adpointed state. Leveraging the property of idempotence, recovery can be obtained \nby simple re-execution. We develop static analysis techniques to construct these regions and demonstrate \nlow over\u00adheads and large region sizes for an LLVM-based implementation. Across a set of diverse benchmark \nsuites, we construct idempotent regions close in size to those that could be obtained with perfect runtime \ninformation. Although the resulting code runs more slowly, typical performance overheads are in the range \nof just 2-12%. The paradigm of executing entire programs as a series of idem\u00adpotent regions we call idempotent \nprocessing, and it has many ap\u00adplications in computer systems. As a concrete example, we demon\u00adstrate \nit applied to the problem of compiler-automated hardware fault recovery. In comparison to two other state-of-the-art \ntech\u00adniques, redundant execution and checkpoint-logging, our idempo\u00adtent processing technique outperforms \nboth by over 15%. Categories and Subject Descriptors D.3.4 [Programming Lan\u00adguages]: Processors code \ngeneration, compilers General Terms Algorithms, Design, Performance, Reliability Keywords Idempotent \nprocessing, idempotent regions 1. Introduction Recovery capability is a fundamental component of modern \ncom\u00adputer systems. It is used to recover from branch misprediction and out-of-order execution [33, 38], \nhardware faults [32, 34], specula\u00adtive memory-reordering in VLIW machines [13, 18], optimistic dy\u00adnamic \nbinary translation and code optimization [11, 14], and trans\u00adactional memory [17, 28]. In each of these \ncases, recovery is used Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. PLDI 12, June 11 16, 2012, Beijing, China. Copyright c &#38;#169; 2012 ACM 978-1-4503-1205-9/12/06. \n. . $10.00 to repair the state of the program in the rare event that an execution failure (i.e. a fault \nor mis-speculation) occurs. Checkpoints provide a conceptually simple solution, and have strong commercial \nprecedent [11, 32, 37]. However, checkpoints are problematic for several reasons. First, software checkpoints \nof\u00adten have high performance overhead and hence, to maintain rea\u00adsonable performance, hardware support \nis often necessary. This hardware support, however, forces interdependencies between pro\u00adcessor structures, \noccupies space on the chip, and entails recur\u00adring energy expenditure regardless of failure occurrence. \nPartic\u00adularly for emerging massively parallel and mobile processor de\u00adsigns, the per-core hardware support \ncomes at a premium, while the recovery support may be desirable only under speci.c or rare circumstances. \nHardware checkpointing resources are also rarely exposed to software, and are even less often con.gurable \nin terms of their checkpointing granularity, limiting their wider applicabil\u00adity. Finally, checkpoints \nhave limited application visibility and are often overly aggressive in saving more state than is required \nby the application [9, 27]. To combat these dif.culties, idempotence the property that re\u00adexecution is \nfree of side-effects has been previously proposed as an alternative to checkpoints. In contrast to explicit \ncheckpoints, idempotence allows the architecture state at the beginning of a code region to be used as \nan implicit checkpoint that is never explicitly saved or restored. In the event of an execution failure, \nidempotence is used to correct the state of the system by simple re-execution. Over the years, idempotence \nhas been both explicitly and im\u00adplicitly employed as an alternative to checkpoints. Table 1 classi\u00ad.es \nprior work in terms of its application domain and the level at which idempotence is used and identi.ed \n[2, 9, 10, 12, 16, 19, 21, 23, 25, 31, 36]. One of the earliest uses is by Mahlke et al. in using restartable \ninstruction sequences for exception recovery in specula\u00adtive processors [23]. More recently, Hampton \nand Asanovi\u00b4 c apply idempotence to support virtual memory on vector machines [16], Tseng and Tullsen \napply idempotence to support data-triggered parallel thread execution [36], and Feng et al. leverage \nidempo\u00adtence for low-cost hardware fault recovery [12]. As the table shows, idempotence has historically \nbeen applied only under speci.c do\u00admains such as exception recovery and multithreading, often only under \nrestricted program scope, and often using only limited or no static analysis. In this paper, we develop \nan analysis framework to enable each of the above uses (and others yet to be invented), irrespective \nof their application domain and their underlying purpose, and across entire programs. In particular, \nwe develop static analysis techniques and a compilation strategy to statically partition programs into \nlarge idempotent regions. We develop a provably correct region parti\u00adtioning algorithm, demonstrate a \nworking compiler implementa\u00adtion, and demonstrate application to at least one speci.c problem domain. \nWe bring together the somewhat disparate uses listed in  Technique name Application domain Program scope \nIdempotence discovery technique Sentinel scheduling [23] Fast mutual exclusion [2] Multi-instruction \nretry [21] Atomic heap transactions [31] Reference idempotency [19] Restart markers [16] Relax [9] Data-triggered \nthreads [36] Idempotent processors [10] Encore [12] iGPU [25] Speculative memory re-ordering Uniprocessor \nmutual exclusion Branch mispredictions and faults Memory allocation Reducing speculative storage Virtual \nmemory in vector machines Hardware fault recovery Data-triggered multi-threading Processor simpli.cation \nHardware fault recovery GPU exceptions and speculation Speculative code regions Atomicity primitives \nWhole program Garbage collector Non-parallelizable code Loops and vector code Selected code regions Selected \ncode regions Whole program Selected code regions Whole GPU program Register-level compiler analysis Programmer \ninspection Compiler antidependence analysis Programmer inspection Memory-level compiler analysis Compiler \nloop analysis Programmer inspection Programmer inspection Unspeci.ed compiler analysis Compiler interval \nanalysis PTX-speci.c compiler analysis Table 1. Uses of .ne-grained idempotence in hardware and software \ndesign (in chronological order). Table 1 and unify them under a single paradigm we call idempotent processing, \nwhich allows the synthesis of multiple such uses in a single system or implementation artifact. In brief, \nour approach operates as follows. First, we note that using a conventional compiler, idempotent regions \nare typically small. Our static analysis eliminates the compilation artifacts re\u00adsponsible for these \nsmall idempotent region sizes by identifying re\u00adgions in a function that are semantically idempotent. \nThese regions are then compiled in such a way that no artifacts are introduced and the idempotence property \nis preserved throughout code generation. To do this, the compiler limits register and stack memory re-use, \nwhich reduces locality and thereby introduces runtime overhead. However, typical overheads are in the \nrange of just 2-12%. In exchange for these overheads, our analysis partitions a func\u00adtion into regions \nthat are close in size to the largest regions that would be constructed given near-perfect runtime information. \nWe show that the problem of .nding very large idempotent regions can be cast as a vertex multicut problem, \na problem known to be NP-complete in the general case. We apply an approximation algo\u00adrithm and a heuristic \nthat incorporates loop information to optimize for dynamic behavior and .nd that overall region sizes \nare in most cases close to ideal. Overall, we make the following contributions: We perform a detailed \nanalysis of idempotent regions in con\u00adventional programs and quantitatively demonstrate that conven\u00adtional \ncompilers arti.cially inhibit the sizes of the idempotent regions in programs, severely limiting their \nusefulness.  We develop static analysis and compiler techniques to preserve the inherent idempotence \nin applications and construct large idempotent regions. We formulate the problem of .nding idem\u00adpotent \nregions as a graph cutting (vertex multicut) problem and optimize for runtime behavior using a heuristic \nthat incorpo\u00adrates loop information.  We present a detailed characterization of our idempotent re\u00adgions, \nwhich can be applied in the context of the various uses previously proposed in the literature.  We demonstrate \nour idempotent processing solution applied to the problem of recovery from transient faults in micro\u00adprocessors. \nOur idempotence-based recovery implementation performs over 15% better than two competing state-of-the-art \ncompiler-automated recovery techniques.  The remainder of this paper is organized as follows. Section \n2 gives a complete overview of this paper. Section 3 presents a quan\u00adtitative study of idempotent regions \nas they exist inherently in ap\u00adplication programs. Section 4 presents our idempotent region con\u00adstruction \nalgorithm. Section 5 gives details of our compiler imple\u00admentation. Section 6 presents our quantitative \nevaluation. Section 7 presents related work. Finally, Section 8 concludes. 2. Overview This section provides \na complete overview of this paper. We de.ne idempotence in terms of data dependences and present a motivating \nexample that illustrates how data dependences can inhibit idempo\u00adtence. We show how these data dependences \ncan be manipulated to grow the sizes of idempotent regions and give an overview of our partitioning algorithm \nthat attempts to maximize the sizes of these regions. Finally, we describe how statically-identi.ed idempotence \ncan be used to recover from a range of dynamic execution failures. 2.1 Identifying Idempotent Regions \nA region of code (assume a linear sequence of instructions for now) is idempotent if the effect of executing \nthe region multiple times is identical to executing it only a single time. Intuitively, this behavior \nis achieved if the region does not overwrite its inputs. With the same inputs, the region will produce \nthe same outputs. If a region overwrites its inputs, it reads the overwritten values when it re\u00adexecutes, \nchanging its behavior. A variable is an input to a region if it is live-in to the region. Such a variable \nhas a de.nition that reaches the region s entry point and has a corresponding use of that de.nition after \nthe region entry point. Below, we use this observation to derive a precise de.nition of idempotence in \nterms of data dependences. We use the term .ow dependence to refer to a read-after-write (RAW) dependence \nand the term antidependence to refer to a write-after-read (WAR) dependence. By de.nition, a live-in \nvariable has a .ow dependence that spans the region s entry point. Because the variable s de.nition must \ncome before the entry to the region, the de.nition is not inside the region, and hence there is no de.nition \nthat precedes the .rst use of that variable inside the region. Hence, a live-in has no .ow dependence \nbefore the .rst use of that variable inside the region. Since a live-in has no .ow dependence, overwriting \na live-in must occur after the point of the use. Thus, an overwritten live-in has an antidependence after \nthe absence of a .ow dependence. It follows that a region of code is idempotent if it contains no antidependences \nnot preceded by a .ow dependence. The table below shows three statement sequences involving a variable \nx and uses the above de.nition to identify whether the sequences are idempotent or not: RAW RAW.WAR WAR \nx = 5 x = 5 y = x Sequence y = x y = x x = 8 x = 8 Idempotent? Yes Yes No The antidependence after no \n.ow dependence chain that breaks the idempotence property as shown on the right we call a clobber  \nFigure 1. An example illustrating how clobber antidependences inhibit idempotence. antidependence. Some \nclobber antidependences are strictly nec\u00adessary according to program semantics. These clobber antidepen\u00addences \nwe label semantic clobber antidependences. The other clob\u00adber antidependences we label arti.cial clobber \nantidependences. The following example demonstrates semantic and arti.cial clob\u00adber antidependences. \nA motivating example. For the remainder of this section, we use the C function shown in Figure 1(a) as \na running example. The function, list_push, checks a list for over.ow and then pushes an integer element \nonto the end of the list. The semantics of the function clearly preclude idempotence: even if there is \nno over.ow, re-executing the function will put the element onto the end of the already-modi.ed list, \nafter the copy that was pushed during the original execution. As we will show, the source of the non\u00adidempotence \nis the increment of the input variable list->size on line 22: without this increment, re-execution would \nsimply cause the value that was written during the initial execution to be safely overwritten with the \nsame value. Figure 1(b) shows the function compiled to a load-store inter\u00admediate representation. The \n.gure shows the control .ow graph of the function, which contains three basic blocks B1, B2, and B3. \nInside each block are shown the basic operations, Si, and the use of pseudoregisters, ti, to hold operands \nand read and write values to and from memory. Figure 1(c) shows the program dependence graph of the function \nfocusing only on .ow dependences (dashed) and antidependences (solid). The antidependences are further \ndis\u00adtinguished as clobber antidependences (dark) and not clobber an\u00adtidependences (light)1. The .gure \nshows four clobber antidepen\u00addences: S1.S5, S2.S5, S1.S10, and S7.S10. The .rst two clobber antidependences \ndepend on S5, which overwrites the pseu\u00addoregister t0, and the second two depend on S10, which overwrites \nthe memory location at t0 +4. The two clobber antidependences that depend on S5 are unnec\u00adessary: they \nare arti.cial clobber antidependences. We can elimi\u00adnate these clobber antidependences simply by writing \nto a differ\u00adent pseudoregister. Figure 2 shows the effect of replacing t0 in S5 1 For simplicity, we \nassume the pointer argument list, the global other_list, and their respective buffer arrays are known \nnot to alias. Figure 2. Renaming t0 in S5 to t4. with a new pseudoregister t4. All uses of t0 subsequent \nto S5 are renamed to use t4 as well, and a new statement S11 is inserted that moves t0 into t4 along \nthe path where S5 is not executed. S11 is placed inside a new basic block B4 which bridges B1 and B3. \nA compiler can then permanently eliminate these arti.cial clobber antidependences by ensuring that t4 \nand t0 are not assigned to the same physical register or the stack slot during register allocation. If \nthey are, then t4 will overwrite t0 and the two clobber antide\u00adpendences will simply re-emerge. To do \nthis, the register allocator can be constrained such that all pseudoregisters that are live-in to the \nregion are also marked as live-out to the region. This enables idempotence in exchange for some additional \nregister pressure. The .nal two clobber antidependences write to the memory location t0 +4 in S10, which \ncorresponds with the store of the list->size increment on line 22 of Figure 1(a). Unfortunately, the \ndestination of this store is .xed by the semantics of the pro\u00adgram: if we didn t increment this variable, \nthen the function would not increase the size of the list, which would violate the semantics of the function. \nBecause we cannot legally rename the store des\u00adtination, we label these clobber antidependences semantic \nclobber antidependences.  Summary. Table 2 summarizes the differences between seman\u00adtic and arti.cial \nclobber antidependences. Semantic clobber antide\u00adpendences act on heap, global, and non-local stack memory, \nwhich we hereafter often refer to as just memory . These memory loca\u00adtions are not under the control \nof the compiler; they are speci.ed in the program itself and cannot be re-assigned. In contrast, arti.cial \nclobber antidependences act on pseudoregister locations: regis\u00adters and local stack memory. These resources \nare compiler con\u00adtrolled, and assuming effectively in.nite stack memory, can be ar\u00adbitrarily re-assigned. \nWhile in practice stack memory is limited, our compiler does not grow the size of the stack signi.cantly \nand we have no size-related dif.culties compiling any benchmarks.  2.2 Constructing Idempotent Regions \nAssuming we can eliminate all arti.cial clobber antidependences, we show in Section 3 that the idempotent \nregions that exist in appli\u00adcation programs are potentially very large. However, the problem of statically \nconstructing large idempotent regions remains surpris\u00adingly non-trivial. In principle, the problem should \nbe as simple as merely identifying and constructing regions that contain no seman\u00adtic clobber antidependences. \nHowever, this solution is circularly dependent on itself: identifying semantic clobber antidependences \nrequires identi.cation of region live-in variables, which in turn re\u00adquires identi.cation of the regions. \nThis circular dependence is il\u00adlustrated below: Our solution to this problem is to transform the function \nso that, with the exception of self-dependent pseudoregister antide\u00adpendences2, all antidependences are \nnecessarily semantic clobber antidependences. We then construct idempotent regions by identi\u00adfying regions \nthat contain no antidependences. Antidependence in\u00adformation does not depend on region live-in information, \nand hence the circular dependence chain is broken. During this process, self\u00addependent pseudoregister \nantidependences are optimistically as\u00adsumed not to emerge as clobber antidependences; those that would \nemerge as clobber antidependences after the region construction are patched in a subsequent re.nement \nstep. Considering only antidependence information, we show that the problem of partitioning the program \ninto idempotent regions is equivalent to the problem of cutting the antidependences, such that a cut \nbefore statement S starts a new region at S. In this man\u00adner, no single region contains both ends of \nan antidependence and hence the regions are idempotent. To maximize the region sizes, we cast the problem \nin terms of the NP-complete vertex multicut prob\u00adlem and use an approximation algorithm to .nd the minimum \nset of cuts, which .nds the minimum set of regions. This maximizes the average static region size, and \nwe then employ heuristics to maxi\u00admize the sizes of regions as they occur dynamically at runtime. We \nrefer to the overall algorithm as our idempotent region construction algorithm. In Section 4 we describe \nthe algorithm in detail and in Section 5 we discuss the speci.cs of our implementation. 2 These are antidependences \nthat occur across loop iterations and have assignments of the form ti = f(ti)). Figure 3. The data dependence \ngraph in SSA form. Figure 3 shows our algorithm applied to our running example. The initial step in the \nprocess is the conversion of all pseudoreg\u00adister assignments to static single assignment (SSA) form [8]. \nThe .gure shows the dependence graph of Figure 1(c) simpli.ed by the SSA transformation. The code structure \nis the same as in Fig\u00adure 1(b) except that an SSA f-node is placed at the head of B3. The f-node we label \nS5 and we fold the original S5 into it. Under SSA, the arti.cial clobber antidependences disappear and \nthe se\u00admantic ones remain. Both semantic clobber antidependences write to memory location mem[t5 + 4] \nin statement S10, one with a may-alias read in statement S1 and the other with a must-alias read in statement \nS7. In general, the problem of .nding the best places to cut the antidependences is NP-complete. However, \nfor this sim\u00adple example the solution is straightforward: it is possible to place a single cut that cuts \nboth antidependences. The cut can be placed before S8, S9, or S10. Regardless of where the cut is placed, \nthe function is ultimately divided into three idempotent regions in to\u00adtal: under our initial de.nition \nof a region as a linear instruction sequence, depending on the outcome of the control decision inside \nthe function, two linear sequences exist up to the point of the cut, and after the cut there is one additional \nsequence.  2.3 Using Idempotence for Recovery In this section, we address how statically-identi.ed idempotence \ncan be used to recover from dynamic execution failures. This is not obvious because an execution failure \nmay have side-effects that can corrupt arbitrary state. For instance, as a result of microprocessor branch \nmisprediction or due to a hardware soft error, a region s inputs may be accidentally modi.ed, or state \nthat is outside the scope of the current region may be modi.ed (resulting in problems for other regions). \nTolerating control .ow errors. We .rst consider the possibility of incorrect control .ow, which can arise \ndue to e.g. a branch mispre\u00addiction and can cause an incorrectly executed region to overwrite an input \nof the correct region or its succeeding regions. Fortunately, tolerating control .ow errors is relatively \nstraight\u00adforward with our method of placing region boundaries using cuts. First, we observe that each \ncontrol .ow path from a given cut C to any subsequent cut constitutes an idempotent region, and hence \nany incorrect execution belongs to an idempotent region with the same entry point at C as the region \nof the correct execution. Hence, by compiling in such a way that the inputs of all regions starting at \nC are preserved between C and all subsequent cut points, execution can be made idempotent regardless \nof control .ow.  Type of Clobber Antidependence Storage Resources Semantic clobber antidependence \nHeap, global, non-local stack memory ( memory ) Arti.al clobber antidependence Registers and local stack \nmemory ( pseudoregisters ) Table 2. Semantic and arti.cial clobber antidependences and the resources \non which they operate. Figure 4. Average dynamic idempotent region sizes in the limit (y-axis is log-scale). \nAt this point, we rework our de.nition of idempotent region to achieve this property. Our previous de.nition \nof a region as an instruction sequence we now label more precisely as an idempotent path, and we rede.ne \nan idempotent region as a collection of idempotent paths that share the same entry point. (Thus, a region \nis a subset of a program s control .ow graph with a single unique entry point and multiple possible exit \npoints, and a path is a trace of instructions from such a region s entry point to one of its exit points.) \nAn idempotent region preserves idempotence regardless of control .ow by considering as inputs the collective \ninputs of all its containing paths. Unfortunately, however, it is now no longer possible to say that \nan input is necessarily overwritten only after the point of a read, as in the de.nition of idempotence \nin terms of a clobber antidependences; with this de.nition of region, a live-in may be written before \nit is read along an incorrect control .ow path. This forces a more conservative compiler analysis. For \nregisters and stack memory, the impact of this conserva\u00adtive analysis is manageable and the static analysis \nand compiler implementation we describe in this paper preserves pseudoregis\u00adter inputs regardless of \ncontrol .ow. However, for other types of storage namely, heap, global, and non-local stack memory we \n.nd the compiler s limited control over these resources too con\u00adstricting. In particular, with the possibility \nfor differing control .ow on re-execution, an idempotent region would not be able to contain a store \nto a given address unless such a store occurred along all paths through the region. Thus, for memory \nwe assume that stores are buffered and not released until control .ow has been veri.ed. We propose to \nre-use the store buffer that already exists in modern processors for this purpose. This allows us to \nreason about inputs from non-local memory more optimistically in terms of clobber an\u00adtidependences, considering \nseparately each path through an idem\u00adpotent region. Considering our modi.ed de.nition of a region, the \n.rst two idempotent paths (formerly, regions ) in the example of Figure 1 merge as one idempotent region. \nTolerating arbitrary errors. While the situation with hardware soft errors may appear much more dire, \nin practice techniques like error-correcting codes (ECC) can effectively protect existing mem\u00adory and \nregister state, leaving instruction execution itself as the only possible source of error. Since idempotence \nalready precludes over\u00adwriting input state, regardless of whether the value written is cor\u00adrupted or \nnot, only three additional requirements are needed: (1) instructions must write to the correct register \ndestinations, (2) ex\u00adecution must follow the program s static control .ow edges, and (3) as with branch \nmisprediction, stores must be buffered until they are veri.ed. Altogether, these four total mechanisms \nECC, reg\u00adister destination veri.cation, control .ow veri.cation, and store veri.cation are widely assumed \nin prior work on error recovery in software [6, 9, 21], and a variety of compatible hardware and soft\u00adware \ntechniques have been previously developed [5, 24, 29, 30]. Recovering non-idempotent instructions. Finally, \nwe note that some instructions such as certain memory-mapped I/O operations and some types of synchronization \ninstructions are inherently non\u00adidempotent. In this work, we consider such non-idempotent instruc\u00adtions \nas single-instruction idempotent regions that either terminate correctly or have no side-effects that \nmake it unsafe for them to be re-executed for recovery. 3. Exploring the Potential: A Limit Study To \nunderstand how much arti.cial clobber antidependences inhibit idempotent region sizes, we performed a \nlimit study to ascertain the nature of the clobber dependences that emerge during program execution. \nOur goal is to understand the extent to which it would be possible to construct idempotent regions given \nperfect runtime information. Methodology. We used the gem5 simulator [4] to measure the lengths of the \nidempotent paths that dynamically execute through a region across a range of benchmarks compiled for \nthe ARMv7 instruction set. For each benchmark, we measured the distribution of path lengths occurring \nover a 100 million instruction period starting after the setup phase of the application. We evaluated \ntwo benchmark suites: SPEC 2006 [35], a suite targeted at conventional single-threaded workloads, and \nPARSEC [3], a suite targeted at emerging multi-threaded workloads. We use a conventional optimizing compiler \nto generate pro\u00adgram binaries, and measure idempotent path length optimistically as the number of instructions \nbetween dynamic occurrences of clobber antidependences. This optimistic (dynamic) measurement is used \nin the absence of explicit (static) region markings in these conventionally-generated binaries. We study \nidempotent regions divided by three different categories of clobber antidependences: (1) only semantic \nclobber antidependences, (2) only semantic clob\u00adber antidependences with regions split at function call \nboundaries, and (3) both semantic and arti.cial clobber antidependences with regions split at function \ncall boundaries. We consider as arti.cial clobber antidependences all clobber antidependences on registers \nand those with writes relative to the stack pointer, which are universally register spills for our compiler. \n These are the clobber antidependences that can generally be elim\u00adinated by renaming pseudoregisters \nand careful register and stack slot allocation. We assume the remaining clobber antidependences are all \nsemantic. We consider separately regions divided by se\u00admantic clobber antidependences that cross function \ncall boundaries to understand the potential improvements of an inter-procedural compiler analysis over \nan intra-procedural one. To explore what is achievable in the inter-procedural case, we optimistically \nassume that call frames do not overwrite previous call frames. We also op\u00adtimistically ignore antidependences \nthat necessarily arise due to the calling convention (e.g. overwriting the stack pointer) and assume \nthe calling convention can be rede.ned or very aggressive inlining can be performed such that this obstacle \nis weakened or removed. Results and conclusions. Our experimental results, shown in Figure 4, identify \nthree clear trends. First, we see that regions di\u00advided by both arti.cial and semantic clobber antidependences \nare much smaller than those divided by semantic clobber antidepen\u00addences alone. The geometric mean path \nlength considering both types is 10.8 instructions, while the length considering just seman\u00adtic clobber \nantidependences is 110 instructions intra-procedurally (a 10x gain) and 1300 inter-procedurally (a 120x \ngain). The second trend is a substantial gain (more than 10x) from allowing idempotent regions divided \nby semantic clobber antide\u00adpendences to cross function boundaries. However, the gains are not reliably \nas large as the 10x gain achieved by removing the arti.\u00adcial clobber antidependences alone: the difference \ndrops to only 4x when we drop the two outliers dealII and blackscholes. The third and .nal trend is that \npath lengths tend to be larger for PARSEC and SPEC FP than for SPEC INT. PARSEC and SPEC FP benchmarks \ntend to overwrite their inputs relatively infrequently due to their memory streaming and compute-intensive \nnature. Overall, we .nd that (1) there is a lot of opportunity to grow idempotent region sizes by eliminating \narti.cial clobber antidepen\u00addences, (2) an intra-procedural static analysis is a good starting point \nfor constructing large idempotent regions, and (3) the great\u00adest opportunity appears to lie with streaming \nand compute-intensive applications. 4. Region Construction Algorithm In this section, we describe our \nidempotent region construction al\u00adgorithm. The algorithm is an intra-procedural compiler algorithm that \ndivides a function into idempotent regions. First, we describe the transformations that allow us to cast \nthe problem of constructing idempotent regions in terms of cutting antidependences. Second, we describe \nthe core static analysis technique for cutting antide\u00adpendences, including optimizations for dynamic \nbehavior. Finally, we describe our register and stack slot allocation to preserve the idempotence of \nour identi.ed regions through code generation. 4.1 Program Transformation Before we apply our static \nanalysis, we .rst perform two code transformations to maximize the ef.cacy of the analysis. The two transformations \nare (1) the conversion of all pseudoregister assign\u00adments to static single assignment (SSA) form, and \n(2) the elimina\u00adtion of all memory antidependences that are not clobber antidepen\u00addences. The details \non why and how are given below. The .rst transformation converts all pseudoregister assignments to SSA \nform. After this transformation, each pseudoregister is only assigned once and all arti.cial clobber \nantidependences are ef\u00adfectively eliminated (self-dependent arti.cial clobber antidepen\u00addences, which \nmanifest in SSA through f-nodes at the head of loops, still remain, but it is safe to ignore them for \nnow). The intent of this transformation is to expose primarily the semantic antide\u00adpendences to the compiler. \nUnfortunately, among these antidepen\u00ad 1. mem[x] = a 1. mem[x] = a 2.b=mem[x] 2.b=a 3. mem[x] = c 3. mem[x] \n= c before after Figure 5. Eliminating non-clobber memory antidependences. dences we still do not know \nwhich are clobber antidependences and which are not, since, as explained in Section 2.2, this determination \nis circularly dependent on the region construction we are trying to achieve. Without knowing which antidependences \nwill emerge as clobber antidependences, we do not know which antidependences must be cut to form the \nregions. Hence, we attempt to re.ne things further. After the SSA transformation, it follows that the \nremaining an\u00adtidependences are either self-dependent antidependences on pseu\u00addoregisters or antidependences \non memory locations. For those on memory locations, we employ a transformation that resolves the aforementioned \nambiguity regarding clobber antidependences. The transformation is a simple redundancy-elimination transformation \nillustrated by Figure 5. The sequence on the left has an antidepen\u00addence on memory location x that is \nnot a clobber antidependence because the antidependence is preceded by a .ow dependence. Ob\u00adserve that \nin all such cases the antidependence is made redundant by the .ow dependence: assuming both the initial \nstore and the load of x must alias (if they only may alias we must conservatively assume a clobber antidependence) \nthen there is no reason to re\u00adload the stored value since there is an existing pseudoregister that already \nholds the value. The redundant load is eliminated as shown on the right of the .gure: the use of memory \nlocation x is replaced by the use of pseudoregister a and the antidependence disappears. Unfortunately, \nthere is no program transformation that resolves the uncertainty for self-dependent pseudoregister antidependences. \nIn the following section, we initially assume that these antidepen\u00addences can be register allocated such \nthat they do not become clob\u00adber antidependences (i.e. we can precede the antidependence with a .ow dependence \non its assigned physical register or stack slot). Hence, we construct regions around them, considering \nonly the known, memory-level clobber antidependences. After the construc\u00adtion is complete, we check to \nsee if our assumption holds. If not, we insert additional region cuts as necessary.  4.2 Static Analysis \nAfter our program transformations, our static analysis constructs idempotent regions by cutting all potential \nclobber antidepen\u00addences in a function. The analysis consists of two parts. First, we construct regions \nbased on semantic antidependence infor\u00admation by cutting memory-level antidependences and placing re\u00adgion \nboundaries at the site of the cuts. Second, we further divide loop-level regions as needed to accommodate \nthe remaining self\u00addependent pseudoregister clobber antidependences. 4.2.1 Cutting Memory-Level Antidependences \nTo ensure that a memory-level antidependence is not contained inside a region, it must be split across \nthe boundaries between regions. Our algorithm .nds the set of splits, or cuts , that creates the smallest \nnumber of these regions. In this section, we derive our algorithm as follows: 1. We de.ne our problem \nas a graph decomposition that must satisfy certain conditions. 2. We reduce the problem of .nding an \noptimal graph decomposi\u00adtion to the minimum vertex multicut problem.   Figure 6. An example region \ndecomposition. 3. To generate a solution, we formulate the problem in terms of the hitting set problem. \n 4. We observe that a near-optimal hitting set can be found ef.\u00adciently using an approximation algorithm. \n  Problem de.nition. For a control .ow graph G =(V,E) we de.ne a region as a sub-graph Gi =(Vi,Ei,hi) \nof G, where hi . Vi and all nodes in Vi are reachable from hi through edges in Ei. We call hi the header \nnode3 of Gi.A region decomposition of the graph G is a set of sub-graphs {G1, \u00b7\u00b7\u00b7 ,Gk} that satis.es \nthe following conditions: each node v . V is in at least one sub-graph Gi,  the header nodes for the \nsub-graph are distinct (for i j,  = hi = hj ), and no antidependence edge is contained in a sub-graph \nGi for 4 1 = i = k. Our problem is to decompose G into the smallest set of sub\u00adgraphs {G1, \u00b7\u00b7\u00b7 ,Gk}. \nFigure 6 gives an example. Figure 6(a) shows a control .ow graph G and 6(b) shows the set of antidepen\u00addence \nedges in G. Figure 6(c) shows a possible region decomposi\u00adtion for G. The shown region decomposition \nhappens to be optimal; that is, it contains the fewest possible number of regions. Reduction to vertex \nmulticut. We now reduce the problem of .nding an optimal region decomposition with the problem of .nd\u00ading \na minimum vertex multicut. De.nition 1. (Vertex multicut) Let G =(V,E) be a directed graph with set of \nvertices V and edges E. Assume that we are given pairs of vertices A . V \u00d7 V . A subset of vertices H \n. V is called a vertex multicut for A if in the subgraph G' of G where the vertices from H are removed, \nfor all ordered pairs (a, b) . A there does not exist a path from a to b in G'. Let G =(V, E) be our \ncontrol .ow graph, A the set of antidependence edge pairs in G, and H a vertex multicut for A. Each hi \n. H implicitly corresponds to a region Gi as follows: The set of nodes Vi of Gi consists of all nodes \nv . V such that there exists a path from hi to v that does not pass through a node in H -{hi}.  The \nset of edges Ei is E n (Vi \u00d7 Vi).  It follows that a minimum vertex multicut H = {h1, \u00b7\u00b7\u00b7 ,hk} di\u00adrectly \ncorresponds to an optimal region decomposition {G1, \u00b7\u00b7\u00b7 ,Gk}of G over the set of antidependence edge \npairs A in G. 3 Note that, while we use the term header node, we do not require that a header node hi \ndominates all nodes in Vi as de.ned in other contexts [1]. 4 This condition is stricter than necessary. \nIn particular, an antidependence edge in Gi with no path connecting the edge nodes implying that the \nthe antidependence is formed over a loop revisiting Gi is safely contained in Gi. However, determining \nthe absence of such a path requires a path\u00adsensitive analysis. We limit our solution space to path-insensitive \nanalyses. Solution using hitting set. The vertex multicut problem is NP\u00adcomplete for general directed \ngraphs [15]. To solve it, we reduce it to the hitting set problem, which is also NP-complete, but for \nwhich good approximation algorithms are known [7]. De.nition 2. (Hitting set) Given a collection of sets \nC = {S1, \u00b7\u00b7\u00b7 ,Sm},a minimum hitting set for C is the smallest set H such that, for all Si . C, H n Si \n= \u00d8. Note that we seek a set H . V such that, for all (ai,bi) . A, all paths p from ai to bi have a vertex \nin H (in other words, H is a hitting set of .= .(ai,bi).Api, where pi is the set of paths from ai to \nbi). This formulation is not computationally tractable, however, as the number of paths between any pair \n(ai,bi) can be exponential in the size of the graph. Instead, for each (ai,bi) . A, we associate a single \nset Si . V that consists of the set of nodes that dominate bi but do not dominate ai. We then compute \na hitting set H over C = {Si|Si for (ai,bi) . A}. Using Lemma 1 it is easy to see that for all antidependence \nedges (ai,bi) . A, every path from ai to bi passes through a vertex in H. Hence, H is both a hitting \nset for C and a vertex multicut for A. We use a greedy approximation algorithm for the hitting set i \nproblem that runs in time O( |Si|). This algorithm chooses Si.C at each stage the vertex that intersects \nthe most sets not already intersected. This simple greedy heuristic has a logarithmic approx\u00adimation \nratio [7] and is known to produce good quality results. Lemma 1. Let G =(V,E,s) be a directed graph with \nentry node s . V and (a, b) be a pair of vertices. If x . V dominates b but does not dominate a, then \nevery path from a to b passes through x. Proof: We assume that a pair of vertices (a, b) are both reachable \nfrom the entry node s. Let the following conditions be true. Condition 1: There exists a path from (a, \nb) that does not pass through the node x.  Condition 2: There exists a path from s to a that does not \npass through x.  If conditions 1 and 2 are true, then there exists a path from s to b that does not \npass through x. This means x cannot dominate b. In other words, conditions 1 and 2 imply that x cannot \ndominate b. Given that x dominates b, one of the conditions 1 and 2 must be false. If condition 1 is \nfalse, we are done. If condition 2 is false, then x dominates a, which leads to a contradiction.  4.2.2 \nCutting Self-Dependent Pseudoregister Antidependences After memory antidependences have been cut, we \nhave a prelimi\u00adnary region decomposition over the function. From here, we con\u00adsider the remaining category \nof clobber antidependences the self\u00addependent pseudoregister antidependences and allocate them in such \na way that they do not emerge as clobber antidependences. In SSA form, a self-dependent pseudoregister \nantidependence manifests as a write occurring at the point of a f-node assignment, with one of the f-node \ns arguments data-dependent on the assigned pseudoregister itself. Due to SSA s dominance properties, \nsuch self-dependent pseudoregister assignments alway occur at the head of loops. Figure 7(a) provides \na very simple example. Note that in the example the self-dependent antidependence is actually two antidependences, \nS1 . S2 and S2 . S1. We refer to it as only a single antidependence for ease of explanation. To prevent \nself-dependent pseudoregister antidependences from emerging as clobber antidependences, the invariant \nwe must en\u00adforce is that a loop containing such an antidependence either con\u00adtains no cuts or contains \nat least two cuts along all paths through the loop body. If either of these conditions is already true, \nno modi\u00adFigure 7. Clobber-free allocation of self-dependent pseudoregister antidependences.  .cation \nto the preliminary region decomposition is necessary. Oth\u00aderwise, we insert additional cuts such that \nthe second condition be\u00adcomes true. The details on why and how are provided below. Case 1: A loop with \nno cuts. Consider the self-dependent antide\u00adpendence shown in Figure 7(a). For a loop that contains no \ncuts, this antidependence can be trivially register allocated as shown in Figure 7(b). In the .gure, \nwe de.ne the register (which could also be a stack slot if registers are scarce) of the antidependence \nout\u00adside the loop and hence across all loop iterations all instances of the antidependence are preceded \nby a .ow dependence. Case 2: A loop with at least 2 cuts. A self-dependent antidepen\u00addence for a loop \nthat contains at least two cuts can also be trivially\u00adregister allocated as shown in Figure 7(c). Here \nthe antidependence is manipulated into two antidependences, one on R0 and one on R1, and the antidependences \nare placed so that they straddle region boundaries. Note that, for this to work, at least two cuts must \nex\u00adist along all paths through the loop body. This is obviously true in Figure 7(c) but in the general \ncase it may not be. Case 3: Neither case 1 or 2. In the remaining case, the self\u00addependent antidependence \nis in a loop that contains at least one cut but there exist one or more paths through the loop body that \ndo not cross at least two cuts. In this case, we know of no way to register allocate the antidependence \nsuch that it does not emerge as a clobber antidependence. Hence, we resign ourselves to cutting the antidependence \nso that we have at least two cuts along all paths in the loop body, as in Case 2. This produces a .nal \nregion decomposition resembling Figure 7(c).  4.3 Optimizing for Dynamic Behavior Our static analysis \nalgorithm optimizes for static region sizes. How\u00adever, when considering loops, we know that loops tend \nto execute multiple times. We can harness this information to grow the sizes of the dynamic paths that \nexecute through our regions at runtime. We account for loop information by incorporating a simple heuristic \ninto the hitting set algorithm from Section 4.2.1. In par\u00adticular, we adjust the algorithm to greedily \nchoose cuts at nodes from the outermost loop nesting depth .rst. We then break ties by choosing a node \nwith the most sets not already intersected as nor\u00admal. This improves the path lengths substantially in \ngeneral, al\u00adthough there are cases where it reduces them. A better heuristic most likely weighs both \nloop nesting depth and intersecting set in\u00adformation more evenly, rather than unilaterally favoring one. \nBetter heuristics are a topic for future work.  4.4 Code Generation With the idempotent regions constructed, \nthe .nal challenge is to code generate speci.cally, register and stack allocate the func\u00adtion so that \narti.cial clobber antidependences are not re-introduced. To do this, we constrain the register and stack \nmemory alloca\u00adtors such that all pseudoregisters that are live-in to a region are also live-out to the \nregion. This ensures that all registers and stack slots that contain input are not overwritten and hence \nno new clobber antidependences emerge. 5. Compiler Implementation We implemented the region construction \nalgorithm of Section 4 using LLVM [20]. Each phase of the algorithm is implemented as described below. \nCode transformation. Of the two transformations described in Section 4.1, the SSA code transformation \nis automatic as the LLVM intermediate representation itself is in SSA form. We implement the other transformation, \nwhich removes all non-clobber memory antidependences, using an existing LLVM redundancy elimination transformation \npass. Cutting memory-level antidependences. We gather memory an\u00adtidependence information using LLVM s \nbasic alias analysis in\u00adfrastructure. The antidependence cutting is implemented exactly as described \nin Section 4.2.1. Cutting self-dependent pseudoregister antidependences. We handle self-dependent register \nantidependences as in Section 4.2.2 with one small enhancement: before inserting cuts, we attempt to \nunroll the containing loop once if possible. The reason is that insert\u00ading cuts increases the number \nof idempotent regions and thereby reduces the size of the loop regions. By unrolling the loop once, we \ncan place the second necessary cut in the unrolled iteration. This effectively preserves region sizes \non average. It also improves the performance of the register allocator by not requiring the insertion \nof extra copy operations between loop iterations (enabling a form of double buffering). Optimizations \nfor dynamic behavior. We optimize for dynamic behavior exactly as described in Section 4.3. Code generation. \nWe extend LLVM s register allocation passes to generate machine code as described in Section 4.4. To \nmaintain the calling convention, functions that contain only a single region are split into two regions \nto allow parameter values to be overwrit\u00adten by return values as necessary. 6. Evaluation For evaluation, \nwe .rst present the characteristics (region sizes and runtime overhead) produced by our idempotent region \nconstruction across a range of applications. We then present results evaluating our technique for recovery \nfrom transient hardware faults. 6.1 Methodology We evaluate benchmarks from the SPEC 2006 [35] and PAR-SEC \n[3] benchmark suites. We compile each benchmark to two different binary versions: an idempotent binary, \ncompiled using our idempotent region construction implemented in LLVM; and an original binary, generated \nusing the regular optimized LLVM com\u00adpiler .ow. Our performance results are obtained for the ARMv7 instruc\u00adtion \nset simulating a modern two-issue processor using the gem5 simulator [4]. To account for the differences \nin instruction count between the idempotent and original binary versions, simulation length is measured \nin terms of the number of functions executed, which is constant between the two versions. All benchmarks \nare fast-forwarded the number of function calls needed to execute at least 5 billion instructions on \nthe original binary, and execution is then simulated for the number of function calls needed to execute \n100 million additional instructions on the original binary.   Figure 8. Distribution of idempotent \npath lengths. Figure 9. Average idempotent path lengths.  6.2 Idempotent Region Characteristics Region \nsizes and path lengths. An important characteristic of our region construction is the size of the idempotent \nregions it statically produces. More important, however, is the length of the paths that dynamically \nexecute through the idempotent regions at runtime. In general, longer path lengths are better for two \nreasons. The .rst has to do with runtime overhead: in the limit as path length ap\u00adproaches in.nity, the \nrelative cost to preserve a region s live-in state approaches zero. In the worst case, all live state \nmust be pushed to the stack before the region s start, and this .xed maximum cost is amortized over the \nregion s execution. The second reason has to do with detection latencies: longer path lengths allow execution \nto proceed speculatively for longer amounts of time while potential (but presumably unlikely) execution \nfailures remain undetected. In practice, however, optimal path length (and hence, region size) depends \non a variety of factors. These factors include the ef\u00adfects of register pressure, aggravated by divergent \ncontrol .ow as regions grow beyond a small set of basic blocks (for reasons relat\u00ading to potentially \nincorrect control .ow as described in Section 2.3). Hence, larger regions are not always better. Additionally, \nwhile longer path lengths better tolerate long detection latencies, mini\u00admizing the recovery re-execution \ncost favors shorter path lengths. This is important particularly when failures are relatively frequent, \nas with e.g. branch prediction. In future work, we plan to explore this optimization space in detail. \nFor this work, we aim to produce the longest possible paths, observing that path lengths are often easily \nreduced as needed to suit application demands. Figure 8 plots the cumulative distribution (weighted by \nexecu\u00adtion time) of the dynamic path lengths executed through our idem\u00adpotent regions across the SPEC \nand PARSEC benchmark suites (in the interest of space, applications from the same suite are not indi\u00advidually \nlabeled). The .gure shows, for instance, that most applica\u00adtions spend less than 20% of their execution \ntime executing paths of length 10 instructions or less. The .gure also shows that path length distributions \nare highly application dependent. Generally, the PAR-SEC applications tend to have a wider, more heavy-tailed \ndistri\u00adbution, while SPEC FP applications have a narrower, more regular distribution. SPEC INT has applications \nin both categories. Figure 9 shows the average length of our idempotent paths compared to those measured \nas ideal in the limit study from Sec-tion 3 (the ideal measurement is for semantic and calls intra\u00adprocedural \nregions divided by dynamic semantic clobber antide\u00adpendences). Our geometric mean path length across \nall benchmarks is roughly 4x less than the ideal (28.1 vs. 116). Two benchmarks, hmmer and lbm, have \nmuch longer path lengths in the ideal case. This is due to limited aliasing information in the region \nconstruc\u00adtion algorithm; with small modi.cations to the source code that improve aliasing knowledge, \nlonger path lengths can be achieved. If we ignore these two outliers, the difference narrows to roughly \n1.5x (30.2 vs. 44.9; not shown). Runtime overheads. Forcing the register allocator to preserve input \nstate across an idempotent region adds overhead because the allocator may not re-use live-in register \nor stack memory resources. Instead, it may need to allocate additional stack slots and spill more registers \nthan might otherwise be necessary. Figure 10 shows the percentage execution time and dynamic in\u00adstruction \ncount overheads. Across the SPEC INT, SPEC FP, and PARSEC benchmarks the geometric mean execution time \nover\u00adheads are 11.2%, 5.4%, and 2.7%, respectively (7.7% overall), These overheads are closely tracked \nby the increase in the dynamic instruction count: 8.7%, 8.2%, and 4.8% for SPEC INT, SPEC FP, and PARSEC, \nrespectively (7.6% overall). The one case where execution time overhead and instruction count overhead \nare substantially different is for gobmk, which has a 26.7% execution time overhead but only a 5.9% instruction \ncount overhead. For this particular benchmark, some code sequences that were previously ef.ciently expressed \nusing ARM predicated exe\u00adcution transform to regular control .ow due to liveness constraints, resulting \nin more control .ow sensitivity. Additionally, most of the added instructions are load and store (spill \nand re.ll) instructions, which have longer latency than regular move instructions. For other applications, \nthe differences mostly trend along the nature of the fundamental data type .oating point or integer. \nIn\u00adteger applications such as those from SPEC INT tend to have higher execution time overheads because \nARM has fewer general purpose registers than .oating point registers (16 vs. 32). Hence, these ap\u00adplications \nare more reliant on register spills and re.lls to preserve liveness. In contrast, .oating point benchmarks \nsuch as SPEC FP and PARSEC have many more available registers. Additionally, the comparatively long path \nlengths of PARSEC benchmarks tend to allow better register re-use since the cost of pushing live-ins \nto the stack is amortized over a longer period of time.  6.3 Idempotence-Based Recovery Our compiler \nimplementation and static analysis are general and span entire programs, and hence they can be used in \nthe context of prior works using idempotence such as those presented in Ta\u00adble 1. As a concrete example, \nhowever, we consider the case for software-only recovery from transient hardware faults (soft errors). \nWe evaluate against two other compiler-based recovery techniques, and for all techniques assume compiler-based \nerror detection using  DMR Baseline INSTRUCTION-TMR CHECKPOINT-AND-LOG IDEMPOTENCE check(r0 != r0 ) \nmajority(r0, r0 , r0 ) br recvr, r0 != r0 retry: ld r1 =[r0] ld r1 =[r0] ld r1 =[r0] mov rp = {retry} \nld r1 = [r0] ld r1 = [r0] ld r1 = [r0] ... addr2 =r3,r4 ld r1 = [r0] addr2 =r3,r4 jmp rp, r0 != r0 add \nr2 = r3 , r4 addr2 =r3 ,r4 add r2 = r3 , r4 ld r1 =[r0] check(r1 != r1 ) add r2 = r3 , r4 br recvr, r1 \n!= r1 ld r1 = [r0] check(r2 != r2 ) add r2 = r3 , r4 br recvr, r2 != r2 addr2 =r3,r4 st [r1] = r2 majority(r1, \nld tmp = [r1] add r2 = r3 , r4 majority(r2, ld tmp = [r1] jmp rp, r1 != r1 br recvr, tmp != tmp jmp rp, \nr2 != r2 br recvr, lp != lp st [r1] = r2 st [lp] = tmp st [r1] = load old value at destination address \n st [lp + 8] = r1 store old value and address to log  addlp =lp +16 advance log pointer  add lp = lp \n+ 16 st [r1] = r2  Figure 12. Overhead of three software recovery techniques relative to the DMR baseline. \ninstruction-level dual-modular redundancy (DMR). The DMR uses detection at load, store, and control .ow \nboundaries, as previously proposed by Reis et al. [29] and Oh et al. [26]. We assume regis\u00adters and memory \nare protected by ECC; thus, errors arise through instruction execution alone. Figure 11 illustrates the \nbehavior of each recovery alternative. The .rst recovery technique, INSTRUCTION-TMR, implements TMR at \nthe instruction level. Our implementation attempts to repli\u00adcate the work of Chang et al. [6], which \nadds a third copy of each non-memory instruction and use majority voting before load and store instructions \nto detect and correct failures. We support the ma\u00adjority voting as a single-cycle operation. The second \ntechnique, CHECKPOINT-AND-LOG, is an imple\u00admentation of software logging similar to logging in software \ntrans\u00adactional memory systems [17]. Before every store instruction, the value to be overwritten is loaded \nand written to a log along with the store address, and the pointer into the log (assigned a dedi\u00adcated \nregister, lp) is incremented. In our implementation, as the log .lls, the log is reset and a register \ncheckpoint is taken, which starts a new checkpointing interval. We assume a 16KB log size (1K stores \nper checkpoint interval) with intelligent periodic polling for log over.ow using a technique similar \nto that proposed by Li and Fuchs [22]. In our simulations, all log traf.c writes through the L1 cache, \nand we optimistically assume that both the register checkpointing and periodic polling contribute no \nruntime overhead. The .nal technique, IDEMPOTENCE, is our idempotence-based recovery technique. Here, \nas each idempotent region boundary is encountered, its address is written to the register rp. In the \nevent that a fault is detected, execution jumps to the address contained in rp (the use of a register \nto hold the restart address is necessary to handle potentially overlapping control .ow between regions). \nResults. Figure 12 presents results comparing the overhead of the three techniques relative to performance \nof the underlying DMR. Across all benchmarks, INSTRUCTION-TMR performs worst with 30.5% geometric mean \nperformance overhead, CHECKPOINT-AND-LOG has 24.0% overhead, and IDEMPOTENCE performs best with only \n8.2% overhead. Compared to INSTRUCTION-TMR, CHECKPOINT-AND-LOG performs worse for applications with frequent \nmemory interac\u00adtions, such as several SPEC INT applications, but better for all other applications where \nits per-instruction overheads are lower. Overall, IDEMPOTENCE outperforms both techniques by a signi.cant \nmar\u00adgin. It avoids the redundant operations added by INSTRUCTION-TMR to correct values in-place, and \navoids the overheads asso\u00adciated with unnecessary logging in CHECKPOINT-AND-LOG. In particular, when \nlogging only the .rst memory value written to a particular memory location is required to be logged. \nHowever, under CHECKPOINT-AND-LOG the occurrence of the .rst write is not statically known and cannot \nbe ef.ciently computed at runtime. IDEMPOTENCE also preserves local stack memory more ef.ciently with \nits fully-integrated compile-time approach. 7. Related Work The application of idempotence in compiler-based \nrecovery has been previously explored, primarily in the context of exceptions and also hardware fault \nrecovery. For exception recovery, Hamp\u00adton and Asanovi\u00b4 c explore the use of idempotent regions for ex\u00adception \nrecovery in vector processors [16], De Kruijf and Sankar\u00adalingam use them for exception recovery in general \npurpose pro\u00adcessors [10], and Mahlke et al. propose restartable (idempotent) instruction sequences under \nsentinel scheduling for exception re\u00adcovery in VLIW processors [23]. For fault recovery, Feng et al. \nand De Kruijf et al. both explore mechanisms to opportunistically employ idempotence over code regions \nthat together cover large parts but not all parts of a program. While De Kruijf et al. man\u00adually identify \nidempotent regions, Feng et al. identify them using a compiler interval analysis. We build upon this \nprior work and make several additional contributions: we develop a compiler analysis to uncover the minimal \nset of semantically idempotent regions across entire programs, we describe the algorithmic challenges \nin com\u00adpiling for these regions, and we explore how they can be used to recover across a range of different \ntypes of execution failures.  In other related work, Shivers et al. [31] and Bershad [2] both explore \nusing idempotence to achieve atomicity on uniprocessors. The work of Li et al. on compiler-based multiple \ninstruction retry is also similar in that they breaks antidependences to create recover\u00adable code regions \n[21]. However, they do so over a sliding window of the last N instructions rather than over static program \nregions. As such, they do not distinguish between clobber antidependences and other antidependences; \nall antidependences must be considered clobber antidependences over a sliding window since any .ow de\u00adpendence \npreceding an antidependence will eventually lie outside the window. Our use of static program regions \nallows for the con\u00adstruction of large recoverable regions with low overheads. More general work on compiler-based \nrecovery includes the work of Chang et al. on recovery of hardware transient faults at the granularity \nof single instructions using TMR on top of DMR [6]. Chang et al. also explore two partial recovery techniques \nin addition to TMR. However, for full recovery functionality, the overheads remain effectively the same \nas TMR, and our results show that idempotence-based recovery has potentially better per\u00adformance than \nTMR. Finally, compilers have been proposed for coarse-grained, checkpoint-based recovery as well. Li \nand Fuchs study techniques for dynamic checkpoint insertion using a com\u00adpiler [22]. To maintain the desired \ncheckpoint interval, they peri\u00adodically poll a clock to decide if a checkpoint should be taken. 8. Conclusion \nThe capability for fast and ef.cient recovery has applications in many domains, including microprocessor \nspeculation, compiler speculation, and hardware reliability. Unfortunately, most prior software-based \nsolutions typically have high performance over\u00adheads, particularly for .ne-grained recovery, while hardware-based \nsolutions involve substantial power and complexity overheads. In this paper, we identi.ed idempotence \nas a basic program property that can be used for general and ef.cient recovery in soft\u00adware. While it \nis intuitive that idempotence can be used to recover from execution failures with no visible side-effects \n(e.g. hardware exceptions), we showed how failures resulting in only incorrect control .ow (e.g. branch \nmisprediction) can also be supported in a straightforward manner. Additionally, we described how failures \ninvolving a wider range of side-effects (e.g. soft errors) can be re\u00adcovered with a relatively small \nset of supporting mechanisms. We demonstrated the potential for idempotence-based recovery by building \na compiler that partitions programs into idempotent re\u00adgions, enabling the paradigm of idempotent processing \nexecution in sequences of idempotent regions. We presented a static analy\u00adsis that partitions applications \ninto semantically idempotent regions and showed a compiler that generates code preserving the idempo\u00adtence \nof these regions with low performance overhead (commonly less than 10%). As an example, we showed how \nour analysis and compiler can be used to recover from hardware transient faults ef\u00ad.ciently, purely in \nsoftware. However, our technique is general and applies to many other uses of idempotence as well. While \nwe demonstrated idempotence as a powerful primitive for program recovery, several questions remain for \nfuture research. First, while we showed that idempotent regions can be large, lim\u00adited program knowledge \nsometimes inhibits region sizes unneces\u00adsarily. Better programmer aliasing information and/or the use \nof more declarative programming styles may allow the construction of much larger idempotent regions. \nSecond, while we constructed these large regions in part as a .rst-order approximation towards minimizing \nperformance overheads, in practice optimal region size depends on a variety of factors. An important \ntopic of future work is to characterize the performance overheads based on such fac\u00adtors. Finally, future \nwork exploring the applicability of idempo\u00adtence with respect to speci.c failure scenarios will further \nhelp in understanding its full potential. Regardless of the outcomes to these questions, however, idempotent \nprocessing and the concept of semantic idempotence are likely to remain as valuable building blocks for \nuse in future research on low-overhead software recov\u00adery solutions. Acknowledgements We thank the anonymous \nreviewers for comments and the Wiscon\u00adsin Condor project and UW CSL for their assistance. Support for \nthis research was provided by NSF under the following grant: CCF\u00ad0845751. Any opinions, .ndings, and \nconclusions or recommenda\u00adtions expressed in this material are those of the authors and do not necessarily \nre.ect the views of NSF or other institutions. References [1] A. V. Aho, M. S. Lam, R. Sethi, and J. \nD. Ullman. Compilers: Principles, Techniques, and Tools. Addison Wesley, 2nd edition, 2007. [2] B. N. \nBershad, D. D. Redell, and J. R. Ellis. Fast mutual exclusion for uniprocessors. In ASPLOS 92. [3] C. \nBienia, S. Kumar, J. P. Singh, and K. Li. The PARSEC benchmark suite: Characterization and architectural \nimplications. In PACT 08. [4] N. Binkert, B. Beckmann, G. Black, S. K. Reinhardt, A. Saidi, A. Basu, \nJ. Hestness, D. R. Hower, T. Krishna, S. Sardashti, R. Sen, K. Sewell, M. Shoaib, N. Vaish, M. D. Hill, \nand D. A. Wood. The gem5 simulator. SIGARCH Comput. Archit. News, 39:1 7, Aug. 2011. [5] E. Borin, C. \nWang, Y. Wu, and G. Araujo. Software-based transparent and comprehensive control-.ow error detection. \nIn CGO 06. [6] J. Chang, G. A. Reis, and D. I. August. Automatic instruction-level software-only recovery. \nIn DSN 06. [7] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein. Introduction to Algorithms. \nThe MIT Press, 2nd edition, 2001. [8] R. Cytron, J. Ferrante, B. K. Rosen, M. N. Wegman, and F. K. Zadeck. \nAn ef.cient method of computing static single assignment form. POPL 89. [9] M. de Kruijf, S. Nomura, \nand K. Sankaralingam. Relax: An archi\u00adtectural framework for software recovery of hardware faults. In \nISCA 10, 2010. [10] M. de Kruijf and K. Sankaralingam. Idempotent processor architec\u00adture. In MICRO 11. \n[11] J. C. Dehnert, B. K. Grant, J. P. Banning, R. Johnson, T. Kistler, A. Klaiber, and J. Mattson. The \nTransmeta code morphing software: Using speculation, recovery, and adaptive retranslation to address \nreal\u00adlife challenges. In CGO 03. [12] S. Feng, S. Gupta, A. Ansari, S. Mahlke, and D. August. Encore: \nLow-cost, .ne-grained transient fault recovery. In MICRO 11. [13] D. M. Gallagher, W. Y. Chen, S. A. \nMahlke, J. C. Gyllenhaal, and W.-m. W. Hwu. Dynamic memory disambiguation using the memory con.ict buffer. \nIn ASPLOS 94. [14] M. Gschwind and E. R. Altman. Precise exception semantics in dynamic compilation. \nIn CC 02. [15] J. Guo, F. H\u00c3ijffner, E. Kenar, R. Niedermeier, and J. Uhlmann. Complexity and exact algorithms \nfor vertex multicut in interval and bounded treewidth graphs. European Journal of Operational Re\u00adsearch, \n186(2):542 553, 2008. [16] M. Hampton and K. Asanovi\u00b4c. Implementing virtual memory in a vector processor \nwith software restart markers. In ICS 06. [17] T. Harris, J. R. Larus, and R. Rajwar. Transactional Memory. \nMorgan &#38; Claypool, 2nd edition, 2010.  [18] Intel. Itanium Architecture Software Developer s Manual \nRev. 2.3. http://www.intel.com/design/itanium/manuals/iiasdmanual.htm. [19] S. W. Kim, C.-L. Ooi, R. \nEigenmann, B. Falsa., and T. N. Vijayku\u00admar. Exploiting reference idempotency to reduce speculative storage \nover.ow. ACM Trans. Program. Lang. Syst., 28:942 965, September 2006. [20] C. Lattner and V. Adve. LLVM: \nA compilation framework for lifelong program analysis &#38; transformation. In CGO 04. [21] C.-C. J. \nLi, S.-K. Chen, W. K. Fuchs, and W.-M. W. Hwu. Compiler\u00adbased multiple instruction retry. IEEE Transactions \non Computers, 44(1):35 46, 1995. [22] C.-C. J. Li and W. K. Fuchs. CATCH Compiler-assisted techniques \nfor checkpointing. In FTCS 90. [23] S. A. Mahlke, W. Y. Chen, W.-m. W. Hwu, B. R. Rau, and M. S. Schlansker. \nSentinel scheduling for VLIW and superscalar processors. In ASPLOS 92. [24] A. Meixner, M. E. Bauer, \nand D. J. Sorin. Argus: Low-cost compre\u00adhensive error detection in simple cores. IEEE Micro, 28(1):52 \n59, 2008. [25] J. Menon, M. de Kruijf, and K. Sankaralingam. iGPU: exception support and speculative \nexecution on GPUs. In ISCA 12, 2012. [26] N. Oh, P. Shirvani, and E. McCluskey. Error detection by duplicated \ninstructions in super-scalar processors. Reliability, IEEE Transactions on, 51(1):63 75, March 2002. \n[27] J. S. Plank, Y. Chen, K. Li, M. Beck, and G. Kingsley. Memory exclu\u00adsion: Optimizing the performance \nof checkpointing systems. Software Practice &#38; Experience, 29(2):125 142, 1999. [28] R. Rajwar and \nJ. R. Goodman. Speculative lock elision: enabling highly concurrent multithreaded execution. In MICRO \n01. [29] G. Reis, J. Chang, N. Vachharajani, R. Rangan, and D. August. Swift: software implemented fault \ntolerance. In CGO 05. [30] G. A. Reis, J. Chang, N. Vachharajani, R. Rangan, D. I. August, and S. S. \nMukherjee. Design and evaluation of hybrid fault-detection systems. In ISCA 05, pages 148 159. [31] O. \nShivers, J. W. Clark, and R. McGrath. Atomic heap transactions and .ne-grain interrupts. In ICFP 99. \n[32] T. J. Slegel et al. IBM s S/390 G5 microprocessor design. IEEE Micro, 19(2):12 23, 1999. [33] J. \nE. Smith and A. R. Pleszkun. Implementing precise interrupts in pipelined processors. IEEE Transactions \non Computers, 37:562 573, May 1988. [34] D. J. Sorin. Fault Tolerant Computer Architecture. Morgan &#38; \nClay\u00adpool, 2009. [35] Standard Performance Evaluation Corporation. SPEC CPU2006, 2006. [36] H.-W. Tseng \nand D. Tullsen. Data-triggered threads: Eliminating redundant computation. In HPCA 11. [37] K. C. Yeager. \nThe MIPS R10000 superscalar microprocessor. IEEE Micro, 16(2):28 40, 1996. [38] T.-Y. Yeh and Y. N. Patt. \nTwo-level adaptive training branch predic\u00adtion. In MICRO 91.    \n\t\t\t", "proc_id": "2254064", "abstract": "<p>Recovery functionality has many applications in computing systems, from speculation recovery in modern microprocessors to fault recovery in high-reliability systems. Modern systems commonly recover using checkpoints. However, checkpoints introduce overheads, add complexity, and often save more state than necessary.</p> <p>This paper develops a novel compiler technique to recover program state without the overheads of explicit checkpoints. The technique breaks programs into <i>idempotent regions</i>---regions that can be freely re-executed---which allows recovery without checkpointed state. Leveraging the property of idempotence, recovery can be obtained by simple re-execution. We develop static analysis techniques to construct these regions and demonstrate low overheads and large region sizes for an LLVM-based implementation. Across a set of diverse benchmark suites, we construct idempotent regions close in size to those that could be obtained with perfect runtime information. Although the resulting code runs more slowly, typical performance overheads are in the range of just 2-12%.</p> <p>The paradigm of executing entire programs as a series of idempotent regions we call <i>idempotent processing</i>, and it has many applications in computer systems. As a concrete example, we demonstrate it applied to the problem of compiler-automated hardware fault recovery. In comparison to two other state-of-the-art techniques, redundant execution and checkpoint-logging, our idempotent processing technique outperforms both by over 15%.</p>", "authors": [{"name": "Marc A. de Kruijf", "author_profile_id": "81464671973", "affiliation": "University of Wisconsin - Madison, Madison, WI, USA", "person_id": "P3471303", "email_address": "dekruijf@cs.wisc.edu", "orcid_id": ""}, {"name": "Karthikeyan Sankaralingam", "author_profile_id": "81100272510", "affiliation": "University of Wisconsin - Madison, Madison, WI, USA", "person_id": "P3471304", "email_address": "karu@cs.wisc.edu", "orcid_id": ""}, {"name": "Somesh Jha", "author_profile_id": "81100352621", "affiliation": "University of Wisconsin - Madison, Madison, WI, USA", "person_id": "P3471305", "email_address": "jha@cs.wisc.edu", "orcid_id": ""}], "doi_number": "10.1145/2254064.2254120", "year": "2012", "article_id": "2254120", "conference": "PLDI", "title": "Static analysis and compiler design for idempotent processing", "url": "http://dl.acm.org/citation.cfm?id=2254120"}