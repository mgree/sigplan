{"article_publication_date": "06-11-2012", "fulltext": "\n Scalable and Precise Dynamic Datarace Detection for Structured Parallelism Raghavan Raman Jisheng Zhao \nVivek Sarkar Rice University Rice University Rice University raghav@rice.edu jisheng.zhao@rice.edu vsarkar@rice.edu \n Martin Vechev Eran Yahav * ETH Z\u00a8urich Technion martin.vechev@inf.ethz.ch yahave@cs.technion.ac.il \nAbstract Existing dynamic race detectors suffer from at least one of the following three limitations: \n(i) space overhead per memory location grows linearly with the number of parallel threads [13], severely \nlimiting the parallelism that the algorithm can handle. (ii) sequentialization: the parallel program \nmust be processed in a sequential order, usually depth-.rst [12, 24]. This prevents the analysis from \nscaling with available hardware parallelism, inher\u00adently limiting its performance.  (iii) inef.ciency: \neven though race detectors with good theoret\u00adical complexity exist, they do not admit ef.cient implementations \nand are unsuitable for practical use [4, 18]. We present a new precise dynamic race detector that leverages \nstructured parallelism in order to address these limitations. Our algorithm requires constant space per \nmemory location, works in parallel, and is ef.cient in practice. We implemented and evaluated our algorithm \non a set of 15 benchmarks. Our experimental results indicate an average (geometric mean) slowdown of \n2.78\u00d7 on a 16\u00adcore SMP system. Categories and Subject Descriptors D.2.4 [Software Engineer\u00ading]: Software/Program \nVeri.cation reliability, validation; D.2.5 [Software Engineering]: Testing and Debugging monitors, testing \ntools; D.3.4 [Programming Languages]: Processors debuggers; F.3.2 [Logics and Meanings of Programs]: \nSemantics of Program\u00adming Languages program analysis General Terms Algorithms, Languages, Veri.cation \nKeywords Parallelism, Program Analysis, Data Races * Deloro Fellow Permission to make digital or hard \ncopies of all or part of this work for personal or classroom use is granted without fee provided that \ncopies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. PLDI 12, June 11 16, 2012, Beijing, China. \nCopyright c &#38;#169; 2012 ACM 978-1-4503-1205-9/12/06. . . $10.00 1. Introduction Data races are a \nmajor source of errors in parallel programs. Com\u00adplicating matters, data races may occur only in few \nof the possible schedules of a parallel program, thereby making them extremely hard to detect and reproduce. \nThe importance of detecting races has motivated signi.cant work in the area. We brie.y summarize existing \nrace detectors and the main contributions of our approach below. Existing Race Detectors FastTrack is \na state-of-the art parallel race detection algorithm which handles classic unstructured fork\u00adjoin programs \nwith locks [13]. While versatile, a key drawback of FastTrack is its worst-case space overhead of O(n) \nper instru\u00admented memory location, where n is the number of threads in the program. This space overhead \nimplies that the algorithm can typi\u00adcally only be used with a small number of parallel threads. Increas\u00ading \nthe number of threads can quickly cause space overheads and slowdowns that render the algorithm impractical. \nFastTrack applies some optimizations to reduce the overhead, but even for locations that are read shared, \nthe algorithm maintains O(n) space. Unfor\u00adtunately, in domains where structured parallelism dominates, \npro\u00adgrams typically use a massive number of lightweight tasks (e.g. consider a parallel-for loop on a \nGPU) and often the parallel tasks share read-only data. There have been various proposals for race detectors \ntargeting structured parallel languages, notably SP-bags [12] and All-Sets [8] for Cilk and its extension \nESP-bags [24] for subsets of X10 [7] and Habanero-Java (HJ) [6]. The SP-bags, All-Sets, and ESP-bags \nalgo\u00adrithms only need O(1) space per instrumented memory location but are limited in that they must always \nprocess the parallel program in a depth-.rst sequential manner. This means that the algorithms cannot \nutilize and scale with the available hardware parallelism. The SP-hybrid algorithm for Cilk [4] is an \nattempt to address the sequentialization limitation of the SP-bags algorithm. However, de\u00adspite its good \ntheoretical bounds, the SP-hybrid algorithm is very complex and incurs signi.cant inef.ciencies in practice. \nThe orig\u00adinal paper on SP-hybrid [4] provides no evaluation and subsequent evaluation of an incomplete \nimplementation of SP-hybrid [18] was done only for a small number of processors; a complete empirical \nstudy for SP-hybrid has never been done. However, the inef.ciency is clear from the fact that the CilkScreen \nrace detector used in Intel Cilk++ [1] has chosen to use the sequential All-Sets algorithm over the parallel \nbut inef.cient SP-hybrid. Further, the SP-hybrid algo\u00adrithm depends on a particular scheduling technique \n(i.e. a work\u00adstealing scheduler).  Collectively, these three limitations raise the following ques\u00adtion: \nIs there a precise dynamic race detector that works in par\u00adallel, uses O(1) space per memory location, \nand is suitable for practical use? In this paper we introduce such dynamic race de\u00adtector targeting structured \nparallel languages such as Cilk [5], OpenMP 3.0 [23], X10 [7], and Habanero Java (HJ) [6]. Our al\u00adgorithm \nruns in parallel, uses O(1) space per memory location, and performs well in practice. Structured Parallelism \nStructured parallel programming simpli\u00ad.es the task of writing correct and ef.cient parallel programs \nin two ways. First, a wide range of parallel programs can be succinctly expressed with a few well-chosen \nand powerful structured paral\u00adlel constructs. Second, the structure of the parallel program can be exploited \nto provide better performance, for instance, via bet\u00adter scheduling algorithms. Third, structured parallelism \noften pro\u00advides guarantees of deadlock-freedom. Examples of languages and frameworks with structured \nparallelism include Cilk [5], X10 [7], and Habanero Java (HJ) [3]. Our Approach A key idea is to leverage \nthe structured paral\u00adlelism to ef.ciently determine whether con.icting memory ac\u00adcesses can execute in \nparallel. Towards that end, we present a new data structure called the Dynamic Program Structure Tree \n(DPST). With our algorithm, the time overhead for every monitoring op\u00aderation is independent of the number \nof tasks and worker threads executing the program. Similarly to FastTrack, SP-bags and ESP\u00adbags, our \nalgorithm is sound and precise for a given input: if the algorithm does not report a race for a given \nexecution, it means that no execution with the same input can trigger a race (i.e. there are no false \nnegatives). Conversely, if a race is reported, then the race really exists (i.e. there are no false positives). \nThese properties are particularly attractive when testing parallel programs as it implies that for a \ngiven input, we can study an arbitrary program schedule to reason about races that may occur in other \nschedules. As we will demonstrate later, our algorithm is ef.cient in practice and signi.\u00adcantly outperforms \nexisting algorithms. Main Contributions The main contributions of this paper are: A dynamic data race \ndetection algorithm for structured paral\u00adlelism with the following properties:  works in parallel. \n  uses only constant space per monitored memory location.  is sound and precise for a given input. \n  A data structure called the Dynamic Program Structure Tree (DPST) that keeps track of relationships \nbetween tasks and can be accessed and modi.ed concurrently.  An ef.cient implementation of the algorithm \ntogether with a set of static optimizations used to reduce the overhead of the implementation.  An evaluation \non a suite of 15 benchmarks indicating an aver\u00adage (geometric mean) slowdown of 2.78\u00d7 on a 16-core SMP \nsystem.  The rest of the paper is organized as follows: Section 2 discusses the structured parallel \nsetting, Section 3 presents the dynamic pro\u00adgram structure tree (DPST), Section 4 introduces our new \nrace de\u00adtection algorithm, Section 5 presents the details of the implementa\u00adtion of our algorithm and \nthe optimizations that we used to reduce the overhead, Section 6 discusses our experimental results, \nSec\u00adtion 7 discusses related work and Section 8 concludes the paper.  2. Background In this section, \nwe give a brief overview of the structured par\u00adallel model targeted by this paper. We focus on the async/.nish \nstructured parallelism constructs used in X10 [7] and Habanero Java (HJ) [6]. The async/.nish constructs \ngeneralize the traditional spawn/sync constructs used in the Cilk programming system [5] since they can \nexpress a broader set of computation graphs than those expressible with the spawn/sync constructs used \nin Cilk [15]. While X10 and HJ include other synchronization techniques such as futures, clocks/phasers, \nand Cilk even includes locks, the core task creation and termination primitives in these languages are \nfundamentally based on the async/.nish and spawn/sync con\u00adstructs. The underlying complexity of a dynamic \nanalysis algorithm is determined by these core constructs. Once a dynamic analysis al\u00adgorithm for the \ncore constructs is developed, subsequent extensions can be built on top of the core algorithm. To underscore \nthe impor\u00adtance of studying the core portions of these languages, a calculus called Featherweight X10 \n(FX10) was proposed [20]. Also, the SP\u00adbags algorithm [12] for Cilk was presented for the core spawn/sync \nconstructs (the algorithm was later extended to handle accumula\u00adtors and locks [8]). The algorithm presented \nin this paper is applicable to async/.n\u00adish constructs (which means it also handles spawn/sync constructs). \nThe algorithm is independent of the sequential portions of the lan\u00adguage, meaning that one can apply \nit to any language where the parallelism is expressed using the async/.nish constructs. For ex\u00adample, \nthe sequential portion of the language can be based on the sequential portions of Java as in HJ or C/C++ \nas in Cilk [15], Cilk++ [1], OpenMP 3.0 [23], and Habanero-C [9]. Next, we in\u00adformally describe the semantics \nof the core async/.nish constructs. A formal operational semantics can be found in [20]. Informal Semantics \nThe statement async { s } causes the parent task to create a new child task to execute s asynchronously \n(i.e., before, after, or in parallel) with the remainder of the parent task. The statement finish { s \n} causes the parent task to ex\u00adecute s and then wait until all async tasks created within s have completed, \nincluding the transitively spawned tasks. Each dynamic instance TA of an async task has a unique Immediately \nEnclosing Finish (IEF) instance F of a .nish statement during program execu\u00adtion, where F is the innermost \ndynamic finish scope containing TA. There is an implicit finish scope surrounding the body of main() \nso program execution will only end after all async tasks have completed. The finish statement is a restricted \njoin: while in the general unstructured fork-join case, a task can join with any other task, with the \nfinish statement, a task can only join on tasks that are created in the enclosed statement. This is a \nfundamental difference between arbitrary unstructured fork-join and the async/.nish (or spawn/sync) constructs. \nIt is such restrictions on the join that make it possible to prove the absence of deadlocks for any program \nin the language [20], and provide an opportunity for discovering analysis algorithms that are more ef.cient \nthan those for the general unstructured fork-join case. As mentioned earlier, async/.nish constructs \ncan express a broader set of computation graphs than Cilk s spawn/sync con\u00adstructs. The key relaxation \nin async/.nish over spawn/sync is the way a task is allowed to join with other tasks as well as dropping \nthe requirement that a parent task must wait for all of its child tasks to terminate. With spawn/sync, \nat any given sync point in a task ex\u00adecution, the task must join with all of its descendant tasks (and \nall recursive descendant tasks, by transitivity) created in between the start of the task and the join \npoint. In contrast, with async/.nish it is possible for a task to join with some rather than all of its \ndescen\u00addant tasks: at the end of a .nish block, the task only waits until the        Figure 1. \nAn example async/.nish program and its .nal DPST. descendant tasks created inside the .nish scope have \ncompleted. More details comparing spawn/sync and async/.nish can be found in [16]. Example Consider the \nexample in Figure 1. For now, ignore the tree on the right and the step annotations, both of which are \ndis\u00adcussed in the next section. Initially, the main task begins execution with the main .nish statement, \nlabeled F1. It executes the .rst two statements S1 and S2 and then forks a new task A1 using the async \nstatement. In turn, A1 executes statements S3, S4, S5 and forks task A2 which executes statement S6. \nNote that statement S6 (in task A2) and statements S7 and S8 in task A1 can execute in parallel. After \nforking A1, the main task can proceed to execute statements S9, S10 and S11 that follow A1. The main \ntask then forks task A3 which executes statements S12 and S13. Note that the statement S11 (in the main \ntask) and statements S12, S13 (in task A3) cannot execute in parallel because the task A3 will be forked \nonly after the completion of S11. After forking A3, the main task has to wait un\u00adtil A1, A2, and A3 have \nterminated. Only after all these descendant tasks complete, the main task can exit past the end of .nish \nF1.  3. Dynamic Program Structure Tree Any dynamic data race detection algorithm needs to provide mech\u00adanisms \nthat answer two questions: for any pair of memory accesses (with at least one write): (i) determine whether \nthe accesses can ex\u00adecute in parallel, and (ii) determine whether they access the same location. In this \nsection, we introduce the Dynamic Program Struc\u00adture Tree (DPST), a data structure which can be used \nto answer the .rst question. The DPST is an ordered rooted tree that is built at runtime to capture parent-child \nrelationships among async, .nish, and step (de.ned below) instances of a program. The internal nodes \nof a DPST represent async and .nish instances. The leaf nodes of a DPST represent the steps of the program. \nThe DPST can also be used to support dynamic analysis of structured parallel programs written in languages \nsuch as Cilk and OpenMP 3.0. We assume standard operational semantics of async/.nish con\u00adstructs as de.ned \nin FX10 [20]. The semantics of statements and expressions other than async/.nish is standard [30]. That \nis, each transition represents either a basic statement, an expression eval\u00aduation or the execution of \nan async or a .nish statement. For our purposes, given a trace, we assume that the execution of each \nstate\u00adment is uniquely identi.ed (if a statement executes multiple times, each dynamic instance is uniquely \nidenti.ed). We refer to an exe\u00adcution of a statement as a dynamic statement instance. We say that a statement \ninstance is an async instance if the statement performs an async operation. Similarly for .nish instances. \nDe.nition 1 (Step). A step is a maximal sequence of statement instances such that no statement instance \nin the sequence includes an async or a .nish operation. De.nition 2 (DPST). The Dynamic Program Structure \nTree (DPST) for a given execution is a tree in which all leaves are steps, and all interior nodes are \nasync and .nish instances. The parent relation is de.ned as follows: Async instance A is the parent \nof all async, .nish, and step instances directly executed within A.  Finish instance F is the parent \nof all async, .nish, and step instances directly executed within F .  There is a left-to-right ordering \nof all DPST siblings that re.ects the left-to-right sequencing of computations belonging to their com\u00admon \nparent task. Further, the tree has a single root that corresponds to the implicit top-level .nish construct \nin the main program. 3.1 Building a DPST Next we discuss how to build the DPST during program execution. \nWhen the main task begins, the DPST will contain a root .nish node F and a step node S that is the child \nof F . F corresponds to the implicit .nish enclosing the body of the main function in the program and \nS represents the starting computation in the main task. Task creation When a task T performs an async \noperation and creates a new task Tchild: 1. An async node Achild is created for task Tchild. If the imme\u00addiately \nenclosing .nish (IEF) F of Tchild exists within task T , then Achild is added as the rightmost child \nof F .Otherwise, Achild is added as the rightmost child node of (the async) node corresponding to task \nT . 2. A step node representing the starting computations in task Tchild is added as the child of Achild. \n 3. A step node representing the computations that follow task Tchild in task T is added as the right \nsibling of Achild.  Note that there is no explicit node in a DPST for the main task because everything \ndone by the main task will be within the implicit .nish in the main function of the program and hence \nall of the corresponding nodes in a DPST will be under the root .nish node. Start Finish When a task \nT starts a .nish instance F : 1. A .nish node Fn is created for F . If the immediately enclosing .nish \nF ' of F exists within task T (with corresponding .nish node F'in the DPST), then Fn is added as the \nrightmost child n of F'.Otherwise, Fn is added as the rightmost child of the n (async) node corresponding \nto task T . 2. A step node representing the starting computations in F is added as the child of Fn. End \nFinish When a task T ends a .nish instance F , a step node representing the computations that follow \nF in task T is added as the right sibling of the node that represents F in the DPST. Note that the DPST \noperations described thus far only take O(1) time. Thus, the DPST for a given program run grows mono\u00adtonically \nas program execution progresses and new async, .nish, and step instances are added to the DPST. Note \nthat since all data accesses occur in steps, it follows that all tests for whether two ac\u00adcesses may \nhappen in parallel will only take place between two leaves in a DPST.  Figure 2. A part of a DPST. \nLCA is the Lowest Common Ancestor of steps S1 and S2. A is the DPST ancestor of S1 which is the child \nof LCA. S1 and S2 can execute in parallel if and only if A is an async node. Example We can now return \nto the example program in Figure 1 and study its steps and .nal DPST. Note the way statement in\u00adstances \nare grouped into steps. When the main task starts executing .nish F1, a node corresponding to F1 is added \nas the root node of the DPST, and a step node step1 is added as the child of F1; step1 represents the \nstarting computations in F1, i.e., instances of state\u00adments S1 and S2. When the main task forks the task \nA1, an async node corresponding to A1 is added as the right-most child of F1 (since the immediately enclosing \n.nish of A1 is F1 and it is within the main task), a step node step2 is added as the child of A1, and \na step node step5 is added as the right sibling of A1. step2 represents the starting computations in \nA1 (i.e., instance of statements S3, S4, and S5) and step5 represents the computation that follows A1 \nin the main task (i.e., instances of statements S9, S10, and S11). After this point, the main task and \nthe task A1 can execute in parallel. Eventually, the DPST grows to the form shown in the .gure.  3.2 \nProperties of a DPST In this section, we brie.y summarize some key properties of a DPST. The proofs of \nthese properties have been omitted due to space limitations, but can be found in [25]. For a given input \nthat leads to a data-race-free execution of a given async-.nish parallel program, all executions of that \nprogram with the same input will result in the same DPST.  Let F be the DPST root (.nish) node. Each \nnon-root node n0 is uniquely identi.ed by a .nite path from n0 to F :  r0r1r2rk-1 n0 -. n1 -. n2 -. \n... ---. nk th where k = 1, nk = F , and for each 0 = i<k, ni is the ri child of node ni+1. The path \nfrom n0 to F stays invariant as the tree grows. For a given statement instance, its path to the root \nis unique regardless of which execution is explored (as long as the executions start with the same state). \nThis property holds up to the point that a data race (if any) is detected. The DPST is amenable to ef.cient \nimplementations in which nodes can be added to the DPST in parallel without any syn\u00adchronization in O(1) \ntime. One such implementation is de\u00adscribedinSection5. De.nition 3. A node A is said to be to the left \nof a node B in a DPST if A appears before B in the depth .rst traversal of the tree. As mentioned above, \neven though the DPST changes during program execution, the path from a node to the root does not change \nand the left-to-right ordering of siblings does not change. Hence, even though the depth .rst traversal \nof the DPST is not fully speci.ed during program execution, the left relation between any two nodes in \nthe current DPST is well-de.ned. De.nition 4. Two steps, S1 and S2,inaDPST G that corresponds to a program \nP with input ., may execute in parallel if and only if there exists at least one schedule d of P with \ninput . in which S1 executes in parallel with S2. The predicate DMHP(S1, S2) evaluates to true if steps \nS1 and S2 can execute in parallel in at least one schedule of a program and to false otherwise (DMHP \nstands for Dynamic May Happen in Parallel to distinguish it from the MHP relation used by static analyses). \nWe now state a key theorem that will be important in enabling our approach to data race detection. Theorem \n1. Consider two leaf nodes (steps) S1 and S2 in a DPST, where S1 = S2 and S1 is to the left of S2 as \nshown in Figure 2. Let LCA be the node denoting the least common ancestor of S1 and S2. Let node A be \nthe ancestor of S1 that is the child of LCA. Then, S1 and S2 can execute in parallel if and only if A \nis an async node. Proof. Please refer to [25]. Example Let us now look at the DMHP relation for some \npairs of steps in the example program in Figure 1. First, let us consider DMHP(step2, step5).Here step2 \nis to the left of step5,since step2 will appear before step5 in the depth .rst traversal of the DPST. \nThe lowest common ancestor of step2 and step5 is the node F1. The node A1 is the ancestor of step2 (the \nleft node) that is the child of F1. Since A1 is an async node, DMHP(step2, step5) will evaluate to true \nindicating that step2 and step5 can execute in parallel. This is indeed true for this program: step2 \nis within A1, while step5 follows A1 and is within A1 s immediately enclosing .nish. Now, let us consider \nDMHP(step6, step5).Here step5 is to the left of step6,since step5 will appear before step6 in the depth \n.rst traversal of the DPST. Their lowest common ancestor is F1, and the ancestor of step5 which is the \nchild of F1 is step5 itself. Since step5 is not an async instance, DMHP(step6, step5) evaluates to false. \nThis is consistent with the program because step6 is in task A3 and A3 is created only after step5 completes. \n 4. Race Detection Algorithm Our race detection algorithm involves executing the given program with \na given input and monitoring every dynamic memory access in the program for potential data races. The \nalgorithm maintains a DPST as described in the previous section, as well as the relevant access history \nfor each shared memory location. The algorithm performs two types of actions: Task actions: these involve \nupdating the DPST with a new node for each async, .nish, and step instance.  Memory actions: on every \nshared memory access, the algorithm checks if the access con.icts with the access history for the relevant \nmemory location. If a con.ict is detected, the algorithm reports a race and halts. Otherwise, the memory \nlocation is updated to include the memory access in its access history.  A key novelty of our algorithm \nis that it requires constant space to store the access history of a memory location, while still guaran\u00adteeing \nthat no data races are missed. We next describe the shadow memory mechanism that supports this constant \nspace guarantee. 4.1 Shadow Memory Our algorithm maintains a shadow memory Ms for every moni\u00adtored memory \nlocation M. Ms is designed to store the relevant parts of the access history to M. It contains the following \nthree .elds, which are all initialized to null: w : a reference to a step that wrote M.  r1 : a reference \nto a step that read M.   r2 : a reference to another step that read M. The following invariants are \nmaintained throughout the execu\u00adtion of the program until the .rst data race is detected. Ms.w refers \nto the step that last wrote M.  Ms.r1 &#38; Ms.r2 refer to the steps that last read M. All the steps \n(a1,a2, ..., ak) that have read M since the last synchronization are in the subtree rooted at LCA(Ms.r1, \nMs.r2).  The .elds of the shadow memory Ms are updated atomically by different tasks that access M. \n 4.2 Algorithm The most important aspect of our algorithm is that it stores only three .elds for every \nmonitored memory location irrespective of the number of steps that access that memory location. The intuition \nbehind this is as follows: it is only necessary to store the last write to a memory location because \nall the writes before the last one must have completed at the end of the last synchronization. This is \nassuming no data races have been observed yet during the execution. Note that though synchronization \ndue to .nish may not be global, two writes to a memory location have to be ordered by some synchronization \nto avoid constituting a data race. Among the reads to a memory location, (a1,a2, ..., ak), since the \nlast synchronization, it is only necessary to store two reads, ai, aj, such that the subtree under LCA(ai, \naj) includes all the reads (a1,a2, ..., ak). This is because every future read, an, which is in parallel \nwith any discarded step will also be in parallel with at least one of ai or aj. Thus, the algorithm will \nnot miss any data race by discarding these steps. De.nition 5. In a DPST, a node n1 is dpst-greater than \na node n2, denoted by n1 >dpst n2,if n1 is an ancestor of n2 in the DPST. Note that, in this case, n1 \nis higher in the DPST (closer to the root) than n2. Algorithm 1: Write Check Input: Memory location M,Step \nS that writes to M 1 if DMHP(Ms.r1, S) then 2 Report a read-write race between Ms.r1 and S 3 end 4 if \nDMHP(Ms.r2, S) then 5 Report a read-write race between Ms.r2 and S 6 end 7 if DMHP(Ms.w, S) then 8 \nReport a write-write race between Ms.w and S 9 else 10 Ms.w . S 11 end   Algorithms 1 and 2 show the \nchecking that needs to be per\u00adformed on write and read accesses to monitored memory locations. When a \nstep S writes to a memory location M, Algorithm 1 checks if S may execute in parallel with the reader \nin Ms.r1 by computing DMHP(S, Ms.r1). If they can execute in parallel, the algorithm re\u00adports a read-write \ndata race between Ms.r1 and S. Similarly, the algorithm reports a read-write data race between Ms.r2 \nand S if these two steps can execute in parallel. Then, Algorithm 1 reports a write-write data race between \nMs.w and S, if these two steps can execute in parallel. Finally, it updates the writer .eld, Ms.w, with \nthe current step S indicating the latest write to M. Note that this happens only when the write to M \nby S does not result in data race with any previous access to M. Algorithm 2: Read Check Input: Memory \nlocation M,Step S that reads M 1 if DMHP(Ms.w, S) then 2 Report a write-read data race between Ms.w and \nS 3 end 4 if \u00acDMHP(Ms.r1, S) .\u00acDMHP(Ms.r2, S) then 5 Ms.r1 . S 6 Ms.r2 . null 7 else if DMHP(Ms.r1, S) \n. DMHP(Ms.r2, S) then 8 lca12 . LCA(Ms.r1, Ms.r2) 9 lca1s . LCA(Ms.r1, S) 10 lca2s . LCA(Ms.r2, S) 11 \nif lca1s >dpst lca12 . lca2s >dpst lca12 then 12 Ms.r1 . S 13 end  14 end When a step S reads a memory \nlocation M, Algorithm 2 reports a write-read data race between Ms.w and S if these two steps can execute \nin parallel. Then, it updates the reader .elds of Ms as fol\u00adlows: if S can never execute in parallel \nwith either of the two read\u00aders, Ms.r1 and Ms.r2, then both these readers are discarded and Ms.r1 is \nset to S.If S can execute in parallel with both the readers, Ms.r1 and Ms.r2, then the algorithm stores \ntwo of the these three steps, whose LCA is the highest in the DPST, i.e., if LCA(Ms.r1, S) or LCA(Ms.r2, \nS) is dpst-greater than LCA(Ms.r1, Ms.r2),then Ms.r1 is set to S. Note that in this case S is outside \nthe subtree under LCA(Ms.r1, Ms.r2) and hence, LCA(Ms.r1, S) will be the same as LCA(Ms.r2, S). If S \ncan execute in parallel with one of the two readers and not the other, then the algorithm does not update \nthe readers because, in that case, S is guaranteed to be within the subtree under the LCA(Ms.r1, Ms.r2). \nThe DMHP(Ms.r2, S) can be computed from DMHP(Ms.r1, S) in some cases. This can be used to further optimize \nAlgorithms 1 and 2. We do not present the details of this optimization here. Atomicity Requirements A \nmemory action for an access to a memory location M involves reading the .elds of Ms, checking the predicates, \nand possibly updating the .elds of Ms.Every such memory action has to execute atomically with respect \nto other memory actions for accesses to the same memory location. Theorem 2. If Algorithms 1 and 2 do \nnot report any data race in some execution of a program P with input ., then no execution of P with . \nwill have a data race on any memory location M. Proof. Please refer to [25]. Theorem 3. If Algorithm \n1 or 2 reports a data race on a memory location M during an execution of a program P with input .,then \nthere exists at least one execution of P with . in which this race exists. Proof. Please refer to [25]. \n Theorem 4. The race detection algorithm is sound and precise for a given input. Proof. From Theorem \n2 it follows that our race detection algorithm is sound for a given input. From Theorem 3 it follows \nthat our race detection algorithm is precise for a given input.    5. Implementation and Optimizations \nThis section describes the implementation of the different parts of our race detection algorithm. 5.1 \nDPST The DPST of the program being executed is built to maintain the parent-child relationship of asyncs, \n.nishes and steps in the program. Every node in the DPST consists of the following 4 .elds: parent: \nthe DPST node which is the parent of this node.  depth: an integer that stores the depth of this node. \nThe root node of the DPST has depth 0. Every other node in the DPST has depth one greater than its parent. \nThis .eld is immutable.  num children: number of children of this node currently in the DPST. This .eld \nis initialized to 0 and incremented when child nodes are added.  seq no: an integer that stores the \nordering of this node among the children of its parent, i.e., among its siblings. Every node s children \nare ordered from left to right. They are assigned se\u00adquence numbers starting from 1 to indicate this \norder. This .eld is also immutable.  The use of depth for nodes in the DPST leads to a lowest common \nancestor (LCA) algorithm with better complexity (than if we had not used this .eld). The use of sequence \nnumbers to maintain the ordering of a node s children makes it easier to check for may happen in parallel \ngiven two steps in the program. Note that all the .elds of a node in the DPST can be initial\u00adized/updated \nwithout any synchronization: the parent .eld initial\u00adization is trivial because there are no competing \nwrites to that .eld; the depth .eld of a node is written only on initialization, is never updated, and \nis read only after the node is created; the num children .eld is incremented whenever a child node is \nadded, but for a given node, its children are always added sequentially in order from left to right; \nthe seq no .eld is written only on initialization, is never updated, and is read only after the node \nis created. 5.2 Computing DMHP A large part of the data race detection algorithm involves checking DMHP \nfor two steps in the program. This requires computing the Lowest Common Ancestor (LCA) of two nodes in \na tree. The function LCA(G,S1,S2) returns the lowest common ancestor of the nodes S1 and S2 in the DPST \nG. This is implemented by starting from the node with the greater depth (say S1) and traversing up G \nuntil a node with the depth same as S2 is reached. From that point, G is traversed along both the paths \nuntil a common node is reached. This common node is the lowest common ancestor of S1 and S2. The time \noverhead of this algorithm is linear in the length of the longer of the two paths, S1 . L and S2 . L. \nAlgorithm 3 computes DMHP relation between two steps S1 and S2. Algorithm 3 returns true if the given \ntwo steps S1 and S2 may happen in parallel and false otherwise. This algorithm .rst computes the lowest \ncommon ancestor L of the given two steps using the LCA function. If the step S1 is to the left of S2, \nthen the algorithm returns true if the ancestor of S1 (which is the child of L) is an async and false \notherwise. If the step S2 is to the left of S1, then the algorithm returns true if the ancestor of S2 \nwhich is the child of L is an async and false otherwise. The time overhead of this algorithm is same \nas that of the LCA function, since it only takes constant time to .nd the node which is the ancestor \nof the left step that is the child of LCA node and then check if that node is an async.   Algorithm \n3: Dynamic May Happen in Parallel (DMHP) Input: DPST G,Step S1,Step S2 Output:true/false 1 Nlca =LCA(G, \nS1, S2) 2 A1 = Ancestor of S1 in G which is the child of Nlca 3 A2 = Ancestor of S2 in G which is the \nchild of Nlca 4 if A1 is to the left of A2 in G then 5 if A1 is an Async then 6 return true 7 else 8 \nreturn false // S1 happens before S2 9 end  10 else 11 if A2 is an Async then 12 return true 13 else \n14 return false // S2 happens before S1 15 end  16 end 5.3 Space and Time Overhead The size of the \nDPST will be O(n),where n is the number of tasks in the program. More precisely, the total number of \nnodes in the DPST will be 3 * (a + f) - 1,where a is the number of async instances and f is the number \nof .nish instances in the program. This is because a program with just one .nish node will have just \none step node inside the .nish of its DPST. When an async or a .nish node is subsequently added to the \nDPST, it will result in adding 2 steps nodes, one as the child of the new node and the other as its sibling. \nThe space overhead for every memory location is O(1), since we only need to store a writer step and two \nreader steps in the shadow memory of every memory location. The time overhead at task boundaries is O(1), \nwhich is the time needed to add/update a node in the DPST. The worst case time overhead on every memory \naccess is same as that of Algorithm 3. Note that the time overhead is not proportional to the number \nof processors (underlying worker threads) that the program runs on. Hence, the overhead is not expected \nto scale as we increase the number of processors on which the program executes. This is an important \nproperty as future hardware will likely have many cores.  5.4 Relaxing the Atomicity Requirement A memory \naction for an access to a memory location M involves reading the .elds of its shadow memory location \nMs, computing the necessary DMHP information and checking appropriate predi\u00adcates, and possibly updating \nthe .elds of Ms. Let us refer to these three stages as read, compute,and update of a memory action. In \nour algorithm, every memory action on a shadow memory Ms has to execute atomically relative to other \nmemory actions on Ms. When there are parallel reads to a memory location, this atomicity requirement \neffectively serializes the memory actions due to these reads. Hence this atomicity requirement induces \na bottleneck in our algorithm when the program is executed on a large number of threads. Note that the \natomicity requirement does not result in a bottleneck in the case of writes to a memory location because \nthe memory actions due to writes have no contention in data race free programs. (In a data race free \nprogram, there is a happens-before ordering between a write and every other access to a memory location.) \nWe now present our implementation strategy to overcome this atomicity requirement without sacri.cing \nthe correctness of our algorithm. This implementation strategy is based on the solution to the reader-writer \nproblem proposed by Leslie Lamport in [19].  Our implementation allows multiple memory actions on the \nsame shadow memory to proceed in parallel. This is done by adding two atomic integers to every shadow \nmemory, i.e., Ms contains the following two additional .elds: startVersion: an atomic integer that denotes \nthe version number of Ms  endVersion: an atomic integer that denotes the version number of Ms.  Both \nstartVersion and endVersion are initialized to zero. Ev\u00adery time any of the .elds Ms.w, Ms.r1,or Ms.r2 \nis updated, Ms.startVersion and Ms.endVersion are incremented by one. The following invariant is maintained \non every shadow memory Ms during the execution of our algorithm: any consistent snapshot of Ms will have \nthe same version number in both startVersion and endVersion. Now, we show how the read, compute, and \nupdate stages of a memory action on Ms are performed. Note that these rules use a CompareAndSet (CAS) \nprimitive which is atomic rela\u00adtive to every operation on the same memory location. Read: Read the version \nnumber in Ms.startVersion into a local variable, X.  Read the .elds Ms.w, Ms.r1,and Ms.r2 into local \nvari\u00adables, W , R1,and R2.  Perform a fence to ensure that all operations above are complete.  Read \nthe version number in Ms.endVersion into a local variable, Y .  If X is not the same as Y , restart \nthe read stage.  Compute: Perform the computation on the local variables, W , R1,and R2. Update: Do \nthe following steps if an update to any of the .elds Ms.w, Ms.r1,or Ms.r2 is necessary.  Perform a CAS \non the version number in Ms.endVersion looking for the value X and updating it with an increment of one. \n If the above CAS fails, restart the memory action from the beginning of read stage.  Write to the \nrequired .elds of Ms.  Write the incremented version number to Ms.startVersion.  When a memory action \non Ms completes the read stage, the above rules ensure that a consistent snapshot of Ms was captured. \nThis is because the read stage completes only when the same ver\u00adsion number is seen in both Ms.startVersion \nand Ms.endVersion. The CAS in the update stage of the memory action on Ms succeeds only when Ms.endVersion \nhas the version number that was found in the read stage earlier. The update stage completes by writing \nto the reader and writer .elds of Ms as necessary, followed by incrementing the version number in Ms.startVersion. \nWhen the update stage completes, both Ms.startVersion and Ms.endVersion will have the same version number \nand thus, the .elds of Ms are retained in a consistent state. The CAS in the update stage of a memory \naction a on Ms also ensures that the .elds of Ms are updated only if it has not already been updated \nby any memory action on Ms, since the read stage of a.If this CAS fails, then there has been some update \nto Ms since the read stage and hence, the computations are discarded and the memory action is restarted \nfrom the beginning of the read stage. Thus, the memory actions are guaranteed to be atomic relative to \nother memory actions on the same memory location. The main advantage of this implementation is that it \nallows mul\u00adtiple memory actions on the same shadow memory Ms to proceed in parallel. But if more than \none of them needs to update the .elds of Ms, then only one of them is guaranteed to succeed while the \nothers repeat the action. This is especially bene.cial when there are multiple parallel accesses to M \nwhose memory actions do not update the .elds of Ms. In our algorithm, this occurs when there are reads \nby step S such that S is in the subtree rooted at LCA(Ms.r1, Ms.r2). These cases occur frequently in \npractice thereby empha\u00adsizing the importance of relaxing the atomicity requirement. Our algorithm is \nimplemented in Java and we use the AtomicIn\u00adteger from Java Concurrency Utilities for the version numbers. \nThe CAS on Atomic Integer is guaranteed to execute atomically with re\u00adspect to other operations on the \nsame location. Also, the CAS acts as a barrier for the memory effects of the instructions on its either \nside, i.e., all the instructions above it are guaranteed to complete before it executes and no instructions \nbelow it will execute before it completes. This is the same as the memory effects of the fence that is \nused in the read stage. The read of an AtomicInteger has the memory effects of the read of a volatile \nin Java. Hence, it does not allow any instruction after it to execute until it completes. Simi\u00adlarly, \nthe write to an AtomicInteger has the memory effects of the write to a volatile in Java. Hence, it does \nnot execute until all the instructions before it complete.  5.5 Optimizations In the implementation \nof our algorithm, we also include the static optimizations that were described in [24]. These optimizations \neliminate redundant updates to the shadow memory location due to redundant reads and writes to the corresponding \nmemory location with a single step. These are static optimizations that perform data .ow analysis on \nthe input program to identify redundant shadow memory updates. The optimizations include: main-task check \nelim\u00adination, read-only check elimination, escape analysis to eliminate task-local checks, loop-invariant \ncheck optimizations, and read\u00ad/write check elimination. We note that these optimizations can be used \nto improve the performance of any race detection algorithm. We have also identi.ed a number of dynamic \noptimizations that can reduce the space and time overhead of the DMHP algorithm even further. We leave \nthose as future work.  6. Experimental Results In this section, we present experimental results for \nour algorithm, which for convenience we refer to as SPD3 (Scalable Precise Dy\u00adnamic Datarace Detection). \nThe algorithm was implemented as a Java library for detecting data races in HJ programs contain\u00ading async \nand .nish constructs [6]. Shadow locations were imple\u00admented by extending the hj.lang.Object class with \nshadow .elds, and by using array views [6, 24] as anchors for shadow arrays. Programs were instrumented \nfor race detection during a bytecode-level transformation pass implemented on HJ s Parallel Intermediate \nRepresentation (PIR) [31]. The PIR is an intermedi\u00adate representation that extends Soot s Jimple IR [29] \nwith parallel constructs such as async and .nish. The instrumentation pass adds the necessary calls to \nour race detector library at async and .nish boundaries and on reads and writes to shared memory locations. \nWe also compare SPD3 with some race detectors from past work, namely Eraser [26], FastTrack [13], and \nESP-bags [24]. For Eraser and FastTrack, we use the implementations included in the RoadRunner tool [14]. \nSince the performance of the FastTrack im\u00adplementation available in the public RoadRunner download yielded \nworse results than those described in [13], we communicated with Figure 3. Relative slowdown of SPD3 \nfor all benchmarks on 1, 2, 4, 8, and 16 threads. Relative slowdown on n threads refers to the slowdown \nof the SPD3 version on n threads compared to the HJ-Base version on n threads.  Table 1. List of Benchmarks \nEvaluated Source Benchmark Description JGF (Section 2) Series (C) LUFact (C) SOR (C) Crypt (C) Sparse \n(C) Fourier coef.cient analysis LU Factorisation Successive over-relaxation IDEA encryption Sparse Matrix \nmultiplication JGF (Section 3) MolDyn (B) MonteCarlo (B) RayTracer (B) Molecular Dynamics simulation \nMonte Carlo simulation 3D Ray Tracer Bots FFT (large) Health (large) NQueens (14) Strassen (large) Fast \nFourier Transformation Simulates a country health system N Queens problem Matrix Multiply with Strassen \ns method Shootout Fannkuch (10M) Mandelbrot (8000) Indexed-access to tiny integer-sequence Generate Mandelbrot \nset portable bitmap EC2 Matmul (1000 2) Matrix Multiplication (Iterative) the implementers and received \nan improved implementation of FastTrack which was used to obtain the results reported in this pa\u00adper. \nFor ESP-bags, we used the same implementation that was used in [24]. Our experiments were conducted on \na 16-core (quad-socket, quad-core per socket) Intel Xeon 2.4GHz system with 30 GB mem\u00adory, running Red \nHat Linux (RHEL 5), and Sun Hotspot JDK 1.6. To reduce the impact of JIT compilation, garbage collection \nand other JVM services, we report the smallest time measured in 3 runs repeated in the same JVM instance \nfor each data point. HJ tasks are scheduled on a .xed number of worker threads using a work\u00adstealing \nscheduler with an adaptive policy [17] 6.1 Evaluation of SPD3 We evaluated SPD3 on a suite of 15 task-parallel \nbenchmarks listed in Table 1. It includes eight Java Grande Forum bench\u00admarks (JGF) [28], four Barcelona \nOpenMP Task Suites bench\u00admarks (BOTS) [11], two Shootout benchmarks [2], and one EC2 challenge benchmark. \nAll benchmarks were written using only .nish and async con\u00adstructs for parallelism, with .ne grained \none-async-per-iteration parallelism for parallel loops. As discussed later, the original ver\u00adsion of \nthe JGF benchmarks contained chunked parallel loops with programmer-speci.ed decomposition into coarse \ngrained one\u00adchunk-per-thread parallelism. The .ne grained task-parallel ver\u00adsions of the JGF benchmarks \nused for the evaluation in this section were obtained by rewriting the chunked loops into unchunked parallel \nloops. In addition, barrier operations in the JGF bench\u00admarks were replaced by appropriate .nish constructs. \nHJ-Base refers to the uninstrumented baseline version of each of these benchmarks. All the JGF benchmarks \nwere con.gured to run with the largest available input size. All input sizes are shown in Table 1. No \ndata race was expected in these 15 programs, and SPD3 found only one data race which turned out to be \na benign race. This was due to repeated parallel assignments of the same value to the same location in \nthe async-.nish version of the MonteCarlo benchmark, which was corrected by removing the redundant as\u00adsignments. \nAfter that, all the benchmarks used in this section were observed to be data-race-free for the inputs \nused. Figure 3 shows the relative slowdown of SPD3 for all bench\u00admarks when executed with 1, 2, 4, 8, \nand 16 worker threads. (Recall that these benchmarks create many more async tasks than the num\u00adber of \nworker threads.) The relative slowdown on n threads refer to the slowdown of the SPD3 instrumented version \nof the benchmark executing on n threads compared with the HJ-Base version execut\u00ading on n threads. Ideally, \na scalable race detector should have a con\u00adstant relative slowdown as the number of worker threads increases. \nAs evident from Figure 3, the slowdown for many of the bench\u00admarks decrease as the number of worker threads \nincreases from 1 to 16. The geometric mean of the slowdowns for all the benchmarks on 16 threads is 2.78\u00d7. \nThough the geometric mean is below 3\u00d7, four of the 15 bench\u00admarks (Crypt, LUFact, RayTracer, and FFT) \nexhibited a slowdown around 10\u00d7 for worker threads from 1 to 16. This is because these benchmarks contain \nlarger numbers of shared locations that need to be monitored for race detection. As discussed later, \nother race detection algorithms exhibit much larger slowdowns for these ex\u00adamples than SPD3. Note that \neven in these cases the slowdowns are similar across 1 to 16 threads. This clearly shows that SPD3 scales \nwell. The slowdown for 1-thread is higher than that for all other threads in many benchmarks. This is \nbecause our implementation Figure 4. Slowdown of ESP-bags and SPD3 relative to 16-thread HJ-Base version \nfor all benchmarks. Note that the ESP-bags version runs on 1-thread while the SPD3 version runs on 16-threads. \n  uses compareAndSet operations on atomic variables. These oper\u00adations are not optimized for the no \ncontention scenario as with 1-thread. Instead, if we use a lock that is optimized for no con\u00adtention \nscenario, the slowdown for 1-thread cases would have been a lot lower. But that implementation does not \nscale well for larger numbers of threads. For example, the lock based implementation is 1.8\u00d7 slower (on \naverage) than the compareAndSet implementa\u00adtion when running on 16-threads. While the two implementations \nare close for many benchmarks (within a factor of 2), there is a difference of upto 7\u00d7 for some benchmarks, \nwhen running on 16\u00adthreads. The compareAndSet implementation is always faster than the lock based implementation \nfor larger numbers of threads. Since our aim was to make the algorithm scalable, we chose the compare-AndSet \napproach.  6.2 Comparison with ESP-bags algorithm In this section, we compare the performance of SPD3 \nwith ESP\u00adbags [24]. Figure 4 shows the slowdown of ESP-bags and SPD3 for all the benchmarks, relative \nto the execution time of the 16-thread HJ-Base version. Note that the ESP-bags version runs on 1-thread \n(because it is a sequential algorithm) while the SPD3 version runs on 16-threads. This comparison underscores \nthe fact that the slowdown for a sequential approach to datarace detection can be signi.cantly larger \nthan that of parallel approaches, when running on a parallel machine. For example, the slowdown is reduced \nby more than a 15\u00d7 factor when moving from ESP-bags to SPD3 for Series and MatMul benchmarks and by more \nthan a 5\u00d7 factor for benchmarks like MolDyn and SparseMatMult that scale well. On the other hand, the \nslowdown for Crypt is similar for ESP-bags and SPD3 because the uninstrumented async-.nish version of \nCrypt does not scale well. On average, SPD3 is 3.2\u00d7 faster than ESP-bags on our 16\u00adway system. This gap \nis expected to further increase on systems with larger numbers of cores.  6.3 Comparison with Eraser \nand FastTrack We only use the JGF benchmarks for comparisons with other algo\u00adrithms since those are the \nonly common benchmarks with past work on Eraser and FastTrack. However, since Eraser and FastTrack work \non multithreaded Java programs rather than task-parallel vari\u00adants like HJ, they used the original coarse-grained \none-chunk-per\u00adthread approach to loop parallelism in the JGF benchmarks with one thread per core. Converting \nthese programs to .ne-grained parallel versions using Java threads was not feasible since creat\u00ading large \nnumbers of threads quickly leads to OutOfMemoryEr\u00adror s. Further, it would also make the size of the \nvector clocks pro\u00adhibitively large in the program in order to provide the same sound\u00adness and completeness \nguarantees as SPD3. So, to enable an apples-to-apples comparison in this section, we created coarse-grained \nasync-.nish versions of the JGF bench\u00admarks with chunked loops for the HJ versions. Since Eraser and \nFastTrack were implemented in RoadRunner, we used the execu\u00adtion of the Java versions of these benchmarks \non RoadRunner with\u00adout instrumentation (RR-Base) as the baseline for calculating the slowdowns for Eraser \nand FastTrack. The differences between RR-Base and HJ-Base arise from the use of array views in the HJ \nver\u00adsion, and from the use of .nish operations instead of barriers as discussed below. Our .rst observation \nwhen running SPD3 on the coarse grained HJ versions of the eight JGF benchmarks was that data races were \nreported for four of the benchmarks: LUFact, MolDyn, RayTracer, and SOR. The data race reports pointed \nto races in shared arrays that were used by the programmer to implement custom barriers. However, all \nthe custom barrier implementations were incorrect be\u00adcause they involved unsynchronized spin loops on \nshared array el\u00adements. Even though the programmer declared the array references as volatile, the volatile \ndeclaration does not apply to the elements of the array. (In all fairness to the programmer, the JGF \nbenchmarks were written in the late 1990 s when many Java practitioners were unaware of the implications \nof the Java memory model.) Our second observation is that the default Eraser and FastTrack tools in the \nRoadRunner implementation did not report most of these data races. The only race reported was by FastTrack \nfor SOR. After communication with the implementers of RoadRunner, we recently learned that RoadRunner \nrecognizes a number of common barrier class implementations by default and generates special Bar\u00adrier \nEnter and Barrier Exit events for them which in turn enables Eraser and FastTrack to take the barriers \ninto account for race de\u00adtection (even though the barriers are technically buggy). Further a -nobarrier \noption can be used to suppress this barrier detection.  We con.rmed that all races were reported with \nthe -nobarrier option. However, all RoadRunner performance measurements re\u00adported in this paper were \nobtained with default settings i.e., without the -nobarrier option. Our third observation is that Eraser \nreported false data races for many benchmarks. This is not surprising since Eraser is known to not be \na precise datarace detection algorithm. To undertake a performance comparison, we converted the four \nbenchmarks to race-free HJ programs by replacing the buggy bar\u00adriers by .nish operations. In some cases, \nthis caused the HJ-base version to be slower than the RR-base version as a result (since RR\u00adbase measures \nthe performance of the unmodi.ed JGF benchmarks with custom barriers). Before we present the comparison, \nit is also worth noting that the implementation of Eraser and FastTrack in RoadRunner include some optimizations \nthat are orthogonal to the race detection algorithm used [14]. Similarly, the static optimiza\u00adtions from \n[24] included in our implementation of SPD3 are also orthogonal to the race detection algorithm. Both \nthese sets of op\u00adtimizations could be performed on any race detection algorithm to improve its performance. \nTable 2. Relative slowdown of Eraser, FastTrack and SPD3 for JGF benchmarks on 16 threads. The slowdown \nof Eraser and Fast-Track was calculated over their baseline RR-Base while the slow\u00addown of SPD3 was calculated \nover its baseline HJ-Base. For bench\u00admarks marked with *, race-free versions were used for SPD3 but the \noriginal versions were used for Eraser and FastTrack. Benchmark RR-Base Time(s) Eraser Slow FastTrack \ndown HJ-Base Time(s) SPD3 Slowdown Crypt 0.362 122.40 133.24 0.585 1.84 LUFact* 1.47 17.95 26.41 5.411 \n1.08 MolDyn* 16.185 8.39 9.59 3.750 13.56 MonteCarlo 2.878 10.95 13.54 5.605 1.86 RayTracer* 2.186 20.23 \n17.45 19.974 5.84 Series 112.515 1.00 1.00 88.768 1.00 SOR* 0.914 4.26 8.36 2.604 4.53 Sparse 2.746 14.29 \n20.59 4.607 1.72 GeoMean - 11.21 13.87 - 2.63 Table 2 shows the slowdowns of Eraser, FastTrack, and \nSPD3 for all the JGF benchmarks on 16 threads. Note that the slowdown of Eraser and FastTrack were calculated \nrelative to RR-Base (with 16 threads), and the slowdown of SPD3 was calculated over HJ-Base (with 16 \nthreads). For benchmarks marked with *, race-free versions were used for SPD3 but the original versions \nwere used for Eraser and FastTrack; this accounts for differences in the execution times of RR-Base and \nHJ-Base for some benchmarks since the async-.nish versions include more synchronization to correct the \nbugs in the original Java versions. Table 2 shows that the relative slowdowns for Eraser and Fast-Track \nare much larger than those for SPD3. On average (geometric mean), the slowdown for SPD3 relative to HJ-base \nis 2.70\u00d7 while that for Eraser and FastTrack are 11.21\u00d7 and 13.87\u00d7 respectively relative to RR-base. \nThere is also a large variation. While the slow\u00addowns are within a factor of 2 for SOR, there is more \nthan a 60\u00d7 gap in slowdowns for Crypt and quite a signi.cant difference for LUFact, MonteCarlo, and SparseMatMult \nas well. The slowdown for SPD3 on MolDyn is larger than the slowdowns for Eraser and FastTrack because \nthe baseline for SPD3 is more than 4\u00d7 faster than the baseline for Eraser and FastTrack. For FastTrack, \nthese slowdowns are consistent with the fact that certain data access pat\u00adterns (notably, shared reads) \ncan lead to large overheads because they prevent the use of optimized versions of vector clocks. For \nthe case with the largest gap in Table 2 (Crypt), Figure 5 shows the slowdown (scaled execution time) \nof RR-Base, Eraser, FastTrack, HJ-Base, and SPD3 for the chunked version of the Figure 5. Slowdown (relative \nto 16-threads RR-Base) of RR-Base, Eraser, FastTrack, HJ-Base, and SPD3 for Crypt benchmark (chun\u00adked \nversion) on 1-16 threads Crypt benchmark on 1-16 threads relative to the 16-thread RR-Base execution \ntime. In this benchmark, RR-Base is the fastest for 16 threads as expected. The execution time of HJ-Base \nis 1.9\u00d7 slower than RR-Base in the 1-thread case and 1.6\u00d7 slower than RR-Base in the 16-thread case. \nSimilarly, the execution time of SPD3 versionis alsoveryclose; it is 4.2\u00d7 slower in the 1-thread case \nand 3\u00d7 slower in the 16-thread case. The execution time of Eraser and FastTrack are 13.7\u00d7 and 16.6\u00d7 slower \nthan RR-Base in the 1-thread case but they increase to more than 100\u00d7 for 8-threads and 16-threads. This \nexample shows that for some programs the performance overheads for Eraser and FastTrack can increase \ndramatically with the number of threads (cores).  6.4 Memory Overhead We now compare the memory overheads \nof the Eraser, FastTrack and SPD3 algorithms on the coarse-grained JGF benchmarks. Again, the baseline \nfor Eraser and FastTrack was RR-Base and the baseline for SPD3 was HJ-Base. To obtain a coarse estima\u00adtion \nof the memory used, we used the -verbose:gc option in the JVM and picked the maximum heap memory used \nover all the GC executions in a single JVM instance. All three instrumented ver\u00adsions trigger GC frequently, \nso this is a reasonable estimate of the memory overhead. Table 3. Peak heap memory usage of RR-Base, \nEraser, FastTrack, HJ-Base, and SPD3 for JGF benchmarks on 16 threads. For bench\u00admarks marked with *, \nrace-free versions were used for SPD3 but the original versions were used for Eraser and FastTrack. Memory \n(in MB) Benchmark RR-Base Eraser FastTrack HJ-Base SPD3 Crypt 209 8539 8535 149 6009 LUFact 80 1790 \n2455 47 203 MolDyn 382 1048 1040 9 35 MonteCarlo 1771 9316 9292 557 584 RayTracer 1106 4475 4466 43 88 \nSeries 80 1067 1062 162 177 SOR 81 1161 1551 47 202 Sparse 225 2120 2171 88 714 Table 3 shows the estimated \nmemory usage of these three algo\u00adrithms and their baselines for JGF benchmarks on 16 threads. The table \nshows that the memory usage of HJ-Base is lower than that of RR-Base in all the benchmarks except Series. \nIn all cases, the memory usage is lower for SPD3, compared to Eraser and Fast-Track with signi.cant variation \nin the gaps. The memory usage of Crypt with SPD3 is quite high because the benchmark has arrays Figure \n6. Estimated heap memory usage (in MB) of RR-Base, Eraser, FastTrack, HJ-Base, and SPD3 for LUFact benchmark \n  of size 20 million and our algorithm maintains shadow locations for all elements of these arrays. \nBut the memory used by SPD3 for Crypt is still less than that of Eraser and FastTrack. The high memory \nusage for Eraser and FastTrack is not surprising because Eraser has to maintain all the locks held while \naccessing a particu\u00adlar location, and FastTrack s vector clocks may grow linearly in the number of threads \nin the worst case. For one of the benchmarks in Table 3 (LUFact), Figure 6 shows the estimated memory \nusage of the three algorithms and their base\u00adlines as a function of the number of threads/cores used. \nNote that both the baselines (RR-Base and HJ-Base) are very close. While the estimated heap usage of \nRR-Base remains constant at 80M, the estimated usage of HJ-Base varies from 33M to 47M as we go from \n1 thread to 16 threads. The estimated heap usage of SPD3 is about 6\u00d7 larger than HJ-Base: it varies between \n192M and 203M across 16 threads. The estimated heap usage of Eraser increases from 833M for 1 thread \nto 1790M for 16 threads (2.1\u00d7 increase). Similarly, the estimated heap usage of FastTrack increases from \n825M for 1 thread to 2455M for 16 threads (3\u00d7 increase). This clearly shows the increase in the memory \nusage for Eraser and Fast-Track as we increase the number of threads for this benchmark.   7. Related \nWork In the introduction, we outlined the key differences between our al\u00adgorithm and FastTrack. In summary, \non one hand, our algorithm uses O(1) space per memory location, while in the worst-case, FastTrack uses \nO(n). On the other, FastTrack handles more gen\u00aderal computation graphs than those supported by our model. \nThe time overhead of our algorithm is characteristic of the application, since it depends on the height \nof the LCA nodes in the DPST. It is independent of the number of threads (processors) the program ex\u00adecutes \non. On the other hand, FastTrack s worst-case time overhead is linear in the number of threads, which \ncan grow very large with increasing numbers of cores. Schonberg [27] presented one of the earliest dynamic \ndata race detection algorithm for nested fork-join and synchronization oper\u00adations. In this algorithm, \na shared variable set is associated with each sequential block in every task. There is also a concurrency \nlist associated with each shared variable set which keeps track of the concurrent shared variable sets \nthat will complete at a later time. The algorithm detects anomalies by comparing complete concur\u00adrent \nshared variable sets at each time step. This algorithm applies only to a single execution instance of \na program, as mentioned in [27]. The space required to store read information in the shared variable \nsets is bounded by V \u00d7 N,where V is the number of vari\u00adables being monitored and N is the number of execution \nthreads 1. This space requirement increases with an increase in the number of threads the program is \nexecuted on, whereas our algorithm s space requirement is independent of the number of threads the program \nis executed on. A limitation of this work is that since access anoma\u00adlies are detected at synchronization \npoints, it does not identify the actual read and write operations involved in the data races. Offset-Span \n(OS) labeling [21] is an optimized version of the English-Hebrew (EH) labeling technique [10] for detecting \ndata races. The idea behind both these techniques is to attach a label to every thread in the program \nand use these labels to check if two threads can execute concurrently. They also maintain the ac\u00adcess \nhistory for every shared variable that is monitored which is then used to check for con.icts. The length \nof the labels associ\u00adated with each thread can grow arbitrarily long in EH labeling2, whereas the length \nof the labels in OS labeling is bounded by the maximum nesting depth of fork-join in the program. While \nthe EH labeling technique needs an access history of size equal to the num\u00adber of threads for every monitored \nvariable in the program, the OS labeling technique only needs constant size to store access his\u00adtory. \nWhile OS labeling algorithm supports only nested fork-join constructs, our algorithm supports a more \ngeneral set of dynamic graphs. Further, though the OS labeling algorithm can execute the input program \nin parallel, it has been evaluated in a sequential set\u00adting only [22]. The effectiveness of this algorithm \nin a parallel im\u00adplementation is not clear. A related work on data race detection for structured parallel \npro\u00adgrams was done as part of the Cilk project [4]. This work gives an algorithm called SP-hybrid, which \ndetects races in the pro\u00adgram with a constant space and time overhead. Their algorithm has the best possible \ntheoretical overheads for both space and time. However, despite its good theoretical bounds, the SP-hybrid \nalgo\u00adrithm is very complex and incurs signi.cant inef.ciencies in prac\u00adtice. The original paper on SP-hybrid \n[4] provides no evaluation and subsequent evaluation of an incomplete implementation of SP\u00adhybrid [18] \nwas done only for a small number of processors. One indicator of the inef.ciency of SP-hybrid can be \nseen in the fact that the CilkScreen race detector used in Intel Cilk++ [1] uses the sequential All-Sets \nalgorithm [8] rather than the parallel SP-hybrid algorithm. Another drawback of their algorithm is that \nit is tightly coupled with Cilk s work-stealing scheduler. Hence, their algo\u00adrithm cannot be applied \ndirectly to other schedulers. In contrast, our algorithm is amenable to an ef.cient implementation, performs \nvery well in practice, supports a more general set of computation graphs than Cilk s spawn/sync and is \nalso independent of the un\u00adderlying scheduler. There has also been work on data race detection algorithms \nfor spawn/sync [12] and async/.nish models [24]. While they require only O(1) space overhead per memory \nlocation, these algorithms must process the program in a sequential depth-.rst manner, fun\u00addamentally \nlimiting the scalability of these approaches. In contrast, the algorithm presented in this work can process \nthe program dur\u00ading parallel execution, while still requiring only O(1) space per memory location. 1 \nIf N refers to the maximum number of threads possible in all executions of a program for a given input, \nthen this algorithm can guarantee data race freedom for all executions of the program for that input. \nIf not, then this guarantee will not hold. 2 Note that the length of the labels is bounded by the maximum \nnesting level of fork-join in EH labeling in the presence of an effective heuristic as reported in [10] \n  8. Conclusion and Future Work In this work, we presented a new dynamic data race detection algo\u00adrithm \nfor structured parallel programs. The algorithm can process the program in parallel, uses O(1) space \nper memory location and admits an ef.cient implementation. The algorithm tracks what can happen in parallel \nvia a new data structure called the dynamic pro\u00adgram structure tree (DPST), and maintains two readers \nand a writer for each shared memory location in order to track potential con\u00ad.icts between different \ntasks. We implemented the algorithm and demonstrated its effectiveness on a range of benchmarks. In future, \nit could be interesting to extend the algorithm to other structured parallel constructs such as HJ s \nphaser construct [6].  Acknowledgments We are grateful to the authors of the RoadRunner tool [14], Cormac \nFlanagan and Stephen Freund, for sharing their implementation of FastTrack that was used to obtain the \nresults reported in [13], and for answering our questions related to both FastTrack and RoadRunner. We \nwould also like to thank John Mellor-Crummey from Rice University for his feedback and suggestions on \nthis work. This work was supported in part by the U.S. National Science Foundation through awards 0926127 \nand 0964520. We also thank the US-Israel Binational Foundation (BSF) for their support.  References \n[1] Intel Cilk++ Programmer s Guide. http://software.intel.com/en\u00adus/articles/download-intel-cilk-sdk/. \n[2] The Computer Language Benchmarks Game. http://shootout.alioth.debian.org/. [3] BARIK,R., ET AL. The \nhabanero multicore software research project. In Proceeding of the 24th ACM SIGPLAN conference companion \non Object oriented programming systems languages and applications (2009), ACM, pp. 735 736. [4] BENDER,M. \nA., ET AL. On-the-.y maintenance of series-parallel relationships in fork-join multithreaded programs. \nIn SPAA 04 (Barcelona, Spain, June27 30 2004), pp. 133 144. [5] BLUMOFE,R. D., ET AL. Cilk: an ef.cient \nmultithreaded runtime system. In PPoPP 95 (Oct. 1995), pp. 207 216. [6] CAVE,V.,ZHAO,J., SHIRAKO,J., \nAND SARKAR, V. Habanero-java: the new adventures of old x10. In PPPJ 11 (2011). [7] CHARLES,P., ET AL. \nX10: An object-oriented approach to non\u00aduniform cluster computing. In OOPSLA 2005 Onward! Track (2005). \n[8] CHENG, G.-I., FENG,M., LEISERSON,C. E., RANDALL,K. H., AND STARK, A. F. Detecting data races in cilk \nprograms that use locks. In SPAA 98 (1998), pp. 298 309. [9] CONG,J., SARKAR,V.,REINMAN,G., AND BUI, \nA. Customizable Domain-Speci.c Computing. IEEE Design and Test, 2:28 (Mar 2011), 6 15. [10] DINNING,A., \nAND SCHONBERG, E. An empirical comparison of monitoring algorithms for access anomaly detection. In PPoPP \n90 (1990), ACM, pp. 1 10. [11] DURAN,A., ET AL. Barcelona OpenMP tasks suite: A set of bench\u00admarks targeting \nthe exploitation of task parallelism in openmp. In ICPP 09 (2009), pp. 124 131. [12] FENG,M., ANDLEISERSON,C.E.Ef.cientdetectionofdeterminacy \nraces in cilk programs. In SPAA 97 (1997), ACM, pp. 1 11. [13] FLANAGAN,C., AND FREUND, S. N. FastTrack: \nef.cient and precise dynamic race detection. In PLDI 09 (2009), ACM, pp. 121 133. [14] FLANAGAN,C., AND \nFREUND, S. N. The roadrunner dynamic analysis framework for concurrent programs. In PASTE 10 (2010), \nACM, pp. 1 8. [15] FRIGO,M., LEISERSON,C. E., AND RANDALL, K. H. The imple\u00admentation of the cilk-5 multithreaded \nlanguage. In PLDI 98 (1998), ACM, pp. 212 223. [16] GUO,Y., ET AL. Work-.rst and help-.rst scheduling \npolicies for async-.nish task parallelism. In IPDPS 09 (2009), IEEE Computer Society, pp. 1 12. [17] \nGUO,Y.,ZHAO,J.,CAVE\u00b4,V., AND SARKAR, V. Slaw: A scalable locality-aware adaptive work-stealing scheduler. \nIn IPDPS (2010). [18] KARUNARATNA, T. C. Nondeterminator-3: A provably good data\u00adrace detector that runs \nin parallel. Master s thesis, Department of Electrical Engineering and Computer Science, MIT,, Sept. \n2005. [19] LAMPORT, L. Concurrent reading and writing. Commun. ACM 20 (November 1977), 806 811. [20] \nLEE,J. K., AND PALSBERG, J. Featherweight x10: a core calculus for async-.nish parallelism. In PPoPP \n10 (2010), ACM, pp. 25 36. [21] MELLOR-CRUMMEY, J. On-the-.y detection of data races for programs with \nnested fork-join parallelism. In Supercomputing 91 (1991), ACM, pp. 24 33. [22] MELLOR-CRUMMEY, J. Compile-time \nsupport for ef.cient data race detection in shared-memory parallel programs. In PADD 93: Pro\u00adceedings \nof the ACM/ONR workshop on Parallel and distributed de\u00adbugging (1993), ACM, pp. 129 139. [23] OpenMP \nApplication Program Interface v 3.0, 2008. [24] RAMAN,R., ET AL. Ef.cient data race detection for async-.nish \nparallelism. In RV 10 (2010), Springer-Verlag, pp. 368 383. [25] RAMAN,R.,ZHAO,J.,SARKAR,V., VECHEV,M., \nAND YAHAV, E. Scalable and precise dynamic datarace detection for structured parallelism. Tech. Rep. \nTR12-01, Department of Computer Science, Rice University, Houston, TX, 2012. [26] SAVAGE,S., ET AL. Eraser: \na dynamic data race detector for multi\u00adthreaded programs. ACM Trans. Comput. Syst. 15, 4 (1997), 391 \n411. [27] SCHONBERG, E. On-the-.y detection of access anomalies. In PLDI 98 (1998), pp. 285 297. [28] \nSMITH,L. A., AND BULL, J. M. A Parallel Java Grande Benchmark Suite. In In Supercomputing 01 (2001), \nACM Press, p. 8. [29] VALL\u00b4 Soot -a Java Optimization Framework. In EE-RAI,R., ET AL. Proceedings of \nCASCON 1999 (1999), pp. 125 135. [30] WINSKEL,G. The Formal Semantics of Programming Languages. MIT \nPress, 1993. [31] ZHAO,J., AND SARKAR, V. Intermediate language extensions for parallelism. In VMIL 11 \n(2011), pp. 333 334.  \n\t\t\t", "proc_id": "2254064", "abstract": "<p>Existing dynamic race detectors suffer from at least one of the following three limitations:</p> <p>(i)<i>space</i> overhead per memory location grows linearly with the number of parallel threads [13], severely limiting the parallelism that the algorithm can handle;</p> <p>(ii)<i>sequentialization</i>: the parallel program must be processed in a sequential order, usually depth-first [12, 24]. This prevents the analysis from scaling with available hardware parallelism, inherently limiting its performance;</p> <p>(iii) <i>inefficiency</i>: even though race detectors with good theoretical complexity exist, they do not admit efficient implem entations and are unsuitable for practical use [4, 18].</p> <p>We present a new precise dynamic race detector that leverages structured parallelism in order to address these limitations. Our algorithm requires constant space per memory location, works in parallel, and is efficient in practice. We implemented and evaluated our algorithm on a set of 15 benchmarks. Our experimental results indicate an average (geometric mean) slowdown of 2.78x on a 16-core SMP system.</p>", "authors": [{"name": "Raghavan Raman", "author_profile_id": "81440614441", "affiliation": "Rice University, Houston, TX, USA", "person_id": "P3471316", "email_address": "raghav@rice.edu", "orcid_id": ""}, {"name": "Jisheng Zhao", "author_profile_id": "81361600781", "affiliation": "Rice University, Houston, TX, USA", "person_id": "P3471317", "email_address": "jisheng.zhao@rice.edu", "orcid_id": ""}, {"name": "Vivek Sarkar", "author_profile_id": "81100597290", "affiliation": "Rice University, Houston, TX, USA", "person_id": "P3471318", "email_address": "vsarkar@rice.edu", "orcid_id": ""}, {"name": "Martin Vechev", "author_profile_id": "81100269652", "affiliation": "ETH, Zurich, Switzerland", "person_id": "P3471319", "email_address": "martin.vechev@inf.ethz.ch", "orcid_id": ""}, {"name": "Eran Yahav", "author_profile_id": "81100285431", "affiliation": "Technion, Haifa, Israel", "person_id": "P3471320", "email_address": "yahave@cs.technion.ac.il", "orcid_id": ""}], "doi_number": "10.1145/2254064.2254127", "year": "2012", "article_id": "2254127", "conference": "PLDI", "title": "Scalable and precise dynamic datarace detection for structured parallelism", "url": "http://dl.acm.org/citation.cfm?id=2254127"}