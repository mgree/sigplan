{"article_publication_date": "06-11-2012", "fulltext": "\n Verifying GPU Kernels by Test Ampli.cation * Alan Leung Manish Gupta Yuvraj Agarwal Rajesh Gupta Ranjit \nJhala Sorin Lerner University of California, San Diego {aleung,manishg,yuvraj,gupta,jhala,lerner}@cs.ucsd.edu \nAbstract We present a novel technique for verifying properties of data par\u00adallel GPU programs via test \nampli.cation. The key insight behind our work is that we can use the technique of static information \n.ow to amplify the result of a single test execution over the set of all inputs and interleavings that \naffect the property being veri.ed. We empirically demonstrate the effectiveness of test ampli.cation \nfor verifying race-freedom and determinism over a large number of standard GPU kernels, by showing that \nthe result of verifying a single dynamic execution can be ampli.ed over the massive space of possible \ndata inputs and thread interleavings. Categories and Subject Descriptors D.2.4 [Software Engineer\u00ading]: \nSoftware/Program Veri.cation Validation; F.3.2 [Seman\u00adtics of Programming Languages]: Semantics of Programming \nLan\u00adguages Program analysis General Terms Languages, Reliability, Veri.cation Keywords Test Ampli.cation, \nDeterminism, GPU 1. Introduction Despite its relative infancy, CUDA and related programming lan\u00adguages \nhave already become one of the most widely used models for parallel programming. The appeal of this model \nis that it offers a way to exploit .ne-grained data-parallelism within the comforts of C/C++ style imperative \nprogramming. The dramatic performance gains offered by the approach have led to its widespread use in \na variety of domains including scienti.c computing, .nance, and computational biology [29]. CUDA programs \nare very hard to get right, for all the usual rea\u00adsons, and a few new ones. First, in the course of execution, \na pro\u00adgram can spawn millions of threads, which are clustered in multi\u00adlevel hierarchies that mirror \na multi-level shared memory hierarchy. Second, for performance reasons, well-weathered synchronization \nmechanisms like locks or transactions are eschewed in favor of bar\u00adriers, and it is up to the programmer \nto make sure that concurrently executing threads do not con.ict by ensuring that their algorithms make \nthreads touch disjoint parts of memory. Third, a key source of ef.ciency is the extremely .ne-grained \nmemory sharing across * This work was supported by NSF Expeditions in Computing grant CCF\u00ad1029783 and \nCAREER grants CCF-0644306 and CCF-0644361. Permission to make digital or hard copies of all or part of \nthis work for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. PLDI 12, June 11 16, 2012, Beijing, China. Copyright c . 2012 ACM 978-1-4503-1205-9/12/06. \n. . $10.00. threads. Memory bandwidth is maximized when threads with suc\u00adcessive identi.ers access, in \nlock-step, addresses that are physically adjacent in memory. This can lead to complex striding patterns \nof memory access that are hard to reason about. An unfortunate consequence of these factors is that CUDA \npar\u00adallelism is dif.cult to analyze with existing static or dynamic ap\u00adproaches. Static techniques are \nthwarted by the complexity of the sharing patterns. A useful static analysis would have to .nd a suc\u00adcinct \nsymbolic representation for the sets of addresses accessed by each thread. It is common for CUDA code \nto use modular arith\u00admetic or bit-shifting to access memory indices according to com\u00adplex linear or even \nnon-linear patterns which makes such analyses dif.cult. Dynamic techniques are challenged by the combinatorial \nexplosion of thread interleavings and space of possible data inputs: any reasonable number of tests would \nrepresent a small subset of the possible behaviors of the system. Finally, mechanisms for en\u00adforcing \nisolation at run-time are likely to impose unacceptable over\u00adheads, and would leave the developer the \nunenviable task of sifting through a huge execution trace to .gure out what went wrong. To attack these \nchallenges, we employ test ampli.cation,a general notion wherein a single dynamic run can be used to \nlearn much more information about a program s behavior than is directly exhibited by that particular \nexecution. Though this notion has been explored in many settings, we contribute a new formulation of \ntest ampli.cation specialized for verifying properties of CUDA kernels, which we call .ow-based test \nampli.cation. Our technique skirts the limitations of existing static and dynamic approaches by combining \ntheir complementary strengths, as follows. First, we run a dynamic analysis where we log the behavior \nof the kernel with some .xed test input and under a particular thread interleaving. Second, we use a \nstatic information .ow analysis to compute the property-integrity inputs, namely, the input variables \nthat actually .ow-to, or affect the integrity of, the variables appear\u00ading in the property to be veri.ed. \nFinally, we amplify the result of the test to hold over all the inputs that have the same values for \nthe property-integrity inputs. Of course, this approach would yield no mileage if all the in\u00adputs were \nproperty-integrity inputs. Our second contribution is to empirically demonstrate over a large number \nof CUDA bench\u00admarks, that for key properties like race-freedom and determinism, the set of property-integrity \ninputs comprises just a small core of con.guration inputs. These are typically parameters that describe \nthe dimensions of the thread-and memory-hierarchies and dataset, and which are highly tuned for a given \narchitecture and algorithm. Thus, test ampli.cation allows us to use a single execution over the con.guration \nto verify properties over the massive space of all pos\u00adsible data inputs (non-con.guration inputs) and \nthread interleav\u00adings that constitute the behaviors of the kernel. This simple insight ties together \nthe complementary strengths of well-known static and dynamic approaches, yielding a clean way to analyze \nCUDA kernels. The static analysis is just a simple .ows-to question that can be resolved via taint-propagation \nand an alias analysis. The dynamic analysis performs the dif.cult task of actually computing the sets \nof addresses that are accessed in complex striding patterns. Using the information .ow, we can then generalize \nthe dynamic analysis over all possible data inputs. In summary, we make the following contributions: \n  We present .ow-based test ampli.cation, a hybrid technique specialized for analyzing GPU kernels which \nuses static infor\u00admation .ow to generalize the result of a dynamic analysis over a large space of inputs \n(Section 3).  We implement .ow-based test ampli.cation for CUDA using the LLVM framework, by implementing \nstatic taint-propagation and devising ef.cient ways to record and analyze traces of CUDA kernels (Section \n4).  We empirically demonstrate the effectiveness of .ow-based test ampli.cation for verifying race \nfreedom and determinism of CUDA kernels, by conducting a systematic evaluation over the kernels available \nin the CUDA SDK [28] and demonstrating that the technique can be used for most kernels. Even when the \ntechnique does not apply, a kernel can often be slightly modi.ed to fall under the scope of our analysis \n(Section 5). As a result, we believe test ampli.cation can be a stepping stone to developing various \nsafety and performance analyses for massively data\u00adparallel programs.  2. Overview We start with an \noverview of CUDA, and then motivate our tech\u00adnique with a textbook CUDA kernel. 2.1 CUDA Basics To illustrate \nthe CUDA programming model we use a simpli.ed version of scalarProd, shown in Figure 1, a program from \nthe CUDA SDK that performs a parallel dot product of two vectors. In a nutshell, the CUDA language is \na variant of C++ extended with syntax to annotate parallel routines called kernels, and the shared data \nstructures that the routines manipulate. The kernels execute on GPU hardware and must exploit the shared \nmemory hierarchy in order to obtain maximum performance. Thread Hierarchy A program written in CUDA decomposes \nparallel computation into a two-level geometric grid of gridDim thread blocks, where each block comprises \nblockDim threads, each performing a subset of the total workload. Due to hardware support, CUDA threads \nhave much less overhead relative to CPU threads: a CUDA program might spawn more than one million threads \nin a single execution. The gridDim and blockDim con.guration pa\u00adrameters are together called the geometry \nof the computation. (Al\u00adthough grids and thread blocks may be multi-dimensional, we omit .ner grained \nx- and y- dimensions for clarity.) Memory Hierarchy Threads communicate via a hierarchical memory model \norganized into three levels, in decreasing order of their thread visibility, size, and latency: global \nmemory which is slowest and shared by all threads, shared memory which is faster but shared only by the \nthreads within a particular block, and regis\u00adters which are fastest but private to each thread. In our \nexample, all threads belong to a single thread block, all with access to the shared array accumRes and \nthe global arrays dA and dB. Parallel Execution Threads are spawned and executed via kernel calls, which \nspecify the kernel that is to be executed and the thread\u00adgeometry, i.e., the delineation of threads into \nthread blocks. For example, a kernel call of the form ScalarProd<<<1, 128>>>(d C, d A, d B, 4096) 1: \n#define accumN 1024 2: void scalarProd( float *d_C , float *d_A , float *d_B , int sizeN) { 3: __shared__ \nfloat accumRes[accumN]; /*******************************************/ /********* Phase 1: Partial Sums \n***********/ /*******************************************/ 4: for( int i = threadIdx ; i < accumN ; i \n+= blockDim) { 5: float sum = 0; 6: for(int j = i; j < sizeN; j += accumN) { 7: sum += d_A[j] * d_B[j]; \n} 8: accumRes[i] = sum; } /*******************************************/ /********* Phase 2: Tree Reduction \n*********/ /*******************************************/ 9: for( int stride = accumN / 2 ; stride > 0 \n; stride >>= 1) { 10: __syncthreads(); 11: for( int i = threadIdx ; i < stride ; i += blockDim) { 12: \naccumRes[i] += accumRes[stride + i]; } } 13: if(threadIdx == 0) *d_C = accumRes[0]; } Figure 1. scalarProd \nkernel begins execution of a single block containing 128 threads, where each thread executes the code \nin the scalarProd routine from Figure 1. Although all threads execute the same kernel, each thread is \ndistinguished by a unique identi.er in the form of its coordinate within the grid, that is by the tuple \nof the variables blockIdx and threadIdx (and the elided x-and y-sub-dimensions.) Thus, a thread can use \nthe values of its blockIdx and threadIdx, and the geometry variables gridDim and blockDim, to distinguish \nits portion of the workload from those of other threads. Synchronization Threads synchronize via barriers: \na thread that calls syncthreads() will wait until all other threads in its thread block have also reached \nthe barrier. No corresponding synchroniza\u00adtion mechanism exists for threads in different thread blocks. \n(While one can encode synchronization mechanisms using global memory, such mechanisms are brittle and \ninef.cient and hence discouraged.)  2.2 Scalar Dot Product Now that we are equipped with a basic understanding \nof the CUDA model, let us turn our attention to the scalarProd benchmark provided with the CUDA SDK. \nThe original program calculates the dot products of 256 pairs of vectors, each of length sizeN = 4096. \nTo simplify exposition, we have reduced the example to calculate the dot product of a single pair of \nvectors of sizeN elements.  The example exhibits three characteristics of CUDA kernels op\u00adtimized for \nperformance: 1) Memory is accessed in strides in or\u00adder to maximize memory bandwidth, 2) Shared memory \nis used to cache frequently accessed data, and 3) Threads cooperate to perform reductions with minimal \nsynchronization and communica\u00adtion. The example also demonstrates that even a seemingly simple computation, \nan elementary linear algebra primitive, can require hard-to-analyze optimizations when adapted for parallelization \nvia CUDA. The implementation uses a parallel algorithm separated into two phases, each with a different \nparallelization strategy, to increase performance. Phase 1: Partial Products In the .rst phase (lines \n4-8), each thread computes accumN/blockDim (i.e., 8) partial dot-products, of sub-vectors of size sizeN/accumN \n(i.e., 4), and stores these par\u00adtial products into the array accumRes. This phase is implemented by the \nnested loops on lines 4 and 6. The outer loop on line 4 iter\u00adates i over threadIdx +0 \u00d7 blockDim, threadIdx \n+1 \u00d7 blockDim, threadIdx +2 \u00d7 blockDim,... and at each offset, uses the inner-loop on line 6 to compute \nthe dot-product of the sub-vectors at indices i +0 \u00d7 accumN, i +1 \u00d7 accumN, i +2 \u00d7 accumN,... the result \nof which is stored in the shared accumRes[i] (line 8). Note that the threads do not operate on contiguous \nvector el\u00adements but instead access elements with a stride of accumN. The strided access pattern is deliberate: \nwe achieve maximum memory bandwidth when neighboring threads access consecutive elements in global memory \nbecause the hardware optimizes the accesses to occur in parallel. This feature is known as memory coalescing. \nIf instead each thread accessed contiguous vector elements, the mem\u00adory accesses would occur serially, \nthereby severely reducing perfor\u00admance. Note also that the partial dot products are stored in shared \nmemory, not global memory, in order to exploit temporal locality of access to intermediate results when \nperforming the subsequent reduction. Phase 2: Tree Reduction In the second phase (lines 9-13), all threads \ncooperate to add up the partial products to a .nal value. A na\u00a8ive implementation would assign a single \nthread to perform the entire sum, reverting to a sequential algorithm. Instead, we can view the reduction \nas follows: each partial product is a leaf in a binary tree, and each parent is the sum of its children. \nWe iterate up the levels of the tree by calculating the parents values until we reach the root of the \ntree. Recall that the the array accumRes contains the partial products. Thus, in each iteration, we simply \noverwrite the left child at index i with the sum of its own value, and that of the right child at index \ni + stride, thereby obtaining the parent value. Each iteration is parallelized by using thread threadIdx \nto compute the parent values at indices threadIdx +0 \u00d7 blockDim, threadIdx +1 \u00d7 blockDim, threadIdx +2 \n\u00d7 blockDim,... as done in the inner-loop on lines 11-12. As before, this strided access pattern enables \nef.ciency via parallel memory access. We iterate up the levels of the tree using the outer loop on line \n9 which shrinks stride by half at each iteration as the number of leaf nodes halves across each iteration. \nNote that the cells accessed by different threads overlap across levels. The syncthreads() at line 10 \nensures that a level has been completed before the com\u00adputation proceeds to the next level, and thereby \npreventing memory con.icts between threads executing on different levels. When the iterations have all \ncompleted, thread 0 copies the .nal value from accumRes[0] into the destination dC at line 13, and the \nkernel ter\u00adminates.  2.3 Veri.cation Thus, the scalarProd kernel makes use of several sophisticated \noptimizations to squeeze performance out of the GPU hardware. Unfortunately, these optimizations are \ntreacherous as it is easy enough for the usual sorts of concurrency-related errors to creep in. The GPU \nsetting exacerbates matters, as dynamic monitoring and protection mechanisms would likely require expensive \nhardware support in order to be ef.cient. Even if such mechanisms could be implemented, errors would \nbe dif.cult to debug due to the scale of the concurrency (millions of threads). Property: Absence of \nRaces Let us consider the concrete prob\u00adlem of verifying race freedom, i.e., verifying that a given thread \nis not writing to a shared location at the same time that another is reading from the location. In the \nCUDA setting, the only synchro\u00adnization primitive is the syncthreads() barrier, and hence, the problem \nreduces to determining that the sets of memory locations read and written by different threads within \nmatching barriers, are disjoint. Dif.culty of Static Veri.cation Unfortunately, as the scalarProd example \nillustrates, existing static analyses are of little use in the face of the complex access patterns that \nare idiomatic in CUDA kernels. Aliasing-, Shape-or Escape-based approaches [30] can work for disjoint \nlinked data structures, but are not precise enough to distinguish disjoint regions within shared arrays. \nMore precise arithmetic abstractions for such regions, such as intervals, octagons, polyhedra and even \ninterval congruences [26] would not suf.ce as the set of addresses accessed by each CUDA thread often \nfollows complex patterns. For example, in the .rst phase of scalarProd the thread threadIdx writes to \nthe array accumRes at indices {threadIdx + m \u00d7 blockDim + n \u00d7 accumN} where m and n range over 0 and \naccumN/blockDim and sizeN/accumN respectively. The second phase is even more chal\u00adlenging, as in the \nkth iteration of the outer-loop, each thread reads the array accumRes at indices {threadIdx + m \u00d7 blockDim \n+ accumN/2k} where m ranges from 0 to accumN/(blockDim \u00d7 2k).  2.4 Our Approach Our solution skirts \nthe limitations of current static veri.cation tech\u00adniques with a combination of dynamic race detection \nand static in\u00adformation .ow analysis. Our dynamic analysis tests for races in a single execution by instrumenting \nthe kernel to log memory ac\u00adcesses and verifying disjointness of read and write addresses. We then amplify \nthe results of the dynamic analysis to apply to all pos\u00adsible executions of a con.guration if we can \nverify that the kernel always performs the same memory accesses, regardless of the val\u00adues of its data \ninputs, a property we call access invariance. Given that this property holds, we then know that a single \nexecution s accesses are in fact representative for all the possible executions.  X \u00b7 = threadIdx, x, \ny, . . . Locals G \u00b7 = g, h, . . . Globals V \u00b7 = X . G Variables e ::= Expressions ||| x n e1 . e2 local-read \nconstant binop c ::= Commands ||||| assume(e) x = e x = g[x] g[x] = e c; c assume var-assignment global-read \nglobal-write sequence Figure 2. Syntax Putting it together, we can effectively guarantee that the kernel \nis race free and deterministic across all data inputs. Dynamic Race Detection In addition to memory addresses \nwe record the identity of the thread performing the access and the lo\u00adcation of the barrier last encountered \nby that thread. To distinguish accesses to shared and global memory, we record the address and extent \nof each shared and global data structure. We then check for races by verifying disjointness of write \nand read addresses between threads that can race to the same global or shared data structure. Given that \nthe log demonstrates no such con.icting access, we provide the guarantee that the kernel is race free \nfor the single instantiation of input values used for that execution. Because the analysis checks for \nthe possibility of a race, not whether a destructive race has actually occurred, a kernel veri.ed to \nbe race free by this analysis is race free regardless of the particular interleaving exercised by the \nexecution, once again for this single instantiation of input values. In addition, since the analysis \nveri.es disjointness of accesses, we can also guarantee that the veri.ed kernel executes deterministically \non that input. However, note that this dynamic testing by itself can provide no guarantees about execution \non any other instantiation of inputs. Information Flow Although the guarantees provided by our dy\u00adnamic \nrace detector are desirable, they extend only to a single in\u00adstantiation of inputs within an enormous \nuniverse of inputs, hence the need for a static analysis to amplify those guarantees further. In particular, \nwe use a static information .ow analysis that tracks .ows from data inputs throughout the kernel. Intuitively, \nwe apply a taint to data inputs values, track the .ow of taint through program variables, then check \nthat no tainted program variable is used as the address operand of a memory instruction. We additionally \ncheck that no memory access is control dependent on a tainted variable. If these two properties hold, \nwe say that the kernel is access in\u00advariant and will exhibit the same memory accesses across all data \ninputs. By itself, this property is interesting but not immediately useful. It is the combination of \nthis property and the result of our dynamic analysis that produces a much more powerful guarantee: the \nveri\u00ad.ed kernel will exhibit no races in any execution, and so the kernel is deterministic. 3. Test Ampli.cation \nvia Information Flow In this section we formalize a general framework for combining tests with information \n.ow, and show how we instantiate it to verify CUDA kernels. We start by making precise the syntax and \nsemantics of kernels, which enables us to de.ne the ingredients of the main Theorem 1 which states how \nthe results of a particular test can be ampli.ed across different inputs via information .ow. Next, we \ndescribe how the general framework is instantiated for the setting of CUDA kernels. 3.1 Syntax Figure \n2 summarizes the syntax of kernels. Informally, a kernel is a collection of concurrently executing threads \nwhich interact via shared memory. Variables Our kernels include two kinds of variables: (thread)\u00adlocal, \ndenoted by x, y, etc. and global g, h, etc.. We write v, w to denote either local or global variables. \nWe assume for simplicity that all global variables are arrays. Note that a scalar global is simply an \narray of size 1. For clarity, we abuse notation to omit the offset 0 when reading or writing such variables. \nExpressions and Commands The set of kernel expressions in\u00adcludes reads of local variables, primitive \nconstants (e.g., 0, 1, 2,...) and binary operations over sub-expressions. Our language of ex\u00adpressions \nis side-effect free. The set of commands includes se\u00adquences of assume(\u00b7) (used to model branches), local \nassignment, and reads from and writes to global variables. Reads and Writes A variable v is read in an \nexpression if it appears in the expression. A variable is read in a command if the command contains an \nexpression in which the variable is read. A variable v is written in a command if the command contains \nan assigment of the form v= e or v[x] = e. Threads A thread t is a tuple (V0, L, l0, C ), where V0 is \na set of input variables, L is a .nite set of program locations, l0 . L is a special entry location, \nand C is a control .ow map from L \u00d7 L to the set of commands. A variable is read (resp. written) on an \nedge if it is read (resp. written) on the command labelling the edge. A variable v is immutable in a \nthread if it is not written on any edge of the thread s control-.ow map. We assume that V0 includes any \nlocal or global variables that may be read before they are written. Finally, we assume that V0 includes \ndistinguished immutable local input variables threadIdx and blockDim that hold the (unique) identi.er \nof the thread and the total number of threads, respectively. Kernels A kernel is a pair of a set of thread-identi.ers \nT id . N and a thread. Intuitively, a kernel (T id, V0, L, l0, C ) has |T id|many distinct threads executing \nthe same commands (given by C ). However, the commands can inspect the value of the input threadIdx, \nand hence, different threads can behave differently. Example Recall the scalarProd kernel from Figure \n1. The body of the kernel can be mapped to a single thread s control-.ow map, with locations corresponding \nto the program labels (3:, 4:, 5:, etc..) in the standard way. The input variables of the thread are \ndA, dB, dC, sizeN, the distinguished threadIdx, and blockDim.  3.2 Semantics Next, we formalize the \nsemantics via states and transitions. Notation Let f map A to B. We write f[a .. b] for the map .x. if \nx = a then b else fx Let A. be a subset of A. The restriction of f to A., is the map from A. to B that \ncoincides with f on A. .  Kernel Transition P . s.. s. tid, C . s.. s. [T-PGM] (T id, V0, L, l0, C ) \n. s.. s. tid, C . s.. s[tid .. l] Thread Transition tid, C . s .. s. l = s(tid) c = C (l, l.) tid, c \n. s .. s. . . [T-THREAD] Command Transition tid,c . s.. s. tid,c1 . s.. stid,c2 . s.. s [T-SEQ] tid,c1; \nc2 . s.. s.. s(tid)(e)= true [T-ASSUME] tid, assume(e) . s.. s s(e)(tid)= n [T-ASGN] tid, x= e . s.. \ns[x .. s(x)[tid .. n]] s(y)(tid)= n . s(g)(n .)= n [T-READ] tid,x =g[y] . s.. s[x .. s(x)[tid .. n]] \ns(y)(tid)= n . s(x)(tid)= n [T-WRITE] tid, g[y] = x . s.. s[g .. s(g)[n . .. n]] Figure 3. Semantics \nStates A state s is a map from T id . X . G to L . (T id . N) . (N . N), such that s(tid) is the program \nlocation of thread tid, s(x)(tid) is the value of the local x in the thread tid, and s(g)(n) is the value \nof the global array g at the index n. We write S for the set of all states. Initial States A state s \nis an initial state for kernel P = (T id, V0, L, l0, C ) written P . s if s(blockDim)= |T id|, and for \neach tid . T id, we have s(tid)= l0 and s(threadIdx)(tid)= tid. Transitions The transition relation of \na kernel is a subset of S \u00d7 S. We write P . s.. s. if the pair s, s. is in the transition relation of \nthe kernel, de.ned formally in Figure 3. Intuitively, the transition relation is the union of the transition \nrelations of the individual threads. Each thread transition atomically moves the thread from its current \nprogram location to a successor location, updating the locals of the thread and the globals whilst leaving \nthe program locations and locals of all other threads untouched. An assume transition [T-ASSUME] succeeds \nonly if the condi\u00adtion holds, a local assignment [T-ASGN], [T-READ] updates the value of the local for \nthe executing thread, and a write [T-WRITE] updates the global array at appropriate address. A command \nis a sequence of assignments and assumes, and [T-SEQ] ensures that the sequence executes atomically, \nthereby allowing us to construct higher level synchronization mechanisms as described later. Branch Determinism \nWe say a control .ow map is branch deter\u00administic if for any thread tid and state s, there is at most \na single s. such that tid, C . s.. s.. There is a simple suf.cient condi\u00adtion for branch determinism, \nnamely 1) the primitive operations . are deterministic, and 2) the program locations have at most two \nsuccessor edges, 3) the nodes with multiple successor edges have edges labeled by assume(e) and assume(\u00ace). \nThe control .ow maps compiled from standard structured languages are branch de\u00adterministic, so we will \nassume this property in the sequel. Traces The traces of a kernel Traces(P ) are .nite sequences of of \nstates t = s0,...,sn such that P . s0 and P . sk .. sk+1, for each 0 = k<n. We write t(k) for the kth \nstate in a trace.  3.3 Test Ampli.cation Now that we have formalized the semantics of kernels, we can \ndescribe what it means for a variable to .ow-to another, and hence the notion of .ow-based test ampli.cation. \nProjection and Equivalence Let Y be a set of variables. The projection of a state s to Y , written s[Y \n] is the restriction of s to the domain Y . We lift projection to sequences of states, i.e., traces, \nin the natural way. A state s is equivalent over Y to s., written s =Y s. if s[Y ]= s.[Y ]. Information \nFlow A variable v .ows-to w written v . FlowsTo(P, w) if there exist traces, t and t . in Traces(P ) \nsuch that t(0) =V \\v t .(0), and t(k) .=w t.(k) for some k. Intuitively, v .ows-to w if there are two \ntraces that agree on all variables except v at the beginning, but which differ on the value of w at some \ntime k [18]. For a set of variables W we de.ne FlowsTo(P, W )= \u00b7 .w.W FlowsTo(P, w) Thread Interleavings \nWe say that a variable w is non\u00addeterministically affected by thread interleavings if there exist traces \nt, t . . Traces(P ) such that t(0) = t .(0) but for some k, t (k)(w) .(k)(w). We make the following observations: \n= t. 1. If w is non-deterministically affected by interleavings then triv\u00adially, for all v we have v \n. FlowsTo(P, w). 2. If w is not non-deterministically affected by thread interleav\u00adings. Then v . FlowsTo(P, \nw) only if v is an input variable.  Properties A property F over VF is a predicate over the (pro\u00adgram) \nvariables VF. A state s satis.es a property, written s |=F if the predicate evaluates to true in s. A \ntrace t satis.es a prop\u00aderty, written t |=F if for each k, we have t(k) |=F. The set FlowsTo(P, VF) is \nthe set of property-integrity inputs, that is, the set of inputs that .ow-to a variable in VF. THEOREM \n1. [Test Ampli.cation] Let F be a property over VF, and Y be a subset of the input variables. If FlowsTo(P, \nVF) . Y  t . Traces(P ) is such that t |=F  t. then .t. . Traces(P ).t(0) =Y (0) implies t. |=F. Informally, \nthe theorem states that if Y contains the set of property-integrity inputs, then we can amplify the success \nof a single satisfying execution to conclude that all all executions that start with the same values \nfor Y , regardless of the values of other inputs and thread scheduling, will also satisfy the property. \n 3.4 CUDA Veri.cation The conclusion of the test ampli.cation theorem is trivial, and of little practical \nvalue if the set Y includes all the program variables. To analyze CUDA kernels, we must reduce Y to be \nas small as possible. We demonstrate that for verifying determinism of CUDA kernels, Y can be distilled \ndown to a small core of con.guration inputs that are highly tuned to a limited set of values for a given \nalgorithm and architecture. Consequently, test ampli.cation allows us to use a dynamic single execution \nover the con.guration to verify properties over a massive space of possible data inputs and thread interleavings \nthat constitute the behaviors of the kernel.  Thus, next, we describe how our framework can be instanti\u00adated \nto verify CUDA kernels. To do so, we need to: 1) encode CUDA semantics, in particular, barrier synchronization, \n2) en\u00adcode the properties that we wish to check, 3) describe the prop\u00aderty variables VF and con.guration \nvariables Y , 4) compute the set FlowsTo(P, VF). Barriers CUDA includes a special barrier operation. \nIntuitively, when a thread reaches a barrier, it waits until all the other threads have reached the same \nbarrier. We encode barriers by introducing two special variables flag, a local variable that has the \nvalue 1 iff the thread tid has reached a barrier, and count, a global variable (initialized to 0) which \nholds the number of threads that have reached the barrier. Now, a barrier operation between l and l. \nis a sequence of three CFG edges: C (l, lwait)= \u00b7 assume(flag = 0); flag = 1; count = count +1 C (lwait,lgo)= \n\u00b7 assume(count = blockDim) C (lgo,l.)= \u00b7 assume(threadIdx = count); flag = 0; count = count - 1 where \nlwait,lgo are two distinct program locations introduced for each barrier operation. Instrumentation: \nRecording Global Accesses To verify that the kernel is deterministic, we need to check in the trace that \nthe set of global addresses written by a thread is disjoint from the addresses accessed by other threads. \nWe need only perform this check on accesses within the same barrier interval because two accesses separated \nby a barrier cannot race [23]. Thus, we track both the set of addresses read or written by each thread, \nas well as timestamps uniquely identifying each barrier interval. We instrument the kernel to track this \ninformation as follows. First, we introduce a special local variable timestamp (initialized to 0) which \nholds a logical timestamp for the current barrier interval. We instrument each barrier-wait assume(count \n= blockDim) to become assume(count = blockDim); timestamp = timestamp +1 We introduce a global array \nlog, that maps T id, to subsets of N\u00d7{Rd, Wr}\u00d7 G \u00d7 N That is, each tuple consists of a barrier timestamp, \naccess type, global array name, and array index, respectively. Next, we instru\u00adment each read and write \ncommand to record the access inside log. Global-reads y=g[x] become y=g[x]; log[threadIdx] = log[threadIdx] \n. (timestamp, Rd, g , x) and global-writes g[x] =y become log[threadIdx] = log[threadIdx] . (timestamp, \nWr, g , x); g[x] =y Property: Determinism Finally, to verify determinism we de.ne the following property \nFDet. .t,i,i, g,n :(t, A, g ,n) . log[i] . (t,A, g ,n) . log[i] . i = i. . A = A. = Rd Intuitively, the \nproperty checks that within each barrier interval, the set of writes in the log of thread tid is disjoint \nfrom the set of accesses contained in the log of another thread tid.. As the instrumentation ensures \nthat log contains all accesses, if the above sets are disjoint, then the sets of accesses of every two \nthreads are disjoint, and the kernel is therefore deterministic [35]. Property and Con.guration Variables \nThe set of property vari\u00adables is the singleton {log} comprising the only variable inspected by FDet. \nThe con.guration variables are those that describe the thread geometry of the given kernel, namely the \nnumber of threads blockDim, the thread identi.er threadIdx, and algorithm-and architecture-speci.c parameters \nlike the sizes of strides accumN and the global arrays sizeN. In general, the set of con.guration variables \ndepends heavily on the semantics of the kernel being an\u00adalyzed and must be chosen based on domain knowledge. \nStatic Information Flow Analysis The last piece of the puzzle is the .ow relation. Needless to say, the \nexact .ows-to set for a set of property variables is not computable. We solve this problem, through a \nforwards taint-propagation based analysis that, given a kernel, a set of property variables VF, and a \nset of con.guration variables Y , checks whether FlowsTo(P, VF) . Y . The analysis implements a form \nof taint-based integrity checking [15, 27]. We formalize the analysis as procedure StaticFlowsTo(P, VF,Y \n), which is implemented as follows. All the input variables except those in Y are tainted. The taints \nare propagated through assignments (data dependencies) and dominating assumes (control dependencies). \nFinally, we check whether any of the property variables VF are tainted. If not, we can be sure that only \nthe con.guration variables .ow to the property variables. As the procedure is .ow-insensitive, it is \nsound in the face of multi-threading. Furthermore, as the kernels are branch deterministic, we can conclude \nthat the statically computed .ows-to relation is a conservative overapproximation of the actual .ows-to \nrelation. PROPOSITION 1. [Static Flows-To] For all branch-deterministic kernels P and variables VF,Y \n, if StaticFlowsTo(P, VF,Y ) then FlowsTo(P, VF) . Y . Access Invariance and Determinism We say that \na kernel P is access invariant with respect to Y if StaticFlowsTo(P, {log},Y ), that is, if only the \ncon.guration variables Y .ow to the property variables. From the soundness of static .ows-to, we get \nthe follow\u00ading as an immediate corollary of Theorem 1. PROPOSITION 2. [CUDA Determinism] Let P be a CUDA \nker\u00adnel with con.guration variables Y . If P is access invariant with respect to Y  t . Traces(P ) \nis such that t |=FDet  then forall t. . Traces(P ), if t (0) =Y t .(0) then t . |=FDet. Returning to \nthe scalarProd example from Figure 1 we see that log has a data dependence on the indices j, i, i and \nstride + i which are used to access shared memory (at lines 7, 8, 12 and 12 respectively). The only inputs \nthat .ow to these expres\u00adsions (i.e., the set of property-integrity inputs) are the parameters threadIdx, \naccumN, sizeN, and blockDim, which are a subset of the con.guration variables, so the kernel is access \ninvariant. Thus, a single trace that satis.es FDet suf.ces to show that the kernel satis.es FDet for \nall executions over the same con.guration. 4. Implementation We have implemented the analyses described \nin the previous sec\u00adtion in the LLVM Compiler Infrastructure [22]. In particular, we have implemented \na static taint-based information .ow analysis that checks whether CUDA kernels are access invariant, \nand a dy\u00adnamic instrumentation-based analysis that checks whether a kernel satis.es the determinism property \nFDet for a particular execution. Though the formalism in the previous section presented a simpli\u00ad.ed, \ntwo-level memory model and one-dimensional thread geom\u00adetry, our implementation handles the multi-dimensional \nthread ge\u00adometries and three-layer memory hierarchy supported by CUDA. Next, we describe the static and \ndynamic analyses in greater detail.  4.1 Static Analysis The static analysis proceeds via three LLVM \npasses. The .rst pass recursively inlines all function calls within a kernel, yielding a single call-free \nkernel. The second pass is a .ow-insensitive intraprocedural pointer analysis based on Andersen s algorithm \n[1] that is used by the third pass, a static taint-based analysis that determines whether data inputs \n(that is, non-con.guration input variables) .ow to the property variables. 1. Kernel inlining For simplicity, \nour current implementation ex\u00adploits the key restriction that CUDA kernels cannot be recursive or contain \nindirect functions calls. In particular, we fully inline func\u00adtions called from kernels, so subsequent \nanalyses can be intraproce\u00addural without loss of precision, with one exception to deal with li\u00adbrary \nfunctions we analyze calls to CUDA library functions using specially crafted transfer functions (i.e., \nsummaries) as described below. (Recent NVIDIA cards do support recursion and function pointers. To handle \nsuch kernels we need only use interprocedural variants of our current analyses.) 2. Pointer analysis \nWe use a pointer analysis to soundly prop\u00adagate taints in the presence of aliasing. In particular, if \na pointer refers to a tainted heap location, then the taints must be propa\u00adgated to (dereferences of) \nall the aliases of the pointer. To this end we have implemented a .ow-insensitive, intraprocedural variant \nof Andersen s algorithm. The precision lost by .ow-insensitivity is mitigated by the fact that the LLVM \nintermediate representation is based on SSA, so the may-point-to set of most variables contain only a \nsingle element. In the CUDA setting, the following modi\u00ad.cations allow us to further improve the precision \nof the pointer analysis.  Our aggressive inlining step makes the analysis context sensi\u00adtive. We inline \nall function calls with the exception of calls to CUDA library functions. To deal with those calls we \ninclude special transfer functions which account for the function s ef\u00adfects as speci.ed in the documentation. \n Kernel formal parameters with pointer type are optimistically assumed to be unique upon entry to the \nfunction. That is, there are no other variables that refer to a block of memory pointed to by a formal \nparameter. Although this assumption is unsound in general, we have manually veri.ed that it holds for \nall bench\u00admarks in our evaluation. This simpli.cation allows us to avoid making the conservative assumption \nthat pointer function pa\u00adrameters may point to arbitrary memory locations.  3. Taint tracking The direct \nway to check the access invari\u00adance property is to implement StaticFlowsTo(P, {log},Y ) inside LLVM. \nUnfortunately, this would require us to rewrite the program to add log which we avoid for the reasons \ndescribed in Section 4.2. Instead, we check that the access invariance property holds by stat\u00adically \nverifying that the following two conditions hold. First, no address operand of a memory instruction may \nbe affected by a data input. In other words the .ows-to set of an address may contain only con.guration \ninputs. Second, no memory instruction is control de\u00adpendent on a tainted value. The latter is subtle \nand best explained by a small example: if(taintedVar) { A[i] = e; } else { B[i] = e; } Whether array \nA or B is written depends on the value of taintedVar, even though the array index i itself might not \nbe tainted at all. Thus, with a different data input, the program may execute the other branch yielding \na race (that does not occur on the test being ampli.ed.) This condition is conservative, as it would \n.ag a violation even when both sides of the branch perform identi\u00adcal accesses. (It is easy to check \nthat these conditions are equivalent to the direct approach of checking whether non-con.guration in\u00adput \nvariables .ow-to the log.) Our implementation veri.es that both conditions are met using a standard worklist \nalgorithm to perform forward taint propagation. We begin with an initial taint set consisting of the \nkernel s data in\u00adputs, then track the propagation of these taints through instructions. During propagation, \nif either condition is found to be violated, we terminate with the conclusion that the kernel is not \naccess invari\u00adant. If instead we reach a .xpoint in which no new taints can be propagated and have not \nyet found a violation, we conclude that the kernel is access invariant. There are two channels by which \ntaint can propagate: explicit .ow resulting from data dependences, and implicit .ow resulting from control \ndependences. Explicit .ow is handled in the straight\u00adforward way: we implement transfer functions for \neach class of instruction or CUDA library function to assign taint to the appro\u00adpriate variables. For \ninstance, we assign taint to the result of any arithmetic instruction with a tainted operand. We handle \ncontrol dependences very conservatively by immediately .agging a viola\u00adtion of the access invariance \nproperty whenever a branch condition variable is tainted. This would .ag an access invariance violation \neven if neither side of the branch performed any memory access at all, but is a straightforward and sound \noverapproximation. If no such control dependences exist, we can conclude that the second condition holds, \nand that no control dependences contribute to a violation of the .rst condition. Finally, we must verify \nthe .rst condition that no addresses are tainted. LLVM programs calculate addresses with an explicit \ninstruction called getelementptr that takes a base pointer and a variable number of operands representing \noffsets from the base. This instruction is designed to express arbitrary address arithmetic to compute \nthe addresses of data structure elements. Whilst propa\u00adgating taints, if we ever encounter a getelementptr \nwith tainted operand, we conclude that the kernel is not access invariant.  4.2 Dynamic Analysis The \ndynamic component of our analysis checks that a particular test execution satis.es the given property \nFDet. Unfortunately, due to the sheer number of threads and frequency of accesses, it is impossible to \nhold the log in memory and check FDet in an online manner. We get around this problem by logging all \nthe accesses on disk and then checking of.ine that the trace satis.es FDet. Thus, our dynamic analysis \nconsists of a single LLVM pass that instruments all memory accesses with extra instructions to log ad\u00addresses \nand thread identi.ers on disk. In order to distinguish ac\u00adcesses to global and shared data structures, \nall allocations and static array declarations for global and shared memory are instrumented to record \nbase address and size. Finally, full 3D thread identi.ers and 2D block identi.ers are logged for each \nexecuting thread. Barrier Interval Timestamps Recall that a problematic trace is one where different \nthreads access the same shared location dur\u00ading the same barrier interval. Thus, in addition to the address \nand thread identi.ers, we record for each access, the barrier interval number during which the access \noccurs, so that the of.ine check only compares accesses within the same interval. Although ad\u00addresses \nand thread identi.ers are readily accessible as variables in the uninstrumented code, barrier intervals \nare not explicitly labeled and must be dealt with specially. We maintain a shared table that contains \none entry per thread to track the current barrier interval of each thread. Each element of the table \ncorresponds to one thread s timestamp local variable as described in Section 3.4. Every call to  syncthreads() \nis replaced with an augmented version that ad\u00additionally increments the thread s corresponding table \nentry. Thus, at any point during execution, the table contains a snapshot of the barrier interval in \nwhich each thread is executing. We simply look up the thread s entry in this table to determine the current \nbarrier interval for logging. (The threads obey the barrier synchronization semantics; the intervals \nare logged solely to facilitate of.ine trace analysis.) Determinism via Race Detection After generating \na complete log by executing the instrumented kernel, we check that the trace satis.es the determinism \nproperty FDet. In the presence of the three-level memory hierarchy, the property is satis.ed if there \nare no global-or shared-memory races. A global memory race occurs when a global data structure is written \nby one thread and accessed by another. Two threads in the same thread block may race only if executing \nin the same barrier interval. Two threads in different thread blocks cannot synchronize, so may race \nregardless of barrier interval. A shared memory race occurs when two threads within the same thread block \naccess a shared data structure within the same barrier interval, and at least one of the accesses is \na write. Threads in different thread blocks cannot share the same shared data structure, and thus cannot \ncon.ict. Of.ine Trace Analysis The sheer magnitude of the logs required that we devise non-trivial means \nfor checking for races using a small memory footprint. Our implementation performs this check via a log \npostprocessing program that .rst translates raw addresses into array-offset pairs, then performs a sweep \nof the resulting log to check for con.icting accesses to each array. To perform the race checks ef.ciently, \nwe .rst separate global and shared memory accesses into separate traces. Each trace is then externally \nsorted by address and barrier interval, which are the primary and secondary sort keys, respectively. \nFinally, a single linear sweep through the sorted traces checks for the aforementioned race conditions. \nThe above process only stores in memory a small window of the trace at any point: for global arrays, \nonly those accesses to a single address at a time, and for shared arrays, only those accesses to a single \naddress within a single barrier interval. These optimizations were essential in practice. Our initial \nna\u00a8ive implementation, which attempted to detect races in unsorted traces, immediately ran into out-of-memory \nerrors due to the size of the traces. Limitations Our implementation does not currently handle sev\u00aderal \naspects of CUDA. First, the dynamic analyses are incompatible with programs using OpenGL because CUDA \nemulation does not support compilation of such programs. We resort to using CUDA emulation because we \ncannot perform logging when executing on real GPU hardware. Second, our analyses do not support code \nthat uses the CUDA Driver API, a low-level framework above which the CUDA language and runtime are built, \nnor code that contains assembly instructions, atomic intrinsics, or warp voting functions. Third, our \ndynamic analysis does not track accesses to texture or constant memory, as they are strictly read-only \nduring kernel ex\u00adecution and thus are not subject to races within the kernel. A .\u00adnal caveat is that \nwe assume that con.icting accesses must be to the same exact address our implementation has no notion \nof con\u00ad.ict between unaligned accesses, though presumably a higher-level type system could ensure the \nabsence of such misaligned accesses. 5. Evaluation Our goal is to evaluate the overall effectiveness \nof our approach in the setting of CUDA programs. To this end, we evaluate the effec\u00adtiveness of our static \ninvariance analysis (Section 5.2), the effective\u00ad ness of our dynamic analysis (Section 5.3), and the \nrunning-time of our approach (Section 5.4). Before presenting these results, we .rst describe our experimental \nsetup (Section 5.1). 5.1 Experimental Setup To evaluate our approach, we started with the 68 benchmarks \nin the NVIDIA CUDA SDK Version 3.0. Of these 68 benchmarks, 7 are small tutorial-like benchmarks with \ntrivial kernels, and so we leave these out. Of the remaining benchmarks, 33 contain features which our \nanalysis cannot run on, due to the limitations previously men\u00adtioned. We therefore omitted these as well, \nwhich leaves us with 28 benchmarks, for which we report results. These 28 benchmarks contain a total \nof 76 kernels. The static analysis runs on one kernel at a time. We manually examined each kernel to \ndetermine the correct set of data inputs (non-con.guration input variables) to supply the static analysis. \nWe tried to minimize the set of con.guration variables by manually examining code to determine the domain-speci.c \ninterpretation of each input. The separation of data and con.guration variables is usually quite immediate. \nAs an example, for the scalarProd benchmark, the set of data input variables is dC, dA, and dB (since \nthese are the input vectors). Since sizeN represents the size of the data set, we consider it to be a \ncon.guration variable. For the dynamic analysis, we run each benchmark containing access invariant kernels \nwith instrumentation turned on. Bench\u00admarks with multiple kernels will produce logs containing entries \nfrom several kernels. A postprocessing step separates the entries corresponding to each kernel. The logs \ncan then be processed one kernel at a time.  5.2 Static Invariance Analysis The results of our static \ninvariance analysis are shown in Figure 4. Of the 76 kernels examined, 52 were shown to be access invariant \nby our static information .ow analysis. These kernels are spread across a variety of benchmarks from \ndifferent domains. Addition\u00adally, even benchmarks that demonstrated variance in some kernels often have \nat least one kernel that is in fact access invariant. For in\u00adstance, although histogram contains 2 kernels \nthat are not access invariant, it contains 2 that are. This aligns with the fact that CUDA algorithms \nare often implemented as multiple distinct kernels, each performing a different phase of the algorithm, \nsome of which may be access invariant while others are not. We have found three distinct patterns that \ncause a kernel to be access variant. For each pattern we provide an example from the benchmark suite. \nDirect Flow The .rst pattern is the most direct: a memory ac\u00adcess address is derived directly from an \ninput value. This pattern is demonstrated by the histogram benchmark, which bins each byte of its input \nbased on the value of its higher order bits. A straightfor\u00adward sequential implementation of histogram \nis sketched in Figure 5. This implementation iterates over each element of data, uses the higher order \nbits to index into the array bin whose elements rep\u00adresent bin counts, then increments the indexed element. \nAlthough the parallel CUDA implementation is complicated by distribution of computation across multiple \nthreads, the fundamental calcula\u00adtion is the same: the higher order bits of each byte are used to ad\u00address \ninto a shared array which contains the bin counts. The access variance of this kernel is a result of \nthe fact that the bin address is derived directly from an input value.  Benchmark LOC #K %I reduction \n695 7 100% sortingNetworks* 571 6 100% convolutionFFT2D 384 3 100% fastWalshTransform 243 3 100% convolutionSeparable \n318 2 100% convolutionTexture 314 2 100% cppIntegration 125 2 100% simplePitchLinearTexture 138 2 100% \ntranspose 146 2 100% binomialOptions* 354 1 100% BlackScholes* 276 1 100% dwtHaar1D 266 1 100% FDTD3d \n818 1 100% matrixMul 216 1 100% scalarProd 136 1 100% simpleCUFFT 137 1 100% simpleTexture 121 1 100% \nvectorAdd 92 1 100% dct8x8 1402 8 87% histogram 431 4 50% lineOfSight 180 2 50% radixSort 1894 10 30% \nMonteCarlo 836 4 25% eigenvalues 1901 4 0% MersenneTwister 287 2 0% quasiRandomGenerator 637 2 0% clock \n69 1 0% dxtc 829 1 0% 28 Benchmarks 13816 76 68% Figure 4. Static Analysis Results: LOC=Lines of Code, \n#K=Number of Kernels, %I=Percent Invariant Kernels, *modi.ed as detailed in Section 5.2 1: void histogram(char \n*data, int *bin, int size) { 2: for(int i = 0, i < size, i++) 3: bin[data[i] &#38;&#38; 0x3FU]++; 4: \n} Figure 5. Sequential Histogram Indirect Flow The second pattern occurs in kernels in which a critical \ncontrol .ow statement depends on a condition derived from input. This is demonstrated by the eigenvalues \nbenchmark. This benchmark calculates the eigenvalues of a matrix with an algorithm that subdivides intervals \ndependent on the values of the input matrix. This dependence is fundamental to the algorithm and cannot \nbe removed. Correctable Indirect Flow The sortingNetworks benchmark performs a bitonic sort, which is \na sorting algorithm with the property that the comparisons it performs are independent of its input values. \nWhether elements are swapped, however, depends on the result of a comparison which ultimately derives \nfrom the magnitude of input values. Intuitively, it would seem that the kernel is almost access invariant, \nsave for this conditional swap. It turns out that we can perform a simple rewrite, as shown in Figure \n6, which effectively converts the control dependence into a data dependence. With this rewrite, sortingNetworks \ncan be veri.ed access invariant because we have removed a control dependence without introducing taint \nto the array indices. We have been able to perform similar transformations to the binomialOptions and \nBlackScholes kernels. /** original version **/ if(A[i] > A[j]) { t = A[i]; A[i] = A[j]; A[j] = t; } /** \ntransformed version **/ bool cond = (A[i] > A[j]); t = A[i]; A[i] = cond * A[j] + (!cond) * A[i]; A[j] \n= cond * t + (!cond) * A[j]; Figure 6. Removing a control dependence Con.guration Variable Selection \nThe difference between a data input and a con.guration variable is a semantic distinction that ultimately \ndepends on the domain-speci.c meaning of each input to the kernel it cannot be fully automated. In our \nexperiments, we have manually examined kernels to extract this distinction. However, the difference between \na property-integrity input, and one which is not, is based on which inputs in.uence the access in\u00advariance \nproperty and which do not. This classi.cation can be auto\u00admated with our invariance analysis. We simply \nrun our invariance analysis on each input in turn, and determine which inputs cause our analysis to report \na violation of the access invariance property. The end result is that we have found a minimal set of \nproperty\u00adintegrity inputs. For all kernels we have classi.ed as access invariant, we would expect our \nset of manually chosen con.guration variables to con\u00adtain the set of property-integrity inputs. In addition, \nwe would ex\u00adpect that for all variant kernels, at least one data input is in fact a property-integrity \ninput. We have veri.ed that both are true. A .nal observation is that in cases where the set of property\u00adintegrity \ninputs is smaller than the set of chosen con.guration vari\u00adables, we can amplify our guarantees to an \neven larger set of inputs. We found this to be the case for convolutionSeparable, which contains two \nkernels, one which performs an image transform on the rows of the image, and another which acts on the \ncolumns. Both kernels take parameters imageW and imageH which indicate the height and width of the image \nbeing transformed, but the row\u00adand column-transforming kernels are only variant with respect to the width \nand height parameters, respectively. Thus, although we manually speci.ed both parameters to be con.guration \ninputs, in fact we could have removed one and still maintained the access in\u00advariance property. In total, \nwe have found that for 11 of the kernels the set of property-integrity inputs is in fact a subset of \nthe set of con.guration variables.  5.3 Dynamic Analysis Our dynamic analysis found that most of the \ndynamic runs were race-free and deterministic. However, our analysis detected a race violation in the \nfull implementation of scalarProd. The race was determined to be a result of a missing syncthreads() \ncall after a complete dot product had been calculated and stored into the output array, which corresponds \nto line 13 of Figure 1. Without this barrier, thread 0 might still be reading values from the accumResult \narray while its neighbors had already begun calculation on the next pair of vectors, potentially performing \na destructive update before the previous result had been completed. In practice, however, this race will \nnot manifest itself due to the warp-synchronous nature of CUDA s execution model: neigh\u00adboring threads \nare logically grouped into warps of 32 threads that execute synchronously on hardware. Thus, because \nthreads 0 and 1 belong to the same warp, they will not exercise the race described.  Races were also \ndetected in two other benchmarks, histogram and reduction. In both cases, the benchmarks once again relied \non warp-synchronous execution to prevent races in the absence of explicit barrier synchronization. It \nis important to note, however, that warp-synchronous execution relies on the behavior of underly\u00ading \nhardware, which may change for future generations of GPUs or different target architectures, and cause \nthese to become real errors.  5.4 Performance Evaluation The running time of our static analysis was \nless than one second for most benchmarks, with none taking more than 10 seconds with two notable exceptions: \nradixSort which took 36m, and reduction which took 3.5m. The reason is that these benchmarks makes heavy \nuse of C++-style templates. When compiled, each template special\u00adizes into multiple kernels, each of \nwhich is analyzed. radixSort instantiates 271 kernels, and our analysis takes an average of 8 sec\u00adonds \nper instantiated kernel. Our analysis takes an average 1.6s on each of the 132 kernels instantiated by \nreduction. To evaluate the performance penalty of our dynamic instru\u00admentation and race detection, we \nmeasured running times for each benchmark with and without the analysis enabled. The geometric mean and \nmedian slowdown were 18X and 12X, respectively, while the total slowdown, measured by summing the running \ntimes of all benchmarks, was 310X. There were two benchmarks, MonteCarlo and convolutionSeparable, that \nhad running times more than an order of magnitude higher than the others, at 2093X and 1324X, due to \nthe size of their logs: convolutionSeparable in partic\u00adular, produced a 508 GB log. We have found that \nslowdowns are highly correlated with the size of the memory traces, leading us to believe that the bottleneck \nin performance is disk I/O for writing log entries. This could be alieviated through the use of more \nso\u00adphisticated buffering techniques, but we leave this for future work. 6. Related Work In this section \nwe discuss the literature related to our work, which fall under two broad categories: race-freedom and \ndeterminism; and test ampli.cation. The literature on enforcing race-freedom and determinism on shared \nmemory multithreaded programs is enor\u00admous: we limit ourselves to work that studies problems similar \nto the domain of GPU programs, with massive, .ne-grained data\u00adsharing. Because test ampli.cation is such \na broad concept, it has appeared in many different forms in the literature. We present an indicative, \nalbeit necessarily incomplete selection of work repre\u00adsentative of the breadth of its application. Race-Freedom \nand Determinism One closely related piece of work by Boyer et al. presents a dynamic analysis for detecting \nraces in CUDA programs [4]. The work uses an instrumentation much like our own, to generate logs that \ncan be used to dynamically detect races and barrier con.icts. Of course, without ampli.cation, the results \nonly hold for one run at a time, which is not likely to be very useful as the instrumentation imposes \nlarge overheads. Our work is greatly inspired by PUG [23], a tool that analyzes CUDA programs via the \nmachinery of symbolic execution. PUG logically encodes the program executions and uses an SMT solver \nto check whether different threads can race within a barrier inter\u00adval. PUG has several optimizations \nthat mitigate the explosion in thread interleavings. One of these is that it is limited to checking a \nprogram with just two threads. Similarly, Vechev et al. [35] use numeric abstract interpretation to compute \nthe sets of indices used to access shared arrays. As both these techniques are fully static, they are \nlimited to programs for which it can infer suitable sim\u00adple (linear) loop invariants, which precludes \nusage on many CUDA kernels which often have complex, non-linear loop invariants com\u00adputed with modular \narithmetic and bitwise operations. There is an enormous literature on dynamic and static race detection \nfor general concurrent programs. Two classical examples include work by Dinning and Schonberg [8], and \nSavage et al. [32] that respectively pioneered the use of static and dynamic locksets in order to check \nfor the absence of races. Several authors have built upon that line of work using types [11], data.ow \nanalysis [10], and even greatly extended it to checking for higher level properties like atomicity [12, \n13] and determinism [31]. Unfortunately, these methods don t apply in our setting as CUDA eschews lock-based \nsynchronization. Several authors have proposed dynamic mechanisms for enforc\u00ading determinism by constraining \nthe scheduler. These include tech\u00adniques that modify the language run-time [3, 25] or the OS [2]. Due \nto the run-time overheads, these methods are unlikely to ap\u00adply in the GPU setting, where programs exhibit \na high degree of .ne-grained sharing, for example to maximize memory bandwidth. The general idea of non-interference \n[18], in which static infor\u00ad mation .ow has it roots, has been proposed as a means of formal\u00adizing correctness \nproperties of multithreaded programs [9]. Previ\u00ad ous work has also studied the connection between information \n.ow and concurrency. In particular, Zdancewic and Myers [37] describe the notion of observational determinism \nwherein a program is se\u00adcure iff its observable behaviors are independent of secure inputs and scheduling, \nand Terauchi [34] describes a type system for ob\u00ad servational determinism. Our novel contribution is \nto demonstrate how of static information .ow can be used to amplify the results of a single execution \nacross all runs over the same con.guration, thereby establishing the property for arbitrary thread interleavings \nand data values, and to empirically demonstrate the effectiveness of this technique for verifying CUDA \nprograms. Test Ampli.cation Recent work on dynamic test generation has employed test ampli.cation with \nthe aim of increasing code cov\u00aderage [5, 6, 16, 33]. Collectively dubbed concolic testing, these techniques \nuse dynamic symbolic execution to collect logical con\u00adstraints representing input-dependent control .ow \nconditions, then solve the constraints to obtain inputs that steer execution along dif\u00adferent paths. \nAt a high level, all these techniques attempt to max\u00adimize code coverage by avoiding the generation of \ntest inputs that redundantly follow the same paths. In this context, the identi.ca\u00adtion of equivalence \nclasses of inputs that exercise the same paths may be viewed as an instance of test ampli.cation: a single \ninput is ampli.ed to represent its equivalence class of inputs. Another form of test ampli.cation has \nalso been explored ex\u00adtensively in work on runtime fault monitoring [17, 19, 21]. By pig\u00ad gybacking symbolic \nexecution on top of a dynamic run, these tech\u00adniques are able to detect property violations across a \nmuch larger space of inputs than those exercised by a particular concrete exe\u00adcution. This method has \nbeen used to .nd buffer over.ows [21], generalized to predict more generic property violations [19], \nand adapted to generate concrete test inputs that exercise those viola\u00adtions [17]. Because these techniques \nonly check properties along the tested execution path, they rely on exhaustive path search to verify \nproperties across all executions. The described dynamic test generation and runtime monitoring techniques \nfall under what we call symbolic-execution-based test ampli.cation, an umbrella term for techniques that \nemploy sym\u00adbolic execution to learn additional information from a concrete exe\u00adcution. Flow-based test \nampli.cation is potentially less precise than symbolic execution, but is also much cheaper, and as demonstrated \nby our evaluation, quite effective in the CUDA setting. Recent work on verifying memory-safety of programs \nwith .oating-point com\u00adputation [15] has even combined dynamic symbolic execution with a static .ow analysis \nwhich establishes that .oating-point values do not interfere with non-.oating point values, thereby allowing \nampli.cation of symbolic execution guarantees over .oating point computations. The effectiveness of .ow-based \ntest ampli.cation stems from the fact that programs in the domain isolate the data val\u00adues being operated \non (e.g. input arrays or .oating point numbers) from those used to determine control-.ow and memory accesses. \n Finally, test ampli.cation has also appeared in work on the anal\u00adysis of general concurrent programs. \nWang and Stoller [36] de\u00ad scribe two dynamic analyses for detecting atomicity violations in concurrent \nJava programs: one that employs Lipton s theory of re\u00adduction [24] to reason about commutativity of concurrent \nevents to infer atomicity, and another that searches for unserializable permu\u00adtations of events to detect \natomicity violations. Both analyses use test ampli.cation in the sense that they examine a single execution \nbut are capable of detecting violations occurring in other possible interleavings. A different instance \nof test ampli.cation appears in work by Chen and Ros\u00b8u [7], which proposes a relaxation of the classical \nhappen-before causality partial order [14, 20] to admit a greater number of compatible interleavings. \nThis relaxation ampli\u00ad.es the partial order to a larger space of possible executions, thus opening subsequent \nanalyses to detect property violations in more interleavings. A key contribution of our work is to demonstrate \nthat enumerating interleavings is simply unnecessary for many CUDA kernels: we can apply static information \n.ow analysis to generalize our determinism guarantee to a massive space of interleavings from a single \nexecution. Acknowledgements We wish to thank Mingxun Wang for his part in many fruitful discussions and \nhis help on initial feasibility experiments. We are grateful to our shepherd, Patrice Godefroid, and \nthe anonymous reviewers for their comments and feedback for improving the paper. References [1] L. Andersen. \nProgram analysis and specialization for the C programming language. PhD thesis, 1994. [2] A. Aviram, \nS.-C. Weng, S. Hu, and B. Ford. Ef.cient system-enforced deterministic parallelism. In OSDI, pages 193 \n206, 2010. [3] T. Bergan, O. Anderson, J. Devietti, L. Ceze, and D. Grossman. Core-Det: a compiler and \nruntime system for deterministic multithreaded execution. In ASPLOS, pages 53 64, 2010. [4] M. Boyer, \nK. Skadron, and W. Weimer. Automated dynamic analysis of CUDA programs. In STMC, 2008. [5] C. Cadar, \nV. Ganesh, P. M. Pawlowski, D. L. Dill, and D. R. Engler. EXE: automatically generating inputs of death. \nIn CCS, pages 322 335, 2006. [6] C. Cadar, P. Godefroid, S. Khurshid, C. S. P.areanu, K. Sen, as. N. \nTillmann, and W. Visser. Symbolic execution for software testing in practice: preliminary assessment. \nIn ICSE, pages 1066 1071, 2011. [7] F. Chen and G. Ros\u00b8u. Parametric and sliced causality. In CAV, pages \n240 253, 2007. [8] A. Dinning and E. Schonberg. Detecting access anomalies in programs with critical \nsections. In Workshop on Parallel and Distributed Debugging, pages 85 96, 1991. [9] P. A. Emrath and \nD. A. Padua. Automatic detection of nondetermi\u00adnacy in parallel programs. In PADD, pages 89 99, 1988. \n[10] D. Engler and K. Ashcraft. RacerX: Effective, static detection of race conditions and deadlocks. \nIn SOSP, pages 237 252, 2003. [11] C. Flanagan and S. Freund. Type-based race detection for Java. In \nPLDI, pages 219 232, 2000. [12] C. Flanagan and S. N. Freund. Atomizer: a dynamic atomicity checker for \nmultithreaded programs. In POPL, pages 256 267, 2004. [13] C. Flanagan and S. Qadeer. A type and effect \nsystem for atomicity. In PLDI, pages 191 202, 2003. [14] P. Godefroid. Partial-Order Methods for the \nVeri.cation of Concurrent Systems: An Approach to the State-Explosion Problem. Springer-Verlag New York, \nInc., Secaucus, NJ, USA, 1996. [15] P. Godefroid and J. Kinder. Proving memory safety of .oating-point \ncomputations by combining static and dynamic program analysis. In ISSTA, pages 1 12, 2010. [16] P. Godefroid, \nN. Klarlund, and K. Sen. DART: directed automated random testing. In PLDI, pages 213 223, 2005. [17] \nP. Godefroid, M. Y. Levin, and D. A. Molnar. Active property checking. In EMSOFT, pages 207 216, 2008. \n[18] J. A. Goguen and J. Meseguer. Security policies and security models. IEEE Symposium on Security \nand Privacy, pages 11 20, 1982. [19] P. Joshi, K. Sen, and M. Shlimovich. Predictive testing: amplifying \nthe effectiveness of software testing. In ESEC/FSE, pages 561 564, 2007. [20] L. Lamport. Time, clocks, \nand the ordering of events in a distributed system. Commun. ACM, 21(7):558 565, 1978. [21] E. Larson \nand T. Austin. High coverage detection of input-related security faults. In USENIX Security, 2003. [22] \nC. Lattner and V. Adve. LLVM: A compilation framework for lifelong program analysis &#38; transformation. \nIn CGO, 2004. [23] G. Li and G. Gopalakrishnan. Scalable smt-based veri.cation of gpu kernel functions. \nIn FSE, pages 187 196, 2010. [24] R. J. Lipton. Reduction: a method of proving properties of parallel \nprograms. Commun. ACM, 18:717 721, December 1975. [25] T. Liu, C. Curtsinger, and E. D. Berger. Dthreads: \nEf.cient deterministic multithreading. In SOSP, pages 327 336, 2011. [26] F. Masdupuy. Semantic analysis \nof interval congruences. In Formal Methods in Programming and Their Applications, LNCS 735. 1993. [27] \nA. Myers. JFlow: Practical mostly-static information .ow control. In POPL, pages 228 241, 1999. [28] \nNVIDIA. CUDA toolkit 3.0. http://developer.nvidia.com/ cuda-toolkit-30-downloads, 2010. [29] NVIDIA. \nCUDA accelerated applications. http://www.nvidia. com/object/cuda_app_tesla.html, 2011. [30] M. Raza, \nC. Calcagno, and P. Gardner. Automatic parallelization with separation logic. In ESOP, pages 348 362, \n2009. [31] C. Sadowski, S. N. Freund, and C. Flanagan. SingleTrack: A dynamic determinism checker for \nmultithreaded programs. In ESOP, pages 394 409, 2009. [32] S. Savage, M. Burrows, G. Nelson, P. Sobalvarro, \nand T. Anderson. Eraser: A dynamic data race detector for multi-threaded programs. ACM Transactions on \nComputer Systems, 15:391 411, 1997. [33] K. Sen, D. Marinov, and G. Agha. CUTE: a concolic unit testing \nengine for c. In ESEC/FSE, pages 263 272, 2005. [34] T. Terauchi. A type system for observational determinism. \nIn CSF, pages 287 300, 2008. [35] M. T. Vechev, E. Yahav, R. Raman, and V. Sarkar. Automatic veri.cation \nof determinism for structured parallel programs. In SAS, pages 455 471, 2010. [36] L. Wang and S. D. \nStoller. Runtime analysis of atomicity for multithreaded programs. IEEE Trans. Softw. Eng., 32:93 110, \nFebruary 2006. [37] S. Zdancewic and A. C. Myers. Observational determinism for concurrent program security. \nIn CSFW, pages 29 43, 2003.    \n\t\t\t", "proc_id": "2254064", "abstract": "<p>We present a novel technique for verifying properties of data parallel GPU programs via test <i>amplification</i>. The key insight behind our work is that we can use the technique of static information flow to amplify the result of a single test execution over the set of all inputs and interleavings that affect the property being verified. We empirically demonstrate the effectiveness of test amplification for verifying race-freedom and determinism over a large number of standard GPU kernels, by showing that the result of verifying a single dynamic execution can be amplified over the massive space of possible data inputs and thread interleavings.</p>", "authors": [{"name": "Alan Leung", "author_profile_id": "81442598931", "affiliation": "University of California, San Diego, La Jolla, CA, USA", "person_id": "P3471267", "email_address": "aleung@cs.ucsd.edu", "orcid_id": ""}, {"name": "Manish Gupta", "author_profile_id": "81502763005", "affiliation": "University of California, San Diego, La Jolla, CA, USA", "person_id": "P3471268", "email_address": "manishg@cs.ucsd.edu", "orcid_id": ""}, {"name": "Yuvraj Agarwal", "author_profile_id": "81310499248", "affiliation": "University of California, San Diego, La Jolla, CA, USA", "person_id": "P3471269", "email_address": "yuvraj@cs.ucsd.edu", "orcid_id": ""}, {"name": "Rajesh Gupta", "author_profile_id": "81408599639", "affiliation": "University of California, San Diego, La Jolla, CA, USA", "person_id": "P3471270", "email_address": "gupta@cs.ucsd.edu", "orcid_id": ""}, {"name": "Ranjit Jhala", "author_profile_id": "81100198278", "affiliation": "University of California, San Diego, La Jolla, CA, USA", "person_id": "P3471271", "email_address": "jhala@cs.ucsd.edu", "orcid_id": ""}, {"name": "Sorin Lerner", "author_profile_id": "81100399150", "affiliation": "University of California, San Diego, La Jolla, CA, USA", "person_id": "P3471272", "email_address": "lerner@cs.ucsd.edu", "orcid_id": ""}], "doi_number": "10.1145/2254064.2254110", "year": "2012", "article_id": "2254110", "conference": "PLDI", "title": "Verifying GPU kernels by test amplification", "url": "http://dl.acm.org/citation.cfm?id=2254110"}