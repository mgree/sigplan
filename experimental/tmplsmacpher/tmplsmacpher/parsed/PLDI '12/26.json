{"article_publication_date": "06-11-2012", "fulltext": "\n Type-Directed Automatic Incrementalization Yan Chen Joshua Dun.eld Umut A. Acar Max Planck Institute \nfor Software Systems {chenyan, joshua, umut}@mpi-sws.org Abstract Application data often changes slowly \nor incrementally over time. Since incremental changes to input often result in only small changes in \noutput, it is often feasible to respond to such changes asymptotically more ef.ciently than by re-running \nthe whole com\u00adputation. Traditionally, realizing such asymptotic ef.ciency im\u00adprovements requires designing \nproblem-speci.c algorithms known as dynamic or incremental algorithms, which are often signi.cantly more \ncomplicated than conventional algorithms to design, analyze, implement, and use. A long-standing open \nproblem is to develop techniques that automatically transform conventional programs so that they correctly \nand ef.ciently respond to incremental changes. In this paper, we describe a signi.cant step towards solving \nthe problem of automatic incrementalization: a programming lan\u00adguage and a compiler that can, given a \nfew type annotations de\u00adscribing what can change over time, compile a conventional pro\u00adgram that assumes \nits data to be static (unchanging over time) to an incremental program. Based on recent advances in self-adjusting \ncomputation, including a theoretical proposal for translating purely functional programs to self-adjusting \nprograms, we develop tech\u00adniques for translating conventional Standard ML programs to self\u00adadjusting \nprograms. By extending the Standard ML language, we design a fully featured programming language with \nhigher-order features, a module system, and a powerful type system, and im\u00adplement a compiler for this \nlanguage. The resulting programming language, LML, enables translating conventional programs deco\u00adrated \nwith simple type annotations into incremental programs that can respond to changes in their data correctly \nand ef.ciently. We evaluate the effectiveness of our approach by considering a range of benchmarks involving \nlists, vectors, and matrices, as well as a ray tracer. For these benchmarks, our compiler incrementalizes \nexisting code with only trivial amounts of annotation. The resulting programs are often asymptotically \nmore ef.cient, leading to orders of magnitude speedups in practice. Categories and Subject Descriptors \nF.3.3 [Logics and Meanings of Programs]: Studies of Program Constructs Keywords Self-adjusting computation; \nincrementalization; type annotations; compiler optimization; performance 1. Introduction Much modern \nsoftware is highly dynamic: it continually receives input and responds by computing the corresponding \noutput. This Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI \n12, June 11 16, 2012, Beijing, China. Copyright &#38;#169; 2012 ACM 978-1-4503-1205-9/12/06. . . $10.00 \n dynamic nature of computation creates both opportunities and chal\u00adlenges. Opportunities arise because \nthe input data, which changes over time as a result of interactions, often changes incrementally by a \nsmall amount at a time. Such incremental changes make it possible to compute the new output ef.ciently \nby reusing interme\u00addiate computations. In the common case, updating the output dy\u00adnamically instead of \nrecomputing it leads to asymptotically more ef.cient response times, which can dramatically improve practi\u00adcal \nef.ciency. The challenges stem from the dif.culty of realiz\u00ading this potential: designing, analyzing, \nand implementing software systems that can operate ef.ciently on dynamically changing data. In the algorithms \nand programming-languages communities, considerable work has been done on dynamic systems; for surveys, \nsee Chiang and Tamassia [1992]; Ramalingam and Reps [1993]; Agarwal et al. [2002]; Demetrescu et al. \n[2005]. The algorithms community devises ad hoc dynamic algorithms for speci.c prob\u00adlems. While these \nalgorithms can be very ef.cient, often achieving optimal complexity, they are generally hard to design, \nanalyze, and implement. Dynamic algorithms are also dif.cult to compose in a modular fashion, limiting \ntheir applicability in large software sys\u00adtems. The programming-languages community develops languages \nfor expressing dynamic programs and compilation techniques for translating these high-level programs \ninto executables that respond ef.ciently to dynamic changes. This approach is often called in\u00adcremental \ncomputation. Incremental computation can dramatically simplify developing dynamic software, but achieving \noptimal ef.\u00adciency has remained elusive. This is perhaps unsurprising, because the problem is inherently \nchallenging: the compiler is ultimately expected to generate code that signi.cantly outperforms the source \ncode, often by an asymptotically signi.cant margin. Recent advances in self-adjusting computation made \nimportant progress on the problem of incremental computation. By propos\u00ading dynamic dependency graphs, \nand a change propagation algo\u00adrithm [Acar et al. 2006] that utilizes a particular form of memo\u00ad ization \ntechniques [Acar et al. 2009, 2008], the approach enables the programmer to express dynamic computations \nvia several lan\u00adguage abstractions. Previous work extended existing languages in\u00adcluding C [Hammer et \nal. 2009, 2011] and ML [Acar et al. 2009; Ley-Wild et al. 2008] to support self-adjusting computation. \nEval\u00ad uations showed that the approach can achieve asymptotically ef.\u00adcient updates for a reasonably \nbroad range of benchmarks [Acar et al. 2009; Hammer et al. 2011], and even help solve major open problems \nin a range of domains including computational geome\u00adtry [Acar et al. 2010] and machine learning [S\u00a8umer \net al. 2011]. More recent work generalized the approach to support parallel computation on multicores, \ntaking advantage of the performance bene.ts of parallelism and incremental computation at the same time \nby exploiting structural similarities between them [Burckhardt et al. 2011; Acar et al. 2011]. The approach \nhas also been applied to large-scale MapReduce computations in distributed systems [Bha\u00adtotia et al. \n2011].  While these advances chart a viable approach to incremen\u00adtal computation by eliminating the \nneed to design and implement sophisticated algorithms for dynamic problems, they still require signi.cant \nprogrammer involvement. For example, writing a self\u00adadjusting program in .ML [Ley-Wild et al. 2008] requires \ncare\u00ad fully annotating the program with speci.c primitives that deter\u00admine how to construct the dynamic \ndependence graph and perform change propagation. In this paper, we design a language, LML, that allows \nthe pro\u00adgrammer to derive self-adjusting software from conventional pro\u00adgrams through simple type annotations \nand implement a compiler for LML. Speci.cally, LML extends the Standard ML language, which has a range \nof features including higher-order functions, an advanced module system, and imperative references, with \nlevel type quali.ers, which enable marking certain data as changeable, that is, subject to modi.cations \nover time. For example, when pro\u00adgramming a ray tracer, we mark the surface properties of objects as \nchangeable. We implement a compiler for our language that gener\u00adates code that can be executed with .xed \ninput as usual, but also can automatically respond to changes by updating its output via change propagation. \nOur approach to automatic incrementalization builds on re\u00adcent advances in self-adjusting computation. \nSpeci.cally, Chen et al. [2011] develop an algorithm for translating purely functional programs decorated \nwith type annotations into self-adjusting pro\u00adgrams. Their work, however, is purely theoretical; it considers \nonly a simpli.ed language, and provides no implementation or empirical results. We extend their algorithm \nfor full Standard ML including a key imperative feature (mutable references), and extend the MLton compiler \n[MLton] for Standard ML (Section 3) to generate ef.\u00ad cient self-adjusting executables from type annotations \nby using the extended translation algorithm. Our compiler takes the user annotations and propagates them \nthrough various phases of the compiler to intermediate code, where it applies the translation algorithm. \nAssuming a runtime system that provides primitives for self-adjusting computation, the trans\u00adlation algorithm \ngenerates code by minimally inserting the self\u00adadjusting primitives via type-directed, local rewrites. \nSuch local rewrites, however, can lead to globally suboptimal code by insert\u00ading redundant calls to self-adjusting \nprimitives. We therefore for\u00admulate a global rewriting system for eliminating such redundant code, and \nprove that the rewriting rules are terminating and con\u00ad.uent (Section 3.4). We implement the run-time \nsystem for self\u00ad adjusting primitives directly in SML. We evaluate our implementation (Section 4) by \nconsidering benchmarks including various primitives on lists, sorting functions, vector operations, matrix \noperations, and a ray tracer. For each of these, we only need to insert some keywords into the program \nto specify the desired behavior. Speci.cally, most benchmarks require trivial decorations, often amounting \nto inserting type quali.ers in one or two lines of code. No changes to the structure of the types, or \nany part of the code itself, are necessary. The executables generated by the compilers respond automatically \nand ef.ciently to small changes to their data. We frequently observe signi.cant asymptotic improvements \nin ef.ciency and obtain signi.cant speedups ranging from 10% (for large changes that affect a signi.cant \nportion of the output) to several orders of magnitude.  2. Overview We illustrate our approach through \na simple example. First, we de\u00adscribe matrix multiplication in ordinary SML. Next, we describe a self-adjusting \nversion, which could be written by hand, that can update its output asymptotically more ef.ciently (by \nnearly a quadratic factor) than a complete re-computation. We then de\u00adscribe our approach, where we obtain \nthe same code automatically by adding a single type quali.er to the code and compiling it with our compiler. \nIn the example, we consider a particular kind of change to the input and brie.y outline other possibilities \nin Sec\u00adtion 2.4. In all cases, the executable generated by our compiler updates the output correctly \nand ef.ciently, leading to orders of magnitude speedups in practice (Section 4). 2.1 Matrix Multiplication \nin SML The SML code in Figure 1 multiplies two matrices, where a matrix is represented as a vector of \nrows, and each row is a vector of in\u00adtegers. Omitting the annotation $C, this is the usual T(n 3)matrix\u00admultiplication \nalgorithm. Using the map function over vectors, it iterates over the rows of the .rst matrix and the \ncolumns of the sec\u00adond (transposed for faster access). By using the map2 function, it multiplies each \nelement of the row with the corresponding element of the column, and using reduce, adds these results \nto generate one element of the output matrix. The function transpose, whose code is not shown here, trans\u00adposes \na matrix. The functions map, map2 and reduce perform stan\u00addard operations on vectors: Given a vector \na = .a1,...,an. and a function f, the call map(a,f) returns a vector .f(a1),...,f(an).). Similarly, given \ntwo vectors a and b and a binary function f,the call map2(a,b,f) returns .f(a1,b1),...,f(an,bn).). Given \na vector a = .a1,...,an., an identity element Z and an associative binary function ., the call reduce(a,Z,.) \nreturns a1 .\u00b7\u00b7\u00b7.an if n>0,and Z if n = 0.  2.2 Self-Adjusting Matrix Multiplication Suppose that we \nare interested in changing the elements of the matrix incrementally and updating the result of multiplication. \nOne approach would be to develop an algorithm speci.c to this problem. For matrices with integers or \n.oating-point numbers, this does not seem particularly dif.cult: we could devise an algorithm that undoes \nthe effect of the changed input element and updates the sum by factoring in its new value. However, such \nan algorithm assumes addition and multiplication are commutative and have inverses; in reality, .oating-point \narithmetic does not have these properties. Moreover, matrix multiplication can be used for a whole array \nof computations, some of which don t admit such inverses. In these cases, it is signi.cantly more challenging \nto come up with an ef.cient incremental algorithm. More generally, the history of research on incremental \nalgo\u00adrithms demonstrates that they can be extremely challenging to de\u00adsign and implement. For example, \na paper on an advanced dynamic algorithm for planar convex hull exceeds 100 pages [Jakob 2002]; that \nalgorithm appears very dif.cult to implement, and to the best of our knowledge, has never been implemented. \nIn contrast, standard non-incremental algorithms for convex hull can be implemented in less than 50 lines \nof SML. Writing self-adjusting matrix-multiply. Self-adjusting computa\u00adtion offers a way to write ef.cient \nincremental algorithms by mod\u00adifying the code for the standard, non-incremental algorithm. The idea is \nto distinguish between stable and changeable data and insert operations that manipulate changeable data. \nIncremental changes can be made to the changeable parts of the input and a change prop\u00adagation algorithm \ncan be used to update the computation. Figure 2 shows code written in a direct style similar to previous \nwork [Acar et al. 2009]. Stable data is handled as usual: the type of a stable integer is simply int. \nChangeable data, however, is stored in mod\u00adi.ables: a changeable integer becomes an int mod. We declare \nmatrix as ((int mod) vector) vector: a vector of vectors of integers, where the integer elements are \nwrapped by modi.ables. This allows changing the individual elements of the matrix and up\u00addating the computation \nautomatically.  type matrix = ((int $C) vector) vector type matrix = ((int mod) vector) vector multiply \n: matrix * matrix . matrix multiply : matrix * matrix . matrix fun multiply (A, B) = fun multiply (A, \nB) = let val Tb = transpose B let val Tb = transpose B in in map (A, fn row . map (A, fn row . map (Tb, \nfn col . map (Tb, fn col . reduce (map2 (row, reduce (map2 (row, col, col, fn (a,b) . a*b), fn (a,b) \n. Mod (Read a(fn a . Read b(fn b . Write (a *b ))))), 0, Mod (Write 0), fn (x, res) . x+res))) fn (x, \nres) . Mod (Read x(fn x . Read res (fn res . Write (x +res ))))))) end end Figure 1. Matrix multiplication \nin SML. Figure 2. Compiler-generated self-adjusting matrix multiplication. When writing a self-adjusting \nprogram by hand, we .rst deter\u00admine the changeable parts of the data, and then edit the code to ex\u00adplicitly \nmanipulate such data through Read, Write and Mod prim\u00aditives. This process is relatively cumbersome and \nerror-prone: we must identify the sections of code that depend on changeable data, conforming to a modal \ntype system. Speci.cally, a Read primitive can be used only inside a changeable section of code, which \nmust be contained within the dynamic scope of a Mod operation, and each Mod operation must end with a \nWrite that places a change\u00adable value in it. As an example, consider the function passed to map2, fn \n(a,b) . a*b. This function directly operates on elements of the ma\u00adtrix, which are changeable integers. \nSince they are changeable in\u00adtegers, they must be stored in modi.ables and have type int mod in the self-adjusting \nprogram. Thus, the function cannot multiply a and b directly. Instead, we write a Read that passes the \ncon\u00adtents of the modi.able a to a function fn a . Read b .... A second Read gets the contents of the \nmodi.able b.The a and b appearing in Write (a *b ) have type int, so we can multiply them normally. Because \na *b depends on changeable data (information .ows from a and b, which are changeable), it is also changeable, \nand must be written to a fresh modi.able: Mod (... Write (a *b )). In contrast, calls to transpose, map, \nmap2 and reduce need not be treated specially, because they operate on stable data (vec\u00adtors); note that \nthe elements of the vector are changeable but the vectors themselves are not. The bodies of these functions, \nhowever, still need to be modi.ed to accommodate changes in the types: they all now operate on vectors \nwhose elements are changeable, and in some cases, their other arguments also change (e.g., the function \nargument of map2). Change propagation. Given the self-adjusting matrix multiplica\u00adtion function, we can \nrun it in much the same way as running the standard version. Such a complete run takes asymptotically \nas long as the complete run but incurs some constant-factor overhead in practice. After a complete run, \nwe can change any or all of the change\u00adable data and update the output by performing change propagation. \nAs an example, consider changing one element of the .rst matrix and performing change propagation. This \npropagation will trigger execution of the n multiplication operations that use this element and the reduce \noperation that computes the new sum. When mul\u00adtiplying n \u00d7 n matrices, it is not dif.cult to show that \nchange prop\u00adagation takes O(n log n)time. Since at least n entries in the output matrix must be updated, \nthis is within a log n factor of the trivial lower bound O(n); in fact, it is likely that O(n log n)is \ntight in the general case. Consequently, change propagation is nearly quadrat\u00adically faster than a complete \nexecution, a huge gain in ef.ciency. In writing this self-adjusting program, we realized this ef.ciency \nwithout designing and implementing an incremental algorithm, but we nevertheless had to make signi.cant \nchanges to the code.  2.3 Type-Directed Self-Adjustment As the description of the self-adjusting matrix \nmultiply suggests, writing self-adjusting programs can require rather invasive changes to the code. In \nour approach, our compiler can derive self-adjusting programs automatically, based on simple type annotations. \nFor example, given the code in Figure 1 and the annotation $C (the .rst line) that makes the element \ntype int changeable, our compiler derives the code in Figure 2 automatically. As a result, starting with \na trivial annotation to the type declarations, our compiler yields a near-quadratic-time improvement \nin run time. We refer to the type annotations as levels and the resulting types as level types. Programmers \nneed only annotate the types of changeable data with $C; all other types remain stable. For example: \n int is a stable integer;  int $C is a changeable integer;  (int $C) vector is a stable vector of changeable \nintegers.  The last type may look odd how can the vector be stable when its elements are changeable? \nBut it simply expresses that each element is individually changeable, while the vector as a whole is \nnot changeable: it would not be possible to replace the entire vector except by changing each element \nindividually.  2.4 Remarks In the above example, we allow changes to individual elements by representing \nmatrices as stable vectors with changeable ele\u00adments: ((int $C) vector) vector. Our approach also makes \nit possible to use many other representations, again deriving self\u00adadjusting code automatically. For \nexample, if we expect entire rows of the input matrices to change at once, we could choose to represent \nmatrices as changeable rows of stable elements, ((int) vector $C) vector. This would track the computation \nonly at the granularity of entire rows and result in optimal updates for full row changes of the input. \nUsing our approach, we could derive a self-adjusting program suitable for this case by changing the type \nannotation, with no changes to the source code itself at all.  Similarly, it can be bene.cial to use \na blocked representation, where the matrix is represented with blocks, smaller matrices of size m \u00d7 m \nfor some constant m (typically between 10 and 100). This representation has superior locality, often \nresulting in better practical ef.ciency. Our compiler easily generates self-adjusting code that treats \nthe blocks, and not individual elements, as change\u00adable. As we show in our experimental evaluation (Section \n4), this blocking technique leads to extremely time-and space-ef.cient be supplied by the run-time system. \nThe self-adjusting primitives include mod, read,and write functions for creating, reading from, and writing \nto modi.able references. At a high level, the transla\u00adtion rules inspect the code locally, insert reads \nwhere changeable data is used (according to type information), and ensure that each read takes place \nwithin the dynamic scope of a call to mod.To ensure this and other correctness properties, the rules \ndistinguish stable and changeable modes. xG . x : t . . $S G . (ref x):(t ref $C self-adjusting computations. \n(Ref) mod (write(x . )) . ) . $S  3. Design and Implementation G . x2 : t. ref $C e (Deref) . . . .. \ne : t . .$S tended Standard ML with a single keyword $C, a type quali.er, G . let x1 = !x2 in e : t \n.$C read x2 as x1 in e and extended the MLton compiler [MLton] for Standard ML to G . e : t . x2 G, x1 \n: t $C To support type-directed, automatic incrementalization, we ex\u00ad . ref $C generate self-adjusting \nexecutables. This extension to SML does . $S G . x1 : tx1 not restrict the language in any way, allowing \nall its features to be . G . x2 : t . $S x e 2 $C used freely, including the module language. (Assign) \n. G . let =(x1:= x2)in e : t .impwrite x1:= x $C in e The most important additions to the compiler are \na translation 2 phase, an optimization phase, and a run-time system. In this sec\u00adtion, we describe the \nstructure of our system, and discuss some of Figure 4. Translation rules for mutable references its key \ncomponents in more detail. Figure 4 shows the translation rules for mutable references, 3.1 Structure \nof the Compiler which we translate to modi.ables. The translation judgment G . . is read in environment \nG and mode e, source . e expression e at type t translates to e . . In stable mode $S,the : t e e The \ncompiler pipeline is shown in Figure 3. Although the surface language has only one type annotation, $C, \nin all intermediate lan\u00ad guages, we explicitly mark all types as stable or changeable us\u00ad ing two type \nquali.ers $S and $C. We modi.ed all of MLton s phases that come before the new translation phase (Translate)to \npropagate these type annotations. The translation phase uses type quali.ers to generate self-adjusting \ncode in the SXML intermedi\u00ad ate language of MLton, as discussed in the next section. The opti\u00ad mization \nphase eliminates some important redundancies of the code produced by the translation phase. For technical \nreasons related to MLton s dead code elimination (see Section 3.5), we stop the com\u00ad translation produces \nstable code that cannot inspect changeable data or directly use changeable code; in changeable mode $C,the \ntranslation produces changeable code that can appear within the body of a read and can manipulate references. \nFor translation of imperative references, we add another primitive impwrite that updates the value of \na modi.able directly. Stable functions may be called with either stable or changeable arguments. For \nexample, the program might use the built-in SML + function on changeable integers. Our translation algorithm \nhan\u00ad dles such polymorphic usage by inserting coercions, which read piler after the optimization phase \nand run an unmodi.ed version of changeable arguments and create a modi.able from the result. MLton on \nthe SXML output, producing an executable.  3.2 Pre-Translation Phases 3.4 Optimization We extended \nthe MLton lexer and parser to handle types with $C annotations, producing abstract syntax in which types \ninclude level information. We systematically added levels to several typed inter\u00admediate languages (CoreML, \nXML , and SXML), and modi.ed each pre-translation phase to accept and propagate levels in types. Thus, \nwe leverage MLton s broad scope it accepts full Standard ML and its various code transformations: elaboration, \ndefunc\u00adtorization, linearization (conversion to A-normal form), dead code elimination, monomorphization \nof ML-polymorphic code, etc.  3.3 Self-Adjusting Translation We apply our translation on MLton s SXML \nintermediate lan\u00adguage, a monomorphic subset of SML in A-normal form. SXML is suitable for the transformation \nto self-adjusting code because our transformation algorithm expects, and produces, monomorphic code in \nA-normal form. Our translation algorithm extends the algorithm of Chen et al. [2011] to support full \nSML, including (recursive) data types and imperative references. The translation algorithm is relatively \ntech\u00adnical, making its presentation dif.cult in the context of this paper, but we give a high-level overview \nand brie.y illustrate some of the extensions needed to handle full SML. The translation algorithm takes \nSXML code and transforms it into SXML code containing self-adjusting computation primitives, whose implementations \nwill Our translation algorithm follows a system of inductive rules, which are guided only by local information \nthe structure of types and terms. This locality is key to simplifying the algorithm and the implementation \nbut it often generates code with redundant oper\u00ad . $S ations. For example, translating fst x,where x \n: int$C \u00d7 t2 , in changeable mode generates the term read (mod (let r = fst x in write(r))) as x . in \nwrite(x .), which is redundant: it cre\u00adates a temporary modi.able for the .rst projection of x and im\u00admediately \nreads its contents. A more ef.cient translation, let x . = fst x in write(x .), avoids the temporary \nmodi.able. Such redundancies turn out to be common, because of the local nature of the translation algorithm. \nWe therefore developed a post\u00adtranslation optimization phase to eliminate redundant operations. Figure \n5 illustrates the rules that drive the optimization phase. Each rule eliminates three operations: reading \nfrom a modi.able, writing to a modi.able, and creating a modi.able. As we show in Section 4.8, this optimization \nphase reduces the execution time for self-adjusting programs by up to 60%. Eliminating write-create-read. \nThe left-hand side of rewrite rule (1) evaluates an expression e1 into a new modi.able, then imme\u00addiately \nreads the contents of the modi.able into x .. The right-hand side evaluates e1 and binds the result to \nx . with no extra modi.able.  Defunctorize: operations primitives run-time library  read (mod (let \nr = e1 in write(r))) as x . in e2 -. let x . = e1 in e2 (1) read (mod e)as x . in write(x .)-. e (2) \nmod (read e as x . in write(x .)) -. e (3) Figure 5. Optimization rules Eliminating create-read-write. \nThe left-hand side of (2) evalu\u00adates e (which, since it is the body of a mod, must end in a write), creates \na modi.able, reads the just-written value into x ., and writes it again. The right-hand side just evaluates \ne. Eliminating read-write-create. Rule (3) is similar to rule (2): the left-hand side reads some modi.able \ne into a variable x .,and immediately writes x . back to a new modi.able; the right-hand side only evaluates \ne. These rules are shrinking reduction rules guaranteed to make the program smaller [Appel and Jim 1997]. \nThe rules are also terminating and con.uent. Termination is immediate, because in each rule, the right-hand \nside is smaller than the left-hand side: rule (1) replaces one let with another and drops a read,a mod \nand a write. In rules (2) and (3), the right-hand side is a proper subterm of the left-hand side. Con.uence \n(the property that all choices of rewrite sequences eventually yield a-equivalent terms) is not immediate, \nbut is straightforward: Theorem 3.1. Rules (1) (3) are locally con.uent. Proof. First, the left-hand \nsides of rules (1) and (2) may over\u00adlap exactly: either rule can be applied to read (mod (let r = e1 \nin write(r))) as x . in write(x .), but the right-hand sides of (1) and (2) are let x . = e1 in write(x \n.)and let r = e1 in write(r), which are a-equivalent. Rules (2) and (3) may overlap critically, but in \nall cases yield a-equivalent terms. One case (the other is similar) is:  read (mod (read e3 as x3 in \nx3)) as x2 in write(x2) (2) -. read e3 as x3 . in write(x3. ) read (mod (read e3 as x3 in x3))as x2 \nin write(x2) (3) -. read e3 as x2 . in write(x2. ) Otherwise, redexes overlap only when an entire left-hand \nside is a subterm of the e in another left-hand side (possibly of the same rule). Such non-critical overlap \ncases follow as in Baader and Nipkow [1998, pp. 137 138]. Since the rules are terminating and locally \ncon.uent, by New\u00adman s lemma [Newman 1942], they are globally con.uent. Thus, we can safely apply them \nin any order, to arbitrary subterms, until no rules apply. In practice, it suf.ces to traverse the program \nonly once: if we traverse it in preorder, we apply rules near the leaves of the tree .rst. That means \nthe subterms of the left-hand sides have already been rewritten, so the right-hand sides will contain \nno more candidate subterms. 3.5 Final Stage The translated, optimized SXML code has explicit self-adjusting \ncomputation primitives, suitable for use with a self-adjusting run\u00adtime library implementing these primitives. \nOur implementation of this library is written in SML, so we might expect to run the full MLton pipeline \non the library and source program together to produce an executable. Since, however, no calls to the \nlibrary appear in the user s source program, dead code elimination deletes the library code from the \nprogram during MLton s Defunctorize phase. Instead of re-engineering MLton s dead code elimination to \nspecially treat library code as live, we take a simpler approach. Taking advantage of the fact that SXML \nis a subset of SML, we cut off the pipeline after producing optimized self-adjusting SXML, combine the \nSXML output of the translation phase with the library, and generate an executable by running the unmodi.ed \nMLton compiler. 3.6 Runtime Environment As described above, we compile the translated user program to\u00adgether \nwith a self-adjusting run-time library. This library imple\u00adments the self-adjusting primitives (Mod, \nRead, Write)used by the translated code. When executed, the library constructs a de\u00adpendency graph during \nthe complete run. The library also provides facilities for changing the input and invoking change propagation \nto re.ect changes to the output.  4. Experiments We present an experimental evaluation of our approach \nand com\u00adpare it to previous work. 4.1 Benchmarks We implemented a number of benchmarks in our LML language, \nincluding standard self-adjusting-computation benchmarks from previous work [Acar et al. 2009; Ley-Wild \net al. 2008], additional benchmarks on vectors and matrices, and a ray tracer. LML makes it relatively \nstraightforward to derive self-adjusting versions of programs. Speci.cally, we simply wrote the standard \ncode for our benchmarks and changed the type declarations to allow for changes to the input data. For \nthe ray tracer, we used an unmodi.ed SML implementation of a sphere ray tracer [King 1998].  Our benchmarks \ninclude some standard list primitives (map, filter, split), quicksort, and mergesort (qsort, msort). \nThese include simple iteration (map, filter, split), accumulator pass\u00ading (qsort), and divide-and-conquer \nalgorithms (qsort, msort). All of these list benchmarks operate on integers: map applies f(i)= i\u00f73+i\u00f75+i\u00f77 \nto each element; filter keeps the elements when f(i)is even; split partitions its input; qsort and msort \nimple\u00adment sorting algorithms. Similarly, our vector benchmarks include vec-reduce, vec-mult (dot product), \nmat-vec-mult (matrix\u00advector multiplication), mat-add, transpose (matrix transpose), mat-mult,and block-mat-mult \n(matrix multiplication on ma\u00adtrices that use a simple blocked representation). The vector and matrix \nbenchmarks implement the correspond\u00ading vector or matrix algorithm with double-precision (64-bit) .oat\u00ading \npoint numbers; when multiplying two doubles, we normalize the result by their sum to prevent over.ows \nwhen operating on large matrices. For our matrix benchmarks, we consider two differ\u00adent representations \nof matrices: the standard representation where the elements are laid out in memory in row-major order, \nand the blocked representation where elements are blocked into small sub\u00admatrices. Our .nal benchmark \nis an off-the-shelf ray tracer that supports point and directional lights, sphere and plane objects, \nand diffuse, specular, transparent, and re.ective surface properties. To support .exible changes to the \ninput data, our list bench\u00admarks permit insertion and deletion of any element from the input; in LML, \nthis requires simply specifying the tail of the lists as changeable. Our vector and matrix benchmarks \npermit changing any element of the input; in LML, this requires simply specifying the vector and matrix \nelements as changeable. Our blocked matrix benchmark permits changing any block (and thus any element) \nof the input. Our ray tracer permits changing the surface properties of objects in the image; thus, for \na .xed input scene (lights and objects) and output image size, we can render multiple images via change \npropagation. The type annotations needed to enable these changes in our self\u00adadjusting versions of the \nbenchmarks were trivial. Each benchmark, including the ray tracer, required changes to no more than a \nfew lines of code in fact, never more than two lines. For each benchmark, we evaluate a conventional \nimplemen\u00adtation and four self-adjusting versions. Three of these are hand\u00adcoded versions from previous \nwork, CPS [Ley-Wild et al. 2008], CEAL [Hammer et al. 2011] and AFL [Acar et al. 2009]. We use these \nbenchmarks exactly as published, except for setting the test parameters and input data consistently to \nenable comparison. The last set, labeled Type-Directed , consists of the self-adjusting programs generated \nby our LML compiler. Our LML list bench\u00admarks use the same memoization strategy as the AFL versions of \nthe list benchmarks; the rest of the benchmarks do not need mem\u00adoization in LML.  4.2 Experimental Setup \nWe used a 2 GHz Intel Xeon with 64 GB memory running Linux. The machine has multiple cores but all benchmarks \nare sequen\u00adtial. We compile all our benchmarks using our LML compiler. For comparison to previous work \non .ML, we use the publicly avail\u00adable .ML compiler [DeltaML]. We execute all benchmarks with the gc-summary \noption to report garbage collection statistics, but exclude garbage-collection times from our experiments \nto fo\u00adcus on the actual computation time; we separately discuss garbage collection in Section 4.10. For \nour measurements, we generate all inputs and all data changes uniformly randomly and sample over all \npossible changes. More speci.cally, the inputs to our integer benchmarks are random permutations of integers \nfrom 1 to n,where n is the input size. The inputs to our .oating-point benchmarks are randomly generated \n.oating-point numbers via the SML library. To increase the cover\u00adage of our evaluation, for each measurement, \nwe average over four different input instances, as well as all input changes over each of these inputs. \nFor each benchmark we measure the complete running time of the conventional and the self-adjusting versions. \nAll reported times are in seconds or milliseconds, averaged over four indepen\u00addent runs. Timings exclude \ncreation of the initial input; in change\u00adpropagation timings, we also exclude the initial, pre-processing \nrun (construction of the test data plus the complete run). To measure ef\u00ad.ciency in responding to small \ndata changes, we compute the prop\u00adagation time for responding to an incremental change. The nature of \nthe change depends on the benchmarks. For list benchmarks, we report the average time to insert or delete \nan element from the input list (average over all elements). For vector and matrix benchmarks, we report \nthe average time to change an element of the vector or matrix by replacing it with a randomly generated \nelement (aver\u00adaged over all positions in the vector or one position per row in the matrix). For the ray \ntracer, we consider a range of changes which we describe later when discussing the ray tracer in detail. \n 4.3 Experiments: Correctness To verify that our compiler generates self-adjusting executables that can \nrespond to changes to their data correctly, we used three approaches: type checking, manual inspection, \nand extensive test\u00ading. Our compiler generates self-adjusting code to a text .le, which we then type-check \nand compile along with a stand-alone self\u00adadjusting-computation library, which we have separately imple\u00admented. \nSML s type system veri.es that the translated code sat\u00adis.es certain invariants. Since we can inspect \nthe translated code manually, we can also spot-check the code, which is not a fool\u00adproof method but increases \ncon.dence. We have used this facility extensively when implementing the compiler. Additionally, we have \ndeveloped a testing framework, which makes a massive number of randomly generated changes to the input \ndata, and checks that the executable responds correctly to each such change by comparing its output with \nthat of a veri.er (reference implementation) that computes the given output using a straightforward, \nnon-incremental algorithm. Using this framework, we have veri.ed the correctness of all the self-adjusting \nexecuta\u00adbles generated by our compiler.  4.4 Experiments: Timings Summary Table 1 shows a summary of \nthe timings that we collected for our benchmarks at .xed input sizes (written in parentheses after the \nbenchmark s name). All times are reported in seconds. The .rst column ( Conv. Run ) shows the run time \nof the conventional (reference) implementation with an input of speci.ed size. The conventional version \ncannot self-adjust, but does not incur the overhead of trace construction as self-adjusting versions \ndo. The second column ( Self-Adj. Run ) shows the run time of the self\u00adadjusting version with an input \nof speci.ed size. Such a self\u00adadjusting run constructs a trace as it executes, which can then be used \nto respond automatically to incremental changes to data via change propagation. The third column ( Self-Adj. \nAvg. Prop. ) shows the average time for a change propagation after a small change to the input (as described \nin Section 4.1, the speci.c nature of the changes depend on the application). The last two columns of \nthe table, Overhead and Speedup , report the ratio of the self-adjusting run to the conventional run, \nand the ratio of the conventional run to change propagation. The overhead is the slowdown that a self-adjusting \nrun incurs com\u00adpared to a run of the conventional program. The speedup mea\u00adsures the speedup that change \npropagation delivers compared to re\u00ad  Application (Input size) Conv. Run (s) Self-Adj. Run (s) Self-Adj. \nAvg. Prop. (s) Overhead Speedup map(106) 0.05 0.83 1.1\u00d710 6 16.7 4.6\u00d7104 .lter(106) 0.04 1.25 1.4\u00d710 \n6 27.7 3.2\u00d7104 split(106) 0.14 1.63 3.2\u00d710 6 11.6 4.4\u00d7104 msort(105) 0.30 5.83 3.5\u00d710 4 19.5 850.92 qsort(105) \n0.05 3.40 4.9\u00d710 4 64.2 108.17 vec-reduce(106) 0.05 0.26 4.4\u00d710 6 5.5 1.1\u00d7104 vec-mult(106) 0.18 1.10 \n6.7\u00d710 6 5.9 2.8\u00d7104 mat-vec-mult(103) 0.17 0.81 1.4\u00d710 5 4.6 1.3\u00d7104 mat-add(103) 0.10 0.36 4.9\u00d710 7 \n3.7 2.0\u00d7105 transpose(104) 2.14 2.15 5.1\u00d710 8 1.0 4.2\u00d7107 mat-mult(400) 10.65 90.22 5.8\u00d710 3 8.5 1.8\u00d7103 \nblock-mat-mult(103) 7.03 8.38 4.6\u00d710 3 1.2 1.5\u00d7103 Table 1. Summary of benchmark timings. computing \nwith the conventional version. An analysis of the data shows that the overheads are higher for simple \nbenchmarks such as list operations (which are dominated by memory accesses), but signi.cantly lower for \nother benchmarks. The overheads for qsort are traditionally high, commensurate with the previous work. \nIn all benchmarks, we observe massive speedups, thanks to the asymp\u00adtotic improvements delivered by change \npropagation. In the com\u00admon case, the overhead is incurred only once: after a self-adjusting run, we \ncan change the input data incrementally and use change propagation, with massive speedups.  4.5 Experiments: \nMerge Sort Although it is not apparent from the summary in Table 1, our experiments show that for all \nour benchmarks, the overheads of self-adjusting versions are constant and do not depend on the input \nsize, whereas speedups, being asymptotically signi.cant, increase with the input size. To illustrate \nthis property, we examine our merge sort benchmark; in Appendix A, we show the corresponding data for \nfour more representative benchmarks. The plot on the left in Figure 6 shows the time (in seconds) for \nthe complete run of self-adjusting merge sort compared to the con\u00adventional version, for a range of input \nsizes (x axis). The .gure suggests that, in both the conventional and self-adjusting versions, the complete-run \ntime grows almost linearly, and they exhibit the same asymptotic complexity, O(n log n). The plot in \nthe middle of Figure 6 shows the time in milliseconds for change propa\u00ad gation after inserting/deleting \none element for each input size (x axis). As can be seen, the time taken by change propagation grows \nsublinearly as the input size increases. This is consistent with the O(log n)bound that we can show analytically. \nThe plot on the right in Figure 6 shows the speedups for different input sizes (x axis): the time for \na run of the conventional algorithm divided by the time for change propagation. The plots show that speedups \nincrease linearly with the input size, consistent with the theoretical bound. To obtain this asymptotic \nimprovement, we only insert one keyword in the code and use our compiler to generate the self-adjusting \nversion. 4.6 Experiments: Matrix Multiplication Our type-based approach gives the programmer great .exibility \nin specifying changeable elements in different granularity. The dif\u00adference in the data representation \ncan lead to dramatically different time and space performance. For example, mat-mult performs ma\u00adtrix \nmultiplication using the standard matrix representation where each element of the matrix is changeable, \nwhile block-mat-mult considers the blocked representation where elements are blocked into groups of 20 \n\u00d7 20 submatrices. As we can see from Table 1, although the blocked version has a smaller speedup, as \nchanging one element requires recomputing the whole submatrix, it has much less overhead compared to \nthe standard representation. To further explore this trade-off, Figure 7 shows the time and space of \nblocked matrix multiplication with block sizes from 20 \u00d7 20 to 50 \u00d7 50. Run time. In Figure 7, the leftmost \nplot shows the time for the complete run of self-adjusting blocked-matrix multiply with dif\u00adferent block \nsizes, as well as the conventional version. The .gure suggests that all benchmarks exhibit the same asymptotic \ncomplex\u00adity, O(n 3). We also observe that as the block size increases, the overhead becomes smaller. \nThis is because we treat each block as a single modi.able, reducing the number of modi.ables tracked \nat run time. The second plot in Figure 7 shows the time for change propagation after changing a block \nfor each input size (x axis) with different block sizes. The time taken by change propagation grows almost \nlinearly as the input size increases, which is consistent with the O(n log n)bound that we can compute \nanalytically. Changing any part of a block requires recomputing the whole block, so prop\u00adagation is faster \nwith smaller block sizes. The third plot in Figure 7 shows speedups for different input sizes. Speedups \nincrease asymp\u00adtotically with input size, consistent with the theoretical bound of O(n 2/ log n). Smaller \nblocks have higher speedups. For example, for a 1000\u00d71000 matrix, the 20\u00d720 block enables 1200\u00d7 speedup, \nwhile the 50 \u00d7 50 block has 280\u00d7 speedup. Space. The rightmost plot in Figure 7 shows the memory used \nby change propagation with different block sizes (the complete run never uses more memory than change \npropagation). As with all other approaches to self-adjusting computation, our approach ex\u00adploits a trade-off \nbetween memory (space) and time (we compare the space usage of other approaches in Section 4.9). We store \ncom\u00ad putation traces in memory and use them to respond to incremen\u00adtal data changes, resulting in an \nincrease in memory usage and a decrease in response time. Typically, self-adjusting programs use asymptotically \nas much memory as the run-time of the computa\u00adtion. The plot shows results consistent with the theoretical \nbound of O(n 3). Although a smaller block size leads to larger speedups, it requires more memory, because \nthe total number of modi.ables created is proportional to the number of blocks in the matrix. As can \nbe seen from the plot, memory consumption can be high. However, it can be reduced by programmer control \nover dependencies, which self-adjusting computation provides [Acar et al. 2009; Hammer et al. 2011]: \nThe programmer can specify larger chunks of data, instead of single units, as changeable. Our approach \nfurther simpli.es such control by requiring only the types to be changed. As a concrete example, our \nmatrix-multiplication benchmark with 50 \u00d7 50 blocks consumes about 300 MB as we change each element of \nthe input matrix and update the output. This is about 10 times the space needed to store the two input \nmatrices and the result matrix, but enables a 280\u00d7 speedup when re-computing the output.  4.7 Experiments: \nRay Tracer Many applications are suitable for incremental computation, be\u00adcause making a small change \nto their input on average causes small changes to their output. But some applications are not. Arguably, \nincremental computation techniques should be avoided or used cau\u00adtiously in such applications. To evaluate \nthe effectiveness of our approach in such limiting cases, we considered ray tracing, where a small change \nto the input can require a large fraction of the out\u00adput image to be updated. In our experiments, we \nrendered an input scene of 3 light sources and 19 objects with an output image size of 512 \u00d7 512 and \nthen repeatedly changed the surface properties  Time for complete run (s)Time for change propagation \n(ms)Speedup of change propagation 0.4 900 0.35 800 700 0.3 600   0.1 200 1 0.05 100 0 0 0Input Size \nInput Size Input Size Figure 6. Time for complete run; time and speedup for change propagation for msort \nTime for complete run (s)Time for change propagation (ms)Speedup of change propagationMemory for change \npropagation Time (ms) 9 25 1400 1200 Speedup 0.25 Time (s) 500 0.2 400 0.15 300   1200 1000 800 600 \n400 1000 800 600 400 20 15 10 Time (s) 2 5 2002001 0 0 0 0Input Size Input Size Input Size Input \nSize Figure 7. Time for complete run; time, speedup and memory for change propagation for blocked matrix \nmultiply Surface Changed Image Di.. (% pixels) Conv. Run (s) Self-Adj. Run (s) Self-Adj. Avg. Prop. (s) \nOverhead Speedup AD 57.22% 4.07 6.32 3.04 1.55 1.34 AM 57.22% 1.91 5.75 8.48 3.02 0.22 BD 8.43% 2.37 \n4.87 0.55 2.05 4.29 BM 8.43% 2.44 4.42 1.00 1.81 2.44 CD 9.20% 2.43 3.97 0.59 1.64 4.09 CM 9.20% 2.16 \n3.86 1.12 1.79 1.92 DD 1.85% 2.44 3.83 0.12 1.57 20.21 DM 1.85% 2.19 3.85 0.20 1.76 10.74 ED 11.64% 4.10 \n6.28 1.27 1.53 3.22 EM 11.74% 2.79 5.83 1.87 2.09 1.49 FD 19.47% 2.85 5.78 1.57 2.03 1.82 FM 19.47% 2.83 \n3.92 2.97 1.38 0.95 GD 27.37% 2.85 3.92 2.58 1.38 1.11 GM 27.47% 2.82 5.36 4.64 1.90 0.61 Table 2. Summary \nof ray tracer timings. of a single surface (which may be shared by multiple objects in the scene). We \nconsidered three kinds of changes: a change to the color of a surface, a change from a diffuse (non-re.ective) \nsurface to a mirrored surface, and a change from a mirrored to diffuse surface. We measured the time \nfor a complete run of both the conventional and the self-adjusting versions, and the average propagation \ntime for a single toggle of a surface property. For each change to the in\u00adput, we also measured the change \nin the output image as a fraction of pixels. Figure 8 illustrates an example. Table 2 shows the timings \nfor various kinds of changes. The .rst column shows the percentage of pixels changed in the output. Each \npair of rows corresponds to changing the surface properties of a set of objects (sets labeled A through \nG) to diffuse (non-re.ective) Figure 8. Two images produced by our ray tracer. To produce the right-hand \nimage, we change the surfaces of the four green balls from diffused surfaces to mirrored surfaces. Change \npropagation yields about a 2\u00d7 speedup over re-computing the output. and mirror surfaces in that order, \nas indicated by superscripts \u00b7 D and \u00b7 M respectively. Our measurements show that even when a large fraction \nof the output image must change, our approach can perform better than recomputing from scratch. We also \nobserve that since mirrors re.ect light, making a surface mirrored often requires performing more work \nduring change propagation than making the surface diffuse. Indeed, we observe that the speedups obtained \nfor mirror changes are consistently about half of the speedups for diffuse changes.  4.8 Experiments: \nCompiler Optimizations In Section 3.4 we described some key optimizations that eliminate redundancies \nin the code. To measure the effectiveness of these optimizations, we measured the running time for our \nbenchmarks compiled with and without these optimizations. Since the optimiza\u00adtions always eliminate redundant \ncalls, we expected them to im\u00adprove ef.ciency consistently, and also quite signi.cantly. As can be seen \nin Figure 9 by comparing the bars labeled Unopt. (green) and Type-Directed (black), our experiments indeed \nshow that the optimizations can improve the time of the complete run and the Time for Complete Run  \n3.5 3 2.5 2 1.5 1 0.5 0 map filter qsort msort Time for Change Propagation 6 5 4 3 2 1 0 map filter \nqsort msort Memory for Change Propagation1.8 1.6 1.4 1.2 1 0.8 0.6 0.4 0.2 0  map filter qsort msort \nFigure 9. Time and memory for complete run and change propa\u00adgation at .xed input sizes, normalized to \nType-Directed (set to 1). We note for readability that the top-down order of the legend corresponds to \nthe horizontal ordering of the bars. time and space for change propagation by as much as 60%. The complete \nrun never uses more space than change propagation does. We will discuss the rest of Figure 9 in Section \n4.9.  4.9 Experiments: Comparison to Previous Work We compare our results with previous work: the combinator \nlibrary (AFL) in SML [Acar et al. 2009], the continuation-passing style (CPS) approach in SML [Ley-Wild \net al. 2008], and the C-based CEAL system [Hammer et al. 2011], which is a carefully engi\u00ad neered and \nhighly optimized system that can be competitive with hand-crafted algorithms [Demetrescu et al. 2004]. \nFigure 9 shows this comparison for the common benchmarks with .xed input sizes of 1 million keys for \nlist operations and 100,000 keys for sorting, with results normalized to Type-Directed (= 1.0). The comparison \nshows that, for both time and space, our ap\u00adproach is within a factor of two of AFL, a carefully engineered \nhand-written library. The principal reason for AFL s performance is its multiple interfaces to the self-adjusting \nprimitives, which the programmer selects by hand. For example, AFL provides an unsafe interface that \nthe quicksort benchmark uses to speed up the parti\u00adtion, creating half as many modi.ables as with the \nstandard inter\u00adface. Our compiler does not directly support these low-level prim\u00aditives, so we cannot \nperform the same optimizations. In AFL, pro\u00adgrammers need to restructure programs in monadic style, explicitly \nSurface Changed Image Di.. (% pixels) Type-Dir. Run (s) CPS Run (s) Speedup vs. CPS Type-Dir. Prop (s) \nCPS Prop (s) Speedup vs. CPS AD 57.22% 6.32 5.88 0.93 3.04 4.36 1.43 AM 57.22% 5.75 7.35 1.28 8.48 13.86 \n1.64 BD 8.43% 4.87 8.06 1.66 0.55 1.01 1.82 BM 8.43% 4.42 7.75 1.75 1.00 1.80 1.80 CD 9.20% 3.97 7.97 \n2.01 0.59 1.15 1.93 CM 9.20% 3.86 7.63 1.98 1.12 1.93 1.72 DD 1.85% 3.83 7.95 2.07 0.12 0.21 1.75 DM \n1.85% 3.85 7.57 1.97 0.20 0.28 1.39 ED 11.64% 6.28 5.88 0.94 1.27 2.52 1.98 EM 11.74% 5.83 12.41 2.13 \n1.87 3.44 1.84 FD 19.47% 5.78 11.96 2.07 1.57 3.00 1.91 FM 19.47% 3.92 9.44 2.41 2.97 5.41 1.82 GD 27.37% \n3.92 9.64 2.46 2.58 4.69 1.82 GM 27.47% 5.36 11.02 2.06 4.64 8.60 1.85 Table 3. Comparison of ray tracer \nwith CPS constructing the dependency graph. This process is similar to doing type inference and translation \nby hand. Our approach makes self\u00adadjusting programs much easier to write, yet their performance is competitive \nwith carefully-engineered self-adjusting programs. Compared to CPS, our approach is approximately twice \nas fast, even though the CPS approach requires widespread changes to the program code and ours does not. \nThe primary reason for the per\u00adformance gap is likely that the CPS-based transformation relies on coarse \napproximations of true dependencies (based on continua\u00adtions); our compiler identi.es dependencies more \nprecisely by us\u00ading a type-directed translation. Additionally, we compared our ray tracer with one based \non CPS, where our approach ( Type-Dir. ) is approximately twice as fast as CPS (Table 3). Our approach \noften uses slightly more space than the CPS based approach, probably because of redundancies in the automatically \ngenerated code that can be eliminated manually in the CPS approach. Compared to CEAL [Hammer et al. 2011], \nour approach is usu\u00ad ally faster but occasionally slightly slower. We .nd this very inter\u00adesting because \nthe CEAL benchmarks use hand-written, potentially unsound optimizations, such as selective destination-passing \nand sharing of trace nodes [Hammer et al. 2011, Sections 7.2 and 8.1], that can result in incorrectly \nupdated output. We also compared our approach to sound versions of CEAL benchmarks, which were a factor \nof two slower than the unsound versions. We use up to .ve times as much space; given that our approach \nuses space consistent with the other ML based approach ( CPS ), this is probably be\u00adcause of differences \nbetween ML, a functional, garbage-collected language, and C. When compared to sound versions of the CEAL \nbenchmarks, which is arguably the more fair comparison, CEAL s space advantage decreases by a factor \nof two. To summarize, even though our approach accepts conventional code with only a few type annotations, \nthe generated programs perform better than most hand-written code in two programming languages, and are \ncompetitive with hand-written code in AFL. Memory usage is comparable to other ML-based approaches. \n 4.10 Experiments: the Effect of Garbage Collection In our evaluation thus far, we did not include garbage-collection \ntimes because they are very sensitive to garbage collection parame\u00adters, which can be speci.ed during \nrun time. For example, our com\u00adpiler allows us to specify a heap size when executing a program. If this \nheap size is suf.ciently large to accommodate the live data of our benchmarks, then the timings show \nthat essentially no time is spent in garbage collection. When we do not specify a heap size, memory is \nmanaged automatically, taking care not to over-expand the heap unnecessarily, by keeping the heap size \nclose to the size of  Time (ms) 0.012 0.01 0.008 0.006 0.004 0.002 0 0 300000 600000 900000 Input Size \nFigure 10. Propagation time for vec-reduce including GC time. the live data. With this setting, our timings \nshow that garbage col\u00adlection behaves differently in the complete run and change propa\u00adgation. The time \ncan vary from negligible to moderate during com\u00adplete runs of self-adjusting executables. For example, \nin blocked matrix multiplication, garbage collection times are less than 10%, but in vector multiplication, \ngarbage collection takes nearly half of the total running time. Previous work on self-adjusting com\u00adputation \nshows similar tradeoffs [Hammer and Acar 2008; Acar et al. 2009]. During change propagation, however, \nwe observe that garbage-collection times are relatively small even when not using a .xed heap. Figure \n10 shows the garbage collection time for vec\u00ad tor reduce, which is close to the worst-case typical behavior \nthat we obtain in our benchmarks. In some applications such as ray\u00adtracing and blocked matrix multiplication, \ngarbage collection times are negligible.  5. Related Work The problem of enabling computation to respond \nef.ciently to changes has been studied extensively. We brie.y examine some ear\u00adlier techniques for incremental \ncomputation and information .ow. Detailed background can be found in several excellent surveys [Ra\u00admalingam \nand Reps 1993; Chiang and Tamassia 1992; Agarwal et al. 2002; Demetrescu et al. 2005; Sabelfeld and Myers \n2003]. Incremental and self-adjusting computation. Earlier work on in\u00adcremental computation, which took \nplace in the 80s and 90s, was primarily based on dependence graphs and memoization. Depen\u00addence graphs \nrecord the dependencies between data in a compu\u00adtation and use a change-propagation algorithm to update \nthe com\u00adputation when the input is modi.ed [Demers et al. 1981; Hoover 1987]. Dependence graphs have \nbeen effective in applications such as syntax-directed computations, but are not general-purpose be\u00adcause \nchange propagation cannot update the dependence structure. As an alternative, researchers have proposed \nmemoization (also called function caching) [Pugh and Teitelbaum 1989; Abadi et al. 1996; Heydon et al. \n2000]. A classic idea [Bellman 1957; Mc-Carthy 1963; Michie 1968], memoization applies to any purely \nfunctional program. It improves ef.ciency when executions of a program with similar inputs involve similar \nfunction calls, but such calls are relatively rare: small modi.cations to input can prevent reuse by \nchanging the arguments to many function calls. More recent work on self-adjusting computation proposed \na particular technique for incremental computation that combines dynamic dependence graphs [Acar et al. \n2006] and a form of computational memoization [Acar et al. 2009] to achieve ef.\u00adcient updates. Variants \nof self-adjusting computation have been implemented in several host languages such as C [Hammer et al. \n2011], Java [Shankar and Bodik 2007], Haskell [Carlsson 2002], and SML [Ley-Wild et al. 2008]. Self-adjusting \ncomputation often achieves asymptotically ef.cient updates for a reasonably broad range of benchmarks \n[Acar et al. 2009; Hammer et al. 2011], can help verify runtime invariants [Shankar and Bodik 2007], \nand even help solve major open problems in many domains including computational geometry [Acar et al. \n2010] and machine learn\u00ading [S\u00a8umer et al. 2011]. More recent work shows that the approach can be generalized \nto parallel computations, taking simultaneous advantage of parallelism and incremental computation time \nby ex\u00adploiting structural similarities between them [Hammer et al. 2007; Burckhardt et al. 2011; Acar \net al. 2011], as well as large-scale distributed systems [Bhatotia et al. 2011]. Of all these previous \napproaches, DITTO [Shankar and Bodik 2007] and Incoop [Bhatotia et al. 2011] have the advantage of being \ncompletely transparent they require no programmer annotations or changes to the code. But they only target \nspeci.c domains invariant checking for DITTO and large-scale MapReduce com\u00adputations for Incoop making \nthem unsuited to general-purpose computations. Of the general approaches, library-based systems in SML \nand in C# [Acar et al. 2009; Burckhardt et al. 2011] re\u00ad quire the programmer to guarantee certain invariants \nfor correct\u00adness. These invariants are nontrivial to check statically or dynami\u00adcally, and motivated \nthe approaches taken by .ML and CEAL. .ML [Ley-Wild et al. 2008] and the most recent version of the CEAL \nlanguage system [Hammer et al. 2011] can ensure that self-adjusting programs respond to changes to their \ninput correctly. As described in Section 2, .ML requires writing self-adjusting programs in an explicit \nstyle by inserting several primitives that can require substantial changes to the code. Recent work on \nCEAL shows that a large fraction of annotations can be eliminated, but at the cost of tracing and recording \nall dependencies, which can lead to signi.cant loss of time ef.ciency and space blowup. The approach \nwe describe in this paper uses a type-directed translation to enable selective dependency tracking, recording \nonly the parts of the computation that can be affected by the changes. It guarantees that the output \nis updated correctly and ef.ciently un\u00adder any changes to the data. The approach is based on the theoret\u00adical \nwork of Chen et al. [2011]. That work, however, considered a minimal language and provided no implementation \nor practical ev\u00adidence that the approach can be realized in practice. As our exper\u00adimental evaluation \nshows, our implementation performs very well, usually outperforming .ML and CEAL even though they require \nheavy programmer involvement, while our approach required only tiny changes to type declarations and \nno changes to the code it\u00adself. Our approach thus allows taking advantage of the bene.ts of self-adjusting \ncomputation without the burden of major program restructuring. Information .ow. A number of information \n.ow type systems have been developed to check security properties, including the SLam calculus [Heintze \nand Riecke 1998], JFlow [Myers 1999] and a monadic system [Crary et al. 2005]. To save energy by ap\u00ad \nproximating subcomputations, Sampson et al. [2011] use informa\u00ad tion .ow to analyze dependencies. The \ntype system of Chen et al. [2011] used many ideas from Pottier and Simonet [2003]. Coco [Swamy et al. \n2011] transforms constructions such as ef\u00ad fects from impure style (as in ML) to an explicit monadic \nstyle (as in Haskell). In other words, it translates effects in lightweight style into effects in a heavyweight \nstyle. But it does not sup\u00adport implicit self-adjusting computation: uses of effects, though lightweight \ncompared to monadic style, must be explicit in the source program. Even such relatively lightweight constructs \nare pervasive in explicit self-adjusting computations and, compared to implicit self-adjusting computation, \nvery tedious to program with.  6. Conclusion We present the design, implementation, and evaluation of \na pro\u00adgramming language and compiler that enable programmers to write programs that respond automatically \nto changes to their data, us\u00ading only simple type annotations. By design, our approach guar\u00adantees that \nthe compiled programs respond to data changes cor\u00adrectly. Through self-adjusting-computation techniques \nand a care\u00adfully designed type-directed mechanism for identifying dependen\u00adcies, the compiler yields \nexecutables that can respond to incremen\u00adtal changes ef.ciently. Conventional benchmarks such as matrix \nmultiplication and ray tracers can be compiled to ef.cient incre\u00admental executables, with only tiny changes \nto their type speci.ca\u00adtions. Our language and compiler take an important step in solving the long-standing \nproblem of creating high-level languages for de\u00adveloping software that can react to incremental change \ncorrectly and ef.ciently with minimal programmer involvement.  Acknowledgments We thank the anonymous \nPLDI reviewers for their very useful comments on the submitted version of this paper, and Matthew A. \nHammer for help with the CEAL benchmarks.  References M. Abadi, B. W. Lampson, and J.-J. L\u00b4evy. Analysis \nand caching of dependencies. In International Conference on Functional Programming, pages 83 91, 1996. \nU. A. Acar, G. E. Blelloch, and R. Harper. Adaptive functional program\u00adming. ACM Trans. Prog. Lang. Sys., \n28(6):990 1034, 2006. U. A. Acar, A. Ahmed, and M. Blume. Imperative self-adjusting computa\u00adtion. In \nProceedings of the 25th Annual ACM Symposium on Principles of Programming Languages, 2008. U. A. Acar, \nG. E. Blelloch, M. Blume, R. Harper, and K. Tangwongsan. An experimental analysis of self-adjusting computation. \nACM Trans. Prog. Lang. Sys., 32(1):3:1 53, 2009. U. A. Acar, A. Cotter, B. Hudson, and D. T\u00a8urko.glu. \nDynamic well-spaced point sets. In Symposium on Computational Geometry, 2010. U. A. Acar, A. Cotter, \nB. Hudson, and D. T\u00a8urko.glu. Parallelism in dynamic well-spaced point sets. In Proceedings of the 23rd \nACM Symposium on Parallelism in Algorithms and Architectures, 2011. P. K. Agarwal, L. J. Guibas, H. Edelsbrunner, \nJ. Erickson, M. Isard, S. Har-Peled, J. Hershberger, C. Jensen, L. Kavraki, P. Koehl, M. Lin, D. Manocha, \nD. Metaxas, B. Mirtich, D. Mount, S. Muthukrishnan, D. Pai, E. Sacks, J. Snoeyink, S. Suri, and O. Wolfson. \nAlgorithmic issues in modeling motion. ACM Comput. Surv., 34(4):550 572, 2002. A. W. Appel and T. Jim. \nShrinking lambda expressions in linear time. J. Funct. Program., 7(5):515 540, Sept. 1997. F. Baader \nand T. Nipkow. Term rewriting and all that. Cambridge Univer\u00adsity Press, 1998. R. Bellman. Dynamic Programming. \nPrinceton Univ. Press, 1957. P. Bhatotia, A. Wieder, R. Rodrigues, U. A. Acar, and R. Pasquini. Incoop: \nMapReduce for incremental computations. In ACM Symposium on Cloud Computing, 2011. S. Burckhardt, D. \nLeijen, C. Sadowski, J. Yi, and T. Ball. Two for the price of one: A model for parallel and incremental \ncomputation. In ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications, \n2011. M. Carlsson. Monads for incremental computing. In International Confer\u00adence on Functional Programming, \npages 26 35, 2002. Y. Chen, J. Dun.eld, M. A. Hammer, and U. A. Acar. Implicit self-adjusting computation \nfor purely functional programs. In Int l Conference on Functional Programming (ICFP 11), pages 129 141, \nSept. 2011. Y.-J. Chiang and R. Tamassia. Dynamic algorithms in computational ge\u00adometry. Proceedings \nof the IEEE, 80(9):1412 1434, 1992. K. Crary, A. Kliger, and F. Pfenning. A monadic analysis of information \n.ow security with mutable state. Journal of Functional Programming, 15(2):249 291, 2005. DeltaML. DeltaML \nweb site. http://ttic.uchicago.edu/~pl/sa-sml/. A. Demers, T. Reps, and T. Teitelbaum. Incremental evaluation \nof attribute grammars with application to syntax-directed editors. In Principles of Programming Languages, \npages 105 116, 1981. C. Demetrescu, S. Emiliozzi, and G. F. Italiano. Experimental analysis of dynamic \nall pairs shortest path algorithms. In ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 369 378, \n2004. C. Demetrescu, I. Finocchi, and G. Italiano. Handbook on Data Structures and Applications, chapter \n36: Dynamic Graphs. CRC Press, 2005. M. Hammer and U. A. Acar. Memory management for self-adjusting computation. \nIn International Symposium on Memory Management, pages 51 60, 2008. M. Hammer, U. A. Acar, M. Rajagopalan, \nand A. Ghuloum. A proposal for parallel self-adjusting computation. In DAMP 07: Declarative Aspects of \nMulticore Programming, 2007. M. Hammer, G. Neis, Y. Chen, and U. A. Acar. Self-adjusting stack ma\u00adchines. \nIn ACM SIGPLAN Conference on Object-Oriented Program\u00adming, Systems, Languages, and Applications (OOPSLA), \n2011. M. A. Hammer, U. A. Acar, and Y. Chen. CEAL: a C-based language for self-adjusting computation. \nIn ACM SIGPLAN Conference on Program\u00adming Language Design and Implementation, 2009. N. Heintze and J. \nG. Riecke. The SLam calculus: programming with secrecy and integrity. In Principles of Programming Languages \n(POPL 98), pages 365 377, 1998. A. Heydon, R. Levin, and Y. Yu. Caching function calls using precise \ndependencies. In Programming Language Design and Implementation, pages 311 320, 2000. R. Hoover. Incremental \nGraph Evaluation. PhD thesis, Department of Computer Science, Cornell University, May 1987. R. Jakob. \nDynamic Planar Convex Hull. PhD thesis, Department of Computer Science, University of Aarhus, 2002. D. \nJ. King. A ray tracer for spheres, 1998. http://www.cs.rice.edu/ ~dmp4866/darcs/nofib/spectral/sphere/. \nR. Ley-Wild, M. Fluet, and U. A. Acar. Compiling self-adjusting programs with continuations. In Int l \nConference on Functional Programming, 2008. J. McCarthy. A basis for a mathematical theory of computation. \nIn P. Braffort and D. Hirschberg, editors, Computer Programming and Formal Systems, pages 33 70. North-Holland, \nAmsterdam, 1963. D. Michie. Memo functions and machine learning. Nature, 218:19 22, 1968. MLton. MLton \nweb site. http://www.mlton.org. A. C. Myers. JFlow: practical mostly-static information .ow control. \nIn Principles of Programming Languages, pages 228 241, 1999. M. H. A. Newman. On theories with a combinatorial \nde.nition of equiva\u00adlence . Annals of Mathematics, 43(2):223 243, 1942. F. Pottier and V. Simonet. Information \n.ow inference for ML. ACM Trans. Prog. Lang. Sys., 25(1):117 158, Jan. 2003. W. Pugh and T. Teitelbaum. \nIncremental computation via function caching. In Principles of Programming Languages, pages 315 328, \n1989. G. Ramalingam and T. Reps. A categorized bibliography on incremental computation. In Principles \nof Programming Languages, pages 502 510, 1993. A. Sabelfeld and A. C. Myers. Language-based information-.ow \nsecurity. IEEE J. Selected Areas in Communications, 21(1), 2003. A. Sampson, W. Dietl, E. Fortuna, D. \nGnanapragasam, L. Ceze, and D. Grossman. EnerJ: Approximate data types for safe and general low\u00adpower \ncomputation. In Programming Language Design and Implemen\u00adtation, pages 164 174, 2011. A. Shankar and \nR. Bodik. DITTO: Automatic incrementalization of data structure invariant checks (in Java). In Programming \nLanguage Design and Implementation, 2007. O. S\u00a8umer, U. A. Acar, A. Ihler, and R. Mettu. Fast parallel \nand adaptive updates for dual-decomposition solvers. In Conference on Arti.cial Intelligence (AAAI), \n2011. N. Swamy, N. Guts, D. Leijen, and M. Hicks. Lightweight monadic pro\u00adgramming in ML. In International \nConference on Functional Program\u00adming (ICFP), Sept. 2011.  A. Appendix Time for complete run (s)Time \nfor change propagation (ms)Speedup of change propagation 1.8 0.008 45000 1.4  35000 0.006  Speedup \n0.002 0.4 10000 0.001 0.2 5000 0 0 0 0 300000 600000 900000 0 300000 600000 900000 0 300000 600000 \n900000  Input Size Input Size Input Size Figure 11. Time for complete run; time and speedup for change \npropagation for split Time for complete run (s)Time for change propagation (ms)Speedup of change propagation \n3.5 0.5 120 1.6 400000.007 1.2 300000.005 Time (s) Time (s) Time (s) Time (s) 1 25000 0.004 0.8 20000 \n0.0030.6 15000  Time (ms) 2.5 0.35 0.45 3 100 0.4 80 Speedup 0.3 2 0.25 60 1.5 0.2 40 0.15 1 0.1 20 \n0.5 0.05 0  0  0 0 20000 40000 60000 80000 100000 0 20000 40000 60000 80000 100000 0 20000 40000 \n60000 80000 100000 Input Size Input Size Input Size Figure 12. Time for complete run; time and speedup \nfor change propagation for qsort Time for complete run (s)Time for change propagation (ms)Speedup of \nchange propagation 1.2 0.012 30000   1 0.01 25000 0.8 0.008 20000 Time (ms) Speedup 0.2 0.002 5000 \n0 0 0Input Size Input Size Input Size Figure 13. Time for complete run; time and speedup for change \npropagation for vec-mult Time for complete run (s)Time for change propagation (ms)Speedup of change propagation \n0.4 0.0016 200000 0.6 0.006 15000 0.4 0.004 10000   1800000.35 0.0014 160000 0.3 0.0012 140000 Time \n(ms) Speedup 600000.1 0.0004 40000 0.05 0.0002 20000 0 0 0Input Size Input Size Input Size Figure 14. \nTime for complete run; time and speedup for change propagation for mat-add 0.25 0.001 120000 100000 80000 \n0.2 0.0008 0.15 0.0006  \n\t\t\t", "proc_id": "2254064", "abstract": "<p>Application data often changes slowly or incrementally over time. Since incremental changes to input often result in only small changes in output, it is often feasible to respond to such changes asymptotically more efficiently than by re-running the whole computation. Traditionally, realizing such asymptotic efficiency improvements requires designing problem-specific algorithms known as dynamic or incremental algorithms, which are often significantly more complicated than conventional algorithms to design, analyze, implement, and use. A long-standing open problem is to develop techniques that automatically transform conventional programs so that they correctly and efficiently respond to incremental changes.</p> <p>In this paper, we describe a significant step towards solving the problem of automatic incrementalization: a programming language and a compiler that can, given a few type annotations describing what can change over time, compile a conventional program that assumes its data to be static (unchanging over time) to an incremental program. Based on recent advances in self-adjusting computation, including a theoretical proposal for translating purely functional programs to self-adjusting programs, we develop techniques for translating conventional Standard ML programs to self-adjusting programs. By extending the Standard ML language, we design a fully featured programming language with higher-order features, a module system, and a powerful type system, and implement a compiler for this language. The resulting programming language, LML, enables translating conventional programs decorated with simple type annotations into incremental programs that can respond to changes in their data correctly and efficiently.</p> <p>We evaluate the effectiveness of our approach by considering a range of benchmarks involving lists, vectors, and matrices, as well as a ray tracer. For these benchmarks, our compiler incrementalizes existing code with only trivial amounts of annotation. The resulting programs are often asymptotically more efficient, leading to orders of magnitude speedups in practice.</p>", "authors": [{"name": "Yan Chen", "author_profile_id": "81361601201", "affiliation": "Max Planck Institute for Software Systems, Kaiserslautern and Saarbr&#252;cken, Germany", "person_id": "P3471228", "email_address": "chenyan@mpi-sws.org", "orcid_id": ""}, {"name": "Joshua Dunfield", "author_profile_id": "81100605091", "affiliation": "Max Planck Institute for Software Systems, Kaiserslautern and Saarbr&#252;cken, Germany", "person_id": "P3471229", "email_address": "joshua@mpi-sws.org", "orcid_id": ""}, {"name": "Umut A. Acar", "author_profile_id": "81100077236", "affiliation": "Max Planck Institute for Software Systems, Kaiserslautern and Saarbr&#252;cken, Germany", "person_id": "P3471230", "email_address": "umut@mpi-sws.org", "orcid_id": ""}], "doi_number": "10.1145/2254064.2254100", "year": "2012", "article_id": "2254100", "conference": "PLDI", "title": "Type-directed automatic incrementalization", "url": "http://dl.acm.org/citation.cfm?id=2254100"}