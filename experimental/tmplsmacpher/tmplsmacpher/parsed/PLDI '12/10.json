{"article_publication_date": "06-11-2012", "fulltext": "\n Diderot: A Parallel DSL for Image Analysis and Visualization Charisee Chiw Gordon Kindlmann John Reppy \nLamont Samuels Nick Seltzer University of Chicago {cchiw,glk,jhr,lamonts,nseltzer}@cs.uchicago.edu Abstract \nResearch scientists and medical professionals use imaging technol\u00adogy, such as computed tomography (CT) \nand magnetic resonance imaging (MRI) to measure a wide variety of biological and phys\u00adical objects. The \nincreasing sophistication of imaging technology creates demand for equally sophisticated computational \ntechniques to analyze and visualize the image data. Analysis and visualization codes are often crafted \nfor a speci.c experiment or set of images, thus imaging scientists need support for quickly developing \ncodes that are reliable, robust, and ef.cient. In this paper, we present the design and implementation \nof Diderot, which is a parallel domain-speci.c language for biomedi\u00adcal image analysis and visualization. \nDiderot supports a high-level model of computation that is based on continuous tensor .elds. These tensor \n.elds are reconstructed from discrete image data using separable convolution kernels, but may also be \nde.ned by applying higher-order operations, such as differentiation (.). Early experi\u00adments demonstrate \nthat Diderot provides both a high-level concise notation for image analysis and visualization algorithms, \nas well as high sequential and parallel performance. Categories and Subject Descriptors D.3.2 [Programming \nLan\u00adguages]: Language classi.cations Very high-level languages; D.3.2 [Programming Languages]: Language \nClassi.cations Concurrent, distributed, and parallel languages; D.3.4 [Program\u00adming Languages]: Processors \nCompilers Keywords Domain Speci.c Languages, Image Analysis, Scien\u00adti.c Visualization, Parallelism 1. \nIntroduction Biomedical researchers use multi-dimensional imaging to study structure and function of \nbiological systems with a variety of imag\u00ading modalities, including confocal microscopy, computed tomog\u00adraphy \n(CT), and magnetic resonance imaging (MRI). Recent ad\u00advances in imaging hardware have increased their \nspeed, resolution, and .exibility. For example, synchrotron radiation microCT pro\u00adduces scalar-valued \nvolume images with 20483 samples (as com\u00adpared to 5123 for clinical CT), and routine MRI scans include \nim\u00adages of vectors (for blood .ow in heart) and tensors (for diffu\u00adsion anisostropy in brain tissue). \nBesides increasing the biomedi\u00adcal value of imaging, the quantities and types of data from modern imaging \nhardware also strain the capabilities of established tools Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. PLDI 12, June 11 16, Beijing, China Copyright \nc . 2012 ACM 978-1-4503-1205-9/12/06. . . $10.00 for image processing. Scientists working with the new \nimaging techniques and applications are the best positioned to create and evaluate methods of image data \nprocessing, but they do not have the programming background to create the ef.cient parallel imple\u00admentations \nrequired to handle the quantity of data involved. In this paper, we present a domain-speci.c language \n(DSL), called Diderot, that simpli.es the portable implementation of paral\u00adlel methods of biomedical \nimage analysis and visualization. Image analysis extracts quantitative or geometric descriptions of the \nim\u00adage structure in order to characterize speci.c properties of the un\u00adderlying organ or tissue. An example \nis extracting ridge lines and valley lines to .nd blood vessels and airways, respectively, from a CT \nlung scan. Visualization combines measurements of local im\u00adage data properties with elements of computer \ngraphics in order to qualitatively depict structures via rendered images. An example is using direct \nvolume rendering to illustrate the over-all shape and inter-relationship of tissue boundaries. Diderot \nsupports a high-level model of computation that is based on continuous tensor .elds. Throughout, we use \ntensors to refer collectively to scalars, vectors, and matrices, which en\u00adcompasses the types of values \nproduced by the imaging modalities mentioned above, as well as values produced by taking spatial derivatives \nof images. Diderot permits programmers to express al\u00adgorithms directly in terms of tensors, tensor .elds, \nand tensor .eld operations, using the same mathematical notation that would be used in vector and tensor \ncalculus (such as . for the gradient). We intend for Diderot to be useful for prototyping image analysis \nand visualization methods in contexts where a meaningful evaluation of the methods requires its application \nto real image data, but the real data volumes are of a size that requires ef.cient parallel computa\u00adtion. \nWith its support for high-level mathematical notation, Diderot will also be useful in educational contexts \nwhere the conceptual transparency of the implementation is of primary importance. This paper makes several \ncontributions. First, we introduce a novel DSL that directly supports higher-order operations on con\u00adtinuous \ntensor .elds. Diderot s type system tracks properties, such as continuity, which helps ensure sensible \nnumerical results. We also describe how principles of tensor calculus may be applied to address the challenges \nof compiling a very-high-level language like Diderot. Lastly, we demonstrate the bene.ts of Diderot relative \nto hand-coded algorithms for parallel computing performance. The paper is organized as follows. In the \nnext section, we re\u00adview the mathematical foundations of image processing on which Diderot is based. \nWe then present Diderot s design in Section 3 and illustrate its features with several examples in Section \n4. We then describe important aspects of our implementation, including the implementation of tensor .elds. \nSection 6 presents performance re\u00adsults, followed by a discussion of related work. We describe future \nplans for the system in Section 8 and then summarize our results in Section 9.  2. Background A review \nof background mathematics will simplify later description of the mathematical elements supported by Diderot, \nand will pro\u00advide the context for our description of the domain of computation for which Diderot is specialized. \nWe adopt tensors as a concrete type in Diderot to provide a gen\u00aderal way of representing the values stored \nin images and produced by operations on images. 0-order tensors, or scalars, capture real\u00advalued samples \nfrom scans typically shown in grayscale (e.g., CT). 1-order tensors, or vectors, describe directional \nquantities such as velocity and spatial derivatives of scalar .elds. 2-order tensors, rep\u00adresented as \nmatrices, describe linear transforms on vectors, .rst derivatives of vector .elds, and second derivatives \nof scalar .elds. Measured image data is discretely sampled on a regular grid (re\u00adgardless of the modality \nor image type), but the underlying objects being scanned exist in a continuous space, which we call world \nspace. Signal processing provides the machinery for reconstructing (or probing) continuous signals from \ndiscrete data via convolution at arbitrary positions [20]. Convolving one-dimensional discrete data V \n[i] with a continuous reconstruction kernel h(x) produces a continuous signal (V . h)(x). Separable convolution \nuses a sin\u00adgle one-dimensional kernel h(x) to create a multi-dimensional ker\u00adnel, for example in three \ndimensions H(x, y, z)= h(x)h(y)h(z). Convolution also provides the means of measuring derivatives in \n.elds. The partial derivatives of F = V .H are found by convolu\u00adtion with separable product of kernels \nand their derivatives, e.g., .F .y .H is found by convolution with .y = h(x)h.(y)h(z). Kernels can be \nchosen according to the needed level of differentiability, while balancing the quality of reconstruction \nwith the computational cost of having larger support (i.e. needing a larger neighborhood of im\u00adage values \nto compute the convolution sum). Diderot is specialized for highly parallel analysis and visualiza\u00adtion \nof continuously differentiable tensor .elds arising from multi\u00addimensional imaging. The continuity of \n.elds is important because relevant features in the image data tend to lie in between pixels and at arbitrary \norientations. Simplifying how programmers can express computations within world space helps prevent the \nresults from needlessly re.ecting the discrete image grid. The differentiability of .elds is required \nbecause many structures of interest are de.ned in terms of spatial derivatives. Extracting the network \nof blood ves\u00adsels from a CT scan is a good example of the need for continuity and differentiability. \nAccurate results depend on tracing the cen\u00adters of vessel pathways in between pixel locations, where \ngradients and Hessians (.rst and second derivatives) are computed to locate the ridge line image features \nthat coincide with the vessels [2, 11]. Tensors may also originate in the image data, as with second-order \ndiffusion tensors measured via MRI, for which a common task is tracing paths along the tensor principal \neigenvector to model brain connectivity [4]. Diderot simpli.es the rapid exploration and im\u00adplementation \nof these types of algorithms by naturally supporting the mathematical ingredients with which they are \nexpressed: vec\u00adtors, tensors, eigensystems, kernels, .elds, and derivatives. We have chosen to develop \na new domain-speci.c language because of weaknesses with existing languages, libraries, and tools. Established \nhigh-level array-processing languages com\u00admonly used for image processing, including Matlab [18], IDL \n[15] and Python/Numpy [25], facilitate operations on entire image ar\u00adrays or their sub-arrays. The computational \ncost is distributed uni\u00adformly over the image, and parallelization according to memory layout is straightforward. \nAn early example of this approach is the image algebra formulation of gray-scale image-to-image compu\u00adtation \ndeveloped by Ritter and Gader [24]. For our intended appli\u00adcations, however, it is not convenient to \nexpress the computation in terms of discrete sets of image data values. The transparent im\u00adplementation \nof the blood vessel extraction and .ber tractography mentioned above, for example, hinges on a notion \nof continuous .elds, a fundamental abstraction available in Diderot. Also, the parallelism we intend \nto capture is not structured according to the underlying image data array, but by the output of the visualization \nor analysis, i.e., rendered pixels in volume rendering, tractography pathways curving through space, \nand points along ridge feature lines. Third, array-processing languages are often optimized for uniform \naccess rather than irregular or sparse access patterns, so techniques that work adequately for two-dimensional \nimages do not scale well to memory-intensive three-dimensional images, es\u00adpecially when the algorithmic \noutput (as with blood vessel extrac\u00adtion) occupies only a small fraction of the volume data. Another \nstrategy for creating image analysis and visualization tools is to use a specialized library that already \nencapsulates many of the useful operations and constructs. A good example is the In\u00adsight Toolkit (ITK) \n[14], a large C++ image processing framework. The drawbacks here, relative to a domain-speci.c language, \nrelate to program conciseness and simplicity. ITK relies heavily on C++ templates to achieve generality \nwith respect to image type and di\u00admension, so the code can be challenging to understand and debug. In \nITK, as with array-processing languages, it is easiest to paral\u00adlelize algorithms that compute over whole \nimage arrays, since this is the structure of the image registration and segmentation routines for which \nITK was originally designed. 3. Language design The class of applications that Diderot targets are characterized \nas consisting of many largely independent subcomputations. For ex\u00adample, the rays in a volume renderer, \nthe paths from .ber tractogra\u00adphy, and the particles in a particle system. In Diderot, these mostly independent \ncomputations are modeled as strands, which execute in a bulk synchronous fashion [26, 31]. Diderot uses \na C-like syn\u00adtax for its computational notation, but this notation is extended with mathematical types \nand operators. 3.1 Types and values Diderot is a monomorphic, statically-typed, language with a fairly \nsmall collection of types. The basic concrete values include strings, booleans, integers, and tensors. \nWe have .ve types of concrete val\u00adues: booleans, integers, strings, tensors, and .xed-size sequences \nof values. The type tensor[s] is a tensor with shape s .{i | 1 < i} *. The number of dimensions in s \n(i.e., the length of s) is the or\u00adder of the tensor. For example, tensor[] is a 0-order tensor and is \nthe type of scalars, while tensor[3] is a 1-order tensor that is the type of 3D vectors. We de.ne type \nsynonyms for common tensor types, including real and vec3. In addition to concrete values, Diderot de.nes \nthree forms of abstract type: images, kernels, and .elds. Images are multidimen\u00adsional arrays of tensor \ndata. The type image(d)[s] is the ab\u00adstract type of image data, where d .{1, 2, 3} speci.es the dimen\u00adsion \nof the data and s speci.es the shape of the tensor values at each image sample. We do not program directly \nwith image data. Instead we use convolution kernels to de.ne a continuous recon\u00adstruction of the discrete \ndata. The type kernel#k is the type of a Ck kernel (i.e., it has k continuous derivatives). Diderot provides \na number of useful built-in kernels, including the C0 tent for linear interpolation (so named because \nof its shape), the C1 interpolating Catmull-Rom cubic spline ctmr, and the C2 (non-interpolating) uniform \ncubic B-spline basis function bspln3 [3]. Finally, con\u00adtinuous tensor .elds have type field#k(d)[s], \nwhere k is the number of continuous derivatives, d is the dimension of the .eld s domain, and s is the \nshape of its range. A .eld is a function from d-dimensional space to tensors with the shape s.  1 input \nreal stepSz = 0.1; // size of steps 2 input vec3 eye; // eye location 3 input vec3 orig; // pixel (0,0) \nlocation 4 input vec3 cVec; // vector between columns 5 input vec3 rVec; // vector between rows 6 input \nreal opacMin; // value with opacity 0.0 7 input real opacMax; // value with opacity 1.0 8 image(3)[] \nimg = load (\"hand.nrrd\"); 9 field#2(3)[] F = img . bspln3;  10 11 strand RayCast (int r, int c) { 12 \nvec3 pos = orig + real(r)*rVec + real(c)*cVec; 13 vec3 dir = normalize(pos -eye); 14 real t = 0.0; 15 \nreal transp = 1.0; 16 output real gray = 0.0; 17 18 update { 19 pos = pos + stepSz*dir; 20 t = t + stepSz; \n21 if (inside (pos, F)) { 22 real val = F(pos); 23 if (val > opacMin) { 24 real opac = 1.0 if (val > \nopacMax) 25 else (val -opacMin)/(opacMax -opacMin); 26 vec3 norm = -normalize(.F(pos)); 27 gray += transp*opac*max(0.0, \n-dir norm); 28 transp *= 1.0 -opac; 29 } 30 } 31 if (t > 40.0) stabilize; 32 } 33 } 34 35 initially \n[ 36 RayCast(ui, vi) | vi in 0 .. imgResV-1, 37 ui in 0 .. imgResU-1 38 ]; Figure 1. Simple direct volume \nrendering code  3.2 Tensor and .eld expressions A key feature of Diderot is that it supports mathematical \nnotation for computing with tensors and .elds. Diderot syntax uses Unicode characters to represent mathematical \nconstants (p) and a rich set of operations on tensors. In addition to standard arithmetic operations, \nthese include dot product (u v), cross product (u\u00d7v), tensor prod\u00aduct (u.v), and vector norm (|u|). Computing \nwith continuous tensor .elds is one of the unique characteristics of Diderot. Field values are constructed \nby convolv\u00ading image data with kernels (img.bspln3), but they can also be de.ned by using higher-order \noperations, such as addition, sub\u00adtraction, and scaling of .elds. Most importantly, Diderot supports \ndifferentiation of .elds using the operators . (for scalar .elds) and .. (for higher-order tensor .elds). \nTwo operations on .elds are testing whether a point x lies within the domain of a .eld F (inside(x, F)) \nand probing a .eld F at a point x (F(x)). As we show in the examples below, probing and differentiating \nare fundamental to extracting geometric information from .elds. 3.3 Program structure A Diderot program \nis organized into three sections: global de.\u00adnitions, which include program inputs; strand de.nitions, \nwhich de.ne the computational core of the algorithm; and initialization, which de.nes the initial set \nof strands. To illustrate this structure and the features of Diderot, we use the simple direct volume \nren\u00adderer shown in Figure 1 as a running example. This computation requires probing both the scalar .eld \nto determine its opacity and the its gradient to determine the surface normal. 3.3.1 Global de.nitions \nLines 1 9 of Figure 1 de.ne the global variables of our example. Global variables in Diderot are immutable. \nThe .rst six of these are marked as input variables, which means that they can be set outside the program \n(they may also have a default value, as in the case of stepSz). The Diderot compiler synthesizes glue \ncode that allows command-line setting of input variables. Line 8 loads image .le \"hand.nrrd\" and binds \nvariable img to it. The type of img is a 3D scalar image, which is checked when the image is loaded. \nThe load function may only be used in global part of the program. Note that we do not specify the representation \nof the image values on disk; i.e., they could be signed ints, .oats, etc. The compiler generates code \nthat maps image values to reals. Line 9 de.nes a scalar .eld F, reconstructed by convolution with the \nbspln3 kernel, providing the C2 continuity re.ected in the type of F. 3.3.2 Strands Much like a kernel \nfunction in CUDA [22] or OpenCL [16], a strand in Diderot encapsulates the computational core of the \nappli\u00adcation. Each strand has parameters (e.g., r and c on line 11), a state (lines 12 13) and an update \nmethod (lines 18 33). The strand state variables are initialized when the strand is created; some vari\u00adables \nmay be annotated as output variables, which de.ne the part of the strand state that is reported in the \nprogram s output. Unlike globals, strand state variables are mutable, but we avoid features, such as \npointers, that would make analysis dif.cult. In addition, strand methods may de.ne local variables (the \nscoping rules are essentially the same as C s). The update method of the RayCast strand starts by advanc\u00ading \nthe strand s position along a ray (lines 20 and 21). It then tests to see if the position lies within \nthe .eld F domain (line 22). If the strand s position is inside the domain, we probe the .eld F and compare \nthe .eld s value to our lower opacity threshold. By chang\u00ading the opacity range, we can pick out different \nfeatures of the im\u00adage (e.g., skin or bone). In lines 27 29, we .nd the contribution of the current position \nto the image using the gradient .eld to compute diffuse lighting. Note that we use Python s syntax for \nconditional expressions (lines 25 and 26). In line 32, we check to see if the ray has gone beyond a distance \nlimit, in which case we stabilize the strand, which means that it ceases to be updated. As we will see \nin later examples, a strand may also have a stabilize method that is invoked when the strand stabilizes. \n 3.3.3 Initialization The last part of a Diderot program is the initialization section, which speci.es \nthe initial set of strands in the computation.1 Diderot uses a comprehension syntax, similar those of \nHaskell or Python, to de.ne the initial set of strands. For example, the vol\u00adume renderer speci.es a \ngrid of initial ray positions in lines 35 38. When the strands are initialized as a grid, it implies \nthat the strands will all stabilize (i.e., they do not die). The grid structure is then preserved in \nthe output. For example, the grid of initial strands created above will produce a grid of pixel values, \none per ray. Diderot also allows one to specify an initial collection of strands by using {} as the brackets \naround the comprehension (in\u00adstead of [] ). In this case, the program s output will be a one\u00addimension \narray of values; one for each stable strand.  3.4 Diderot s type system Diderot has a monomorphic type \nsystem that captures the important mathematical properties of the program, such as the continuity of \n1 In the current version of the language, described in this paper, strands are only created at startup. \nEventually, we plan to support dynamic strand creation (see Section 8).  G . V : image(d)[s]G . h : \nkernel#k G . V .h : field#k(d)[s] G . F : field#k(d)[ ] k> 0 k. = k - 1 G ..F : field#k.(d)[d] G . F \n: field#k(d)[s, d.] k> 0 k. = k - 1 G . .. F : field#k.(d)[s, d.,d] G . F : field#k(d)[s]G . p : tensor[d] \nG . F (p): tensor[s] Figure 2. Key typing judgments for Diderot .elds. Figure 2 presents the most important \nrules in the type system. The .rst rule shows how the properties of the image data and convolution kernel \ndetermine the type of resulting .eld. The next two rules capture the fact that differentiation reduces \nthe continuity of the .eld, but increases its order. Lastly, the typing rule for probe is similar to \nfunction application (as one would expect). 4. Examples Diderot s language design and computational model \nsupport a range of visualization and analysis methods. In this section, we illustrate the language s \nexpressiveness with several examples of computations typical of visualization and analysis algorithms. \n4.1 Implicit Surface Curvature One physical property of interest is the curvature of implicit sur\u00adfaces \nwithin the image. Just as the gradient of a .eld is used to .nd the implicit surface normal, the second \nderivative of the .eld, the Hessian, determines curvature (i.e., the change in normal due to motion along \nthe surface). The principal curvatures .1 and .2 can be computed from the tensor G de.ned by [17] -PHPG \n= |.F | where P = I - n . n T .F n = |.F | H = ...F The eigenvalues of G are 0, .1, and .2, so the principal \ncurvatures appear in the tensor invariants of G trace(G)= .1 + .2 |G| = .12 + .2 2 where |G| is the Frobenius \nnorm of G. With some algebra, we get trace(G)+d .1 = 2 trace(G)-d .2 = 2 where d =2|G|2 - trace(G)2 To \nvisualize surface shape in a volume rendering, we can use a bivariate function of .1 and .2 to assign \ncolor based on local curvature. Diderot code that implements this is shown in Figure 3. Notice that the \nmathematical speci.cation given above translates directly into Diderot; one can easily see that the code \nis an imple\u00admentation of the method that one might derive on the whiteboard. This example also illustrates \nthe use of a .eld to implement a color assignment function (i.e., the RGB .eld). We sample this .eld \nusing bilinear interpolation, which is provided by the tent kernel. The resulting image from applying \nthis technique to a synthetic data 1 // RGB colormap of (kappa1,kapp2) 2 field#0(2)[3] RGB = tent . load(xfer); \n3 \u00b7\u00b7\u00b7 4 update { 5 \u00b7\u00b7\u00b7 6 vec3 grad = -.F(pos); 7 vec3 norm = normalize(grad); 8 tensor[3,3] H = ...F(pos); \n9 tensor[3,3] P = identity[3] -norm.norm; 10 tensor[3,3] G = -(P H P)/|grad|; 11 real disc = sqrt(2.0*|G| \n2 -trace(G) 2); 12 real k1 = (trace(G) + disc)/2.0; 13 real k2 = (trace(G) -disc)/2.0; 14 // find material \nRGBA 15 vec3 matRGB = 16 RGB([max(-1.0, min(1.0, 6.0*k1)), 17 max(-1.0, min(1.0, 6.0*k2))]); 18 \u00b7\u00b7\u00b7 19 \n} Figure 3. Computing the surface color based on implicit surface curvatures Figure 4. Volume rendering \nwith color determined by implicit surface curvatures (.1,.2) set is shown in Figure 4 (this .gure also \nincludes an image of the bivariate colormap function). 4.2 Line Integral Convolution (LIC) Line integral \nconvolution (LIC) is a vector .eld visualization method that can be concisely expressed in Diderot. LIC \nvisual\u00adizes a vector .eld by blurring an underlying noise texture (a scalar .eld) along vector .eld streamlines \n[7]. Streamlines are paths ev\u00aderywhere tangent to the vector .eld, computed by numerical inte\u00adgration; \nin our benchmark we use the midpoint method (a second\u00adorder Runge-Kutta method). For each pixel in the \noutput, we de.ne a strand that computes a streamline. The pixel value is the aver\u00adage of noise texture \nsamples taken along all computed vertices of the streamline seeded at that pixel. The Diderot implementa\u00adtion \nin Figure 5 simultaneously computes downstream (forw) and upstream (back) segments of the streamlines \nof the vectors synthetic vector .eld, sampling the rand scalar .eld at each step, and stopping each after \nstepNum steps. The output graylevel con\u00adtrast is modulated by the the vector .eld velocity at the seedpoint \n|V(pos0)|, creating the LIC result shown in Figure 6.  4.3 Particle-based feature sampling One class \nof applications that Diderot targets is the use of parti\u00adcles to detect and sample image features, such \nas isocontours [21]. As a simple example, we consider detecting isocontours in a 2D grayscale image. \nFigure 7 gives the strand de.nition for this pro\u00adFigure 5. Line Integral Convolution (LIC)  1 field#1(2)[2] \nV = load(\"vectors.nrrd\") . ctmr; 2 field#0(2)[] R = load(\"rand.nrrd\") . tent; 3 4 strand LIC (vec2 pos0) \n{ 5 vec2 forw = pos0; 6 vec2 back = pos0; 7 output real sum = R(pos0); 8 int step = 0; 9 10 update { \n11 forw += h*V(forw + 0.5*h*V(forw)); 12 back += h*V(back -0.5*h*V(back)); 13 sum += R(forw) + R(back); \n14 step += 1; 15 if (step == stepNum) { 16 sum *= |V(pos0)| / real(1 + 2*stepNum); 17 stabilize; 18 } \n19 } 20 }  Figure 6. Line Integral Convolution (LIC) on synthetic data gram. This program de.nes an \ninitial collection of strands that are positioned in a 2D grid pattern. The image value at a strand s \nini\u00adtial position determines the isovalue f0 that it will search for. The strand s update method uses \nNewton-Raphson iteration to .nd the root of F (x)= f(x) - f0 by motion along the normalized gra\u00addient \n.F/|.F |. The strand s search can terminate in one of two ways: if the length of the position update \n|delta| falls below epsilon, then the strand stabilizes its position will be the out\u00adput. Otherwise, \nif the strand wanders outside the .eld domain or takes too many steps, then the strand dies and produces \nno output. Since some strands die during execution, the .nal collection of sta\u00adble strands will be a \nsubset of the initial collection. Figure 8 shows a visualization of running this algorithm on a grayscale \nversion of a portrait of Denis Diderot by Louis-Michel van Loo. The .nal posi\u00adtions of the stable strands \nare rendered as green dots. Note that the image is depicted with nearest-neighbor interpolation to show \nits individual pixels as squares, while the continuous interpolation (af\u00adforded by convolution with the \nCatmull-Rom cubic spline ctmr) creates isocontours that smoothly trace between pixels. 5. Implementation \nCompiling a very-high-level language like Diderot requires a mix of traditional compiler techniques with \ndomain-speci.c transfor\u00admations and optimizations. In this section, we give an overview of our language \nimplementation and describe the techniques that we 1 field#1(2)[] f = ctmr . load(\"ddro.nrrd\"); 2 3 strand \nsample (int ui, int vi) { 4 output vec2 pos = \u00b7 \u00b7 \u00b7; 5 // set isovalue to closest of 50, 30, or 10 6 \nreal f0 = 50.0 if f(pos) >= 40.0 7 else 30.0 if f(pos) >= 20.0 8 else 10.0; 9 int steps = 0; 10 update \n{ 11 if (!inside(pos, f) || steps > stepsMax) 12 die; 13 vec2 grad = .f(pos); 14 vec2 delta = // the \nNewton-Raphson step 15 normalize(grad) * (f(pos) -f0)/|grad|; 16 if (|delta| < epsilon) 17 stabilize; \n18 pos -= delta; 19 steps += 1; 20 } 21 } Figure 7. Detecting isocontours Figure 8. Isocontour detection \nin a grayscale image have developed for compiling the higher-order .eld operations. We also provide a \nbrief description of our runtime system. 5.1 Compiler overview The Diderot compiler comprises roughly \n19,000 lines of Standard ML code, which is organized into three main phases: the front-end, optimization \nand lowering, and code generation. The front-end consists of parsing, type checking, and simpli\u00ad.cation. \nAlthough Diderot is a monomorphic language, most of its operators have instances at multiple types. For \nexample, addi\u00adtion works on integers, tensors of all shapes, and .elds. Since hav\u00ading to type check each \noperator as a special case would be pro\u00adhibitively complicated, we use a mix of ad hoc overloading and \npolymorphism in the type checker. The internal representation of types includes kinded type variables, \nshape variables, and dimen\u00adsion variables. The type checking process introduces constraints between the \nvariables, which are solved by uni.cation. Once a pro\u00adgram is type checked, the operator instances are \ninstantiated at spe\u00adci.c monotypes. The typed AST is then converted into a simpli.ed representation, \nwhere temporaries are introduced for intermediate values and operator are applied only to variables. \nAt this point we also duplicate code, as necessary, to ensure that .elds are statically determined. For \nexample, if the source program contained the line  real y = (F1 if b else F2)(x); the simpli.ed representation \nis transformed to code that is equiva\u00adlent to real y = F1(x) if b else F2(x); This transformation is \nnecessary because of the way we compile probe operations. While it could result in exponential code growth, \nwe believe that in practice code growth will not be signi.cant. The optimization and lowering occurs \nover a series of three in\u00adtermediate representations (IRs) based on Static Single Assignment (SSA) form \n[9]. These IRs share a common control-.ow graph rep\u00adresentation, but differ in their types and operations. \nHighIR is essentially a desugared version of the source language with source-level types and operations. \nMidIR supports vectors, transforms between coordinate spaces, loading image data, and kernel evaluations. \nAt this stage, .elds and probes have been compiled away into lower-level code. LowIR supports basic operations \non vectors, scalars, and memory objects. The translations between these representations replaces higher\u00adlevel \noperations with their equivalent lower-level operations. For example, .eld probes in the HighIR are expanded \ninto the convo\u00adlution of image samples and kernel evaluations in the MidIR. We discuss this particular \nexpansion in more detail below. The .nal phase is code generation. We have separate backends for different \ntargets: sequential C code with vector extensions [12], parallel C code, OpenCL [16], and CUDA [22] (planned). \nBecause these targets are all block-structured languages, our .rst step in code generation is to convert \nthe LowIR SSA representation into a block-structured AST. The target-speci.c backends translate this \nrepresentation into the appropriate representation and augment the code with type de.nitions and runtime \nsupport. The output is then passed to the host system s compiler.  5.2 Implementing .eld operations \nDiderot s .elds are abstract values that represent continuous func\u00adtions. As such, we use a symbolic \nrepresentation of .eld values in the compiler that is used to generate code for the inside test and for \nprobing a .eld. In the simplest case, code that probes a tensor .eld is translated into code that maps \nthe world-space position to image space and then convolves the image values from the neigh\u00adborhood of \nthe position using a kernel. This translation occurs when the program is converted from HighIR to MidIR; \nwe then expand kernel evaluations into vectorized arithmetic when we convert from MidIR to LowIR. While \nthe case of probing a .eld that is de.ned directly by convolving an image with a kernel is straightforward, \nin general the problem is more complicated, since .elds can be de.ned by higher-order operations such \nas scaling, addition, and differenti\u00adation. Thus, before we can translate HighIR to MidIR, we must normalize \nthe .eld computations. Effectively, this process lowers higher-order operations that work on .elds to \noperations that work on tensors. We call this process .eld normalization. We can formalize .eld normalization \nas a translation from a lan\u00adguage of .eld expressions to a normalized language of .eld probes and tensor \noperations. Figure 9 gives the grammars of these two languages; we have simpli.ed the presentation by \nomitting many operators available in Diderot. In the source language (Figure 9a), we have tensor-valued \nexpressions (denoted by e) and .eld-valued (f1 + f2)(x) . f1(x)+ f2(x) (e * f)(x) . e * f(x) .(f1 + f2) \n..f1 + .f2 .(e * f) . e *.f .(V . h) . V . .h V . .i+1 .(V . .ih) . h Figure 11. Probing a 2D .eld F \n= V . h at x, where M is the mapping from world coordinates to image-space coordinates expressions (denoted \nby f). Using the rewrite rules given in Fig\u00adure 10, we can transform source-language expressions into \nthe nor\u00admalized language of Figure 9b. The normalized language enforces three key invariants on the structure \nof the computation: 1. All differentiation operations have been pushed down to the ker\u00adnels in convolutions. \nWe add a superscript to the . operator to specify the level of differentiation. .2 should not be confused \nwith the standard notation for the Laplacian. 2. The .elds involved in probe operations are de.ned directly \nas convolutions. 3. Arithmetic operations on .elds have been lowered to operations on tensors.  These \nproperties are key to being able to synthesize code for the probe operations, as is described in the \nnext section.  5.3 Synthesizing probe operations Once we have normalized the .eld operations as described \nabove, we must still synthesize code to implement .eld probes. These operations get compiled down into \ncode that maps the world-space coordinates to image space and then convolves the image values in the \nneighborhood of the position, as is illustrated by Figure 11. As discussed in Section 2, we use separable \nkernels to reconstruct the value of a .eld at a speci.c point. For example, if F = V . h is a 2-dimensional \nscalar .eld, then probing F at the location x is de.ned by the equation .ss F (x)= V [n + .i, j.]h(fx \n- i)h(fy - j) i=1-sj=1-s where s is the support of h, n = .M-1 x. and f = M-1 x - n. One of the main \nchallenges of synthesizing code for probe oper\u00adations is that the shape of the result and the nesting \nof summations can be arbitrarily complicated. For example, assume that F is a 2D scalar .eld of type \nfield#k(2)[ ] and G is a 2D vector .eld of type field#k(2)[2]. Then the probe expressions .. G(x) and \n...F (x) both have type tensor[2, 2], but the code re\u00adquired to implement the two probes is very different. \nTensor cal\u00adculus provides the notational tools to manage this process, in that the differentiation operator \n(.) can be treated as a vector of partial\u00ade ::= f(x) .eld probe  | \u00b7\u00b7\u00b7 f ::= V . h convolution |.f differentiation \n| f + f .eld addition | e * f .eld scaling (a) source language e ::= f (x) .eld probe | e + e tensor \naddition | e * e tensor scaling | \u00b7\u00b7\u00b7 f ::= V . h convolution .i h ::= h kernel with i = 0 levels of \ndifferentiation (b) normalized language Figure 9. The source and target languages of .eld normalization, \nwhere V denotes images and h denotes kernels differentiation operators. For example, in 2D we have . \n. = .x . .y We can use this representation to handle the probing of the gradient of a scalar .eld F .F \n(x)=(V . .h)(x) . h .x = V . . (x) h .y ij V [n + .i, j.]h.(fx - i)h(fy - j) ij V [n + .i, j.]h(fx - \ni)h.(fy - j) Note that the partial-differentiation operators tell us where to use h and where to use \nthe .rst derivative h. in the reconstruction. This approach generalizes to multiple levels of differentiation \nin a natural way. For example, the Hessian of F is de.ned as ...F , which is normalized to V . .2h, which \ncan be written as V . ...h. Thus we have .2 .2 .2 .2 hh .x2 .xy .x2 .xy ...h = h = .2 .2 .2 .2 hh .xy \n.y2 .xy .y2 The resulting x \u00d7 2 matrix de.nes the structure of the result of probing the Hessian of F \n. When the image data is not scalar (e.g., the 2D vector .eld G mentioned above), we have further outer \niteration over the shape of the image s range. For example, if G = V . h is a 2D vector .eld, then the \nprobe G(x) will be expressed as G(x)=(V . h)(x) V1 . h =(x) V2 . h ij V1[n + .i, j.]h(fx - i)h(fy - j) \nij V2[n + .i, j.]h(fx - i)h(fy - j) Note that while the result of this probe has the same type as .F \n(x), the underlying computation is different. Our implementation must also correctly handle mapping be\u00adtween \ncoordinate spaces. An image dataset comes with orienta\u00adtion information that can be represented as a \ntransform M map\u00adping from position in the image s index space to position in world space. We use the \ninverse mapping M-1 to convert the position of a probe, which is in world space, back to image space, \nbut we also have to consider the mapping of the probe s result back to world space. Gradients are measured \nin image space by convolution with kernel derivatives (as described above). Being a covariant quantity, \ngradients are converted to world space by M-T , the inverse trans\u00adpose of the transform M for converting \npositions (a contravari\u00adant quantity) to world space [27]. On the other hand, when prob\u00ading a vector \n.eld created by convolution of a vector-valued image dataset, the vectors are assumed to be already represented \nin world space, and hence need no post-probe transformation. The .nal step in generating executable code \nfor .eld probes is to expand the kernel evaluations. This expansion takes place as part of the translation \nfrom MidIR to LowIR. The kernels that Diderot supports are all piecewise polynomial, so it straightforward \nto symbolically differentiate them. The process described in this section results in code that is easily \nvectorized (in fact the MidIR and LowIRs support vectorized operations).  5.4 Domain-speci.c optimizations \nIn addition to the transformations required to support the opera\u00adtions on .elds that we have described \nabove, the Diderot compiler performs a number of other optimizations. Speci.cally, we imple\u00adment an extended \nform of constant folding and dead-code elim\u00adination that shrinks (or contracts) the program [1] and we \nelim\u00adinate redundant computations using value numbering [5]. While these are optimizations that are found \nin many compilers, when they are combined with the domain-speci.c operators in our IR, they produce domain-speci.c \noptimizations that a general-purpose compiler would be unlikely to achieve. For example, if a program \nprobes both a .eld F and the gradient .eld .F at the same po\u00adsition, there are redundant convolution \ncomputations that can be detected and eliminated. Another example is the symmetry of the Hessian, which \nis also detected by our value-numbering pass.  5.5 Runtime support The Diderot runtime is comprised \nof common code for loading image data from Nrrd .les [29] and writing the program s output to either \na text or Nrrd .le.2 The common part of the runtime also provides support for initializing input variables. \nIn addition to the common code, there is target-speci.c code for managing strands. Recall that Diderot \nuses a bulk-synchronous parallelism model [26, 31]. In this model, execution is divided into super steps; \nduring a super-step each strand s update method is evaluated once. The program executes until all of \nthe strands are either stabilized or dead. For the sequential target, the runtime implements this model \nas a loop nest, with the outer loop iterating once per super-step and the inner loop iterating once per \nstrand. The parallel version of the runtime is implemented using POSIX threads. The system creates a \ncollection of worker threads (the default is one per hardware core/processor) and manages a work\u00adlist \nof strands. To keep synchronization overhead low, the strands in the work-list are organized into blocks \nof strands (currently 4096 strands per block). During a super-step, each worker grabs and updates strands \nuntil the work-list is empty. Barrier synchronization is used to coordinate the threads at the end of \na super step. 2 The Nrrd .le format is designed for multi-dimensional image data and includes metadata \nfor the image s coordinate system and axes.  6. Performance evaluation In addition to providing a very-high-level \nprogramming model, we are also interested in providing high-performance; especially on modern multicore \nsystems. In this section we present results from four benchmark programs that represent typical workloads \nfor im\u00adage analysis, including parallel scaling results for the Diderot im\u00adplementations. We also compare \nDiderot s performance with hand\u00adwritten C programs that use the Teem library [30]. Teem includes convolution-based \nreconstruction of values and derivatives from discretely sampled .elds, which closely matches the mathematical \nabstraction of a .eld supported by Diderot, but with a less conve\u00adnient API, as described in Section \n7. 6.1 Experimental method Our test machine is an 8-core MacPro with 2.93 GHz Xeon X5570 processors and \n12Gb of memory running Mac OS X 10.7.2. We used the Apple clang C compiler (version 3.0) to compile the \nTeem version of the benchmarks and as a backend to the Diderot compiler. In both cases, code was compiled \nwith optimization level -O3. For each benchmark, we report the average of 40 runs on a lightly-loaded \nmachine; the standard deviations for these experi\u00adments were typically less than 0.1 seconds. 6.2 Benchmarks \nWe present results from four benchmark programs, which are sum\u00admarized in Table 1. In the table we give \nthe lines of code of both the Teem version (written in C) and the Diderot version. We fur\u00adther split \nout the lines of code in the computational core of the benchmark. For the Teem version, this number is \nthe loop-nest that performs the computation, while for the Diderot program it is the update method.3 \nFrom this table it can be seen that Diderot pro\u00advides a signi.cant advantage in conciseness over using \nthe Teem library, which, itself is a big advantage over writing the code di\u00adrectly in C. The lines-of-code \nnumbers do not include comments, blank lines, or timing code. The table also includes the number of initial \nstrands for each program. These benchmark programs, which were chosen to be represen\u00adtative of the kinds \nof applications for which Diderot has been de\u00adsigned, are described in more detail below. vr-lite: A \nsimple volume renderer that displays surfaces using Phong shading, which depends on the gradient in the \nscalar .eld. This simple program is typical of one that might be used in an educational context. illust-vr: \nA more complex volume renderer that produces illustra\u00adtive (or non-photo-realistic) renderings, using \nvarious curvature computations based on the gradient and Hessian. This example showcases the tensor calculations \nthat are awkward to express in other languages. lic2d: Computes a line integral convolution [7] visualization \nof a vector .eld, in which a noise texture is blurred along vector .eld streamlines. This program highlights \nconvolution and differen\u00adtiation in a vector .eld. ridge3d: An initial uniform distribution of points \nwithin a portion of CT scan of a lung is moved iteratively towards the cen\u00adters of blood vessels, using \nNewton optimization to compute ridge lines [11]. This program computes the eigenvalues and eigenvectors \nof the Hessian, and permits the implementation to closely resemble the mathematical de.nition of a ridge \nline. 3 For the Teem versions of the benchmark, the computational core does not include the setup code \nused to specify the .eld properties that are being probed.  6.3 Measurements For each benchmark program, \nwe measured the wall-clock time it took to execute the computation kernel. This measurement ex\u00adcludes \nthe initial loading of the image data, other initialization, and the writing of the result. Typically, \nthe excluded time was on the or\u00adder of 0.2 seconds. Table 2 presents the results of our experiments. \nFor each benchmark program, we present the execution time for the Teem version, the sequential Diderot \nversion, and the paral\u00adlel version on 1, 2, and 8 processors. The Teem programs use a mix of single and \ndouble-precision arithmetic in their implementa\u00adtion (the Teem Library uses doubles internally). When \ncompiling a Diderot program, on the other hand, the user must decide if re\u00adals are represented as single \nor double-precision .oats. To verify that the difference in precision is not reason that Diderot is faster, \nwe measured both the single and double-precision versions of the Diderot programs. From Table 2, it can \nbe seen that while using double-precision arithmetic does result in a signi.cant slow-down, it does not \nwholly explain the difference. We have done some de\u00adtailed pro.ling of the Teem versions of several of \nthe benchmarks, and we believe that a major part of the difference is Teem s use of callbacks to implement \n.eld probes. Diderot s use of vector opera\u00adtions (i.e., SSE) may also account for some of the difference. \nThus, compared to Teem, Diderot provides a more concise programming notation, better performance, and \neasier support for parallel com\u00adputation.  6.4 SMP performance The Diderot execution model is designed \nfor both simplicity and ef.cient execution on parallel hardware. Since strands execute in\u00addependently, \nwe expect ef.cient scaling on parallel hardware. In Figure 12, we present the parallel speedup curves \nfor the single\u00adprecision version of our benchmarks. We use the sequential version of these programs without \nthe overhead of scheduling (i.e., the Seq. column from Table 1). As we expect, all of the benchmarks \nscale well. For vr-lite, we see some tailing-off at eight threads, which we believe is because of lack \nof work (notice from Table 1 that vr-lite has the fewest strands). With some experimentation, we found \nthat the biggest limitation to parallelism was the lock that controls ac\u00adcess to the work-list. With \nsmaller blocks of strands (recall that we use 4,096 strands per block), we saw a signi.cant reduction \nin par\u00adallel scaling. Other than adjusting the strand-block size, we have  Program LOC (total:core) \nTeem Diderot # strands Description vr-lite 223:44 68:26 165,600 Simple volume-renderer with Phong shading \nrunning on CT scan of hand illust-vr 324:61 83:39 307,200 Fancy volume-renderer with cartoon shading \nrunning on CT scan of hand lic2d 260:66 53:32 572,220 Line Integral Convolution in 2D running on synthetic \ndata ridge3d 360:55 44:24 1,728,000 Particle-based ridge detection running on lung data Table 1. The \nbenchmark programs Program Teem Diderot single-precision Seq. 1P 2P 8P Diderot double-precision Seq. \n1P 2P 8P vr-lite 26.77 14.92 14.95 7.59 2.62 16.52 16.44 8.35 2.92 illust-vr 132.85 54.17 54.40 27.55 \n8.00 80.63 82.16 41.18 11.86 lic2d 3.22 2.02 2.03 1.02 0.30 2.47 2.47 1.24 0.37 ridge3d 11.18 8.40 8.36 \n4.22 1.14 9.34 10.27 5.16 1.39 Table 2. Average performance results over 40 runs (times in seconds) \nnot done any performance tuning of the runtime, so we expect that we will be able to improve performance \nand scaling. 7. Related work There are a variety of domain-speci.c languages and frameworks that provide \nsimilar features supported in Diderot. Many of these are examples of using the power of DSLs to provide \nprogramers with high-level parallel programming models. Teem is a collection of libraries that support \nimage analy\u00adsis and visualization algorithms [30]. Teem provides support for convolution-based measurements \nvia kernels, but as a C library, the programming model is much less direct. A Diderot programmer can \ndeclare a scalar .eld F and then use F(pos) and .F(pos) for the .eld value and gradient at some point \npos. A Teem pro\u00adgrammer would have to create a probing context in which image data and kernels are set, \nspecify the list of all quantities that are to be computed for every probe, and then update the probe \ncontext to allocate buffers to store probe results. After calling the probe function at a particular \nlocation pos, the programmer then copies the value and gradient out of the probe buffer. Shadie is a \nDSL for direct-volume rendering applications that is targeted at GPUs [13]. The framework is based on \nthe notion of shaders, which are functions that de.ne what happens along a given ray for the entire visualization. \nSimilar to Diderot, shaders support the ability to perform computations on continuous .elds and their \nderivatives. But Shadie is limited to direct-volume ren\u00addering applications, whereas Diderot supports \nother visualization applications such as LIC, .ber tractography, and particle systems. Furthermore, Shadie \ns support for .eld operations is restricted to a set of built-in functions (e.g. cubic_query_3d for probing \nthe gradient .eld reconstructed using a cubic spline), whereas Diderot provides a collection of orthogonal \noperations on .elds. Scout is a high-level DSL for image visualization and analy\u00adsis [19]. Scout supports \na data-parallel programming model based on the concept of shapes, which are regions of voxels in the \nim\u00adage data. Scout is designed for a different class of algorithms than Diderot. Speci.cally, algorithms \nthat are de.ned in terms of com\u00adputations over discrete voxels, such as stencil algorithms, rather than \nover a continuous tensor .elds. Spiral is a DSL for digital signal processing algorithms [23]. Its implementation \nencapsulates signi.cant mathematical knowl\u00adedge of the various complex algorithms in digital signal process\u00ading, \nwhich allows Spiral to generate .ne-tuned code for a given platform. This notion of developing a high-level \nmathematical pro\u00adgramming model is also emphasized in Diderot. As with Spiral, Diderot allows its users \nto focus more on the mathematics and al\u00adlowing the system to generate high-performance code for their \nplat\u00adform. Although Spiral provides a powerful mathematical model, it targeted at the somewhat different \ndomain of signal processing. Delite is an ongoing project to support the development of em\u00adbedded parallel \nDSLs [6, 8] and was the inspiration for the Diderot project. The framework aids in decreasing the burden \nof paralleliz\u00ading DSL programs (e.g., scheduling) and has been used for machine learning [28] and mesh-based \nPDEs [10]. While the Delite project and Diderot project share the idea that parallel DSLs are an ef\u00adfective \nway to provide portable parallelism, they differ in the way that the DSL is presented to the user. Delite \nembeds the DSL in Scala, which limits the notational .exibility of the design, whereas Diderot s syntax \nis designed to .t its application domain. Further\u00admore, Diderot s runtime has been designed to allow \nDiderot pro\u00adgrams to be embedded as libraries in any host language that sup\u00adports calling C code, whereas \nDelite applications must be written in Scala, which is not a common language in the visualization com\u00admunity. \n8. Future work While the current version of Diderot is suf.cient to implement many interesting visualization \nalgorithms, we have plans to extend the language design, add additional targets, and otherwise improve \nthe system. These plans include obvious extensions, such as adding functions and simple data structures, \nas well as extensions targeted at image analysis algorithms. We describe some of the latter plans here. \n8.1 Portable parallelism One implementation challenge of Diderot is to extract maximum performance across \na wide range of different parallel architectures. Diderot already includes an ef.cient multicore parallel \nimplemen\u00adtation. We also have an OpenCL backend that is targeted at GPUs, but we plan to extend our implementation \nto support clusters, in\u00adcluding GPU clusters.  8.2 Richer concurrency model The current design of Diderot \nlimits strand creation to initialization time and does not provide any mechanism for strand communi\u00adcation. \nWhile the number of active strands can dynamically vary, because of die, it only monotonically decreases. \nFor algorithms, such as particle systems, where strands are used to explore the im\u00adage space, it is useful \nto be able to dynamically create new strands on the .y. But it only makes sense to create new strands \nwhen there is a region that is relatively empty of strands. Therefore, we plan to extend the programming \nmodel with three mechanisms: a mech\u00adanism for creating new strands, a mechanism for reading the state \nof nearby strands, and a mechanism for global computations (i.e., reductions) over the whole set of active \nand stable strands. Keep\u00ading with our bulk-synchronous semantics, these mechanisms will be tied to the \nsuper steps. New strands will come into existence at the end of the super step, strands will only be \nable to read the state of other strands as it was at the beginning of the super step, and the global \ncomputations will occur at the end of the super step.  8.3 More tensor math We plan to extend our implementation \nto support larger set of tensor and .eld operations, such as divergence (. ) and curl (.\u00d7). To support \nthis richer set of operators, however, we will have to make changes in our internal representation. Speci.cally, \nwe are exploring the use of Einstein notation as a compact way to represent tensor computations. 9. Conclusion \nWe have presented Diderot, a parallel domain-speci.c language for image analysis and visualization algorithms. \nDiderot provides the programmer with a very-high-level programming model based on the concepts and notations \nof tensor calculus, which allows the al\u00adgorithms to be expressed in their natural mathematical notation. \nWe have described the techniques that we use to implement this high\u00adlevel model and have presented performance \ndata the demonstrates that we can have both high-level notation and high-performance in the same system. \nAcknowledgments We thank the NVIDIA Corporation for their generous donation of both hardware and .nancial \nsupport. This material is based upon work performed in part while John Reppy was serving at the National \nScience Foundation. References [1] A. W. Appel and T. Jim. Shrinking lambda expressions in linear time. \nJFP, 7:515 540, Sept. 1997. [2] S. R. Aylward and E. Bullitt. Initialization, noise, singularities, and \nscale in height ridge traversal for tubular object centerline extraction. IEEE TMI, 21(2):61 75, Feb \n2002. [3] R. Bartels, J. Beatty, and B. Barsky. An Introduction to Splines for Use in Computer Graphics \nand Geometric Modeling. Morgan Kaufmann Publishers, New York, NY, 1987. [4] P. J. Basser, S. Pajevic, \nC. Pierpaoli, J. Duda, and A. Aldroubi. In vivo .ber tractograpy using DT-MRI data. Magnetic Resonance \nin Medicine, 44:625 632, 2000. [5] P. Briggs, K. D. Cooper, , and L. T. Simpson. Value numbering. SP&#38;E, \n27(6):701 724, June 1997. [6] K. J. Brown, A. K. Sujeeth, H. Lee, T. Rompf, H. Cha., M. Odersky, and \nK. Olukotun. A heterogeneous parallel framework for domain\u00adspeci.c languages. In PACT 11, Oct. 2011. \n[7] B. Cabral and L. C. Leedom. Imaging vector .elds using line integral convolution. In SIGGRAPH 93, \npages 263 270, New York, NY, Aug. 1993. ACM. [8] H. Cha., Z. Devito, A. Moors, T. Rompf, A. K. Sujeeth, \nP. Hanrahan, M. Odersky, and K. Olukotun. Language virtualization for heteroge\u00adneous parallel computing. \nIn OOPSLA 10, pages 835 847, Oct. 2010. Part of the Onward! 2010 Conference. [9] R. Cytron, J. Ferrante, \nB. K. Rosen, M. N. Wegman, and F. K. Zadeck. Ef.ciently computing static single assignment form and the \ncontrol dependence graph. ACM TOPLAS, 13(4):451 490, Oct 1991. [10] Z. DeVito, N. Joubert, F. Palacios, \nS. Oakley, M. Medina, M. Barrien\u00adtos, E. Elsen, F. Ham, A. Aiken, K. Duraisamy, E. Darve, J. Alonso, \nand P. Hanrahan. Liszt: A domain speci.c language for building portable mesh-based PDE solvers. In SC \n11, pages 1 12, Nov. 2011. [11] D. Eberly. Ridges in Image and Data Analysis. Kluwer Academic Publishers, \nBoston, MA, 1996. [12] Using vector instructions through built-in functions. Free Software Foundation. \nURL http://gcc.gnu.org/onlinedocs/gcc/ Vector-Extensions.html. [13] M. Hasan, J. Wolfgang, G. Chen, and \nH. P.ster.. Shadie: A domain\u00adspeci.c language for volume visualization. Draft paper; available at http://miloshasan.net/Shadie/shadie.pdf, \n2010. [14] L. Ibanez and W. Schroeder. The ITK Software Guide. Kitware Inc., 2005. [15] IDL: Interactive \nData Language. ITT Visual Information Solutions. http://www.ittvis.com/ProductServices/IDL.aspx. [16] \nThe OpenCL Speci.cation (Version 1.1). Khronos OpenCL Work\u00ading Group, 2010. Available from http://www.khronos.org/ \nopencl. [17] G. Kindlmann, R. Whitaker, T. Tasdizen, and T. Moller. Curvature\u00adbased transfer functions \nfor direct volume rendering: Methods and applications. In VIZ 03, pages 67 74, Los Alamitos, CA, Oct. \n2003. IEEE Computer Society Press. [18] MATLAB The language of technical computing. The Mathworks, Inc. \nhttp://www.mathworks.com/products/matlab. [19] P. McCormick, J. Inman, J. Ahrens, J. Mohd-Yusof, G. Roth, \nand S. Cummins. Scout: A data-parallel programming language for graph\u00adics processors. Journal of Parallel \nComputing, 33:648 662, Nov. 2007. ISSN 0167-8191. [20] E. H. W. Meijering, W. J. Niessen, and M. A. Viergever. \nQuantitative evaluation of convolution-based methods for medical image interpola\u00adtion. Medical Image \nAnalysis, 5(2):111 126, June 2001. [21] M. D. Meyer, P. Georgel, and R. T. Whitaker. Robust particle \nsystems for curvature dependent sampling of implicit surfaces. In SMI 05, pages 124 133, June 2005. [22] \nNVIDIA CUDA C Programming Guide (Version 4.0). NVIDIA, May 2011. Available from http://developer.nvidia.com/ \ncategory/zone/cuda-zone. [23] M. Puschel, J. Moura, J. Johnson, D. Padua, M. Veloso, B. Singer, J. Xiong, \nF. Franchetti, A. Gacic, Y. Voronenko, K. Chen, R. Johnson, and N. Rizzolo. SPIRAL: Code generation for \nDSP transforms. Proc. of the IEEE, 93(2):232 275, Feb. 2005. [24] G. X. Ritter and P. D. Gader. Image \nalgebra techniques for parallel image processing. JPDC, 4(1):7 44, Feb. 1987. [25] NumPy: Numerical Python. \nScienti.c Tools for Python. http: //numpy.scipy.org. [26] D. Skillicorn, J. M. Hill, and W. McColl. Questions \nand answers about BSP. Scienti.c Programming, 6(3):249 274, 1997. [27] B. Spain. Tensor Calculus: A Concise \nCourse. Dover, Mineola, NY, 2003. [28] A. K. Sujeeth, H. Lee, K. J. Brown, T. Rompf, H. Cha., M. Wu, \nA. R. Atreya, M. Odersky, and K. Olukotun. Optiml: An implicitly parallel domain-speci.c language for \nmachine learning. In ICML 11, June 2011. [29] NRRD: Nearly Raw Raster Data. Teem Library, . http://teem. \nsf.net/nrrd. [30] Teem website at http://teem.sf.net. Teem Library, . [31] L. G. Valiant. A bridging \nmodel for parallel computation. CACM, 33 (8):103 111, Aug. 1990.    \n\t\t\t", "proc_id": "2254064", "abstract": "<p>Research scientists and medical professionals use imaging technology, such as <i>computed tomography</i> (CT) and <i>magnetic resonance</i> imaging (MRI) to measure a wide variety of biological and physical objects. The increasing sophistication of imaging technology creates demand for equally sophisticated computational techniques to analyze and visualize the image data. Analysis and visualization codes are often crafted for a specific experiment or set of images, thus imaging scientists need support for quickly developing codes that are reliable, robust, and efficient.</p> <p>In this paper, we present the design and implementation of Diderot, which is a parallel domain-specific language for biomedical image analysis and visualization. Diderot supports a high-level model of computation that is based on continuous tensor fields. These tensor fields are reconstructed from discrete image data using separable convolution kernels, but may also be defined by applying higher-order operations, such as differentiation (&#8711;). Early experiments demonstrate that Diderot provides both a high-level concise notation for image analysis and visualization algorithms, as well as high sequential and parallel performance.</p>", "authors": [{"name": "Charisee Chiw", "author_profile_id": "81502797249", "affiliation": "University of Chicago, Chicago, IL, USA", "person_id": "P3471166", "email_address": "cchiw@cs.uchicago.edu", "orcid_id": ""}, {"name": "Gordon Kindlmann", "author_profile_id": "81502767758", "affiliation": "University of Chicago, Chicago, IL, USA", "person_id": "P3471167", "email_address": "glk@cs.uchicago.edu", "orcid_id": ""}, {"name": "John Reppy", "author_profile_id": "81100590527", "affiliation": "University of Chicago, Chicago, IL, USA", "person_id": "P3471168", "email_address": "jhr@cs.uchicago.edu", "orcid_id": ""}, {"name": "Lamont Samuels", "author_profile_id": "81502809960", "affiliation": "University of Chicago, Chicago, IL, USA", "person_id": "P3471169", "email_address": "lamonts@cs.uchicago.edu", "orcid_id": ""}, {"name": "Nick Seltzer", "author_profile_id": "81502809853", "affiliation": "University of Chicago, Chicago, IL, USA", "person_id": "P3471170", "email_address": "nseltzer@cs.uchicago.edu", "orcid_id": ""}], "doi_number": "10.1145/2254064.2254079", "year": "2012", "article_id": "2254079", "conference": "PLDI", "title": "Diderot: a parallel DSL for image analysis and visualization", "url": "http://dl.acm.org/citation.cfm?id=2254079"}