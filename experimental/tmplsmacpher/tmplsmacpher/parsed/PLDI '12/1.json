{"article_publication_date": "06-11-2012", "fulltext": "\n Adaptive Input-aware Compilation for Graphics Engines Mehrzad Samadi Amir Hormati Mojtaba Mehrara University \nof Michigan, Ann Arbor Microsoft Research, Redmond NVIDIA Research, Santa Clara mehrzads@umich.edu amir.hormati@microsoft.com \nmmehrara@nvidia.com Janghaeng Lee Scott Mahlke University of Michigan, Ann Arbor University of Michigan, \nAnn Arbor jhaeng@umich.edu mahlke@umich.edu 20 Abstract 18 While graphics processing units (GPUs) provide \nlow-cost and ef-16 16 .cient platforms for accelerating high performance computations, 14 the tedious \nprocess of performance tuning required to optimize ap\u00ad plications is an obstacle to wider adoption of \nGPUs. In addition to the programmability challenges posed by GPU s complex mem- GFLOOPS 12 10 8 ory hierarchy \nand parallelism model, a well-known application de\u00adsign problem is target portability across different \nGPUs. However, even for a single GPU target, changing a program s input charac\u00adteristics can make an \nalready-optimized implementation of a pro\u00adgramperformpoorly.In this work, wepropose Adaptic, an adaptive \ninput-aware compilation system to tackle this important, yet over\u00adlooked, input portability problem. \nUsing this system, programmers develop their applications in a high-level streaming language and let \nAdaptic undertake the dif.cult task of input portable optimiza\u00adtions and code generation. Several input-aware \noptimizations are introduced to make ef.cient use of the memory hierarchy and cus\u00adtomize thread composition.At \nruntime, aproperly optimized version of the applicationis executedbased on the actualprograminput.We \nperformahead-to-head comparisonbetweentheAdapticgenerated andhand-optimizedCUDAprograms.The results show \nthatAdaptic is capable of generating codes that can perform on par with their hand-optimized counterparts \nover certain input ranges and outper\u00adform them when theinput falls out of the hand-optimized programs \n comfort zone .Furthermore, we show thatinput-aware results are sustainable acrossdifferentGPU targetsmakingitpossibletowrite \nand optimize applications once and run them anywhere. Categories and Subject Descriptors D.3.4 [Programming \nLan\u00adguages]: Processors Compilers General Terms Design, Languages, Performance Keywords Streaming, Compiler, \nGPU, Optimization, Portability  1. Introduction GPUs are specialized hardware accelerators capable of \nrendering graphics much faster than conventional general-purpose processors. They are widely used in \npersonal computers, tablets, mobile phones, and game consoles. Modern GPUs are not only ef.cient at manipu\u00adlating \ncomputer graphics, but also are more effective than CPUs for algorithms where processing of large data \nblocks is done in parallel. This is mainly due to their highly parallel architecture. Recent works have \nshown that in optimistic cases, speedups of 100-300x [20], and Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. PLDI 12, June 11 16, 2012, Beijing, China. \nCopyright c &#38;#169; 2012 ACM 978-1-4503-1205-9/12/06. . . $10.00 6 4 2 0 Input Size Input Size Figure \n1: Performance ofthetransposedmatrix vector multiplication bench\u00admark from the CUBLAS library on an NVIDIA \nTesla C2050. The X-axis showstheinputdimensionsinnumber of rowsxnumberof columnsformat. in pessimistic \ncases, speedups of 2.5x [15], are achievable using modern GPUs compared to the latest CPUs. While GPUs \nprovide inexpensive, highly parallel hardware for accelerating parallel workloads, the programming complexity \nre\u00admains a signi.cant challenge for application developers. Developing programs to effectively utilize \nGPU s massive compute power and memory bandwidth requires a thorough understanding of the appli\u00adcation \nand details of the underlying architecture. Graphics chip man\u00adufacturers, such as NVIDIA and AMD, have \ntried to alleviate part of the complexity by introducing new programming models, such as CUDA and OpenCL. \nAlthough these models abstract the underlying GPU architecture by providing uni.ed processing interfaces, \ndevel\u00adopers still need to deal with many problems such as managing the amount of on-chip memory used \nper thread, total number of threads per multiprocessor, and the off-chip memory access pattern in or\u00adder \nto maximize GPU utilization and application performance [24]. Therefore, programmers must manually perform \na tedious cycle of performance tuning to achieve the desired performance. Many prior efforts have tried \nto address this programmability challenge mainly along three interrelated angles. The works in [2, 3, \n12,14]provide high-level abstractions atthe language level toenable easier expression of algorithms. \nThese abstractions are later used by the compiler to generate ef.cient binaries for GPUs. Adding anno\u00adtations \nto current models (CUDA or OpenCL) or popular languages (C or Fortran) to guide compiler optimizations \nis another method used in [8, 31, 34]. Finally, works in [1, 25, 33] try to automatically generate optimized \ncode from a basic, possibly poorly performing, parallel or sequential implementation of an application. \nThe hard problem of .nding the optimal implementation of an algorithm on a single GPU target is further \ncomplicated when at\u00adtempting to create software that can be run ef.ciently on multiple GPU architectures. \nFor example, NVIDIA GPUs have different ar\u00adchitectural parameters, such as number of registers and size \nof shared memory, that can make an implementation which is optimal for one architecture sub-optimal for \nanother. The situation is even worse if the goal is to have an optimal implementation for GPUs across \nmul\u00adtiple vendors. We call this issue the targetportability problem.  The portability issue is not speci.c \nto moving applications across different GPU targets. Even for a .xed GPU target, changing the problem \nsize and dimensions can make a speci.c implementation of an algorithm sub-optimal, resulting in poor \nperformance portability. Figure 1 illustrates this issue for the transposed matrix vector mul\u00adtiplication \n(TMV) benchmark from the CUBLAS library . All input matrices have the same number of elements but arranged \nin differ\u00adent shapes. The benchmark performs consistently between 12 and 17 GFLOPs over the input dimension \nrange of1Kx4K to128Kx32 on an NVIDIA Tesla C2050 GPU. However, when input dimensions fall out of this \nrange, the performance degrades rapidly by up to a factor of more than 20x. The main reason for this \neffect is that the number of blocks and threads in the application are set based on the number of rows \nand columns in the input matrix. Therefore, this benchmark works ef.ciently for a certain range of these \nvalues and for other input dimensions, either there is not enough blocks to run in parallel and hide \nmemory latency (towards the left end of X-axis in the .gure), or the data chunk that each block is operating \non is too small to amortize the overhead of parallel block execution (to\u00adwards the right end of X-axis \nin the .gure). We call this the input portability problem. There are various reasons for this problem \nin GPUs such as un\u00adbalanced workload across processors, excessive number of threads, and inef.cient usage \nof local or off-chip memory bandwidth. Unbal\u00adanced workloads occur when a kernel has a small number of \nblocks causing several processors to be idle during execution, which leads to under-utilization of GPU \nresources and poor performance. Exces\u00adsive number of threads result in sequential thread execution due \nto lack of enough resources in the GPU to run all threads in parallel. Finally, inef.cient memory bandwidth \nusage can be due to the pat\u00adtern of memory accesses which are determined based on the size or dimensions \nof the input data in some programs. Therefore, mem\u00adory optimizations must be adapted based on the input \nto ef.ciently utilize the memory bandwidth. One solution to the input portability problem is to have \nthe pro\u00adgrammer design and develop different algorithms for each input range and size. However, this \nwould impose a considerable imple\u00admentation and veri.cation overhead as applications become larger, more \ncomplex, and need to work across a wide range of inputs. For instance, as shown later, .ve distinct kernel \nimplementations are needed to ef.ciently utilize the processing power of a GPU across the complete input \nspectrum in the TMV benchmark. Multi-kernel applications complicate matters even more as programmers \nmust deal with the cross-product of choices for each kernel as the input is varied. Clearly, automatic \ntools will become essential to guarantee high performance across various input dimensions. In this work, \nwe focus on tackling the input portability prob\u00adlem while providing GPU target portability. We employ \na high\u00adlevel streaming programming model to express target algorithms. This model provides explicit communication \nbetween various pro\u00adgram kernels and its structured and constrained memory access lets the compiler make \nintelligent optimization decisions without having to worry about dependencies between kernels. An adaptive \ninput\u00adaware compilation system, called Adaptic, is proposed that is capa\u00adble of automatically generating \noptimized CUDA code for a wide range of input sizes and dimensions from a high-level algorithm de\u00adscription. \nAdaptic decomposes the problem space based on the input size into discrete scenarios and creates a customized \nimplementa\u00adtion for each scenario. Decomposition and customization are accom\u00adplished through a suite \nof optimizations that include a set of memory optimizations to coalesce memory access patterns employed \nby the high-level streaming model and to ef.ciently execute algorithms that access several neighboring \nmemory locations at the same time. In ad\u00addition, a group of optimizations are introduced to effectively \nbreak up the work in large program segments for ef.cient execution across many threads and blocks. Finally, \ntwo optimizations are introduced to combine the work of two segments so that execution overhead can be \nreduced. An enhanced version of the performance model introduced in [10] is employed to predict application \nbehavior for each range of input size and dimensions. Based on these predictions, optimizations are applied \nselectively by the compiler. At runtime, based on the pro\u00advided input to the program, the best version \nof the generated code is selected and executed to maximize performance. This method frees application \ndevelopers from the tedious task of .ne-tuning and possibly changing the algorithm for each input range. \nThe speci.c contributions offered by this work are as follows: We propose a system that treats input \nportability as a .rst class programmability challenge for GPUs and provide means to solve it.  We propose \ninput-aware optimizations to overcome memory re\u00adlated performance de.ciencies and break up the work fairly \nbe\u00adtween working units based on the input size and dimensions.  We develop an adaptive compilation and \nruntime system that op\u00adtimizes performance for various input ranges by conforming to the user input and \nidentifying and adjusting required optimiza\u00adtions.   2. Background Exposed communication and an abundance \nof parallelism are the key features making stream programming a .exible and architectu\u00adre-independent \nsolution for parallel programming. In this paper, we employ a stream programming model based on Synchronous \nData Flow (SDF) model. In SDF, computation is performed by actors, which are independent and isolated \ncomputational units, communicating only through data-.ow buffers such as FIFOs. SDF, and its many variants, \nexpose input and output processing rates of actors. This provides many optimization opportunities that \ncan lead to ef.cient scheduling decisions for assignment of actors to cores, and allocation of buffers \nin local memories. One way of writing streaming programs is to include all the com\u00adputation performed \nin an actor inside a work method. This method runs repeatedly as long as the actor has data to consume \non its input port. The amount of data that the work method consumes is called the poprate. Similarly, \nthe amount of data each work invocation pro\u00adduces is called the push rate. Some streaming languages, \nincluding StreamIt [27], also provide non-destructive reads, called peek, that do not alter the state \nof the input buffer. In this work, we use the StreamIt programming language to implement streaming programs. \nStreamIt is an architecture-independent streaming language based on SDF and allows the programmer to \nalgorithmically describe the computational graph. In StreamIt, actors can be organized hierar\u00adchically \ninto pipelines(i.e., sequential composition), split-joins (i.e., parallel composition), and feedbackloops \n(i.e., cyclic composition). To ensure correct functionality in StreamIt programs, it is impor\u00adtant to \ncreate a steady state schedule which involves rate-matching of the stream graph. There is a buffer between \neach two consecutive actors and its size is determined based on the program s input size and pop and \npush rates of previous actors. Rate-matching assigns a repetition number to each actor. In a StreamIt \nschedule, an actor is enclosed by a for-loop that iterates as many times as this repetition number. Finally, \nsince StreamIt programs are incognizant of input size and dimensions, Adaptic s input code is the same \nfor all inputs but the output implementation will be different for various input sizes. The techniques \nthat we propose in this paper are evaluated on StreamIt but are applicable to other streaming languages \nas well.  3. Adaptic Overview The Adaptic compiler takes a platform-independent StreamIt pro\u00adgram, ranges \nof its possible input size and dimension values, and the target GPU as input, and generates optimized \nCUDA code based on those ranges and the target. A StreamIt program consists of several actors that can \nbe described as .ne-grained jobs executed by each thread. Each actor in the stream graph is converted \nto a CUDA kernel with a number of threads and blocks. By performing  Figure 2: Compilation FlowinAdaptic. \ninput-aware stream compilation, Adaptic decides how many threads and blocks to assign to the CUDA kernel \ngenerated for each actor. Figure 2 shows the Adaptic s compilation .ow which consists of four main components: \nbaseline input-unaware optimizations, per\u00adformance model, input-aware optimizations, and CUDA code gen\u00aderation. \nIn addition, a matching runtime system selects appropriate kernels and sets their input parameters according \nto the program in\u00adput at execution time. This section gives an overview of these four components as well \nas the runtime kernel management, while Sec\u00adtion 4 details our proposed input-aware optimizations. Input-unaware \nOptimizations: This step performs a set of input\u00adunaware optimizations on the program and decides whether \neach actor should be executed on the CPU or GPU. This decision may be changed later by input-aware optimizations. \nInput-unaware opti\u00admizations are similar to those introduced in [12]. They include op\u00adtimizations such \nas loop unrolling, data prefetching, and memory transfer acceleration. They can be used to generate CUDA \ncode that is reasonably optimized and works for all input sizes, but its perfor\u00admance peaks for certain \nranges of input and is suboptimal outside those ranges. Performance Model: Adaptic relies on a high-level \nperformance model to estimate the execution time of each kernel and to decide on using different optimizations \nfor various problem sizes and GPU targets. This model is similar to the one described in [10], and classi.es \nCUDA kernels into three categories of memory-bound, computation-bound, and latency-bound. Memory-bound \nkernels have enough warps to ef.ciently hide the computation latency. Execution time of each warp is \ndominated by memory accesses. In computation-bound kernels, since most of the time is spent on computation, \nthe execution time can be estimated as the total computation time. It should be noted that in these kernels, \na large number of active warps is also assumed so that the scheduler would be able to hide memory access \nlatencies with computation. The last category, latency-bound kernels, are those that do not have enough \nactive warps on each streaming multiprocessor (SM), and the scheduler cannot hide the latency of the \ncomputation or memory by switching between warps. Execution time of these latency-bound kernels is estimated \nby adding up the computation and memory access times. A kernel is latency-bound if there are not enough \nactive warps for hiding the latency. There are two situations that lead to a small number of active warps: \nnot enough data par\u00adallelism in the kernel or high shared resource consumption in each thread block. \nIn order to determine the type of each kernel, Adaptic counts the number of active warps on each SM. \nBased on this number and the target GPU, it determines whether the kernel is latency-bound or not. If \nnot, Adaptic treats that kernel as both memory-bound and computation-bound and calculates the corresponding \nexecution cy\u00adcles based on the number of coalesced and non-coalesced memory accesses, computation instructions \nand the number of synchroniza\u00adtion points, all of which are dependent on the input and can be com\u00adputed \nat compile time as a function of input size and dimensions. The maximum of these two numbers determines \nthe .nal kernel cat\u00adegory. Based on these categories, The performance model estimates the execution time \nof a kernel both before and after applying each opti\u00admization as a function of input dimensions. The \nperformance break\u00adeven points determine the dimensions at which the corresponding optimization should \nbe enabled or disabled. Input-aware Optimizations: During each input-aware optimiza\u00adtion phase, its potential \nperformance impact for all input ranges is estimated using the model. These input ranges are provided \nby previ\u00adous input-aware phases. If the optimization is bene.cial, it is added to the optimization list \nfor the whole range. However, if the opti\u00admization is only suitable for a subset of that range, Adaptic \ndivides the range into smaller subranges, and populates optimization lists for each new subrange accordingly. \nIn other words, Adaptic divides up operating input ranges to subranges if necessary, and applies dif\u00adferent \noptimizations to each subrange. Therefore, separate kernels should be later generated for these subranges. \nCode Generation: At the end of the compilation .ow, the code generation stage generates optimized CUDA \nkernels for each input range based on optimization lists constructed by the optimization phase and the \nperformance model. Since the performance model uses the target speci.cations to make optimization decisions, \ncode generation is different for different targets. In addition, necessary code for runtime kernel management \nis also generated by the code generation unit based on the kernels and their operating input ranges. \nAll these codes are later translated to a binary using the native CUDA compiler. Runtime Kernel Management: \nA runtime kernel management unit is developed to dynamically select a properly optimized kernel at runtime \nbased on the program input. This unit also determines the values of parameters that should be passed \nto each kernel at launch time including the number of blocks, number of threads per block, and the size \nof allocated shared memory. In order to remove kernel management overhead at runtime, this unit is completely \nexecuted on the CPU during the initial data transfer from CPU to GPU.  4. Input-aware Optimizations \nAs mentioned in Section 1, several factors such as inef.cient use of memory bandwidth, unbalanced workload \nacross processors, and excessive number of threads lead to ineffectiveness of input-unaware optimizations \nin achieving high performance across different inputs. The goal of input-aware optimizations in this \nwork is to deal with these inef.ciencies. Two memory optimizations are introduced in Section 4.1 to solve \ninef.cient use of local or off-chip memory bandwidth. In addition, two other sets of optimizations, namely \nactor segmentation and actor integration are detailed in Sections 4.2 and 4.3 respectively to tackle \nboth unbalanced processor workload and excessive number of threads in the context of streaming.  Global \nMemory Global Memory Global  Global Memory Memory  (a) (b) Figure 3: Memoryrestructuring optimization.(a)Global \nmemory accesspattern of an actor withfourpops andfourpushes.Accesses are not coalescedbecause accessed \naddressesarenotadjacent.(b) Accesspatterns after memory restructuring.Accessed addressesareadjacent at \neachpointintimeand accesses areall coalesced. 4.1 Memory Optimizations In this section, two memory optimizations, \nmemory restructuring and neighboring access optimization are explained. These optimiza\u00adtions are useful \nfor several classes of applications that are suitable for GPUs. 4.1.1 Memory Restructuring One of the \nmost effective ways to increase the performance of GPU applications is coalescing off-chip memory accesses. \nWhen all memory accesses of one warp are in a single cache line, the memory controller is able to coalesce \nall accesses into a single global memory access. Figure 3a illustrates how an actor with four pops and \nfour pushes accesses global memory. In this example, each actor in each thread accesses four consecutive \nmemory words. The .rst pop operations in threads 0 to 64 access memory word locations 0, 4, 8,. . . , \n252, second pop operations access locations 1, 5, 9,. . . , 253, etc. Since these locations are not consecutive \nin memory, non-coalesced global memory accesses occur, leading to poor performance. There are two ways \nto coalesce these memory accesses. One way is to transfer all data to the shared memory in a coalesced \nmanner and since shared memory is accessible by all threads in a block, each thread can work on its own \ndata. In this method, each thread fetches other threads data from global memory as well as part of its \nown data. The same method can be applied for write-backs to global memory as well. All threads write \ntheir output to shared memory and then they transfer all data in a coalesced pattern to the global memory. \nAlthough using shared memory for coalescing accesses can improve performance, it has two shortcomings: \nnumber of threads is limited by the size of shared memory and the total number of instructions is increased \ndue to address computations. We use another method for coalescing accesses and that is to restructure \nthe input array in a way that each pop access in all threads accesses consecutive elements of one row \nof the input in global memory. Figure 3b shows how this restructuring coalesces all memory accesses without \nusing shared memory. This method has the advantage of minimizing the number of additional instructions \nand does not limit the number of threads by the size of shared memory. In addition, since this optimization \nis not using shared memory to coalesce off-chip memory accesses, shared memory can be utilized to store \nreal shared data. This optimization is not applicable when there are two or more actors with mismatching \npush and pop rates in the program. In those cases, rate matched buffers between kernels also have to \nbe restruc\u00adtured, which involves extra write and reads from global memory, leading to poor performance. \nHowever, as the work in [26] shows, since most consecutive actors in streaming benchmarks have matching \nrates, using memory restructuring would be bene.cial. The CPU can restructure data at generation time \nand transfer it to the global memory of the GPU. The GPU launches kernels and when all of them are .nished, \nthe CPU reads back the output data. Due to the dependency of pop and push rates of some of the actors \nare on input size, this optimization can have different effects for various sizes. In addition to coalescing \nglobal memory accesses, memory re\u00adstructuring can also be applied to shared memory to remove bank con.icts. \nAfter applying this optimization, all threads access con\u00adsecutive addresses in shared memory. Since adjacent \naddresses in shared memory belong to different shared memory banks, there would be no bank con.icts. \n 4.1.2 Neighboring Access Optimization A common pattern in many applications is to access a point and \nits neighboring locations to calculate the data for that point. These benchmarks are very common in simulation \nbenchmarks, for in\u00adstance, the temperature of each point on a surface is computed based on the temperature \nof its neighbors. In streaming programs, non\u00addestructive reads (peek) are used to read the neighbors \ndata while the address of the main point increases linearly in each iteration. Figure 4a shows an example \nStreamIt code for a .ve-point stencil actor that has neighboring access pattern and Figure 4b illustrates \nthe access pattern for this code. In this example, each element is de\u00adpendent on its top, bottom, right, \nand left elements. Each thread .rst reads all top elements, which are consecutive, leading to coalesced \nmemory accesses. The same pattern holds for bottom, right, left and center elements. However, the main \nproblem with this class of actors is excessive global memory accesses. For instance, accessing all top, \nbottom, left, right and center elements in each thread simply means accessing the whole input .ve times. \nAn ef.cient way to alleviate this problem is to use shared mem\u00adory such that each block brings in one \ntile of data to shared mem\u00adory and works on that. Since the data close to tile edges is needed for both \nneighboring tiles, tiles should be overlapping. These over\u00adlapping regions, called halo, are brought \nin for each tile at all four edges. Since branch divergence occurs only within a warp, both tile and \nhalo widths should be multiples of warp size to make all ac\u00adcesses of each warp coalesced and prevent \ncontrol .ow divergence for address calculations. Since halo regions are fetched for each tile, they should \nbe as small as possible to minimize extra memory accesses. In order to decrease the overhead of extra \nmemory accesses, Adaptic uses super tiles. In such a tile, the ratio of the halo size to the main tile \nsize is decreased by merging several simple tiles. Each super tile is assigned to one block and each \nthread computes several output elements in different tiles. In this case, each block brings in a super \ntile from global memory to shared memory, performs the computation, and writes back the super tile to \nglobal memory. Figure 5 shows a super tile assigned to a block in our example. Dark gray elements construct \nthe main tiles while the light gray elements are halo parts. The number in each element indicates the \nthread index reading that element s address. In this example, warp size is set to 2 and there are 8 threads \nin each block. Each tile is 4x2 and by merging four tiles together, one super tile with 4x8 elements \nis formed. Since all width values should be multiples of warp size to maintain memory coalescing, the \nwidth of right and left halo parts in this example are set to 2. As a result, One of the halo values \non each  5-Point Stencil(pop,peek:size, push:size) for (index=0; index<size; index++) if (not on edge) \nTop = peek(index width) Bottom = peek(index + width) Right = peek(index + 1) Left = peek(index 1) Center \n= peek(index 1) push(func(Top,Bottom,Right,Left,center))  (a) (b) Figure 4: (a) An example StreamIt \ncode of a .ve-point stencil actor. (b) Memory accesspattern of thisactor. Figure 5: Asupertile assigned \nto oneblock.Darkgray addressesaremain partandlightgrayparts arehaloparts.Numbersin each smallboxindicates \nwhich thread reads this address. side (left and right) are not used in the computations for this super \ntile. Increasing the size of super tiles leads to an increase in the allocated shared memory for each \nblock, which in turn, could result in lower number of concurrent blocks executed on each GPU SM. Since \nthis issue may change the type of kernel from computation\u00adbound or memory-bound to latency-bound, the \ndata size processed by each block should be chosen carefully. For each application, Adaptic has to calculate \nthe size and shape of a super tile. In general, the size of each super tile should not be more than the \nmaximum shared memory per block, which is a constant value based on the target GPU. The super tile s \nsize is dependent on the input size. For small input sizes it is bene.cial to use smaller super tiles \nin order to have more blocks. Large super tiles are advantageous for large input sizes to reduce excessive \nmemory accesses. Using the performance model and the input size, Adaptic can calculate the size for each \nsuper tile. Then, given the size, Adaptic needs to .nd the shape (width and height) of a super tile. \nFor this purpose, Adaptic uses a reuse metric to maximize the number of served memory accesses while \nminimizing the size of extra halo regions. Adaptic uses the following reuse metric to .nd the optimal \nshape and size for each tile: Element Accesses T ile Reuse Metric = Halo Size In this formula, Element \nAccesses is the number of times each element in the shared memory is accessed during the computation \nof the whole output matrix, and the summation is taken over all elements in the tile. Since the best \ntile is the one with small halo region that can compute a large chunk of output, Adaptic chooses rectangular \ntiles with maximum possible Reuse Metrics. Once the size and shape of super tiles and halo regions are \ndetermined, the output CUDA code will be similar to the code shown in Figure 6. First, the kernel reads \nin the super tile and all its halos to the shared memory, after which synchronization makes shared memory \nvisible to all threads. Subsequently, each block starts working on its own data residing in the shared \nmemory to perform the computation and output the results.  4.2 Actor Segmentation Optimizations in \nthis category attempt to divide the job of one large actor between several threads/blocks to increase \nthe performance. In order to have balanced workload across processors with ef.cient number of threads, \nthis segmentation should be done based on the input size. The two main optimizations in this category \nare stream reduction and intra-actor parallelization. Reduction is one of the important algorithms used \nin many GPU applications. The goal of stream Neighboring-access Kernel <<<Blocks, threads>>> Top halo \nshared memory Bottom halo shared memory Right halo shared memory Left halo shared memory for tile \nin tile  shared memory sync(); Figure 6: A generic neighboring access CUDA code. First, different halo \nparts and the super tile are moved from global to shared memory. Subse\u00adquently, computations areperformed \nontheshared memorydata. reduction optimization is to ef.ciently translate reduction operations to CUDA \nin streaming programs. Intra-actor parallelization s goal is to break the dependency between iterations \nof large loops and make them more amenable to execution on GPUs. 4.2.1 Stream Reduction A reduction \noperation generally takes a large array as input, per\u00adforms computations on it, and generates a single \nelement as output. This operation is usually parallelized on GPUs using a tree-based approach, such that \neach level in the computation tree consumes the outputs from the previous level and produces the input \nfor the next level. In uniform reduction, each tree level reduces the number of elements by a .xed factor \nand the last level outputs one element as the .nal result. The only condition for using this method is \nthat the reduction operation needs to be associative and commutative. A naive way of implementing the \ntree-based approach in a stream graph is to represent each tree node as an individual actor with small \npop/push rates. Executing one kernel for each small actor would make the kernel launching overhead signi.cant \nand degrade the per\u00adformance dramatically. Another method of representing reduction is by using one .lter \nthat pops the whole input array and pushes the .nal result as shown in Figure 7a. The actor can not be \ntranslated to an ef.cient kernel due to the limited number of possible in-.ight threads. On the other \nhand, Adaptic automatically detects reduction op\u00aderations in its streaming graph input using pattern \nmatching. After this detection phase, it replaces the reduction actor with a highly optimized kernel \nin its output CUDA code based on the input size and the target GPU. This reduction kernel receives Narrays \ndiffer\u00adent arrays with Nelements elements each as input, and produces one element per array as output. \nData is initially read from global mem\u00adory, reduced and written to shared memory, and read again from \nshared memory and reduced to the .nal result for each array. In this work, we introduce two approaches \nfor translating reduction actors to CUDA kernels. When the array input size, Nelements , is small compared \nto the total number of input arrays (Narrays) Adaptic produces a single reduction kernel in which each \nblock computes the reduction output for one input array (Figure 7b). Thus, this kernel should be launched \nwith Narrays blocks. This approach is bene.cial for large array counts so that Adaptic can launch enough \nblocks to .ll up the resources during execution. However, when the array input size (Nelements ), is \nlarge com\u00adpared to total number of input arrays (Narrays), the reduction output for each array is computed \nindividually by two kernels (Figure 7c). The .rst kernel, called the initial reduction kernel, chunks \nup the input array and lets each block reduce a different data chunk. The number of these blocks, Ninitial \nblocks is dependent on the value of Nelements and the target GPU. Since there is no global syn\u00adchronization \nbetween threads of different blocks, results of these blocks (Ninitial blocks * Narrays elements) are \nwritten back to global memory. Subsequently, another kernel, called the merge ker\u00adnel, is launched to \nmerge the outputs from different blocks of the initial reduction kernel down to Narrays elements. In \nthe merge ker\u00adnel, each block is used to compute the reduction output of one in\u00adput array. Therefore, \nthis kernel should be launched with Narrays blocks.   Figure 7: Stream reductiontechnique.(a) StreamIt \ncodefor a reduction actor.(b) Eachblockis responsiblefor computing outputfor one chunk ofdataintwo phases.Inthe \n.rstphase,each thread readsfromglobal memory and writesreduction outputtotheshared memory andinthesecondphase,shared \nmemorydata isreduced tooneoutputelement.(c) Inthetwokernel approach,differentblocksof the .rstkernel \nwork ondifferentchunksofdataand thesecondkernel reads all reduction kernel s output and compute .nal \nresult. Initial Kernel Reduction<<<reductionBlocks, threads>>> /* Global memory reduction phase */ Result \n= 0; numberOfThreads = BlockDim * gridDim; for ( index=tid; index<size; index+= numberOfThreads) Result \n= Result Input[Index]; SharedData[tid] = Result; /* Shared memory reduction phase */ activeThreads = \nblockDim; while (activeThreads > WARP_SIZE){ if (tid <activethreads) activeThreads /=2; L1 sync(); \nSharedData[tid] = SharedData[tid+activeThreads]; } Stride = WARP_SIZE; if (tid < WARP_SIZE) while (stride \n> 1){ sync(); L2 SharedData[tid] = SharedData[tid + stride]; stride /=2;} if tid = 0 Output[bid] = \nSharedData[0]; Figure 8: The initial reduction kernel s CUDA code. Figure 8 shows Adaptic s resulting \nCUDA code for the initial reduction kernel. In the .rst phase, the input array in global memory is divided \ninto chunks of data. Each thread computes the output for each chunk, and copies it to shared memory. \nThe amount of shared memory usage in each block is equal to T hreads per Block * Element Size. As discussed \nin Section 4.1.1, all global memory accesses are coalesced as a result of memory restructuring and there \nwould be no bank con.icts in shared memory in this phase. In the next phase, the results stored in shared \nmemory are re\u00adduced in multiple steps to form the input to the merge kernel. At each step of this phase, \nthe number of active threads performing reduc\u00adtion are reduced by half. Loop L1 in Figure 8 represents \nthese steps. They continue until the number of active threads equals the number of threads in a single \nwarp. At this point, reducing the number of threads any further would cause control-.ow divergence and \ninfe\u00adrior performance. Therefore, we keep the number of active threads constant and just have some threads \ndoing unnecessary computation (Loop L2 in Figure 8). It should be noted that after each step, syn\u00adchronization \nis necessary to make shared memory changes visible to other threads. Finally, the thread with tid =0 \ncomputes the .nal reduction result and writes it back to the global memory.  4.2.2 Intra-actor Parallelization \nThe goal of intra-actor parallelization is to .nd data parallelism in large actors. As mentioned before, \nit is dif.cult to generate opti\u00admized CUDA code for actors with large pop or push rates, consisting of \nloops with high trip counts. This optimization breaks these actors into individual iterations which are \nlater ef.ciently mapped to the GPU. Using data .ow analysis, Adaptic detects cross-iteration de\u00adpendencies. \nIf no dependency is found, Adaptic simply assigns each iteration to one thread and executes all iterations \nin parallel. It also replaces all induction variable uses with their correct value based on the thread \nindex. In some cases, Adaptic breaks the dependence between differ\u00adent iterations by eliminating recurrences. \nOne common source of recurrence is accumulator variables. This happens when a loop con\u00adtains an accumulator \nvariable count incremented by a constant C in every iteration (count = count + C). This accumulation \ncauses cross-iteration dependencies in the loop, making thread assignment as described impossible. However, \nintra-actor parallelization tech\u00adnique breaks this dependence by changing the original accumulation construct \nto count = initial value+ induction variable* C and making all iterations independent. In general, this \noptimization is able to remove all linear re\u00adcurrence constructs and replace them by independent induction \nvariable-based counterparts. This is similar to the induction vari\u00adable substitution optimization that \nparallelizing compilers perform to break these recurrences and exploit loop level parallelism on CPUs. \n 4.3 Actor Integration This optimization merges several actors or threads together to bal\u00adance threads \nworkloads based on the input size in order to get the best performance. Two types of actor integration \noptimization are introduced in this paper. Vertical integration technique reduces off\u00adchip memory traf.c \nby storing intermediate results in the shared rather than global memory. Horizontal integration technique \nreduces off chip memory accesses and synchronization overhead and also lets the merged actors share instructions. \n 4.3.1 Vertical Integration During this optimization, Adaptic vertically integrates some actors to improve \nperformance by reducing memory accesses, removing kernel call overhead, and increasing instruction overlap. \nThe rea\u00adson for its effectiveness is that integrated actors can communicate through shared memory and \nthere is no need to write back to the global off-chip memory. Also, integrating all actors together results \nin one kernel and global memory accesses of this one kernel are co\u00adalesced by the memory restructuring \noptimization. However, since input and output buffers of the middle actors in the integrated kernel are \nallocated in the shared memory, the number of active threads ex\u00adecuting these actors are limited by the \nsize of shared memory. This limitation often prevents Adaptic from integrating all actors together. Based \non the performance model, Adaptic .nds the best candidates for this optimizations. Since push and pop \nrates of some actors can be dependant on the input size, this optimization is bene.cial for some ranges \nof input size.   Another optimization made possible after actor integration is replacing transfer actors \nwith index translation. Transfer actors are the ones that do not perform any computation and only reorganize \ninput buffer s data and write it to the output buffer. Since input and output buffers of the middle actors \nin integrated kernels are both allocated in the shared memory, there is no need to read the data from \ninput buffer, shuf.e it, and write it to the output buffer. This task can be done by index translation. \nIndex translation gets thread indexes based on the transfer pattern, generates the new index pattern, \nand passes it to the next actor.  4.3.2 Horizontal Integration The goal of horizontal integration is \nremoving excessive computa\u00adtions or synchronizations by merging several threads or actors that can run \nin parallel. There are two kinds of horizontal integration techniques: horizontal actor integration and \nhorizontal thread inte\u00adgration. In streaming languages, we use a duplicate splitter to allow different \nactors to work on the same data. In this case, instead of launching one kernel for each actor, one kernel \nis launched to do the job of all the actors working on the same data. Therefore, in addition to reducing \nkernel overheads, memory access and synchro\u00adnization overheads are also reduced. For example, assume \nthere is a program that needs maximum and summation of all elements in an array. Instead of running two \nkernels to compute these values, Adaptic launches one kernel to compute both. In this case, off-chip \nmemory accesses and synchronizations only happen once instead of twice. Horizontal thread integration \nmerges several consecutive threads working on consecutive memory locations in one kernel. This method \nreduces the number of threads and blocks used by the ker\u00adnel. Merged threads can share part of the computation \nthat had to be done independently in each of the original threads and decrease the number of issued instructions. \nWhen the number of thread blocks is high, it is bene.cial to use horizontal thread integration to reduce \nthe number of threads and blocks and allow them to run in parallel. Otherwise it is better not to integrate \nthreads and have more threads with less work to increase the possibility hiding memory latency by switching \nbetween threads.  5. Experiments A set of benchmarks from the NVIDIA CUDA SDK and the CUBLAS library \n3.2 are used to evaluate Adaptic. We developed StreamIt versions of these benchmarks, compiled them with \nAdaptic, and compared their performance with the original hand-optimized benchmarks. We also present \nthree case studies to better demon\u00adstrate and explain the effectiveness of Adaptic s compilation algo\u00adrithms. \nThe .rst case study is performed on a CUBLAS benchmark to investigate the effect of our optimizations \nover a wide range of inputs. Then, we present two more case studies on real world appli\u00adcations, biconjugate \ngradient stabilized method and support vector machine [4], executed on two different GPUs, to demonstrate \nhow Adaptic performs on larger programs with many actors and on dif\u00adferent GPU targets. Adaptic compilation \nphases are implemented in the backend of the StreamIt compiler [27] and its C code generator is modi.ed \nto generate CUDA code. Both Adaptic s output codes and the original benchmarks are compiled for execution \non the GPU using NVIDIA nvcc 3.2. GCC 4.1 is used to generate the x86 binary for execution on the host \nprocessor. The target system has an Intel Xeon X5650 CPU and an NVIDIA Tesla C2050 GPU with 3GB GDDR5 \nglobal memory with NVIDIA driver 260.04. The other sys\u00adtem used for experiments in Sections 5.2.2 and \n5.2.3 has an Intel Core 2 Extreme CPU and an NVIDIA GeForce GTX 285 GPU with 2GB GDDR2 global memory. \n5.1 Input Portability In order to show how Adaptic handles portability across different input problem \nsizes, we set up seven different input sizes for each benchmark and compared their performance with the \noriginal CUDA code running with the same input sizes. It should be noted that these seven input sizes \nare chosen from the working range of the CUDA benchmarks because there are many sizes for which the SDK \nbenchmarks would not operate correctly. Figure 9 shows the results for eight CUDA benchmarks that were \nsensitive to changes in the input size, while results for input\u00adinsensitive benchmarks are discussed \nin Section 5.3. As can be seen, Adaptic-generated code is better than the hand-optimized CUDA code for \nall problem sizes in Scalar Product, MonteCarlo, Ocean FFT, and ConvolutionSeparable from the SDK, and \nIsamax/Isamin, Snrm2, Sasum, and Sdot from CUBLAS. A combination of ac\u00adtor segmentation and actor integration \nwere used to optimize all CUBLAS benchmarks. In addition to these optimizations, memory restructuring \nwas applied to Sdot. Sdot is computing the dot product of two vectors. For large vec\u00adtors, using the \ntwo kernel reduction is bene.cial, but for small sizes, in order to reduce kernel launch overhead, Adaptic \nuses the one ker\u00adnel reduction. Using input-aware optimizations leads to upto 4.5x speedup in this benchmark \ncompared to the original program. Con\u00advolution Separable has two actors, and processes data row-wise \nin one and column-wise in the other. Memory optimizations are effec\u00adtive for this benchmark as both of \nthese two actors have neighbor\u00ading memory access pattern. Therefore, as the input becomes smaller, Adaptic \nreduces the super tile sizes adaptively to retain the high number of blocks and, therefore, achieves \nbetter performance than the baseline hand-optimized code. OceanFFT also has a neighbor\u00ading access actor \nand Adaptic uses different tile sizes to improve per\u00adformance over the hand-optimized code. Scalar Product \ncomputes scalar products of pairs of vectors. The original benchmark uses the single kernel reduction, \nand it achieves good performance when there are many pairs of vectors in the input. However, for fewer \npairs of vectors, it is better to use the whole GPU to compute the result for each pair. Using the two \nkernel reduction for those inputs, Adaptic is able to achieve upto 6x speedup compared to the original \nhand\u00adoptimized version. MonteCarlo performs about the same as the original hand\u00adoptimized version. The \nreason is that the original benchmark already has two kernels performing the same task, but optimized \nfor differ\u00adent ranges of input problem sizes. In other words, MonteCarlo has originally been developed \nin an input portable way. Therefore, the output of Adaptic is similar to the original version and the \nperfor\u00admance is the same for all sizes but it is generated automatically by the compiler. Since Adaptic \ngenerates different kernels for some actors in the streaming program, the output binary size could be \nlarger than the original binary optimized for one speci.c range. In our experiments including the case \nstudies, Adaptic s output binaries were on average 1.4x and upto 2.5x larger than their input-unaware \ncounterparts, which is quite reasonable considering the fact that some kernels could have upto .ve different \nversions for various input ranges. However, because each program also has kernels with one versions, \nthe combination leads to this moderate code size increase. These results further show the fact that our \napproach in Adaptic is able to adaptively generate optimized CUDA code for different problem sizes without \nany source code modi.cations.  5.2 Case studies 5.2.1 Transposed Matrix Vector multiplication In this \nsection, we look into the effects of our optimizations on the performance of the transposed matrix vector \nmultiplication bench\u00admark from CUBLAS over a wide range of input sizes and dimen\u00adsions. As was mentioned \nin Section 1, the original benchmark can\u00adnot provide sustainable performance gains for different input \ndimen\u00adsions. However, with the aid of input-aware optimizations, Adaptic is able to generate .ve different \nkernels with different structures, where each kernel is parametrized to get better performance for a \n Speedup(X) 6 5 4 3 2 1 0  Figure 9: Adaptic-generated codespeedups normalized tothehand-optimizedCUDA \ncodefor7differentinputsizes. 45 40 35 0 4x256K8x128K16x64K32x32K64x16K128x8K256x4K512x2K1Kx1K2Kx5124Kx2568Kx12816Kx6432Kx3264Kx16128Kx8256Kx4 \n4x1M 8x512K16x256K32x128K64x64K128x32K256x16K512x8K1Kx4K2Kx2K4Kx1K8Kx512 16Kx25632Kx12864Kx64128Kx32256Kx16512Kx81Mx4 \n16x1M32x512K64x256K128x128K256x64K512x32K1Kx16K2Kx8K4Kx4K8Kx2K16Kx1K32Kx51264Kx256128Kx128256Kx64512Kx321Mx16 \n Input Size Figure 10: Transposed matrix vector multiplicationperformance comparison ofCUBLAS andAdaptic. \n BBaselliine AtActor SSegmentatiion MMemory OOptiimiizatiions AtActor IIntegratiion speci.c range \nof input dimensions. At runtime the proper kernel is launched based on the program input. 10 9 In the \n.rst kernel, which is bene.cial for matrices with many 8 columns and few rows, Adaptic uses the two \nkernel version of re\u00ad 7 Sppeedup(XX) duction. For each row, one kernel is launched and the whole GPU \n6 6 is used to compute the dot product of one row with the input vec\u00ad 5 4 tor. The second kernel is a \nsingle-kernel reduction function where 3 each block is responsible for one row. This kernel achieves \nits best 2 performance for square matrices. In the third kernel, in addition to the single-kernel reduction \nfunction, by using horizontal thread in\u00adtegration, Adaptic adaptively merges several rows and each block \nis responsible for computing several dot products instead of one. This kernel is bene.cial for matrices \nwith more rows than columns. The fourth kernel is also similar to the single-kernel reduction, except \nthat in its shared memory reduction phase, each thread is responsi\u00adble for computing one output. The \nlast kernel generated by Adaptic achieves its best performance for matrices with many rows and few columns. \nIn this case, the size of each row is small and the corre\u00adsponding actor has small pop rates. For this \nkind of actor, our base\u00adline optimizations are effective in generating ef.cient code. There\u00adfore, Adaptic \ndoes not need to add optimization to that. In this kernel, each thread is responsible for computing the \ndot product of a single row and the input vector. Figure 10 compares the performance of this benchmark \nwith Adaptic-generated code for three different matrix sizes over a range of matrix dimensions. As it \ncan be seen, although for some input dimensions Adaptic s performance is really close to CUBLAS, for \nmost of them Adaptic outperforms CUBLAS by a large margin.  5.2.2 Biconjugate gradient stabilized method \nThe biconjugate gradient stabilized method (BiCGSTAB) is an iter\u00adative method used for .nding the numeral \nsolution of nonsymmet\u00adric linear systems such as Ax=B for x where A is a square matrix. This method has \n11 linear steps that can be written easily with the CUBLAS library for GPUs. We wrote this program both \nin StreamIt and CUDA with CUBLAS functions and measured the performance of the two for different sizes \nof A. Figure 11 shows an in-depth com\u00ad 1 1 0 512x512 1024x1024 2048x2048 4096x4096 8192x8192 Input Size \nFigure 11: Performance of the Adaptic-generated Biconjugate gradient stabilized methodbenchmark normalized \ntotheCUBLASimplementation on twodifferent GPU targets. parison and breakdown of the effects of Adaptic \ns individual op\u00adtimizations on this benchmark for different input sizes across two GPU targets -NVIDIA \nTesla C2050 and GTX285. The baseline in this .gure is the generated code after only applying size-unaware \nop\u00adtimizations. The Sgemv, Sdot, Sscal and Saxpy CUBLAS functions were used to implement the CUDA version \nof this benchmark. The problem of using the CUBLAS library is that the programmer should split each step \ninto several sub-steps to be able to use CUBLAS func\u00adtions. Execution of these sub-steps leads to more \nmemory accesses and kernel launch overhead. On the other hand, Adaptic merges all these sub-steps together \nand launches a single kernel for one step. As shown in Figure 11, most of the speedup for small sizes \ncomes from the integration opti\u00admization. Since most of the execution time is spent in matrix vector \nmultiplication for large sizes such as 8192x8192, the effect of inte\u00adgration is not as high for these \nsizes. However, actor segmentation that generates smaller actors and increases parallelism, and memory \nrestructuring play more important roles in achieving better perfor\u00admance for larger sizes.  Baseline \n Actor Segmentation Memory Optimizations Actor Integration PPerformaance normmalized tto GPUSSVM 1 \n0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 01 0.1 0 Adult Web MNIST USPS  Average Datasets Figure 12: Performance \nof theAdaptic-generated SVM training benchmark compared to the hand-optimized CUDA code in the GPUSVM \nimplementa\u00adtion on two different GPU targets.  5.2.3 Nonlinear Support Vector Machine Training Support \nVector Machines (SVMs) are used for analyzing and recog\u00adnizing patterns in the input data. The standard \ntwo class SVM takes a set of input data and for each input predicts which class it be\u00adlongs to among \nthe two possible classes. This classi.cation is based on a model, which is generated after training with \na set of example inputs. Support vector machine training and classi.cation are both very computationally \nintensive. We implemented a StreamIt version of this algorithm based on the implementation in [4]. Figure \n12 shows the performance of the Adaptic-generated code compared to the GPUSVM [4] hand\u00adoptimized CUDA \ncode in this benchmark for four different input datasets. On average, Adaptic achieves 65% of the performance \nof the GPUSVM implementation. The reason for the large performance gap in Adult and USPS datasets is \nthat GPUSVM performs an application-speci.c optimization where it utilizes unused regions of the GPU \nmemory to cache the results of some heavy computations. In case those computations have to be performed \nagain, it simply reads the results in from the memory. Therefore, for input sets which cause a lot of \nduplicate computations, including Adult and USPS, GPUSVM performs better than Adaptic-generated code. \nIn this program, unlike the previous example, actor integration is not very effective and most of the \nperformance improvement comes from actor segmentation. On average, actor segmentation, mem\u00adory restructuring, \nand actor integration improve the performance by 37%, 4%, and 1%, respectively.  5.3 Performance of \nInput Insensitive Applications Although the main goal of Adaptic compiler is to maintain good performance \nacross a wide range of inputs, it also performs well on the benchmarks that are not sensitive to input. \nOur experiments show that a combination of Adaptic optimizations makes the average performance of our \ncompiler-generated code on par with the hand\u00adoptimized benchmarks, while writing StreamIt applications \nas the input to Adaptic involves much less effort by the programmer com\u00adpared to the hand-optimized programs. \nWe ran Adaptic on a set of benchmarks from CUBLAS and the SDK (BlackScholes, VectorAdd, Saxpy, Scopy, \nSscal, Sswap, and Srot, DCT, QuasiRandomGenerator, and Histogram), and on average the performance of \nAdaptic s output is within 5% of the original CUDA versions. This shows that Adaptic does not cause slowdowns \nfor applications that are not sensitive to input size.  6. Related Work The most common languages GPU \nprogrammers use to write GPU code are CUDA and OpenCL. Although these new languages par\u00adtially alleviate \nthe complexity of GPU programming, they do not provide an architecture-independent solution. There is \nan extensive literature investigating many alternative methods to support target portability. Works in \n[2, 3, 7, 9, 12, 14, 21, 30] focus on generating opti\u00admized CUDA code from higher levels of abstraction. \nThe Sponge compiler [12] compiles StreamIt programs and generates optimized CUDA to provide portability \nbetween different GPU targets. The work in [30] compiles stream programs for GPUs using software pipelining \ntechniques. The work in [5] compiles Python programs for graphic processors. Copperhead [3] provides \na nested set of par\u00adallel abstractions expressed in the Python programming language. Their compiler gets \nPython code as input and generates optimized CUDA code. It uses built-in functions of Python such as \nsort, scan, and reduce to abstract common CUDA program constructs. The work in [14] automatically generates \noptimized CUDA programs from OpenMP programs. MapCG [9] uses MapReduce framework to run high level programs \non both multi-core CPUs and GPUs. Brook for GPUs [2] is one of the .rst papers about compilation for \nGPUs, which extends the C language to include simple data\u00adparallel constructs. Compiling Matlab .le to \nCUDA is also investi\u00adgated in [21]. CnC CUDA [7] use Intel s Concurrent Collections programming model \nto generate optimized CUDA code. All these works look into improving the programmability of GPUs, and \nin some cases, provide target portability. However, Adaptic provides portability across different inputs \nas well as GPU targets. In addition, Adaptic employs various input-aware optimizations and its output \nperformance is comparable to hand written CUDA code. Several other works have focused on automatically \noptimizing CUDA kernels [10, 33, 34]. The work in [33] performs GPU code compilation with a focus on \nmemory optimizations and parallelism management. The input to this compiler is a naive GPU kernel func\u00adtion \nand their compiler analyzes the code and generates optimized CUDA code for various GPU targets. CUDA-Lite \n[34] is another compilation framework that takes naive GPU kernel functions as in\u00adput and tries to coalesce \nall memory accesses by using shared mem\u00adory. Hong et al. [10] propose an analytical performance model \nfor GPUs that compilers can use to predict the behavior of their gener\u00adated code. None of these works \nprovide means to address the input portability problem. The problem of input-aware optimizations has \nbeen studied in several previous works [17, 18, 28, 29]. The only work that tackles the input portability \nproblem in the context of GPUs is introduced in [18]. In this work, programmers should provide optimizations \npragmas for the compiler and then compiler generates different pro\u00adgrams with different optimizations \nbased on those pragmas. Their approach is to run all these different versions of one program for dif\u00adferent \ninputs and save all the results into a database. For each input, they check this database and .nd the \nbest version and run it on the GPU. There are other works that have focused on generating CUDA code from \nsequential input [1, 8, 16, 25, 31]. hiCUDA [8] is a high\u00adlevel directive-based compiler framework for \nCUDA programming where programmers need to insert directives into sequential C code to de.ne the boundaries \nof kernel functions. The work in [1] is an au\u00adtomatic code transformation system that generates CUDA \ncode from input sequential C code without annotations for af.ne programs. In [31], by using C pragma \npreprocessor directives, programmers help compiler to generate ef.cient CUDA code. In [25], program\u00admers \nuse C# language and a library to write their programs and let the compiler generate ef.cient GPU code.The \nwork in [16] proposes an extension to a Java JIT compiler that executes program on the GPU. Gordon et \nal. [6] perform stream-graph re.nements to statically determine the best mapping of a StreamIt program \nto a multi-core CPU. Researchers have also proposed ways to map and optimize synchronous data-.ow languages \nto SIMD engines [11], distributed shared memory systems [13]. In a recent work [26], the authors talk \nabout the usefulness of different features of StreamIt to a wide range of streaming applications. Works \nin [22, 23, 32] map reduction to heterogeneous systems with GPUs. Mapping stencil loops to GPUs and tiling \nsize tradeoff are also studied by [1] and [19]. However, Adaptic applies input\u00adaware optimizations adaptively \nand more generally on streaming applications to provide input portability  7. Conclusion GPUs provide \nan attractive platform for accelerating parallel work\u00adloads. However, their programming complexity poses \na signi.cant challenge to application developers. In addition, they have to deal with portability problems \nacross both different targets and various inputs. While target portability has received a great deal \nof atten\u00adtion in the research community, the input portability problem has not been investigated before. \nThis problem arises when a program opti\u00admized for a certain range of inputs, shows poor performance along \ndifferent input ranges.  In this work, we proposed Adaptic, an adaptive input-aware com\u00adpiler for GPUs. \nUsing this compiler, programmers can implement their algorithms once using the high-level constructs \nof a stream\u00ading language and compile them to CUDA code for all possible in\u00adput sizes and various GPUs \ntargets. Adaptic, with the help of its input-aware optimizations, can generate highly-optimized GPU ker\u00adnels \nto maintain high performance across different problem sizes. At runtime, Adaptic s runtime kernel management \nchooses the best performing kernel based on the input. Our results show that Adap\u00adtic s generated code \nhas similar performance to the hand-optimized CUDA code over the original program s input comfort zone, \nwhile achieving upto 6x speedup when the input falls out of this range.  Acknowledgement Much gratitude \ngoes to the anonymous referees who provided excel\u00adlent feedback on this work. This research was supported \nby ARM Ltd., the Gigascale Systems Research Center, and the National Sci\u00adence Foundation under grant \nCNS-0964478.  References [1] M. M. Baskaran, J. Ramanujam, and P. Sadayappan. Automatic C-to-CUDA code \ngeneration for af.ne programs. In Proc. of the 19th International Conference on Compiler Construction, \npages 244 263, 2010. [2] I. Buck et al. Brook for GPUs: Stream computing on graphics hardware. ACMTransactions \nonGraphics, 23(3):777 786, Aug. 2004. [3] B. Catanzaro, M. Garland, and K. Keutzer. Copperhead: compiling \nan embedded data parallel language. In Proc. of the 16th ACM SIGPLAN SymposiumonPrinciples andPracticeofParallelProgramming, \npages 47 56, 2011. [4] B. Catanzaro, N. Sundaram, and K. Keutzer. Fast support vector ma\u00adchine training \nand classi.cation on graphics processors. In Proc. of the 25th International Conference on Machine learning, \npages 104 111, 2008. [5] R. Garg and J. N. Amaral. Compiling python to a hybrid execution envi\u00adronment. \nIn Proc. of the3rdWorkshop onGeneralPurposeProcessing onGraphics ProcessingUnits, pages 19 30, 2010. \n[6] M. I. Gordon, W. Thies, and S. Amarasinghe. Exploiting coarse\u00adgrained task, data, and pipeline parallelism \nin stream programs. In 14th International Conference on Architectural Support for Programming Languages \nand Operating Systems, pages 151 162, 2006. [7] M. Grossman, A. Simion, Z. Budimli, and V. Sarkar. CnC-CUDA: \nDeclarative Programming for GPUs. In Proc. of the 23rd Workshop on Languages and Compilers for Parallel \nComputing, pages 230 245, 2010. [8] T. Han and T. Abdelrahman. hiCUDA: High-level GPGPU pro\u00adgramming. \nIEEE Transactions on Parallel and Distributed Systems, 22(1):52 61, 2010. [9] C. Hong, D. Chen, W. Chen, \nW. Zheng, and H. Lin. Mapcg: writing parallel program portable between CPU and GPU. In Proc. of the 19th \nInternational Conference on Parallel Architectures and Compilation Techniques, pages 217 226, 2010. [10] \nS. Hong and H. Kim. An analytical model for a GPU architecture with memory-level and thread-level parallelism \nawareness. In Proc. of the 36thAnnualInternationalSymposium onComputerArchitecture, pages 152 163, 2009. \n[11] A. Hormati, Y. Choi, M. Woh, M. Kudlur, R. Rabbah, T. Mudge, and S. Mahlke. Macross: Macro-simdization \nof streaming applications. In 18th International Conference on Architectural Support for Program\u00admingLanguages \nandOperating Systems, pages 285 296, 2010. [12] A. H. Hormati, M. Samadi, M. Woh, T. Mudge, and S. Mahlke. \nSponge: portable stream programming on graphics engines. In 19th Inter\u00adnational Conference on Architectural \nSupport for Programming Lan\u00adguages andOperating Systems, pages 381 392, 2011. [13] M. Kudlur and S. Mahlke. \nOrchestrating the execution of stream programs on multicore platforms. In Proc. of the 08 Conference \non Programming Language Design and Implementation, pages 114 124, June 2008. [14] S. Lee, S.-J. Min, \nand R. Eigenmann. OpenMP to GPGPU: a compiler framework for automatic translation and optimization. In \nProc. of the 14thACMSIGPLANSymposiumonPrinciples andPracticeofParallel Programming, pages 101 110, 2009. \n[15] V. W. Lee, C. Kim, J. Chhugani, M. Deisher, D. Kim, A. D. Nguyen, N. Satish, M. Smelyanskiy, S. \nChennupaty, P. Hammarlund, R. Singhal, and P. Dubey. Debunking the 100x GPU vs. CPU myth: an evaluation \nof throughput computing on CPU and GPU. In Proc. ofthe37thAnnual International Symposium on Computer \nArchitecture, pages 451 460, 2010. [16] A. Leung, O. Lhot\u00b4ak, and G. Lashari. Automatic parallelization \nfor graphics processing units. In Proc. of the 7th International Conference on Principles and Practice \nof Programming in Java, pages 91 100, 2009. [17] X. Li, M. J. Garzar\u00b4an, and D. Padua. A dynamically \ntuned sorting li\u00adbrary. In Proc. of the 2004 International Symposium on Code Genera\u00adtion and Optimization, \npages 111 , 2004. [18] Y. Liu, E. Z. Zhang, and X. Shen. A cross-input adaptive framework for GPU program \noptimizations. In 2009 IEEE International Symposium onParallel and Distributed Processing, pages 1 10, \n2009. [19] J. Meng and K. Skadron. Performance modeling and automatic ghost zone optimization for iterative \nstencil loops on GPUs. In Proc. of the2009InternationalConference onSupercomputing, pages 256 265, 2009. \n[20] NVIDIA. GPUs Are Only Up To 14 Times Faster than CPUs says Intel, 2010. http://blogs.nvidia.com/ntersect/2010/06/gpus-are-only-up\u00adto-14-times-faster-than-cpus-says-intel.html. \n[21] A. Prasad, J. Anantpur, and R. Govindarajan. Automatic compilation of MATLAB programs for synergistic \nexecution on heterogeneous proces\u00adsors. In Proc.of the 11ConferenceonProgrammingLanguageDesign and Implementation, \npages 152 163, 2011. [22] V. T. Ravi, W. Ma, D. Chiu, and G. Agrawal. Compiler and runtime sup\u00adport for \nenabling generalized reduction computations on heterogeneous parallel con.gurations. In Proc. of the \n2010 International Conference onSupercomputing, pages 137 146, 2010. [23] D. Roger, U. Assarsson, and \nN. Holzschuch. Ef.cient stream reduc\u00adtion on the GPU. In Proc. of the 1st Workshop on General Purpose \nProcessing onGraphicsProcessing Units, pages 1 4, 2007. [24] S. Ryoo, C. I. Rodrigues, S. S. Baghsorkhi, \nS. S. Stone, D. B. Kirk, and W. mei W. Hwu. Optimization principles and application performance evaluation \nof a multithreaded GPU using CUDA. In Proc. of the 13thACMSIGPLANSymposiumonPrinciples andPracticeofParallel \nProgramming, pages 73 82, 2008. [25] D. Tarditi, S. Puri, and J. Oglesby. Accelerator: using data parallelism \nto program GPUs for general-purpose uses. In 14th International Conference on Architectural Support for \nProgramming Languages and Operating Systems, pages 325 335, 2006. [26] W. Thies and S. Amarasinghe. An \nempirical characterization of stream programs and its implications for language and compiler design. \nIn Proc. of the 19th International Conference on Parallel Architectures and Compilation Techniques, pages \n365 376, 2010. [27] W. Thies, M. Karczmarek, and S. P. Amarasinghe. StreamIt: A lan\u00adguage for streaming \napplications. In Proc. of the 2002 International Conference onCompilerConstruction, pages 179 196, 2002. \n[28] N. Thomas, G. Tanase, O. Tkachyshyn, J. Perdue, N. M. Amato, and L. Rauchwerger. A framework for \nadaptive algorithm selection in stapl. In Proc. of the 10th ACM SIGPLAN Symposium on Principles and Practice \nofParallelProgramming, pages 277 288, 2005. [29] K. Tian, Y. Jiang, E. Z. Zhang, and X. Shen. An input-centric \nparadigm for program dynamic optimizations. In Proceedings oftheOOPSLA 10, pages 125 139, 2010. [30] \nA. Udupa, R. Govindarajan, and M. J. Thazhuthaveetil. Software pipelined execution of stream programs \non GPUs. In Proc. of the2009 InternationalSymposium onCodeGeneration andOptimization, pages 200 209, \n2009. [31] M. Wolfe. Implementing the PGI accelerator model. In Proc. of the 3rdWorkshop onGeneralPurposeProcessing \nonGraphicsProcessing Units, pages 43 50, 2010. [32] X.-L. Wu, N. Obeid, and W.-M. Hwu. Exploiting more \nparallelism from applications having generalized reductions on GPU architectures. In Proc. of the 2010 \n10th International Conference on Computers and Information Technology, pages 1175 1180, 2010. [33] Y. \nYang, P. Xiang, J. Kong, and H. Zhou. A GPGPU compiler for memory optimization and parallelism management. \nIn Proc. of the 10 Conference on Programming Language Design and Implementation, pages 86 97, 2010. [34] \nS. zee Ueng, M. Lathara, S. S. Baghsorkhi, and W. mei W. Hwu. CUDA-Lite: Reducing GPU programming complexity. \nIn Proc. of the 21st Workshop on Languages and Compilers for Parallel Computing, pages 1 15, 2008.  \n \n\t\t\t", "proc_id": "2254064", "abstract": "<p>While graphics processing units (GPUs) provide low-cost and efficient platforms for accelerating high performance computations, the tedious process of performance tuning required to optimize applications is an obstacle to wider adoption of GPUs. In addition to the programmability challenges posed by GPU's complex memory hierarchy and parallelism model, a well-known application design problem is target portability across different GPUs. However, even for a single GPU target, changing a program's input characteristics can make an already-optimized implementation of a program perform poorly. In this work, we propose Adaptic, an adaptive input-aware compilation system to tackle this important, yet overlooked, input portability problem. Using this system, programmers develop their applications in a high-level streaming language and let Adaptic undertake the difficult task of input portable optimizations and code generation. Several input-aware optimizations are introduced to make efficient use of the memory hierarchy and customize thread composition. At runtime, a properly optimized version of the application is executed based on the actual program input. We perform a head-to-head comparison between the Adaptic generated and hand-optimized CUDA programs. The results show that Adaptic is capable of generating codes that can perform on par with their hand-optimized counterparts over certain input ranges and outperform them when the input falls out of the hand-optimized programs' \"comfort zone\". Furthermore, we show that input-aware results are sustainable across different GPU targets making it possible to write and optimize applications once and run them anywhere.</p>", "authors": [{"name": "Mehrzad Samadi", "author_profile_id": "81482654881", "affiliation": "University of Michigan, Ann Arbor, MI, USA", "person_id": "P3471130", "email_address": "mehrzads@umich.edu", "orcid_id": ""}, {"name": "Amir Hormati", "author_profile_id": "81319493286", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P3471131", "email_address": "amir.hormati@microsoft.com", "orcid_id": ""}, {"name": "Mojtaba Mehrara", "author_profile_id": "81319497515", "affiliation": "NVIDIA Research, Santa Clara, CA, USA", "person_id": "P3471132", "email_address": "mmehrara@nvidia.com", "orcid_id": ""}, {"name": "Janghaeng Lee", "author_profile_id": "81500650371", "affiliation": "University of Michigan, Ann Arbor, MI, USA", "person_id": "P3471133", "email_address": "jhaeng@umich.edu", "orcid_id": ""}, {"name": "Scott Mahlke", "author_profile_id": "81100622742", "affiliation": "University of Michigan, Ann Arbor, MI, USA", "person_id": "P3471134", "email_address": "mahlke@umich.edu", "orcid_id": ""}], "doi_number": "10.1145/2254064.2254067", "year": "2012", "article_id": "2254067", "conference": "PLDI", "title": "Adaptive input-aware compilation for graphics engines", "url": "http://dl.acm.org/citation.cfm?id=2254067"}