{"article_publication_date": "06-11-2012", "fulltext": "\n JANUS: Exploiting Parallelism via Hindsight Omer Tripp Roman Manevich John Field Tel Aviv University \nThe University of Texas at Austin Google omertrip@post.tau.ac.il roman@ices.utexas.edu j.eld@google.com \nMooly Sagiv Tel Aviv University msagiv@post.tau.ac.il Abstract This paper addresses the problem of reducing \nunnecessary con.icts in optimistic synchronization. Optimistic synchronization must en\u00adsure that any \ntwo concurrently executing transactions that commit are properly synchronized. Con.ict detection is an \napproximate check for this condition. For ef.ciency, the traditional approach to con.ict detection conservatively \nchecks that the memory locations mutually accessed by two concurrent transactions are accessed only for \nreading. We present JANUS, a parallelization system that performs con\u00ad.ict detection by considering sequences \nof operations and their composite effect on the system s state. This is done ef.ciently, such that the \nruntime overhead due to con.ict detection is on a par with that of write-con.ict-based detection. In \ncertain common scenar\u00adios, this mode of re.nement dramatically improves the precision of con.ict detection, \nthereby reducing the number of false con.icts. Our empirical evaluation of JANUS shows that this precision \ngain reduces the abort rate by an order of magnitude (22x on average), and achieves a speedup of up to \n2.5x, on a suite of real-world benchmarks where no parallelism is exploited by the standard approach. \nCategories and Subject Descriptors D.1.3 [Programming Tech\u00adniques]: Concurrent Programming General Terms \nExperimentation, Measurement, Performance Keywords speculative execution, concurrency, con.ict detection, \ntransactional memory 1. Introduction Optimistic synchronization enables synchronizing parallel and dis\u00adtributed \ncomputations without the use of blocking. If such optimism causes improper synchronization, then the \nmis-synchronized work is undone and the entire system is restored to a consistent state. Optimistic synchronization \nis a promising approach to paral\u00adlelizing software. It offers ease of use (compared to writing ex\u00adplicitly \nconcurrent programs) and clear correctness guarantees; Permission to make digital or hard copies of all \nor part of this work for personal or classroom use is granted without fee provided that copies are not \nmade or distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. PLDI 12, June 11 16, 2012, Beijing, China. Copyright c &#38;#169; \n2012 ACM 978-1-4503-1205-9/12/06. . . $10.00 int work = 0; /* parallel */ foreach (item in items) process(item, \nwork); process(Item item, int work) {work += weightOf(item); Result result = processItem(item); if (result.isSuccessful()) \n// item processed successfully? work -= weightOf(item); foreach (Item sitem in item.subitems) process(sitem, \nwork); } // recursive call Figure 1. Program with available parallelism that is hard to exploit namely: \natomicity, whereby the changes made by a transaction become visible to other transactions only at commit \ntime; seri\u00adalizability, whereby committed transactions appear to execute in some serial order; and deadlock \navoidance. Thanks to these strong guarantees, embodiments of this approach in particular, software transactional \nmemory [16] (STM) have received much attention. Optimistic synchronization is speculative in nature. \nTypically, when a con.ict is detected between two concurrent transactions, one of the transactions is \naborted, resulting in wasted work. Con\u00ad.ict detection is commonly carried out by checking whether the \nconcurrent transactions access a mutual memory location, where at least one of them writes it. This is \nknown as the write-set approach. The write-set approach is ef.cient, but at the same time, overly conservative \nin ensuring serializability when the concurrent transactions in their entirety commute. Replacing the \nwrite-set approach for con.ict detection, which tests every pair of operations performed by the concurrent \ntransactions for commutativity, with a more re.ned detection algorithm, which considers a broader view \nof the concurrent histories in its commutativity judgments, can po\u00adtentially reduce unnecessary aborts, \nthereby increasing the level of concurrency. Illustrative Example Consider the program in Figure 1, inspired \nby the JFileSync application [2] (which we later discuss), where a collection of items is processed, \nand pending work (corresponding to items whose processing wasn t successful) is accumulated into variable \nwork. Assuming that processItem is a pure function (i.e., free of side effects) and processItem calls \nare largely successful, the above loop admits a high degree of available parallelism. As long as updates \nto work are performed atomically, distinct items can be processed concurrently. Since most iterations \nrestore work to its value at the beginning of the iteration, speculation is preferable to locking in \nensuring the atomicity of accesses to work: If each iteration is executed as a transaction, then in most \ncases, concurrent transactions are not in con.ict having acted as the identity function on the shared \nstate (work).  Using the write-set con.ict-detection approach, however, con\u00ad.icts would be detected \nbetween any pair of transactions whose executions interleave, causing (at least) one of the transactions \nto abort. In effect, the execution of all transactions will be serialized, resulting in slowdown (compared \nto sequential execution of the loop). In contrast, with more accurate detection, which considers sequences \nof memory accesses rather than single operations, the synchronization algorithm can exploit the observation \nthat most transactions preserve the original value of work, which enables a high level of concurrency \nand a potential speedup (depending on the cost of internal bookkeeping and load balancing). Current Approaches \nRecent techniques that we are aware of for improving speculative parallelization are challenged by this \nexam\u00adple. Transactional checkpointing [17], dependence-aware transac\u00ad tional memory [21] and elastic \ntransactions [11] are all effective in reducing the cost and magnitude of aborts, yet these techniques \ncannot utilize the available parallelism in the above loop, which can only be appreciated by considering \nthe (dynamic) interplay be\u00adtween increments and decrements of variable work. Abstract locking [14, 19, \n20] would also be futile in this case. Even if an abstraction speci.cation is provided, transactions \nwould con.ict at the semantic level when attempting to increment and decrement variable work concurrently. \nOther forms of user speci.\u00adcation, such as early release [15] or the annotations supported by the Alter \nsystem [24], are also of limited value here, because there is no particular con.ict kind that can be \nuniversally suppressed without changing the semantics of the program. Contributions Straightforward enforcement \nof sequence-based con.ict detection is prohibitively expensive, especially relative to write-set-based \ndetection. This paper describes an ef.cient tech\u00adnique for accomplishing such re.ned judgments while \nkeeping runtime overhead on a par with that of the write-set-based ap\u00adproach. The key to our technique \nis a training phase (Section 5.1), where sequences of operations that are likely to appear in con.ict \nqueries during parallel execution of the client application are extracted from single-threaded, synchronization-free, \ntraining runs of the application, and symbolic commutativity conditions are computed of.ine for pairs \nof such sequences. For the information learned during training to generalize to new runs, our technique \nuses generalization (Section 5.2). This is done by mapping a concrete sequence (observed during training) \nto a regular abstraction of that sequence, such that commutativity conditions involving the original \nsequence remain valid for its abstraction. For the information learned during training to be of practical \nvalue, our technique uses projection (Section 5.3), a commutativ\u00ad ity testing algorithm ensuring that \nthe dynamic context needed during parallel execution to check if a cached evaluation (learned during \ntraining) can be used for a commutativity query is essen\u00adtially the same as in write-set detection: Only \nthe read and write sets of operations are recorded. This also allows falling back to the write-set approach \nif the query cannot be answered from the cache. We have implemented our sequence-based detection approach \nin JANUS, a parametric parallelization system based on optimistic synchronization (Section 4). The JANUS \nalgorithm is abstract enough to encompass both memory-level statements and opera\u00adtions on abstract data \ntypes (ADTs), which enable further boosting of the detection algorithm s accuracy. In the latter case, \nthe pro\u00adgrammer speci.es a representation function mapping a concrete data type to its abstract state, \nas well as the semantic interpretation of operations over the ADT. Our evaluation of JANUS on a suite \nof .ve real-world bench\u00admarks (Section 7) highlights the importance of sequence-based de\u00ad tection: JANUS \nachieved speedups of up to 2.5x, with few aborts, where its counterpart using write-set detection did \nnot utilize any of the available parallelism in the benchmarks, and suffered from 22x more aborts. The \npaper provides proof sketches for the correctness of the algorithms comprising our system. We defer full \nformal proofs to an accompanying technical report [22]. 2. Motivation The simple example of Figure 1 \nillustrates one of several semantic patterns appearing in real-world applications (cf. Table 5) that \nmotivate re.ned, sequence-based reasoning about commutativity between histories. Below is a non-exhaustive \nlist of such patterns, which we next illustrate using popular open-source applications: Identity The \ntransaction manipulates the shared state, but upon termination, restores it to its con.guration prior \nto its execution. Reduction A sequence of values is reduced to a single value by an associative and commutative \noperator. Shared-as-local Shared memory is used as a scratch pad . The transaction overwrites values \nset by other transactions, and only reads values written by its own execution. Programmers typi\u00adcally \nuse this pattern as an optimization for reducing allocation overhead. Equal-writes Distinct transactions \nassign the same value to a shared memory location. This pattern is useful when the con\u00addition enabling \ntwo (concurrent) transactions to write equal values can be checked ef.ciently at runtime. Spurious-reads \nA read memory location cannot in.uence the transaction s observable effect (cf. [15]). JFileSync The \nidentity pattern is exempli.ed by Figure 2, where (a simpli.ed version of) the JFileSync [2] .le-synchronization \nutil\u00ad ity is presented. The main loop of JFileSync iterates over pairs of .les or directories recursively, \nand computes .le synchroniza\u00adtion metadata for each pair via the compareFiles call. Each iteration updates \nthe number of items left to handle, as well as the overall weight of pending items, when a new work item \nis encountered (.elds itemsStarted and itemsWeight of monitor, respectively). These values are popped \nonce processing of that item completes. Compile-time parallelization of this loop is impaired by data \nde\u00adpendencies between iterations due to monitor and its .elds. More\u00adover, even if the compiler is able \nto assert that monitor can be privatized, the shared progress object responsible for interacting with \nobservers of the computation (via calls to fireUpdate) must remain shared for cancelation requests to \napply to all active transactions. This mandates careful synchronization of concurrent accesses to monitor, \nwhich presents a challenge not only for the compiler, but also for developers attempting manual parallelization \nof this code. Speculative parallelization is thus appealing, but the write-set approach is too conservative. \nJGraphT-1 Figure 3, showing the code of the JGraphT greedy graph-coloring algorithm, illustrates the \nspurious-reads pattern: Two transactions that execute in parallel starting from the same valuation of \nmaxColor can only con.ict if both assign new (and different) values to maxColor. If one (or both) of \nthe transactions merely reads this variable, then there is no threat of con.ict, and so read write con.icts \ncan safely be suppressed. A compiler would fail to parallelize this algorithm because of loop-carried \ndependencies involving not only maxColor, but also the  /* parallel */ for (JFSDirectoryPair pair : \npairs) { monitor.itemsStarted.add(2); monitor.itemsWeight.add(1); monitor.rootUriSrc = pair.getSrc(); \nmonitor.rootUriTgt = pair.getTgt(); if (!progress.isCanceled()) { ...; weight = ...; monitor.itemsStarted.add(srcDirs.length+tgtDirs.length); \nmonitor.itemsWeight.add(weight); progress.fireUpdate(); ...; /* compareFiles is recursive, also making \nbalanced add-remove calls */ compareFiles(...); monitor.itemsStarted.remove(monitor.itemsStarted.size()-1); \nmonitor.itemsWeight.remove(monitor.itemsWeight.size()-1); } monitor.itemsStarted.remove(monitor.itemsStarted.size()-1); \nmonitor.itemsWeight.remove(monitor.itemsWeight.size()-1); progress.fireUpdate(); } Figure 2. JFileSync: \nidentity (monitor.{itemsStarted,itemsWeight}) and shared-as-local (monitor.{rootUriSrc,rootUriTgt}) RuleSets \nrs = ruleSetFactory.createRuleSets(rulesets); /* parallel */ for (DataSource dataSource : files) { String \nniceFileName = dataSource.getNiceFileName(...); ...; ctx.sourceCodeFilename = niceFileName; ctx.sourceCodeFile \n= new File(niceFileName); ...; rs.start(ctx); ...; rs.end(ctx); ...; } /* in GenericClassCounterRule.java; \ncalled transitively by rs.start */ @Override public void start(RuleContext ctx) { ctx.setAttribute(COUNTER \nLABEL, new AtomicLong()); super.start(ctx); } Figure 4. PMD: shared-as-local (ctx) shared color and usedColors \narrays, where usedColors follows the shared-as-local pattern. These would also impair speculation based \non the write-set approach. Manual parallelization of the code (e.g., using map reduction) is also not \nimmediate, because the greedy algorithm mandates ordered traversal over the nodes of the graph. PMD A \nnear match for the shared-as-local pattern appears in PMD, a popular code scanner for Java whose main \nloop is shown in Figure 4. Most of the .elds of the shared RuleContext instance, ctx, are treated as \nlocal by the loop s iterations. For example, each iteration .rst writes sourceCodeFile and sourceCodeFilename, \nand only later reads these .elds. However, sharing between iterations is enabled via attributes stored \nin ctx (by calling {set,get}Attribute). This means that ctx cannot be privatized. Compile-time parallelization \nof the PMD loop is thus inhibited. Manual transformation is also subtle: Calls to {set,get}Attribute \noccur deep in the call stack from the main loop, and are thus hard to track and observe. Sound handling \nof the persistent attributes fur\u00adther requires manual transformation of the RuleContext class, such that \nthe .elds that are amenable to privatization become separate from those that must remain global. Synchronization \nmust then be added to mediate accesses to the global RuleContext state. Fi\u00adnally, write-set-based speculation \nwill perform poorly because all iterations update the local sourceCodeFile and sourceCodeFilename .elds. \nWeka Last, Figure 5 presents code from the Weka data-mining library that is responsible for rendering \na graph to a display device. This algorithm illustrates the equal-writes pattern in that distinct iterations \naccessing the same pixel do not con.ict if they have set the Graphics object to the same color. Since \nthe iterations are not invariantly independent, a compiler cannot statically parallelize this code, nor \nis it clear which manual transformations can be applied in this case. Speculative paralleliza\u00adtion of \nthe loop that is oblivious to pixel colors and only considers the accessed pixels is also overly pessimistic, \nbut accounting for int[] order = ...; // permutation of nodes .xing traversal order int[] color = new \nint[neighbors.length]; int maxColor = 1; BitSet usedColors = new BitSet(neighbors.length); /* parallel \n*/ for (int i = 0; i < neighbors.length; i++) { int v = order[i]; usedColors.clear(); for (int j = 0; \nj < neighbors[v].length; j++) { int nb = neighbors[v][j]; if (color[nb] > 0) { usedColors.set(color[nb]); \n}} color[v] = 1; while (usedColors.get(color[v])) { color[v]++; } if (color[v] > maxColor) { maxColor \n= color[v]; }}  Figure 3. JGraphT-1: shared-as-local (usedColors) and spurious-reads (maxColor) Graphics2D \ng = ...; /* parallel */ for (int index=0; index<nodes.size(); index++) {GraphNode n = (GraphNode) nodes.elementAt(index); \nif (n.nodeType == NORMAL) { g.setColor(this.getBackground().darker().darker()); g.fillOval(x+n.x+...,y+n.y,nodeWidth, \nnodeHeight); g.setColor(Color.White); ...; g.drawString(n.lbl,x+n.x+...,y+n.y+nodeHeight/2+...); ...; \ng.setColor(Color.black); } else { g.drawLine(x+n.x+...,y+n.y,x+n.x+...,y+n.y+nodeHeight); } int x1, y1, \nx2, y2; ...; g.drawLine(x+x1, y+y1, x+x2, y+y2); ...; } Figure 5. Weka: equal-writes (graphics) Figure \n6. Outline of the JANUS architecture pixel values entails customization of the detection algorithm and \nmay further lead to prohibitive instrumentation overhead. 3. Overview The examples of Section 2 motivate \nmore accurate con.ict de\u00adtection. However, boosting the accuracy of the detection algo\u00adrithm comes at \nthe cost of degrading performance. The write-set heuristic, employed in many realistic parallelization \nsystems (e.g., [11, 14, 17, 20, 21]), tracks the read and write sets of a transaction, such that con.ict \ndetection reduces to a check whether there is a memory location that one of the concurrent transactions \nwrites and the other accesses (either reading from it or writing to it). Moving beyond the write-set \napproach, toward more accurate detection algorithms, involves two potential performance penalties: First, \nif the algorithm bases its judgment on information that is be\u00adyond read and write sets (e.g., the commutativity \npolicies in [19]), then further instrumentation overhead for collecting this addi\u00adtional information \nis incurred. Second, the complexity of the de\u00adtection algorithm itself may be prohibitive. This rules \nout na\u00a8ive re\u00adalization of sequence-based commutativity. On the other hand, the codes in Figures 2 5 \nall real-world applications illustrate several common scenarios where sequence\u00adbased reasoning provides \nsigni.cant value over the write-set approach. This remains true in the presence of state abstrac\u00adtion \n[14, 20], where con.icts are tracked atop the semantic state of data structures implementing ADTs. Though \nthe (likely im\u00adpractical) solution of instrumenting the program s state to record arbitrary trace information \nis theoretically possible, doing so does not only degrade performance, but also involves manual user \ninter\u00advention in deciding which information to record.  Our Approach In this paper, we present an automated \ntechnique for performing con.ict detection over (potentially long) sequences, such that (i) there is \nno instrumentation overhead beyond that of the write-set approach, and (ii) the complexity of the detection \nal\u00adgorithm is also comparable to write-set-based detection. Our tech\u00adnique, based on a preliminary learning \nphase, favors applications where learning holds promise, in that the effect of transactions on the shared \nstate remains similar across different inputs. For other applications, where this is not the case, our \napproach will behave analogously to the write-set approach. The .ow of our solution is illustrated in \nFigure 6. The key as\u00ad pect of our approach is of.ine learning of commutativity informa\u00adtion: The application \nis pro.led using training data, and commuta\u00adtivity conditions are evaluated for sequences arising during \ntrain\u00ading that are expected to occur (frequently) in production mode. These candidate sequences essentially \ncomprise dependent opera\u00adtions within the trace. During parallel execution, the conditions that were \nlearned of.ine are used to test commutativity over sequences that are similar to the ones identi.ed in \ntraining. To ensure that the runtime overhead of our system is compara\u00adble to that of the write-set heuristic, \nthe of.ine commutativity in\u00adformation we compute relates to sequences of operations over indi\u00advidual \nmemory locations (Section 5.3). At runtime, this enables re\u00ad construction of single-location-based sequences \nfrom the read and write sets of operations, and thus the dynamic context needed for sequence-based detection \ndoes not impose any instrumentation cost beyond that of the write-set approach. The process prescribed \nby our approach consists of the follow\u00ading stages: 1. The input to our system is a user speci.cation \nmapping con\u00adcrete data structures to their (abstract) relational representation. The semantic state of \na data structure is speci.ed as a set of relations, and operations over the data structure are expressed \nusing relational primitives. The BitSet class used in Figure 3, for instance, can be encoded as a 2-ary \nrelation mapping inte\u00adgral values to boolean values. A relational description of the get operation is \nthen speci.ed as a select query, and similarly, setting the bit at index n to value x translates into \nremoving the (unique) tuple whose .rst component is n and then insert\u00ading (n, x). The primary reason \nfor the relational speci.cation is that it facilitates reasoning about commutativity between se\u00adquences \nof operations. 2. Next, the subject application is exercised in sequential (i.e., single-threaded) mode \nusing training inputs, such that no syn\u00adchronization is required. In these runs, sequential dependencies \nare tracked between trace operations, and sequences of depen\u00addent operations that belong in different \ntransactions are con\u00adsidered. In the example of Figure 3, for instance, two such se\u00ad quences may be { \nwork+=2; work-=2; work+=1; work-=1; } and { work+=3; work-=3; }. 3. Commutativity conditions in the \nform of designated input states are then computed for each pair of such sequences, where concrete values \nare substituted by symbolic values (e.g., { work+=x; work-=x; } for the second sequence). Generalization \nfrom concrete observations to arbitrary sequences is done using a theorem prover to ensure soundness. \n 4. To lift the learned commutativity information to new runs of the application, the sequences recorded \nduring training are gen\u00aderalized into an abstract regular form by allowing arbitrarily  many occurrences \nof idempotent subsequences within the gen\u00aderalized sequence (Section 5.2). The abstraction of { work+=x; \nwork-=x; }, for example, is { work+=x; work-=x; }+ (where + denotes the Kleene-cross operator). 5. Finally, \nin production mode, a commutativity query is answered positively based on the cached information if the \nsequences in the query match a cached pair of sequences, and the input state belongs in the designated \ninput states for the matched pair. If no match is found in the cache, then our technique falls back to \nthe write-set approach. 4. The JANUS Parallelization Protocol In this section, we de.ne the protocol \nenforced by our paralleliza\u00adtion system, where we leave the con.ict-detection algorithm un\u00adspeci.ed (until \nSection 5, which de.nes sequence-based con.ict detection). We refer the reader to [22] for a formal description \nof the transition system underlying our protocol. 4.1 Parallelization Protocol JANUS accepts as input \n(i) an initial con.guration of the shared state, (ii) a list of tasks, each consisting of a program prog \nto be run and initial data values (o . .), and (iii) a speci.cation whether to commit the tasks in the \norder in which they were given. JANUS maintains two state variables: an atomic integer (Clock) implementing \nthe versioning of the shared state, and a read-write lock (lock) for synchronizing concurrent activities. \n(In Figure 7, we denote use of the read (write) lock provided by lock when invoking an operation via \nthe with lock.r (lock.w) syntax.) JANUS repeatedlytriestoexecute theinputtasksasynchronously, in parallel, \nuntil its task pool is drained (DOPARALLEL). Each ex\u00adecution attempt (RUNTASK call) encloses the argument \ntask in a transaction, where the task s identi.er and the time of creation of the transaction are recorded \n(CREATETRANSACTION). CRE-ATETRANSACTION copies the global state (Sh), as well as the task-speci.c data \n(o . .), into the local state of the freshly de\u00ad.ned transaction. (Note that CREATETRANSACTION requires \nonly a read lock, which enables multiple simultaneous transaction ini\u00adtializations.) Next, RUNTASK executes \nthe transaction sequentially (RUNSE-QUENTIAL). In the case of in-order execution, the transaction can \ntry to commit (after sequential execution) only once all its preced\u00ading transactions (ordered by task \nidenti.ers) have committed. As soon as this happens, or if the ordered .ag is not set, the transac\u00adtion \nrepeatedly attempts to commit until it either succeeds or fails. This is done by retrieving the sequence \nof operations committed be\u00adtween the time of start of the transaction (t.Begin) and the present time \n(now), and checking for con.icts between this portion of the system s history (ops c) and the operations \nperformed by the trans\u00adaction (t.Log). (Note that no lock is acquired for con.ict detection, which we \nlater compensate for in COMMIT.) If con.icts are detected between the histories, then the trans\u00adaction \naborts and RUNTASK will be called again from scratch. Otherwise, COMMIT is called. (Note that this is \nthe only call that is governed by a write lock.) COMMIT .rst checks whether the considered system history \nhas evolved between the point where DETECTCONFLICT was invoked and the present time (using the now tcheck \ntest). If the history is valid (i.e., it hasn t evolved), = then (i) clock is incremented, and (ii) all \nthe operations the transac\u00adtion performed on privatized copies of global objects are replayed on their \ncounterparts in global memory (REPLAYLOGGEDOPERA-TIONS). Otherwise, con.ict detection is re-attempted. \nVersioning To reduce the cost of state privatization, which is a key aspect of the JANUS algorithm, (fully) \npersistent data struc\u00adtures [10] can be used. A persistent data structure preserves the pre\u00ad  Types: \nTransaction = Record { tid : integer // transaction identi.er Begin : integer // transaction begin time \nLoc : state // transaction-local state SharedPrivatized : state // privatized shared state SharedSnapshot \n: state // snapshot of shared state at Begin Log : list // history of operations } Inputs: ((prog1,o1 \n. .1) ,..., (prog,om . .m)): tasks m Sh0: initial global (i.e., shared) state ordered: in-order execution \n.ag Variables: Clock: atomic integer initialized to 1 lock: read-write lock with read lock r and write \nlock w DOPARALLEL: for i := 1 ...m do asynchronously r := RUNTASK(progi,oi . .i,i) while \u00acr RUNTASK(prog,o \n. ., i): t := CREATETRANSACTION(o . ., i) with lock.r RUNSEQUENTIAL(prog,t) // manipulates t.{Loc, SharedPrivatized, \nLog}if ordered wait until t.tid = GET(Clock) do while true now := GET(Clock) with lock.r opsc := GETCOMMITTEDHISTORY(t.Begin, \nnow) if DETECTCONFLICTS(t.SharedSnapshot, t.Log, opsc) return false // abort if COMMIT(t, now) with \nlock.w return true CREATETRANSACTION(o . ., i): t := FRESHTRANSACTION ( tid := i Begin := GET(Clock) \nLoc := (o . .) SharedPrivatized := Sh SharedSnapshot := Sh Log := E ) COMMIT(t, tcheck): now := GET(Clock) \nif now = tcheck return false // abort INCREMENT(Clock) REPLAYLOGGEDOPERATIONS(t.Log, Sh) return true \n We say that a con.ict-detection algorithm is sound if it does not permit a transaction that does not \ncommute with its con.ict history to commit. We say that a con.ict-detection algorithm is valid if it \ndoes not prevent a transaction with an empty con.ict history from committing. Theorem 4.1. Assume that \nthe protocol of Figure 7 is instantiated with a sound and valid con.ict-detection algorithm. Then 1. \n(termination) every run of the protocol is guaranteed to termi\u00adnate if the sequential execution of all \ntasks (operation RUNSE-QUENTIAL) terminates; and 2. (serializability) assuming termination, (i) every \nordered run of the protocol is guaranteed to terminate in the same .nal state as its sequential counterpart, \nand (ii) every unordered run of the protocol is guaranteed to terminate in the same .nal state as a sequential \nexecution of the tasks where their order of execution corresponds to the commit order in the concurrent \nrun.  Proof Sketch. For termination, observe that a RUNTASK call by task t can fail iff there is another \ntask, tI, whose RUNTASK call succeeds. Further observe that failure due to successful RUNTASK calls of \nother tasks can only occur .nitely many times, since the number of tasks scheduled for execution is .xed \nand known in advance, which guarantees that a scheduled task will eventually commit. For correctness, \nwe rely on the soundness of the con.ict\u00addetection algorithm. In-order execution is enforced by preventing \na transaction from attempting a commit until the global counter matches its identi.er, which implies \nthat all preceding tasks have committed already. 5. Con.ict Detection with Hindsight The focus of this \nsection is on the sequence-based con.ict-detection algorithm employed by JANUS. The techniques enabling \nthis algo\u00adrithm are discussed in turn in the following subsections. 5.1 The Training Phase The purpose \nof training is to specialize the detection algorithm in advance of parallel execution by building a cache \nof commutativity conditions relating to sequences of operations. In production mode, this cache saves \nthe (expensive) work of performing sequence\u00adbased commutativity checking. Instead, the cache is searched \nfor a match for the concurrent sequences in their input state. If no match is found, then the detection \nalgorithm defaults to write-set-based Figure 7. Pseudo-code of the parallelization protocol enforced \nby JANUS, where the con.ict-detection algorithm is unspeci.ed vious version of itself when modi.ed; a \ndata structure is fully per\u00adsistent if every version can be both accessed and modi.ed, which in our case \npermits concurrent modi.cation of the shared state by multiple simultaneous transactions. Con.ict Detection \nThe ideal test for con.icts is an explicit com\u00admutativity check. Assume that transaction t started running \nat time t0, in state s0, and is performing a con.ict test at time tk. We de\u00adnote by b = (b1,...,bn) the \nsequence of operations correspond\u00ading to transaction t. We refer to the operations corresponding to transactions \nthat committed within time interval [t0,tk], denoted by a = (a1,...,am), as the con.ict history of transaction \nt. Then t is in con.ict with its con.ict history iff [ a \u00b7 b]](s0)= [ b \u00b7 a]](s0). detection. Values \nand Dependencies. In support of dependence-analysis techniques that we introduce later in this paper, \nwe make the as\u00adsumption that the values assigned to objects are separable into subvalues. Formally, we \nassume a subvalue lattice with a partial I ordering v. v, a join operation U, a meet operator n, and \na I subtraction operator de.ned for two ordered values v. v by I def I v - v= min{w | w U v= v}. Let \nop be an operation, such that the invocation of op in state s yields state sI: [ op]](s)= sI. Further, \nlet (o, v) be an object-value pair in state s, and (o, vI) its corresponding object-value pair in sI. \nWe denote the invocation of op in state s by ops. The frame of ops s restriction to o, denoted as opf \n(o), is the maximal unchanged s subvalue of o: v f = vnvI. The written subvalue of ops s restriction \nw to o is de.ned as ops (o)=(v - v f ) U (vI - v f ). Finally, the read subvalue of ops s restriction \nto o, given by opr (o), is the s r minimal subvalue v . v f , such that [ op]](s)= [ op]](s[o . (v \\ \nv f ) U v r]). Intuitively, v r is the portion of the frame that determines the outcome of op in state \ns.  A dependency arises between two operation instances, ops1 and f, if there exists an object o . dom(s1) \nn dom(s2), such that ops2 wr wr (op (o) U op opop (o)) = .. (1) (o)) n (f(o) U f s1 s1 s2 s2 That is, \nops1 and fboth access a common subvalue of ops2 v, either for reading or for writing. (Input dependencies \nare subsumed by this de.nition.) We note that our de.nition of a dependency, inspired by [23], is general \nenough to apply both in a concrete semantics and in an abstract semantics induced by state coarsening. \nMining Sequences. We begin by explaining how the sequences are obtained. For a speci.c training payload, \nthe training algorithm tracks dependencies between operations, within as well as across tasks, according \nto Equation 1. This results in a global dependence graph, G = (V, E), where the nodes of G are the operation \nl instances from the run, and edge v1 . v2 . E denotes that v1 depends on v2 over location l. Next, for \neach location l accessed during the run, the (unique) maximal dependence path corresponding to l in G \nis retrieved. The path is then partitioned according to task boundaries. This yields a collection of \nsubpaths, each corresponding to the operations in\u00advolving l within a single task. These subpaths capture \ndependent sequences of operations that may participate, during parallel exe\u00adcution, in con.ict queries. \nWe then compute commutativity conditions for each pair of such sequences. The conditions refer to the \ninput state in which the sequences are evaluated. Our current prototype has limited symbolic capabilities, \nwhich we continuously evolve on demand. We presently account for equality and inequality tests appearing \nin the sequence, and further support certain useful distinctions that are particular to container ADTs \n(such as the presence of a key in a Map object).  5.2 Generalization via Sequence Abstraction Concrete \nsequences of operations on shared locations often vary dynamically as a function of the input. For example, \nthe add subtract sequences induced by work in Figure 2 are length-wise proportional to the complexity \nof the input items. Caching com\u00admutativity information that holds for particular concrete sequences (those \nmined from sequential runs of the concurrent system) is thus of restricted value, as these sequences \nare tightly coupled to the training payloads. To address this, we exploit the observation that distinct \ncon\u00adcrete sequences having the same regular description are, under certain conditions, con.ict-wise equivalent. \nFor instance, sequence { work+=1;work-=1; } arising for the program in Figure 1 in\u00adduces the same con.icts \nas { work+=1;work-=1;work+=1;work-=1 }, and more generally, as any sequence in the language of regular \nex\u00adpression ({ work+=1;work-=1 })+ . This is stated formally in the following claim, which relies on \nthe notion of idempotence to assert a suf.cient condition for con.ict-wise equivalence between concrete \nsequences exhibiting the same regularity. Lemma 5.1. Let s := s1 \u00b7 s2 \u00b7 s3 be a concrete sequence of \noperations, and assume that s2 is an idempotent subsequence of s. Then operation CONFLICT from Figure \n8 cannot distinguish between s and sequence s I := s1 \u00b7s2 \u00b7s2 \u00b7s3. That is, given sequence s, s con.icts \nwith s in input state s according to CONFLICT iff s I con.icts with s in this state. Proof Sketch. Let \nsI be the intermediate state after evaluating s2 within s starting at entry state s. Then if we evaluate \ns I at state s, then the intermediate state before the second evaluation of s2, as well as after it, \nis sI. This implies that (i) the .nal states following evaluation of both s and s I at state s are identical, \nand moreover, Sequence-based Con.ict Detection Using Projection Inputs: () tt op1, . . . , op: operations \nperformed by current transaction m () cc op1, . . . , op: committed operations n G: entry state of current \ntransaction DETECTCONFLICTS: () t mt := DECOMPOSE( op1, . . . , opt ) m () cc mc := DECOMPOSE( op1, . \n. . , op) for loc . DOM(mt) n DOM(mc) if CONFLICT(G, loc, mt(loc),mc(loc)) return true return false \nn DECOMPOSE((op1, . . . , opk)): subseqs := \u00d8 for i := 1 ...k {loc1, . . . , locl} := GETACCESSEDLOCATIONS(opk) \nfor j := 1 ...l subseqs := subseqs[locj . subseqs(locj ) \u00b7 opk] return subseqs CONFLICT(s, l, (op1, \n. . . , opk) , (op1, . . . , op)): w r1 := GETREADSUBSEQUENCES((op1, . . . , opk)) r2 := GETREADSUBSEQUENCES((op1, \n. . . , op)) w for rsubseq . r1 if \u00acSAMEREAD(s, rsubseq, (op1, . . . , op)) return true w for rsubseq \n. r2 if \u00acSAMEREAD(s, rsubseq, (op1, . . . , opk)) return true if \u00acCOMMUTE(s, l, (op1, . . . , opk) , \n(op1, . . . , op)) return true w return false SAMEREAD(s, l, (op1, . . . , opk) , (op1, . . . , op)): \nw sI := (op1, . . . , opk) (s) 1 s2 I := (op1, . . . , opw)\u00b7(op1, . . . , opk) (s) v1 := sI (l) 1 v2 \n:= sI (l) 2 return v1 = v2 GETREADSUBSEQUENCES((op1, . . . , opk)): rsubseqs := \u00d8 for i := 1 ...k if \nISREAD(opi) rsubseqs := rsubseqs . {(op1, . . . , opi)}return rsubseqs Figure 8. Algorithm for con.ict \ndetection using projection (ii) read operations within the .rst and second occurrences of s2 within s \nI yield identical results. Generalization mitigates the problem of cache misses, and fur\u00adther allows \nuse of small (yet suf.ciently representative) inputs during training to accelerate cache population. \nInstead of relat\u00ading commutativity conditions to concrete sequences, which man\u00addates an exact match for \na commutativity query arising at runtime, JANUS relates commutativity conditions to abstract sequences. \nThese are computed of.ine, during the training phase, based on the concrete sequences due to the sequential \nruns. JANUS iteratively searches in a bottom-up fashion for idempotent subsequences within the concrete \nsequence, and applies the Kleene-cross abstrac\u00adtion to these subsequences.  5.3 Con.ict Detection Using \nProjection Figure 8 presents an idealized version of JANUS projection\u00ad based algorithm for con.ict-detection. \nThe algorithm checks whether mc the operations opit m of the current transaction, t, con.ict with i=1 \noperations (opic)n , which were committed by other transactions i=1 while t was running.  m ti and \ncommute with respect to l in starting state s, yields the conclusion that l s value is invariant under \nthe order of execution of t1 and t2. i=1 consists of the dependent operations accessing a single location. \nci (op This is enabled by recording the read and write sets of each oper- The above claim, as well as \nits proof, extend naturally to the ations. Next, the sequences corresponding to shared locations (per \ncase where multiple transactions commit during the execution of athe loc . DOM(m t ) n DOM(m c ) restriction) \nare analyzed for con\u00ad .icts (in CONFLICT). Importantly, private locations and the se\u00adquences of operations \napplied to them are safely ignored, which concurrent transaction. Relaxed Consistency Beyond the baseline \nchecks, and similarly to [24], JANUS supports a user-provided speci.cation of consis- tency relaxations \nfor data structures of choice. The user may spec\u00ad ify that updates to (instances of) a data structure \nbe committed value resulting from a read operation within s1 (s2) is in.uenced by in the presence of \nread-after-write (RAW) and/or write-after-write whether s2 (s1) is performed before s1 (s2) or not (the \nSAMEREAD (WAW) con.icts involving the data structure s state. tests), or (ii) s1 and s2 do not commute \nover l. The speci.cation that RAW con.icts are tolerable for a data The algorithm in Figure 8 is idealized \nin that in practice, CON\u00adstructure translates into dropping the SAMEREAD checks for FLICT consults its \ncache populated during the training phase shared locations de.ned by that data structure during con.ict \nde\u00adinstead of directly executing the SAMEREAD and COMMUTE tection (cf. Figure 3). Analogously, tolerance \nof WAW con.icts is checks. These are executed of.ine over the sequences due to train\u00adhandled by dropping \nthe .nal COMMUTE test (cf. Figure 4).ing. If a miss is registered, then JANUS default behavior is to \nJANUS also performs limited automatic inference of relax\u00adfall back to write-set-based detection. Alternatively, \nJANUS can be ation speci.cations. For example, if out-of-order parallelizationcon.gured to perform the \nsequence-based check online, which is is permitted, then JANUS ignores WAW dependencies chainingunlikely \nto be acceptable in performance (though memoization can two transactions, because these imply (under \ntransitive reduction)be used to support online training ). Correctness While the location-wise commutativity \ncheck is nat\u00ad ural, the SAMEREAD tests are less obvious. Lemma 5.2 af.rms that the transactions access \nthe shared locations in con.ict by .rst de.ning them and only then (potentially) reading them, and the \n.\u00adnal value of the shared locations is immaterial (as indicated by the out-of-order speci.cation). This \npattern is exempli.ed in Figure 4.that together, these two checks pose as a sound transaction-wide commutativity \njudgment. The following example (in Java syntax) 6. Relational Instantiation demonstrates that COMMUTE \nalone does not suf.ce: x = 0, y = 0; In this section, we present a relational realization of the techniques \n{ b = x==0; if (b) y = 1; x = 1; } /* 1st transaction */ { described in Section 5. Representing object \nstates and operations in 1; } /* 2nd transaction */ x= relational form provides a natural way of computing \nthe composite In this program, the subsequences corresponding to both x and y commute. ({ b = x==0; x \n= 1; } commutes with { x = 1; }, and { y = 1; } trivially commutes with E.) Yet the two transactions \ndo not commute. This is because the (control) dependence between x and y is (incorrectly) ignored. The \nrequirement that intermediate reads be unaffected by the order of execution of the concurrent transactions \nis a conservative approximation of the .ow (through local state) between shared locations. Lemma 5.2. \nLet t1 and t2 be two transactions, such that t1 com\u00ad mc m i mits during the execution of t2. Further \nlet ops1 mc and = op 1 i=1 effect of a sequence of operations. The relational instantiation is also general \nenough to encompass both standard (memory-level) transactions, whose executions con\u00adsist of a sequence \nof statements, and transactions where certain data structures are equipped with abstraction speci.cations. \nThese spec\u00adi.cations enable treating method invocations atomically by consid\u00adering their effect over \nabstract states. The latter setting is similar in spirit to [20] and [14], where ADT semantics are taken \ninto account in con.ict detection to reduce the rate of spurious con.icts. in ops2 = op 2 i=1 be the \nhistories corresponding to t1 and t2, re-6.1 Preliminaries spectively, and s the system state when t2 \nbegins execution. We Relations, Tuples and Dependencies We instantiate the de.ni\u00ad tions from 5.1 to the \ndomain of relations. Assume a set v of un\u00ad typed values drawn from a universe V, which includes the integers: \nZ . V.A tuple t = (c1 : v1,...,ck : vk) maps a set of columns (ci) to values from V. We use tc to denote \nthe valuation of tuple l 1 : l . L1} ({ops2 : l . L2}) the location-centric locations accessed by ops1 \n(ops2). Then t1 and t2 commute in s if ldenoteby {ops subsequences induced by ops1 (ops2), where L1 (L2) \ndenotes the the following two conditions hold for every l . L1 n L2: 1. (commutativity) ops l 1 l 2 commute \nwith respect to l:and ops t on column c.A relation is a set of tuples over identical columns. l l l \n1 \u00b7 ops2]](s))(l) = ([[ops2 \u00b7 ops1]](s))(l). I def I l ([[ops We de.ne a partial ordering on relations \nr . r by r = r ll 2. (intermediate reads) every read of l within ops1 (ops2) results the subset relation, \njoin by set union, meet by set intersection, and l l in the same value regardless of whether ops1 (ops2) \nis evalu\u00ad ated in state s or in state [ ops l l 2]](s) ([ ops1]](s)). Proof Sketch. We assume that the \ntwo conditions above hold, and subtraction by set subtraction. We write t |= f if tuple t satis.es formula \nf, where f is in the language of the grammar in Table 1. For any t, t |= true, and similarly, t |= false. \nFinally, t |= c = v iff tc = v. prove for an arbitrary shared location l that its value is the same in \nboth orders of execution of t1 and t2. First, we observe that control .ow within t1 (t2) remains the \nsame regardless of whether t2 (t1) A relation r has a functional dependency (FD) d := C1 . C2 if any \npair of tuples in r that are equal on columns C1 are also equal on columns C2. We refer to C1 (C2) as \nthe domain (range) of d.is executed before t1 (t2). This is guaranteed by the invariance We say that \ntuples t and tI match in relation r, and denote this byof intermediate reads from the shared state under \nthe order of t ~r tI, if (i) r de.nes an FD c, such that t and tI are equal on all execution of t1 and \nt2, and these assuming that the transactions are deterministic are the only way of in.uencing the .ow \nof a transaction. The projection of t1 s (t2 s) history on l therefore the columns in the domain of c, \nor (ii) r does not de.ne any FD and t and tI are equal on all common columns. l l l 1 (ops2) regardless \nof the order of execution of t1 and t2. This, together with our assumption that ops1 and ops remains \nsequence ops States and State Transformers In the relational representation, the value of an object comprises \none or more relations: o . l 2  f := true | false | c = v | (atomic) \u00acf | (negation) f . f | f . f \n(binary) Table 1. Production rules for formulas Operation Effect insert rt rI =(r \\{tI : t ~r tI}) .{t} \nI remove rt r= r \\{t} I w := select rf r= r, w = {t . r : t |= f} Table 2. Primitive relational operations \nand their meaning {ri}k Each relation ri is assumed to have at most one FD, i=1. where moreover, the \ndomain and range of the FD partition the relation s columns. Intuitively, this specializes the relation \nas a function mapping locations to their associated values . The effect of primitive relational operations \nis listed in Table 2. insert .rst removes the tuples matching its argument tuple from its argument relation, \nand then adds that tuple to the relation. remove ensures that its argument tuple is not in the relation. \nFinally, select de.nes a relation containing the tuples from its argument relation that match the selection \ncriterion, expressed as a propositional formula over the grammar of Table 1. State transformers both \nconcrete and abstract are expressed as sequences over the primitive relational operations. JANUS allows \nspecifying different transformers for invocations of the same oper\u00adation with different arguments. 6.2 \nSequence-based Con.ict Detection We now explain how the techniques described in Section 5 are realized \nusing the relational representation. Tracking Dependencies The footprint of primitive relational op\u00aderations \nis de.ned in Table 3. For sound tracking of dependencies between operations, we consider tuple t as belonging \nin the read set of operation remove rt if r does not contain t [23]. The footprint of a transformer, \nt = (r1,...,rk), is the cumu\u00adlative footprint of the relational operations it consists of: write(t)= \nwrite(ri) 1=i=k read(t)= read(ri) 1=i=k These de.nitions support the projection algorithm by enabling \ndependence-based decomposition of histories, as performed by the DECOMPOSE operation in Figure 8. Equivalence \nTesting Both SAMEREAD and COMMUTE test for equivalence between different views of an object s value. \nCOM-MUTE considers the entire state of the object, whereas SAMEREAD considers only the relation de.ned \nby a read operation, which cap\u00adtures part of the object s state. To perform equivalence judgments, we \nrely on a logical repre\u00adsentation of the content of a relation, as speci.ed in Table 4: The content of \na relation is expressed as a restriction formulated as a propositional formula on the values from V contained \nin the relation. Thus, the removal of (the tuples in) relation w from rela\u00adtion r is re.ected in the \nformula describing the content of r (fr) by conjoining it with the negation of the formula corresponding \nto w. Similarly, adding w to r is re.ected as a disjunction from the formula describing the content of \nw. The update rules corresponding to primitive operations are slightly more complex. For example, the \nrule for insert prescribes that .rst the tuples matching the tuple t to be inserted are removed. i This \nis accomplished by conjoining formula c = tc with c.Cdom fr. Next, to express that t is then added to \nthe relation, formula ( c.C c = tc is disjoined from the intermediate formula. Table 3. The footprint \nof primitive relational operations Operation Formula I frI r= r \\ w = fr .\u00acfw I frI = fr . fw r= r . \nw I frI = fr . fw r= r n w insert rt frI =(fr . c = tc) . _ c.C c = tc c.Cdom frI remove rt = fr .c.C \nc = tc fw = fr . . w := select r. Table 4. Logical representation of transformations on a relation Describing \nthe content of a relation in propositional form allows implementing (symbolic) equivalence tests as calls \nto a SAT solver. Given two representations fr and .r of the content of relation r, JANUS checks for equivalence \nbetween fr and .r by asking the SAT solver for a satisfying assignment for \u00ac(fr . .r). If the solver \nfails to .nd such an assignment (without timing out), then fr and .r are con.rmed to be equivalent. 7. \nEmpirical Evaluation Inthissection,wedescribeourprototypeimplementationof JANUS, as well as its experimental \nevaluation on a suite of .ve real-world benchmarks, where we tested (i) the gain from sequence-based \ncon.ict detection (compared to standard detection), and (ii) the signi.cance of basing this mode of detection \non of.ine training (with and without sequence abstraction). 7.1 Preliminaries Prototype Implementation \nJANUS is implemented as a (static) Java library that exposes an interface for running client-provided \ntasks in parallel (via the run, runInOrder and runOutOfOrder meth\u00adods), as well as for controlling various \naspects of the execution (e.g., enabling pro.ling, con.guring the pro.ling policy, setting the number \nof threads, modifying the thread management policy, etc). For logging purposes, JANUS automatically inserts \ninstrumenta\u00adtion hooks into its client application at runtime. For this, we reused functionality from \nChord [1], a framework for transforming and analyzing Java bytecode based on the Joeq compiler infrastruc\u00adture \n[25] and the Javassist bytecode-instrumentation library [9]. JANUS also uses the Sat4j SAT solver [5] \nfor resolving equivalence queries (cf. Section 6.2). The current JANUS prototype does not automatically \nidentify candidate loops for parallelization. Instead, the user decides where to apply JANUSby manually \ntransforming a loop structure into a JANUScall. (This decision can be guided by automated tools, such \nas [23], which we are planning to integrate into JANUS.) JANUS accepts user-provided abstraction speci.cations \nas ex\u00adtensions of its core algorithm written in Java. When an instance o of a speci.ed data structure \nis created, JANUS maps it to a fresh set of relations (per the speci.cation). Later, when an operation \nis in\u00advoked on o, JANUS applies the model associated with the operation to the (abstract) relational \nstate of the object, thereby accounting for the effect of the operation. The binding between concrete \nex\u00adecution events and the user speci.cation is handled automatically (via instrumentation). Benchmarks \nWe used .ve real-world benchmarks, all taken from the SourceForge code repository [6], for our experiments. \nThese are listed in Table 5. All the benchmarks are sequential (though PMD has an execution mode where \ncertain subcomputations are run in parallel), and their core functionality involves processing a collection \nof items in a loop. (This obviated the need to select which loop to parallelize.)  In JGraphT, we considered \ntwo distinct algorithms within the GreedyColoring entry point: (i) an optimized version of the greedy \nalgorithm for graph coloring (color), and (ii) an implementation of the saturation-degree heuristic for \nnode ordering for the greedy coloring algorithm (largestSaturationFirstOrder). In JFileSync, we used \nthe JFSComparison entry point, which computes the differences (at all hierarchies) between pairs of directories. \nIn PMD, we con\u00adsidered the main entry point (PMD), which iterates over a collec\u00adtion of Java source .les \nand analyzes them (intraprocedurally) for correctness and quality problems. Finally, in Weka we used \nthe GraphVisualizer entry point, which renders a graph to a display de\u00advice by traversing its nodes. \nFor each of the benchmarks, we manually transformed the rele\u00advant loop by casting its iterations into \ntask objects and replacing the loop construct by a call to JANUS to run the tasks in parallel. These \ntransformations proved straightforward, being purely syntactic, and can be automated with relative ease. \nExperimental Setup Our experimental design consists of two sets of experiements. In the .rst, we compared \nbetween two versions of JANUS whose sole difference lies in whether or not sequence\u00adwide reasoning is \nused in terms of overall performance and retry rate. The write-set-based version checks for con.icts \nusing the standard approach of breaking the concurrent histories into their constituent operations and \ntesting each of the resulting operation pairs for con.icts. The sequence-based version instead embodies \nthe detection algorithm of Section 5. We stress that the write-set\u00adbased algorithm is implemented as \na subset of its sequence-based counterpart, which cancels out differences due to implementation choices \n(e.g., code optimizations). The second experiment is concerned with the signi.cance of of.ine training, \nand measures the fraction of misses in sequence\u00adbased con.ict queries as an indication of how well JANUS \ngener\u00adalizes from training runs. For pair (s1,s2) of sequences, a miss occurs (during a production run) \nif there is no matching entry in the commutativity speci.cation built during of.ine training. In our \nmeasurements, we count only unique queries (and thus multiple hits/misses for the same query are counted \nas one). Our evaluation comprises two modes of detection: with and without sequence ab\u00adstraction (Section \n5.2), which provides insight into the value of this technique. In our experiments, we ran JANUS 5 times \nin training mode and 10 times in production mode. All the measurements prsented in this section are based \non averages over all production runs excluding the .rst (cold) run. Inputs for the training and production \nruns were based on available clients of the benchmarks (mainly unit tests). A characterization of the \ninputs appears in Table 6. For scalability, we authored abstraction speci.cations for shared data structures. \nTo identify these data structures, we used the Hawkeye tool [23]. We conducted our experiments using \nan IBM J9 V1.6.0 VM running on a Linux machine with an Intel Core i7 920 (Nehalem) processor that has \nfour 2.67GHz cores, each multiplexing 2 hard\u00adware threads, for a total of 8 threads.  7.2 Performance \nResults Figure 9 presents the speedup results for each of the benchmarks using 1 8 threads with both \nversions of JANUS. The ratio of overall retries to the number of transactions is shown in Figure 10. \nResults on cache misses in the 8-threads con.guration appear in Figure 11. Speedup and Retries The performance \nresults shown in Fig\u00adure 9 demonstrate the dramatic effect of sequence-based reason-ing: The sequence-based \nversion of JANUS achieves an average speedup of 1.5x on 8 threads, where JFileSync enjoys a speedup of \nclose to 2.5x. Conversely, the write-set-based version degrades performance with an average slowdown \nof 1.66x (or, equivalently, speedup of 0.6x) on 8 threads. These trends hold not only on aver\u00adage, but \nalso for each of the benchmarks individually. JGraphT-2 is the only benchmark where speedup under sequence\u00adbased \ndetection is negligible. Our analysis indicates that this is because the transactions in this benchmark \nmake intensive access to shared memory (comprising 6 data containers) all across their execution. Sequence-based \ndetection is highly effective for this benchmark, as Figure 10 clearly indicates, but the JANUS proto\u00adcol \nis geared toward transactions that make infrequent access to shared memory, such that the costs of privatization \nand fusion are amortized by the local work performed by the transaction. The speedup data is compatible \nwith the retry statistics pre\u00adsented in Figure 10. For all benchmarks, the number of retries under write-set-based \ndetection is prohibitive: For PMD and JGraphT-2, the number of retries is directly proportional to the \nnumber of tasks, regardless of the number of threads. For the remaining benchmarks, the number of retries \nincreases with the number of threads, the most extreme case being JGraphT-1 where the average number \nof retries per task is 4 with 8 threads. In contrast, sequence-based detection yields a very low retries\u00adto-transactions \nratio, the average being 0.07 compared to 1.51 with write-set-based detection, which is a 22x improvement. \nA partial explanation for this signi.cant gap between the detection algo\u00adrithms is the commutative patterns \nexhibited by the benchmarks, which are listed in Table 5. Misses The data on misses in Figure 11 indicates \nthat (i) the com\u00admutativity speci.cation due to the training runs generalizes well to production runs \nunder sequence abstraction, but (ii) without se\u00adquence abstraction, generalization deteriorates signi.cantly. \nMore concretely, the average miss rate is less than 17% with sequence abstraction, with no more than \n30% misses for all benchmarks (the worst being JGraphT-1), and 38% without applying abstraction to the \nsequences observed in training, where JGraphT-1 exhibits ap\u00adproximately 80% misses in this con.guration. \nSequence abstrac\u00adtion enables a 2.24x improvement in generalization from training runs, which points \nto the signi.cance of this heuristic. Sequence abstraction is most useful for the JGraphT bencham\u00adrks. \nFor the other benchmarks, generalization is already robust even without this heuristic (=24% misses), \nwhich leaves little room for improvment. Indeed, access patterns to shared memory are highly dynamic \nin the JGraphT algorithms (e.g., the calls to usedColors.set inside an inner loop of variable length \nin Figure 3), making se\u00ad quence abstraction signi.cant for these benchmarks. Discussion Manual or static \nidenti.cation of commutative pat\u00adterns in the benchmarks we ve considered can be challenging, if not \nimpossible: First, these benchmarks are all of real-world scale (well beyond 100K LOC). They comprise \nmultiple abstraction lay\u00ad  Name Version Description Prevalent Patterns JFileSync [2] JGraphT [3] PMD \n[4] Weka [7] 2.2 0.8.1 4.2 3.6.4 Utility for synchronizing pairs of directories (1) Greedy graph-coloring \nalgorithm (2) Saturation-degree node-ordering algorithm for heuristic graph coloring Java source code \nanalyzer Machine-learning library for data-mining tasks Identity Shared-as-local, spurious-reads Shared-as-local, \nequal-writes Shared-as-local, reduction Equal-writes Table 5. Benchmark characteristics Benchmark Input \nDescription Training Data Production Data JFileSync [2] JGraphT [3] PMD [4] Weka [7] List of directory \npairs Parameters for creation of random simple graph List of Java source .les Parameters for creation \nof random Bayesian network Random lists of length 5 100 nodes; average degree of 5 Random lists of length \n10 100 nodes; average degree of 10 Random lists of length 25 1000 nodes; average degree of 5 Random lists \nof length 100 1000 nodes; average degree of 10 Table 6. Inputs for training and production runs  ers, \nand the behaviors that induce commutative patterns typically span nontrivial interprocedural control \n.ows (cf. Figure 4). Moreover, in some cases the patterns do not hold invariantly, or hold only for some \nof the con.icted memory locations. This is the case, e.g., with JGraphT-2, where the ordering algorithm \nma\u00adnipulates several shared arrays whose access patterns are deter\u00admined dynamically, according to the \ninput graph. JANUS exploits the available parallelism, which cannot be identi.ed statically or captured \nconcisely via a pattern, but can be effectively leveraged based on data from training runs (only 16% \nmisses for JGraphT-2). Though the speedup for JGraphT-2 in speci.c is modest, we are encouraged by our \noverall performance results. While the speedups we ve obtained are not directly proportional to the number \nof threads, we have managed to considerably improve the perfor\u00admance of four out of the .ve real-world \nbenchmarks we experi\u00admented with (by 1.6x or more) via automatic parallelization. We also believe that \nthese results can be further improved: First, our prototype implementation of JANUS can bene.t from more \ncareful engineering. For example, our current implementa\u00adtion doesn t reclaim the logs of garbage transactions \nwhose concur\u00adrent transactions have also terminated, and implements privatiza\u00adtion transformations in \na na\u00a8ive fashion, instead of using full persis\u00adtency. Second, the hardware we used does not enable full \n8-thread parallelization: The 8 available hardware threads are supported by only 4 cores, each multiplexing \n2 threads. 8. Related Work There is an extensive body of research on synchronization in gen\u00aderal, and \noptimistic synchronization in speci.c, that dates back to Bernstein s work from the 1960s on the use \nof commutativity in concurrency control [8]. Due to space constraints, we restrict our discussion to \nclosely related research. Use of Abstraction Herlihy and Koskinen [14] develop transac\u00adtional boosting, \na methodology for transforming linearizable ob\u00adjects into transactional objects. Boosting leverages ADT \nsemantics for more accurate con.ict detection by avoiding spurious con.icts due to the ADT s concrete \nrealization. The Galois parallelization system [20], designed to exploit available parallelism in irregular \napplications, embodies a similar approach. In [18], Koskinen et al. de.ne a uni.ed framework for de\u00ad \nscribing both memory-level transactions and transactional boost\u00ading. Their framework allows con.ict detection \nboth at the concrete level, where simple statements are tested for commutativity, and at the abstract \nlevel, where the abstract effect of ADT operations is considered. Similarly, Tripp et al. [23] describe \na uni.ed frame\u00ad work for simultaneously tracking both concrete and abstract de\u00adpendencies in sequential \nexecutions of a program, where concrete dependencies involve simple statements and abstract dependencies \nare due to a semantic con.ict between ADT operations. The goal of this analysis is to uncover data.ow \nimpediments to parallelization, where abstraction is used to reduce the rate of false impediments.  \nJANUS also enables state abstraction being parameterized in the state representation: It can track con.icts \natop concrete program states, but the user can also apply abstraction speci.cations for shared data structures. \nA unique feature of JANUS, beyond state abstraction, is sequence-wide reasoning: JANUS is able to account \nfor complete histories of operations during con.ict detection. Sequence-based detection is expressible \nin the (general) gate\u00adkeeping [19] con.ict-detection paradigm, where the gatekeeper keeps track of all \nthe actions performed by a transaction to enable state rollback, as well as to evaluate commutativity \nconditions that refer to the history of the computation. JANUS can be seen as the .rst (automated) realization \nof the gatekeeping scheme, which is in general nontrivial to implement, as the user is required to write \nexplicitly concurrent (and highly optimized) synchronization code. Concrete STM Various techniques have \nbeen proposed for avoid\u00ading false aborts and/or reducing their cost in standard STM. We mention some \nof these techniques below. Dependence-aware transactional management [21] delays the decision whether \nto abort a con.icted transaction until any of the concurrent transactions in con.ict with it commits. \nThis is done by maintaining the dependency relation between transactions, such that if the con.icted \ntransaction depends only on transactions that have aborted, then it need not abort. Transaction checkpointing \n[17] reduces the cost of aborts by taking snapshots of intermediate (local and global) states during \nthe execution of a transaction. Similarly, an elastic transaction [11] is able to respond to a con.ict \nsituation by dropping its work thus far into a transaction that immediately commits and initiating a \nnew transaction (which might itself be elastic). Closed and abstract nested transactions [12, 13] mitigate \nthe cost of benign con.icts by restructuring large transactions, such that operations that are likely \nto suffer from such con.icts are moved into smaller, nested transactions. Nested transactions can then \nbe rerun without re-executing their enclosing transaction. The above techniques are all orthogonal to \nour approach. We expect that their incorporation into our synchronization algorithm should be seamless. \nWe leave such integrations for future research. User Annotations Udupa et al. [24] build a a parallelization \nsys\u00ad tem, which based on programmer annotations violates certain dependencies while preserving overall \nprogram functionality. This enables reordering of loop iterations, as well as tolerance of stale reads. \nThe system can also infer which annotations are likely to improve performance through use of a test-driven \nframework. The early-release [15] technique, developed by Herlihy et al., allows the programmer to specify \nobject-speci.c semantics that the transaction can then utilize to shrink its read set in certain computation \n.ows. An example of this is Figure 3. Similarly to these works, JANUS has facilities for expressing consistency \nrelaxations. The speci.cation can either be granular and refer to speci.c shared objects, or it can be \ncoarse and hold simultaneously for all the shared objects. JANUS also supports lim\u00adited inference capabilities \n(cf. Section 5.3). The inferred annota\u00ad tions are sound, and cannot result in incorrect parallelization. \nThis is in contrast to user-provided annotations, which both JANUS and the works above assume rather \nthan verify to be correct. 9. Conclusion We have presented JANUS, a speculative parallelization system \nthat performs accurate con.ict detection by testing entire sequences of operations rather than individual \noperations for con.icts. A key feature of JANUS is of.ine training: JANUS utilizes pro.ling data to build \na specialized, sequence-based commutativity speci.cation that can be queried ef.ciently at runtime. Our \nevaluation of JANUS on .ve real-world benchmarks indi\u00adcates that sequence-based detection is effective. \nJANUS achieves a speedup of up to 2.5x, where its instantiation with write-set-based detection fails \nto exploit the available parallelism, and even de\u00adgrades performance, with 22x more false con.icts. Acknowledgments \nWe thank Adam Morrison, Guy Gueta and the anonymous referees for their insightful feedback on earlier \nversions of the paper. References [1] The chord analysis framework. http://code.google.com/p/jchord/. \n[2] The j.lesync utility. http://jfilesync.sourceforge.net. [3] The jgrapht java graph library. http://www.jgrapht.org. \n[4] Pmd java source code scanner. http://pmd.sourceforge.net. [5] The sat4j sat solver. http://www.sat4j.org/. \n[6] The sourceforge code repository. http://sourceforge.net/. [7] The weka machine-learning library. \nhttp://weka.sourceforge.net. [8] A. Bernstein. Analysis of programs for parallel processing. IEEE Transactions \non Electronic Computers, pages 757 762, 1966. [9] S. Chiba and M. Nishizawa. An easy-to-use toolkit for \nef.cient java bytecode translators. In Proceedings of the 2nd international conference on Generative \nprogramming and component engineering, pages 364 376, 2003. [10] J. R. Driscoll, N. Sarnak, D. D. Sleator, \nand R. E. Tarjan. Making data structures persistent. J. Comput. Syst. Sci., 38:86 124, 1989. [11] P. \nFelber, V. Gramoli, and R. Guerraoui. Elastic transactions. In DISC, pages 93 107, 2009. [12] T. Harris, \nS. Marlow, S. Peyton-Jones, and M. Herlihy. Composable memory transactions. In Proceedings of the tenth \nACM SIGPLAN symposium on Principles and practice of parallel programming, pages 48 60, 2005. [13] T. \nHarris and S. Stipic. Abstract nested transactions. The 2nd ACM SIGPLAN Workshop on Transactional Computing, \n2007. [14] M. Herlihy and E. Koskinen. Transactional boosting: a methodology for highly-concurrent transactional \nobjects. In PPoPP. ACM, 2008. [15] M. Herlihy, V. Luchangco, P. A. Martin, and M. Moir. Nonblocking memory \nmanagement support for dynamic-sized data structures. ACM Trans. Comput. Syst., 23(2):146 196, 2005. \n[16] M. Herlihy and J.E.B. Moss. Transactional memory: Architectural support for lock-free data structures. \nIn ISCA, 1993. [17] E. Koskinen and M. Herlihy. Checkpoints and continuations instead of nested transactions. \nIn SPAA, pages 160 168, 2008. [18] E. Koskinen, M. J. Parkinson, and M. Herlihy. Coarse-grained trans\u00adactions. \nIn POPL, pages 19 30, 2010. [19] M. Kulkarni, D. Nguyen, D. Prountzos, X. Sui, and K. Pingali. Ex\u00adploiting \nthe commutativity lattice. In PLDI, 2011. [20] M. Kulkarni, K. Pingali, B. Walter, G. Ramanarayanan, \nK. Bala, and L. P. Chew. Optimistic parallelism requires abstractions. In PLDI, 2007. [21] H. E. Ramadan, \nI. Roy, M. Herlihy, and E. Witchel. Committing con.icting transactions in an stm. In PPOPP, pages 163 \n172, 2009. [22] O. Tripp, R. Manevich, J. Field, and M. Sagiv. JANUS: Exploiting parallelism via hindsight. \nTechnical report, Tel Aviv University, 2012. [23] O. Tripp, G. Yorsh, J. Field, and M. Sagiv. Hawkeye: \neffective dis\u00adcovery of data.ow impediments to parallelization. In OOPSLA, pages 207 224, 2011. [24] \nA. Udupa, K. Rajan, and W. Thies. Alter: exploiting breakable de\u00adpendences for parallelization. In Proceedings \nof the 32nd ACM SIG-PLAN conference on Programming language design and implementa\u00adtion, pages 480 491, \n2011. [25] J. Whaley. Joeq: A virtual machine and compiler infrastructure. Sci. Comput. Program., 57:339 \n356, 2005.    \n\t\t\t", "proc_id": "2254064", "abstract": "<p>This paper addresses the problem of reducing unnecessary conflicts in optimistic synchronization. Optimistic synchronization must ensure that any two concurrently executing transactions that commit are properly synchronized. Conflict detection is an approximate check for this condition. For efficiency, the traditional approach to conflict detection conservatively checks that the memory locations mutually accessed by two concurrent transactions are accessed only for reading.</p> <p>We present JANUS, a parallelization system that performs conflict detection by considering sequences of operations and their composite effect on the system's state. This is done efficiently, such that the runtime overhead due to conflict detection is on a par with that of write-conflict-based detection. In certain common scenarios, this mode of refinement dramatically improves the precision of conflict detection, thereby reducing the number of false conflicts.</p> <p>Our empirical evaluation of JANUS shows that this precision gain reduces the abort rate by an order of magnitude (22x on average), and achieves a speedup of up to 2.5x, on a suite of real-world benchmarks where no parallelism is exploited by the standard approach.</p>", "authors": [{"name": "Omer Tripp", "author_profile_id": "81435610768", "affiliation": "Tel Aviv University, Tel Aviv, Israel", "person_id": "P3471179", "email_address": "omertrip@post.tau.ac.il", "orcid_id": ""}, {"name": "Roman Manevich", "author_profile_id": "81100232411", "affiliation": "The University of Texas at Austin, Austin, TX, USA", "person_id": "P3471180", "email_address": "roman@ices.utexas.edu", "orcid_id": ""}, {"name": "John Field", "author_profile_id": "81100419562", "affiliation": "Google, New York, NY, USA", "person_id": "P3471181", "email_address": "jfield@google.com", "orcid_id": ""}, {"name": "Mooly Sagiv", "author_profile_id": "81460640494", "affiliation": "Tel Aviv University, Tel Aviv, Israel", "person_id": "P3471182", "email_address": "msagiv@post.tau.ac.il", "orcid_id": ""}], "doi_number": "10.1145/2254064.2254083", "year": "2012", "article_id": "2254083", "conference": "PLDI", "title": "JANUS: exploiting parallelism via hindsight", "url": "http://dl.acm.org/citation.cfm?id=2254083"}