{"article_publication_date": "06-11-2012", "fulltext": "\n Concurrent Data Representation Synthesis Peter Hawkins Alex Aiken * Kathleen Fisher Computer Science \nDepartment, Stanford Computer Science Department, Stanford Computer Science Department, Tufts University \nUniversity University hawkinsp@cs.stanford.edu aiken@cs.stanford.edu k.sher@eecs.tufts.edu Martin Rinard \nMIT Computer Science and Arti.cial Intelligence Laboratory rinard@csail.mit.edu Abstract We describe \nan approach for synthesizing data representations for concurrent programs. Our compiler takes as input \na program written using concurrent relations and synthesizes a representation of the relations as sets \nof cooperating data structures as well as the placement and acquisition of locks to synchronize concurrent \naccess to those data structures. The resulting code is correct by construction: individual relational \noperations are implemented correctly and the aggregate set of operations is serializable and deadlock \nfree. The relational speci.cation also permits a high-level optimizer to choose the best performing of \nmany possible legal data representations and locking strategies, which we demonstrate with an experiment \nautotuning a graph benchmark. Categories and Subject Descriptors D.3.3 [Programming Lan\u00adguages]: Language \nConstructs and Features Abstract data types, Concurrent programming structures, Data types and structures; \nE.2 [Data Storage Representations] Keywords Synthesis, Lock Placement 1. Introduction Consider the problem \nof implementing concurrent operations on a directed graph. We must decide how to represent the graph \nas a collection of data structures, perhaps using a lookup table mapping each node to the set of its \nadjacent nodes. We will need to pick concrete representations for both the lookup table (e.g., a concurrent \nhashmap) and the adjacency sets (e.g., linked lists). We must also decide how concurrency will be realized. \nWe could add our own * This work was supported by NSF Grant CCF-0702681 and Stanford s Army High Performance \nResearch Center. The views expressed are those of the author and do not re.ect the of.cial policy or \nposition of the Department of Defense or U.S. Government. Distri\u00adbution Statement A (Approved for Public \nRelease, Distribution Unlimited) Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. PLDI 12, June 11 16, 2012, Beijing, China. Copyright c &#38;#169; 2012 \nACM 978-1-4503-1205-9/12/06. . . $10.00 Mooly Sagiv Tel-Aviv University msagiv@post.tau.ac.il synchronization \nusing locks and/or we could use a concurrent container data structure to implement the lookup table, \nthe sets of adjacent nodes, or both. Assume for the moment that we decide both containers will be concurrent. \nWe must of course ensure there is enough synchroniza\u00adtion to avoid harmful races, but not so much that \nwe either limit scalability or introduce deadlocks. Using off-the-shelf concurrent containers can simplify \nthis task, but even using concurrent con\u00adtainers for both data structures does not automatically imply \nthat high-level graph operations that touch both structures (such as in\u00adserting or removing an edge from \nthe graph) are correct. In fact, recent work in bug detection for concurrent programs has shown that \nprogrammers frequently fail to use standard concurrent containers correctly, especially when they must \ncompose multiple concurrent operations [20]. On the other hand, it may be more ef.cient to have only \nthe top-level lookup table be concurrent and use non-concurrent data structures for the sets of adjacent \nnodes if it is very infrequent that threads try to access the same node simultaneously the extra overhead \nof a concurrent data structure for the adjacency sets won t be worthwhile. This design has different \ncorrectness requirements and would likely result in a different choice of where to place any needed synchronization \nto guarantee correctness. The right answer to the decision of whether to use a concurrent or non\u00adconcurrent \ndata structure for the adjacency sets likely depends on the typical workload and it will be dif.cult \nto modify the interlinked synchronization and data structures if we decide later that the graph should \nbe implemented differently. In this paper we present an approach to synthesizing concurrent data representations, \nmeaning that from a high-level speci.cation of data we produce both the concrete data structures and \nthe corre\u00adsponding synchronization to implement the speci.cation. In our ap\u00adproach, programs are written \nusing concurrent relations (Section 2), a generalization of standard concurrent collections to relations \nwith a concurrent interface to perform insertions, deletions, and lookups of tuples. Our compiler automatically \nsynthesizes all aspects of the data representation, including the choice of data structures and how they \ninteract, the number and placement of locks to guard access to those data structures (including, for \nexample, whether locking should be .ne-grain or coarse-grain), an order in which locks can be acquired \nto guarantee deadlock freedom, and all of the code to correctly manage the interplay of the data structures \nand synchro\u00adnization.  By specifying a program s access to data using concurrent relations and avoiding \na premature commitment to a particular representation, much of the low-level detail of programs is removed, \nmaking them easier to read and maintain, while simultaneously making it much easier to change the implementation \nif desired. Furthermore, concurrent relations give a high-level and pointer\u00adfree speci.cation of data, \nwhich is good for compilers, because the compiler is now free to choose the concrete representation of \nthe data without the usual requirement that it .rst perform a complex and usually brittle pointer analysis. \nPrograms written using relational data speci.cations are simpler, correct by construction, and can be \nautomatically optimized in ways that are out of reach of compilers for languages with traditional data \nstructure de.nitions. Beginning with Cohen and Campbell [5] researchers have inves\u00ad tigated how to compile \nprograms written using relations as the main (and sometimes only) aggregate form of data into low-level \ndata representations. Our method builds on two recent results (Section 4): we use the decompositions \nof Hawkins et al. [12] to describe how relations can be decomposed into a set of cooperating data structures, \nand we use the theory of lock placements [13] to describe the space of possible locking strategies. Our \nspeci.c contributions are: We introduce concurrent relations, a generalization of standard concurrent \ncontainer data structures to containers of tuples, with concurrent operations to insert, remove, and \nquery relations (Section 2).  The selection of data structures is subtler than in the non\u00adconcurrent \ncase, because there is the added dimension of using concurrent container structures, which may or may \nnot require additional synchronization depending on the relational speci.\u00adcation, and, in addition, different \nconcurrent containers provide varying guarantees about the safety of concurrent access. We give a taxonomy \nof containers and their properties relevant to concurrent data representation synthesis (Section 3). \n We extend the relational decomposition language [12] to support concurrent relations. Just as the original \ndecomposition language describes how to assemble a representation of a relation from a library of container \ndata structures, concurrent decompositions describe how to compose concurrent and non-concurrent data \nstructures together with locks to implement a concurrent relation primitive (Section 4.1).  We show \nhow to integrate lock placements [13], which describe a space of possible locking strategies on data \nstructures, with the problem of selecting the data structures themselves. The choice of data structures \nand lock placements is done in such a way that the resulting code is guaranteed to ensure the serializability \nof relational operations (Section 4.2).  We adapt and generalize the problem of selecting a good im\u00adplementation \nof the relational primitives, called query planning, to concurrent relations (Section 5). One of the \nmajor issues is ensuring deadlock freedom, which we accomplish by selecting a global lock ordering that \nall relational operations obey by con\u00adstruction. Deadlock is not addressed in the previous work on lock \nplacements [13].  The optimal decomposition depends on the usage patterns of the data structure and \nthe target machine. We present results from a full implementation, which includes an autotuner that allows \nus to discover a good combination of both locks and container data structures automatically for a training \nworkload. We perform an evaluation of a concurrent graph benchmark, showing that the best data representation \nvaries with the workload, and thus it is important to have the .exibility to easily alter the representation \nof concurrent data (Section 6).  2. Concurrent Relations We advocate a view in which programmers write \nprograms that oper\u00adate on relations and a compiler selects the concrete representation of the relations. \nA relational speci.cation is a set of column names C together with a set of functional dependencies .. \nFunctional depen\u00addencies (FDs) specify which columns are uniquely determined by other columns. For example, \nwe can represent the edges of a directed graph as a relation with three columns: src (source node), dst \n(des\u00adtination node), and weight, together with a functional dependency src, dst . weight, which speci.es \nthat every edge of the graph has a unique weight. The relational speci.cation is a contract between the \nclient of our compiler and the generated code: If the client obeys the functional dependencies, then \nthe compiler guarantees that the generated code preserves the semantics of the relational operations. \nValues, Tuples, Relations We assume a set of untyped values v drawn from a universe V that includes the \nintegers (Z . V). A tuple t = (c1: v1,c2: v2,... ) maps a set of columns {c1,c2,... }to values drawn \nfrom V. We write dom t for the columns of t.A tuple t is a valuation for a set of columns C if dom t \n= C.A relation r is a set of tuples {t1,t2,... } over identical columns C. We write t(c) for the value \nof column c in tuple t. We write t . s if the tuple t extends tuple s, that is t(c)= s(c) for all c in \ndom s. We say tuple t matches tuple s, written t ~ s, if the tuples are equal on all common columns. \nTuple t matches a relation r, written t ~ r, if t matches every tuple in r. A tuple t is a key for a \nrelation r if the columns dom t functionally determine all columns of r. We use the standard notation \nof relational algebra. Union (.), intersection (n), set difference (\\) have their usual meanings. The \noperator pC r projects relation r onto a set of columns C. A relation r has a functional dependency (FD) \nC1 . C2 if any pair of tuples in r that are equal on columns C1 are also equal on columns C2. Relational \nOperations We provide four atomic operations for creating and manipulating concurrent relations. In the \nfollowing speci.cation we represent relations as ML-style references to a set of tuples: ref x creates \na new reference to x, !x retrieves the current value of x, and x . y sets the current value of x to y. \nempty () = ref \u00d8 remove rs = r . !r \\{t . !r | t . s} query rsC = pC {t . !r | t . s} insert rst = if \n.u. u . !r . s . u then r . !r .{s . t} Informally, empty () creates a new empty relation. Operation \nremove rs removes tuples matching s; in practice, our implementa\u00adtion requires that s is a key for the \nrelation. Operation query rsC returns columns C of all tuples in r matching tuple s. The most interesting \noperation is insert rst, which inserts a new tuple x, where x is the union of the columns of tuples s \nand t, into a relation r, provided there is no existing tuple in r matching s. We require that s and \nt have disjoint domains. Insert generalizes the put-if-absent operation provided by standard concurrent \nkey-value maps: put-if-absent(k,v) inserts value v into the map if no other value is already associated \nwith key k, and would be written insert r (key: k)(value: v) Insert operations may violate functional \ndependencies, and it is the client s obligation to ensure functional dependencies are observed. The form \nof the insert operation allows clients to test whether functional dependencies will be satis.ed by a \nnew tuple even in the presence of concurrent updates. The relational compiler ensures that the implementations \nof the relational operations are linearizable [15] (equivalently serializable, since the relational operations \nare single operation transactions on a single object). Operations on an object are linearizable if every \n Concurrency-safety Data Structure L/L L/W S/W W/W L/S S/S HashMap yesno no no TreeMap yesno no no ConcurrentHashMap \nyes yes weak yes ConcurrentSkipListMap yes yes weak yes CopyOnWriteArrayList yes yes yes yes Figure \n1. Concurrency safety properties of selected containers from the JDK. Possible operations are lookup \n(L), scan (S), or write (W). For an operation pair a/\u00df, concurrently executing operations a and \u00df on \na container is either unsafe ( no ), safe but only weakly consistent ( weak ), or both safe and linearizable \n( yes ). operation appears to take place atomically at a single point in time in between its invocation \nand response. Continuing with our graph example, we create a new, empty graph relation r0 using the empty \n() operation. Inserting an edge insert r0 (src: 1, dst: 2)(weight: 42) results in a new relation r1 = \n{(src: 1, dst: 2, weight: 42)}.A subsequent insertion insert r1 (src: 1, dst: 2)(weight: 101) leaves \nthe relation unchanged, because relation r1 already contains an edge with the same src and dst .elds. \nWe can retrieve the dst and weight .elds corresponding to the successors of node 1 in a relation r using \nthe operation query r (src: 1){dst, weight}, and .nally we can delete edges with a dst of 2 using the \noperation remove r (dst: 2). 3. A Taxonomy of Concurrent Containers Decompositions describe how to implement \nconcurrent relations as a combination of both concurrent and non-concurrent data structures. Before diving \ninto the details of the decomposition language (Sec\u00adtion 4), we .rst describe the concurrency properties \nof the container data structures found in the wild, which form the building blocks of concurrent decompositions. \nContainer Interface A container is a data structure that imple\u00adments an associative key-value map interface \nconsisting of read op\u00aderations lookup(k) and scan(f), and a write operation write(k, v). The lookup \noperation lookup(k) returns the value associated with a key k, if any.  The scan operation scan(f ) \niterates over the map, and invokes the function f(k, v) once for each key k and its associated value \nv in the map. A scan may or may not return the entries of the map in sorted order.  The write operation \nwrite(k, v) sets the value associated with a key k to v. Here v is an optional value, in the style of \nML. If v is Some w, then the operation updates the value associated with key k to w, whereas if v is \nNone, representing the absence of a value, then any existing value associated with k is removed. The \nwrite operation subsumes operations to insert, update, and remove entries from a map.  3.1 Concurrency \nSafety and Consistency We next discuss two related properties of containers, concurrency safety, which \ndescribes whether it is safe for two operations to occur in parallel, and consistency, which characterizes \nwhat a container guarantees about the possible orders of events in a concurrent execution. Figure 1 lists \nthe concurrency safety and consistency properties of a selection of Java containers from the JDK; as \nwe show, containers differ greatly in their support for concurrency. Concurrency Safety For a given data \nstructure, we say a pair of operations a/\u00df is concurrency-safe if two threads may safely exe\u00adcute operations \na and \u00df in parallel with no external synchroniza\u00adtion. A container is concurrency-safe if all pairs of \noperations are concurrency-safe. Concurrency safety is strictly a statement about the correct usage of \nthe interface of a data structure; it is irrelevant how the data structure guarantees safety internally, \nwhether by locks, atomic instructions, or by some other means. Consider the data structures described \nin Figure 1. Almost all data structures support parallel read operations; for example concurrent threads \nmay safely read or iterate over a Java HashMap in parallel without synchronization. Exceptions exist; \nfor example, it would not be safe for threads to perform concurrent reads of a splay tree because splay \ntree read operations rebalance the tree. Only a few containers permit write operations in parallel with \nother operations. For example, it is unsafe to read from or write to a HashMap object while another thread \nis writing to the same HashMap. By contrast a ConcurrentHashMap or a CopyOnWriteArrayList allow concurrent \nlookup and write oper\u00adations, or pairs of concurrent write operations. On a concurrency\u00adsafe container, \nsuch as a ConcurrentHashMap, the lookup and write operations are linearizable even in the absence of \nany exter\u00adnal concurrency control. For concurrency-unsafe operations, such as reading a splay tree, linearizability \nis the responsibility of an external concurrency control primitive, such as a lock. The scan operation, \nhowever, behaves differently. Even many containers that allow iteration in parallel with mutation do \nnot guar\u00adantee that iteration is linearizable. We identify two different pos\u00adsibilities. Some containers, \nsuch as ConcurrentHashMap provide weakly consistent concurrent iteration; that is, concurrent iteration \nis safe, but may or may not re.ect inserts and removals that oc\u00adcur in parallel with the iteration. Iteration \nover a weakly-consistent container may not be linearizable, that is, the result of the iteration may \nnot correspond to the set of entries present in the container at any instant in time. Conversely for \ncontainers that provide snapshot iteration, such as a CopyOnWriteArrayList, iteration behaves as if it \noperated over a linearizable snapshot of the container. 4. Concurrent Decompositions The concurrent decomposition \nlanguage describes how to assemble container data structures into representations of relations that support \nconcurrent serializable transactions that implement the various relational operations. By combining concurrent \nand non-concurrent data structures with locks we can build a representation of a relation with strong \nconcurrency guarantees, even if the constituent data structures themselves have limited support for concurrency. \nWe extend the relational decomposition language [12] to support concurrent operations from multiple threads. \nTwo key ideas underlie safe and scalable concurrent decompositions: leveraging existing concurrent containers \n(Section 4.1) to the full extent possible, and supplementing containers with locks as necessary to ensure \nthe safety and serializability of concurrent transactions over the complete decomposition (Section 4.2). \nAs a focus of this paper is extending decompositions to support concurrency, we .rst review the de.nition \nof decompositions. 4.1 Decompositions A decomposition describes how to represent a relation as a combi\u00adnation \nof primitive container data structures. A decomposition is a static description of the heap, similar \nto a type. We use a graphical notation for decompositions isomorphic to the let-binding notation in the \nliterature [12]. Figure 2(a) shows a decomposition of a .lesys\u00ad tem directory tree relation, based on \nthe directory entry cache in the Linux kernel. The relation has three columns parent, name, and child \nand obeys a functional dependency parent, name . child. Each parent directory entry has zero or more \nchild directory entries, each with a distinct .le name .  Formally, a decomposition d is a rooted, directed \nacyclic graph, consisting of a set of vertices V = {u, v, . . . } and a set of edges drawn from V \u00d7 V \n. A decomposition has a unique source vertex . with no incoming edges. All vertices must be reachable \nfrom the source vertex. Each node v of a decomposition has an associated type At B, written v: At B. \nIntuitively, A is the set of columns whose representation is speci.ed by paths from the root node to \nv, and B is the residual set of columns represented by the subgraph reachable from v. Each edge uv of \nthe decomposition has an associated set of columns cols(uv), and a choice of container ds(uv) the compiler \nshould use to implement the edge. In Figure 2(a), the edge .x from the root indicates that the relation \nis implemented by a TreeMap from each parent value to the residual relation of all (name, child) pairs \nfor that parent. Recursively, this subrelation is implemented by another TreeMap from name to the child \ndirectory (edge xy). Finally, the functional dependency guarantees that the child directory is a singleton \ntuple and is implemented by its single value (edge yz). This structure (a map from parents to the set \nof child directory names) enables ef.cient iteration over the children of a directory, which is useful \nwhen, for example, unmounting a .lesystem. To enable ef.cient directory lookup the decomposition also \nincludes a global hashtable mapping (parent, name) pairs to child objects (edge .y). We assume that decompositions \nare adequate [12], that is de\u00ad compositions are capable of representing all relations satisfying the \nrelational speci.cation. The adequacy conditions imply that for ev\u00adery edge uv where u:AtB and v:CtD \nwe have C . A.cols(uv). Decomposition Instances The run-time (dynamic) counterpart of a decomposition \nd is a decomposition instance d, which represents a particular concrete relation. Each node v:AtB in \na decomposition d has a set of instances {vt} where dom t = A, each corresponding to an object in memory. \nEach edge uv of a decomposition has a set of edge instances {uvt}, where if u:AtB then dom t = A.cols(uv). \nIf u: At B and dom t . A, we write ut to denote the node instance upA t; similarly if dom t . A . cols(uv) \nwe write uvt to denote uvpA.cols(uv) t. A formal characterization of well-formed decomposition instances \nand an abstraction function that maps well\u00adformed decomposition instances back to the relations they \nrepresent can be found in the literature [12]. Figure 2(b) depicts an instance of the decomposition of \nFig\u00ad ure 2(a) representing the relation containing 3 directory entries: {(parent: 1, name: a , child: \n2) , (parent: 2, name: b , child: 3) , (parent: 2, name: c , child: 4)}.  4.2 Logical Locks, Transactions, \nand Serializability Locks Given a decomposition d, we compile each relational op\u00aderation into a transaction \ntailored to d. For safety and consistency transactions must acquire locks that protect the invariants \nupon which a transaction relies. By lock we mean a class of pessimistic synchronization primitives that \nmay be held by a transaction in ei\u00adther of two different modes, namely shared or exclusive. A lock may, \nbut need not, permit multiple transactions to hold shared access simultaneously; however if a transaction \nholds exclusive access to a lock the lock must not allow any other transaction to hold either shared \nor exclusive access. (a) (b) 1, a {parent } . 2 1 {parent, name} . 2, c {name} x a 2, b c b y {child} \n 4 3 2  Figure 2. (a): A decomposition representing a directory tree rela\u00adtion with three columns {parent, \nname, child} and (b): an instance of decomposition (a). Each edge of the decomposition is labeled with \na set of columns {\u00b7 \u00b7 \u00b7 }, together with the label of the node whose lock protects instances of that \nedge (Section 4.3). Each edge of the instance is labeled with a valuation for the corresponding decomposition \nedge s columns. Solid edges indicate a TreeMap, dashed edges represent a ConcurrentHashMap, and dotted \nedges represent singleton tuples. Logical Locks To ensure that transactions are serializable, the data \nin decomposition instances are protected by logical locks. We asso\u00adciate a distinct logical lock with \nevery edge uvt of a decomposition instance. Logical locks protect the state, either presence or absence, \nof an edge instance. If a transaction observes the presence or ab\u00adsence of an edge it must hold shared \naccess to the corresponding logical lock, and if a transaction adds or removes an edge it must hold exclusive \naccess to the corresponding logical lock. Logical locks are de.ned for every possible edge instance, \nirrespective of whether the edge is actually present in a particular decomposition instance or not. For \nnow, we leave the implementation of each logical lock uvt abstract. In Section 4.3, we implement logical \nlocks using a smaller set of physical locks attached to the nodes of a decomposition in\u00adstance. By placing \nrestrictions on the possible mappings from logical locks to physical locks we can ensure that containers \n(concurrent or not) and compositions of containers are used safely. Two-Phase Locking Protocol Each transaction \nconsists of a se\u00adquence of locks, reads, writes, and unlocks of the edges of a decom\u00adposition instance. \nThe purpose of introducing logical locks is that by having a distinct lock for every edge (including \nedges that are absent) we are able to state a very simple and obviously correct protocol for transactions \non decomposition instances. To ensure consistency of transactions, we use a standard two-phase locking \nprotocol on logical locks: Transaction operations must be logically well-locked; that is, if a transaction \nobserves the state (either present or absent) of an edge instance uvt via a lookup or scan operation \nthen it must hold shared access to the logical lock of uvt, and if a transaction adds, removes, or updates \nan instance edge uvt via a write operation then it must hold exclusive access to the logical lock of \nuvt.  Transactions must be logically two-phase, that is, transactions must be divided into a growing \nphase during which logical locks are acquired and a shrinking phase during which logical locks are released. \nAll lock acquisitions must precede all lock releases.  It is a classic result that well-locked and two-phase \ntransactions are serializable [10].  4.3 Physical Locks and Lock Placements By associating a unique \nlogical lock with every edge instance we can use a simple two-phase locking protocol to ensure that transactions \nare serializable. Such an approach would be impractical, however.  (a) (b) (c) src . dst . src x dst \ny src . dst . dst u src v dst x src y weight . weight w weight y weight z  Figure 3. Three concurrent \ndecompositions for a directed graph re\u00adlation: (a) a stick , with a single coarse lock around non-concurrent \ndata structures, (b) a split decomposition, with locks at different granularities, and (c) a diamond \n, with a mixture of speculatively\u00adlocked concurrent data structures and non-concurrent data structures. \nSolid edges indicate TreeMap containers, dashed edges represent ConcurrentHashMap containers, and dotted \nedges represent sin\u00adgleton tuples. Each edge is labeled with a set of columns on the left and the associated \nlock placement on the right. Each edge instance corresponds to a entry in a container in the heap, and \nit would often be too slow to actually use locks at such a .ne granularity, not to mention the practical \nproblem that there are in.nitely many logical locks de.ned for container entries that are absent. Further, \nas shown in Figure 1, in practice different containers have different levels of support for safe concurrency, \nand so while we must use locks to protect some containers from all concurrent accesses, in other cases \nwe can rely on the container to mediate concurrent access. Finally, since we treat container implementations \nas black boxes, we have no way to attach locks to the edge instances directly. Instead of literally maintaining \none lock for every possible edge instance, we implement logical locks using a smaller set of physical \nlocks attached to instances of nodes in a decomposition. We describe the correspondence between logical \nand physical locks using a lock placement [13], which is a mapping from the set of logical locks onto \nthe set of physical locks. Many logical locks may map onto the same physical lock, and acquiring a physical \nlock corresponds to taking all of the corresponding logical locks. By choosing different lock placements \nwe can describe different granularities of locking. Since physical locks are attached to node instances, \nin general there may be an unbounded number of physical locks. Physical Locks To each node v in a decomposition \nwe attach a set of physical locks {v 0 ,v 1 ,... }. If there is only a single physical lock attached \nto a node we simply write v for both the node and its unique physical lock. Lock Placements A lock placement \n. is a function mapping the logical lock associated with each edge onto a physical lock on a node that \nimplements it. We de.ne lock placements on the (static) decomposition which we extend to (dynamic) decomposition \ninstances in the obvious way; if a lock placement . maps the logical lock on edge e to the physical lock \non node v, then at runtime the compiler maps the logical lock on edge instance et onto the physical lock \non node instance vt. Recall the directed graph example from Section 2. The relation in question has three \ncolumns {src, dst, weight} related via the functional dependency src, dst . weight. Figure 3 shows three \npossible decompositions, each with a different choice of data struc\u00adtures and lock placements. Figure \n3(a) uses a coarse-grain lock placement .1(e)= . for all edges e, which protects all edges of the decomposition \nusing a single lock at the root .. Since there is only one instance of the root node . in any decomposition \ninstance, the same lock is used to protect everything. Further, since the logical locks of all edge instances \nare mapped to the same physical lock ., the lock serializes access to the entire decomposition data structure, \nensuring that each (non-concurrent) TreeMap is only accessed by one transaction at a time. Figure 3(b) \ndepicts a .ne-grain locking strategy decomposition in which each edge is protected by a lock at its head \n(i.e., objects in a container are protected by a single lock on the container itself), using the lock \nplacement .2(a\u00df)= a for all edges a\u00df. Edges .u and .v are protected by a lock at ., whereas edges uw, \nvy, wx and yz are protected by locks at u, v, w and y, respectively. Both of the example lock placements \ndescribed so far use a single lock to protect all the entries in each container. Figure 3(c) makes use \nof speculative locking (Section 4.5), one of two extensions which allow different dynamic instances of \nan edge in the same container to be protected by different locks. We defer further discussion of this \nexample to Section 4.5. Well-Formed Lock Placements We require that all lock place\u00adments satisfy the \nfollowing conditions: The lock placement .(uv) of each edge uv either must dominate the edge s source \nvertex u or be equal to v. (The latter case occurs in the speculatively-placed locks of Section 4.5.) \nBy de.nition this condition ensures that the lock placement for an edge lies on every path from the root \nof the decomposition including that edge. This condition ensures the instance of the node .(uv) named \nby the lock placement is unique for each edge instance uvt. The domination requirement also simpli.es \nquery planning (Section 5), since it ensures that a query plan will always encounter the necessary locks \nfor each edge no matter how the edge is reached.  All edges between an edge and its lock placement share \nthe same placement. That is, .x any edge uv and take any edge xy in a path in the decomposition from \n.(uv) to u. Then we have .(xy)= .(uv). This requirement ensures that if a lock protects an edge, then \nthe lock also protects the path from the lock to that edge, thereby ensuring that if we hold a lock then \nthe set of edges protected by that lock cannot change.  Logical Lock Implication Since we implement \nlogical locks by mapping them onto a smaller set of physical locks, a transaction cannot acquire logical \nlocks directly. Instead, a transaction must acquire physical locks that imply access to the logical locks \nthat the transaction requires. We say that a set of physical locks P held by a transaction imply exclusive \nor shared access, respectively, to the logical lock of edge instance uvt under lock placement . if: \nthe transaction holds exclusive or shared access, respectively, to the corresponding physical lock, that \nis, .(uv)t . P , and  the mapping between the logical lock to the corresponding physical lock is stable, \nthat is, there exists a path wt from the root of the decomposition instance to ut such that the transaction \nholds shared access to every edge in wt.  The stability criterion means that a physical lock only covers \na logical lock if the transaction also holds locks that guarantee that the logical lock does in fact \ncorrespond to that physical lock; if not, a concurrent transaction might alter the heap and change the \nassociation between logical and physical locks. For example, consider a concurrent hashtable where the \nelements of each hash bucket are guarded by a lock. If a transaction moves an element v from bucket b1 \nto bucket b2 the lock guarding access to v changes, and any transaction that was concurrently accessing \nv by acquiring the lock on b1 no longer holds the correct lock for v for the lock placement. Thus, in \nthe presence of updates that can change the structure of the heap, it is not suf.cient to just hold the \nlocks L guarding access to the particular data, but it is also necessary to hold locks on whatever portion \nof the heap structure guarantees that L remains the correct set of locks to hold!  Since lock placements \nare de.ned using a decomposition struc\u00adture, for locking using a placement to be well-de.ned we must \nensure that transactions always yield heap states that are valid instances of the corresponding decomposition. \nOne of the bene.ts of data representation synthesis is that we are guaranteed that the operations emitted \nby the compiler preserve the decomposition structure by construction.  4.4 Lock Striping Lock striping \nis a technique for boosting the throughput of a transac\u00adtion by using a set of locks instead of a single \nlock. Consider again the decomposition of Figure 3(b), in which the lock placement maps the logical lock \non each edge to a physical lock at the source of the edge. While this lock placement ensures safe and \nconsistent transac\u00adtions, by protecting each container with a single lock we serialize access to containers, \nand hence we cannot make effective use of concurrent containers such as ConcurrentHashMap. To leverage \nconcurrent containers we can partition the elements of the container into a number of stripes, each with \nits own lock. For example, in Figure 3(b), rather than mapping all instances of edges .u and .v to a \nsingle physical lock at node ., we can use k physical locks .0,....k-1. We then use a lock placement \nthat stripes the logical locks attached to instances of .u and .v across the k physical locks: of the \nedge, instead we map the logical locks for absent edges onto physical locks at the edge s source. Up \nto this point, we have required that the lock guarding an edge e appear on all paths from the root before \ne is reached. For speculative locks, this invariant does not hold we do not know what lock to acquire \nuntil we have reached the object we wish to protect. The key is that it is safe to perform unlocked reads \nof a concurrency-safe container to guess the identity of the lock that we should acquire. Since the container \nis concurrency-safe, reading without holding a lock is safe, however we have no guarantees that any information \nthat we read will remain stable. Once we have guessed and acquired a lock, we can check to see if our \nguess was correct. There are two possibilities either we guessed correctly, in which case we already \nheld the lock that protects the edge and our read was stable, or we guessed incorrectly, in which case \nthe edge must point somewhere else. In the latter case we can release the lock we guessed and try again. \nWhile speculatively acquiring a lock is not physically two-phase, a transaction can be viewed as acquiring \nlogical locks in a two phase manner [13]. Speculative lock acquisition differs from the well-known but \nbroken double-checked locking idiom in two key ways .rstly, we always acquire a lock and recheck reads \nunder that lock, and secondly we require that concurrent containers are linearizable, that is, with semantics \nanalogous to a Java volatile .eld. For example, the decomposition depicted in Figure 3(c) uses a mixture \nof both speculative and non-speculative locking in particular, the locks that protect edges .x and .y \nare placed at the target of each edge on nodes x and y respectively. To take a lock on an edge instance \n.xt a transaction must .rst speculatively lookup entry t in the map without locking, acquire the lock \non . or xt if the edge instance is absent or present, respectively, and then verify that .3(e, t)= . \n.. .. the correct lock was taken. The data structure implementing edge .i if e = .u, i = t(src) mod k \n.x is a ConcurrentHashMap, which is concurrency-safe, so it is .i if e = .v, i = t(dst) mod k (1) safe \nto speculatively read an edge without holding its lock. at otherwise, where e = a\u00df . . . . .. . . ut \nif e = .u, et is present .i if e = .u, et is not present,i = t(src) mod k vt if e = .v, et is present \n.i if e = .v, et is not present,i = t(dst) mod k at otherwise, where e = a\u00df The lock placement takes \nas input an edge e annotated with a tuple t; the .elds of tuple t are used to select one of the k physical \nlocks at .. If we do not know the relevant tuple .elds in advance, for example if .4(e, t)= we want to \niterate over the container, we can always conservatively take all k locks. Lock striping is only applicable \nfor containers that are concur\u00adrency-safe. For a concurrency-unsafe container, such as a TreeMap, 5. \nQuery Planning and Lock Ordering we are limited to at most one lock for the entire container to ensure \nthat no two threads access the container concurrently. By increasing the value k we can reduce lock contention \nto arbitrarily low levels, at the cost of making operations such as iteration that access the entire \ncontainer more expensive.  4.5 Speculative Lock Placements When striping logical locks across physical \nlocks, as the number of physical locks k increases in the limit each container entry has its own individual \nlock. Rather than preallocating locks for an unbounded number of objects, we can achieve this limiting \ncase more ef.ciently by using a technique called speculative locking [13], motivated by transactional \npredication [3]. Speculative locking lazily constructs a unique physical lock for each logical lock. \nThe key to speculative locking is the identity of the lock that protects an edge instance depends on \nthe state of the edge instance itself. We map the logical lock to a distinct physical lock for each edge \ninstance present in a container by placing the lock in the node that is the target of the edge instance. \nFor serializability the lock placement must also be de.ned for edge instances that are absent from the \ndecomposition, not just those edges that are present. Since we cannot place locks for non-existent edge \ninstances at the target In Section 4 we introduced concurrent decompositions, which describe a relational \nspeci.cation using both concurrent and non\u00adconcurrent containers in combination with locks. In this section \nwe show how to compile the relational operations of Section 2 into code tailored to a particular concurrent \ndecomposition. Existing work [12] described how to compile relational opera\u00adtions in a non-concurrent \ncontext. There are two additional compli\u00adcations we must deal with when generating concurrent implemen\u00adtations \nof relational operations we must ensure that a transaction takes the locks that protect the decomposition \nedges it touches, and we must ensure that transactions are deadlock-free. 5.1 Deadlock-Freedom and Lock \nOrdering A common strategy for ensuring that a set of concurrent transactions is deadlock-free is to \nimpose a total order on locks. If all transactions acquire locks in ascending lock order, then we are \nguaranteed that concurrent transactions are deadlock-free. We ensure deadlock-freedom for concurrent \ndecomposition op\u00aderations by imposing a total lock order on the physical locks of a decomposition; it \nis the responsibility of the query planner to generate code that respects this order.  q ::= x | let \nx = q1 in q2 | lock(q, v) | expressions unlock(q, v) | scan(q, uv) | lookup(q, uv) Figure 4. Concurrent \nquery language. We only show the fragment necessary for implementing query operations. All query plans \nmust obey a single static order on all possible physical locks of a decomposition. The precise set of \nphysical locks in existence may change as we allocate and deallocate node instances, but the relative \norder of any given pair of physical locks never changes during runtime. We order physical locks .rstly \non a topological sort of the decomposition nodes to which they belong. We order different instances of \nthe same node lexicographically on the values of the key columns. Finally, we order the physical locks \nattached to each node instance by number. For example, consider the decomposition of Figure 3(c). We \n.x a topological order of the nodes, say .<x<y<z<w; meaning that all locks attached to . are ordered \nbefore all locks attached to instances of x, and so on. We lift the topological order on nodes to a total \norder on node instances .<xs0 <xs1 < \u00b7\u00b7\u00b7 <yt0 <yt1 < \u00b7\u00b7\u00b7 , where the tuple sequences (si) and (ti) are \nin lexicographic order. Finally, since there may be more than one physical lock per node due to lock \nstriping, we lift the total order on node instances to a total order on physical locks: 0101 01 .<.< \n\u00b7\u00b7\u00b7 <x <x < \u00b7\u00b7\u00b7 <x <x \u00b7\u00b7\u00b7 . s0 s0 s1 s1 As an aside, it is necessary that we totally order the physical \nlocks of a decomposition, not the logical locks; a query only acquires physical locks directly and it \nis the order of those physical locks that is pertinent to deadlock.  5.2 Query Language Once we have \n.xed a total order on the physical locks of a decom\u00adposition, the query planner must generate well-locked, \ntwo-phase code that respects the lock order for each possible query. A key requirement of a query plan \nis that it must make explicit which locks to acquire and in which order. The query trees in the literature \n[12] are not suitable for reasoning about locks since they have no notion of sequencing of expressions. \nIn the concurrent setting, we extend query trees to a fragment of a strict, impure functional language, \nshown in Figure 4. The let\u00ad binding construct of the concurrent query language can describe the order \nof execution of operations with side-effects, in particular lock and unlock operations. A query expression \nq is one of: a variable ref\u00aderence x, let-binding let x = q1 in q2, a lock acquisition lock(q, v), a \nlock release unlock(q, v), an edge lookup lookup(q, v), or an edge iteration scan(q, v). We discuss the \nsemantics of expressions shortly. Query States Evaluating any expression in the query language yields \na set of query states.A query state is a pair (t, m) of a tuple t containing a subset of the relation \ns columns, together with a mapping m from decomposition nodes v to the corresponding node instance vt. \nIf a vertex v with type v: AtB appears in the domain of m, tuple t must contain suf.cient columns such \nthat vt is uniquely de.ned, that is, A . dom t. Query Expressions We now describe the semantics of each \nquery expression. Variable lookup and variable binding are standard. Let bindings also allow us to sequence \noperations with side effects, such as locks; we use a don t-care variable let _ = q1 in q2 to denote \nexecuting q1 just for its side effects, discarding its return value, and then executing q2. A lock acquisition \nlock(q, v) acquires the physical locks associ\u00adated with the instance of node v in each query state in \nset q. Like all expressions in the query language, lock acts on a set of query states q, locking the \ninstance of physical lock v in each element of the set. The lock operation must acquire locks in accordance \nwith the lock order. While the query planner always produces the query plans with lock expressions in \ncorrect node order, the lock operator must sort node instances into the correct lexicographic order before \nacquiring locks. The counterpart unlock(q, v) unlocks the instances of node v in the set q; unlike the \nlock operation the unlock operation does not need to enforce sorted order on its arguments. Recall that \nfor each node instance ut an edge uv in a decomposi\u00adtion corresponds to a container data structure that \nmaps a set of key columns cols(uv) to a set of node instances of vte . The operation scan(q, uv) iterates \nover the contents of the container, returning the natural join of the input query states q together with \nthe entries of the map. If the query states in q contain a superset of the key columns cols(uv), we can \ninstead use the more ef.cient operation lookup(q, uv), which looks up the particular entry vt in the \ncon\u00adtainer. Both the lookup and scan operations require that the input query states contain an instance \nof the source vertex u. For example, suppose we wanted to iterate over all of the tuples of a directory \nentry relation represented using the decomposition of Figure 2(a) under a coarse lock placement which \nplaces all locks at the root node (.(e)= . for all e). One possible query plan is: 1: let _ = lock(a, \n.) in 2: let b = scan(scan(a, .y), yz) in (2) 3: let _ = unlock(a, .) in 4: b The query plan takes as \ninput a variable a, consisting of a singleton query state containing the location of the decomposition \nroot .. The query plan .rst locks the unique instance of the root vertex . in set a (line 1), and then \niterates over instances of the edge .y from the root vertex in set a (line 2, scan(a, .y)); the iteration \nyields a set of query states that contain instances of node y together with valuations of the parent \nand name .elds. For each such query state, we then iterate over the singleton instances of edge yz (line \n2, scan(\u00b7\u00b7\u00b7, yz)), yielding a valuation for the child .eld; we store the resulting set of query states \nas a set b. We release the acquired locks (line 3), and return our .nal query states b (line 4). To make \nthe execution concrete, suppose we execute the query plan (2) on the decomposition instance of Figure \n2(b). The query plan receives the input query state a = {(() , {. . .0})} as input, which contains the \nlocation of the decomposition root but no valuations for relation columns. The lock statement acquires \nthe lock attached to .0, which is the unique instance of . in set a. Evaluation of the expression scan(a, \n.y) in line 2 yields states ((parent: 1, name: a ), {. . .0,y . y3}) ((parent: 2, name: b ), {. . .0,y \n. y2}) ((parent: 2, name: c ), {. . .0,y . y1}). Applying the expression scan(\u00b7\u00b7\u00b7, yz) in line 2 yields \nthe states ((parent: 1, name: a , child: 2), {. . .0,y . y3 z . z3}) ((parent: 2, name: b , child: 3), \n{. . .0,y . y2 z . z2}) ((parent: 2, name: c , child: 4), {. . .0,y . y1 z . z1}) which we store as \nset b. Finally, we unlock the lock at .0 and return the entries of b as the query result. Query plan \n(2) was not the only possible query plan, even under the same decomposition and lock placement. Another \npossible query  plan uses edges .x and xy instead of the edge .y. 1: let _ = lock(a, .) in 2: let b \n= scan(scan(scan(a, .x), xy), yz) in (3) 3: let _ = unlock(a, .) in 4: b Now suppose we want to make \nthe same query on the same decomposition, under the lock placement shown in Figure 2(a), in which a lock \non every node protects the edges with their source at that node. The equivalent of query plan (3) under \nthe new .ner\u00adgrained lock placement is: 1: let _ = lock(a, .) in 2: let b = scan(a, .x) in 3: let _ = \nlock(b, x) in 4: let c = scan(b, xy) in 5: let _ = lock(c, y) in (4) 6: let d = scan(c, yz) in 7: let \n_ = unlock(c, y) in 8: let _ = unlock(b, x) in 9: let _ = unlock(a, .) in 10: d Query Planner To pick \na good implementation for each query, the compiler uses a query planner that .nds the query plan with \nthe lowest cost as measured by a heuristic cost estimation function. The concurrent query planner is \nbased on the non-concurrent query planner described in the literature [12]; like the non-concurrent query \nplanner, the concurrent query planner enumerates valid query plans and chooses the plan with the lowest \ncost estimate. The main extension for concurrency is the query planner must only permit queries that \nacquire and hold the right locks in the right order. Internally the query planner only considers plans \nwith two phases, a growing phase consisting of a sequence of lock, scan, and lookup statements, and a \nshrinking phase containing a matching sequence of unlock statements in reverse order; such plans are \ntrivially two-phase. To ensure that queries acquire the correct locks in the correct order, we extend \nthe de.nition of query validity to require that lock statements in a query plan appear in the decomposition \nnode lock order, and that lookup and scan operations must be preceded by a lock of the corresponding \nphysical lock. As in the non-concurrent case, we reuse the query planning infrastructure to compile mutation \noperations. Code for mutations is generated by .rst constructing a concurrent query plan that locates \nand locks all of the edges that require updating; the code generator then emits code that uses the query \nresults to perform the required updates, just as in the non-concurrent case, sandwiched between the growing \nand shrinking phases of the query plan. Query Expression Compilation Each query expression evaluates \nto a set of query states. Internally we compile each query expression into an iterator over query states. \nWe compile let-bindings by evaluating the right-hand side of the binding and storing the results into \na temporary set of query states; subsequent references to a bound variable compile to iterations over \nthe stored state set. In general the lock statement must sort the locks that it acquires. However, in \nsome cases the set of locks may already be in the correct order, so it is super.uous to sort them. For \nexample, consider the acquisition of the locks on node b in query 4 (line 3). Edge .x is represented \nby a TreeMap in the decomposition, which stores its entries in sorted order; a scan over the edge will \ntherefore yield entries in sorted order, which coincides with the correct lock order. Conversely, if \nedge was represented using a HashMap then iteration would return entries in an unpredictable order, so \nthe code would have to sort the locks before acquiring them. The compiler uses a simple static analysis \nto detect lock statements where it can avoid sorting. 6. Experimental Evaluation We have developed a \nprototype implementation of concurrent data representation synthesis, targeted at the Java virtual machine. \nThe prototype is implemented as a Scala [18] compiler plugin; relations and relational operations are \ntranslated into Scala ASTs, which the Scala compiler backend converts to JVM bytecode. In this section \nwe evaluate the performance of the resulting implementation. 6.1 Autotuner A programmer may not know \nthe best possible representation for a concurrent relation. To help .nd an optimal decomposition for \na particular relational speci.cation, we have implemented an auto\u00adtuner which, given a concurrent benchmark, \nautomatically discovers the best combination of decomposition structure, container data structures, and \nchoice of lock placement. Existing work [12] described an autotuner capable of identifying a good decomposition \nin the absence of concurrency. We extend the idea of autotuning to a concurrent setting. To enumerate \ndecompositions, the autotuner .rst chooses an adequate decomposition structure, exactly as for the non-concurrent \ncase [12]. Next, the autotuner chooses a well-formed lock placement; every edge of a decomposition needs \na corresponding physical lock. Finally the autotuner chooses a data structure implementation for each \nedge. If the chosen lock placement serializes access to an edge, the autotuner picks a non-concurrent \ncontainer, whereas if concurrent access to a container is permitted by the lock placement then the autotuner \nchooses a concurrency-safe container.  6.2 Evaluation We evaluate the generated code using a synthetic \nbenchmark mod\u00adeled after the methodology of Herlihy et. al [14] for comparing concurrent map implementations, \nextended to the more general con\u00adtext of a relation. We .x a particular relational speci.cation, together \nwith a set of relational operations. For any given choice of decom\u00adposition, the benchmark uses k identical \nthreads that operate on a single shared relation. Starting from an initially empty relation, each thread \nexecutes 5 \u00d7 105 randomly chosen operations. We plot the total throughput of all threads in operations \nper second against the number of threads to obtain a throughput-scalability curve. By varying the distribution \nof relational operations we can evaluate the performance of the relation under different workloads. For \nour benchmarks we use the directed graph relation described in Section 4.3, together with four relational \noperations, namely .nd successors, .nd predecessors, insert edge, and remove edge. The .nd successor \noperation chooses a random src value and queries the relation for the set of all dst, weight pairs corresponding \nto that src. The .nd predecessor operation is similar but chooses a random dst and queries for src, weight \npairs. The insert edge operation chooses a random src, dst, weight triple to insert into the relation; \nto ensure that the relation s functional dependency is not violated we use the compare-and-set functionality \nof the insert operation to check that no existing edge shares the same src, dst parameters. Finally the \nremove operation chooses a random (src, dst) tuple and removes the corresponding edge, if present. We \nperformed our experiments on a machine with two six-core 3.33Ghz Intel X5680 Xeon CPUs, each with 12Mb \nof L3 cache, and 48Gb memory in total. Hyperthreading was enabled for a total of 24 hardware thread contexts. \nAll benchmarks were run on a OpenJDK 6b20 Java virtual machine in server mode, with a 4Gb initial and \nmaximum heap size. We repeated each experiment 8 times within the same process, with a full garbage collection \nbetween runs. We discarded the results of the .rst 3 runs to allow the JIT compiler time to warm up; \nthe reported values are the average of the last 5 runs.  Operation Distribution: 70-0-20-10 Operation \nDistribution: 35-35-20-10 \u00b7103 \u00b7103 8  Throughput (ops/sec) Throughput (ops/sec) Throughput (ops/sec) \nThroughput (ops/sec) 6 4 2 2 00 Number of Threads Number of Threads Operation Distribution: 0-0-50-50 \nOperation Distribution: 45-45-9-1 \u00b7103 \u00b7103 15 15  10 5 0 Number of Threads Number of Threads  Stick \n1 Stick 2 Stick 3 Stick 4 Split 1 Split 2 Split 3 Split 4 Split 5 Diamond 0 Diamond 1 Diamond 2 Handcoded \nFigure 5. Throughput/scalability curves for a selection of decompositions. Each thread performs 5 \u00d7 105 \nrandom graph operations. Each graph is labeled x-y-z-w, denoting a distribution of x% successors, y% \npredecessors, z% inserts, and w% removes. Stick decompositions are structurally isomorphic to Figure \n3(a) but have different choices of data structures and lock placements, similarly split to Figure 3(b), \nand diamond to Figure 3(c). Figure 5 presents throughput-scalability curves for a selection of decompositions. \nWe generated 448 variants of the three decomposi\u00adtion structures shown in Figure 3 using the autotuner, \nvarying the choice of lock placement, lock striping factor (chosen for simplicity to be either 1 or 1024), \nand selection of containers from the options ConcurrentHashMap, ConcurrentSkipListMap, HashMap, and TreeMap. \nFor clarity of presentation we selected 12 representative decompositions that cover a spectrum of different \nperformance lev\u00adels across the 4 benchmarks; we compare the performance of both the automatically generated \nimplementations and a hand-written implementation. One obvious feature of the results is that the stick \ndecomposi\u00adtions, which are variants of the decomposition shown in Figure 3(a), perform relatively well \nfor the two workloads (70-0-20-10 and 0-0\u00ad50-50) that consist only of successor, insert, and remove operations. \nFor the workloads that include .nding predecessors (35-35-20-10 and 45-45-9-1), split (Figure 3(b)) and \ndiamond (Figure 3(c)) perform far better. Finding successors in a stick decomposition is much more ef.cient \nthan .nding predecessors, which requires iter\u00adating over all edges in the graph. Coarsely-locked data \nstructures scale poorly; three of the decom\u00adpositions shown in the graph (Stick 1, Split 1, Diamond 1) \nuse a single coarse lock to protect the entire decomposition; each con\u00adtainer uses a coarsely locked \nHashMap to represent the top level of edges in the decomposition, and a TreeMap to represent the second \nlevel of edges. Another decomposition (Split 2) uses striped locks and concurrent maps on the left side \nof the decomposition (.u, uw, wx), but uses a single coarse lock to protect the other edges of the graph, \nleading to similarly poor performance.  Sticks 2, 3, 4 use a striped lock at the root to protect a ConcurrentHashMap \nof HashMap containers, a ConcurrentHash-Map of TreeMap containers, and a ConcurrentSkipListMap of HashMap \ncontainers, respectively; all scale much better than the coarsely-locked data structures. Decompositions \nwhich do not share nodes between the two sides of the decomposition outperform decompositions that do. \nFor example, Split 3 and Diamond 1 both use ConcurrentHashMap containers to represent the top-level edges \nand HashMap containers to represent the second level edges, differing only in the sharing structure of \nthe decomposition; the split decomposition performs better in most cases. Split 4 is a variant of Split \n3 with TreeMap containers in place of the HashMap containers. Interestingly, there is a small but consistent \neffect where Split 3 is the best choice for the 35-35-20-10 workload and Split 4 is better for the 45-45\u00ad9-1 \nworkload. Split 5 and Diamond 2 are also similar to Split 3 and Diamond 2, except with ConcurrentSkipListMap \ncontainers in place of ConcurrentHashMap containers; once again, the split decomposition outperforms \nthe diamond decomposition. The handcoded implementation (which was written before the automated experiments) \nis essentially Split 4, and produces almost identical results; the difference in performance between \nthe two is probably due to extra boxing in the generated code that could be eliminated with improvements \nto the code generator. But clearly the automatically generated code is competitive with the hand-written \ncode but requires much less programmer effort, and unless one was willing to write many different hand-coded \nversions, the autotuner will be able to .nd variants that outperform any single hand-written code for \nparticular workload characteristics. It is interesting to note that diamond decompositions outper\u00adformed \nsplit decompositions in the non-concurrent case [12]; the result here is reversed for two reasons. The \nsplit decomposition pro\u00adduces less lock contention, since a pair of transactions may query for successors \nand predecessors in parallel without interfering with one another. Much of the bene.t for sharing in \nthe non-concurrent case came from the fact that it is possible to remove an object from the middle of \nan intrusive doubly-linked list in constant time. Since it is impossible to write such intrusive containers \nin a generic fashion in the Java type system, we do not gain the advantage of more ef.cient removals \nfrom shared nodes. The prominent decrease in throughput evident in Figure 5 when increasing from 6 to \n8 threads is an artifact of the thread scheduler and the memory hierarchy of the test machine. The test \nmachine has two six-core CPUs, each with two hardware contexts per core. The benchmark harness schedules \nup to the .rst six threads on different cores of the same CPU, sharing a common on-chip L3 cache. The \nharness schedules the next six threads on the second CPU; when threads are split across two CPUs they \nmust communicate via the processor interconnect, rather than via a shared on-chip cache. Communication \noff-chip is substantially slower than on-chip communication, producing the notch in the graph. Overall \nthe experiments show the bene.ts of automatic synthesis of both data structures and synchronization: \nsophisticated imple\u00admentations competitive with hand written code can be produced at much lower cost \nin programmer effort, while at the same time providing guarantees about the correctness of the implementation \nof the high-level concurrent relational program. 7. Discussion and Related Work Our results touch upon \na great deal of previous work in several distinct domains. For space reasons our survey is necessarily \nbrief. We build upon previous work in data representation synthesis for sequential data structures [12] \nand the general theory of lock placements [13]. Our contributions beyond previous works include the extension \nof the programming interface to concurrent relations, the integration of different kinds of concurrent \nand non-concurrent data structures as building blocks, the extensions needed to integrate decompositions \nand lock placements, the redesign of query planning in the concurrent setting including guaranteeing \ndeadlock freedom, and a complete implementation and experiments. As mentioned in Section 1, the idea \nof compiling programs that work on relations into specialized data structures originated with [5], and \nits various developments [1, 2, 22]. Earlier work explored data structure selection for sets in SETL \n[7, 19]. Neither line of work addresses the synthesis of concurrent data representations. The closest \nto our work in spirit is Paraglider [23], which pro\u00ad vides semi-automatic assistance in synthesizing \nlow-level concurrent algorithms and data structures. Paraglider focuses on the correct implementation \nof a single concurrent data structure, while our work is about assembling multiple concurrent and non-concurrent \ndata structures into more complex abstractions. Thus, Paraglider is complementary to our approach, and \nwe could extend our menu of concurrent building blocks with Paraglider-generated components. Various \nauthors have investigated techniques for inferring locks to implement atomic sections [4, 6, 9, 11, 16, \n17, 24]. A related problem is automatically optimizing programs with explicit locking by combining multiple \nlocks into one [8]. A key part of this class of work is constructing a mapping from program objects to \nthe locks that protect them, which is similar to, but more specialized than, lock placements. This body \nof work also takes the data structures as .xed and attempts to infer the needed locks, while we consider \nthe possible data representations and lock placements simultaneously. Our system can be viewed as implementing \na pessimistic soft\u00adware transactional memory [21]. Future extensions of our work could synthesize optimistic \nconcurrency control primitives in addi\u00adtion to pessimistic locks. Unlike traditional software transactional \nmemory systems, which perform on-line dynamic analysis to deter\u00admine the read and write sets of transactions, \nour system performs much of the same analysis statically, resulting in run-time code with considerably \nlower overhead. Furthermore, our approach is able to automatically change the data structures and granularity \nof locking used to improve overall performance. It is also worth noting that speculative locking was \n.rst introduced in the context of advanced software transactional memory systems [3]. Finally the original \npaper on two-phase locking made explicit the idea of locking not just objects, but program predicates \n[10]. Logical locks are such predicates, which we realize in practice by mapping logical locks onto physical \nlocks using a lock placement. 8. Conclusion We have described an approach for synthesizing data representations \nfor concurrent programs. Beginning with a program written using high-level concurrent relations, our \nsystem automatically selects the decomposition of the relation into a set of subrelations, chooses concrete \ndata structures for the sub-relations, and selects a locking strategy that guarantees all relational \noperations are both serializ\u00adable and deadlock free for the chosen representation. Because the high-level \ndescription admits multiple choices for each of these dimensions (subject to correctness constraints \nthat rule out some possibilities), programs written in this style describe a space of pos\u00adsible implementations, \na fact that we are able to exploit by using a combination of static and dynamic techniques to search \nthis space to .nd a high-performance implementation. We have described an ex\u00adtensive experiment on a \nconcurrent graph benchmark, demonstrating the wide range of possible implementations and their trade-offs. \n References [1] Don Batory and Jeff Thomas. P2: A lightweight DBMS generator. Journal of Intelligent \nInformation Systems, 9:107 123, 1997. ISSN 0925-9902. doi: 10.1023/A:1008617930959. [2] Don Batory, Gang \nChen, Eric Robertson, and Tao Wang. Design wizards and visual programming environments for GenVoca generators. \nIEEE Transactions on Software Engineering, 26(5):441 452, May 2000. ISSN 0098-5589. doi: 10.1109/32.846301. \n[3] Nathan G. Bronson, Jared Casper, Hassan Cha., and Kunle Olukotun. Transactional predication: high-performance \nconcurrent sets and maps for stm. In Proceeding of the 29th ACM SIGACT-SIGOPS Symposium on Principles \nof Distributed Computing, PODC 10, pages 6 15, New York, NY, USA, 2010. ACM. ISBN 978-1-60558-888-9. \ndoi: 10. 1145/1835698.1835703. [4] Sigmund Cherem, Trishul Chilimbi, and Sumit Gulwani. Inferring locks \nfor atomic sections. In Proceedings of the ACM SIGPLAN conference on Programming Language Design and \nImplementation, pages 304 315, New York, NY, USA, 2008. ACM. ISBN 978-1-59593-860-2. doi: 10.1145/1375581.1375619. \n[5] Donald Cohen and Neil Campbell. Automating relational operations on data structures. IEEE Software, \n10(3):53 60, May 1993. doi: 10.1109/52.210604. [6] Dave Cunningham, Khilan Gudka, and Susan Eisenbach. \nKeep off the grass: Locking the right path for atomicity. In Laurie Hendren, editor, Compiler Construction, \nvolume 4959 of Lecture Notes in Computer Science, pages 276 290. Springer Berlin / Heidelberg, 2008. \nISBN 978-3-540-78790-7. doi: 10.1007/978-3-540-78791-4_19. [7] Robert B. K. Dewar, Arthur Grand, Ssu-Cheng \nLiu, Jacob T. Schwartz, and Edmond Schonberg. Programming by re.nement, as exempli.ed by the SETL representation \nsublanguage. ACM Transactions on Programming Languages and Systems (TOPLAS), 1(1):27 49, January 1979. \nISSN 0164-0925. doi: 10.1145/357062.357064. [8] Pedro C. Diniz and Martin C. Rinard. Lock coarsening: \nEliminating lock overhead in automatically parallelized object-based programs. Journal of Parallel and \nDistributed Computing, 49(2):218 244, 1998. ISSN 0743-7315. doi: 10.1006/jpdc.1998.1441. [9] Michael \nEmmi, Jeffrey S. Fischer, Ranjit Jhala, and Rupak Majumdar. Lock allocation. In Proceedings of the 34th \nannual ACM SIGPLAN-SIGACT symposium on Principles of Programming Languages (POPL), pages 291 296, New \nYork, NY, USA, 2007. ACM. ISBN 1-59593-575\u00ad 4. doi: 10.1145/1190216.1190260. [10] K. P. Eswaran, J. N. \nGray, R. A. Lorie, and I. L. Traiger. The notions of consistency and predicate locks in a database system. \nCommunications of the ACM, 19:624 633, November 1976. ISSN 0001-0782. doi: 10.1145/360363.360369. [11] \nRichard L. Halpert, Christopher J. F. Pickett, and Clark Verbrugge. Component-based lock allocation. \nIn Proceedings of the 16th Interna\u00adtional Conference on Parallel Architecture and Compilation Techniques, \nPACT 07, pages 353 364, Washington, DC, USA, 2007. IEEE Com\u00adputer Society. ISBN 0-7695-2944-5. doi: 10.1109/PACT.2007.23. \n[12] Peter Hawkins, Alex Aiken, Kathleen Fisher, Martin Rinard, and Mooly Sagiv. Data representation \nsynthesis. In Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation \n(PLDI), pages 38 49, New York, NY, USA, 2011. ACM. ISBN 978-1-4503-0663-8. doi: 10.1145/1993498.1993504. \n[13] Peter Hawkins, Alex Aiken, Kathleen Fisher, Martin Rinard, and Mooly Sagiv. Reasoning about lock \nplacements. In Proceedings of the European Symposium on Programming (ESOP), LNCS. Springer Berlin / Heidelberg, \n2012. To appear. [14] Maurice Herlihy, Yossi Lev, Victor Luchangco, and Nir Shavit. A provably correct \nscalable concurrent skip list. In Conference On Principles of Distributed Systems (OPODIS), 2006. [15] \nMaurice P. Herlihy and Jeannette M. Wing. Linearizability: a correct\u00adness condition for concurrent objects. \nACM Trans. Program. Lang. Syst., 12:463 492, July 1990. ISSN 0164-0925. doi: 10.1145/78969. 78972. [16] \nMichael Hicks, Jeffrey S. Foster, and Polyvios Pratikakis. Lock inference for atomic sections. In Workshop \non Languages, Compilers and Hardware Support for Transactional Computing, 2006. [17] Bill McCloskey, \nFeng Zhou, David Gay, and Eric Brewer. Autolocker: Synchronization inference for atomic sections. In \nProceedings of the 33rd ACM SIGPLAN-SIGACT Symposium on Principles of Program\u00adming Languages (POPL), \npages 346 358, New York, NY, USA, 2006. ACM. ISBN 1-59593-027-2. doi: 10.1145/1111037.1111068. [18] Martin \nOdersky, Philippe Altherr, Vincent Cremet, Iulian Dragos, Gilles Dubochet, Burak Emir, Sean McDirmid, \nSt\u00b4 ephane Micheloud, Niko\u00adlay Mihaylov, Michel Schinz, Erik Stenman, Lex Spoon, and Matthias Zenger. \nAn Overview of the Scala Programming Language, second edition. Technical Report LAMP-REPORT-2006-001, \nEcole Polytech\u00ad \u00b4nique F\u00b4erale de Lausanne (EPFL), Lausanne, Switzerland, 2006. ed\u00b4 [19] Edmond Schonberg, \nJacob T. Schwartz, and Micha Sharir. Automatic data structure selection in SETL. In Proceedings of the \nACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages (POPL), pages 197 210, New York, \nNY, USA, 1979. ACM. doi: 10.1145/ 567752.567771. [20] Ohad Shacham, Nathan Bronson, Alex Aiken, Mooly \nSagiv, Martin Vechev, and Eran Yahav. Testing atomicity of composed concurrent operations. In Proceedings \nof the ACM International Conference on Object Oriented Programming Systems Languages and Applications \n(OOPSLA), pages 51 64, New York, NY, USA, 2011. ACM. ISBN 978-1-4503-0940-0. doi: 10.1145/2048066.2048073. \n[21] Nir Shavit and Dan Touitou. Software transactional memory. Dis\u00adtributed Computing, 10:99 116, 1997. \nISSN 0178-2770. doi: 10. 1007/s004460050028. [22] Yannis Smaragdakis and Don Batory. DiSTiL: A transformation \nlibrary for data structures. In Conference on Domain-Speci.c Languages, pages 257 271, October 1997. \n[23] Martin Vechev and Eran Yahav. Deriving linearizable .ne-grained concurrent objects. In Proceedings \nof the ACM SIGPLAN conference on Programming Language Design and Implementation (PLDI), pages 125 135, \nNew York, NY, USA, 2008. ACM. ISBN 978-1-59593-860-2. doi: 10.1145/1375581.1375598. [24] Yuan Zhang, \nVugranam Sreedhar, Weirong Zhu, Vivek Sarkar, and Guang Gao. Minimum lock assignment: A method for exploiting \nconcurrency among critical sections. In Languages and Compilers for Parallel Computing, volume 5335 of \nLecture Notes in Computer Science, pages 141 155. Springer Berlin / Heidelberg, 2008. ISBN 978-3-540-89739-2. \ndoi: 10.1007/978-3-540-89740-8_10.    \n\t\t\t", "proc_id": "2254064", "abstract": "<p>We describe an approach for synthesizing data representations for concurrent programs. Our compiler takes as input a program written using <i>concurrent relations</i> and synthesizes a representation of the relations as sets of cooperating data structures as well as the placement and acquisition of locks to synchronize concurrent access to those data structures. The resulting code is correct by construction: individual relational operations are implemented correctly and the aggregate set of operations is serializable and deadlock free. The relational specification also permits a high-level optimizer to choose the best performing of many possible legal data representations and locking strategies, which we demonstrate with an experiment autotuning a graph benchmark.</p>", "authors": [{"name": "Peter Hawkins", "author_profile_id": "81331494100", "affiliation": "Stanford University, Stanford, CA, USA", "person_id": "P3471283", "email_address": "hawkinsp@cs.stanford.edu", "orcid_id": ""}, {"name": "Alex Aiken", "author_profile_id": "81100399954", "affiliation": "Stanford University, Stanford, CA, USA", "person_id": "P3471284", "email_address": "aiken@cs.stanford.edu", "orcid_id": ""}, {"name": "Kathleen Fisher", "author_profile_id": "81331492634", "affiliation": "Tufts University, Medford, MA, USA", "person_id": "P3471285", "email_address": "kfisher@eecs.tufts.edu", "orcid_id": ""}, {"name": "Martin Rinard", "author_profile_id": "81100087275", "affiliation": "Massachusetts Institute of Technology, Cambridge, MA, USA", "person_id": "P3471286", "email_address": "rinard@csail.mit.edu", "orcid_id": ""}, {"name": "Mooly Sagiv", "author_profile_id": "81460640494", "affiliation": "Tel-Aviv University, Tel-Aviv, Israel", "person_id": "P3471287", "email_address": "msagiv@post.tau.ac.il", "orcid_id": ""}], "doi_number": "10.1145/2254064.2254114", "year": "2012", "article_id": "2254114", "conference": "PLDI", "title": "Concurrent data representation synthesis", "url": "http://dl.acm.org/citation.cfm?id=2254114"}