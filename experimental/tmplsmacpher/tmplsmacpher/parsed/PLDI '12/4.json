{"article_publication_date": "06-11-2012", "fulltext": "\n Deterministic Parallelism via Liquid Effects * Ming Kawaguchi Patrick Rondon Alexander Bakst Ranjit \nJhala University of California, San Diego {mwookawa,prondon,abakst,jhala}@cs.ucsd.edu Abstract Shared \nmemory multithreading is a popular approach to parallel programming, but also .endishly hard to get right. \nWe present Liq\u00aduid Effects, a type-and-effect system based on re.nement types which allows for .ne-grained, \nlow-level, shared memory multi\u00adthreading while statically guaranteeing that a program is determin\u00adistic. \nLiquid Effects records the effect of an expression as a for\u00admula in .rst-order logic, making our type-and-effect \nsystem highly expressive. Further, effects like Read and Write are recorded in Liquid Effects as ordinary \nuninterpreted predicates, leaving the ef\u00adfect system open to extension by the user. By building our sys\u00adtem \nas an extension to an existing dependent re.nement type sys\u00adtem, our system gains precise value-and branch-sensitive \nreason\u00ading about effects. Finally, our system exploits the Liquid Types re\u00ad.nement type inference technique \nto automatically infer re.nement types and effects. We have implemented our type-and-effect check\u00ading \ntechniques in CSOLVE, a re.nement type inference system for C programs. We demonstrate how CSOLVE uses \nLiquid Effects to prove the determinism of a variety of benchmarks. Categories and Subject Descriptors \nD.2.4 [Software Engineer\u00ading]: Software/Program Veri.cation Validation; D.1.3 [Pro\u00adgramming Techniques]: \nConcurrent Programming Parallel Pro\u00adgramming General Terms Languages, Reliability, Veri.cation Keywords \nLiquid Types, Type Inference, Dependent Types, C, Safe Parallel Programming, Determinism 1. Introduction \nHow do we program multi-core hardware? While many models have been proposed, the model of multiple sequential \nthreads con\u00adcurrently executing over a single shared memory remains popu\u00adlar due to its ef.ciency, its \nuniversal support in mainstream pro\u00adgramming languages and its conceptual simplicity. Unfortunately, \nshared memory multithreading is .endishly hard to get right, due to the inherent non-determinism of thread \nscheduling. Unless the pro\u00adgrammer is exceptionally vigilant, concurrent accesses to shared * This work \nwas supported by NSF grants CCF-0644361, CNS-0720802, CCF-0702603, and a gift from Microsoft Research. \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI \n12, June 11 16, 2012, Beijing, China. Copyright c . 2012 ACM 978-1-4503-1205-9/12/06. . . $10.00. data \ncan result in non-deterministic behaviors in the program, po\u00adtentially yielding dif.cult-to-reproduce \nheisenbugs whose ap\u00adpearance depends on obscurities in the bowels of the operation sys\u00adtem s scheduler \nand are hence notoriously hard to isolate and .x. Determinism By Default. One way forward is to make \nparallel programs deterministic by default [2] such that, no matter how threads are scheduled, program \nbehavior remains the same, elim\u00adinating unruly heisenbugs and allowing the programmer to rea\u00adson about \ntheir parallel programs as if they were sequential. In recent years, many static and dynamic approaches \nhave been pro\u00adposed for ensuring determinism in parallel programs. While dy\u00adnamic approaches allow arbitrary \ndata sharing patterns, they also impose non-trivial run-time overheads and hence are best in situa\u00adtions \nwhere there is relatively little sharing [5]. In contrast, while static approaches have nearly no run-time \noverhead, they have been limited to high-level languages like Haskell [7] and Java [2] and require that \nshared structures be refactored into speci.c types or classes for which deterministic compilation strategies \nexist, thereby restricting the scope of sharing patterns. Liquid Effects. In this paper, we present Liquid \nEffects, a type\u00adand-effect system based on re.nement type inference which uses recent advances in SMT \nsolvers to provide the best of both worlds: it allows for .ne-grained shared memory multithreading in \nlow\u00adlevel languages with program-speci.c data access patterns, and yet statically guarantees that programs \nare deterministic. Any system that meets these goals must satisfy several criteria. First, the system \nmust support precise and expressive effect speci.\u00adcations. To precisely characterize the effect of program \nstatements, it must precisely reason about complex heap access patterns, in\u00adcluding patterns not foreseen \nby the system s designers, and it must be able to reason about relations between program values like \nloop bounds and array segment sizes. Second, the system must be exten\u00adsible with user-de.ned effects \nto track effects which are domain\u00adspeci.c and thus could not be foreseen by the system s designers. Finally, \nthe system must support effect inference, so that the pro\u00adgrammer is not overwhelmed by the burden of \nproviding effect an\u00adnotations. 1. Precise and Expressive Effect Speci.cations. Our Liquid Ef\u00adfects type-and-effect \nsystem expresses the effect of a statement on the heap as a formula in .rst-order logic that classi.es \nwhich ad\u00addresses in the heap are accessed and with what effect they were accessed for example, whether \nthe data was read or written. Ex\u00adpressing effects as .rst-order formulas makes our type-and-effect system \nhighly expressive: for example, using the decidable theory of linear arithmetic, it is simple to express \nheap access patterns like processing chunks of an array in parallel or performing strided ac\u00adcesses in \na number of separate threads. Using .rst-order formulas for effects ensures that our system can express \ncomplex access pat\u00adterns not foreseen by the system s designers, and allows us to incor\u00adporate powerful \noff-the-shelf SMT solvers in our system for reason\u00ading about effects. Further, by building our type-and-effect \nsystem as  void sumBlock (char *a, int i, int len) { if (len <= 1) return; int hl =len /2; cobegin { \nsumBlock (a, i, hl); sumBlock (a, i + hl, len -hl); } a[i] += a[i + hl]; } int sum1 (char *a, int len) \n{ sumBlock (a, 0, len); return a[0]; } Figure 1. Parallel Array Summation With Contiguous Partitions \nan extension to an existing dependent re.nement type system and allowing effect formulas to reference \nprogram variables, our system gains precise value and branch-sensitive reasoning about effects. 2. Extensibility. \nOur effect formulas specify how locations on the heap are affected using effect labeling predicates like \nRead and Write. These effect labeling predicates are ordinary uninterpreted predicates in .rst-order \nlogic; our system does not treat effect la\u00adbeling predicates specially. Thus, we are able to provide \nextensibil\u00adity by parameterizing our system over a set of user-de.ned effect labeling predicates. Further, \nwe allow the user to give commuta\u00adtivity declarations specifying which pairs of effects have benign interactions \nwhen they simultaneously occur in separate threads of execution. This enables users to track domain-speci.c \neffects not envisioned by the designers of the type-and-effect system. 3. Effect Inference. Our type \nrules can be easily recast as an al\u00adgorithm for generating constraints over unknown re.nement types and \neffect formulas; these constraints can then be solved using the Liquid Types technique for invariant \ninference, thus yielding a highly-automatic type-based method for proving determinism.  To illustrate \nthe utility of Liquid Effects, we have implemented our type-and-effect checking techniques in CSOLVE, \na re.nement type inference system for C programs based on Liquid Types. We demonstrate how CSOLVE uses \nLiquid Effects to prove the deter\u00adminism of a variety of benchmarks from the literature requiring precise, \nvalue-aware tracking of both built-in and user-de.ned heap effects while imposing a low annotation burden \non the user. As a result, CSOLVE opens the door to ef.cient, deterministic program\u00adming in mainstream \nlanguages like C. 2. Overview We start with a high-level overview of our approach. Figures 1 and 2 show \ntwo functions, sum1 and sum2 respectively, which add up the elements of an array of size 2k in-place \nby dividing the work up into independent sub-computations that can be executed in parallel. At the end \nof both procedures, the .rst element of the array contains the .nal sum. In this section, we demonstrate \nhow our system seamlessly combines path-sensitive reasoning using re.nement types, heap effect tracking, \nand SMT-based reasoning to prove that sum1 and sum2 are deterministic. 2.1 Contiguous Partitions The \nfunction sum1 sums array a of length len using the auxiliary function sumBlock, which sums the elements \nof the contiguous segment of array a consisting of len-many elements and begin\u00adning at index i. Given \nan array segment of length len, sumBlock computes the sum of the segment s elements by dividing it into \ntwo contiguous subsegments which are recursively summed using sumBlock. The recursive calls place the \nsum of each subsegment in the .rst element of the segment; sumBlock computes its .nal result by summing \nthe .rst element of each subsegment and stores it in the .rst element of the entire array segment. Cobegin \nBlocks. To improve performance, we can perform the two recursive calls inside sumBlock in parallel. This \nis accomplished using the cobegin construct, which evaluates each statement in a block in parallel. Note \nthat the recursive calls both read and write to the array passed to sumBlock; thus, to prove that the \nprogram is deterministic, we have to show that the part of the array that is written by each call does \nnot overlap with the portion of the array that is read by the other. In the following, we demonstrate \nhow our system is able to show that the recursive calls access disjoint parts of the array and thus that \nthe function sumBlock is deterministic. Effect Formulas. We compute the region of the array accessed \nby sumBlock as an effect formula that describes which pointers into the array are accessed by a call \nto sumBlock with parameters i and len. We note that the .nal line of sumBlock writes element i of a. \nWe state the effect of this write as the formula 1 Sa[i] = \u00b7 . = a + i. We interpret the above formula, \nSa[i], as the pointer . accessed by the expression a[i] is equal to the pointer obtained by incrementing \npointer a by i. The .nal line of sumBlock also reads element i + hl of a. The effect of this read is \nstated as a similar formula: Sa[i+hl] = \u00b7 . = a + i + hl. The total effect of the .nal line of sumBlock \nstating which portion of the heap it accesses is then given by the disjunction of the two effect formulas: \nSa[i] . Sa[i+hl] = \u00b7 . = a + i . . = a + i + hl. The above formula says that the heap locations accessed \nin the .nal line of sumBlock are exactly the locations corresponding to a[i] and a[i + hl]. Having determined \nthe effect of the .nal line of sumBlock as a formula over the local variables a, i, and hl, we make two \nobservations to determine the effect of the entire sumBlock function as a formula over its arguments \na, i, and len. First, we observe that, in the case where len =2, a[i + hl]= a[i + len - 1]. From this, \nwe can see inductively that a call to sumBlock with in\u00addex i and length len will access elements a[i] \nto a[i + len - 1]. Thus, we can soundly ascribe sumBlock the effect \u00b7 SsumBlock = a + i = . . .< a + \ni + len. That is, sumBlock will access all elements in the given array segment. Determinism via Effect \nDisjointness. We are now ready to prove that the recursive calls within sumBlock access disjoint regions \nof the heap and thus that sumBlock behaves deterministically when the calls are executed in parallel. \nWe determine the effect of the .rst recursive call by instantiating sumBlock s effect, i.e., by replacing \nformals a, i, and len with actuals a, i, and hl, respectively: \u00b7 SCall 1 = a + i = . . .< a + i + hl. \n1 Here we associate effects with program expressions like a[i]. However, in the technical development \nand our implementation, effects are associated with heap locations in order to soundly account for aliasing \n(section 4).  declare effect Accumulate; declare Accumulate commutes with Accumulate; void accumLog \n(char *l, int j) effect (&#38;l[j], Accumulate(v) &#38;&#38; !Read(v) &#38;&#38; !Write(v)); void sumStride \n(char *a, int stride, char *log) { foreach (i, 0, THREADS) { for (int j = i; j < stride; j += THREADS) \n{ a[j] += a[j + stride]; accumLog (log, j); } } } int sum2 (char *a, int len) { log = (char *) malloc \n(len); for (int stride = len/2; stride > 0; stride /= 2) sumStride (a, stride, log); return a[0]; } Figure \n2. Parallel Array Summation With Strided Partitions The resulting effect states that the .rst recursive \ncall only accesses array elements a[i] through a[i + hl - 1]. We determine the ef\u00adfect of the second \nrecursive call with a similar instantiation of sumBlock s effect: SCall 2 = \u00b7 a + i + hl = . . .< a + \ni + len - hl. All that remains is to show that the effects SCall 1 and SCall 2 are disjoint. We do so \nby asking an SMT solver to prove the unsatis.\u00adability of the conjoined access intersection formula SUnsafe \n=SCall 1 . SCall 2, whose inconsistency establishes that the intersection of the sets of pointers . accessed \nby both recursive calls is empty.  2.2 Complex Partitions In the previous example, function sum1 computed \nthe sum of an array s elements by recursively subdividing the array into halves and operating on each \nhalf separately before combining the results. We were able to demonstrate that sum1 is deterministic \nby showing that the concurrent recursive calls to sumBlock operate on disjoint contiguous segments of \nthe array. We now show that our system is capable of proving determinism even when the array access patterns \nare signi.cantly more complex. Function sum2 uses the auxiliary function sumStride, which sums an array \ns elements using the following strategy: First, the array is divided into two contiguous segments. Each \nelement in the .rst half is added to the corresponding element in the second half, and the element in \nthe .rst half is replaced with the result. The problem is now reduced to summing only the .rst half of \nthe array, which proceeds in the same fashion, until we can reduce the problem no further, at which point \nthe sum of all the array elements is contained in the .rst element of the array. Foreach Blocks. To increase \nperformance, the pairwise additions between corresponding elements in the .rst and second halves of the \narray are performed in parallel, with the level of parallelism de\u00adtermined by a compile-time constant \nTHREADS. The parallel threads are spawned using the foreach construct. The statement foreach (i, l, u) \ns; executes statement s once with each possible binding of i in the range [l, u). Further, all executions \nof statement s are performed in parallel. The foreach loop within sumStride spawns THREADS many threads. \nThread i processes elements i, i + THREADS, i + 2 * THREADS, etc., of array a. While this strided decomposition \nmay appear contrived, it is commonly used in GPU programming to maximize memory bandwidth by enabling \nadjacent threads to access adjacent array cells via memory coalescing [1]. To prove that the foreach \nloop within sumStride is determin\u00adistic, we must show that no location that is written in one iteration \nis either read from or written to in another iteration. (We ignore the effect of the call to the function \naccumLog for now and return to it later.) Note that this differs from the situation with sum1: in order \nto demonstrate that sum1 is deterministic, it was not neces\u00adsary to consider read and write effects separately; \nit was enough to know that recursive calls to sumBlock operated on disjoint por\u00adtions of the heap. However, \nthe access pattern of a single iteration of the foreach loop in sumStride is considerably more compli\u00adcated, \nand reasoning about noninterference between loop iterations will require tracking not only which locations \nare affected but pre\u00adcisely how they are affected that is, whether they are read from or written to. \nLabeled Effect Formulas. We begin by computing the effect of each iteration of the loop on the heap, \ntracking which locations are accessed as well as which effects are performed at each location. We .rst \ncompute the effect of each iteration of the inner for loop, which adds the value of a[j + stride] to \na[j]. We will use the unary effect labeling predicates Read(.) and Write(.) to record the fact that a \nlocation in the heap has or has not been read from or written to, respectively. We capture the effect \nof the read of a[j + stride] with the effect formula \u00b7 S1 = Read(.) .\u00acWrite(.) . . = a + j + stride. \nNote that we have used the effect label predicates Read and Write to record not only that we have accessed \na[j + stride] but also that this location was only read from, not written to. We record the effect of \nthe write to a[j] with a similar formula: S2 \u00b7 = Write(.) .\u00acRead(.) . . = a + j As before, the overall \neffect of these two operations performed sequentially, and thus the effect of a single iteration of the \ninner for loop is the disjunction of the two effect formulas above: \u00b7 Sj =S1 . S2 Iteration Effect Formulas. \nHaving computed the effect of a single iteration of sumStride s inner for loop, the next step in proving \nthat sumStride is deterministic is to compute the effect of each iteration of the outer foreach loop, \nwith our goal being to show that the effects of any two distinct iterations must be disjoint. We begin \nby generalizing the effect formula we computed to describe the effect of a single iteration of the inner \nfor loop to an effect formula describing the effect of the entire for loop. The effect formula describing \nthe effect of the entire for is an effect formula that does not reference the loop induction variable \nj, but is implied by the conjunction of the loop body s single-iteration effect and any loop invariants \nthat apply to the loop induction variable j. Note that the induction variable j enjoys the loop invariant \nj < stride . j = i mod THREADS. Given this invariant for j, we can now summarize the effect of the for \nloop as an effect formula which does not reference the loop induction variable j but is implied by the \nconjunction of the above invariant on j and the single-iteration effect formula Sj :  \u00b7 Sfor = (Write(.) \n. (. - a) = i mod THREADS) . (Write(.) . .< a + stride) . (Read(.) . . = a + stride) (1) We have now \ncomputed the effect of sumStride s inner for loop, and thus the effect of a single iteration of sumStride \ns outer foreach loop. Effect Projection. We de.ne an effect projection operator p(S,E) which returns \nthe restriction of the effect formula S to the effect label E. In essence, the effect projection is the \nformula implied by the conjunction of S and E(.). For example, the projection of the Write effect of \niteration i of the foreach loop represents the addresses written by thread i. Determinism via Iteration \nEffect Disjointness. To prove that the foreach loop is deterministic, we must show that values that are \nwritten in one iteration are not read or overwritten in another. First, we show that no two foreach iterations \n(i.e., threads) write to the same location, or, in other words, that distinct iterations write through \ndisjoint sets of pointers. We establish this fact by asking the SMT solver to prove the unsatis.ability \nof the write-write intersection formula: SUnsafe = p(Sfor , Write) . p(Sfor , Write)[i .. i.] . i .. \n. 0 = i, i. = i< THREADS. The formula is indeed unsatis.able as, from Equation 1, \u00b7 p(Sfor , Write) = \n(. - a) = i mod THREADS . .< a + stride (2) and so, after substituting i. the query formula is (. - a) \n= i mod THREADS . (. - a) = i. mod THREADS = i. which is unsatis.able when i .. Next, we prove that no \nheap location is read in one iteration (i.e., thread) and written in another. As before, we verify that \nthe intersection of the pointers read in one iteration with those written in another iteration is empty. \nConcretely, we ask the SMT solver to prove the unsatis.ability of the write-read intersection formula \nSUnsafe = p(Sfor , Write) . p(Sfor , Read)[i .. i.] . i =.i. where, from Equation 1, the locations read \nin a single iteration of the foreach loop are described by the Read projection p(Sfor , Read) = \u00b7 . = \na + stride. After substituting the the write effect from Equation 2, we can see that the write-read-intersection \nformula is unsatis.able as .< a + stride . . = a + stride is inconsistent. Thus, by proving the disjointness \nof the write\u00adwrite and write-read effects, we have proved that the foreach loop inside sumStride is deterministic. \n 2.3 User-De.ned Effects By expressing our effect labeling predicates as ordinary uninter\u00adpreted predicates \nin .rst-order logic, we open the door to allowing the user to de.ne their own effects. Such user-de.ned \neffects are .rst-class: we reason about them using the same mechanisms as the built-in predicates Read \nand Write, and effect formulas over user-de.ned predicates are equally as expressive as those over the \nbuilt-in predicates. We return to the sumStride function to demonstrate the use of user-de.ned effects. \nThe for loop in sumStride tracks how many times the entry a[j] is written by incrementing the j-th value \nin the counter array log using the externally-de.ned function accumLog. We assume that the accumLog function \nis implemented so that its operation is atomic. Thus, using accumLog to increment the count of the same \nelement in two distinct iterations of the foreach loop does not cause non-determinism. Specifying Effects. \nWe create a new user-de.ned effect predicate to track the effect of the accumLog function using the statement \ndeclare effect Accumulate; This extends the set of effect predicates that our system tracks to E = \u00b7 \n{Read, Write, Accumulate}. We then annotate the prototype of the accumLog function to specify accumLog \ncauses the Accumulate effect to occur on the j-th entry of its parameter array l: void accumLog (char \n*l, int j) effect (&#38;l[j], Accumulate(v) &#38;&#38; !Read(v) &#38;&#38; !Write(v)); Specifying Commutativity. \nWe specify that our user effect Accumulate does not cause nondeterminism even when it occurs on the same \nlocation in two separate threads using the commutativ\u00adity annotation declare Accumulate commutes with \nAccumulate; This annotation extends the set of pairs of effects that commute with each other that is, \nwhich can occur in either order without affecting the result of the computation. In particular, the commuta\u00adtivity \nannotation above extends the set of commutable pairs to C = \u00b7 {(Read, Read), (Accumulate, Accumulate)}. \nThe extended set formally speci.es that in addition to pairs of Read effects (included by default), pairs \nof of Accumulate effects also commute, and hence may be allowed to occur simultaneously. Generalized \nEffect Disjointness. Finally, we generalize our method for ensuring determinism. We check the disjointness \nof two effect formulas S1 and S2 by asking the SMT solver to prove the unsatis.ability of the effect \nintersection formula: SUnsafe = .(E1,E2) . (E\u00d7 E\\ C).p(S1,E1) . p(S2,E2). That is, for each possible \ncombination of effect predicates that have not been explicitly declared to commute, we check that the \ntwo effect sets are disjoint when projected on those effects. Thus, by declaring an Accumulate effect \nwhich is commuta\u00adtive, we are able to verify the determinism of sum2. Effect Inference. In the preceding, \nwe gave explicit loop invariants and heap effects where necessary. Later in this paper, we explain how \nwe use Liquid Type inference [28] to reduce the annotation burden on the programmer by automatically \ninferring the loop invariants and heap effects given above. Outline. The remainder of this paper is organized \nas follows: In section 3, we give the syntax of LCE programs, informally explain their semantics, and \ngive the syntax of LCE s types. We de.ne the type system of LCE in section 4. In section 5, we give an \noverview of the results of applying an implementation of a typechecker for LCE to a series of examples \nfrom the literature. We review related work in section 6. 3. Syntax and Semantics In this section, we \npresent the syntax of LCE programs, informally discuss their semantics, and give the syntax of LCE types. \n v ::= ||| x n &#38;n Values variable integer pointer b p ::= || ::= int(i) ref(l, i) f(v) Base Types \ninteger pointer Re.nement Formulas a e F ::= |||| ::= ||||||||||| ::= v a1 . a2 a1 +p a2 a1 ~ a2 a *v \n*v1 := v2 if v then e1 else e2 f(v) malloc(v) let x = e1 in e2 letu x = unfold v in e fold l e1 . e2 \nfor each x in v1 to v2 {e} f (xi) { e } Pure Expressions value arithmetic operation pointer arithmetic \ncomparison Expressions pure expression heap read heap write if-then-else function call memory allocation \nlet binding location unfold location fold parallel composition parallel iteration Function Declarations \nt i c l h S ::= ::= || ::= ::= || ::= || ::= || {. : b | p} n n+m i : t l lj e h * l .. c e S * l .. \np Re.nement Types Indices constant sequence Blocks Heap Locations abstract location concrete location \nHeap Types empty heap extended heap Heap Effects empty effect extended effect P ::= F e Programs s ::= \n(xi : ti)/h1 . t/h2/S Function Schemes Figure 3. Syntax of LCE programs 3.1 Programs The syntax of LCE \nprograms is shown in Figure 3. Most of the ex\u00ad pression forms for expressing sequential computation are \nstandard or covered extensively in previous work [28]; the expression forms for parallel computation \nare new to this work. Values. The set of LCE values v includes program variables, in\u00adteger constants \nn, and constant pointer values &#38;n representing ad\u00addresses in the heap. With the exception of the \nnull pointer value &#38;0, constant pointer values do not appear in source programs. Expressions. The \nset of pure LCE expressions a includes values v, integer arithmetic expressions a1.a2, where . is one \nof the standard arithmetic operators +, -, *, etc., pointer arithmetic expressions a1 +p a2, and comparisons \na1 ~ a2 where ~ is one of the comparison operators =, <, >, etc. Following standard practice, zero and \nnon-zero values represent falsity and truth, respectively. The set of LCE expressions e includes the \npure expressions a as well as all side-effecting operations. The expression forms for pointer read and \nwrite, if-then-else, function call, memory al\u00adlocation, and let binding are all standard. The location \nunfold and fold expressions are used to support type-based reasoning about the heap using strong updates \nAs they have no effect on the dynamic semantics of LCE programs, we defer discussion of location un\u00adfold \nand fold expressions to section 4. Parallel Expressions. The set of LCE expressions includes two forms \nfor expressing parallel computations. The .rst, parallel com\u00adposition e1 . e2, evaluates expressions \ne1 and e2 in parallel and returns when both subexpressions have evaluated to values. The second parallel \nexpression form is the parallel iteration ex\u00adpression form for each x in v1 to v2 {e}, where v1 and v2 \nare inte\u00adger values. A parallel iteration expression is evaluated by evaluating the body expression e \nonce for each possible binding of variable x to an integer in the range [v1,v2). All iterations are evaluated \nin parallel, and the parallel iteration expression returns when all iter\u00adations have .nished evaluating. \nBoth forms of parallel expressions are evaluated solely for their side effects. Figure 4. Syntax of LCE \ntypes Functions and Programs. A function declaration f (xi) { e } de\u00adclares a function f with arguments \nxi whose body is the expression e. The return value of the function is the value of the expression. An \nLCE program consists of a sequence of function declarations F followed by an expression e which is evaluated \nin the environment containing the previously-declared functions.  3.2 Types The syntax of LCE types \nis shown in Figure 4. Base Types. The base types b of LCE include integer types int(i) and pointer types \nref(l, i). The integer type int(i) describes an integer whose value is in the set described by the index \ni; the set of indices is described below. The pointer type ref(l, i) describes a pointer value to the \nheap location l whose offset from the beginning of location l is an integer belonging to the set described \nby index i. Indices. The language of LCE base types uses indices i to approx\u00ad imate the values of integers \nand the offsets of pointers from the starts of the heap locations where they point. There are two forms \nof indices. The .rst, the singleton index n, describes the set of in\u00adtegers {n}. The second, the sequence \nindex n +m, represents the sequence of offsets {n + lm}8 . l=0 Re.nement Types. The LCE re.nement types \nt are formed by joining a base type b with a re.nement formula p to form the re.nement type {. : b | \np}. The distinguished value variable . refers to the value described by this type. The re.nement formula \np is a logical formula f(., v) over LCE values v and the value variable .. The type {. : b | p} describes \nall values v of base type b that satisfy the formula p[. .. v]. Our re.nements are .rst\u00adorder formulas \nwith equality, uninterpreted functions, and linear arithmetic. When it is unambiguous from the context, \nwe use b to abbreviate {. : b | true}. Blocks. A block c describes the contents of a heap location as \na sequence of bindings from indices i to re.nement types t. Each binding i : t states that a value of \ntype t is contained in the block at each offset n from the start of the block which is described by the \nindex i. Bindings of the form m : t , where types are bound to singleton indices, refer to exactly one \nelement within the block; as such, we allow the re.nement formulas within the same block to refer to \nthe value at offset m using the syntax @m. We require all indices in a block to be disjoint.  Heap Types. \nHeap types h statically describe the contents of run\u00adtime heaps as sets of bindings from heap locations \nl to blocks c.A heap type is either the empty heap e or a heap h extended with a binding from location \nl to block c, written h * l .. c. The location l is either an abstract location l, which corresponds \nto arbitrarily many run-time heap locations, or a concrete location lj , which cor\u00adresponds to exactly \none run-time heap location. The distinction be\u00adtween abstract and concrete locations is used to implement \na form of sound strong updates on heap types in the presence of aliasing, unbounded collections, and \nconcurrency; we defer detailed discus\u00adsion to section 4. Locations may be bound at most once in a heap. \nHeap Effects. A heap effect S describes the effect of an expression on a set of heap locations as a set \nof bindings from heap locations l to effect formulas p. A heap effect is either the empty effect e or \nan effect S extended with a binding from location l to an effect formula p over the in-scope program \nvariables and the value variable ., written S * l .. p. Intuitively, an effect binding l .. p describes \nthe effect of an expression on location l as the set of pointers v such that the formula p[. .. v] is \nvalid. Note that we only bind abstract locations in heap effects. Effect Formulas. The formula portion \nof an effect binding can de\u00adscribe the effect of an expression with varying degrees of precision, from \nsimply describing whether the expression accesses the loca\u00adtion at all, to describing which offsets into \na location are accessed, to describing which offsets into a location are accessed and how they are accessed \n(e.g. whether they are read or written). For example, if we use the function BB(.) to refer to the beginning \nof the block where . points, we can record the fact that expression e accesses the .rst ten items in \nthe block at location l (for either reading or writing) with the effect binding l .. BB(.) = . . .< \nBB(.) + 10, i.e., by stating that all pointers used by e to accesses location l satisfy the above formula. \nTo record that an expression does not access location l at all, we ascribe it the effect formula false. \nWe add further expressiveness to our language of effect formu\u00adlas by enriching it with effect-speci.c \npredicates used to describe the particular effects that occurred within a location. We use the unary \npredicates Read(v) and Write(v) to indicate that pointer v was read from or written to, respectively. \nUsing these predicates, we can precisely state how an expression depends upon or affects the contents \nof a heap location. For example, we can specify that an expression does not write to a location l using \nthe effect binding l .. \u00acWrite(.). On the other hand, we may specify precisely which offsets into l are \nwritten by using Write to guard a predicate describing a set of pointers into l. For example, the effect \nbinding l .. Write(.) . (BB(.) = . . .< BB(.) + 10) describes an expression which writes to the .rst \nten elements of the block at l. Effect predicates like Read and Write may only appear in heap effect \nbindings; they may not appear in type re.nements. Function Schemes. A LCE function scheme (xi : ti)/h1 \n. t/h2/S is the type of functions which take arguments xi of corresponding types ti in a heap of type \nh1, return values of type t in a heap of type h2, and incur side-effects described by the heap effect \nS. The types of function parameters may depend on the other function parameters, and the type of the \ninput heap may depend on the pa\u00adrameters. The types of the return value, output heap, and heap effect \nmay depend on the types of the parameters. We implicitly quantify over all the location names appearing \nin a function scheme. 4. Type System In this section, we present the typing rules of LCE . We begin with \na discussion of LCE s type environments. We then discuss LCE s expression typing rules, paying particular \nattention to how the rules carefully track side effects. Finally, we describe how we use tracked effects \nto ensure determinism. Type Environments. Our typing rules make use of three types of environments. A \nlocal environment, G, is a sequence of type bindings of the form x:t recording the types of in-scope \nvariables and guard formulas p recording any branch conditions under which an expression is evaluated. \nA global environment, F, is a sequence of function type bindings of the form f :S which maps functions \nto their re.ned type signatures. Finally, an effect environment . is a pair (E, C). The .rst component \nin the pair, E, is a set of effect label predicates. The second component in the pair, C, is a set of \npairs of effect label predicates such that (E1,E2) . C states that effect E1 commutes with E2. By default, \nE includes the built-in effects, Read and Write, and C includes the pair (Read, Read), which states that \ncompeting reads to the same address in different threads produce a result that is independent of their \nordering. In our typing rules, we implicitly assume a single global effect environment .. Local environments \nare well-formed if each bound type or guard formula is well-formed in the environment that precedes it. \nAn effect environment (E, C) is well-formed as long as Cis symmetric and only references effects in the \nset E, i.e., C . E \u00d7 E, and (E1,E2) . Ciff (E2,E1) . C. G ::=. | x : t;G | p;G (Local Environment) F \n::=. | f :s;F (Global Environment) E ::={Read, Write}| E; E C ::=(Read, Read) | (E,E); C . ::=(E, C) \n(Effect Environment)  4.1 Typing Judgments We now discuss the rules for ensuring type well-formedness, \nsub\u00adtyping, and typing expressions. Due to space constraints, we only present the formal de.nitions of \nthe most pertinent rules, and defer the remainder to the accompanying technical report [16]. Subtyping \nJudgments. The rules for subtyping re.nement types are straightforward: type t1 is a subtype of t2 in \nenvironment G, written G . t1 <: t2, if 1) t1 s re.nement implies t2 under the assumptions recorded as \nre.nement types and guard predicates in G and 2) t1 s index is included in t2 s when both are interpreted \nas sets. Heap type h1 is a subtype of h2 in environment G, written G . h1 <: h2, if both heaps share \nthe same domain and, for each location l, the block bound to l in h1 is a subtype of the block bound \nto l in h2. Subtyping between blocks is pairwise subtyping between the types bound to each index in each \nblock; we use substitutions to place bindings to offsets @i in the environment. Because effect formulas \nare .rst-order formulas, subtyping be\u00adtween effects is simply subtyping between re.nement types: an ef\u00adfect \nS1 is a subtype of S2 in environment G, written G . S1 <: S2, if each effect formula bound to a location \nin S1 implies the for\u00ad  mula bound to the same location in S2 using the assumptions in G. Type Well-Formedness. \nThe type well-formedness rules of LCE ensure that all re.nement and effect formulas are well-scoped and \nthat heap types and heap effects are well-de.ned maps over loca\u00adtions. These rules are straightforward; \nwe brie.y discuss them be\u00adlow, deferring their formal de.nitions [16]. A heap effect S is well-formed \nwith respect to environment G and heap h, written G,h . S, if it binds only abstract locations which \nare bound in h, binds a location at most once, and binds locations to effect formulas which are well-scoped \nin G. Further, an effect formula p is well-formed in an effect environment (E, C) if all effect names \nin p are present in E. A re.nement type t is well-formed with respect to G, written G . t, if its re.nement \npredicate references only variables con\u00adtained in G. A heap type h is well-formed in environment G, written \nG . h, if it binds any location at most once, binds a corresponding abstract location for each concrete \nlocation it binds, and contains only well-formed re.nement types. A world consisting of a re.nement type, \nheap type, and heap effect is well-formed with respect to environment G, written G . t/h/S, if, with \nrespect to the environment G, the type and heap type are well-formed and the heap effect S is well-formed \nwith respect to the heap h. The rule for determining the well-formedness of function type schemes is \nstandard. Pure Expression Typing. The rules for typing a pure expression a in an environment G, written \nG . a : t , are straightforward, and are deferred to [16]. These rules assign each pure expression a \nre.nement type that records the exact value of the expression. Expression Typing and Effect Tracking. \nFigure 5 shows the rules for typing expressions which explicitly manipulate heap effects to track or \ncompose side-effects. Each expression is typed with a re\u00ad.nement type, a re.nement heap assigning types \nto the heap s con\u00adtents after evaluating the expression, and an effect which records how the expression \naccesses each heap location as a mapping from locations to effect formulas. We use the abbreviation void \nto indi\u00adcate the type {. : int(0) | true}. Pointer dereference expressions *v are typed by the rule [T-READ]. \nThe value v is typed to .nd the location and index into which it points in the heap; the type of the \nexpression is the type at this location and index. The rule records the read s effect in the heap by \ncreating a new binding to v s abstract location, l, and uses the auxiliary function logE.ect to create \nan effect formula which states that the only location that is accessed by this dereference is exactly \nthat pointed to by v (i.e., . = v), that the location is read (i.e., Read(.)), and that no other effect \noccurs. The rules for typing heap-mutating expressions of the form *v1 := v2, [T-SUPD] and [T-WUPD], \nare similar to [T-READ]. The two rules for mutation differ only in whether they perform a strong update \non the type of the heap, i.e., writing through a pointer with a singleton index strongly updates the \ntype of the heap, while writing through a pointer with a sequence index does not. Expressions are sequenced \nusing the let construct, typed by rule [T-LET]. The majority of the rule is standard; we discuss only \nthe portion concerning effects. The rule types expressions e1 and e2 to yield their respective effects \nS1 and S2. The effect of the entire let expression is the composition of the two effects, S1 . S2, de.ned \nin Figure 5. We check that S2 is well-formed in the initial environment to ensure that the variable x \ndoes not escape its scope. Rule [T-PAR] types the parallel composition expression e1 . e2. Expressions \ne1 and e2 are typed to obtain their effects S1 and S2. We then use the OK judgment, de.ned in Figure \n5, to verify that effects S1 and S2 commute, i.e., that the program remains deterministic regardless \nof the interleaving of the two Typing Rules F, G,h . e : t/h2/S G . v : ref(lj ,i) h = h1 * lj .. ...,i:t,... \n[T-READ] F, G,h .*v : t/h/ l .. logE.ect(v, Read) h = h1 * lj .. ...,n:b, . . . G . v1 : ref(lj ,n)G \n. v2 : b h. = h1 * lj .. ...,n:{. : b | . = v2},... [T-SUPD] F, G,h .*v1 := v2 : void/h./ l .. logE.ect(v1, \nWrite) +m) G . v1 : ref(lj ,n G . v2 : th = h1 * lj .. ...,n +m : t,... [T-WUPD] F, G,h .*v1 := v2 : \nvoid/h/ l .. logE.ect(v1, Write) F, G,h . e1 : t1/h1/S1 F, G;x:t1,h1 . e2 : t2/h 2/S 2 G . t 2/h 2/S \n2 [T-LET] F, G,h . let x = e1 in e2 : t2/h 2/S1 . S 2 F, G,h . e1 : t1/h ./S 1 F, G,h . e2 : t2/h ./S \n2 h . . F, G, S{1,2} F, G . OK(S 1, S2) h abstract [T-PAR] F, G,h . e1 . e2 : void/h ./S 1 . S 2 G . \nv1 : int(i)G . v2 : int(i) G1 = G;x:{. : int(i) | v1 = .<v2} F, G1,h . e : t/h/S y fresh G2 =G1;y :{. \n: int(i) |. v1 = .<v2 . . = x} F, G2 . OK(S, S[x .. y]) F, G,h . t/h/S . G1 . S <: S . h abstract [T-FOREACH] \nS. F, G,h . for each x in v1 to v2 {e} : void/h/ G . v : {. : ref( . l, iy) | . =0} h = h0 * l .. nk \n:tk,i+ :t+ . = [@nk .. xk] xk fresh G1 = G;xk :.tk lj fresh h1 = h * lj .. nk :{. = xk},i+ :.t+ F, G1;x:{. \n: ref(lj ,iy) | . = v},h1 . e : t2/ h2/S G1 . h1 G . t 2/h 2/S S = S .{( l .. logE.ect(BB(.)+ nk, Read))} \n nk [T-UNFOLD] F, G,h . letu x = unfold v in e : t2/h 2/S h = h0 * l .. c 1 * lj .. c2 G . c2 <: c1 [T-FOLD] \nF, G,h . fold L : void/h0 * l .. c 1/e Effects Checking G . OK(S1, S2) .(E1,E2)= p(p1,E1) . p(p2,E2) \n.(E1,E2) . (E\u00d7 E) \\ C.Unsat([[G]] . .(E1,E2)) G . OK(p1,p2) .l . Dom(S1 . S2).G . OK(LU(S1,l), LU(S2,l)) \nG . OK(S1, S2) Effects Operations \u00b7 S1 . S2 = {l .. LU(S1,l) . LU(S2,l)}l. Dom(S1,2) \u00b7 S(l) l . Dom(S) \nLU(S,l)= false o.w. \u00b7 p(p, E)= p[E .. Check] . Check(.) \u00b7 logE.ect(v, E)= E(.) . . = v .E..E\\{E} \u00acE.(.) \nFigure 5. Typing Rules concurrently-executing expressions. We give e1 . e2 the effect S1 . S2. We require \nthat the input heap for a parallel composition expression must be abstract, that is, contain only bindings \nfor parallel compositions; this forces the expressions which are run in parallel to unfold any locations \nthey access, which in turn enforces several invariants that will prevent the type system from unsoundly \nassuming invariants in one thread which may be broken by the heap writes performed in another. We elaborate \non this below.  Rule [T-FOREACH] types the foreach parallel loop expression. The loop induction variable \ni ranges over values between v1 and v2, exclusive; we type the body expression e in the environment enriched \nwith an appropriate binding for i and compute the re\u00adsulting heap and per-iteration effect S. We require \nthat the heap is loop-invariant. To ensure that the behavior of the foreach loop is deterministic, we \nmust check that the effects of distinct iterations do not interfere. Hence, we check non-interference \nat two arbitrary, distinct iterations by adding a binding for a fresh name j to the en\u00advironment with \nthe invariant that i .j, and verifying with OK = that the effect at an iteration i, S, commutes with \nthe effect at a distinct iteration j, S[i .. j]. We return S., which subsumes S but is well-formed in \nthe original environment, ensuring that the loop induction variable i does not escape its scope. As with \n[T-PAR], we require that the input heap is abstract, for the same reasons. Strong Updates and Concurrency. \nThe rules [T-UNFOLD] and [T-FOLD] are used to implement a local non-aliasing discipline for performing \nstrong updates on the types of heap locations when only one pointer to the location is accessed at a \ntime. The mechanism is similar to freeze/thaw and adopt/focus [34, 10]. These rules are treated in previous \nwork [28]; we now discuss how these rules handle effects and brie.y recap their handling of strong updates. \nRule [T-UNFOLD] types the letu construct for unfolding a pointer to an abstract location, l, to obtain \na pointer to a correspond\u00ading concrete location, lj , bound to the variable x. [T-UNFOLD] constructs \nthe block bound to lj by creating a skolem variable xj for each binding of a singleton index nj to a \ntype tj ; the binding is a singleton index in a concrete location, so it corresponds to exactly one datum \non the heap. We apply an appropriate substitution to the elements within the block, then typecheck the \nbody expression e with respect to the extended heap and enriched environment. Well\u00adformedness constraints \nensure that an abstract location is never un\u00adfolded twice in the same scope and that x does not escape \nits scope. We allow two concurrently-executing expressions to unfold, and thus simultaneously access, \nthe same abstract location. When a lo\u00adcation is unfolded, our type system records the re.nement types \nbound to each of its singleton indices in the environment. If we do not take care, the re.nement types \nbound to singleton indices and recorded in the environment may be invalidated by an effect (e.g. a write) \noccurring in a simultaneously-executing expression.Thus, an expression which unfolds a location implicitly \ndepends on the data whose invariants are recorded in its environment at the time of un\u00adfolding. To capture \nthese implicit dependencies, when a pointer v is unfolded, we conservatively record a pseudo-read [33] \nat each sin\u00ad gleton offset nk within the block into which v points by recording in the heap effect that \nthe pointer BB(v)+nk is read, where BB(v) is the beginning of the memory block where v points. Then, \nthe in\u00advariant recorded in the environment at the unfolding is preserved, as any violating writes would \nget caught by the determinism check. Rule [T-FOLD] removes a concrete location from the heap so that \na different pointer to the same abstract location may be un\u00adfolded. The rule ensures that the currently-unfolded \nconcrete loca\u00adtion s block is a subtype of the corresponding abstract location s so that we can be sure \nthat the abstract location s invariant holds when it is unfolded later. The fold expression has no heap \neffect. Program Typing. The remaining expression forms do not manipu\u00adlate heap effects; as their typing \njudgments are straightforward and covered in previous work [28], we defer their de.nition [16]. We note \nthat the rule for typing if expressions records the value of the branch condition in the environment \nwhen typing each branch. The judgments for typing function declarations and programs in LCE are straightforward \nand are deferred to [16]. Program LOC Quals Annot Changes T (s) Reduce 39 7 7 N/A 8.8 SumReduce 39 1 \n3 0 1.8 QuickSort 73 3 4 N/A 5.9 MergeSort 95 6 7 0 32.4 IDEA 222 7 5 3 59.7 K-Means 458 7 10 16 63.7 \nTable 1. (LOC) is the number of source code lines as reported by sloccount, (Quals) is the number of \nlogical quali.ers required to prove memory safety and determinism, (Annot) is the number of annotated \nfunction signatures plus the number of effect and commutativity declarations. (Changes) is the number \nof program modi.cations, (T) is the time in seconds taken for veri.cation.  4.2 Handling Effects We \nnow describe the auxiliary de.nitions used by the typing rules of subsection 4.1 for checking effect \nnoninterference. Combining Effects. In Figure 5, we de.ne the . operator for combining effects. Our decision \nto encode effects as formulas in .rst-order logic makes the de.nition of this operator especially simple: \nfor each location l, the combined effect of S1 and S2 on location l is the disjunction of of the effects \non l recorded in S1 and S2. We use the auxiliary function LU(S,l) to look up the effect formula for location \nl in S; if there is no binding for l in S, the false formula, indicating no effect, is returned instead. \nEffects Checking. We check the noninterference of heap effects S1, S2 using the OK judgment, de.ned in \nFigure 5. The effects are noninterfering if the formulas bound to locations in both heap ef\u00adfects are \npairwise noninterfering. Two effect formulas p1 and p2 are noninterfering as follows. For each pair of \nnon-commuting effects E1 and E2 in the set E, we project the set of pointers in p1 (resp., p2) which \nare accessed with effect E1 (resp., E2) using the effect projection operator p. Now, if the conjunction \nof the projected for\u00admulas is unsatis.able, the effect formulas are noninterfering. User-De.ned Effects \nand Commutativity. We allow our set of effects E to be extended with additional effect label predicates \nby the user. To specify how these effects interact, commutativity annotations can be provided that indicate \nwhich effects do not result in nondeterministic behavior when run simultaneously. Then, the user may \noverride the effect of any function by providing an effect S as an annotation, allowing additional .exibility \nto specify domain-speci.c effects or override the effect system if needed. 5. Evaluation We have implemented \nthe techniques in this paper as an extension to CSOLVE, a re.nement type-based veri.er for C. CSOLVE \ntakes as input a C program and a set of logical quali.ers, or formulas over the value variable ., to \nuse in re.nement and effect infer\u00adence. CSOLVE then checks the program both for memory safety er\u00adrors \n(e.g. out-of-bounds array accesses, null pointer dereferences) and non-determinism. If CSOLVE determines \nthat the program is free from such errors, it outputs an annotation .le containing the types of program \nvariables, heaps, functions, and heap effects. If CSOLVE cannot determine that the program is safe, it \noutputs an error message with the line number of the potential error and the inferred types of the program \nvariables in scope at that location. Type and Effect Inference. CSOLVE infers re.nement types and effect \nformulas using the predicate abstraction-based Liquid Types [28] technique; we give a high-level overview \nhere. We note that there are expressions whose re.nement types and heap effects can\u00adnot be constructed \nfrom the types and heap effects of subexpres\u00adsions or from bindings in the heap and environment but instead \nFigure 6. DPJ merge function  <region r1,r2,r3 | r1:* # r3:*, r2:* # r3:*> void qualif Q(V: ptr) : &#38;&#38;[_ \n<= V; V {<, >=} (_ + _ + _)] merge(DPJArrayInt<r1> a, DPJArrayInt<r2> b, DPJArrayInt<r3> c) reads r1:*, \nr2:* writes r3:* { void merge(int* ARRAY LOC(L) a, if (a.length <= merge_size) int* ARRAY LOC(L) b, seq_merge(a, \nb, c); int la, int lb, int* ARRAY c) { else { if (la <= merge_size){ int ha=a.length/2, sb=split(a.get(ha),b); \nseq_merge(a, b, la, lb, c); final DPJPartitionInt<r1> a_split=new DPJPartitionInt<r1>(a,hd); } else { \nfinal DPJPartitionInt<r2> a_split=new DPJPartitionInt<r2>(b,sb); int ha = la / 2, sb = split(a[ha],b,lb); \nfinal DPJPartitionInt<r3> c_split=new DPJPartitionInt<r3>(c,ha+sb); cobegin { cobegin { merge(a_split.get(0),b_split.get(0),c_split.get(0)); \nmerge(a,b,ha,sb,c); merge(a_split.get(1),b_split.get(1),c_split.get(1)); }}} merge(a+ha,b+sb,la-ha,lb-sb,c+ha+sb); \n}}} must be synthesized. For example, the function typing rule requires us to synthesize types, heaps, \nand heap effects for functions. This is comparable to inferring pre-and post-conditions for functions \n(and similarly, loop invariants), which reduces to determining appropri\u00adate formulas for re.nement types \nand effects. To make inference tractable, we require that formulas contained in synthesized types, heaps, \nand heap effects are liquid, i.e., conjunctions of logical qual\u00adi.er formulas provided by the user. These \nquali.ers are templates for predicates that may appear in types or effects. We read our in\u00adference rules \nas an algorithm for constructing subtyping constraints which, when solved, yield a re.nement typing for \nthe program. Methodology. To evaluate our approach, we drew from the set of Deterministic Parallel Java \n(DPJ) benchmarks reported in [2]. For those which were parallelized using the methods we support, namely \nheap effects and commutativity annotations, we veri.ed determinism and memory safety using our system. \nOur goals for the evaluation were to show that our system was as expressive as DPJ s for these benchmarks \nwhile requiring less program restructuring. Results. The results of running CSOLVE on the following bench\u00admarks \nare presented in Table 1. Reduce is the program given in section 2. SumReduce initializes an array using \nparallel iteration, then sums the results using a parallel divide-and-conquer strategy. MergeSort divides \nan array into quarters, recursively sorting each in parallel. It then merges pairs of sorted quarters \nin parallel, and .nally merges the two sorted halves to yield a single sorted array. The .nal merges \nare performed recursively in parallel. QuickSort is a standard in-place quicksort adapted from a sequential \nversion included with DPJ. We parallelized the algorithm by partitioning the input array around its median, \nthen recursively sorting each par\u00adtition in parallel. K-Means was adapted from the STAMP bench\u00admarks \n[23], the C implementation that was ported for DPJ. IDEA is an encryption/decryption kernel ported to \nC from DPJ. Changes. Some benchmarks required modi.cations in order to conform to the current implementation \nof CSOLVE; this does not indicate inherent limitations in the technique. These include: multi\u00addimensionalizing \n.attened arrays, writing stubs for matrix malloc, expanding structure .elds (K-Means), and soundly abstracting \nnon\u00adlinear operations and #define-ing con.guration parameters to constants to work around a bug in the \nSMT solver (IDEA). Annotations. CSOLVE requires two classes of annotations to com\u00adpute base types for \nthe program. First, the annotation ARRAY is re\u00adquired to distinguish between pointers to singletons and \npointers to arrays. Second, as base type inference is intraprocedural, we must additionally specify which \npointer parameters may be aliased by annotating them with a location parameter of the form LOC(L). Qualitative \nComparison. Since the annotations used by CSOLVE and DPJ are very different, a quantitative comparison \nbetween them is not meaningful. Instead, we illustrate the kinds of annota\u00adtions used by each tool and \nqualitatively compare them. Figures 6, 7 contain DPJ, and CSOLVE code, resp., implementing the recur-Figure \n7. CSOLVE merge function sive Merge routine from MergeSort, including region annotations and required \nwrapper classes. In the latter, Merge takes two arrays a and b and recursively splits them, sequentially \nmerging them into array c at a target size. In particular, each statement in the cobegin writes to a \ndifferent contiguous interval of c. In DPJ, verifying this invariant requires: First, wrapping all arrays \nwith an API class DPJArrayInt, and then wrapping each contiguous subarray with another class DPJArrayPartitionInt. \nThis must be done because DPJ does not support precise effects over partitions of native Java arrays. \nSecond, the method must be explicitly declared to be polymorphic over named regions r1, r2, and r3, corresponding \nto the memory locations in which the formals reside. Finally, Merge must be explicitly annotated with \nthe appropriate effects, i.e., it reads r1 and r2 and writes r3. In CSOLVE, verifying this invariant \nrequires: First, specify\u00ading that a and b are potentially aliased arrays (the annotation LOC(L) ARRAY). \nSecond, we specify the quali.ers used to synthe\u00adsize re.nement types and heap effects via the line starting \nwith qualif. This speci.cation says that a predicate of the given form may appear in a type or effect, \nwith the wildcard replaced by any program identi.er in scope for that type or effect. Using the given \nquali.er, CSOLVE infers that a and b s location is only read, and that c s location is only written at \nindices described by the formula c = .<c + la + lb, which suf.ces to prove determinism. Unlike DPJ, CSOLVE \ndoes not require invasive changes to code (e.g. explicit array partitioning), and hence supports domain-and \nprogram-speci.c sharing patterns (e.g. memory coalescing from Figure 2). However, this comes at the cost \nof providing quali.ers. In future work, abstract interpretation may help lessen this burden. 6. Related \nWork We limit our discussion to static mechanisms for checking and en\u00adforcing determinism, in particular \nthe literature that pertains to rea\u00adsoning about heap disjointness: type-and-effect systems, semantic \ndeterminism checking, and other closely related techniques. Named Regions. Region-based approaches assign \nreferences (or objects) to distinct segments of the heap, which are explicitly named and manipulated \nby the programmer. This approach was introduced in the context of adding impure computations to a functional \nlanguage, and developed as a means of controlling the side-effects incurred by segments of code in sequential \npro\u00adgrams [20, 21]. Effects were also investigated in the context of safe, programmer-controlled memory \nmanagement [31, 15, 19]. Ideas from this work led to the notion of abstract heap locations [34, 13, 10] \nand our notion of fold and unfold. Ownership Types. In the OO setting, regions are closely related to \nownership types which use the class hierarchy of the program to separate the heap into disjoint, nested \nregions [9, 29]. In addition to isolation, ownership types can be used to track effects [8], and to reason \nabout data races and deadlocks [6, 4, 22].  Such techniques can be used to enforce determinism [30], \nbut regions and ownership relations are not enough to enforce .ne\u00adgrained separation. Instead, we must \nprecisely track relations be\u00adtween program variables. We are inspired by DPJ [2], which shows how some \nsharing patterns can be captured in a dependent region system. However, we show the full expressiveness \nof re.nement type inference and SMT solvers can be brought to bear to enable complex, low-level sharing \nwith static determinism guarantees. Checking Properties of Multithreaded Programs. Several authors have \nlooked into type-and-effect systems for checking other prop\u00aderties of multithreaded programs. For example, \n[11, 24] show how types can be used to prevent races, [12] describes an effect disci\u00ad pline that encodes \nLipton s Reduction method for proving atom\u00adicity. Our work focuses on the higher-level semantic property \nof determinism. Nevertheless, it would be useful to understand how race-freedom and atomicity could be \nused to establish determin\u00adism. Others [27, 3] have looked at proving that different blocks of operations \ncommute. In future work, we could use these to auto\u00admatically generate effect labels and commutativity \nconstraints. Program Logics and Abstract Interpretation. There is a vast lit\u00aderature on the use of logic \n(and abstract interpretation) to reason about sets of addresses (i.e., the heap). The literature on logically \nreasoning about the heap includes the pioneering work in TVLA [35], separation logic [26], and the direct \nencoding of heaps in .rst\u00ad order logic [17, 18], or with explicit sets of addresses called dy\u00ad namic \nframes [14]. These logics have also been applied to reason about concurrency and parallelism: [25] looks \nat using separation logic to obtain (deterministic) parallel programs, and [35] uses ab\u00ad stract interpretation \nto .nd loop invariants for multithreaded, heap\u00admanipulating Java programs. The above look at analyzing \ndisjoint\u00adness for linked data structures. The most closely related to our work is [32], which uses intra-procedural \nnumeric abstract interpretation to determine the set of array indices used by different threads, and \nthen checks disjointness over the domains. Our work differs in that we show how to consolidate all the \nabove lines of work into a uniform location-based heap abstraction (for separation logic-style disjoint \nstructures) with type re.nements that track .ner-grained invariants. Unlike [25], we can verify access \npatterns that require sophisticated arithmetic reasoning, and unlike [32] we can check separation between \ndisjoint structures, and even indices drawn from compound structures like arrays, lists and so on. Our \ntype system allows context-sensitive reasoning about (recur\u00adsive) procedures. Further, .rst-order re.nements \nallow us to verify domain-speci.c sharing patterns via .rst class effect labels [21]. References [1] \nNvidia cuda programming guide. [2] S. V. Adve, S. Heumann, R. Komuravelli, J. Overbey, P. Simmons, H. \nSung, and M. Vakilian. A type and effect system for deterministic parallel java. In OOPSLA, 2009. [3] \nF. Aleen and N. Clark. Commutativity analysis for software parallelization: letting program transformations \nsee the big picture. In ASPLOS, 2009. [4] Z. R. Anderson, D. Gay, R. Ennals, and E. A. Brewer. Sharc: \nchecking data sharing strategies for multithreaded c. In PLDI, 2008. [5] A. Aviram, S.-C. Weng, S. Hu, \nand B. Ford. Ef.cient system-enforced deterministic parallelism. In OSDI, 2010. [6] C. Boyapati, R. Lee, \nand M. C. Rinard. Ownership types for safe programming: preventing data races and deadlocks. In OOPSLA, \n2002. [7] M. Chakravarty, G. Keller, R. Lechtchinsky, and W. Pfannenstiel. Nepal: Nested data parallelism \nin haskell. In Euro-Par, 2001. [8] D. G. Clarke and S. Drossopoulou. Ownership, encapsulation and the \ndisjointness of type and effect. In OOPSLA, 2002. [9] D. G. Clarke, J. Noble, and J. M. Potter. Simple \nownership types for object containment. In ECOOP, 2001. [10] R. DeLine and M. F\u00a8ahndrich. Enforcing high-level \nprotocols in low-level software. In PLDI, 2001. [11] C. Flanagan and S. N. Freund. Type-based race detection \nfor java. In PLDI, pages 219 232, 2000. [12] C. Flanagan and S. Qadeer. A type and effect system for \natomicity. In PLDI, 2003. [13] J. Foster, T. Terauchi, and A. Aiken. Flow-sensitive type quali.ers. In \nPLDI, 2002. [14] B. Jacobs, F. Piessens, J. Smans, K. R. M. Leino, and W. Schulte. A programming model \nfor concurrent object-oriented programs. TOPLAS, 2008. [15] T. Jim, J. G. Morrisett, D. Grossman, M. \nW. Hicks, J. Cheney, and Y. Wang. Cyclone: A safe dialect of c. In USENIX, 2002. [16] M. Kawaguchi, P. \nRondon, A. Bakst, and R. Jhala. Liquid effects: Technical report. http://goto.ucsd.edu/~rjhala/liquid. \n[17] S. K. Lahiri and S. Qadeer. Back to the future: revisiting precise program veri.cation using smt \nsolvers. In POPL, 2008. [18] S. K. Lahiri, S. Qadeer, and D. Walker. Linear maps. In PLPV, 2011. [19] \nC. Lattner and V. S. Adve. Automatic pool allocation: improving performance by controlling data structure \nlayout in the heap. In PLDI, 2005. [20] K. R. M. Leino, A. Poetzsch-Heffter, and Y. Zhou. Using data \ngroups to specify and check side effects, 2002. [21] D. Marino and T. D. Millstein. A generic type-and-effect \nsystem. In A. Kennedy and A. Ahmed, editors, TLDI, pages 39 50. ACM, 2009. [22] J.-P. Martin, M. Hicks, \nM. Costa, P. Akritidis, and M. Castro. Dynamically checking ownership policies in concurrent c/c++ programs. \nIn POPL, pages 457 470, 2010. [23] C. C. Minh, J. Chung, C. Kozyrakis, and K. Olukotun. Stamp: Stanford \ntransactional applications for multi-processing. In IISWC, 2008. [24] P. Pratikakis, J. S. Foster, and \nM. W. Hicks. Locksmith: context\u00adsensitive correlation analysis for race detection. In PLDI, 2006. [25] \nM. Raza, C. Calcagno, and P. Gardner. Automatic parallelization with separation logic. In ESOP, pages \n348 362, 2009. [26] J. C. Reynolds. Separation logic: A logic for shared mutable data structures. In \nLICS, pages 55 74, 2002. [27] M. C. Rinard and P. C. Diniz. Commutativity analysis: A new analysis technique \nfor parallelizing compilers. TOPLAS, 19(6), 1997. [28] P. Rondon, M. Kawaguchi, and R. Jhala. Low-level \nliquid types. In POPL, 2010. [29] M. Smith. Towards an effects system for ownership domains. In In ECOOP \nWorkshop -FTfJP 2005, 2005. [30] T. Terauchi and A. Aiken. A capability calculus for concurrency and \ndeterminism. TOPLAS, 30, 2008. [31] M. Tofte and J.-P. Talpin. A theory of stack allocation in polymorphi\u00adcally \ntyped languages, 1993. [32] M. T. Vechev, E. Yahav, R. Raman, and V. Sarkar. Automatic veri.cation of \ndeterminism for structured parallel programs. In SAS, 2010. [33] J. Voung, R. Chugh, R. Jhala, and S. \nLerner. Data.ow analysis for concurrent programs using data race detection. In PLDI, 2008. [34] D. Walker \nand J. Morrisett. Alias types for recursive data structures. 2000. [35] E. Yahav and M. Sagiv. Verifying \nsafety properties of concurrent heap-manipulating programs. TOPLAS, 32(5), 2010.   \n\t\t\t", "proc_id": "2254064", "abstract": "<p>Shared memory multithreading is a popular approach to parallel programming, but also fiendishly hard to get right. We present <i>Liquid Effects</i>, a type-and-effect system based on refinement types which allows for fine-grained, low-level, shared memory multi-threading while statically guaranteeing that a program is deterministic. Liquid Effects records the effect of an expression as a for- mula in first-order logic, making our type-and-effect system highly expressive. Further, effects like Read and Write are recorded in Liquid Effects as ordinary uninterpreted predicates, leaving the effect system open to extension by the user. By building our system as an extension to an existing dependent refinement type system, our system gains precise value- and branch-sensitive reasoning about effects. Finally, our system exploits the Liquid Types refinement type inference technique to automatically infer refinement types and effects. We have implemented our type-and-effect checking techniques in CSOLVE, a refinement type inference system for C programs. We demonstrate how CSOLVE uses Liquid Effects to prove the determinism of a variety of benchmarks.</p>", "authors": [{"name": "Ming Kawaguchi", "author_profile_id": "81435598242", "affiliation": "University of California, San Diego, San Diego, CA, USA", "person_id": "P3471144", "email_address": "mwookawa@cs.ucsd.edu", "orcid_id": ""}, {"name": "Patrick Rondon", "author_profile_id": "81435603774", "affiliation": "University of California, San Diego, San Diego, CA, USA", "person_id": "P3471145", "email_address": "prondon@cs.ucsd.edu", "orcid_id": ""}, {"name": "Alexander Bakst", "author_profile_id": "81502806897", "affiliation": "University of California, San Diego, San Diego, CA, USA", "person_id": "P3471146", "email_address": "abakst@cs.ucsd.edu", "orcid_id": ""}, {"name": "Ranjit Jhala", "author_profile_id": "81100198278", "affiliation": "University of California, San Diego, San Diego, CA, USA", "person_id": "P3471147", "email_address": "jhala@cs.ucsd.edu", "orcid_id": ""}], "doi_number": "10.1145/2254064.2254071", "year": "2012", "article_id": "2254071", "conference": "PLDI", "title": "Deterministic parallelism via liquid effects", "url": "http://dl.acm.org/citation.cfm?id=2254071"}