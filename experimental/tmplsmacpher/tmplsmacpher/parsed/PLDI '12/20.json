{"article_publication_date": "06-11-2012", "fulltext": "\n Design and Implementation of Sparse Global Analyses for C-like Languages Hakjoo Oh Kihong Heo Wonchan \nLee Woosuk Lee Kwangkeun Yi Seoul National University {pronto,khheo,wclee,wslee,kwang}@ropas.snu.ac.kr \nAbstract In this article we present a general method for achieving global static analyzers that are precise, \nsound, yet also scalable. Our method generalizes the sparse analysis techniques on top of the abstract \ninterpretation framework to support relational as well as non-relational semantics properties for C-like \nlanguages. We .rst use the abstract interpretation framework to have a global static analyzer whose scalability \nis unattended. Upon this underlying sound static analyzer, we add our generalized sparse analysis tech\u00adniques \nto improve its scalability while preserving the precision of the underlying analysis. Our framework determines \nwhat to prove to guarantee that the resulting sparse version should preserve the precision of the underlying \nanalyzer. We formally present our framework; we present that existing sparse analyses are all restricted \ninstances of our framework; we show more semantically elaborate design examples of sparse non\u00adrelational \nand relational static analyses; we present their implemen\u00adtation results that scale to analyze up to \none million lines of C pro\u00adgrams. We also show a set of implementation techniques that turn out to be \ncritical to economically support the sparse analysis pro\u00adcess. Categories and Subject Descriptors F.3.2 \n[Semantics of Pro\u00adgramming Languages]: Program Analysis Keywords Static analysis, abstract interpretation, \nsparse analysis 1. Introduction Precise, sound, scalable yet global static analyzers have been un\u00adachievable \nin general. Other than almost syntactic properties, once the target property becomes slightly deep in \nsemantics it s been a daunting challenge to achieve the four goals in a single static an\u00adalyzer. This \nsituation explains why, for example, in the static er\u00adror detection tools for full C, there exists a \nclear dichotomy: either bug-.nders that risk being unsound yet scalable or veri.ers that risk being unscalable \nyet sound. No such tools are scalable to globally analyze million lines of C code while being sound and \nprecise enough for practical use. In this article we present a general method for achieving global static \nanalyzers that are precise, sound, yet also scalable. Our ap\u00adproach generalizes the sparse analysis ideas \non top of the abstract Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. PLDI 12, June 11 16, 2012, Beijing, China. Copyright c . 2012 ACM 978-1-4503-1205-9/12/06. \n. . $10.00 interpretation framework. Since the abstract interpretation frame\u00adwork [9, 11] guides us to \ndesign sound yet arbitrarily precise static analyzers for any target language, we .rst use the framework \nto have a global static analyzer whose scalability is unattended. Upon this underlying sound static analyzer, \nwe add our generalized sparse analysis techniques to improve its scalability while preserving the precision \nof the underlying analysis. Our framework determines what to prove to guarantee that the resulting sparse \nversion should preserve the precision of the underlying analyzer. Our framework bridges the gap between \nthe two existing tech\u00adnologies abstract interpretation and sparse analysis towards the design of sound, \nyet scalable global static analyzers. Note that while abstract interpretation framework provides a theoretical \nknob to control the analysis precision without violating its correctness, the framework does not provide \na knob to control the resulting an\u00adalyzer s scalability preserving its precision. On the other hand, \nex\u00adisting sparse analysis techniques [6, 14, 15, 19, 20, 24, 40, 42, 44] achieve scalability, but they \nare mostly algorithmic and tightly cou\u00adpled with particular analyses.1 The sparse techniques are not \ngen\u00aderal enough to be used for an arbitrarily complicated semantic anal\u00adysis. Contributions Our contributions \nare as follows. We propose a general framework for designing sparse static analysis. Our framework is \nsemantics-based and precision\u00adpreserving. We prove that our framework yields a correct sparse analysis \nthat has the same precision as the original.  We present a new notion of data dependency, which is a \nkey to the precision-preserving sparse analysis. Unlike conventional def-use chains, sparse analysis \nwith our data dependency is fully precise.  We design sparse non-relational and relational analysis \nwhich are still general as themselves. We can instantiate these designs with a particular non-relational \nand relational abstract domains, respectively.  We prove the practicality of our framework by experimentally \ndemonstrating the achieved speedup of an industrial-strength static analyzer [23, 26, 28, 35 38]. The \nsparse analysis can analyze programs up to 1 million lines of C code with interval domain and up to 100K \nlines of C code with octagon domain.  Outline Section 2 explains our sparse analysis framework. Sec\u00adtion \n3 and 4 design sparse non-relational and relational analyes, respectively, based on our framework. Section \n5 discusses several issues involved in the implementations. Section 6 presents the ex\u00adperimental studies. \nSection 7 discusses related work. 1 A few techniques [7, 39] are in general settings but instead they \ntake coarse-grained approach to sparsity.  2. Sparse Analysis Framework 2.1 Notation Given function \nf . A . B, we write f|C for the restriction of function f to the domain dom(f ) n C. We write f\\C for \nthe restriction of f to the domain dom(f) - C. We abuse the notation f|a and f\\a for the domain restrictions \non singleton set {a}. We write f[a .. b] for the function got from function f by changing the value for \na to b. We write f[a1 .. b1, \u00b7\u00b7\u00b7 ,an .. bn] for w f[a1 .. b1] \u00b7\u00b7\u00b7 [an .. bn]. We write f[{a1, \u00b7\u00b7\u00b7 ,an} \n.. b] for f[a1 .. f(a1) . b, \u00b7\u00b7\u00b7 ,an .. f (an) . b] (weak update). 2.2 Program A program is a tuple \n.C,... where C is a .nite set of con\u00adtrol points and ... C \u00d7 C is a relation that denotes control .ows \nof the program; c . .. c indicates that c is a next control point of c .. Each control point is associated \nwith a command, de\u00adnoted cmd(c). A path p = p0p1 ...pn is a sequence of control points such that p0 .. \np1 .. \u00b7\u00b7\u00b7 .. pn. We write Paths = lfp.P.{c0c1 | c0 .. c1}.{p0 ...pnc | p . P . pn .. c} for the set of \nall paths in the program. Collecting Semantics Collecting semantics of program P is an invariant [ P \n] . C . 2S that represents a set of reachable states at each control point, where the concrete domain \nof states, S = L . V, maps concrete locations (L) to concrete values (V). The collecting semantics is \ncharacterized by the least .xpoint of semantic function F . (C . 2S) . (C . 2S) such that,  F (X)= .c \n. C.fc( X(c .)). (1) c...c where fc . 2S . 2S is a semantic function at control point c. Because our \nframework is independent from target languages, we leave out the de.nition of the concrete semantic function \nfc. 2.3 Baseline Abstraction We abstract the collecting semantics of program P by the following Galois \nconnection . .-- C . 2S --.C . S (2) a where a and . are pointwise liftings of abstract and concretization \n.S function aS and .S (such that 2S .--.---- S ), respectively. aS We consider a particular, yet general, \nfamily of abstract domains where abstract state S is map L . V where L is a .nite set of abstract locations, \nand V is a (potentially in.nite) set of abstract values. All non-relational abstract domains, such as \nintervals [9], are members of this family. Furthermore, the family covers some numerical, relational \ndomains. Practical relational analyses exploit packed relationality [4, 13, 34, 43]; the abstract domain \nis of form Packs . R where Packs is a set of variable groups selected to be related together. R denotes \nnumerical constraints among variables a.F . F .a, then the soundness of abstract semantics is followed \nby the .xpoint transfer theorem [11].  2.4 Sparse Analysis by Eliminating Unnecessary Propagation The \nabstract semantic function given in (3) propagates some ab\u00adstract values unnecessarily. For example, \nsuppose that we analyze statement x := y using a non-relational domain, like interval do\u00admain [9]. We \nknow for sure that the abstract semantic function for the statement de.nes a new abstract value only \nat variable x and uses only the abstract value of variable y. Thus, it is unnecessary to propagate the \nwhole abstract states. However, the function given in (3) blindly propagates the whole abstract states \nof all predecessors c . to control point c. To make the analysis sparse, we need to eliminate this un\u00adnecessary \npropagation by making the semantic function propagate . . . abstract values along data dependency, not \ncontrol .ows; that is, we make the semantic function propagate only the abstract values newly computed \nat one control point to the other where they are actually used. In the rest of this section, we explain \nhow to make abstract semantic function (3) sparse while preserving its precision and soundness.  2.5 \nDe.nition and Use Set We .rst need to precisely de.ne what are de.nitions and uses . They are de.ned \nin terms of abstract semantics, i.e., abstract se\u00admantic function f c, not concrete semantics. De.nition \n1 (De.nition set). Let S be the least .xpoint lfpF of the original semantic function F . De.nition set \nD(c) at control point c is a set of abstract locations that are assigned a new abstract value by abstract \nsemantic function f c, i.e. D(c) . {l . .f c L |.s .S(c ).( s)(l)=.s (l)}. c...c De.nition 2 (Use set). \nLet S be the least .xpoint lfpF of the original semantic function F . Use set U(c) at control point c \nis a set of abstract locations that are used by abstract semantic function f c, i.e. U(c) . {l . L |.s \n.S(c .).f c( s)|D(c) .= f c( s\\l)|D(c)}. c...c Example 1. Consider the following simple subset of C: \ncmd . x := e |*x := e e . x | &#38;x |*x. The meaning of each statement and each expression is fairly \nstan\u00addard. We design a pointer analysis for this as follows: s . S Var . 2Var = s [x .. E (e)( s)] cmd(c)= \nx := e f c( s) s [y .. E (e)( s)] cmd(c)= *x := e in those groups. In such packed relational analysis, \neach variable = and s (x)= {y}cmd(c)= *x := e . . .. pack is treated as an abstract location (L ) and \nnumerical constraints amount to abstract values (V ). Examples of the numerical con\u00ad s [ s(x) w .. E \n(e)( s)] s (x) e = x straints are domain of octagons [34] and polyhedrons [12]. In prac-E (e)( {x} \ns (y) e = *x y.s (x) e =&#38;x s)= . tice, relational analyses are necessarily packed relational [4, \n13] because of otherwise unacceptable costs. Abstract semantics is characterized as a least .xpoint of \nabstract semantic function F . (C . S ) . (C . S ) de.ned as, F (X ) = .c . C.f c( X (c .)). (3) c...c \nwhere f c . S . S is a monotone abstract semantic function for control point c. We assume that F is sound \nwith respect to F , i.e., . 11 Now suppose that we analyze program 10x := &#38;y; .*p := &#38;z; 12y \n:= x; (superscripts are control points). Suppose that points\u00adto set of pointer p is {x, y} at control \npoint 11 . according to the .xpoint. De.nition set and use set at each control point are as follows. \nD(.10)= {x} U(10= .) \u00d8 D(.11)= {x, y} U(11= .) {p, x, y}D(.12)= {y} U(12= .) {x}  Note that U( 11.) \nbecause of the weak update ( w .) contains D( 11 ..): w the semantics of weak update s [l .. v]= s [l \n.. s (l) . v] is de.ned to use the target location l. This implicit use information, which does not explicitly \nappear in the program text, is naturally captured by following the semantics.  2.6 Data Dependencies \nOnce identifying de.nition and use sets at each control point, we can discover data dependencies of abstract \nsemantic function F between two control points. Intuitively, if the abstract value of abstract location \nl de.ned at control point c0 is used at control point cn, there is a data dependency between c0 and cn \non l. Formal de.nition of data dependency is given below: De.nition 3 (Data dependency). Data dependency \nis ternary rela\u00adtion .. C\u00d7 L \u00d7 Cde.ned as follows: l c0 . cn . .c0 ...cn . Paths,l . L . l . D(c0) n \nU(cn) ..i . (0,n).l .. D(ci) The de.nition means that if there exists a path from control point c0 to \ncn, a value of abstract location l can be de.ned at c0 and used at cn, and there is no intermediate control \npoint ci that may change the value of l, then a data dependency exists between control points c0 and \ncn on location l. One might wonder why the data dependency excludes not only a path that always kills \nthe de.nition but also a path that might, but not always, kill the de.nition. In the latter case, the \nde.nition that might be killed is, by De.nition 2, included in the use set of the de.nition point. Example \n2. In the program presented in Example 1, we can .nd two data dependencies, 10. 11. . .12. . x. and 11x \nComparison with Def-use Chains Our notion of data depen\u00addency is different from the conventional notion \nof def-use chains. If we want to conservatively collect all the possible def-use chains, we should exclude \nonly the paths from de.nition points to use points when there exists a point that always kills the de.nition. \nWe can slightly modify De.nition 3 to express def-use chain relation .du as follows: l c0 .du cn . .c0 \n...cn . Paths,l . L . l . D(c0) n U(cn) ..i . (0,n).l .. Dalways(ci) where Dalways(c) denotes the set \nof abstract locations that are al\u00adways killed at control point c. Example 3. We can .nd three def-use \nchains, 10x11. x . .du ., 10.du x .12, and .11 12 .du . in Example 1. The reason why we use our notion \nof data dependencies instead of def-use chains is that our data dependencies preserve the preci\u00adsion \nof the analysis even when approximations are involved. On the other hand, sparse analysis with approximated \ndef-use chains may lose precision, which becomes evident in Section 2.8. 2.7 Sparse Abstract Semantic \nFunction Using data dependency, we can make abstract semantic function sparse, which propagates between \ncontrol points only the abstract values that participate in the .xpoint computation. Sparse abstract \nfunction F s, whose de.nition is given below, is the same as the original except that it propagates abstract \nvalues along to the data dependency, not along to control .ows: F s(X ) = .c . C.f c( X (cd)|l). l cd.c \nAs this de.nition is only different in that it is de.ned over data dependency (.), we can reuse abstract \nsemantic function f c, and its soundness result, from the original analysis design. The following lemma \nstates that the analysis result with sparse abstract semantic function is the same as the one of original \nanaly\u00adsis. Lemma 1 (Correctness). Let S and Ss be lfpF and lfpF s. Then, .c . C..l . D(c).Ss(c)(l)= S(c)(l). \nThe lemma guarantees that the sparse analysis result is identical to the original result only up to the \nentries that are de.ned in every control point. This is fair since the sparse analysis result does not \ncontain the entries unnecessary for its computation.  2.8 Sparse Analysis with Approximated Data Dependency \nSparse analysis designed until Section 2.7 might not be practical since we can decide de.nition set D \nand use set U only with the original .xpoint lfpF computed. To design a practical sparse analysis, we \ncan approximate data dependency using approximated de.nition set D and use set U (D and U are arbitrary \nsets that respect some properties in De.nition 5). De.nition 4 (Approximated Data Dependency). Approximated \ndata dependency is ternary relation .a. C \u00d7 L \u00d7 C de.ned as follows: l c0 .a cn . .c0 ...cn . Paths,l \n. L . l . D (c0) n U (cn) ..i . (0,n).l .. D (ci) The de.nition is the same except that it is de.ned \nover D and U . The derived sparse analysis is to compute the .xpoint of the following abstract semantic \nfunction: Fa(X ) = .c . C.f c( X(cd)|l). l cd.ac One thing to note is that not all D and U make the \nderived sparse analysis compute the same result as the original. First, both D (c) and U (c) at each \ncontrol point should be an over\u00adapproximation of D(c) and U(c), respectively (we can easily show that \nthe analysis computes different result if one of them is an under-approximation). Next, all spurious \nde.nitions that are in\u00adcluded in D but not in D should be also included in U . The follow\u00ading example \nillustrates what happens when there exists an abstract location which is a spurious de.nition but is \nnot included in the approximated use set. Example 4. Consider the same program presented in Example 1. \nexcept that we now suppose the points-to set of pointer p being {y}. Then, de.nition set and use set \nat each control point are as follows: D(.10)= {x} U(10= .) \u00d8 D(.11)= {y} U(11= .) {p}D(.12)= {y} U(12= \n.) {x}. Note that U(11) does not contain D(11) because of strong update. The following is one example \nof unsafe approximation. D (10U (10 .)= {x}.)= \u00d8 D(.11)= {x, y} U(11= .) {p}D (.12)= {y} U(12= .) {x}. \nThis approximation is unsafe because spurious de.nition {x} at control point .11is not included in approximated \nuse set U ( 11 With this approximation, abstract value of x at 10 . is not propagated to 12. x., but \n., while it is propagated in the original analysis ( 10. 121012U( 11 .. x.). However, if {x}. .a .), \nthen the abstract value will be propagated through two data dependencies, .10.a 11  x. and .11.a .. \nNote that x is not de.ned at 11 x12., thus the propagated abstract value for x is not modi.ed at 11 We \ncan formally de.ne safe approximations of de.nition set and use set as follows: De.nition 5. Set D (c) \nand U (c) are a safe approximation of de.nition set D(c) and use set U(c), respectively, if and only \nif (1) D (c) . D(c) . U (c) . U(c); and (2) D (c) - D(c) . U (c).  The remaining things is to prove \nthat the safe approximation D and U yields the correct sparse analysis, which the following lemma states: \nLemma 2 (Correctness of Safe Approximation). Suppose sparse abstract semantic function F a is derived \nby the safe approximation D and U . Let S and Sa be lfpF and lfpF a. Then, .c . C..l . D (c).Sa(c)(l)= \nS(c)(l). Precision Loss with Conservative Def-use Chains While ap\u00adproximated data dependency does not \ndegrade the precision of an analysis, conservative def-use chains from approximated de.nition set and \nuse set make the analysis less precise even if the approxi\u00admation is safe. The following example illustrates \nthe case of impre\u00adcision. Example 5. Consider the program in Example 1, assuming the points-to set of \np being {x}. The following approximated de.nition and use sets .) \u00d8 D(.10)= {x} U(10= D(.11)= {x, y} \nU(11= .) {p, y} D(.12)= {y} U(12= .) {x}. establish the following two def-use chains: 11.du ., and 10.du \n. x12. x12is similarly modi.ed as in . (we assume here that relation .du De.nition 4). With these conservative \ndef-use chains, the points-to set of x propagated to control point 12 . is {y}.{z}, which is bigger set \nthan {z}, the one that appears in the original analysis.  2.9 Designing Sparse Analysis Steps in the \nFramework In summary, the design of sparse analysis within our framework is done in the following two \nsteps: (1) Design a static analysis based on abstract interpretation frame\u00adwork [9]. Note that the abstract \ndomain should be a member of the family explained in Section 2.3. (2) Design a method to .nd a safe \napproximation D and U of de.nition set D and use set U (De.nition 5).  3. Designing Sparse Non-Relational \nAnalysis As a concrete example, we show how to design sparse non\u00adrelational analyses within our framework. \nFollowing Section 2.9, we proceed in two steps: (1) We design a conventional non\u00adrelational analysis \nbased on abstract interpretation. Relying on the abstract interpretation framework [9, 10], we can .exibly \ndesign a static analysis of our interest with soundness guaranteed. (2) We design a method to .nd D and \nU and prove that they are safe ap\u00adproximations (De.nition 5). The sparse analysis designed in this section \nis the core of our interval domain-based static analyzer, Intervalsparse, which will be evaluated in \nSection 6. For brevity, we restrict our presentation to the following simple subset of C, where a variable \nhas either an integer value or a pointer (i.e. V = Z+ L): cmd . x := e |*x := e |{ x<n} where e . n | \nx | &#38;x |*x | e+e Assignment x := e corresponds to assigning the value of expres\u00adsion e to variable \nx. Store *x := e performs indirect assignments; the value of e is assigned to the location that x points \nto. An as\u00adsume command { x<n} makes the program continue only when the condition evaluates to true. 3.1 \nStep 1: Designing Non-sparse Analysis Abstract Domain From the baseline abstraction (in Section 2.3), \nwe consider a family of state abstractions 2S . .S .S such that, --- --- aS (Because it is standard, \nwe omit the de.nition of aS.) S = L . V L = Var V = Z\u00d7 PP = 2 L An abstract location is a program variable. \nAn abstract value is a pair of an abstract integer Z and an abstract pointer P . A set of integers is \nabstracted to an abstract integer (2Z . .Z .Z). Note --- --- aZ that the abstraction is generic so we \ncan choose any non-relational numeric domains of our interest, such as intervals ( Z = {[l, u] |l, u \n. Z . {-8, +8} . l = u} . {.}). For simplicity, we do not abstract pointers (because they are .nite): \npointer values are kept by a points-to set (P =2L ). Other pointer abstractions are also orthogonally \napplicable. Abstract Semantics The abstract semantics is de.ned by the least .xpoint of semantic function \n(3), F , where the abstract semantic function f c . S . S is de.ned as follows: f c( s)= . . s [x .. \nE (e)( s)] cmd(c)= x := e s [ s(x). .. E (e)( s)] cmd(c)= *x := e P w . s [x...s (x).Z.Z aZ({z .Z|z<n}),s \n(x).P.] cmd(c)= { x<n} Auxiliary function E (e)( s) computes abstract value of e under s . Assignment \nx := e updates the value of x. Store *x := e weakly2 updates the value of abstract locations that *x \ndenotes. { x<n} con.nes the interval value of x according to the condition. E . e . S . V is de.ned as \nfollows: E(n)( s)= .aZ({n}), .. E(x)( s)= s (x)  E(&#38;x)( s)= .., {x}.  E(*x)( s)= {s (a) | a . \ns (x).P}  E(e1+e2)( s)= .v1.Z+ Z v2.Z,v1.P. v2.P. where v1 = E (e1)( s),v2 = E (e2)( s) Note that the \nabove analysis is parameterized by an abstract nu\u00admeric domain Z and sound operators + Z and .Z .  3.2 \nStep 2: Finding De.nitions and Uses The second step is to .nd safe approximations of de.nitions and uses. \nThe framework provides a mathematical de.nitions regarding correctness but does not provide how to .nd \nsafe D and U . In the rest part of this section, we present a semantics-based, systematic way to .nd \nthem. We propose to .nd D and U from a conservative approximation of F . We call the approximated analysis \nby pre-analysis. Let D pre and F pre be the domain and semantic function of such a pre\u00ad 2 For brevity, \nwe consider only weak updates. Applying strong update is orthogonal to our sparse analysis design.  \nanalysis, which satis.es the following two conditions. .pre .---- ---. apre Fpre . apre C . S -apre Dpre \n. F . By abstract interpretation framework [9, 10], such a pre-analysis is guaranteed to be conservative, \ni.e., apre (lfpF ) . lfpF pre . As an example, in experiments (Section 6), we use a simple abstraction \nas follows: . .pre . { apre = X. X(c) | c . dom(X )} C . .---- S ----. S . apre Fpre = . c.C s) s. f \nc( The abstraction ignores the control .ows of programs and com\u00adputes a single global invariant (a.k.a., \n.ow-insensitivity). We now de.ne D and U by using pre-analysis. Let T pre . C . S be the pre-analysis \nresult in terms of original analysis, i.e., T pre = .pre (lfpF pre ). The de.nitions of D and U are naturally \nderived from the semantic de.nition of f c. . . {x} cmd(c)= x := e D (c)= T pre (c)(x).P cmd(c)= *x := \ne . {x} cmd(c)= { x<n} D is de.ned to include locations whose values are potentially de\u00ad.ned (changed). \nIn the de.nition of f c for x := e and { x<n} , we notice that abstract location x may be de.ned. In \n*x := e, we see that f c may de.ne locations s (x).P for a given input state s at program point c. Here, \nwe use the pre-analysis: because we can\u00adnot have the input state s prior to the analysis, we instead \nuse its conservative abstraction T pre (c). Such D satis.es the safe approx\u00adimation condition (De.nition \n5), because we collect all potentially de.ned locations, pre-analysis is conservative, and f is monotone. \nBefore de.ning U , we de.ne an auxiliary function U. e . S . 2L . Given expression e and state s , U(e)( \ns) .nds the set of abstract locations that are referenced during the evaluation of E (e)( s). Thus, U \nis naturally derived from the de.nition of E . U(n)( s)= \u00d8 U(x)( s)= {x}U(&#38;x)( s)= \u00d8 U(*x)( s)= {x}. \ns (x).P U(e1+e2)( s)= U(e1)( s) .U(e2)( s) When e is either n or &#38;x, E does not refer any abstract \nlocation. Because E (x)( s) references abstract location x, U(x)( s) is de.ned by {x}. E(*x)( s) references \nlocation x and each location a . s (x), thus the set of referenced locations is {x}. s (x).P. U is de.ned \nas follows: (For brevity, let s c = T pre(c)) . . U(e)( sc) cmd(c)= x := e  U(c)= {x}. s c(x).P.U(e)( \nsc) cmd(c)= *x := e . {x} cmd(c)= { x<n} Using T pre and U, we collect abstract locations that are potentially \nused during the evaluation of e. Because f c is de.ned to refer to abstract location x in *x := e and \n{ x<n} , U additionally includes x. Note that, in *x := e, U (c) includes s c(x).P because f c performs \nweak updates. Lemma 3. D and U are safe approximations. Sparse Pointer Analysis as Instances We can instantiate \nthis de\u00adsign of non-relational analysis to the recent two successful scalable sparse analysis presented \nin [19, 20]. Semi-sparse analysis [19] applies sparse analysis only for top\u00adlevel variables whose addresses \nare never taken. We do the same thing by designing pre-analysis which computes a .xpoint T pre such that \nT pre(c)(x).P = L for all xs that are not top-level vari\u00adables. Staged Flow-Sensitive Analysis [20] uses \nauxiliary .ow\u00adinsensitive pointer analysis to get an over-approximation of def\u00aduse information on pointer \nvariables. By coincidence, our sparse non-relational analysis already does the same analysis for pointer \nvariables except it also tracks numeric constraints of variables. We can design pre-analysis whose precision \nis incomparable to the original one, as in [20], although the framework cannot guarantee the correctness \nanymore. 4. Designing Sparse Relational Analysis As another example, we show how to design sparse relational \nanalyses. In Section 4.1, we de.ne the family of relational analyses that our framework considers. In \nSection 4.2, we de.ne safe D and U for the analysis. We consider packed relational analysis [4, 34]. \nA pack is a semantically related set of variables. In the rest of this section, we assume a set of variable \npacks, Packs . 2Var such that Packs = Var, are given by users or a pre-analysis [13, 34]. In a packed \nrelational analysis, abstract states (S ) map variable packs (Packs) to a relational domain (R ), i.e., \nS = Packs . R . The distinguishing feature of sparse relational analysis is that de.nition sets and use \nsets are de.ned in terms of variable packs. For example, at a simple statement x := 1, all the variable \npacks that contain x may be de.ned and used at the same time, while only variable x may be de.ned and \nnot used in non-relational analysis. As a result, data dependencies are also de.ned in terms of variable \npacks, i.e., .. C \u00d7 Packs \u00d7 C. We denote a pack of variables x1, \u00b7\u00b7\u00b7 ,xn as ..x1, \u00b7\u00b7\u00b7 ,xn... 4.1 Step \n1: Designing Non-sparse Analysis For brevity, we consider the following pointer-free language: (S = Var \n. Z). cmd . x := e |{ x<n} where e . n | x | e+e Including pointers in the language does not require \nnovelty but verbosity. We focus only on the key differences between non\u00adrelational and relational sparse \nanalysis designs. Abstract Domain From the baseline abstraction (in Section 2.3), .S we consider a family \nof state abstractions 2S .--.---- S such that, (aS aS --- is de.ned using aR such that 2S . .R .R.) --- \na R S = L . V L = Packs V = R Abstract Semantics In packed relational analysis, we sometimes need to \nknow actual values (such as ranges) of variables. For ex\u00adample, suppose we analyze a := b with Packs \n= {..a, c.., ..b, c..}. Analyzing the statement amounts to updating the abstract value for pack ..a, \nc... However, because variable b is not contained in the pack, we need to obtain the value of b from \nthe abstract value as\u00adsociated with ..b, c... Here, the value for b is obtained by project\u00ading the relational \ndomain elements for ..b, c.. into a non-relational value, such as intervals. To this end, we transform \nthe original pro\u00adgram into an internal form that replaces such variables with their actual values: suppose \nthe actual value of b in terms of intervals is .1, 2. then a := b is transformed into a := .1, 2.. Formally, \nwe assume abstract semantic function R for re\u00ad R. cmdrel . R . lational domain R is de.ned over the following \ninternal language: rel . rel rel cmdrel . x := e rel |{ x< Z } where e Z | x | e +e  .Z --- where Z \nis a (non-relational) abstract integer (2Z .--.Z). Note - aZ that this language is not for program codes, \nbut for our semantics de.nition. We now de.ne the semantics of the packed relational analysis. The abstract \nsemantics is de.ned by the least .xpoint of semantic function (3), where the abstract semantic function \nf c is de.ned as follows: f c( s)= s [p1 .. R (cmd1)( s(p1)),...,pk .. R (cmdk)( s(pk))] where {p1,...,pk} \n= pack(x) cmdi = T (pi)( s)(cmd(c)) For variable x, pack(x) returns the set of packs that contains x, \ni.e., pack(x)= {p . Packs | x . p}. For both x := e and { x<n} , we update only the packs that include \nx. T is the function that transforms cmd into cmdrel . Given a variable pack p, state s , and command \ncmd, T (p)( s) . cmd . cmdrel returns transformed command for a given command. T (p)( s)(x := e)= x := \nTe(p)( s)(e) T (p)( s)({ x<n} )= { x< Te(p)( s)(n)} where Te(p)( s) . e . e rel transforms expressions: \nTe(p)( s)(n)= aZ({n}) x if x . p Te(p)( s)(x)= px( s) otherwise Te(p)( s)(e1+e2)= Te(p)( s)(e1)+Te(p)( \ns)(e2) where px . S . Z is a function that projects a relational domain element onto variable x to obtain \nits abstract integer value. To be safe, px should satis.es the following condition: .s . S .px( s) . \naZ ({s(x)|s . .R (.p.pack(x)s (p))})  4.2 Step 2: Finding De.nitions and Uses We now approximate D and \nU . In the previous section, we already presented a general, semantics-based method to safely approximate \nD and U for a given abstract semantics. Because our language in this section is pointer-free, simple \nsyntactic method is enough for our purpose. The distinguishing feature of sparse relational analysis \nis that the entities that are de.ned and used are variable packs, not each variable. From the de.nition \nof f c, we notice that packs pack(x) are potentially de.ned both in assignment and assume: pack(x) cmd(c)= \nx := e D(c)= pack(x) cmd(c)= { x<n} U is de.ned depending on the de.nition of px. On the assumption \nthat Packs contains singleton packs of all program variables, we may de.ne px as follows: px( s)= prel( \ns({x})) x where pxrel . R . Z which project a relational domain element onto variable x to obtain its \nabstract integer value, which is supplied by each relational domain, e.g., see [34]. Now, we de.ne U \nas follows: pack(x) . {..l.. | l . V } cmd(c)= x := e U(c)= pack(x) cmd(c)= { x<n}  where V = V(e) \n- pack(x), and V(e) denotes the set of variables that appear inside an expression e. Lemma 4. D and U \nare safe approximations. 5. Implementation Techniques Implementing sparse analysis presents unique challenges \nregarding construction and management of data dependencies. Because data dependencies for realistic programs \nare very complex, it is a key to practical sparse analyzers to generate data dependencies ef.ciently \nin space and time. We describe the basic algorithm we used for data dependency generation, and discuss \ntwo issues that we expe\u00adrienced signi.cant performance variations depending on different implementation \nchoices. Generation of Data Dependencies We use the standard SSA al\u00adgorithm to generate data dependencies. \nBecause our notion of data dependencies equals to def-use chains with D and U being treated as must-de.nitions \nand must-uses, any def-use chain generation algorithms (e.g., reaching de.nition analysis, SSA algorithm, \netc) can be used. We use SSA generation because it is fast and reduces the size of def-use chains [44]. \nInterprocedural Extension With semantics-based approach in mind, interprocedural sparse analysis is no \nmore dif.cult than its intraprocedural counterpart. Designing a method to .nd safe def\u00adinitions and uses \nfor semantic functions regarding procedure calls is all that we need for interprocedural extension. However, \nduring the implementation, we noticed that this nat\u00adural extension may not be scalable in practice. The \nmain problem was due to unexpected spurious dependencies among procedures. Consider the following code \nand suppose we compute data depen\u00addencies for global variable x. int f() { x=0;1 h(); a=x;2 } int h() \n{ ... } // does not use variable x int g() { x=1;3 h(); b=x;4 } xx Data dependencies for x not only include \n1 . 2 and 3 . 4 but also xx include spurious dependencies 1 . 4 and 3 . 2, because there are control \n.ow paths from 1 to 4 (as well as 3 to 2) via the common procedure calls to h. In real C programs, thousands \nof global vari\u00adables exist and procedures are called from many different call-sites, which generates \noverwhelming number of spurious dependencies. In our experiments, such spurious dependencies made the \nanalysis hardly scalable. Staged pointer analysis algorithm [20] takes this approach but no performance \nproblem was reported; we guess this is because pointer analysis typically ignores non-pointer statements \n(by sparse evaluation techniques [7, 39]) and number of pointer variables are just small subset of the \nentire variables. However, our analyzers trace all semantics of C, i.e., value .ows of all types in\u00adcluding \npointers and numbers. Thus, we generate data dependencies separately for each proce\u00addure. In this approach, \nwe need to specially handle procedure calls: we treat a procedure call as a de.nition (resp., use) of \nall abstract locations de.ned (resp., used) by the callee. In addition, we treat the entry point of each \nprocedure as de.nitions of all the abstract locations that are used in the body of the procedure. With \nthese in\u00adformation, can we generate data dependencies of each procedure independently from other procedures. \nAfter generating dependen\u00adcies of procedures, we connect inter-dependences from call state\u00adments to the \nentry of callee with abstract locations that are used by the callee. The advantage of this approach is \nthat the number of spurious dependencies are reduced: variable x is not propagated to procedure h because \nh does not use x. However, the disadvantage of this approach is that data dependencies are not fully \nsparse. For example, consider a call chain f . g . h and suppose x is de.ned in procedure f and used \nin procedure h. Even when x is not used in\u00adside g, value of x is propagated to h only after it is .rst \npropagated to g. In our experiments, this incomplete sparseness of the analy\u00adsis could not make the resulting \nsparse analysis scalable enough.  We solved the problem by applying the following optimization to the \ngenerated data dependencies (.) until convergence: suppose ll a . b, b . c, and that l is not de.ned \nnor used in b, then we l remove those two dependencies and add a . c. This optimization makes the analysis \nmore sparse, leading to a signi.cant speed up. For the interprocedural extension, we use the .ow-insensitive \nanalysis (de.ned in Section 3.2) to prior resolve function pointers. Because the pre-analysis is fairly \nprecise3, the precision loss caused by this approximation of the callgraph would be reasonably small \nin practice [33]. Using BDDs in Representing Data Dependencies The second practical issue is memory consumption \nof data dependencies. An\u00adalyzing real C programs must deal with hundreds of thousands of statements and \nabstract locations. Thus, naive representations for the data dependencies immediately makes memory problems. \nFor example, in analyzing ghostscript-9.00 (the largest benchmark in Table 1), the data dependencies \nconsist of 201 K abstract loca\u00adtions spanning over 2.8 M statements. Storing such dependency relation \nin a naive set-based implementation, which keeps a map (. C \u00d7 C . 2L ), did not work for such large programs \n(It only worked for programs of moderate sizes less than 150 KLOC). For\u00adtunately, the dependency relation \nis highly redundant, making it a good application of BDDs. For example, .c1,c3,l.. (.) and .c2,c3,l. \nare different but share the common suf.x, and .c1,c2,l1. and .c1,c2,l2. are different but share the common \npre.x. BDDs can effectively share such common suf.xes and pre.xes. We treat each relation .c1,c2,l., \nby bit-encoding each control point and ab\u00adstract location, as a boolean function that is naturally represented \nby BDDs. This way of using BDDs greatly reduced memory costs. For example, for vim60 (227 KLOC), set-based \nrepresentation of data dependencies required more than 24 GB of memory but BDD\u00adimplementation just required \n1 GB. No particular dynamic variable ordering was necessary in our case. 6. Experiments In this section, \nwe evaluate sparse non-relational and relational static analyses designed in Section 3 and Section 4, \nrespectively. The evaluation was performed on top of SPARROW [23, 25, 26, 28, 35 38], an industrial-strength \nstatic analyzer for C programs. For the non-relational analysis, we use the interval domain [9], a representative \nnon-relational domain that is widely used in prac\u00adtice [1, 2, 4, 13, 26]. For the relational analysis, \nwe use the octagon domain [34], a representative relational domain whose effective\u00adness is well-known \nin practice [4, 13, 28, 43]. We have analyzed 16 software packages. Table 1 shows charac\u00adteristics of \nour benchmark programs. The benchmarks are various open-source applications, and most of them are from \nGNU open\u00adsource projects. Standard library calls are summarized using hand\u00adcrafted function stubs. For \nother unknown procedure calls to exter\u00adnal code, we assume that the procedure returns arbitrary values \nand has no side-effect. Procedures that are unreachable from the main procedure, such as callbacks, are \nmade to be explicitly called from the main procedure. All experiments were done on a Linux 2.6 sys\u00adtem \nrunning on a single core of Intel 3.07 GHz box with 24 GB of main memory. 6.1 Interval Domain-based \nSparse Analysis 3 The pointer abstraction of our pre-analysis is basically the same with inclusion-based \npointer analysis, which is the most precise form of .ow\u00adinsensitive pointer analysis [18]. In addition, \nour pre-analysis combines numeric analysis and pointer analysis, which further enhances the precision \nof the pointer analysis [2, 13]. Setting The baseline analyzer, Intervalbase, is the global abstract \ninterpretation engine of SPARROW. The abstract domain of the analysis is an extension of the one de.ned \nin Section 3 to support additional C features such as arrays and structures. The analysis ab\u00adstracts \nan array by a set of tuples of base address, offset, and size. Abstraction of dynamically allocated array \nis similarly handled ex\u00adcept that base addresses are abstracted by their allocation-sites. A structure \nis abstracted by a tuple of base address and set of .eld lo\u00adcations (the analysis is .eld-sensitive). \nThe .xpoint is computed by a worklist algorithm using the conventional widening opera\u00adtor [9] for interval \ndomain. Details of the analysis can be found in [23, 26, 28, 35 38]. The baseline analyzer is not a straw-man \nbut much engineering effort has been put to its implementation. In particular, the analysis exploits \nthe technique of localization [38, 41, 45], which localizes the analysis so that each code block is analyzed \nwith only the to-be-accessed parts of the input state. We use the access-based technique [38], which \nwas shown to be faster by up to 50x than the conventional, reachability-based localization technique \n[38]. From the baseline, we made Intervalvanilla and Intervalsparse. Intervalvanilla is identical to \nIntervalbase except that Intervalvanilla does not perform the access-based localization. We compare the \nperformance between Intervalvanilla and Intervalbase just to check that our baseline analyzer is not \na straw-man. Intervalsparse is the sparse version derived from the baseline. The sparse analysis con\u00adsists \nof three steps: pre-analysis (to approximate def-use sets), data dependency generation, and actual .xpoint \ncomputation. As de\u00adscribed in Section 3, we use a .ow-insensitive pre-analysis. The .xpoint of sparse \nabstract semantic function is computed by a worklist-based .xpoint algorithm. The analyzers are written \nin OCaml. We use the BuDDy library [30] for BDD implementation. Results Table 2 gives the analysis time \nand peak memory con\u00adsumption of the three analyzers. Because three analyzers share a common frontend, \nwe report only the analysis time. For Intervalbase, the time includes the pre-analysis [38]. For Intervalsparse, \nDep in\u00adcludes times for pre-analysis and data dependency generation. Fix represent the time for .xpoint \ncomputation. The results show that Intervalbase already has a competitive performance: it is faster than \nIntervalvanilla by 8 55x, saving peak memory consumption by 54 85%. Intervalvanilla scales to 35 KLOC \nbefore running out of time limit (24 hours). In contrast, Intervalbase scales to 111 KLOC. Intervalsparse \nis faster than Intervalbase by 5 110x and saves memory by 3 92%. In particular, the analysis scalability \nhas been remarkably improved: Intervalsparse scales to 1.4M LOC, which is an order of magnitude larger \nthan that of Intervalbase. There are some counterintuitive results. First, the analysis time for Intervalsparse \ndoes not strictly depend on program sizes. For ex\u00adample, analyzing emacs-22.1 (399 KLOC) requires 10 \nhours, tak\u00ading six times more than analyzing ghostscript-9.00 (1,363 KLOC). This is mainly because some \nreal C programs have unexpectedly large recursive call cycles [27, 37, 46]. Column maxSCC in Ta\u00adble 1 \nreports the sizes of the largest strongly connected component in the callgraph. Note that some programs \n(such as nethack-3.3.0, vim60, and emacs-22.1) have a large cycle that contains hundreds or even thousands \nof procedures. Such non-trivial SCCs markedly increase analysis cost because the large cyclic dependencies \namong procedures make data dependencies much more complex. Second, data dependency generation takes longer \ntime than ac\u00adtual .xpoint computation. For example, data dependency gener\u00adation for ghostscript-9.00 \ntakes 14,116 s but the .xpoint is com\u00adputed in 698 s. The seemingly unbalanced timing results are partly \nbecause of the uses of BDDs in dependency construction. While BDD dramatically saves memory costs, set \noperations for BDDs Table 1. Benchmarks: lines of code (LOC) is obtained by running wc on the source \nbefore preprocessing and macro expansion. Functions reports the number of functions in source code. Statements \nand Blocks report the number of statements and basic blocks in our intermediate representation of programs \n(after preprocessing). maxSCC reports the size of the largest strongly connected component in the callgraph. \nAbsLocs reports the number of abstract locations that are generated during the interval domain-based \nanalysis .  Program LOC Functions Statements Blocks maxSCC AbsLocs gzip-1.2.4a 7K 132 6,446 4,152 2 \n1,784 bc-1.06 13K 132 10,368 4,731 1 1,619 tar-1.13 20K 221 12,199 8,586 13 3,245 less-382 23K 382 23,367 \n9,207 46 3,658 make-3.76.1 27K 190 14,010 9,094 57 4,527 wget-1.9 35K 433 28,958 14,537 13 6,675 screen-4.0.2 \n45K 588 39,693 29,498 65 12,566 a2ps-4.14 64K 980 86,867 27,565 6 17,684 sendmail-8.13.6 130K 756 76,630 \n52,505 60 19,135 nethack-3.3.0 211K 2,207 237,427 157,645 997 54,989 vim60 227K 2,770 150,950 107,629 \n1,668 40,979 emacs-22.1 399K 3,388 204,865 161,118 1,554 66,413 python-2.5.1 435K 2,996 241,511 99,014 \n723 51,859 linux-3.0 710K 13,856 345,407 300,203 493 139,667 gimp-2.6 959K 11,728 1,482,230 286,588 2 \n190,806 ghostscript-9.00 1,363K 12,993 2,891,500 342,293 39 201,161 Programs Intervalvanilla Intervalbase \nSpd.1 Mem.1 Intervalsparse Spd.2 Mem.2 Time Mem Time Mem Dep Fix Total Mem D(c) U(c) gzip-1.2.4a 772 \n240 14 65 55 x 73 % 2 1 3 63 2.4 2.5 5 x 3 % bc-1.06 1,270 276 96 126 13 x 54 % 4 3 7 75 4.6 4.9 14 x \n40 % tar-1.13 12,947 881 338 177 38 x 80 % 6 2 8 93 2.9 2.9 42 x 47 % less-382 9,561 1,113 1,211 378 \n8 x 66 % 27 6 33 127 11.9 11.9 37 x 66 % make-3.76.1 24,240 1,391 1,893 443 13 x 68 % 16 5 21 114 5.8 \n5.8 90 x 74 % wget-1.9 44,092 2,546 1,214 378 36 x 85 % 8 3 11 85 2.4 2.4 110 x 78 % screen-4.0.2 8 N/A \n31,324 3,996 N/A N/A 724 43 767 303 53.0 54.0 41 x 92 % a2ps-4.14 8 N/A 3,200 1,392 N/A N/A 31 9 40 353 \n2.6 2.8 80 x 75 % sendmail-8.13.6 8 N/A 8 N/A N/A N/A 517 227 744 678 20.7 20.7 N/A N/A nethack-3.3.0 \n8 N/A 8 N/A N/A N/A 14,126 2,247 16,373 5,298 72.4 72.4 N/A N/A vim60 8 N/A 8 N/A N/A N/A 17,518 6,280 \n23,798 5,190 180.2 180.3 N/A N/A emacs-22.1 8 N/A 8 N/A N/A N/A 29,552 8,278 37,830 7,795 285.3 285.5 \nN/A N/A python-2.5.1 8 N/A 8 N/A N/A N/A 9,677 1,362 11,039 5,535 108.1 108.1 N/A N/A linux-3.0 8 N/A \n8 N/A N/A N/A 26,669 6,949 33,618 20,529 76.2 74.8 N/A N/A gimp-2.6 8 N/A 8 N/A N/A N/A 3,751 123 3,874 \n3,602 4.1 3.9 N/A N/A ghostscript-9.00 8 N/A 8 N/A N/A N/A 14,116 698 14,814 6,384 9.7 9.7 N/A N/A Table \n2. Performance of interval analysis: time (in seconds) and peak memory consumption (in megabytes) of \nthe various versions of analyses. 8 means the analysis ran out of time (exceeded 24 hour time limit). \nDep and Fix reports the time spent during data dependency analysis and actual analysis steps, respectively, \nof the sparse analysis. Spd.1 is the speed-up of Intervalbase over Intervalvanilla. Mem.1 shows the memory \nsavings of Intervalbase over Intervalvanilla. Spd.2 is the speed-up of Intervalsparse over Intervalbase. \nMem.2 shows the memory savings of Intervalsparse over Intervalbase. D (c) and U (c) show the average \nsize of D (c) and U (c), respectively. such as addition and removal are noticeably slower than usual \nset operations.  6.2 Octagon Domain-based Sparse Analysis Setting We implemented octagon domain-based \nstatic analyz\u00aders Octagonvanilla, Octagonbase, and Octagonby replac\u00ad sparse ing interval domains of SPARROW \nwith octagon domains. Non\u00adnumerical values (such as pointers, array, and structures) are han\u00addled in \nthe same way as the interval analysis. Octagonbase per\u00adforms the access-based localization [38] in terms \nof variable packs. is the same except for the localization. Octagon Octagonvanilla sparse is the sparse \nversion of Octagonbase. To represent octagon domain, we used Apron library [22]. In all experiments, \nwe used a syntax-directed packing strategy. Our packing heuristic is similar to Min\u00b4e s approach [13, \n34], which groups abstract locations that have syntactic locality. For examples, abstract locations involved \nin the linear expressions or loops are grouped together. Scope of the locality is limited within each \nof syntactic C blocks. We also group abstract locations involved in actual and formal parameters, which \nis necessary to capture rela\u00adtions across procedure boundaries. Large packs whose sizes exceed a threshold \n(10) were split down into smaller ones. Results While Octagonvanilla requires extremely large amount \nof time and memory space but Octagonbase makes the analysis re\u00adalistic by leveraging the access-based \nlocalization. Octagonbase is able to analyze 35 KLOC within 5 hours and 10GB of memory. With the localization, \nanalysis speed of Octagonbase increases by 8x 9x and memory consumption decreases by 54% 62%. Though \nOctagonbase saves a lot of memory, the analysis is still not scalable at all. For example, tar-1.13 requires \n6 times more memory than gzip-1.2.4a. This memory consumption is not reasonable consider\u00ading program \nsize and interval analysis result. Thanks to sparse analysis technique, Octagonbecomes sparse more practical \nand scales to 130 KLOC within 18 hours and 29 GB of memory consumption. Octagonis 13 56x faster than \nsparse Octagonbase and saves memory consumption by 75% 95%.  6.3 Discussion Sparsity We discuss the \nrelation between performance and spar\u00adsity. Column D (c) and U (c) in Table 2 and Table 3 show how many \nabstract locations are de.ned and used for each basic block on av\u00aderage. It clearly shows the key observation \nin sparse analysis for real programs; only a few abstract locations are de.ned and used in each program \npoint. For example, the interval domain-based analy\u00ad  Programs Octagonvanilla Octagonbase Spd.1 Mem.1 \nOctagonsparse Spd.2 Mem.2 Time Mem Time Mem Dep Fix Total Mem D(c) U(c) gzip-1.2.4a 2,078 2,832 273 \n1,072 8 x 62 % 7 14 21 269 13.8 14.5 13 x 75 % bc-1.06 9,536 6,987 1,065 3,230 9 x 54 % 20 35 55 358 \n25.2 31.7 19 x 89 % tar-1.13 8 N/A 9,566 5,963 N/A N/A 55 133 188 526 38.3 39.3 51 x 91 % less-382 8 \nN/A 16,121 8,410 N/A N/A 92 340 432 458 42.6 45.4 37 x 95 % make-3.76.1 8 N/A 17,724 12,771 N/A N/A 91 \n240 331 666 51.4 55.7 53 x 95 % wget-1.9 8 N/A 15,998 9,363 N/A N/A 107 181 288 646 31.9 32.9 56 x 93 \n% screen-4.0.2 8 N/A 8 N/A N/A N/A 2,452 13,981 16,433 9,199 372.4 376.1 N/A N/A a2ps-4.14 8 N/A 8 N/A \nN/A N/A 296 8,271 8,566 1,996 97.7 99.0 N/A N/A sendmail-8.13.6 8 N/A 8 N/A N/A N/A 7,256 57,552 64,808 \n29,658 467.6 492.3 N/A N/A Table 3. Performance of octagon analysis: all columns are the same as those \nin Table 2 sis of a2ps-4.14 de.nes and uses only 0.1% of all abstract locations in one program point. \nOne interesting observation from the experiment results is that the analysis performance is more dependent \non the sparsity than the program size. For instance, even though ghostscript-9.00 is 3.5 times bigger \nthan emacs-22.1 in terms of LOC, ghostscript-9.00 takes 2.6 times less time to analyze. Behind this phenomenon, \nthere is a large difference on sparsity; average D (c) size (and U (c) size) of emacs-22.1 is 30 times \nbigger than the one of ghostscript-9.00. Variable Packing For maximal precision, packing strategy should \nbe more carefully devised for each target program. However, note that our purpose of experiments is to \nshow relative performance of Octagonover Octagonbase, and we applied the same pack\u00ad sparse ing strategy \nfor all analyzers. Though our general-purpose packing strategy is not specialized to each program, the \npacking strategy reasonably groups logically related variables. The average size of packs is 5 7 for \nour benchmarks. Domain-speci.c packing strate\u00adgies, such as ones used in Astr\u00b4 ee [34] or CGS [43], reports \nthe similar results: 3 4 [34] or 5 [43]. 7. Related Work Existing sparse analysis techniques are divided \ninto two groups: .ne-grained sparse analyses in particular settings, e.g., [15, 19, 20, 40, 44], and \n coarse-grained sparse analyses in general settings, e.g., [7, 14, 21, 39].  In this paper, we present \na method to obtain .ne-grained sparse analyses in a general setting. Sparse Pointer Analysis Sparse pointer \nanalysis techniques [19, 20, 29] are not general enough to be used for arbitrarily com\u00adplicated semantic \nanalysis. Recently, scalability of .ow-sensitive pointer analysis has been greatly improved using sparse \nanalysis; in 2009, Hardekopf et al. [19] presented a pointer analysis algo\u00adrithm that scales to large \ncode bases (up to 474 KLOC) for the .rst time, and after that, .ow-sensitive pointer analysis becomes \nscalable even to millions of lines of code via sparse analysis tech\u00adniques [20, 29]. We already showed \nthat our framework subsumes two scalable sparse pointer analyses presented in [19, 20]. In addi\u00adtion, \nthe techniques are tightly coupled with pointer analysis and it is not obvious how to generalize them \nand prove their correctness. We provide a general framework that enables a family of abstract interpretation \nto be automatically turned into sparse analysis ver\u00adsions. One noteworthy point is that designing a correct \nsparse analyses in general was easy in our case because our method is semantics\u00adbased (by abstract interpretation). \nFor example, sparse pointer anal\u00adysis [20] relies on . and \u00b5 functions [8] to correctly model point\u00aders, \nwhich is essentially independent of the analysis semantics. In our case, these extra techniques are unnecessary \nbecause we derive sparse analyses faithfully from the abstract semantics of the origi\u00adnal analysis. Sparse \nData.ow Analysis Traditional sparse analysis techniques are in a simpler setting than the one postulated \nin our frame\u00adwork. Sparse analysis techniques were .rst pioneered for optimiz\u00ading data.ow analysis [15, \n40, 44]. Reif and Lewis [40] developed a sparse analysis algorithm for constant propagation and Wegman \net al. [44] extended it to conditional constant propagation. Dhamdhere et al. [15] showed how to perform \nsparse partial redundancy elim\u00adination. These algorithms are fully sparse in that precise def-use chains \nare syntactically identi.able and values are always propa\u00adgated along to def-use chains (in an SSA form). \nHowever, these techniques only consider the programs without pointers. Sparse Evaluation Sparse evaluation \ntechniques [7, 14, 21, 39] are generally applicable but have limitations in sparseness. The goal of sparse \nevaluation [7, 14, 21, 39] is to remove statements whose abstract semantic functions are identity function. \nFor exam\u00adple, in typical pointer analyses, statements for numerical compu\u00adtation are considered as identity \nand we can remove those state\u00adments before analysis begins. Sparse evaluation techniques are not effective \nwhen the underlying analysis does not have many identity functions, which is the case for static analyses \nthat consider full semantics, including numbers and pointers (our case). Localization Localization [32, \n38, 41, 45] is used in general set\u00adtings but not powerful enough. When analyzing code blocks such as \nprocedure bodies, localization attempts to remove irrelevant parts of abstract states that will not be \nused during the analysis. It is widely used as a key cost-reduction technique in many semantics\u00adbased \nstatic analysis, such as shape analysis [41, 45], higher-order .ow analysis [32], and numeric abstract \ninterpretation [38]. How\u00adever, localization cannot avoid unnecessary propagation of abstract values along \ncontrol .ows. Scalable Global Analyzers Our interval and octagon domain\u00adbased analyzers achieve higher \nscalability (up to 1 MLOC and 130 KLOC, respectively) than the previous general-purpose global analyzers. \nZitser et al. [47] report that PolySpace C Veri.er [31], a commercial tool for detection of runtime errors, \ncannot analyze sendmail because of scalability problem. Both our interval and oc\u00adtagon domain-based analyzers \ncan analyze sendmail. Airac [26, 35], a general-purpose interval domain-based global static analyzer, \nscales only to 30 KLOC in global analysis. Recently, a signi.cant progress has been reported by Oh et \nal. [38], but it still does not scale over 120 KLOC. Other similar (interval domain-based) ana\u00adlyzers \nare also not scalable to large code bases [1, 2]. Neverthe\u00adless, there have been scalable domain-speci.c \nstatic analyzers, like Astr\u00b4 ee [4, 13] and CGS [43], which scale to hundreds of thou\u00adsands lines of \ncode. However, Astr\u00b4 ee targets on programs that do not have recursion and backward gotos, which enables \na very ef.\u00adcient interpretation-based analysis [13], and CGS is not fully .ow\u00adsensitive [43]. There are \nother summary-based approaches [16, 17] for scalable global analysis, which are independent of our abstract \ninterpretation-based approach.  BDDs We propose a new usage of Binary Decision Diagram (BDD) [5] in \nprogram analysis. We represent data dependency relation in BDDs. Most of the previous uses are limited \nto compact representations of points-to sets in pointer analysis [3, 19, 20]. Acknowledgments We thank \nLucas Brutschy and Yoonseok Ko for their contributions to the implementation. We thank Deokhwan Kim, \nDaejun Park, and all members of Programming Research Laboratory in Seoul National University for their \nuseful comments and suggestions. We would also like to thank the anonymous PLDI reviewers for their constructive \nfeedback on this paper. This work was supported by the Engineering Research Center of Excellence Program \nof Korea Ministry of Education, Science and Technology(MEST) / National Research Foundation of Korea(NRF) \n(Grant 2012-0000468) and the Brain Korea 21 Project, School of Electrical Engineering and Computer Science, \nSeoul National University in 2011. References [1] X. Allamigeon, W. Godard, and C. Hymans. Static analysis \nof string manipulations in critical embedded C programs. In SAS, 2006. [2] G. Balakrishnan and T. Reps. \nAnalyzing memory accesses in x86 binary executables. In CC, 2004. [3] M. Berndl, O. Lhot\u00b4ak, F. Qian, \nL. Hendren, and N. Umanee. Points-to analysis using bdds. In PLDI, 2003. [4] B. Blanchet, P. Cousot, \nR. Cousot, J. Feret, L. Mauborgne, A. Mine,\u00b4 D. Monniaux, and X. Rival. A static analyzer for large safety-critical \nsoftware. In PLDI, 2003. [5] R. E. Bryant. Graph-based algorithms for boolean function manipula\u00adtion. \nIEEETC, 1986. [6] D. R. Chase, M. Wegman, and F. K. Zadeck. Analysis of pointers and structures. In PLDI, \n1990. [7] J.-D. Choi, R. Cytron, and J. Ferrante. Automatic construction of sparse data .ow evaluation \ngraphs. In POPL, 1991. [8] F. C. Chow, S. Chan, S.-M. Liu, R. Lo, and M. Streich. Effective representation \nof aliases and indirect memory operations in ssa form. In CC, 1996. [9] P. Cousot and R. Cousot. Abstract \ninterpretation: A uni.ed lattice model for static analysis of programs by construction or approxima\u00adtion \nof .xpoints. In POPL, 1977. [10] P. Cousot and R. Cousot. Systematic design of program analysis frameworks. \nIn POPL, 1979. [11] P. Cousot and R. Cousot. Abstract interpretation frameworks. J. Log. Comput., 1992. \n[12] P. Cousot and N. Halbwachs. Automatic discovery of linear restraints among variables of a program. \nIn POPL, 1978. [13] P. Cousot, R. Cousot, J. Feret, L. Mauborgne, A. Min\u00b4e, and X. Rival. Why does astr\u00b4ee \nscale up? Formal Methods in System Design, 2009. [14] R. K. Cytron and J. Ferrante. Ef.ciently computing \nf-nodes on-the\u00ad.y. TOPLAS, 1995. [15] D. M. Dhamdhere, B. K. Rosen, and F. K. Zadeck. How to analyze \nlarge programs ef.ciently and informatively. In PLDI, 1992. [16] I. Dillig, T. Dillig, and A. Aiken. \nSound, complete and scalable path\u00adsensitive analysis. In PLDI, 2008. [17] I. Dillig, T. Dillig, and A. \nAiken. Precise reasoning for programs using containers. In POPL, 2011. [18] B. Hardekopf and C. Lin. \nThe ant and the grasshopper: fast and accurate pointer analysis for millions of lines of code. In PLDI, \n2007. [19] B. Hardekopf and C. Lin. Semi-sparse .ow-sensitive pointer analysis. In POPL, 2009. [20] B. \nHardekopf and C. Lin. Flow-sensitive pointer analysis for millions of lines of code. In CGO, 2011. [21] \nM. Hind and A. Pioli. Assessing the effects of .ow-sensitivity on pointer alias analyses. In SAS, 1998. \n[22] B. Jeannet and A. Min\u00b4e. Apron: A library of numerical abstract domains for static analysis. In \nCAV, 2009. [23] Y. Jhee, M. Jin, Y. Jung, D. Kim, S. Kong, H. Lee, H. Oh, D. Park, and K. Yi. Abstract \ninterpretation + impure catalysts: Our Spar\u00adrow experience. Presentation at the Workshop of the 30 Years \nof Abstract Interpretation, San Francisco, ropas.snu.ac.kr/\\char \\ ~kwang/paper/30yai-08.pdf, January \n2008. [24] R. Johnson and K. Pingali. Dependence-based program analysis. In PLDI, 1993. [25] Y. Jung \nand K. Yi. Practical memory leak detector based on parame\u00adterized procedural summaries. In ISMM, 2008. \n[26] Y. Jung, J. Kim, J. Shin, and K. Yi. Taming false alarms from a domain-unaware C analyzer by a bayesian \nstatistical post analysis. In SAS, 2005. [27] C. Lattner, A. Lenharth, and V. Adve. Making Context-Sensitive \nPoints-to Analysis with Heap Cloning Practical For The Real World. In PLDI, 2007. [28] W. Lee, W. Lee, \nand K. Yi. Sound non-statistical clustering of static analysis alarms. In VMCAI, 2012. [29] L. Li, C. \nCifuentes, and N. Keynes. Boosting the performance of .ow\u00adsensitive points-to analysis using value .ow. \nIn FSE, 2011. [30] J. Lind-Nielson. BuDDy, a binary decision diagram package. [31] MathWorks. Polyspace \nembedded software veri.cation. http:// www.mathworks.com/products/polyspace/index.html. [32] M. Might \nand O. Shivers. Improving .ow analyses via GCFA: Ab\u00adstract garbage collection and counting. In ICFP, \n2006. [33] A. Milanova, A. Rountev, and B. G. Ryder. Precise and ef.cient call graph construction for \nc programs with function pointers. Journal of Automated Software Engineering, 2004. [34] A. Min\u00b4e. The \nOctagon Abstract Domain. HOSC, 2006. [35] H. Oh. Large spurious cycle in global static analyses and its \nalgorith\u00admic mitigation. In APLAS, 2009. [36] H. Oh and K. Yi. An algorithmic mitigation of large spurious \ninter\u00adprocedural cycles in static analysis. SPE, 2010. [37] H. Oh and K. Yi. Access-based localization \nwith bypassing. In APLAS, 2011. [38] H. Oh, L. Brutschy, and K. Yi. Access analysis-based tight localization \nof abstract memories. In VMCAI, 2011. [39] G. Ramalingam. On sparse evaluation representations. Theoretical \nComputer Science, 2002. [40] J. H. Reif and H. R. Lewis. Symbolic evaluation and the global value graph. \nIn POPL, 1977. [41] N. Rinetzky, J. Bauer, T. Reps, M. Sagiv, and R. Wilhelm. A semantics for procedure \nlocal heaps and its abstractions. In POPL, 2005. [42] T. B. Tok, S. Z. Guyer, and C. Lin. Ef.cient .ow-sensitive \ninterproce\u00addural data-.ow analysis in the presence of pointers. In CC, 2006. [43] A. Venet and G. Brat. \nPrecise and ef.cient static array bound checking for large embedded c programs. In PLDI, 2004. [44] M. \nN. Wegman and F. K. Zadeck. Constant propagation with condi\u00adtional branches. TOPLAS, 1991. [45] H. Yang, \nO. Lee, J. Berdine, C. Calcagno, B. Cook, D. Distefano, and P. O Hearn. Scalable shape analysis for systems \ncode. In CAV, 2008. [46] H. Yu, J. Xue, W. Huo, X. Feng, and Z. Zhang. Level by level: making .ow-and \ncontext-sensitive pointer analysis scalable for millions of lines of code. In CGO, 2010. [47] M. Zitser, \nD. E. S. Group, and T. Leek. Testing static analysis tools using exploitable buffer over.ows from open \nsource code. In FSE, 2004.    \n\t\t\t", "proc_id": "2254064", "abstract": "<p>In this article we present a general method for achieving global static analyzers that are precise, sound, yet also scalable. Our method generalizes the sparse analysis techniques on top of the abstract interpretation framework to support relational as well as non-relational semantics properties for C-like languages. We first use the abstract interpretation framework to have a global static analyzer whose scalability is unattended. Upon this underlying sound static analyzer, we add our generalized sparse analysis techniques to improve its scalability while preserving the precision of the underlying analysis. Our framework determines what to prove to guarantee that the resulting sparse version should preserve the precision of the underlying analyzer.</p> <p>We formally present our framework; we present that existing sparse analyses are all restricted instances of our framework; we show more semantically elaborate design examples of sparse non-relational and relational static analyses; we present their implemen- tation results that scale to analyze up to one million lines of C programs. We also show a set of implementation techniques that turn out to be critical to economically support the sparse analysis process.</p>", "authors": [{"name": "Hakjoo Oh", "author_profile_id": "81447600247", "affiliation": "Seoul National University, Seoul, South Korea", "person_id": "P3471206", "email_address": "pronto@ropas.snu.ac.kr", "orcid_id": ""}, {"name": "Kihong Heo", "author_profile_id": "81502711386", "affiliation": "Seoul National University, Seoul, South Korea", "person_id": "P3471207", "email_address": "khheo@ropas.snu.ac.kr", "orcid_id": ""}, {"name": "Wonchan Lee", "author_profile_id": "81486643380", "affiliation": "Seoul National University, Seoul, South Korea", "person_id": "P3471208", "email_address": "wclee@ropas.snu.ac.kr", "orcid_id": ""}, {"name": "Woosuk Lee", "author_profile_id": "81501663633", "affiliation": "Seoul National University, Seoul, South Korea", "person_id": "P3471209", "email_address": "wslee@ropas.snu.ac.kr", "orcid_id": ""}, {"name": "Kwangkeun Yi", "author_profile_id": "81493644271", "affiliation": "Seoul National University, Seoul, South Korea", "person_id": "P3471210", "email_address": "kwang@ropas.snu.ac.kr", "orcid_id": ""}], "doi_number": "10.1145/2254064.2254092", "year": "2012", "article_id": "2254092", "conference": "PLDI", "title": "Design and implementation of sparse global analyses for C-like languages", "url": "http://dl.acm.org/citation.cfm?id=2254092"}