{"article_publication_date": "06-11-2012", "fulltext": "\n A Dynamic Program Analysis to .nd Floating-Point Accuracy Problems Florian Benz Andreas Hildebrandt \nSebastian Hack Saarland University Johannes-Gutenberg Universit\u00a8at Mainz Saarland University fbenz@stud.uni-saarland.de \nandreas.hildebrandt@uni-mainz.de hack@cs.uni-saarland.de Abstract Programs using .oating-point arithmetic \nare prone to accuracy problems caused by rounding and catastrophic cancellation. These phenomena provoke \nbugs that are notoriously hard to track down: the program does not necessarily crash and the results \nare not necessarily obviously wrong, but often subtly inaccurate. Further use of these values can lead \nto catastrophic errors. In this paper, we present a dynamic program analysis that sup\u00adports the programmer \nin .nding accuracy problems. Our analysis uses binary translation to perform every .oating-point computation \nside by side in higher precision. Furthermore, we use a lightweight slicing approach to track the evolution \nof errors. We evaluate our analysis by demonstrating that it catches well\u00adknown .oating-point accuracy \nproblems and by analyzing the Spec CFP2006 .oating-point benchmark. In the latter, we show how our tool \ntracks down a catastrophic cancellation that causes a complete loss of accuracy leading to a meaningless \nprogram result. Finally, we apply our program to a complex, real-world bioinformatics application in \nwhich our program detected a serious cancellation. Correcting the instability led not only to improved \nquality of the result, but also to an improvement of the program s run time. Categories and Subject Descriptors \nG.1.0 [Numerical Analysis]: General Computer arithmetic, Error analysis, Multiple precision arithmetic \nKeywords Dynamic program analysis, program instrumentation, .oating-point accuracy 1. Introduction Floating-point \nnumbers are almost always mere approximations of the true real numbers: there is an uncountably in.nite \nnumber of real values in any open interval on the real line, but any digi\u00adtal computer provides only \n.nite storage for their representation. Due to this approximation, .oating-point arithmetic is prone \nto ac\u00adcuracy problems caused by insuf.cient precision, rounding errors, and catastrophic cancellation. \nIn fact, writing a numerically stable program is challenging. However, few programmers are aware of the \nintricacies that come with .oating-point arithmetic. And even if a programmer is familiar with the problem \nper se, he will often be unable to detect or even prevent it: theoretical results often allow Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI 12, June 11 \n16, 2012, Beijing, China. Copyright c &#38;#169; 2012 ACM 978-1-4503-1205-9/12/06. . . $10.00 only a \nworst-case analysis that soon becomes overly pessimistic for complex cases. We lack the proper tools \nto support the program\u00admers in ef.ciently and effectively .nding .oating-point problems at runtime. In \nthe past, .oating-point accuracy problems caused pro\u00adgram failures that cost millions of dollars or even \nhuman life. And even if a numerical inaccuracy does not result in such catastrophic events, it can cause \nserious harm if it remains undetected: in many application scenarios, correct results can hardly be distinguished \nfrom incorrect ones. In such cases, an undetected numerical inac\u00adcuracy in a popular application can \nlead to countless .awed results. One of the best-studied examples of an undetected accumulation of rounding \nerrors was the failure of the MIM-104 Patriot air defense system in the Gulf War. As a result 28 US Army \nsoldiers were killed by an Iraqi Scud missile on February 25, 1991. A government investigation [18] revealed \nthat the system s internal clock had drifted by 0.3 seconds after operating for 100 hours. This was due \nto a rounding error when converting integer values to .oating-point values. Another well-studied example \noccurred at the Vancouver Stock Exchange. As reported by the Wall Street Journal [20], the index of the \nStock Exchange was initialized at 1000 points in January 1982. For each trade, the index was updated \nby .rst adding the weighted stock price to the previous index and then truncating the result to three \ndecimal places. This operation occurred about 3000 times a day and the accumulated truncations led to \na loss of around one point per day. The error remained undiscovered for 22 months. After the discovery, \nthe index was corrected from 524.811 to 1098.892 points. The calculation of the index was then changed \nfrom truncating to rounding. Both examples drastically demonstrate that .oating-point issues can have \ndramatic consequences. However, there is only a surpris\u00adingly little number of tools that assist the \nprogrammers (who are usually totally unaware of the intricacies of .oating-point arith\u00admetic) in tracking \ndown these problems although the need for easily usable tools for the dynamic analysis of numeric stability \nhas been recognized in the literature and cast in the form of demands (e.g. by Kahan [12]). In the past, \nboth static and dynamic program analyses have been proposed. Static analyses like Fluctuat [5] provide \nsound over-approximations of .oating-point problems such as rounding errors. However, they suffer from \nimprecision and are heavily de\u00adpendent on other analyses (e.g. pointer analyses) to disambiguate the \ndata .ow of a program. Therefore, static techniques work ex\u00adceptionally well in special domains like \nembedded systems where the code does usually not exhibit complex and dynamic data struc\u00adtures. For large, \npossibly object-oriented programs, however, they are less suitable. In contrast, the dynamic program \nanalyses pro\u00adposed in the past [1, 2, 13] do either not scale to large-scale systems or make compromises \nin their analysis power to achieve scalability. In this paper, we bridge this gap by proposing a dynamic \npro\u00adgram analysis that detects strictly more problems than previous approaches and scales to relevant \nreal-world applications. In sum\u00admary, we make the following contributions:  Our analysis simulates \nreal arithmetic by performing each .oating-point computation in higher precision side by side to the \noriginal calculation. For every .oating-point variable in the program, we introduce a shadow variable \nin higher precision. We use those shadow values to detect catastrophic cancella\u00adtions and rounding errors. \nIn contrast to a static analysis, we do not detect all such errors but, because of the higher preci\u00adsion \nof the shadow values, we detect signi.cantly more than all previous dynamic analyses. For the sake of \nprecision, we formally describe our dynamic program analysis by means of an operational semantics.  \nIn addition to the shadow value, we collect additional data for every .oating-point operation. This information \nis used by a light-weight slicing technique which helps the programmer to reconstruct the path of operations \nalong which an error propa\u00adgated after the program run. This helps in localizing the source of inaccuracies \nthat propagated to other program points where they were actually detected.  We implemented the presented \nanalysis in the Valgrind [16] bi\u00adnary instrumentation framework. We evaluate the effectiveness of our \ntool on pathological .oating-point accuracy problems from the literature as well as on large-scale real-world \npro\u00adgrams: the SPEC CFP2006 benchmark suite of .oating-point programs, the Biochemical Algorithms Library \n(BALL), and the GNU linear programming kit (GLPK). In all investigated programs, our program detects \naccuracy problems with various consequences, ranging from unnecessarily slow convergence to meaningless \noutput. We are not aware of any previous auto\u00admated study of .oating-point accuracy problems on programs \nof that scale. Finally, we also present experiments that show the limits of our approach.  The rest \nof this paper is structured as follows: The next section summarizes the foundations of .oating-point \narithmetic relevant for this paper and gives an overview of several important classes of accuracy problems. \nSection 3 outlines the concepts of the analysis, Section 4 presents how the programmer interacts with \nthe analysis, and Section 5 discusses issues of a concrete implementation of the analysis. Finally, in \nSection 6 we present an extensive case study of our analysis. rounding can be represented by the transformation \nx . fl (x). Rounding to the nearest representable number is the default and the only rounding mode considered \nhere. In case of a tie, it is rounded to the nearest even number. The unit roundoff u, also called the \nmachine epsilon, is de.ned as the largest positive number such that fl (1 + u)=1 The unit roundoff depends \non the base \u00df and the precision p: u = \u00df/2 \u00b7 \u00df-p Let F . Rbe the set of numbers which can be exactly \nrepresented by .oating-point arithmetic. If x . Rlies in the range of F, then fl (x)= x (1 + e) |e| <u \nFor all a, b . F, the following properties hold fl (a . b)=(a . b)(1 + e) |e| . u ..{+, -, \u00d7,/} fl (a \n. b)= fl (a . b) ..{+, \u00d7} The standard requires that all basic operations are computed as if done exactly \nand rounded afterwards. This is stronger than the properties above as it requires e =0 if a . b . F. \nIn .oating\u00adpoint arithmetic, addition and multiplication are commutative but no operation is associative. \n 2.2 Error Sources The errors sources can be separated into three groups: rounding, data uncertainty, \nand truncation. Rounding errors are unavoidable due to the .nite precision. Un\u00adcertainty in the data \ncomes from the initial input or the result of a previous computation. Input data from measurements or \nestima\u00adtions is usually only accurate to a few digits. Truncation arises when a numerical algorithm approximates \na mathematical function. Many numerical methods take .nitely many terms of a Taylor-, Laurent-, or Fourier \nseries; the terms omitted constitute the trun\u00adcation error. A truncation error can only be analyzed with \nknowl\u00adedge about the function the algorithm computes. Therefore, a tool working on the program as is \ncan only track rounding errors and uncertainty from previous computations. Usually, the discrepancy between \na .oating-point number and the corresponding exact number is measured as a relative error. Throughout \nthis paper, the following de.nition is used exact value - approximate value relative error =exact value \n(0.1)=0.0001100 A .oating-point number has the form 10 2 x = \u00b1s \u00d7 \u00dfe The relative error of 0.1 in single \nprecision is close to the unit 2. Foundations We brie.y repeat the necessary foundations of IEEE 754 \n.oating\u00adpoint arithmetic in this section. We follow the notation and termi\u00adnology of Goldberg [8] and \nHigham [10]. The interested reader is referred to these publications for more detail. 2.1 The IEEE 754 \nStandard Note that if the exact value is 0, the relative error is 8. 2.2.1 Error Accumulation A good \nexample for a number which cannot be exactly represented in .oating-point arithmetic is the decimal number \n0.1, which is periodical in the binary system  where for each value, the sign, the signi.cant s (also \ncalled man\u00ad tissa), and the exponent e are stored. IEEE 754-1985, the arguably most important .oating \npoint standard, uses \u00df =2. The preci\u00ad  roundoff of 2-24 ( 5.96 \u00d7 10-8)(0.1)- (0.1) 10 sp 5.94 \u00d7 10 \n-8 sion is the maximum number of digits which can be represented (0.1)10 with the signi.cant. IEEE 754-1985 \nde.nes four precisions: single, single-extended, double, and double-extended. Here we only con\u00adsider \nsingle and double precision since these are the most common in today s programming languages. As the \nrepresentation uses only a .nite number of bits, an error due to the approximation is unavoidable. Denoting \nthe evaluation of an expression in .oating-point arithmetic by the function fl, The error introduced \nby an approximate representation of a con\u00adstant is in the same range as the rounding error after a computa\u00adtion. \nThis error cannot be avoided because a representation of a real number with a .nite number of bits can \nnever be exact for all numbers. Figure 1 shows a C program with large error accumulation. In the end, \nthe value of time is 1999.6588. Thus, time has a  float time = 0.0f; int i; for (i = 0; i < 20000; i++) \n{ time += 0.1f; } Figure 1. C program with large error accumulation relative error of 1.7 \u00d7 10-4. This \nis signi.cantly higher than the error of the constant 0.1f. If the same program is run with time in double \nprecision and the constant is still represented in single precision (i.e. only replacing float with double) \nthe relative error of time decreases to 1.5 \u00d7 10-8 . If, in addition to the double precision variable \ntime, the constant is also represented in double precision (i.e. removing the f behind the constant) \nthe relative error is reduced to 3.6 \u00d7 10-13 . In general, an error introduced by a constant limits the \naccuracy of the result. The actual in.uence depends on the precision and the numerical stability of the \nalgorithm. An algorithm is numerically stable if the in.uence of rounding errors to the result is provably \nal\u00adways small. Thus, numerical stability is a property of an algorithm. A similar property, called the \ncondition, describes the problem the algorithm tries to solve. The condition is independent from the \nal\u00adgorithm and describes the dependency between the input and the output of a problem. The condition \nnumber is a measurement for the asymptotically worst case of how strongly changes in the input can in.uence \nthe output. If small changes in the input lead to large changes in the output, the condition number is \nlarge. Thus, it is eas\u00adier to design algorithms for problems with low condition numbers. In the example \nabove, it does not matter if the error comes from a constant, because always adding the same number with \nan error tends to accumulate the error. Large errors usually occur due to the insidious growth of just \na few rounding errors and it is hard to .nd their origins.  2.2.2 Insuf.cient Precision The precision \nused in a program is usually a trade-off between per\u00adformance and suf.cient accuracy. The example program \nin Figure 2 uses a constant which is slightly smaller than the unit roundoff. /* float unit roundoff \nis 0.00000006f */ float e = 0.00000005f; float sum = 1.0f; int i; for (i = 0; i <5; i++) { sum += e; \n} Figure 2. C program with an error due to insuf.cient precision In the end, the value of sum is 1.0. \nThus, the relative error of sum is 2.5 \u00d7 10-7. Any precision slightly higher than single precision leads \nto a correct result. Insuf.cient precision is usually hard to discover manually because the error introduced \nby a single operation is small. However, large errors can occur due to error accumulation. Often, double \nprecision is thus used to alleviate these problems. However, while double precision indeed suf.ces for \nmany application scenarios, it can still be insuf.cient, depending on the details of the problem. 2.2.3 \nCatastrophic Cancellation If two nearby quantities are subtracted, the most signi.cant digits match and \ncancel each other. This effect is called cancellation and can be catastrophic or benign. A cancellation \nis catastrophic if at least one operand is subject to rounding. Otherwise, if both quan\u00adtities are exactly \nknown, the cancellation is benign. A cancellation can occur when subtracting or adding two numbers because \nboth operands can be positive or negative. An example for a cancella\u00adtion is the following computation \n33 0 1.002 \u00d7 10- 1.000 \u00d7 10=2.000 \u00d7 10 This cancellation is catastrophic if the digit 2 in the .rst operand \nis the result of an error and benign otherwise. An operation with a cancellation can expose an error \nintroduced by an earlier computa\u00adtion, but the same operation can be exact if no error has been intro\u00adduced \nbefore. Some formulas can be rearranged to replace a catas\u00adtrophic cancellation with a benign one, but \nonly within the scope of the particular formula, i.e., this cannot guard against rounding errors present \nin the input. One example is the expression x 2 - y 2 , where x 2 and y 2 are likely to be subject to \nrounding. In this case, a more accurate formula would be (x - y)(x + y), because here, the subtraction \nis done directly on the variables x and y. Assum\u00ading that x and y are exactly known, a possible cancellation \nis thus guaranteed to be benign in the second variant. The number of canceled digits can be calculated \nwith the fol\u00adlowing formula max{exponent(op1), exponent(op2)}- exponent(res) where op1 and op2 are the \noperands, and res the result. A cancel\u00adlation has happened if the number of canceled digits is greater \nthan zero. To conclude, for the analysis of .oating-point software, it is crucial to decide whether a \ncancellation is catastrophic or benign.   2.3 Cancellation of Rounding Errors Inaccurate intermediate \nresults can still lead to an accurate .nal result. Therefore, not all inaccurate values indicate a problem. \nOne example from Higham [10] for a calculation where more inaccurate intermediate results lead to a more \naccurate .nal re\u00adsult is the calculation of (e x - 1) /x with a unit roundoff u = 2-24 . To obtain a \nmore accurate result, it is better to com\u00adpute (e x - 1) / ln (e x). For x =9 \u00d7 10-8 the exact result \nis 1.00000005. The following calculations show that the second for\u00admula has more inaccurate intermediate \nresults but the .nal result has a relative error that is 1.92 \u00d7 107 times lower than the error of the \n.rst formula ()( ) e x-11.19209290\u00d710-7 fl = fl =1.32454766 x 9.00000000\u00d710-8 ()( ) e x-11.19109290\u00d710-7 \nfl = fl =1.00000006 ln(ex)1.19209282\u00d710-7  2.4 Summary The dynamic program analysis we present in this \npaper addresses all issues discussed in this section: Error accumulation and insuf.cient precision is \ndetected by our analysis by following values through the whole program with a side-by-side computation \nin higher precision. In the end, the accumulation and insuf.cient precision can be detected by comparing \nthe original and the shadow value. Of course, our analysis is not correct in the sense that we pretend \nthat the shadow values in higher precision are perfect substitutes for the exact real values. However, \nour experience shows that side-by\u00adside calculation in higher precision helps in detecting severe accuracy \nproblems.  Our analysis detects catastrophic cancellations by calculating the canceled bits and the \nbits that are still exact for every .oating-point addition and subtraction. This is explained in Section \n3.1.  The cancellation of rounding errors can lead to false positives in intermediate results. Our analysis \naddresses this problem by determining variables that are likely the .nal result and thus   helps the \nuser to determine if the .nal result is affected. Error traces created by the analysis can then reveal \nthat the inaccurate intermediate results led to an accurate .nal result. The error trace is explained \nin Section 3.2. 3. The Analysis In the following, we describe a dynamic program analysis which assists \nthe programmer in .nding accuracy problems. The analy\u00adsis main ingredient is a side-by-side computation \ndone with an ar\u00adbitrary but .xed precision1 which is higher than the precision used in the client program. \nThe analysis tracks two kinds of information: For every original value that is computed by the program \n(this com\u00adprises variables which are written to memory as well as temporary results kept in registers), \nit stores a shadow value which consists of several components among which is a counterpart of the orig\u00adinal \nvalue in higher precision. Additionally, we compute analysis information for every executed .oating-point \ninstruction. Our anal\u00adysis essentially tracks the difference of a .oating-point value in the client program \nand its corresponding shadow value. If this differ\u00adence becomes too large, it is likely that the client \nprogram suffers from accuracy problems. The exact composition of the analysis in\u00adformation is shown in \nthe .rst section of Figure 6. We use two different sets of information (per variable, per in\u00adstruction) \nbecause they are both relevant: Accuracy problems usu\u00adally materialize as inaccurate contents of variables. \nFrom there, it is helpful to .nd the instructions that cause the problem and poten\u00adtially reconstruct \nhow the error propagated to that variable. An important detail is that the side-by-side computation in \nour analysis does not affect the semantics of the program: For example, a comparison which decides whether \na branch is taken can have a different outcome in higher precision than in the original precision. We \nwould detect the accuracy problem but still follow the branch based on the result of the computation \nin original precision. As a consequence, the result of the analysis is not equal to the result of the \nprogram as if it would have been executed in higher precision. This is the desired behavior, because \nwe want to track down prob\u00adlems in the program running in the original precision. The results of the \nside-by-side computation are leveraged in two ways (further detailed in the following sections): By treating \nthe shadow values as a potentially better approximation to the exact real value, we compute the relative \nerror and the cancellation bad\u00adness of every instruction. The latter detects catastrophic cancella\u00adtions \nand indicates how much more precise a value would have to be for the cancellation to be benign (see Section \n3.1). Furthermore, we use the shadow values to calculate the relative error on every instruction and \noriginal value. This information can be used to .nd the cause of the accuracy problem (see Section 3.2). \nFinally, we want to stress that a dynamic program analysis is never complete. The capability of the presented \nanalysis to detect .oating-point accuracy problems is based on the assumption that the shadow values \nare a substitute for the real exact values. 3.1 Cancellation Detection Because cancellations occur often \nand benign cancellations have no impact on the accuracy, it is important to detect if a cancella\u00adtion \nis catastrophic or benign. To this end, we de.ne cancellation badness as an indicator for catastrophic \ncancellations. The cancel\u00adlation badness relates the number of bits canceled by an operation to the bits \nthat where exact before the operation. The exact bits of a .oating-point variable v are determined with \nrespect to the exact value v which is in our analysis stored in the shadow value of v as 1 The precision \ncan be speci.ed by the user before the analysis starts. a .oating-point number in higher precision2: \n. . p if v = v ebits( v, v) := if exponent( 0 v)= exponent(v). ebitsI( v, v) otherwise with ebitsI( v, \nv) := min {p, | exponent( v) - exponent( v - v)|} I Consider an instruction v. v1 . v2. The number of \nbits canceled by executing the instruction is calculated by cbits := max{exponent(v1), exponent(v2)}- \nexponent(vI) The cancellation badness cbad now relates the least number of exact bits of the operands \nto the number of canceled bits: cbad := max{0, 1+ cbits - min{ebits( v1,v1), ebits( v2,v2)}} If the badness \nis greater than zero, a catastrophic cancellation hap\u00adpened. The value of cbad itself indicates how much \nmore precise the operands would have to be for the cancellation to not be catas\u00adtrophic. If the badness \nis 0, the cancellation was benign. Consider the following example: 1.379 - 1.375 = 0.004 Here, three \ndigits are canceled. Assuming that the .rst operand has four exact digits and the second operand has \nthree exact digits, the cancellation badness is one. Therefore, the cancellation is catas\u00adtrophic and \nthe result is completely inaccurate. If the same calcu\u00adlation is done with a second operand with four \nexact digits, the cancellation badness is zero and thus the cancellation is benign. Assuming that the \nexact fourth digit is 8, the accurate result 1.379 - 1.378 = 0.001 has only the exponent in common with \nthe inaccurate result above. For every instruction the sum of the cancellation badness of ev\u00adery execution \nof that instruction (.eld scb in an instruction s analy\u00adsis information in Figure 6) and the maximum \ncancellation badness (mcb) that occurred over all executions of that instruction are main\u00adtained. Each \nshadow value tracks the maximum cancellation bad\u00adness that occurred in the corresponding original value \n(.eld mcb in an shadow value s analysis information in Figure 6) and a pointer to the instruction where \nthis maximum occurred (mcb src).  3.2 Finding Operations that Cause Errors Consider the pathological \nexample in Figure 3. Na\u00a8ively, we would 1 float e = 0.00000006f; 2 float x = 0.5f; 3 float y = 1.0f + \nx; 4 float more = y + e; 5 float diff e = more -y; 6 float diff 0 = diff e -e; 7 float zero = diff 0 \n+ diff 0; 8 float result = 2 * zero; Figure 3. A program with insuf.cient precision and catastrophic \ncancellation. expect result to be 0.0 after executing line 8. However, the oper\u00adations in line 5 and \n6 cause catastrophic cancellations which render the computed values inexact. This error propagates into \nvariable result which is not 0.0 at the end. However, the operation on which result depends, the addition \nin line 7, does not contribute to this inaccuracy. All exact bits of diff 0 are preserved. Hence, the \nprogrammer here wants to .nd the operations in line 5 and 6. Of 2 In the following, the tilde stands \nfor operations and numbers (and sets thereof) in higher precision.  course, in large programs, the operations \ncontributing to the prob\u00adlem can be spread over several modules. Thus, it is hard to pinpoint the problem \nby just looking at the source code. A dynamic slice (a data dependence graph of the execution) of the \nexecution would help the programmer to locate this problem. However, recording the trace of the whole \nprogram execution and computing the relative errors afterwards can drastically slow down the analysis, \nbecause the result of every operation has to be written to disk. Instead, we pursue a more light-weight \napproach: For every instruction in the program, we maintain two values: The sum of the relative error \nof every execution (sre in Figure 6) of that instruction and the maximum relative error (mre) that occurred \n1.32 \u00d7 10 -2 over all executions of that instruction. Please note that for a shadow value of zero, the \nconcept of a relative deviation becomes meaningless. To guard against such cases, our implementation \nallows the user to specify a threshold t0 such that |shadow value - original value|relative error = max \n(|shadow value| , |t0|) The sum over all relative errors is used to compute an average of the relative \nerrors. If an instruction has a small average relative error, it is usually unlikely that this instruction \nis involved in signif\u00adicant accuracy problems. Hence, this value is used when the anal\u00adysis results are \npresented to the user: Instructions with a compara\u00adbly high average relative error are listed before \ninstructions with a lower average relative error. The maximum relative error is used to enable a light-weight \nslicing approach: Whenever the maximum relative error of an in\u00adstructions changes, i.e. the instruction \nis executed again with a rel\u00adative error that is higher than all the relative errors seen in previ\u00adous \nexecutions, we store the instructions which computed the argu\u00adments to that instruction (.eld mre src \nin an instruction s analysis information in Figure 6). In this way, we obtain insight into the ori\u00adgins \nof the values causing the maximum error. After the analysis run, this information is used to reconstruct \na slice along which the maximum error propagated. Add32F0x4 (bad.c:4) 3.95 \u00d7 10 -8 Sub32F0x4 (bad.c:5) \n9.87 \u00d7 10 -1 Sub32F0x4 (bad.c:6) 1.32 \u00d7 10 -2 Add32F0x4 (bad.c:7) 0 0 Add32F0x4 (bad.c:8) Figure 4. \nError trace for the variable result in the truncated C program on the left which suffers from insuf.cient \nprecision in line 4 and catastrophic cancellations in lines 5 and 6. The edges are labeled with the introduced \nerror of the operation they .ow out of. To reveal the origins of an error, one has to .nd the operations \nwhich are the cause. Therefore, after the analysis run, the contri\u00adbution of an operation to the maximum \nrelative error is calculated. The contribution is called the introduced error of an operation. The introduced \nerror is calculated as the smallest difference of the max\u00adimum error of the operation itself and the \nmaximum error of one of the operations which is a direct predecessor in the error trace. Special cases \noccur when an operation is its own predecessor or when an operation has no predecessor. In these cases, \nthe maxi\u00admum error of the operation itself is taken instead of the difference. Note that the introduced \nerror can be negative, because errors can be canceled. Figure 4 shows an error trace for the example \nabove. The edges in the graph are labeled with the introduced error of the operation producing the value \nthat .ows on the edge. One can directly see that the subtraction in line 5 introduces a large error of \n9.87 \u00d7 10-1 . Note that the error trace is not necessarily a part of the dynamic data-dependence graph \nof the program run. This is because the maximum relative errors of the instructions in the error trace \nmight be observed at different instances of the instructions. Hence, the data .ow suggested in the error \ntrace might not have taken place in the execution of the program, since we only update the source of \nthe error when the maximum relative error changes (see Figure 6). However, in our experiments we made \nthe experience that the error trace very often resembles the actual data .ow and is thus very helpful \nfor getting a .rst impression on how an error propagates through a program.  3.3 Operational Semantics \nIn this section, we formally describe our analysis using a structural operational semantics. We base \nour formalization on Valgrind s VEX intermediate representation language, mainly because the tool implementing \nour analysis is implemented in Valgrind. However, the subset of VEX we use here is so generic that it \ndoes not affect the applicability of our analysis in other frameworks. Figure 5 shows the excerpt of \nthe VEX grammar relevant for our analysis. const ::= constant temp ::= temporary variable exp ::= temp \n| const stmt ::= temp := load(exp) | store(exp, exp) | temp := get(exp) | put(exp, exp) | temp := exp \n| tmp := .u exp | temp := exp .b exp program ::= stmt* Figure 5. Simpli.ed grammar of the VEX intermediate \nrepresen\u00adtation used in Valgrind. Load and store describe memory accesses whereas put and get are for \naccessing registers. The VEX intermediate representation consists of a sequence of numbered statements. \nValgrind .attens the intermediate represen\u00adtation to simplify the instrumentation. As a result, an expression \nis only a temporary variable or a constant and can not be a tree of expressions. Statements consist of \nmemory and register writes, and assignments. The right side of an assignment can be a memory read, register \nread, temporary variable, constant, unary operation, or binary operation. Figure 6 summarizes the formalization \nof our analysis. Its .rst section shows the analysis information for instructions and vari\u00adables. Elements \nof analysis information are stored in various maps: \u00b5t (\u00b5m) implements the store that maps temporaries \n(memory ad\u00addresses) to original values. .t (.m) implements the store that maps temporaries (memory addresses) \nto shadow values. O maps addresses of instructions to elements of instruction analysis infor\u00admation. \nThe notation A[x . y] stands for .w.w = x ? y : A(w). In the inference rule section, we only show the \nrules for load, store, and the binary operation. Each inference rule is of the form: side-by-side computation \noriginal computation (con.guration) instruction . (con.gurationI) instructionI The rules for put and \nget are purely technical and do mostly resem\u00adble the ones for load and store. Most interesting is the \nrule for the binary operator and the computation of the various analysis infor\u00admation components outlined \nin the last section of the .gure. For  Analysis information tuple I for an instruction: Name Domain \nDescription sre <F Sum of relative errors mre <F Maximum relative error mre src N\u00d7 N Pair of instruction \naddresses that point to the instructions that computed the operands that caused the maximum relative \nerror scb N Sum of cancellation badnesses mcb N Maximum cancellation badness Shadow value tuple S for \na variable: Name Domain Description val <F Value in higher precision td N Depth of the computation tree \npc N Address of the instruction that wrote to mcb N the variable lastly Maximum cancellation badness \nmcb src N Instruction that caused maximum can\u00ad cellation badness S : Addr . Instr Maps an address to \nan instruction \u00b5m : Addr . F Maps an address to an original value \u00b5t : Temp . F Maps a temporary to an \noriginal value O : Addr . I Maps an address to an instruction analysis information .m : Addr . S Maps \nan address to a shadow value .t : Temp . S Maps a temporary to a shadow value s =.m [a].t I =.t [n I \n. s] a = \u00b5t [n] v = \u00b5m [a] \u00b5t I = \u00b5t [n I . v] . = S[pc + 1] F-LOAD I II (\u00b5t,\u00b5m, .t, .m, S, pc) n := \nload (n) (\u00b5t ,\u00b5m, .t , .m, S, pc +1) . s =.t [n2].m I =.m [a . s] a = \u00b5t [n1] v = \u00b5t [n2] \u00b5m I = \u00b5m [a \n. v] . = S[pc + 1] F-STORE (\u00b5t,\u00b5m, .t, .m, S, pc) store (n1,n2) (\u00b5t,\u00b5m I , .t, .m I , S, pc +1) . .t \nI =.t [n I . s I]OI = O[pc . o I] v1 = \u00b5t [n1] v2 = \u00b5t [n2] v I = v1 . v2 \u00b5t I = \u00b5t [n I . v I] . = S[pc \n+ 1] F-BINOP I II (\u00b5t,\u00b5m, .t, .m, S, pc) n := n1 . n2 (\u00b5t ,\u00b5m, .t , .m, S, pc +1) . Abbreviations used: \nrel. error rerr := |(s I .val - v I) /d|d := s I .val =0? s I .val : t0 lookup(n, v) := .w. (n, w) . \n.t ?.t[n]: init init := (v, 0, -, 0, -) Analysis information update for an instruction: o = O[pc] I o.sre \n= o.sre + rerr o I .mre = max{o.mre, rerr} o I .mre src = rerr > o.mre ?(s1.pc,s2.pc) : o.mre src  \nI o.scb = o.scb + cbad o I .mcb = max{o.mcb, cbad} Shadow value of the result of a binary operator: s1 \ns2 I s.pc I s.val I s.td I s.mcb I s.mcb src = lookup(n1,v1) = lookup(n2,v2) = pc = s1.val .<s2.val = \n1+max{s1.td,s2.td}= max{s.mcb, cbad}= cbad > s.mcb ? pc : s.mcb src Figure 6. Structural operational \nsemantics for the original and the side-by-side computation the sake of brevity, we also omit the unary \noperator. Conceptually, it equals the binary operator. However, computations that involve both operands \nof a binary operator (such as the calculation of the maximum cancellation badness) have to be adapted \naccordingly. 4. User Interface Our tool can automatically collect information for the whole execu\u00adtion \nof a client program. Thereby all .oating-point instructions and variables are analyzed and variables \nare determined that are likely the .nal result by the depth of the execution tree leading to them (.eld \ntd in an shadow value s analysis information in Figure 6). In addition, our implementation enables the \nuser to perform a more targeted analysis by adding client requests to the source code of the analyzed \nprogram; client request are instructions that are only interpreted by our analysis and ignored in an \nexecution without Valgrind. With the client request, the user can exclude parts of the pro\u00adgram from \nthe analysis or can run the analysis only on speci.ed parts. This not only allows to specify which parts \nare to be an\u00adalyzed, but also grants access to the information gathered by the analysis during execution. \nThus, via client requests, the relative er\u00adror of a variable can be accessed and the error trace of a \nvariable can be constructed at every point in the program where the variable is present. Furthermore, \nan original value can be replaced with its shadow value, so that the user can correct a value without \n.xing all the code leading to this value in the client program. Thus, the user is given a simple way \nto check whether the correction of a value in.uences the result of a program. Another powerful instrument \nare stages (originally proposed by Kahan [12]). The basic idea behind this concept is to add break points, \ncalled stages , into a program, to observe the errors at each break point, and to complain if two consecutive \nstages differ too much. This is a semi-automatic technique that assumes that the programmer has an idea \nwhere interesting stages are located. In practice, interesting stages almost always correspond to loop \niterations. Using our tool, the programmer can de.ne multiple sets of stages that are analyzed independently. \nAn example where stages prove to be useful is presented in Section 6.1. This example shows that stages \nhelp .nding accuracy problems even if the shadow values on their own fail to do so because of their .nite \nprecision. 5. Implementation Details The analysis presented in Section 3 instruments every .oating\u00adpoint \nmachine instruction in the program. However, our early ex\u00adperiments showed that this is not suf.cient. \nThe basic reason is that on the assembly level, strict typing is no longer present. As a result, the \ntypes used by Valgrind do not necessarily match the type used in the source code of the client program. \nAn example is shown in Figure 7. There, the compiler chose to load the bit pattern represent\u00ading a .oating-point \nconstant into an integer register whose contents is then stored to memory. Later on, that memory cell \nis read into an SSE register and a .oating-point operation is performed on the loaded value. This means \nthat the type with which constants are stored in memory does not have to match the type with which they \nare loaded. A na\u00a8ive implementation would not only instrument the .oating-point but also all integer \ninstructions. However, this would result in a slow and error-prone implementation with a large mem\u00adory \noverhead. Instead, we start the instrumentation as late as possible. As a result, a .oating-point variable \nis tracked from the point where it  movl $0x3356bf95, %eax movl $0x3356bf95, %eax movl %eax, -4(%rbp) \nmovl %eax, -4(%rbp) ... ... addss -4(%rbp), %xmm0 fadds -4(%rbp) Figure 7. Extracts from the assembly \ncode for a SSE addition (left) and a x87 FPU addition (right) of two .oating-point numbers. There are \nno indications that the loaded values are used in .oating\u00adpoint instructions later on. is the result \nof a .oating-point operation. For a constant or a value given as an input to the program there is no \nneed for a shadow value as the shadow value would be exactly the same. In the operational semantics in \nFigure 6 this is expressed by the function lookup. Furthermore, we use copy propagation to avoid instrumentation \nof instructions that do not affect shadow values such as type cast operations. However, all reads and \nwrites to the main memory have to be instrumented as in each such operation a .oating-point value could \nbe involved. This scheme has the advantage that it does not only simplify instrumentation and increases \nthe ef.ciency of the analysis but it also makes the analysis more robust. Floating-point operations which \nare not yet instrumented can only lead to undetected errors but do not affect the correctness of our \nanalysis. This is due to the fallback to the original value if no shadow value exits. 6. Evaluation The \ncapabilities and the performance of the tool are evaluated in this section. Unless stated otherwise, \nall analyses are performed on an x86-64 system with a shadow value precision of 120 bit. 6.1 Wrong Limit \nof Convergent Sequence The sequence un converges to 6, but if computed with any .nite precision it converges \nto 100. The sequence is de.ned as . . 2 if n =0 un = -4 if n =1 . 1130 3000 111 - + if n> 1 un-1 un-1un-2 \nThis sequence has been analyzed by Kahan [12] and Muller et al. [15]. The strange behavior can be explained \nwith the solution of the recurrence + \u00df \u00b7 6n+1 + . \u00b7 5n+1a \u00b7 100n+1 un = a \u00b7 100n + \u00df \u00b7 6n + . \u00b7 5n where \na, \u00df, and . depend on u0 and u1. For u0 =2 and u1 = -4 one gets a =0, \u00df = -3, and . =4. However, if computed \nwith .nite precision, roundoff errors cannot be avoided and result in an a that is not exactly zero. \nThis leads to the different limits. double u=2, v= -4, w; for (int i = 3; i <= 100; i++) { FPDEBUG_BEGIN \n_STAGE (0); w = 111. -1130./v + 3000./(v*u); u = v; v = w; printf(\"u%d = %1.17g\\n\", i, v); FPDEBUG_ \nEND _STAGE (0); } FPDEBUG_PRINT_ERROR(&#38;\"u _100\", &#38;v); Figure 8. C code which tries to compute \nthe sequence un Therefore, it is clear that after analyzing the computation of un in Figure 8, no error \ncan be observed because the shadow value is also 100. However, both values converge at a different rate \nwhich can be observed with stages. The stage report in Figure 9 reveals that the relative error grows \nfaster than linear in the iterations 4, 18 and in all iterations in between. Thus, the stage concept \nallows the detection of this strange behavior which would otherwise have gone unnoticed. Stage 0: (0) \n0x7ff000200 (15) executions: [4, 18] origin: 0x40089A Figure 9. The stage report for the code in Listing \n8 reveals prob\u00adlems starting in iteration 4 and ending in iteration 18. As 15 prob\u00adlems are reported, \nall iterations between 4 and 18 are affected. 6.2 Walker s Floating-Point Benchmarks Walker [19] provides \ntwo .oating-point benchmarks called fbench and ffbench. The program fbench is an implementation of a \ncomplete optical design ray tracing algorithm. The largest relative error discovered is 9 \u00d7 10-13. Thus, \nthe algorithm used in fbench does not suffer from accuracy problems for the sample data. The program \ncomes with its own version of the trigonometric functions used, but there is no difference in stability \nif the program is run with the trigonometric functions from the C library. The second benchmark, ffbench, \nperforms a two-dimensional Fast Fourier transform. Only 4 out of 131,089 shadow values have an error. \nThe largest relative error is 9.6 \u00d7 10-2. Two additions produce inaccuracies due to catastrophic cancellation. \nHowever, the inaccuracy stays local and has no in.uence on the .nal result. A program where more than \n99% of the variables are accurate is rare. Even considering that ffbench computes and then inverts a \nFast Fourier transform, so that the program should result in the same value as the initial one, it requires \nthat no signi.cant digits are lost in between. Therefore, it depends on the numerical stability of the \nalgorithm and on the initial values.  6.3 Analysis of expf Let us now give an example of the limitations \nof our approach: The function expf from the GNU C Library computes e x in single pre\u00adcision. The implementation \nof the function analyzed here is from sysdeps/ieee754/flt-32/e expf.c (GNU C Library 2.12.2). Executing \nthe function expf in higher precision leads to more in\u00adaccurate results because the algorithm used is \ndesigned for single precision and uses precomputed values. The function is based on Gal s accurate tables \nmethod [6]. First, n, t, and y are computed such that x can be expressed as t x = n \u00b7 ln(2)+ +delta[t]+ \ny 512 and then e x can be approximated as t x 2n 512 +delta[t] ee \u00b7 (1 + p(y + delta [t]+ n \u00b7 ln (2))) \nwhere p is a second-degree polynomial approximating e x - 1 and delta [t] and e t +delta[t] are obtained \nfrom tables. 512 The problem arises when computing n and t. The value of n is computed with 1 n = x \u00b7 \n+ THREEp22 - THREEp22 ln (2) where THREEp22 = 12582912.0 is a single precision constant and the operations \nare performed with single precision.  The remaining part dx of x is computed with dx = x - n \u00b7 ln (2) \nOne would expect n \u00b7 ln (2) to be equal to x. But for x =0.09 the value of n is 0 because the large value \nof THREEp22 leaves no signi.cant digits of x when added. This results in dx = x =0.09 but the shadow \nvalue is 1.27 \u00d7 10-9. The shadow value is not exactly 0 because 1 \u00b7 ln(2) is not exactly 1 as the .rst \nfactor is a ln(2) single precision constant and the second factor a double precision one. The value of \nt is computed with t = dx + THREEp42 - THREEp42 512 where THREEp42 = 13194139533312.0 is a single precision \nconstant but the operations are performed with double precision. With the previous value of dx, one gets \nt x for the original 512 t value but 512 =0 for the shadow value. Thus, t converted to an integer is \n46 in the original program but 0 in the side-by-side computation. Because of these differences the values \ntaken from the precom\u00adputed table do not .t for the shadow value. For x =0.09 this re\u00adsults in a relative \nerror between the original and the shadow value of 1.56 \u00d7 10-4. However, the original value only has \na relative er\u00adror of 2.18 \u00d7 10-8 compared to the exact result. This relative error is smaller than the \nsingle precision unit roundoff and thus cannot be more accurate.  6.4 SPECfp2006 SPECfp2006 is the .oating-point \ncomponent of the SPEC CPU2006 benchmark suite [3]. Most of the .oating-point benchmarks from SPECfp2006 \ncan be analyzed with our tool. The only exceptions are zeusmp which fails because Valgrind does not support \ndata seg\u00adments bigger than 1 GB, and dealII which does not terminate be\u00adcause it relies on the internal \n80-bit representation of the x87 FPU. Valgrind however behaves IEEE 754 compliant and only works with \n64 bits for double precision even if it is an x87 FPU instruc\u00adtion. The reasons for the problems were \ndiscovered by Weaver [21]. SPECfp2006 comes with three data sets for each benchmark: train, test, and \nref. The ref data sets are the largest. Because of the slowdown caused by the instrumentation overhead, \nwe focus on the test dataset. All benchmarks were performed on a quad-core AMD Opteron processor with \n2.5 GHz and 64 GB of RAM. Figure 10 shows the results for all SPECfp2006 benchmarks we executed. The \nside-by\u00adside computation ran with a precision of 120 bit. Benchmark Original Analyzed Slowdown bwaves \n47.5 s 7920 s 167 x gamess 0.7 s 381 s 544 x milc 30.9 s 7860 s 224 x gromacs 2.1 s 991 s 472 x cactusADM \n4.7 s 4777 s 1016 x leslie3d 59.8 s 17467 s 292 x namd 19.8 s 18952 s 957 x soplex 0.027 s 5.0 s 185 \nx povray 0.9 s 400.0 s 444 x calculix 0.07 s 17.1 s 244 x GemsFDTD 5.5 s 1146 s 208 x tonto 1.26 s 404 \ns 321 x lbm 9.55 s 2893 s 303 x wrf 7.68 s 2623 s 342 x sphinx3 4.41 s 938 s 213 x Figure 10. Results \nof the SPECfp2006 benchmarks with the test data sets  6.4.1 Analysis of CalculiX During our benchmarking, \nwe observed a potential loss of accuracy in the benchmark CalculiX. Figure 11 shows the .rst entry of \nthe mean error .le which is sorted by the introduced error. This entry led to a deeper analysis of DVdot, \na function in SPOOLES, a linear equation solver used by CalculiX, to compute the dot product of two vectors. \nAs it turns out, it is not the multiplication which causes large inaccuracies, but rather the addition \nin this line. This shows that a globally computed introduced error is not a perfect indicator for the \nreal origin of the problem. Computed only with knowledge about the function DVdot, the introduced errors \nreveal the real origin. DVdot (Utilities_DV.c:245) Mul64F0x2 (116,010) avg error: 1.70635239241881 * \n10^1 max error: 1.55742240304371 * 10^6 cancellation badness -max: 0, avg: 0.00 introduced error (max \npath): 1.55.. * 10^6 Figure 11. Information about the operation with the largest intro\u00adduced error in \nCalculiX. The output contains information about the function name, the place in the source code, the \nVEX IR operation name, the execution count, the average and the maximum relative error and average and \nmaximum cancellation badness of all execu\u00adtions of the operation in comparison to the side-by-side computa\u00adtion, \nand the introduced error computed with the preceding opera\u00adtions. After spotting a potential problem, \nwe manually modi.ed the function DVdot in SPOOLES to print out input causing large in\u00adaccuracies and \nto see if the insertion of a more accurate value in\u00ad.uences the .nal result (see in Figure 12). The reset \ncauses the deletion of all shadow values and thus, no previous inaccuracies in\u00ad.uence the observation. \nIf one starts the tool with the analysis dis\u00adabled, then the begin and end client requests cause that \nonly DVdot is observed and one obtains a good output of the mean errors of the operations in DVdot. After \nthe computation of sum, the error of the variable is checked against the error bound of 10-2. If the \nerror is greater, the error and the content of the arrays are printed. At the end of the function, the \nvariable sum is set to the shadow value. As the shadow value provides a signi.cantly better result, this \nis a correction of the value of sum. double DVdot(int size , double y[], double x[]) { FPDEBUG_ RESET \n(); FPDEBUG_ BEGIN (); double sum = 0.0; for (int i= 0; i < size; i++) { sum += y[i] * x[i]; } double \nerrorBound = 1e-2; if (FPDEBUG_ERROR_ GREATER(&#38;sum, &#38;errorBound)) { FPDEBUG_PRINT_ERROR(&#38;\"sum\", \n&#38;sum); for (int j= 0; j < size; j++) { printf(\"x[%d] = %.20g\\n\", j, x[j]); printf(\"y[%d] = %.20g\\n\", \nj, y[j]); } } FPDEBUG_INSERT_SHADOW (&#38;sum); FPDEBUG_END (); return sum; } Figure 12. Shortened DVdot \nfunction with error detection and correction During a run with the provided test data set, the function \nDVdot produces a relative error greater than 10-2 in 28 cases. In three of these cases the relative error \nis even greater than 10-1 . This means that in the worst case, no digit is correct. In that case, the \noriginal and the shadow value differ by a factor of nearly 1.76.  The error correction in.uences 8.5% \nof the .oating-point num\u00adbers in the output of CalculiX. Some of the numbers differ in every digit. The \ntest data set describes a cantilever beam under shear forces. The correction does not affect the computed \nstresses but the computed displacements. However, all of the in.uenced displace\u00adments are smaller than \n10-10; therefore, the in.uence may be neg\u00adligible for this simulation. Nevertheless, one sees that the \nna\u00a8ive im\u00adplementation of an inconspicuous mathematical function can have an impact on the accuracy of \na whole computation. The whole mean error .le, sorted by the introduced error, is shown in Figure 13. \nThe multiplication only has operands which are seen as exact and thus only produces a relative error \nsmaller than the double precision unit roundoff of 2-53 ( 1.11 \u00d7 10-16). Whereas the addition receives \nthe results of the multiplications as an input and produces large errors due to catastrophic cancellation. \nDVdot (Utilities_DV.c:248) Add64F0x2 (116,010) avg error: 2.72694149800805 * 10^-4 max error: 1.61405640329709 \n* 10^0 cancellation badness -max: 3, avg: 0.01 introduced error (max path): 1.61.. * 10^0 DVdot (Utilities_DV.c:248) \nMul64F0x2 (116,010) avg error: 3.96985521613801 * 10^-17 max error: 1.10661002489201 * 10^-16 cancellation \nbadness -max: 0, avg: 0.00 introduced error (max path): 1.10.. * 10^-16 Figure 13. Mean errors of .oating-point \noperations in DVdot The average relative error of the addition is large because a dif\u00adference between \nthe original and the shadow value can be intro\u00adduced by a single addition but is considered by every \naddition later on.  6.5 BALL Our original motivation for developing a dynamic analysis frame\u00adwork for \nnumerical stability came from the .eld of structural bioin\u00adformatics, where huge amounts of often noisy \ndata are routinely analyzed using highly complex numerical methods. In this section, we present our .rst \napplication of the developed tool to a subset of the Biochemical Algorithms Library (BALL) [11], a rapid \napplica\u00adtion development framework for structural bioinformatics. Our .rst experiments were performed \non version 1.4 of BALL and were focused on the optimization of molecular structures against the Amber \nforce .eld. To this end, we used the program add hydrogens which is distributed along with BALL s source \ncode. First, add hydrogens reads a molecular input structure from a PDB-.le, performs a number of required \npre-processing steps (such as normalization of atom names and types, inference of miss\u00ading bonds and \nof missing atoms). It then sets up one of currently three force .elds (we used the default, Amber96) \nand minimizes the potential energy with respect to the atomic coordinates of the newly added atoms (in \na typical use-case, these will mostly be hy\u00addrogens) using one of a number of con.gurable optimizers \n(we kept the default conjugate gradient for 500 optimization steps). A global analysis of add hydrogens \nreports several potential problematic areas in BALL s functionality for structure optimiza\u00adtion, i.e., \nin the Amber implementation and the molecular opti\u00admizer. These areas have been analyzed in-depth, but \nonly one area has an accuracy problem that veri.ably in.uences the .nal result. However, this area includes \ncalls to methods which are reported as potentially problematic but are not problematic if analyzed alone. \nThe main problem encountered is a catastrophic cancellation in a double precision subtraction. In the \nworst case, 23 bits are canceled. While this number would be relatively unproblematic at double precision \nwith 53 bit, the operands here are both results of single precision computations and thus can have at \nmost 24 exact bits. Therefore, it is not surprising that the cancellation badness is greater than zero. \nThe inaccuracy affects the computation of the conjugate gradi\u00adent computation, i.e., the search for a \ndescent direction, in comput\u00ading the minimum energy conformation of a molecular system. If the inaccuracy \nis .xed by computing the numbers used in the sub\u00adtraction in double precision, the run-time for minimizing \nthe energy of the protein 2PTH with an upper bound of iterations is reduced by 28% and much more importantly \n the result is closer to the optimum. Thus, with minimal effort in the application of our novel tool, \nwe have been able to identify a problematic situation in a frame\u00adwork that has been in use since 1996, \nhave been able to rectify it using higher precision, and have not only succeeded in improving the quality \nof the computation, but surprisingly also reduced the runtime.  6.6 GLPK Linear programming solvers \nare prone to rounding errors and there\u00adfore are interesting targets for an analysis. We performed an \nanaly\u00adsis of the GNU Linear Programming Kit (GLPK) [7] with an inte\u00adger linear problem mentioned by Neumaier \nand Shcherbina [17] min -x20 s.t. (s + 1) x1 - x2 ? s - 1, -sxi-1 +(s + 1) xi - xi+1 ? (-1)i (s + 1) \nfor i = 2 : 19, -sx18 - (3s - 1) x19 +3x20 ? - (5s - 7) , 0 xi 10 for i = 1 : 13, 0 xi 106 for i = 14 \n: 20, all xi integers, The problem consists of 20 integer variables. For s =6, the problem is solvable, \nbut several solvers are unable to .nd the solution x = (1, 2, 1, 2,..., 1, 2)T GLPK in version 4.47 reports \nthat the problem has no integer feasible solution . The important constraint is 0 xi 106 for i = 14 : \n20 For GLPK we found that the problem is solvable if 106 is replaced by a number between 2 and 21871. \nTherefore, we analyzed and compared the runs for the upper bounds 21871 and 21872. For the upper bound \n21871, our analysis reveals that -21871 is stored in a double precision value and occurs correctly in \noperations that are in.uenced by a tree of preceding operations with depth 159. As the largest tree of \npreceding operations measured has also a depth of 159, it is likely that this operation has an in.uence \non the .nal result. Increasing the upper bound by one to 21872 leads to noticeable changes in the result \nof the analysis. As before, -21872 is also stored in a double precision value and occurs in a operation \nthat is in.uenced by a tree of depth 161. Here, 161 is the maximum depth of a tree of preceding operations. \nHowever, the original value is 0 and the shadow value is exactly -21872. This shows that .oating-point \nissues detected by our analysis lead to the wrong result of GLPK. Our analysis gives more details on \nthe highly inaccurate value and automatically outputs an error trace for it, but a deeper analysis is \nbeyond the scope of this paper.  7. Related Work A manual rounding error analysis is one of the best \nways to prove the stability of algorithms and to .nd errors because it works with a mathematical model \nof .oating-point arithmetic. But because it has to be done by a human expert, it is only feasible for \nsmall programs. Surprisingly few tools have been developed to assist the pro\u00adgrammer in locating .oating-point \naccuracy problems. Recent ap\u00adproaches are based on a static analysis (e.g. Fluctuat [9]) or try to dynamically \ndetect or correct errors (e.g. automated error cor\u00adrection by Langlois [14]). A static analysis has the \nadvantage that properties can be proven, however, often only with large error bounds. Furthermore, static \napproaches suffer from the incapabil\u00adity of disambiguating heap accesses as common in static analyses. \nHence, such approaches only work well on scalar computations. Brown et al. [2] designed an analysis to \ndetermine if .oating\u00adpoint operations can be optimized by going to a lower precision or by replacing \n.oating-point with .xed-point arithmetic. This is of interest e.g. for synthesizing FPGAs. Their tool \nFloatWatch also builds on Valgrind. To employ .xed-point arithmetic, they track the overall range of \nall .oating-point values. In addition, they track the maximum difference between single and double precision \ncomputations by performing all double precision operations side by side in single precision. An et al. \n[1] present a dynamic binary analysis called FPInst based on DynInst to calculate errors side by side. \nIn contrast to our analysis, the shadow values do not contain values in higher precision but an absolute \nerror in double precision. Their functions for computing the error of instructions are derived from work \nby Dekker [4]. The formulas from Dekker enable higher precision .oating-point arithmetic on top of lower \nprecision arithmetic, but here the formulas are used to track the error throughout a program. Applied \nto the simple error accumulation shown in Section 2.2.2, the functions presented by An et al. give an \nerror that alternates and does not monotonously increase like the real error. This shows that Dekker \ns formulas can not always be used to compute error accumulation and also explains the large discrepancies \nbetween the computed error and the real error in their examples. Lam et al. [13] present a dynamic binary \nanalysis based on DynInst to .nd .oating-point operations where digits are canceled. This work can be \nseen as the one most similar to ours. Their analy\u00adsis works by tracking all cancellations and reporting \nthem. To min\u00adimize the output, the number of reports for the same instruction is decreased logarithmically. \nFinally, the number of cancellations per instruction and the average number of canceled bits per instruc\u00adtion \nare computed. However, the approach does not distinguish be\u00adtween benign and catastrophic cancellations, \nreducing its applica\u00adbility in practice. Because most cancellations are benign, the user is left with \nmany false positives. 8. Conclusions In this paper, we presented a dynamic program analysis to detect \n.oating-point accuracy problems. To our knowledge, it is the .rst dynamic analysis that detects catastrophic \ncancellations and uses a light-weight slicing approach at tracing accuracy problems through the program. \nWe implemented our analysis in the Valgrind dynamic binary instrumentation framework and exercise it \non large-scale benchmark programs in all of which we detect accuracy problems of varying severity: one \nproblem slows down the convergence of an algorithm, other ones cause catastrophic cancellation leading \nto totally insigni.cant output of the problem. We showed how our analysis helps in tracking down and \nsuccessfully .xing the causes of these issues. 9. Acknowledgments This work is partly supported by the \nIntel Visual Computing Insti\u00adtute in Saarbr\u00a8 ucken. References [1] D. An, R. Blue, M. Lam, S. Piper, \nand G. Stoker. FPInst: Floating point error analysis using dyninst, 2008. http://www.freearrow.com/downloads/.les/fpinst.pdf. \n [2] A. W. Brown, P. H. J. Kelly, and W. Luk. Pro.ling .oating point value ranges for recon.gurable implementation. \nIn Proceedings of the 1st HiPEAC Workshop on Recon.gurable Computing, pages 6 16, 2007. [3] S. P. E. \nCorporation. SPEC CPU2006 benchmarks. http://www.spec.org/cpu2006/. [4] T. J. Dekker. A .oating-point \ntechnique for extending the avail\u00adable precision. Numerische Mathematik, 18:224 242, 1971. 10.1007/BF01397083. \n [5] D. Delmas, E. Goubault, S. Putot, J. Souyris, K. Tekkal, and F. V\u00b4 edrine. Towards an industrial \nuse of FLUCTUAT on safety-critical avionics software. In FMICS 09, pages 53 69. Springer-Verlag, 2009. \n[6] S. Gal. An accurate elementary mathematical library for the IEEE .oating point standard. ACM Trans. \nMath. Softw., 17:26 45, March 1991. [7] GNU Linear Programming Kit, ver. 4.47. http://www.gnu.org/software/glpk/. \n [8] D. Goldberg. What every computer scientist should know about .oating-point arithmetic. ACM Comput. \nSurv., 23:5 48, 1991. [9] E. Goubault and S. Putot. Static analysis of .nite precision computations. \nIn VMCAI 11, pages 232 247, Berlin, Heidelberg, 2011. Springer-Verlag. [10] N. J. Higham. Accuracy and \nStability of Numerical Algorithms. Society for Industrial and Applied Mathematics, Philadelphia, PA, \nUSA, second edition edition, 2002. [11] A. Hildebrandt, A. K. Dehof, A. Rurainski, A. Bertsch, M. Schumann, \nN. Toussaint, A. Moll, D. Stockel, S. Nickels, S. Mueller, H.-P. Lenhof, and O. Kohlbacher. BALL -biochemical \nalgorithms library 1.3. BMC Bioinformatics, 11(1):531, 2010. [12] W. Kahan. How futile are mindless \nassessments of roundoff in .oating-point computation?, 2006. http://www.cs.berkeley.edu/ wkahan/Mindless.pdf. \n[13] M. O. Lam, J. K. Hollingsworth, and G. W. Stewart. Dynamic .oating-point cancellation detection. \nIn WHIST 11, 2011. [14] P. Langlois. Automatic linear correction of rounding errors. BIT Numerical Mathematics, \n41:515 539, 2001. [15] J.-M. Muller, N. Brisebarre, F. de Dinechin, C.-P. Jeannerod, V. Lef`evre, G. \nMelquiond, N. Revol, D. Stehl\u00b4e, and S. Torres. Handbook of Floating-Point Arithmetic. Birkh\u00a8auser Boston, \n2010. [16] N. Nethercote and J. Seward. Valgrind: a framework for heavyweight dynamic binary instrumentation. \nIn PLDI 07, pages 89 100. ACM, 2007. [17] A. Neumaier and O. Shcherbina. Safe bounds in linear and mixed\u00adinteger \nlinear programming. Mathematical Programming, 99:283 296, 2004. 10.1007/s10107-003-0433-3. [18] U. G. \nA. Of.ce. Patriot missile defense: Software problem led to system failure at Dhahran, Saudi Arabia, GAO \nreport IMTEC 92-26, 1992. http://www.gao.gov/products/IMTEC-92-26/. [19] J. Walker. Floating-point benchmarks. \nhttp://www.fourmilab.ch/fbench/, retrieved on 2011-03-03. [20] The Wall Street Journal November 8, 1983, \np.37. [21] V. Weaver. SPEC CPU2006 problems of Valgrind. http://thread.gmane.org/gmane.comp.debugging.valgrind.devel/1488/, \nretrieved on 2011-03-03.    \n\t\t\t", "proc_id": "2254064", "abstract": "<p>Programs using floating-point arithmetic are prone to accuracy problems caused by rounding and catastrophic cancellation. These phenomena provoke bugs that are notoriously hard to track down: the program does not necessarily crash and the results are not necessarily obviously wrong, but often subtly inaccurate. Further use of these values can lead to catastrophic errors.</p> <p>In this paper, we present a dynamic program analysis that supports the programmer in finding accuracy problems. Our analysis uses binary translation to perform every floating-point computation side by side in higher precision. Furthermore, we use a lightweight slicing approach to track the evolution of errors.</p> <p>We evaluate our analysis by demonstrating that it catches wellknown floating-point accuracy problems and by analyzing the Spec CFP2006 floating-point benchmark. In the latter, we show how our tool tracks down a catastrophic cancellation that causes a complete loss of accuracy leading to a meaningless program result. Finally, we apply our program to a complex, real-world bioinformatics application in which our program detected a serious cancellation. Correcting the instability led not only to improved quality of the result, but also to an improvement of the program's run time.In this paper, we present a dynamic program analysis that supports the programmer in finding accuracy problems. Our analysis uses binary translation to perform every floating-point computation side by side in higher precision. Furthermore, we use a lightweight slicing approach to track the evolution of errors. We evaluate our analysis by demonstrating that it catches wellknown floating-point accuracy problems and by analyzing the SpecfiCFP2006 floating-point benchmark. In the latter, we show how our tool tracks down a catastrophic cancellation that causes a complete loss of accuracy leading to a meaningless program result. Finally, we apply our program to a complex, real-world bioinformatics application in which our program detected a serious cancellation. Correcting the instability led not only to improved quality of the result, but also to an improvement of the program's run time.</p>", "authors": [{"name": "Florian Benz", "author_profile_id": "81502804238", "affiliation": "Saarland University, Saarbr&#252;cken, Germany", "person_id": "P3471296", "email_address": "fbenz@stud.uni-saarland.de", "orcid_id": ""}, {"name": "Andreas Hildebrandt", "author_profile_id": "81321492465", "affiliation": "Johannes-Gutenberg Universit&#228;t Mainz, Mainz, Germany", "person_id": "P3471297", "email_address": "andreas.hildebrandt@uni-mainz.de", "orcid_id": ""}, {"name": "Sebastian Hack", "author_profile_id": "81315488920", "affiliation": "Saarland University, Saarbr&#252;cken, Germany", "person_id": "P3471298", "email_address": "hack@cs.uni-saarland.de", "orcid_id": ""}], "doi_number": "10.1145/2254064.2254118", "year": "2012", "article_id": "2254118", "conference": "PLDI", "title": "A dynamic program analysis to find floating-point accuracy problems", "url": "http://dl.acm.org/citation.cfm?id=2254118"}