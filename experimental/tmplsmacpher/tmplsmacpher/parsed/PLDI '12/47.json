{"article_publication_date": "06-11-2012", "fulltext": "\n Multicore Acceleration of Priority-Based Schedulers for Concurrency Bug Detection Santosh Nagarakatte \nSebastian Burckhardt Milo M. K. Martin Madanlal Musuvathi University of Pennsylvania Microsoft Research \nUniversity of Pennsylvania Microsoft Research santoshn@cis.upenn.edu sburckha@microsoft.com milom@cis.upenn.edu \nmadanm@microsoft.com Abstract Testing multithreaded programs is dif.cult as threads can interleave in \na nondeterministic fashion. Untested interleavings can cause fail\u00adures, but testing all interleavings \nis infeasible. Many interleaving exploration strategies for bug detection have been proposed, but their \nrelative effectiveness and performance remains unclear as they often lack publicly available implementations \nand have not been evaluated using common benchmarks. We describe NeedlePoint, an open-source framework \nthat allows selection and comparison of a wide range of interleaving exploration policies for bug detection \nproposed by prior work. Our experience with NeedlePoint indicates that priority-based probabilistic concurrency \ntesting (the PCT algorithm) .nds bugs quickly, but it runs only one thread at a time, which destroys \npar\u00adallelism by serializing executions. To address this problem we pro\u00adpose a parallel version of the \nPCT algorithm (PPCT). We show that the new algorithm outperforms the original by a factor of 5\u00d7 when \ntesting parallel programs on an eight-core machine. We formally prove that parallel PCT provides the \nsame probabilistic coverage guarantees as PCT. Moreover, PPCT is the .rst algorithm that runs multiple \nthreads while providing coverage guarantees. Categories and Subject Descriptors D.2.5 [Software Engineer\u00ading]: \nTesting and Debugging General Terms Algorithms, Reliability, Veri.cation Keywords Concurrency, priority-based \nscheduling, multithread\u00ading, probabilistic concurrency testing, parallel testing 1. Introduction Multithreaded \nprograms are dif.cult to test and debug because their behavior depends on the speci.c interleaving of \nshared memory ac\u00adcesses, which in turn depends on how the threads are interleaved by multicore hardware \nand thread scheduling software. Because thread interleaving is non-deterministic and largely unpredictable, \nan astronomical number of interleavings is possible even for small programs. However, due to the statistical \nnature of the interleav\u00adings, repeatedly testing the program on the same platform results in redundant \nexploration of similar interleavings. The untested un\u00adcommon interleavings can have concurrency bugs \nthat escape the Permission to make digital or hard copies of all or part of this work for personal or \nclassroom use is granted without fee provided that copies are not made or distributed for pro.t or commercial \nadvantage and that copies bear this notice and the full citation on the .rst page. To copy otherwise, \nto republish, to post on servers or to redistribute to lists, requires prior speci.c permission and/or \na fee. PLDI 12, June 11 16, 2012, Beijing, China. Copyright c . 2012 ACM 978-1-4503-1205-9/12/06. . . \n$10.00 testing process but manifest in deployed systems, especially if dif\u00adferences in the deployed system \ns hardware or software in.uence observed interleavings. One way to address this problem is to increase \nthe coverage achieved during testing by steering executions towards uncommon schedules. We classify prior \nproposals for such controlled schedul\u00ading into two categories. Best-effort tools insert explicit thread \nyields at selected points at runtime [7, 11, 12, 22 25]. The strategies for inserting yield points vary, \nranging from random selection [7, 24] to heuristics based on identifying symptomatic bug patterns in \nthe code, such as data races [25], potential atomicity violations [22, 23], and potential deadlocks [12]. \nOn the other hand, guaranteed\u00adcoverage tools such as CHESS [21] and PCT [5] provide provable guarantees \nof the schedule coverage achieved during concurrency testing. To provide such coverage guarantees, both \nthese tools re\u00adquire absolute control of the thread scheduling achieved by running the threads serially \none at a time. Although coverage guarantees are appealing in practice, the serial execution limits the \napplicability of these tools, a disadvantage not shared by best-effort approaches. To better understand \nthe relative effectiveness and performance of prior best-effort and guaranteed-coverage tools, we set \nout to re\u00adproduce and compare prior research proposals by reimplementing several published algorithms. \nIn this process, we identi.ed a com\u00admon set of mechanisms required by a wide range of bug detection techniques. \nThese mechanisms include instrumenting synchroniza\u00adtion accesses, identifying blocked threads, and handling \nstarvation. A systematic approach to providing these mechanisms is useful be\u00adcause manual instrumentation \nor annotations become quickly im\u00adpractical when controlling schedules of large applications. Motivated \nby this observation, we developed NeedlePoint, a uni.ed framework for implementing a wide range of controlled \nscheduling approaches ranging from simple random sleeps to prioritized scheduling. The framework separates \nthe mechanisms required to implement the various scheduling policies from the policies themselves. This \nseparation allowed us to implement sev\u00aderal previously proposed approaches as policies that plug into \nthe framework. In particular, we reimplemented the following strate\u00adgies: (1) a randomized version of \npreemption bounding [19], (2) AtomFuzzer [22], (3) simple random sleeps, and (4) probabilistic concurrency \ntesting (PCT) [5]. We then used Needlepoint to test a collection of multithreaded programs containing \nknown concurrency bugs and also bench\u00admarks from the SPLASH and Parsec suites. Our experience with NeedlePoint \nshows empirically that PCT is highly effective at .nd\u00ading bugs. However, it also reveals that PCT s performance \nsuffers signi.cantly from its inherent restriction of scheduling only one thread at a time. Although \nrunning one thread is good enough for running small unit tests, it becomes a problem and causes signi.\u00adcant \nslowdowns when testing large programs with a large number of threads or when testing highly parallel \nbenchmarks.  To alleviate this problem, we propose a new testing algorithm called PPCT (parallel PCT). \nLike best-effort tools, PPCT can schedule multiple threads at a time, thereby making better use of hardware \nparallelism. At the same time, PPCT provides the same coverage guarantees as PCT. To the best of our \nknowledge, PPCT is the .rst algorithm to provide coverage guarantees without requiring serial execution \nof threads. We provide a brief overview of the PCT algorithm [5] before explaining our improvement. PCT \nassigns thread priorities cho\u00adsen uniformly randomly at the beginning of the program. At every step of \nthe algorithm, PCT schedules the unblocked thread with the highest priority. The scheduled thread executes \ninstructions up to the next synchronization event before yielding the control back to the scheduler. \nDuring this execution, PCT performs a small num\u00adber of priority changes at steps chosen uniformly randomly \nat the beginning of the execution. The guarantees provided by PCT de\u00adpend on the depth of a concurrency \nbug, which is the number of scheduling constraints suf.cient to trigger the buggy interleaving. For instance, \nordering violations have a depth 1 and atomicity vio\u00adlations have a depth 2. Given a program with n threads \nthat executes a maximum of k instructions, PCT triggers a bug of depth d with 1 probability at least \nnkd-1 . For instance, PCT can .nd ordering vio\u00adlations with probability at least 1 . n The key intuition \nbehind PPCT is that when searching for bugs of depth d, it is necessary to control the scheduling of \nonly d threads. In particular, PPCT schedules all unblocked threads with a priority greater than d at \nevery step of the algorithm. In the rare case when all unblocked threads have a priority smaller than \nor equal to d, PPCT resorts to serial scheduling and schedules the un\u00adblocked thread with the highest \npriority. We adapt the formal proof of PCT to show that PPCT provides the same coverage guarantees as \nPCT. Empirically, our experiments con.rm that PPCT detects bugs with either the same or higher rate when \ncompared to PCT. Com\u00adpared to PCT, however, PPCT provides signi.cant speedups both on multi-core and \nsingle-core machines. PPCT provides speedups over PCT even on a single-core machine by providing more \n.exi\u00adbility to the operating system scheduler to schedule a thread from a larger pool of enabled threads. \nOn a multi-core machine with eight cores, replacing PCT by PPCT reduces the overhead of executing the \nprogram from 34\u00d7 to just 6\u00d7 on an average. Unlike a slow\u00addown of 34\u00d7, a slowdown of 6\u00d7 is still tolerable \nfor testing interac\u00adtive applications, and it enabled us to run priority based exploration with the Chrome \nweb browser and other applications. 2. Background on Controlled Scheduling This section provides background \non the problem of schedule se\u00adlection when testing multithreaded programs and the use of con\u00adtrolled \nscheduling to address the problem. 2.1 Schedule Selection Of the many aspects of testing multithreaded \nprograms, this paper focuses on the schedule selection problem, which involves effec\u00adtively .nding, among \nan astronomical number of possible thread schedules, those schedules that drive a program to an error. \nTo this effect, we assume the existence of a test harness that provides nec\u00adessary inputs to a multithreaded \nprogram and a test mechanism that determines a test failure, such as a program crash, an assertion vio\u00adlation, \nor an incorrect output. Data-race detection is related to and is different from the sched\u00adule selection \nproblem. A data race occurs when two threads con\u00adcurrently access the same shared variable without appropriate \nsyn\u00adchronization, such as a missing lock. Although they are indications of erroneous programming, a data \nrace is neither necessary or suf.\u00adcient for a concurrency error [8, 14].1 Schedule selection may also \nhelp data race detectors to uncover additional races. 2.2 Controlled Scheduling Controlled scheduling \ntechniques attempt to steer the program to\u00adwards less common interleavings. This is typically accomplished \nby controlling the scheduling of threads and by allowing exactly one thread to execute at a given time. \nThese techniques can sys\u00adtematically explore all possible interleavings (for smaller codes) or use various \nheuristics to guide which interleavings are explored (for larger codes). Further, they can provide repeatability \nby enforcing the same thread scheduling decisions, which aids debugging. To control the order in which \noperations from various threads are executed, such systems typically use OS-level threads but con\u00adtrol \nthe execution in user mode by preventing all but one thread from making progress at a given time. A scheduling \npolicy deter\u00admines the thread that makes progress. A running thread invokes the user-level scheduler \nat each scheduling point. Scheduling points occur at thread creation, synchronization operations, and \nthread termination. To achieve full coverage of sequentially consistent behaviors, one also needs to \ninsert scheduling points at certain shared-memory accesses, such as accesses to volatile variables (in \nJava), atomic variables (in C++), and variables participating in a data race. Instrumenting such shared-memory \naccesses, however, incurs additional overhead, requiring the tool designer to make an explicit overhead \nversus coverage tradeoff. Controlled scheduling allows the user-level scheduler to determine the running/not-running \nstatus of each thread at each scheduling point. The status of each thread is maintained in shared memory. \nWhen the main thread spawns a thread, the newly created thread s status defaults to not-running. Whenever \nthe running thread encounters a scheduling point, it invokes the scheduling policy which can then either \n(1) decide to execute the currently running thread or (2) choose another thread to execute, in which \ncase the scheduling policy sets the other thread s status to running and sets the current thread s status \nto not-running. Inactive threads (ones that are marked as not-running by the user-level scheduler) check \ntheir status whenever scheduled by the OS-level scheduler, either discovering they have become the new \nactive thread or yielding otherwise. To ensure forward progress for busy-waiting and other non-trivial \nsynchronization idioms, the inactive threads also periodically invoke the scheduling policy Lock-based \nsynchronization can potentially block, e.g., if the lock is held by some other thread. A controlled scheduling \nsystem must identify the threads that are blocked and should not sched\u00adule blocked threads, as doing \nso can cause the system to livelock. For simple locks, a controlled scheduling system can maintain a \nhashtable of all acquired locks with information about threads ac\u00adquiring them. On a lock acquire operation, \nif the lock is already held by some other thread, then (1) the thread s status is changed to blocked, \n(2) the thread is added to the list of threads waiting for the lock, and (3) the scheduling policy is \ninvoked as described above, which chooses one of the non-blocked threads. On a successful lock acquire, \nthe hashtable is also updated with the information about the acquired lock and the acquiring thread. \nOn a lock release, the scheduler changes the state of all blocked threads on the released lock from blocked \nto not-running.  2.3 Challenges with Controlled Scheduling Implementing controlled scheduling is conceptually \nsimple, but challenging in practice due to the following two reasons. First, real world programs use \na plethora of synchronization primitives. Many programs de.ne custom synchronization using the atomic \nprimi\u00ad 1 Here, we explicitly ignore weak-memory-model issues issues [3, 18]  tives provided by the hardware \nor use adhoc synchronization [26]. Accurately interpreting and handling all these synchronization op\u00aderations \nin a controlled scheduling system is dif.cult and infeasible in practice. Further, even minor mistakes \nin interpreting synchro\u00adnization operations can lead to erroneous livelocks in the controlled scheduling \nsystem and/or missed bugs. Second, the scheduling de\u00adcisions made by the underlying scheduling policy \nto steer the multi\u00adthreaded program towards unexplored and buggy interleavings can have pathological \ninteractions with the synchronization operations in the program. For example, scheduling a thread that \nis execut\u00ading busy-wait synchronization operations or arbitrary spin loops to completion with preemption \nbounded exploration or priority based scheduling can cause starvation. These problems make interleav\u00ading \nexploration of real world programs with controlled scheduling challenging. 3. NeedlePoint Scheduling \nFramework Although a large number of concurrency bug detection techniques have been proposed to address \nthe schedule selection problem [8, 13, 15, 16, 23], publicly available concurrency bug detectors are \nprimarily data race detectors such as Helgrind [1] and Intel Thread Checker [2]. Thus, the ef.cacy of \nthe various previously proposed concurrency bug detectors with respect to each other is unknown. To address \nthis problem, we present NeedlePoint, a uni.ed framework for implementing a wide range of controlled \nscheduling approaches ranging from simple sleep insertions to randomized prioritization with support \nfor repeatable execution. NeedlePoint has two overall goals. First, the NeedlePoint framework is designed \nto handle the real-world complexities of instrumentation for insert\u00ading scheduling points and interpreting \nfull-.edged synchronization libraries, resulting in a tool robust enough for use in testing real\u00adworld \nconcurrent software with state-of-the-art controlled schedul\u00ading policies. Second, NeedlePoint aims to \nprovide researchers in concurrency bug detection a framework to build upon when de\u00adsigning and evaluating \nnew scheduling policies. 3.1 Mechanisms NeedlePoint s key contribution is the separation of mechanisms \nrequired to implement the various scheduling policies from the scheduling policies themselves. This separation \nof concerns be\u00adtween the mechanisms required to implement controlled schedul\u00ading and the scheduling policies \nenabled us to implement many pre\u00adviously proposed controlled scheduling techniques and test real\u00adworld \nprograms with them. We identi.ed three key mechanisms for building a wide range of concurrency bug detectors: \n(1) instrument\u00ading synchronization operations, (2) identifying blocked threads, and (3) ensuring starvation \nfreedom. 3.1.1 Instrumenting Synchronization Operations NeedlePoint invokes the underlying scheduling \npolicy at every dy\u00adnamic program point where threads can interleave. We name these dynamic program points \nas schedule points. NeedlePoint uses bi\u00adnary instrumentation to identify such schedule points, namely \nall synchronization operations, atomic operations, and user speci.ed synchronization operations. Furthermore, \nto detect bugs with data races, NeedlePoint can be con.gured to instrument every memory access and invoke \nthe scheduling policy on such accesses. We have built NeedlePoint using the Pin [17] dynamic binary instrumen\u00adtation \nframework running on a x86 Linux machine. Pin enables NeedlePoint to identify synchronization operations \n(POSIX threads API by default), atomic operations (x86 instructions with a lock pre.x), and memory operations. \nNeedlePoint uses Pin to insert a call to the scheduling framework at these scheduling points. 3.1.2 \nBlocking Information Accurately interpreting all synchronization operations becomes im\u00adpractical in the \npresence of wide range of synchronization primi\u00adtives and adhoc synchronizations [26] used by real world \nprograms. Rather than interpreting the blocking semantics of each synchro\u00adnization operation, we decided \nto infer the blocked status. Needle-Point lets the synchronization operation execute and infers whether \nit is blocked based on the number of yields performed by other threads spin-waiting to be scheduled. \nThis scheme is based on the following intuition: a thread executing a synchronization operation that \ndid not block will subsequently encounter another schedul\u00ading point. In contrast, a thread trying to \nacquire a lock that is al\u00adready held will block and thus not encounter another scheduling point. NeedlePoint \ncounts the number of yields performed by the threads waiting to be scheduled. When this count exceeds \na thresh\u00adold, NeedlePoint infers that the running thread is actually blocked on a synchronization operation. \nThis threshold is parameterizable. Setting a smaller threshold can result in an unblocked thread being \nerroneously considered as blocked by NeedlePoint, which can hurt repeatability. In contrast, larger thresholds \ncause slowdowns in the execution of the program. In our experiments, a threshold of ten yields was suf.cient \nto ensure repeatability (for serial scheduling policies). 3.1.3 Fairness and Starvation Freedom We found \nthat many programs use busy wait synchronization (e.g. waiting for a queue to become empty and sleeping, \nspinning, barriers, spin loops, and others) and adhoc synchronization [26]. Scheduling a thread to completion \n(as in preemption bounding or as in priority based scheduling) without inducing preemptions or thread \nswitches may cause starvation resulting in program livelocks. To avoid such starvation, NeedlePoint must \nchoose some other thread to be scheduled. However, making a large number of such choices arbitrarily \nin the presence of busy wait synchronization can cause the tool to miss bugs losing the bene.ts of controlled \nscheduling. To address this problem of starvation, we make the following observation inspired from prior \nwork on a fair scheduler [20]: a program performing busy wait synchronization will encounter numerous \nscheduling points, which inturn invoke the scheduling policy. Hence, if we override the default scheduling \npolicy with a small probability, starvation will be avoided. Thus, the NeedlePoint framework overrides \nthe scheduling policy with a small probability for a single scheduling point thereby ensuring starvation \nfreedom. As these starvation freedom mechanisms are invoked uniformly for each schedule point rather \nthan invoking them based on the amount of time spent waiting, NeedlePoint ensures repeatability using \nthe same random seed.  3.2 Policies We implemented .ve previously proposed concurrency bug detec\u00adtors \nusing the NeedlePoint mechanisms. Random sleep (RS): The random sleep policy runs more than one unblocked \nthread at any point in time. At every schedule point, this policy introduces a small delay with an OS \nsleep call with a small probability. We used a probability of 1/30 to peform the sleep as it performed \nwell in our experiments.  Preemption always (PA): This policy runs one thread at a time and attempts \nto introduce a preemption at every schedule point. At every schedule point, the scheduler performs a \npreemption switching the current running thread to a randomly chosen non\u00adblocked thread.  Preemption \nbounding (PB) [19]: This policy performs an ex\u00adploration with a predetermined number of preemptions by \nrun\u00ad   Program Lines of code Schedule points Threads Bug type Bug depth Pbzip2 15,188 1210 3 Ordering \nviolation 2 Memc 11,182 845 4 Atomicity violation 2 t+15 2300 t+25 3400 t+35 3900 am 79132 WSQ-1 541 \n1916 4 Atomicity violation 3 WSQ-2 1086 2 WSQ-3 1717 2 Trans 33,622 38118 2 Ordering violation 1 NSPR \n1,100 5361 3 Deadlock 2 Table 1. Concurrency bugs used for NeedlePoint s evaluation that we obtained \nfrom prior research [5, 10, 16, 27, 28]. WSQ is an im\u00adplementation of the work stealing queue with lock \nfree data struc\u00adtures. WSQ-1, WSQ-2, and WSQ-3 are the three distinct concur\u00adrency bugs in the WSQ implementation. \nMemc is the memcached daemon. Trans is the Transmission BitTorrent client. ning one thread at a time. \nAt the beginning of the program, few scheduling points are chosen as the preemption points using a distribution. \nWhen a pre-determined preemption point is en\u00adcountered, a forced preemption is induced and a non-blocked \nthread is chosen to become the running thread. Unlike prior re\u00adsearch [19] that used a round robin scheme \nto select the next running thread when a thread is blocked, this policy chooses a random thread because \nwe found it to be more effective at detecting bugs. AtomFuzzer (AF) [22]: This policy directs the search \nto .nd atomicity violations by running one thread at a time. At ev\u00adery schedule point, if the thread \nattempts to acquire a lock that was previously acquired by the same thread, then it pauses the thread \nand schedules another thread in an effort to trigger an atomicity violation.  PCT [5]: Our prior work \nuses priorities to make a few random choices at the beginning of the program and direct the search. The \npolicy assigns priorities to the thread before execution. It runs the highest priority thread that is \nnon-blocked at every step. At predetermined schedule points, the priority of the executing thread is \nchanged to a predetermined priority.  The NeedlePoint framework is around 6K lines of C++ code. The \nindividual policies that implement random sleeps, preemption always, randomized version of preemption \nbounding [19], Atom-Fuzzer [24], and PCT [5] are 75, 125, 221, 208 and 258 lines of C++ code respectively, \nsignifying the ease of writing a custom scheduler with NeedlePoint. As a proof of concept, we have tested \nlarge multi-threaded programs including the Chrome web browser with various scheduling policies in the \nNeedlePoint framework. 4. Evaluation of Previous Techniques This section provides an evaluation of previously \nproposed tech\u00adniques using the NeedlePoint framework on a common set of con\u00adcurrency bugs. 4.1 Concurrency \nBugs We evaluate the effectiveness of various scheduling policies with the previously known bugs listed \nin Table 1. These bugs have been widely used in prior research in this area [5, 10, 16, 27, 28]. Al\u00adthough \nmany bug reports provided test cases or patches that in\u00adtroduce sleeps at appropriate places to trigger \nthe bug, we did not patch or modify the application to increase the likelihood of .nding bugs. For the \nmemcached bug, we designed a test harness and cre\u00adated various instances of the memcached bug by varying \nthe num\u00adber of memcached operations performed before performing the in\u00adcrement operation that has the \natomicity violation. These instances are listed as t + x in Table 1. We also created an instance of mem\u00adcached \nbug where NeedlePoint introduces a schedule point before every memory access and it is listed am in Table \n1. 4.2 Comparison of Various Scheduling Policies To compare the bug detection abilities of various scheduling \npoli\u00adcies in Section 3.2, we ran each application listed in Table 1 with each scheduling policy 100,000 \ntimes. We checked each run to see if the bug was triggered. Figure 1 reports the number of executions \nin which the bug was triggered. Similarly, Figure 2 reports the num\u00adber of executions that triggered \nthe bug with the different instances of the memcached con.gurations created by our test harness with \nthe various scheduling policies. Both the graphs have .ve bars for each bug where each individual bar \nrepresents the number of buggy executions. The bug detection ef.cacy of the scheduling policies varies \nwith the bug. Simple choices such as random sleep and preemption al\u00adways are effective for some bugs. \nFrom Figure 1, we observe that the random sleep policy performs reasonably well with the mem\u00adcached bug \nbut does not detect other bugs. We found that random sleep policy works best when the test case is small \nand the bugs are localized. Further Figure 2 shows that the random sleep policy s bug detection ability \ndecreases with the increase in the amount of work done before the buggy access for the memcached bug. \nFigure 1 and Figure 2 show that preemption bounding is reason\u00adably effective in triggering most of the \nbugs except the Pbzip bug. Preemption bounding requires more executions to trigger some bugs as they \nexecute a large number of schedule points. Figure 1 also shows that AtomFuzzer, which is directed to \n.nd atomicity vi\u00adolations, performs better than preemption bounding for many of the atomicity violations. \nOur prior work PCT triggers all these bugs with similar or bet\u00adter ef.cacy than other scheduling policies. \nOnly PCT detected the Pbzip bug. Pbzip uses adhoc synchronization with spin loops to sig\u00adnal when the \ncompression is over. The program crashes with a seg\u00admentation fault when the main thread frees a synchronization \nvari\u00adable that is later used by one of the other threads. In the presence of such adhoc synchronization, \nthe threads need to be scheduled with a few random choices to trigger the bug. A combination of priority \nbased scheduling with a robust starvation freedom mechanisms en\u00adable PCT to trigger the Pbzip bug effectively. \nTable 2 summarizes the ability of the scheduling policies to trigger the bug at least once in 1000 executions \non average. We observe that our prior work PCT triggers all these bugs at least once within the 1000 \nruns.  4.3 De.ciencies of PCT As a result of running one thread at a time, PCT suffers from two major \nproblems in testing long running parallel applications. First, running one thread at at time cannot leverage \nmulticores to speedup each execution. Further even on a single core, pathological interactions with PCT \nscheduling decisions and the OS scheduling decisions can cause signi.cant performance slowdowns. Second, \nmany multithreaded applications that use adhoc synchronization and busy-wait synchronization incur large \nslowdowns. PCT, which uses priority based scheduling with only one thread executing at any time, causes \nstarvation in the presence of such synchronization idioms. PCT relies NeedlePoint s starvation freedom \nmechanisms to make progress thereby incurring large slowdowns that effectively Figure 1. Bug detection \nabilities on common concurrency bugs for .ve different scheduling policies described in Section 3.2: \nRandom Sleep (RS), Preemption Always (PA), Preemption Bounding (PB), AtomFuzzer (AF), and Probabilistic \nConcurrency Testing (PCT).   Figure 2. Bug detection ability of various scheduling policies with the \nvariants of the memcached bug generated by the test harness. Program Random Sleep Preemption Atom Fuzzer \nPCT Always Bound Memc Yes Yes Yes Yes Yes Pbzip2 No No No No Yes NSPR No No Yes No Yes WSQ-1 No Yes No \nYes Yes WSQ-2 Yes Yes No Yes Yes WSQ-3 No No Yes Yes Yes Trans No Yes Yes Yes Yes Memc-am No No Yes No \nYes Table 2. Do they trigger the bugs in 1000 runs? prohibit the usage of PCT to test such applications. \nTo enable test\u00ading of large parallel programs with large inputs rather than unit tests, we pursued parallelization \nof PCT. The key contribution of our parallelization effort is that our parallel PCT (PPCT) algorithm \nruns multiple threads while retaining the same probabilistic guar\u00adantee of PCT running one thread at \nat time. 5. Parallel PCT (PPCT) In this section we provide background on the bug depth metric de\u00ad.ned \nby PCT and used by our PPCT algorithm to classify con\u00adcurrency bugs. We subsequently provide the PPCT \nalgorithm for a particular bug depth. 5.1 Background on PCT s Bug Depth Concurrency bugs in multithreaded \nsoftware occur when instruc\u00adtions are scheduled in an order not envisioned by the programmer. Bug depth \nis de.ned as the minimum set of these ordering con\u00adstraints between instructions from different threads \nthat are suf.\u00adcient to trigger the bug. It is possible for different sets of ordering constraints to \ntrigger the same bug. In such a case, we focus on the set with the fewest constraints. For bugs of greater \ndepth, more orderings need to enforced by the scheduler to trigger the bug, in\u00adcreasing the hardness \nof .nding it. Figure 3 shows examples of common concurrency errors with ordering constraints, represented \nby arrows, that are suf.cient to trigger the bug. Any schedule that satis.es these ordering constraints \nis guaranteed to trigger the bug irrespective of how it schedules instructions not relevant to the bug. \nFor the examples in Figure 3 the depth respectively is 1, 2, and 2. In practice, we have found that many \nconcurrency bugs to have small depths [5]. 5.2 Intuition Behind the PPCT Algorithm Both the original \nPCT algorithm and the PPCT algorithm use thread priorities to probabilistically enforce ordering constraints \nthat drive the program to an error. The key difference between the two algorithms is the number of choices \nthey provide to the adver\u00adsary, which in our case is the underlying operating system sched\u00aduler, to schedule \nthreads at each step. The PCT algorithm allows the adversary exactly one thread, the highest priority \nthread, to schedule at each step. In contrast, the PPCT algorithm maintains two sets of threads, a higher \npriority set and a lower priority set. At each step, the adversary is allowed to pick any thread in the \nhigher priority set. If this set is empty, then the adversary is required to pick the highest priority \nthread in the lower priority set. In other words, while PCT serializes the execution of all threads, \nPPCT serializes the execution only of the threads in the lower\u00adpriority set. The threads in the higher-priority \nset can be executed in parallel. Importantly, the PPCT algorithm guarantees that the num\u00adber of threads \nin the lower priority set is bounded by the param\u00adeter d, the depth of the bug the algorithm is attempting \nto trigger. Thus, any implementation of the algorithm is required to control the scheduling of only at \nmost d threads, while the operating sys\u00adtem is freely allowed to schedule the remaining threads on multiple \ncores as it deems .t. Apart from the crucial difference above, the PPCT algorithm functions exactly like \nthe PCT algorithm, assigns random priorities to the threads and changes priorities at randomly chosen \npoints in the execution. We start with an informal description of the PPCT algorithm in the next subsection, \nbefore giving precise pseudocode and a proof of the guarantees in Section 6.  5.3 Informal Description \nof the PPCT Algorithm Given inputs: number of threads n, total number of dynamic in\u00adstructions k and \nthe depth of the bug being explored d, PPCT works as follows.  Figure 3. Three typical concurrency \nbugs and ordering edges suf.cient to .nd each. (A) This ordering bug manifests whenever the test by thread \n2 is executed before the initialization by thread 1. (B) This atomicity violation manifests whenever \nthe test by thread 2 executed before the assignment by thread 1, and the latter is executed before the \nmethod call by thread 2. (C) This deadlock manifests whenever thread 1 locks A before thread 2, and thread \n2 locks B before thread 1. Pick a random low priority thread: At the beginning of the program, pick \na thread uniformly at random and assign it a priority d. In addition, insert this thread into the lower \npriority set L. Insert all other threads into the higher priority set H.  Pick random priority change \npoints: At the beginning of the program, pick d - 1 priority change points k1,...,kd-1 in the range [1, \nk]. Each ki has an associated priority value of i.  Scheduler choice: At each step, the scheduler picks \nany non\u00adblocked thread in H to schedule. If H is empty or if all threads in H are blocked, the scheduler \npicks the highest priority thread in L.  Priority change: After a step, increment a step counter. If \nthe step counter matches ki for some i, change the priority of the just executed thread to i and insert \nit into L.  5.4 Coverage Guarantees of PPCT Given a program with n threads that executes at most k \nsteps, the PPCT algorithm described above .nds every bug of depth d with probability at least 1 in every \nrun of the algorithm. This proba\u00ad nkd-1 bilistic guarantee exactly matches the original PCT algorithm, \nde\u00adspite allowing more parallelism. We explain and prove this proba\u00adbilistic bound in Section 6 below. \n 5.5 Starvation and Priority Based Scheduling Not allowing a lower priority thread to execute can cause \nstarvation. In the presence of starvation, to make progress, a lower priority thread must be scheduled \nfor a single step. We randomly choose a thread with priority lower than d to run for a single step (execute \none schedule point) by bumping its priority to the highest priority in the system and reverting back \nthe priority of the thread to the original priority when it completes executing the step. We use the \nmechanisms provided by NeedlePoint to identify when we should make the policy decision to avoid starvation. \n6. Proof of PPCT s Probabilistic Guarantees In this section, we give a detailed description of PPCT and \npresent a proof of its probabilistic guarantee. We start with a high-level in\u00adformal description of the \nproof strategy and the differences between PPCT and PCT. We then walk through the proof in detail and \nfully formalize all concepts (bugs, programs, schedules) in the process. 6.1 Overview The basic idea \nis to .nd bugs by guessing a directive that leads to the bug. A directive is essentially a sequence of \nprogram points. We say a directive .nds a bug if all schedules that follow the directive s sequencing \npoints trigger the bug. Directives exist for all bugs (be\u00adcause if we restrict the scheduler enough, \nwe can guarantee that it .nds the bug). The size of the directive (i.e. the number of sequenc\u00ading points \nit contains) re.ects how hard it is to trigger the bug. We call the size of the smallest directive that \ncan .nd a given concur\u00adrency bug the depth of that bug. As explained in Section 5.1, our intuition about \ntypical concurrency bugs suggests that many bugs have a relatively small depth and can thus bene.t from \na scheduler that is tuned to do well at covering small depths. Although we know that a directive of size \nd exists for all bugs of depth d (by de.nition), in the concurrency testing scenario we do of course \nnot know the actual bugs and which directives will .nd them. It turns out, however, that PPCT has a reasonable \nprobability of just guessing the directive. In the remainder of this section, we formalize and prove \nthis claim. The basic proof strategy is to construct and compare two schedulers: A randomized scheduler \nthat schedules the program under test based on a few random choices. This scheduler has no knowl\u00adedge \nabout the structure of the program under test or the bug we are targeting.  A directed scheduler that \nschedules the program under test based on an explictly supplied directive. This directive identi\u00ad.es \npoints of interest in the program under test, and it prescribes an order in which they should be executed. \nNote that we did not actually implement such a directed scheduler: its only purpose is to construct a \nproof.  We then show that given a directive of size d, the randomized scheduler and the directed scheduler \nproduce the same schedule with probability at least 1 (where n is the number of threads nkd-1 and k is \nthe number of schedule points in the program). This im\u00ad plies that the randomized scheduler .nds any \nbug of depth d with 1 probability at least nkd-1 , without any knowledge about the pro\u00adgram or the bug. \n 6.2 PPCT vs. PCT Once we understood in what way PCT does in fact require serial execution (it needs \nto execute the sequencing points of the direc\u00adtive in order) and in what way it does not (threads that \nare not at a sequencing point can be executed in any order and in parallel), we were able to leave the \noriginal algorithm and proof almost com\u00adpletely intact. Thus the presented algorithms for the randomized \nand the di\u00adrected scheduler are exactly the same as in [5] except for one line, where the scheduler is \nnow allowed to pick from more threads. The proof of the probabilistic guarantees is also exactly the \nsame, ex\u00adcept for the proof of Lemma 12, which shows that the scheduling restrictions we removed (by \nproviding more choices to the sched\u00aduler) do not compromise the invariants of the original algorithm. \nAlthough these changes are few in number and localized, they are dif.cult to understand out of context. \nThus we reproduce the complete proof here.  Require: program P, d = 0 Require: n = maxthreads(P), k \n= maxsteps(P) Require: random variables k1,...,kd-1 .{1,...,k} Require: random variable p . Permutations(n) \n 1: procedure RandS(n,k,d) begin 2: var S : schedule 3: var p : array[n] of N 4: S . e // set initial \npriorities 5: for all t .{1,...,n} do 6: p[t] . d + p(t) - 1 7: end for 8: while enP(S) .= 0/ do 9: \n/* schedule thread of admissible priority */ 10: t . element of enP(S) such that p[t] > d or p[t] maximal \n11: S . St /* are we at priority change point? */ 12: for all i .{1,...,d - 1} do 13: if length(S)= ki \nthen 14: p[t]= d - i 15: end if 16: end for 17: end while 18: return S 19: end Figure 4. The PPCT randomized \nscheduler.  6.3 De.nitions We brie.y recount some standard notation for operations on sequences. Let \nT be any set. De.ne T * to be the set of .nite sequences of elements from T . For a sequence S . T *, \nde.ne length(S) to be the length of the sequence. We let e denote the sequence of length 0. For a sequence \nS . T * and a number n such that 0 = n < length(S), let S[n] be the n-th element of S (where counting \nstarts with 0). For t . T and S . T *, we write t . S as a shorthand for .m : S[m]= t. For any S . T \n* and for any n,m such that 0 = n = m = length(S), let S[n, m] be the contiguous subsequence of S starting \nat position n and ending at (and including) position m. For two sequences S1,S2 . T * , we let S1S2 denote \nthe concatenation as usual. We do not distinguish between sequences of length one and the respective \nelement. We call a sequence S1 . T * a pre.x of a sequence S . T * if there exists a sequence S2 . T \n* such that S = S1S2. A set of sequences P . T * is called pre.x-closed if for any S . P, all pre.xes \nof S are also in P. DEFINITION 1. De.ne T = N to be the set of thread identi.ers. De.ne Sched = T * to \nbe the set of all schedules. De.ne a program to be a pre.x-closed subset of Sched. For a given program \nP . Sched, we say a schedule S . P is complete if it is not the pre.x of any schedule in P other than \nitself, and partial otherwise. Thus, we represent a program abstractly by its schedules, and each schedule \nis simply a sequence of thread identi.ers. For example, the sequence 1221 represents the schedule where \nthread 1 takes one step, followed by two steps by thread 2, followed by another step of thread 1. We \nthink of schedules as an abstract representation of the program state. Not all threads can be scheduled \nfrom all states, as they may be blocked. We say a thread is enabled in a state if it can be scheduled \nfrom that state. DEFINITION 2. Let P . Sched be a program. For a schedule S . P, de.ne enP(S) to be the \nset {t . T | St . P}. De.ne maxsteps(P)= max{length(S) | S . P} and maxthreads(P)= max{S[i] | S . P} \n(or 8 if unbounded). Finally, we represent a concurrency bug abstractly as the set of schedules that \n.nd it: DEFINITION 3. Let P . Sched be a program. De.ne a bug B of P to be a subset B . P.  6.4 The \nAlgorithm We now introduce the PPCT randomized scheduler (Fig. 4), which is identical to the original \nPCT scheduler [5] except for line 10, in which the scheduler now has more choice when picking the next \nthread to schedule. As in the original algorithm, we expect RandS(n,k,d) to be called with a conservative \nestimate for n (number of threads) and k (number of steps). During the progress of the algorithm, we \nstore the current schedule in the variable S, and the current thread pri\u00adorities in an array p of size \nn. The thread priorities are initially assigned random values (chosen by the random permutation p). In \neach iteration, we pick an admissible thread for scheduling. As in the original PCT algorithm, we can \nalways pick the enabled thread of maximal priority to execute next. In addition, however, we also allow \nany thread to be picked whose priority is larger than d.2 Once we have (nondeterministically) picked \na thread t, we execute it for one step. Then we check if we have reached a priority change point (determined \nby the random values ki), and if so, we change the priority of t accordingly. This process repeats until \nno more threads are enabled (that is, we have reached a deadlock or the program has terminated).  6.5 \nProbabilistic Coverage Guarantee In this section, we precisely state and then prove the probabilistic \ncoverage guarantees for our randomized scheduler, in three steps. First, we introduce a general mechanism \nfor identifying dynamic events in threads, which is a necessary prerequisite for de.ning or\u00addering constraints \non such events. Next, we build on that basis to de.ne the depth of a bug as the minimum number of ordering \ncon\u00adstraints on thread events that will reliably reveal the bug. Finally, we state and prove the core \ntheorem. 6.5.1 Event Labeling The .rst problem is to clarify how we de.ne the events that par\u00adticipate \nin the ordering constraints. For this purpose, we introduce a general de.nition of event labeling. Event \nlabels must be unique within each execution, but may vary across executions. Essentially, an event labeling \nE de.nes a set of labels LE (where each label a . LE belongs to a particular thread threadE (a)) and \na function nextE (S,t) that tells us what label (if any) the thread t is going to emit if scheduled next \nafter schedule S. More formally, we de.ne: DEFINITION 4. Let P be a program. An event labeling E is a \ntriple (LE ,threadE ,nextE ) where LE is a set of labels, threadE is a func\u00adtion LE . T, and nextE is \na function P \u00d7 T . (LE . {.}), such that the following conditions are satis.ed: 1. (Af.nity) If nextE \n(S,t)= a for some a . LE, then threadE (a)= t. 2. (Stability) If nextE (S1,t) . = nextE (S1S2,t) , then \nt . S2. 3. (Uniqueness) If nextE (S1,t)= nextE (S1S2,t)= a for some a . LE, then t ./S2. 4. (NotFirst) \nnextE (e,t)=. for all t . T.  Sometimes, we would like to talk about labels that have already been emitted \nin a schedule. For this purpose we de.ne the auxiliary 2 This scheduling policy corresponds precisely \nto the informal description in Section 5.3, where (1) all threads with priority larger than d are in \nthe high priority set and (2) all other threads are in the low priority set.  functions labelE and labelsE \nas follows. For S . P and 0 = m < length(S), we de.ne labelE (S,m)= a if the label a is being emitted \nat position m, and we de.ne labelsE (S) to be the set of all labels emitted in S (more formally, labelE \n(S,m)= a if nextE (S[0,m - 1],S[m]) = a, and labelE (S, m)=. otherwise; and labelsE (S)= {labelE (S, \nm) | 0 = m < length(S)}).  6.5.2 Bug Depth We now formalize the notion of ordering constraints and bug \ndepth that we motivated earlier. Compared to our informal introduction from Section 5.1, there are two \nvariations worth mentioning. First, we generalize each edge constraint (a,b) (where a and b are event \nlabels) to allow multiple sources (A,b), where A is a set of labels all of which have to be scheduled \nbefore b to satisfy the constraint. Second, because we are using dynamically generated labels as our \nevents, we require that the ordering constraints are suf.cient to guide the scheduler to the bug without \nneeding to know about ad\u00additional constraints implied by the program structure (as motivated by the example \nin Fig. 3). We formulate the notion of a directive D of size d, which con\u00adsists of a labeling and d constraints. \nThe idea is that a directive can guide a schedule towards a bug, and that the depth of a bug is de\u00ad.ned \nas the minimal size of a directive that is guaranteed to .nd it. DEFINITION 5. For some d = 1, a directive \nD for a program P is a tuple (E, A1, b1,A2,b2,..., Ad ,bd ) where E is an event labeling for P, where \nA1,...,Ad . LE are sets of labels, and where b1,...bd . LE are labels that are pairwise distinct (bi \n..j). The size = bj fori = of D is d and is denoted by size(D). DEFINITION 6. Let P be a program and \nlet D be a directive for P. We say a schedule S . P violates the directive D if either (1) there exists \nan i .{1,...,d} and an a . Ai such that bi . labelsE (S), but a ./labelsE (S), or (2) there exist 1 = \ni < j = d such that b j . labelsE (S), but bi ./labelsE (S). We say a schedule S . P satis.es D if it \ndoes not violate D, and if bi . labelsE (S) for all 1 = i = d. DEFINITION 7. Let P be a program, B be \na bug of P, and D be a directive for P. We say D guarantees B if and only if the following conditions \nare satis.ed: 1. For any partial schedule S . P that does not violate D, there exists a thread t . enP(S) \nsuch that St does not violate D. 2. Any complete schedule S that does not violate D does satisfy D and \nisin B.  DEFINITION 8. Let P be a program, and let B be a bug of P. Then we de.ne the depth ofB tobe \ndepth(B)= min{size(D) | D guarantees B}  6.5.3 Coverage Theorem The following theorem states the key \nguarantee: the probability that one invocation RandS(n, k,d) of our randomized scheduler (Fig. 4) detects \na bug of depth d is at least 1 nkd-1. THEOREM 9. Let P be a program with a bug B of depth d, let n = \nmaxthreads(P), and let k = maxsteps(P). Then 1 Pr[RandS(n,k, d) . B] = nkd-1 PROOF. Because B has depth \nd, we know there exists a directive D for B of size d. Of course, in any real situation, we do not know \nD, but by Def. 8 we know that it exists, so we can use it for the pur\u00adposes of this proof. Essentially, \nwe show that even without knowing D, here is a relatively high probability that RandS(n,k,d) follows \nthe directive D by pure chance. To prove that, we .rst construct an Require: program P, d = 0 Require: \nn >= maxthreads(P) Require: k1,...,kd-1 = 1 Require: p . Permutations(n) Require: random variables k1,...,kd-1 \n.{1,...,k} Require: random variable p . Permutations(n) Require: bug B Require: directive D =(E,A1,b1,...,Ad \n,bd ) for B 1: procedure DirS(n,k,d,D) begin 2: var S : schedule 3: var p : array[n] of N 4: S . e // \nset initial priorities 5: for all t .{1,...,n} do 6: p[t] . d + p(t) - 1 7: end for 8: [ assert: p[threadE \n(b1)] = d ] 9: while enP(S) .= 0/ do /* schedule thread of admissible priority */ 10: t . element of \nenP(S) such that p[t] > d or p[t] maximal 11: S . St /* change priority .rst time we peek a b-label */ \n12: for all i .{1,...,d - 1} do 13: if nextE (S,t)= bi+1 and p[t]=.d - i then 14: p[t]= d - i 15: [ assert: \nlength(S)= ki ] 16: end if 17: end for 18: end while 19: return S 20: end Figure 5. The directed scheduler. \nauxiliary algorithm DirS(n,k, d,D) (Fig. 5) that uses the same ran\u00addom variables as RandS, but has knowledge \nof D and constructs its schedule accordingly. Comparing the two programs, we see two differences. First, \nLine 13 uses a condition based on D to decide when to change pri\u00adorities. In fact, this is where we make \nsure the call to DirS(n, k,d,D) is following the directive D: whenever we catch a glimpse of thread t \nexecuting one of the labels bi (for i > 1), we change the priority of t accordingly. Second, DirS has \nassertions which are not present in RandS. We use these assertions for this proof to reason about the \nprobability that DirS guesses the right random choices. The in\u00adtended behavior is that DirS fails (terminating \nimmediately) if it executes a failing assertion. The following three lemmas are key to our proof construc\u00adtion. \nThe proofs of these lemmas are unchanged from the original proofs [5] except for Lemma 12, and the proofs \nof these lemmas are included in the subsections 6.5.4 through 6.5.7 below. LEMMA 10. The probability \nthat DirS(n,k,d,D) succeeds is at 1 least nkd-1 . LEMMA 11. If DirS(n, k,d,D) succeeds, then RandS(P, \nn,d)= DirS(n,k,d,D). LEMMA 12. If DirS(n, k,d,D) succeeds, it returns a schedule that .nds the bug. We \ncan formally assemble these lemmas into a proof as fol\u00adlows. Our sample space consists of all valuations \nof the random variables p and k1,...,kd-1. By construction, each variable is dis\u00adtributed uniformly and \nindependently (thus, the probability of each valuation is equal to n!kd-1). De.ne S to be the event (that \nis, set of all valuations) such that DirS(n,k,d,D) succeeds, and let S be  its complement. Pr[RandS(n, \nk,d) . B] = Pr[RandS(n,k,d) . B | S] \u00b7 Pr[S] + Pr[RandS(n,k,d) . B | S] \u00b7 Pr[S] = Pr[RandS(n,k,d) . B \n| S] \u00b7 Pr[S] = Pr[DirS(n,k,d, D) . B | S] \u00b7 Pr[S] = 1 \u00b7 Pr[S] (by Lemma 11) (by Lemma 12) = 1 nkd-1 (by \nLemma 10)  6.5.4 Auxiliary Lemmas To prepare for proving the three core lemmas used in the proof above, \nwe state and prove the following four auxiliary lemmas. LEMMA 13. If nextE (S,t)= b j right before executing \nline 11 of DirS(n,k, d,D), and if S does not violate D, then p[t]= d - j + 1. PROOF. Distinguish cases \nj > 1 and j = 1. Case j > 1. Def. 4 implies that the .rst action by any thread is not labeled, so our \nassumption nextE (S,t)= bj implies that there is a m < length(S) such that S[m]= t. Choose a maximal \nsuch m. Then we know nextE (S[0, m],t)= bj (because Def. 4 implies that the next label does not change \nif other threads are scheduled). Thus, in the iteration that added S[m] to the schedule, the test nextE \n(S[0,m],t)= bi+1 on line 13 must have evaluated to true for i = j - 1 (and for no other i, because the \nbi are pairwise distinct by Def. 4). So we must have assigned p[t] . d - j + 1 on line 14. Because we \nchose m maximal, t was never scheduled after that, so its priority did not change and must thus still \nbe p[t]= d - j + 1. Case j = 1. If we are about to execute line 11, then the as\u00adsertion on line 8 must \nhave succeeded, so at that time it was the case that p[t]= d - j + 1. After that, p[t] could not have \nchanged: suppose line 14 was executed at some point to change p[t]. Say the value of variable S at that \npoint was S[0,m]. Then the con\u00addition nextE (S[0,m],t)= bi+1 on line 13 must have evaluated to true for \nsome i. Now, because we know nextE (S,t)= bj and be\u00ad cause bj .> m such that = bi+1 (by Def. 4), there \nmust exist a mS[m.]= t (by Def. 4). But that implies labelE (S,m.)= bi+1, and since bj ./labelsE (S), \nS violates D which contradicts the assump\u00adtion. . LEMMA 14. Let 1 = t.= n,andlet p[t.]= d - j + 1 for \nsome j = 2 at the time line 10 is executed. Then either b j . labelsE (S), or nextE (S,t.)= bj. PROOF. \nThe only way to assign priorities less than d is through the assignment p[t]= d - i on line 14. So this \nline must have executed with t = t. and i = j -1. Thus, the condition nextE (S,t)= bi+1 was true at that \npoint, which is identical to nextE (S,t.)= bj. If t. is not scheduled after that point, this condition \nis still true; conversely, if t. is scheduled, then it must execute the label bj, implying bj . labelsE \n(S). . LEMMA 15. During the execution of DirS(n,k,d,D) the assertion on line 15 is executed at most once \nfor each i .{1,...,d - 1}. PROOF. Consider the .rst time the assertion length(S)= ki on line 15 is executed \nfor a given i. Then it must the case that nextE (S,t)= bi+1. Because labels don t repeat, the only chance \nfor this condition to be true again is for the same i (because the bi are pairwise distinct) during the \nimmediately following iterations of the while loop, and only if threads other than t are scheduled. But \nin that scenario, the priority p[t] does not change, so the sec\u00adond part p[t] .d -i of the condition \non line 13 can not be satis.ed. = LEMMA 16. If DirS(n, k,d,D) succeeds, it executes the assertion length(S)= \nki on line 15 at least once for each i .{1,...,d - 1}. PROOF. Because DirS(n,k,d,D) succeeds, it produces \na complete schedule S which does not violate D. Thus, bi . labelsE (S) for all i .{1,...,d}. Thus, for \neach i .{2,...,d}, there must be an m such that S[0,m],threadE (bi)) = bi. Thus, the condition on line \n13 must be satis.ed at least once for each i .{1,...,d - 1}, so we know the assertion length(S)= ki on \nline 15 gets executed for each i .{1,...,d - 1}. . 6.5.5 Proof of Lemma 11 To prove the claim, we now \nshow that the two respective conditions on lines 13 length(S)= ki (1) nextE (S,t)= bi+1 and p[t] .(2)= \nd - i evaluate the same way, for any given iteration of the while and for loops (identi.ed by current \nvalues of S and i, respectively). Clearly, if (2) evaluates to true for some S and i, DirS executes the \nassertion on line 15, thus guaranteeing that (1) also evaluates to true. Conversely, if (1) evaluates \nto true for some S and i, consider that DirS must execute an assertion of the form length(S.)= ki at \nsome point (by Lemma 16); but it turns out that this must happen in the very same iteration because the \nlength of S uniquely identi.es the iteration of the while loop, so the condition (2) must be satis.ed. \n 6.5.6 Proof of Lemma 10 DirS(n,k,d, D) succeeds if and only if if (1) the assertion p[threadE (b1)] \n= d on line 8 passes, and (2) the assertion length(S)= ki on line 15 passes every time it is executed. \nThe probability of the former passing is 1/n (because a random permutation assigns the lowest priority \nto any given thread with probability 1/n), while the probability of each latter passing is 1/k (because \nthe random variables ki range over {1,..., k}). By Lemma 15, the assertions length(S)= ki are executed \nat most once for each i. Thus, all of the assertions involve independent random variables, so we can \nmultiply the individual success probabilities to obtain a total success probability for DirS of at least \n(1/n) \u00b7 (1/k)d-1.  6.5.7 Proof of Lemma 12 To prove the lemma, we will .rst prove that the following \ninvariant holds during the execution of DirS(n,k, d,D): the variable S is a schedule that does not violate \nD. Clearly, this invariant implies the claim of the lemma (the sched\u00adule returned in the end .nds the \nbug), because if the while-loop terminates, S is a complete schedule, and by Def. 6, any complete schedule \nthat does not violate D is in B. Proving this invariant for PPCT is slightly harder than for PCT, since \nthe scheduler can pick any thread with priority larger than d, not just the highest priority thread. \nHowever, it turns out that this has no ill effect since only threads of priority d or lower are capable \nof breaking the invariant! More formally, we can prove the invariant as follows, proceed\u00ading indirectly. \nIf S violates D, the .rst moment it does so must be right after executing S . St on line 11. Now, consider \nthe state right before that. S does not violate D, but St does, so by Def. 5, there exists an i such \nthat nextE (S,t)= bi. By Lemma 13 this implies that p[t]= d - i + 1. Now, by Def. 6 there must exist \nan alternate choice t.. enP(S) such that St. does not violate D. However, the scheduler did not pick \nt., but t, and t has priority at most d, so t. must have priority less than d. Therefore p[t.]= d - j \n+ 1 for some j > i. By Lemma 14, that implies that either bj . labelsE (S) or bj . nextE (S,t.). But \nboth of these lead to a contradiction: bj . labelsE (S) means that S violates D (because bi ./labelsE \n(S)), and bj . nextE (S,t.) means that St. violates D.  7. Experimental Evaluation of PPCT The goal \nof the experimental evaluation of PPCT is to (1) evaluate the effectiveness of PPCT with respect to bug \n.nding and (2) un\u00adderstand and evaluate the runtime speedups with PPCT algorithm compared to PCT. We \nimplemented PPCT using the NeedlePoint framework. The PPCT scheduling policy obtains the set of non\u00adblocked \nthreads from NeedlePoint and chooses the thread to be scheduled at any given time. PPCT ensures starvation \nfreedom by scheduling one of the lower priority threads for a single schedule point when indicated by \nthe NeedlePoint starvation freedom pol\u00adicy. Using NeedlePoint, implementing a complete PPCT schedul\u00ading \npolicy is just 230 lines of C++ code. 7.1 Effectiveness in Finding Bugs Figure 7.2 shows the effectiveness \nof PPCT in comparison to PCT in terms of .nding concurrency bugs in Table 1. The left and right bars \nshow the number of executions in which the bug was detected by PPCT and PCT respectively. The graph shows \nus that PPCT .nds concurrency bugs either as effectively as PCT and in many cases better than PCT. This \nempirically validates the fact that paral\u00adlel bug exploration with PPCT still retains the detection guarantees \nformally proved in Section 6. We found two new bugs when we were testing multithreaded applications with \nPPCT. During our experimentation with Parsec benchmarks, we found a concurrency bug in the Parsec 2.0 \nbench\u00admark Streamcluster. The root cause of the bug was a missing barrier that caused non-deterministic \noutputs. This bug was simultaneously discovered by others resulting in a patch that is distributed with \nthe latest Parsec versions. We also found a new bug in the Transmis\u00adsion BitTorrent client in which a \nplatform speci.c robustness assert was triggered. 7.2 Performance Evaluation To evaluate the execution \ntime performance overhead of testing par\u00adallel applications, we use benchmarks from the Parsec benchmark \nsuite [4]. We also added the Pbzip2 that performs parallel compres\u00adsion as it is included in our bug \nbenchmarks. We use the native inputs to run the Parsec benchmarks. We performed all the experi\u00adments \non a quad-core dual-socket Intel Core 2 machine (eight cores total). In our performance experiments with \nmultiple cores, we re\u00adstrict the number of threads to be equal to the number of cores in the system. \nThere are three sources of runtime overhead incurred when run\u00adning any scheduling policy with NeedlePoint: \n(1) binary instru\u00admentation overhead, (2) overhead due to NeedlePoint s mechanism to identify blocked \nthreads, and (3) overhead due to serializations caused by the underlying scheduling policy. Introducing \na sched\u00adule point using Pin before synchronization operations and atomic operations without doing anything \nat these schedule points slows down the program by 2\u00d7 compared to native execution on eight cores. Further, \nintroducing a schedule point at every memory ac\u00adcess along with the synchronization accesses introduces \nanother 5\u00d7 slowdown. These represent the overhead of performing binary instrumentation. Figure 6. Bug \ndetection of PPCT compared to PCT. The overheads due to NeedlePoint s mechanisms for identifying blocked \nthreads and scheduling policy serializations are dependent on the individual scheduling policies. Further, \nNeedlePoint mech\u00adanism s overhead for identifying blocked threads can be reduced in the presence of more \ninformation about synchronization opera\u00adtions. Reducing the serializations in the scheduling policy will \nre\u00adduce the third source of overhead. PPCT runs multiple threads and reduces the overhead due to serializations \nwith PCT. Figure 7 presents the execution time overhead of running PCT and PPCT over native execution \non eight cores (smaller bars are better as they represent lower runtime overheads). The graph con\u00adtains \nthree bars for each benchmark. The height of the leftmost bar represents the overhead of running PCT \non eight cores. On an av\u00aderage, the slowdown with PCT is 34\u00d7 compared to the native exe\u00adcution on eight \ncores. The height of the middle bar represents the overhead of run\u00adning PPCT on a single core (which \nwe performed by pinning all the threads in the process to a single core). It shows that running PPCT \neven on a single core reduces a signi.cant portion of the overhead with PCT. This reduction is attributed \nto two reasons: (1) real programs have busy wait synchronization and in the presence of such constructs, \nthe programs makes slow progress while run\u00adning one thread at a time and (2) the underlying operating \nsystem scheduler has more .exibility to choose among the enabled threads to be run with PPCT when compared \nto PCT. On an average, PPCT on a single core incurs an overhead of 11\u00d7 on a average compared to native \nexecution on eight cores. The height of the rightmost bar in Figure 7 presents the overhead of PPCT running \non eight cores. On an average, the slowdown with PPCT is 6\u00d7 compared to native execution on eight cores. \nPPCT serializes at most d threads when performing an explo\u00adration to trigger a bug of depth d. In contrast, \nPCT serializes all the threads. Figure 8 reports the speedup obtained with PPCT when compared to PCT \nfor executions with various bug depths running on eight cores. There are four bars for each benchmark \nrepresenting speedups over PCT exploration for bug depths from 1 to 4. On an average, PPCT provides 439%, \n353%, 278% and 168% speedups on eight cores when compared to PCT for executions with bug depths 1, 2, \n3 and 4 respectively. These results show that PPCT can provide signi.cant speedups over PCT even for \nhigher bug depth explorations. This speedup likely encourages the testing of long running parallel programs \nwith PPCT even in the late stages of testing.  7.3 Testing with Real World Applications Our goal is \nto enable controlled scheduling to be applied to real\u00adworld applications. However, we found PCT suffers \ndebilitating slowdowns as a result of running one thread at a time, which pre\u00advented us from testing \nlarge real-world applications such as the Chrome web browser, the Intel Thread Building Blocks (TBB) \ntest  Figure 7. Runtime slowdown of PCT, PPCT on a single core and PPCT when compared to native multithreaded \nexecution. Figure 8. PPCT speedup over PCT that on a multicore machine for explorations with various \nbug depths. Bug depth of each PPCT explo\u00adration is indicated at the bottom of the bar. suite, and applications \nusing the LevelDB key value store. While experimenting with the Chrome browser, we found that Chrome \ncreates approximately 30 threads in the single process mode, and PCT was executing the operations extremely \nslowly running one thread at a time. Further inspection revealed that starvation with PCT in the presence \nof busy-waiting synchronization also added additional slowdowns. These slowdowns resulted in browser \nwarn\u00adings and prevented us from successfully applying PCT to Chrome. In contrast, PPCT s ability to execute \nmultiple threads reduces the performance overhead signi.cantly, enabling us to successfully test the \nChrome web browser with PPCT without triggering browser warnings. 8. Related Work Existing work on concurrency \nbug detection widely falls into best effort detection and detection with coverage guarantees. A com\u00admonly \nused best effort technique is stress testing where a pro\u00adgram is run multiple times under heavy loads \nto trigger crashes. To introduce scheduling variety, researchers have proposed various heuristics to \n(1) detect suspicious activity in a program (such as variable access patterns that indicate potential \natomicity violations [15, 16, 23], typestate errors [8], crashes [28] or lock acquisition orderings that \nindicate potential deadlocks [12]) and (2) direct the schedule towards suspected bugs [11, 22]. Other \napproaches such model checking [6] systematically enu\u00admerate the possible schedules either exhaustively \nor within some bound, such as the bound on the length of the execution [9] or the number of preemptions \n[21] and provide mathematical guarantees. Burckhardt et. al [5] proposed a randomized algorithm for sched\u00adule \nselection providing probabilistic guarantees of .nding a bug in every run of the algorithm. All these \nprior techniques that provide guarantees run one thread at a time making them impractical for testing \nhighly parallel applications and require mechanisms to care\u00adfully prevent starvation. PPCT is primarily \nmotivated by the need to achieve the scalability of the stress and heuristic-directed testing without \nlosing the ability to provide coverage guarantees. 9. Conclusion This paper compared several previously \nproposed techniques for concurrency bug detection using a common set of concurrency bugs on a common \nframework. We found that simple policies do work well for concurrency bugs that are localized but not \nso well for hidden bugs. For hidden bugs, we propose PPCT, that not only detects these bugs but also \ndetects them while running programs more than .ves times faster than the serial PCT while retaining the \nmathematical coverage guarantees. Together, NeedlePoint and PPCT provide a framework and a scheduling \npolicy for testing large real-world programs. Acknowledgments The authors thank Nicholas Jalbert and \nJie Yu for providing infor\u00admation about RADBench and sharing the concurrency bugs respec\u00adtively. We also \nthank the reviewers for their feedback on the pa\u00adper. This paper is based upon work supported by the \nNational Sci\u00adence Foundation (NSF) under Grant No. CCF-0644197 and CCF\u00ad0905464. Any opinions, .ndings \nand conclusions or recommenda\u00adtions expressed in this paper are those of the authors and do not necessarily \nre.ect the views of NSF. References [1] Helgrind: A Thread Error Detector. http://valgrind.org/docs/ \nmanual/hg-manual.html.  [2] Intel Thread Checker. http://software.intel.com/en-us/ articles/intel-thread-checker-documentation/. \n[3] S. Adve and K. Gharachorloo. Shared Memory Consistency Models: A Tutorial. Computer, 29(12):66 76, \n1996. [4] C. Bienia. Benchmarking Modern Multiprocessors. PhD thesis, Princeton University, January 2011. \n[5] S. Burckhardt, P. Kothari, M. Musuvathi, and S. Nagarakatte. A Ran\u00addomized Scheduler with Probabilistic \nGuarantees of Finding Bugs. In Proceedings of the 15th International Conference on Architectural Support \nfor Programming Languages and Operating Systems (ASP-LOS), 2010. [6] E. Clarke, O. Grumberg, and D. Peled. \nModel Checking. MIT Press, 1999. [7] O. Edelstein, E. Farchi, E. Goldin, Y. Nir, G. Ratsaby, and S. Ur. \nFramework for testing multi-threaded Java programs. Concurrency and Computation: Practice and Experience, \n15(3 5):485 499, 2003. [8] Q. Gao, W. Zhang, Z. Chen, M. Zheng, and F. Qin. 2ndStrike: Toward Manifesting \nHidden Concurrency Typestate Bugs. In Proceedings of the 16th International Conference on Architectural \nSupport for Pro\u00adgramming Languages and Operating Systems (ASPLOS), 2011. [9] P. Godefroid. Model Checking \nfor Programming Languages using VeriSoft. In Proceedings of the 24th ACM Symposium on Principles of Programming \nLanguages (POPL), 1997. [10] N. Jalbert, C. Pereira, G. Pokam, and K. Sen. RADBench: A Con\u00adcurrency Bug \nBenchmark Suite. In Proceedings of the 3rd USENIX Conference on Hot Topics in Parallelism (HotPar), 2011. \n[11] P. Joshi, M. Naik, C.-S. Park, and K. Sen. CalFuzzer: An Extensible Active Testing Framework for \nConcurrent Programs. In Proceedings of the 21st International Conference on Computer Aided Veri.cation \n(CAV), 2009. [12] P. Joshi, C.-S. Park, K. Sen, and M. Naik. A Randomized Dynamic Program Analysis Technique \nfor Detecting Real Deadlocks. In Pro\u00adceedings of the ACM SIGPLAN 2009 Conference on Programming Language \nDesign and Implementation (PLDI), 2009. [13] H. Jula, D. Tralamazza, C. Zam.r, and G. Candea. Deadlock \nImmu\u00adnity: Enabling Systems to Defend Against Deadlocks. In Proceedings of the 8th USENIX Symposium on \nOperating Systems Design and Im\u00adplementation (OSDI), 2008. [14] S. Lu, S. Park, E. Seo, and Y. Zhou. \nLearning from Mistakes: A Comprehensive Study on Real World Concurrency Bug Characteris\u00adtics. In Proceedings \nof the 13th International Conference on Archi\u00adtectural Support for Programming Languages and Operating \nSystems (ASPLOS), 2008. [15] S. Lu, J. Tucek, F. Qin, and Y. Zhou. AVIO: Detecting Atomicity Vio\u00adlations \nvia Access Interleaving Invariants. In Proceedings of the 12th International Conference on Architectural \nSupport for Programming Languages and Operating Systems (ASPLOS), 2006. [16] B. Lucia and L. Ceze. Finding \nConcurrency Bugs with Context-Aware Communication Graphs. In Proceedings of the 42nd Annual IEEE/ACM \nInternational Symposium on Microarchitecture (MICRO), 2009. [17] C.-K. Luk, R. Cohn, R. Muth, H. Patil, \nA. Klauser, G. Lowney, S. Wal\u00adlace, V. J. Reddi, and K. Hazelwood. Pin: Building Customized Pro\u00adgram \nAnalysis Tools with Dynamic Instrumentation. In Proceedings of the ACM SIGPLAN 2005 Conference on Programming \nLanguage Design and Implementation (PLDI), 2005. [18] J. Manson, W. Pugh, and S. Adve. The Java Memory \nModel. In Pro\u00adceedings of The 32nd ACM SIGPLAN/SIGACT Symposium on Princi\u00adples of Programming Languages \n(POPL), 2005. [19] M. Musuvathi and S. Qadeer. Iterative Context Bounding for System\u00adatic Testing of \nMultithreaded Programs. In Proceedings of the ACM SIGPLAN 2007 Conference on Programming Language Design \nand Implementation (PLDI), 2007. [20] M. Musuvathi and S. Qadeer. Fair Stateless Model Checking. In Proceedings \nof the ACM SIGPLAN 2008 Conference on Programming Language Design and Implementation (PLDI), 2008. [21] \nM. Musuvathi, S. Qadeer, T. Ball, G. Basler, P. A. Nainar, and I. Neamtiu. Finding and Reproducing Heisenbugs \nin Concurrent Pro\u00adgrams. In Proceedings of the 8th USENIX Symposium on Operating Systems Design and Implementation \n(OSDI), 2008. [22] C.-S. Park and K. Sen. Randomized Active Atomicity Violation De\u00adtection in Concurrent \nPrograms. In Proceedings of the 16th ACM SIG-SOFT International Symposium on the Foundations of Software \nEngi\u00adneering (FSE), 2008. [23] S. Park, S. Lu, and Y. Zhou. CTrigger: Exposing Atomicity Viola\u00adtion Bugs \nfrom their Hiding Places. In Proceedings of the 14th Inter\u00adnational Conference on Architectural Support \nfor Programming Lan\u00adguages and Operating Systems (ASPLOS), 2009. [24] K. Sen. Effective Random Testing \nof Concurrent Programs. In Pro\u00adceedings of 22nd IEEE/ACM International Conference on Automated Software \nEngineering (ASE), 2007. [25] K. Sen. Race Directed Random Testing of Concurrent Programs. In SIGPLAN \nConference on Programming Language Design and Imple\u00admentation (PLDI), 2008. [26] W. Xiong, S. Park, J. \nZhang, Y. Zhou, and Z. Ma. Ad Hoc Synchro\u00adnization Considered Harmful. In Proceedings of the 9th USENIX \nSym\u00adposium on Operating Systems Design and Implementation (OSDI), 2010. [27] J. Yu and S. Narayanasamy. \nA Case for an Interleaving Constrained Shared-Memory Multi-Processor. In Proceedings of the 36th Annual \nInternational Symposium on Computer Architecture (ISCA), 2009. [28] W. Zhang, C. Sun, and S. Lu. ConMem: \nDetecting Severe Concur\u00adrency Bugs through an Effect-Oriented Approach. In Proceedings of the 15th International \nConference on Architectural Support for Pro\u00adgramming Languages and Operating Systems (ASPLOS), 2010. \n     \n\t\t\t", "proc_id": "2254064", "abstract": "<p>Testing multithreaded programs is difficult as threads can interleave in a nondeterministic fashion. Untested interleavings can cause failures, but testing all interleavings is infeasible. Many interleaving exploration strategies for bug detection have been proposed, but their relative effectiveness and performance remains unclear as they often lack publicly available implementations and have not been evaluated using common benchmarks. We describe NeedlePoint, an open-source framework that allows selection and comparison of a wide range of interleaving exploration policies for bug detection proposed by prior work.</p> <p>Our experience with NeedlePoint indicates that priority-based probabilistic concurrency testing (the PCT algorithm) finds bugs quickly, but it runs only one thread at a time, which destroys parallelism by serializing executions. To address this problem we propose a parallel version of the PCT algorithm~(PPCT). We show that the new algorithm outperforms the original by a factor of 5x when testing parallel programs on an eight-core machine. We formally prove that parallel PCT provides the same probabilistic coverage guarantees as PCT. Moreover, PPCT is the first algorithm that runs multiple threads while providing coverage guarantees.</p>", "authors": [{"name": "Santosh Nagarakatte", "author_profile_id": "81435608524", "affiliation": "University of Pennsylvania, Philadelphia, PA, USA", "person_id": "P3471321", "email_address": "santoshn@cis.upenn.edu", "orcid_id": ""}, {"name": "Sebastian Burckhardt", "author_profile_id": "81350574118", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P3471322", "email_address": "sburckha@microsoft.com", "orcid_id": ""}, {"name": "Milo M.K. Martin", "author_profile_id": "81502772577", "affiliation": "University of Pennsylvania, Philadelphia, PA, USA", "person_id": "P3471323", "email_address": "milom@cis.upenn.edu", "orcid_id": ""}, {"name": "Madanlal Musuvathi", "author_profile_id": "81488665482", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P3471324", "email_address": "madanm@microsoft.com", "orcid_id": ""}], "doi_number": "10.1145/2254064.2254128", "year": "2012", "article_id": "2254128", "conference": "PLDI", "title": "Multicore acceleration of priority-based schedulers for concurrency bug detection", "url": "http://dl.acm.org/citation.cfm?id=2254128"}