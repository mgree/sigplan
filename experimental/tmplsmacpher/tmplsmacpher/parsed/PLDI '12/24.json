{"article_publication_date": "06-11-2012", "fulltext": "\n Type-Directed Completion of Partial Expressions Daniel Perelman Sumit Gulwani Thomas Ball Dan Grossman \nUniversity of Washington Microsoft Research Redmond University of Washington perelman@cs.washington.edu \n{sumitg,tball}@microsoft.com djg@cs.washington.edu Abstract Modern programming frameworks provide enormous \nlibraries ar\u00adranged in complex structures, so much so that a large part of mod\u00adern programming is searching \nfor APIs that surely exist some\u00adwhere in an unfamiliar part of the framework. We present a novel way \nof phrasing a search for an unknown API: the programmer simply writes an expression leaving holes for \nthe parts they do not know. We call these expressions partial expressions. We present an ef.cient algorithm \nthat produces likely completions ordered by a ranking scheme based primarily on the similarity of the \ntypes of the APIs suggested to the types of the known expressions. This gives a powerful language for \nboth API discovery and code completion with a small impedance mismatch from writing code. In an auto\u00admated \nexperiment on mature C# projects, we show our algorithm can place the intended expression in the top \n10 choices over 80% of the time. Categories and Subject Descriptors D.2.6 [Software Engineer\u00ading]: Programming \nEnvironments Integrated Environments; D.2.13 [Software Engineering]: Reusable Software Reuse Models; \nI.2.2 [Arti.cial Intelligence]: Automatic Programming Program syn\u00adthesis General Terms Languages, Experimentation \nKeywords program synthesis, partial expressions, code comple\u00adtion, type-based analysis, ranking 1. Introduction \nModern programming frameworks such as those found in Java and .NET consist of a huge number of classes \norganized into many namespaces. For example, the .NET Framework 4.0 has over 280,000 methods, 30,000 \ntypes, and 697 namespaces. Discovering the right method to achieve a particular task in this huge frame\u00adwork \ncan feel like searching for a needle in a haystack. Program\u00admers often perform searches through unfamiliar \nAPIs using their IDE s code completion, for example Visual Studio s Intellisense, which requires the \nprogrammer to either provide a receiver or it\u00aderate through the possible receivers by brute force. Fundamentally, \ntoday s code completion tools still expect programmers to .nd the right method by name (something that \nimplicitly assumes they will know the right name for the concept, which may not be true [4]) and to .ll \nin all the arguments. Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. Our approach: We de.ne a language of partial expressions, in which programmers can indicate \nin a superset of the language s concrete syntax that certain subexpressions need to be .lled in or possibly \nreordered. We interpret a partial expression as a query that returns a ranked list of well-typed completions, \nwhere each completion is a synthesized small code snippet. This model is sim\u00adple, general, and precisely \nspeci.ed, allowing for a variety of uses and extensions. We developed an ef.cient algorithm for generating \ncompletions of a partial expression. We also developed a ranking scheme primarily based on (sub)typing \ninformation to prefer more precise expressions (e.g., a method taking AVerySpecificType rather than Object). \nCode-completion problems addressed: We have used our partial expression language and its implementation \nfor .nding comple\u00adtions to address three code-completion problems: 1. Given k arguments and without distinguishing \none as the object\u00adoriented receiver, predict a method call including these and possibly other arguments. \nNote that the name of the method being called is not given; the method name, along with the order of \narguments to it, is the output of our system. 2. Given a method call with missing arguments, predict \nthese ar\u00adguments (with simple expressions such as variables or .eld and property1 lookups of variables). \n 3. Given an incomplete binary expression such as an assignment statement, predict .eld and property \nlookups (i.e., given e pre\u00addict e.f) on the left or right side of the operation.  Results: We demonstrate \nthat our approach ranks the correct re\u00adsult highly most of the time and often outperforms or complements \nexisting widely deployed technologies like Intellisense. To collect a large amount of empirical data, \nwe have chosen to leave IDE in\u00adtegration and user studies to future work. Instead, we take existing codebases \nand run our tool after automatically replacing existing method calls, assignments, and comparisons with \nappropriate par\u00adtial expressions. On our corpus of programs, results include: For over 80% of method \ncalls (and over 90% if we know the call s return type), there are two or fewer arguments to the call \nsuch that with only those arguments, our system will rank the intended method name within the top 10. \n If a simple argument (e.g., a variable) is omitted from a method call, our system can .ll it back in \ncorrectly (the top-ranked choice) 55% of the time.  When a .eld or property lookup is omitted from an \nexpression, we can use surrounding type context to rank the missing prop\u00aderty in the top 10 over 90% \nof the time.  Overall, our work demonstrates that IDEs could use already\u00adavailable type information \nto help programmers .nd methods they want and save keystrokes much more than they do today. PLDI 12, \nJune 11 16, 2012, Beijing, China. Copyright &#38;#169; 2012 ACM 978-1-4503-1205-9/12/06. . . $10.00 1 \nProperties are syntactic sugar for writing getters and setters like .elds. c  Figure 1. Work.ow Contributions \nThis paper includes the following contributions: Identi.cation of the need for a search facility based \non partial expressions which we have detailed using illustrative examples in Section 2.  Design of the \npartial expressions language and how partial ex\u00adpressions relate to complete expressions as formally \nde.ned in Section 3.  An ef.cient algorithm for completing partial expressions and ranking the results \ndescribed in Section 4, which integrates the ranking procedure and off-line indexing of large libraries. \n A large experimental evaluation of the quality and performance of our algorithm on real code presented \nin Section 5.  An evaluation of the relative and absolute importance of each  ranking feature detailed \nin Section 5.4. Section 6 reviews related work and Section 7 concludes the paper. 2. Illustrative Examples \nThis section describes three examples that use partial expressions in our system and then considers performing \nthe same tasks using prior work. Section 2.1 describes how our system handles these ex\u00ad amples. Section \n2.2 discusses normal code completion. Section 2.3 covers the closest related work, the Prospector tool[10]. \n2.1 Our system Synthesizing Method Names Suppose you are writing code using an image editing API (specif\u00adically, \nthe Paint.NET image editor2) and want to .gure out how to make an image smaller. Your .rst instinct may \nbe to write img.Shrink(size). Unfortunately, that API does not exist; the actual API for shrinking an \nimage is public static Document ResizeDocument( Document document, Size newSize, AnchorEdge edge, ColorBgra \nbackground) We will step through how our tool handles this example, using Figure 1 which shows the work.ow \nof our tool. For this example, you would write the query ?({img, size}) in the partial ex\u00adpression language \ndescribed in Section 3. The query is passed to the algorithm represented by the large box described in \nSection 4 which also has access to the code context which says that img and size are local variables \nof types Document and Size, respectively. The .rst ten elements of the ranked result set are shown in \nFigure 2. The static ResizeDocument method is the .rst choice. Synthesizing Method Arguments Suppose \nyou already know there is a method Distance that returns the distance between two Point objects, but \nare not sure where one 2 http://www.getpaint.net/ PaintDotNet.Actions.CanvasSizeAction .ResizeDocument(img, \nsize, 0, 0) PaintDotNet.Functional.Func.Bind(0, size, img) PaintDotNet.Pair.Create(size, img) PaintDotNet.Quadruple.Create(size, \nimg, 0, 0) PaintDotNet.Triple.Create(size, img, 0) PaintDotNet.PropertySystem .StaticListChoiceProperty \n.CreateForEnum(img, size, 0) System.Drawing.Size.Equals(size, img) System.Object.ReferenceEquals(size, \nimg) PaintDotNet.Document.OnDeserialization(img, size) PaintDotNet.PropertySystem.Property .Create(0, \nsize, img) Figure 2. The .rst ten results generated and ranked by our system for the query ?({img, size}). \npoint this.BeginLocation this.Center this.EndLocation DynamicGeometry.Math.InfinitePoint shapeStyle.GetSampleGlyph() \n.RenderTransformOrigin this.shape.RenderTransformOrigin this.ArcShape.Point this.Figure.StartPoint this.Shape.RenderTransformOrigin \nFigure 3. The .rst ten results generated and ranked by our system for the ? in the query Distance(point, \n?). of the endpoints is de.ned. The query Distance(point, ?) produces a list of Points that could be \n.lled in as the second argu\u00adment. This includes any locals, .elds, or static .elds or methods or recursively \nany .elds of those of type Point. For example, Figure 3 shows the results of that query in the context \nof the EllipseArc class of the DynamicGeometry library. There, point is the only local variable of type \nPoint. In this case, the actual argument was this.Center which appears third in the list. Synthesizing \nField Lookups For a more targeted version of the above, the search can be nar\u00adrowed by specifying the \nbase object to look under. We will consider synthesizing .eld lookups in the context of a comparison \noperator. The query point.?*m >= this.?*m includes point and this along with zero or more .eld lookups \nor zero-argument instance method calls after them. The top ten ranked completions for this query are \nlisted in Figure 4. Note that by completing both holes si\u00ad multaneously, only completions where the two \nsides have .elds of compatible types are shown.  2.2 Code Completion Today, programmers can use code \ncompletion such as Intellisense in Visual Studio to try to navigate unfamiliar APIs. Intellisense completes \ncode in sections separated by periods ( . ) by using the type of the expression to the left of the period \nand textually search\u00ading through the list for any string the programmer types. If there is no period, \nthen Intellisense will list the available local variables, types, and namespaces. This often works well, \nparticularly when the programmer has a good idea of where the API they want is or if there are relatively \nfew choices. On the other hand, it performs poorly on our examples.  point.X >= this.P1.X point.X >= \nthis.P2.X point.X >= this.Midpoint.X point.X >= this.FirstValidValue().X point.Y >= this.P1.Y point.Y \n>= this.P2.Y point.Y >= this.Midpoint.Y point.Y >= this.FirstValidValue().Y point.X >= this.Length point.Y \n>= this.Length Figure 4. The .rst ten results generated and ranked by our system for the query point.?*m \n>= this.?*m. Synthesizing Method Names Using Intellisense to .nd the nonexistent Shrink method, a pro\u00adgrammer \nmight type img.shr , see that there is no Shrink method, and then skim through the rest of the instance \nmethods. As that will also fail to .nd the desired method, the program\u00admer might continue by typing in \nPaintDotNet. and use Intel\u00adlisense to browse the available static methods, eventually .nding PaintDotNet.Actions.CanvasSizeAction \nwhere the method is located. Hopefully documentation on the various classes and namespaces shown by Intellisense \ns tooltip will help guide the pro\u00adgrammer to the desired method, but this is dependent on the API designer \ndocumenting their code well and the documentation us\u00ading terminology and abstractions that the programmer \nunderstands. A programmer would likely search through many namespaces and classes before happening upon \nthe right one. Synthesizing Method Arguments If the user has already entered Distance(point, and then \ntriggers Intellisense, Intellisense will list of every namespace, type, variable, and instance method \nin context even though many choices will not type-check (as the programmer may intend to call a method \nor perform a property lookup on one of those objects). When the list is brought up, the most recently \nused local variable of type Point, which would be point in this case, will be selected. The programmer \nwill have to read through many unrelated options to locate the other values of type Point. Note that \nEclipse s code completion is actually signi.cantly different in this scenario. It will list all of the \nlocal variables valid for the argument position along with common constants like null. If a more complicated \nexpression is desired, the user has to cancel out and request the normal code completion which is similar \nto Visual Studio s. Synthesizing Field Lookups Given point. , Intellisense will list all .elds and methods \nof that object. The listing will go only one level deep: if the user wants a .eld of a .eld, they have \nto know which .eld to select .rst.  2.3 Prospector The Prospector tool by Mandelin et al.[10] is an \nAPI discovery tool which constructs values using mined jungloids which con\u00advert from one input type to \none output type and are combined into longer jungloids. The tool uses a local variable to construct a \nvalue of the output type. The motivating example in this prior work is converting an IFile to an ASTNode \nin the Eclipse API which re\u00adquires a non-obvious intermediate step involving a third type: IFile file \n= ...; ICompilationUnit cu = JavaCore.createCompilationUnitFrom(file); ASTNode ast = AST.parseCompilationUnit(cu, \nfalse); (a) e ::= call | varName | e.fieldName | e:=e | e<e call ::= methodName(e1, ... ,en) (b) ee::= \nea | ? |0 ea ::= e | ea.?f | ea.?*f | ea.?m | ea.?*m | c call | ee:=ee| ee<ee c call ::= ?({ee1, ... \n,een}) | methodName(ee1, ... ,een) Figure 5. (a) Expression language (b) Partial expression language \nThe Prospector UI triggers queries only at assignments to vari\u00adables, but that is a minor implementation \ndetail. Synthesizing Method Names As Prospector can consider only one type as input, a program\u00admer might \nquery for a conversion from Size to Document or from Document to Document, which does not quite match \nthe program\u00admers intuition of wanting to resize the document. Prospector will return methods with arguments \nit cannot .ll in. It prefers fewer un\u00adknown arguments, so ResizeDocument would likely be rather far down \nin the list of options for either query. Synthesizing Method Arguments Queried for type Point, Prospector \nwould give a similar list to the one our tool creates, although it does not consider globals as possible \ninputs to its algorithm. Speci.cally, Prospector would give any locals of the proper type and recursively \n.nd any .elds of the proper type. It may also .nd chains that involve downcasts found to work elsewhere \nin the codebase, which our tool would not .nd. Synthesizing Field Lookups Prospector does not take suggestions \nof starting points from the user, although its UI could theoretically be modi.ed to do so. On the other \nhand, Prospector has only one target type and cannot make more complicated expressions like the one above \nwith a >= operator. The closest corresponding use of Prospector would be to guess the type for either \nside of the comparison and have Prospector .nd .elds of that type. 3. Partial expression language Queries \nin our system are partial expressions. A partial expression is similar to a normal (or complete ) expression \nexcept some information may be omitted or reordered. A partial expression can have many possible completions \nformed by .lling in the holes and reordering subexpressions in different ways. Complete expression syntax \nBefore de.ning partial expressions, we .rst de.ne a simple ex\u00adpression language given by the e and call \nproductions in Fig\u00adure 5(a), which models features found in traditional programming languages. Our simple \nlanguage has variables, .eld lookups, as\u00adsignments, a comparison operator, and method calls. (Other opera\u00adtors \nare omitted from the formalism.) Also, the receiver of a method call is considered to be its .rst argument \nin order to simplify nota\u00adtion as when reordering arguments, an argument other than the .rst may be chosen \nas the receiver. Partial expression syntax Partial expressions are de.ned by the ee, ecall productions \na, and cin Figure 5(b). Partial expressions support omitting the following classes of unknown information: \nEntire subexpressions. ? gives no information about the struc\u00adture of the expression, only that it is \nmissing and should be .lled in. On the other hand, 0 should not be .lled in: it indicates a  e . ee \n. ee . e e .? . ee .?m . e.m() e .?f . e.f e .?*f . e .?f.?*f e .?*m . e .?m.?*m e .?m . e .?f e 1 . \ne1 e 2 . e2 e 1 . e1 e 2 . e2 e 1:=e 2 . e1:=e2 e 1<e 2 . e1<e2 e i . ei m(e 1, ... ,e i, ... ,e n) \n. m(e 1, ... ,ei, ... ,e n) e i . ei ?({e 1, ... ,e i, ... ,e n}) . ?({e 1, ... ,ei, ... ,e n}) k = n, \nej = 0 for j>n s . Sk ?({e1,...,en}) . m(es1 ,...,esk ) v is a live local or global variable ? . v.?*m \n e 1 . e 2 e 2 . e 3 e 1 . e 3 Figure 6. Semantics of partial expressions. The .nal result must type-check \nin the context of the query, treating 0 as having any type. subexpression to ignore due to being independent \nof the current query (so making it a ? would only add irrelevant results) or simply being a subexpression \nthe programmer intends to .ll in later, perhaps due to working left-to-right. Field lookups. The ea \nproduction de.nes a series of four .? suf\u00ad.xes which are slightly different ways of saying that an expres\u00adsion \nis missing one or more .eld lookups or the desired expres\u00adsion is actually the result of a method call \non the expression. The f suf.x is short for field and can be completed as a sin\u00adgle .eld lookup or nothing. \nThe .?* suf.xes complete as the .? versions repeated as many times as needed.  Simple method calls. \nThe .?m suf.x is like the .?f suf.x. The m is short for zero-argument method call and can be completed \nas a call to an instance method with zero additional arguments or also as a .eld lookup or nothing. \nWhich method to call. ?({e 1,e 2}) represents a call to some unknown method with two known arguments, \nwhich may them\u00adselves be partial expressions.  Number and ordering of arguments to a method. For un\u00adknown \nmethods, there may also be additional arguments missing or the arguments may be out of order, which is \nrepresented by the use of set notation for the arguments in ?({e 1,e 2}).  Partial expression semantics \nFigure 6 gives the full semantics of the partial expression language. The . judgement nondeterministically \ntakes a partial expression to a complete expression with the exception that any 0 subexpressions remain. \nWith the exception of the .?* rules, each rule removes or re.nes some hole, making the partial expression \none step closer to a complete expression. The bottom rule allows for the composition of other rules. \nThe top leftmost rule allows any of the .? suf.xes to be omitted. For type checking, 0 is treated as \na wildcard: as long as some choice of type for the 0 works, the expression is considered to type check. \nThe actual algorithm implemented does not use these rules exactly, although it matches their semantics. \nThe partial expressions language semantics never add opera\u00adtions like multiplication or new method calls \n(other than to zero\u00adargument methods). The idea is that any place where computation is intended should \nbe explicitly speci.ed, and the completions sim\u00adply list speci.c APIs for the computations. The exception \nfor zero\u00adargument methods is made because they are often used in place of properties for style reasons \nor due to limitations of the underlying language. Examples The .rst example from Section 2, ?({img, size}), \nis a method call with an unknown name and two complete expressions as arguments. It can be expanded to \nany method that can take those two variables in any two of its argument positions, so Triple.Create(0, \nsize, img) is a valid completion. Note that no attempt is made to .ll in the extra argument. This is \ndone to reduce the number of choices when recommending methods; for other applications fully completing \nthe expression may be useful. The user may afterward decide to convert the 0 to ? or some other partial \nexpression. Our second example from Section 2, Distance(point, ?) can take one step to one of Distance(point, \npoint.?*m),  Distance(point, this.?*m),  Distance(point, shapeStyle.?*m)  or many other possibilities. \nAny local in scope or global (static .eld or zero-argument static method) could be chosen to appear before \nthe .?*m. Whatever is selected is completed to some expression of type Point. Any .? suf.x can be omitted \nwhen completing an ex\u00adpression, so point.?*m can be completed as point which is the .rst option in Figure \n3. this.?*m can also become one or more lookups by going to this.?m.?*m in one step and the .?m becomes \nsome .eld. For ArcShape, this.ArcShape.?*m is further com\u00adpleted to this.ArcShape.Point. Any of the completions \nmen\u00adtioned so far would have been valid for .?f instead of .?m as well. On the other hand, for shapeStyle.?*m, \nthe .rst .?m from the .?*m is completed with an instance method .GetSampleGlyph() that returns an object \nwith a .eld RenderTransformOrigin of type Point which the remaining .?*m can complete to. An unknown \nmethod s arguments may themselves be partial expressions. For example, ?({strBuilder.?*m,e.?*m}) could \nexpand to Append(strBuilder, e.StackTrace) (which would normally be written as strBuilder.Append(e.StackTrace)). \nThe third example from Section 2, point.?*m >= this.?*m, also uses .?*m, so the completions work as above, \nbut, as there are two of them in the expression related by the >=, there must be a def\u00adinition of >= \nwhich is type compatible with the two completions. In this example, all the comparable .elds have types \nint or double. But suppose Point had a .eld Timestamp of type DateTime; then Point.Timestamp >= this.P1.Timestamp \nwould be a valid completion, but Point.X >= this.P1.Timestamp would not. 4. Algorithm This section describes \nan algorithm (represented by the boxed sec\u00adtion of Figure 1) for completing partial expressions. The \nalgorithm takes a partial expression and an integer n as input and returns an ordered list of n proposed \ncompletions. The algorithm has access to static information about the surrounding code and libraries: \nthe types of the values used in the expression, the locals in scope, and the visible library methods \nand .elds. Bounding n is important be\u00adcause some partial expressions have an in.nite list of completions. \nWhat constitutes a valid completion is de.ned by Figure 6. The algorithm described in this section does \ncompletion .nding and ranking simultaneously in order to compute the top n comple\u00adtions ef.ciently. Section \n4.1 describes the ranking function. Sec\u00adtion 4.2 describes the completion .nder and the integrated algo\u00adrithm \nwhose design is informed by the ranking function.  = 22 s.subexps(expr) score(s) the more general type, \nso the type distance between the two argu\u00ad score(expr) + td(type(s) , type(param(s))) ments to the operator \nis used. s.subexps(expr) +2 \u00b7 dots(expr) + s.subexps(expr) Depth The next term prefers expressions with \nfewer subexpres\u00ad abstype(s) sions. The ranking scheme prefers shorter expressions by comput\u00ad = abstype(param(s)) \ning the complexity of the expression which is approximated by the scorec(call)= score(call) number of \ndots in the expression and multiplying that value by 2 to weight it more heavily. For example, dots( \nthis.foo )=1 so it would get a cost of 2 while dots( this.bar.ToBaz() ) = 2 so it would get a cost of \n4. To avoid double counting, any dots which +(isInstance(call) . isNonLocalStatic(call)) 0, 3 - (nsArgs(call) \n= reciever(call)) s + max  \u00b7 s.nsArgs(call) ns(type(a))  are part of subexpressions are not counted \nhere and instead are in\u00ad scorecmp(expr1, expr2)= score(expr1 < expr2) +3\u00b7(name(expr1) = name(expr2)) \nnsArgs(call)= {a . subexps (call) | type(a) is not primitive} Figure 7. The ranking function. Note that \nboolean values are con\u00adsidered 1 if true and 0 if false and that abstract types (abstype(\u00b7)) are considered \nnot equal if both are undefined. 4.1 Ranking This section de.nes a function that maps completed expressions \nthat may contain 0 subexpressions to integer scores. This function is used to rank the results returned \nby the completion .nder in ascending order of the ranking score (i.e., a lower score is better). The \nfunction is de.ned such that each term is non-negative, so if any subset of the terms are known, their \nsum is a lower bound on the ranking score and can be used to prune the search space. The computation \nis a sum of various terms summarized in Fig\u00adure 7. score(\u00b7) applies to all expressions while scorec(\u00b7) \nis a spe\u00adcialized version with tweaks for method calls and scorecmp(\u00b7, \u00b7) is a specialized version with \na tweak for comparisons. The com\u00adputation is de.ned recursively, so for methods or operators with arguments, \nthe sum of the scores of their arguments is added to the score. The scoring function incorporates several \nfeatures we designed based on studying code examples and our own intuition. This section explains these \nfeatures in detail. Section 5.4 evaluates each feature s contribution to our empirical results. Type \ndistance The primary feature in the ranking function is type distance , for example from a method call \nargument s type to the type of the corresponding method parameter. Informally, it is the distance in \nthe class hierarchy, extended to consider prim\u00aditive types, interfaces, etc. For example, if Rectangle \nextends Shape which extends Object, td(Rectangle, Shape)=1 and td(Rectangle, Object)=2. Far away types \nare less likely to be used for each other, so method calls and binary operations where the arguments \nhave a higher type distance are less likely to be what the user wanted. Formally, the type distance from \na type a usable in a position of type \u00df to that type \u00df, td(a, \u00df), is de.ned as follows: cluded via the \nsubexpressions score term. In-scope static methods Instance method calls will tend to have a type distance \nof zero for the receiver, so type distance has an implicit bias against static method calls. Noting that \nstatic methods of the enclosing type can be called without quali.cation, just like instance methods with \nthis as the receiver, our ranking algorithm should similarly not disfavor such in-scope static methods. \nThis is .xed by adding a cost of 1 if either the method is an instance method or the method is a static \nmethod that is not in scope. Common namespace As related APIs tend to be grouped into nearby namespaces, \nthe algorithm prefers calls where the types of all the arguments with non-primitive types and the class \ncontain\u00ading the method de.nition are all in the same namespace. Primitive types, including string, are \nignored in this step because they are used with varying semantics in many different libraries. Further\u00admore, \ndeeper namespaces tend to be more precise so a deep com\u00admon namespace indicates the method is more likely \nto be related to all of the provided arguments. Speci.cally, the algorithm takes the set of all namespaces \nof non-primitive types among the argu\u00adments, treats them as lists of strings (so System.Collections is \n[ System , Collections ]), .nds the (often empty) common pre.x, and uses its length to compute the namespace \nscore . To avoid this boosting the scores of instance calls with only one non\u00adprimitive argument, the \nsimilarity score is 0 in that case. In order to have the namespace similarity term be non-negative, namespace \nsimilarities are capped at 3, and 3 minus the length of the common pre.x is used as the common namespace \nterm. Same name Comparisons are often made between correspond\u00ading .elds of different objects. Whether \ntwo .elds have corre\u00adsponding meanings can be approximated by checking if they have the same name. That \nis, p.X is more likely to be compared to this.Center.X than to this.Center.Y. To capture this, a 3 point \npenalty is assigned to comparisons where the last lookups on the two sides do not have the same name. \nThe value is intentionally chosen to be greater than the cost of a lookup, so a slightly longer expression \nthat ends with a .eld of the right name is considered better than a shorter one that does not. Abstract \ntype inference td(a, \u00df)= . . .. . .. undefined 0 no implicit conversion of a to \u00df if a = \u00df We now introduce \nan important re.nement to the basic ranking function that partitions types into abstract types based \non us\u00adage, which is particularly important for commonly used types like 1 1 + td(s(a), \u00df) if a and \u00df \nare primitive types otherwise string. Abstract types may have richer semantics than string such as path \nor font family name . Our approach is based on the Lackwit tool that infers abstract types of integers \nin C[11]. s(a) is the explicitly declared immediate supertype of a which to minimizes td(s(a),\u00df). Note \nthat td(a, \u00df) is used by the ranking function only when it is de.ned as that corresponds to the expres\u00adsion \nbeing type correct. The type distance term is the sum of the type distances from the type of each argument \narg to the type of the corresponding formal parameter param(arg). For method calls this is well-de.ned; \nbi\u00adnary operators are treated as methods with two parameters both of Abstract types are computed automatically \nusing type inference. An abstract type variable is assigned to every local variable, formal parameter, \nand formal return type, and a type equality constraint is added whenever a value is assigned or used \nas a method call argument. As all constraints are equality on atoms, the standard uni.cation algorithm \ncan be implemented using union-.nd. In order to avoid merging every abstract type .ToString() or .GetHashCode() \nis called on, methods de.ned on Object are treated as being distinct methods for every type. All other \nmethods have formal parameter and formal return type terms associated with their de.nition which are \nshared with any overriding methods. A more principled approach might involve a concept of subtyping for \nabstract types, but that would greatly complicate the algorithm for what would likely be minimal gain. \n For example, consider the following code from Family.Show:3 string appLocation = Path.Combine( Environment.GetFolderPath( \nEnvironment.SpecialFolder.MyDocuments), App.ApplicationFolderName); if (!Directory.Exists(appLocation)) \nDirectory.CreateDirectory(appLocation); return Path.Combine(appLocation, Const.DataFileName); Directory.Exists, \nDirectory.CreateDirectory, and Path.Combine take appLocation as their .rst argument, so the analysis \nconcludes their .rst arguments are all the same abstract type. Furthermore, from the .rst statement, \nthat must also be the abstract type of the return values of Path.Combine and Environment.GetFolderPath. \nOn the other hand, there is no evidence that would lead the analysis to believe the sec\u00adond argument \nof Path.Combine is of that abstract type, instead App.ApplicationFolderName and Const.DataFileName are \nbelieved to both be of some other type. Intuitively, a programmer might call those two types directory \nname and .le name . In the ranking function, the type distance computation is re.ned by adding an additional \ncost of 1 if the abstract types do not match.  4.2 Completion .nder This section presents a general \nalgorithm for computing the top n ranked completions of a partial expression, .rst giving a naive im\u00adplementation \nand then discussing optimizations. The main logic is Algorithm 1 which returns a generator that returns \nall completions of a partial expression in order by score. The yield return state\u00adment returns a single \ncompletion, and when the next completion is requested, execution continues on the next statement. The \n.rst n el\u00adements of AllCompletions(e ) are the top n ranked completions of e . Note that for any partial \nexpressions containing .?*f or .?*m, this generator will usually continue producing more completions \nforever, but can be called only n times to get just the top n comple\u00adtions. It may be easier to .rst \nread the algorithm while ignoring the score variable, which is necessary to handle the unbounded result \nset for .?*f or .?*m. Then it is a simple recursive algorithm which computes every possible completion \nof its subexpressions and uses those completions to generate every possible completion of the en\u00adtire \nexpression (e.g. all methods that can take those arguments). Note that ? is interpreted as vars.?*m where \nvars is a special subexpression whose list of completions is every local and global variable in scope. \nWe now discuss various useful optimizations. Cache subexpression scores A subexpression s score will \nbe needed for every completion it appears in. To compute it only once, the algorithm is rede.ned such \nthat it returns a set of pairs of completions and their scores. Compute completions not in score order \nIn the algorithm given above, completions with a score not equal to score are discarded and regenerated \nlater. To avoid that work, 3 http://www.vertigo.com/familyshow.aspx Algorithm 1: AllCompletions(e ) \ninput : e : a partial expression output : a generator of all completions in order by score subexps . \nthe list of immediate subexpressions of e ; Let subcomps be a map from subexps to completions; foreach \ns . subexps do subcomps[s] . AllCompletions(s); end foreach score . [0, 8) do foreach concreteSubs . \nall choices of exactly one completion for each subexpression from subcomps whose score(concreteSubs) \n= score do foreach type-correct completion c of e using subexpressions concreteSubs where score(c) = \nscore do yield return c ; end end end Figure 8. The method index. The supertypes of IList other than \nObject are omitted for brevity. completions with a score greater than score can be saved to be out\u00adput \nlater. For most partial expressions, it makes sense to compute all of the completions for a given value \nof concreteSubs at once. In the case of .?*f and .?*m queries, the algorithm will never be done computing \nevery possible completion, but foo.?*f can be thought of as the union of .?f queries .rst on foo, then \non the results of foo.?f, etc. Then each completion set is .nite and the basis for the future completions. \nIn pseudocode, this is implemented as inserting each completion c into subcomps. Indexing As written, \nhow the algorithm iterates over possible completions is unspeci.ed. This is especially a problem for \nunknown methods as simply iterating over all methods in a huge framework would take too long. An index \nis maintained that maps every type to a set of methods for which at least one of the arguments may be \nof that type. Then, given a query like ?({e1,e2}), each of the argument types is looked up to see how \nmany methods would have to be considered for that type and the smallest set is chosen. That set will \nalmost always be orders of magnitude smaller than the set of all methods. Part of a method index is shown \nin Figure 8. In order to save memory, the method index is organized such that looking up a type t gets \na set of methods which have parameters of the exact type t along with pointers to the method indexes \nfor the immediate super\u00adtypes of t . Due to the type distance part of the ranking algorithm explained \nin Section 4.1, each method index visited will give pro\u00adgressively worse ranked results.  Methods are \na prime candidate for indexing as there are many methods and few that take a speci.c type. Although the \ncurrent implementation does not do so, queries for multiple .eld lookups could also be made more ef.cient \nusing an index that indicates for each type which types are reachable by a .?*f or .?*m query, how many \nlookups are needed, and which lookups can lead to a value of that type. For example, a Line type with \nPoint .elds p1 and p2 and a GetLength() method would have an entry denoting that the type double is reachable \nin = 2 lookups using a .?*f query with the next lookup being one of p1 or p2 while it is reachable in \n= 1 lookup using a .?*m query with the next lookup being one of GetLength(), p1, or p2. Avoid computing \ntype-incorrect completions If the possible valid types for the completions were known, then the type \nreachability index would be more useful: otherwise results of every type have to be generated anyway \nfor completeness. At the top level, the context will often provide a type unless the expression being \ncompleted is the initial value for a variable annotated only as var. On the recursive step of the algorithm, \nthe possible types may not be known or may not be precise enough to be useful: there are methods that \ntake multiple arguments of type Object, so even knowing one of the argument types does not narrow down \nthe possibilities for the rest. On the other hand, binary operators and assignments are relatively restrictive \non which pairs of types are valid, so enumerating the types of the completions for one side could signi.cantly \nnarrow down the possibilities for the other side. Grouping computations by type Which completions are \nvalid is determined solely by the types of the expressions involved. Hence, instead of considering every \ncompletion of every subexpression separately, the completions of each subexpression can be grouped by \ntype after grouping by score to reduce the number of times the algorithm has to check if a given type \nis valid in a given position. This also allows type distance computations to be done once for all subexpressions \nof the same types. Any remaining ranking features are computed separately for each completion as grouping \nby them is no faster than computing their terms of the ranking function. 5. Evaluation We implemented \nthe algorithm described using the Microsoft Re\u00adsearch Common Compiler Infrastructure (CCI).4 CCI reads \n.NET binaries and decompiles them into a language resembling C#. Un\u00adfortunately, we were unable to work \non actual source code because at the time the experiments were performed, no tools for analyzing the \nsource code of C# programs existed and even if they did exist, open source C# programs are relatively \nrare. We performed experiments where our tool found expressions in mature software projects, removed \nsome information to make those expressions into partial expressions, and ran our algorithm on those partial \nexpressions to see where the real expression ranks in the results. All experiments were run on a virtual \nmachine allocated one core of a Core 2 Duo E8400 3GHz processor and 1GB of RAM. One minor issue is that \nany precomputation, speci.cally ab\u00adstract type inference, would see the expression we are trying to re\u00adcreate \nwhen in actual practice the expression would not yet exist. To avoid this situation, we re-run abstract \ntype inference for each expression, eliminating the expression and all code that follows it in the enclosing \nmethod we do consider the rest of the program. We describe three case studies whose signi.cance we have \npreviously discussed in Section 2 and show that the ranking scheme 4 https://cciast.codeplex.com/ Table \n1. Summary of quality of best results for each call Program # calls # top 10 # top 10..20 Paint.Neta \n3188 2288 525 WiXb 13192 11430 512 GNOME Doc 208 167 22 Bansheed 91 82 2 .NETe 2801 2345 145 Family.Showf \n586 510 23 LiveGeometryg 1110 1072 3 Totals 21176 17894 (84.5%) 1232 (5.8%) a http://www.getpaint.net/ \nimage editor (main .exe) b http://wix.codeplex.com/ Windows Installer XML c http://do.davebsd.com/ application \nlauncher d http://banshee.fm/ media player e .NET Framework v3.5 libraries System.Core.dll, mscorlib.dll \nf http://www.vertigo.com/familyshow.aspx WPF example application g http://livegeometry.codeplex.com/ \ngeometry visualizer Figure 9. The proportion of calls of each type with the best rank of at least the \ngiven value is effective and the algorithm is ef.cient. After that, we analyze the importance of the \nindividual ranking features in Section 5.4. Finally, we discuss threats to validity in Section 5.5. \n5.1 Predicting Method Names Our .rst experiment shows that queries consisting of one or two arguments \ncan effectively .nd methods. We ran our analysis on 21,176 calls across parts of seven C# projects listed \nin Table 1. We generated queries by .nding all calls with =2 arguments (including the receiver, if any) \nand giving one or two of the call s arguments to the algorithm. We evaluated the algorithm on where in \nthe results list the actual method appeared. While putting the correct result as the .rst choice is ideal, \nwe do not consider it necessary for usefulness since users can quickly skim several plausible results. \nFigure 9 shows the results overall and partitioned between static and instance calls. Almost 85% of the \ntime, the algorithm is able to give the correct method in the top 10 choices. An additional 5% of the \ntime, the correct method appears in the next 10 choices out of a total of hundreds of choices on average. \nNotably, the algorithm fares signi.cantly better on instance calls than static calls. This is not too \nsurprising as the search space is much larger for static calls. This might also indicate that the current \nheuristics prefer instance calls more strongly than they should. Unfortunately, we cannot algorithmically \ndetermine which ar\u00adgument subset a user would use as their search query. Instead, we show that usually \nfor some set of no more than 2 arguments the correct method being highly ranked. Our intuition, which \nwould need a user study to validate fully, is that evaluating our approach by choosing for the best possible \nsubset of arguments is reasonable because programmers are capable of identifying the most useful ar\u00adguments \n(e.g., PreciseLibraryType instead of string or Pair).   Figure 10 shows that a single argument is often \nenough for the algorithm to determine which method was desired, in this case de.ned as putting the method \nin the top 20 choices. Not shown in the graph is that adding a third argument leads to only negligible \nimprovement in these results; note that even knowing all of the arguments to a method might not be enough \nto place it in the top 20 choices. Above the bars is the percentage of calls the algorithm was able to \nguess using only two arguments, which is high for any number of arguments. The intuition is that most \nof the arguments are not important, although there are also more opportunities for an argument to be \nof a rarely used type. The low value for 10 argument calls is due to there being very few such calls \nand most of them being from a large family of methods which all have the same method signature. Comparison \nto code completion Figure 11 compares our ranking algorithm to Intellisense. The y\u00adaxes are read as the \nleft side measuring the proportion of calls our system did better on and the right side measuring the \nproportion of calls Intellisense did better on. We modeled Intellisense as being given the receiver (or \nreceiver type for static calls) and listing its members in alphabetic order. Intellisense knows which \nargument is the receiver but is not using knowledge of the arguments. It was considered to list only \ninstance members for instance receivers and only static members for static receivers. Given this ordering, \nwe were able to compute the rank in the alphabetic list of the correct answer. We then subtracted that \nrank from the rank given by our algorithm, so negative numbers mean our algorithm gave the correct answer \na higher rank. Figure 12. Difference in rank between our algorithm .ltering its results for those matching \nthe correct return type and Intellisense About 45% of the time, our position is at least 10 higher than \nit is with Intellisense. Since Intellisense displays at most 10 results at a time, this means it is not \ninitially displayed by Intellisense. Subtracting the ranks is oversimplifying the comparison. First, \nthe different tools have different inputs. Our tool does not require the receiver but is helped by being \nprovided a second argument. Second, the Intellisense results are listed in alphabetic order which is \nlikely easier to skim through than the results from our tool which will be ordered by their ranking scores. \nThe take-away is not that our tool is better than Intellisense; they serve different purposes. Instead, \nwe wanted to show that our tool is often able to greatly reduce the number of choices a user would have \nto sift through compared to Intellisense even if the user knew the correct receiver. Figure 12 shows \na similar comparison to the one in Figure 11 except that our algorithm additionally knew the desired \nreturn type (or void) and only suggested methods whose return type matched. The assumption of a known \nreturn type is not used elsewhere both because, in the context of API discovery, the user may often not \nknow what return type to use, and the var keyword in C# and equivalents in other languages allow a user \nto omit return types. Speed For 98.9% of the calls analyzed, the query with the best re\u00adsult ran in under \nhalf a second, which is fast enough for interactive use. As a caveat, these times do not include running \nthe abstract type inference algorithm. That could take as long as several minutes for a large codebase \nbut can be done incrementally in the background. These times were measured using CCI reading binaries \nas op\u00adposed to getting the information from an IDE s incremental com\u00adpiler. How that affects performance \nis unclear, but any such effects were minimized by memoizing a lot of the information from CCI, so the \nvast majority of the time was spent in our algorithm.  5.2 Predicting Method Arguments Our second experiment \ninvestigated how often arguments to a method could be .lled in by knowing their type. Looking at the \nsame method calls as the previous experiment, for each argument in each call, a query was generated with \nthat argument replaced with ?. There were a total of 69,927 arguments across the 21,176 calls. 23,927 \nwere considered not guessable due to having an expression form that our partial expression completer \ndoes not generate like an array lookup or a constant value. Figure 13 shows how well our algorithm is \nable to predict method arguments, with the lower line ignoring the low-hanging fruit of local variables. \nOver 80% of the time, the algorithm is able to suggest the intended argument as one of the top 10 choices \ngiven out of an average of hundreds of choices. Use of expressions other than local variables in argument \nposi\u00adtions is common as shown in Figure 14. Programmers must some\u00ad how discover the proper APIs for these \nexpressions: Intellisense only suggests local variables given an argument position. The not guessable \nexpressions are those that involve constants or computa\u00adtion like an addition or a non-zero argument \nmethod call that could be guessed by neither our technique nor Intellisense. Our partial ex\u00adpression \nlanguage captures more of the expressions programmers use as arguments including .eld/property lookups \nwhich are rela\u00adtively common and require browsing to .nd using Intellisense.   As our experiments are \non decompiled binaries and not the orig\u00adinal source, these arguments may not be exactly what the program\u00admer \nwrote. In particular, expressions might be stored in temporary variables that they did not write or temporaries \nthey did write might be removed, putting their de.nition in an argument position. Speed Our tool is capable \nof enumerating suggested arguments in under a tenth of a second 92% of the time and under half a second \nover 98% of the time, which is fast enough for an interactive tool.  5.3 Predicting Field Lookups Our \nthird experiment determines how often .eld/property lookups could be omitted in assignments and comparisons \n(on either side). Our corpus includes 14,004 assignments where the target ends with a .eld lookup, 7,074 \nwhere the source does, and 966 where both do. For those assignments, Figure 15 shows the rank of the \ncorrect answer when our algorithm was given the assignment with the .nal .eld lookups removed and .?m \nadded to the end of both sides of the assignment. The correct answer was in the top 10 choices over 90% \nof the time when one .eld lookup was removed, but only about 59% of the time when a .eld lookup was removed \nfrom both sides, going up to 75% when considering the top 20 choices. There were a total of dozens of \nchoices on average. Our corpus includes 620 comparisons where the left side ends in a lookup, 162 of \nwhich end in two lookups; 620 comparisons where the right side ends in a lookup, 174 of which end in \ntwo lookups; and 125 where both sides end in a lookup. Of those, Figure 16 shows the ranks our tool gave \nto the expression in the Figure 15. Proportion of assignments where a .eld lookup could be removed from \none or both sides and guessed with a given rank Figure 16. Proportion of comparisons where .eld lookups \ncould be removed from one or both sides and guessed with a given rank source given the query containing \nthe original expression with the lookups removed and .?m.?m added to the end of both sides. The numbers \nare signi.cantly better than for assignments be\u00adcause there are fewer possibilities: few types support \ncomparisons. One lookup can be placed within the top 10 for nearly every in\u00adstance in our corpus. If \nwe allow 20 choices, then two lookups where one lookup is on each side can be guessed 89% of the time \nwhile two lookups on the same side can be guessed 85% of the time if they are are on right and 69% of \nthe time of they are on the left. The discrepancy between the latter two appears to be that com\u00adparisons \nagainst constants are usually written with the complicated expression on the left and the constant on \nthe right, and the name matching feature is not helpful for comparisons to constants. Speed 99.5% of \nthese queries ran in under half a second.  5.4 Sensitivity analysis of ranking function To see which \nparts of the ranking function were most important, we re-ran the experiments with various modi.ed ranking \nfunctions. Each modi.ed version either included only one of the terms or left out one of the terms, in \naddition to versions that left out and included both the type distance term and the abstract type term. \nTable 2 shows the data for the proportion of expressions where the correct answer was in the top 20 choices \nfor different variants of the ranking function for each of the experiments. Methods Type distance and \nabstract type distance are the only features that matter. Leaving out the namespace and in-scope static \nterms seems to make almost no difference. Furthermore, the two type distance terms separately are both \ngood, with abstract type distance alone being a little better, but not quite as good as the two together, \ncon.rming that both are useful. Arguments It seems that only the depth feature seems to matter. Leaving \nit out makes the results much worse while leaving any other term out has almost no effect. In fact, looking \nat the +d Table 2. Ranking function term sensitivity. Each cell is the proportion where correct answer \nwas found in the top 20 choices with various modi.cations of the ranking function. All is the full ranking \nfunction. For the rest, - means without certain terms, + means with only certain terms: n =namespaces, \ns =in-scope static, d =depth, m =matching name, t =normal type distance, and a =abstract type distance. \n Count All -n -s -d -m -t -a -at +n +s +d +m +t +a +at Methods All 21176 0.90 0.90 0.90 - -0.85 0.84 \n0.43 0.43 0.43 - -0.84 0.85 0.90 Instance 13904 0.96 0.96 0.96 - -0.87 0.94 0.31 0.31 0.31 - -0.94 0.87 \n0.96 Static 7272 0.78 0.78 0.78 - -0.80 0.65 0.65 0.68 0.65 - -0.64 0.81 0.78 Arguments Normal 45325 \n0.90 0.90 -0.72 -0.90 0.90 0.90 0.72 -0.90 -0.72 0.72 0.72 No variables 14925 0.77 0.77 -0.65 -0.76 0.77 \n0.76 0.64 -0.76 -0.65 0.64 0.65 Assignments Target 14004 0.97 - -0.87 -0.97 0.97 0.97 - -0.97 -0.81 0.87 \n0.87 Source 7074 0.89 - -0.87 -0.90 0.89 0.90 - -0.90 -0.87 0.89 0.87 Both 966 0.75 - -0.76 -0.74 0.75 \n0.73 - -0.73 -0.75 0.75 0.76 Comparisons Left 620 1.00 - -0.23 1.00 1.00 1.00 1.00 - -1.00 0.65 0.25 \n0.68 0.25 Right 620 1.00 - -0.51 1.00 1.00 1.00 1.00 - -1.00 0.53 0.62 0.85 0.62 Both 125 0.89 - -0.46 \n0.87 0.88 0.89 0.88 - -0.87 0.46 0.42 0.37 0.42 2xLeft 162 0.69 - -0.14 0.69 0.70 0.69 0.70 - -0.69 0.41 \n0.18 0.57 0.18 2xRight 174 0.85 - -0.63 0.81 0.81 0.85 0.81 - -0.77 0.58 0.71 0.73 0.71 column, using \njust the depth term gives almost exactly the same results as the full ranking scheme. Assignments For \njust one lookup removed from either side, once again only depth matters, but when a lookup is missing \nfrom both sides, the type distance computation becomes important. In fact, in the case of a lookup missing \nfrom both sides, leaving out the depth component improves the results. This is not too surprising as \nthere are likely many possible assignments which require adding only one lookup to either side. The interesting \npart is that apparently these lookups can be distinguished from the proper one by looking at more detailed \ntype information (recall that only assignments which are type correct are even being considered). Comparisons \nDepth once again seems to be most important. Ex\u00adcept on the 2xRight row, depth appears to be the only \nsigni.cant feature. The different values for that row vary little, indicating that each ranking feature \nis somewhat useful, but there is little gain from combining them. On average, there were hundreds of \ntype-correct options, so the ranking function is de.nitely doing something to place the correct option \nin the top 20.  5.5 Threats to validity As the experiments were run on decompiled code instead of ac\u00adtual \nsource code, they may not apply to how programmers actually write code. Particularly, the decompiled \ncode may have simpler ex\u00adpressions due to a compiler factoring out additional local variables from complex \nexpressions. This did not appear to be the case from looking at code being analyzed as non-trivial expressions \nwere vis\u00adible like method calls and binary operations in method argument positions. On the other hand, \nthe decompiled code looked different across projects (speci.cally, some projects had local variables \nwith names like local0 while others had actual names), so there may be some compiler-dependence involved. \nWorking on completed projects as opposed to codebases in the process of being developed means that abstract \ntype inference algorithm may have had more information than it would have had in a real development scenario. \nNote that since the rest of the features work only on the current expression, they are unaffected by \nthe maturity of the codebase. On the other hand, more development likely corresponds to more APIs existing \nin the codebase which could appear in the results of a query. The subset of the arguments that got the \nbest results was always used, not the subset of arguments that the programmer would be most likely to \nthink of when writing a query. Naturally, the latter is dif.cult to determine automatically. Looking \nat the results, the queries generated tended to seem reasonable. The main reason this could be a problem \nis when one of the options to a method is a type used only as an option to a small number of methods \nlike the System.IO.FileMode enum which is used only for methods that open .les. Such types seemed to \nbe rare, although no quantitative analysis was done to con.rm that. More importantly, every single expression \nin the analyzed code\u00adbases was used, as opposed to restricting the analysis to only the ones that the \nprogrammer would .nd dif.cult to write. Naturally, the latter is dif.cult to determine automatically. \nIt seems safe to assume that the vast majority of API usages in a codebase were easy for the programmer \nto write, so the experiment results cannot be taken as direct evidence that our proposed tool would be \neffec\u00adtive in answering queries. Instead, the experiment results should be taken as an argument that \nexpressions in a program tend to be well speci.ed by a small subset of the information needed to exactly \nde.ne the expression, and therefore the technique of searching for expressions using a partial expression \nas a query should tend to work well in practice. The experiments worked only on C# code. We expect to \nobtain similar results on code written in languages with a rich type system such as Java. 6. Related \nWork Prospector[10] is perhaps the closest related work. With Prospec\u00ad tor, as discussed in Section 2.3, \na user makes a query for a con\u00ad version from one type to another and gets what the authors call a jungloid \nwhich is a series of operations including method calls, .eld lookups, and downcasts from examples in \nthe code. That pa\u00adper noted that shorter jungloids tend to be more likely to be correct and also that \njungloids that cross package boundaries are less likely to be correct, both of which are ideas used by \nour ranking function. PARSEWeb[17] performs the same task as Prospector except it mines code examples \nfrom web searches. InSynth[6] also produces expressions for a given point in code using the type as well \nas the context to build more complicated expressions using a theorem prover. InSynth s ranking algorithm \nis based on machine learning from examples. It, like Prospector, differs from our work in that it generates \nexpressions from scratch with no input from the programmer to guide it. Their evaluation was on small \nsnippets of Java example code translated to Scala which is dif.cult to compare to our evaluation on mature \nC# projects.  Typsy[1] searches for APIs by generating expressions involving any number of method and \nconstructor calls and .eld lookups given a list of arguments, a return type, and a library package to \nsearch within. Typsy will only return expressions with all arguments .lled in, so it will generate expressions \nto construct any missing argu\u00adments for methods it .nds. The expressions are ranked by their size similar \nto the depth term in our system, although our depth term does not count the number of method and constructor \ncalls because it will not generate new ones not speci.ed by the programmer. API Explorer[3] supports \nqueries both for methods taking a given argument and for how to construct a value of a given type. When \nquerying for methods, a keyword can also be provided, so the results are .ltered to contain only methods \nwith synonyms of that keyword in the name. The ordering of the results uses a computation similar to \nour type distance feature. Both types of queries can be expressed in our partial expressions language, \nexcept we do not have a way to .lter by keywords, and the version used for our experiments does not generate \nconstructor calls when asked for an unknown method. Strathcona[7] and XSnippet[14] both use context to \nproduce queries which may be helpful to the programmer. In a similar vein, CodeBroker[18] performs searches \nfor APIs based on the documentation of the method currently being written in order to recommend APIs \nthe user may not even be aware of. Little and Miller[9] propose a system using keyword program\u00ad ming \nto generate method calls where the user gives keywords and the system generates a method call that includes \narguments that have most or all of the keywords. Their system attempts to be closer to natural language \nthan ours at the cost of a lower success rate. Hou and Pletcher[8] recommend various ways of .ltering \nand sorting APIs to show the most relevant choices .rst. Their system is mainly based on manual annotations \nof APIs as well as considering how often an API is used. Their work is complementary as it could be used \nto .lter out irrelevant methods from our results. Searching for functions by type has been recommended \nfor functional programming languages[13][20]. Those proposals differ in that the type signature alone, \nalong with modi.cations to, for example, handle both curried and uncurried functions, tends to be suf.cient \nfor a search. In imperative languages with subtyping, inexact matches are more likely to be meaningful \nand side-effects make it more likely that many options have the same type. For discovery of entire modules \nat once, speci.cation matching can search by speci.cation[21]. Semantics-based code search[12] similarly \nsearches based on speci.cations including tests and key\u00adwords but additionally may make minor modi.cations \nto the code to .t the details of the speci.cation. SNIFF[2] returns snippets matching natural language \nqueries by mining multiple examples from existing code, matching them based on the documentation of the \nAPIs they use, and combining them based on their similarities to eliminate the usage-speci.c parts of \nthe snippets. Unlike our algorithm, this technique requires the API being searched to be well-documented. \nMatchMaker[19] handles API discovery at a different scale: given two types, it generates the glue code \nto connect those two types by generalizing examples from existing code. Program sketching[15][16] is \na form of program synthesis[5] where the programmer writes a partial program with holes and provides \na speci.cation the solution must satisfy. Our technique is similar but considers only a single expression \nat a time and avoids the need for an explicit speci.cation by using type information to .lter the results. \n7. Conclusions and Future Work This paper has shown that type-directed completion of partial ex\u00adpressions \ncan effectively .ll in short code snippets that are com\u00adplicated enough to be dif.cult to discover using \ncode completion. Furthermore, our ranking scheme is able to sift through hundreds of options to often \nplace the correct answer among the top results. Future work would be to implement an IDE plug-in and \nperform a user study to determine if it is useful in real development situa\u00adtions as well as possibly \nseeing if developers have other ideas for how such a plugin could be used or for similar ideas for lightweight \nsearches. Extending the algorithm to other programming languages is also future work. The features are \nat least partially tied to C#/Java and will need to be adapted to make sense in other languages. Acknowledgments \nThis work was funded by a grant from Microsoft Research. We thank the anonymous reviewers for their valuable \nfeedback. References [1] C. Bolton. Typsy: a type-based search tool for Java programmers. In C. Miller, \neditor, Selected 2001 SRC Summer Intern Projects, Techni\u00adcal Note 2001-004. Compaq Systems Research Center, \nDec. 2001. [2] S. Chatterjee, S. Juvekar, and K. Sen. SNIFF: A search engine for Java using free-form \nqueries. ETAPS/FASE, 2009. [3] E. Duala-Ekoko and M. P. Robillard. Using structure-based recom\u00admendations \nto facilitate discoverability in APIs. ECOOP, 2011. [4] G. Furnas, T. Landauer, L. Gomez, and S. Dumais. \nThe vocabulary problem in human-system communication. CACM, 30, Nov 1987. [5] S. Gulwani. Dimensions \nin program synthesis. In PPDP, 2010. [6] T. Gvero, V. Kuncak, and R. Piskac. Interactive synthesis of \ncode snippets. In Computer Aided Veri.cation (CAV) Tool Demo, 2011. [7] R. Holmes and G. C. Murphy. Using \nstructural context to recommend source code examples. ICSE, 2005. [8] D. Hou and D. M. Pletcher. Towards \na better code completion system by API grouping, .ltering, and popularity-based ranking. RSSE, 2010. \n[9] G. Little and R. C. Miller. Keyword programming in Java. ASE, 2007. [10] D. Mandelin, L. Xu, R. Bod\u00b4ik, \nand D. Kimelman. Jungloid mining: helping to navigate the API jungle. PLDI, 2005. [11] R. O Callahan \nand D. Jackson. Lackwit: A program understanding tool based on type inference. ICSE, 1997. [12] S. P. \nReiss. Semantics-based code search. ICSE, 2009. [13] M. Rittri and M. Rittri. Retrieving library identi.ers \nvia equational matching of types. CADE, 1992. [14] N. Sahavechaphan and K. Claypool. XSnippet: mining \nfor sample code. OOPSLA, 2006. [15] A. Solar Lezama. Program Synthesis By Sketching. PhD thesis, EECS \nDepartment, University of California, Berkeley, Dec 2008. [16] A. Solar-Lezama. The sketching approach \nto program synthesis. APLAS, 2009. [17] S. Thummalapenta and T. Xie. PARSEWeb: a programmer assistant \nfor reusing open source code on the web. ASE, 2007. [18] Y. Ye and G. Fischer. Supporting reuse by delivering \ntask-relevant and personalized information. ICSE, 2002. [19] K. Yessenov, Z. Xu, and A. Solar-Lezama. \nData-driven synthesis for object-oriented frameworks. OOPSLA, 2011. [20] A. M. Zaremski and J. M. Wing. \nSignature matching: a tool for using software libraries. ACM Trans. Softw. Eng. Methodol., April 1995. \n[21] A. M. Zaremski and J. M. Wing. Speci.cation matching of software components. ACM Trans. Softw. Eng. \nMethodol., 6:333 369, 1996.   \n\t\t\t", "proc_id": "2254064", "abstract": "<p>Modern programming frameworks provide enormous libraries arranged in complex structures, so much so that a large part of modern programming is searching for APIs that surely exist\" somewhere in an unfamiliar part of the framework. We present a novel way of phrasing a search for an unknown API: the programmer simply writes an expression leaving holes for the parts they do not know. We call these expressions <i>partial expressions</i>. We present an efficient algorithm that produces likely completions ordered by a ranking scheme based primarily on the similarity of the types of the APIs suggested to the types of the known expressions. This gives a powerful language for both API discovery and code completion with a small impedance mismatch from writing code. In an automated experiment on mature C# projects, we show our algorithm can place the intended expression in the top 10 choices over 80% of the time.</p>", "authors": [{"name": "Daniel Perelman", "author_profile_id": "81502643269", "affiliation": "University of Washington, Seattle, WA, USA", "person_id": "P3471222", "email_address": "perelman@cs.washington.edu", "orcid_id": ""}, {"name": "Sumit Gulwani", "author_profile_id": "81100315615", "affiliation": "Microsoft Research Redmond, Redmond, WA, USA", "person_id": "P3471223", "email_address": "sumitg@microsoft.com", "orcid_id": ""}, {"name": "Thomas Ball", "author_profile_id": "81100472343", "affiliation": "Microsoft Research Redmond, Redmond, WA, USA", "person_id": "P3471224", "email_address": "tball@microsoft.com", "orcid_id": ""}, {"name": "Dan Grossman", "author_profile_id": "81405594870", "affiliation": "University of Washington, Seattle, WA, USA", "person_id": "P3471225", "email_address": "djg@cs.washington.edu", "orcid_id": ""}], "doi_number": "10.1145/2254064.2254098", "year": "2012", "article_id": "2254098", "conference": "PLDI", "title": "Type-directed completion of partial expressions", "url": "http://dl.acm.org/citation.cfm?id=2254098"}