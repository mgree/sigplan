{"article_publication_date": "06-11-2012", "fulltext": "\n Dynamic Synthesis for Relaxed Memory Models Feng Liu Nayden Nedev Nedyalko Prisadnikov Princeton University \nPrinceton University So.a University fengliu@princeton.edu nnedev@princeton.edu nedy88@gmail.com Martin \nVechev Eran Yahav * ETH Z\u00a8urich Technion martin.vechev@inf.ethz.ch yahave@cs.technion.ac.il Abstract \nModern architectures implement relaxed memory models which may reorder memory operations or execute them \nnon-atomically. Special instructions called memory fences are provided, allowing control of this behavior. \nTo implement a concurrent algorithm for a modern architecture, the programmer is forced to manually reason \nabout subtle relaxed behaviors and .gure out ways to control these behaviors by adding fences to the \nprogram. Not only is this process time consuming and error-prone, but it has to be repeated every time \nthe implementation is ported to a different architecture. In this paper, we present the .rst scalable \nframework for han\u00addling real-world concurrent algorithms running on relaxed archi\u00adtectures. Given a concurrent \nC program, a safety speci.cation, and a description of the memory model, our framework tests the pro\u00adgram \non the memory model to expose violations of the speci.ca\u00adtion, and synthesizes a set of necessary ordering \nconstraints that prevent these violations. The ordering constraints are then realized as additional fences \nin the program. We implemented our approach in a tool called DFENCE based on LLVM and used it to infer \nfences in a number of concurrent al\u00adgorithms. Using DFENCE, we perform the .rst in-depth study of the \ninteraction between fences in real-world concurrent C programs, correctness criteria such as sequential \nconsistency and linearizabil\u00adity, and memory models such as TSO and PSO, yielding many in\u00adteresting observations. \nWe believe that this is the .rst tool that can handle programs at the scale and complexity of a lock-free \nmemory allocator. Categories and Subject Descriptors D.1.3 [Concurrent Pro\u00adgramming]; D.2.4 [Program \nVeri.cation] General Terms Algorithms, Veri.cation Keywords Concurrency, Synthesis, Relaxed Memory Models, \nWeak Memory Models * Deloro Fellow Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. PLDI 12, June 11 16, 2012, Beijing, China. Copyright c . 2012 ACM 978-1-4503-1205-9/12/06. \n. . $10.00 1. Introduction Modern architectures use relaxed memory models in which mem\u00adory operations \nmay be reordered and executed non-atomically [2]. These models enable improved hardware performance but \npose a burden on the programmer, forcing her to understand the effect that the memory model has on their \nimplementation. To allow program\u00admer control over those executions, processors provide special mem\u00adory \nfence instructions. As multicore processors increasingly dominate the market, highly-concurrent algorithms \nbecome critical components of many systems [28]. Highly-concurrent algorithms are notoriously hard to \nget right [22] and often rely on subtle ordering of events, which may be violated under relaxed memory \nmodels [14, Ch.7]. Placing Memory Fences Manually reasoning where to place fences in a concurrent program \nrunning on a relaxed architecture is a challenging task. Using too many fences (over-fencing) hin\u00adders \nperformance, while missing necessary fences (under-fencing) permits illegal executions. Manually balancing \nbetween over-and under-fencing is very dif.cult, time-consuming and error-prone [4, 14]. Furthermore, \nthe process of placing fences is repeated whenever the algorithm changes, and whenever it is ported to \na different architecture. Our goal is to automate the task of fence placement and free the programmer \nto focus on the algorithmic details of her work. Au\u00adtomatic fence placement is important for expert designers \nof con\u00adcurrent algorithms, as it lets the designer quickly prototype with different algorithmic choices. \nAutomatic fence placement is also important for any programmer trying to implement a concurrent al\u00adgorithm \nas published in the literature. Because fence placement de\u00adpend on the speci.c architecture, concurrent \nalgorithms are usually published without a detailed fence placements for different archi\u00adtectures. This \npresents a nontrivial challenge to any programmer trying to implement an algorithm from the literature \non relaxed ar\u00adchitectures. Dynamic Synthesis Existing approaches to automatic fence in\u00adference are either \noverly conservative [26], resulting in over\u00adfencing, or have severely limited scalability [17, 18]. The \nmain idea in this paper is to break the scalability barrier of static ap\u00adproaches by performing the synthesis \nbased on dynamic execu\u00adtions. To identify illegal executions, we introduce a .ush-delaying demonic scheduler \nthat is effective in exposing illegal executions under relaxed memory models. Given a program P , a speci.cation \nS, and a description of the memory model M , guided execution (using the .ush-based sched\u00aduler) of P \nunder M can be used to identify a set of illegal execu\u00ad 1 void put(int task) { 2 t = T; 3 items[t] = \ntask; 4 fence(st-st); //F2 5 T = t + 1; 6 fence(st-st); //F3 7 } 1 int steal() { 2 while (true) { 3 \nh = H; 4 t = T; 5 if (h >= t) 6 return EMPTY; 7 task = items[h]; 8 if(!cas32(&#38;H,h,h+1)) 9 continue; \n 10 return task; 11 } 12 } 1 int take() { 2 while (true) { 3 t = T -1; 4 T = t; 5 fence(st-ld); //F1 \n6 h = H; 7 if (t < h) { 8 T = h; 9 return EMPTY; 10 } 11 task = items[t]; 12 if (t > h) 13 return task; \n14 T = h + 1; 15 if(!cas(&#38;H,h,h+1)) 16 continue; 17 return task; 18 } 19 }  Figure 1: Simpli.ed \nversion of the Chase-Lev work-stealing queue. Here, store-load fence F1 prevents the non-SC scenario \nof Fig. 2a under TSO; F1 combined with store-store fence F2 prevents the non-SC scenario of Fig. 2b under \nPSO. F1, F2 and store-store fence F3 prevent the linearizability violation of Fig. 2c under PSO. tions \n(violating S). The tool will then automatically synthesize a program P . that avoids the observed illegal \nexecutions (under M), but still permits as many legal executions as possible. Evaluation Enabled by our \ntool, we perform the .rst in-depth study of the subtle interaction between: i) required fences in a num\u00adber \nof real-world concurrent C algorithms (including a lock-free memory allocator), ii) correctness criteria \nsuch as linearizabiltiy and (operation-level) sequential consistency [14], and iii) relaxed memory models \nsuch as TSO and PSO. Main Contributions The main contributions of this paper are: A novel framework \nfor dynamic synthesis of synchronization under relaxed memory models. The framework breaks the scal\u00adability \nbarrier of static synthesis approaches by performing the synthesis based on dynamic executions.  A .ush-delaying \ndemonic scheduler which delays the effect of global stores on shared memory and attempts to expose illegal \nexecutions under relaxed memory models.  An implementation of our tool in LLVM. The implementation required \na number of extensions of the LLVM interpreter (e.g. adding support for multi-threading) which are useful \nfor general concurrency analysis and for plugging-in new memory models.  An evaluation on a range of \nreal world concurrent C algorithms, including work-stealing queues and a lock-free memory alloca\u00adtor. \nWe believe this to be the .rst tool that can handle algorithms at the scale of a lock-free memory allocator. \n An in-depth study on the connection between: i) relaxed mem\u00adory models such as TSO and PSO, ii) required \nfences in prac\u00adtical concurrent algorithms, and iii) correctness criteria such as linearizabiltiy and \n(operation-level) sequential consistency. Our results yield a number of insights, useful in understanding \nthe interplay between relaxed memory models and concurrency.  Motivating Example Fig. 1 shows a pseudo-code \nof the Chase-Lev work-stealing queue [7] (note that our implementation handle the complete C code). A \nwork-stealing queue is a special kind of double-ended queue that provides three operations: put, take, \nand steal. A single owner thread can put and take an item from the tail of the queue, and multiple thief \nthreads can steal an item from the head of the queue. In the implementation of Fig. 1, H and T are global \nshared variables storing head and tail indices of the valid section of the array items in the queue. \nThe operations put and take operate on one end of the array, and steal operates on the other end. The \nput operation takes a task as parameter, and adds it to the tail of the queue by storing it in items[t] \nand incrementing the tail index T. The take operation uses optimistic synchronization, repeatedly trying \nto remove an item from the tail of the queue, potentially restarting if the tail and the head refer to \nthe same item. take works by .rst decrementing the tail index, and comparing the new value with the head \nindex. There are three possible cases: new tail index is smaller than head index, meaning that the queue \nis empty. In this case, the original value of tail index is restored and the take returns EMPTY (line \n9).  new tail index is larger than head index, take then uses it to read and return the item from the \narray.  new tail index equals to head index, in which case, the only item in the queue may be potentially \nstolen by a concurrent steal operation. A compare-and-swap (CAS) instruction is used to check whether \nthe head has changed since we read it into h. If the head has not changed, then there is no concurrent \nsteal, and the single item in the queue can be returned, while the head is updated to the new value h+1. \nIf the value of head has changed, take restarts by going to the next loop iteration.  Similarly, the \nimplementation of steal reads the head and tail indexes of the array .rst, and if the head index is larger \nor equal to the tail index, either the queue is empty or the only item is taken by the owner, and the \nthief returns EMPTY. Otherwise, it can read the items pointed by the head. In case no other thieves are \nstealing the same element, a CAS instruction is used to update the head index atomically. If the CAS \nsucceeds, the item can be returned, otherwise, the steal function needs to retry. Correctness Criteria \nand Terminology The implementation of Fig. 1 works correctly on an ideal processor without memory model \neffects. Under this ideal processor, the two CAS instructions in the algorithm are suf.cient for correctly \nupdating the head index H. In this work, we focus on two important speci.cations: (operation\u00adlevel) sequential \nconsistency and linearizability. Note that the term operation-level sequential consistency refers to \nthe behavior of the algorithm and not to the memory model itself. Precise de.nitions and extensive discussion \ncomparing (operation-level) sequential consistency and linearizability can be found in [14, Ch. 3.4-3.5]. \nAs we focus on relaxed memory models, in the rest of the paper we use the term sequential consistency \nto mean operation-level sequential consistency (and not hardware-level sequential consis\u00adtency). Informally, \nboth, (operation-level) sequential consistency and linearizability can be expressed as follows: Sequential \nconsistency requires that for each concurrent execu\u00adtion, there exists a serial execution that yields \nthe same result.  Linearizability requires that for each concurrent execution, there exists a serial \nexecution that yields the same result and maintains the ordering between non-overlapping operations in \n  2. Overview the concurrent execution. We believe this work is the .rst detailed comparison between \nIn this section, we show how different synchronization require\u00adlinearizability and sequential consistency \nspeci.cations for real ments arise for different memory models and speci.cations. We concurrent algorithms. \nuse a practical work-stealing queue algorithm as an example.  (a) Sequential Consistency Violation on \nTSO (b) Sequential Consistency Violation on PSO (c) Linearizability Violation on TSO/PSO. Note that the \nexecution satis.es Sequential Consis\u00adtency Figure 2: Executions of a work-stealing queue violating different \nspeci.cations under different memory models. Arguments of take and steal are the return values of the \noperations. Returning to our algorithm, unfortunately, a relaxed memory model may reorder accesses to \nshared variables, leading to unex\u00adpected results. For example, in the total store order (TSO) memory \nmodel, loads can bypass earlier stores in the same thread, thus an\u00adother thread can load an inconsistent \nvalue and produce an incorrect result. In the partial store order (PSO) memory model, both loads and \nstores can bypass earlier stores. Correctness under TSO Fig. 2a shows an execution of the Chase-Lev queue \nunder the TSO memory model violating sequential con\u00adsistency. In this execution, thread 1 performs a \ntake(1) operation, and thread 2 performs steal(1), trying to steal the same item. Thread 1 .rst updates \nthe tail index of the queue to 0, and then loads 1 from the head index. For thread 1, the head equals \nto the tail, and it takes the .rst element of the queue, 1, successfully. However, since the update of \nthe tail index by thread 1 is buffered (noted in boldface), thread 2 can only load the original 1 from \nthe tail in\u00addex, and returns the same item 1. The same element is popped by two different threads, and \nwe cannot .nd a serial execution that has the same result. The original implementation violates the sequen\u00adtial \nconsistency on TSO memory model. To .x this problem, we need to guarantee that the buffered store of \nthe tail index in thread 1 is committed to main memory and visible to other threads before the head index \nis loaded. This can be accomplished by inserting a memory fence after the store (F1 in Fig. 1), which \nenforces a .ush of the buffered store to main memory. Correctness under PSO Unfortunately, fence F1 is \nnot suf.cient when running the algorithm on the PSO memory model. Under PSO, a possible store-store reordering \ncan still lead to violation of sequential consistency as shown in Fig. 2b. In this execution, thread \n1 performs put(1) and thread 2 performs steal(1). Thread 1 tries to store the item 1 in the .rst position \nof the queue and updates the tail index in main memory. Concurrently, thread 2 loads the new tail index, \n.nds there is a single item in the queue and loads the item. However, the real value of the item is buffered \nby thread 1, and thread 2 reads an uninitialized value 0. For this execution, we cannot .nd a serial \nexecution that can fetch an item which has not been put by any of the threads. To .x this problem, before \nthread 1 updates the tail index, we need to ensure that the corresponding item is updated .rst. Thus \na memory fence F2 is added after the item store instruction. In general, this store-store reordering \ncan also lead to a memory safety violation if for instance we use the value to index some array. load(x, \nr) load contents of global variable x into local variable r. store(r, x) store value of local variable \nr into global variable x. cas(x, r, s, q) an atomic compare-and-swap action. If the value in global variable \nx is as same as that in local variable r, then store in x the value in local variable s and set local \nvariable q to true. Otherwise, do nothing and set q to false. call(f,.a) function call of f with argument \nlist .a. return(f, r) return from a function f with value r. fork(t, l) create a new thread t to execute \nstatement starting at label l. join(t, u) thread t joins to thread u. fence memory fence instruction. \nself() return the id of the calling thread. flush(x) .ush the value of shared variable x to main memory \n(speci.c to a given memory model, de.ned later). This statement is inserted by the scheduler, and we \nuse it to model the relaxed memory model effect. Table 1: Basic statements of our language. Linearizability \nLinearizability is stricter than sequential consis\u00adtency. Linearizability requires at least one additional \nfence in this algorithm. Fig. 2c shows an execution of the implementation that satis.es SC under PSO \n(Fig. 2b), and shows that it violates linearizability. The schedule is similar to the one of Fig. 2b: \nthread 1 executes put(1), and thread 2 executes steal(-1) after thread 1 .nishes put(1). Even though \nthe store to the .rst item by thread 1 is committed to main memory before the tail index is updated, \nthread 2 can load an incorrect value of the tail index due to the buffering in thread 1, resulting in \nan empty steal. Because put(1) of thread 1 and steal(-1) of thread 2 are non-overlapping, we cannot .nd \na serial execution where the thief fails to steal from a queue that is non-empty. The violation occurs \nbecause the update to tail is not committed to main memory. To .ush the value of the tail index to main \nmemory, another fence (F3 in Fig. 1) needs to be inserted before the method returns. This violation can \nalso occur under TSO, and a memory fence is required there as well. 3. Language In this section, we present \nthe syntax of our language and the semantics of the two memory models used in the paper: Total Store \nOrder (TSO) and Partial Store Order (PSO). Syntax We consider a basic concurrent programming language \naugmented with procedure calls, where the basic statements are listed in Tab. 1. We denote the set of \nstatements by Stmt. Given a program, we use Label to denote the set of program labels. We also use the \nspecial labels Inactive and F inished to denote that a thread has not yet started or has completed execution. \nStatements in the program are uniquely labeled and for a label l . Label, we use stmt(l) to denote its \ncorresponding statement in the program. Semantics 1 Operational semantics under PSO. stmt(pc)= load(x, \nr) B(x)= .G(x)= v .. (LOAD-G) L(r)= v pc = n(pc) stmt(pc)= load(x, r) B(x)= b \u00b7 v .. (LOAD-B) L(r)= v \npc = n(pc) stmt(pc)= store(r, x) B(x)= bL(r)= v (STORE) B.(x)= b \u00b7 v pc . = n(pc) B(x)= v \u00b7 b (FLUSH) \nB.(x)= bG.(x)= v stmt(pc)= fence .x.B(x)= . (FENCE) pc . = n(pc) stmt(pc)= cas(x, r, s, q) G(x)= L(r) \nL(s)= vB(x)= . ... (CAS-T) G(x)= vL(q)= true pc = n(pc) stmt(pc)= cas(x, r, s, q) G(x) .= L(r) B(x)= \n. .. (CAS-F) L(q)= false pc = n(pc) stmt(pc)= fork(t, l) pc(t)= Inactive (FORK) .x.B(t, x)= . pc(t)= \nl .l.L(t, l)=0 pc = n(pc) stmt(pc)= join(t, u) pc(u)= F inished .x.B(u, x)= . (JOIN) pc . = n(pc) Semantics \nWe use T hread to denote a .nite set of thread identi\u00ad.ers and tid . T hread to denote a thread identi.er. \nA transition system is a tuple .s0, S,T ., where S is the set of program states, s0 . S is the initial \nprogram state, and T is the transition relation: T . S \u00d7 T hread \u00d7 Stmt \u00d7 S. tid : stmt A transition \ns ------. s . . T holds if s, s . . S and if executing statement stmt in state s by thread tid results \nin state s .. For a transition t, we use tr(t) to denote its executing thread, label(t) for the label \nwhose statement was executed, and stmt(t) (overloaded) to obtain the statement. A .nite execution p is \na sequence of transitions starting from the initial state: tid1 : stmt1tid2 : stmt2tidn : stmtn p = s0 \n- ------. s1 - ------. ... --------. sn. A program state s is de.ned as a tuple .pc, L, G., where: pc \n. PC, where PC = T hread . Label is a map of threads to program labels.  L . Env where Env = T hread \n. Local . D is a map from threads to local variables to values.  G . GV ar where GV ar = Shared . D \nis a map from global shared variables to values.  Here D represents the domain from which the program \nvari\u00adables take values. When updating mappings, we use M.(x)= v as a shorthand for M. = M[x .. v]. Given \na function pc . PC, if the thread tid is clear from the context, we use pc for pc(tid), stmt(pc) to denote \nthe statement at pc, and n(pc) to denote the statement following pc. Semantics under Memory Models The \nfull small-step (interleav\u00ading) transition semantics on sequentially consistent machines is standard. \nHere, we only give semantics for the statements affected by the memory model: TSO and PSO. To accomplish \nthat, we aug\u00adment the program state with a store buffer B, which is used to model the memory model effects: \n PSO: B . T hread . Shared . D *. That is, we keep a buffer of values for each (thread, shared variable) \npair.  TSO: B . T hread . (Shared \u00d7 D) *. That is, we keep a single buffer of values for all variables \nin a thread.  The semantics of the relevant statements for PSO are given in Semantics 1. The semantics \nof TSO are the same except that we work with the per-thread buffer, and so they are more restrictive. \nFor instance, in the global load rule LOAD-G, instead of checking that B(x)= ., we will check that .(y, \nd) . B.y . = x. Given a program P and memory model M, we use [ P ] M to denote the set of all possible \nprogram executions starting from the initial state. 4. Dynamic Synthesis In this section, we discuss \nthe algorithm for dynamically synthe\u00adsizing memory fences. This algorithm can be combined with any demonic \nscheduler that tries to expose violating executions. The basic idea is to repair the program iteratively: \nwhenever bad exe\u00adcutions are encountered, they are repaired so that new executions cannot repeat the \nsame errors. The high level dynamic synthesis approach is shown in Algo\u00adrithm 1. The input to the algorithm \nis a concurrent program P ,a memory model M and a speci.cation S. First, the algorithm at\u00adtempts to trigger \nan execution that violates the speci.cation. If such an execution is found, then the function avoid computes \nall possi\u00adble ways to avoid the execution and adds that to the formula .. Next, we have a non-deterministic \nchoice ? between enforcing the possible solutions from . into the program or choosing to ac\u00adcumulate \nrepairs from more violating executions. In practice, ? can be determined in various ways: user-speci.ed \nbound, dynam\u00adically computed value, a heuristic, and so on. When the algorithm .nds no violating execution, \nit aborts with the program P . (and if necessary, enforces any outstanding constraints left in .). Next, \nwe elaborate on the details of avoid and enforce. Algorithm 1: Dynamic Synchronization Synthesis 1 2 \n3 4 5 6 7 8 9 10 11 12 13 Input: Program P , Speci.cation S, Memory Model M Output: Program P . with \nrestricted synchronization P . = P ; . = true; while true do select p . [ P .] M such that p .|= S if \nno such p exists then return enforce(., P .) d = avoid(p, M) if d = false then abort cannot be .xed . \n= . . d if ? then P . = enforce(., P .) . = true;  Semantics 2 Instrumented semantics under PSO. stmt(pc)= \nload(x, r) . = v . (LOAD-G) .= v .. pc] | y .(y) .. B {[ly = x . B= . . ly (y)} stmt(pc)= store(r, x) \nB(x)= l. = v . (STORE) .= v .. pc] | y .(y) .. B{[ly = x . B= . . ly (y)}) B..(x)= l \u00b7 pc B(x)= l \u00b7 b \n(FLUSH) B..(x)= b stmt(pc)= fence (FENCE) .x.B(x)= . stmt(pc)= casx,r,s,q B(x)= l. = v . (CAS) .= v .. \npc] | y .(y) .. B{[ly = x . B= . . ly (y)}) B..(x)= . 4.1 Avoiding Executions When avoiding an execution, \nwe would like to build all of the possible ways of repairing it. To accomplish that, we .rst de.ne an \nordering predicate that captures one way to repair an execution. Ordering Predicates Given a pair of \nprogram labels l and k (in the same thread), we use [l . k] to denote an ordering predicate. Informally, \nthe intended meaning of [l . k] is that in any execution involving these two labels, the statement at \nlabel l should have a visible effect before the statement at label k. Next we de.ne what it means for \nan execution p to violate an ordering predicate [l . k], denoted as p .|=[l . k]. An execution p .|=[l \n. k] if and only if: .i, j. 0 = i<j< |p|such that: 1. label(pi) = l, label(pj ) = k and tr(pi)= tr(pj \n). 2. stmt(l)= store(r, x) and stmt(k) .{store(p, y), load(y, p)}  3. x . = y. 4. .f. i<f <j: stmt(pf \n).flush(x), and tr(pf )= tr(pi). = Informally, if we have an execution where a store is followed by \nsome later store or a load (all by the same thread), where there is no .ush of the .rst store to main \nmemory in between the two operations, then the execution violates the predicate. The intuition is as \nfollows: if the second operation is a read, then the read took effect before the .rst store became visible; \nif the second operation is a store, then, because .ush can trigger at any point in an execution, it is \npossible to perform a .ush of the second store immediately after the second store and make it visible \nbefore the .rst store is made visible. The above is a formulation for PSO. For TSO, we only need to change \nthe second point to stmt(k) . {load(y, p))}. If p does not violate [l . k], we say that p satis.es [l \n. k], and write p |=[l . k]. Computing a Repair for an Execution Given an execution, we would like to \ncompute all possible ways to avoid it. We accomplish this by de.ning the instrumented semantics of Semantics \n2 (here, we show the semantics for PSO, the semantics for TSO are simi\u00adlar). The semantics update an \nauxiliary map B. which records the program labels of statements accessing the buffer: B. . T hread . \nShared . Label * In the initial state, the buffers are empty, i.e. .x.B.(x)= ., and . = false. Let us \nlook at the STORE rule. The premise says that if the (auxiliary) buffer contains the sequence l, then \n(i) the program counter pc will be appended to the buffer, and (ii) we will create an ordering predicates \nbetween pc and each label belonging to other buffers with the same thread: the idea is that if there \nare pending stores in the other buffers (i.e. their labels are in their respective auxiliary buffer), \nthen we can repair the current execution by ordering any of those stores before pc. The reason that we \nuse . when building . is that any of the enumerated ordering predicate is suf.cient to repair the execution. \n We note that even though in this work we only apply this repair procedure, by avoid function in the \nalgorithm, for a violating exe\u00adcution (under some speci.cation), the procedure is independent of whether \nthe execution is violating or not. Hence, we can easily use this procedure as-is for any execution. For \ninstance, recent work has shown that it may be interesting to repair correct executions [31]. Computing \na Repair for a Set of Executions Finally, in Algo\u00adrithm 1, we combine repairs for each execution into \na global repair formula .. The reason we use . when combining individual avoid predicates is because \nwe need to make sure that each execution is eliminated. When the algorithm terminates, the formula . \nwill contain all of the possible ways to repair all of the (violating) ex\u00adecutions seen during this algorithm \ninvocation. Next, we discuss how to enforce the formula . into the program. 4.2 Enforcing Constraints \nGiven the repair formula ., a satisfying assignment of the formula is a set of truth assignments to ordering \npredicates. Such an as\u00adsignment represents a potential solution for all of the violating ex\u00adecutions \nthat contributed to the formula. There are various ways to enforce such an assignment in the program. \nEnforce with Fences In this work, we chose to enforce the assign\u00adment [l . k]: true by inserting a memory \nfence between the labels l and k. This prohibits the reordering of the memory ac\u00adcesses at these two \nlabels. Algorithm 2 takes in a formula . and a program, .nds a minimal satisfying assignment to the formula \nand enforces the predicates with a fence. In practice, we insert a more speci.c fence (store-load or \nstore-store) depending on whether the statement at k is a load or a store. Enforce with Atomicity We \ncan enforce [l . k]: true as an atomic section that includes both labels l and k. Such an atomicity constraint \n(that may require more than two shared accesses) can be realized in various ways: with locks, or with \nhardware transaction memory (HTM) mechanisms [9] (HTM is especially suitable if the number of shared \nlocations is bounded). Enforce with CAS On TSO, we can enforce the fence with CAS to a dummy location. \nThat is, we can use cas(dummy, r, s, q), where dummy is a location not used by the program (similarly \nr, s and q are never read in the thread). Regardless of whether such a CAS fails or succeeds on the dummy \nlocation, in order to proceed, it requires that the buffer is .ushed (similarly to a fence). On PSO, \nit is also possible to use CAS to enforce the predicate, by having a CAS on the same location as the \n.rst store in the predicate, i.e., cas(x, r, s, q). However, we would need to make sure that the CAS \nalways fails (so does not modify x). Hence, enforcing order using CAS on PSO will only work when the \ncontents of x and r are provably different. 5. Implementation 5.1 Architecture Our framework is based \non the LLVM infrastructure. To support our dynamic analysis and synthesis framework, we extended the \nLLVM interpreter (called lli) with the following features, not sup\u00adported by the original LLVM framework: \n Algorithm 2: enforce(., P ) Input: Formula ., Program P Output: Program P . with fences that ensure \nan assignment satisfying . 1 if . = true then return P 2 I = {[l1 . k1]...[ln . kn]} be an assignment \nto predicates such that I |= . and I is a minimal satisfying assignment of .. 3 P . = P 4 foreach [l \n. k] . I do 5 insert a fence statement right after label l in P . 6 return P .  Figure 3: Schematic \nview of the structure of our tool. Rectangles denote existing work, while oval shapes denote our work. \n Multi-Threading: we added support for user-level threads. This includes support for statements fork \nand join as outlined in Sec\u00adtion 3 as well as the self statement and per-thread local execu\u00adtion contexts. \n Relaxed Memory Models: we added support for both TSO and PSO memory models. This includes the relevant \nwrite buffers (outlined earlier), and extending LLVM intermediate instruc\u00adtions (e.g., stores and loads) \nto work with the memory model. We also support compare-and-swap (CAS) instructions, often used to implement \nlocks and concurrent algorithms.  Scheduler: we added scheduling support allowing us to plug-in different \n(demonic) schedulers for controlling the actions of the memory system (i.e., .ushing) and thread operations. \n Speci.cations: we added support for checking memory safety errors such as array out of bounds and null \ndereferencing. We also added support for checking conditions such as linearizabil\u00adity and sequential \nconsistency [14] (this required us to write a number of executable (sequential) speci.cations for all \nof the concurrent algorithms we analyzed).  Synthesis: to repair a violating execution, we recorded \nall shared memory accesses in the violating execution. The resulting ac\u00adcess history will be used to \nbuild a repair formula . as outlined in Semantics 2.  We believe that these extensions are of wide \ninterest: for in\u00adstance, we can use the framework for testing sequential programs for memory safety errors. \nOur extensions are parametric and the user simply selects the relevant parameters (e.g., which memory \nmodel, what properties to test) and runs their concurrent program with our version of the LLVM interpreter \nlli. Figure 3 shows the overall structure of our synthesis framework. The input to DFENCE is a concurrent \nC algorithm and a (concurrent) client that calls the methods of the algorithm. LLVM-gcc then compiles \nboth into a single bytecode .le. Then, the interpreter can exercise the program with the given scheduler \nand memory model. The sequence of calls and returns that appear in the execution is collected, and is \nthen checked against the given speci.cation (e.g., linearizability). If the execution violates the speci.cation, \na repair for this single execution is calculated. A global repair for a set of violating executions can \nbe calculated via a SAT solver and enforced into the given bytecode by inserting fences, producing a \nnew bytecode representation. The new bytecode is fed back to the interpreter and the process repeats \nuntil no more violations are found or a user-speci.ed timeout (or other bound e.g., number of times the \nprogram has run) is reached. All the phases of the .ow in Figure 3 are completed by the framework automatically. \n 5.2 Extensions The following provides additional details on each extension. Multi-Threaded Support Supporting \nuser level threads requires handling fork, join, and self. As outlined in Section 3, fork creates a new \nthread, join is used to wait for a thread to complete before proceeding and self returns the thread identi.er \nof the caller. The reason we added fork and join is for handling clients that exercise concurrent algorithms. \nThe reason we added self was because some concurrent algorithms require this method. In the interpreter, \nwe added a new map data structure, called ThreadStacks that maps a thread identi.er to a list of execution \ncontexts (stack frames). When the thread terminates, the vector becomes empty. When fork is encountered, \na new thread ID and a list of execu\u00adtion contexts is created in ThreadStacks. The execution of normal \nbytecodes proceed as-is (they access the particular execution con\u00adtext), and when a new function is called, \na new execution context will be created. The call to join completes only if the target thread has .nished, \nthat is, its execution context list is empty, otherwise, the caller will block. We added a function called \nGetEnabledThreads which returns the set of enabled threads. A thread is enabled if its list of execu\u00adtion \ncontexts is not empty. This function is typically used by the scheduler to decide which thread to schedule \nnext. Relaxed Memory Model Support We maintain a write buffer for each thread (TSO) or each shared variable \n(PSO). The write buffer is simulated with a FIFO queue. The write buffers are managed by a dynamic map, \nand a new write buffer will be created when the memory address is visited for the .rst time. In this \nlazy approach, we do not need to change the functions that allocate memory such as malloc and mmap. Note \nthat functions which de\u00adallocate memory, such as free and munmap, do not .ush the write buffers. Write \nbuffers are only assigned to shared variables and thread\u00adlocal variables access the memory directly. \nThe implementation of shared variable accesses follows closely Semantics 1. To handle programs with locks, \nwe implement lock acquire as a loop calling a CAS instruction to write 1 to the lock variable, and the \nfunction returns when CAS succeeds. Lock release writes 0 to the lock variable and returns. Both functions \nare wrapped with memory fences before and after the bodies, simulating the volatile feature of the lock \nvariable. Scheduler Support In our framework, the scheduler is designed as a plug-in, and is relatively \nindependent of the interpreter. This allows experimenting with different schedulers for exposing violations. \nTo control scheduling, the probability of stores being delayed in the write buffer is gated by a parameter \nwe call .ush probability. The scheduler considers this parameter when deciding on the next step. At each \nstep in the execution, based on the .ush probability, the scheduler can decide which thread will take \na step or which .ushing action should be performed (for instance, for PSO, it can chose to .ush only \nvalues for a particular variable):  To reduce scheduling overhead, we use a form of partial-order reduction \nwhere a thread is not context switched if it keeps accessing local memory (thread-local variables). \n At each scheduling point, an enabled thread is selected ran\u00addomly, and execution proceeds with the selected \nthread.  Once a thread is selected, if its write buffers are empty, the thread makes a step. Otherwise, \nthe scheduler randomly decides (with a user-provided .ush probability) whether to .ush the write buffer \nor proceed to the next step.  In our work, we found this strategy to be effective in expos\u00ading violations, \nbut the framework allows the user to try out other scheduling strategies. Speci.cations DFENCE supports \nboth checking for memory safety properties such as array out of bounds, and null de-referencing, as well \nas properties such as linearizability and sequential consistency. For space reasons, we do not elaborate \non the precise de.nition of sequential consistency and linearizability. These, as well as ex\u00adtensive \ndiscussion comparing the two can be found in [14, Ch. 3.4\u00ad3.5]. Here we only discuss the essence of these \nconditions. Given a program execution, both of these criteria require us to .rst obtain a history (call \nit H) by extracting the sequence of calls and returns that appear in the execution. To check whether \nH is sequentially consistent, we compute two items: (i) sequentialization of H, call it seq(H), and (ii) \na witness history W that simply executes the relevant speci.cation on the sequence of method calls appearing \nin seq(H). If we .nd a sequentialization of H, where W = seq(H), then we say that H is sequentially consistent, \notherwise, if we can\u00adnot .nd any sequentialization of H which matches a witness, we say that H is not \nsequentially consistent. Linearizability is a more restricted variant of sequential consistency: it restricts \nhow seq(H) is computed. However, linearizability is compositional, while se\u00adquential consistency is not. \nThere are three key points to take away: Checking linearizability or sequential consistency requires \na se\u00admantic sequential speci.cation of the algorithm, e.g., specify\u00ading how the push method of a stack \nbehaves sequentially. In\u00addeed, we had to provide such speci.cations for our experiments. The speci.cations \nare reusable for analyzing other concurrent algorithms: for instance, once we have the speci.cation of \na queue, we can use it to analyze any concurrent queue algorithm.  Linearizability is a more restricted \nproperty than sequential consistency and is the one commonly used in the literature for concurrent algorithms. \nHowever, the interplay between sequen\u00adtial consistency, linearizability and relaxed memory models has \nnot yet been well studied, and as we will see, interestingly, there are algorithms that look to be sequentially \nconsistent, yet are non-lineraizable under relaxed memory models. The advantage of such algorithms is \nthat they require less synchronization.  Checking both of these properties requires computing all seq(H) \ns of a history H, an operation clearly exponential in the length of H.  To check for linearizability \nand sequential consistency, the tool records a concurrent history and checks whether the concurrent history \nhas a witness sequential history, as discussed above. To detect memory safety violations, we keep auxiliary \ninforma\u00adtion such as the starting address and length, for global and heap memory units. For globals, \nthis can be obtained by scanning the global segment of the intermediate bytecode. For heap, we can ob\u00adtain \nthe information from calls to malloc and mmap (the auxiliary state is deleted upon calling free or munmap). \nWhen a load or a .ush instruction occurs, the analysis checks whether the (to be) accessed address is \nwithin the current avail\u00adable memory (by checking in the auxiliary state). To reduce the overhead of \nthe checking, we record each memory unit in a self balanced binary tree with the starting addresses as \nthe keys. If we .nd that the target address is not inside any memory units, then a memory safety violation \nis triggered. Synthesis Following Algorithm 1 from Section 4, once a violating execution p is discovered, \nthe global avoid formula . is updated to take avoiding p into account. In Algorithm 1, ? indicates the \nnon-deterministic choice of when the enforce procedure is invoked to produce a new program with a .x \ncovering all violating execu\u00adtions accumulated so far. In DFENCE, we realized ? as an iteration count, \ncounting up to a user-de.ned threshold. To reduce overhead, DFENCE only records the relevant (shared \nmemory accessing) in\u00adstructions and not the whole execution. To compute a satisfying assignment for the \nboolean formula ., we use the off-the shelf MiniSAT [10] solver. Each (non-repeated) clause in the formula \nis assigned a unique integer. The satisfying assignment produced by MiniSAT has no guarantee of minimality. \nTo obtain (all) minimal solutions, we call MiniSAT repeatedly to .nd out all solutions (when we .nd a \nsolution, we adjust the formula to exclude that solution), and then we select the minimal ones. Enforcing \nOnce a minimal solution is selected, the labels in the ordering predicates are used to index the instruction \npointers in the LLVM IR. The synchronization (e.g., fences) is then inserted into the IR by calling LLVM \ns instruction manipulation methods. The new IR is well-formed and is used for the next synthesis iteration. \nWe optimize the insertion further by applying a merge phase which combines certain redundant fences: \nwe apply a simple static analysis which eliminates a fence if it can prove that it always follows a previous \nfence statement in program order, with no store statements on shared variables occurring in between. \n6. Evaluation In this section, we evaluate the effectiveness of DFENCE in auto\u00admatically inferring fences \nfor realistic concurrent algorithms. We used LLVM-GCC version 4.2 for compiling C programs into LLVM \nbytecode and LLVM runtime version 2.7 [19] for our implementation. Our implementation is designed to \nhook at the relevant places in LLVM and hence we expect it to be portable to newer versions. We used \nthe open-source MiniSAT [10] solver. 6.1 Methodology We applied DFENCE to a number of challenging concurrent \nC algo\u00adrithms under different settings by varying four dimensions: (i) the memory model (TSO, PSO) (ii) \nthe speci.cation (memory safety, sequential consistency, and linearizability) (iii) the clients, and \n(iv) scheduler parameters. In the .rst part of the evaluation, we evaluate the quality of fence inference \nunder different choices along these four dimen\u00adsions. Many of the algorithms used in our experiments \nhave already been extensively tested on one memory model, usually TSO (x86). In this case, we .rst removed \nthe fences from the algorithms and then ran DFENCE to see if it could infer them automatically. When \nwe did not have the available fences for a given model, say PSO, we ran our tool and inspected the results \nmanually to make sure the inferred fences were really required. In the second part of our evaluation, \nwe explore some of the connections between the different dimensions. In particular, we set out to answer \nthe following questions: 1. Memory Model: How does the choice of memory model affect the choice of scheduler \nused to expose violations? How does it affect the choice of clients? 2. Client: How does the choice \nof client affect the ability to expose a violation, and how does it relate to the choice of speci.cation \nand memory model? 3. Speci.cation: How does the choice of speci.cation affect the resulting fence placements? \n  6.2 Benchmarks For our experiments, we used 13 concurrent C algorithms described in Tab. 2: .ve work-stealing \nqueues (WSQs), three idempotent work-stealing queues (iWSQ s), two queue algorithms, two set al\u00adgorithms, \nand one lock-free concurrent memory allocator. All eight of the work-stealing queues share the same operations: \nput, take and steal. The Chase-Lev WSQ example has already been dis\u00adcussed in Section 2. We chose WSQs \nbecause they are a key technique for sharing work and are at the heart of many runtimes. We are not aware \nof any previous study which discusses extensive analysis or inference for WSQs. Further, we believe that \nthis is the .rst study to analyze and infer fences for Michael s lock-free memory allocator, arguably \none of the largest and most complex concurrent C algorithms available. DFENCE consumes the C code directly \nand handles all low-level features of the implementation. 6.3 Results Tab. 3 summarizes our results. \nUnder each of the three speci.ca\u00adtions, we show the fences that DFENCE synthesized for a given memory \nmodel (we note that memory safety checking is always on, hence, Linearizability and Sequential Consistency \ncolumns include fences inferred due to memory safety violations). We describe each fence with the triple \n(m, line1: line2) to mean that we discovered the need for a fence in method m between lines line1 and \nline2. Weuse - to indicate the end of the method in the triple. For Chase-Lev s WSQ, we use the locations \nindicated in Fig. 1. For Cilk s THE WSQ, the line numbers correspond to our C imple\u00admentation. For all \nWSQ and iWSQ, the line numbers are the same as those found in [24]. Similarly for the rest of the algorithms. \nFor all iWSQ algorithms, the Memory Safety column also includes the no garbage tasks returned speci.cation. \nAnalysis of iWSQ algorithms under Linearizability or SC requires more involved sequential speci.cations \nand is left as future work. The last column shows the number of locations where a fence can be potentially \ninserted this corresponds to the total number of store instructions in the LLVM bytecode. Note that \nthis number is an order of magnitude greater than the actual number of inferred fences, meaning that \nwithout any help from the tool, we would need to manually examine an exponential number of location combina\u00adtions. \nBy Source Lines of Code (LOC), we mean the actual C code and by Bytecode LOC we mean the LLVM bytecode \n(which is a much larger number). 6.3.1 Additional and Redundant Fences The iWSQ algorithms of [24] are \ndesigned to not require store\u00adload fences in the owner s operations (on TSO). Our experimental results \nsupport this claim: the tool did not .nd the need for store\u00adload fences on both TSO and PSO. On PSO, \nfor all three iWSQ s, our tool .nds the need for an additional inter-operation fence (store-store) at \nthe end of take. In addition, for FIFO iWSQ, the tool suggests an additional inter\u00adoperation fence at \nthe end of put. We call these fences inter\u00adoperation as they appear at the end of operations (and can \neven be placed right outside the end of a method return). These inter\u00adoperation fences are not mentioned \nin [24] and the C code was only tested on TSO. Also, for the C implementation of the THE algorithm, our \ntool discovered a redundant (store-load) fence in the take operation. We believe these results suggest \nthat the tool may be helpful to a designer in porting concurrent algorithms to different memory models. \n 6.3.2 Number of Executions In Algorithm 1, we used ? to denote a non-deterministic choice of when to \nstop gathering violating executions and implement their repair. This lets us control the number of executions \n(say K) we gather before performing a repair on the incorrect subset of those executions. We refer to \neach repair phase as a round. Our analysis terminates whenever we see K executions and in those K execu\u00adtions \nthere are no incorrect executions. For example, one scenario with K = 1000 per round can be: Round 1: \nrun the program with 1000 executions, discover 500 bad executions and repair them using 2 fences.  Round \n2: run the new program again for 1000 executions, discover 100 bad execution and repair them using 1 \nfence.  Round 3: run the new program again for 1000 executions and discover no bad executions. The analysis \nterminates (having in\u00adtroduced a total of three fences to repair the observed execu\u00adtions).  Fig. 4 \nshows how the number of executions per round affects the synthesis results of Cilk s THE algorithm with \nsequential con\u00adsistency speci.cation on the PSO memory model (the x-axis is log). The multiple rounds \npoints should be interpreted as follows: Take a value on the x-axis, say K. This value stands for the \nnumber of executions per round.  Take the corresponding value on the y-axis, say F . This value stands \nfor the number of inferred fences. The number of rounds is always less than F +1. The +1 is to account \nfor the last round which does not .nd any violations and veri.es the .xing.  For example, if we take \nK = 1000, the corresponding y-axis value is F =3. Hence, the number of rounds is certainly = 4.  We \nobserve that with about 1000 executions per round, and with at most four rounds, we can infer all three \nof the required fences. Alternatively, we can try to gather as many executions as possi\u00adble and repair \nthem all at once. In that experiment we set the num\u00adber of rounds to one. The results are the one round \npoints in the .gure. These results indicate that we will need at least 200,000 ex\u00adecutions to infer all \nof the three fences, an increase of about 65x in the number of executions one needs to consider! The \nintuition is that once we .nd some violations and repair them, we eliminate many executions that need \nnot be inspected fur\u00adther. If we do not eliminate violating executions relatively quickly, then we can \nkeep seeing the same executions for a while (or exe\u00adcutions caused by the same missing fence(s)), and \nit may take sig\u00adni.canly longer to .nd all distinct violating executions. Therefore, DFENCE invokes the \nrepair procedure after examining a (relatively) small number of executions.  6.4 Client The choice of \nclient is an important factor in our experiments. If the client cannot exercise the inputs which could \ntrigger violations, even an ideal scheduler would be unable to expose errors. Client vs. Speci.cation \nThe worst case time required for checking linearizability or sequential consistency of an execution is \nexpo\u00adnential in the length of the execution. Algorithm Description Chase-Lev s WSQ [7] put, take operate \non the same end of the queue, steal works at the opposite end. Both take and steal implemented using \nCAS. Cilk s THE WSQ [12] put and take operate on the same end of the queue, steal works at the opposite \nend. Both take and steal use locks. This is the core algorithm used in the MIT s Cilk runtime. Anchor \niWSQ (Fig.3 [24]) put and take operate on the same end of the queue, steal works at the opposite end. \nOnly steal uses CAS. LIFO WSQ same as LIFO iWSQ except that all operations use CAS. FIFO WSQ same as \nFIFO iWSQ except that take uses CAS to update the head variable. Anchor WSQ same as Anchor iWSQ except \nthat all operations use CAS. MS2 Queue [23] enqueue and dequeue operate at different ends of a linked \nlist. Both use locks. MSN Queue [23] enqueue and dequeue operate at different ends of a linked list. \nBoth use CAS. LazyList Set [13] add, contains and remove operate on a sorted linked list. All three use \nlocks. Harris s Set [8] add, contains and remove operate on a sorted linked list. All three are implemented \nwith CAS. Michael s Allocator [21] The C lock-free memory allocator exports two functions: malloc and \nfree. These operations use CAS. Table 2: Algorithms used in our experiments. We use iWSQ as a shortcut \nfor idempotent work stealing queues. Benchmark Memory Safety Sequential Consistency Linearizability Source \nLOC Bytecode LOC Insertion Points TSO PSO TSO PSO TSO PSO Chase-Lev s WSQ 0 0 F1 F1, F2 F1, F2 F1, F2, \nF3 150 696 96 Cilk s THE WSQ 0 0 (put, 11:13) (take, 5:7) (put, 11:13) (take, 5:7) (steal, 6:8) - - 167 \n778 105 FIFO iWSQ 0 (put, 4:5) (put, 5:-) (take, 5:-) - - - - 149 686 102 LIFO iWSQ 0 (put, 3:4) (take, \n4:-) - - - - 152 702 101 Anchor iWSQ 0 (put, 3:4) (take, 4:-) - - - - 162 843 107 FIFO WSQ 0 0 0 (put, \n4:5) (put, 5:-) (put, 4:5) (put, 4:5) (put, 5:-) 143 789 91 LIFO WSQ 0 0 0 (put, 3:4) 0 (put, 3:4) 136 \n693 92 Anchor WSQ 0 0 0 (put, 3:4) 0 (put, 3:4) 152 863 101 MS2 Queue 0 0 0 0 0 0 62 351 46 MSN Queue \n0 0 0 (enqueue, E3:E4) 0 (enqueue,E3:E4) 81 426 43 LazyList Set 0 0 0 0 0 0 121 613 68 Harris s Set 0 \n0 0 (insert, 8:9) 0 (insert, 8:9) 155 695 86 Michael s Memory Allocator 0 (MFNSB, 11:13) (DescAlloc, \n5:8) (DescRetire, 2:4) 0 (MFNSB, 11:13) (DescAlloc, 5:8) (DescRetire, 2:4) (free, 16:18) 0 (MFNSB, 11:13) \n(DescAlloc, 5:8) (DescRetire, 2:4) (free, 16:18) 771 2699 244 Table 3: Fences inferred for each algorithm \nunder different speci.cations and memory models. The table also shows the size of each algorithm in terms \nof Source LOC, LOC in bytecode, and insertion points. MFNSB stands for MallocFromNewSB. 0 in the table \nmeans that the tool did not .nd any fences; - means cannot satisfy the property (like Cilk s THE WSQ) \nor that a speci.cation is not yet available . For WSQ s and iWSQ s, the numbers do not include fences \nin the expand function. For linearizability, the complexity can be reduced to linear if the programmer \nprovides linearization points, as commonly required in veri.cation [29]. However, for many algorithms \n.nding lineariza\u00adtion points is very dif.cult. Further, for properties such as sequen\u00adtial consistency, \nthe concept of a linearization point is not applica\u00adble. This means that when we are checking these two \nproperties, it is important to have the client produce relatively short executions, yet rich enough to \nexpose violations. Client vs. Coverage A good client should achieve good coverage, at the least, it would \nallow for all program points in each method to be visited. For example, in the case of a WSQ, it is important \nto explore schedules where concurrent threads interact in a queue that is sometimes empty and sometimes \nnot. This is because different violations can occur in both cases. Thus it is important that a client \ncan generate both cases. Figure 4: Number of Inferred Fences vs. Number of Rounds/Executions-per-Round. \n Figure 5: Effect of Flush Probability for Cilk s THE WSQ on PSO. 6.5 Scheduler vs. Memory Model We \n.nd that setting the right .ush probability (described in Sec. 5.2) is a key parameter for effectively \ntriggering violations. With small values for the .ush probability, meaning less .ushing of the write \nbuffers, we expose more violations and hence synthesize more missing fences, than we do with greater \nvalues. If the .ush proba\u00adbility is too high, then the behavior of the program becomes as if the memory \nmodel is sequentially consistent (the intuition is that we are .ushing more frequently, so the buffers \nare mostly empty), and hence it is more dif.cult to .nd violations. Fig. 5 illustrates how the number \nof synthesized fences corre\u00adlates with the .ush probability for Cilk s THE WSQ under PSO with K = 1000. \nWe can observe two tendencies: When the .ush probability is large, i.e., greater than 0.8, the number \nof discovered ordering predicates decreases. With fewer predicates, some required fences cannot be inferred. \n When the .ush probability is small, i.e., less than 0.4, the same unnecessary predicates may appear \nin most buggy executions, causing redundant fences. Thus the number of redundant fences is increased. \n  There are two ways to eliminate redundant fences. First, we can increase the .ush probability. Second, \nwe can collect more vio\u00adlating executions by increasing the execution number per round. If one of the \nviolating executions does not contain the reordering which caused the generation of a redundant fence, \nthen the redun\u00addant fence will not be inferred. Flush Probability vs. PSO For PSO, we found that a .ush \nprob\u00adability around 0.5 is suitable for optimal inference for all bench\u00admarks and speci.cations. With \nthis value, the number of execu\u00adtions should be set high enough to collect suf.cient predicates. For \nan accurate inference, we .nd that the number of collected vio\u00adlating executions should be around 10 \ntimes greater than the maxi\u00admum number of possible predicates collected for the program. This maximum \nnumber can be obtained by testing executions with a low .ush probability, and in Fig. 5 this number is \n36. Flush Probability vs. TSO Interestingly, we .nd that the optimal .ush probability for the same program \non TSO can be quite a bit smaller than the value used for PSO. In our experiments, the .ush probability \nfor TSO is usually set to 0.1. The intuition is that for the same program, the number of write buffers \nin TSO is usually much smaller than the number of buffers in PSO. This means that with the same .ush \nprobability, the chance that a buffer is .ushed in TSO is higher than in PSO. It follows that if we use \ntoo high of a .ush probability for write buffers, we are less likely to .nd violations caused by memory \nmodel reorderings. We .nd that for the same speci.cation, under PSO, the program requires more fences \nthan under TSO. This is reasonable, since PSO allows both store-store and store-load reordering, while \nTSO only allows a store-load reordering. 6.6 Fences vs. Speci.cation Memory Safety is Ineffective As \nwe see in the table of results, and perhaps counter-intuitively, we .nd that memory safety speci.ca\u00adtions \nare almost always not suf.ciently strong to trigger violations. For instance, with WSQ s, this is because \na violating execution is more often exhibited in the form of losing an item or in returning duplicate \nitems, rather than in accessing out of bounds memory. We note that one trick that may make memory safety \nmore effective in triggering violations is to use a speci.c client: instead of elements of a primitive \ntype, one stores pointers to newly allocated memory in the queue. Then, the client frees the pointer \nimmediately after it has fetched it from the queue. In that way, one may be able to detect duplicate \nitems. We leave this experiment as future work. We .nd that safety speci.cations are useful for the memory \nallocator as there are many pointer dereferences performed in the code, and the buffering of an updated \npointer value can cause a dereference of NULL pointer in another thread. Linearizability vs. Sequential \nConsistency Another interesting point is that for the same memory model, linearizability gener\u00adally requires \nmore fences than sequential consistency. On one side, this can be expected as linearizability is a stronger \nproperty, but on the other, it is not obvious that by slightly weakening linearizability to sequential \nconsistency, one does not end up with a completely incorrect algorithm (in the sense of losing items \nfor WSQ s or violating memory safety). For the four WSQ s (excluding THE), the fence added to guarantee \nlinearizability is used to trigger the .ushing at the linearization point. Algorithm without fences on \nTSO Interestingly, by weakening the correctness criteria from linearizability to sequential consistency, \nwe obtain an algorithm, FIFO WSQ on TSO, for which the tool does not .nd any fences (except the slow \npath expand function). Note that unlike iWSQ algorithms, we did not weaken the actual speci.cation of \nFIFO, that is, we did not need an idempotent vari\u00adant of FIFO. Our .nding is also inline with a recent \nresult which shows that it is impossible to eliminate certain orders with lineariz\u00adability [3], but by \nslightly weakening it, that may be possible. Cilk s THE is not linearizable As our tool can check properties \nwithout any relaxed memory model effects, we were able to de\u00adtect that Cilk s THE WSQ is not linearizable \nwith a deterministic sequential speci.cation (even without any relaxed memory model effects). However, \nthe tool indicates that the algorithm is sequen\u00adtially consistent and is indeed accepted to be correct \n[1]. DFENCE infers the necessary fences under the sequential consistency speci\u00ad.cation. 6.7 Memory Allocator \nFor the memory allocator, we used the client sequence mmmfff | mfmf, where m stands for malloc and f \nstands for free. In this sequence, the free function always frees up the oldest memory unit allocated \nby the same thread. As far as we know, no previous work can syn\u00adthesize synchronization for concurrent \nalgorithms of this scale. For the allocator, we found that three fences were required to satisfy the \nmemory safety property, i.e., running to termination without segmentation fault. If we speci.ed the additional \ncriteria such as sequential consistency or linearizability, one additional fence in the free function \nis inferred. All four of the fences that our tool inferred are discussed in the paper which describes \nthe memory al\u00adlocator algorithm [21]. The C implementation of the allocator has 11 store-store memory \nfences and DFENCE was able to infer only 4 of them. The problem of how to uncover the other 7 fences \nis left as future work. 7. Related Work Exhaustive Approaches for Relaxed Memory Models The works in \n[16, 25] describe explicit-state model checking for the Sparc RMO model, but neither of these approaches \nperform fence infer\u00adence. The work in [15] describes an explicit-state model checker and a simple inference \ntechnique for the .NET memory model. Recent work [17, 18] focuses on fence inference based on model checking \n(with abstraction). In [4], a different approach is taken where instead of working with explicit state, \nthey convert programs into a form that can be checked against an axiomatic model speci.\u00adcation. Collectively, \nthe fundamental limitation of these exhaustive approaches is that they are not inherently scalable for \nlarger con\u00adcurrent programs and often require manual intervention (to specify abstractions or a model \nof the program). In contrast, our focus is squarely on handling large concurrent C implementations, which \nin turn dictates our dynamic synthesis approach. Delay Set Analysis A number of works on fence inference \n[11, 20, 27] rely on concepts such as delay sets and con.ict graphs [26]. Particularly, the Pensieve \nproject implements fence inference based on delay set analysis. This kind of analysis is necessarily \nmore conservative than ours for two reasons: .rst, it is static in nature, and second, the correctness \ncriteria are more restrictive than the ones used in this work. For instance, their criteria would prevent \nbehaviors that are linearizable and should be allowed. Other Approaches The works in [5, 6] present algorithms \nthat .nd violations under the TSO and PSO memory models based on correctness criteria similar to the \nones used in the delay set analysis approaches mentioned above, meaning that [5, 6] can be needlessly \nconservative. Further, both of these works do not support fence inference. In [30], the authors propose \nalgorithms that automatically infer synchronization constructs such as atomic sections. Their work does \nnot deal with relaxed memory models. 8. Conclusion We introduced dynamic synthesis for concurrent programs \nrunning on relaxed memory models. The key idea is to break the scalability barrier of standard synthesis \napproaches by instead focusing on dy\u00adnamic executions: when a set of violating executions is discovered, \nthe program is automatically repaired to avoid these executions, and the synthesis procedure continues \nwith the repaired program. We implemented our techniques in LLVM and evaluated its ef\u00adfectiveness on \na comprehensive list of concurrent C algorithms in\u00adcluding a complex lock-free memory allocator. Each \nalgorithm was evaluated with two memory models (TSO and PSO), three speci.\u00adcations (linearizability, \nsequential consistency and memory safety), various repair strategies, and different scheduling parameters \n(e.g. .ush probability). We demonstrated the effectiveness of our ap\u00adproach by automatically synthesizing \nnecessary fences for each of the concurrent algorithms. As future work, we plan to extend our tool with \nmore advanced demonic schedulers to discover violations even quicker. We also plan to add support for \nother memory models and to evaluate our tool on a wider set of concurrent C programs. References [1] \nPersonal Communication with the Cilk Team, 2011. [2] ADVE, S. V., AND GHARACHORLOO, K. Shared memory \nconsis\u00adtency models: A tutorial. IEEE Computer 29 (1995), 66 76. [3] ATTIYA, H., GUERRAOUI, R., HENDLER, \nD., KUZNETSOV, P., MICHAEL, M. M., AND VECHEV, M. Laws of order: expensive synchronization in concurrent \nalgorithms cannot be eliminated. In POPL 11 (New York, NY, USA), ACM, pp. 487 498. [4] BURCKHARDT, S., \nALUR, R., AND MARTIN, M. M. K. Check-Fence: checking consistency of concurrent data types on relaxed \nmem\u00adory models. In PLDI (2007), pp. 12 21. [5] BURCKHARDT, S., AND MUSUVATHI, M. Effective program veri.\u00adcation \nfor relaxed memory models. In CAV (2008), pp. 107 120. [6] BURNIM, J., SEN, K., AND STERGIOU, C. Testing \nconcurrent pro\u00adgrams on relaxed memory models. In ISSTA (2011), pp. 122 132. [7] CHASE, D., AND LEV, \nY. Dynamic circular work-stealing deque. In SPAA (2005), pp. 21 28. [8] DETLEFS, D. L., FLOOD, C. H., \nGARTHWAITE, A. T., GARTH-WAITE, E. T., MARTIN, P. A., SHAVIT, N. N., JR., AND STEELE, G. L. Even better \ndcas-based concurrent deques. In DISC 00. [9] DICE, D., LEV, Y., MARATHE, V. J., MOIR, M., NUSSBAUM, \nD., AND OLESZEWSKI,M. Simplifyingconcurrentalgorithmsbyexploit\u00ading hardware transactional memory. In \nSPAA 10, ACM, pp. 325 334. [10] E \u00b4ORENSSON, N. In SAT EN, N., AND S \u00a8An extensible sat-solver. (2003), \npp. 502 518. [11] FANG, X., LEE, J., AND MIDKIFF, S. P. Automatic fence insertion for shared memory multiprocessing. \nIn ICS (2003), pp. 285 294. [12] FRIGO, M., LEISERSON, C. E., AND RANDALL, K. H. The imple\u00admentation \nof the cilk-5 multithreaded language. In PLDI 98. [13] HELLER, S., HERLIHY, M., LUCHANGCO, V., MOIR, \nM., SCHERER, W., AND SHAVIT, N. A lazy concurrent list-based set algorithm. In OPODIS 05, pp. 3 16. [14] \nHERLIHY, M., AND SHAVIT, N. The Art of Multiprocessor Program\u00adming. Morgan Kaufmann, Apr. 2008. [15] \nHUYNH, T. Q., AND ROYCHOUDHURY, A. Memory model sensitive bytecode veri.cation. Form. Methods Syst. Des. \n31, 3 (2007). [16] JONSSON, B. State-space exploration for concurrent algorithms under weak memory orderings. \nSIGARCH Comput. Archit. News 36,5 (2008), 65 71. [17] KUPERSTEIN, M., VECHEV, M., AND YAHAV, E. Automatic \ninfer\u00adence of memory fences. In FMCAD 10. [18] KUPERSTEIN, M., VECHEV, M., AND YAHAV, E. Partial-coherence \nabstractions for relaxed memory models. In PLDI 11, pp. 187 198. [19] LATTNER, C., AND ADVE, V. LLVM: \nA Compilation Framework for Lifelong Program Analysis &#38; Transformation. In CGO 04, pp. 75 87. [20] \nLEE, J., AND PADUA, D. A. Hiding relaxed memory consistency with a compiler. IEEE Trans. Comput. 50, \n8 (2001), 824 833. [21] MICHAEL, M. M. Scalable lock-free dynamic memory allocation. In PLDI 04 (2004), \npp. 35 46. [22] MICHAEL, M. M., AND SCOTT, M. L. Correction of a memory management method for lock-free \ndata structures. Tech. rep., 1995. [23] MICHAEL, M. M., AND SCOTT, M. L. Simple, fast, and practical \nnon-blocking and blocking concurrent queue algorithms. In PODC (1996), pp. 267 275. [24] MICHAEL, M. \nM., VECHEV, M. T., AND SARASWAT, V. A. Idem\u00adpotent work stealing. In PPoPP (2009), pp. 45 54. [25] PARK, \nS., AND DILL, D. L. An executable speci.cation and veri.er for relaxed memory order. IEEE Trans. on Computers \n48 (1999). [26] SHASHA, D., AND SNIR, M. Ef.cient and correct execution of parallel programs that share \nmemory. ACM Trans. Program. Lang. Syst. 10, 2 (1988), 282 312. [27] SURA, Z., WONG, C., FANG, X., LEE, \nJ., MIDKIFF, S., AND PADUA, D. Automatic implementation of programming language consistency models. LNCS \n2481 (2005), 172. [28] SUTTER, H., AND LARUS, J. Software and the concurrency revolu\u00adtion. Queue 3, 7 \n(2005), 54 62. [29] VECHEV, M., YAHAV, E., AND YORSH, G. Experience with model checking linearizability. \nIn SPIN 09, pp. 261 278. [30] VECHEV, M., YAHAV, E., AND YORSH, G. Abstraction-guided synthesis of synchronization. \nIn POPL (2010). [31] WEERATUNGE, D., ZHANG, X., AND JAGANATHAN, S. Accentu\u00adating the positive: atomicity \ninference and enforcement using correct executions. In OOPSLA 11, pp. 19 34.   \n\t\t\t", "proc_id": "2254064", "abstract": "<p>Modern architectures implement relaxed memory models which may reorder memory operations or execute them non-atomically. Special instructions called memory fences are provided, allowing control of this behavior.</p> <p>To implement a concurrent algorithm for a modern architecture, the programmer is forced to manually reason about subtle relaxed behaviors and figure out ways to control these behaviors by adding fences to the program. Not only is this process time consuming and error-prone, but it has to be repeated every time the implementation is ported to a different architecture.</p> <p>In this paper, we present the first scalable framework for handling real-world concurrent algorithms running on relaxed architectures. Given a concurrent C program, a safety specification, and a description of the memory model, our framework tests the program on the memory model to expose violations of the specification, and synthesizes a set of necessary ordering constraints that prevent these violations. The ordering constraints are then realized as additional fences in the program.</p> <p>We implemented our approach in a tool called DFence based on LLVM and used it to infer fences in a number of concurrent algorithms. Using DFence, we perform the first in-depth study of the interaction between fences in real-world concurrent C programs, correctness criteria such as sequential consistency and linearizability, and memory models such as TSO and PSO, yielding many interesting observations. We believe that this is the first tool that can handle programs at the scale and complexity of a lock-free memory allocator.</p>", "authors": [{"name": "Feng Liu", "author_profile_id": "81490665186", "affiliation": "Princeton University, Princeton, NJ, USA", "person_id": "P3471288", "email_address": "fengliu@princeton.edu", "orcid_id": ""}, {"name": "Nayden Nedev", "author_profile_id": "81502812683", "affiliation": "Princeton University, Princeton, NJ, USA", "person_id": "P3471289", "email_address": "nnedev@princeton.edu", "orcid_id": ""}, {"name": "Nedyalko Prisadnikov", "author_profile_id": "81502800759", "affiliation": "Sofia University, Sofia, Bulgaria", "person_id": "P3471290", "email_address": "nedy88@gmail.com", "orcid_id": ""}, {"name": "Martin Vechev", "author_profile_id": "81100269652", "affiliation": "ETH Zurich, Zurich, Switzerland", "person_id": "P3471291", "email_address": "martin.vechev@inf.ethz.ch", "orcid_id": ""}, {"name": "Eran Yahav", "author_profile_id": "81100285431", "affiliation": "Technion, Haifa, Israel", "person_id": "P3471292", "email_address": "yahave@cs.technion.ac.il", "orcid_id": ""}], "doi_number": "10.1145/2254064.2254115", "year": "2012", "article_id": "2254115", "conference": "PLDI", "title": "Dynamic synthesis for relaxed memory models", "url": "http://dl.acm.org/citation.cfm?id=2254115"}