{"article_publication_date": "01-01-1978", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee.&#38;#169; \n1978 ACM 0-12345-678-9 $5.00 Conference Record of the Fifth Annual ACM Symposium on Principles of Programming \nLanguage? SYMBOLIC PROGRAM ANALYSIS IN ALMOST LINEAR TIME John H. Reif Department of Computer Science \nThe University of Rochester Abstract A global flow model is assumed; as usual, the flow of control is \nrepresented by a digraph called the control flow graph. The objective of our pro\u00adgram analysis is the \nconstruction of a mapping (a cover) from program text expressions to symbolic expressions for their value \nholding over all execu\u00adtions of the program. The particular cover con\u00adstructed by our methods is in general \nweaker than the covers obtainable by the methods of [Ki, FKU, Rll,but our method has the advantage of \nbeing very efficient; requiring O(!, +aa(a)) extended bit vector operations (a logical operation or a \nshift to the first nonzero bit) on all control flow graphs (whether reducible or not), where a is the \nnumber of edges of the control flow graph, L is the length of the text of the program, and ~ is Tarjan \ns function (an extremely slowly growing function). 1. Introduction The flow of control through a program \nP is represented by the control flow graph F = (N,A,s) where each node n CN is a block of assignment \nstatements and each edge (m,n) CA specifies possi\u00adble flow of control from n to m, and all flow of control \nbegins at the start block SCN. A ~ in F is a sequence tra=ng nodes in N linked by edges in A. We assume \nthat for each nsN-{s}, there is at least one path from s to n. For m, nsN, m dominates n if all paths \nfrom s to n con\u00adtain m (m properly dominates n if in addition, n+m). The dominator relation may be represented \nby a dominator tree such that m dominates n iff m is an ancestor of n. The father of n is the immediate \ndominator of n. Let z={X,Y,Z,. . .} be the set of program variables occurring globally within P. A program \nvariable XSZ is defined in some node nsN if X occurs on the left hand side of an assignment statement \nof n. For each neN-{s} and p~~gram var\u00ad iable Xez, we have an input variable X to denote the value of \nX on entrance to n. Let EXP be the set of expressions built from input variables and fixed sets of contant \nsigns C and k-adic function signs o. For each n&#38;N and program var\u00adifi$le XEX defined in n, let the \noutput expression x be an expression in EXP for the value of X on exit from n in terms of the input variables \nin block n. A-expression is an output expression or a subexpression of an output expression. For each \nmEN such that n dominates m, pro\u00adgram variable X is defined between nodes ~and m if X is output on some \nn-avoiding path from an immediate successor of n to an immediate prede\u00adcessor of m (otherwise, X is \ndefinition-free between fland u). For each nEN-{s}, let IN(n~nbe the set of program variables XCX such \nthat X is a text expression (i.e., X+n appears within an output expression of n). The weak environment \nis a partial mapping W from input variables to N; for each input variable X+n such that XE IN(n), W(X+n) \nis the earliest (i.e., closest to the start node s) dominator of n such that X is definition-free between \nW(X+n) and n. We now discuss various applications of the weak environment. For each text expression t \nlocated at ncN, the birthpoint of t is the earli\u00ad est dominator of n to which the computation associated \nwith t may be moved. Code motion is the process of moving code out of co~l=s, into new locations where \nthe code is used less frequent\u00adly. This code improvement requires approximate knowledge of birthpoints, \nas well as other know\u00adledge including the cycle structure of the control flow graph. (We may not wish \nto move code as far as the birthpoint since the birthpoint may be con\u00adtained in control cycles avoiding \nn; see [CA, AU, E, G,R21 for further discussion of code motion optimizations. ) In [R33 it is shown that \nin the arithmetic domain, the problem of determining birthpoints is recursively unsolvable. [Ki, FKU, \nRll present algorithms which may be used to compute approximate birthpoints (i.e., nodes which are dominated \nby the true birthpoint); however, the time cost of the best of these algorithms is lower-bounded by C(]ZI \nIA]+!.), We may use the weak environment W to construct a function BIRTHPT mapping text expressions to \napproximations of their respective birthpoints. For each text expression t, BIRTHPT(t) is the latest \n(as far as possible from the start node s) node in {W(X+n)lX+n is a text expression of t}, relative to \nthe dominator relation. Thus, for each text expression, birth\u00adpoint(t) dominates BIRTHPT(t). An expression \ne&#38;EXP covers text expression t if e represents the value of t over all executions of the program. \nThe origin of e is the latest in the chain of nodes on the dominator tree occurring within e (i.e., in \nthe superscripts of the input variables contained in e). A cover is a mapping from text expressions to \ncovering expressions, and is minimal if the origin of the covering expres\u00adsions in its range are earliest \nin the dominator ordering (i.e., as close as possible to the start node s). Note that for each text expression \nt, the origin of the minimal cover of t is the birthpoint of t, From the weak environment W we can compute \nthe simple cover which is a cover $ such that for each text expression t, $(t) is derived from t by substituting \n$(Xw) for each input variable X+n such that m= W(X+n) properly dominates n. (Note that this definition \nrequires that X be defined at m; if not, we add at block m the dummy assignment X:=X so that Xb=XW is \na new text expression. At most O(L) dummy assignments must be so added.) See Figure 1 for an example \nof a simple cover. s T Y:= X+y nl P A further application of the weak environment involves the global \nvalue graphs of [Se, RII to represent the flow of values through the program. For certain special global \nvalue graphs, the algo\u00ad rithm of [Rll constructs a cover in time almost linear in the size of the global \nvalue graph. By a simple, but somewhat inefficient method, we can construct such a special global value \ngraph of size O(lxllAl+k). However, by another method which uti\u00adlizes the weak environment we can construct \na glo\u00adbal value graph of size O(dlAl+i), where d is a parameter of the program P which may be as large \nas Iz] but is often constant for block-structured programs. Hence, the very efficient (but weak) symbolic \nevaluation of this section may serve as a preprocessing step, to speed up a more powerful method for \nsymbolic evaluation presented in [RI]. The organization of this paper is as follows: In the next section \nwe define the relevant graph terminology. In Section 3 we describe an algorithm which constructs a function \nIDEF giving those pro\u00adgram variables defined between nodes and their immediate dominators. The IDEF computation \nis of a class of path problems that may be efficiently solved by an algorithm due to Tarjan [T41 on redu\u00adcible \nflow graphs; we extend his algorithm so as to compute IDEF efficiently on all flow graphs. Section 4 \npresents an algorithm for constructing the weak environment; this algorithm requires the previously computed \nfunction IDEF and contains an interesting data structure for efficiently main\u00adtaining multiple symbolic \nenvironments. Section 5 concludes the paper with the construction of the simple cover from the weak environment. \nAs in [RI], we collapse the dags (labeled, acyclic digraphs), representing linear blocks of code, into \na global dag representing the simple cover. 2. Graph Theoretic Notions Adigraph G=(V,E) consists of a \nset V of elements called nodes and a set Eof ordered pairs of nodes called edges. The edge (u,v) departs \nfrom u and enters v. We way u is an immediate prede\u00adcessor of v and v is an immediate successor of u. \nThe outdegree of a node v is the number of immedi\u00adate successors of v, and the indegree is the number \nof immediate predecessors of v. A ath from u to w in G is a sequence of nodes p=%-u=~2~.~,~k=w) where \n(vi,vi+l)~E for all i, l~i<k. The length of the path p is k-1. The path p may be built by composing subpaths: \nP=(vl, . . ..vi) (vi>. ..>vk). The path p is a cycle if U=W. A strongly connected component of G is a \nmaximal set of nodes such that any pair is contained in a cycle. A node u is reachable from a node v \nif either U=V or there is a path from v to u. We shall require various sorts of special digraphs. A rooted \ndigraph (V,E,r) is a triple such that (V-a digraph and r is a distin\u00adguished node in V, the root. A flow \ngraph is a rooted digraph such that the root r has no prede\u00adcessors and every node is reachable from \nr. A digraph is labeled if it is augmented with a map\u00adping whose domain is the vertex set. An oriented \nS!QEQ!lis a digraph au9mented with an orderingof the edges departing from each node. A digraph G is acyclic \nif G contains no cycl es. If u is reachable from v, u is a descend\u00adant of v and v is an ancestor of u \n(these rela\u00adtions are proper if u~des with no proper ancestors are called roots and nodes with no proper \ndescendants are leaves Immediate succes\u00adsors are called sons. A-~1 ordering consis\u00adtent with either the \ndescendant or the ancestor relation is a topological ordering of G. A flow graph T is a tree if every \nnode v other than the root has a~que immediate prede\u00adcessor, the father of v. A topological ordering \nof a tree is a preordering if it proceeds from the root to the leaves and is a postordering if it begins \nat the leaves and ends at the root. A~\u00adn~ng tree of a flow graph G=(V,E,r) is a tree with node set V, \nan edge set contained in E, and a root r. Let G=(V,E,r) be a flow graph. A node u dominates a node v \nif every path from the root to v includes u (u properly dominates v if in addition u+v). It is easily \nshown that there is a unique tree TG, called the dominator tree of G, such that u dominates v in G iff \nu is an ancestor of v in TG. The father of a node in the dominator tree is the immediate dominator of \nthat node. 3. The IDEF Computation We describe here an efficient algorithm for computing a function \nIDEF, giving those program variables defined between nodes and their immediate dominators. The IDEF computation \nis of a class of path problems that may be efficiently solved by an algorithm due to Tarjan [T21 on reducible \nflow graphs; we extend his algorithm so as to compute IDEF efficiently on all flow graphs. The essence \nof Tarjan s algorithm is to partition the nodes in N so that for each n, meN, n and m are in the same \ndominator strongly connected component (DSCC) iff n and m have the same immediate dominator w and there \nexists a w-avoiding control cycle con\u00adtafning both m and n. In the case where each DSCC contains but \na single node, then by [HUII the flow graph is reducible, and Tarjan s algorithm runs in time O(lAla(lA/)). \nOtherwise Tarjan s algorithm must fall back on the less efficient node listing tech\u00adniques requiring \ntime in the worst case quadratic in INI. For our special problem (the computation of IDEF), the DSCCSmay \nbe solved efficiently, yielding an algorithm of cost O(lAICL(lA\\)) bit vector operations. We now define \nthe problem more formally. Let F=(N,A,s) be the control flow graph and let DT be the dominator tree of \nF. For each node nEN let OUT(n) be the set of program variables defined at n and for each node m properly \ndominated by n, let DEF(n,m) be the set of program variables defined between n and m. Also, for each \nnEN-{s} let IDOM(n) be the immediate dominator of n, and let IDEF(N)= DEF(IDOM(n),n), i.e., the set of \nprogram variables defined between IDOM(n) and n. The above equation may be inverted as follows: DEF(n,m) \n=(fi IDEF(zi)) vkUIOUT(zi), i=2 i=2 where (n=z1,z2,. ... zk=m) is the dominator chain from n to m. Thus, \ngiven the dominator tree DT, DEF and IDEF can be computed from each other, The algorithm for computing \nIDEF proceeds in a postorder (leaves to root) scan of the dominator tree DT of F. We compute in one pass \nIDEF(n) for all sons n of a fixed node w. Clearly this is tri\u00advial if w is a leaf of the dominator tree \n( son and father refer to the dominator tree DT). Otherwise, a digraph is formed by connecting to\u00adgether \nthose sons of w in DT that are connected in F by paths that avoid w (such paths pass through proper descendants \nin DT of w only). The strongly connected components of this digraph are DSCCSand been processed, all \nhave been collapsed into w, and the procedure may be repeated on the sons of some other node w . may \nthen be processed in topological order; as each is processed, it is identified with the parent node w \nitself. Thus when all sons of w have To be precise, a set of nodes ScN is con\u00addensed by the following \nprocess: 1) Delete the nodes in S from the node set N and add in their place the set S (which is considered \nto be a new node). 2) Delete each edge entering a node in S and substitute a corresponding edge entering \nthe new node S. 3) Similarly, substitute an edge departing from the new node S for any edge departing \nfrom an element of s. 4) Finally, delete any new trivial loops which both depart from and enter the new \nnode S. Now let ~ consist of the set of edges in A departing from a node other than w and entering a \nson of w. Such an edge must depart from a proper descendant of w; otherwise, the node it enters would \nnot be dominated by w. For each proper descendant m in DT of w, let H(m,w) be the unique son in DT of \nw on the dominator chain from w to m, i.e., w immediately dominates H(m,w) which domi\u00adnates m. Let GW=(IDOM-l[W],EW) \nbe a digraph with nodes the sons in DT of w and edges Ew={(H(m,w),n) l(m,n) SAW}. It is easy to show \nthat: Lemma 3.1: For each n,n CIDOM-l[W], there exists a path in ~ from n to n iff there exists a w\u00adavoiding \npath in F from n to n . Note that by the above lemma, each strongly connected region of Gw is a DSCC. \nThe digraph G~, derived from Gw by condensing each DCSS, is called the condensation of Gw and is obviously \nacyclic. We shall process each DSCC of Gw in topological order of G; (from roots to leaves). In the special \ncase where each DSCC of Gw consists of the singleton set, then F is called reducible ([HU1] give various \nother characteriza\u00adtions of flow graph reducibility), and Tarjan s algorithm runs in the O(lA\\a(\\A\\ )). \nHowever, in the case that F is nonreducible, various DSCCS will contain two or more nodes and Tarjan \ns algo\u00adrithm becomes considerably more expensive and complex. The theorem below expresses IDEF(n) in \nterms of DEF on previously computed domains; this theorem holds even when the cardinality of a DSCC is \ngreater than 1, giving an efficient method for computing IDEF for all F, both reducible and non\u00adreducible. \nFix a topological order (from roots to leaves) of G~ and consider a DSCC of Gw, say S. Let mEN be a descendant \nofw in DT such that H(m,w) is either (1) in S or (2) in some DSCC S of ~ such that there is a path in \n~ from S to S (i.e., S precedes S in the topological order). Then let H (m,w,S) be H(m,w) in case (1) \nandw in case (2). That is, H (m,w,S) is just H(m,w), the unique son in DT of w which is an ancestor of \nm in DT, unless S contains H(m,w), in that case; H(m,w) is to be viewed as collapsed into w. The partial \nfunction H (m,w,S) plays a critical role in the inductive correctness proof of our algorithm. Note that \nfor each node nsS and edge (m,n)c ~, H(m,w) satisfies either (1) or (2). We wish to compute IDEF(n). \nWe may assume that due to previous computations, DEF(w,m) is known in case (l), and the value DEF(H(w,m),m) \nis known in case (2). Our immediate goal is to relate IDEF(n) to these pre\u00adviously computed values. Call \na DSCC S of ~ trivial if S contains a single node {n} and (n,n)4E,ti ,, and otherwise non\u00adtrivial. Now \ndefine Q:={ } ifS is trivial and otherwise, if S is nontrivial 1et Q\\ =n~XIT(n). Also, define Q~=n~5 \n(DEF(H (m,w, S),m)U OUT(m)). (m,n)c~ Theorem 3.1: For each ncS, IDEF(n)=Qlu Q2. 7Note that this characterization \nof ID~F(nf pro\u00ad vides an algorithm for computing IDEF(n) for all sons n of w, by induction on the topological \nordering of GJ.) Proof: Suppose XeIDEF(n), so there is a path ~<=kul,... ,Uk=n) such that X =O!JT(ui) \nfor some Case 1: If ui sS, then S must be nontrivial and Xc OUT(uj)5Q~. Case 2: Otherwise, suppose ui4S. \nLet Uj be the -node occurring after ui in p such that uj sS; then (uj-l,uj)EAw. Case 2.1: Iflil=uj-1 \nthen XC fJIJT(ui)= OUT(uj-l)~Q~. Case 2.2: Otherwise, suppose uj#u,j-l. Then -S) is some Uj , l;j;;~T \ns~;~ot~~~eu;h;;d Lij-l are descendants of Uj u. = H (uj-19w,S). Then X~OUT(ui )~DEF(ujl,uj-l)= D~F(H \n(uj-l,w,S),uj-l )5Q~. Now we must show that XCQ~uQ$ implies Xc IDEF(n) for each n&#38;S. If XeQ1, then \nX is output from some node n ES and S dust be nontrivial. Since n is a son in DT of w, there is a w-avoiding \npath in F from an immediate successor of w to n . Also, since S is a nontrivial DSCCof Gw, there must \nbe a path in ~ from n to n. So by Lemma 3.1, there is a w-avoiding path in F from n to n. Thus, we can \nconstruct a w-avoiding path in F from an immediate successor of w to an immediate predecessor of n, and \nso Xc IDEF(n). On the other hand, if XSQ< then Xc DEF(H (m,w,S),m) uOUT(m) for some (m,n )&#38;&#38; \nand n ES. Since w dominates H (m,w,S), DEF(H (m,w,S),m) ~DEF(w,m). Also, since there is an edge (m,n \n)s A, DEF(w,m) uOUT(m)~DEF(w,n ). Finally, since n,n are both in S, IDEF(n)u DEF(w,m) aDEF(H (m,w,S) \n,m). Thus, DEF(w,n) =DEF(w,n ~, and we conclude that X&#38; IDEF(n). 0 m \\ \\ /\\ I+(m,w)=ti (m,w, ) 00 \nw= H [KI!w, ) Case (2) .--+ is an edge of GW m is an edge of DT ~ 2 . Cases (1) and (2) of the definition \nof Hr. Now we use the techniques of Tarjan [T31 to implement our algorithm based on Theorem 3.1. We construct \na forest of labeled trees, with node set N. Each edge (n,m) has a label VAL(n,m) containing a set of \nprcgram variables (in our implementation, the set will be represented b a bit vector). Ini\u00ad tially, there \nis a forest of N trees, each con\u00adsisting of a single node. !Ie shall require three 11 types of instructions: \n1) FIND(n) gives the root of the tree currently cont~ining node n. 2) EVAL(n) givesiu2VAL(ni,Ri+1) where \n(r=nl,nz, . . ..nk=n) is the unique path to n from the current root r of the tree containing n. 3) LINK(m,n,z) \ncombines the trees rooted at n and m by adding edge (n,m), so n is made the father of m, and sets VAL(n,m) \nto z. Tarjan [T2] has shown that a certain algorithm for processing a sequence of r FIND and LINK in\u00adstructions \ncosts O((lNI+r)u(\\Nl+r)): elementary opera\u00adtions. This algorithm involves path compression on balanced \ntrees and is frequently used in the imple\u00admentation of UNION-FIND disjoint set operations. Also, Tarjan \n[T31 gives an almost linear time algo\u00adrithm (again utilizing path compression) for pro\u00adcessing a sequence \nof FIND, LINK, and EVAL instruc\u00adtions, given that the sequence is known beforehand, except for the values \nwhich are to label the edges in the LINK operations. The following algorithm for computing IDEF uses, \nlike the algorithm of [T4], a preprocessing stage that executes all FIND and LINK instructions but not \nEVAL instructions; this allows us in the second pass to efficiently process the EVAL as well as the FIND \nand LINK instructions. Algorithm A INPUT Program flow graph F=(N,A,s) and OUT. OUTPUT IDEF. !Y3.!!l declare \nIDEF: sequence of integers oflengthl Nl; Compute the dominator tree LIT of F; Number the nodes in N by \na postordering of DT; Scan the below so as to determine the sequence of EVAL, FIND, and the first two \narguments of the LINK instructions; for w:= 1to INI do ;y :=% :=the empty set { }; Ll~ f~rall (m,n)e \nAsuch that IDOM(n)=wandm#wdo beai n -add (m,n) to ; add (FIND(m),n to Ew; comment FIND(m) =H(m,w); end; \nL2 : Let G be the condensation of Gw=(~DOM-l[wl,Ew) ; L3 : ~each strongly connected component s of Gw \nin topological order of G&#38; ~ 7 M comment FIND(m) =H (m,w,S); Q$ := the empty set { }; comment set \nQ$ to Q ; if S is nontrivial i o for all ncS do O< := Ocv O~(n): commefit addJQ to QS~ forallne S $o \n for_all (m,ii_/l_cAW do begin L4: LINK(n,w,QS); comment apply Theorem 3.1; IDEF(n) := Q<; end; end; \n~, end; . Theorem 3.2: Algorithm A correctly computes IDEF. Proof: (Sketch. ) By induction in postordering \nof DT. Initially, each node nsN is contained in a trivial tree with root n and EVAL(n) gives the empty \nset { }. Suppose, on entering the main loop at LO on the with iteration, for any node m dominated byw \n1) FIND(m) =H(m,w), 2) EVAL(m) =DEF(H(m,w),m). We require a second induction, this one on the computed \ntopological ordering of . We assume that just before processing the strong 7 y connected region S in \n~, for each m dominated by w l )FIND(m) =H (m,w,S) 2 )EVAL(m)=DEF(H (m,w,S),m). By the primary induction \nhypothesis, (l ) and (2 ) clearly hold for the first DSCCof Gw.in the topo\u00adlogical ordering. We first \nset Qs to Q~ = { } if S is trivial = USOUT(n) if S is nontrivial and then add to Os the set W (EVAL(m)v \nOUT(m)) nES (m,n)sAw = :S(DEF(H (m,w,S),m)U OUT(m)) (m,n)EAw = Q;. Hence by Theorem 3.1, for each ncN, \nIDEF(n) is corvectly set to QS=Q$uQ~. Let S be the DSCC immediately following S in the topological ordering. \nAfter executing LINK(n,w,QS) at L4, for each node m dominated by w such that H(m,w) :S, FIND(m) now gives \nw= H (m,w,S) and EVAL(m) now gives DEF(H(m,w),m)v Qs = DEF(w,m) = DEF(H (m,w,S ),m) thus completing the \nsecond induction proof. Fur\u00adthermore, just before visiting node W, we have visited all the elements of \nIDOM-l[WI, and so for each m properly dominated by w 1) FIND(m) =w=H(m,w), 2) EVAL(m) =DEF(w,m) =DEF(H(m,w),m) \nthus completing the first induction proof. I Theorem 3.3: Algorithm A costs an almost linear number of \nbit vector operations. Proof: The dominator tree may be constructed in almost linear time by an algorithm \ndue to Tarjan [T31. Now consider the w th iteration of the main loop. Let rw= lIDOM-l[Wl\\+\\A 1. Step \nL1 clearly costs O(r ) elementary and F~ND operations. Step L2 costs ~(rw) elementary steps to discover \nthe strongly connected components of Gw using an algo\u00adrithm due to Tarjan [Tl] plus time linear in rw \nto condense each strongly connected component of Gw. Finally, at Step L3, we require O(rw) elementary \nsteps to topologically sort the condensed, acyclic digraph G~ by an algorithm due to Knuth [Knl, PIUS \nO(rw) bit vector, EVAL, and LINK operations in the loop at L3. The total time cost of this execution \nof the main loop is this O(r ) bit vector, EVAL, LINK, and FIND operations. ~ut 2 IAl+l ~-Nrw. Hence, \nthe preliminary scan of Algorithm A requires O(IAI) LINK and FIND operations implementable in time almost \nlinear in a by the method analyzed in [T2]. With the symbolic sequence of EVAL, LINK, and FIND operations \nnow determined, the second (primary) execution of Algorithm A requires O(lAla(lAl)) bit vector operations \nby the method of [T41. o 4. Computing the Weak Environment We now present an algorithm for efficiently \ncomputing the weak environment W. For each ncN, let IN(n) be the set of program variables input at n. \nFor the moment, it is useful to represent W by the set of partial functions {Wnln &#38;N} such that for \neach ncN, Wn(X)=W(X+n) for all XSX. For each ncN-{s} we have Wn(X)=n if Xc IDEF(n) =WM(X) if X41DEF(n) \nand m is the immediate dominator of n. We shall process the nodes in N in preorder (from root to leaves) \nof the dominator tree. Note that for each nEN-{s}, we must store Wm , Wm~,... ,Wmk where (s=mo,ml,. . \n. ,mk=n) i~ the chain of nodes on the dominator tree from the start node s to n. To efficiently maintain \nthese multiple environments, we keep an array of stacks WS such that just before processing node n, the \ntop element of WS(X) is Wn(X) for all Xs IN(n). A prepass is required to insure that elements are not \nredundantly pushed onto these stacks. The tot-l cost of this data structure for maintaining multiple \nenvironments is O(j) extended bit vector operations. To assure that WS is not modified needlessly, we \ncompute R(n) =those program variables X such that XW is an input variable for some node m properly dominated \nby n and such that X is defini\u00adtion-free from n to m. Intuitively, R(n) is a set of program variables \nwhose value is constant on exit to n to some node properly dominated by n. We compute R by a swift postorder \nwalk of the dominator tree DT using the rule: Lemma 4.1: R(n)= mcIDOM-~(n~((lN(m) R(m))-IDEF(m)). The \nfollowing lemma shows that to correctly maintain WS, we need add node n to the stack WS(X) just in case \nXc R(n)n IDEF(n). Lemma 4,2: There exists some m such that W(Xm)=n and X is input at node m iff XeR(n)61DEF(n). \nProof: By definition of R, if X&#38;R(n) then there -s some node mcN properly dominated by n, X is input \nat node m, and furthermore, X is definition-free from n to m. Suppose W(X+m)=n and X is input at node \nm. Then clearly X is definition-free from n to m so XeR(n). But suppose X41DEF(n). Then W(X~) properly \ndominates n, which contradicts our assumption that W(X~)=n. Hence, xc R(n)SIDEF(n). u The usual stack \noperations will be required: 1) TOP(S) gives the top element of stack S, 2) PUSH(S,Z) installs z as \nthe top element of stack S, 3) POP(S) deletes the top element of S. Algorithm B INPUT Program flow graph \nF=(N,A,s), IN, and OUT. OUTPUTthe weak environment W.  !Ew. Compute IDEF by Algorithm A (as a side effect, \nthe dominator tree DT is constructed); declare WS := a vector of stacks length Izl; procedure WEAKVAL(n): \n Llw all Xe IN(n) do W(X+n) := TOP(WS(X)); M := R(n)~IDEF(nE L2: for all XsMdo PUSH(WS(X),n); L3: for \nall mEID~-l[nI do WEAKVAL(m); L4: fi all X~Mdo POP(W~X)); end WEAKVAL; L5: for all n in postorder of \nDT~ be in +)Rn :={}; for all mEIDtlM-l[n] do =(n) := R(n) w((R(myw IN(m))-IDEF(tn));  I?@ L6: mall program \nvariables Xc&#38; PUSti(WS(X),s); L7: WEAKVAL(S); end; Theorem 4.1: Algorithm B correctly computes the \nweak environment. Proof: It is sufficient to show that on each exe\u00adcution of WEAKVAL(n) at label Ll: \n(*) W(X+n)=TOP(WS(X)) for all Xs IN(n). This clearly holds on the execution of WEAKVAL(s) at L7, since \nat label L6 all program variables X have the top of WS(X) set to s. Suppose that (*) holds for a fixed \nn&#38;N. Ob\u00adserve that all nodes pushed in the stacks at L2 are popped out of the stacks at L4. With \nthis ob\u00adservation, we may easily show by a separate induc\u00adtion that the state of WS on exit of any call \nto WEAKVALis just as it was on entrance to the call. The state of WS on entrance to WEAKVAL(m) is the \nsame for all mcIDOM-l[nl, Hence, by Lemma 4.2, the claim (*) holds for m, completing our induction proof. \n= We shall assume that a single bit vector of length ]zI may be stored in a constant number of words, \nand we have the usual logical and arith\u00admetic operations on bit vectors, as well as an operation which \nrotates the bit vector to the left up to the first nonzero element. This operation is generally used \nfor normalization of floating point numbers; here it allows us to determine the position of the first \nnonzero element of the bit vector in a constant number of such bit vector operations. Theorem 4.2: Algorithm \ncosts O(L+\\Al~(lAl)) bit vector operations. Proof: Each execution of WEAKVAL(n) requires w-;~l]!l 1!jn&#38;jD~(~~!J \n~j~tvector oPera- INI <n~NIIDOM- [nll and ,P,~ z /R(n)~IDEF(n)l ncN  and so the total cost of all executions \nof WEAKVAL is O(L+IAI) bit vector operations. By Theorem 3.3, the computation of IDEF by Algorithm A \ncosts O(lAla(lAl)) bit vector operations. Hence, the total cost of Algorithm B in bit vector operations \nis O(L+lAla(lAl)). = 5. Conclusion: Computing Approximate Birthpoints and the Simple Cover Given the \nweak environment constructed by Algorithm B, we can now easily compute approximate birthpoints and construct \nthe simple cover. Recall that a~is an acyclic, labeled di\u00adgraph. Here we assume that the leaves are labeled \nwith either constant signs or input variables. The interior nodes of a dag are labeled with k-adic function \nsigns. For each nsN, the set of text expressions located at n are represented by the dag D(n). A dag \nis minimal if it has no redundant sub\u00addag and if no proper subdag may be replaced with an equivalent \nconstant sign. Note that nodes of the dag D(n) represents text expressions whereas the nodes of the control \nflow graph F represent blocks of assignment state\u00adments. Here we wish to construct the function BIRTHPT, \nwhich as defined in Section 1 maps from text expressions to their approximate birthpoints in N. Again, \nfor each nsN, we process the nodes of D(n) in topological order, from leaves to roots. Let v to a node \nin D(n). If v is a leaf labeled with a constant then set BIRTHPT(v) to the start node s. If v is a leaf \nlabeled with an input variable of form X+n then set BIRTHPT(v) to n. Recursively, if v is an interior \nnode with every son u previously visited, set BIRTHPT(v) to the latest BIRTHPT(u) (relative to the dominator \nordering, with the start node s first) for any such son u. We use a large, global dag to represent the \nsimple cover. This dag is constructed as follows: 1) First, combine the dags of all the nodes in N. Associate \nthe singleton set {v} with each node v in the resulting dag. 2) Next, compute by Algorithm B the weak \nenvironment W. For each nEN and input variable X+n such that m= W(X+n) properly dominates n, collapse \nthe node corresponding to % X+n into the node containing X , the output expression for X at m. 3) Finally, \nminimize the resulting dag. The above construction takes time O(!.+IAI ), except for the construction \nof the weak environment which by Theorem 4.2 takes O(Q+IAIIX(IAI)) bit vector operations. Hence our method \nfor construc\u00adtion of the simple cover requires O(f.+/Al~(lA] )) bit vector operations.  (s ( 23 ! \\, \n, .. \u00ad . -. D(n2) ~JWIS 5 The dags of the program in Figure 1. J. ACM, 23, 1 (Jan. 1976), pp. 158-171 \n ~ V,={V,} [Kil Kildall, G.A., A unified approach to globs program optimization, Proc. ACM Symp. on Principles \nof Programming Languages / Boston, MA (Oct. 1973), pp. 194-206. Czz9 [Knl Knuth, D.E., The art of computer \nprogram\u00ad % { 6} ming, Vol. 1: Fundamental Algorithms, \\ Addison-Wesley, Reading, MA (1968). [Rl] Reif, \nJ.H. and Lewis, H.R., Symbolic evaluation and the global value graph, 4th ACM Symp. on Principles of \nProgram\u00adming Languages (Jan. 1977). [R21 Reif, J.H., Code motion, Conf. on Theoretical Computer Science, \nUniversity of Water\u00ad100, Ontario, Canada (1977). [R31 Reif, J.H., Combinatorial aspects of symbol\u00adic \nprogram analysis, Ph.D. Thesis, Harvard University, Division of Engi\u00adneering and Applied Physics (1977). \n[SC] Schwartz, J.T., Optimization of very high V, { ,, 3, 8} 4= ~4 } level languages --value transmission \nand its corollaries, Computer Languages, 1, 2 (1975), pp. 161-194. [Tl] Tarjan, R.E., Depth-first search \nand linear F&#38;YE? !! Dag representation of the simple cover. graph algorithms, SIAM J. Computing, \n1, 2 (June 1972), pp. 146-160. References [T21 Tarjan, R., Efficiency of a good but not linear set union \nalgorithm, JACM, 22 [AU] Aho, A.V. and Unman, J.D., Introduction to (April 1975), pp. 215-225. Compiler \nDesign, to appear. [T3] Tarjan, R., Applications of path compression [El Earnest, C., Some topics in \ncode optimiza-on balanced trees, Stanford Computer tion, JACM, 21, 1, (Jan. 1974), pp. 76-Science Dept., \nTechnical Report 512 102. (August 1975). [FKUI Fong, E.A., Kam, J.B., and Unman, J.D., [T41 Tarpan, R., \nSolving path problems on Application of lattice algebra to loop directed graphs, Stanford Computer optimization, \nConf. Record of the 2nd Science Dept., Technical Report 528 ACM Symp. on Principles of Programming (Oct. \n1975). Languages (Jan. 1975), pp. 1-9. [G] Geschke, C.M., Global program optimization, Carnegie-Mellon \nUniversity, Ph.d. Thesis, Dept. of Computer Science (Oct. 1972). [GWI Graham, S., and Wegman, M., A fast \nand usually linear algorithm for global flow analysis, JACM, 23, No. 1 (Jan. 1976), pp. 172-202. [HUII \nHecht, M.S. and Unman, J.D., Flow graph reducibility, SIAM J. Computing, 1, No. 2 (June 1972), pp. 188-202. \n[HU21 Hecht, M.S. and Unman, J.D., Analysis of a simple algorithm for global flow problems, SIAM J. of \nComputing, 4, 4 (Dec. 1975), pp. 519-532. [KU1] Kam, J.B. and Unman, J.D., Global data flow problems \nand iterative algorithms,  \n\t\t\t", "proc_id": "512760", "abstract": "A global flow model is assumed; as usual, the flow of control is represented by a digraph called the control flow graph. The objective of our program analysis is the construction of a mapping (a cover) from program text expressions to symbolic expressions for their value holding over all executions of the program. The particular cover constructed by our methods is in general weaker than the covers obtainable by the methods of [Ki, FKU, R1], but our method has the advantage of being very efficient; requiring O(&ell; + a&alpha;(a)) extended bit vector operations (a logical operation or a shift to the first nonzero bit) on all control flow graphs (whether reducible or not), where a is the number of edges of the control flow graph, &ell; is the length of the text of the program, and &alpha; is Tarjan's function (an extremely slowly growing function).", "authors": [{"name": "John H. Reif", "author_profile_id": "81100567232", "affiliation": "The University of Rochester", "person_id": "PP14196622", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/512760.512769", "year": "1978", "article_id": "512769", "conference": "POPL", "title": "Symbolic program analysis in almost linear time", "url": "http://dl.acm.org/citation.cfm?id=512769"}