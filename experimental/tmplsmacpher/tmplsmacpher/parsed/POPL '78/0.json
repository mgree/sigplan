{"article_publication_date": "01-01-1978", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee.&#38;#169; \n1978 ACM 0-12345-678-9 $5.00 Conference Record of the Fifth AXUNJal ACM symposium on Principles of Programming \nLanguages COMPILATION AND DELAYED EVALUATION IN APL by Leo J. Guibas and Douglas K. Wyatt Xerox Palo \nAlto Eesearck Center 3333 Coyote Hill Road Palo Alto, Cal. 94304 0. Introduction Most existing APL implementations \nare interpretive in nature, that is, each time an APL statement is encountered it is executed by a body \nof code that is perfectly general, i.e. capable of evaluating any APL expression, and is in no way tailored \nto the statement on hand. This costly generality is said to be justified because APL variables are typeless \nand thus can vary arbitrarily in type, shape, and size during the execution of a program. What this argument \noverlooks is that the operational semantics of an APL statement are not modified by the varying storage \nrequirements of its variables. The first proposal fox a non fully interpretive implementation was the \nthesis of P. Abrams [II, in which a high level interpreter can defer performing certain operations by \ncompiling code which a low level interpreter must later be called upon to execute. The benefit thus gained \nis that intelligence gathered from a wider context can be brought to bear on the evaluation of a subexpression. \nThus on evaluating (.4+B)[11, only the addition .4[11+B[11 will be performed. More recently, A. Perlis \nand several of his students at Yale [9,10] have presented a scheme by which a full-fledged APL compiler \ncan be written. The compiled code generated can then be very efficiently executed on a specialized hardware \nprocessor. A similar scheme is used in the newly released HP/3000 APL [12]. This paper builds on and \nextends the above ideasin several directions. We start by studying in some depth the two key notions \nall this work has in common, namely compilation and delayed evaluation in the context of APL. By delayed \nevaluation we mean the strategy of deferring the computation of intermediate results until the moment \nthey are needed. Thus large intermediate expressions are not built in storage; instead their elements \nare streamed in time. Delayed evaluation for APL was probably first proposed by Barton (see [8]). Many \nAPL operators do not correspond to any real data operations. Instead their effect is to rename the elements \nof the array they act upon. A wide class of such operators, which we will call the grid selectors, can \nbe handled by essentially pushing them down the expression tree and incorporating their effect into the \nleaf accessors. Semantically this is equivalent to the drag-rdotig transformations described by Abrams. \nPerforming this optimization will be shown to be an integral part of delayed evaluation. In order to \nfocus our attention on the above issues, we make a number of simplifying assumptions. We confine our \nattention to code compilation for single APL expressions, such as might occur in an APL Calculator , \nwhere user defined functions are not allowed. Of course we will be critically concerned with the re-usability \nof the compiled code for future evaluations. We also ignore the distinctions among the various APL primitive \ntypes and assume that all our arrays are of one uniform numeric type. We have studied the situation without \nthese simplifying assumptions, but plan to report on this elsewhere. The following is a list of the main \ncontributions of this paper. o We present an algorithm for incorporating the selector operators into \nthe accessors for the leaves of the expression tree. The algorithm runs in time proportional to the size \nof the tree, as opposed to its path length (which is the case for the algorithms of [10] and [12]). Although \narbitrary reshapes cannot be handled by the above algorithm, an especially important case can: that of \na conforming reshape. The reshape APB is called conforming if PB is a suffix of A. o By using conforming \nreshapes we can eliminate inner and outer products from the expression tree and replace them with scalar \noperators and reductions along the last dimension. We do this by introducing appropriate selectors on \nthe product arguments, then eventually absorbing these selectors into the leaf accessors. The same mechanism \nhandles scalar extension, the convention of making scalar operands of scalar operators conform to arbitrary \narrays. o Once products, scalar extensions, and selectors have been eliminated, what is left is an expression \ntree consisting entirely of scalar operators and reductions along the last dimension. As a consequence, \nduring execution, the dimension currently being worked on obeys a strict stack-like discipline. This \nimplies that we can generate extremely efficient code that is independent of the ranks of the arguments. \n Several APL operators use the elements of their operands several times. A pure delayed evaluation strategy \nwould require multiple reevaluations. o We introduce a general buffering mechanism, called slicing, which \nallows portions of a subexpression that will be repeatedly needed to be saved, to avoid future recomputation. \nSlicing is well integrated with the evaluation on demand mechanism. For example, when operators that \nbreak the streaming are encountered, slicing is used to determine the minimum size buffer required between \nthe order in which a subexpression can deliver its result, and the order in which the full expression \nneeds it. o The compiled code is very efficient. A minimal number of loop variables is maintained and \naccessors are shared among as many expression atoms as possible. Finally, the code generated is well \nsuited for execution by an ordinary minicomputer, such as a PDP-11, or a Data General Nova. We have implemented \nthis compiler on the Alto computer at Xerox PARC. The plan of the paper is this: We start with a general \ndiscussion of compilation and delayed evaluation. Then we motivate the structures and algorithms we need \nto introduce by showing how to handle a wider and wider class of the primitive APL operators. We discuss \nvarious ways of tailoring an evaluator for a particular expression. Some of this tailoring is possible \nbased only on the expression itself, while other optimizations require knowledge of the (sizes of) the \natom bindings in the expression. The reader should always be alert to the kind of knowledge being used, \nfor this affects the validity of the compiled code across reexecutions of a statement. 1. The Intentional \nRepresentation of Expressions APL, Iike many other very high level languages, is characterized by its \nability to manipulate large objects. Thus APL deals with multi-dimensional arrays, SETL [11] deaIs with \nsets, LISP deals with lists, etc. The word large refers to a comparison between the size of the primitive \nobjects of the language and the complexity of its primitive operations on them, contrasted with the size \nand complexity of the objects manipulated inside the processor of a present-day computer. Thus an array \ntypically occupies several storage locations and the evaluation of an array sum A+B in APL requires the \nexecution of a number of machine instructions proportional to the size of the arrays A or B. Note that \nthe semantics of APL, although they completely determine the meaning of an expression in the language, \ndo not fully specify how that expression is to be computed. For example, the semantics of the language \nleave us free, in evaluating A+B, toadd the corresponding elements of A and 1? in whichever order we \nplease. Thus we can regard APL expressions more as a specification of the result we desire to compute, \nrather than as a detailed azgorithoz for evaluation on a serial computer. For the majority of APL operators \nthe cleavage between what the semantics of the language require and what the evaluator is free to choose \nfalls along the following lines. The semantics specify what data operations are to be performed, i.e. \nhow each element of the result array depends on some of the elements of the operand arrays. The order \nin which the result elements are to be evaluated, however, that is the control of the computation, is \nusually left unspecified. We can often use this freedom in sequencing to advantage, by matching the order \nin which the result may be required (e.g. for display, according to standard APL conventions) with the \norders in which the operands most conveniently be traversed. In the traditi~n~~ APL evaluators an operation \nis executed only after its operands have been fully evaluated. This has the advantage (assuming the usual \nconvention of storing arrays in row major format) that at any moment there is a very efficient way of \ntraversing the arguments of an operation (i.e. the row major order). However, this is not the only possibility. \nSuppose, for example, that we wish to display the result of I$A+B, where A and B are evaluated matrices. \nThen, if we traverse A and B in column major order, we can display the result without ever having to \ngenerate the intermediate array A+B. At the expense of slightly more cumbersome traversal, we have avoided \ngenerating a possibly large intermediate array. Furthermore we can optimize our freedom in sequencing \nover the entire expression we wish to evaluate. There is a simple way of sequencing through A++/Bo. xC \nso that elements of the result can begin to be displayed before any of the implied subexpressions have \nbeen fully evaluated. Thus we come to the other extreme, that of evaluation on demand, or delayed evaluation. \nSuch evaluation strategies have been discussed previously in the context of very high level languages. \nSee, for example, [3,4]. In the above description of equivalent evaluation techniques we have not dealt \nwith the issue of side effects. The equivalence is valid only as long as al! operations return proper \nvalues. This unfortunately is not always so in APL, because of undefined forms such as 1+0, or 1*.5. \nThe traditional evaluation strategy would report an error in computing 2+6 6 6+2 1 0, because of the \ndivision by O. However, delayed evaluation would return 3 6, since the division by O was never required, \nso it never occurred. This raises numerous issues which we will not discuss in this paper. 2. The Stylized \nAccess Modes One way to accomplish evaluation on demand is to regard each APL expression as an object \ncapable of responding to certain questions. Some of the questions we may want to ask are 1) how many \ndimensions do you have? 2) what is your I-th dimension? 3) what is your [l; J; . . . ;K1-th element? \nThis brings us to an object oriented view of expressions analogous to that Of SIMULA [2] or SMALLTALK \n[5] classes, ALPHARD [13] forms, or CLU [7] clusters. Naturally we can arrange that the ability to respond \nto the above messages is nicely obtained through recursion. Assuming that fully evaluated arrays (such \nas the atoms of an expression) can respond in the obvious way, our task is simply to associate with each \nAPL operator procedures for responding to the above questions, given that the operator can ask these \nsame questions of its operands. For example, in 2+(A+B) the subexpression (A+B) can respond to the request \nfor an element by having + issue requests for the appropriate elements to A and 1?, and then use its \nlocal expertise to perform the addition. In the above scheme we have essentially regarded each APL expression \nas a random access storage device. It is clear that keeping each subexpression in a state of readiness \nto provide an arbitrary element will involve very substantial overhead. Furthermore, this ability to \naccess elements in random order is not frequently needed in the evaluation of APL expressions. Much more \ncommon is the situation in which we need all elements of an expression, one at a time, in the order in \nwhich they would occur if the expression had been evaluated and the corresponding array stored in row \nmajor form (rauel order). We will name this important way of accessing an expression rauel mode. In this \nmode we wish to regard an expression as a coroutine, which upon successive calls will deliver successive \nelements of the array it represents. By restricting ourselves to highly stylized access modes, such as \nravel access, we have a much better prospect of an efficient implementation. In order to understand what \naccess modes are useful, we have to understand in detail how the various APL operators use the elements \nof their operands to produce the elements of the result. For example, for the compression operator / \nit will certainly be advantageous to have its argument be able to respond to the message skip as well \nas to the message next . The argument may in fact generate the element being skipped and just throw it \naway, or it may be able to propagate the skip message further down the expression to attain a real saving \nin the computation. As another example, we may wish to break the message next of ravel mode into two \ndistinct messeges: advance and fetch . The reason for this is that several evaluated atoms (e.g. in A+BxC) \nmay be able to share the same accessor (further explained later) and thus we can get by with a single \nadvance message for all three atoms. The perspective offered by the above discussion is that of associating \nwith each node in the expression tree an access mode. The access modes are determined from the top down. \nAn operator is told that the subtree it heads needs to be accessed in a certain way. Then by knowing \nhow the elements of its result. depend on the operand elements, it decides in which modes its arguments \nmust be accessed. Thus access modes correspond to inherited attributes, in the sense of Knuth [6]. 3. \nThe Compilation of Streams In this section we limit ourselves to APL expressions containing only scalar \noperators. AS the reader may suspect, handling such expressions is relatively trivial. However, confinement \nto a domain where the task is well understood will allow us to focus our attention on setting the context \nfor the following developments. We will further limit the present discussion by disallowing as nowconforrnable \nscalar expressions where all atoms do not have identical shapes. We will deal with the very important \nspecial caae of scalar atoms (which conform to any array according to the APL rules) in section 8. Consider \nhow to evaluate A+BxC. A clean way of obtaining the delayed evaluation effect is by implementing each \nscalar operator such as + or x as a reentrant coroutine. A different instance of the coroutine is used \nfor each occurrence of the operator in the expression. Naturally all subexpressions (including the atoms) \nare accessed in ravel mode. Unfortunately, interpreting via reentrant coroutines is attractive only as \nlong as the cost of a coroutine call and return is small compared to the processing performed between \nsuccessive invocations of the coroutine. Assuming costs for machine operations such aa are common today, \nthen in A+BxC for example, each element of the result generated requires one <ddition, one multiplication, \nand fourteen coroutine control transfer instructions. There are other . hidden costs as well. Each instance \nof the atom accessing coroutine, if implemented in the obvious way, will be maintaining its own local \ncopy of a counter and an offset into the atom array, when clearly these variables can be shared (and \nthus updated only once). We have here the classical argument for compilation. Before we can discuss compilation \nin detail, however, we need to say a few more words about the machine model we have in mind. We assume \na stack machine with all APL scalar operators as primitives. In addition, the execution environment contains \ncertain data structures specifying how arrays are to be traversed, called accessors. The notion of an \naccessor was first introduced by Perlis in [10] (where it is called a ladder). A detailed discussion \nof these structures will be given in the next section. In the context of the current section an accessor \ncan be thought of simply as the index of the array element we are currently accessing. Thus in the evaluation \nof A+Bx C all three atoms can clearly share the same accessor. Our instruction repertoire will include \nthe instructions advance(l), which advances accessor 1 to the next array position, and fetch(l,a), which \npushes on the stack the element referenced by accessor 1 in atom a. It will become clear in the next \nsection that the above two operations can be implemented with a few machine instructions on most computers. \nCompilation is now straightforward. Assume that we have formed the expression tree during the lexical \nanalysis of the expression. In a first pass, the dimensions pass, the conformability of the atoms is \nchecked (and storage for the result can be allocated if we are executing an assignment, e.g. Z+-A+BXC). \nNext, in the push pass, an aceessor is created to be shared by all atoms, and initialized to point to \nthe first element. In the code generation pass a traversal of the expression tree in endorder suffices \nto generate a codestream performing the scalar computations. For the example Z+A+BXC the code would be: \nthe next iteration. Note that this code is correct irrespective of the dimensionality and size of the \natoms (albeit not of their type). This information has been confined within 1. Note also that we have \nobtained the effect of Abrams beating optimization with no extra work. Finally the above code needs to \nbe encapsulated by an appropriate 100P, and we are ready to execute. fetch(l,B) fetch(l,c) multiply fetch(~,A) \nadd store(l, Z) advance. In the above 1 denotes the shared accessor of all atoms; the last instruction \nadvances this accessor in preparation for 4. Operators that Break the Streaming In the previous section \nwe saw how simple it is to stream the evaluation of an expression composed solely of scalar operators. \nWe now take a brief look at the other end of the spectrum, namely operators that cause any reasonable \nstreaming mechanism to break down. Such operators include @ (rotation), A , Q , and arbitrary subscripting \n[1. The evaluation of these operators requires either a random access mode, or partial evaluation in \ntemporary storage. There other cases where evaluation is necessary. For example the argument of (monadic) \nI and the left argument of / (compression) must be fully evaluated before even the conformability can \nbe checked. Finally, subevaluations may be useful even when they are not necessary. Such storage time \ntrade offs will be taken up in section 8. Thus the compiler generates a number of code streams corresponding \nto broken subexpressions. At run time these codestreams are invoked to replace a broken subexpression \nby (possibly portions of) an evaluated atom. 5. The Universal Selector In this section we discuss compilation \nof expressions involving a subset of the selection operators of APL, as well as scalar operators. The \nselection operators we will handle are + , + , @ , $ (reversal), and [El ;E2; . . . ;ENI (subscripting, \nwhere the E1 are arithmetic progreasions, i.e., expressions equivalent to A+Bx 1C for some integer scalars \nA, B, and C). We will name the above selection operators the grid selectors, for reasons that will become \nclear shortly. The grid selectors operate on an array argument (the right argument, except for subscripting), \nby extracting and/or renaming a portion of it. This is done according to a second argument, the control \nargument (one may consider monadic b and O to have default control arguments). The control argument must \nalways be fully evaluated in order to check conformability with operations higher up in the tree, Thus \nit will be convenient to think of the control argument as being part of the selector, and not an object \nto which delayed evaluation is applicable. We can think of the elements of an array A as occupying lattice \npoints in a space of PPA dimensions, within a bounding box of size PA. Note that each of the grid selectors, \nwhen applied to A, results in an object that occtipies a sublattice of the original lattice. In other \nwords, moving across any coordinate of the result array can be viewed as moving along some set of coordinates \nof A, using equal size steps. Let us invent a generalized selector, called the uniuersal selector, that \ncan represent any such selection operation. Then multiple selectors applied to the same array can be \ncomposed into a single instance of the universal selector. Thus, if we start with an APL mzpre~sion involving \nonly scalar operators and grid selectors, we can think of a (null) universal selector starting at each \natom and traversing the path to the root of the expression tree. In this process the universal selector \nabsorbs into itself any grid selectors it encounters. We will speak of the stage when this processing \nis done as the push pass. When this process is complete, our expression will have only scalar operators \nleft. An instance of the universal selector will be associated with each leaf, indicating the composition \nof all selectors that must be applied to that evaluated array. Although the composition of selectors \ncan be most naturally thought of as happening from the bottom up, we will in fact carry out this process \ntop down. The reasons for this are twofold. Firstly the cost of the push pass now becomes proportional \nto the size of the expression tree, as opposed to the path length of the tree. Secondly we will be able \nto leave with each node an instance of the universal selector which represents the composition of all \nselectors above that node. As we will see in section 8, this will provide us with essential information \nneeded in storage/time trade off decisions at that node. The end result is quite similar to the normal \nform for select expressions first described by Abrams. However, the implementation of the transformations \nis much less cumbersome than Abrams solution. The data structure that represents a universal selector \nis called a stepper. A stepper U will be associated with every node of the expression tree. U will represent \nthe state of the universal selector before the selector represented by the node (if any) has been absorbed. \nA node N incorporates itself into the stepper if it is a selector node, and passes this stepper on to \nits offspring. The newly formed stepper U is characterized by n, the rank of N s offspring, and four \narrays q, s, d, and 1, defined over the interval [l,n]. These arrays encode the way in which elements \nof the current node partake in the formation of the final result: q[i] is an integer in [ljr] and denotes \nwhich coordinate of the result array the i-th coordinate of the current node corresponds to; the set \nof all q[i] with q[i] = j is the set of coordinates of the current node which have been collapsed into \nthe j-th coordinate of the result (recall that a transpose or subscripting may reduce the number of dimensions) \ns[i] denotes the index, along the i-th coordinate, of the element of the current node which contributes \nto tile first element of the result (e.g. the [O;O; ...; 01 element of the result in O-origin) d[i] indicates \nby how much to move along the i-th coordinate of the current node in order to arrive at the next element \nof the result along coordinate g[i]; (it can be negative) l[i] indicates the size of the result along \nthe q[i]-th coordinate; note that l[i] = l[j] if q[i] =W Initially, for a null accessor at the root Rwe \nhave n = ppR, q[i] = i, s[i] = 0, d[i] = 1, and L[i] = Pi?[i I, for all i. Let us now see how to incorporate \nvarious grid selectors using the stepper structure. When stepper U is about to absorb selector S, which \nin turn is applied to a node N, we will let p denote ppN, and the array r denote PN. Primed quantities \nindicate the new valuea. 1. Monadic Transpose. A monadic transpose @ can be absorbed in U by the following \nsimple program: n en; FOR i IN [I,n ] DO BEGIN a [il + u[n+l-il: j[i] + Jn+l-ij\u00add [i] + d[n+l-i]; Z [i] \n+ l[n+l-i]; END; 2. Dyadic i%anspose. Let c[i], i in [ l,p], denote the control argument. The following \nprogram shows how to absorb this transpose into U: n + p; FOR i IN [l,n ] DO BEGIN q [i] e q[c[i]]; s \n[i] + s[c[i]]; d [i] + d[c[i]]; L [i] + l[c[i]]; END; 3. Take. As above, let c[i] denote the control \nargument. We have: n +n; FOR i IN [l,n ] DO BEGIN q [i] + g[i]; s [i] + IF c[i] < THEN s[i] + $i]*(r[i]+c[i]) \nELSE s[i]; 4. Reversal. Reversal along the k-th coordinate can be implemented as follows: n +n; FOR i \nIN [l,n ] DO BEGIN \u00ad q [i] 4-q[i]; s [i] + IF i . k THEN r[i]-s[i]+l ELSE s[i]; d [i] + IF i = k THEN \n-d[i] ELSE d[i]; Z [i] + l[i]; END; The above examples should be sufficient to illustrate how the stepper \nU can absorb the various grid selectors into itself. Once all the steppers have stopped propagating by \nreaching the leaves of the tree, the grid selectors can be completely removed from the expression. We \nknow from section 3 how to compile code for a tree of scalar operators, so the remaining issue is how \nto use the steppers to compile code for accessing the evaluated atoms of the expression. Note that each \nelement of an atom is used at most once in computing some element of the final result. For each atom \nA there is an associated stepper U. We will use U to compute a new data structure, called an accessor, \nwhich will allow us to step through the elements of A in the proper order. A itself is assumed to be \nstored in ravel order. The accessor T obtained from U consists of n, the current position into the (stored \nrepresentation of the) array A; a, the starting value of ~; and two arrays y[i] and il[i], defined for \ni in [1, max q[j]]. Intuitively, y[i] denotes the distance by which we have to increment T to obtain \nthe next element of A needed for computing the next element of the result along the i-th dimension. The \nrelated quantity ~[i] denotes the distance by which ~ has to be incremented to attain the same goal as \nabove, but now assuming that we have completely cycled through all dimensions higher than i in the result. \nMore formally, let the shape PA be [hl,kz,...,kn] and define h[i] = ki+lki+2 kn (h[n] = 1).Then for \n1< i < max q[j] we have y[i] = ~ d[j]h[j], and q[j]=i 4 ~[i] = W -~ y[jlh[jl, and i< j<n a= ~ S[jlh[j]. \nl~<n In order to understand the meaning of accessors, we now describe how they are used in the code compilation, \nObserve that, once the grid selectors have been removed from the expression tree, the shapes resulting \nfrom selector to its atom must be ldentlcal. (This follows from the requirement on the conformability \nof scalar operator arguments -again disallowing scalar extension. ) Let this common shape, which is also \nthe shape of the expression result, be [cl,c2,...,cm]. This shape is used to form a data structure globaltothe \nexpression, called the coordinate ladder. The coordinate ladder is described by twc arrays, count[i] \nand Lirnii[i], for i in [O,m-1]. During execution, the array count[i] indicates the coordinates of the \nresult element currently being produced. The array lirnit[i] is initialized by timit[i] * Ci, i in [II,m-1], \nand is constant throughout execution. Also included is a global variable coord, indicating the coordinate \ncurrently being worked on. Using the coordinate ladder, an accessor T then implements the following operations: \napPIY~ng each UIliVerd init(Z ): T.v + T.a (using the PASCAI. notation for field extraction) fetch(T,A): \npush on the stack the contents of [(base address of A) + ~,m]; it may seem redundant to specify both \nT and /l -we are anticipating the sharing of accessors discussed below aduance( 1 ): T.w + T.v + T. S[coord] \nskip(T): T.TI + T.n + T.y[coord]; this operation arises in the implementation of compression and will \nnot be treated further in the current section We are now ready to describe the compiled code for our \nexpression. Let 2 1, T2, .... Ts denote the list of accessors generated during the elimination of grid \nselectors. The compiled code has the form: coord + -1; <Initializations>; init(T1); ..... init(T$); \nLoop: WHILE coord ~ last_coord DO BEGIN coord + coord+l; courzt[coordl + O; END: <code for scalar operations, \nas described in section 3>; ... ,. ..... Advance: advance; ..... advance; count[coord] + courzt[coord] \n+ 1; IF courzt[coord] < limit[coord] THEN GO TO LOOP ELSE IF coord = O THEN DONE ELSE BEGIN coord + coord-l; \nGO TO Advance; END; We will call the code following all the Advance instructions the Universal Looper. \nThere are two important optimization we perform on the above code. They are called Accessor Sharing and \nCoordinate Compression; we deal with them in turn. By Accessor Sharing we refer to the fact that the \nsame accessor can often be shared by several atoms in the expression. We can accomplish this sharing \nas follows. An atom A which is a descendant of some selector S in the tree will be called visible from \nS, if there is no other selector on the path from S to ,4. Add a dummy grid selector to the top of the \nexpression tree. An accessor is generated not by each leaf, but rather by each selector that has a non-empty \nset of atoms visible below it. When a stepper reaches this selector, it can be used to generate an accessor \nthat will be shared by the set of atoms in question. The next optimization, Coordinate Compression, is \nimportant because it frequently happens that the applied grid selectors affect only a few of the coordinates \nof the atoms involved. Thus the ravel order in which these atoms are stored in memory corresponds to \na large extent with the order in which they need to be accessed so as to produce the result. Specifically, \nif for some coordinate c it is true that all accessors generated have ti[c] = O (and y[c] is not needed, \ni.e. there is no compression / along that coordinate), then coordinate c can be merged into coordinate \nc+l. For practical APL expressions the above optimizations are very important. Consider A+BxC, for example. \nAll three leaves A, b , and C are visible from the dummy selector at the root, thus they can all share \nthe same accessor T. Coordinate compression will then collapse the ppA loops implicit in T and the coordinate \nladder into just one huge loop that goes around x/pA times. This code is certainly the best we can hope \nto generate for the above expression. And in general, these optimizations allow us to get by with the \nsmallest number of accessors and loops possible. Note that the push pass must happen every time the shapes \nof the expression atoms change. However, the compiled code previously generated will still be valid. \nThe same code can be reused with the newly generated accessors, as described above. 6. Reduction Reduction \nhas two novel aspects. Firstly it must generate its own looping code, which is not part of the universal \nlooper. Secondly it has a number of nasty special cases, which will be briefly mentioned at the end of \nthe section. What happens when a stepper goes through a reduction node in the expression tree? Assume \nthe reduction is along the h-th dimension of offspring node N. The newly formed stepper will have another \ndimension added to it. The semantics of APL require that this new dimension be traversed in the reverse \ndirection. An additional variable m, the depth of the coordinate ladder at the current point in the tree, \nmust also be maintained. (In the previous section m was constant; it was always the rank of the final \nresult). A stepper, in going through a reduction, in effect also ensures that the reduced coordinate \nh has become the last coordinate of the reduction s argument. Here is the reduction absorption code. \nn + n+l; m 4-m+l FOR i IN [l,k) DO BEGIN q [i] + q[i]; s [i] e s[t]; d [i] e d[i]; Z [i] + l[i]; END; \nCOMMENT add a new coordinate -recall that the semantics of APL require that it be traversed in the reverse \ndirection; q [k] + m ; s [k] + PN[kl -1; d [k] + -1; l [k] + pN[kl; A reduction node must compile a loop \nthat applies the FOR i IN [k.+ I,n ] DO BEGIN q [i] e q[i-l]; S [i] + Sri-l]; d [i] + d[i-l]; l [i] + \nl[i-l]; END; appropriate binary operation between all elements along the reduced coordinate. The length \nof the reduced coordinate is saved in the expression frame. At run-time the compiled code pushes that \nlength on the coordinate ladder and initiates a loop starting with the appropriate identity element and \nrepeatedly fetches, advances, and operates on the next element of the argument subexpression. When the \ncoordinate is exhausted the coordinate ladder is popped (i.e. coorrl is decremented), and the result \nreturned. The details of these operations are straightforward and will not be described. Note that the \ncoordinate ladder gets used as a stack. The compiled code always manipulates the global rung pointer \ncoord, and thus is never aware of the dimension number of the coordinate being worked on. As a consequence, \nthe compiled code remains valid for reexecution of the same expression, as long as the types of the expression \natoms do not change. The ranks and dimension vectors can change without invalidating the code. Figure \n6.1 illustrates the various transformations described in this and the previous section. The reader is \nadvised to study this example in detail. As a final note, boundary conditions for reduction give rise \nto many complications. Consider the expressions =/ I (+-+ I ) , =/ A (+ A ), =/ AA (*1), =/ AAA (++O). \nDue to lsck of space we do not discuss techniques for handling these complications, TOP c1 ~ q:lz NOTE: \nORIGIN= s: 00 d:ll P:23 1,23 23t m r?il u ,~o p:45 d:ll 1:23 q:zll *:  Iootq 022p:57w d:lll 1, 322 \n$ [11 j El s q:zll   ~: 052 CI, l-11 p:57 4 1,322./[21 q: 2131  S:057Zc.: 57811 d: l-1-11 ;;; ;~ \n1 ;;= . p:537s4   s: :55 i;?i ti@ Fig. 6.1. Propagation of steppers 7. A Specialized Reshape, with \nApplication to Inner and Outer Product, and Scalar Extension In this section we illustrate the power \nof the universal selector mechanism introduced in section 5. We show how this mechanism can handle a \nspecial case of dyadic reshape, which we will call conforming reshape. Using this as our tool we can \nthen transform expressions containing inner or outer products into equivalent expressions containing \nonly scalar operators, grid selectors, reductions, and conforming reshapes. These are expressions we \nalready know how to compile. The same can be done with the scalar extension problem we have postponed \nuntil this section, that is the problem of scalar operators with one scalar and one non-scalar argument. \nA dyadic reshape .4pB will be called conforming if pB is a suffix of A. (Equivalently, ( PB ) = ( -PPB \n) +/l). Note that if B is scalar, this is always the case. Such a reshape preserves the structure of \n~ it only adds dummy copies along the new dimensions. A conforming reshape can be incorporated into a \nstepper by marking the coordinates introduced by the reshape as dummy (setting their d s to o). It turns \nout that by introducing appropriate conforming reshapes and transposes on the arguments, we can transform \nan outer product into a scalar operator, and an inner product into a scalar operator followed by a reduction. \nHow this is done is in fact most succinctly expressed in APL itself. Let @ and @ be any dyadic scalar \noperators; ++ stands for equivalent to . Outer Producti Ao. @B 4. .@ ++ ((( PPB)Ol(PPB )+ PPA)Q((PB), \nPA) PA)@((PA), PB)PB Inner Producti AIB. @zB (note: -14p 4 = I+pl?) WA + (l+pB), p~ EA + (PpA)-1 VA \n+ ,pJ/A 2A + (XA$-I+VA), -l+I?A TA + jjA&#38;IJ/ApA WB + (-I+pA), pB KB +ppA VB + ~pWB ZB + (( KB+vB)/vB), \nvB[KB] TB + ZBQ,l/BpB A@.@B +-+ @/TAwTB We have broken the inner product transformation up into a series \nof subexpressions for the sake of clarity. The reader can verify that each argument of the product is \noperated on by a conforming reshape and (possibly) a transpose, Note that if we were thinking of evaluating \nan APL expression in the straightforward way, the above transformations would be extremely expensive, \nas we are in effect creating may copies of the arguments of each (inner or outer) product. Since the \nabove transformations show that these operators are redundant, one suspects that they were introduced \ninto the language in order to provide efficient implementations of certain common operations. With our \ndelayed evaluation strategy the multiple copies will of course never be generated and they introduce \nabsolutely no overhead at run-time. Scalar extension can be handled in an entirely analogous way. In \nthe conformability pass scalar operators with one scalar and one non-scalar argument can make a note \nof this fact. Later, during the push pass, these operators can just in effect introduce a conforming \nreshape on to the scalar argument that will make it conform to the non-scalar one. (Using the same principle \nof dummy expansions we can easily implement more general kinds of conformability than APL allows). \n 8. Slicing In this section we introduce a general technique for buffering portions of an array as its \nelements are computed, which we will call slicing. This technique is an integral part of our compilation \nwith delayed evaluation strategy. Slicing gets used to store subexpressions whose value will be required \nmany times, thus saving recomputation. It also gets used to moderate the effects of operators that break \nthe streaming. The results of such operators are often not needed in their entirety, but only certain \nslices . An appropriate buffering scheme ;etween the full expression and the subexpression headed by \nthe breaking operator can then save space. A k-slice of array (or subexpression) A is defined as ,4Cil \n;i2; . . . ink;;...;;], where il, i2, ....in.k are valid indices for array A, with n = PPA. In other \nwords, a k-slice is a k-dimensional array obtained from A by arbitrarily fixing a value for all but the \nlast k coordinates, then letting these k coordinates vary through all their allowed values. We will call \ninner coordinates higher. Note that for each k, as we traverse A in ravel order, we will generate a complete \nset of k-slices of A. Our buffering scheme will work by always computing and saving a slice of appropriate \nsize for a given subexpression. There are numerous situations in evaluating APL expressions in which \na subexpression of modest size should be saved in order to avoid wasted recomputation. Consider as examples \nA+ I1OOOO, where A is a very complex scalar, or Ao. xB, where again A is complex and B is large. Note \nthat we already have the tools to discover when these situations arise. In both of the above casea a \nconforming reshape was introduced during the processing of the expression. This conforming reshape leads \nto steppers with d s equal to O along certain coordinates (to be called the dummy coordinates), thus \nsignaling the re-use of certain elements. Such a conforming reshape indicates the need to save a slice \nof its selected result. By selected we mean that only that portion of the true slice need be generated \nwhich will eventually partake in the production of the final result. The slice size can be determined \nonce the conforming reshape has been absorbed into the stepper. Let s be the coordinate just lower than \nthe outermost dummy coordinate. (Take s = -1, if the outermost dummy coordinate is coordinate O). Storage \nwill be allocated for all non-dummy coordinates of the stepper which are higher that s. Coordinate s \nitself will be called the slicing coordinate. The slice naturally acts as a buffer between the full expression \nand the subexpression below the conforming reshape. The code for the subexpression is placed in a separate \ncodestream. The main and subexpression codestreams communicate data via the slice. Control is accomplished \nvia a consuming accessor (in the main code) and a producing accessor (in the subexpression code). The \nconsuming accessor is built from the stepper in the usual way, except that advancing along dimension \ns resets to the origin (and advancing along any dimension lower that s is a no-op). The stepper which \nthe subexpression receives has all dummy coordinates removed. This modified stepper is then propagated \ndown the subexpression in the usual way. Finally the producing accessor is built from a trivial stepper \nfor the subexpression s selected result, except again that advancing along dimension s resets to the \norigin. How does control pass back and forth between the two codestreams? Let us first note that each \ncodestream will be responsible for its own accessors. Yet we want all codestreams to share the global \ncoordinate ladder, for obvious efficiency reasons. It turns out that the following simple policy solves \nthe coordination problem. Every time a slice (producing or consuming) accessor is advanced, control passes \nto the partner codestream, if coord (the coordinate being advanced) is lower or equal to the slicing \ndimension. This elegant rule also subsumes initialization difficulties. At the beginning we set coord \n= -1and start by advancing the main codestream along that dimension. Of course slicing may recursively \nhappen within the subexpression, and so on. In general there will be several separate codestreams, one \nfor each piece of the entire expression that was introduced by slicing. (This may be smaller than the \nnumber of conforming reshapes in the expression, but this is a further optimization we do not discuss \nhere.) The above coordination rule works in the general case as well. For instance, each scalar which \nis needed many times will be computed exactly once, no matter where it appears in the entire expression. \nl his happens because the stepper for a scalar always consists entirely of dummy dimensions, and thus \nthe scalar becomes available through a slice with slicing coordinate equal to -1. Therefore the scalar \nwill be computed exactly once, namely when coord = -1 and the various accessors are advanced at the beginning \nof time. The same idea can be used to save space when encountering operators that break the streaming. \nSuch operators stop the propagation of a stepper S coming down from the root. However, rather than evaluating \nthe entire subexpression, we can often proceed by only having the subexpression a slice at a time. Thus, \nfor example, @3@WM RIX can easily be evaluated a row at a time, etc. The smallest required slice is a \nk-slice, with k the smallest integer such that all but the last k coordinates of the subexpression correspond \nto ravel order traversal. This addition of memory to our delayed evaluation strategy is not entirely \nwithout cost at run-time. If the run-time bindings of the atoms are such that the slice is neecled only \nonce, then we are clearly doing unnecessary memory references. This, however, is a somewhat rare event, \nand furthermore tends to come into effect only when expressions are small, in which case we can afford \nthe slowdown. The benefits of generality and overall efficiency for the compiled code seem well worth \nthe price. Figure 8.1 shows the run-time environment for the execution of the expression 1 +(SI+,52 )xA, \nwith S1, S2 scalars. The subexpression S1 +S2 has been sliced using a one-element buffer. Liskov, Snyder, \nAtkinson, and Schaffert, [71 coordinate Iadde! Abstraction Mechanisms in CLU , Proceedings of count Iimtt \n rung ACM Conference on Language Design for o0 2 Reliable Software, March 1977, pp. 166-178 10 1 [81 \nW. M. McKeeman, An Approach to Computer Expression Sindings 2 0 2 Language Design , Ph.D. Dissertation, \nStanford T+ (s1+S2 )xA pT =212 SI University, 1966 scalars PA =31$5 52 33 [91 Terry Miller, Compiling \na Dynamic Language , slice at 51+52 Ph.D. Thesis, Yale University, 1977 n .a. accessor slice main [10] \nAlan J. Perlis, Steps Toward an APL Compiler \u00adcode Updated , Research Report #24, Computer codestreamstream \nScience Department, Yale University, March local data 1975 S1 [11] Jacob T. Schwartz, On Programming: \nAn n u+ S2 Interim Report on the SETL Project; Part I: slice ? Generalities , Computer Science Department, \nCourant Institute of New York University, 0 February 1973 local data PA G [12] Eric J. van Dyke, A Dynamic \nIncremental Compiler for an Interpretive Language u Hewlett-Packard Journal, July 1977, pp. 17-23 &#38; \nstorage pool [13] Wulfl London, and Shaw, Abstraction and Vertfzcation in Alphard: Introduction and Methodology \n, Carnegie-Mellon University and Fig. 8.1. The Run-Time Environment USC Information Sciences Institute \nTechnical Report, 1976 9. Conclusion We have seen how to compile good code for a dynamic language. The \ngenerated code must be preceded by a preamble stating the assumptions for its validity. In our case these \nassumptions consist mostly of assertions about the expression s atom types. In ordinary APL usage, it \nis extremely unlikely that these assumptions will be violated during multiple executions of the expression. \nIf that should happen, then the compiler must be re-invoked on the expression. Note that if our machine \nwere able to interpret bytecodes relative to a type specification, even that step would not be necessary. \nAcknowledgements: The authors would like to thank Alan J. Perlis, Ronald L. Rivest, Alan Kay, and Peter \nDeutsch for valuable comments on the paper. 10. References [1] Philip Abrams, An APL Machine , SLAC Report \n#114, Stanford University, February 1970 [2] Birtwistle, Dahl, Myhrhaug, and Nygaard, SIMULA BEGIN, Auerbach, \n1973 [3] A. P. Ershov, On the Essence of Compilation , Proceedings of IFIP Conference on Formal Description \nof Programming Concepts, August 1977, pp. 1.1-1.28 [4] Peter Henderson and James H. Morris, Jr., A Lazy \nEvaluator , Proceedings of the 3rd ACM Symposium on Principles of Programming Languages, January 1976, \npp. 95-103 [5] Alan Kay et. al., SMALLTALK-72 INSTRUCTION MANUAL, Xerox PARC Technical Report, SSL 76-6, \n1976 [6] Donald E. Knuth, Semantics of Context Free Languages , MAth. Sys. Th. 2, 127, 1968 8  \n\t\t\t", "proc_id": "512760", "abstract": "<p>Most existing APL implementations are interpretive in nature,that is, each time an APL statement is encountered it is executedby a body of code that is perfectly general, i.e. capable ofevaluating any APL expression, and is in no way tailored to thestatement on hand. This costly generality is said to be justifiedbecause APL variables are typeless and thus can vary arbitrarily intype, shape, and size during the execution of a program. What thisargument overlooks is that the operational semantics of an APLstatement are not modified by the varying storage requirements ofits variables.</p><p>The first proposal for a non fully interpretive implementationwas the thesis of P. Abrams [1], in which a high level interpretercan defer performing certain operations by compiling code which alow level interpreter must later be called upon to execute. Thebenefit thus gained is that intelligence gathered from a widercontext can be brought to bear on the evaluation of asubexpression. Thus on evaluating (<i>A</i>+<i>B</i>)[<i>I</i>],only the addition <i>A</i>[<i>I</i>]+<i>B</i>[<i>I</i>] will beperformed. More recently, A. Perlis and several of his students atYale [9,10] have presented a scheme by which a full-fledged APLcompiler can be written. The compiled code generated can then bevery efficiently executed on a specialized hardware processor. Asimilar scheme is used in the newly released HP/3000 APL [12].</p><p>This paper builds on and extends the above ideas in severaldirections. We start by studying in some depth the two key notionsall this work has in common, namely <i>compilation</i> and<i>delayed evaluation</i> in the context of APL. By delayedevaluation we mean the strategy of deferring the computation ofintermediate results until the moment they are needed. Thus largeintermediate expressions are not built in storage; instead theirelements are \"streamed\" in time. Delayed evaluation for APL wasprobably first proposed by Barton (see [8]).</p><p>Many APL operators do not correspond to any real dataoperations. Instead their effect is to rename the elements of thearray they act upon. A wide class of such operators, which we willcall the <i>grid selectors,</i> can be handled by essentiallypushing them down the expression tree and incorporating theireffect into the leaf accessors. Semantically this is equivalent tothe <i>drag-along</i> transformations described by Abrams.Performing this optimization will be shown to be an integral partof delayed evaluation.</p><p>In order to focus our attention on the above issues, we make anumber of simplifying assumptions. We confine our attention to codecompilation for single APL expressions, such as might occur in an\"APL Calculator\", where user defined functions are not allowed. Ofcourse we will be critically concerned with the re-usability of thecompiled code for future evaluations. We also ignore thedistinctions among the various APL primitive types and assume thatall our arrays are of one uniform numeric type. We have studied thesituation without these simplifying assumptions, but plan to reporton this elsewhere.</p><p>The following is a list of the main contributions of thispaper.</p><p>\" We present an algorithm for incorporating the selectoroperators into the accessors for the leaves of the expression tree.The algorithm runs in time proportional to the size of the tree, asopposed to its path length (which is the case for the algorithms of[10] and [12]).</p><p>Although arbitrary reshapes cannot be handled by the abovealgorithm, an especially important case can: that of a<i>conforming</i> reshape. The reshape <i>A</i>&#241;<i>B</i> iscalled conforming if &#241;<i>B</i> is a suffix of A.</p><p>\" By using conforming reshapes we can eliminate inner and outerproducts from the expression tree and replace them with scalaroperators and reductions along the last dimension. We do this byintroducing appropriate selectors on the product arguments, theneventually absorbing these selectors into the leaf accessors. Thesame mechanism handles <i>scalar extension,</i> the convention ofmaking scalar operands of scalar operators conform to arbitraryarrays.</p><p>\" Once products, scalar extensions, and selectors have beeneliminated, what is left is an expression tree consisting entirelyof scalar operators and reductions along the last dimension. As aconsequence, during execution, the dimension currently being workedon obeys a strict stack-like discipline. This implies that we cangenerate extremely efficient code that is <i>independent of theranks of the arguments.</i></p><p>Several APL operators use the elements of their operands severaltimes. A pure delayed evaluation strategy would require multiplereevaluations.</p><p>\" We introduce a general buffering mechanism, called<i>slicing,</i> which allows portions of a subexpression that willbe repeatedly needed to be saved, to avoid future recomputation.Slicing is well integrated with the evaluation on demand mechanism.For example, when operators that break the streaming areencountered, slicing is used to determine the minimum size bufferrequired between the order in which a subexpression can deliver itsresult, and the order in which the full expression needs it.</p><p>\" The compiled code is very efficient. A minimal number of loopvariables is maintained and accessors are shared among as manyexpression atoms as possible. Finally, the code generated is wellsuited for execution by an ordinary minicomputer, such as a PDP-11,or a Data General Nova. We have implemented this compiler on theAlto computer at Xerox PARC.</p><p>The plan of the paper is this: We start with a generaldiscussion of compilation and delayed evaluation. Then we motivatethe structures and algorithms we need to introduce by showing howto handle a wider and wider class of the primitive APL operators.We discuss various ways of tailoring an evaluator for a particularexpression. Some of this tailoring is possible based only on theexpression itself, while other optimizations require knowledge ofthe (sizes of) the atom bindings in the expression. The readershould always be alert to the kind of knowledge being used, forthis affects the validity of the compiled code across reexecutionsof a statement.</p>", "authors": [{"name": "Leo J. Guibas", "author_profile_id": "81452606669", "affiliation": "Xerox Palo Alto Research Center, Palo Alto, Cal.", "person_id": "P170488", "email_address": "", "orcid_id": ""}, {"name": "Douglas K. Wyatt", "author_profile_id": "81100283995", "affiliation": "Xerox Palo Alto Research Center, Palo Alto, Cal.", "person_id": "P329971", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/512760.512761", "year": "1978", "article_id": "512761", "conference": "POPL", "title": "Compilation and delayed evaluation in APL", "url": "http://dl.acm.org/citation.cfm?id=512761"}