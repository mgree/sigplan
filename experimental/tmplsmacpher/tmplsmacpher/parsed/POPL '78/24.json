{"article_publication_date": "01-01-1978", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee.&#38;#169; \n1978 ACM 0-12345-678-9 $5.00 conference Record of the Fifth AIMiUd ACM Sym~sium on Principles of Programming \nLanguages A NEN METHODFOR COMPILER CODEGENERATION (Extended Abstract)* !-?. Steven Glanville+ and Susan \nL. Graham Computer Science Division University of California Berkeley, California 94720 Abstract An algorithm \nis given to translate a rela\u00adtively low-level intermediate representation of a program into assembly \ncode or machine code for a target computer, The algorithm is table driven. A construction algorithm is \nused to produce the table from a functional description of the target machine. The method produces high \nquality code for many commercially available computers. By replacing the table, it is possible to retarget \na compiler for another kind of computer. In addi\u00adtion techniques are given to prove the correctness of \nthe translator. Introduction Since the early history of compilers, researchers have attempted to systematize \nand auto\u00admate the production of compilers. The most suc\u00adcesful aspect of this attempt has been syntax \nanal\u00adysis. It is now commonplace to use a table-driven syntax analyzer which is automatically constructed \nfrom a generalized context-free grammar specifying the syntax of the source language [see, for exam\u00adple, \nAho-Ullman 77]. Such an analyzer is easily obtained, provably correct, and modular. Attempts have been \nmade to simplify the prod\u00aduction of compilers for a specific source lan\u00adguage by organizing compilers \nso as to isolate target-machine specific aspects of the translation. It is then possible to retarget \nthe compiler by changing those portions of the compiler which con\u00adcern the architecture of the machine \n[see, for example, !~elsh-Quinn 72 or Poole 74]. In this paper we describe a method for auto\u00admatically \ngenerating a table-driven coder for a compiler from a functional description of the. machine instructions. \nThe technique we use is in certain respects similar to methods used for table\u00addriven syntax analysis \nand has many of the same advantages --namely, modularity, correctness, com\u00adpactness, and ease of use. \nThe code generator produces quite good code; in particular, it is able to detect the instances in which \nspecialized instructions can be used. It is possible for the code generator to utilize many kinds of \noptimiza\u00adtion information, should it be available. In * Research sponsored by the National Science Founda\u00adtion \nunder Grant MCS74-07644-A03. Current address: General Systems Division, Hewlett-Packard, Santa Clara, \nCA 95050. addition, our methods should prove valuable in designing retargetable compilers. In the sections \nthat follow, we summarize previous work on this topic, give our code genera\u00adtion algorithm and our table \nconstruction algo\u00adrithm, explain the way in which this code is incorporated into a compiler, and summarize \nour implementation experience. The discussions are necessarily brief because of the length limitation \non this summary. The reader should be aware that by code generation we mean not the entire synthe=of \nthe source program but rather the restricted task of choosing a sequence of object code instructions. \nThus such issues as choices of representation, storage allocation, and machine-independent opti\u00admization \nare outside the scope of this work. Background For our purposes, previous research in code generation \nfalls into three classes. The first class are attempts to deal with code generation mathematically, usually \nin order to produce opti\u00admal or near-optimal code [Aho-Johnson 76, for example] . Significant results \nhave been obtained. However, research has been with idealized models of computers. Real computers tend \nnot to have mathematically clean instruction sets. (There are always special instructions that implement \ncertain computations more efficiently). Thus this approach must be extended for use in production compilers. \nThe approach is in certain respects complementary to our techniques and one can be incorporated into \nthe other. The other two classes of research have tended to focus on implementation methods for real \ncompu\u00adters, often with loss in efficiency of the gener\u00adated code. ,. The second approach to code generation \nis to provide information about the computer in proce\u00addural form, using special purpose code generation \nlanguages and interpreters. Examples of this approach are the UNCOL ideas [Strong 58, Steel 61], the \nPL/I optimizer of Elson and Rake [Elson 70], the method developed for PL/C [Wilcox 71], and the work \nof [Donegan 73]. These methods are an improvement over strictly ad hoc techniques, but require considerable \nhand=oding of tedious low\u00adlevel details, making correctness difficult to ascertain and retargeting a \nchore. The third class of methods and the one into which our work falls uses information about the target \nmachine supplied in a descriptive form or data base. The macro approach to code generation falls into \nthis class, as does the t41T thesis of Miller [Miller 71] and the Yale dissertation of ~leingart[~ileingart \n 73]. our approachto code generation was initially stimulated by the work of Miller and Meingart and \nwe have built on their ideas. The Coding Algorithm The output of the front end of the compiler is assumed \nto be a linearized intermediate repre\u00adsentation (IR) of the source program. The IR con\u00adsists of a sequence \nof parenthesis-free prefix expressions. In such a representation, operators are followed by their operands. \n(For simplicity, we assume in this paper that all operators are unary or binary. The techniques we describe \nare readily extended to n-ary operators. ) The imple\u00admentation decision concerning representation and \nstorage allocation, as well as all but the low\u00adlevel optimization, are already incorporated in the IR \nversion of the program. Each instruction of the computer is described by a prefix expression together \nwith certain semantic information and an assembly --or machine --language template. (The examples should \nmake these notions clearer. ) The coding algorithm performs a pattern-match similar to parsing in which \nthe IR sequence of prefix expressions is translated to a sequence of instructions, However, the situation \ndiffers from syntax analysis in the following respects. Since most operators can access their operands \nin a variety of ways, the target machine description is normally ambiguous. Indeed, an important factor \nin code generation is the way in which these ambi\u00adguities are resolved. Secondly, the reduce move of \nthe code generator is considerably more compli\u00adcated than in syntax analysis, since it selects among \na variety of instructions or instruction sequences on the basis of both syntactic and seman\u00adtic information. \nFinally, error situations siq\u00adnalled by the code generator always signify com\u00adpiler bugs. As an example, \nFigure 1 describes a small set of machine instructions. Notice that in form the instruction descriptions \nresemble context free 1* r.2 ::= (+ T+klr.lr.2) add r.2, k.l, r.l ; 2 r.1::= (+r.l T+k.lr.2) add r.l, \n/c.l, r.2 ; 3* r.1::= (+ t kl r.1) add r.l, k.l ; 4* r.1 ::= (+ r.1 T k.1) < add t .l, k.l ; 5 r.1 ::= \n(+ r.1 r.2) add r.l, r.2 ; 6* r.2 ::= (+ r.1 r.2) add r.2, r.l ; 7* 1::= (:=1 +/c.lr.lr.2) store r.2,*k.l, \nr.l ; 8* k;:= (:= + k,lr,l r.2) <store r.2, k.l, r.l ; 9* k::= (:= tk.lr.1) <store r.l, k.l ; 10 k::= \n(:= k.1 r.1) <store r.l, k.l ; 11* k::= (:= r.1 r.2) store r.2, r.l ; 12* r.2 ::= (t + k.1 r.1) load \nr.2, k.l, r.l ; 13 r.2 ::= (+ k.1 r.1) load r.2, =k.l, r,l ; 14* r.2 ::= (+ r.1 k.1) load r.2, =k.l, \nr.l ; 15 r.2 ::= (t r.1) load r.2, *r.l ; 16* r.1 ::= (~ k,l) load r.l, k.l ; 17* r.1 ::= (k.1) load \nr.l, =k.l ; Fig. 1. Sample Instruction Set Description grammar rules. (For each instruction a correspond\u00ading \nassembly language instruction is also given,) The symbol to the left of ::= designates the des\u00adtination \nof the result of the computation; the pre\u00adfix expression to the right describes the instruc\u00adtion computation. \n(The parentheses aremeta-sym\u00adbols used for readability.) A left hand side of lambda indicates ter value, \ni.e. the side effects. that ins there is no truction is resulting executed regis\u00adfor its By convention, \n r designates a general pur\u00ad pose register and k denotes a constant, typically an address offset. The \nstore operator is := and the contents operator is +. The symbols following the . represent semantic qualifications. \nThe qualifications on r.1 and r.2 indicate that they denote possibly distinct registers. The repetition \nof a qualification in an instruction indicates repetition of the same register or constant. Thus the \nfirst ADD instruction adds the value in the memory location addressed by the first constant plus the \nfirst register to the contents of the second register, leaving the sum in the second register. Observe \nthat the commutativity of oper\u00ad ands is indicated explicitly in instruction descriptions (rules 3 and \n4, for example). The addition of semantic restrictions to the instruction description allows a greater \nnumber of special instructions to be described. It also com\u00adplicates the choice of output instructions \nin the shift-reduce code generation algorithm. A single syntactic instruction pattern may correspond \nto more than one instruction as the introduction of semantic restrictions may require the duplication \nof some instruction patterns by forcing some com\u00admutative operations to be represented by two iden\u00adtical \npatterns (e.g. add r.l,r.2 and add r.2,r.1) and by allowing one instruction to be specified as a special \ncase of another (e.g. inc r.1 and add r.1 = k.1). It also may be that two distinct instructions compute \nthe same expression but leave the result in registers of different classes (e.g. loadx x.l,ci and loadr \nr.l,a). The IR version of the source language consists of a sequence of prefix expressions composed of \nthe same syntax symbols that are used in the descrip\u00adtion of the instruction set. However, in the IR, \nsemantic qualifications carry specific information, whereas in the machine description they simply dif\u00adferentiate \noperators. The translation from source language to IR will in general depend on the seman\u00adtics of the \nsource language and the implementation decisions. For instance, in a block-structured language, the statement \nA := B+C might be: := +k.a r.7 +++k.b +r.7 +k.c (*) where a, b, c designate constants and r.7 is the \nlocal base register. (Thus by inference, the base of the data segment containing B is obtained by following \na static link, A is local, and C has an absolute address.) The overall structure of the algorithm is \nthat of an LR(l)-like deterministic shift-reduce parser. The language generated by the target machine \ndescription productions or rules with lambda replaced by X and the rule S :;=X* added consti\u00adtutes the \nlanguage being analyzed and the IR is the input. Figure 2 gives the overall code generation Algorithm \n1 -The Code Generator Input: ACTION and NEXT functions (represented in matrices) derived from machine \nM s description, and the IR of a program, P, being compiled. Output: An assembly language program for \nP on machine M. Method: Perform a Shift-Reduce parse of the IR input, emitting target instructions whenev\u00ader \nreductions are performed. Itritia[izafiow Set the parser s state, S, and the stack to ~. Step 1: Set \nthe look-ahead symbol, u, to the next input symbol. If all the input has been read, set u to the end \nof input symbol, $. Step 2: Perform the action in ACTION IS, u]: shift: Push u onto the stack. Then set \nS to the value of NEXT [S, u], push S onto the stack, and advance the input one symbol. Go to step 1. \nreduce R: Output an instruction from set R or an equivalent sequence of instructions (see next section). \nIf the rule used is r ::= a then pop the stack 21al times and set S to the state on the top of the resulting \nstack. If r # k then push r onto the stack, set S to the value of NEXT [S,r] and push S onto the stack. \nGo to step 21 . accept: The code generator halts. Code has been generated for the entire IR input string. \nThis can only happen when all of the input has been read (i.e.. u is $) and the stack is empty (i.e. \nS is qo). error: Issue an error message and halt. The IR input has no parse using the underlying grammar \nfor M s instruction set. TIr r E N, then ACTION IS,I] must be shift. Fig. 2. The Code Generation Algorithm \nalgorithm, which is very similar to an LR(l) recog-are identical but the cost of the increment nize except \nfor the special treatment of rules instruction is less. It is also possible, if all with left part 1 \nand the code-emitting semantics the instructions in the set have semantic restric\u00adof the reduce action.t \nSemantic information is tions, that more than one instruction is generated. carried along with the state \ninformation on the This situation is discussed in the next section, coder s stack. For example, when \na shift is per- The coding algorithm works in conjunction with formed with the input symbol r standing \nfor a a conventional register allocation routine. Theregister, the specific register represented by machine \ndescription specifies how many and what that r is also pushed onto the stack. Then when a kind of registers \nexist on the target computer, and reduce is done, the semantic information necessary which ones are available \nto the register allocator. to generate a final instruction can be read off Each nonterminal in the instruction \nset description,the stack. in general, represents a logical register or a The instruction used when a \nreduce operation class of logical registers. Each logical register contains more than one instruction \nis picked by a is associated with an actual machine register or simple heuristic. Instructions are ordered \nby the pair of registers. In this way information such as table constructor into a best instruction first \nthe fact that dO is the register pair <rO,rl> is sequence. At code generation time, the instruc-included \nin the machine description. tions in the set for a specific reduce action are Register allocation occurs \nas a subtask to a tested in that order until an instruction is found reduce operation. After an instruction \npattern has that is semantically compatible with the informa\u00adbeen semantically verified, if the result \nof the tion on the top of the stack, Since all instruc\u00adinstruction is non-A, and if the result register \nis tions in a reduce set have the same instruction not semantically linked to any other register in pattern, \nthe cheapest , according to some cost the instruction pattern, the register allocatorcriteria, instructions \nare tested first. If the provides a free register of the appropriate class, length of an object instruction \nin bits is used as after setting its use count to 1. If the resultthe cost factor, a 16 bit long increment \ninstruc\u00adregister, r, is semantically linked to a register tion will be tested before a 32 bit long add \nin the instruction pattern, then r must be used asimmediate instruction, since the basic patterns the \ntarget register and its use count set to one (since it contains a newly computed value). The For brevity, \nwe have assumed that the reader is routine is easily generalized to incorporate infor\u00ad familiar with \nLR(l) parsing and its terminology. mation about reuse of common subexpressions. Figure 3 contains the \nmove table used by the Input: := + k.a r.7 + + + k.b + r.7 T k.c code generator for the instruction set \nin Figure 1. In the table, the ACTION and NEXT functions are represented in the following format: Columns \nof the table are headed by language symbols and rows correspond to parser states. An entry in row q, \ncolumn v of the table is of the form a:b , where a always represents ACTION(q,v) and b is NEXT(q,v) when \na is shift, and the rule set by which to reduce when a is reduce. The ACTION values shift and reduce \nare abbreviated respectively S and R. A blank designates the error action. Using the instruction set \nof Fig. 1, we show in Fig. 4 the sequence of steps taken by the code generator for the IR input expression \n(*). For each step we indicate the action, the resulting stack, and the intuitive current state. The \ninstruction sequence produced by the algorithm is 15,16,1,8. The reader can observe that other code Code \nGenerator Move Table $rk+f := I* ACCEPT s: 2 2* s: 3 s: 4 s: 5 S: 6 3* s: 7 S: 8 s: 9 s: 10 4 s: 11 \nS: 8 s: 9 s: 10 5* s: 12 s: 13 s: 9 s: 14 6* s: 15 S: 16 s: 17 s: 10 7 R: 11 R: 11 R: 11 R: 11 R: 11 \nR 11 8* R: 17 R: 17 R: 17 R: 17 R: 17 R; 17 9* s: 12 S: 18 s: 9 s: 14 10* s: 15 s: 19 s: 20 s: 10 11 \n R: 10 R: 10 R: 10 R: 10 R: 10 R: 10 [2* s: 21 s: 22 s: 9 S: 23 [3* S: 24 S: 8 s: 9 s: 10 [4* s: 15 S: \n25 S: 26 s: 10 L5* R: 15 R: 1.5 R: 15 R: 15 R: 15 R: 15 [6* S: 27 S: 8 s: 9 s: 10 [7 s: 12 S: 28 s: \n9 s: 14 [8* S:29 S:8 S:9 s: 10 [9* R: 16 R: 16 R: 16 R: 16 R: 16 R: 16 !O* s: 12 s: 30 s: 9 s: 14 !1 \n R: {5,6) R: {5,6] R: {5,6) R: {5,6] R: (5,6} R: {5,6} Q* R: 14 R: 14 R: 14 R: 14 R: 14 R: 14 13* s; \n15 s: 31 S: 32 s: 10 !4* S:33 S:8 S:9 s: 10 !5* S: 34 S: 8 s: 9 s: 10 !6* s: 12 s: 35 s: 9 s: 14 !7* \nR: 9 R: 9 R; 9 R: 9 R: 9 R: 9 !8* S: 36 S: 8 s: 9 s: 10 !9* R: 13 R: 13 R: 13 R: 13 R: 13 R: 13 10* S:37 \nS:8 s:9 s: 10 ~1 R:4R:4R:4 R: 4 R: 4 R: 4 ,2* s: 12 S: 38 S: 9 s: 14 3* R: 8 R: 8 R: 8 R: 8 R, 8 R, \n8 4* R: 3 R: 3 R: 3 R: 3 R: 3 R; 3 5* s: 39 S: 8 s: 9 s: 10 6* s: 40 S: 8 s: 9 s: 10 7* R: 12 R: 12 R: \n12 R: 12 R: 12 R: 12 8* s: 41 S: 8 s: 9 s: 10 9* S:42 S;8 S;9 s: 10 o* R: 7 R: 7 R: 7 R:7R:7R;7 1 R: \n2 R:2R:2 R: 2 R: 2 R: 2 2* R: 1 R: 1 R: 1 R: 1 R: 1 R: 1 Fig. 3. Code Generation Table for Example 1. \nshift Stack is I-1 :=2 Have matched the first symbol of instructions 7-11.  2. shift P1:=2+5 Matched \nfirst two symbols of rule 8, first of rules 1-6, 13, 14.  3. shift 1 1 :=2+ 5k.a13 Matched first three \nof rule 8, first two of rule 13, all of rule 17. 4. shift t-l := 2+5k.a13r.724 Matched first four of \nrule 8, all of rule 13.  5. shift t--l := 2+ 5k.a13r.724+9 !Iatched first symbol of rules 1-6, 13, 14. \n 6. shift } l:=2+5k.a 13r.724+9 +14 !Jatched first two of rules 1 and 3, first of  rules 12, 15, 16. \n 7. shift l-l:= 2+5k.a13r.724+ 9+14+26 Matched first three of rule 1, first two of rule 12, first of \nrules 1-6, 13, 14. 8. shift }l:=2+5k.a 13r.724+9 +14+ 26k.b35 Matched first four of rule 1, first three \nof rule 12, first 2 of rule 13, all of rule 17. 9. shift l-l:= 2+ 5k.a13r.724+9 +14+ 26k.b35 + 10 Matched \nfirst symbol of rules 12, 15, 16. 10. shift }l:=2+5k.a 13r.724+9 +14+ 26k.b35 +lOr.715 Matched all \nof rule 15.  11. reduce rule 15 Assign result to rl. Emit LOAD rl,*r7\u00b0 shift left side P1 :=2+5k.a13r.724+9+14 \n +26k.b35r.l 39 Matched first five symbols of rule 1, all of rules 12, 13. 12. shift l-l:=2+5k.a13r.724+9 \n+14+26k.b35 r.139 +10 Matched first of rules 12, 15, 16.  13. shift l-l:= 2+5k.a13r.724+ 9+14 +26k.b35 \n r.1 39+10 k.c19 Platched all of rules 16 and 17.  14. reduce rule 16 Assign result to r2. Emit LOAD \nr2,c shift left side fil := 2+ 5k.a13r.724 +9+14 +26k.b35r.l 39r.2 Matched all of rule 1. 15. reduce \nrule 1 Emit ADO r2,b,rl shift left side + 1 := 2+ 5k.a13r.724r.233 Matched all of rule 8. 16. reduce \nrule 8 Emit STORE r2,a,r7\u00b0 Stack now contains t-1. If more input remained the code generator would read \nthe next symbol. Since there is not, the algorithm terminates.  Fig. 4. Example of Code Generation sequences \n(e.g. instructions 17,5,17,15,5,15,16,5, 11) are possible for this input. The table construction algorithm \nsketched in the next section always produces a conflict-free table, hence the coding algorithm is always \ndeter\u00administic. The coding algorithm has the following aspects which are not obvious from the small exam\u00adple. \nA reduce action first checks the semantic restrictions (i.e. qualifications) such as specific constants, \nmatching registers, even/odd-ness of the matched patterns (there may be more than one match), etc. I,Jhen \na particular instruction is selected, the register allocator is called if a result register (left hand \nside) is needed. In certain instances sketched in the next section, the reduce action emits more than \none instruction. (Typical\u00adly this happens if a long instruction is matched but its semantic constraints \nare not). The seman\u00adtic qualifications usually arise in conjunction with specialized instructions. For \nexample, an increment instruction is only applicable if the constant has value 1. The correctness considerations \nare sketched in the next section. The reader can see that among them are the requirement that the IR \nvocabulary be a subset of the target machine vocabulary and that the operators have the same computational \nmeaning in both. The Table Construction Algorithm The table constructor first treats the target machine \ndescriptions as context-free grammar rules (ignoring semantics) and constructs the set of LR(0) states. \n(In Fig. 1, r (register) is the only nonterminal. The example generates 42 states. ) There are many inadequate \nstates caused by shift-reduce or reduce-reduce conflicts. Most of these conflicts represent ambiguities. \nFor instance, in the example, rule 16 followed by rule 5 is the equivalent of rule 4. The construc\u00adtion \nalgorithm seeks to resolve these conflicts in favor of short instruction sequences. ble use the approach \nin [Aho-Johnson-Ul lman 75] for the reso\u00adlution of these ambiguities --namely, shift\u00adreduce conflicts \nare resolved by shifting. This heuristic usually causes the use of more power\u00adful instructions. Reduce-reduce \nconflicts are often resolved by the semantic restrictions. If not, the longest instruction is used. These \nheu\u00adristics are reflected in the example in steps 3, 4, 8 and 13. The initial phase of the table con\u00adstructor, \nin which the conflicts are resolved in this way, is given in Fig. 5. In resolving shift\u00adreduce conflicts, \nSLR(l) lookahead information is used to insure that reduce actions are included where needed. Given a \ngeneral context-free grammar, the conflict-resolution rules would not necessarily yield a recognize for \nthe entire language gener\u00adated by the grammar. However, the construction is language-preserving for uniform \ninstruction sets. (A proof is contained in Glanville 77). Let V be the set of vocabulary symbols for \nthe instruction set description. An instruction set is said to be uniform if it satisfies the following \ncondition: Any left (similarly right) operand of a binary operator b is a valid left (respectively right) \noperand of b in~prefix expression of V* containing b. Any operand of a unary operator u is a valid operand \nof u in ~ prefix expression of V* containing u. An instruc\u00adtion set is uniform if its description is \nuniform. The essential idea of uniformity is that operands to an operator are valid independent of context. \nWe give some examples to clarify this notion. Consider the instruction set described in Fig. 1. The operands \nof := are all either registers (rule 11) or special cases of registers, as evidenced by rules 12, 13, \n16, and 17. The operands of the leftmost + in rules 1-6, 13, 14 are all either registers (rules 5 and \n6) or prefix expressions which become registers by rules 12, 16, or 17. Since a register is an argument \nof + (rule 15) and of + (rule 5), the arguments of the other occur\u00adrences of + and + represent special \ncases (i.e. ambiguities). In summary, whenever :=, +, and t occur, their operands are any prefix expressions \ncorresponding to registers. Note that in this example, both + and := have the same set of left operands \nas right operands. Such a situation need not be true in general. An example of an instruction set which \nis not uniform is: r :,= (+$kk) r ..=(+k+k)!. ::=(:=~r) A In this example k is a valid first operand \nof + only if the second operand is + k and vice-versa. Letel. ..ei.. .ek be a prefix expression, where \nk>2andl<i <k. As is well known, the prefix ex~ression c~rre~ponds to a (unique) binary tree with nodes \nlabel led by the ei s, where el labels the root and non-leaf nodes are labelled by opera\u00adtors. The subtrees \nrooted by children of an opera\u00adtor node represent its operands (see, for example, [Meyers 74] ). For \nexample theright hand side of the first instruction in Fig. 1 corresponds to the tree I /+\\ k.1 r.1 In \norder to check that an instruction set is uni\u00adform, we define the relations which are tree-equi\u00advalents \nof the usual string relation FOLLOW. Using these tree relations, we obtain a simple test for uniformity \nwhich is incorporated in the table constructor. The definitions are presented with respect to a particular \ninstruction description G. Recall that V is the set of operator and operand symbols used in the instruction \ndescription. LetB~Vbe the binary operators; let U ~V be the unary opera\u00adtors. Define the relation LEFT \non (BUU)XV as: b LEFT v iff =an instruction r ::= a.bvB , where r may be A. Similarly, define the relation \nRIGHT on BxV as: b RIGHT v iff =an instruction r ::= abBvy for some prefix expression B#k, where r may \nbe A. Clearly, b LEFT v if v is the leftmost symbol of a left operand of b in some instruction and b \nRIGHT v if v is the leftmost symbol of a right operand of b in some instruction. By convention unary \nopera\u00adtors have left operands. To extend these notions from operands in the same instruction to all operands, \nwe need the usual grammatical leftmost and rightmost descendent Algorithm 2 The Initial SLR(l) Code \nGenerator Constructor Input: A TMDL instruction set description for machine M. Output: The initial ACTION \nand NEXT functions (represented in matrices) for a code generator for machine M. An error message if \nthe instruction set is not uniform. Method: Construct a set of states for the context free grammar underlying \nthe given instructions. As it is being done, fill in the NEXT and ACTION values to produce the code generator, \nresolving conflicts. Test each state for appropriate LEFT FIRST and RIGHT FIRST ac\u00adtions. procedure generatestate~ \nbegin qo --close({i 6 I \\i = [k -T . cd]); k --O; n + O; I* highest state number *I while k < n do begin \n/*cOMpukSIJccessOrs f qk*/ VvEVdo if~j~qk with i=[x-a. ~ Bl then ACTION [qk, VI+ shift; q --close({[a-a \nv./3ll[x~a . UP] f qkl); if 3 gj = q then NEXT[qk, v] = qj else n= n + 1; qn+-q ; NEXT[qk, v] e qn; \nelseif 3i sqkwith i= [x-a.1then R -{[x~a .1 ~ qkl v E FOLLOW(x), A i = qk, i = [x ~a .] such that both \nlength(a ) > length(a) and v ~ FOLLOW(X ) ); ACTION [qk, v] -reduce R; else ACTION [qk, VI -erro~ /*uniformity \ncheck for qk*/ ~itemsof the form [r~a. v~] 6 qk, a#X do begin x -parent(u,a v/3); if leftchild(v,av(3) \nthen d uwithx LEFT FIRST udo begin if ACTION [qk, u] = error then output ( Not Uniform ); end; else / \nx < B and v begins the second operand / v uwithx RIGHT FIRST udo begin if ACTION [qk, u] = error then \nelse output ( <Not Uniform ); end; end; /*advance to next state*/ k+k+l; end; ACTION [qO,$] accePt; \n end generatesraces; function close(q); begin repeat q-q+{ [x+. all b-p. x Ylcq, x#A) uutil q does not \nchange; return (q); end close; Fig. 5. Initial Table Constructor By the usual product of relations, \nb LEFT FIRST v relations. Define the relation FIRST on VXV as: u FIRST v iff =a derivation u =* va for \nsome u=V*, u =V, where derivations are defined in the usual way. Thus u FIRST v if there is some derivation \nin G from u that yields v as the leftmost symbol. Similarly, define LAST on VXV as: u LAST v iff =a derivation \nu -* a,v for some aeV*, u=V. iff v can appear as the first symbol in the left operand to b in a derivation \nin G, and b RIGHT FIRST v iff v can appear as the first symbol in the right operand to b in a derivation \nin G. Using these definitions, we obtain the following theorem. A more formal statement of the theorem \nand a proof appear in [Glanville 77]. Theorem 1. Let G be an instruction set description. Let the code \ngenerator tables be computed by the algorithm in Fig. 4, and let Q be the corresponding set of states. \nThe following conditions are equivalent. 1) G is a uniform instruction set. 2a) For all u, v in V, if \nu LEFT FIRST v, then for every state in Q containing an item [r+a*xB], for some r, a, x, B, where u is \nthe parent of x in the tree corresponding to axe and x is the left child of u, it is the case that ACTION[q,v] \n= shift. b) For all u, v in V, if u RIGHT FIRST V, then for every state in Q containing an item [r+a*xf3] \nfor some r, a, x, B, where u is the parent of x in the tree corresponding to O,XBand x is the right child \nof u, it is the case that ACTION(q,v) c {shift, reduce R}. In other words, the theorem says that if the \ngrammar is uniform, then if a left (respectively, right) operand of an operator u is expected in a given \ncontext, then the first symbol of any possi\u00adble left (respectively, right) operand of u is legal and \nconversely. It is the test suggested by this theorem that is incorporated in the table constructor. In \nthat test, parent(v,avf3) returns the parent of v in the tree corresponding to the prefix expression \ncIv13. The predicate leftchild(v,avf3) returns true if v is the left child of its parent in the tree \ncorres\u00adponding to the prefix expression UV6 and false otherwise. ~~ote that it suffices to check for \nerror because only state q. can have an accept action. Also note that the possibility of a reduce in \n2a) is ruled out by the form of the instruction descriptions (i.e. u must be an operator and must be \nthe last symbol in a, and no right hand side of an instruction description can end in an operator). Since \nit is the relations LEFT FIRST and RIGHT FIRST which characterize uniform instruction sets, the reader \nmay wonder why the table construc\u00adtor instead uses the string relation FOLLObJin computing ACTION and \nNEXT. The reason is that the use of FOLLOWis considerably simpler computation\u00adally and yields tables \nwhich are equivalent except for deferred error detection (see [Glanville 77] for a proof). Since errors \nstem only from compil\u00ader bugs, delayed error detection seems acceptable. For instruction set descriptions, \nwe can define FOLLOWby: For ueV, let FOLLOWI(U) = {vlfor some r::=cixvB. where ., r may be A, x LAST \n~ and y FIRSTV} . Let A-LAST = {uI= instruction A::=ax for some a~ V* and x LASTu} . (It is easily shown \nthat A-LAST consists only of operand symbols.) Then for u = V, FoLLowl (u) if u *A-LAsT FOLLOW(u) = FOLLOWl(u)U \n{root-level { operators} if u G I-LAST . The root-level operators are those which occur as leftmost operators \nof instructions with destination A. Using the notion of uniformity, it can be shown that Theorem 2. Let \nG be an instruction descrip\u00adtion. Then Algorithm 2 declares error if and only if G is not uniform. If \nG is uniform, then Algorithm 1 using the tables generated by Algorithm 2 but disregarding semantic qualifications \nfails to reach the ACCEPT state and to generate code for an input IR only if 1) the code generator loops \n2) the input IR is not syntactically within the sequence of prefix expressions described by the instruction \nset. It remains to eliminate the possibility of looping, to give sufficient conditions for the IR input, \nand to deal with the semantic issues. We consider these topics in turn. Since the prefix expression describing \nan instruction qontains at least one symbol, looping could occur only because of a sequence of chain\u00adreductions \ncorresponding to register-to-register moves. The possibility of potential looping is easily detected. \nEach potential loop is then broken by a kind of state-splitting of some of the states computed by Algorithm \n2. (Details are contained in [Glanville 77]). As previously described, the reduce action generates code \nby checking the semantic qualifica\u00adtions of the matched instruction pattern against the semantic qualifications \nindicated on the stack. If the set of instructions associated with a given reduce action are all semantically \ncon\u00adstrained, it is possible that none of the instruc\u00adtions will be compatible with the semantics on \nthe stack. In this case, the code generator would semantically block. Semantic blocking can be avoided \nby the use of a default list of shorter instructions that contain no semantic restrictions and together \ncompute the desired expression. Con\u00adsider the memory-to-memory add instruction: a ::= (:=k.l++k.l+k.2) \nmadd k.2,k,l A ::= (:= k.l++k.2+k.1) madd k.2,k.1 The basic instruction pattern is :=k++k+ k . If this \npattern occurs in the IR, but the constants associated with each of the three k s are distinct, then \nthe instruction cannot be used. If there are instructions with the patterns r ::=+ k , r ::= ++k r , \nand A:: =:=k r , then they could be issued, simulating a non-restricted memory-to\u00ad memory add instruction, \nand the code generation could proceed from there as though the longer instruction had been issued. Default \ninstruction lists for all reduce R actions having no semantically unrestricted instructions are constructed \nby the table con\u00adstructor. The lists are obtained by simulating the action of the coderusing as input \nthe right hand side of the semantically restricted instruc\u00adtion and using only those instructions that \nare shorter than the one under consideration. The construction, in effect, builds a code generator for \nthe subset of shorter instruction patterns and generates code for the restricted instruction (cf. Algorithm \n4.8 of [Glanville 77]). In the presence of semantically restricted instruction patterns, the code generation \nAlgorithm 1 will choose from a list of instruc\u00adtions in step 2 when a reduce action is performed. Assuming \nthat this list has already been sorted by the table constructor, the instructions must be tested sequentially \nuntil one is found that is semantically compatible. In the event that no instruction is acceptable, the \ndefault list of instructions is used to implement that computation. There are several classes of semantic \nrestric\u00adtions that may have to be satisfied. Constants in the IR input may have to be equal to specific \nvalues, such as 1 in an increment instruction, and logical registers may have to be equivalent to specific \nactual reqisters. Multiple occurrences of a symbol in any-class may have to refer to the same actual \nvalue or register. If the result is to appear in a register, then there must not be any references to \nthe value in that register out\u00adside of that instruction ~attern. Such references could exist only if \nsome kort of common subexpres\u00adsion elimination has been done. Finally, any additional semantic restrictions \nrequired to properly describe a particular target computer may be added to the code generator. The proof \nof correctness only requires that a default instruc\u00adtion or list of instructions be available for each \nrestricted instruction, so that it will always be possible to generate code. If two instructions have \nthe same instruction pattern but different result locations, the actual instruction used is arbitrary. \nAdherence to the uniformity condition insures that the coder will still accept the input regardless of \nthe choice made. The item [x~Bo] will not be included in a state of a uniform instruction set unless \na reduc\u00adtion using it on a valid input is still valid. For practical reasons, one may preorder the instructions \nby cost, by the instruction that leaves the result in a register class with more actual registers (in \nan attempt to avoid having to save registers in temporaries), or by any other ordering desired, and the \ncoder will still func\u00adtion correctly. It is also possible to examine the operator for which the register \nis an operand, to determine the instruction that leaves the result in a location that is closer to a \nvalid operand to that operator, thus possibly avoiding a subsequent register move instruction. It is \nshown in [Glanville 77] that if looping and semantic blocking are eliminated by the exten\u00adsions to the \ntable constructor sketched above, and if the instruction description accurately describes the target \nmachine, then the code generator pro\u00adduces correct code for all well-formed input. The input IR is well-formed \nif 1) the input is a sequence of prefix expressions; 2) the operators and operands of the IR are from \nthe same set as the operators and operands of the instructions, and have the same meaning; 3) the sequence \nof prefix expressions is valid, that is, it is in the language generated or described by the instruction \nset. Condition 2 must, of course, be checked by the implementor. The implementor either can prove that \nthe routines generating the IR (i.e. the front end of the compiler) meet specification 1), or can provide \na simple routine to test the input to the code generator. Fortunately, if the instruction set is uniform, \ncondition 3 is also relatively easy to check. Let LEFTIR, RIGHTIR, and FIRSTIR be the rela\u00adtions satisfied \nby the IR. Let LEFTcG, RIGHTcG, and FIRSTCG be the relations of a uniform instruc\u00adtion description described \npreviously. Then an intermediate representation (IR) is valid for a code generator (CG) provided that: \nEFTIR ~LEFTCG RIGHT~R~RIGHTcG , and lRSTIRSFIRSTCG Since the compiler probably cannot generate all IR \nexpressions generated (i.e. computed) by a uniform instruction set, the inclusion might be proper in \nsome instances. It is the responsibility of the implementor (or of some other part of a compiler-writing \nsys\u00adtern) to specify LEFTIR, RIGHTIR, and FIRST1~. However, specifying these relations is considerably \nsimpler than characterizing the set of strings that are possible IR expressions. How the Code Fits into \na Compiler It should be clear to the reader that the code generator is not a separate pass in a compiler \nbut is conceptually a co-routine. The purpose of the code generator is to isolate and automate the low-level \ndecisions of instruction selection. As far as retargetable compilers are con\u00adcerned, the coder does not \nencapsulate all of the machine-dependent aspects of a compiler. However, many of the remaining factors \n(word size, number and versatility of registers, etc.) are relatively easily specified via a checklist \nand a compiler designed to be retargetable can be reconfigured using the larger architectural information. \nFor example, in a statically-named language one might use a display in registers if there were many registers \nand a linked list otherwise --the choice depends on a simple consideration and, for a given source language, \nfew possibilities exist. By a kind of conditional compilation of the compiler, one can choose the alternative \nfor a given target machine. These issues are discussed further in [Glanville 77]. Most code optimization \ncan also be done at a stage prior to generation of the IR input, par\u00adticularly if an architectural checklist \nis used. For example, reordering of expression trees to evaluate more com?lex arguments to a binary opera\u00adtor \nfirst, recognition of common subexpressions, etc. can all be done earlier (hence in a portable fashion). \nBy attaching semantic information to the operands in the IR, usage information can be used to guide the \nregister allocator. A slight addition to the IR and the register allocation scheme is required to utilize \ncommon subexpression (CSE) information. A binary opera\u00adtor, O, is added to the IR that takes an integer \nconstant as its first operand and an arbitrary expression as its second. The meaning of this operator \nis that the second operand is a common subexpression thatis to be used the number of times specified \nby its first operand. Each occur\u00adrence of the operator O, i.e. each designation of an expression as a \nCSE, is implicitly numbered sequentially from 1 as it is read. The first place that a CSE is used is \nwhere it is computed, as an ordinary operand. Subsequent uses are indi\u00adcated by a use CSE operator, 8, \nwhich is a unary operator taking a constant as its operand, indi\u00adcating which CSE it represents. For \nexample, the two statements: A:=B+C; D:=B+C; would have an IR representation of: := k.aOk.2 + +k.b+k.c \n:= k.d@k.l Upon encountering a define CSE operator, the code generator sets the use count of the register \ncon\u00ad taining the CSE S value to the number of times that it will be used. Thus, that value will be preserved \nuntil its final use because that regis\u00ad ter will remain busy. This is equivalent to add\u00ad ing a special \ngrammar rule: r.1 ::= (O k.lr.1) for each logical register class to the machine description with the \nsemantics that the use count for register r.1 is set to k.1. The mechanism does not have to actually \nbe implemented in this manner --it is simpler to hard-wire it into the code generator. Likewise, when \na use CSE operator is encountered, the actual register that contains that value is substituted into the \nIR stream, equivalent to a grammar rule of the form r.1 ::= (@k.1) with the associated semantics of using \nthe actual register that contains CSE number k.1 for r.li. As usual, post-coder peephole optimization \ncan also be done (see, for example, [blulf et al 75]). Easy change of the target code is possible only \nif the IF? remains mostly unchanged. In this case, one need only run a new machine description through \nthe table constructor and change the tables. Since the set of inputs must be a subset of the language \ngenerated by each target machine description, our approach does not work well for fundamentally different \narchitectures. It appears that one IR can be used with most general register machines of the IBM 370, \nPDP-11, PDP-10, UNIVAC-1108, variety, but that a different IR might be appropriate for true stack machines. \nSince most of the machines in use today are general register machines, this does not seem to be a serious \ndrawback. It appears that this approach to code genera\u00adtion is useful even in a compiler that is not \nto be retargeted. By incorporated the decision logic in the coding algorithm and the machine description \nin a table, the size of the synthesis phase of the compiler should be reduced. In addition, we con\u00adjecture \nthat a compiler organized in this fashion will be easier to modify and maintain. Experimental Results \nA code generator and table constructor were implemented in PASCAL. Several target computer descriptions, \nincluding the IBM 370 and the POP-11 were input to the table constructor. (Our first experiments were \nfor the PDP-11. The change to the IBhl 370 required little more than a new description like that of Fig. \n1 and took about one hour.) PASCAL source programs were hand-translated into IR and input to the code \ngenerator. Although our experiments to date have not been extensive, it appears that the quality of the \nresulting code is very good. The PDP-11 code generator produces code that is competitive with the compiler \nfor C, the Unix systems programming language [Ritchie 77]. It does this without any kind of prior optimization \nof the IR input. While its primary task is code generation, and it is not intended to be an opti\u00admizer, \nthe code generator is particularly good at finding specialized instruction patterns, such as increment \nor add to memory, in the IR. Since our POP-11 code generator produces assembly code (as do most Unix \ncompilers), the short jump instructions, etc. , of the Unix Assembler as well as the peephole optimizer \nused by the C compiler can also be exploited. Similar remarks apply to the code generator for the IBM \n370. References Aho , A.V., Johnson, S.C. &#38; Unman, J.D., Determin\u00adistic parsing of ambiguous grammars, \nCACM18:8 (August 1975), 441-452. Aho , A.V. &#38; Johnson, S.C., Optimal code generation for expression \ntrees, JACM 23:3 (July 1976), 488-501. Aho , A.V. &#38; Unman, J.D., Principles of Corn iler Design, \nAddison-klesley, Reading, -*MA Doneaan. M.K.. An acmroach to the automatic qenera\u00ad tion of code generators, \nPh.D. Thesis, Rice University, Houston, Texas, May 1973. Elson. M. &#38; Rake, S.T., Code generation \ntechniques for large-language compiiers, IBM Sys. ~. 9:3 (1970). 166-188. Glanv~lle, R.S. , A machine-independent \nalgorithm for code generation and its use in retargeta\u00adble compilers, Ph.D. Thesis, Computer Science \nDivision, University of California, Berkeley, November 1977. Meyers, M.J., Linear representations of \ntree struc\u00adture: a mathematical theory of parenthesis\u00adfree notations, Technical Report STAN-CS-74\u00ad222, \nComputer Science Department, Stanford University, Palo Alto, CA (July 1974). Miller, P.L., Automatic \ncreation of a code genera\u00adtor from a machine description, Technical Report MAC TR-85, Project MAC, MIT, \nCambridge, MA (May 1971). Pool@, P. C., Portable and adaptable compilers, in Compiler Construction. An \nAdvanced Course, G. Goos &#38; J. Hartmanis.>ds. , Lectures Notes in Computer Science, vol. 21; Springer-Verlag, \nNew York (1974), 427-497. Ritchie, D.PI., C Reference Manual, Bell Laborato\u00adries, Nurray Hill, N.J. \n(April 1977). Steel, T.B. Jr., A first version of UNCOL, =. MJCC 19 (1961), 371-378. Strong, J. et al, \nThe problem of programming com\u00admunication with changing machines: a proposed solution, CACN 1:8 (August \n1958). Ileingart, S.W., An efficient and systematic method of compiler code generation, Ph.D. Thesis, \nYale University, New Haven, CT, 1973. \\~Jelsh, J. &#38; Quinn, C., A PASCAL compiler fOr ICL 1900 series \ncomputer, Software: Practice and Experience 2:1 (Jan.-Nar. 1972), 73-77. blilcox, T.R., Generating machine \ncode for high\u00adlevel programming languages, Technical Report 71-103, Department of Computer Science, Cornell \nUniversity, Ithaca, NY (September 1971). !Julf, !4. et al., The Design of an ~ Compiler, American Else~er \nPublishing Co., Inc. , New York (1975).\n\t\t\t", "proc_id": "512760", "abstract": "An algorithm is given to translate a relatively low-level intermediate representation of a program into assembly code or machine code for a target computer. The algorithm is table driven. A construction algorithm is used to produce the table from a functional description of the target machine. The method produces high quality code for many commercially available computers. By replacing the table, it is possible to retarget a compiler for another kind of computer. In addition techniques are given to prove the correctness of the translator.", "authors": [{"name": "R. Steven Glanville", "author_profile_id": "81100520834", "affiliation": "University of California, Berkeley, California and Hewlett-Packard, Santa Clara, CA", "person_id": "PP39046722", "email_address": "", "orcid_id": ""}, {"name": "Susan L. Graham", "author_profile_id": "81452606376", "affiliation": "University of California, Berkeley, California", "person_id": "PP14173434", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/512760.512785", "year": "1978", "article_id": "512785", "conference": "POPL", "title": "A new method for compiler code generation", "url": "http://dl.acm.org/citation.cfm?id=512785"}