{"article_publication_date": "01-01-1978", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee.&#38;#169; \n1978 ACM 0-12345-678-9 $5.00 COXIferenCe Record of the Fifth Annual ACM Symposium on Principle% of Progranuuirig \nLanguages A FORWARD MOVE ALGORITHM FOR LR ERROR RECOVERY Thomas J. Pennello Frank DeRemer Board of \nStudies in Information Sciences University of California Sanka Cruz, CA. 95064 Abstract. A lforward nove \nalgorithm , and some of its formal properties, is presented for use in a practical syntactic error recovery \nscheme for LR parsers. The algorithm fir.ds tvalid fraqm9nt (comparable to a val;d prefix) just to the \nright of a poict. of error detectio~.. For expositional purposes the algorithm is presented as parsing \narbitrarily far beyond the point of error detsction in a parallel mode, as long as all parses agree on \nthe read or reduce action to be taken at each parse step. Ic practice the forvard wove is achieved serially \nby adding tzecovery statestt to the LR machine. Based on the formal properties of the Eoruard move UP \npropose an error recovery algorithm that uses the accumulated right context. The performance of the recovery \nalgorii.hm is illust.rste?l in a specific case * nd discuss~d in g~neral. Key words and phrases: syntax \nerrors, error recovery, psrsing, LE?(k), SLR(IS), LALR(k). CR categories: 4.12, 4.42, 5.23. Over the \npast tuenty years much effort. has been invested into the science of deterministic parsing: that. is, \ndetermining tb. e phrase structure of a sentence generated by a context-free gcam.aE [H&#38;U 69] 3uricq \na single scan, usua lly from left to right. The t Uo pinnacles of this research are the U(k) and LR (k) \ngrammars and their parsers, respectively t~p-down and bottom-up techniques [AF.U 72]. Unfortunately, \nthe more adept parsing techniques have gotten, the more difficult it has seened to achieve flexible error \nsecovery. It seems that the more the parser knows shout the input possibilities and specializes itself \nvia state transitions to restricted parts of itself, the more difficult it is for it, in the face of \na detected error, to back out and get global information necessary for goo!l error recovery. In the vords \nof Graham and Rhodes [G&#38;F 75]: ItThe fa ct that the next move of the parser c an depend on the entire \ncorrect prefix already analyzed makes it difficult or impossible to start up the parser after th~ error \n[detection] point ?. This paper is a contribution toward giving LR parsers some such global capabilities. \nZndeed, we show that it easy to extend them to start up after an error is 6etected and to parse arbitrarily \nfar ahead, gatherinq right context. This context can then be used to guide the selection and evaluation \nof repair attempts. Thus, ue decompose the notion of error recovery icto (1) qathering right context \nand (2) a repair strategy. ~~~~gy. Graham and Rhodes [GZR 75] propose~ an error recovery scheme for deterministic \nbottom-up parsers that involves lqcondensinq contex t about the point at which an error vas detected. \n1 ~~back~ard move q condenses cont~xt to the left and a- %forvard move gathers cont~xt to the right. \nSuch context is valuable input to an error repair strategy. Graham and Rhodes show how the condensation \nis done for simple precedence pars+rs and give an error repair strategy that uses the condensed context. \nUe have adapted the general idea of Gzaham and Rhodes to LR parsers [ Ai5J 7~ ], by which we mean LR \n(k) parsers and all their variants: LALR (k), SLR (k) , etc. Some of the i~vestigation has already been \nreported io Pennello s !lasterWs thesis [Pen 77] (see also rO H 76]). The present paper repeats some \nof the thcor,=tical results and adds some algorithms and some empirical results from a recent implementation. \nBriefly, we found the I back. uard novegf to be detrimental in en~ugh cases thst we abandoned it, in \nfavor of a philosphy of trying to do only what is consistent with every context for as long as possible, \nresorting to guesses only when ve know of no othe r way to proceed. Pennollo }~Parallel parsese exposition \nOfdeveloped a the ~forward movett for LR parsers that facilitates ur.derstanding and proof of results, \nand we show how to ~serialize it so that in practice the parser simply has some extra ~8recovery states!$ \nthat work just as the other states do, but are rfwovery mode. several theorems were developed, primarily \nentered only in valid fragment frelating to th~ derived accumulated during the forward move. Druseikis \nand Ripley [D&#38;R 77] ve ha which werepozted so mP similar results note as the issues arise. Preview. \nWe first present the forward .-.- y move algor~thm (FNA) in its parallel form. Then we state several \nproperties of FflA Thesethat are relevant to error repair. properties suggesk ways that the forward context \nmay be used in a repair strategy. Perhaps the most impocta~t of these indicates that the forward context \nmay be used to efficiently verify that a repair atkempt is ~consistent$~ with the input text parsed by \nFfiA. next we present. several ~lgorithms that are used in the repair strategy. ?he analysisthzr~ starts \nwith the simplifying assumption of but a single err ofor insertion, replacement, or deletion of a single \nterminal symbol. ?be effect of delayed versus immediate detection is discussed, and then multiple errors \nare treated. For clarity and simplicity the algorithms zre presented without regard to cert ain practicalities, \nuhich are then discussed in the text. Finally, we show how to serialize FBA fos a practical implementation. \nWe present soiue statistics on how aany extra states are needed for some well known programming languages. \nIhen we demonstrate hon our error recovery performed on the example Algol program of Graham and Rhodes \n[G6R 75]. Terminolo~~. -= --------_ we revi~w ba sic notation and terminology for strings, grammars, \nand parsers. A ~g~gm~ggy (or alphabet) V is --..--\u00ad a finite set. of symbols. V* d~;l~t,~~ the set of \nall stcings of symbols from V. V+ denotes V* lQss s , tl]fi CZmpty ~trin~. Tf x is a non~mpty string, \nFirst x d;notes the first symbol of x and Rest x denotes x stripped OE its first Syubol. ( Uf? typically \n30 not put parentheses around arguments t.o functions when the m~aning is clear, as above.) A context-free \n$lc~~~~:he :.rm:;*lsa quadruple (N,T,S,P), i.e. ------____ # nonterminals,start ~ymbol, and - - _--- \n___ ----\u00ad&#38;XQ~~~;~g~~, respectively; ;~-~efine V = NUT. Each production is a pair (A,w), leftpg~~ \nand ~~gh~ part, written A -> W, uhere h in !$ and n ~~ -V*.-> is the g~g~~~h~~-~erivation relation, \nin which the rightmost no~terminal is replaced at each step; ->+ is its transitive closure > -------\u00ad \n->* is and its tEar!Sit ive-reflexive closure. We assume a production S -> Ssl ~Q P, where S$ ~~ N, 1 \n&#38; ! , and neither S nor 1 appear in any other production. The h~qqug~ generated by G is L(G) = {w \n~~ T* { s->+ w]. An LR parser, i.e. an LR(l) , LALR(l), SLR(I), or other such parser [A&#38;J 74],-for \nG=(N,T,S,P) is a sextuple (K,V,P,S!L AE2, SIGHA,REDUC!?) where K is a finite set of states START &#38;q \nK is the ------r star t stat e, SIGI IA is the trazsiti~n -----_____ function mappicg K x V into K, a~z--~~f)!?? \n ----_--- P maps K xVinto 2 . If SIGMA(q,h) =p, ve also write this as the ~g~~g~nig~ 9 --h---> p. From \nSIGHA and REDUCE me derive the p~g~gg fleci~ion function PI), --_-A___ -------- M mapping K x V into \n2 where x = [read, accept] U P; PD indicates, for a given stat e and input symbol, all possible actions \nth~ parser may take; if the grammar G is LR, PD aluays yields at ruost a singleton set. PD is defined \nas PD(q,h) J--> ~s {accept I h = 1 and q for some q :2 K} U REDUCE(g,h) . In figures we represent LR \nparsers as state diagrams i r. which states are connected by arcs labellefl with elements of V, according \nto SIGtlA. For each state in which REDHCE indicates a possible choice of a reduction by production p, \nwe list p and its l-symbol look-ahead ~g~* -------__________ [h ~~ V i P :Q REDUcE:&#38;h)]. Figure 1 \ndepicts a state diagram the LALR(l) pars er for a commoz arithmetic expression graumar; in this figure, \nfor Qxample, PD(**O,i) = [read], SIGflA(**O,i) = i and PD(iO,+) = [P -> i}. Fig. 1. A simple arithmetic \nexpression NotP t.hiit.and Pi) IIlay a REI)IJC!? take a ~.~~ ,>n(~ nontcrrninal as a I-gum er]t.. LR \nparser constructor algorithms e:tsiiy generalize to inrlut?e no;~t.erninals in look-ahead sets. 2!? assti~e \nthnir inclusion, hut also give alt?rrate means of implementing the results of this paper should their \nipclusion in so m~ implem~ritation he too difficult. Also note that in Figure 7, ther~ h~ppen to bb no \nnonterminals ir, look-ahead sets duz to the nature of the grammar. A pg~~ P in an LR parser is a sequerice \nof states !I1 r . . . . qn Sucti that % -:1--.> ql, @--> g2, . . . ,$10 n -----> g Me define Top P = \nqn. Me n.l n say that P g~ells u -Ue 1 2 . n; sp~lllng p = We AXI define ---_~-alternate notation for \nP is cqO :w], given the parser diagram. we abbreviate F?TARCR] ;~a~;]; thus [ ] denotes START alone. \nWe say that w accesses q iff Top ~------\u00ad[W]=g. The concatenation of two paths [q:yl and [q :y ] where \nTop [q:y]= q* is xr~tten [qsy][g~!yl] and d~notes [9:YY~l-If, for sora$? q, q*, and h, SIGFIA(q$,h) = \nthen i!~~~~~~gg-~ymbol q = h -(the accessing syuihol for ---each state is unique) . {+, ), LI:T-P**T \n{+, ),.L,**): P+i grammar and its LALR(l) parser. An LR parser ~~g~$ggratio~ is a pair (Z,R) where \nZ is a path and R ~g T+. For example, wh?n the first three symbols of the sentence i+i+i~ have been reduced \nto E+? by the parser of Figure 1, it is in the configuration ([E+T],+il) = ( (START,E ~ ,+ ~ ,T ~)8+il). \nThe parser moves from one configuration to the next by reading or reducing: thus ve define a ~py~ as \nan element of the set {read} u P. I\u00ad denotes a move from one configuration to another; [-+ is its transitive \nclosure and I-* is its transitive-reflexive closure. We sometimes use 1-. . . 1-for 1-+. If c 1-Co by \nmove n, we sometimes urit e c Ifi c . We define 1-as follows: Given some ([q:Yl#R), consider the possible \nvalues of M = PD(Top [q:y],First R) : case (read}: Let h = First l?. ?hen ([q:Yl,R) 1-([q:yh],Rest R). \nread case [A -> w]: If [q:y] = [q:y w] for some then ([q:Y],R) ([q:y ~;;R)\u00adA- >w case {accePt) or {) \nor Ifll > 1: There is no C such that ([q:y],R) 1-c. (I.e. the parser makes no MOVC hut instead halts, \nacc~ptinq or rejecting thr input, or unakle to proceed det.crminist.ically. ) The ~~gqgugo recoqnizcd \nby an LR parser is (w ~H ~+ 1 ([ ],w) {-+ QGg~L&#38;] where we ah bIevi. ate ([s ],1) hy ,3(-C+Q.(Note \nthat PD(Top [S ],1) = {accept] .) Th~ USU3 1 LR parsing algorithm is easily deduced from the 1-relation. \nA final note: VP present algorithms that ma y return results in two different ways : via a return value \nand/or via Ilresult!l paramet.eES. E.g., the so-called ~,if F(x,y,Z) ~~~~~ A* ~ -. tests phra se the \nbooi;a~ result. of F which is called with the three input expr~ssio~s (actual parameters) x, y, and z, \nand uith the two result parameters, A and 9; some sid~-effect will happen to A and B according to the \ndefi.ni.tion of F. Returning from such a function is ~return indicated by a statement such as ------ \nTrue g~~j~g u, v . Uben an error i.s detected, we wish t-o perform a Vgforward movetf that parses the \ninput after the point of the error detection.. The parse cannot depend upon the left context already \ndeveloped on the stack to proceed, since it is precisely that left context that causes the parser to \ndetect the error. ?hus, ue d~vise an algorithm that parses ahead starting vith no left context. our formulation \nof the f~forward move algorithm~t kPeps parsing input ---\u00ad ~Qx~ until it must refer to the 8~missing4v \nleft context to proceed. At that point it halts, and we use the developed foruard context in an error \nrepair strategy. vhic ConsZder h the an symbol Algol-like lldolt can language appear in in a wforn or \n ~~while{l construct. Suppose the trno productions i.nvolvi.~g these constructs are Stint -> for Id \n:= Exp step Exp u~til Exp do Stint (1) Stint > while Exp do Stat (2) where we capitalized nonterminals \nand left terminals u~capitaliz~d. Now consider the erroneous phrase for X ?= 1 step 1 un+<l .30 begin \nJ := x end where ue have omitted th+ limiting nforll expression in the construct. The forvard move, if \nstarted with its input head at the symbol dolt, reduces l*do begin J :=x end~! to Zt goes no Ildo Stmtll. \nfurther than this because it neeh tp know left context to determine whether at this point i.t must reduce \nby production (1) or (2) (each of which -end in l~do stmt~a, not coincidentally) . The reason t-he forward \nmove can parse this much of the input is because in both places that do appears in thp grammar, it is \nfollowed im~ediately by Stnt. Thus, lldOll is context to th~ left of the Rot necessary to reduce Ildo \nheqin J := X end tO ~~aO stnt~ . This Situatiofi occurs often enough in programming languages that it \nis not uncommon for the forward mov~ t.o ma ke quite some progress in thp input text before it needs \nto refer to left context. The essential idea of our algorithm is to carry out all possible parses of \nthe input text, as long as all parses agree as to the next move to make (i.e. they must all manipulate \nthe stack in the same way at each parse step) and no parse rpfers to nonexistent left context. We present \nthe algorithn first as having a stack upon which we push sets of states ra ther than states; the se sets \nof states keep track of the parallel parses. At each step of the forward move we inquire of each state \nin the set OF the top of the stack what its decision is with regard to the next symbol in the input. \nIf all the states in that top state set that accept the next input symbol agree as to the next m-oVe \n* ana this next move does not refer to nonexistent left context, then we make that next movi=. For exanplen \nin the case of the llreadst move, we push OR the stack the set of all the states that can be reached \nfrom any state in the tO~ state set by taking a transition on the next input symbol. Of course, manipulating \nsets of states is not practical, but we shLc)w how the forward nove algorithm can be easily converted \ninto an algorithm that manipulates states only, essentially Ii ke the conversion .of a fiondeterministic \nfinite-state machine to a deterministic one. The converted algorithm is as fast as the LR parsing algorithm. \nLet ? he the set. K of all the parser states. We now present the forward nove algorithm. The algorithm \nhas an initialization step that causes it to consume at least one symbol of the input, followea by repeated \nparse steps.  ~9S!C&#38;M! Fo$ward_move (FnA) ~~m~ R \u00adthe remaini~q input 2!QP3J the forward context \ndeveloped and the input not consumed Init: 2Q Z be the stack consisting only of ? First R Push [q? \\ \nq -----> q , q ~E ?} on Z R <- Rest R repeat-\u00ad--\u00ad ~~~ h = First R, Q = op Z, and tlovEs = u PO (qch) \n q~CQ select NO VES: ----.\u00adgmg [rPad}: --,, --> qf and q &#38;Q Q} Push (q Iq O!l z  Push {qC I q -A---> \nql and qLETop Z)onZ else return (Spelling Z,R) ~$ # w does not reside on the stack ~Q~~ [] # We hit an \nerror QK [accept] #Risl QL otherwise: # IPDI > 1 return (Spelling Z,R)  end re~eat end FfiA - Notice \nthat the repeated parse steps of FHA are identical to those that the parser normally follows, save the \nuotherwiset~ caseg the manipulation of sets of states instead of states, an d the check, just prior to \na re3uctio~, that the entire right part resides on the stack. FMA essentially follows all paths that \nallow the parsing of the input text. It halts in case ~lotherwise!t when t Wo differe~t paths end up \nin states that disagree as-to how to continue the parse, or in case [A -> w} when all paths end up thein \nstates requirinq a reduction over 9. . or ixi c-%se [accept?.-vhen We read the entire input, or in case \n{1 when we encounter another error, i. e. no path can be continued. The set ?IOVES computes by FUA represents \nall the possible ways that tbe states in the top state set Q wish to tres t the input svmbol h. Note \nthat states q AS Q ~hat c~nnot accept h (i. e. for which PD(q,h) = {1) have no effect on the parsifig \ndecision unless all States in Q cannot accept h (case [1): we extend each path as far as we can, even \nthough other paths terminate. illustrate the halts of cases wotheruisett hy EXamplt?s 1 and [A -~ew] \nan3 2 below, where the parser of concern 1S that of Figure 1. Example 1. Let the erroneous input string \nhe i(i) 1. The parser stops with state stack [i]. The followinq displays the execution of FNA on the \nrczaind~r of th~ input. FflA step Stack after Best of just made FllA step input -----------------------\u00ad \n-------\u00ad -------\u00ad {P -> i) ? ((0) ($)} )1 (T-> P} ? (~) [~ol )1 [E > T} ? [~1 (~1 )1 [read} ? (~1 {~1 \n[)~1 1 (P -> (E)} ? {Po) 1 (T-> P} L ? TO T1 T2) The algorithm halts here because PD(T.O,l) u p~(~l,l) \nU PD(T2,1) ={E->E+T, T -> P** T, E-> T] Of c~urse, the expression between the parentheses could have \nbeen arbitrarily long with the same result. ~~gg~e 2. Input is 01. The parser halts with-state stack \n[(]. PEA step Stack Rest --------------------.--------------. -- Xnit ? [)03 1 Halt: pD(lo,l) = [P -> \n( E )], and there are less than three items on the stack above the ?. ~n Example 1, we face the possibilities \nof reducing by three different productions. E -> T is the prop er reduction only if what immediately \nprecedes the T is a 11( u or nothing; E -> E + T is the proper reduction only if what immediately precedes \nthe T is IIE +lt; and T -> P ** T is correct only if WP **II preceaes the T; but no context exists to \nthe left of {T o,Tl,T21-Th US, we cannot continue parsing without making a guess, and must halt. Iri \neffect, the three different situations in the parser in which it can read a T yield three ciifferent \ndecisions as to vhat. to do with the T. Tn Example 2, W* attempt to reduce with P->(E), but find that \n!( !?W does II 11 not preceae ) on the stack. The attempted reauction gives us an indication of what \nthe user intended, however, and may provide useful information for an error repai r strategy called rstack \nforcingts, as we explain in the next section. Init ? [~1 i)l The initialization step Tnit of FHA [read} \n? [~) {io) )1 guarantees that the algorithm produces a forward context of lf?n(; th at least. onf?. If \nwe c?id not cause FMA to read the first symbole th~n it. Uould consider all reductions that have the \nfirst synbol in their look-ahead s~ts; possible choices between a read and sorm reductions might have \ncaused FMA to halt immediately i.n case ttothp~~is~lg, making no progress whatsoever. (we assune also \nfor the remainder of this paper that we never invoke FMA on the input consisting anly of 1, otherwise \nwe would immediately read L in step Init.) In section 5 ue precompute the state sets of FH&#38; as states. \nThis allows us to extend the concepts of transitions and paths tO Fl?ii s stzte sets. Hence. if FBA consumes \ntext u from string uv and produces forward context U, we !nay uri.te (?,UV) ~:* ([?:U],V). The relation \nl-- FMA: \\ that can d~duced from EEA is ?xactly the same as that of the LR parsing algorithm, but to \nprevent confusion between the LP parsing algorithm and F19A, we prefix moves of F19A by lFMA: v, as above. \nSUppOSf? FMA: (?,UV) 1-* ([?:IJ],V). u satisfies important properties that we explore in this section. \nEssentially, U is such that during a parse of any sentence endi~g in UV, u must be reduced to u. We formalize \nand indicate the significance of this property in this section. ?0 do SO, we define some new terminology. \n1! yg~ifi Qrpfix is any prefix of yu, where S ->* yAv > YWV for some y ~, V*, A-> w~~P, and v in ?*. \nThe string spelled by the stack at any point during l.R pacsing is a valid prefix. A y!~~$ ~g~~mcmt is \na suffix of a valid prefix; i.e. valid fragmei;ts are suffixes of the stri nqs spelled by the pazs~rgs \nstack. For example, for the grammar of Figure 1, E ->* E+p~+i -> E+(l?)**i so any prefix of E+(E) is \na valid prefix, e.g. E+(, and any suffix of E+{ is a valid fragment, e. g. + (. We now define the concept \ncentral to this paper. Definition ~. U in V* is a derived . ----------------. valzd ~gggg~~$ (p~~)-~f \nsentenc+ suffix x iff (1) U->* u and x= uv for some u, v in T*, and  (2) for every vali~-prefix y such \nthat  ([ YI,UV) 1--. 1-Igscl?l, read ([YI,UV) I-* ([yll],v).  Thus , during a p.%rse of any sc.ntence \nending in uv, at so me poirit the parser must reduce u to the? valid fraqmer t U. (The requirement that \nthe first I-is I\u00adread rela t-s t.o the fact that TEA reads as its first move.) In the context of error \nrecovory, this concept Las the followinq significance: Suppose the parser encounters an error and halts \nin cortfigur~tion (ZrUV) with uv a suffix of a sen+ence, and that an error repa ir algorithm S,uqgests \n[y ] as a possible replacement for Z. We could verify that ([ Y ],nv) 1-* 2SSRP~ by actually tryi~g the \nparse, but if many such [y ]s were to be tested, reparsing uv each time would be Costl y. The significance \nof having sone DVF T of uv is that in U we have a partially parsedll need version of u and Rot repeat \nthis partial parse, for the DVF property states that u must b~ reduced to Q no matter uhat the string \nto the left of UV. A necessary (not sufficient) confiition Y I,uv) I-* accep) is as follOWS: that ([ \n----Let y be such that ([y ],uv) I-* ([Ylruv) and PD{TOp [y],First uv) = {read]; then it must be the \ncase that {[YI?U$ ) I-* ([ YJJl#v) . This is by definition of a DVF acce~$only if ([y~,tiv) I-* ~gg~p~. \nFor any gzven [yt], then, this requires only that we conpute [y] and deterinine whether a path exists \nfrom Top [y] spelling U. Determining the existence of the path [yU] is considerably cheaper than reparsing \nu if u is much longer than U, and gives us an inexpensive test to determine if the proposed stack repair \nand due to the fact that ([Y lruv) I-* [Y ] is good enough to cause the parser to consume u. For futuw \nconvenience we preserLt t he algorithm Zonsume-DVF that performs the computation just described. &#38;~gco~hE \nConsume_DVF inpg~ {q:y] and u --a path and forward context. output a boolean value --indicating whether \n[q:y] can consume IJ -\u00adand g~~~gg either the successfully computed path or an error message. while ([g:Y],U) \n~-(Z.u) for 3 &#38; P (the productions) and for some path Z @ y <-SPell~nq z @ # i.e. reduce.  ~~~ MOVES \n= PD(Top [g:y], First U) ~~ MOVES # [read) ~~f?~ ZCAGEE False %il!~!lg [9:Y] gi # Now ue must Le able \nt-o find a path ~~ path [q:yiJ] exists ~~~~ returnTKUQ nivinl [q:YU] else return False giving path \nended in error ! f~ end Consume-DVF Not~ that the ~ffwt. is the sane when FMA does not always compute \nth~ MDVF. the computation of [q:y] is basefi on First This is because IMA is restricted to tlsing U rather \nthan First u, namely, if pa th the sane parsing technique as the parser [q:yU] exists hot.h m~t.ho,ls \nproduce the aud therefore to fitite look-ahead. An same result. HowFv~r, if [q:y!l] dons not algorithm \nsuperior to FMA might sca n all exis t, usi~ g First [J rather than First u of x, perhaps fiiscoverinq \nsome aay cause the situatior. to be detertefl contextually sigr.ificant symbol located earlier: !IOVFS \nmight l!Ot bQ equal to tow~ rds the end of x that could help it (read). Me get this earlier detection!$ \nparse earlier text. An FJIA bas~d on an capability because First U may be a SLR machine might be bested \nby one based nonterminal representing not only First u on an LALR machine for the same qramrnar. but \nalso some text to its right, and hence But given the limitation of th~ base First u may be a member of \nlook-ahead sets parser, F3A does as hcst it can. These of which First [J is not a member. !ioke restrictions \nare encoded in the following further that Consun~-DVF takes axy path as theorem that formaliz~s FtfIl*s \nperforruance its first argument, rather than just. paths in terms its base pars~r. beginnirtg at STAP?; \nthis is because we eventually intend to use it with paths produced by FflA also. Theorem ~. Consider \nsuffix uv of a -- --\u00adsentence. If there ~xists integer r >= 1 and a sequence of mov?s Mr.. Yr suck that \nOur nrain theorem, the proof of which is in [p~n 773, is that the forward (i) El = read, context returned \nby F?IA is a DVF: (ii) there exists some state q Q K and U &#38; V* such that g~gnlggg ~. For some sentence \nsuffix (q,uv) 1-. . . -([q:U],v), and UV, if FMA:(?,uv) I-* ([?:c],v), then U is LY, h. a DVF of UV. \n(iii) there ex&#38;&#38; no valid prefix y, ifiteg~r k < r, and configurations This theorem results, \nintuitively, from C and C such that the fact that FMA parses uv with no initial left context. Thus U \nmust be reduced to u no matter what the left context of u. then Frna:(?,uv) 1-. . . ~- ([?: U], V). y \n 1r We claimed in sectio~ 2 that F1 IA parses as much as it can until it must refer to ~------\u00ad no~existent \nleft context. We Corollary. FXA applied to a sentence formalize this intuitio~ below. Suffzx makes the \ngreatest number r of moves as possible, where r is as defined in Theorem 2. Qgg&#38;_h?:g&#38;~~ ~. U \n~~ V* is the naximal DVF (KDVF) of sentence suffix x iff the ~--\u00adfollownig three conditions inlply that \nApart from the significance of DVF*S ([yUS],V*) I-* ((YIJ],v): in validating error repairs, DVF?S sati,sfy \nother useful properties. The I next moven (1) U is a DVF of x where U ->* u property is helpful in selecting \nerror and x = uv for some u, v Q T*, repairs. Let uv be a sentence suffix, and (2) U is any other DVF \nof x where U be the DVF of uv returned by FllA. u! ->* ut and x=u~vo  for some u , V8 ~~ T*, ROVES= \n{M I ([YU],V) ); c (3) there exists y such that  u for some C] ([Yl#u@ 1----1-qgQJJJ. read y ~lJ v* \n [yU] a valid prefix Thus, by the definition of DVF~s, In other words, MOVES contains all the moves \nthat the parser may make from some MDVF ~!as far up the configuration ([Y~ l,v). Intuitively, derivation \ntree of yuv as possible. v since FMA parses uithout knowing y, at must be a suffix of v*, so that (u1 \n>= each step !IOVES represents the set of lull, i.e. U derives the longest possible noves for all possible \nyes. prefix of X. ~fv= V* then we see that U is reduced as much as possible, since then u ->* u . &#38;n \nalqorithrn that produces the The utility of the next Iuove property MDVF would thereforq as far as it \nis illustrated as follows. For the Algol $hg:tuv) an l-~ (\\Y~;:#) I-* ([ YJJ],V), so road could into \ntho input, ard reduce. as much example of the previous section, MOVES as it could. Woul a conta in the \ntwo mov~s indicating f!=~du~~ by production (1)$ and reduce by production (2) , where U = do Stmttl. \nThe next move ~Jropec+y says that if WP find som~ such that the parser m~k~s move H Y from con fiqurilt \nion ([ yU ],v) , then H must be one Ot thosi> two reductions. Either reductian puts a constraint on y: \nit U!Ust end in Pit her l~for Td := Exp Step ??Xp Until EXp Or u}lile ~x~ . UP may thus sometimes use \nthe elements of MOVESto guide us i.n thp selection of yts. We call these rcducti. ors long reduct. ions \ne because i.f per forrne~ during the forward move, they would attempt. to pop the ? state set (in the \nhlgol example F?!A halted in case t$otherwiset~) . such long reductions can sometimes provide instant \nsolutions t to some errors. Xn this example, a cornparisoo of the stack with the set MOVES shows that \nwe should patch up the stack by inserting the missicg Exp anfi continue. Iri practice, we Ray sinply \nsearch the stack pr~ceding the point of error detection for some state that can read the left part of \neither production (we call this technique Ilstack forcing ) . we mention this further in the next s~ction. \nMOVES may contain elements that are not lonq reductior.s, such as read~t or a ~Ssho rt reduction , but \nwe do not yet know ho!i best to make use of this information. Ne formalize the next move property as \nfollous: ~l)$gcm~g s. Let x be a sentence suffix, U be a 13VF of x such that U >* u and x = Uv, and MOVES \n= u PD(q,First v). Then q @ ToP[?:~l (1) MOVES = {M I ([ YIJI,V) i; c u for some C} y :2 v* [yU] a valid \nprefix  (2) IMOVESI >= 1 (For a similar result see ~D&#38;R 77].) We can further use the next no ve \nproperty. to help pinpoint errors. If FI!A(?,uv) I-* ([?:U],V) but halts in case (). i-e-HOVES = {}., \nthen uv is not a sentence suffix. That Ls, an error has occurred somewhere in the text UV, because there \nexists no y such that S ->* yuv. More specifical ly, since we are dealing with LR(l) parsers, the error \nhas ocurred in the !Iwindowt! comprised of the first IUI+l symbols of UV. In summary, we have shown that \nFMA (1) provides an inexpensive test for stack replacements, (2) sometimes points us directly to the \nrepair we need to continue the parse, and (3) sometimes finds a *uindou$$ within which an error has occurred. \nWe do not kr.ov how, in the general case, to come up with stack replacements. In a more specialized case \nin which we assume som~ knowledge of the types of errors, ue have a chance of designing stack replacements. \nu ~~pair ~~rat.~gies Using !Vl~ .s -. ----------- ---- Given F!IA and its formal properties we now proceed \nto develop an algorithm that finds a uses ble configuration in which to restart the parser. In our initial \nanalysis we make the rsimple error assumption~ (SEA), viz. the non-s~ntence z in question resulted from \na sentence via a single !~mutilatior. !$: an i.fisertion, a replacement, or a deletion of a sing le terminal \nsymbol. Tnsertion: z = yt.x and S ->* yx but not S ->* ytx Replacement: z = ytx and S ->* yt~x but not \nS ->* Y tx Deletion: z = yx azd S->* ytx but not S->* yx In the next few paragraphs we assume an LR(k), \nas opposed to sLR(k) or LALR(k) , parser and we even assume that the parser detects the error at the \npoint of mutilation. Then we generalize graduslly and discuss the consequences. Suppose the parser detects \nan error in configuration (Z,tx) . Thus, t is an unexpected symbol in the left context spelled by 2. \nSuppose further that we have reason to believe that an insertion of t occurred. HOW could we confirm \nthat suspicion? P straight-forward way is simply to determin~ if [Z,x) I-* acce~~; ---= i. e. delete \nt a nd resume parsl~g. Similarly, if we thought the mutilation was the replacement of some terminal te \nby t, we must resume with (Z,t x), and if the deletion of soroe t~ just prior to t, then (Z,t tx). NOW; \nin the error recovery context we have no clue as to which of the above repairs may nork, so we must try \nthem all. Furthermore, if none of them work, we can cor!clude for an X,R(k) parser and un4er SEA that \nthe mutilation occurred left of the point of error detection, i.e. the parser somehov incorporated the \nmutilation on its stack. In the case of an SLR(k) or LAI.R (k) parser, even if the correct unmutilationvt \nis founti, the above trials may not work since the parser nay have made reductions (by looking ahead \nat or ignoring the unexpected symbol) that the corresponding Ll?(k) parser would not have nade. Repairs \nin these cases will involve some form of hacking up the parser. But before considering those implications, \nlet us consider the use of F14A to reduce the cost of trial parses. To limit the repeated parsicg of \nx we apply FlfA to x recursiv~ly until it has been reduced to DVI? segments Ul,..., [Jn. We call th \nis process F!IA+, which can be defined as follows: FPIA+ (x) = ~g x = ~ thf=r! 1 (2]s!? U such that F:IL: \n(?, x) 1-* ([?: U], V) followed by FMA+ (v) Furthermore, hefor~ trying the insertions, not having found \na deletion or replacement that will uork, we may att.ach ~ t to the extended forward coritext~l U1 . \n. . u * by using Consumt?_DVF above, thus producing ..,, U m for sorue m ~ n+l. We have 1 m = n+l if \nt cannot he attached to v1; m = n if t can be attached to l!l, but the resulting DVF cannot be attached \nto U2, m = n-l if t. Ul, aml u2 get combined but what we mean by ot 3; and 0 nw attachW is specified \nby the following algorithm. algorithm Attach Gp~~-L;-c --the symbol to he attached and the sequence of \nDVFS to attach it to. output a 13001pan value, and givin~ - \u00adthe resulting sequence of DVFS. let Pbe \na path -such that Consurae-DVF(?@h) g~~~~ P while C is not. null ~~ ~g~ Pt be a path variable ~~ Consume-DVF \n(P, First C) ~ves P? then p <-PI ;c<-Pest c -\u00adelseif P~ = #lPath ends in errOIY then return False --------T\u00ad# \nnot g~ving anything; irrelevant. --\u00adelse ggggg~ True cJ~~&#38;gg Augment(Spelling P, C) Q $)) gg?g~~ \nTrue giving Spelling P end Attach ,--\u00ad ln an above WP have assumed the operation, Augment, on sequences \nthat !rovides a sequence of length n+? by adding a neir element, the left operafid, to the front (left) \nof a sequence of length n, the right operand. Non-imEediate detection. Suppose that -------- -. _ -----__ \n\u00adnone of the deletions, replacem~nts, or insertions succeed. An easy uay to proceed n~xt is to start \nbacking dovn the stack, on Q symbol at a tim~, trying deletions and replacem~~ts of Path symbol h, then \nif none of these succeed, attachi~g h to the previous extendetl forwacd cont~xt and trying ins~rkions \nin front of h, just as we did for the unexpected symbol , which has now keen least) . Me summariz~ this \nentire zt.rategy as follows. g~gog~~~g Error_recov*ry ~c~E~ (Z,R) --the erco~eous configuration. ~~~g~~ \nthe repaired configuration. le*. h and h =First R, Z* =Z, EFC an:l EFC1 = F!!A+(Rf?st R) while Zm is \nnot empty do # try del~tion, replacements, # attachmerrt, then ins~rtions. ~~~ C be a configuration variable \n:g Try (ZI, {~], EFC) g~ZSS C $hf?!! return C ~~ h <-Accessing-symbol (?op Z*): Pop Z Qg ~g ?ry-stack_forcinq \n(Z, V,ht,EFC*) gives C then return C :$ ~~~gu~ (s ,1) # i.e. give up by # r~turning ~~~g~~. end Error_recovery \nNote that we return from Error-recovzry, when W* Try a repair that succeeds, with the repaired configuration \nc. However, if afi error is detected by Attach, we exit from the vhilp loop, having isolated the mutilation \nto within a lCwindowt$ comprising the text from the leftnost token of the phfase associated with the \nsymbol h up to the original unexpected symbol, inclusive. (A message should be prir.ted to this effect.) \nPerhaps in this case we should just delete all the text in the nindou and call Error recovery recursively. \nTnstead we have ~nflicated above to ?ry_stack_forcifig as suggested in section 3. At. this point furLher \ninvestigation and development 02 the overall algorithm is needed. Backing down the stack ,is, of course, \nan attempt at ~epairing damage caused by thP mutilation. in some cases the mutilation will not have affected \nthe phrases around it. A replaced symbol, for example, may still be on the stack, unreduced or simply \nreduced. Then a deletion, replacement, or insertion Df a single symbol ray precisely undo the mutilation. \nIn fact, to increase the likelihood of a USQfUl repair we have found it worthwhile to Try nonterminals \nas well as terminals, i.e. TRY{Z~,V,EFC), as replacements and insertions. This pays off when, for example, \na mutilation has affected only one phrase. On the other hand, a mutilation only belatedly detected can \nhave caused an arbitrarily large amount of adamagegt to occur on the stack, in th~ sense that many reductions \nmay have Occurred that would not have on the unmutilat.ed string. For example, inserting a semicolon \nbefore an operator in the right part of an assignment statement, e.g. 11X= x+ y; * Z; It, typically \nresults in the text to the left of the semicolon being reduc~d to a ~tatement; hut then ue are left with \nan ex pr~ssion fraqm~nt to the right o: the SCmlCOIOn. Ideally, in such C~S~S UP would like to partially \n~tunparselt somP symbol(s) on the stack, then Try the repairs. Another example is the PL/T conditional \nstatement, which when Q~ the ~~ is d~leted, may look like an return False * @~i.Gg nothing; irrelevant. \n---\u00ad statement up to the then:assignment ---\u00adend Try II**.; . II x = Y + z ~~Fi~ ... else... , . Here \nthe unwanted reduction of X = Y + Z to statement Fight occur with a !.ALR (I) or SLR(I) parser but not \nwith an LR(I) m~g~~~rn~~. Th~ algorithms presented parser. above are idealistic in several ways. We have \nalready mentioried limting F?IA+ rather than allowing it to proceed to the end of Ue are still investigating \np~ssible the program being parsed. Now wt? consider approaches to recovering from such the possibility \nof several mutilations to massive damage, approaches potentially one program. Ln this case FMA+ may ena \nin aborr t which something formal can be said; error before accumulating the desired are looking for \ngrammatical and we number of symbols; thus we simply accept restrictions that might limit such damage. \nthe first repzir that successfully reaches liowever, since that research is the subsequent *rror detection \npoint. incomplete, ve refrain from discussing the ideas here, other than to note that *stack mentioned \nin section 3 above forcing HoMevFr, it may happen that FMA+ will appears t~ have good dev~loprrient \nnot detect a subsequent error, aue to its potential. Ultimately, a~y scheme used lack of *upper level \nparsing. Any such Bust have a significantly greater error will have to be detectecl after a potential \nfor facilitati~g upper Ievelw repair is Dade. For ~xample, suppose repair (F?lA will 11 parsing after \nmaking a ... . X< Y + z then then ... else .. ..1$ -----T-\u00ad have done the lower level parsing) than \nwas created by aelet~ng . an -~~ and it has potectial for causing afi avalanche inserting a ~~gg, and \nsuppose one relevant of spurious error Eessages, e. g. if the production is: li_clause -> ~~ Bexp ~~gg. \nrepair discards several left bracket An error is aetect.ed at the *< 1, !Y~t is symbols. skippea, acd \nFHA+ is invoked. After the first ~~~~ the parser rriay be ready to reduce vithout needing to look aheacl: \nbut Finally, note that we nay give up by no reduction can be made due to incomplete telling the parser \nit is done, i.e. by parsing to the left. so FBA+ calls FHA returning to it the accep~ing ---- again, \nstarting at the second then, ana configuration. This is not ur.reasofiable the inserted ~~g~ is not deiected-~~-FHk+. \nsince we have already partially parse~ the Again the solution of choice is to take input beyond the point \nof error detection the repair that results in getting the opportunity and we are only giving up the \nfarthest into the extended foruara context to parse the remaining upper li?v~l, thus before detecting \na subsequent error. losing the opportunity to detect some other errors. Now in practice we 60 not parse \nall of the remaining input, but Semantics. Given this error recovery ----~---= rather ue stop FNA+ at \na convefiient point scheme It ls unlikely to be worth trying after it has produced at least seven, say, \nto continue to drive tsemar,tic routinest~ symbols of extend~d forward context. We that perforn static \nsemantic analysis consider a trial Successfult then, if and{or code gezeratiofi, even after the after \nthe repair, all of the Qxtended firs t error is encountered, since parsing forward context can be parsed; \nthen we proceeds in a non-canonical order uhi le return to the parser the resulting stack recovering. \nOn ttr e Othl?r hanrl. in a and the rem~ining input. For expositional compiler whose parser builds an \npurposes, however, ve continue the abstract-syntax tree, which is to be presentation in terms of the \nsimpler, if traversed subsequently t.o parsing for impractical, approach of partial parsing furt her \nanalysis and code generation, ue to the end of the program. The practical may contiuue tree--building \nduring FMA+; modification s a re not clifficult to make, and simple repairs vill le~d to kniting but \nthey woulil obscure the presentation. th~ subtrees together appropriately for subsequent ~lroc~ssing. \nOf course, gross cig~go&#38;~g Try repairs will result in a mangled tree, but ~llput z, v, EFC --a state \nstack, that in tur-r just presents an error v~~abulary, and a sequence of DVF$S. detection and recov~ry \nproblem to the output a Boolean value, subsequent processor. ---.? Presumably, if a g*y&#38;Qg a configuration. \nformal technique such as an ~laffix gram mar [KOS 71] is adapted to d~scribe the static semantics of \nlanguaq~s b~sed on abstact-syntax trees, tlien autonatic techniques can be used to perrorm the analysis \n(see , ~.g., [Wat 75] and [DeR 77]), and thus our recavery algorithm may prove useful th~re, too. hl~g~w~~g \n~gfl~. Havinq nonterminals in look-ahead sets allows us to construct an improved version of FMA+. Whe~ \nF??A is applied to a sentence suffix, it may halt by encountering an inadequacy (case ~otherwisett) ; \ni.e. the r,ext (terminal) symbol is insufficient to resolve the parsing cocflict. However, FMA+ immediately \napplies FMA to that symbol and what follows, resulting in a DVF that may begin with a nontermi~al that \nj~ sufficient to resolve the co~flict. This is because the nor.termi.nal nay represent . arbitrarily \nlong look-ahead, i..e. the ~~rase that. was reduced to it and perhaps one synbol beyond (due to the usual \nlook-ahead) . It behooves us then to review recursively the decision at the end of the prior IJVF each \ntime a new one is computed th at. begins with a nonterninal. This approaches the non-canonical parsing \nof the LR(k,t) style as suggested by Knuth [Knu 65]. algorithm Super_FMA+ input x --a sentence suffix. \noutgut a sequence of DVFS derived from x. ~~-x-~ 1 ~~g~ return 1 else @~ U, v be such that---\u00ad FKA:(?,x) \nI-* ([?:~],V) ~,~ S be an empty stack of paths Push [?:U] on S while v # L$2 Jg!? u , VI be such that \nFNA:(?,v) 1-* ([?:U4],V?) v <-. ~t &#38;~ First U* j.. N (the nor.terminals) ;&#38;$?? vhile S is not \n~mpty @2 ~g~ Z be a path variable ~~ Consume_DVF{Top S, Ut) gives Z then u! <-Spelli~g Z: Pop S g~~~ \nPush [?:g~ ] on S; exit# exit from i~ner loop gg gg gi Q~ return the sequence of DVFS spelled by the \npaths on S, followed by 1 ~ ~~~ Super-F?lA+ Even this algorithm can he improved. Each time we trestart~~ \nF?lA, at the beginning of the outer!siost while loop, we begin with ~?tt, representing no knowledge of \nleft context whatsoever. But Ve do know somethinq shout the left. costext in this case, viz. the possibilities \nare restricted to those implicfi by the top state in th? top path OF s. Assum~ ?MA hds halted with Q \non the top of its stack. Then inst.e~d of restarting with st~te set ?, we r~start with state sot ~~nQRS(q), \nwhere . RS(q) = ->* ytx ->* yx, (9 Is Y, Y j.fi V*, x j~ ?*, y azcessc.s q, y accesses ql] The states \nfrom which First x can be read, after y is reduced to some y~, are in RS{q). The idea of using a restricted \nrest art state has been suggested by ?ai [?ai 77] for use in non-canonical SLR(l) parsirig, although \nhis restart states are different from ours and do not apply to LR(k) parsers in general. In this section \nwe re-describe FEA as an algorithm that manipulates not state sets but pre-computed states, thereby making \nit practical. FNA computes st~te sets dynamically by referring to the parserss states --see cases [read} \nand [A -> w) of FMA. ~here is no reason why we cannot precompute these state sets and the transitions \nbetween them; this giv+s rise to a separate set of states for l?tlA. We compute these states as follows. \nLet K be the set of parser states. The set K* of FMA states is computed by beginning with K = {?]. Repeatedly \nadd to K the successors of state sets in K*, where for s ~G V, the s-successor of Q ~ --s---> 9P and \nq $, Q). ~~ K?is (q Iq use ERSIGI lA to nean the thus computed transition function for Ks. NOW we define \nthe decision function PD(Q,h), where Q iS a state in Kr and h a terminal, in terms of the states Q. Simply, \nPD(Q,h) = IJ PD(g,h) q~rtQ Observe that the co!uput.ation of MOVES in FHA is just this same computation. \nThus, algorithm below achieves the same effect as F;;: algorithm FHAS ~gpl~ --as in FMA output --as \nin FclA Init: &#38;g$ Z be the stack consisting only of ? Push ERSIGMA(?.h) on Z: R <-Rest R re~ea t \nlet h = First. R, Q= Top Z, and MOVES = PD(Q,h) select MOVES: g~~~ [read): Push ER51GRA(Q,h) on Z R \n<-Rest R g~~g (A -> w]: #Reiluce, if possible: it \\Zl > Iwl ~~~Q -POP ]wI states off of Z Push ERSIG?l \nA (Top Z,A) on Z els~ rot.urn (Sp@llinq 7,,R) ~~ ..-. -----\u00ad# Ua hit an error SHE (1 QK [accept] #Risl \nQg otherwise: # \\PDl > 1 E!?~QE!l (SPelling Z,R) end repe~~ -- -\u00adend FMAS It should be ~vi8ent that \nF!IA and FMA~ are equivalent.. FHA* is as fast as the LR parsing algorithm save the check i~ case [A \n-> w} that IWI states reside on the stack. Now that FllA mz~ipulates states rather thar. state sets, \nwe can suggest a space optimization. Suppose for some q ~Q K, [q] AG ~ (this octiurs often) . If --%.-> \nq~ is a tra~sit.ion, then9 -----> [9) s {q ) is also a transition-Once F14A* pushes a st~te {q] on its \nstack, an 4 until it. sonetime latsr pops {g], it will behave as if it had pushed state q on its stack. \nThus we may ~ share state {q] in KC with state q in K; states in K$ having transitions into . can be \nmodified to instead have the same transitions into q. such sharing reduces the storage required for the \nparser/error recovery package. {91 The following ~$~~~ ~~g~i~g CL~~Zg.~E, satisfied hy (but. not only \nby) the singleton states in Kt, determines whether stat e shasing nay occur: For any q 2Q K, Q ~E K , \nQ may share with q iff for every Y in V, if y spells a path from g to q and a path from Q to Q then PD(q \n,h) = PD(Q$,h) for every t~rminal h-In other words, the parsi~g decisioxs that Q~ and naiie must tie \nthe same. States in K1 other than singleton sets satisfy this criterion. see this, let to = (1 To {A-> \nt.} and tl = {A -> t., B-> t.], both members of K. Let [tO,tll ~Q K . Then if PD(tl,h) Note that tO \nu tl = tl. = for every h ~E V, [tO,tl] pD({to.tll,h) may he shared with tl. This is the same as requiring \nthat the look-ahead for production h -> t in state t.O be a subset of the look-ahead for production \nA -> t in state t . Won-singleton states that can 1 be shared occur in practice, but they are non-trivial \nto determine. Sinylet.on states are easy to find when generating ~1. Fiqure 2 shows the state diayrarn \nfor K: with statps shared with states in the state diagram of Fiqur~ 1. Due to state sharifig, t~~ percentage \nof extra states needed over th+ original parser is only about 20-69 %, dependi.~q upon the grammar. For \nPascal (Wir 73] we need an increase of 55%, for XPL [HHW 70] 22%, and for PAL [AEU 72] 27%. The significant \ndifferences between our approach and that of Ilruseikis and Ripley [D&#38;R 76,77] is that they compute \nthe states K! via the LR(0) co~structor algorithm, using actual s~ts of LR items (see [A&#38;J 74]) and \nthey do not show how to compute the look-ahead sets needed by FHA$ for LALR(I) or LR(l) parsers. our \ntechnique works equally well for SLR(I), LALR (l), LR(l), or any other LR-style parser. stop +0 )~ \n T {T0,TI,T2 {+, ),J-1: E--T 0 {+,),A): E-E+T t-{+, ),q : T-P*T P  P. (  + (()1= +-- top 1= o W+O \n\\ e I )0 i o Fig. 2. State diagram representing transitions between states in K . Singleton states in \nK have been shared with the corresponding states in K. Reductions associated with ? have been omitted, \nsince FMA never considers them. 6 Conclusions. -s ----------\u00ad The proof of an error recov?ry algorithm \nis in its performance in a practical environment, quite a part from any nice theoretical properties it \nmiqht have. Druseikis ar.d Ripley [DGl? 76] were kind enough to share with us a ta pe containing erroneous \nstudent Pascal programs. tie ran our preliminary 1 I@: 2 ~~~gge-? array A8 B (1..5 1..10): 3 integ-r \n?, J, K, L; 4 up: I+J>K+L*U~~CQ ~~Llf?lse KXs2; 5 k 1,2 := B(3*( T+J, J 4! / K) 7 L2: end FqROR Line \n2, token 10, unexpected l S F(3I LOWS Blockhead Boun8s-list Expressiori . . 1s5~~ Ff)RUARDExpression \n) ? ; Declaration ; =---\u00ad? -. -------? L~b?l-Definition ggg~~~ ,tt was inserted after 11!51~ and before \nN7W. ERRORLine 4, token h, unexpected fl+w FOLLOWSBlock-body 3118 FOFh AFD? Primary ? Relationop Fxpr \n? them? Q (ERROR) --* --A -----A ~l!g ~~g $tifl~ was inserted after Blockboay and befor? I . ERROR----- \nLine 4, token 14, unexpected IL1lJ FOLLOWS------- Blockbody If-then-cl g~ FOR HARD------\u00ad ? else---\u00ad \n? m~tf (ERROR) REPA~R-.--=_ JQ uas inserted after C@ and before Lln. g~MQ~ Line 4, token 17, unexpected \n ISfI FOLLOHS------- Blockbody If_then cl Else_clause nK~~ FORWARD--\u00ad---\u00ad ? Primary ? ; ?-11A II (ERROR) \nR7pAI~-Q\u00ad--A *~Is~~ was replaced with :=tt after K and befor~ Pcimary9*. ERROR--\u00ad-- Line 5, token \n2, unexpected I* FOLLOWS------- Blockbody l~An FORWARD- ----\u00ad ?, ? Wzn (ERROR) REPAIR-----\u00ad ($ was \ninsert~d aft~r A and before 1 . ERRORLine 5, token 5, unexpected Q:=s~ FOLLOWS -------Blockbody \nA t ( Expression , 2 ? IIBII FORWARD? Primary ? * ( Expression REPAIRW) M was inserted after i2it and \nbefore I:=ft. -------?( (ERROR) ERRORLine 5, token 14, unexpected W,!* FOLLOWSBlockbody Variable := \n!BII ( Term * ( Expression FORWARD? Primary ? * (ERROR) ~gg~~~ q ~$ was inserted after \\$Expression l \nana bef~r~ ,n. mllQg Line 5, token 17, unexpected 1/w FOLLOWSElockbody Variable := B ( Expression , \n?erm * FORWARD? Primary ? ) (ERROR) REPAIR&#38;8<Identifier>w was inserted after ~S*~c an3 beforz / . \n~g~m~ Line 6, token 1, unexpected iJ FOLLOWSBlockbody Variable := ltl?~$ ( Expression , Expression ) \nFORWARD? Primary ? Relationop Expression ? then (ERROR) REPAIR ; was icserted after ) and before ~~~~. \nERRORLine 6, token 6, unexpected ~~then~l FOLLOklSBlockhody ~~ Expression then - -FORWARD? Statement \n? ; ? Label-definition ? --\u00ad -------eni REPAIRCOthenw was deleted, after Ithen?t an~ before 9Statemsnt89. \nEND OF PARSE. ----- --- Figure 3. Pun of error recovery algorithm on program of Graham and Rhodes. implementation \non son~ of them, gi. v~rr a [ASJ 714] Aho, Alfred V. ara s. c. I$LR Johnson, Parsing88, Co?puting Pascal \ngrammar deduced more -c~r-lpss -----\u00admechanically from the Pasc2 1 syntax ------- Surveys 6 ~~~, June \n197u. diagram [Wir 73]. [A&#38;U 72] Aho, Alfred V. and LJeffrey D. Unman, ~tlg Theory , Qg Parsin~L \n------.----Each repair self?cted by the alqorithm -----------and --- - -Vol T ranslatinna Compil~ng, \n1., uexce~lentn if it repaired the Preatice-?iall,--~cc. , Englewood Cliffs, vas ra ted text as a human \nreader would have, goodtt N.J., 1972. if not but. it still resulted in a reasonable program and no spurious \n~rrors, [DeR 71] DeRemf!r, Frank, Simple LR(k) Grammarss , Comm ACil ~~ ~~~, July 1971. 11 oor t if \nit resulted one more --- P in or ----: spurious errors (in fact, none resulted in more than one spurious \nerror) , and wunrePaireaw if no repair was s~lected but [DeR 77] DeRerner, Frank, 81Trez-affix Dendrogra \nrrimarslV, r~search proposal to the parser. ?he results f~llov: NSF, Information Sciences, ?niv~rsity \nof California, Santa Cruz, CA. 95064, 1977. we continued to parse via FMA+ rather than Excellent Good \nPoor Unrepaired Total [DSR 76] Druseikis, Frederick C. and G. 76 (i:%) (:J%) (1;%) (::%) (!00%) David \nBipley, tError RecovPry for Simple LR(k) Parsersgv, Dept. of Ve have counted spurious errors in these \nComputer Science, Univ. of Arizona, ?Ucson, Az. 85721, 1976. statistics. Note that 70!% were good or \nexcellent. With some tuning, we hop~ to [D&#38;P. 77] Druseikis, Frederick C. and G. reduce the poor \nand unrepaired responses David ~iplpy, I*Extended SLR(k) Parsers in number. The unrepaired cases both \nrob for Error Recovery and Repair!r, Dept. us of upper-level parsing and sometimes of Computer Science, \nUniv. of .lrizona, aavezsely affect recovery from other Tucson, Az. 85721, Feb. 1977. neaxby errors. \nHe have no idea how much different these statistics might be for more *sophisticated t errors made by \n~@Practical Syntactic Error [G&#38;l? 75] Recoverym, ggcmg: AC!! ~EJ jj~~, Nov. 1975. seasoned systen \nprogrammers intimately --\u00ad fa~iliar with the language. [H&#38;U 69] Eopcroft, John E. an d Jeffrey As \nanother concrete deruonstration of D. Vllman, j?ggEg~ &#38;zEgsuges ~Qfi ~~g~g Pelation to Automata, \nAddison-Wesley, Reading, Bass., 1969. the algorithm~s performance, we present in ----------______ -- \nFigure 3 the erroneous Algol-like sample program used by Graham azd Rhodes to illustrate the performar!ce \nof their error Vllffix [Kos 71] Koster, C.H.A., Grarumarsn, in lii~ol 68 ~Eplementation, recovery algorithm \n[G&#38;R 75]. We used the _-- -- ------\u00adsame Algol subset grammar as they, ana our J.E.L. Peck (cd.) \n, Amsterdam, North Holland, 1971. repairs are identical to theirs, without the need for a weiqhting \nfor symbols or a pattern natching algorithm. We rate each [MHU 70] HcKeernan, W.H., J.J. Horning, repair \nas texcellent#@, except the a nd D*B. Vortman, ~ ___ ~___ Cornpller Generator,Prentice-Hall, Inc., \nand / line 5, Wgoodll Englewoo3 Cliffs, N.J., 1970. insertion of the <identifier> between * --------\u00ad \non which we rate since we have no idea what the human might have done. [0 H 76] O Hare, Michael F. , \n#eModification of the LR(k) Parsing ?echnigue to Include Automatic Ue should note that backing dovn \nthe Syntactiz Error Recovery~ , snnior stack rarely resulted in a good repair, thesis, Univ. of Calif. \nat Santa except in the critical case of the deleted Cruz, Santa Cruz, CA. 9596fi, June ~ in the program \nabove. Thus, there is 1976. t9Error some question as to whether that technique [Pen 77] Pennello, flhomas \ni.,7is vorth its computational cost ; it Recovery for LR Parsers , M.sc. should at least b~ delayed until \nno Thesis, Information Sciences, more-productive techniques have succeeded. University of California \nat Santa Cruz, Clearly more research, trials, and errors Santa Cruz, CA. 95064, Jure 1977. (no pun intended) \nare in order. Currently [Mat 75] Watt, David, ~ryh e Parsing we do Probleir! for Affix Grammarstt, Department \nnot even have Super_F!YA+ of Compu+.er Science, implemented, nor have we tried adding the L!nivezsity \nof improved restart states. Glasgow, Glasgow, Scotland, 1975. References \n\t\t\t", "proc_id": "512760", "abstract": "A \"forward move algorithm\", and some of its formal properties, is presented for use in a practical syntactic error recovery scheme for LR parsers. The algorithm finds \"valid fragment\" (comparable to a valid prefix) just to the right of a point of error detection. For expositional purposes the algorithm is presented as parsing arbitrarily far beyond the point of error detection in a \"parallel\" mode, as long as all parses agree on the read or reduce action to be taken at each parse step. In practice the forward move is achieved serially by adding \"recovery states\" to the LR machine. Based on the formal properties of the forward move we propose an error recovery algorithm that uses the accumulated right context. The performance of the recovery algorithm is illustrated in a specific case and discussed in general.", "authors": [{"name": "Thomas J. Pennello", "author_profile_id": "81100357706", "affiliation": "University of California, Santa Cruz, CA", "person_id": "P326313", "email_address": "", "orcid_id": ""}, {"name": "Frank DeRemer", "author_profile_id": "81100392087", "affiliation": "University of California, Santa Cruz, CA", "person_id": "P85796", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/512760.512786", "year": "1978", "article_id": "512786", "conference": "POPL", "title": "A forward move algorithm for LR error recovery", "url": "http://dl.acm.org/citation.cfm?id=512786"}