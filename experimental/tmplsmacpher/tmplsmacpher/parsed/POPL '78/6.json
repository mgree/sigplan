{"article_publication_date": "01-01-1978", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee.&#38;#169; \n1978 ACM 0-12345-678-9 $5.00 Conference Record of &#38;he Fifth Annual ACM Symposium on Principles of \nProgramming Languages Monoids for Rapid Data Flow Analysis Barry K. Rosen Computer Sciences Department \nIBMThomasJ.Watson Research Center Yorktown Heights, NewYork 10598 1. INTRODUCTION The earliest data \nflow analysis research dealt with concrete problems (such as detection of available expressions) and \nwith low level representations of control flow (with one large graph, each of whose nodes represents \na basic block). Several recent papers have introduced an abstract approach, dealing with any problem \nexpressi\u00adble in terms of a semilattice L and a monoid M of isotone maps from Lto L,under various algebraic \nconstraints. Examples include [CC77; GW76; KU76; Ki73; Ta75; Ta76; We75]. Several other recent papers \nhave introduced a high level representation with many small graphs, each of which represents a small \nportion of the control flow information in a program. The hierarchy of small graphs is explicit in [Ro77a; \nRo77b] and implicit in papers that deal with syntax directed analysis of programs written within the \nconfines of classical struc\u00adtured programming [DDH72, Sec. 1.7]. Examples include [TK76; ZB74]. The abstract \npapers have retained the low level representa\u00adtions while the high level papers have retained the concrete \nproblems of the earliest work. This paper studies abstract conditions on L and Mthat lead to rapid data \nflow analysis, with emphasis on high level representations. Unlike some analysk methods oriented toward \nstructured programming [TK76; WU75; ZB74], our method retains the ability to cope with arbitrary escape \nand jump statements while it exploits the control flow information implich in the parse tree. The general \nalgebraic framework for data flow analysis with semilattices is presented in Section 2, along with some \npreliminary lemmas. Our rapid monoids properly include the fast monoidsof [GW76]. Section 3relates data \nflow problems tothe hierarchies of small graphs introduced in [Ro77a; Ro77b]. High level analysis begins \nwith local information expressed by mapping the arcs of a large graph into the monoid M, much as in low \nlevel analysis. But each arcinour small graphs represents a set (often an infinite set) of paths in the \nunderlying large graph. Appropriate members of M are associated with these arcs. This globalized local \ninformation is used to solve global flow problems in Section 4. The fundamental theorem of Section 4 \nis applied to programs with the control struc\u00adtures of classical structured programming in Section 5. \nFor a given rapid monoid M, the time required to solve any global data flow problem is linear in the \nnumber of statements in the program. (For varying M, the time is linear in the product of thk number \nby t~, where t~ is a parameter of M introduced in the definition of rapidi\u00adty. ) For reasons sketched \nat the beginning of Section 6, we feel obliged to cope with source level escape and jump statements as \nwell as with classical structured programming. Section 6 shows how to apply the fundamental theorem of \nSection 4 to programs with arbi\u00ad trary escapes and jumps. The explicit time bound is only derived for \nprograms whhout jumps. Acomparisort between theresuhsobtakted byourmethod andthose obtained by [GW76]is \nirt Section7, which also contains examples of rapid monoids in the full paper. Finally, Section 81istsconclusions \nandopenproblenls. Proofs of Iemmas are omitted to save space. The full paper will resubmitted to a journal. \nWe proceed from the general to the particular, except in some places where bending the rule a little \nmakes a significant improvement in the expository flow. Common mathematical notation is used. To avoid \nexcessive parentheses, the value of a function f at an argument xisfxrather than f(x). If fx is itself \na functionthen (fx)yis the result of applying fxto y. The usual s and > symbols areused for arbitrary \npartial orders as well as fortheusual order among integers. Afunc\u00ad tionfroma partiallyo rderedset (posef) \nto aposet is tsokmeiff x < y implies fx < fy. (Isotone maps are sometimes called monotonic in the literature. \n) Ameetsemilatticeis aposet with a binary operation A such that x A yis the greatest lower bound of the \nset tx, y]. A meet semilattice wherein every subset has a greatest lower bound is complete. In particular, \nthe empty subset has a greatest lower bound T, so a complete meet semilattice hasa maximum element. Amonoid \nis a set together with an associative binary operation o that has a unit element 1:10m=mo1=mforallm. \nInallourexamples the monoid M will be a monoid of functions: every member of M is a function (from asetinto \nitself ), the operation o istheusualcompos\u00ad ition (f o g)x = f(gx), and the unit 1 is the identity function \nwith 1X . x for all x. Two considerations governed the notational choices. First, we speak in ways that \nare common in mathematics and are convenient here. Second, we try to facilitate comparisons with [GW76; \nKU76; Ro77bJ, to the extent that the disparities among these works permit. Onedisparity is between themeet \nsemilatticesof [GW76; KU76; Ki73] and the join semilattices of [Ro77b; Ta75; We75], where least upper \nbounds are considered instead of greatest lower bounds. Tospeak ofmeets ismorenatural in applications \nthat are intuitively stated in terms of what must happen on all paths in some class of paths in a program, \nwhile to speak of joins is more natural in applications that areintuitively stated in terms of what can \nhappen on some paths. Bychecking whether there areany pathsin the relevant class and by using thertde \nthat 3 is equivalent to -V-, join oriented applications can be reduced to meet oriented ones (and vice \nversa). A general theory should speak in one way or the other, and we have chosen meets. For us, strong \nassertions about a program s data flow are high in the semilattice. 2. ALGEBRAIC FRAMEWORK Throughout \nthis paper L will be a complete meet semdattice whh a maximum element T ~. The greatest lower bound of \na set X will be denoted A X. In addition to L there is given a monoid M of isotone maps f : L + L, such \nthat M k closed under pointwke meets: f A g k the map such that (f A g)x = fx A gx for all x in L. This \nframework k a natural generalization of the information propagation space [GW76, p. 175], wherein L is \nthe set of all subsets of a given finite set and s k set inclusion. With f s giff fx s gx for all Xin \nL, M k a meet semilattice with the pointwise meet operation. We will further assume that M has a maximum \nT ~ which agrees with the maximum in the complete semilattice of all isotone maps on L: T ~x = T ~ for \nall x in L. (This assumption can be weakened. ) We say that M k a closed monoid of isotorre maps on L. \nThe monoid M is idempotent iff each f in M is idempotent in the mual sense: fof=f. (2.1,1) rhe monoid \nM is ~ast [GW76, p. 175] iff each f in M satisfies fof2fAl, (2.1.2) which is a weaker condition than \nidempotence. On the other hand, every fast monoid that arises in [GW76] is actually idempotent as well. \nThere are important data flow monoids that are not idempotent, and they are also not fast. For example, \nneither form of the constant propagation monoid CP in [KU76, p. 167] is fast. To deal with such monoids \nwe begin by recalling the trick used to deal with them in [GW76, p. 182]. The fastness closure f* of \nany f in M k the map f*= A{(f Al) ire N], (2.1.3) where N is the set of all natural numbers. More generally, \nthe follow\u00ad ing definition considers monoids wherein certain expressions involv\u00ad ing greatest lower bounds \nlike (2. 1.3 ) can be approximated. DEFINITION 2.2. The monoid M is rapid iff there is given a binary \noperation @ on M and a positive integer t~ such that, for all f, g in M, and g @ f can be computed from \ng and f witlin at most t~ steps, where any A or o operation is counted as a single step. In particular, \nwe can define g @ f to be g o f* whenever f* is in M and we know how to find it in t@ steps. Thus the \nmonoids consid\u00adered in [GW76] are all rapid. In rapid monoids where g o f* < g @ f we will sometimes \nbe able to get sharper data flow information than in [GW7 6], as will be seen in Section 7. A simple \nbut important example of rapidity is provided by any idempotent monoid: we have g,@ f = g A g 0 f with \nt~ = 2. The monoids for traditional global flow problems like available expressions [U173] are rapid \nfor this reason. By letting @ be a binary operation and by putting ~ after composition with g in Def. \n2,2, we avoid needing to assume g(x A y) = gXA gy. (2.3) The assumption (2.3) will be called dish-ibutiviry \n[KU76, p. 160]. It does not always hold. Data flow problems arise when members of M are associated with \nthe arcs of a dkected graph. Only finite graphs will be considered here. Our view of graphs is more general \nthan is usual in data flow research. A graph G consists of finite sets NG (the nodes) and AG (the arcs) \ntogether with maps SG and tGfrom AG to NG. These are the source and target maps. In pictures, arcs are \ndrawn as arrows from sources to targets. Of course the G subscripts are sometimes omitted if there k \nno doubt as to which graph is intended. A path is a finite sequence c = (c,, .... CK) of arcs such that \ntck = sck+ I whenever 1 < k < K. The null sequence with K = O is allowed and is denoted L A nonnull path \nis from the node SC, to the node tcK. We borrow map notation with c : SC, + tcK and speak of sources \nand targets for nonnull paths as well as for arcs. DEFINITION 2.4. Given a graph G and a map f : AG + \nM, the extension of f to paths is also denoted f and is defined by fc = 1 if c = X and f(cl, ....CK) \n= fcK 0 0 fcl if c # A. When a program is represented by a graph G, we have a map f : A + M with fc : \nL ~ L for each arc c. This local information tells how an assertion x in L associated with sc is propagated \nto tc as a transformed assertion y = (fc)x when control flows along c. There is also a set E c N of designated \nentry nodes and an initial assignment E : E + L of entry assertions, with En being true whenever control \nenters the program at n. Note that En need not be true when\u00adever control reaches n from within the program \nitself. Knowing what is true at entry nodes at the time of entrance, we want to determine what is true \nat all nodes at all times. DEFINITION 2.5. A global flow problem IJ is a quadruple (G, f, E, E), where \nGisagraph, f: AG -M, E~NG, and E: E+L. A solurion for P is any I : NG + L such that, whenever m is in \nE and c : m+ nisapath in G, In < (fc)Em. Def. 2.5 generalizes the definition of a safe assignment [GW76, \np. 177] in several ways that are crucial here. We do not assume that E consists of a single node m and \nthat all nodes in G are reachable from m. We do not assume that L has a minimum ~ or that Em -J. for \nm in E. If L does have a minimum then we can solve any problem by letting In = L for all n. This solution \nk uninteresting. A maximum solution would be ideal, but for some choices of M there can be no algorhhm \nto find one [KU77, Thin. 7]. As in acceptable assignments [GW76, p. 177], we therefore consider solutions \nthat are large enough to be interesting but that may be computable with a reasonable amount of effort. \nDEFINITION 2.6. A fixpoint for a global flow problem p = (G, n in G. The technical realization of the \nsimilar intuitions behind f, E, E) is any J : NG + L such that, for all m in E and c in AG, [Ta75] is \nquite unlike what happens here. Jm s Em and Jtc s (fc)Jsc. (1) A good solution for P is any solution \nI such that, for each fixpoint J and each n in NG, In 2Jn. (2) Comparing Def. 2.6(1) with Def. 2.5, it \nis clear that any fixpoint is a solution. In many examples L is well founded: there are no infi\u00adnite \ndescending chains. In this case, a maximum fixpoint can be found by the obvious fixpoint algorithm. When \ndistributivity (2.3) holds, the maximum fixpoint k the only good solution. However, dktributivity fails \nfor some important problems, and here there are good solutions that are not fixpoints. Examples are in \n[GW76, Fig. 4; Ha77b]. Like the methods of [GW76; Ta75], our method of finding a good solution will sometimes \nfind one that is better than the maxi\u00admum fixpoint. Instead of attacking a global flow problem directly, \nhigh level analysis considers a hierarchy of problems posed with smaller graphs, each of which represents \na small portion of the control flow information about a program. Good solutions for the auxiliary problems \nare combined in a good solution for the original problem. Because G and E will vary in much smaller ranges \nthan f and E as we move through the hierarchy of auxiliary problems, we will find it helpful to consider \nglobal flow schemes: pairs (G, E) such that G is a graph and E is a set of nodes in G. Thus a scheme \ncan be fleshed out to aproblem by adding f :AG + M and E :E -+ L. Perhaps we can simultaneously solve \nall the problems derived from a given scheme by working with formal expressions. DEFINITION 2.7. A flow \nexpression for a given scheme (G, E) is any formal expression built with set of operators { A, 0, @ ], \nusing paths in G as variables and a symbol T as a constant. Given a flow expression X and f : AG ~ M, \nthe value [X : f] in M k only defined if M k rapid or X is free of @. In that case [X: f]= fcif Xisapathc \nand[X:f]=TMif Xis T; (1) [X: f]=[Y:f] u[Z:f]if Xis Yrr Zwithu in{ A, O.@}. (2) Now we want to assign \nflow expressions to nodes in a scheme, so that any problem derived by fleshing out the scheme with f \nand E can be solved by evaluating expressions. DEFINITION 2.8. A solution for a scheme (G, E) is any \nmap X, assigning a flow expression X(m, n) to each (m, n) in E x fUG, such that, whenever misin Eand \nc:m + nisapath inG, [X(m, n) :f] < fcfor allf:AG + M. DEFINITION 2.9. A flow cover for a scheme (G, E) \nis any solution X such that, whenever J is a fixpoint for a global flow prob\u00adlem (G, f, E, E) derived \nfrom (G, E), Jn s [X(m,n) : f] Jm for all (m, n)inE x NG. In the terminology of [Ta75, p. 9], the value \n[X(m, n) : f] is a tag for the triple (m, n, P), where P is the set of all paths from m to LEMMA 2.10. \nLet P = (G, f, E, E) be a global flow problem and suppose (G, E) has a flow cover X. Then a good solution \n1 for P is obtained by setting, for each node n in G, In=~{[X(m, n): f] Emlmisin E ]. (Lemmas 2.11 2.13 \nin the full paper show that any scheme (G, E) has a flow cover if M is rapid. The flow covers constructed \nin proving the lemmas are optimized for the most common schemes in Sections 5 and 6.) LEMMA 2.14. Let \nX be a flow cover for (G, E) and let H be the result of deleting some arcs from G. For all (m, n) hr \nE x NH, let Y(m, n) be like X(m, n) except for replacing each path c that involves a deleted arc with \nthe corresponding expression d o T, where d is the largest suffix of c as a string of arcs that is free \nof deleted arcs. Then Y is a flow cover for (H, E). 3. HIERARCHIES OF GRAPHS In [Ro77a] the control flow \nin a program is represented by a hierarchy of small graphs rather than by one large graph. The rele\u00advant \nabstract definitions from [Ro77a, Sec. 2] will be restated here for ease of reference. As in [Ro77a], \nit is convenient to assume that no two arcs have the same sources and targets, so that the arc c may \nbe identified with the pair (SC, tc) of nodes. A nesting srructure for G is a finite partially ordered \nset Z with a maximum m, together with a set Na of nodes and a set Aa of arcs for each a in Z. The following \nproperties are required: N.rr = NG and An = AG; (3.1.1) /f < a in 2 implies (N(3,~ Na and A/3 Q Act); \n(3.1.2) (c in AG has SC, tc in Na) impfies (c is in As). (3.1.3) A nesting structure with entrances \nand exits consists of G and 2 as in (3. 1) together with, for each a in 2, sets of designated entrances \nand designated exits DENTRas Na and DEXITa Q Na (3.2.1) such that, whenever ~ < a in 2, DENTRti fl N~ \n= DENTR/3 and DEXITa fl N~ = DEXIT/3. (3.2.2) Entrance and exit sets are defined by ENTRa=DENTRa U{tc~Nalc \n6AG&#38;-scc N~]; (3.2.3) EXITa=DEXITa U{sce Nalc cAG&#38;_tc eNa}. (3.2.4) Given a nesting structure \nwith entrances and exits, consider any a in 2, n in ENTRa, and p in EXITa. Let H(a, n, p) = 1 if there \nis a path from n to p in G touching only nodes of Na (3.3) and fI(a, n, p) = O otherwise. These path \nbits can be computed bottom-up, beginning with choices of a that are minimal in Z. Path bits for a can \nbe determined from previously computed path bits for parts ~ of a, where PARTa={ ~<al Noyin Xhas/J< y< \na]. [3.4) As in [Ro77b, Sec. 4], we construct the induced graph Ga for each a in 2. The set of nodes \nin Ga is NGa = No. U U ~. PART. (ENTRP U EXITB) (3.5.1) where No. = Na U ~<pARTaNE (3.5.2) The set of \nreal arcs of Ga k RAGri={ cc Atilsc, tc=NGa]. (3.5.3) For each ~ in PARTa there is a set of imaginary \narcs IAGa[/J] = {(n,p) IncENTRD &#38;P6EXITP &#38;~(p, n,P) = 1]. (3.5.4) The total set of arcs k AGa \n= RAG. U U ~ ~ pARTa lAG@l (3.5.5! with the obvious definitions of sources and targets. (Some arcs are \nboth real and imaginary.) DEFINITION 3.6. A nesting structure with entrances and exits is locally covered \nby assigning a flow cover to each induced flow scheme (Ga, ENTRa). One can solve any global flow problem \nP = (G, f, E, E) without computing on G, provided that G has a locally covered nesting struc\u00adture and \nE = DENTRT. The algorithm does compute with Z and the induced graphs, and it uses a given flow cover \nXa for each induced scheme (Ga, ENTRa). In Section 5 we will exploit structured pro\u00adgramming to optimize \naway most of the work with induced graphs. The algorithm requires some preprocessing that globalizes \nthe local information in f : AG + M. Instead of askhtg only what must happen when control flows along \nan arc of G, we ask what must happen when control flows along any of the paths summarized by an arc in \nan induced graph Ga. For each a in 2 there are maps fa :AGa ~ M and F. :ENTRa x NGa ~ M (3.7.1) such \nthat, for all c in AGa and all (m, p) in ENTRa x NGa, fac = (if c c RAGa then fc else TM) A ~{ Fpc I \n~ 6 PARTa &#38; C c IAGa[(3] ]; (3.7.2) Fa(m, p) = [Xcr(m, p) : fa]. (3.7.3) If a is minimal in Z then \nIAGa = @ and (3.7.2) defines fa to agree with f. If a is nonminimal then f. still agrees with f on real \narcs that are not imaginary, but on imaginary arcs fa is determined by the various Fp maps for /3 in \nPARTa. These will be available when we try to compute fa, provided we begin at the bottom in a linearization \nof the partial order on ~. Now consider the maximum n in 2. Because E = DENTRr s NGrT,there is a global \nflow problem P.=(Gin,f~, E, E), (3.8.1) and Lemma 2.10 yields 10: NGm ~ L, a good solution to PO, (3.8.2) \nFor each part /3 of n we can now specify a global flow problem P[/Jl = (G[ll, f[pl, E[/Jl, EIBI) (3.8.3) \nwhere G[~] is the graph whose set of nodes is ND and whose set of arcs is AD rl (N/3 x N/J), with sources \nand targets as in G. (This is not the induced graph G/3 and will generally be much larger. ) The map \nf [/3] is the restriction of f to arcs in G[/J]. For entry information we use E[(3] = ENTR~ and E[/J] \n= (10 restricted to E[/3]), (3.8.4) so that IJIP] depends on the choice of 10 in (3.8.2). It can be shown \nthat G[~] has a nesting structure with 2 [~] = { a in 2 I a < /3 }, so that induction on the size of \n2 should lead to good solutions 1[/3] of p[/JI for each P in PARTrT. Combining these with 10 should yield \na good solution to the original problem P. Several technical points must be checked. Because M may not \nbe distributive, we do not have anyttilng as simple as the Induced Graph Theorem [R077b, Sec. 4] to estabfish \nthe relevance of the auxiliary problems P. andP[61 to the original problem P. The first step is to note \nthat p[~] does indeed inherit a nesting structure, with induced maps (3.7) as well as en\u00adtrances and \nexits, from P. Moreover, the construction (3.7) applied to p = p[/J] yields the same induced maps as \n(3.7) appfied to P: f a=faand F a = Fa for all a s ~. (These remarks are formulated more precisely as \nLemma 3.9 in the full paper. ) We can therefore use induction on 12 I in proving the following three \nlemmas on the relevance of the auxiliary problems. LEMMA 3.10. If J is a fixpoint for IJ then the restriction \nJo of J to nodes in NGw is a fixpoint for PO. LEMMA 3.11. Suppose /3 is in PART~ and c : m -+ p in G[13], \nwhere m is in ENTR/3 and p is in EXIT&#38; Then F6(m, P) S fc. LEMMA 3.12. Extend 10 from (3.8.2) to \nhave domain NG by setting Ion = T ~ whenever n is not in NGrr. Suppose that 1[/3] is a good solution \nto P[(J] for each ~ in PARTn. Then I : NG + L de\u00ad fined by is a good solution to P. 4. HIGH LEVEL DATA \nFLOW ANALYSIS Given a global flow problem P = (G, f, E, E) with E = DENTRT such that G is locally covered \nby a flow cover Xa for each induced scheme (G a, ENTRa), we can use the results of the previous section \nto find a good solution for P. We begin by computing the globalized local information of (3.7). Descending \nthrough X recur\u00adsively, we solve the auxiliary problems of (3.8) and combine the results as in Lemma \n3.12(1). Lemmas 3.9 and 3.12 lead to a correct\u00adness proof by induction on I 2 I But how much does all \nthis cost? Can the method be optimized to exploit regularities in well structured programs? To address \nsuch questions we program the algorithm more formally, but still at a very high level. We also impose \nan additional condition on the nesting structure: for all a, /3 in X, .(a</30r ~<a)implies Nan N~=@. \n(4.1) This is true in all the examples of interest here, and it greatly simpli\u00adfies the analysis of computational \ncomplexity. A global flow problem satisfying all the conditions imposed so far is said to be we(l nested. \n(These conditions are reviewed in stating Theorem 4.4 below. ) The following procedure RECURSOLVE takes \nas argument some y in 2. The well nested problem p is accessed by RECUR-SOLVE as a global variable. The \npurpose of calling RECURSOLVE is to have a side effect on I, a global variable whose values are maps \nfrom the nodes of G into L. The effect of call RECURSOLVE(y) is to change In for each n in Ny, so that \nI on Ny becomes a solution to a problem P = P[Y] defined as in (3.8.3) but with arbitrary y in 2 in \nplace of (3 restricted to be a part of m. To acheive this effect, RE-CURSOLVE begins by calling LOCALSOLVE \nto find a good solu\u00adtion for p 1~ defined as in (3.8.1) but with y in place of n, (The procedures do \nnot construct p and p ~, but we do in the correct\u00adness proof. ) RECURSOLVE : proc (y : member of the \nposet Z) [ call LOCALSOLVE(Y); #On nodes in Gy, I is now a good solution to P ~ for y.# for all /3 in \nPARTY do call RECURSOLVE(~) Note that RECURSOLVE does not explicitly combine the solutions to the subproblems \nP [~] as might be expected from Lemma 3. 12(1). We have already begun to optimize the method in light \nof (4. 1). Using 03@ as a placeholder for implicit procedure bodies (such as the body of RECURSOLVE above), \nwe write the program SOLVE that gets p as input and puts out a good solution 1. SOLVE : [ dcl P well \nnested global flow problem; dcl I map from NG into L; dcl fa map from AGa into M for all a in X; dcl \nFa map from ENTRa x NGa into M for all a in 2; dcl LOCALMAPS proc ooo; #This procedure sets up f. and \nF. for each a in 2.# #LOCALMAPS has access to induced graphs and to f.# dcl LO CALSOLVE proc(y : member \nof 2) ooo; #Thk procedure finds a good solution as in Lemma 2. 10.H dcl RECURSOLVE proc(y : member of \nZ) ooo; get P; call LOCALMAPS; for all nin NG do ifn~ Ethen In-Enelse In+TL; call RECURSOLVE(m); put(I) \n1 For LOCALMAPS we assume an arbitrary listing (6 = a,, a2, .... rr) of 2, such that ai s ~j implies \ni < j. Thus a variable ranging over 2 can be stepped from 8 to m like an integer variable. The programming \nfor LOCALMAPS is omitted here because it is the obvious implementation of (3.7) in light of (4.1): if \nc is in IAGa then c is in IAGa[~] for unique &#38; Finally, LOCALSOLVE is as suggested by Lemma 2.10. \n~[.s[ = LEMMA 4.2. Let EITHERSOLVE stand for RECURSOLVE or LOCALSOLVE. For each a in 2, EITHERSOLVE is \ncalled with actual parameter a exactly once in the course of SOLVE s computa\u00adtion. When control passes \nthrough call EITHERSOLVE(CY) in SOLVE, any node n such that In changes must be in Na. LEMMA 4.3. Whenever \ncontrol reaches call LOCALSOLVE(y) in SOLVE, each m in ENTRy has Im = E[y]m. Whenever control leaves \ncall LOCALSOLVE(Y) in SOLVE, I restricted to NGY is a good solution to p[y]. To study the correctness \nand computational complexity of SOLVE it will be appropriate to assume that A and o always require one \nstep while applkation of a member of M to a member of L also takes one step. See [GW76, p. 178] for more \non the reasonable\u00adness of this assumption. For the time bound hr the following theorem we assume that \neach iterator for all VAR in SET is defined by step\u00adping VAR through SET in some fixed order. THEOREM \n4.4. Let p = (G, f, E, E) be a well nested global flow problem: there k given a nesting structure for \nG with entrances and exits such that E = DENTRn and (4.1) holds, and there is given a flow cover Xa for \neach induced global flow scheme (Ga, ENTRa). Then SOLVE with input P finishes with output I such that \nI is a good solution for p. Moreover, let all iterators be defined by stepping through their index sets. \nThen the time required (apart from input/output) is f2(Ta1+ Ta, +... +Tal>l + INGI ), (1) where (al, \n.... alzl ) is a Iistingof X and each a in 2 has (2) Ta = I AGa I + (Sa)I + ,,. + (Sa)pairs(a) + pairs(a)> \nwhere pairs(a) is I ENTRa x NGa I and each (Sa)j is the time re-a : if...then /31 else ~2 (conditional \nstatements) quired to evaluate [Xa(m, p) : fa] by LO CALMAPS for the j-th pair (m, p) in ENTRa x NGa. \nProof. Correctness follows from Lemmas 3.9, 3.12, 4.2, and 4.3 by induction on the depth of recursion \nin RECURSOLVE. The time required by LOCALMAPS for each a is 0( I AGa I + (Scr) ~ + + (Sa)Pair~(a) ), \nwhile the time required by the call on LOCALSOLVE with y = a is O ( pairs(a) ). BY Lemma 4.2, the calls \non LOCALMAPS and RECURSOLVE in SOLVE require time 0( Tal + ... + Ta I~ I ). Finally, the I NG ! term \ncomes from initializing I. 5, CONTROL STRUCTURES: A SPECIAL CASE The input to a compiler is a program, \nnot a global flow problem. While constructing a problem from a program for the sake of optimi\u00adzation, \na compiler can also contruct a nesting structure with entrances and exits. When the graph and nesting \nstructure are chosen in the way described in [Ro77a; Ro77b], the relation between the parse tree and \nthe hierarchy of small graphs is so transparent that most calcula\u00adtions with graphs can be optimized \naway. (Here we are optimizing the compiler itself, before using it to optimize programs. ) For pur\u00adposes \nof this section, a program is always a single statement: we use ALGOL-fike syntax. Most types of statement \nare complex: built up from smaller statements by control operators like if ...then...else... or whale.. \ndo... The simple statements contain no smaller statements. This section explains how G is constructed, \nbut h is quhe terse. See any one of [R076; Ro77a; Ro77b] for a more leisurely discussion with examples. \nWe deliberately confuse a statement node in the parse tree with the corresponding fragment of program \ntext. The set Z of all statements in a program is partially ordered by ~ s a iff ~ is a descendant of \na in the parse tree. The maximum n in X is the whole program, while the minimal elements 8 are the simple \nstatements. The graph G and the hierarchy of induced graphs can both be con\u00adstructed in a bottom-up pass \nthrough the parse tree. For G itself, each statement contributes nodes and arcs given by N~=Noa UUp<a \nN/3and Aa=Ao~U UP<. AB. (5.1.1) where the new nodes Noa and arcs Aoa are determined by the control operator \nused to form a when a is complex. We will allow control operators like case that take a varying number \nof arguments, and then a phrase like the control operator used to form should be interpret\u00ad ed as the \ncontrol operator and number of arguments used to form . For all statements considered here, Nocr has \ntwo distinguished nodes entering a and leaving a, and the designated entrances and exits of (3.2) are \ngiven by DENTRa = { entering a } and DEXITa = { leaving a }. (5.1.2) The three operators emphasized in \n[Ro77b] contribute no new nodes beyond entering a and leaving a to G. They are a : while...do ~ (while \nstatements) ~ : [P,; .... (k] (sequential compound statements) Figure 5.1 is a more concise version \nof Figures 1-3 in [Ro77b], giving names to the arcs in Aoa and indicating their sources and targets, \nfor each of these three control operators. The arcs named q or qk for integers k are not in Aoa. They \nare used to dkplay how we expect the induced graph Ga to look when a is a conditional or a while or a \nsequential compound statement. Conditions under which these expectations are fulfilled will be studied \nshortly, but first we consider four more control operators. They are a : if...then /3 (one part conditional \nstatements) a : case... of ~l; .... j3K esac (case statements) a : do/3 until... (until statements) \n a : for.. from.. to,. by.. .do /3 (stepped iteration statements)  With varying punctuation and choices \nof keywords, the until and stepped iteration statements are very widespread. The concrete syntax above \nreflects the author s liklng for short mnemonic keywords and clean ALGOL-like punctuation. These two \nstatements add new nodes testing a as well as entering a and leaving a to G, The new arcs are as indicated \nin Figure 5,2. As in Figure 5.1, the arcs named q or qk are not in A. but are expected to be in the induced \ngraph Ga. In contrast to the pictures commonly drawn in discussions of structured programming or graph \ngrammars for generating classes of flowcharts [DDH72, Sec. 1.7; LM75], Figures 5.1 and 5.2 have no sourceless \nor targetless arcs and no unstated conventions for joining graphs along such dangling arcs. The advantages \nof our greater explicitness be\u00adcome apparent when control structures other than those of classical structured \nprogramming are diagramed, as in [R077 a]. The seven control operators considered here will be collectively \ncalled CSP operators (for classical structured programming). In addition to (5. 1), the important properties \nof CSP operators are as follows. Associated with each statement a built from a CSP operator is a graph \nEGa, the expected induced graph for a, that is determined (apart from the names of the nodes and arcs) \nby the operator used to form a. Thus EGa, and EGa2 for any two while statements a, and a2 are as in Figure \n5.1, but with a, and a2 in place of a and with names of the loop bodies of a, and a2 in place of ~. Apart \nfrom renaming and variations in the number K of statement arguments taken by each control operator like \ncase or sequential composition, there are only finitely many expected induced graphs, one for each CSP \noperator in our language, and each such graph EG has at most one cycle in EG; (5.2,1) INEGI and IAEGI \nare O(K); (5.2.2) there is a path from entering a to leaving a in EG; (5.2.3) no arc has source leaving \na in EG. (5.2.4) The finear functions of K in (5.2.2) will change if the family of CSP operators is changed. \nWe have omitted more elaborate loop building operators because the escape statements in the next section \nare sim\u00ad 52 pier and more powerful means to the same ends. The importance of (5.2.4) will emerge in Lemma \n6.6. To compose structured programs in the classical sense of [DDH72, Sec. 1.7], one builds all complex \nstatements with CSP oper\u00adators. Each simple statement has no effect on the flow of control: control enters \nthe statement, something happens, and control leaves the statement. This motivates the following definition. \nDEFINITION 5.3. The expected induced graph EG8 for a simple statement 8 has two nodes entering 8 and \nleaving 8 and one arc (entering 8, leaving 8). A simple statement 8 is classical iff the nodes and arcs \ncontributed by 8 to G are given by N&#38; = { entering 6, leaving 8 j; (1) Atl = { (entering 6, leaving \n8) }. (2) A program is classical iff every simple statement in it is classical and every complex statement \nis built by a CSP operator. LEMMA 5.4. Let G be the graph for a classical program. Then each statement \na has ENTRa = { entering a ] and EXITa = { leaving a ] and Ga = EGa. We can now do much of the work \nof data flow analysis for classi\u00adcal programs at the time of language definition, long before any specific \nprograms are compiled. With each expected fknv scheme (EGLx, { entering a }) we can associate an expected \nflow cover EXa as soon as pictures like those in Figure 5.1 and Figure 5.2 are available. By Lemma 5.4, \nEXa will indeed be a flow cover for the actual in\u00adduced scheme (Ga, ENTRa) when we encounter a specific \nstatement a in a specific program. By Theorem 4.4, SOLVE can find a good solution to a global flow problem \nby moving through the parse tree and evaluating formal expressions tabulated by the expected flow covers. \nHaving fixed the set of CSP operators, we can estimate the costs of evaluating tbe expressions rather \nsharply. The elaborate time bound in Theorem 4.4 will reduce to the assertion that the cost of data flow \nanalysis is 0( [ 2 I t@ ). The expected flow covers are displayed in the following series of lemmas, \nbeginning with the sim\u00adpler ones. Since m is always entering a in EXa(m, p), it will be con\u00advenient to \nwrite * rather than entering a. LEMMA 5.5. Let a be a simple statement. Then EXa is a flow cover for \nthe expected flow scheme, where EXa(*, *) = k and EXa(*, leaving a) = (c) for the arc c from * to leaving \na in EGa. (We omit Lemma 5.6 for one part conditional statements.) LEMMA 5.7. Let a be a two part conditional \nor case statement with parts fll, .... PK. Then EXa is a flow cover for the expected flow scheme, where \nEXa(fr, *) = A; EXa(fr, entering&#38;) = (bk); EXa(fr, leaving&#38;) = (bk, qk) fOr k = 1, .... K; EXa(i?, \n[eavinga) = [(el)o(bl, q,)] A ... A [(eK)O(bK, qK)]. (We omit Lemma 5.8 for sequential compound statements.) \nLEMMA 5.9. Let a be a while statement and let $ be (b, q, a) in Figure 5.1 for a. Then EXa is a flow \ncover for the expected flow scheme, where EXa(fr, *) = A @ ~ and EXa(*, entering ~) = (b) @ $; EXa(*, \nleaving ~) = (b, q) @ $; EXa(*, leaving a) = (e) @ $. (We omit Lemma 5.10 for until statements and Lemma \n5.11 for stepped iteration statements. The y are much like Lemma 5.9 but use expressions [... @ $] o \n(i) as needed.) THEOREM 5.12. Let F be a global flow problem derived from a classical program. Then P \nis well nested. Using the given flow cover EXa for each statement a, SOLVE finds a good solution. The \ntime required (apart from input/output) is 0( I Z I .t~ ). Proof That P is well nested follows from Lemmas \n5.4-5.11, with (4. 1) holding because the parse tree is indeed a tree. By Theo\u00adrem 4.4, SOLVE finds a \ngood solution. All that remains is to special\u00adize the time bound in the theorem. When the pairs (m, p) \nin ENTRa x NGa are ordered in the obvious ways suggested by the displays of EXa in the lemmas, the j-th \nEXa(m, p) evaluation in LOCALMAPS requires (Scr)j steps with (Sa)j s t@ + 2 except (Scr)2K+2 = 2K -1 \nin Lemma 5.7. Therefore Ta in Theorem 4.4 is 0( I AGa I + ]NGa I.t~ + INGa I ). By t~ 2 1and (5.2.2), \nTa is 0( Kt@ ) for K = IPARTa I, Therefore Tal + ... + Talxl is 0( IZ It@ ). But INGI isO( 1X1 )by(5.2.2) \nand(5.1.1) and lNoal s lNGal, so the total time is 0( I2 I.t@ ). For example, consider the syntax directed \navailable expressions analysis in [TK76, p. 362]. The stated system of equations is inade\u00adquate because \nCase 4 uses OUT(S1. x) for x # IN(S1) but the four cases only define OUT(S, x) for x = IN(S). After the \nobvious correc\u00adtions it is clear that the method of [TK76] is a special case of SOLVE that exploits the \nidempotence of the monoid M. 6. CONTROL STRUCTURES: ESCAPES AND JUMPS Complex statements in programs \ncommonly have goals, as when one writes a statement that searches a file for the record with a given \nkey. For a large file with a complicated indexing structure, the search statement could be quite complicated. \nFor example, a : (find the record) might be elaborated in a top-down manner as a : [B,: (look in fast \nmemory); P2 : (look in slow if necessary) ] (6.1.1) if part of the file is in fast memory and part is \nin slow. If /11 does find the record, then there is no need to proceed to @z. The goal of a has been \naccomplished and the programmer wants to ensure that control will leave a without anything else happening. \nIn a language like Bliss/ 11 [WU75] this intention can be expressed directly by writing 8: leave LABEL(a) \n(6.1.2) anywhere within a, perhaps as the then part of a conditional state\u00adment. Of course LABEL(a) is \nthe identifier used to label a in the program: any statement important enough to be left is important \nenough to be labelled. As an operator, leave takes the name of a statement as an argument rather than \nthe statement itself. With (6.1.2) we have N8 = { entering IS, leaving 3 ] as in Def. 5.3(l), but A8 \n= { (entering 8, leaving a) } instead of Def. 5.3(2). The expected induced graph from Def. 5.3 will not \nbe relevant for leave. Designat\u00aded entrances and exits are still as in (5.1,2), but now entering 8 is \nalso in EXIT~ for any f3 with 8 < ~ < a by (3.2.4). Of course one can avoid (6, 1) and stay within classical \nstructured programming by writing something like a : [ NOTYET := true; (31 : (look in fast memory); if \nNOTYET then /32 : (look in slow memory) 1 (6.2.1) and being careful to put enough assignments NOTYET \n: = false (6.2.2) into the elaboration of ~ ~. (Tests of NOTYET will also be needed. ) What useful purpose \nis served by this exercise? One can agree with Ledgard and Marcotty s criticisms of some of the literature \nrecom\u00admending escapes or jumps [LM75, p. 638] while contending that escapes (and perhaps jumps) should \nnevertheless be provided. Note that clarity and reliability are the main reasons for prefer\u00adring (6. \n1) to (6.2), The greater efficiency of (6.1), before the optimi\u00adzation contemplated on page 638 of [LM75 \n], is just a pleasant by\u00adproduct of expressing programmer intentions as directly as possible. No good \nis done when the programmer cleverly codes (6.1) as (6.2) and then the compiler cleverly optimizes (6.2) \nback to (6.1), Finally, note that leave is quite unlike the escape statements emphasized in [LM75] and \nits main references, Unlike escaping from the r-th en\u00adclosing loop or escaping to the next iteration \nof the r-th enclosing loop s body, leave escapes from whatever the programmer wants to escape from, referring \nto it by whatever name the programmer wants to use. There is no extraneous counting and no entangling \nof escapes with loops. If nonclassical simple statements are to be allowed at all, there is no point \nin allowing statements that are simultaneously less powerful, less readable, and more bothersome to implement \nthan is leave. One sometimes wants more power than leave provides. The event driven case statement [Kn74; \nZa74] addresses this need, but less conveniently than the llfollowc,d statementll introduced in [Ro77b, \nSec. 8]. To save space, we will only deal explicitly with the simpler escape statement leave in applying \nTheorem 4.4. Those who wish to handle more powerful escapes will find it easy to generalize this section \nin light of [Ro77b, Sec. 8]. DEFINITION 6.3. A complex statement a is semiclassical iff it is bui!t by \na CSP operator and satisfies ENTRa = { entering a ]; (1) RAGa -IAGa = Aoa U Up E PART. {ec Aalsec N~&#38;te= \nleaving a}. (2) A simple statement 8 is semiclassical iff it is either classical or a leave statement. \nA program is semiclassical iff each simple statement is semiclassical and each complex statement is built \nby a CSP operator. Equations (1) and (2) above are as in [Ro77b, (4.5) and (4.7)], where semiclassical \nis only applied to complex statements. The other applications have been added to bring out the parallels \nbetween semiclassical programs and the classical programs of Def, 5.3. (We omit Lemma 6.4, which says \nthat all complex statements in a semiclas\u00adsical program are semiclassical and goes on to show how to \nderive the actual induced graphs for such statements from their expected in\u00adduced graphs.) LEMMA 6.5. \nLet a be a semiclassical simple statement, Then Xa is a flow cover for the induced flow scheme (Ga, ENTRa), \nwhere Xa = EXa if a is classical; Xa(*, *) = A and Xcr(*, leaving a) = T if a is a leave statement. LEMMA \n6.6. Let a be a complex statement in a semiclassical program. Let Ya be the result of adjusting the expected \nflow cover EXa to avoid using any imaginary arc of EGa that is not in Ga, as in Lemma 2.14. Let EAa be \nthe set of all e in RAGa IAGa AOa such that If(f3, entering ~, se) = 1, where ~ is the part of a with \ne e A/l. Then Xa is a flow cover for the induced flow scheme (Ga, {*]), where each node p in NGa has \nXrs(*, p) determined by one of the following cases: Xa(-ir, p) = Y(l(*, p) if p is in EGa but p # leaving \na; (1) Xa(*, p)= T if p < EXIT/3 DEXIT/3 and fI(~, entering ~, p) = (); (2) Xa(*, p) = (i) o XCV(*, \nentering (3) if p c EXIT~ DEXIT~ and i = (enlering ~, p) is in IAGa; (3) Xa(*, p) = Ya(*, p) A A ~ ~ \n~Ae [(e) o Xa(ir, se)] if p = leaving a. (4) We can now generalize Theorem 5.12 to allow escape statements. \nThe new time bound in the following theorem is similar to the bound for the method of Graham and Wegman \nas applied to semiclassical programs [GW76, Thins. 4.2 and 5.4]. THEOREM 6.7. Let P be a global flow \nproblem derived from a semiclassical program. Then P is well nested. Using the flow cover Xa from Lemma \n6.5 or Lemma 6.6 for each statement a, SOLVE finds a good solution. The time required (apart from input/output) \nis 0( I X ] .t~ + (newexits) ), where (newexits) is the sum over all a in Z of the numbers I EXITa I \n 1. Proof We proceed much as in the proof of Theorem 5.12, using Lemmas 5.4 5.11 and 6.4 6.6. The Xa(m, \np) evaluations for p in EGa with p # leaving a are ordered as suggested by the displays of EXa in the \nlemmas from Section 5. Then Xa(m, p) is evaluated for each p in CTa not in EGa (and hence in EXIT(3 -DEXIT/3 \nfor ~ a part of a). Finally, we evaluate Xa(m, leaving a). As in Theorem 5.12, the time (Sa)j for the \nj-th Xa(m, p) evaluation with p in EGa has (Sa)j < t@ + 2 except perhaps for the last one, with p = leaving \na. In this one case Lemma 6.6(4) yields (S~)i s 1 + (old) + (Ieavenum 1) + (leavenum), where (old) is \nthe old bound from the proof of Theorem 5.12 on the time required to evaluate EXa and hence Ya for (m, \nleaving a) and (leavenum) is the number of leave LABEL(a) statements within a. There are also (leavenum) \n+ I EXITa I 1 choices of j with p not in EGa and (Sa)j < 1 in Lemma 6.6(2) and Lemma 6.6(3). The term \nTa from Theorem 4.4(2) is therefore 0( lAGal +(1 NEGal l).(t~ +2)+ lEXITal -1+ (old) + (leavenum) + pairs(a) \n), where EGa has node set NEGa and arc set AEGa such that I AGcr I < I AEGa I + 2.(leavenum); pairs(a) \n< I NEGcr I + (leavenum) + I EXITa I 1. Now (leavenum) is at most the sum of all the numbers I EXIT~ \nI 1 for ~ in PARTa. For K = \\PARTa I, IAEGa I and INEGa I are 0( K ) by (5.2.2) and (old) is linear \nin K.t~, so Ta is O(K.t~ + (l EXIT/311 1) + ... + (l EXITf3Kl 1) + (l EXITa[ l)). But I NG I is still \n0( I Z I ), so the total time from Theorem 4.4(1) is O(Tal + ... + Ta I~ I ), which is the desired bound. \nVague assertions about the importance of single-entry/single\u00adexit control structures are ubiquitous in \nthe literature and folklore of structured programming. One precise meaning for such assertions might \nbe that syntax and semantics are so simply related as to permit syntax directed data flow analysis at \na cost linear in some reasonable measure of program size. When the size of a program is the number of \nstatements in it, multiplied by t@, Theorem 5.12 shows that classi\u00ad cal structured programming is sufficient \nfor such analyzability. Theo\u00adrem 6.7 shows that the escapes needed for practical structured pro\u00adgramming \ndo not destroy this analyzability, provided they ar~ used in rnoderafion: the number (newexits) should \nbe fairly small compared to the size of the program. Because LOCALMAPS in SOLVE uses a given flow cover \nfor each statement s induced flow scheme, the time bound in Theorem 6.7 would be of little interest if \nfinding flow covers required more time than using them did. Happily, Lemma 6.6 shows that we can pass \nfrom the expected flow cover to the actual one very quickly, provided we know the various path bits fI(/3, \nm, n). Finding all these bits can be done in time 0( 12 I + (newexits) ) by the obvious adaptation of \nthe rules for computing whether a variable can be preserved along some path through a statement [Ro77b], \nand the use of Xa(m, p) in LOCALMAPS can be reinterpreted as including discovery of Xa(m, p) without \nchanging the time bound. This sug\u00adgests a strategy for dealing with programs that are not known to be \nsemiclassical. For definiteness, let us assume that the only kind of statement besides those already \nconsidered is the simple jump: $: goto LABEL(a). (6.8) With (6.8) we have Nc$ = { entering 8, leaving \n8 } as in Def. 5.3(l), but A8 = { (entering 8, entering a) ] instead of Def. 5.3(2). As with (6. 1.2), \nthe expected induced graph EG8 is irrelevant, designated entrances and exits are still as in (5. 1.2), \nand entering 8 is also in EXIT~ for any ~ with 8 < ~ < a except in the special case where a happens to \nbe 8. But now we may have nondesignated entrances as well. In (3,2.3) we find that entering a is also \nin ENTR~ for any /3 with =(8 s ~) and a < ~. There is a new real arc in Gy for the small\u00adest y that includes \nboth 3 and a. Unlike the arcs added by escape statements, these new arcs may add many new paths to Gy, \ninchrding cycles. There is no neat formula like Def. 6.3(2). Our stategy for coping with jumps is based \non a cautiously opti\u00ad mistic version of Murphy s Law: WHATEVER CAN GO WRONG WILL, BUT NOT OFTEN. A compiler \ns lexical analysis phase can easily detect the presence of jumps in a program. Well written pro\u00adgrams \nin well designed languages will often be free of jumps and hence semiclassical. Most of the statements \nin a program that does have jumps will still be semiclassical, though they may have exits due to goto \nas well as to leave. Lemma 6.6 actually holds for any semi\u00adclassical complex statement, even when the \nprogram as a whole is not semiclassical. The flow cover discovery in LOCALMAPS can be written as an easy \ntest for whether a is semiclassical, followed by use of Lemma 6.5 or Lemma 6.6 in the common case where \nthis is true. When a is complex and not semiclassical, the flow cover discovery routine will need to \nconstruct the induced graph Ga. The time re\u00adquired by the crude flow cover algorithm (from the omitted \nLemma 2. 13) is cubic in the size of Ga, but it is only the size of Ga which occurs. No practical estimates \nof the size ratio between this graph and the graph of the entire program are available. If jumps are \nused in such a way that few statements fail to be semiclassical and those that do have small induced \ngraphs, then even crude handling of these statements by LOCALMAPS need not drastically change the running \ntime of SOLVE on large programs. As with escape statements, mod\u00aderation is the key. In an ambitious optimizing \ncompiler along the lines of [Kn74; L077] we must also cope with introduced jumps. The original input \nto the compiler has few if any jumps, but subsequent processing can add them. Fortunately, there is no \nneed to treat added jumps like original jumps. Precisely because jumps are added according to well under\u00adstood... \n[and] well documented mechanical operations [Kn74, p. 282], there is no need to apply Lemma 2.13 to the \nnew induced graphs. Part of understanding and documenting the operations is drawing figures like [Ro77b, \nFig. 5] for while loops governed by Boolean expressions with side effects. As with the loops of classical \nstructured programming, we can find the flow cover long before the compiler encounters a specific while \nstatement in a specific program. In writing the compiler we need only write the use of this known flow \ncover into the handling of while statements by LOCALMAPS. 7. SEMILATTICES OF FINITE HEIGHT DEFINITION \n7.1. Let H be a positive integer. The semilattice L has height H iff there is a strictly descending chain \nX. > Xl >... > xH such that all other strictly descending chains are at most thk long. If L has height \nH then M is rapid, but a tighter bound on t~ in Def. 2.2 can be obtained if we consider the effective \nheight instead of the actual height. To define effective heights we recall the general cartesian product \nconstruction. Given two sets S~ and S2, the usual cartesian product S, x S2 is well known. The general \nconstruction is similar, but it begins with an arbitrary family of sets: for each q in some set Q, we \nare given a set Sq. Given such a family, a Q -?uple is any map x with domain Q, such that the value Xq \nof x at q is a mem\u00adber of Sq We wrote Xq rather than our usual xq to stress the similari\u00adty between general \nQ-tuples and the more familiar n-tuples (xl, .... Xn), which correspond to the special case Q = {1, .... \nn}. The carte\u00adsian product of the family { Sq I q c Q } is the set of all Q-tuples. It is denoted X ~ \n~ ~ Sq. If each Sq happens to be a semilattice, then the product is also a semilattice, with the obvious \nordering x < y in Xq , ~ S~ iff x,< Yqin Sqfor all q. DEFINITION 7.2. A factoriza~ion of L and M is any \nfamily { (Lq, Mq) \\ q . Q ] of pairs, where each Lq is a complete meet semilattice and Mq is a closed \nmonoid of isotone maps on L, such that there are semilattice isomorphisms that make the following diagram \ncommute: L< Xq. QLq (The left downward arrow is application of a map to an argument, while the right \ndownward arrow is application of the q-th map to the q-th argument.) For example, let Q be the set of \nall expressions in a program. For each expression q, we can indicate whether the expression is available \nor not by using the semilattice Lq = {O, 1 ] with the usual ordering and the monoid Mq generated by the \nmaps describing how a block of text kills or generates the expression [GW76, p. 178; U173, p. 193]. The \nheight of Lq is 1. Instead of solving a separate global flow problem for each expression q, it is usual \nto use a bit vector with one position for each expression: the parallelism in AND and OR operations on \narrays I Q [ long of bits is exploited to get the net effect of solving I Q I problems in {0, 1] by solving \none problem in Xq . Q Lq. The above definition says why this works in a way that is completely independent \nof the details of AND and OR operations, the choice of each Lq, and so on. Factorization can exist and \nbe useful in many other contexts besides that of bit vectors. In the bit vector example )(q ~ Q Lq has \nheight I Q \\ , but it acts as if it had height 1, the common height of all the Lq, for all purposes of \ndata flow analy\u00adsis. After one more defkition we can prove a lemma that establishes thk kind of behavior \nin the general context. DEFINITION 7.3. The semilattice L has effective height H (relative to the monoid \nM) iff there is a factorization { (Lq, Mq) I q c Q } of L and M such that some Lq has height H and all \nLq have height at most H. LEMMA 7.4. Suppose L has effective height H, Given g and f in M, the loop products \n@, and @2 defined by g@lf=g f*andg@2f=A{g0fr lr~N} can be computed in at most t@= Flog2Hl +2stepsfor@= \n@l; (1) to =2H+1steps for@=(Q2. (2) COROLLARY 7.5. If L has effective height H then M is rapid with \nt@ s 2H + 1. If M is also distributive then @, = @~ and t~ < r log2H 1 +2. For any of the traditional \nbit vector problems the effective height is 1. Effective heights can be unpleasantly high in the constant \npro\u00adpagation monoid CP [KU76, p. 167] or in analyzing the ranges of values of variables [Ha77b]. In these \nmore ambitious forms of data flow analysis the monoids are also not distributive, so that @, is cheaper \nto compute but f@2 yields sharper information. The pro\u00adgrammer could be asked to specify which loop product \nis to be used when invoking an ambitious optimizing compiler. After run time measurements prior to optimization \nhave revealed which parts of the program are critical, the compiler could even be told to use @2 in a \nfew critical places and @, elsewhere. With high level analysis the programmer and the compiler both visualize \ncontrol flow in relation to the syntactic structure, so detailed communication is possible without an \nelaborate interface. Because of technical complexities in comparing SOLVE with any low level method, \nwe will only present one simple but instructive general comparison theorem. Assume L has effective height \nH. Obvious generalizations of the method of Graham and Wegman as presented in [GW76] allow it to be applied \nto any global flow prob\u00adlem for a classical (as in Def. 5.3) program. For example, the as\u00adsumption En. \n= -L is written into the definitions [GW76, pp. 176, 177] but is never used. (Thk is fortunate: the assumption \nfails in the attempted application to live-dead analysis [GW76, p. 199]. ) For a classical program the \nmethod finds the same solution that SOLVE does when @ is defined in a way that is a little better than \n@,. The same comparison holds for any semiclassical program, but a complete proof would be much too tedious \nto appear here, For each f in M let f( ) = (if f is fast then f A 1 else f ), (7.6.1) so that M is rapid \nwith g (@lGw f = g 0 f( ), (7.6.2) which may have g (@Gw f > g @, f when f is fast. THEOREM 7.7. Let \nP = (G, f, { entering r ], E) be a flow problem derived from a classical program rr. Let IGW be the good \nsolution found by the method of Graham and Wegman and let 1~0 be the good solution found by SOLVE with \n@Gw . Then IGW = l.s.. Proof To each node v in G, the method of [GW76] assigns a map @v in M such that \n@vEno = IGWV, where no = entering T. A variable graph G t and a variable assignment f i of members of \nM to arcs in G 1 are initialized to G and f. In the statement of Algorithm A [GW76, p. 184], G is denoted \nG and the computations on f are implicit. Details are in the lemmas from [G W76, Sec. 4]. Algorithm \nA eventually reduces G to a graph with no as the only node. If this graph has no arcs then @v = 1 for \nv = no. Otherwise it has one arc and @v= [f t(v, v)]{ ) for v = no For v # no, @v is determined when \nv is removed from G by the T2 or T3 transformation. In both cases v has a unique inarc (u, v) just \nbefore its removal from G , and 0, = f (u, v) o 41U. Of course @u is not known at this time, but associating \nthe pair (u, f (u, v)) with v will allow @v to be computed later in a pass over the nodes of G that reverses \nthe order in which they were removed from G The algorithm is such that (u, v) can only appear in this \nway in G if u dominates v in G, and then f (u, v) at the time v is removed from G will summarize what \nis known about paths from u to v that do not return to u before reaching v. Thus the intuition behind \nf t (u. v) is much like that behind fa c in (3.7.2), but the pairs (u, v) that arise in [GW76] are governed \nby properties of the entire control flow graph. Only in classical structured programming do these properties \nrelate to source level syntax in the simple way described below. If a is one of the three kinds of loop \nstatement, let head a be entering a if a is a while statement and testing a if a is a stepped itera\u00adtion \nstatement. Otherwise a has the form until do /3 and head a is entering /3. The set T initially computed \nby Algorithm A is precisely the set of all head a for a a loop statement in r. The initial choice of \nh from T is the head of an innermost loop, and subsequent computa\u00adtions of T just remove the previous \nh value and choose the next one to be innermost among the remaining members of T. For h = head a and \n{~] = PARTa the corresponding set S in Algorithm A includes h, resting a if a is an until or stepped \niteration statement, and all nodes of N~ that are still in G There are no other nodes in S. The call \non Reducesec(S, h) removes all nodes in S {h] except those with out\u00adarcs whose targets are not in S. \nIf a is an until statement then testing a persists after Reduceset(S, h), but otherwise all nodes in \nS {h] have been removed. At this point the next loop to be processed is chosen. When all loops have \nbeen processed in this way, G will be acyclic except for arcs from nodes to themselves. The above relation \nbetween Algorithm A and the syntax of n underlies the proof that @v = Vv for all nodes v, where kV is \ndefined by moving top-down through 2. If v is in Non then Tv = FT(no, v). Otherwise v is in NO/3 for \nunique ~ in PARTa for unique a, and Vu for u = entering a is already available. Let ~v = FB (entering \nB, V) o Fa(u, entering /3) o kU. That @v = T, can be proved by annotating Algorithm A with appropriate \ninductive assertions. This is tedious but not difficult. Each call on LOCALSOLVE in SOLVE has ENTRy = \n{ errtering y }, so I~Ov = ~vEnO follows from the program\u00adming in Section 4. Finally, IGWV = @vEno = \nYvEnO = ISOV. 9 COROLLARY 7.8. As in Theorem 7.7, the good solution 12 found by SOLVE with f@2 has IGW \n<12. ProoJ For f,, f2in Mwith f, <f2wehave g(@Gw fl <g@2 f2 and f, @ ~W g < f2 @2 g. This implies that \ninterpreting a flow expression with (@Gw for @ yields a value < the value obtained with @2 for@. Therefore \n1~0 s 12 and so IGW s 12. 8. CONCLUSIONS AND OPEN PROBLEMS Monoids that arise in data flow analysis can \nbe classified by the presence or absence of three properties: idempotence, distributivity, and rapidity. \nSolutions to global flow problems that are greater than any fixpoint are preferred when M is not distributive, \nas when con\u00adstant propagation uses compile time arithmetic [KU76, p. 167] or when elaborate analyses \nof the ranges of values of variables are exploited [Ha77b]. For the very ambitious compilers contemplated \nin [Ca77; Ha77a; Ha77b; Kn74; L077], one wants solutions better than fixpoints but cannot hope for optimal \nsolutions [KU77, Thin. 7]. Like [GW76; Ta75], our algorithm SOLVE finds a good solution: one at least \nas large as any fixpoint. In analyzing structured programs that avoid escapes and jumps, SOLVE does as \nwell as [GW76] and some\u00adtimes better. By choosing the loop product (@ in various ways, the implementor \ncan trade time for sharpness of information. Roughly speaking, [GW76] corresponds to choices that emphasize \nspeed. (Similarly for [Ta7 5].) For programs that fall within classical struc\u00adtured programming or that \nuse escapes but not jumps, the time bounds for SOLVE and [GW76] are similar. Because we did not assume \nI E [ = 1 in Section 2 and because the basic hierarchy in Section 3 is symmetric regarding entrances \nand exits, it is an easy exercise to adapt SOLVE to problems like the detection of dead variables, wherein \ninformation flows backwards along arcs and is given initially for exit nodes at the time of exit. Like \nmost methods, that of [GW76] can also be so adapted. The lack of entrance/exit symmetry makes the task \nmore difficult [G W76, pp. 199, 200] than with SOLVE. Contrary to what might be expected from the trend \nin program\u00adming complexity observed in going from [KU76] to [GW76] to [Ta75], SOLVE is a remarkably simple \nalgorithm. This combination of power and simplicity is obtained by using a hierarchical representa\u00adtion \nof control flow instead of the usual large graph representing the entire program after translation to \na relatively low level intermediate text. Low level methods solve a problem regardless of where it came \nfrom. High level methods like SOLVE remember and exploit the structure of the program that gives rise \nto a problem. We have used structure expressed by the parse tree, but the general formulation of SOLVE \nand its correctness/cost theorem in Section 4 are applicable to other hierarchies as weu, such as the \none derived from a low level representation by interval analysis [AC76; U173]. Ambitious compilers need \nto update the results of data flow analysis to reflect program changes, so that the opportunities that \none optimization creates for another are seized. The advantages of high level analysis for thk purpose \nare sketched in [Ro77b, Sec. 12] and are implicit in the first two paragraphs of the proof of Theorem \n7.7 here. High level analysis also leads directly to concise but informative data flow diagnostics at \nsource level [Ro77a, Sec. 6]. In addition to the compiling applications we have emphasized, high level \nrepresenta\u00adtions of control flow are useful in denotational semantics [Ro77a, Sec. 3] and program proving \n[La77, p. 140; R076; Ro77a, Sec. 7]. The problem of efficiently finding a flow cover for any global \nflow scheme not covered by the lemmas in Sections 5 and 6 has been left open here. Such schemes will \nbe extremely rare in structured programming, even with the escapes and occasional jumps of practical \nstructured programming. The problem is still worthwhile, and it could become urgent in an application \nwhere the hierarchy does not come from the parse tree. The techniques in [Ta75] may be useful here. Another \nopen probIem lacks a crisp mathematical formulation but is quite important. Many small examples are known \nwhere [GW76] or SOLVE finds a good solution better than any fixpoint. But real compilers deal with programs, \noften very large ones, that are written to compute something rather than to illustrate the pros and cons \nof data flow analysis methods. In ambitious compilers with nondistribu\u00adtive data flow monoids, are these \nnonfixpoint solutions significantly better, in that they permit more extensive optimization? The tradi\u00adtional \ndata flow monoids are idempotent as well as distributive. Data flow problems can be solved very quickly, \nbut the answers are less informative than with ambitious monoids. But are they significantly less informative \nfor optimization in the real world? Such open ques\u00adtions will not be answered simply YES/NO and will \nrequire a combi\u00adnation of theoretical and experimental investigation. REFERENCES AC76. Allen, F. E., \nand Cocke, J. A program data flow analysis procedure. Comm. ACM 19 (1976), 137-147. Ca77. Carter, J.L. \nA case study of a new code generating technique for compilers. Comm. ,4 CM 20 (1977), to appear. CC77. \nCousot, P., and Cousot R. Abstract interpretation: a unified lattice model for static analysis of programs \nby construction or approximation of Flxpoints. Proc. 4th ACM Symp. on Princi\u00adples of Programming Languages \n(January 1977), 238-252. DDH72. Dahl, O. J., Dijkstra, E. W., and Hoare, C.A.R. Structured Programming. \nAcademic Press, London and New York, 1972. GW76. Graham, S.L., and Wegman, M. A fast and usually linear \nalgorithm for global flow analysis. J. ACM 23 (1976), 172\u00ad 202. Ha77a. Harrison, W.H. A new strategy \nfor code generation the general purpose optimizing compiler. Proc. 4rh ACM Symp. on Principles of Programming \nLanguages (January 1977), 29-37. Ha77b. Harrison, W.H. Compiler analysis of the value ranges for variables. \nIEEE Trans. on Software Engineering 3 ( 1977), 243-250. KU77. Kam, J. B., and Unman J.D. Monotone data \nflow analysis frameworks. Acts Informatica 7 ( 1977), 305-317. KU76. Kam, J. B., and Unman, J.D. Global \ndata flow analysis and iterative algorithms. 1 ACM 23 (1976), 158-171. Ki73. Kildall, G.A. A unified \napproach to global program optimiza\u00adtion. Proc. ACM Symp. on Principles of Programming Languages (October \n1973), 194-206. Kn74. Knuth, D.E. Structured programming with goto statements. Computing Surveys 6 ( \n1974), 261-302. La77. Lamport, L. Proving the correctness of mtdtiprocess pro\u00adgrams. IEEE Trans. on Software \nEngineering 3 (1977), 125\u00ad 143. LM75 Ledgard, H. F., and Marcotty, M. A genealogy of control structures. \nComm. ACM 18 ( 1975), 629-639. L077. Loveman, D.B. Program improvement by source to source transformation. \nJ. ACM 24 (1977), 121-145. R076. Rosen, B.K. Correctness of parallel programs: the Church-Rosser approach. \nTheoretical Computer Science 2 (1976), 183-207. Ro77a. Rosen, B.K. Applications of high level control \nflow. Proc. 4th ACM Symp. on Principles of Programming Languages (January 1977), 38-47. Ro77b. Rosen, \nB.K. High level data flow analvsis. Comm. ACM 20 (1977), 712-724. TK76. Taniguchi, K., and Kasami, T. \nAn O(n) algorithm for com\u00adputing the set of available expressions of D-charts. Acts Inforrnatica 6 ( \n1976), 361-364. Ta75. Tarjan, R.E. Solving path problems on directed graphs. Rept. STAN-CS-75-528, Computer \nSci. Dept., Stanford U., No\u00advember 1975. Ta76. Tarjan, R.E. Iterative algorithms for global flow analysis. \nRept. STAN-CS-76-547, Computer Sci. Dept., Stanford U., March 1976. U173 . Unman, J.D. Fast algorithms \nfor the elimination of common subexpressions. Acts Znformatica 2 (1973), 191-213. We75. Wegbreit, B. \nProperty extraction in well founded property sets. IEEE Trans. on Software Engineering 1 ( 1975), 270\u00ad \n285. WU75. Wulf, W. A., et al. The Design of an Optimizing Compiler. American Elsevier, New York, 1975. \nZa74. Zahn, C.T. A control statement for natural top-down struc\u00ad tured programming. Lecture Notes in \nComputer Sci. 19 (1974), 170-180. ZB74. Zelkowitz, M. V., and Bail, W.G. Optimization of structured \nprograms. Software Practice and Experience 4 (1974), 51-57. a n b -m  bl q ql b2 t e! qz b3 Figure \n5.1. The node entering a appears as an upper half circle E marked a. The node leaving a appears as a \nlower half circle E marked a. Conditional, while, and sequential compound E statements are illustrated. \nbl E ** q qK v Figure 5.2. The node testing a appears as a diamond marked a. a One part conditional, \ncase, until, and stepped iteration, statements are illustrated. i aecf a -3-\u00ad \n\t\t\t", "proc_id": "512760", "abstract": "<p>The earliest data flow analysis research dealt with concreteproblems (such as detection of available expressions) and with lowlevel representations of control flow (with one large graph, eachof whose nodes represents a basic block). Several recent papershave introduced an abstract approach, dealing with any problemexpressible in terms of a semilattice L and a monoid M of isotonemaps from L to L, under various algebraic constraints. Examplesinclude [CC77; GW76; KU76; Ki73; Ta75; Ta76; We75]. Several otherrecent papers have introduced a high level representation with manysmall graphs, each of which represents a small portion of thecontrol flow information in a program. The hierarchy of smallgraphs is explicit in [Ro77a; Ro77b] and implicit in papers thatdeal with syntax directed analysis of programs written within theconfines of classical structured programming [DDH72, Sec. 1.7].Examples include [TK76; ZB74]. The abstract papers have retainedthe low level representations while the high level papers haveretained the concrete problems of the earliest work. This paperstudies abstract conditions on L and M that lead to rapid data flowanalysis, with emphasis on high level representations. Unlike someanalysis methods oriented toward structured programming [TK76;Wu75; ZB74], our method retains the ability to cope with arbitraryescape and jump statements while it exploits the control flowinformation implicit in the parse tree.</p><p>The general algebraic framework for data flow analysis withsemilattices is presented in Section 2, along with some preliminarylemmas. Our \"rapid\" monoids properly include the \"fast\" monoids of[GW76]. Section 3 relates data flow problems to the hierarchies ofsmall graphs introduced in [Ro77a; Ro77b]. High level analysisbegins with local information expressed by mapping the arcs of alarge graph into the monoid M, much as in low level analysis. Buteach arc in our small graphs represents a set (often an infiniteset) of paths in the underlying large graph. Appropriate members ofM are associated with these arcs. This \"globalized\" localinformation is used to solve global flow problems in Section 4. Thefundamental theorem of Section 4 is applied to programs with thecontrol structures of classical structured programming in Section5. For a given rapid monoid M, the time required to solve anyglobal data flow problem is linear in the number of statements inthe program. (For varying M, the time is linear in the product ofthis number by t<sub>@</sub>, where t<sub>@</sub> is a parameter ofM introduced in the definition of rapidity.) For reasons sketchedat the beginning of Section 6, we feel obliged to cope with sourcelevel escape and jump statements as well as with classicalstructured programming. Section 6 shows how to apply thefundamental theorem of Section 4 to programs with arbitrary escapesand jumps. The explicit time bound is only derived for programswithout jumps. A comparison between the results obtained by ourmethod and those obtained by [GW76] is in Section 7, which alsocontains examples of rapid monoids in the full paper. Finally,Section 8 lists conclusions and open problems. Proofs of lemmas areomitted to save space. The full paper will resubmitted to ajournal.</p><p>We proceed from the general to the particular, except in someplaces where bending the rule a little makes a significantimprovement in the expository flow. Common mathematical notation isused. To avoid excessive parentheses, the value of a function f atan argument x is fx rather than f(x). If fx is itself a functionthen (fx)y is the result of applying fx to y. The usual&iexcl;&#220; and &iexcl;&#221; symbols are used for arbitrarypartial orders as well as for the usual order among integers. Afunction from a partially ordered set (<i>poset</i>) to a poset is<i>isotone</i> iff x &iexcl;&#220; y implies fx &iexcl;&#220; fy.(Isotone maps are sometimes called \"monotonic\" in the literature.)A <i>meet semilattice</i> is a poset with a binary operation&iexcl;&#196; such that x &iexcl;&#196; y is the greatest lowerbound of the set {x, y}. A meet semilattice wherein every subsethas a greatest lower bound is <i>complete.</i> In particular, theempty subset has a greatest lower bound T, so a complete meetsemilattice has a maximum element. A <i>monoid</i> is a settogether with an associative binary operation &amp;compfn; that hasa unit element <b>1</b> : <b>1</b> &amp;compfn; m = m &amp;compfn;<b>1</b> = m for all m. In all our examples the monoid M will be a<i>monoid of functions:</i> every member of M is a function (from aset into itself), the operation &amp;compfn; is the usualcomposition (f &amp;compfn; g)x = f(gx), and the unit <b>1</b> isthe identity function with <b>1</b>X = x for all x. Twoconsiderations governed the notational choices. First, we speak inways that are common in mathematics and are convenient here.Second, we try to facilitate comparisons with [GW76; KU76; Ro77b],to the extent that the disparities among these works permit. Onedisparity is between the meet semilattices of [GW76; KU76; Ki73]and the <i>join</i> semilattices of [Ro77b; Ta75; We75], whereleast upper bounds are considered instead of greatest lower bounds.To speak of meets is more natural in applications that areintuitively stated in terms of \"what must happen on all paths\" insome class of paths in a program, while to speak of joins is morenatural in applications that are intuitively stated in terms of\"what can happen on some paths.\" By checking whether there are anypaths in the relevant class and by using the rule that <b>3</b> isequivalent to &amp;dlcrop;V&amp;dlcrop;, join oriented applicationscan be reduced to meet oriented ones (and vice versa). A generaltheory should speak in one way or the other, and we have chosenmeets. For us, strong assertions about a program's data flow arehigh in the semilattice.</p>", "authors": [{"name": "Barry K. Rosen", "author_profile_id": "81100316668", "affiliation": "IBM Thomas J. Watson Research Center, Yorktown Heights, New York", "person_id": "P28116", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/512760.512767", "year": "1978", "article_id": "512767", "conference": "POPL", "title": "Monoids for rapid data flow analysis", "url": "http://dl.acm.org/citation.cfm?id=512767"}