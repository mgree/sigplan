{"article_publication_date": "01-01-1978", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee.&#38;#169; \n1978 ACM 0-12345-678-9 $5.00 A Portable Compiler: Theory and Practice S. C. Johnson Bell Laboratories \nMurray Hill, New Jersey 07974 EXTENDED ABSTRACT A compiler for the C language has recently been constructed \nwhich is now compiling C for about half a dozen machines. The compiler was influenced in various ways \nby recent theoretical developments. This paper gives an overview of the compiler structure and algorithms, \nemphasizing those areas where theory was helpful, and discussing the approaches taken where theory was \nlacking. Introduction 2. Provide reasonable code quality initially. For example, interpretive code was \nThe C languagel is the principal pro\u00adruled out. However, the structuregramming language for the UNIX \n2T operat\u00adshould allow for later tuning to pro\u00ading system, running on PDP-11 computers. duction quality \noutput. Not only is the system itself almost entirely written in C, together with most of the com-3. \nBe self diagnosing. The number of peo\u00adpilers and other support software, but most ple who both intimately \nunderstood of the applications which use UNIX are writ-the target machine s instruction set ten in C \nas well. and the inner workings of the compiler was expected to be small; therefore theThere are now \nseveral hundred instal\u00adcompiler should refuse to producelations of the UNIX system; this, together code \nrather than produce wrong code. with the general acceptance of C, has led to interest in providing C \ncapability on 4. Use state of the art tools. In particular, hardware other than the PDP-11. Whhin a it \nwas of interest to determine to what year after C was first introduced, Snyder3 extent the compiler could \ntake advan\u00adhad constructed a portable compiler, and tage of recent results in code genera\u00admade C available \non the Honeywell 635. tion theory. Snyder s compiler was very slow, and con\u00ad 5. Be compatible. On some \nsystems, not\u00ad tained some significant implementation ably IBM and Honeywell, we were difficulties. Moreover, \nthe C language was constrained to be compatible with evolving rapidly, and has continued to grow; existing \nsupport software and libraries. this made it difficult to keep different imple-For this reason, we had \nless say over mentations of C compatible. Accordingly, the procedure calling sequence than we we became \ninterested in providing a C com\u00adwould have liked, and also had to write piler that would: the compiler \nto produce assembler out\u00ad 1. Be easily movable. Moving to a new put conforming to the local assembler \npiece of hardware should take only a conventions. few months work. Moreover, It was recognized early \nthat the key to improvements in the C language achieving all of these goals was flexibility: should be \neasily transmitted to all the previous experiences with moving compilers compilers. had shown that the \nonly thing to expect was the unexpected! Issues like addressability in tUNIX is a Trademarkof Bell Laboratories \nthe IBM 370, character handling on the Honeywell 6000, or support of autoincre\u00adment and autodecrement \non the PDP-11 could easily end up dominating the design. This would be a grave mistake, since other similar \nissues were certain to arise on other machines, and would have to be dealt with as they arose. It is \nworthwhile mentioning some of the things which were not goals: 1. We were not interested in the desert \nisland portability problem: on a desert island with a bare machine, to provide a tape which. allows the \nbootstrapping of a language or program onto the machine. We are far too enamored of the working environment \nin the UNIX system, and find it too useful in the production of software, to be willing to give it up. \nMoreover, we were aided by extensive, if somewhat erratic, inter-computer communication facili\u00adties, \nwhich made cross-compiling toler\u00adable as a method of bootstrapping the compiler onto the target machine. \n 2. Compiler speed was not a primary goal, although we were not going to let inefficiency get out of \nhand! 3. We postponed active consideration of additional optimizing phases. We were more interested \nin portability than  optimization, and felt we should do research in one thing at a time. With these \ngoals well laid out, we investigated the relevant theory. Theory and Algorithms There seemed on the \nface of it to be several areas where theory might be applica\u00adble to the construction of portable com\u00adpilers. \nThere is potential for theoretical insight into such parts of the compiler as lexical analysis, parsing, \nsymbol table management, and code generation. It should be made clear what role theory ought to play \nin a practical compiler. It would be naive to expect to go to a theoretician and find a paper entitled \nAn Algorithm for Compiling C on the XYZ 1000 . The theory does not need to solve the whole problem, or \neven a very large part of it. What the theory should do is provide insight into the basic problems, at \nleast for simplified machines, and this insight should lead to a mechanism for structuring what is necessarily \na complicated program. To para\u00adphrase Hamming, The purpose of theory is insight, not theorems. Since \nthe theory is not expected to do everything, care must be taken when packaging the theory in tools, or \napplying it in a compiler, to permit graceful escapes to handle issues which must be dealt with in the \nlanguage, but happen not to be addressed by the theory. Examples might be the handling of comments in \na context free parser, or the handling of condition codes in a code generator. The first area where theory \ncould be applied to the compiler was in lexical and syntax analysis. Fortunately, the programs Yacc4 \nand Lex5 were available on the UNIX system, and provided a useful packaging of some relevant theory. \nYacc allows the user to provide a context-free grammar, together with some code fragments to be executed \nas each rule is recognized. An LALR(l) parser is constructed, incorporating the code fragments, and this \nserves as a parser. Moreover, through controlled use of ambi\u00adguous grammars, the many operators and precedence \nlevels of C can be easily dealt with. Lex does a similar job with lexical analysis; the theory applied \nis that of regular expressions. Both programs produce fast, practical modules from their respective specifications. \nOur use of these tools has greatly aided the initial design, construction, and maintenance of the compiler. \nThe situation is not totally rosy, however. Cer\u00adtain areas of the C language, notably declaration processing \nand initialization, require the simulation of top down or corou\u00adtine organizations in order to naturally \npro\u00adcess the semantics. The bottom up nature of the parser generated by Yacc made these actions harder \nto write than they might have been under other circumstances. Neverthe\u00adless, the overall benefits of \nstructuring the parser with these tools far outweighed such localized problems. Symbol table management \nis an area regarded as well understood by theoreti\u00adcians. Algorithms for hashing and list pro\u00adcessing \nare well known, widely taught, and extensively analyzed in theory. Neverthe\u00adless, the application of \nthese standard tech\u00adniques to C was exceptionally difficult. The symbol table manager was rewritten about \na dozen times before becoming acceptably efficient and bug free. It then had to be totally redone again \nwhen block structure was added to C. There is an interaction of issues here which is characteristic of \nmany practical applications of theoretical work. For example, measurements showed that the speed of symbol \ntable access was an important component of the compiler speed. Thus, simple but inefficient methods, \nlike linear search, would not do. Hash tables were, of course, attractive. On the other hand, the limited \naddress space of the PDP-11 meant that the linking and chaining methods of resolving collisions, so \nattractive in theory, had to be looked at with a jaun\u00addiced eye. Block structure meant that one had to \nremove symbols from the hash table upon block exit. However, some entries made within an inner block \n(for example, as a result of function calls) had to remain throughout the rest of the function in order \nthat the target assembler could be given the necessary information at the appropriate time and place. \nThe algorithm finally chosen was a hash table algorithm in which collisions are resolved by linear search \nthrough successive entries. Yes, that has very bad behavior when the table gets full ! It permits us, \nhow\u00adever, to do the necessary management of the symbol table upon block exit with a very small amount \nof rehashing. This example allows us to make another point about the interaction of theory with practical \napplications. The hashing algorithm has no interesting or significant theoretical properties. It does \nhave an important practical property, however: it is fast enough. More precisely, the amount of time \nspent doing symbol table management is now a small enough fraction of the total compiler time that there \nis little to be gained in tuning the algorithm further. In fact, tuning the symbol table further could \nlead to a net loss in clarity and maintainability in the compiler. A practical program will con\u00adtain \ndozens, perhaps hundreds, of algo\u00adrithms. It is neither necessary nor desirable that all of them be optimal, \nespecially since the criterion of optimality in practice is usu\u00adally a mixture of space and time considera\u00adtions. \nLocal optimization is often harmful. It usually is desirable, and often necessary, that a few selected \nalgorithms be very goo~ often, the decision as to which algorithms to carefully craft can, and should, \nbe delayed until after the preliminary program is work\u00ading and measured. The most interesting situation \nwith respect to the interaction of theory and prac\u00adtice is in the code generator. Here is a case where \nthere has not been a lot of theory, but that which has been developed was able to give us a great deal \nof insight. The machine models and models of computation studied fall far short of the complexities of \nreal machines and real languages. As an example, there has been very little theoreti\u00adcal activity dealing \nwith the treatment of condition codes, register classes, even-odd register pairs, generalized assignment \nopera\u00adtors, conversion operators, register vari\u00adables, bit fields, etc. Most of these issues must be \ndealt with in a C compiler for current machines. Thus, the most we can expect from the theory is insight, \nbut that is enough. Optimal code generation, in its full generality, is a hard problem. Bruno and Sethi,6 \nusing a very simple machine model with only a single register, showed that optimal code generation is \nNP-complete when common subexpressions are to be identified and eliminated. Aho, Johnson, and Ullman7 \nproved further that, even with a machine with an infinite number of regis\u00adters (so there is no register \nallocation prob\u00adlem), and with common subexpression optimization limited to a very simple form, it is \nstill NP-complete simply to decide where to begin the computation! Thus, we decided early to postpone \nthe consideration of common subexpressions for the moment. This decision is somewhat easier to live with \nin C than in some other languages, since the language itself contains many operators (such as the increment \nand decrement operators) which permit simple construc\u00adtions which would lead to common subex\u00adpressions \nin other languages. In fact, experience with the PDP-11 C compiler showed that a compiler which does \nnot do common subexpression elimination was still practical for most applications. Accordingly, the compiler \nwas designed to compile code for one expression at a time, and common subexpressions were not identified. \nHaving ruled out common subexpres\u00adsions, we looked at the literature on code generation for trees. Ershov,8 \nand, more generally, Sethi and Ullman,9 showed that it was possible to generate optimal code for very \nsimple expressions, on very simple machines, in time linear in the size of the expression. Aho and Johnsonl \nextended this work, showing that it is possible to com\u00adpute optimal code for trees in linear time, provided \nthat the machine register structure is sufficiently uniform. In order to do this, however, it is necessary \nin some cases to generate code for subexpressions in an order that does not follow a simple treewalk \nof the expression tree. The key idea is that if a subexpression is to be computed and stored into a temporary, \nit is best to do so before beginning the main expression. All of the registers would be free, and thus \nthe subex\u00adpression could be computed with all resources available. Clearly, it is necessary to identify \nthose subexpressions which must be stored. Accordingly, the code generator was divided into two pieces; \none piece identifies subtrees to be computed and the other computes them. In order to be able to clearly \nfix the blame for an incorrect decision, the part of the code generator which computes the sub\u00adtrees \nwas rendered unable to ask for tem\u00adporary storage locations. Thus, the assertion made by the first part \nis that the subtrees passed to the second part must be comput\u00adable without the use of any temporaries. \nIf the second part runs out of registers, it aborts the compilation (pointing an accusing finger at the \nfirst part, of course). This assertion serves as a firewall in the compiler. Logical errors are often \nself\u00addetected, and the blame easily attached to one or the other part of the compiler. The occasional \nduplication of logic in both parts (for example, in the detection of special cases) is more than made \nup for by the absence of the good grief I m out of regis\u00adters; what should I do now? module in many compilers. \nWhen it became clear that this philoso\u00adphy of containment was paying off well, a similar attitude was \nadopted towards register allocation. If a subtree is to be computed into a register of a given type, \nor a particular register (for example, because of a function return or call) this must be known before \nthe subtree computation is begun. The actual requirement is even stronger: the only register-to-register \ntransfers that are gen\u00aderated by the compiler (except as a response to specific operators in the source \ncode) come from one spot in the compiler, and, in particular, must result from a transfer of a computed \nvalue into the register which was its intended destination from the outset. If the compiler attempts \nto move a computed value into a busy register, the computation is aborted. This implies that the register \nallocation phase must operate before the actual generation of code. As with the stor\u00ading of results, \nthe difficulties that this causes are largely duplication of logic in two places in the compiler; the \nadvantages are, again, a clear fixing of blame for bad code, and the elimination of another error-prone \nmodule in many compilers (the put the result here no matter what and then clean up the mess module). \nThe key to the Sethi-Ullman and Aho-Johnson techniques was roughly the same: attempt to estimate in a \nbottom-up manner the resources needed to compute the subtrees. This estimation could then be used to \nselect subtrees to be stored and to order the computation by identifying the toughest nuts to crack first. \nThe dynamic programming approach of Aho and Johnson seemed to be very difficult to apply to the very \nrich set of C operators. It would be expensive in space and time, and its very complexity would seem \nto lead to errors. Also, one of the major assumptions of this theory (that the registers were symmetrical) \nis one of the most frequently violated in practical machines, and there seemed to be little to be done \nabout repair of the algo\u00adrithm under these conditions. However, the insight provided by the theory suggests \nthat it is worthwhile to attempt to estimate or bound the resources, and, in particular, the registers, \nwhich were needed by subtrees. Accordingly, the com\u00adpiler attempts this estimation; the numbers generated \n(somewhat inaccurately referred to as Sethi-Unman numbers) are useful not only in deciding which subtrees \nare to be stored, but also in generating the final code sequences. 1.00 There are a number of other things \nwhich this estimation permits which are use\u00adful, but somewhat different from the usual notion of register \nallocation . For example, many machines have some operations which must have the right operand in memory. \nThis is painlessly achieved by giving such operands very high Sethi-Ullman numbers, forcing them to be \nstored. Similarly, the ability to do arithmetic operations on data of various types is usually limited. \nFor exam\u00adple, few machines can multiply or divide a register by a quantity stored in a single byte; such \nquantities have to be loaded into a register before they can be dealt with. Those operations which are \nimmediately addressable are signified by giving the right\u00adhand operand a Sethi-Ullman number of O. This \nconveniently signals to the compiler that this effect has been recognized, and can be assumed in later \ncomputations. Needless to say, most of the Sethi-Ullman computa\u00adtion is machine dependent in the specific \ndetails, but the general notion (that of esti\u00admation and control of resources) has a very broad degree \nof generality. There are distinct limitations in this estimation process, unfortunately. The notion of \nestimation of resources breaks down to a greater or lesser extent in almost all of the practical machines \nexamined. This does not mean that the notion is useless or must be discarded; merely, that it moves, \nin certain situations, from the solid ground of algorithm to the quicksand of heuristic. One common difficulty \nis that, on many machines, operations such as multipli\u00adcation and division require the use of paired \nregisters, often adjacent or even-odd register pairs. Some of the theoretical implications of this are \ndiscussed by Aho, Johnson, and Unman. 11 This situation is unpleasant in theory, and a major irritation \nin practice. It is difficult to estimate the number of regis\u00adters required to do such computations. Even \nif the estimation is done correctly, it is still an effort to decide how to allocate the registers to \navoid or minimize register-to\u00adregister moves. With, for example, four registers, it is possible to have \ntwo free registers, but still be unable to do a multiply operation without prior register-to-register \nmoves. If this is allowed, a good deal of the effect of the firewall prohibiting register\u00adto-register \ntransfers is lost. This situation is particularly irritating since it seems to be largely unnecessary. \nWhile it is true that multiplication, for example, does mathematically take two n bit quantities and \nproduce a 2*n bit result, in practice nearly all programming languages (including C) throw away the most \nsignificant n bits of this product (in some cases with an overflow indication if appropri\u00adate). Thus, \nwhile the full length operation is occasionally useful, it would also seem desirable to have single length \nmultiply, divide, and remainder operations which were similar to the add and subtract opera\u00adtions. This \nwould allow direct application of the Sethi-Ullman and Aho-Johnson results, with a resulting simplification \nof the com\u00adpilers. At one point, there were hardware considerations which argued for special treatment \nof such register pairs, but that no longer seems relevant with today s hardware. A related issue is that \nof having different register classes; for example, index registers, general purpose regis\u00adters, floating \npoint registers, etc. Here, the notion that a single number character\u00adizes the computation is inadequate. \nThe situation is even more complex when certain computations can be carried out in various different \nclasses of registers. Thus, a com\u00adputation might require two index registers and one general purpose \nregister, or one index register and two general purpose regis\u00adters. A single number is inadequate. The \ndynamic programming approach of Aho and Johnson could probably be extended to deal with this situation, \nbut would be expensive and rather unattractive for the other reasons mentioned above. The practical response \nto these difficulties is to sidestep them, in vari\u00adous ugly machine-dependent ways. Another difficulty \nwith the estimation mechanism arises with long, or double length integer quantities. In general, these \nrequire two registers, and, often, must be in register pairs as well. The additional difficulty, after \nthe register pair issue is dealt with, is merely keeping track of the sizes of the operands. This is \ntedious, but not con\u00adceptually difficult. Similarly, the presence of complicated assignment operators \nin C, such as the /= operator, leads to complicated estimation calculations. ( a /= b means a = a / b, \nexcept that any side-effects that may result from computing a are done only once.) At a lower level, \nthe generation of the specific code sequences is also nontrivial. The language C has a potentially infinite \nnumber of types, and several dozen opera\u00adtors. The machine architecture gives rise to many other cases \nas well: operands in regis\u00adters, in memory, on the stack, and constant operands, to mention just a few. \nThere are hundreds, possibly thousands, of code sequences to be dealt with. Some method of abstraction \nis crucial to permit the com\u00adpiler to generate correct code without an impossible data management problem. \nPro\u00adviding this abstraction power invites bugs which arise from incorrect abstracting of the machine \ndesign manual. Nevertheless, without some means of abstracting and organizing, the compiler becomes unwork\u00adable. \nAs an example, on many machines, the add, subtract, and, or, and exclusive or operations are very similar \nin addressing mode and condition code treatment. An abstraction facility allows the compiler to share \nlogic and decision facilities for these operators, and yet still keep track of the correct opcodes. Similar \nabstraction is possi\u00adble with address modes (for example, names and constants are often treated in similar \nways) and with the types of the operands (all pointers are often treated the same, and often pointers \nare treated the same as integers). One of the more interesting results of this abstraction process arose \nfrom the reali\u00adzation that machine opcodes frequently return more than one value. For example, a store \ninstruction may yield useful values in the register being stored, in the memory location which is the \ntarget of the store, and in the condition code registers. The exact result which is remembered for later \nuse depends on the goal of the computation at that time, and some machine-dependent heuristics. This \nabstraction issue is avoided in most books and articles on compiler design. The C compilers have suffered \nfrom quite a number of bugs in this area, however. The use of the abstraction mechanisms provided in \nthe C compiler often encourages the addi\u00ad tion of some addressing or opcode features to the compiler \nwhich do not actually exist in the machine. We may have identified this problem, but we have not really \nsolved it. As a final attempt at correctness, we adopted a discipline which has been extremely successful: \nmodeling. The com\u00adpiler views the input expression tree as a model of a computation which is to be done, \nand the compiler keeps a model of the state of the machine (for example, which regis\u00adters are free). \nThe emission of instructions (for example, a load instruction) is viewed as a transformation on the input \n(for exam\u00adple, converting a name node into a reg node) and also on the machine state (for example, marking \na scratch register busy). There is a part of the basic code generator simply con\u00adcerned with making these \ntransformations correctly. At the center of the low level code generation is a table containing the basic \ntree match templates, input rewriting rules, resource requests, and assembler code to be emitted. Each \ntable entry has its own validity, and is independent of all the other entries. Code generation consists \nof select\u00ading transformations which, one by one, reduce the input, meanwhile producing assembler code. \nIf the model of the regis\u00adters is correct, and the table entries correctly match the semantics of the \ninput operators with the rewriting rules and register requests, then if the input is reduced to null \n(usually a single node) the output code must be correct. Once again, the code generation prob\u00ad lem has \nbeen divided. The table entries supply a rich set of correct transformations. The rest of the compiler \nis concerned with selecting subtrees and applicable table entries which reduce the input tree; as a side \neffect of this reduction, output code is pro\u00adduced. This notion is akin to the Competence/Performance \ndistinction made by Pratt. 12 The table entries represent the competence; they are correct facts which \nlink the input, the machine model, and the output. The performance aspect is the col\u00adlection of algorithms \nand heuristics which select subtrees and applicable table entries, and work at reducing the input problem \nremaining to be solved to nothing. Of course, the table may be inade\u00adquate: there may not be enough transforma\u00adtions \nspecified to reduce all legal input trees to the desired state. Moreover, the heuris\u00adtics may fail to \nselect a proper ordering which would permit a reduction to take place. In this case, the compiler will \nsimply give up, reporting its inability to handle the input. However, if the compiler heuristics do successfully \nreduce a given input, one has very high confidence that the resulting code is correct, since the process \nby which the code is produced is the concatenation of a number of small, verifiable steps. In fact, the \ncompilers produced using this model have had an excellent recor~ there have been very few compiler bugs \nthat have not been detected by the compiler itself. To summarize the code generation experience: the \nexisting theory was not directly applicable but did suggest a number of principles for organizing the \ncode genera\u00adtor. The data management problem of deal\u00ading with many possible code sequences seems rather \ndifficult. Nevertheless, by arranging that the transformations of the problem to be solved remain conceptually \nseparate from the logic which decides the method of attack, the compiler becomes quite self-diagnosing. \n Practical Experience The compiler sketched above has served as the base of about a half dozen compilers, \nand a number of others are planned. Compilers are being actively main\u00adtained on Honeywell 6000 series, \nIBM 370, Interdata 8/32, Data General Nova machines, and others. Some statistics on the amount of portability \nachieved are of interest. There are roughly 8000 lines of source code in the Interdata 8/32 C com\u00adpiler. \nThe first pass, which does syntax and lexical analysis, symbol table management, builds expression trees, \nand generates a bit of machine dependent code such as subrou\u00adtine prologs and epilogs, consists of 4600 \nlines of code, of which 600 are machine dependent. In the second pass, which does the bulk of the code \ngeneration, 1000 out of 3400 lines are machine dependent. Thus, out of a total of 8000 lines, 1600, or \n20\u00b0/0, are machine dependent; the other 80V0 are shared with the the Honeywell, IBM, and other compilers. \nAs the Interdata compiler is tuned, these figures should rise a bit; for the IBM, the fraction which \nis machine dependent is 22\u00b0/0; for the Honeywell, 250/o. These figures both overstate and understate \nthe actual difficulty of moving the compiler, The figures actually represent the number of lines on those \ninput files which contain machine dependent code. The actual number of lines which must be modified for \na new machine is a half or a third of this number. On the other hand, the hardest part of moving the \ncompiler is understanding the code generation issues, the C language, and the target machine well enough \nto be able to make the modifications. Since we do not have a version of this compiler for the PDP-11, \nwe have no good figures of comparison with the original UNIX PDP-11 compiler. The new compiler is con\u00adsiderably \nbigger, however, and would prob\u00adably be two or three times slower. For example, the portable compiler \ndoes a great deal of 32 bit arithmetic internally, to facili\u00adtate cross-compiling; the PDP-11 compiler \ndoes much of this to 16 bit accuracy, which is far more efficient. The problem of dialects of C is an \nimportant one. C is more parameterized than other languages; if char\u00adacters are naturally 9 bits on a \ngiven machine, C does not insist on simulating 8. Thus, a number of differences in machine architecture \ncan been seen in the language. This paper machine dependence seems to make little difference to the actual \nportabili\u00adty of programs; we have moved substantial programs including compiler compilers, pho\u00adtotypesetting \nsoftware, and file comparators from UNIX to the IBM and Honeywell sys\u00adtems, and have moved the UNIX operating \nsystem itself, and a major part of the UNIX software, to the Interdata 8/32. Thus, the compiler has demonstrated \na practical degree of compatibility with the PDP-11 version, and the C language itself has proved to \nbe a fine vehicle for portability. In particular, our C compilers on the IBM, Honeywell, and UNIX systems \nare far more compatible than the corresponding Fortran compilers. The Future 6. The theory which underlies \nthe con\u00adstruction of LALR(l ) parsers is mature enough that it can be packaged into a tool 7. that can \nbe used by someone who knows nothing about the theory. The theory of regular expressions is similarly \ndeveloped. In the case of symbol tables, the theory is there, but it seems to resist being packaged; \nperhaps this will come in time. Code gen\u00aderation is still a very fertile area for theoreti\u00ad 8. cal and \npractical work; a practical tool seems far off. The greatest future need, however, seems to lie in the \narea of semantics. Portability and semantics are clearly closely related. On the face of it, portability \n9. is precisely the practical flowering of the notion of semantics, the practical separation of meaning \nfrom representation. Having moved a program, one needs to know what 10. it is one has moved. Unfortunately, \nthis view is not widely held among semanticists! Rather than study the (admittedly compli\u00adcated) interaction \nbetween meaning and representation, the trends in semantics today are towards separation of meaning totally \nfrom representation, Our greatest need is 11. for a language, similar in spirit to BNF, which can be \nused to discourse about semantic problems, and make semantic compromises and contracts. We have not found \nthat any of the current approaches are attractive. 12. References 1. D, M. Ritchie, C reference manual \nUNIX document 2. D. M. Ritchie and K. Thompson, The UNIX Time-Sharing System, Comm. ACM 17, 7, pp. \n365-375 (July 1974). 3. A. Snyder, A Portable Compiler for the Language C, Master s Thesis, M. I. T., \nCambridge, Mass. (1974). 4. S. C. Johnson, YACC -Yet Another Compiler-Compiler, Comp. Sci. Tech. Rep. \nNo. 32, Bell Laboratories, Mur\u00adray Hill, New Jersey (July 1975). 5. M. E. Lesk, LEX -A Lexical A naiyzer \nGenerator, Comp. Sci. Tech. Rep. No. 39, Bell Laboratories, Murray Hill,  New Jersey (October 1975). \nJ. Bruno and R. Sethi, Code Genera\u00adtion for a One-Register Machine, JACA423, 3, pp. 502-510 (July 1976). \nA. V. Aho, S. C. Johnson and J. D. Unman, Code Generation for Expressions with Common Subexpres\u00adsions, \n J. ACM 24, 1, pp. 146-160 (Jan. 1977). Also in ACM Symp. on Principles of Programming Languages, pp \n19-31, 1976. A. P. Ershov, On Programming of Arithmetic Operations,, Dok/. A. N. USSR 118, 3, pp. 427-430 \n(1958). (English translation in Comm. ACM 1 (8), pp. 3-6,, 1958) R. Sethi and J. D. Unman, The gen\u00aderation \nof optimal code for arithmetic expressions, J. ACM 17, 4, pp. 715\u00ad728 (Oct. 1970). A. V. Aho and S. \nC. Johnson, Optimal Code Generation for Expres\u00adsion Trees, J. ACM 23, 3, pp. 488\u00ad501 (1975). Also in \nProc. ACM Symp. on Theory of Computing, pp. 207-17, 1975 A. V. Aho, S. C. Johnson and J. D. Unman, Code \nGeneration for Machines with Multiregister Opera\u00adtions, Proceedings of 4th SIGACT-SIGPLAN Symposium \non Princ@es of Programming Languages pp. 21-28 (January 1977). v. R. Pratt, The Competence/Performance \nDichotomy in Programming, Proceedings of the Fourth ACM Symposium on Principles of Programming Languages, \np. 194 (Janu\u00adary 1977).\n\t\t\t", "proc_id": "512760", "abstract": "A compiler for the C language has recently been constructed which is now compiling C for about half a dozen machines. The compiler was influenced in various ways by recent theoretical developments. This paper gives an overview of the compiler structure and algorithms, emphasizing those areas where theory was helpful, and discussing the approaches taken where theory was lacking.", "authors": [{"name": "S. C. Johnson", "author_profile_id": "81332506933", "affiliation": "Bell Laboratories, Murray Hill, New Jersey", "person_id": "PP48024621", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/512760.512771", "year": "1978", "article_id": "512771", "conference": "POPL", "title": "A portable compiler: theory and practice", "url": "http://dl.acm.org/citation.cfm?id=512771"}