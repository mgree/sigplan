{"article_publication_date": "01-01-1978", "fulltext": "\n Efficient Computation of Expressions with Common Subexpressions (extended abstract) Permission to make \ndigital or hard copies of part or all of this work or personal or classroom use is granted without fee \nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear \nthis notice and the full citation on the first page. To copy otherwise, to republish, to post on servers, \nor to redistribute to lists, requires prior specific permission and/or a fee.&#38;#169; 1978 ACM 0-12345-678-9 \n$5.00 Bhaskaram Prabhala Computer Science Department Indiana Universi&#38; Bloomington, Indiana 47401 \n and Ravi Sethi Bell Laboratories Murray Hill, New Jersey 07974 1. Introduction 1.1. Background. After \ndeciding on the order in which operations in an expression like a b*c must be per\u00adformed, a compiler \ndetermines the exact computer in\u00adstructions or machine code that will carry out the opera\u00adtions. Our \ntheoretical understanding of code generation can be summarized by: (a) optimal code from simple expressions \nlike a b*c, in which all subexpressions are distinct, can be generated very efficiently over a broad \nclass of machines [2,8, 15,181, and (b) if we wish to avoid redundant computation of common subexpressions \nthen generation of op\u00adtimal code becomes very difficult, even for quite simple machines [3,9].  Elaborating \non (b) above, Bruno and Sethi [9] show that the problem of generating optimal code for a one re\u00adgister \nmachine is NP-complete. Aho et al. [3] add that even when all common subexpressions have exactly one \noperation, optimal code generation is NP-complete for one register machines and also for infinite register \nmachines. 1 Expressions will be represented by dags (directed acy\u00adclic graphs) as in Figure 1. If no \ncommon subexpressions occur, then the dag representation is a tree. Real machines have many features \nnot found in models of machines, so any theoretical results can at best provide insight into the design \nof code generators. Johnson [12] eloquently describes the application of theoretical results on code \ngeneration from trees to the design of a portable code generator. See also the discussion in Wulf et \nal. [201. 1The infinite register machine has two kinds of in5truction5: r-r ops and r+s, where r and \ns are registers. Since an opera\u00adtion overwrites a register, the problem is one of minimizing moves. @1977, \nBell Telephone Laboratories, Inc. Common subexpressions do indeed occur in practice, so heuristics are \ninvoked to accommodate them. We would like to develop theoretical results that indicate how common subexpressions \nmight be handled. In this paper we study a class of clags from which op\u00adtimal code can be generated efficiently. \nWe study a class of series-parallel graphs, which we call collapsible graphs, that include trees and \nare general enough to permit large common subexpressions, but from which optimal code can be generated \nin polynomial time. Figure 2 shows a collapsible graph taken from an algorithm in the C.A C1l. Our interest \nin series parallel graphs was kindled by Abdel-Wahab and Kameda [1] who consider a scheduling problem \nthat is related to code generation. Their results imply that the minimal number of registers needed to \ncompute a series-parallel graph can be found in O (n log n) time. Much remains to be done, since their \nmodel does not permit store instructions. Further information on code generation may be found in [51. \n1.2. The machine model. As in Prabhala and Sethi [15], we consider a hardware stack which can hold at \nmost d> 1 elements. We say the stack has depth d. Coupled with the stack is a random access memory large \nenough to hold all initial and computed values. A machine is Figure 1: A dag represents the result of \na sequence of assign\u00adments. The dag for t:= b*q t:= (a t)+tis shown. specified by selecting at least \none binary operation in addi\u00adtion to LOAD, DSTO (destructive store, removes the top element from the \nstack) and NSTO (nondestructive store, leaves the stack unchanged). Arithmetic operations are divided \ninto four categories, depending on whether the left or right operand is on top of the stack, and on wheth\u00ader \none or both operands are on the stack. For example, in ORSS, the O denotes an operation, the R denotes \nthat the right operand is on top of the stack, and SS denotes that the two operands occupy the top two \npositions on the stack. In OLSM, the left operand is on top of the stack, and the right is in memory. \nThe remaining two combina\u00adtions are OLSS and ORSM. Each category like ORSS is really a class of operations, \nsince ORSS -1-is addhion and ORSS + is division, and so on. This model also accommodates other addressing \nmodes like indirection and immediate operands (see the model in [61), if we allow the cost of say ORSS \n0 to be different from ORSS +. Associated with each instruction 1 is a cost c(I). The cost of a sequence \nof instructions is determined by sum\u00adming the costs of the instructions in the sequence. 1.3. Relation \nwith register models, We consider stack machines for two reasons. (a) The NP-completeness results from \ndags for one-register machines carry over to stack machines since a stack of depth 1 is the same as a \none-register machine. Finding a polynomial algorithm for optimal code generation from collapsible graphs \nfor stack machines is therefore significant. (b) The second reason is pragmatic. Stack machines are easier \nto analyze since access to values on the stack is restricted. The nature of a stack machine guarantees \ncontiguity and normal form results of the type that Aho and Johnson [2] had to prove for trees on register \nmachines. On stack machines bounc\u00ading between subtrees, needed for optimality on register machines in \n[1,4], just cannot occur. Since a register machine can simulate a stack machine, any algorithms we develop \ncan be used as heuristics on register machines. Consider a register machine with instructions of the \nform r + r ops, where r and s are registers. Suppose that a value a is computed into r and a value b \nis computed into s before the instruction r + r ops k performed. The order in which a and b are computed \ndoes not matter. On a stack machine, if a is computed before b, then a will be below b in the stack. \nInsensitivity to the order of computation of a and b is achieved if both ORSS and OLSS instructions are \navailable i.e. either the left or the right operand may be on top of the stack. For this reason, we will \npay special attention to stack machines with both ORSS and OLSS instructions. It will be convenient to \nconsider machines which have all four kinds of operations i.e. OLSM, ORSM, ORSS, OLSS. We call such machines \ncommutative stack machines. The results proved for commutative machines can often be carried over to \nthe case when one or both of the stack-memory operations are not available. 1.4. Collapsible graphs. \nAll graphs in this paper are directed and acyclic. The number of edges entering (resp. leaving) a node \nis called the indegree di(x) (resp. ou<de\u00adgree dO(x)) of x Node x is called a shared node if 223 di(x) \n>2. A dag is a collapsible graph, if it can be reduced to a graph consisting of a single node by applying \na sequence of the following transformations: 1. Delete edge (x)y) and node y if d, (y)=l and d.(y)=O. \n 2. Delete edges (x,y), (yJz) and node y, and add edge (x,z) if di@)=dO@)=l. 3. If there are two edges \nfrom node x to node y,  then delete one of them. Transformations 2 and 3 by themselves define series\u00adparallel \ngraphs, formulated by Riordan and Shannon [17]. Transformation 1 allows tree-like subgraphs. An example \nof a collapsible graph is given in Figure 2. 1.5. Results. We will convey an idea of the results in this \npaper in the context of an example. Suppose the collapsible graph in Figure 3(a) is comput\u00aded on a machine \nwith all four kinds of operations: OLSM, ORSM, ORSS, OLSS. The last node to be computed is u. If no unnecessary \nstores occur then we expect that one of nodes v and w will be computed; its value being in the stack \nwhile the other of v and w is computed. Once both v and w are on the stack, then either an OLSS or an \nORSS will be the last instruction that computes u. 1.5.1. Traversals. Suppose the last instruction is \nan OLSS. Since w must then be below v on the stack, w is computed before v. All values in the subdag \nfor w must therefore be computed before any value in the subdag for v is computed (the exception being \nthe shared node of course). Since there are no copy instructions, the first time the value of the shared \nnode x is on top of the stack, it must be stored so that it can be reused. The instructions of a program \nto compute the collapsi\u00adble graph define a tree traversal of the graph. As in Fig\u00adure 3(b), when w is \ncomputed before v, the subdag of the shared node x will be traversed when w is computed. We will talk \nof attaching the subtree of x to w. 1.5.2. Stack positions. During a tree traversal of a graph there \nwill in general be a number of intermediate results that will be used later. For example, in Figure 3(b) \nthe value of w is computed and held on the stack while the subtree for v is computed, Since an intermedi\u00adate \nresult occupies a stack position, associated with each tree traversal is the n,umber of stack positions \nneeded to make that particular traversal, For the collapsible graph in Figure 3(a), the order of computation \nindicated in Figure 3(c) requires one less stack position than the order in 3(b) . In section 2 we will \ndetermine a traversal of a collapsible graph that minimizes the number of stack positions needed on a \ncommutative machine. 1.5.3. Testing collapsibility. A linear algorithm is given in section 3 to test \ncollapsibility of a binary dag (each node has two or no sons). This algorithm provides the framework \nfor the labelings in sections 2 and 4. 1.5.4. Optimal code. Minimizing the number of stack positions \nused, and minimizing the cost of a program may sometimes be competing goals. In Figure 3, attaching the \nsubtree at x to v as in 3(c) minimizes the number of stack positions, but requires one more instruction \nthan the ar\u00adrangement in 3(b). The reason is that in 3(b), once the VG JJZ Figure 2: A collapsible graph \nfrom C.,4CM 16, 10 (Oct. 73) page 637. The @ symbol indicates an indexing operation, with @ (VG,JJZ) \nrepresenting the address VG(JJZ). Since no advantage can be taken of shafed leaves on stack machines, \nleaves with the same label have not been identified. d-J (a) (b) (c) Figure 3: On a stack machine, a \nprogram to compute a collapsible graph corresponds to a tree traversal of the graph. The problem is one \nof finding the best traversal. .n,v 1 I j 1 I 1 1 ; Figure 4: The decision of which father to attach \ny may depend on which father z is attached to, and vice versa. a stack-memory operation. In 3(c) a load \nwill be needed for one of the sons of w. In minimizing the number of instructions, we need to watch (a) \nstack positions, (b) loads, and (c) stores. Another issue to be dealt with is sketched in Figure 4. In \nFigure 4, deciding on the right father to attach the sub\u00adtree for y may depend on where z is attached. \nAnd the best place to attach z may depend on the requirements for u and v, where the requirement for \nu depends on where Y is attached. These difficulties will be resolved in section 4 by using dynamic program \nning. We adapt the dynamic program\u00adming approach of [2,4] to construct an O (d3 n) algo\u00adrithm. Note that \nthe instruction set can be restricted by making the costs of some of the instructions infinite. 2. Stack \nRequirements Central to the optimal algorithms for trees in [2, 8, 10, 14, 15, 16, 18] is an estimate \nof the resource require\u00adments (in number of registers) of an expression. This es\u00adtimate influences evaluation \norder, and also guides the choice of values to be stored. This estimate also plays a role in the practical \ncode generators described by Johnson [121 and Wulf et al. [20]. The purpose of this section is to indicate \nhow such an estimate can be prepared for col\u00adlapsible graphs evaluated on commutative stack machines. \nIn the process we will gain some insights into evaluation order in the presence of common subexpressions. \nAs the example in Figure 3 indicates, a program to compute a collapsible graph defines a tree traversal \nof the graph in which each shared node is attached to one of its fathers. Since tree traversals play \nan important role in computing collapsible graphs on stack machines, we will first consider some properties \nof tree traversals. 2.1. Classical Iabel[ing of trees. Suppose that the given collapsible graph is a \ntree K Then as proved in [8, 15, 16, 181 the following labelling gives the minimal number Figure 5: Think \nof there being an unknown subtree at node z. The subtree requires a stack positions. of stack positions \nrequired to compute T. Recall that we are considering commutative machines, which have all four kinds \nof operations OLSM, ORSM, ORSS, OLSS. Let u be a node in a tree. The label ru is defined as fol\u00adlows \nrU =Oifuis aleaf r = r, if uhas sons vand w,and r, > rw ru = r,+l if u has sons vand w, and ry = rW. \n2.2. An auxiliary problem. The first extension of the simple tree case is the following problem. Problem \nAP1. consider a tree T with a distinguished leaf z. The computation of z requires an unknown number a \n> 0 of stack positions. (See Figure 5.) Ex\u00adpress the minimal number of stack positions required to compute \nthe tree without stores as a function of a and the tree. D If we ignore the fact that the leaf z is distinguished, \nthen the minimal number of stack positions required to compute each node without stores can be determined \nus\u00ading the classical labelling above. For node A this label is r.. Let R. represent the number of stack \npositions required for u if z requires a positions. If node u is not an ancestor of z, then clearly, \nRU= rU. We will show how another number 8. can be deter\u00admined so that the requirement Ru of an ancestor \nu of z is given by the conditional R. = a <8. ~ r., max(l+ru, a) (1) In the above expression we write \np~a, b for if p then a else b. Determine Su as follows. 1. 8,= r, Consider an ancestor u of z. Let the \nsons of u be v and w, and without loss of generality, let z be in the subtree for v. 2. if r, > r. then \n8.=8, 3. if r.= r. then 8.= rU 4. if rv= rW l then 8U =8, 5. if r, < r. l then 8U = r~ l THEOREM 1: \nConsider an instance of problem AP1, with  a distinguished leaf z that requires a >0 positions. For \neach ancestor u of z, node u requires RU positions as ex\u00ad Figure 6: We can decide on whether the subtree \nfor z should be attached pressed in equation (1). Proo$ We prove the theorem by induction on the height2 \nof node u. basis: height O. Then u being an ancestor of z must be z. Since 8,=r,=0, R, simplifies to \na, which is given as the requirement of z. inductive step: height > 0. Let v and w be the sons of u, \nand without loss of generality let z be in the subtree for v. Case 1: rv>r.. This case follows from the \ninductive hypothesis since rU=r, and 8U=8,, so R. is exactly R,. Moreover if v is computed before w then \nR,=R. positions are enough to compute u. Case 2: r,=r.. In this case 8U=ru, so R simplifies to max(ru, \na), which is clearly a lower bound. Since rU=rV+l, computing v and then w requires no more than Ru registers. \nCase 3: rv=rW l, and hence rU=rw. From the definition of 6. we get Ru = a<~,~rw, max(l+rw, a) The only \npart to worry about is when Ru=l+rw i.e. CY>8, and a<l+rw. Under these conditions, (using r,=r. l, and \nthen a < I+rw): R, = max(l+r,, a) = max(rw, a) = rw Since the subtrees for both v and w require at least \nrw po\u00adsitions, Ru must be at least 1+rW. Again 1+rw positions are enough since we can hold w on the stack \nand then compute v using RV= r. positions, Case 4: r.< rW l, and hence rU=rW and 8u=ru 1, yield\u00ading RU \n= a<rU l-rU, max(l+rU, a) Again the part of interest is when Rti=l +r., which occurs when a > r. l and \na <1 +rU. The only possible value for a is rU. Thus ru=r.=a. Since the subtrees of both v and w require \nat least rW positions under these conditions, R. must be at least l+r.=1 +rti. As in case 3, 1+rW positions \nare enough. D 2.3. One shared node. Consider node u as shown in Figure 6. The number of positions required \nto compute u depend on the order in which nodes v and w are comput\u00ad 2 A leaf is at height O. A nonleaf \nx is at height 1 plus the max\u00adimum over the heights of the sons of .x 226 to x or y without looking at \nthe structure of the subtree for z. ed. If w is computed before v then we need max (Rw, 1+r.) positions \nfor u, where R. is determined relative to node z, as in Theorem 1. Interestingly enough we can show the \nfollowing. LEMMA 1: max(Rv, l+r.) < max(RW, l+rv) if rv>r.. Proof We prove the lemma by considering two \ncases Case I: a< 8,, In this case RV=rv. Since we are given that r, ~ rWwe get max(Ru, l+rW) = max(r., \nl+r~) < max(RW, l+r.). Case 2: a>ti,. In this case R,=max(l+r,, a). Since r.> r., we get max(R,, l+rw) \n= max(max(l+r,, IX), l+rw) = max(l+rv, a) max(l+rv, a) > max(max(l+r., a), l+r.) > max(Rw, I+r,). The \nlemma is therefore true. D The fascinating implication of Lemma 1 is that in order to decide if the subtree \nfor z should be attached to node x or y, it suffices to treat z as just another leaf use the classical \nIabelling scheme on the subtrees created for v and w, and then attach the subtree for z to node x or \ny, depending on which of x and y would be computed first if z was a leaf, (In contrast, consider the \ndiscussion in [20, p. 53-54] where a similar problem is faced,) The following auxiliary problem is a \ngeneralization of these observations to the case where the shared node has k fathers. Problem AP2. Consider \na tree T with a set of dis\u00adtinguished leaves Z. During any computation of the tree, exactly one leaf \nin Z (the first to be computed) requires an unknown number a of stack positions. The remaining elements \nof Z are treated as ordinary leaves. Find an ele\u00adment z in Z such that assigning the requirement a to \nz minimizes the total number of positions required for the tree T THEOREM 2: Problem AP2 can be solved \nas follows. Traverse the tree in r-label order i.e. at node u with sons procedure SEARCH(u); begin mark \nu old ; S(U)=O; for each son vof udo if v is marked new then SEARCH(v); if v is shared then if S(U)=O \nthen S(U):=U elseif S(u)= v then begin Meg(v):=indeg(v)-l; if Irrdeg(v)=l then S(U): =S(V) end else \nerror elseif S(V)=O then null elseif S (u)=O then .S(o) :=S (u) elseif .S(u) <S (v) then error else begin \nZ:=s(u); Meg(z):=imieg(z)-l; if irrdeg(z)=l then S(U): =S(Z) end end end SEARCH (root); if S(roor) \n#0 then error  Figure 7: A binary dag is collapsible if and only if the above algo\u00adrithm halts without \nerror. Note that Meg (v) represents the in\u00addegree of v. v and w, traverse v before w if rv< rW. Assign \nthe require\u00adment a to the first distinguished leaf encountered in this traversal, Prooj2 Let T be a tree \nwith k distinguished leaves. We prove the lemma by induction on k. basis: k< 2. If k is 1 then the problem \nis trivial, and if k is 2 then the basis follows from Lemma 1. inductive step: k >2. Let u with sons \nv and w be such that at least one distinguished leaf is in the subtrees for each of v and w. We will \nconcentrate on node u since minimizing the requirement of node u minimizes the re\u00adquirement for the whole \ntree. From the inductive hypothesis we can find dis\u00adtinguished leaves x in the subtree for v and Y in \nthe sub\u00adtree for w such that x (resp. y) is the first distinguished leaf to be reached in an r-label \norder traversal of the sub\u00adtree for v (resp. w). If r,> r., then from Lemma 1, the requirement of node \nu is minimized by assigning a to node z thereby proving the theorem. D 2.4. Discussion. Starting with \nFigure 3, we have re\u00ad ferred several times to the fact that the computation of a collapsible graph defines \na tree traversal that attaches each shared node to one of its fathers. A precise definition of attach \ncan be given by show\u00ading that a program P in which all stores are nondestruc\u00adtive, defines a depth first \ntraversal D of a collapsible graph G. The key idea is that if a node w is below v in the stack, then \nall nodes reachable from w must be traversed before v and the nodes reachable from v are traversed. Associat\u00aded \nwith traversal D is an integer s(D) which gives the number of stack positions used by D. The minimum \nover all traversals gives the number of stack positions needed to compute G. Theorem 2 can be applied \nto determine the minimal stack requirement and a corresponding evaluation order of a collapsible graph \nwith more than one shared node. Just find a subgraph with one shared node; determine the local evaluation \norder for this subgraph, then attach the shared node to one of its fathers. This leaves the collap\u00adsible \ngraph with one less shared node, and the process can be repeated. In fact this process can be implemented \nvery easily in the context of the algorithm in section 3 to test if a dag is collapsible. 3. Algorithm \nto Collapse Binary Dags Figure 7 gives an algorithm to test collapsibility of a binary dag (each node \nhas two or no sons). The purpose of S(x) at node x is as follows. If S (u)=O when we backtrack from node \nu, then there are no edges entering a node in the subdag for u, from out\u00adside the subdag for u. If S(u) \nis nonzero, then S(u) represents the node nearest u that has an edge entering it form outside the subdag \nfor u. Any other nodes in the subdag for u that have edges entering from the outside must be dominated \nby u for the entire dag to be collapsi\u00adble. When a son v of u is processed, we first check if v is shared. \nIf v is indeed shared, then either v becomes S(u), or v was already S(u) and the newly found edge (u, \nv) represents a duplicate edge that will be removed by transformation 3 in the definition of collapsible \ndags. If v is not a shared node, then S (v)=O says that the subdag for v collapses to a single node, \nso v collapses into u by transformation 1. Otherwise if S (U)=O, then we will eventually get a chain \nu, v,S (u) where v will be eliminated by transformation 2. Finally, if S (u)=S ( v) #0, then we have \nfound two paths from u to S(u) that will eventually collapse by transformation 3. We will now show that \nthe algorithm in Figure 7 works as claimed. LEMMA 2: Let G and G be any dags such that G transforms to \nG in one step under transformations 1,2,3. Then G is accepted by the algorithm in Figure 7 if and only \nif G is accepted by the algorithm. Proofi The idea of the proof is to show that given an ac\u00adcepting computation \nof G, we can construct an accepting computation of G and vice versa. There are three cases, one for each \ntransformation. Case 1: Suppose edge (u, v) and node v are eliminated by transformation 1. Then v is \na leaf and SEARCH(v) leaves S (v)=O. If the call SEARCH(v) is eliminated, then we get a computation of \nG that accepts exactly when the computation of G accepts. Case 2: Node v is eliminated by transformation \n2. This case is very similar to Case 1. Case 3: This is the interesting case. If v is not shared in G \n, then v must be the only son of u in G , (Recall that we start with binary dags, and the number of sons \nof a node is nonincreasing under the transformations.) In this case when we backtrack from v in G we \nwill set S(U)= S(V). Turning to dag G, node v in G has two edges entering it, both from node u When the \nfirst edge from u to v is traversed, S(u) will be set to v. When the next edge (u, v) is traversed, S(u) \nwill be set to S(v) as in the computation of G . The case when v in G has more than one edge entering \nit is similar, since the inde\u00adgrees are adjusted appropriately. D Since the graph consisting of a single \nnode is accepted, it follows immediately from Lemma 2 that the algorithm in Figure 7 accepts all collapsible \ngraphs. LEMMA 3: If the two points in the algorithm where ele\u00adments of indeg are assigned to are never \nreached, but the binary dag is accepted, then the dag is a tree. Proo$ When a shared node v occurs, its \nfather u has S(u) set to v. The only way an ancestor x of v can have S(x) =0, is if the points mentioned \nin the statement of the lemma are reached. Since these points are never reached, and S(root) =0, the \ngraph must be a tree. D THEOREM 3: A binary dag is collapsible if and only if the algorithm in Figure \n7 terminates normally. Moreover the algorithm takes time at most linear in the size of the dag. Proof \nIt is immediate from Lemma 2 that if the dag is collapsible, then the algorithm will terminate normally. \nWe will show by contradiction that all dags accepted by the algorithm are collapsible. Suppose the theorem \nis false, and there exists a dag that is accepted by the algorithm, but is not collapsible. Let G be \nthe smallest, in number of nodes and edges, dag that is not collapsible, but is accepted by the algorithm. \nWe show that then there exists a smaller dag than G that is not collapsible, but is accepted by the algorithm, \nthere\u00adby deriving a contradiction. Consider the first time that one of the points where ele\u00adments of \nindeg are assigned to is reached. From Lemma 3 and the fact that G is not collapsible, one of these points \nmust be reached. Let the point be reached during an execution of SEARCH ( w), and let x be the son of \nw that is being processed. If x is shared, then there must be two edges from w to x By deleting one of \nthese edges, we get a smaller noncollapsible graph than G that is ac\u00adcepted (from Lemma 2). It remains \nto consider the case when x is not shared. Consider the execution of SEARCH(x). Since S(x) #0, there \nmust be exactly one son, say y, of z such that y has a shared node in its subdag. From the algorithm, \nfor any other sons y of x, S (. Y ).=0. But then from Lemma 3, the subdag for y must be a tree. Elimination, \nof y and its subtree yields a smaller noncollapsible graph than G, so y cannot exist and y is the only \nson of x But then x has one edge entering it from w and one edge leaving it to y, so x itself can be \neliminated yielding a smaller noncol\u00adlapsible graph. It is evident form the algorithm that the time taken \nis linear in the size of the dag. The theorem must therefore be true. D The algorithm in Figure 7 provides \na framework in which the number of stack positions needed for a collapsi\u00adble graph (as in section 2) \ncan be determined. While S(u) is being computed, the R. values can also be com\u00adputed. Similarly, the \nalgorithm provides the framework in which the dynamic programming approach of section 4 yields optimal \ncode from collapsible graphs. 4. Optimal Code In this section we consider a machine with a fixed number \nd of stack positions and a cost c(1) associated with each instruction I. We will show how least cost \npro\u00adgrams for this machine can be generated from collapsible dags. If a collapsible dag is large enough, \nit will not be possi\u00adble to compute the dag in one piece i.e. it may be neces\u00adsary to start by computing \nsome node + storing the value of z thereby emptying the stack, and then working on the remaining nodes. \nChanging our viewpoint, consider a node u in a collap\u00adsible dag. We are interested in the part of the \ncomputa\u00adtion that leaves the value of u on top of the stack. Since the subdag for u may be very large, \nit may be necessary to empty the stack one or more times during the compu\u00adtation of u. Let us represent \nthe computation of u by PQ where at the end of P the stack is empty, and at all times during Q at least \none value is on the stack. There are other nodes besides u that have to be com\u00adputed, and when we assemble \nthe instructions for the en\u00adtire dag, we need to ensure that there enough stack posi\u00adtions to compute \nu. Since P leaves the stack empty, the instructions P can appear right at the beginning when all d positions \nare available. So it is only the stack requirement of Q that matters when all instructions are put together. \nHenceforth, when we talk of the number of positions used to compute u, we will refer to the number of \nposi\u00adtions used during Q. Aho and Johnson [2] make a simi\u00adlar point. Consider the subgraph CWYbetween \nnodes w and y in Figure 4. Suppose there are no unprocessed shared nodes in CWY. One source of uncertainty \nis the number of stack positions used to compute the subdag for y. Let k be a parameter representing \nthe number of positions used to compute y. Given a value of k, we can determine the minimal cost of computing \nCw. This minimal cost is not enough to solve the problem however. In Figure 3, minimizing the number \nof instruc\u00adtions used to compute the dag in (a) uses one more stack position than necessary. So we will \nuse another parame\u00ad ter j which represents the number of stack positions with which (&#38;y is computed, \nAt each node u in CWY we will determine KU (j,k), the minimal cost of computing node u using at most \nj posi\u00adtions, during which y requires k positions. Note that it makes sense to consider k > j since node \ny can be precomputed and stored as discussed above. In the process of computing K. (j,k) we will use \nanoth\u00ader quantity SU(j, k) which represents the cost of comput\u00ading u subject to the condition that at \nleast one store oc\u00adcurs on the path from u to the shared node y. The cost of computing and storing y \nis not included in either K.(j, k) or SU(j, k). If j=O then the value of u must ap\u00adpear in memory at \nthe end of the computation, otherwise it must be on top of the stack. Similarly, if k >0 then then the \nvalue of y appears free of cost on top of the stack if y is reached with k positions available. If k> \nO positions are not available, then y has to be loaded. If k=O, then y is taken from memory. The first \nquantity we consider is KY (j,k). If k <j or if j=O, then KY (j, k) is O. Otherwise, it is the cost of \nload\u00ading y. Since the stack must be cleared sometime after y is computed and before u appears on the \nstack in Sti(j, k), it follows that SY(j, k) is O if j=O and is the cost of load\u00ading y otherwise. For \nall nodes u, if y is not in the subdag for u, then KU (j, k)=Su (j)k) and k is ignored. Let u have left \nson v and right son w, and let y appear in the subdags of both v and w. There are four cases, depending \non the instruction Z that computes u. Case: I is an OLSM. Cll(j,k) = KW(O,k) + K,(j, O) + C(Z) C12(j,k) \n= S,(j,k) + KW(O, O) + c(I) Cl(j,k) = min[Cll(j,k),Clz(j, k)] Case : I is an ORSS. C~l(j,k) = K,(j,k) \n+ Kw(j 1, O) + c(1) CjZ(j,/r) = Sw(j-l, /c) + K,(j, O) + c(I) Cq(j,/c) = min[Cql(j,k))Cqz(j, k)l Cz(j, \nk) and CA(j, k) correspond to ORSM and OLSS in\u00adstructions respectively. Using these costs, we get C5(j, \n/c) = min[Cl, CJ, Cj, CAl KU(d,k) = C5(d,k) KU(O,k) = KU(d,k) + c(STO) For all other values of j KU(j,/r) \n= min[CS(j,/c) ,Ku(O,k)+c (LOAD) The computation of S. (j, k) is similar. Returning to Figure 4, suppose \nthat Sw(i, j) and Kw (i,j) have been determined relative to node y. We will identify node y by writing \nS; ( i,j) and K; (i,j). In order to proceed, we need to determine K;(i, k) and Sfi(i,k). K~(i, k) is \nthe minimal value of K~(j, k) plus K;( i,j) as j varies between O and d. Again the computation of Sfi \n(j, k) is similar. Note that it takes O (d3) time to deter\u00admine K~. THEOREM 4: Given a machine M with \nd stack positions and a binary collapsible dag D with n nodes, optimal code can be generated from D for \nM in O (d3. n) time. ProoJ The overall approach is similar to that in [2,41. The cost matrices KU and \nSU are determined. This com\u00adputation can be embedded in the algorithm to recognize collapsible dags in \nFigure 7. Once the minimal cost is found, the exact sequence of instructions that led to the minimal \ncost can be found, 1 5. Discussion We have studied a class of collapsible dags from which optimal code \ncan be generated in polynomial time for stack machines. The results can probably be extended in a number \nof directions. 5.1. Heuristics. In section 2 we considered commuta\u00adtive machines which have all four \nkinds of operations. The major distinction between these machines and regis\u00adter machines is that the \nvalues of common subexpres\u00adsions can be retained and later reused without having to store them. One possible \nheuristic would be to determine the evaluation order for a dag with a stack machine in mind, and then \ndetermine code for the register machine with this evaluation order in mind. A related problem is: given \na fixed order of evaluation for a dag, how difficult is it generate optimal code for a register machine. \n5.2. General J70W graphs. In [7,11,13,21] the evalua\u00adtion order is assumed to be given, and the number \nof re\u00adgisters used in the presence of control flow is optimized. An obvious next step is integrate known \nresults and treat the full problem of code generation. References 1. H. M. Abdel-Wahab and T. Kameda, \nScheduling to minimize maximum cumulative cost subject to series-parallel precedence constraints, Operations \nResearch, to appear. 2. A. V. Aho and S. C. Johnson, Optimal code genera\u00adtion for expression trees, \nJACA4 23, 3 (July 1976) 488-501. 3. A. V. Aho, S. C. Johnson and J. D. Unman, Code generation for expressions \nwith common subexpres\u00adsions, JACA424, 1 (Jan. 1977) 146-160. 4. A. V. Aho, S. C. Johnson and J. D. Unman, \nCode generation for machines with multiregister opera\u00adtions, Fourth ACM Symposium on Principles of Pro\u00adgramming \nLanguages (Jan. 1977) 21-28. 5. A. V. Aho and Ravi Sethi, How hard is compiler code generation, Lecture \nNotes in Computer Science 52 (1977) 1-15. 6. A. V. Aho and J. D. Unman, Principles of Compiler Design, \nAddison Wesley, Reading, Massachusetts (1977). 7. J. C. Beatty, Register assignment algorithm for gen\u00aderation \nof highly optimized object code, IBM J Res. Develop. 18, 1 (Jan. 1974) 20-39. 8. John Bruno and T, \nLassagne, The generation of op\u00adtimal code for stack machines, J,.4CM 22, 3 (July 1975) 382-396. 9. \nJohn Bruno and Ravi Sethi, Code generation for a one-register machine, JACM 23, 3 (July 1976) 502-510. \n 10. A. P. Ershov, On programming of arithmetic opera\u00ad  tions, CACM1, 8 (Aug. 1958) 3-6. terminal series \nparallel networks, J. Math. Phys/cs 11. William Harrison, A class of register allocation algo\u00adrithms, \nRC 5342 IBM Thomas J. Watson Research Center, Yorktown Heights, New York (1975). 12. S. C. Johnson, \nA portable compiler: Theory and practice, F@h ACM Symposium on Principles of Pro\u00adgramming Languages (Jan. \n1978). 13. Joonki Kim and C. J. Tan, Register assignment algo\u00ad 14. Ikuo Nakata, On compiling algorithms \nfor arithmetic expressions, C ACM1O, 8 (Aug. 1967) 492-494. 15. Bhaskaram Prabhala and Ravi Sethi, \nA comparison of instruction sets for stack machines, 9th Annual ACM Symposium on Theory of Computing \n(May 1977). 16. R. R. Redziejowski, On arithmetic expressions and trees, CACM12, 2 (Feb. 1969) 81-84. \n 17. J. Riordan and C. E. Shannon, The number of two  rithm II, RC 6262 IBM Thomas J. Watson Research \nCenter, Yorktown Heights, New York (1976). 21 (1942) 83-93. 18. Ravi Sethi and J. D. Unman, The generation \nof op\u00adtimal code for arithmetic expressions, Y,4CA4 17, 4 (Oct. 1970) 715-728. 19. W. M. Waite, Optimization, \nin Bauer and Eickel  (Eds.), Compder Construct/on: An Advanced Course, Springer Verlag (1974) 549-602. \n20. W. A. Wulf et. al., The Design of an Optlmlzing Com\u00adpiler, American Elsevier (1975). 21. E. F. Yhap, \nGeneral register assignment in the pres\u00adence of data flow, RC 5645 IBM Thomas J. Wat\u00adson Research Center, \nYorktown Heights, New York (1975). 22. A. V. Aho, J. E. Hopcroft and J. D. Unman, The Design and Analysis \nof Computer Algorithms, Addison Wesley (1974). 23. R. E. Tarjan, Depth first search and linear graph \nal\u00adgorithms, SIAM J. Computing 1,2 (June 1972) 146-160.  \n\t\t\t", "proc_id": "512760", "abstract": "", "authors": [{"name": "Bhaskaram Prabhala", "author_profile_id": "81100293783", "affiliation": "Indiana University, Bloomington, Indiana", "person_id": "P30211", "email_address": "", "orcid_id": ""}, {"name": "Ravi Sethi", "author_profile_id": "81100354362", "affiliation": "Bell Laboratories, Murray Hill, New Jersey", "person_id": "PP39039094", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/512760.512784", "year": "1978", "article_id": "512784", "conference": "POPL", "title": "Efficient computation of expressions with common subexpressions", "url": "http://dl.acm.org/citation.cfm?id=512784"}