{"article_publication_date": "08-01-1997", "fulltext": "\n The Effectiveness of Flow Analysis for Inlining J. Michael Ashley University of Kansas Snow Hall 415 \n Lawrence, Kansas 66045 j ashley@eecs. ukans. edu Abstract An interprocedural flow analysis can justify \ninlining in higher-order languages. In principle, more inlining can be performed as analysis accuracy \nimproves. This pa\u00adper compares four flow analyses to determine how effec\u00adtively they justify inlining \nin practice. The paper makes two contributions. First, the relative merits of the flow analyses are measured \nwith all other variables held con\u00adstant. The four auaIyses include two monovariant and two polyvariant \nanalyses that cover a wide range of the accuracy/cost spectrum. Our measurements show that the effectiveness \nof the inliner improves slightly as anal\u00adysis accuracy improves, but the improvement is offset by the \ncompile-time cost of the accurate analyses. The second contribution is an improvement to the previously \nreported inlining algorithm used in our experiments. The improvement causes flow information provided \nby a polyvariant analysis to be selectively merged. By merg\u00ading flow information depending on the inlining \ncontext, the algorithm is able to expose additional opportuni\u00adties for inlining. This merging technique \ncan be used in any program transformer justified by a polyvariant flow analysis. The revised algorithm \nis fully implemented in a production Scheme compiler. 1 Introduction A flow analysis is a tool that can \nassist program trans\u00adformers in assessing the legality of transformations. An interprocedural flow analysis \ncomputes flow information across procedure boundaries. While straightforward for first-order languages, \ninterprocedural flow analysis is more complex for higher-order languages. Since pro cedures are data, \nboth control-flow and data-flow in\u00adformation must be computed simultaneously. Work in This work supported \nby NSF grant CCR-9623753. Permission to make digital/hard copy of part or all this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for profit or \ncommercial advan\u00adtage, the copyright notice. the title of the publication and its da!e appear, and notice \nis given that copying is by permission of ACM, Inc. To copy otherwise, to republish, to post on servers, \nor to redistribute to lists, requires prior specific permission and/or a fee. ICFP 97 Amsterdam, ND Q \n1997 ACM 0-89791 -918-1/97/0006.,.$3,50 the last severaJ years, however, has made flow analyses a more \nviable tool for program transformations [5, 12, 26], including compiler optimizations [2, 4, 17, 18, \n23]. A flow analysis is either monovariant or polyvariant. A monovariant analysis uses a single abstract \nevalua\u00adtion context to model the evaluation of an expression. Thus, a single abstract value is associated \nwith each ex\u00adpression in the program. A polyvariant analysis is more accurate, modeling the evaluation \nof an expression using multiple abstract evaluation contexts. Multiple values may be associated with \neach expression, one value for each abstract evaluation context. Program transformers can use the additional \ninformation by cloning and spe\u00adcializing an expression to a particular evaluation context using the more \naccurate flow information. Compilers can use flow information to justify opti\u00admization. One such optimization \nis inlining. Inlining replaces a procedure call s operator with the code of the called procedure. Besides \neliminating calls, a compiler may be able to take advantage of inlining to improve in\u00adt raprocedural \noptimizations such as register allocation, constant folding, and copy propagation. In order to legally \ninline a procedure at a call site, the compiler must be able to establish that only one procedure will \narrive at the call site and that it is legal to substitute the procedure s text at that call site. Furthermore, \nin order for inlining to be effective the compiler must also be able to determine if eliminating the \ncall would speed up theprogram. Issues involved with this latter decision include register usage and \nthe code size of the inlined program. This paper measures the effectiveness of flow anal\u00adysis for inlining \nusing an improved version of Jagau\u00adnathan and Wright s inlining algorithm [18]. The algo rithm uses dataflow \ninformation to establish the legality of inlining and to guide decisions on whether or not calls should \nbe inlined. It is improved so that when flow infor\u00admation must be merged, the merging is done selectively \nto retain as many inlining candidates as possible. This modification has no effect when a monovariant \nanalysis is used since a monovariant analysis merges all informa\u00adtion as the anaJysis is run. It can, \nhowever, increase in\u00ad lining opportunities when a polyvariant analysis is used. Since the algorithm is \ndependent on the quality of flow information, the implementation is evaluated using four flow analyses \nof differing levels of accuracy. One is a fast, monovariant analysis that makes at most two passes over \na program. The second is equivalent to OCFA [24]. The third is a polyvariant analysis more accurate than \nOCFA but less accurate than lCFA [24]. The fourth analysis is similar to lCFA. All four anal\u00ad yses are \ninstantiations of a single, parameterized flow analysis framework [2]. The four analyses are evaluated \nusing a variety of benchmarks. The polyvariant analyses are able to jus\u00adtify somewhat more inlining than \nthe monovariant anal\u00adyses but at a significantly greater cost. Code growth is also larger when a polyvariant \nanalysis is used, although this may not hold in a compiler that is better able to optimize larger intraprocedural \ncontexts created by in\u00adlining. The rest of the paper is out lined as follows. Section 2 describes the \nassumptions made about the flow analy\u00ad sis and in particular specifies assumptions made about flow values \nand contours. Section 3 describes the inlin\u00ading algorithm, and Section 4 describes its implementa\u00adtion. \nSection 5 evaluates the algorithm using the four four flow analyses described above. Section 6 provides \nrelated work, and Section 7 gives our conclusions.  Flow analysis assumptions Figure 1 describes the \nsource and target language of the inlining algorithm. It is a language of labeled expres\u00adsions that corresponds \nto the relevant subset of Scheme, which we take as a representative higher-order language. Applications \nare further decorated with keys, which may be used by a polyvariant flow analysis to guide splitting \ndecisions. The three sets EzpLabels, ProcLabels, and Keys are all assumed to be finite and specific to \nthe par\u00adticular program being transformed. For any program e we assume a function P : Labels * E such \nthat if e: is a subexpression of e, P(l) = e!. Given a program e for inlining, the inlining transfor\u00admation \nassumes an off-line flow analysis has analyzed the program and produced a function F that describes the \nfruits of the anaJysis. F : Labels x Co~urs d 7( V~es) K e contours = Keys* VXes = {true, false} U \n (ProcLabeis x Co~urs) The function F takes a label and a contour describ\u00ading an analysis context. It \nreturns a set of abstract values. A contour is a sequence of keys, and an ab\u00adstract value is a finite \napproximation of a possibly in\u00adfinite set of actual values. Although Scheme supports a rich set of data \ntypes, and flow analyses typically re\u00adflect that in a rich domain of abstract values, the only values \nthat are important to the inlining transformation are booleans and abstract closures. Furthermore, while \nmany analyses will produce abstract closures cent ain\u00ading potentially useful information such as abstract \nen\u00advironments, the inlining algorithm only needs the label and the contour. The label identifies the \nlambda ab\u00adst ract ion from which the closure was derived, and the contour describes the analysis context \nin which the ab\u00adstract closure was created. The function F can there\u00adfore be viewed as a projection function \nthat maps rela\u00adtively rich abstract values tot he simpler abstract values needed by the inliner. Many \nflow analyses, both mono variant [2, 4, 6, 15, 16, 23, 25] and polyvariant [3, 11, 17] produce results \nthat can be cast into the form of func\u00adtion F. Polyvariant analyses use contours to segregate ab\u00adstract \nvalues and improve the accuracy of the analy\u00adsis. Most analyses create a new contour at each vari\u00adable \nbinding point and use it while analyzing the code in the lexical contour established by the binding point. \nHow contours are created, however, differs from analysis to analysis, and the choice diet ates an analysis \nsplit\u00adting strategy. There are two popular strategies. The first uses the variables bindings to guide \ncontour cre\u00adation [11]. The second uses the calling context in which the bindings are established. The \nsecond strategy is often referred to as the call-string approach [15, 17, 25]. Consider the following \nprogram as an example. (let ((f (A (x) x))) (begin (f 2) (f 5))) In this program the procedure bound \nto f is called at two points. Depending on the splitting strategy, a poly\u00advariant analysis might create \neither one or two contours in which to analyze the procedure s body. If the strategy is based on the \ntypes of the bindings, only one contour would be created since the procedure is caJled with only integers. \nIf the strategy is based on call sites then two contours would be created. One contour would repre\u00adsent \nthe evaluation context when the procedure is ap\u00adplied to 2. The other would represent the context when \nthe procedure is applied to 5. Determining which split\u00adting strategy is better depends on the consumer \nof the flow information. Although it is not a requirement, the inlining trans\u00adformation in Section 3 \nassumes a splitting strategy based on call sites. The grammar of the source and target lan\u00adguage supports \nthis strategy. At a call site (e. . . . en) ~, the code of an applied abstract closure (/, K) is assumed \nto be analyzed in the contour t : K, i.e., the contour obtained by prepending the sequence K with the \ntag t. eCE M p q 1 E Labels t = = E G = G MP[(A (XI clxl (begin ExpLabels P? ocLabels ExpLabels u Keys \n. ..xn) 13)* l?l Ez)l(if ProcLabels El E2 E3)I(EOEI . .. En)t/(letrec ((x EI))E2) Figure 1: Source and \ntarget language for the inlining algorithm. Many implemented analyses are based on this split\u00adting strategy. \nAn analysis is monovariant if Keys is a singleton set and all call sites are tagged with the same key. \nThe analysis is similar to Shivers s lCFA analysis if each call site is tagged with a unique key. A less \nac\u00adcurate analysis that is suitable for inlining is obtained by tagging with a unique key each call site \nthat c has an operator that is a variable reference refer\u00ad ring to a syntactically recognized let\u00ad or \nletrec\u00ad bound procedure, and . does not represent a direct recursive call to the procedure. All other \ncalls are tagged with the generic key tg. This strategy is the one used for polyvariant analysis of programs \nin this paper. Examples Some examples are useful for understanding the differ\u00adences between monovariant \nand polyvariant analyses. The examples are written in the syntax of Figure 1 but let expressions are \nused to abbreviate direct applica\u00adtions of lambda expressions, and the examples are only partially labeled \nto avoid clutter. It is also assumed that the abstract value of a nonprocedure value is its type ex\u00adcept \nfor booleans, which maintain their trut h value. For example, the abstract value of 5 is {integer}, and \nthe abstract value of #t is {true}. As a first example consider the following. (let ((f {A (x) Xp))) \n(begin (f 2),, (f #f),,)) A rnonovariant analysis of this expression begins by an\u00adalyzing the expression \n(J (x) xP) in the empty contour (). The resulting abstract closure is then applied twice. The first application \ncauses the body XPto be analyzed in the contour (tg).The second causes the body to be analyzed again \nin the contour (tg). The analysis is forced to merge the information from the two call sites, and therefore, \nF(xP, (t~ )) = {integer, jake}. Below is the same program but annotated for a poly\u00advariant analysis. \n(let ((f (A (x) xP))) (begin (f 2)to (f #f)tl)) In this case a polyvariant analysis begins by analyz\u00ading \nthe expression (J (x) xP) in the empty contour (). The first application causes the body XPto be analyzed \nin the contour (to).The second application, however, causes the body to be analyzed in the different \ncon\u00adtour (tl ). Consequently, F(xP, (to))= {integer} and I (xP, (tl ) ) = {false}. This is more precise \nthen the in\u00adformation collected by the previous monovariant analy\u00adsis. As another example consider this \nprogram partially labeled for a monovariant analysis. (let ((f (~ (a) (~ (b) (if a b #f))~ )qO)) (l;; \n((( f~f (h) (f #t)t, (f #f)tg))) g In this example the abstract value of f is the singleton set containing \nthe closure (q., ()). Since the result of the czdl (h) is unknown, the closure is applied to both #t \nand #f. The result of the first call is the abstract value {(ql, (t~))}, and the result of the second \ncall is the same value. Thus the abstract value of g is { (ql, (tg) ) }. At the call (g 5) thebody oft \nhis second closure is evaluated in the contour (t~, t~). Since the value of a is {true, fake}, the result \nof the call is {integer, faise}. For a polyvariant analysis the above program is writ\u00adten (let ((f (~ \n(a) (~ (b) (if a b #f))g )gO)) (let ((g (if (h) (f #t)*O (f #f)tl))) (I3 S)t,)) In this case the abstract \nvalue of g is the set {(qo, (to)), (qo, (~1))}. The first closure represents the lambda expression evaluated \nin a context where a is bound to {true}, and the second represents the expres\u00adsion evaluated in a context \nwhere a is bound to {@e}. At the call site (g 5) both closures are applied. Apply\u00ading the first closure \nreturns the abstract value {integer} while thesecond returns the abstract value {jalse}. The result of \nthe whole program is the union of these two values: { intege~, @se}. 3 The inlining algorithm The inlining \nalgorithm is specified in Figure 2. It is an improved version of the algorithm presented by Ja\u00adgannathan \nand Wright [18]. In the figure the notation P(A) denotes the powerset of A and FV is a function that \ntakes an expression and returns a sequence of the variables that occur free in that expression. The inliner \nis a function Z that takes an expression, a contour set, and a loop map. It returns an expression with \npossibly some or all call sites inlined. The con\u00adtour set and loop map represent the inlining context, \nand the role of each will be explained together with the algorithm. Given a program e, the value of Z[e]OO \nis the inlined program. The target language of the inlin\u00ading algorithm is a closure-converted [26] variant \nof the source. Closure-conversion is required, since procedures may be inlined outside of the lexical \nscope of their free variables. The behavior of the function Z depends first on the form of input expression. \nConstants and variables are mapped to themselves. A begin expression is rebuilt by inlining its constituent \nsubexpressions. A A expres\u00adsion must be residualized in all possible calling contexts This explains why \nthe inliner must maintain a set of cent ours as opposed to a single contour. During flow analysis a procedure \nactivation occurs in a single analy\u00adsis context, but during transformation a procedure may be residualized \nwithout knowing how it will be applied. The inliner must therefore assume that the procedure will be \nevaluated in all contexts in which the proce\u00addure may be applied. Hence the need for a contour set. The \npossible calling contexts for the lambda expression are obtained by using the function add to prepend \neach contour in the contour set y with each of the elements of Ke~s. For example, if -Yis the set {(tl), \n(tO)} and Keys is the set {t~, to, tl }, then the new contour set is {(~g, ~1), (tg, ~o), (~o>tl)l(to, \nto), {tl, ~1), (~1, ~o)}. A conditional expression is inlined depending on the abstract value of its \ntest. If the abstract vzdue repre\u00adsents a value that is both true and false, both arms of the conditional \nare inlined and the entire conditional is reconstructed. If t he value is definitely true or definitely \nfalse, then one arm or the other can be pruned in the output expression. If the abstract value is neither \ntrue nor false, then the arms of the conditional are consid\u00adered dead, and the test is evaluated for \neffect. A letrec expression is inlined by rebuilding the ex\u00adpression using inlined versions of the subexpressions. \nThe right-hand side is evaluated in an updated loop map. The updated loop map is used to record the inlin\u00ad \ning context of a letrec-bound expression if the value of the expression is a merged abstract closure \nderived from a single lambda abstraction. An application can be transformed in one of three ways. If \nno inlining can be justified the call is simply reconstructed. The other two cases accommodate an inlined \ncall. In the first case the following conditions must hold: the operator evaluates to a single merged \nabstract closure, the transformed procedure body is small enough to inline according to the predicate \ninline?, and there is not already a binding for the procedure, confirmed by exkining the ~omain of the \nloop map p. If these conditions hold, the procedure is inlined at the call site and recursively bound \nto a fresh variable f. Al\u00adthough not specified in the figure, the procedure may be inlined directly at \nthe operator position of the call if there are no references to f in the transformed proce\u00addure s body. \nThe second way in which an application may be in\u00adlined is if the operator evaluates to a single merged \nab\u00adstract closure, and there already exists an entry in the loop map for the procedure. In this case, \nthe applica\u00adtion is reconstructed with the the new operator being a reference to the binding variable \nfor the preexisting specialization. As an example consider the program in Figure 3(a). It is the factorial \nprogram annotated for and analyzed with a lCFA analysis. The application (f 5) is inlined as well as \nthe recursive call. The recursive call creates a new copy of the procedure, because the labels on the \ntwo call sites are different. At the next recursive call, however, the inliner finds an entry in the \nloop map and residualizes the recursive call. Merging abstract values Since the inlining context is composed \nof a set of con\u00adtours, the abstract value of an expression cannot be ob tained directly using the function \nF. Instead, the ab\u00adstract values of the expression in all the contexts of ~ must be merged. In the definition \nof M, the set V repre\u00adsents the union of the abstract values of the expression in each of the contours \nspecified by ~. The function M further maximizes the utility of the set V by merging the abstract closures \nin the set. The abstract closures derived from a label p are collapsed into a single merged abstract \nclosure. The merged clo\u00adsure consists of the label p and a set of contours built from the abstract closures \nin V that contain the label p. While this merging is not helpful for residualizing Ex coi&#38;sXLOOpS+E \n7qcomLrs) p E Loops = ProcLabels x C o~urs * Variables Z[(begin e, e~)]~p = (begin T[el]Tp ~[e~] y~) \n I[(J (xl . . . Xn) e)]vp = (A [21 . . . Zm] (xl . . . Xn) Z[e]add(Keys, T)p) where (zl, ..., Zm) = FV((A \n(XI . . . Xn) e)) if true? (kf(~, ~)) Afa/se?(~(/, y)) if true? (kf(l, y)) A =jalse?(~(l,v)) if =true?(ll(~, \n~)) Afake?(~(/, y)) otherwise T[(letrec ((x e;)) ez)]-y~ = (letrec ((x T[el]add(Keys, -y)p )) ~[ez]~p) \np[k,tg) + Z] if ~(L7) = {(9,-/)} where p = otherwise {P Z[(e$ el ... en) t]~p = ((letrec ((f (A (w xl \n. . . xn) if iniine?(~[eo]~ ~ ) and (q, T ) # dorn(p) ((A [z, . . . z~] Z[e]~ p ) where M(io, 7) = {(q, \n~ )} (closure-ref w 1) P(q) = (A (xl ... xm) e) [ (2,,..., Zm) = FV((A (xl . . . Xn) e)) -f = affd({t}, \n7 ) (closure-ref w m))))) ~ is fresh f) P = P[(9!7 ) + f] ~[eo]~~ ~[el]~p . . . I[en]7p), { I(f Z[eO]~p \nT[el]-yp . . . Z[eJyp) ~ if p((q, add({t}, ~ )) = ~ where M(10,7) = {(q, v )) M: ikf(i,7) = Labels x \nCo~urs ~ P({true, (V n {true, /alse}) U {(q, ~) [q where V = IJ{F(I, IC)I~ e y} false} U (ProcLabels \nc ProcLabels A y = x Co%urs)) {K I (q, ~) E V} A y # 0} add add(T, ~) : = Keys x Co~urs {t:~ltE~A~C~} \n+ Co~un Figure 2: The inlining algorithm. (l;:r;t)jf (A (x) (if (= x O) 1 (*x (f (-x 1)),,))))) (letrec \n((f (~ (x) (if (= x O) 1 (*x (f (-x 1)),,))))) ((A (g, x) (if (=xO) :*x ((l;~c ((g, (A (g, x) (if (= \nx O) 1 (*x (g, f (-x l))t,))))) f (-x 1)),,))) f 5) to)  (a) The factorial program before and after \ninlining, (let ((f (~ (a) (~ (b) (if a b #f))q )qO)) (let ((g (if (h) (f Xt)i, (f tf)tl))) (g 5)t2)) \n (let ((f (J (a) (~ (b) (if a b #f))q )qO)) (let ((g (if (h) ((A (g. a) (J (b) b)q ) f #t)to ((A (g, \na) (A (b) #f) ) f #f)t, ))) ((A (g, b) (let ((a (closure-ref &#38; l))) (if a b #f))) g S)t,))  (b) \nA higher-order program before and after inlining. Figure 3: Two examples of inlining justified by a lCFA \nanalysis. The programs are only partially labeled. conditionals, the merged closures can be useful when \ninlined, the expression (A (b) (if a b #f)) is residu\u00adresidualizing calls. alized in both contexts. By \nmaintaining the inlining context it is possible for the inliner to prune the con- Figure 3(b) is an example \nof inlining with merged ditionals in both cases. When the call (g 5) is inlined closures. It is the last \nexample from Section 2 analyzed the conditional cannot be pruned because it is inlined using a polyvariant \nanalysis. Since the abstract value in a context where the abstract value of a is the set of f is a singleton \nset containing one closure, both calls {true, ~alse}. to f would be inlined assuming that the inline? \npredi\u00adcate was satisfied. At the call site (g 5), however, the abstract value of the operator is the \nset consisting of 4 Implementation two closures: { (ql, (to) ), (ql, (tl )) }. From the definition of \nM, this value would be transformed into the value The inlining algorithm is fully implemented in the \nChez {(91, {(to), (tl)})}. Thus without merging the closures, Scheme [9]compiler. It is implemented as \nspecified with the call (g 5) could not be inlined, but after merging it two differences. First, the \ninliner s output is not closure\u00ad can be inlined. converted at the source level. Subsequent simplification \n The example also illustrates how maintaining the in-may eliminate free variables, so it is desirable \nto avoid lining context can improve residualization of lambda selecting a closure s layout until later \nin compilation. expressions. When the calls (f #t) and (f #f) are Second, a free variable referenced \nthrough an operator s closure is avoided if the free variable is visible at the and ( x 1) can be folded. \nThis then permits the outer reference point. The implementation approximates inline?. Instead of querying \nwhether a specialized procedure body is small enough for inlining, the implementation examines the original \nprocedure body wit h conditionals pruned where flow information deems it safe to do so. A sim\u00adple node \ncount of the parse tree is computed and the procedure is inlined if t he count is under a threshold set \nbefore inlining is begun. The inliner is run after the flow analysis and before procedure call optimizations \nthat are also justified by flow information [2]. Immediately after inlining a sim\u00adplification pass is \nperformed to profit from the larger intraprocedural contexts introduced by inlining. Also, the inlining \nalgorithm cannot preserve flow information to justify procedure call optimizations, so the flow anaL \nysis must be run again after simplification. After the second flow analysis compilation is cent inued. \n4.1 Simplification Inlining introduces opportunities for constant folding, copy propagation, and useless-binding \nelimination. A on~pass simplifier performs these operations on the in\u00adliner s output. With one exception \nit is a simple version of the optimizer described by Waddell and Dybvig [27] but does not perform procedure \nintegration as part of copy propagation. The exception is the treatment of inlined procedure calls that \nresult in a recursive call to the inlined proce\u00addure. Consider this possible output from the inliner \nas an example. ((letrec ((go (A (gl x) (go gl x)))) go) f 5) The variable gl is unneeded, and the program \nshould be transformed into (~rec ((go (A (x) (go x)))) go) A naive simplifier would be unable to perform \nthis transformation, however, since the variable gl is refer\u00adenced for value. The implemented simplifier \nis aware, however, of recursive bindings introduced by the inliner and recursive calls to those bindings. \nThe simplifier is therefore able to eliminate such useless bindings when\u00adever possible. As two complete \nexamples, the simplified versions of the inlined programs from Figure 3 are given in Fig\u00adure 4. In Figure \n4(a), the outer letrec expression is eliminated because there is no longer a reference to f. The bindings \nfor b and x are eliminated since sll refer\u00adences to them are propagated. Assuming that the calls to -and= \nare inlined primitives, the outer calls (= x O) conditional to be folded as well. The simplified output \nin Figure 4(b) is obtained in a similar fashion. Constants are propagated to their reference points, \nand the bindings rendered useless by propagation are eliminated. 4.2 Preserving flow information The \ninlining algorithm preserves flow information about nonprocedure values but it violates the soundness \nof in\u00adformation about procedures. Consider this program zm\u00adnotated for and analyzed with a monovariant \nanalysis. (let ((f (A (a) (A (b) 5)q )qO)) (k; ($ ;;f (h) (f 1),, (f 2),,))) 9 After the analysis the \nabstract value of g is the set {(91,(L)}}. Assume that during inlining the calls to f are inlined but \nthe call tog is not. After inlining and simplification the output is (let ((g (if (h) (~ (b) 5)q (~ (b) \n5)q ))) (g 3),,) At this point the abstract value of g is unsafe. The value indicates that only one \nprocedure may arrive at the call site at run time when in fact either of two procedures may arrive. Thus \nthe information is unsound for Oow\u00adbased procedure call optimizations. Consequently, the flow analysis \nmust be run again after inlining in order to recompute flow information. If the inliner performs closur~conversion \nat the source level it does not matter how accurate the analysis is. In our implementation, however, \nan ardysis at least as accurate as OCFA must be used in order to correctly resolve closure-ref operations \nlater in the compiler. 5 Evaluation The goal of our evaluation is to determine how the qual\u00adity of flow \ninformation affects the inlining algorithm s ability to optimize programs. To do this, four flow anal\u00adyses \nwere used to evaluate the inliner. In all four analy\u00adses, conditional tests were used when possible to \nrestrict the abstract values of variables in the arms of condition\u00adals [16]. The analyses are subOCFAl, \nOCFA, sublCFAl, and lCFA. OCFA and lCFA are relatively well-understood anal\u00adyses. The subOCFA1 and sublCFA1 \nanalyses are ap\u00adproximations. From the point of view of abstract inter\u00adpretation, OCFA is the monovariant \nanalysis obtained by allowing the interpreter to evaluate an expression an (*5 ((letrec ((go (/4 (x) \n(if (= x O)tg 1 (*x (g, (-x l))tg))))) go) 4)t, ) (a) The simplified factorial program. (let ((g (if \n(h) (~ (b) b)q (~ (b) #f)q ))) (let ((a (closure-ref g l))) (if a 5 #f))) (b) The simplifiedhigher-orderprogram, \nFigure 4: The two examples from Figure 3 after simplication has been performed. unlimited number of times. \nFor a constraint-based anal\u00adysis, this corresponds to allowing the solver to update a constraint variable \nan unlimited number of times. Lim\u00aditing the number of evaluations, however, yields a class of analyses \nwe call subOCFA. In particular, subOCFA. is the analysis obtained by limiting the analysis ton evalu\u00adations \nbefore forcing it to select a safe solution that may be less precise than the least solution. The class \nforms a hierarchy with subOCFAo the least accurate analysis in the class and OCFA the limit as n goes \nto infinity. The development of the sublCFA class of analyses is analogous. The relative accuracy of \nthe two classes is clear in some cases but not in others. OCFA is less accurate than lCFA, and for any \nn, subOCFAn is less accurate than sublCFA.. On the other hand, for any n, OCFA is neither more nor less \naccurate than sublCFAfl. For some programs OCFA will be more accurate and for oth\u00aders sublCFAm will be \nmore accurate. The implementation was evaluated using the bench\u00admarks described in Table 1. The benchmarks \nwere compiled as whole programs. Furthermore, the dy\u00adnamic, graphs, lattice, matrix, maze, nbody, and \nsplay benchmarks had many primitive procedures explicitly included in their source. Examples of such \nprocedures include map, assq, and cadr. These modified bench\u00admarks were the same benchmarks used by Jagannathan \nand Wright [18]. Table 2 gives statistics on the cost of analyzing the benchmarks for each of the four \nflow analyses. These statistics were collected on an Intel 80686 processor with a 512k level two cache, \n128Mb of memory, and 100MB of swap space. The statistics support the claim that polyvariant analyses \nare in practice more expensive than monovari\u00adant analyses. In all cases, more cpu time is required by \nthe polyvariant analyses than by the monovariant anal\u00adyses. Furthermore, the amount of cpu time required \nby a polyvariant analysis can be significant. For example, the lCFA analysis needs three seconds to analyze \nma\u00adtrix yet the benchmark is only 575 lines of code, and the lCFA analysis exhausted virtual memory analyzing \nthe interpret benchmark. Table 3 shows the run-time reduction in calls in or\u00adder to measure the analyses \neffectiveness at identify\u00ading sites where inlining may occur. The data includes only calls that could \nbe inlined, i.e., calls to top-level bound variables and inlined primitives are omitted. It also does \nnot include direct calls that are equivalent to let expressions. The inliner was run at thresholds 10 \nand 30, and the ratios are given as the number of calls performed by the inlined program over the number \nof calls performed by a baseline with no irdining or sim\u00adplification performed. All numbers are less \nthan one, since inlining cannot introduce new calls other than calls that are treated as let expressions. \nThe data supports the claim that more inlining is justified w the accuracy of the analysis improves, \nbut the relatively inaccurate analyses are also able to justify a significant amount of inlining. Table \n4 shows change in object code size. Again, the numbers are given as ratios over a baseline with no inlining \nor simplification performed. The threshold 10 is relatively safe as it does not cause any significant \nincrease in code growth. At threshold 30, however, there is sometimes signif\u00adicant code growth. Furthermore, \ninlining justified by a polyvariant analysis can cause code growth larger than when justified by a monovariant \nanalysis. As illustrated in Figure 3(a), inlining when justified by a polyvariant amdysis causes unrolling \nthat does not occur when in\u00adlining is justified by a monovariant analysis. The aim in these cases is \nto take advantage of the polyvariant analysis extra precision to perform pruning or other optimizations \nthat would not be possible with a mono variant analysis. Figure 4(a) shows this is possible, but as the \nnumbers for the benchmark dynamic show, this optimism may dramatically increase code growth if code benchmark \nI lines I Description . boyer 500 logic programming benchmark originally by Bob Boyer conform 450 type \nchecker by Jim Miller dynamic 2,275 a dynamic type inference applied to itself earley 650 Earley s parser \nby Marc Feeley graphs 500 counts directed graphs with distinguished root and k vertices, each having \nout-degree at most 2 interpret 1,000 Marc Feely s Scheme interpreter evaluating takl lattice 200 enumerates \nthe lattice of maps between two lattices mat rix 575 tests whether an n x n matrix satisfies a particular \nproperty maze 800 computes path through a random maze using a union-find algorithm nbody 1,500 Greengard \nmult ipole algorithm for computing gravitational forces peval 600 Feeley s simple Scheme partial evaluator \nsimplex 175 simplex algorithm splay 950 an implementation of splay trees Table 1: Benchmarks used to \nevaluate the inlining algorithm. benchmark subOCFAl OCFA sublCFAl lCFA boyer 0.0 0.0 0.0 0.1 conform \n0.0 0.1 0.2 0.3 dynamic 0.3 0.3 2.0 2.1 earley 0.1 0.1 0.4 0.4 graphs 0.0 0.0 0.2 0.2 interpret 0.4 6.1 \n1.2 lattice 0.0 0.0 0.1 0.2 mat rix 0.0 0.1 0.5 3.0 maze 0.0 0.0 0.2 0.2 nbody 0.1 0.2 1.1 2.0 peval \n0.1 0.1 0.4 0.4 simplex 0.0 0.0 0.2 0.2 splay 0.1 0.1 0.8 1.0 average 0.1 0.5 0.6 0.8 Table 2: Statistics \non CPU time used to analyzes each benchmark. Times are in seconds. simplification does not occur. cause \nthe compiler already optimizes calls, and there-Figure 5 gives run-time performance improvements fore, \neliminating procedure calls does not improve per\u00adwith simplification enabled. The numbers are given as \nformance significantly. For example, the lattice bench\u00ad ratios over a baseline with no inlining or simplification \nmark had 20% fewer procedure calls when optimized performed. Speedups improve as the inlining threshold \nusing the lCFA analysis over the OCFA analysis, but it is increased, but this may not hold on machines \nwith was only 370 faster. large caches, since code growth may interfere with cache behavior. The lattice \nbenchmark shows the largest im\u00ad 6 Related work provement, running 20% faster when optimized using the \nlCFA analysis rather than the OCFA analysis. Most Our work is most closely related to Jagannathan and \nbenchmarks, however, improve by only a few percent if Wright s [18]. Our work elaborates on their results \nby at all. comparing a number of different flow analyses and im- The data on speedups and reduction in \ncalls indi-proving upon their inlining algorithm. They judged cates that inlining can improve performance \nprimarily their inlining algorithm using a single polyvariant flow by enabling intraprocedural optimizations \nelsewhere in analysis while we evaluated the algorithm using four the compiler, e.g., the simplification \nphase run after in-analyses, both monovariant and polyvariant. Our alg~ Iining. Procedure call overhead \nis not very large be-rithm improves upon their algorithm in that it exposes Table 3: Ratio of run-time \ncalls over the number or czdls performed by a baseline with no inlining performed. Ratios are measured \nfor inlining thresholds 10 and 30. benchmark subOCFAl OCFA sublCFAl lCFA subOCFAl OCFA sublCFAl lCFA \nboyer 0.98 0.98 0.98 0.98 0.89 0.89 0.84 0.84 conform 0.31 0.31 0.31 0.31 0.07 0.07 0.06 0.06 dynamic \n0.34 0.34 0.34 0.34 0.32 0.32 0.19 0.19 earley 0.59 0.59 0.59 0.59 0.52 0.52 0.45 0.45 graphs 0.91 0.91 \n0.91 0.91 0.67 0.67 0.50 0.50 interpret 0.83 0.83 0.83  0.83 0.83 0.83 lattice 0.99 0.99 0.99 0.99 0.53 \n0.53 0.34 0.34 matrix 0.69 0.69 0.77 0.69 0.53 0.53 0.53 0.46 maze I 0.31 0.31 0.31 0.31 0.29 0.29 0.29 \n0.29 nbody 0.96 0.96 0.96 0.96 0.74 0.74 0.53 0.53 peval \\ 0.93 0.93 0.93 0.93 0.49 0.49 0.45 0.44 simplex \n0.32 0.32 0.32 0.32 0.32 0.32 0.30 0.30 splay 0.57 0.55 0.61 0.54 0.48 0.45 0.52 0.44 average 0.67 0.67 \n0.68 0.66 0.51 0.51 0.45 0.40 10 30 benchmark subOCFAl OCFA sublCFAl lCFA subOCFAl OCFA sublCFAl lCFA \nboyer 1.03 1.03 1.03 1.03 1.47 1.47 2.33 2.33 conform 0.94 0.94 0.94 0.94 1.06 1.06 1.19 1.19 dynamic \n2.00 2.00 2.08 2.00 3.56 3.56 4.80 4.72 earley 1.07 1.07 1.07 1.07 1.06 1.06 1.13 1.13 graphs 0.71 0.71 \n0.71 0.71 0.73 0.73 0.82 0.82 interpret 1.23 1.23 1.23 2.06 2.06 2.21 lattice 1.05 1.05 1.05 1.04 1.20 \n1.20 1.51 1.51 matrix 1.03 1.03 1.03 1.02 1.11 1.11 1.24 1.23 maze 0.86 0.86 0.86 0.86 0.88 0.88 0.89 \n0.89 nbody 1.06 1.06 1.06 1.06 1.29 1.29 1.38 1.37 peva.1 ] 1.11 1.11 1.11 1.11 1.20 1.20 1.31 1.31 simplex \n0.89 0.89 0.89 0.89 0.89 0.89 0.92 0.92 I splay 1.30 1.33 1.23 1.27 1.42 1.44 1.42 1.46 average 1.10 \n1.10 1.10 1.08 1.38 1.38 1.63 1.57 Table 4: Ratio of object code size over the size of the baseline at \ninlining thresholds 10 and 30. Numbers smaller than 1 indicate a reduction in code size. 10 benchmark \nsubOCFAl OCFA sublCFA1 boyer 1.01 1,01 1.01 conform 0.70 0.70 0.70 dynamic 0.92 0.92 0.91 earley 0.93 \n0.93 0.93 graphs 0.89 0.89 0.89 interpret 0.89 0.89 0.89 lattice 1.02 1.02 1.02 matrix 0.98 0.98 0.98 \nmaze 0.84 0.84 0.84 nbody 0.98 0.98 0.98 peval 0.96 0.96 0.96 simplex 0.72 0.72 0.72 splay 0.95 0.94 \n0.93 average 0.91 0.91 0.90 Table5: fitioof runtime over the bweline atinlining the inlined program \nis faster than its baseline. more opportunities for inlining. It does this by build\u00ading more precise \nflow information for contexts in which Iambda expressions are residualized. In particular, if their algorithm \nis rewritten in terms of ours, the func\u00adtion M would not merge closures, and lambda e~res\u00adsions would \nbe residwdized in the contour set Contours. Jagannathan and Wright report better speedups for some benchmarks \ngiven in this paper. The differences may be due to the fact that their implementation in\u00adlines procedures \nbound in the top-level environment. Calls to top-level procedures are significantly more ex\u00adpensive than \ncalls to procedures lexically bound by let or letrec expressions. This unintended side-effect of their \nimplementation masks the actual benefits of in\u00adlining. In our experiments, on the other hand, each benchmark \nwas given in complete form to the analysis, the inliner, and the rest of the compiler. Hence the com\u00adpiler \nwas able to treat procedures that would have been bound in the top-level environment as lexically-bound \nprocedures. As a result, our speedups and code growth are comparable to those reported by others [10, \n13]. Static analyses have been used to reduce method dis\u00adpatch overhead in object-oriented languages \nby either irdining calls or converting them to static dispatches [1, 14, 20, 21, 22]. Agesen and Holze \nobtain good speedups for Cecil as well, averaging about 10% improvement. Plevyak and Chien obtain excellent \nspeedups for C++ when combining their optimizations with others [21]. Their combination of inlining with \nother optimizations makes the precise benefits of inlining difficult to deter\u00admine, but it appears to \nmake at least a 50 % improve\u00adment in speed on their benchmarks while no statistics on code growth were \ngiven. These results are to be ex\u00adpected, since C++ and Cecil are ideally suited for inlin\u00ad 30 lCFA \nsubOCFAl OCFA sublCFAl lCFA 1.01 0.99 0.99 0.97 0.98 0.70 0..58 0.58 0.57 0.57 0.93 0.90 0.90 0.98 0.95 \n0.93 0.91 0.91 0.90 0.90 0.89 0.83 0.83 0.79 0.79 0.89 0.90 0.89 1.02 0.97 0.97 0.75 0.75 0.98 0.95 0.95 \n0.95 0.92 0.84 0.79 0.79 0.79 0.79 0.98 0.94 0.94 0.91 0.91 0.96 0.87 0.87 0.88 0.87 0.72 0.72 0.72 0.72 \n0.72 0.92 0.92 0.92 0.90 0.90 0.91 0.87 0.87 0.85 0.84 thresholds lOand3O. Numbers smaller than l indicate \nthat ing, where method dispatch is expensive and methods to be inlined are small. Our improved inlining \nalgorithm is related to the spe\u00adcialization phsse of an off-line partial evaluator [19]. In\u00adlining a \ncall is amdogous to unfolding a call. To the best of our knowledge, merging abstract closures to expose \nmore opportunities for unfolding is a technique not used in specialization algorithms for partial evaluators. \n7 Conclusions We have evaluated four flow analyses to determine their effectiveness for justifying inlining. \nThe analyses rep\u00adresent a range of cost/accuracy tradeoffs, from a fast analysis that is less accurate \nthan OCFA to a polyvariant analysis similarly to lCFA. The inlining algorithm in\u00adcorporates a novel notion \nof merging that exposes more opportunities for inlining when justified by information collected by a \npolyvariant analysis. The improvement to the inlining algorithm can ex\u00adpose additional opportunities \nfor inlining when a poly\u00advariant analysis is used. The improvement selectively merges flow information \ndepending on the specialization context of the inliner. The merging folds multiple ab\u00adstract closures \nobtained from a single lambda expression into a single abstract closure. This merging technique could \nbe used by any program transformer that uses the results of a pol yvariant flow anslysis. The analysea \nand inlining algorithm were evaluated using several benchmarks. More procedure calls were eliminated \nas the accuracy of the amdyses improved. Speedups also improved somewhat as analysis accuracy increased, \nbut the speedups obtained from the accurate analyses were not significantly better than the improve\u00ad \nments from the less precise analyses. Given the cost of accurate flow analyses, it may not be worthwhile \nto use a polyvariant analysis during development and testing and instead use it only during the final \nstages of de\u00advelopment. Using a parameterized flow analysis frame\u00adwork [2], however, allows the compiler \nto be constructed such that either analysis can be run without changing the opt imizat ions passes. While \nspeedups improved using a polyvariant anal\u00adysis, code growth also increased. Better heuristics are needed \nto determine when it is profitable to inline proce\u00addures. One possibility is to combine simplification \nwith inlining in order to better judge what effect inlining will have on code size. Taking cues from \nwork on inlining for Cecil and C, it may also be worthwhile to combine flow analysis with dynamic profiling \ninformation [7] to make inlining decisions. The choice of source and target language in Figure 1 assumes \na splitting strategy based on call strings. The inlining is not dependent on this choice, however, and \nother strategies could be substituted with little diffi\u00adculty. One possibility is to base the decision \non types. This would perhaps lead to more pruning in a called pr~ cedure, causing the inliner to integrate \nthe procedure in cases where it otherwise would not. The inlining algorithm does not preserve the sound\u00adness \nof flow information. Thus the flow analysis must be run again if subsequent compiler passes depend on \nflow information. This complicates a compiler whose opti\u00admization are based on flow analyses, since opt \nimita\u00adtions must be ordered based on what flow analysis is run and at what time during compilation. The \ncompiler does not currently take full advantage of inlining, and speedups would perhaps improve if it \ndid. While the simplification pass is able to use the inlining results to perform more constant folding, \ncopy propagation, and useless-binding elimination, the regis\u00adter allocator [8] does not take advantage \nof the larger procedure bodies introduced by inlining. As Davidson and Holler [13] indicate, it can even \nbe detrimental to inline without cooperation from the register allocator. Acknowledgments: We thank Suresh \nJagannathan, Andrew Wright, and Marc Feeley for making their benchmarks available. References [1] Ole \nAgesen and Urs Holze. Type feedback vs. con\u00adcrete type inference: A comparison of optimiza\u00adtion techniques \nfor object-oriented languages. In 00PSLA 95, 10th Annual Conference on Object-Oriented Programming Systems, \nLanguages, and Applications, pages 91-107. ACM, 1995. [2] J. Michael Ashley. .4 practical and flexible \nflow analysis for higher-order languages. In Proceedings of the ACM Symposium on Pn nciples of Program\u00adming \nLanguages, pages 184 194. ACM, 1996. [3] J. Michael Ashley and Charles Consel. Fix\u00adpoint computation \nfor polyvariant static analyses of higher-order applicative programs. ACM 7kans\u00adactions on Programming \nLanguages and Systems, 16(5):1431-1448, 1994. [4] Andrew E. Ayers. Abstmct Analysis and Optimiza\u00adtion \nof Scheme. PhD thesis, MIT, 1993. [5] Anders Bondorf. Similix Manual, System Version 5.0. DIKU, University \nof Copenhagen, Denmark, 1993. [6] Anders Bondorf and Jesper Jorgensen. Efficient analyses for realist \nic off-line partial evaluation. Journal of l%nctional Progmmming, special issue on Partial Evaluation, \n1993. [7] Robert G. Burger and R. Kent Dybvig. An infrtx+ truct ure for profile-driven dynamic recompilation. \nSubmitted for publication. [8] Robert G. Burger, Oscar Waddell, and R. Kent Dy\u00adbvig. Register allocation \nusing lazy saves, eager re\u00adstores, and greedy shuffling. In Proceedings of the ACM SIGPLAN 1995 Conference \non Progmmming Language Design and Implementation, pages 130\u00ad  138. ACM, 1995. [9] Cadence Research Systems, \nBloomington, Indiana. Chez Scheme System Manual, Rev. 2.4,July 1994. [10] Pohua P. Chang, Scott A. Mahlke, \nand William Y. Chen. Profile-guided automatic inline expansion for C programs. Software -Pmctice and \nExperience, 25(5):349-369, May 1192. [11] Charles Consel. Polyvariant binding-time anal\u00adysis for higher-order, \napplicative languages. In Proceedings of the Symposium on Partial Evalua\u00adtion and Semantics-Based Program \nManipulation, PEPM 93, pages 66-77, 1993. [12] Charles Consel. A tour of Schism: A partial evalua\u00adtion \nsystem for higher-order applicative languages. In Proceedings of the Symposium on Partial Evalu\u00adation \nand Semantics-Based Progmm Manipulation, PEPM 93, pages 145-154, 1993. [13] Jack W. Davidson and Anne \nM. Holler. A study of a C function inliner. Software -Pmctice and Experience, 18(8):775-790, August 1988. \n [14] Jeffrey Dean, Craig Chambers, and David Grove. Selective specialization for object-oriented lan\u00adguages. \nIn Proceedings of the ACM SIGPLAN 1995 Conference on Programming Language Design and Implementation, \npages 93-102. ACM, 1995. [15] W. L. Harrison III. The interprocedural analysis and automatic parallelization \nof Scheme programs. Lisp and Symbolic Computation, 2(3/4):179-396, 1989. [16] Nevin Heintze. Set-based \nanalysis of ML programs. In Proceedings of the 1994 ACM Conference on LISP and Functional Pmgmmming, \npages 306-317. ACM, 1994. [17] Suresh Jagannathan and Andrew Wright. Effec\u00adtive flow analysis for avoiding \nrun-time checks. In Proceedings of the 1995 International Static Anal\u00ad ysis Symposium, volume 854 of \nLecture Notes in Computer Science, pages 207 224. Springer-Verlag, 1995. [18] Suresh Jagannathan and \nAndrew Wright. Flow\u00addirected irdining. In Proceedings of the ACM SIG-PLAN 1996 Conference on Pmgmmming \nLanguage Design and Implementation, pages 193-205. ACM, 1996. [19] Neil D. Jones, Carsten K. Gomard, \nand Peter Ses\u00adtoft. Partial Evaluation and Automatic Program Genemtion. Prentice-Hall, 1993. [20] Hemant \nD. Pande and Barbara G. Ryder. Static type determination and aliasing for C++. Tech\u00adnical Report LCSR-TR-250-A, \nRutgers University Department of Computer Science, October 1995. [21] John Plevyak and Andrew A. Chien. \nAutomatic in\u00adterprocedural optimization for object-oriented lan\u00adguages. Submitted for publication. [22] \nJohn Plevyak and Andrew A. Chien. Precise con\u00adcrete type inference for object-oriented languages. In \nOOPSLA 94,Ph Annual Conference on Object-Oriented Progmmming Systems, Languages, and Applications, pages \n324-340. ACM, 1995. [23] Manuel Serrano and Marc Feeley. Storage use ad\u00adysis and its applications. In \nProceedings of the 1996 ACM SIGPLAN International Conference on Functional Programming, pages 50-61. \nACM, 1996. [24] Olin Shivers. Control-flow analysis of higher-order languages. PhD thesis, Carnegie Mellon \nUniversity, 1991. CMU-CS-91-145. [25] Olin Shivers. The semantics of Scheme control-flow analysis. In \nProceedings of the Symposium on Par\u00adtial Evaluation and Semantics-based Program Ma\u00adnipulation, PEPM 91, \nNew Haven, Connecticut, 1991. [26] Paul A. Steckler and Mitchell Wand. Lightweight closure conversion. \nACM Transactions on Program\u00adming Languages and Systems, 19( 1):48-86, 1997. [27] Oscar Waddell and R. \nKent Dybvig. Fast and ef\u00adfective procedure integration. Submitted for publi\u00adcation.  \n\t\t\t", "proc_id": "258948", "abstract": "An interprocedural flow analysis can justify inlining in higher-order languages. In principle, more inlining can be performed as analysis accuracy improves. This paper compares four flow analyses to determine how effectively they justify inlining in practice. The paper makes two contributions. First, the relative merits of the flow analyses are measured with all other variables held constant. The four analyses include two monovariant and two polyvariant analyses that cover a wide range of the accuracy/cost spectrum. Our measurements show that the effectiveness of the inliner improves slightly as analysis accuracy improves, but the improvement is offset by the compile-time cost of the accurate analyses. The second contribution is an improvement to the previously reported inlining algorithm used in our experiments. The improvement causes flow information provided by a polyvariant analysis to be selectively merged. By merging flow information depending on the inlining context, the algorithm is able to expose additional opportunities for inlining. This merging technique can be used in any program transformer justified by a polyvariant flow analysis. The revised algorithm is fully implemented in a production Scheme compiler.", "authors": [{"name": "J. Michael Ashley", "author_profile_id": "81332488447", "affiliation": "University of Kansas, Snow Hall 415, Lawrence, Kansas", "person_id": "PP39069151", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/258948.258959", "year": "1997", "article_id": "258959", "conference": "ICFP", "title": "The effectiveness of flow analysis for inlining", "url": "http://dl.acm.org/citation.cfm?id=258959"}