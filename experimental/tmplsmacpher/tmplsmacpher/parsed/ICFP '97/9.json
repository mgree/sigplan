{"article_publication_date": "08-01-1997", "fulltext": "\n Lambda-Splitting: AHigher-Order Approach to Cross-hloduleO ptimizations Matthias Blume Andrew W. Appel \nPrinceton University Abstract We describe an algorithm for automatic inline expansion exroes module \nboundaries that works in the presence of higher-order functions and free variables; it rearranges bindings \nand scopes M necessaryto move nonexpansivecode from one module to an\u00adother. We describe-and implement \nthe algorithm as transfor\u00admations on A-calculus. Our inliner interacts well with separate compilation \nand is efficient, robust, and practical enough for ev\u00aderyday use in the SML/NJ compiler. Inlining improvee \nperfor\u00admance by 4 8% on existing code, and makes it possible to use much more data abstraction by consistently \neliminating penalties for modularity Introduction Abstraction and modular design of software promote \nclarity and provide clear lines along which large projects can be subdivided. But one often pays a large \nperformance penalty for using abstraction. Cross-module irdining can bridge the gap between abstract \ndesign and high performance by trans\u00adparently moving the border between compilation units. We need not \nexplain in detail the dieadvantages of mak\u00ading the programmer inlin+expand by hand: burdening the programmer, \nblurring modular design by expcsing im\u00adplementation details, and making it difficult to adjust the amount \nof inlining dynamically for development (recompil\u00ading) or (shrink-wrapping mode. But the automatic cross-module \ninlining schemee ueed to date have not treated free variables, nested scopes, higher\u00adorder functions, \nor link-time side effects from module-level initializers [DH88, CHT91, CMCH92]. They cannot move a function-body \nfrom module A to module B if the function has a flee variable that is not exported from A and cannot \nbe copied into B. This limite the generality of existing ap\u00adproaches, especially when applied to higher-order \nfunctional languages. One might think of inlining fimctions after cloeure-conver\u00adsion, when functions \nhave no free variables, but thie does not solve the problem because the free variables have become function \narguments, and the callers in other scopes have no access to the corresponding actual-parameter values. \nOur new technique, A-splitting, is fully automatic. It exposes implementation details on an as-needed \nbasis only Permission to make digital/harri copy of part or all this work for personal or classroom use \nis granted without fee provided that copies are not made or distributed for profit or commercial advan\u00ad \ntage, the copyright notice, the title of the publication and its date appear. and notice is given that \ncopying is by permission of ACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute \nto lists, requires prior specific permission and/or a fee, to the compiler of client code; abstraction \nand modularity are never compromised at the source level. Furthermore, by tuning a few compilr+time parameters \none can adjust the aggreaeivenees of cross-module irdining or turn it off com\u00adpletely. In contrast to \nprevious experiment [Sch77, CHT91] that did not explain how to preserve efficient separate compila\u00adtion \nwhile inlining, our technique is fully integrated with SML/NJ s separate compilation system: it cleanly \nexports irdinable portiona of one compilation unit through the bi\u00adnary object file into the importing \nmodule. Our approach should be applicable to a wide range of languages and compilers with intermediate \nlanguages based on A-calculi [KKR+ 86, Pey87]. A-calculus has the advantage over competing intermediate \nrepresentations of being a well\u00adstudied logical system. It also proved to be very convenient for expressing \nthe necessary tranaformatione in our proto\u00adtype. Although the emphasis on J-calculus favors functional \nprogramming languages as a target for our technique, the ideas could be adapted to other languages and \nimplementa\u00adtions. See appendix A for a description of the intermediate language used. Our compiler makes \ncroee-moduling inlining decisione automatically. Thie frees the programmer from the bur\u00adden of having \nto worry about many drnicr~optimizatione, The relationship between functions to be irdined and source\u00adlevel \nconstructs ie not always obvious to the programmer, especially in implementatione that pass type information \nat runtime, and which therefore may encode polymorphism as abstraction and type specialization as function \napplica\u00adtion [Oho92, T0194, HM95]. Resource-conscious programmers often like to say ex\u00adplicitly that \nthey believe certain procedure calls should be inline-expanded. Performance hints from a profiling feed\u00adback \nsystem [CMCH92] can play a similarly important role. Our fkrnework can easily be adapted to take account \nof external hints. J-splitting moves function bodies from one module to another, but ensuring that these \nfunctions are then inlined within the importing module is up to the existing intrarnod\u00adule inliner [App92], \nwhich does a good but not perfect job. Since our intermodule inlining algorithm preeerves eeparate compilation, \nit muet make decisions without seeing the client modules, so we rely on simple syntactic cues instead \nof pow\u00aderful datafiow analyees [JW96] that could aid inlining deci\u00adsions. ICFP 97 Amsterdam, ND @ 1997 \nACM 0-89791 -918 -1/97 /0006 . ..$3.50 Example Consider a simple procedure, which maintains the maximum \nof the values it has been presented with so far. We param\u00adetrize the implementation so it can be instantiated \nwith different comparison predicates It. fun extremeFun (lt, xO) = let val e =ref xO fun get () =!e \nfun checkO x = if lt (!e, x) then e :=x else () fun check [1 = () I check (h :: t) = (checkO h; check \nt) in (get, check, lt) end It is often desirable to reline function calls, especially if the functions \nare small and calls occur frequently. This is true regardless of whether the function in question is \nde\u00adfined within the same or a different compilation unit. Let our example above be one compilation unit, \nwhile another unit instantiates extremeFun in order to be able to use the resulting procedures check \nand get: val (get, check, lt) = extremeFun (op < , 1000O) ; ... Calls to get are really just accesses \nto reference cefl e; we would like to inline-expand get, but e is not exported presumably because we \ndid not want it to be mutated other than via check. But after typ~checking, the compiler should be allowed \nto inline the function. The transformation presented here achieves this effect even though e is originally \nnot exported and everything is parameterized and therefore hidden within the body of extremeFun. J-splitting \ntakes one compilation unit and re\u00adwrites it as two parts. The first part contains those sections of the \ncode that cannot or should not be inlined, while the other will be made available for being inlineexpanded \ninto client modules. It is common for a compilation unit to have more than one client, which means that \nthe inlinable portion of that unit will be duplicated. For correctness it is necessary to avoid the duplication \nof side effects, and to prevent exces\u00adsive growth in total code size we must keep the irdinable portion \nsmall. Appendix C shows the measure that we use to estimate code size. Of course, at compile time the \npresence of side effects cannot be decided precisely, so we rely on a safe approxi\u00admation that can easily \nbe characterized syntactically. It is very similar to the notion of expansiveness. Accordingly, we call \nthe first part of a split the expansive portion-even though in general it will also contain a lot of \nnon-expansive codeand use the subscript e as a label; code examples are marked with the letter E. The \nsecond, inlinable part of a split marked with sub\u00adscript i or a letter I-contains no expansive code, \nonly values (variables, constants, J-abstractions) and other effect-free code (records, immutable vectors, \ndatatype constructors). It will be compiled together with its client code. A formal definition of what \nwe consider permitted to be inlined is given in appendix B. In effect, A-splitting redraws the lines \nbetween compilation units and lets an existing local opti\u00admizer look beyond the boundaries of the original \ncompilation units. In our example, assuming the size of extremeFun to ex\u00adceed the limit that we impose \non inlinable code, the trrms\u00adformation will divide it into extremeE and extreme I: fun extremeE (xO, \ncheckOFun) = let val e=ref XO val checkO = checkOFun e fun check [1 = () I check (h :: t) = (checkO h; \ncheck t) in (e, check) end The expansive portion extremeE is compiled to machine code; the inlineable \nportion extremeI is kept in symbolic form and will be copied verbatim into the client s code: local [ \nfun extremeI (it, xO) = let fun checkOFun e = let fun checkO x = if lt (!e, x) then e :=x else ()  in \ncheckO end val (e, check) = extremeE (xO, checkOFun) funget () =!e in (get, check, lt) end -end The \nintramodule inliner reduces this client into: val (e, check) = extremeE (10000, fn e =>fn (x: int) => \nif!e<xthene:=x else ()) funget () =!e val lt = (op <): int * int -> bool ... Thus, A-splitting achieves \nthe following effects: 1. extremeE exports reference cell e. 2. The compiler can now rewrite invocations \nof get in the client aa accesses to e. 3. Information about the fact that lt is the same as op < has \nnot been lost on its way through the body of extremeFun. 4. The function checkO is lifted out of extremeE \nand put into extremeI so that the actual comparison c can be inlined.  Our example shows what can be \ngained by a higher-order inlining algorithm. Previous approaches would not irdine get, check, and lt \nbecause they do not appear at top level. Functions such as extremeFun in our example really play the \nrole of parameterized modules. In ML terminology these are called ~vnctors, and there is special syntax \n(which we functor F -= :) =1 t functor E (t~e t ) Figure 1: Functor splitting. We propagate type t through \nthe functor into the client by copying FI into each client and fhxhrcing it. But F contains an expansive \nportion (ovals) that cannot be copied; this is abstracted as FE. F, and FE, may be very large, but only \nthe inlinable portion FI, which our algorithm keeps small, gets copied into clients. didn t use here \nto avoid complicating the presentation) to\u00adgether with ramifications for the static semantics of the \nlan\u00adguage. Standard ML provides only first-order functors, but this has been taken a step further in \nSML/NJ by offering a higher-order module system [Mac90, MT94]. Functors can be defined within funct ors, \nthey can be passed as functor arguments, and they may be returned as part of functor instantiations. \nTherefore, in the intermediate representa\u00adtion there really is not much of a difference between ordi\u00adnary \nfunctions and functors, because they are represented the same. Functions are non-expansive themselves, \nso one could without sacrificing correctnem-place them into the inlin\u00adable portion as a whole, thereby \neffectively delaying the compilation of parameterized modules until they are instan\u00adtiated. This would \nmammize the compiler s chances of gen\u00aderating optimized code taylored to each individual instan\u00adtiation, \nbut it also duplicates all of the functor s body ev\u00adery time it is usec-potentially leading to intolerable \nin\u00adcreases in size. But large functors sometimes have a small core that should be delayed and inlined \nfor efficiency, while the remainder can be compiled where it is declared. A-splitting is able to accomplish \nthat (almos see the end of section 5.2). 3 Separate compilation and linking The meaning of any compilation \nunit can be understood as a function that maps meanings for its imports to meanings for its exports. \nThis allows us to view cross-modulel inlining as a problem of manipulating such functions. Imports correspond \nto the free identifiers in the source program; new definitions that other compilation units can refer \nto are considered exports. We can assign meaning to a system of several compilation units by solving \nthe corre\u00adsponding system of equations, which describe the import\u00adexport relations. In languages that \npermit circular depen\u00addencies among modules< this will in general involve calcu\u00adlating a fixed point; \nwithout circularities it becomes much simpler. lIn this paper we use the term module to always refer \nto compila\u00adtion units, not to elements of the SML module language. Standard liL has no circular dependencies \namong com\u00adpilation units. Separate compilation and linking can be modeled using nonrecursive higher-order \nfunctions. Type checking already impeses an ordering on compilation steps, guaranteeing that whenenver \na module B refers to some\u00adthing exported from module A, then A must be compiled before B, thereby ensuring \nthat inlining information from A will be available at the time when B is compiled. Appel and MacQueen \n[AM94] describe how Standard ML of New Jersey implements separate compilation. Since this is the basis \nfor our approach we will summarize it here: A compilation unit consists of a number of definitions for \ntypes, values, signatures, structures, and functors. Types and signatures are relevant only for dealing \nwith static se\u00admantics and can be ignored as far as linking, a dynamic concept, is concerned. The SML/NJ \nintermediate represen\u00adtation (A-1anguage [App92] ) represents structures as records, and functors as \nfunctions, and does not distinguish between the core and module languages. Each compilation unit is turned \ninto a J-expression, where references to identifiers from other compilation units appear as free variables. \nBy abstracting over those variables we get a closed expression, which can now be compiled to machine \ncode without further need to refer to any kind of context information. Thus, a compilation unit becomes \na function taking im\u00adported values to exported values. Exported values are col\u00adlected in a record which \nis the result of exemrting the unit. Another unit, which imports those values, simply takes that record \nas a function argument, For example, a unit consisting of definition vala=landb=2 would be represented \nas fun A () = (1, 2). The compiler remembers that a sits in the first position and b in the sec\u00adond. \nSimilarly, a unit consisting of the definitions valc=3andd=4 becomes fun E () = (3, 4). Now, if we compile \na third unit containing vale =a*b+ c-d and f=a+c the compiler will construct a A-expreeaion of the form \nfunc [A,B]= let val (a, b) =A val (c, d) =B ln(a*b+c-d, a+c) end 4 A-splitting For simplicity let us \nignore the issue of importing from mul\u00adtiple sources, so we need not deaf with vectors of imports. The \ncode for a unit U is a closed function mapping imports to exports: U:I ~ E. The objective of A-splitting \nis to choose a triplet (T, Ui:T ~ E, U. :I ~ T), such that the composition U~o U, yields U again, Here \nT denotes the domain of possible values that are passed from U, to U~ and corresponds to a type if the \nintermediate language is typed. We call the triplet a 1?-decomposition* or simply a split. For example, \na trivial split for any U : 1 ~ E is 2B is the composition combinator A~AgAz. f(gz), (E, id~, [J), where \nid.z : 1; ~ E is the identity f~ln~tion 011 E. Now consider two units, where the second imports the exports \nof the first: Ul : 11 ~ El and U2 : 12 ~ E2, El = 12. The meaning of the program comprised of Ul and \nU2 can be understood in terms of their composition U* oUl. For any split (T, U:, Ul thisis the same as \nU2o(U: oU:), i and by associativity (U cU;)oU$. In other words, we can compile U: as one unit, U2oU; \nas the other, and still obtain the correct result. Cross-module inlining can therefore be achieved by \ncleverly placing the things to be inlined by U2 into U,~. In general, U1 will be referred to by several \nother units, and we must be careful with what goes into U:, because this code will be duplicated in each \none. Therefore, our al\u00adgorithm never places expansive code or code that is deemed too big into the i-portion \nof a split. The idea of splitting functions into pieces can also be used to facilitate intramoduler optimizations \nlike call for\u00adwarding [BDGK94] or partial inlining [Gou94]. 4.1 The basic algorithm In SML/NJ a compilation \nunit translates into a sequence of nested variable bindings (let-bindings), some of which can be recursive. \nThe rightmost body of the rightmost let\u00adexpression then builds a record of exported values. The entire \nconstruction is finally wrapped into a J-abstraction together with some code for selecting values for \nthe source code s free variables from the argument. Eventually the code corresponding to any compilation \nunit U will be of the following general form, where the set {w,,... ,wk} is a subset of {vi,...,v~}: \nfn vO=> let val VI=Ein let val V2=E2 in ... let val vn =En in (wI, w2, .... wk) end ... end end To construct \nthe code for U. and U, we select all first j such that the bindings val vj = Ej are guaranteed to be \neffect-free. This will ensure that no side effects will be dupli\u00adcated by our algorithm. A formal definition \nof what we con\u00adsider guaranteed to be effect-free is given in appendix B. In most cases the set of bindings \nchosen in this step will be rather large, so we employ a combination of several heuris\u00adtics to further \nreduce it. The remaining bindings are then used to build Ui s body, and finally we need to find T and \nU. to complete the split (T, U,:T ~ E, b . :I ~ T). The elements of T must com\u00admunicate values for all \nthe free variables of U, s body. These free variables will always be a subset of {IJO,w,. . . . Wn}, \nso they are readily available at the time when the original code for U would have constructed its export \nrecord. llmther\u00admore, since only non-expansive code has been placed into U1 there will be no harm in \nduplicating some or even all of it in U.. Therefore, the task of constructing U, becomes surprisingly \nsimple: We can use U almost unchanged; only the export record has to be replaced with a record holding \nsaid free variables. of course, some of the bindings in U, may become unnecessary. But the intramodular \noptimizer will delete dead bindings. 4.2 A-normal form To break large expressions into inlinable pieces \nwe use a variant of A-normal form [FSDF93] as our calculus for X splitting. The property we are interested \nin is that ev\u00adery intermediate result will be explicitly bound to a vari\u00adable, because this increases \nthe number of bindings while on average reducing the size of the expressions being bound. Thus, splitting \ndecisions can be made with finer granularity. The structure of the intermediate language described in \nap\u00adpendix A automatically enforces this. Continuation-passing style [Ste78, App92] would do as well; \nbut the SML/NJ com\u00adpiler transforms to CPS at a late stage after type informa\u00adtion is discarded. We would \nrather preserve the ability to do monomorphic instantiation of polymorphic functions [SA95] ajler cross-module \ninlining has been done. As an example of how A-normaf form helps inlining, con\u00adsider that the expression \nbound to x in let val x= (1, ref O) valy=2 in (x, y) end is expansive, because ref O is expansive. But \nin A-normal form we have: let val tmp=ref O val x = (1, tmp) valy=2 in (x, y) end In this program x is \nbound to a non-expansive expression, which can very well be moved into the i-section of the split, thus \nexposing the 1 in the first field of the tuple. Thus, we can move more code into U,. 4.3 A-contract \nWe found it useful to run a simple optimizer A-contract over the code before attempting to split it. \nBy un-nesting let-bindings and performing other code rearrangements it brings the code into the general \nform expected by the split\u00adting algorithm. It also performs value propagation, @con\u00adtractions, and some \ndead code elimination. After the code has been straightened out we can rely on simple syntactic cues \nwhen looking for variable bindings to be exported in symbolic form as part of the inlinable portion of \nthe split. For example, if the right-hand side of a variable binding is another binding form itself, \nthen the order of the two can be exchanged. This technique is knows as let-floating [JPS96]: let val \nb= B let val a= in let val a=Alet val b= B in Cend in Aend end in Cend 9 lb B a .. bB A A c a c No \na-conversion is necessary here because variable names are guaranteed to be distinct. When abstraction \nand application are not adjacent, a /r\u00adeduction can make the let-binding apparent: let valf= fnv.>B let \nval v = A in f Aend in B end ,f v V A I BB f*A Such transformations move more bindings to top level \n, which improves the performance of A-splitting. It seems unfortunate that J-contract duplicates some \nop\u00adtimization that are afso performed by SML/NJ s CPS opti\u00admizer. We run the A-splitter early to be able \nto take advan\u00adtage of type information that is not present in later stages of the current SML/NJ compiler. \nPerhaps, in compilers where the intermediate language is typed at all times [TMC+96] A\u00adsplitting can \nbe moved to such a later stage, and the need for J-contract and its associated redundancy can be eliminated. \n 4.4 Call counting One major soure of inefficiencies related to separate compi\u00adlation is the need for \na generic function call protocol, which has to be used whenever the compiler is unable to consis\u00adtently \nidentify all call sites of a given function or all functions callable at a given calf site. In particular, \nthis is always the case when the function is defined in one compilation unit and used in another. Cross-module \ninlining as proposed here is an attempt to improve the situation by moving function definitions into \nthe modules where they are used or vice versa. But unless we abandon the idea of separate compilation \ncompletely there will always be some functions that are not defined where they are called. Therefore, \nthere is a danger that croes\u00admodule inlining will make things worse than they were be\u00adfore! To illustrate \nthis, let us consider the following scenario: Compilation unit B contains a call to a function f; f isde\u00adfined \nin module A calls gas well as h, which are also defined in A. (* module A *) frm go=... fum ho=... fumfo= \n(... go . ..ho . ..) (* module B *) .... f () ... If for efficiency we move only f s definition into \nmodule 1?, then for every call to the original f that had to use the generic, non-optimized protocol \nwe now have two such calls to gand h: (. Module A .) fun go=... fun ho=... (. Module B *) .... (... \ngo ...ho ...) ... To avoid this effect we would like to keep track of the number of runtime function \ncalls that use the non-optimized protocol for the reason that caller and callee are separated by module \nboundaries. This quantity cannot be calculated precisely, but we can use a static estimator N(. . .), \nwhich is able to identify many of the bad cases [B1u97]. Moreover, inherent imprecision do not have any \nbearing on correctness of the overall afgorithm. 4.5 B-decomposition algorithm Some bindings need not \nb appear in U, because the variables they bind are not referenced. For a binding to be useful in Ui it \nmust be reachable in the directed use-definition graph, which contains an edge going from every use of \na variable to its definition, and where the root node corresponds to the expression that constructs the \nexport record. The use-clef graph for extremeFun is .-.. > get _lt (get, check, It) ----> check - -_ \n~hecko Xo - --\u00ad where there is no node for e because ref XO is expansive. To avoid code blowup, we will \nput into Ui only a subset of the reachable portion of the graph, Our algorithm for constructing U, proceeds \nas follows: 1. Calculate the initiaf set A of bindings that are avail\u00adable for inlining: all non-expansive \nbindings to non\u00adfunctions, all bindings to smalf functions, and bindings derived from recursively splitting \nlarge nonrecursive functions. Small is defined by a tunable parameter to the algorithm. Code size is \nestimated using the mea\u00adsure S(. ..) defined in appendix C; non-expansiveness is judged using the predicate \nP(. .) from appendix B. This part of the algorithm is the Available function of figure 2.  2. Construct \nthe use-definition graph for this set (as in UseDef of figure 2). 3. Perform a breadth-first search \non the graph starting at the root (the export record fl. Within each level of the graph, search is biased \nto favor smaller expressions. It stops when either the entire graph has been traversed or the totaf size \nof nodes searched exceeds a pre-set threshold. This step automatically eliminates all dead definitions. \nThe greedy approach favors variables that are used by shorter use-clef chains from the root node; we \ncall this a distance hew--istzc. It also provides a cap for the total size of Ui, thereby preventing \nnonlin\u00adear increases in total code size. (See the ToBeInlined function of figure 2.) 4. From the space \ntraversed by the greedy breadth-first search we pick the set of bindings for U~ that has the smallest \ncall count Af(. . .). 5. The bindings in the set picked for constructing the body of U~ are used in \nthe same order that they ap\u00adpeared in within U s code. This ensures correct scop\u00ading for bound variablee. \n 6. Any remaining free variables wifl not be inlined; they will be calculated inside Ue and their values \nwill be passed at link-time (as function arguments) to the client module (see B-decompose in figure 2). \n B-decompose(w = A(VI, . . . . v~). let bindings inO = (E, A) -Available(bindings) 1 t ToBeInlined(A, \nF ) Iord + the ordered sublist of E whose elements are C I (z,,... , ~fc) t ~(let ~ord in fl return ( \nw, = A(vl. . . . ,v~).let E in (xl, . . .,z~), Wi = A(zl, . . ..z~letet 1 r in F, W= A(V; , . . ,~~) \nw,(we?l, ,v~)) )  S-decompose(tu = A(v1, . . . W). let bindings in F )= (E,A) t Available(bindings) \n1 + ToBeInlined(A, 3 lord -the ordered subli.st of E whose elements are E I (z,,,,., Zk) -~(let lord \nin fi\\{~l, . . ..~n} return ( we = A(vl ,.. .,vnletet Ein (zl, . . ..z~). W,= A(7Jl, . . ..~n A(Zl.l, \n. . . ,Zk ) let ~ord in P, W= A(W; ,.. . ,un ) . Wi(v;,. . . ,u~)(we(~i,. .,~~)) ) Available(bl,.. ., \nb~) = Et bl,... ,b~ foreachb~E ifS(b) <K A+ Au{b} else (b., b~,b ) + S-decompose(b) In the list E, replaxe \nb by be, bi, b A -Au{ bi, b } else if P(b) A+AU{b} return (E, A) UseDef(b) = The set of bindings v \n= M such that v 6 F(b) ToBeInlined(A, N = Q-{~ Q +{j while Q # {} A ~beB s(b) < K let b be the element \nof Q with smallest S(b) A+ A\\{b}  Q+ Q\\{b} B* Bu{b} Q -Q U (UseDef(b) U A) if Af(B) < Af(I) I-B if \nQ={} Q+Q return IQ + { } Figure 2: A-splitting algorithms F is the expression that constructs the export \nrecord and serves as the starting point for the breadth\u00adfirst search in function ToBeInlined. E is the \nordered list of bindings ot be kept in U.; the in\u00adtrarnodular optimizer may later delete dead bind\u00adings, \nbut this is not part of the splitting algorithm. A is the set of bindings available for h-dining. B is \nthe current candidate for the inlinable part Ui. I is the best candidate seen so far. Q, Q are the two \nparts of the breadth-first search queue. The separation enables us to implement a bias in favor of smaller \nbindings within each level of the breadth-first search tree. K is the maximum size of any function body \nthat can be inlined without being recursively split. K is the maximum size of the irdineable code Ua. \nF computes the free variables of an expression (see ap\u00adpendix app:lang). P tells whether an expression \nis nonexpansive (see ap pendix B). S tells the size of an expression (see appendix C). ~ approximates \nhow many out-of-module calls a set of bindings makes. Figure 3: Legend to the algorithm in figure 2 \n 5 Functions Abstractions are syntactic values. so large functions will be large even in A-normal form. \nInlining small functions is part of the overall goal, but big functions pose a problem. While it is possible \nto move the entire code of any function into Ui without sacrificing correctness, we cannot do this indiscrim\u00adinately \nbecause that would severely inflate the code. On the other hand, when it is not feasible to move the \nentire func\u00adtion, then we would like to make some of its parts available for inlining (see figure I), \nAfter all, functors are encoded as functions, and highly functorized code could hardly ben\u00adefit at all \nfrom cross-module inlining if we do not solve this problem. As we will explain, the A-splitting algorithm \ncan be applied recursively to function bodies, 5.1 Recursive functions Our algorithm does not attempt \nto take recursion apart; it either moves entire clusters of mutually recursive func\u00adtions into Ui when \nthe cluster as a whole is deemed small enough, or it leaves them completely alone. In order to be able \nto do this at as fine a granularity as possible we cal\u00adculate strongly connected components of the use-definition \ngraph for every recursive let. Each component is then con\u00adsidered separately. In most cases, a strongly \nconnected component of func\u00adtions represents a genuine loop or similar repetitive compu\u00adtation. This \nshould not be spread across multiple compi\u00adlation units, because the overhead of cross-module function \ncalls would be amplified considerably,  5.2 Functors and nonrecursive functions B-decompceition works \nbecause compilation units are rep\u00adresented as functions. one could think of these compiler\u00adconstructed \nfunctions as implicit functors. From this obser\u00advation we derive our idea for dealing with actual functors \nand other large functions-we split them as well. The re\u00adsulting expansive and inlinable parts will be \nplaced into the corresponding expansive and inlinable portions of the sur\u00adrounding large function or \ncompilation unit. For correctness we had to be careful that our splitting algorithm will never place \nexpansive code into the inlinable portion of a compilation unit. When splitting functions this restriction \ndoesn t exist, because side effects can only occur at the time the function is called. Therefore, side \neffects will be duplicated only if the function in question is explicitly invoked multiple times, which \nthen makes this the correct behavior as mandated b~, the language definition. Nevertheless, it is convenient \nto maintain the invariant that the inlinable portion of any split is nonexpansive, It frees us from having \nto pay attention to the ordering of effects within a function. Also, it guarantees that invoca\u00adtions \nof any inlinable portions themselves will be free of effects. Thus, it becomes possible to relax our \nnotion of non-expansiveness (normally, function invocation is always considered expansive), so we can \nmove such invocations into the inlinable portion of the enclosing function (appendix B). Recursive decomposition \nAs a first attempt, the same technique-13-decomposition that is used to split the functions representing \nentire compi\u00ad lation units can also be utilized when splitting functions too large to be placed into \nthe inlinable portion. This idea re\u00ad cursively extends to large functions encountered within large functions \nand so forth, In our introductory example, if we assume that checkO and check are too big, then the Yunct \nor extremeFun would be split like this: fun extremeE (lt, xO) = let val e =ref XO fun get () =!e (* dead \n*) fun checkO x = if lt (!e, x) then e :=x else () fun check [] = () I check (h :: t) = (checkO h; check \nt) in (e, check, lt) end fun extreme I (e, check, lt) = let funget () =!e in (get, check, lt) end fun \nextremeFun arge = extremeI (extremeE arge) Notice that extremeFun is the B-composition of extremeI and \nextremeE. Definitions for extremeI as well as therecon\u00adstructed extremeFun will reexported as part of \nUi, allowing the client to inline calls to get and rewrite them as accesses to reference cell e. S-decomposition \nFunctor splitting should provide three benefits: 1 Parts of the functor s body become part of Ui and \ncan therefore be inlined into client code, 2 Existing simple connections between argument and re\u00adsult \nremain visible in U~. Since the functor argument is supplied where the functor is instantiated, this \nwill enable client code to inline parts of a functor s result if they correspond to inlinable parts of \nthe argument. 3. Part or all of the functor s argument are inlined into the functor s body. Unfortunately, \nonly the first point is addressed by B-decom\u00adposition. This seems surprising at first, because it works \nso nicely at the level of compilation units. What is it that dis\u00adtinguishes explicit functors from the \nones the com iler con\u00ad ? structs implicitly? In SML, if a compilation unit U refers to U1, then for reasons \nof type checking Ul must be compiled before U*. This means that the code for constructing U2 s input \nis already available when U2 is compiled. In other words, we really compile U2 oU: and not U2, With functors \nthe situation is different. Functors are compiled separately from the code that constructs their ar\u00adguments. \nBut we want to propagate actuaf functor argu\u00adments to functor bodies, so they can be inline-expanded \nthere; and functor arguments into functor results (for clients to use). B-decomposition propagates code \nonly from functor bodies to functor results. A variation on B-decompmition we call it Sdecomposi\u00adtion \naddresses points 1 and 2 above. The functor argument is known at the time when extreme I is invoked, \nso instead of first funneling it all the way through extrerneE vw can simply pass it directly to extreme \nI. Therefore, the S-decomposition for F: I~Eis such that F = SFiFe. S denotes the stronger version of \ncomposition, which distributes the argument to both func\u00adtions: Sjgx = ~z(gz). Under S-decomposition \nour example program turns into the following code, where Fi (here: extreme I) establishes an explicit \nlink between the functor s argument and lt: fun extremeE (lt, xO) = let val e =ref XO fun get() !e(*dead*) \nfun checkO x = if lt (!e, x) then e :=x else () fun check [] = () I check (h :: t) = (checkO h; check \nt) in (e, check) end fun extremeI ((it, xO), (e, check)) = let frmget () =!e in (get, check, lt) end \nfun extremeFun args = extremeI (args, extremeE args) Differences between our algorithms for 13-and S-decom\u00adposition \nare minor. S-decomposition can use exactly the same methodology, in fact. share most of the implementa\u00adtion \nof B-decomposition as well. There is one small twist: At the point where the free variables of F~ are \ncollected we explicitly exclude the original formal argument of F, be\u00adcause it will be passed to Fa directly \n(see the S-decompose function of figure 2). l-decomposition Actual functor arguments can be inline-expanded \nin the functor body only if they are used in the inlinable portion of the functor split, because only \nthen will both definition and use of the argument be available to the compiler at the same time. In our \nexample even S-decomposition does not promote the definition of checkO into extreme I. Therefore, the \ncomparison predicate will not be irdined there. The only mention of checkO is in check, which itself \nis not in extreme I. Still, checkO is bound to a syntactic value, so A-splitting could factor it out \nand move it to extreme I, from which it will be passed as an argument to extremeE. The resulting code \nwas shown in section 2. When splitting a functor F we can generally leave it up to Fi how F, is invoked. \nEven though F, is compiled some\u00adwhere else and will be treated as a black box, its interface can still \nbe choeen arbitrarily. This can be modeled by pass\u00ading F, itself, not its result, to Fi. Thus, a ~-decomposition \nof F:I ~ E is a triplet (T, Fi:T+I+E, F.:T) such that F = ~F~F, = F, F..3 Normally T will itself be \nof the form T1+ T2 for some T1 and Tz. 3~= ~f.~z.(.fm) is the Church-numeral 1. The equality F = F,Fe \nis used to recover F from its pieces: any occurence of F in the client can be replaced with F, F,. But \nthe code of F* will then be available, so we can immediately ~-reduce F~F, and work with the result instead \nof F, itself. We already did this when we presented the introductory example, which explains why extremeE \nis not being passed to extreme I as an argument but rather gets invoked directly. Both B-and S-decompositions \ncan be viewed se special cases of the more general approach of ~-decomposition. In particular, for any \nX-decomposition (T, Fi, Fe) of F :I + E (where X is either B or S) the triplet (1 --+ T, XFi, Fe) forms \nsuch a corresponding ~-decomposit ion. We do not show an algorithm for ~-decomposition, be\u00adcause we have \nnot found a good set of heuristics to separate the core of a function (or functor) from the non-inlineable \nbody (as illustrated in figure 1). Therefore, our implemen\u00adtation cannot quite transform extremeFun as \nshown in sec\u00adtion 2, but can transform it via B-and S-decomposition as shown in section 5.2. 6 Implementation \nand results We implemented J-splitting (B-decomposition for compila\u00adtion units and recursive S-decomposition \nfor functions) in SML/NJ. For the general framework only very small modi\u00adfications to existing code in \nthe compiler were necessary. In particular, no changes had to be made to either the front\u00adend (including \nsymbol table management ), the optimizer, or the various architecture-specific back-ends. A-splitting \nfits neatly between existing compiler phases. We changed the format of object files to accommodate symbolic \nA-code and modified SML/NJ s compilation man\u00adager [B1u95]. Dependency analysis now has to take into account \nthat compilation units are connected via static en\u00advironments (i.e., symbol tables) and inlinable code. \nThis does not change the shape of the dependency graph, but it has an impact on how modifications to \none unit propagate to other, dependent units. Many widely used SML benchmarks cannot be used to mewure \ncross-module inlining because they contain only a single compilation unit. Therefore, we applied J-splitting \nto SML/NJ s compiler itself and used several of its phases as our benchmarks. The compiler is a 100,000 \nline SML pro\u00adgram comprised of more than 600 individual compilation units (source files). With a successful \ntest of this magni\u00adtude we are confident of the correctness and viability of the implemental ion. 6.1 \nProblems with A-contract In our prototype the introduction of A-contract had one dis\u00adturbing effect: \nIn some cases execution slows down (Fig\u00adure 4). The numbers indicate that whether or not A-contract is \nadvantageous on its own depends on architecture and benchmark. In our case the overall result is dominated \nby the elaborate benchmark, which improves on the Pentium but slows down on the Alpha. To explain this \nwe note that ~-contractions can increase the size of certain closures, and if such closures are con\u00adstructed \nrepeatedly-for example in a loop-then execution time may go up: . ... ............. [ DEC Alpha 21064 \nIntel Pentium Figure 4: A-contract on Alpha and Pentium. Individual compiler passes of SML/NJ light bars \nin the graph~rve as benchmarks for our study. With the exception of the dark bar on the right, which \nshows overall performance of the whole compiler, width was chosen proportional to the time spent in each \nphase. The heighta of the bars indicate speedup or slowdown caused by J-contract (shorter is better). \nJ-contract is not the focus of this work; it is an auxiliary transformation that allows our splitting \nalgorithm to find many more opportunitiea for cross-module inlining. The numbers here show that depending \non architecture and benchmark A-contract has sometimes positive, sometimes negative effects on running \ntime, The icontmct and lsplit bars graphically illustrate that the algorithms presented in this paper \ndo not make up a large portion of compilation time. ............ .........---............... 1 ,.. .. \nIs............... ~~~~z~.z~gg i= g~ ~ A &#38;A4L300mszooa&#38;t) a~ g+yy~a%mg g 5 !l~ ,~,lzQ:3mm-w \n~c.u if * < a$~&#38;@--&#38;&#38; 3:5 s ~ggg ~ $ -s   g =wi!$ z DEC Alpha 21064 Intel Pentium Figure \n5: Improvement due to cross-module inlining. A-splitting yields improvement on almost all benchmarks, \nand improves total execution time by 8% on the Alpha, 5% on the Pentium. The baseline (dotted line at \n1.0) shows a compiler with A-contract but without cross-module irdining; the grey bars show runtime (smaller \nis better) with both A-contract and inlining. 120 let val f= fn x => g (a,b,c,d,e,x) in let fun loop \n() = (. ifny=>f (z,y)) . . . loop ()) in loop () end end In this example, let us replace the variable \nf in (f (b, y)) with its definition: let fun loop )= (... (fn y=> g (a, b,c, d,e, (z, y))) . . .loop \n() in loop () end Theanonymous functioninside loop nowhaasevenfree vari\u00ad ables (g,a,b,c,d,e,z), while \nbefore it had only two (f and z). We hope to improve the situation with a better clmure\u00adconversion algorithm, \nmuch later in the compiler after A\u00adcontract, A-splitting, and intramodular optimization. Even without \n~-reductions and with let-floating [JPS96] alone we sometimes found performance to be slightly di\u00adminished. \nReordering let-bindings in itself does not change the size of any closure, it only moves bound variables \ncloser to the expressions they are bound to. However, this may enable later phasea of the compiler s \nCPS optimizer to rec\u00adognize and reduce more (3-redexes, which again may cause some clwmres to become \nbigger. In order to obtain a better idea of how much cross\u00admoduie irdining itself affects overall performance, \nwe will use SML/NJ with A-contract enabled as the baseline for our comparisons. This compiler already \ndoes aggressive in\u00adtramodular inlining [App92]; we measure only the increased performance from irdining \nacross module boundaries and from pulling inlinable code out of higher-order functions. 6.2 Timing results \nfrom A-splitting Figure 5 shows timing results for running the benchmarks with A-splitting enabled. The \nnumbers reported correspond to compiler passes that took at least one second of user time. The net speedup \nof 8 ?ZOfor the Alpha is reduced to only 4% if we take the effects of J-contract into account. On the \nPentium we find a somewhat different picture. A\u00adcontract seems to have a positive net impact here, although \nsome phases of the compiler also suffer a significant slow\u00addown. Overall speedup due to cross-module \ninlining is at 6% with and 5% without counting effects fkom A-contract. We believe our results are not \ndue to random variations in cache-conflict performance. llmctional programs with properly tuned garbage \ncollectors are not subject to many data-cache conflicts [Rei94, GA95], and informal inspection of the \nresults did not reveal the kind of instruction-cache conflict variability [App92. p. 194] to which SML/NJ \nwas subject on previous-generation architectures. I sl 1 0 DEC Alpha 21064 Figure 6: Inlining applied \nto a cleanly modularized pro\u00adgram. We reinserted signature constraints and abstractions that had been \nremoved by the efficiency-conscious software engineers. t!hat ?i-spfi%~$%$ ers essentially all of the \nperformance cost of modular abstraction. %&#38;X5ul$%%li3R~ ~!&#38;Y~&#38;~hwO sow d h ve 6.3 SML/NJ \nwithout ad-hoc inlining Cross-module inlining is so crucial for performance that the implementors of \nSML/NJ decided to compromise abstrac\u00adtion and modularity in some places, because for a selection of the \nmost important operations inline-expaneion is abso\u00adlutely necessary. To achieve a limited form of cross-module \ninlining the compiler treated certain variable bindings specially: If the right-hand side was known to \nbe in a pre-selected set of primitive operators, then this fact was recorded as part of the static information \navailable for the variable bound on the left-hand side. In effect, inlining information was treated in \na fashion similar to types. Unfortunately, the mechanism only applied to a fixed set of pm-selected values, \nand it also was not robust with respect to simple source code modifications. For example, it was dis\u00adabled \nby simply adding a few semantics-preserving signature constraintsaomething the unsuspecting user might \ndo by accident. Even so, figure 5 shows that A-splitting plus ad-hoc in\u00adlining is better than ad-hoc \ninlining alone. As an experiment we changed the compiler sources to eliminate old-style ad-hoc inlining. \nThe results (Figure 6) indicate that a fully automatic technique like J-splitting can provide benefits \nclose to those of previous ad-hoc ap\u00adproaches, while at the same time being more robust and less intrusive. \nComparing raw numbers shows that A-splitting still fares a little worse than the special-case solution \n(about 5?70). However, the contest is not entirely fair. The ad-hoc ap\u00adproach implemented in SML/NJ not \nonly propagates prim\u00aditive operators from compilation unit to compilation unit, it also forces the intrumodular \noptimizer to inline-expand every single use. In effect, it overrides the decisions the optimizer would \nhave made on its own. A-splitting doea not do that. Instead, it relies entirely on the existing opti\u00ad \nmizer s heuristics Ill OU] experiment all the operators han\u00addled by the old approach ~vere also propagated \nby the ne~v A-splitting. The discrepancy in performance can therefore be explained by shortcomings in \nintramodular optimization technology.  6.4 Compile-time cost of inlining In our preliminary implementation, \nthe A-contract phase takes 4.5% of total execution time, and A-split takes 1.590. In figure 5 these phases \nare labeled lcontract and lsplit, respectively. In addition, the intramodule CPS-optimization phase slows \ndown because it has more (useful!) work to do. Compile time increases by 7% overall, code size goes up \nby about Syo. Our implementation of J-contract could undoubtedly be improved; but even so, the cost of \nour cross-module inliner seems entirely reasonable.  7 Conclusions A-calculus is a powerful language \nfor expressing programs and program transformation. It is no surprise that cross\u00admodule inlining, which \nis such a transformation, will benefit from being cast in this framework. Compilation units can be viewed \nas functions. The pro\u00adcess of linking applies them to imports, thus obtaining ex\u00adports. By splitting \nthe functions into expansive and inlin\u00adable parts we are able to regroup the code in such a way that \ncross-module inlining turns into the task of performing ordinary intramodular optimizations. The functions \nthat represent compilation units are split in such a way that alge\u00adbraic function composition (the 1? \ncombinator) can be used to recombine the parts. Higher-order constructions such as functions or SML S \nfunctors can be split recursively, thus allowing them to be inlined partially. This approach gives us \nthe benefits of in\u00adlining in the presence of parameterized modules and still prevents excessive code \ngrowth. For functors B-decomposition alone is relatively weak. It aflows client code to inline-expand \nonly parts of the func\u00adtor s body. S-decomposition is a simple variation, which enables client code also \nto inline-expand parts of the func\u00adtor s argument in the case that the functor re-exports them. Here \nthe originaf functor can be reconstructed by applying the S combinator to its parts. An even stronger \nversion, ~-decomposition, is a general\u00adization of the other two techniques. It can be used to inline \nthe actual functor argument into parts of the functor s body even if those parts are not exported themselves. \nWe plan to complete an implementation for this in our prototype soon. Our experiments show modest speedup \nfor several large benchmarks that had already been hand-optimized. If we start with cleanly structured \ncode, then gains are substan\u00adtial and competitive wit h previous ad-hoc approaches that resorted to sacrificing \nabstraction and modularity. 8 Future work We intend to find appropriate heuristics for ~-decompcsi\u00adtion. \nHIP closure-conversion algorithm, though already quite sophisticated [SA94], tnust be improved to undo \nharmful effects of A-contract (&#38;reductions and let-floating), Now that the penalties for using abstract \ndata types across module boundaries have been eliminated, we can clean up all the ad-hoc inlining of \narithmetic primitives that per\u00advade SML lhTJ. But we can do even more. Standard ML has always had the \nproblem that efficient implementations of datatypes break down at functor-parameter boundaries [App93]. \nBy representing concrete data types as abstract data types, so that constructors and pattern-matching \nrepresented using functions and function calls, we can solve this problem, Without inlining this would \nhave been totally impractical, but we expect inlining will eliminate any penalty in almost afl cases. \nA The intermediate language We use the following calculus as our intermediate represen\u00adtation. It is \nreminiscent of A-normal form, with the only difference being that we need not distinguish between ordi\u00adnary \nfunction application and tail calls. Variable bindings are established by fn, fix, and let. Each variable \nis bound at most once; variables occuring in closed functions are bound exactly once. We distinguish \nbetween pure and impure pri\u00admops; the former are guaranteed not to incur effects upon application. Even \nread effects such as accesses to reference variables are considered impure here. var -+Vo Ivl 1. const \n-+intlr edl c.. primop + pure pure + impure impure val + var var + const const + prirrlop  exp + val \nval -+ fn (var, ezp) ~ fix ((var, var, ezp) . . .. ezp) + let (var, eqrr,ezp) ~ app (val, val) ~ record \n(val ., .) ~ select (int, vaf)  + if (val, ezp, ezp)  Expressions only consisting of a variable access, \na con\u00adstant, or a primitive operator use the val clause; fn intre\u00adduces a J-abstraction. Recursive function \nbindings are es\u00adtablished using fix; an expression of the form fiX ((fl,vl,El),(f2,v2,E2 ),. . .,B) corresponds \nto the SML expression let fun fl VI =El fun f2 V2=E2 in Bend A let-expression locally binds a variable; \napp denotes the application of one value to another. Records are constructed using record and selected \nfrom using select. The if-clause specifies a conditional expression. 7V (. ..) and X(. ..) calculate \nthe sets Of fr( e variables for e.qj fun Fe x=... and wd, respectively: fun Fi (fe, x) = . . . fun Fx=Fi(Fe \nx,x) valm=Fa .Fv(var v) = {v} .?V(const c) = 0 fv(pure p) = 0 Y%(impure p) = 0 F(val z) = Fv(x) f(fn \n(v, e)) = Y(e) \\ {v} >(fix (1,b)) = (Ff(l) u F(b)) \\ q(l) f(let (v, e, b)) = F(e) U (F(b)\\ {v}) $(app \n(zl, z2)) = S/(w) u FV(X2) Hrecord (zI,.. .)) = u ~v(xi)  ,= 1,.,. ~(select (2, z)) = 3V(X) .F(if (z, \nt, e)) = 7v(Z) U ~(~) U ~(e) ~f((~l, vl, el),. ..) = U {Y(ei) \\ {vi}} ,=l,.. , 13f((~l, vl, el),.. .) \n= {fI,...} B Expansiveness 13xpressions can be moved into the inlinable portion of a split, if the predicate \n?(. .) ( permitted ) holds for them: P(val v) = T P(fn (v, e)) = T P(fix (1, e)) = P(e) P(let (v. e, \nb)) = P(e) A P(b) T(app (pure p,z)) = T P(app (impure p, z) = F P(app (var v, z)) = purefun (v) P(record \n1) = T P(select (i, z)) = T P(if (x. t, e)) = F The use of the auxiliary function purefun deserves some \nexplanation: Normally all function applications are deemed expansive and, thus, are not allowed to occur \nwithin the inlinable part. However, we sometimes can be sure that cer\u00adtain functions have no side effects, \nin which case there is no harm in moving a corresponding application. In particular, this happens when \nour algorithm splits functions recursively, because it always maintains the invariant that the inlinable \nportion of a split is effect-free. To illustrate this point consider a compilation unit that defines \na functor and then also instantiates it: frm Fx =... valm=Fa S-decomposition on F transforms this into: \nIt is now possible to export the definitions for Fi and F, but we still cannot also export m s definition, \nbecause its right\u00adhand side is an application. This is unfortunate, because m is constructed by applying \nFe as well as Fi, and Fi itself was constructed in such a way that its applications should be\u00adcome inlinable. \nIn other compilation units that instantiate F this will not pose a problem, because A-contract will turn \nsuch applications into explicit let-bindings. But within the same compilation unit J-contract has already \nfinished its work before F was ever split into Fe and Fi. We therefore explicitly keep track of such \ncases and @-reduce the calf to F on the fly: fun Fe x=... fun Fi(fe, x)=... fun Fx=Fi(Fe x,x) val tmp \n=Fe a val m= Fi (tmp, a) Since Fi refers to the inlinable portion of a split, we assert: purefun (Fi) \n= T, and m s definition can also be exported and inlined into other compilation units. Currently, purefun(. \n.) returns false in all other cases, It would be possible to further relax this by checking the bodies \nof other functions as well to see whether or not they are expansive. We have not done this, because in \nmany cases such function applications are already ~-reduced during the A-contract phase. C Size estimates \nWe use a simple, syntax-driven size estimate S(. .) to limit the amount of code that will be exported \nsymbolically to be inlined into other compilation units. This estimate is necessarily imprecise, because \nsubsequent optimization can radically alter the code. The values A, B,C, D, E,F, G, H,I, K, L as welf \nas the function Sp(. .) are adjustable parameters to the algorithm and are chosen to approximately reflect \nthe relative cost of implementing the respective language feature. Sv(var v) = A Sv(const c) = B Sv(pure \np) = c Sv(impure p) = c S(val z) = Sv(x) S(fn (v, e)) = S(e) + DIF(e) \\ {v} I+ E S(fix (l, b)) = Sf(l) \n+ D1.Ff(l) u Bf(l)l + S(b) + F S(let (v, e, b)) = S(e) + S(b)+ G S(app (pure p,z)) = Sp(p)+ Sv(z) S(app \n(impurep, x)) = %l(P) + sv(~) S(app(zl, zz)) = SV(ZI) +&#38;(z2) + H S(record(zl,. ..)) = 1+ ~ SV(Z, \n) L=l,  S(select (2, z)) = SV(Z) + K S(if(z, t,e)) = SV(Z) + S (t) + S(e) + L ..) = ~ S(e,) ~f((~l,m,el), \n := 1,.,, For Standard ML of New Jersey, we use the following coefficients: ABC DE FGHIKL 00021205114 \n and SP(p) is between 1 and 4, for most p. References [AM94] Andrew W. Appel and David B. MacQueen. \nSeparate compilation for Standard ML. In Proc. SIGPLAN 94 Symp. on Ping. Language Design and Implemen\u00adtation, \npagea 1.3 23. ACM Press, June 1994. [App92] Andrew W. Appel. Compiling with Continuations. Cambridge \nUniversity Press, Cambridge, England, 1992. [App93] Andrew W. Appel. A critique of Standard ML. J. Functional \nPmgmmm2ng, 3(4) :391 430, 1993. [BDGK94] Keen De Bosschere, Saumya Debray, David Gune\u00admarr~and Sampath \nKannan. Call forwarding: a sim\u00adple mterprocedural optimization technique for dy\u00adnamically typed languages. \nIn POPL 94: 21ST ACM SIGPLA N-SIGACT Symposium on Principles of Prwgmmmmg Languages, pages 409-420, 1994. \n[Blu95] Matthias Blume. Standard ML of New Jersey com\u00adpilation manager. Manual accompanying SML/NJ software, \n1995. [B1u97] Matthies Blume. Hiemmhical Modularity and Znter\u00admodular Optimization. PhD theais, Princeton \nUni\u00adversity, expected 1997. [CHT91] Keith D. Cooper, Mary W. Hall, and Linda Torczon. An experiment with \ninline substitution. Softwam Pmctice and ,@zperience, 21(6):581-601, June 1991. [CMCH92] Pohua P. Chang, \nScott A. Mahlke, William Y. Chen, and Wen-Mei W. Hwu. Profile-guided automatic in\u00adline expansion for \nc programs. Software-Pmctice and Experience, 22(5):349-369, May 1992. [DH88] Jack W. Davidson and Anne \nM. Holler. A study of a C function inliner. Softwam Pmctzce and Ezper-ience, 18(8):775 790, August 1988. \n[FSDF93] Cormac Flanagan, Amr Sabry, Bruce F. Dubs, and Matthiaa Felleisen. The Essence of Compiling \nwith Continuations. In 1993 Conference on Pmgmmming Language Design and Implementation., pages 21-25, \nJune 1993. [GA95] Marcelo J. R. Gon#ves and Andrew W. Appel. Cache performance of fast-allocating programs. \nIn Pmt. Seventh Int 1 Conf. on Functional Progmm\u00adming and Computer Architecture, pagee 293 305. ACM Press, \n1995. [Gou94] Jean Goubault. Generalized boxing, congruences and partial inlining. In Static Analysis \nSymposwm 94, number 864 in Lecture Notes in Computer Science, pages 147-161. Springer-Verlag, 1994. [HM95] \nRobert Harper and Greg Morrisett. Compiling poly\u00admorphism using intensional type analysis. In Twenty\u00adsecond \nAnnual ACM Symp. on Principles of Prog. Languagesl pages 13&#38;141, New York, Jan 1995. ACM Press. [JPS96] \n[JW96] [KKR+86] [Mac90] [MT94] [Oho92] [Pey87] [Rei94] [SA94] [SA95] [Sch77] [Ste78] [TMC+96] [T0194] \nSimon Peyton Jones. W ill Partain, and Andr6 San\u00adtos. Let-floating: moving bindings to give faster pre\u00adgrams. \nIn 1996 SIGPLAN International Conference on Functional Pmgmmmmg, pagea 3 12, May 1996, Suresh Jagannathan \nand Andrew Wright. Flow\u00ad directed Inlining. Proceedings of the ACM SIGPLAN 96 Conference on Pmgrummmg \nLanguage Design and Implementation, pages 193 205, May 1996, D. Kranz, R. Kelsey, J. Rees, P. Hudak, \nJ. Philbin, and N. Adams. ORBIT: An optimizing compiler for Scheme. SIGPLAN Notices (Prvc. Sigplan 86 \nSymp. on Compiler Construction), 21(7):21%33, July 1986, David B. MacQueen. A higher-order type system \nfor functional programming, In Resemrh Topics in Func\u00adtional Progmmming, pagee 35348, Reading, MA, 1990. \nAddison-Wesley. David B. MacQueen and Mads Tofte. A semantica for higher-order functors. In Proc. European \nSymposium on Progmmming (ESOP 94), pagea 409-423, April 1994. Atsushi Ohori. A compilation method for \nml-style polymorphic record calculi. In Nineteenth Annual ACM Symp. on Principles of Ping. Languages, \npages 154 165. ACM Press, Jan 1992, Simon L. Peyton Jon=. The Implementation of Func\u00adtional Progmmming \nLanguages. Prentic&#38;Hall, New York, 1987. Mark B. Reinhold. Cache performance of garbage collected \nprograms. In Proc. SIGPLAN 94 Symp. on Pr-og. Language Design and Implementation, pagea 206217. ACM Press, \nJune 1994. Zhong Shao and Andrew W. Appel. Spac&#38;efficient closure representations. In Pruc. 1994 \nACM Conj. on Lisp and Functional Programming, pagea 150-161, New York, 1994. ACM Press. Zhong Shao and \nAndrew W. Appel. A type-baaed compiler for Standard ML. In Proc 1995 ACM Conf. on Progmmming Language \nDesign and Implementa\u00adtion, pages 116 129. ACM Press, 1995. Robert W. Scheifler, An analysis of inline \nsubstitution for a structured programming language. Commrmi\u00ad cations of the ACM, 20(9):647-654, 1977. \n Guy L. Steele. Rabbit: a compiler for Scheme. Tech\u00adnical Report AI-TR-474, MIT, Cambridge, MA, 1978, \nD. Tarditi, G. Morrisett, P. Cheng, C. Stone, R. Harper, and P, Lee. TIL: A Typ&#38;Directed Op timizing \nCompiler for ML. In f 996 SIGPLAN Con\u00adference on Pr-ogmmmmg Language Design and Im\u00adplementation, 1996. \nAndrew Tolmach. Tag-free garbage collection using explicit type parameters. In Pmc, 199,/ ACM Conf. on \nLisp and 10.mctional Programming, pages 1 11, ACM Press, 1994. 124  \n\t\t\t", "proc_id": "258948", "abstract": "We describe an algorithm for automatic inline expansion across module boundaries that works in the presence of higher-order functions and free variables; it rearranges bindings and scopes as necessary to move nonexpansive code from one module to another. We describe---and implement---the algorithm as transformations on &amp;lambda;-calculus. Our inliner interacts well with separate compilation and is efficient, robust, and practical enough for everyday use in the SML/NJ compiler. Inlining improves performance by 4--8% on existing code, and makes it possible to use much more data abstraction by consistently eliminating penalties for modularity.", "authors": [{"name": "Matthias Blume", "author_profile_id": "81100215091", "affiliation": "Princeton University", "person_id": "PP43117671", "email_address": "", "orcid_id": ""}, {"name": "Andrew W. Appel", "author_profile_id": "81100498630", "affiliation": "Princeton University", "person_id": "PP14174176", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/258948.258960", "year": "1997", "article_id": "258960", "conference": "ICFP", "title": "Lambda-splitting: a higher-order approach to cross-module optimizations", "url": "http://dl.acm.org/citation.cfm?id=258960"}