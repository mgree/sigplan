{"article_publication_date": "08-01-1997", "fulltext": "\n Monadic State: Axiomatization and Type Safety John Launchbury Amr Sabry Oregon Graduate Institute Department \nof Computer Science P.O. Box 91000 University of Oregon Portland, OR 97291-1000 Eugene, OR 97403 jl@cse \n.ogi .edu sabry@cs. uoregon. edu Abstract Type safety of imperative programs is an area fraught with \ndifficulty and requiring great care. The SML solution to the problem, originally involving imperative \ntype variables, has been recently simplified to the syntactic-value restriction. In Haskell, the problem \nis addressed in a rather different way using explicit monadic state. We present an operational semantics \nfor state in Haskell and the first full proof of type safety. We demonstrate that the semantic notion \nof value provided by the explicit monadic types is able to avoid any problems with generalization. Introduction \nWhen Launchbury and Peyton Jones introduced encapsu\u00adlated monadic state [11, 12], it came equipped with \na de\u00adnotational semantics and a model-theoretic proof that dif\u00adferent state threads did not interact \nwith each other. The encapsulation operator runST had a type which statically guaranteed freedom of interaction, \nand the guarantee relied on a parametricity proof. What the paper failed to provide was any formal reasoning \nprinciple at the syntactic level, or any proof of type safety. This paper makes up for those shortcomings. \nIn particular we: . axiomatize the monudic-state opemtions: this allows us to view the monad of state \ntransformers as an ab\u00adstract type and understand formally how it should be\u00adhave. Previously, the choice \nwas between an informal understanding, or a rather heavy-weight denotational description. . prove type \nsafet~: if I have a variable that claims to contain a list of integers, say, will it truly do so? or \nmight the type system have become confused? By us\u00ading the axiomatization as a reduction relation we are \nable to use standard techniques to show type safety of this system.  . prove syntactic non-interference: \nour proof shows that if ever a state-read or -write is about to be attempted, Permission to make digital/hard \ncopy of part or all this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advan. tage, the copyright notice, the title of \ntha publication and its date appear, and notice is given that copying is by permission of ACM, Inc. To \ncopy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior spacific \npermission and/or a fee ICFP 97 A meterdam, ND  @ 1997 ACM 0-89791 -918 -1/97 /0006,.. $3.50 then the \ntype system guarantees that the reference is local. Our formal investigation reveals two subtle points \nthat the previous informal reasoning failed to uncover. First, arbitrary beta-reduction is unsound in \na compiler which encodes state-transformers as state-passing functions any compiler that implemented \nthe denotational semantics di\u00adrectly would have to take great care never to duplicate the state parameter. \nSecond, the recursive state operator f ixST cannot be interpreted naively in call-by-name, but really \nneeds call-by-need to make sense.  2 Imperative Types We begin by reviewing the issue of type safety. \nThe need for a special care arises in SML because of examples like the following: let val r =ref (fn \nx=>x) in (fn .=>!r4) (r :=not) end The variable r is given the type Va.(a -) a)ref, which sub\u00adsequently \nunifies with both the integer and the boolean uses. Intuitively, the problem is that the let has generalized \nover type variables that actually occur free in the state. The solution adopted for many years is to \nhave a class of imperative type variables [26] and only to generalize over these if the expression being \nbound by let is syntac\u00adtically a value. Due to Wright s observations [29] this has since been simplified \n[15] to treat all type variables as if they were imperative type variables, and so collapsing the two \ntier structure. Why do these problems not occur in Haskell when using monadic state? The precise answer \ncomes from the proof later in the paper, of course, but we can provide intuition here. When working within \nthe state monad (or any other explicit monad, for that matter) the facilities provided by let are given \nby the use of monadic extension (a bind op\u00aderator in Wadler s terminology [28], thenST in the State in \nHaskell papers, or >>= in Haskell 1.3 notation). This takes a computation a term of type MA where A4 \nis the monadic type constructor together with a function of type A+ M1?. Intuitively, the first term \nis executed, delivering a value of type A which is then passed as an argument to the function whose subsequent \ncomputation is also executed. The value is pawed according to regular function applica\u00adtion, so the type \nA in A+ MB is a regular type, not a type scheme. The intermediate values within a computation are lambda-bound, \ntherefore, rather than being let-bound. That s the intuition. The fact that it works relies on the correct \ninterplay of a number of different aspects of the system (in what comes later, for example, if the typing \njudg\u00adment for runST were relaxed, then type safety would be lost). One way of viewing all this is that \nthe Haskell type system makes a distinction between computations with no effects (which we call semantic \nvalues) and computations that may have effects (which we simply refer to aa computations). Se\u00admantic \nvalues are bound with the let construct and their types can be generalized; results of computations are \nbound with the >>= construct and their types cannot be general\u00adized. Hence, like SML, Haskell s let only \nperforms gener\u00adalization over values, not over the results of computations. However, unlike SML, the \nHaskell notion of value is seman\u00adtic and hence richer. For example, in Haskell the expression ((Az.z) \n(Ay.y)) is classified as a semantic value whose type can be generalized, but not in SML. 3 State in Haekell \nOur source language is an extension of the call-by-name X calculus with several constants and two language \nconstructs: l~t and runST (later we will add a third). Definition 1 (Syntax of Terms) Let z range over \na set Vars of variables {x, x1,x2,... ,y, z,.. .}. The set Of terms A is inductively defined as follows: \nSimple Constants: k ::= O1O[ll l +1 Primitive Store Operations: ., s ..-neuVar e I readVar e I writeVar \ne e Syrtactic Values: u ::= k IAxe Ie >>= e I returnST e I s Terms: .. e .. zlvlee lrrmSTellet{zi =ei}iine \nWe let k range over an unspecified set of simple constants like numbers and addition. The constants newVar, \nreadVar, and writeVar express the usual operations on reference cells. The constants returnST and >>= \nare the unit and bind oper\u00adations of the state monad respectively. Expressions built up from these statetransformer \noperations are treated as syn\u00adtactic values. These syntactic values will be used to specify the operational \nsemantics of the language, but not to guide generaUzation of type variables. The expression (runST e) \nis an eliminator for state-transformer expressions e. The operational intuition behind runST is that \nit executes e in a newly created state thread, returning the final vafue pro\u00adduced by e while discarding \nthe finaf state. The state in the thread is neither accessible nor visible from outside the (rurISTe) \nexpression. In the examples, we sometimes use the Hsskell 1.3 do notation [19]. This construct acts like \na non-recursive let over computations and is definable in terms of >>=. For exarhple, the following code: \nlet omega = omega in runST (newVex (3+2) >> \\ p -> readVar p >>= \\ VI ->  readVar p >>= \\ V2 -> vriteVar \nomega 6 >>= \\. -> returnST (v1+v2) ) where we have used the Haskell notation for lambda expres\u00adsions \n(\\x-Je) instead of the mathematical notation (Xz.e), could be rewritten as follows: let omega = omega \nin runST (do p c-newVar (3+2) VI <-readVar p V2 <-readVar p vriteVax omega 6 returnST (v1+v2) ) To get \nan informal feeling for the types and semantics of the expressions, we intuitively explain the evaluation \nof the above fragment. First, we bind omega to some (semantic) value and then create a new state thread. \nIn this thread, we allocate a reference cell p and initialize it to (3+2). The cell is then dereferenced \ntwice and the results are added. Once the values of V1 and V2 are determined, no more state op\u00aderations \nare performed as the final result is now defined. In other words we have a lazy store semantics in which \nstate threads are executed on demand, and values may be re\u00adturned before the computation has been completed. \nHence the assignment to the incalculable location omega does not affect the final result which is 10. \nWhy bother with lazy stores? The answer is that lazy stores have a clean interaction with the lazy semantics \nof the underlying language, and provide elegant ways to express imperative functional programs, as the \nfollowing example illustrates. The example is drawn from stream-based simulation, in particular simulating \na local data cache within a micro\u00adprocessor (for our purposes here we will assume no cache misses). The \ncontents of the stream represent the values of the input and output wires over time. The input streams \nto the cache will contain addresses, data, and a boolean read/write flag. The output stream will contain \nthe mem\u00adory contents for a read, and O if a write was performed. As the cache will contain thousands \nof randomly acces\u00adsible locations, a destructive array is the obvious choice for modeling the contents. \nWe therefore use the store prim\u00aditives readArr and writeArr, which are the the obvious generalizations \nto arrays of the operations on single loca\u00adtions. Here s the code in Haskeil, where the first pzmuneter \nto cache is the size, the second parameter contains the three input streams, and the result is the output \nstream: cache :: Int -> ( [Int] , [Int] , [Bool] ) -> [Intl cache size ins = runST (do err <-newArray \n(O, size-1) O loop err) loop err ((a:as), (d:ds), (True:bs)) = do x <-readArr an a xs <-loop err (as, \nds, bs) return (x: IS) loop err ( (a:as), (d:ds), (False:bs) ) = do x <-writeArr err a d xs <-loop err \n(as, ds, bs) return (O: XS) As the state is lazy, the result of each operation becomes available immediately \nafter it is performed there is no need r+e:r rkC3:T r +e:MutVar To T r . returnST e:STT T rkneWar e:STT \n(MutVer r T) r ~readVar e:STToT r ke;:htverr T r~e2:T rk vriteVar el ez : ST T Unit r U {Z : Vai.T} \nF Z : T[Ti/ai] r~.Z:T +T r+e :~ r+e:sra ~ rt-ee :7 rErunSTe:T aO@Fv(r ) vj.ru {Zi:T,};+ ej r u{z,: Vaji \n.T, }l 1-e :T : T] aji ~ v(Ti) wn  ri-let {Zi =ea}: inel :Tf Figure 1. Typing Rules to wait until \nall the state operations are performed. In\u00addeed, if es we expect the list were infinite, there would \nbe no end to the state operations. Conceptually, as the (infi\u00adnite) loop loop executes, it defines more \nand more of the output stream. Alternatively, as more and more of the out\u00adput stream is demanded by the \nrest of the program, more and more of the state operations execute. However, and this is an important \ncaveat, the state operations are linearly or\u00addered by the use of >>=, so the various reads and writes \nwill all be performed in the correct order whatever the pattern of demand. In summary, the cache component \nuses state internally purely for efficiency. Externally, however, it is simply a lazy stream transformer, \nwhich is exactly what the overafl struc\u00adture of the simulation requires We have the best of both worlds. \n3.1 Types We take ST # ~ as the abstract type of state computations. Intuitively a computation of type \nST # -T takes a state as an argument and delivers a value of type ~ together with a new state. Each state \nthread is indexed by a unique type. Uniqueness is given by universal quantification, and this will be \nseen to guarantee non-interference among different state threads. The type (IfutVar r r) is the type \nof references aflocated from a state indexed by # and containing values of type r. Definition 2 (Syntax \nof Types) Let a range over a set of type variables {a, al, crz, cr , . .}. The set of types 2s in\u00adductively \ndejined as follows: r , r ::= Unit lIntl . I ( fipes) cll T+ 7 / STr # I KutVar To # u ::= Va.a I 7 \n(ape schemes) The typing rules for our language are in Figure 1 and they use the conventions above, \nthat variously decorated instances of a, r and cr stand for type variable, type, and type scheme respectively. \nIn particular, a is just a regular type variable and ~ a regular type. The decoration is used to provide \na reminder that this variable/type occurs in the state-type position of an ST or HutVar type constructor. \nWe let r scope over type environments (partial mappings from term variables to type schemes). A type \njudgment 17 E e : r means that under the assumptions in the type environment r, expression e has type \nr. To these rules must be added the standard rules for the integer constants etc. Of these rules, the \nonly one to excite interest is the type of runST. If we were not restricted to Milner-style polymor\u00adphism \n[14], we might make runST a constant with the type: runST :: Va . (VcrO.ST cr a) + a To fit the Hindley-Milner \ncontext, we make runST a language construct with a typing judgment whose side condition sim\u00adulates the \nnested polymorphism. The reasoning behind the type of runST is as follows. Every operation which manipulates \na state thread is infected with the type of that state thread: when >>= is used to combine operations, \nthe types of the state-thread have to be the same (i. e., they become unified); every location returned \nby newVar has the same state thread type as the thread that created it; and every time a readVar or uriteVar \nis performed its HutVar argument belongs to the same state thread in which the read or write is actually \nperformed. Then when a state thread is encapsulated by runST the type system will only accept the encapsulation \nifi 1. the type of the state is still a variable; and 2. that variable is universally quantifiable. \n  If these two conditions hold then the state thread should make no demands on its environment to provide, \nsay, a location to be read or written. If it did, the type of the state thread would have been unified \nwith the state type of the location in the environment, and universal quantification could not take place. \nLaunchbury and Peyton Jones [12] showed that the in\u00adtuition pans out by using a parametricity proof over \nthe denotational semantics presented in the next section. In particular, they proved that the result \nof running a state thread is independent of an arbitrary encryption of the lo\u00adcations generated by all \nother state threads. The result we present in this paper is stronger, in that we show syntactic non-interference \n(which certainly implies the earlier behavioral non-interference), and in addition we show type safety. \nDomains: p~Env = IIr( Varsr -) Dr) O E Store = (N+ (Ur D.)), Meaning function: E:A + Ur D, &#38;[runST \ne]p  where (v, 0) = &#38;[e]p (lift O) &#38;[returnST]p v O = ~,tj ~, &#38;[>>=]p u v 6  where (v \n, 0 ) = v 0 &#38;[nevVar]p v O  (1,1) (t, qe \u00ad u]) ifO=~ where t?= new(0), otherwise &#38;[readVar]p \nt o  (l., L) (et, e) if t @ ctom(0) otherwise &#38;[vriteVar]p t?u O  [ (_L,l) ((), W-VI) if f @ \ndom(tl) otherwise Figure 2. Denotationaf Semantics 3.2 Denotational Semantics The denotational semantics \nof the language is standard [12]. We present the semantics of reference cells and the state monad combinators \nin Figure 2. We use this semantics as the model against which our axioms are verified. Another way to \nexpress the denotations of the state com\u00adbinators is to give them Haskell definitions and apply the standard \nmapping to these definitions. For example, the de\u00adnotation of (e >>= e ) could be derived from the following \nHaskell definition: \\s->let (x,s ) =esine xs In fact this is how the Glasgow Haskell compiler implements \nthe state combinators. (See Section 8 for more details.) 4 Axiomatic Semantics The goal is to specify \nthe semantics of our language via a set of local axioms that can be used anywhere inside a term (perhaps \nas optimization). The interesting axioms are clearly the ones related to reference cells and the store. \nThe axiomatization of references and stores is generally well\u00adunderstood for many languages [3, 5, 7, \n8, 9, 13, 23, 24], but is fairly subtle for our language. The elegance of lazy stores has a price: it \ncomplicates the semantics of the language. Indeed, some expected axioms are unsound due to the laziness \nof the store. In many lan\u00adguages it is reasonable to expect that writing an expression e to a location \nz and then immediately reading the location z returns e [3, 16, 17, 23]. In our syntax, the potentiaf \naxiom is: netrVar e >>= Az.readVar x >>= e neuVar e >>= Ax. e e Unfortunately the axiom is unsound \nusing the axiom we would be able to transform: let omega = omega in do d <-vriteVer omega 6 x <-nerdlar \nO a <-readVar x returnST a into the rather different: let omega = omega in do d <-vriteVar omega 6 x \n<-nevVar O returnST O Why are they different? The second makes less demand on the store than does the \nfirst. The evaluation of the first term needs access to the store in order to read the contents of location \nx. But the store cannot be computed since the first state transformer in the thread diverges (it cannot \ntell which physical location should be updated by the uriteVer). In contrast the evacuation of the second \nterm does not need the store at all, and immediately returns O. Had the location in the uriteVar subexpression \nbeen a known location, the evaluation of both terms would have been equivalent. This informaf analysis \nsuggests a way to fix the problem: only use the axiom in special contexts where the store is guaranteed \nto be well-defied. Unfortunately there appears to be no easy syntactic way to represent well-defined \nstores without resorting to a se\u00adquence of newVers followed by a sequence of writevers as the store can \ncontain cyclic references. To avoid messy syn\u00adtactic patterns, we introduce a special construct: sto \n{@l, el), . . .,(pn, en)} e that represents a well-defined initialization for the store. In this store, \nlocation Pi contains ei; the expressions e; are naturafly allowed to refer to the other pj so we can \nrepresent cyclic structures in the store. This term form is similar in structure to the pfl.e term form \nof Wright and Felleisen [8, 13, 29] though the axiomat ization is rather different to take account of \nencapsulation. Definition 3 (Syntax of Terms) Let p range over a set of locations {p, p,, p,,.. .}. We \neztend the syntaz of Defini\u00adtion 1 as jollows: .. v .. . 1P .. . ..lstof?e .. ; ::= {(pi,ei)}l (Store \nBindings) The typing rule and denotational meaning for the new constructs are in Figure 3. We view locations \nas a particu\u00adlar bread of term variable, with sto acting as a binding site. Thus type environments can \ncontain assumptions about the type of locations, just es they do for the original brand of Typing Rules: \n W. rU {P, : MutVar an 7,}, F e,:r, ru{p:7-}t-p:~ r u {Pi : MutVsr QO T,}i r+sto{p, -ea}, F e:r e: STaOT \n@o @ ~V(T, r) Semantics: &#38;~]p = p(p) &#38;[stO 0 e]p = StO {(/1, &#38;[e;]p ) I (p,, e;) t 0} t[e]p \nwhere: p = pu {(pi, /t)} Stosv = ml (v (hjl s)) Figure 3. Additional Typing Rules and Denotations variables, \nbut unlike for term variables, locations can only be bound to types and not type schemes [26]. It is \nstraight\u00adforward to check that both the type and denotation of sto generalize those of runST in the sense \nthat:   sto t?e = rsmST e In order to typecheck the sto expression, e has to typecheck (the Vj is \nvacuous). The side condition on the type judgment follows from the corresponding side condition on the \ntype judgment for rsmST. We therefore do not consider runST independently in the remainder of the paper. \nBefore giving the axioms, we need to formalize one last aspect of the kuy stores; we must define the \nposition within a state thread from which it is possible to immediately re\u00adturn without performing the \nrest of the stateful computa\u00adtion. Definition 4 (Return Contexts l?) The contexts are in\u00adductively defined \nas: R ::= [ ] Ie >>= Ax.R In other words, we can ignore all the computations to the left of >>=; these \nwill not performed unless they are somehow explicitly demanded. In terms of the do notation, it means \nthat we should skip over the list of commands and attempt to execute the lzst one first. Figure 4 presents \nthe correct axiomatization of the se\u00admantics. The correctness of the axioms is easily established by \nchecking that the two sides of each equation are denota\u00adtionally equivalent. The first three axioms are \nas expected in an applied lambda-calculus. The next three axioms use the new sto construct as motivated \nabove; each primitive store opera\u00adtion performs its intended operation on the properly initial\u00adized store \nfragment. The structural axioms correspond to the three monad laws. Finally the return axioms show how \nto compute the result of a state thread; there is an axiom for each kind of syntactic value. Operational \nSemantics Having defied the axioms, we need evaluation contezts that guide the use of the rules in a \nstandard reduction sequence leading to the answer. Because of the lazy nature of our store, the definitions \nof the reductions is actually intertwined with the definition of evaluation contexts [1, 2]. Intuitively, \nneeded variables within subterms correspond to those vari\u00adables that occur in evaluation context positions. \nTherefore, we define evaluation contexts first. 5.1 Evaluation Contexts Defining evaluation contexts \nalready requires much under\u00adstanding about the semantics of our language. In our case the definition \nis rather involved and we proceed slowly. The definition of return contexts (Definition 4) shows that \nwe should skip over the list of commands and attempt to execute the last one. If this last command requires \na vari\u00adable that results from an earlier computation, then we must attempt to perform that computation. \nAlso if a command attempts to perform an operation that is strict in the store like newVar, readVar, \nor writeVsr, then we must also step back and perform all the earlier computations. Formally we can express \nthese chains of dependencies ss follows. The definition uses the yet-to-be-defined evaluation contexts \nE. At this point the reader may pretend that all evaluation contexts are the empty context to get the \nintuition behind the concept of dependencies. Definition 5 (Dependencies D) Z he first three clauses \nexpress that van able z is needed by a state transformer. The last two clauses express that variable \nx is needed because an\u00adother sequence of variables was recursively needed. D ::= Az.R[E[z]] I Az.R[returnST \n17[z]] I kc.R[e >>= E[z]] I Az.R[E[z] >>= D] [ Az.R[s >>= D] The definitions of dependencies and evaluation \ncontexts are mutually recursive. Definition 6 (Evaluation Contexts E) The set of con\u00adtezts is inductively \ndejined as: .. I iio19R[k] E [11 Eelk E I sto O R[returnST E] I sto r3 R[e >>= E] I sto O R[E >>= D] \nI sto 9 (readVex E >>= D) I sto O (writeVar E e >>=D) The first three clauses in the definition of evaluation \ncontexts define the usual contexts for call-by-name languages. The remaining contexts are used when evaluated \na state thread. The next three contexts combined keep demanding the right argument of >>= until they \nreach the last state transformer in an R sequence. If that state transformer is a returnST then we demand \nthe value of its subexpression. If on the Computational Axioms: (k.e)e = e[e /z] let {z, = e~}, in e \n= e[(let {z, = e,}~ in ej)/Zj], kv = a(k, v) if defined sto O (neuVar e >>= e ) = sto 13U{(p, e)} (e \np) sto OU {(p, e)} (readVar p >>= e ) = sto OU {(p, e)} (e e) sto OU {(p, e)} (uriteVar p e >>= e ) = \nsto OU {(p, e )} (e ()) Structural Axioms: returnST e >>= e e e (el >>=ez) >>=e = e~ >>=Az,(ez z >>=e) \ne >>= kr.returnST z e Return Axioms: sto O R[returnST k] k  sto O R[returnST (Aye)] Ay.sto 8 R[returnST \ne]  sto O R[returnST (e >>= e )] (sto O R[returnST e]) >>= (st. O R[returnST e ]) sto f? R[returnST \n(returnST e)] . returnST (sto O R[returnST e])  sto f3 R[returnST (neuVar e)] newVar (st 00 R[returnST \ne]) sto O R[return3T (readVsr e)] . xeadVar (sto O R[returnST e])  sto O R[returnST (vriteVer e e )] \n vriteVar (sto O R[returnST e]) (sto O R[returnST e ])  sto O R[returnST p] P if p $? dorn(0) Figure \n4. Axioms other hand, the last state transformer demands a variable, then we backtrack following the \npreviously defined chains of dependencies demanding state transformers on the left of >>=. Finally the \noperations readVar and vriteVar are strict in their first argument which is the location to read or write. \nFor example, using evaluation contexts and dependen\u00adcies, we could rewrite the following term: sto {} \n(newVar (3+2) >>= \\x -> readVar x >>= \\a -> readVar x >>= \\b -> uriteVar x (let y=y in y) >>=\\_ > returnST \n(a+b)) as: sto {} (newVar (3+2) >>= D) The reasoning is that the last state transformer demands the \nvariable a: sto {} (neuVar (3+2) >>= \\x -> readVar x >>= \\a -> R[returnST E[a]]) where E is ([ ]+b). \nThen, using the definition ofD, the demand for a propagates to a demand for readVar z which demands the \nresult of thenewVar. In contrast the term: sto {} (uriteVar (let y=y in y) 6 >>= \\_ -> neuVar O >>= \\x \n-> readVar x >>= \\a -> returnST a) would be decomposedss follows: E[(let y=y in y)] This formalizes \nthe observation in the previous section that the evaluation of the first term terminates but the evaluation \nof the second term diverges. 5.2 Faulty Expressions Iftypechecking guarantees anything, it is that certain \nbad expressions never occur. Apart from the usual errors (for example, adding aboolean to a character) \nwe are interested in avoiding a whole group of bad expressions that have to do with the state. These \nare expressions that attempt to read or write to a state location which is not part of the local thread, \nor which return a state location as thereault of an encapsulated thread. The fact that thetype syatem \ncatches these is perhaps the noteworthy aspect of this formulation of state. Definition 7 (Faulty Terms) \nLetp nange otrerkationsj v mnge over syntactic values, andw mnge over the following syntactic values: \nk, p, or (Az.e). An expression e is faulty if it is one of the following: . v e , and v is neither a \nlambda expression nor a wn\u00adstant k (a non-function in function position), . k v, and J(k, v) is undejined \n(undejined basic opera\u00adtion), . sto d (readVar v >>= D), sto o (uriteVsr v e >>= D), and v is not a \nlocation (a non-location in loention po\u00adsition). . sto e (readVer p >>= D), sto @ (writeVsr p e >>= \nD), andp is not in the domain of 6 (interference between separate state threads).  . sto O R[returnST \np], andp is in the domain of O (ez\u00adporting private locations), . sto 8 R[w], sto O R[w >>= D] (a non-state-operation \nwhere one was ezpected), . sto 8 (R[e >>= v]), and v is not a lambda expression {a non-function in function \nposition),  Computational Reductions: (Ax.e)e --i e[e /z] let {Zi =e, }, in e + e[(let {Zi = ei}, \nin eJ)/xj]j kv -(5(k, v) if defined sto O (neWar e >>= D) --+ stOOU{(p, e)} (Dp) sto OU{(p, e)}(readVarp>>=D) \n+ sto/3U{(p, e)} (De) sto OU{(p, e)} (writeVer pe >>= D) ~ sto OU{(p, e )} (D ()) Structural Reductions: \n sto OR[returnSTe>>=D] + sto6 R[De] sto O R[(el >>=e2) >>= D] ~ sto OR[el >>=,kc.((ez z) >>= D)] sto \nO R[s] -+ sto O R[s >>= Az.returnST z] Return Reductions: (Orient Return Axioms from left tonight.) \nFigure 5. Standard Reductions 5.3 Reductions We now use evaluation contexts to restrict the axioms in \ntwo ways. First, the patterns of some of the axioms are restricted to avoid infinite reduction sequences \nthat perform no useful work, and to avoid interference between the axioms. For example, we certainly \nwould not want to repeatedly rewrite an expression e to (e >>= kc.returnST z). Second, during evaluation, \nwe only perform reductions that are demanded by an evaluation context. Figure 5 presents the reductions \nof the language. The main restrictions with respect to the axioms are that the use of the structural \naxioms has been restricted to cases where it ia actually useful to make progress in a computation. Also \nthe primitive store operations are not performed unless their results are demanded via iichain of dependencies \nD. Given the complexity of our evaluation contexts, reduc\u00adtions, and faulty expressions, how do we know, \nfor example, that we didn t forget one kind of faulty expression. The fol\u00adlowing proposition which is \nused to prove type soundness later, verifies that the above definitions are consistent and complete. \nProposition 1 Every term e is either a syntactic value or can be uniquely partitioned into the fown E[ \nT] where 2 is either: . a variable not bound in E, . a faulty term (see Definition 7), or . a redez \n(see Figure ~).  Proof. The proof is by induction on the structure of e and proceeds by cases. All cases \nare straightforward except the case e = (sto 0 e ) which requires an additional induction as follows: \n(i) First we show by induction on the number of occur\u00adrences of >>= in e (and using the main inductive \nhy\u00adpothesis too) that e must be in one of the following forms: R[E[Z ]]or Z?[e >>= E[TJ] or R[E[T] >>= \nD] where T is faulty, a redex, or a variable bound in neither R nor E,  R[v], where v is not of the \nform >>=,  R[e >>=v] or R[v >>=D] (ii) Second we show that the main claim applies to the expression \n(sto O e ) where e is given by one of the forms in (i). Having split the proof as above, both subproofs \nare now straightforward. 1 Corollary 1 EveW closed term e is either a syntactic value, or the form E[T] \nwhere T is either faulty or a redez. 6 Type Soundness The type soundness proof closely follows the subject \nreduc\u00adtion proofs by Wright and Felleisen [29], providing extra evidence that their techniques are widely \napplicable. There are two cases where the proofs make clear the r61e of the typings we provide, and in \nparticular, the way in which the type rule for runST provides safe encapsulation. These cases will be \ndone in some detail. Once the operational semantics and type system have been defined, the general form \nof the syntactic type sound\u00adness proof is as follows: (i) Show that reduction in the operational semantics \npre\u00adserves well-typing. This is called subject reduction. (ii) Show that faulty expressions are not \ntypable.  If programs are closed and well-typed, then we can put together the previous results as follows: \nBy (i), evaluation of the program will only produce well-typed terms. By Corol\u00adlary 1, every such term \nis either faulty, or a syntactic value, or contains a standard redex. The fist case is impossible by \n(ii). Thus either the program reduces to a value of the correct type, or it diverges. We prove the above \npoints (i) and (ii) in the remainder of the section. 6.1 Subject Reduction The subject-reduction lemma \nstates that a well-typed term remains well typed under reduction. Hence, if ever a readVar or ririteVar \nis performed, for example, the expression ex\u00adtracted from the state will not introduce a type error when \nit is substituted into the receiving term. In other words, if a variable claims to hold an Int + Int \nfunction, then indeed it does (and not a Bool + Bool function as in the introduction). The reason that \nthis works out correctly is that no lo\u00adcations can ever be assigned a type scheme such as {ps : Va.MutVar \na a}. If such a typing were possible, then we could easily duplicate the counterexample in the introduc\u00adtion. \nAs usual, the proof of subject reduction relies on other standard lemmas, most notably the substitution \nlemma. Lemma 1 (Substitution) J-fI [z * Vcr,.#] R e : T and z $4 dorn(17) and r 1-e : # and {~l}i n FV(I \n) = 0 then r i-e[e /z] : T In our context this lemma is a generalization of Wright and Felleisen s Lemma \n4.4 in two ways: we require subst itu\u00adtion of arbitrary expressions rather than of syntactic values only; \nand we need to show the sto form causes no problems. The fist of these generalizations is handled by \na trivial ex\u00adtension of the proof of Lemma 4.4, and the second similarly from the proof of the corresponding \nlemma dealing with the pt?.e form (Lemma 5.3). Once the substitution lemma is shown, subject reduction \nfollows fairly easily. Lemma 2 (Subject Reduction) l~l_ 1-e : r and e + e then I t-e :~ Proof. The proof \nproceeds by case analysis on the reduc\u00adtions e + e . Most of the cases are standard, and are a minor \ngeneralization of the proof found in Wright and Felleisen, so we will not rehearse them here. The only \nin\u00adteresting cases are for the various instances of st o. We will exhibit two instances to show the general \nform. Case: sto {Pi * e~}~ (readVar p~ j>= D) + sto {p; + e;}; (D ek) where there exists an i such that \npi =pk. By assumption we know that: r k Sto {p; + e;}, (readVar pk >>=D) : T, but in order to be able \nto deduce this we must have been able to show aJl of the following: . for all j, ru {p, : llutVSr Cr \nTi}i F ej :Tj, . I U{p~:MutVar~O~~}lFD :#+STCYOT where cr @ FV(~, I ) (the latter two judgments follow \nafter an application of the rule for >>= and for readVar). It is clear from the second of these that \nT and rk are equal. In order to typecheck the right hand side we need to be able to show that: . for \nall j, ru {pi: MUtVSr Cl Ti}i F ej :Tj, . ru {pi : MUtVar Cr Ti}i ~ ek : f , . rl-J{pi:Mutva.raO~i}i+~ \n:T +STCY T where, again, CYo@ FV(r, f ). Given that r is equal to rk, all these follow from the above. \nCase: sto {p: H ei }i (returnST (Aye)) + Ay.sto {Pi -ei }a (returnST e) Again, by assumption we know \nthat: f 1-sto {pl * ei}i (returnST (Aye)) : r + r (here we have jumped to the conclusion that the result \ntype must be a function type it just simplifies the presentation). In order to be able to deduce this \njudgment we must have been able to show: . fOrallj, f u{pZ :HutVar CYO~1}1 hr?j :Tj, . rU{y:~} U{pl:f4utVar \na ~t},~e:~ where a @ FV(~ + ~ , r). In order to typecheck: r 1-Ay.sto {p: -ei}i (returnST e) : ~ + ~ \n we must show that: r U {y :T} 1-sto {Pi H ei}i (returnST e) : T which, in turn, requires all of the \nfollowing: . forallj, rU{y:~}U {pi :FhltVarc1\u00b0 7i}il_ej :Tj, where, th~ time, a @ FV(#, I U {y : ~}). \nIn fact, the side condition is the only thing that requires any thought, and that follows immediately \nfrom the fact that FV(~ + ~ , I ) = FV(#, r u {g : T}) since y does not occur in r. This final case is \none place which motivates the choice of the side condition on a in the type rule for sto. It clearly \nwould not be enough simply to restrict a from appearing in the free type variables of r without mentioning \nthe result type r. The other cases all follow the same form, so concludin the proof. d 6.2 Faulty Expressions \nLemma 3 If an expression e is jaulty, then it is not typable. Proof. Each case in the Definition of \nfaulty expressions (Definition 7) is treated separately. We show how expres\u00adsions with interfering state \nthreads are not typable: Case: sto {pi I+ ei}i (readVar p >>= D) where p # {pi}a. To typecheck the expression \nin a context r, we must show the following . for all ~, I U {pi : 14utVar O ~i}, H ej :Tj, and . r U \n{Pi : HutVar ~ Ti}i 1-readVar p >>=D : ST~ T where a @ FV(T, I ). To satisfy the second requirement, \nwe must show that there exists a # such that: r U {Pi : 14utV~ ~ ~i}i b readVar p: ST CZor which in turn \nrequires that: r U {Pi : MltVSr Cr Ti}i F p : MltVSr Cr T By assumption, p@{pi }i, hence we require: \nwhich implies that I_ must contain an entry p: Hutlfar a #. But the side condition on sto states that \na is not a free type variable in r: a contradiction. In other words, type\u00adchecking fails if there is \nany possibility of a segmentation fault across state threads. . 7 Other State Operations The State in \nHsskell paper [12] presented two other oper\u00adations on the state. The first eqVar tests for equality of \nlocations; it has the type: eqVar :: HutVex ff r +HutVar a r +Bool It introduces no difficulties to the \nforegoing material. The second, fixST, does. The purpose off ixST is to allow recursive bindings within \nthe state monad. The usual recursion gained from let al\u00adlows us to define recursive state transformers \n(while-loops and the like), but f ixST provides and entirely new facility. Using the do-notation we might \nlike to write: data IntNVar s = Pair Int (MutVer s IntNVar) makeLoop = do v <-newVar (Pair 7 w) w <-newVer \n(Pair 2 v) returnST w in which the v and w are both in scope for each of the newVar operations. Upon \nexecuting these operations, the store would construct a cycle, and the location w would be returned. \nNote that even though the definition is recursive, thetwo store operations are each performed once only. \nFor recursive definitions like this, one would expect to use a fixed point operator, and this case is \nno exception. We want an operator: fixST:(cr +STaOa)+STcrO a From adenotational perspective thisisfine. \nWecould define the meaningof fixST aa: &#38;[fixST]pj@ = u~>og~(l,l) whereg(p) = f@Ip)O or more loosely, \nas the expansion: fixST e=\\s-> let (x,s ) =exs in (x, s ) Using f ixST and the usual do-notation, we \ncould define our recursive store above by: makeLoop = fixST (\\w -> do v t-newVar (Pair 7 u) w <-newVer \n(Pair 2 v) returnST w ) There arehowever twoproblems with all this. First, the inclusion of fixST breaks \nsome axioms that would otherwise besound. However, that isapricewehave topayifwedesire the functionahtyof \nfixST. Second, andmore seriously, the call-by-name axiomatization of fixST is problematic. We will describe \neach of these in more detail. 7.1 Sequencing Axioms Intheabsenceof fixST, itisreasonable toexpect thata \nread and a write that refer to different variables can be performed in any order as they will not atfect \neach other. Because of aliaaing this is often difficult to determine, but we might expect the following \nto hold: neullar e >>= Ax.writeVar y e >> e = writeVer y e >> newVex e >>= kc. e (x # y) as x and y clearly \nrefer to different variables. Unfortu\u00adnately such an axiom is unsound in the presence of fixST as it \nequates the following two terms: fixST (\\y->neuVax O >>= \\x-> uriteVar y 1 >> returnST x) fixST (\\y->writeVer \ny 1 >> nevVar O >>= \\x-> returnST x)  According to the semantics, the denotation of the first term is \nnon-bottom and the denotation of the second term is bot\u00adtom. Tounderstand the problem, weexpand andinline \nthe state combinators: \\sO->let (y, sl) = let (x, s2) = nevVar O sO (_, s3) = writeVar y 1 S2 in (x, \ns3) in (y, sl) \\sO-jlet (y, sl) = let (.,s2) = writeVar y 1 sO (x, s3) neuVar O S2 in (x,s3) in (y,sl) \n When applied to a store, the evaluation of the first term de\u00admands (x, s3) which demands (x, s2). Thus \nthe first com\u00adputational step is to create a location x with initial value O. This binds y to the new \nlocation which makes the write operation well-behaved. In contrast the evaluation of the second term \ndemands (x, s3) which demands S2 (remember that nevVar is strict in its store argument) which demands \ny which demandsx which demands s2. In other words the evaluation of the second term diverges. To address \nthis problem we are simply careful not to include axioms that change the order of state operations, even \nwhen such changes are apparently safe. These axioms are not needed for evaluation anyway. 7.2 Call-by-Name \nand fixST In general theaxiomatic semantics of fixed point computa\u00adtionsis expressed byunwinding the \nrecursion. For example, the usual semantics for the fixed point combinator on values fix is: fix e= e(fix \ne) Using this idea, it is a simple exercise to derive the following semantic equivalence for flxST: \nsto O (fixST e >>= e) = sto 8 (e (sto 8 (fixST e )) >>= e) (*) To understand the intuition, remember \nthat (sto 8 e) evafu\u00adates the comput ation e in the state tbread O. If this evalua\u00adtion terminates, it \nyields a final value and a final state. The final state is ignored and only the final value is returned. \nThus the equivalence illustrates that only values (but not stores) are propagated across the unwindings. \nSo what s the problem? The problem is that the right hand of the equivalence does not typecheck! Consider \na location in e ; both the outer and inner state threads may attempt to access that location. This is \nexactly the kind of situation that the typing of runST is designed to avoid! The discussion points to \na fundamental problem with fixST. The construct fixSTexpresses thecomputation ofa recursive value that \ntakes one input store and returns one final store. Any unwinding of the recursive computation must do \nsome non-standard manipulation of the store, e.g., duplicating a store, or ignoring a store. As we have \nseen duplication of the input store is likely to produce untypable terms and it is impossible to ignore \na store using our com\u00adbinators. In other words, it appears that ina call-by-name world the combinator \nfixST needs to be restricted in or\u00adderto make sense. Here we restrict itstype to prevent any recursive \nvalues that use the store: With thisrestriction our axiom(*) issoundandtypable. In versions of Haskell \nincorporating fixST, there is no such type restriction. We believe that the full rule will only make \nsense in an explicit call-by-need setting in which re\u00adductions do not duplicate state threads. 8 Implementing \nMonadic State in GHC The Glasgow Haskell compiler (GHC) implements monadic state by expanding the monadic \ncombinators to pure Haakell. This is fine so long as either we refrain from performing arbi\u00adtrary call-by-name \ntransformations on the code, or we give up on destructive update. Part of the motivation for this current \nwork was the desire to be able to retain both. In more detail, the common implementation strategy [12] \nfor Haskell s extension with built-in monads is to: 1. translate the source programs by expressing and \nin\u00adlining returnST, >>=, and runST in the intermediate language of the compiler, 2. apply full compiler \noptimizations to the resulting in\u00adtermediate programs, and  3. instruct the code generator not to generate \nany code to paas the state around and to generate destructive ver\u00adsions of newVar, readVar, and smiteVar \nthat operate on a global store. Following this strategy, consider the following source pro\u00adgram: runST \n( newVer O >>= Ap. vriteVar p 5 >> readVer p >>= Au. returnST v) whose evaluation according to the denotational \nsemantics produces 5. After irdining the monadic combinators (runST, >>=, and returnST), and doing some \nsimplifications, we get a program in the compiler s intermediate language: fst (let (p, sl) = newVar \nO sO (., s2) = uriteVar p 5 S1 (v, s3) = readVar p S2 in (v, s3)) where sO is the initial store. If the \ncompiler only per\u00adforms call-by-need optimizations that only duplicates syn\u00adtactic values, the evacuation \nof this intermediate program produces the correct answer 5 even if the operations newVar, uriteVar, and \nreadVar ignore the store argument and per\u00adform side-effects on a global store. However, using the call\u00adby-name \nreasoning principles that are valid in Haskell, we can transform this program as follows: fst (let dl \n= ne@Jar O sO d2 = writeVar (fst dl) 5 (snd dl) (v, s3) = readVar (fst dl) (snd d2) in (v, s3) ) . fst \n(let dl = nedlar O sO (v, s3) = readVar (fst dl) (snd (uriteVax (fst dl) 5 (snd all))) in (v,53)) . f \nst (let (v, s3) = read (fst (newVar O sO) ) (snd (writeVar (f st (newVar O sO) ) 5 (snd (newVar O sO) \n) ) ) in (v, s3)) If newVar were a pure function, then all the occurrences of (newVar O sO) would evaluate \nto the same location, and the program would evaluate to the expected answer 5. How\u00adever, an implemental \nion of the operations newVar, writevar, and readVar that ignores the store argument and performs side-effects \non a global store will not produce the answer 5. To understand why, note that the expression (newVar \nO sO) haa been duplicated several times; each evaluation of this expression will create a fresh location. \nIt follows that the location in which 5 is written is not the same location from which we attempt to \nread. The counterexample reveals that cafl-by-name and call\u00adby-need evaluations of the intermediate program \ndo not co\u00adincide. In other words, the compiler s intermediate language is not purely functional and hence \nmust be optimized with care. Not only can /3 steps in the compiler cause severe performance problems, \nfor example by duplicating expen\u00adsive computations [2], but more drastically, they are un\u00adsound. Fortunately, \neven before the monadic extensions, most Haskell compilers were cweful not to duplicate work and hence \nrefrained from using ~ steps for performance rea\u00adsons. Consequently, the addition of assignments to the \nback end did not cause any problems for such compilers. 9 Related Work The J or system [17] is very similar \nin spirit to state in Haskell, and hence to the work presented here. A pure construct was introduced \nthat played the role of runST in that it encapsulated imperative computations, guarantee\u00ading their external \npurely functional behavior. Two methods were presented by which this can be achieved. The first was to \ndemand an explicit expansion of the whole of the spine of the monadic computation so that a run-time \ncheck could ensure that the variables referenced were indeed local. Of course this would be prohibitively \nexpensive in practice, and it seems impossible to generalize to lazy state. As an alternative, a type \nsystem was proposed which statically ensured that the state threads were pure [4]. Like early versions \nof hlL, the type system had two sorts of type variables (applicative and imperative), In addition, the \ntyp\u00ad ing judgment for pure demanded that only applicative types appeared in the type environment and \nin the result type much more restrict ive than the Haskell solution. Unfort u\u00ad nately, the type system \nis now known to be incorrect. Re\u00ad ductions may change the set of free variables in a term, so the purity \ncondition, which only restricts the types of free variables, can be circumvented. As a consequence, subject \nreduction fails. The problem was corrected by adapting the Haskell solution (201. The work on re~ion \ninference is also remarkably simi\u00ad lar [27]. Our sto construct is essentially creating a new region \nand initializing it. However, in cent rast to the re\u00ad gion language, an expression in our language cannot \naccess variables in several regions, The type-based encapsulation works well in Haskell be\u00ad cause the \nexplicit use of the state monad (and others) pro\u00ad vided a ready home for the extra type variable. Could \nsuch a thing be done in ML? One method might be through something like effects annotations [25] or, indeed, \nthrough a region inference system, but there are many details to be worked out, Finally, the parametric \nmodels of local variables have strorw semantic similarities to the work here [181. A deno\u00ad ,, tational \nsemantics h= to generate new local variables every time a new block is entered. By using parametricity, \nthese variables can be hidden from the outside world. In our set\u00adting, where variables are first class \nvalues, we need to have a similar feature within the language, hence the type of runST. 10 Conclusion \nand Future Work In terms of its relation to future developments in under\u00adstanding and controlling effects, \nthe most exciting aspect of this paper is the clarification of the mechanism for encap\u00adsulation. This \nmechanism is so powerful that it permits the type system to guarantee that references (and hence effects) \ncannot be perceived outside of the encapsulation barrier, This means that a computation could use state \ninternally to achieve efficiency, yet show a guaranteed pure face to the outside world, without having \nto do any expensive run-time checks. 10.1 Typechecked Segmentation Given the spread of run-time mechanisms \nused for checking locality of references, from operating system segmentation checks to mechanisms for \nencapsulating effects in functional languages [10, 21, 22] it is perhaps surprising to discover that \nthe type system is quite strong enough to do it statically. Of course, the fact that type systems can \nfigure out the lifetimes of references has been known for some time [6]. What distinguishes our solution \nbased on runST is that it requires such minor changes to the language. To give some ideas of the accuracy \nachieved by this mech\u00adanism it is worth noting that it is quite feasible to have one state thread manipulate \nlocations belonging to another quite sepzwate thread. As long as no attempt is made to derefer\u00adence these \nother locations, the type system does not unify the state-type parameters of the locations with the state\u00adtype \nof the thread. Under these conditions, the host thread could build and traverse a graph containing the \nforeign lo\u00adcations, perhaps duplicate or discard the locations, or build them into data structures, eventually \nto be returned, pre\u00adsumably, to the owning thread for it to dereference at will. Through all this the \ntype system is able to track that the threads do not interfere with each other, and that they are indeed \nseparate state threads. 10.2 Nested Scopes The principle behind runST can be generalized to provide nested \nscope. We could introduce two constants: blockST :: (Vf?,ST (cY,@ T) + ST a r importVar :: Mutvar c1T \n+ Mltvar (Cr,p) 7\u00ad (actually, like runST, we would introduce blockST as a km\u00adguage construct with a \ntyping judgment that simulated the nested polymorphism in its type). Using importVar we can explicitly \nallow variables from an enclosing scope to be ma\u00adnipulated by the inner scope. For example, f =do a <-newVar \nO b <-nevVar True blockST (g (importVar a)) v <-readVsr a returnST v gx= do c <-nevVer hello vriteVar \nx 1 returnST () The type for blockST guarantees that the variable c is only used in the inner scope. \nIt is not exported to the outer scope in any way. This provides a firm notion of local pointer, one that \ncannot be accessed outside the block. In practice, we might like even finer control than this. Extending \nthe system to provide only read access in inner scopes (and not write access) is easy to achieve (14utVars \nneed to take two state variables: one to say which thread can do reads, the other to say which can do \nwrites), but the more challenging control of, say, dividing an array into two distinct parts to be worked \non concurrently seems much harder to achieve. 10.3 Type-encapsulated Exceptions State is a natural first \napplication for this technique, but it is bound to be applicable to others like exceptions and con\u00adtinuations. \nThe trick to success here is finding a formulation of the basic operations which is both natural and \nconvenient while succumbing to the extra type variable technique. As an example, we present a formulation \nof exceptions which allows us to use the same type-encapsulation tech\u00adnique. We need two new abstract \ntype constructors: the monad of exception-raising computations ET a -r, and the type Exn o of exceptions \nraised in thread cr; together with the following operations: rrmET :: 7+ (Va.ETa T)+ ~ newExn :: ET a \n(Exn a) raiseExn :: Exna+ETar handleExn :: ETcrr+(Exn a, ETa~)+ETcr ~ The first argument to runET is \na default value used to replace any uncaught exception. The operation neuExn dynamically createa a new \nexception, which can be handled by any han\u00addler that is executed within the same monadic thread. 10.4 \nCall-by-need Semantics Within the narrower scope of state in Haskell, this paper suggests that there \nis something significant to be gained in moving to an explicit call-by-need semantics of monadic state, \nin that f ixST would lose its side condition. It would be in this setting also that we should expect \nto be able to use an axiomatic semantics to establish formally the correctness of destructive implementations \nof monadic state. Acknowledgments We would like to thank Zena Ariola for many discussions about the axiomatic \nand operational semantics. The re\u00adviewers provided many comments that improved the pre\u00adsentation. References \n[1]ARIOLA, Z. M., AND FELLELSEN,M. The call-by-need lambda calculus. To appear in the Journal oj Functional \nPmgmm\u00adming, 1996. [2] ARIOLA, Z. M., FELLEISEN, M., MARAIST, J., ODERSKY, M., AND WADLER, P. A call-by-need \nlambda calculus. In ACM Symposium on Pn nciples of Pmgmmming Languages (1995), pp. 233-246. [3] BOEHM, \nH.-J. Side effects and aliasing can have simple ax\u00adiomatic descriptions. ACM Zkcmsactions on Programming \nLanguages and Systems 7, 4 (Oct. 1985), 637-655. [4] CHEN, K., AND ODERSKY, M. A type system for a lambda \ncalculus with assignment. In Theoretical Aspects of Com\u00adputer Software (1994), Springer Verlag, LNCS \n789. 15] CRANK, E., AND FELLMSEN, M. Parameter-passing and the lambda calculus. In ACM Symposium on Principles \nof Pro\u00ad gmmming Languages ( 1991), pp. 233-244. [6] DAMAS, L. M, fipe Assignment in Pmgmmming Lan\u00adguages. \nPhD thesis, University of Edinburgh, 1985. [7] FELLEISEN, M., AND FRIEDMAN, D. A calculus for assign\u00adments \nin higher-order languages. In ACM Symposium on Principles oj Progmmming Languages (1987), pp. 314-325. \n[8] FELLEISEN,M., AND HIEB, R. The revised report on the syntactic theories of sequential control and \nstate. Theoretical Computer Science f 02 (1992), 235-271. Technical Report 89-100, Rice University. [9] \nHOARE, C., HAYES, 1., JIFENG, H., MORGAN, C., ROSCOE, A., SANDERS,J., SORENSEN,I., SPIVEY, J., AND SUFRIN, \nB. Laws of programming. Communications of the A CM 30, 8 (1987), 672-686. [10] LAUNCHBURY, J. Lazy imperative \nprogramming. Technical Report, Yale University, 1993. ACM SIGPLAN Workshop on State in Programming Languagea. \n[11] LAUNCHBURY,J., AND PEYTON JONES, S. L, Lazy functional state threads. In ACM SIGPLA IV Conference \non Progmm\u00adming Language Design and Implementation (1994), pp. 24 35. [12] LAUNCHBURY,J., ANDPEYTON JONES,S. \nL. State in Haskell. Lisp and Symbolic Computation 8 (1995), 193-341. [13] MASON, 1., AND TALCOTT, C. \nL. Equivalence in functional languages with effects. Joumai of Functional Progmmming f, 3 (July 1991), \n287 327. [14] M] LNER, R. A theory of type polymorphism in program\u00adming. Journal oj Computer and System \nSciences 17 (1978). [15] MILNER, R., TOFTE, M., HARPER, R., AND MACQUEEN, D. The Definition of Standard \nML, Revised 1996. Forthcoming, 1996. [16] ODERSKY, M, A syntactic theory of local names. Technical Report \nYALEU/DCS/RR-965, Yafe Univemity, 1993. [17] ODERSKY, M., RABIN, D., AND HUDAK, P. Call by name, as\u00adsignment, \nand the lambda calculus. In ACM Symposium on Principles of Programming Languages (Jan. 1993), pp. 43\u00ad \n56. [18] O HEARN, P. W., AND TENNENT, R. D. Relational para\u00admetricity and local variables. In ACM Symposium \non Prin\u00adciples of Pmgmmming Languages (1993). [19] PETERSON, JOHN, ET AL. Report on the programm\u00ading \nlanguage Haskell (version 1.3). Technical RSpOrt YALEU/DCS/RR-1106, Yale University, 1996. [20] RABIN, \nD. Calculi for Functional Pmgmmming Languages with Assignments. PhD thesis, Yale University, 1996. Tech\u00adnical \nReport YALEU/DCS/RR-l 107. [21] RIECKE, J, G. Delimiting the scope of effects. In Confer\u00adence on Functional \nProgmmming and Computer Architec\u00adturrz (1993), pp. 146 155. [22] RIECKE, J. G., AND VISWANATHAN, R. Isolating \nside effects in sequential languages. In ACM Sympaaium on Principles of Progmmming Languages (1995), \npp. 1 12. [23] SABRY, A., AND FIELD, J. Reasoning about explicit and implicit representations of state. \nT@chnical Report YALEU/DCS/RR-968, Yale University, 1993. ACM SIG-PLAN Workshop on State in Programming \nLanguages, pages 17-30. [24] SWARUP, V., REDDY, U., AND IRELAND, E. Assignments for applicative languages. \nIn Conference on Amctiond Pm\u00adgmmming and Computer Architectunz (1991), pp. 192-214. [25] TALPIN, J., \nAND JOUVELOT, P. The type and effect disci\u00adpline. In IEEE Symposium on Logic in Computer Science (June \n1992), pp. 162-173. [26] TOFTE, M. Type inference for polymorphic references. In\u00adjomnation and Computation \n89, 1 (November 1990), 1-34. [27] TOFTE, M., AND TALPIN, J. Implementing the cafl-by-value cafculus using \na stack of regions. In ACM Symposium on Principles of Progmmming Languages (1994), pp. 188-201. [28] \nWADLER, P. Comprehending monads. In ACM Con~erence on Lisp and Functional Progmmming (1990), pp. 61 78. \n[29] WRIGHT, A. K., AND FELLEtSEN, M. A syntactic approach to type soundness. Technical Report 91-160, \nRice University, April 1991. Final version in Information and Computation 115 (l), 1994, 38-94. 238 \n   \n\t\t\t", "proc_id": "258948", "abstract": "Type safety of imperative programs is an area fraught with difficulty and requiring great care. The SML solution to the problem, originally involving imperative type variables, has been recently simplified to the syntactic-value restriction. In Haskell, the problem is addressed in a rather different way using explicit monadic state. We present an operational semantics for state in Haskell and the first full proof of type safety. We demonstrate that the <i>semantic</i> notion of value provided by the explicit monadic types is able to avoid any problems with generalization.", "authors": [{"name": "John Launchbury", "author_profile_id": "81100462557", "affiliation": "Oregon Graduate Institute, P.O. Box 91000 Portland, OR", "person_id": "PP39043890", "email_address": "", "orcid_id": ""}, {"name": "Amr Sabry", "author_profile_id": "81100016804", "affiliation": "Department of Computer Science, University of Oregon, Eugene, OR", "person_id": "P16266", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/258948.258970", "year": "1997", "article_id": "258970", "conference": "ICFP", "title": "Monadic state: axiomatization and type safety", "url": "http://dl.acm.org/citation.cfm?id=258970"}