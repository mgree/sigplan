{"article_publication_date": "08-01-1997", "fulltext": "\n Tupling Calculation Eliminates Multiple Data Traversals Zhenjiang Hu Hideya Iwasaki Department of Information \nEngineering Department of Computer Science University of Tokyo Tokyo University of Agriculture and Technology \nMasato Takeichi Akihiko Takano Department of Information Engineering Advanced Research Laboratory University \nof Tokyo Abstract Tupling is a well-known transformation tactic to obtain new efficient recursive functions \nby grouping some recursive func\u00adtions into a tuple. It may be applied to eliminate multiple traversals \nover the common data structure. The major diffi\u00adculty in tupling transformation is to find what functions \nare to be tupled and how to transform the tupled function into an efficient one. Previous approaches \nto tupling transfor\u00admation are essentially based on fold/unfold transformation. Though general, they \nsuffer from the high cost of keeping track of function calls to avoid infinite unfolding, which pre\u00advents \nthem from being used in a compiler. To remedy this situation, we propose a new method to expose recursive \nstructures in recursive definitions and show how this structural information can be explored for calcu\u00adlating \nout efficient programs by means of tupling. Our new tupling calculation algorithm carseliminate most \nof multiple data traversals and is easy to be implemented. 1 Int roduct ion Tupling [Bir84, Chi93] is \na well-known transformation tactic to obtain new efficient recursive functions without multiple tmversals \nover the common data structure (or multiple data traversals for short ), which is achieved by grouping \nsome recursive functions into a tuple. As a typical example, con\u00adsider the function deepest, which finds \na list of leaves that are farthest away from the root of a given tree: deepest (Leaf (a)) =: [a] deepest \n(Node(l, r-)) =: deepest (t), depth(l) > depth(r-) . deepest(l) ++-deepest(r), depth(l) = depth(r) . \ndeepest (r-), otherwise depth (Leaf (a)) ~~ o depth (Node(l, r)) =, 1+ maz(depth(l), depth(r))  The \ninfix binary function it concatenates two lists and the function mas gives the maximum of the two arguments. \nBe\u00ading concise, this definition is quite inefficient because deepest and depth traverse over the same \ninput tree, giving many Permission to make digital/hard copy of part or all this work for personal or \nclassroom use ia granted without fee provided that copies are not made or distributed for profit or commercial \nadvan\u00adtage, the copyright notice, the title of the publication and its date appear, and notice is given \nthat copying is by permission of ACM, Inc. To copy otherwise, to republish, to post on servars, or to \nredistribute to lists, requires prior specific permission and/or a fee. ICFP 97 Amsterdam, ND 0 1997 \nACM 0-89791 -918 -1/97 /0006 ...$3.50 Hitachi, Ltd. repeated computations in computing the depth of subtrees. \nIt, however, can be improved with tupling transformation by grouping deepest and depth to a new function \n(say cM), i.e. dd t = (deepest t, depth t), giving the following efficient program. deepest t let (u,v) \n= dd t in u dd (Leaj (a)) ~ ([a], O) dd (Node(l, r)) = (dpl, 1 + all), dl> dr\u00ad = (dpi i-t dpr, 1+ all), \ndl = d. [dvr. 1+ dr). otherwise where (dpl~ dl) = dd 1 (dpr, dr-) = ddr The main problem in the tupling \ntransformation is to find what functions are to be tupled and how an efficient definition for the tupled \nfunction is derived. Traditional approaches [Pet87, PP91, Chi93] to solving this problem are based on \nthe well-known fold/unfold transformations [BD77], using tupling analysis to discover an eureka tuple \nand using fold/unfold transformation to derive an efficient program for the tupled function. This is \nquite general but comes at price. In the fold/unfold transformation, it has to keep track of function \ncalls and to use clever control to avoid infinite unfolding. This process introduces substantial cost \nand complexity, which is actually prevented from being implemented in a real compiler of functional languages, \n To remedy this situation, we turn to another transforma\u00adtion technique known as program calculation \n[MFP91, SF93, MH95], which is based on the theory of Constructive Algo\u00adrithmic [Fok92]. Different from \nthe previous fold/unfold transformation whose emphasis is on the generality of trans\u00adformation process, \nprogram calculation deals with programs in some specific recursive forms, such as catamorphisrn, ana\u00admorphism \nand hylomorphism [MFP91, TM95, HIT96b], and performs transformation based on some local calculational \nlaws. Because of its simplicity of transformation process, program calculation turns out to be easier \nto be imple\u00admented. This work is greatly inspired by the success of applying the program calculation \ntechnique to the fusion transforma\u00adtion [GLJ93, LS95, TM95, HIT96b]. We would like to ex\u00adplore a further \npossibility to apply the technique of program calculation to tupling transformation, which to the best \nof our knowledge has never been examined. We are interested in this exploration for two reasons. First, \nwe believe that tupling transformation tactic should be more practical to be used in a compiler. Second, \nsince tupling and fusion are two most related transformation tactics [Chi95], it is quite natural to \nstudy tupling transformation in the framework where fusion transformation is studied. In this paper, \nwe demonstrate how to proceed tupling transformation by means of program calculation. Our main contributions \nare as follows. . First, we propose a new tupling algorithm to remove multiple data traversals in a program. \nIt is applica\u00adble to any lazy functional program. Two important features of our tupling algorithm are: \n We identify a class of functions called tuplable functions which are potentially suitable to be tu\u00adpled \nwith other functions and enjoy many useful transformation rules for tupling calculation.  We calculate \nan efficient definition for the tupled function in a rather cheap and mechanical way rather than by the \nexpensive fold/unfold trans\u00adformation,  . Second, our tupling algorithm is given in calculational forms. \nTherefore, our tupling algorithm preserves the advantages of transformation in calculational forms, as \nwe have seen in the discussion of shortcut deforesta\u00adtion in [GLJ93, TM95]. That is, our algorithm is \nvery general in that it can be applied to recursions over any data structures other than lists, and our \nalgorithm is correct and is guaranteed to terminate. . Third, our tupling algorithm can coexist well \nwith the shortcut deforestation. Both of them basically rely on the manipulation over catamorphisms. \nTherefore, it is natural to combine these two techniques in our frame\u00adwork. In contrast, the previous \nstudy on this com\u00adbination based on fold/unfold transformation [Chi95] requires more complicated control \nto avoid infinite un\u00adfoldings in case fusion and tupling are applied simul\u00adtaneously.  The organization \nof this paper is as follows. We begin by reviewing in Section 2 the basic concepts of the constructive \nalgorithmic in order to explain the Mutu Tupling Theorem which is the basis of our tupling algorithm. \nWe then pro\u00adpose our main tupling theorem in Section 3. To prove our main theorem, we first investigate \non transformation prop\u00aderties of tuplable functiom and find how to perform tupling transformation for \nthem in Section 4. And then we give our tupling calculation algorithm in Section 5. Related work and \ndiscussion are given in Section 6. Mutu Tupling Theorem Generally, tupling transformation is very complicated \nwhile its termination is far from being trivial [Chi93]. The cal\u00adculational approach that will be taken \nhere is less general, but should be more practical. We don t guarantee to re\u00admove multiple traversals \nover the same data structures by all functions (indeed in a general program we could not hope to do so), \nbut we do allow all legal programs = input and the transformed program will be expected to be made more \nefficient. Basically, our tupling transformation is based on a single simple rule, the Mutu Tupling Theorem \n[Fok89], which is weI1-known in the community of Constructive Al\u00adgorithmics. 2.1 Constructive Algorithmic \nTo understand the Mutu Tupling Theorem, the basis of our tupling algorithm, we should briefly review \nthe previous work [Hag87, MFP91, Fok92] on Constructive Algorithmic, explaining the basic concepts and \nthe notations that will be used in the rest of this paper. Functors In Constructive Algorithmic, polynomial \nendofunctors are used to capture the structure of data types, which are built up only by the following \nfour basic functors. . The identity functor I on type X and its operation on functions are defined as \nfollows. l.Y =X, If=f . The constant functor !A on type X and its operation on functions are defined \nas follows.  !Ax=A, !A~=id where id stands for the identity function. . The product X x Y of two types \nX and Y and its operation to functions are defined as follows. XXY = {(z, y)lz Ex, yEY} (f X9) (%?J) \n= (fz)9v) ml (a,b) = a 7T2 (a, b) =b (f A9)a = (fa, ga) . The separated sum X + Y of two types X and \nY and its operation to functions are defin-ed as follows. X+Y = {1} XX U{2}XY (f +9) (Lx) = (1, fz) (j \n+9) (2, Y) = (2,9Y) UV9) (u~) = f~ (fvg)(2, y) = gy. Although the product and the separated sum are \ndefmed over two parameters, they can be naturally extended for n parameters. For example, the separated \nsum over n pa\u00adrameters can be defined by Z~=lXi = U~=l({i} x X:) and (E~=~f:) (j, Z) = (j, f, z) for1< \nj<n. Data Types Rather than being involved in theoretical study which can be found in [Hag87, MFP91, \nFok92], we illustrate by some examples how data types can be captured by endofunctors. In fact, from \na common data type definition, an endofunctor can be automatically derived to capture its structure [SF93]. \nAs a concrete example, consider the data type of cons lists with elements of type A, which is usually \ndefined byl List A = Nil [ Cons (A, Lid A). 1Note that f~~notational convenience, we sometimes use [ \n1fOr ~~~ and infix operator : for Cons. Thus, for example, z : zs stands for Cons(z, ZS) and [a] for \nCons(a, Nil). In our framework, we si]all use the following cndofunctor to describe its recursive structure: \nFL,, =!l+!Ax Z where 1 denotes the final object, corresponding to 02. Be\u00adsides, we use inp~A to denote \nthe data constructor in List A: ZnF,. A  Nil v Cons. In fact, the List A is the least solution of X \nto the equation X = 2np~A (FL~ X) as discussed in [Hag87]. The in~~ has its inverse, denoted by OUtFLA \n: List A + FLA (List A~, which captures the data destructor of List A, i.e., outFLA = ks. case zs of \nNil -+ (l,()); Cons (a, as) + (2, (a, as)). Another example is the data type of binary trees with leaves \nof type A usuall~ defined by Zkee A = Leaj A I Node ( lTee A, ZYee A). The corresponding functor FT~ \nand data constructor inFTA are: FT~=!A+Z XI, inFTA = Leaf v Node. Cat amorphisms Catamorphisms [MFP91, \nSF93], one of the most important concepts in Constructive Algorithmic, form a class of im\u00adportant recursive \nfunctions over a given data type. They are the functions that promote through the type construc\u00adtors. \nFor example, for the cons list, given e and @ , there exists a unique catamorphism, say cata, satisfying \nthe fol\u00adlowing equations. cda[] = e cda (z:ZS) = z 6) (cata xs) In essence, this solution is a relabeling \nit replaces every occurrence of [] with e and every occurrence of : with @ in the cons list. Because \nof the uniqueness property of catamorphisms (i.e., for this example e and @ uniquely de\u00adtermines a catarnorphism \nover cons lists), we are likely to use special braces to denote this catamorphism as cda = {e v @)F.~. \nIn generaf, a catamorphism over any data type captured by functor F is characterized by: With catamorphisms \nmany functions can be defined. For example, the function sum, which sums up all the elements in a list, \ncan be defined as (O v @sD3. Catamorphisms are eficient and manipulable: they are efficient in the sense \nthat they traverse over the input data structure once; they are manipulable because they enjoy many useful \ntransformation properties [MFP91, SF93]. To help readers to get used to the notation in this paper, we \nStrictly speaking,Nil shouldbe writtenas Nal(). In this paper, the form of t () will be simply denoted \nas t. 3When o ambiguity happens, we usually omit the subscript f F in QI+)F, demonstrate how to inlinc \nsu~n = (jO v plus) into a familiar program by the following calculation. sum = (O v plus]  { catamorphism \ncharacterization } sum o inFL~ = (O v plus) o FLA sum  { ~nFL~ = (NIJv Cons), FL~f =zd+idx \\ } sum \no (Nil v Cons) = (Ov plus) o (id + id x sum) { Laws forv, +ando} (sum NiZ) v (sum o Cons) = O v (plus \no (id x sum))  { bylawsof v } sumNil=O; sumoCons= plus o (id x sum) That is, sum Nil =0 sum (Cons (x, \nZS)) = plus(x, sum ZS). 2.2 Mutual Recursions and Thpling Just like that the fusion in calculatiorral \nforms relies on a single Acid Rain Theorem [T M95], our tupling algorithm basically depends on the Mutu \nTupling Theorem [Fok89, Fok92] . Theorem 1 (Mutu Tupling) ~oinF=@oF(jAg), goinp=@o F(f Ag) fAg=[@A@)P \nThis theorem was originally devised for the purpose of manipulating mutual recursions by turning them \ninto sin\u00adgle catamorphisms. However, from the viewpoint of tupling transformation, it provides a simple \ncalculatiorral rule. It not only tells which functions should be tupled ~ and g should be tupled if \nthey are mutually defined traversing over the same data structure in a specific regular way, but also \ntells how to calculate out the definition for the tupled func\u00adtion tupling @ and @ within a catamorphism. \nTherefore, a direct advantage of applying the Mutu Tupling Theorem is that any possible multiple traversals \nover the same data structure by ~ and g in the original program can be suc\u00adcessfully eliminated, leading \nto an efficient program. As an example, recall the definition of deepest given in the intro\u00adduction, \nwhere deepest and depth are mutually defined and traverse over the same input tree. We can apply the \nMutu Tupling Theorem to obtain an efficient version. First, we rewrite them into the form as required \nin the theorem: deepest o inpTIn, = @ o FT1m,(deepest A depth) where @= @l v #2 @Ia = [a]   4J2((ti,ld),(tr,ifId> \nhr h-))= tl, = tl+ttr,ifld=hr = tr, otherwise depth o inF~,n, = @ o FT,n, (deepest A depth) where $=*1 \nv $2 @la=O @2 ((W W, (tr, hr)) = 1 + maz(hl,hr) Note that FT1m,f = id+ f xf and lnFT,mt= Leaf v Node. \nNow applying the Mutu Tupling Theorem soon gives the following efficient linear recursion: which can \nbe inlined to the efficient program as given in the introduction with some simplification. pr-g ::= \ndl;;d~ program d .. .. eql; ..; egn function definition eq ::= fp u., v.n=e equation for defining function \nf us ::= Vl(vsl> , vsm) argument e .. .. variable I ~e~,..,en) tuple expression I Av.. e lambda expression \nI el ea application I let VSl = ~1 el; . ~ ; v.. = ~n en in e let expression P .. variable i ;~l,...,pn) \ndata constructor pattern Figure 1. The language 3 A Pratt ical Tupling Theorem The Mutu Tupling Theorem \nis attractive in the sense that tupling transformation turns out to be a simple symbolic manipulation, \nas seen in its application to deepest in Sec\u00adtion 2. It, however, still far from being practical. The \nmost serious problem is that the functions to be tupled must be defined in a specified restrictive way, \nwhich cannot be ac\u00adcepted in our functional programming. Nevertheless, the Mutu Tupling Theorem actually \nguides us to give our main tupling theorem, which can be applied to any lazy functional program for eliminating \nmultiple data traversals. 3.1 Programs To demonstrate our techniques, we use the lazy functional language \ngiven in Figure 1. A program is a sequence of function definitions. Functions may be non-recursive or \nre\u00adcursive, and recursive functions are defined in a pattern\u00admatching style. This language is nothing \nspecial except that we assume that our recursive functions are inductively de\u00adfined over a single (not \nmultiple) recursive parameter and this recursive parameter is (without loss of generality) the first \nparameter of the function, though we expect our algo\u00adrithm to have a natural extension to the more general \ncase. To enhence readability, we take liberty to use some familiar syntactic sugars, such as the infix \nnotation, function com\u00adposition and even czwe analysis structure as in the definition of deepest. In \naddition, we assume that the renaming has been done whenever there is any danger of name-confhcts. As \nan example, consider the following program in our language naively solving the well-known repsort problem \n[Bir84]: transforming a binary tree into one of the same shape in which the leaf values have to be replaced \nwith those of the original tree but arranged in increasing order. repsort t = rep t (sort (leaves t [])) \n where rep (Leaf (n)) rrw = Leaf (lnwd mS) rep (Node(l, r)) ms = Node (rep 1 (take (size 1) rns), rep \nr (drop (size 1) ins)) leaves (Leaj (n)) x.9 = n : 2s lenves (Node(l, r)) zs = leaves i (leaves r ZS) \nsize (Leaj (n)) = 1 size (Node(l, r)) = plus (size 1, size r) In this program the tree is traversed \na fist time in order to discover and sort the list of leaf values. The tree is then traversed a second \ntime with function rep. This function selects appropriate thunks of the sorted sequence in order to pass \nthem on to the left and right subtrees (take k z takes the first k elements from list z and drop k z \ndrops the first k elements from list z). At each step, the number of values selected depends on the size \nof the left subtree, so implicit in the al orithm is a third traversal determining 5 sizes. This is \na O (n ) algorithm in the worst case, n being the number of leaves. This problem is of interest because \nthere are efficient but non-obvious programs aa given in [Bir84, Tak87]. We shall adopt it as our running \nexample and demonstrate how our tupling calculation can provide a systematic way to derive a similar \nefficient version without multiple data traversals, while only informal studies were given before. 3.2 \nMain Theorem To propose our main theorem, we should be more precise about multiple data traversals that \nare expected to be re\u00admoved by our tupling calculation. Definition 1 (Multiple Data Traversal) An expression \ne is said to have multiple data tmversals if there exist at least two occurrences of recursive calls \nlike fp and ~ p where p and p are pattern expressions and p is equal to or a sub-pattern of p . 1 The \nintuitive observation behind this definition is that the common data p would be traversed twice; once \nby ~ and again by f . Particularly, if ~ is the same as f , we usually say that the expression e has \nredundant recursive calls to f. For example, the following two expressions ~ (sort (leaves t [])) Node \n(~ (take (~) ins), rep r (drop (~) rns)) in our running program have multiple data traversals aa un\u00adderlined. \n4A pattern ~Wre~~ion is an expression constructed by variablm and data constructors, looklng like a pattern. \nThat s why we d like to use p to represent a pattern expression. We don t guarantee to eliminate multiple \ndata traversals by any functions in a program, because it is impossible to tuple any two arbitrary recursive \nfunctions. Therefore some restrictions on recursions should be placed. To this end, we shall define our \ntuplable functions that me potentially tuplable with other functions and enjoy many useful trans\u00adforrnat \nion properties (see Section 4) for calculating efficient versions for themselves and for a tuple of themselves. \nDefinition 2 (Tuplable Function) Assume that jl, . . .. ~~ are mutually defined by equations: ja P:j \n~s~ ..~~ml= e~j, (i=l,., ~;j=l, ... n:). The jl,. , ~~ are called tuplable (recursive) finctions, if \nfor every occurrence of recursive calls to ~1, , ~m in all e;j S, say f~ e{ el -.. enk, e is a sub-pattern \nof pij. . The restriction we impose on the definition of tuplable functions is that the recursive parameter \nof the tuplable functions should be strictly decreasing (i.e., a sub-pattern) across successive recursive \ncalls to themselves. For example, it is easy to check the two equations for the definition of rep and \nsee that rep is a tuplable function; e.g., for the second equation: rep (Node {l,r)} ms = Node (rep ~ \n(take (size 1) rns), rep z (drop (size t) m$)) the parameters of all the recursive calls to rep in the \nRHS as underlined me sub-patterns of Node (1, r), the pattern in the LHS. But it excludes the function \nlike joo defined by joo(zl : S2 : Zs) =Z1 +}00/2 * X2 : $S)+foofzl : X9} because two underlined parts \nare not sub-patterns of (ZI : 22: 2%). Though restricted, tuplable functions cover most of in\u00adterestingrecursive \nfunctions in our usual functional program\u00adming. As a matter of fact, they cover s~called mutumor\u00adphisms \n[Fok89, Fok92] which are considered to be the most general and powerful recursive form over data structures, \nincluding catarnorphisms and paramorphisms (primitive re\u00adcursive functions) [Mee92] w their speciaf cases. \nNow we are ready to give our main theorem. Theorem 2 (Main) All multiple data traversals by tup-Iable \nfunctions in a program can be eliminated by tupling calculation. . In other words, our main theorem says \nthat given a pro\u00adgram we can perform tupling calculation to obtain another equivalent version without \nmultiple data traversals by tupla\u00adble functions. We will prove the theorem later by proposing our tupling \ncalculation algorithm. Returning to our running example, we can calculate the following efficient version \nfor mpsort in which all multiple traversals over the tree struc\u00adture by tuplable functions rep, leaves \nand size are eliminated. repsort t = let (tll, t12) = 947, t in tll (sort (tlz [])) let ((r, s), 1) = \ng~l ~in (r, O (( Arns. Leaj (head rrw), 1), AZS. n : ZS) let ((r/, sl), U) = g$l 1 ((rr, sr), lr) = g~l \nr in ((hns. Node(ri (take S1T7zs), rr (drop S1 ins)), plus (s1, ST-)), Azs. (U (b Zs))) 4 Manipulating \nTuplable Functions Before addressing the proof of the main theorem, we should investigate transformation \nproperties of tuplable functions in order to manipulate them. 4.1 Standardizing Tuplable Functions into \nManip\u00adulable Form First of all, we standardize tuplable functions into a ma\u00adnipulable form by making \nfull use of finctors in capturing recursive calls to tuplable functions in their definitions. Lemma 3 \n(Standardizing) Every tuplable function ~ can be transformed into the following form: f=q$o(~hoot@a~2hoout~A \n. . . AF lhoout~) (1) where (i) h=flA..-AfnAglAgm,Agm, where~l,...,f~ denote functions mutually defined \nwith ~ and one of them is f, and gl, ..., gm denote tuplable functions in the definition of ~ while traversing \nover the same recursive data as ~; (ii) F =F l 0F and out; = F loutF oout; l; (iii) 1 is a finite natural \nnumber. Proof Sketch: Intuitively, out> cag be considered as un\u00adfolding i steps of input data, and F \nf o out~ can be con\u00adsidered as mapping t to all recursive parts of the input data that has been unfolded \ni steps. Therefore, this lemma reads that one can extract all the recursive calfs to the tuplable functions \nin the definition of f and embed them in the ex\u00adpression FhooutF a F2hoout~ A . . . A F hoout$. According \nto the restriction in the definitions of tuplable functions that the parameters of recursive calls to \nmutually\u00addetined functions, f 1,... , f., should be sub-parts of input, it soon follows that every recursive \ncalls to fi in the RHS can be embedded in a term F~(fl A . . . A fn)oouti forj E{l,2,... , 1}, where \n1 denotes the maximum number of unfolding steps of the input data for all recursive calls to f,,... , \nfn in the definition. Similarly, the recursive calls to other tuplable functions, say gl, ..., g~, which \nare on the same data as f can be embedded in an term ~j(gl A . . Agm)oou&#38;~ for j c {1, 2,. ... 1}5. \nTo summarize, all recursive calls on the same data M ~ in the original definition body can be covered \nby the expression 6 FhooutF A F2hoout~ &#38; . A FLhoout~. Thus ~ can be transformed into the form \nof (1) by using r#J to combine recursive calls with other parts forming the body of the original definition. \n. As an example, consider how to standardize the defini\u00adtion of tuplable function rep into the form of \n(1). First, we move the non-recursive parameters from the LHS to the RHS by lambda abstraction: rep (Leaf \n(n)) = Ares. Leaf (head rns) (2) rep (Node(/, r)) = Ares. Node(rep 1 (take (size 1) Tns){3) rep r (drop \n(size 1) ins)) Fkom the patterns in the LHS, we see that the input data need at most one step of unfolding, \nthus we have 1 = 1. The Standardizing Lemma tells us that rep can be described in the following form: \nrep = #rep o (F2-h o OUtFT) (4) Here, h should be a tuple of two parts; all functions mutually detined \nwith rep and all other tuplable functions traversing over the same tree as rep does, and thus h = rep \n&#38; size. Let s see how to calculate I#Jrep.Assume orep =01 v 1$2. Rp (Leaf(n)) = { Equation (4) } \n(@rep o (&#38;ho autFT)) (s%af (~)) { Definition Of OUtFT } (#rep o FTh) (1, n) = {~Tf=id+fXf} @rep (1,~) \n= { Assumption } du n rep (Node (1, r)) = { Equation (4) } (@rep o (FTh o cmt~=)) (Node (L r)) = { DSfinitiOIi \nof OIdJ? } (drep o FTh) (2, (L$ = {FTf=ict+fxf} #mp (2, (h L h r)) = { Assumption } 02 (h I,h r) { Definition \nof h } +2 ((rep A size) 1,(rep A size) r) = { Expansion } +?((repl, sizel), (repr, sizer))  Comparing \nthe above with Equations (2) and (3) gives: 41 n = Ares. Leaf (head ms) +2((rl,(rr,= kns. Node(rl (take \nsl rtas), s1),sr)) rr (drop S1ins)).  sf&#38;ner~]Yit is po~~ib]eforj < 0. But thk we can be ~ilY \nremoved through simple unfoldings of gl, . . ., g~, because gl, . . . g\u00adare t uplable functions. 61f \nthe$e data are applied by some non-tuplable finction, saY k e where k is a non-tuplable function, we \ninsert the special tuplable function id = (inF]F, identity function, between them as k (id e). In fact, \nthe above standardizing procedure can be made automatic without much difficulty, although the exact algo\u00adrithm \nis omitted here. Below are some other examples. leaves = ~lea~e~ o FT leaves o O%tF~ Where dleaves = \n(An.krs. n : ZS) v (A(l, r).ks. 1(rzs)) size = ~size 0 FT Size 0 O@F~ where ~size = 1 v PhAS An example \nthat needs unfolding of input data more than one step is the naive definition for jib function over the \nNat\u00adural Numbers (see Section 4.4). 4.2 Calculating Tuplable Functions The main advantage we take from \nthe above standardization process is that tuplable functions become manipulable. Figure 2 gives some \nuseful transformation rules for ma\u00adnipulating tuplable functions in the form of (l). Rule (Rl ) shows \nhow to increase 1, Rule (R2) shows how to add a new function hn+l to h , and Rule (R3) shows how to exchange \npositions of functions inside h. Furthermore, Rule (R4) gen\u00aderalizes the Mutu Tupling Theorem. In the \nrest of this section, we shall show how to improve tuplable functions through the elimination of multiple \ndata traversals by calculation. Let ~ be a tuplable function and z be the data over which ~ traverses. \nTake a look at a tuplable function ~ in our standard form: f=@o(FhooutF AF2hoout~A ... &#38;F hoout~). \n There are two possibilities that the definition may have mul\u00adtiple traversals over z: (a) his a tuple \nof several tuplable functions some of which are difTerent from ~. In this case, these tuplable func\u00adtions \ntraverse over the same z as ~.  (b) 1>1. In this case, the computation of Fhi o out~ po\u00ad  tentially \ncovers that of Fi+l h o out}+l (i.e., redundant recursive calls). Therefore, to remove multiple data \ntraversals of z in the defiltion of ~ requires us to find a suitable way to remove these two possibilities. \nTo remove the possibility (a), we derive from ~ a new tuplable function hma.. It tuples all tuplable \nfunctions that traverse over the same data structures as ~, including .f as part of its computation, \ni.e., j=rfohrn.z  where H denotes a projection function7 . To do so, zawmming that h= f] A... A f: j \nws reexpr-every tuplable function in h into our standard form: f; =~io(Fh~oodFAF2hio~t$A . . AF ihio~t$) \nRepeat this procedure for the new hi until we find 7We define a projxtio fintiion as an expression built \nby the projections rr{s, the identity function id, the function compaeition o and the product x. out\\) \n f=@ O(F(hIA -Ahn)OOUtFA . .. A@(hl A . Ahn)o (R2) f=   (@o(~~x x~l~)) o(~~oo~tFA A~J~oo4) where \n11=~1 A.., ATn,~=hlfJ...A6.6 h~+l A ~[(hl A h2) 00Ut~) f=@ O(~(hl Ah2)OOUtF A... (R3) j = (@o(Fez x.. \nx ~(ez))o (~(hz a hl) oout~ A . . . A ~~(hz A hl) oout~) where ex (z, y) = (y, x) f= #o(~(f Ag)oout~A \n~2(f Ag)oout~A .-AFf(j Ag)oout~) = @o(F(f Ag)oout~A F2(f Ag)oout~A ...AF{(fAg)oout~) 9 (R4) fAg=(~A \n@)o(F(f Ag)ooutF AF2(fag)oout~A . . AF1(fAg)oout~) I Figure 2. Basic Rules for Manipulating Tuplable \nFunctions covering all functions in h s part of each f~s definition. Now we return to prove Equation \n(5) by the following Now we may assume that f = ~ioh~a. for some i. By calculation. Rule (RI), (R2) and \n(R3), we adapt each definition of fj to h ~a= =q$oqoF(h~a= A u1 A . W~.=-l)OOUtF the form of { Definition \nof q } ~ hmaz = ~o(Fm A ~.. AFrlmo=)o f; = @\\o (Fh~~z o out~ A F2/z~=z Oold:A ...A F(hma= A ~1 A . \n. . A ~J~am_l) OO?JtF F*m hmoz Oout~rna=). ~ h{A} ma. @ ((Fnl oF(hmoz u, A . Au/~a=_l)) A According \nto the generalized Mutu Tupling Theorem (i.e., . Rule (R4)), we can tuple all of them and obtain: (Frl~~ \nO F(hmc= A UI A ... A U~m~= l)) )O OdF hmaz = @O(Fhmaz OoutF A ~  { Functor F and projection functions \n~i s } F2hnaz o Out$ ~ ... A hm.. = # o(Fhnm. A .A F~ zh~a= Ooutl&#38; ) Fu[m.. l)oOutF ~ h{A} where@=~\\A...A~~. \nm.. = rj o (Fhma. o OutF a . . . A FU~~.=-l o O~tF) After solving (a), we are able to deal with (b) by \nintro-~ { Expanding the definitions of ~i s } ducing lm~z 1 new functions U1,..., ul~o= 1: True UI \n= Fh~aZ OOUtF  In summary, we have the following theorem. U2 = Flbl OOUtF Theorem 4 (Optimizing Tuplable \nFunction) Let f be a tuplable function and z be the data over which f traverses. Wm.=-l f can be calculated \ninto an efficient version in the form of the = Ful~~=_2 Oi7dF composition of a projection function and \na catamorphism, It follows from the later proof that where multiple traversals over z by tuplable functions \nare eliminated. . hmaz =#oqo F(hmaz A tll A ... Wma=-l)OOUtF (5) To make our idea be more concrete, we \napply the above where q = Fnl A ... A Fnl~o.. By the Mutu Tupling algorithm for calculating an efficient \nversion of the tuplable Theorem, we can easily derive that function rep. Recall that we have reached \nthe point where ... h mom A ~1A A UJma=_l rep = @rep o F~(rep A size) o OUtF, = (I#oq A F T1 A .-. \nA FmJ~a=_ll)F. size = ~s;ze O FT SUe OOutFT. Therefore, f is transformed to We then know that h~a. is \nrep A size for this case. To find solution to h~a., we use Rule (R2) for adapting size to the f =(~iO~~)O(#O~A \nF~l A . . A l ~{~am-l]~ form so that the (generalized) Mutu Tupling Theorem can be applied:  a composition \nof a projection function and a catamorphism. size = #size o FTXZ o FT (rep A size) o outFT. Since both \nprojection functions and catamorphisms con\u00adtain no multiple data traversals of the input data, we come \nIt follows from the Mutu Tupling Theorem that to a new version of f without multiple data traversals \nover rep= mlo ~#rep A (~~~e ~T~2)DFT. the input. 4.3 Tupling !lMplable Functions we replace them and \nget Tuplable functions can be optimized by calculation as shown in Theorem 4, resulting in a form of \ncomposition of a projec\u00adtion function with a catamorphism. The tuplable functions in this form are suitable \nto be tupled among each other. That is, tupling these tuplable functions will give a tupla\u00adble function \nin this form again, = stated in the following theorem. Theorem 5 (Tupling Tuplable Functions) Let be \nn tuplable functions where Hi stands for a projection function. Then, 4.4 Another Example In this section, \nwe consider a classical example often used to illustrate the super-linear speedup achieved by the tradi\u00adtional \ntupling with the invention of a tuple of two functions. We shall demonstrate that such speedup can be \nobtained mechanically by our calculation over tuplable functions. A naive definition of the fibonacci \nfunction is: fib Zero = Zero fib (Succ(Zem)) = Succ(Zero) fib (Succ(lkx(n))) = ph@ib(Succ(rz)),jib(n)) \n where fib is a recursion over the natural number data type: IV= Zero I SUCC(N) which is defined by \nthe functor FN: jib = @o (Az. case z of Zero+ ((1, ()), (1,())) Succ(n) + ((2, jibn), (2, case n of Zero \n-+ (1, ()) Succ(n ) + (2,j lbn )))  On the other hand, note that the original definition of jib can \nbe transformed into the following using the case structure: jib = kc. case x of Zero -) Zero Succ(n) \n+ case n of Zero + Succ(Zero) Succ(n ) + plus(fib n,jibn ). Matching the above two definitions of jib \ngives the definition for @ @= N~)U).--(~,v) of ((1, ()), (1, ())) -+ Zero (( Zfl), (Z!/ ))-+ case y of \n( 1, ()) + Succ(Zero) (2, f,) + plus(f,, f,). Now according to Theorem 4, we get the result of A little \nsimplification and inlining of the catamorphism will lead to the following efficient linear program: \njibn =z where (z,y) =f n f Zero = (Zero, (l, ())) f (Succ(n)) = (case y of (1, ()) -+ Succ(Zero) (2, \nf2)+ Pwfl, f2), Rfl)) where (fl, y ) = f n in which all redundant recursive calls to fib due to multiple \ntraversals of the input have been successfully eliminated. Obviously, this definition gives an inefficient \nexponential al\u00adgorithm because of many redundant recursive calls to fib. Checking that jib is a tuplable \nfunction, we soon know that all redundant recursive calls to jib can be removed. To do so, as the first \nstep, we standardize the definition. The LHS of the definition of fib tells us that we need at most two \nsteps of unfolding of its input. Therefore, according to Lemma 3, jib should be standardized to the following. \nNoting that =!l+~ FN =!l+(!l+~) OldF~ = Az. case z of Zero+ (1, ()) Succ(n) -+ (2, n) out;N = Az. case \nx of Zero + (1, ()) Succ(rl) + (2, caae n of Zero + (1, ()) Succ(n ) + (2, n )) F: 5 Tupling Calculation \nAlgorithm We are now ready to prove our main theorem given in Sec\u00adtion 3.2. We shall propose a tupling \ncalculation algorithm which can eliminate ail multiple data traversals by tuplable functions by means \nof calculation. 5.1 The Algorithm Our tupling calculation algorithm haa been summarized in Figure 5. \nIt starts with the function to be optimized, aim\u00ading to attain a new veraion from it such that there \nare no multiple data traversals by any two tuplable functions. If the function to be optimized is a tuplable \nfunction, this is easy; just applying Theorem 4 to its definition to turn it into a composition of a \nprojection function and a catamorphism. Again we should remember to optimize the function inside the \ncatamorphkm. Otherwise, the function to be optimized is a non-recursive function, i.e., f p V.l ... vsn, \n=e  Tupling Calculation Algorithm T: 1. Start with the function to be optimized.  2. If the function \nto be optimized has been done, then return. 3. Ii the function to be optimized is a tuplable function, \naccording to Theorem 4 we can perform tupling calculation to turn it into the form of II o [+} where \nII denotes a projection function. Then, apply T for optimizing function @. 4. If the function to be \noptimized is a non-tuplable function, for each equation  fpv,l ..v,n,=e select out all calls to recursive \nfunctions in the form of f e in e and classify them into two sets: Cl for the calls to non-tuplable functions; \nCz for the calls to tuplable functions. . For each function in Cl, if it has not been optimized, apply \nT for its optimization. . For each function in Cz, if it has not been optimized, apply T for its optimization. \nWe then group recursive functions in Cz; in each group functions are applied to the same expression. \nLet ~1, . . . . LZ be groups we have got. For every group Ga= {g,l e,, . . . ,gini ei}, we can calculate \nan efficient version for gGi=g:l A... A gini according to Jkewm 5. Let t, 1, ~ , tin, be fresh variables \nfor each group, we  turn the original equation into: f pv,, . us , = let (tll)jtlnl)=9~, el (t~li \n , r*r) = 9G er in e[gijei I+ tij, fori=l, . . ..randj= l,. .,ni] Figure 3. The Tupling Calculational \nAlgorithm where e contains no occurrences of f, or is a recursive but not a tuplable function, i.e., \nWe then step to eliminate multiple data traversals by tupla\u00adble functions in e (or e~). This is achieved \nby optimizing all non-tuplable functions used in e (i.e., the functions in the set Cl) as well as all \ntuplable functions (i.e., the functions in the set C2), and then grouping optimized tuplable functions \nin Cz. Several remarks should be made on this grouping process. First, for the sake of simplicity we \nhave assumed that all expressions being applied by the tuplable functions in Cz are not overlapped. In \nother words, for any two calls fl el and fz ez in C2, el is not a subpattern of ez and vice versa. For \nexample, we do not allow the two calls of f, Z.s, f2 (z :2X) because zs is a subpattern of x : XS. But \nthis restriction can be removed. Recalling that any tuplable function f can be turned into H o f where \n~ = uq$~ according to Theorem 4, for a call like f (inF (el, . . . , en)), we can promote the tuplable \nfunction ~ into the expression inF (el, o.~,en), i.e., f (in~(el,.., en)) = lI(#(Ff (cl,.. ,en))) distributing \nf to some e; s by F. Second, thanks to the above assumption, we are able to restrict ourselves to the \ntupling of the functions that are applied to the same expressions in C2, as shown in the algo\u00adrithm. \nTo see how the algorithm works practically, consider our running example repsort. We start with optimizing \nrepsort. As it is not a recursive definition, we go to Step 2. For the equation repsort t = rep t (sort \n(leaves t)) we select out all calls to recursive functions in RHS and get two recursive calls, namely \nrep t and leaves t. As they are tuplable functions, we then have Cl = {} and Cz = {rep t, leaves t}. \nNow we process on Cz. First, we should optimize all tuplable functions, rep and leaves, that appear in \nC2. To optimize rep, we go to Step 3. As shown in Section 4.2, we have got the following solution for \nrep (after a straightfor\u00adward simplification): rep = ml o (J#j v &#38;] f#I~n = (km. Leaf (head ins), \n1) d; ((rl, s~), (rr, sr)) = (Ants. Node(ri (take S1 rns), rr (drop S1 ins)), plus (s1, sr)) Here we \nneed to optimize ~j and ~~ in order to get the final efficient version for rep. We don t address this \noptimization here, which is similar to repsort. In fact, if take and drop are defined as tuplable functions, \nwe can tuple them when optimizing +;. So much for rep. Similarly, we can optimize leaves and have the \nresult given in Section 4.2. Next, we should group recursive calls in Cz. Here we only have a single \ngroup G1 = {rep t, leaves t} in which all tuplable functions are applied to the same t. Sowe define g~~ \n= rep A leaves, Tab]e 1. Experimental Results (repsort) Input Size Time (sees/10 times) Allocations (bytes) \n(leaves no.) before tuphng after tuphng before tupling after tuphng 800 0.08 0.06 95,920 157,544 1,600 \n0.18 0.08 198,100 313,740 3,200 0.38 0.18 410,068 626,060 6,400 0.84 0.30 849,364 1,250,700 12,800 1.76 \n0.58 1,758,916 2,500,136 According to Theorem 5, we can calculate that = (m xid)o (I((@i v @i) ~r m) \nA (dqe~ ~~ ~T~2)DFT 9G1 By introducing two new variables tll and tlz, we thus have obtained our result: \nrepsort t= let (t~l, tlz) = gg, t in tll (sort (~lz [])). Summarizing all above, inlining the definition \nof ggl and a little simplification will lead to the familim program given in Section 3.2, where all multiple \ntraversals over trees have been successfully eliminated, which is as efficient aa those in [Bir84, Tak87]. \n5.2 Properties of the Algorithm Our tupling calculational algorithm enjoys many important properties. \nFirst, our algorithm is correct. This follows from the fact that each step of our transformation is based \non meaning-preserved transformation rules and theorems. Second, our algorithm terminates. This is because \n(i) the number of functions to be optimized by our algorithm is lim\u00adited for a given program. Basically, \nour algorithm optimizes those functions which are used directly or indirectly by the main function, and \n(ii) each application of transformation rufes in the algorithm terminates; e.g., the two significant \ntransformations based on Theorem 4 and 5 terminate as easily verified. Third, our algorithm guarantees \nthat all multiple data traversals by tuplabie function can be eliminated. It expects spectacular efficiency \nimprovement under the lazy evalua\u00adtion, in the sense that the new program obtained from the algorithm \nis faster than the original. But this speedup comes at the price of using some extra memory for tupling \nand with an assumption of existence of an efficient implementation of tuple. The most significant speedup \nattained from our tupling calculation algorithm is due to the removal of all redundant recursive calls \nto tuplable functions in a program. Take a look at the examples in this paper. Our algorithm: . eliminates \nthe redundant recursive calls to depth in deepest, giving a linear algorithm from the original quadratic \none; . eliminates the redundant recursive calls to fib, giving a linear algorithm from the original \nexponential one; . eliminates the redundant recursive calls to size in rep, giving a linear rep from \nthe original quadratic one.  To be more concrete, we evaluate two versions of repsort before and after \ntupling transformation in this paper using the Glasgow Ha.skell c~mpiler (version ghc-O.29).-Table shows \nthe experimental results. The input to the programs for our testing are balanced binary trees, thus the \nnumber of their leaves can be used as a measure of input size. The execution time and the allocations \nare obtained using the profiling mechanism provided by the Glasgow Haskell Com\u00adpiler. It can be seen \nthat with the size of the input tree going larger the new version obtained by our tupling trans\u00adformation \nbecomes much more faster than the original one, but with about 50% additional allocations. It is worth \nnoting that by the ordinary implementation of the tuple data structure [Pey88] our algorithm do not guar\u00adantee \nspectacular efficiency improvements. This is a com\u00admon problem among all existing tupling methods. Consider \nfor example the following average function: average z = sumz / length z where sum = joldr (+) O length \n= foldr (AzJy.1 + y) O which can be transformed into the following by our algo\u00adrithm: average x = let \n(s, 1) = sl x in sll where S1= foldr (krA(s,l).(z +s, 1 + 1)) (0,0). Obviously, average wins over average \nin that it reduces twice traversals of the input list z by sum and length respectively into once. On \nthe other hand it also increases time cost (besides space cost) for unpacking and packing a pair when \nS1traverses over x. If the cost is bigger than what we gain by tupling transformation, the transformed \nprogram will be slower than the original. The average indeed gives such an example. We will not be involved \nin solving this problem in this paper. It would be interesting to see that average should lead to efficiency \nimprovement (in time) under the assump\u00adtion that the program (~ o Fml A @ OF7r2] can be more efficiently \nimplemented than (14DFAWDF. This assumption requires an efficient implementation of tu\u00adple, avoiding \nthe cost for packing and unpacking of the tu\u00adpled value. 6 Related Work and Discussion The use of generic \ncontrol structures which capture patterns of recursions in a uniform way is of great significance in \nprogram transformation and optimization [MFP91, Fok92, SF93, TM95]. Our work is much related to these \nstudies. In particular, our work was greatly inspired by the success of applying this approach to fusion \ntransformation as studied in [SF93, GLJ93, LS95, TM95]. We made the tirst attempt to apply this calculational \napproach to the tupling transformation. Previous work, as intensively studied by Chin [Chi93], tries \nto tuple arbitrary functions by foid/unfold transformations. In spite of its gen\u00aderality, it has to keep \ntrack of all the function calls and devise clever control to avoid infinite unfolding, resulting in high \nruntime cost which prevents it from being employed in a real compiler system. We follow the experience \nof work of fusion in calculational forms [TM95, HIT96b] and base our tupling theorem on a simple calculational \nrule: the Mutu Tupling Theorem. We define a class of tuplable functions which is more general than Chin \ns TO Ckiss[Chi93] in that we allow the parameters other than the recursive one to be arbitrary. Chin \nneeds such restriction in order to guarantee the ter\u00admination of fold/unfold transformation, while termination \nis not a problem in our approach. In [Chi93], Chin focused on the tupling analysis to find what functions \nshould be tu\u00adpled but he didn t show how the efficient program can be obtained in a more systematic way. \nIn sharp contrast, we propose a concrete tupling algorithm to calculate efficient version eliminating \nmultiple data traversals. Though being simple and less general than Chin s, our tupling transfor\u00admation, \naa demonstrated, can be applied to improve a wide class of functions. More important, our algorithm should \nbe easily implemented, which promises to be used in a practical compiler. Tupling and fusion are two \nmuch related transformations for improving functional programs. It is worth noting that our tupling algorithm \ncan well coexist with fusion under the transformation in calculational form. Our tupling al\u00adgorithm improves \nthe recursion by constructing a catamor\u00adphism (Theorem 4), making ease for fusion transformation. A relevant \nstudy can be found in [HIT96a] where tupling and fusion are used together to derive list homomorphisms \n(i.e., catamorphisms over append lists). Elimination of multiple traversals over data structures has \nbeen studied for a long time. Our work is related to these works. Bird [Bir84] suggested the use of circular \npro\u00adgrams. Takeichi [Tak87] used a different technique called lambda hoisting with introduction of common \nhigher order functions. In particular, we are much influenced by Pet\u00adtoroasi s work [Pet87] of using \nlambda abstraction in con\u00adjunction with the tupling tactic. However, the transfor\u00admations in the previous \nwork require more or less human\u00adinsights, which are hard to be made automatic. Other re\u00adlated work includes \nmemorization [Mic68, Hug85], tabulation [Bir80, Coh83, CH95] and incremental algorithms [Liu96]. All \ntransformation algorithms introduced in this paper have been implemented in a rapid prototyped way. It \nis completely mechanical and does not rely on heuristics. Al\u00adthough we have to wait for the detailed \nexperimental results to say that this system is effective for practical programs, we are absolutely convinced \nthat our calculational approach to tupling transformation makes a good progress in code op timization \nof functional programs. In addition, our tupling calculational transformation is expected to be added \nto the HYLO system [OHIT97], a calculational system for improv\u00ad ing functional programs, which is now \nunder development in the University of Tokyo. Acknowledgement This paper owes much to the thoughtful \nand helpful dis\u00adcussions with Fer-Jan de Vries, Masami Hagiya, Muzuhito Ogawa and other members of the \nTokyo CACA seminars. Thanks are also to referees who provided detailed and help ful comments. References \n[BD77] R.M. Burstall and J. Darlington. A transformation system for developing recursive programs. Journal \nof the ACM, 24(1):44 67, January 1977. [Bir80] R. Bird. Tabulation techniques for recursive pro\u00adgrams. \nACM Computing Surveys, 12(4):403-417, 1980. [Bir84] R. Bird. Using circular programs to eliminate mul\u00adtiple \ntraversals of data. Acts Informatica, 21:239 250, 1984. [CH95] W. Chin and M. Hagiya. A transformation \nmethod for dynamic-sized tabulation. Acts Informatica, 32:93-115, 1995. [Chi93] W. Chin. Towards an automated \ntupling strategy. In Prac. Conference on Partial Evaluation and Pro\u00adgram Manipulation, pages 119-132, \nCopenhagen, June 1993. ACM Press. [Chi95] W. Chin. Fusion and tupling transformations: Syn\u00adergies and \nconflits. In Prac. Fuji International Workshop on llmctional and Logic Progmmming, pagea 106-125, Susono, \nJapan, July 1995. World Scientific. [Coh83] N.H. Cohen. Eliminating redundant recursive calls. ACM Transaction \non Progmmming Languages and Systems, 5(3):265-299, July 1983. [Fok89] M. Fokkinga. Tupling and mutumorph~ms. \nSquig\u00adgolist, 1(4), 1989. [Fok92] M. Fokkinga. Law and Order in Algorithmic. Ph.D thesis, Dept. INF, \nUniversity of Twente, The Netherlands, 1992. [GLJ93] A. Gill, J. Launchbury, and S. Peyton Jones. A short \ncut to deforestation. In Proc. Conference on Functional Programming Languages and Com\u00adputer Architecture, \npages 223 232, Copenhagen, June 1993. [Hag87] T. Hagino. Category Theoretic Approach to Data !!@pes. \nPh.D thesis, University of Edinburgh, 1987. [HIT96a] Z. Hu, H. Iwasaki, and M. Takeichi. Construc\u00adtion \nof list homomorphisms via tupling and fusion. In 21st International Symposium on Mathemati\u00adcal Foundation \nof Computer Science, LNCS 1113, pages 407 418, Cracow, September 1996. Springer-Verlag. [HIT96b] Z. \nHu, H. Iwasaki, and M. Takeichi. Deriv\u00ading structural hylomorphisms from recursive defini\u00adtions. In ACM \nSIGPLAN International Conference on Functional Programming, pages 73 82, Philadel\u00adphia, PA, May 1996. \nACM Press. [Tak87] [TM95] [Hug851.-. J. Hughes. Lazy memo-functions. In Proc. Con\u00adferenc; on Functional \nProqmmminq Lanquaqes and Computer Architecture (LNCS 201) ,pag~s 1~9-149, Nancy, France, September 1985. \nSpringer-Verlag, Berlin. [Liu96] Y.A. Liu. Incremental Computation: A Semantics-Based Systematic i%ansforrnation \nApproach. PhD thesis, Department of Computer Science, Cornell University, 1996. [LS95] J. Launchbury \nand T. Sheard. Warm fusion: De\u00adriving build-catas from recursive definitions. In Proc~ Conference on \nFunctional Programming Lan\u00adguages and Computer Architecture, pages 314-323, La Jolla, California, June \n1995. [Mee92] L. Meertens. Paramorphisms. Computing, 4(5):413-424, 1992. Formal Aspects of [MFP91] E. \nMeijer, M. Fokkinga, and R. Paterson. Func\u00adtional progr amming with banan=, lenses, envelopes and barbed \nwire. In Proc. Conference on Func\u00adtional Programming Languages and Computer Ar\u00adchitecture (LNCS 523), \npages 124 144, Cambridge, Massachusetts, August 1991. [MH95] E. Meijer and G. Hutton. Bananas in space: \nExte\u00adding fold and unfold to exponential types. In Proc. Conference on Functional Programming Lan\u00adguages \nand Computer Architecture, pages 324-333, La Jolla, California, June 1995. [Mic68] ~at~4~18,~~~~ 1 f~:~ions \n,. and machine learning. IOHIT971 Y. Onoue. Z. Hu. H. Iwaaaki. and M. Takeichi. At\u00ad~alculational fusiori \nsystem ~LO. In IFIP TC 2 Working Conference on Algorithmic Languages and Calculi, Le Bischenberg, France, \nFebruary 1997. Chapman8cHa11. [Pet87] A. Pettorossi. Program development using lambda abstraction. In \nZnt 1 Conf. on Foundations of Soft\u00adware Technology and Theoretical Computer Science, pages 420-434, Pune, \nIndia, 1987. Springer Verlag (LNCS 287). [Pey88] S.L. Peyton Jones. The implementation of func\u00adtional \nprogmmming languages. Prentice-Hall, 1988. [PP91] M. Proietti and A. Pettorossi. Unfolding-definition\u00adfolding, \nin this order, for avoiding unnecessary variables in logic progrrarns. In 3rd Int 1 Symp., PLILP 91, \npages 247 258, Paasau, Germany, Au\u00adgust 1991. LNCS 528. [SF93] T. Sheard and L. Fegaraa. A fold for all \nseasons. In Proc. Conference on Functional Programming Lan\u00adguages and Computer Amhitecture, pages 233-242, \nCopenhagen, June 1993. M. Takeichi, Partial parametrization eliminates multiple traversals of data structures. \nActs Infor\u00ad matica, 24:57 77, 1987. A. Takano and E. Meijer. Shortcut deforestation in calculational \nform. In Proc. Conference on Func\u00adtional Programming Languages and Computer Ar\u00adchit ect ure, pages 306 \n3 13, La Jolla, California, June 1995.  \n\t\t\t", "proc_id": "258948", "abstract": "Tupling is a well-known transformation tactic to obtain new efficient recursive functions by grouping some recursive functions into a tuple. It may be applied to eliminate multiple traversals over the common data structure. The major difficulty in tupling transformation is to find what functions are to be tupled and how to transform the tupled function into an efficient one. Previous approaches to tupling transformation are essentially based on fold/unfold transformation. Though general, they suffer from the high cost of keeping track of function calls to avoid infinite unfolding, which prevents them from being used in a compiler.To remedy this situation, we propose a new method to expose recursive structures in recursive definitions and show how this structural information can be explored for calculating out efficient programs by means of tupling. Our new tupling calculation algorithm can eliminate most of multiple data traversals and is easy to be implemented.", "authors": [{"name": "Zhenjiang Hu", "author_profile_id": "81100253989", "affiliation": "Department of Information Engineering, University of Tokyo", "person_id": "PP15027466", "email_address": "", "orcid_id": ""}, {"name": "Hideya Iwasaki", "author_profile_id": "81100065591", "affiliation": "Department of Computer Science, Tokyo University of Agriculture and Technology", "person_id": "P110505", "email_address": "", "orcid_id": ""}, {"name": "Masato Takeichi", "author_profile_id": "81100466948", "affiliation": "Department of Information Engineering, University of Tokyo", "person_id": "PP15032927", "email_address": "", "orcid_id": ""}, {"name": "Akihiko Takano", "author_profile_id": "81100419988", "affiliation": "Advanced Research Laboratory, Hitachi, Ltd", "person_id": "PP14148460", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/258948.258964", "year": "1997", "article_id": "258964", "conference": "ICFP", "title": "Tupling calculation eliminates multiple data traversals", "url": "http://dl.acm.org/citation.cfm?id=258964"}