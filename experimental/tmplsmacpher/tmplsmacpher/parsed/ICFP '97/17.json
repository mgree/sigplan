{"article_publication_date": "08-01-1997", "fulltext": "\n Tj-pe SIJecialisation for Imperative Languages Dirk Dussart* John Hughest Peter Thiemann~ Abstract We \nextend type specialisation to a computational lambda calculus with first-class references. The resulting \nspecialiser has been used to specialise a self-interpreter for this typed computational lambda calculus \noptimally. Furthermore, this spccialiser can perform operations on references at spe\u00adcialisation time, \nwhen possible. Keywords: program transformation, specialisation, type systems, monads.  1 Introduction \nPartial evaluation is an important program specialisation technique [18]. Partial evaluation is a program \ntransforma\u00adtion, which transforms a program and parts of its input (the static part) into a new program. \nThe partial evaluator per\u00adforms the operations for which enough data is available and reconstructs the \nrest. A partial evaluator operates on both wakes and symbolic values (code or values or a combina\u00adtion \nof code and values), whereas an interpreter operates only on values. Partial evaluators appear in two \nvariants, online and ofline. In online partial evaluators all speciali\u00adsation decisions are taken by \ninspecting the symbolic values, whereas in the o%line variants aU specialisation decisions are taken \nbefore actually running the specialiaer. The decisions are communicated to the specialiser by means of \ntwo-level annotated programs [27]. These annotations can be intro\u00adduced by hand or by mesns of a binding-time \nanalysis. The specialize, in this case, 1s an interpreter for two-level pro\u00ad grams. Type specialisation \n[17] is an extension of offline partial evaluation, where the specialiser is defined by a collection \nof Department of Computer Science, K.U.Leuven, Celestijnenlaan 200A, B-300 1 Leuven, Belgium. Dirk,OussartQcs.kuleuven. \nac. be. Supported by the Belgian National Fund for Scientific Research (N. F. W.O.). This work was (done \nwhile visiting Chalmer~. tchalmer~ Rknish H@kola, S-41296 G6teborg, Sweden. rjmh@cs. chtdners. se. : \nWilhelm-Schickard-Instit ut fiir Informatik, Univer\u00ad aitiit Tiibingen, Sand 13, D-72076 Tiibingen, Germany. \nthiemrmn@inforn atik. unl-tue Qingen. de. This work was done while visiting Chalmers. Permission to make \ndigital/l>ard copy of part or all this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for profil or commercial advan - Iage, the copyright notice, \n{he title of Ihe publication and its dale appear. and notice is given that copying w by permission of \nACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute to lists, require: \nprior specific permission and/or a fee ICFP 97 Amsterdam, ND @ 1997 ACM O-89791 -91 fl-l K1710006 ..$3.50 \nnon-standard typing rules. The rules specify specialisation judgments, I 1-e : ~ Q e : /, which denote \ntransforma\u00ad tions of expressions e of type ~ into expressions e of type T . e is the code part or dynamic \npart of the result of ap\u00ad plying the transformation, and ~ plays the same rde as the symbolic values \nin partial evaluation, and may cent ain val\u00ad ues. The static Darts can be used in static comrrutations \nand the dynamic parts are treated as black-boxes. In this setting, specialisation corresponds to building \na proof of a specialisation judgement. The advantage over partial evrdu\u00adation is that symbolic values, \nthe residual types, are propa\u00adgated via type unification The combination of a rich lan\u00adguage of residual \ntypes and the superior value propagation strategy, type unification, has enabled type specialisation \nto achieve optimal specialisation of a self-interpreter for a typed lambda calculus (171. a feat that \nhas not been achieved with standard partial ev~uation. Optimal specialisation in this context means that \nspecializing the interpreter with respect to some term yields the same term up to a-conversion, The problem \nhere is the presence of a universal type in the self\u00adinterpreter. Standard partial evaluation is not \nable to get rid of the tagging and untagging operations of the universal type [19]. Thw work serves two \npurposes. First, we show that type specialisation is modular with respect to the addition of new type \nconstructors (here: references and computations). Sec\u00adond, we show that type specialisation is not restricted \nto purely functional languages, but that it also applies to spe\u00adcializing operations with computational \neffects, such as per\u00adforming operations on references at specialisation time for a language like ML. \nMore formally, we extend type special\u00adisation to an extension of Moggi s computational met al an\u00adguage \nA-[ with first-class references. We demonstrate the usefulness of this extension by instantiating it \nwith the store monad. The result is a type specialiser for A~l, that is Amt extended with ML-style operations \non references. A dis\u00adtinguishing feature of our specialiser is the fact that it can Derform some oDerationa \non references staticsh. This sDe\u00ad .. cmhzer achieves optimal specialisation for a sel~-interpreter for \nA~l and it also yields satisfactory results on other prob\u00adlems (to be described below). Overview For \nthe reader unfamiliar with type specialisa\u00adtion, we start with a short type specialisation primer. Fur\u00adther \ninformation can be found in the original paper on type specialisation [17]. Then we introduce the language \nA~l, an extension of Moggi s computational lambda calculus with  1More information can be found in \nSection 2 204 first-class references. As a first step towards the full spe-by reducing expressions \nto their normal form, and dynamic cia.liser, we state the specialisation rules for the treatment of dynamic \nreference operations. We apply this specialiser to a self-interpreter for A~l and achieve optimal specialisa\u00adtion. \nIn a second step we add specialisation rules for static references and apply this specialiser to an interpreter \nfor a lazy functional language that implements laziness using updatable closures. Next, we discuss the \naddition of store prompts and state threads and state the new postprocess\u00ading rules. Finally, we assess \nour work with respect to partial evaluation, consider relatecl work, and conclude. 2 Type Specialisation \nPrimer Type specialisation is a form of program specialisation in which types play a central r61e. Rather \nthan saying that one term specialises to another term, we say that a source term and type specialise \nto a residual term and type: e : r _ e : #. Source types carry information to control specialisa\u00adtion, \nin particular binding-times: most type formers come in static and dynamic variants, and we mark the dynamic \nones by underlining as usual. Residual types are sufficiently re\u00adfined to carry all static information, \nwhich means that static values need not appear at all in residual terms. For exam\u00adple, static integer \nconstants are specialised to dummy val\u00adues, 42 : int L+ . :42, where the residual type records which \ninteger the dummy value represents. The residual type in\u00adformation is enough to specialise expressions \nthat use at atic values appropriately: for example, the lift operation, which converts a static integer \nto a dynamic one, specialises to the constant value given by the residual type of its argument: lift \n(2+3) :i@+5:int. Well-typedness of the source term can be checked once and for all before specialisation, \nbut well-typedness of the residual term can be checked only as it is constructed, dur\u00ading specialisation. \nThus the speciali.ser embodies a residual type-checker, and it is the unifications in this type-checker \nthat propagate static information during specialisation. For example, specializing (where Q is dynamic \nfunction application) yields let f= Az.4 in f . : int where unification assigns z the residual type \n3, and this is sufficient information to perform the static addition and specialise the lift to 4. Type \nspecialisation can propagate static information more effectively than partial evaluation can. Modifying \nthe example slightly, we can specialise lktf=~z. z+l &#38;lift (f Q3):@ to let f=~z. min4:int since \nunification assigns z the residual type 3, which en\u00adables the static addition to be performed, allowing \nf to be assigned residual type 3 ~ 4, which enables the lift to be specialised to a constant. This can \nbe done eoen though f ia a dynamic function, whose calla mag not be unjolded. Conventional partial evaluators \ndo not allow dynamic func\u00adtions to return static results, since they derive static values function applications \nmust not be reduced. Type specialisation is inherently monouariant; if we modify our example to then \nit can no longer be specialised, because f would need to be assigned both 3 ~ int and 4 ~ int as residual \ntypes. But polyvariance is easily added via a new source type poly T, with constructor poly and selector \nspec. An expression poly e specialises to a t uple of specialisations of e, while spec e specialises \nto a selection from the tuple. If we rewrite our example as ~f =poly~z.lift (z+l)kspec fQ3*specf Q4:~ \nthen it specialises to let f= (Jz.4, Jz.5) in m f . +m f . : int where f has residual type (3 ~ int, \n4 ~ int). The resid\u00adual type information is enough to enable spec to choose the right version at each \ncall, and conversely the type informw tion at each spec is enough to enable poly to decide which versions \nto create. Alternatively, we could change f in the example into a 8tatic function let f= Az.lift (z+l)inf3ff4: \ni&#38;t (note that the A and the applications are no longer under\u00adlined). Static functions are unfolded \nduring specialisation, so in this example each call of f is replaced by a speciali\u00adsation of its body, \nand of course the two speaalisations can be different. In order to unfold static function applications, \nthe specialiser represents a static function as a closure. The static parts of the closure (bound variable \nname, body, and residual types of the free variables) make up the residual type of a static ~, while \nthe dynamic part (a tuple of the free variable values) becomes the residual term. The resid\u00adual type \nof a static function says nothing about the type of its argument or result, and so such a function can \nbe freely applied to different static arguments at different calls. In this example ~ has no free variables, \nand so its residual term is the empty tuple (or dummy value). The result of specialisation is let f=cin4+5:int \n As these examples reveal, static computations leave an undesirable residue of dummy values in the specialised \npro\u00adgrams. But an optimal specialieer must remove static com\u00adputations completely. Fortunately, most \ndummy values can be removed by a post-processing phase we call eoid eraaure. Completely static expressions \nyield residual types with only a single element, for example 3, 4, 3 ~ 4, or 3 x 4. All such expressions \ncan safely be replaced by the dummy value ., and all such types by void. Now using the isomorphisms void~rzr \nvoidxrzr we can eliminate void components, void parameters, and of course let-bound variables of void \ntype. Void erasure often 205 .._ e .. zlnllift elAz. el eel let z=einel Az. ele Qel l@. z=eimelwhelspec \ne terms T ::= intlinlr-+rl~fi71poly T types  r,z:T1 z:T J71-n:int I ~e:int rl e:~ rl e:poly~ I /-lifte:i@ \nr 1-poly e : poly T I 1-spece:r I ,2:~11--e:n I l-el:~l-+n rl ez:rl I &#38;el :71 I ,2:r1~e2:n I ~e1e2:~2 \nI 1-letz=elinez:m  ~~l ~z Figure 1: The Two-level Source Language r ::= n [ int \\T -+ r [ (r . . . \nr ) residual types I ~e:int-e :nr+n:int~.:n l ,z:~-e:~ l-z:~~e:~ I }lifte:i@-n:int Figure 2: Basic simplifies \nresidual programs drastically; for example, the last two specialisations above become just let f=(4,5) \ninmj+mzf:int and 4+5:int Optimal specialisation of typed interpreters demands that wc specialise the \nuniversal type that such interpreters use to represent values appropriately. For example, an in\u00adterpreter \nfor Xcalculus with integers might use the type data Univ= Num ~ \\Fun ( Univ z Univ) to represent values. \nThis is an example of a static Sum type: the constructors encode the type of int erpreted values, and \nmust be known statically. We therefore embed such static constructors into residual types, rather than \nresidual terms. For example Num (lift (1 + 2)) : Univ I-+ 3: Num int The residual type records the fact \nthat a Num constructor was applied, and so it need not appear at all in the residual term. Case expressions \nover static sums can be simplified to one branch, given the residual type of the inspected ex\u00adpression, \nand so neither run-time type tags, nor run-time tag checks, need appear in the residual programs. An \nin\u00adterpreter for A~l that can be specialised optimally will be given below. Specialisation Rules Of \ncourse, partial evaluators can also imriement static .\u00ad sum types, and keep track of constructors statically. \nBut to specialise interpreters optimally, we need to be able to return values of a static sum type from \na dynamic jhction. For the type specialiser which is deriving static information by type inference, this \nis no problem. For a partial evaluator which is using reduction, it is impossible. In Figure 1 we give \nthe syntax and tw~level type system for a large fragment of our meta-language. It is essentially a two-level \nversion of the simply-typed A-calculus. The full language also includes sums and products, which we have \nomitted here. We allow types to be recursive with no spe\u00adcial syntax. The point we wish to emphasise \nhere is that types can be &#38;e/y formed according to the syntax given; there are no well-formedness \nconditions, such as that the argument and result of a dynamic function type must them\u00adselves be dynamic. \nNeither are there side conditions in the typing rules, requiring for example that the variable bound \nin a dynamic let itself be of a dynamic type. Such extra conditions are an essential part of other two-level \nJ-calculi; they are needed to make partial evaluation possible, The key advantage oft ype specialisation \nis that these conditions can be dropped. The standard semantics of the meta-language is just the usual \nsemantics for the (call-by-name) A-calculus: static and dynamic constructs are not distinguished, and \nboth poly e and spec e have the same semantics as e. These are anno\u00adtations to control specialisation, \nand as such do not affect 206 e .. .. .lqelmletz~eine T .. .. lMr r/ e:r 17+el:itfrl I ,2:r1[-e2:Afn \nI +qe.:kf~ I /-mlet z*elinez:Afm Figure 3: The Computational Metalanguage the meaning of the original \nprogram. Type specialisation can be specified quite simply via spe\u00adcialisation ruiea that let us infer \njudgments of the form r ~e:~f--ie :~ where the context 17 tells us how to specialise variables, via a \nsequence of entries of the form z : u C+ e : u . A Belection of specialisation rules appears in Figure \n2; we have omitted the rules for static functions, which are a little more compli\u00adcated to express. Compare \nthe rules for static and dynamic let expressions: the only difference is that a static let is un\u00adfolded, \nwhile a dynamic one is not. Precisely the same static information about the bcmnd variable z is available \nin both cases. Again, this is in contrast to partial evaluation, where a variable bound by a dynamic \nlet must itself be dynamic. The specialisation rules are more complicated to imple\u00adment than the usual \ntype inference rules, because they are not all syntax-directed. A description of the implementa\u00adtion \nis beyond the scope of this paper, but can be found elsewhere [17]. 3A Metalanguage with Effects Languages \nwith effects nre invariably harder to treat for\u00admally than purely functional languages. One reason is \nthat evaluation order suddenly becomes import ant, even for con\u00adstructs in the pure subset. Adding references \nand unre\u00adstrict ed assignment to cmr met alanguage would therefore change the semantics of every construct, \nand potentially in\u00advalidate all the specialisation rules presented above. To avoid this, we have chosen \nto work with Moggi s com\u00adputational met alanguage AA [25], which explicitly distin\u00adguishes between a \nvalue oft ype ~, and a computation (with effects) delivering a result of type r. The latter is assigned \na computation type ikf ~. Trivial computations (with no ef\u00adfects) can be created by the unit operator \nq, and the results . of computations can be inspected using relet z ~ el m ez, which combines the comput \nat ions el and e~ in sequence, and binds z to the value that el delivers in the evaluation of ez. The \nextensions to the syntax and type-system of our meta-Ianguage are shown in Figure 3. If el and ez are \ncomputations with effects, then it is a type error to write el + C2. Instead, we must write  relet 21 \n* el in relet 22 + ez in q(21 + 22) making the evaluation order explicit. Similarly, to paas the result \nof el to a function ~, we must write relet z ~ elin ~z, making it explicit that el is evaluated before \nthe call. To write f el instead would not compute el first: it would pass the entire computation as a \nparameter to ~, to be invoked later (or not) at ~ s pleasure. Think of a value of type M r asa suspended \ncomputation, that can be invoked by relet. Many different kinds of effects can be represented in Moggi \ns -framework, depending on how the types M T and operators q and relet are interpreted. To represent \nside ef\u00adfects on a store for example, we can interpet a computation as a function from the store beforehand \nto a result and the store afterwards: Mr = stO~ + (T, store) qe = h.(e, 8) relet z * el in ez = As.let \n(2,8 ) = el sin ezs Note that the translation passes the store that el produces to ez, and that z is \nin scope in ez. Operations to create and assign to references are interpreted as suitable functions with \na computation type. An interpretation for M, v and relet is called a monad, provided it satisfies the \nso-called monad law9: relet z ~ q el in ez = letz=elinez mletz4=einqz = e relet z * (relet u * el in \nez) in es= relet y * elinrelet z * ezin es where in the third law, y must not occur free in es. Haskell \nprovides imperative operations in exactly this way, but of course other imperative languages do not dis\u00adtinguish \ncomputations explicitly. However, it is well known that a ~-calculus with implicit effects can be simply \ntrans\u00adlated (in several different ways) into A~l, each translation making a different evaluation order \nexplicit [15]. By ex\u00adtending the call-by-value translation we can map, say, ML or Algol programs into \nour metalanguage. Thus we claim that type specialisation is also applicable to such languages; they simply \nhave to mapped into an intermediate form which makes evaluation order explicit first, Moggi s language \nis a conservative extension of the ,4\u00adcalculus; the meaning oft he pure subset remains unchanged. Isomorphisms \nsuch as void ~ r R T remain true if a function with this type has effects, then T must be a com\u00adputation \ntype, and both sides represent a suspended com\u00adputation. This is its big advantage for us: the type spe\u00adcialisation \nand void erasure rules for the pure metalanguage remain valid when effects are added. We need only define \nnew rules for the new constructs. The specialisation rules for q and relet cannot be given yet: they \ndepend on which kind of effects we are interested in. But we can note that these operations do not come \nin static and dynamic versions. We will indeed be interested in both static and dynamic effects, but \nsince both kinds may be used in one and the same computation, we need only one computation type, and \ntherefore only one version of the monad operators. 207 Figure 4: Operations on Dynamic References T \n::= .[refr residual types I +.5:7-e :# r Fne:M~+ne :MT I Jel:Mrl-e~:Mr~ 17}mletz ~ r,z:rl+z :r~~e2:Mn+e~ \ne1ine2:M~~mletz ~ e~inej:Mr~ :MT~ , z fresh I Fe:~-e :# I 1-r&#38;e: Jf(reJr) %refe :M(ref /) l?l-e:@~-e \n:ref# I E~e:&#38;f7~!e :M# I l-el:r@~%e~:ref# I t-e2:7_eJ:r I 1-el~eZ: Mvoid-e[:=ej :Mvoid Figure 5: \nSpecialisation Rules for Dynamic State 4 Specializing Operations on State yields relet r * ref . : Mint. \nWe approach the specialisation of imperative operations in mletze~r in three steps. First, we consider \nthe situation when all inq3 these operations are deferred until run-time. This is al-The static value \n2 is not present at run-time, but is pre\u00ad ready enough to achieve optimal specialisation for a self\u00adserved \nin the residual type of r (which is ref 2). When r interpreter. Then in Sec. 4.3, we move on to consider \nstatic is dereferenced we know statically that the result is 2, even effects. Finally in section 4.4 \nwe show how to combine the though r itself is dynamic. two. ThLs requires us to factorise the monad M \ninto a We can see from the specialised expression that the ref\u00ad static state transformer and a second \nmonad, so that the erence r still exists in theresidual program, but it is use\u00ad specialiser can manipulate \nthe static state. less because its contents are of void type. The final phsse of type specialiaation, \nvoid erasure, is extended to remove 4.1 Specializing a Dynamic State such references and operations on \nthem. It will thus trans-We shall extend the metalanguage with operations to cre-form the above term \nto q 3. (We assume here that reading st e, inspect, and update dynamic references. The additional and \nwriting are the only operations on references; if testing references for equality were also available \nthen erasing voidsyntax and typing rules are given in Figure 4; @ creates a reference, ~ reads its cent \nents, and := assigns to it. Of references would not be correct.) course, all three operations have a \ncomputation type. Since the residual type of a dynamic reference includes Under the assumption that all \nmonadic operations are the residual type of the value it contains, all the values we dynamic the specialisation \nrules turn out to be rather amign to it must have the same static part. In the exam\u00ad straightforward \n(see Fig. 5). The monadic operations simply ple above, the only value we can assign to r is 2! When specialise \ntheir subexpressions and reconstruct themselves. a dynamic reference contains a partially static value, \nthen In spite of this simplicity it is still possible to achieve inter-the static part must be the same \nat each assignment, but esting results, since dynamic references can contain static the dynamic part \ncan vary. For example, a reference of type information in their residual type, which is propagated to \n@ Uniw (where Uniu was defined in Sec. 2) created by the rest of the computation via unification. For \nexample, rnlet r ~ @_ (Num (lift 2)) in ... specializing could later be assigned Num (lift 3) or IVum \n(lift 4), because mletr*r@2 :Mi@ in rnletz~~r all these values have residual type Num int. It could not \nbe assigned a value of the form Fun f. The residual program in v(lift (z+ 1)) would just cent ain a \nreference to an int, which would be assigned the (untagged) values 2, 3 and 4. 208 When we want a reterence \nto contain many different static values, we are obliged to use a static reference. The situation is comparable \nto the difference between static and dynamic functio~s: a dynamic function can be specialised to only \none static argument, whereas a static function (which is unfolded) can be applied to any number. 4.2 \nOptimal Specialisation This specialiser is already capable of optimal special\u00adisation. To show this, \nwe give a self-interpreter for the meta-language in Figure 6. (The interpreter is recursive: see [17] \nfor specialisation rules for recursion). Since the interpret ed language is typed, we represent values \nby a static sum type ( Uniu). When an expression of type Uniu is specialised, its residual type (for \nexample Nurn int or Fun ( Ntim int ~ Num int )) tells us what type of value it represents. All type tags \nare known at specialisation time, and so both they and the corresponding tag tests are sim\u00adplified away \nby the specialiser. As a consequence, this in\u00adterpreter can only be specialised to well-typed programs \n supplying an ill-typed input will lead to a failure during residual type inference. No static references \nare needed to write the self\u00adinterpreter, because the only use of references in the inter\u00adpreter is to \nmodel references in the interpreted program. Such references can only oe assigned values of one type \n(be\u00adcause the interpreted language is well-typed), and so dur\u00ading specialisation all the values assigned \nwill have the same static part. When we specialise this interpreter to a term, we obtain essentially \nthe same term as the result, up to a-conversion. The only operations in the interpreter which give rise \nto residual code are those marked dynamic, the lift in the in\u00adterpretation of constants, and the monadic \noperations relet and V. By inspection, it is clear in most cases that eual specialises to a copy of its \nsecond argument, provided the same is true for the recursive calls. For example, the case for constants \ncontains one lift, which generates a residual constant; the case for J-expressions contains one dynamic \nJ, which generates a resiciual A-expression, and so on. Only two cases break this pattern: those for \nRf and As each gen\u00aderate an additional relet and q. However, these can slways be removed by a post-processor \nwhich applies the monad laws. In the case of Rf, the residual code is always of the form relet r ~ ref \ne in q r, which by the second monad law is equal to ref e. In the case of As, the residual code is oftheform \nrelet z < eI:= ezinq .. Since zisof type void this is equivalent to relet z + el := ez in q z, which \nis equal to el := ez by the second monad law. The postpro\u00adcessor performs both these simplifications. \nSpecialisation of the interpreter is therefore optimal. (In fact the post-processor may simplify additional \nre\u00addexes, so to be more accurate: when we sDecialise this in\u00adterpreter to a term we obtain a reduct of \nthe original term. We claim that this specialisation should still be considered optimal it would be \nunreasonable to refuse to do so be\u00adcause our specialiaer sometimes performs more simplifica\u00adtions than \noptimalit y requires! ) 4.3 Specializing a Static State Dynamic references are created and updated at \nrun-time. Consequently we cannot know their contents during special\u00adisation, and so we insist that all \ntheir contents have the same static part, which we do know at specialisation time, So for example, the \nexpression mletr*reJ2inmlet z*r~3in~r would cause a specialisation error, since r is used with both ref \n2 and ref 3 as residual types. We could avoid the error by making the reference contents dynamic: mletr \n* @lift 2inmletz * r~lift3in~r Now specialisation succeeds since r has residual type ref int, but on \nthe other hand no useful simplification is performed. In constrast, static references are created and \nupdated during specialisation; the specialiser keeps track of their con\u00adtents in a static state, and \nso they may freely be assigned values with diflerent residual types at different times. If our example \nis rewritten using static references mletr*ref2inmletze r:=3in!r then specialisation succeeds and produces \nne:Af3 In this section we shall consider specialisation of a lan\u00adguage with onlV static references. The \nnew operations and their types are analogous to those for dynamic references in Figure 5, the only difference \nbeing that we omit the under\u00adlines. When we specirdised a dynamic state above, we were able to treat \nthe monad and its operations as a black box: because all effects were deferred to run-time, the specialiaer \ndid not need to know anything about them. But now that we are treating static effects that is no longer \nthe case. Let us therefore say explicitly that the computation type is State -+ (r, State) computations \nare represented as functions. The static state itself is a tuple of the contents of the existing static \nreferences. We actually build up tuples out of pairs, so let us define a little syntactic sugar for values \nand types: (2,22 . ..zn) = (z,, (zZ,. . .(zn, o))) (UIU2 . . . an) = (ul, (uz, .,. (an, void))) We will \nalso write A(z1 . . . ~n) .e; it should be obvious how to translate such pattern matching into combinations \nof pro\u00adjections. In source programs the static state is implicit, but in residual programs we will represent \nit explicitly as a tuple, and pass it explicitly to and from residual computations. But now we encounter \na problem. The residual type of the static state is a tuple type, whose components are the resid\u00adual \ntypes of the contents of references. But a computation may modify those residual types! Thus, in the \nresidual pro\u00adgram, a computation may modify not only the contents of the state, but its type! Residual \ncomputations have types of the form  STUU T=U+(T, U ) where u is the residual type of the state beforehand, \nand a its type afterwards. This type is not a monad, which means that we cannot express the residual \nprogram in terms of the monad operators. However, ST has much in common with a monad we can for example \ngive sensible definitions of 209 data E= Cnintl Vrintl Lrnint EI ApEE lFz EIUn EIMJint EEIRj EIDf EI \nAsEE data U = IVum &#38; I Fun (U~U) I Ret (w U) I VOid I MOn (MU) I Wrong type Env = int + U let upd \n= Acnu.Ai.Au.Aj.if i = j then w else env j in Ietrec ernd = Aenv .Ae. case e of Cnn : Num (lift n) Vr \ni env i Lmie : fin (~v. eva[ (upd env i u)e) Ap el e~ : case evai env el of Fun f : jQ(eva~ env e~) \nFz e case eval cnv e of Fun f :&#38;f Une : Mon(q ( eval env e)) Mlielez : case eval enu el of  Monml \n: relet w* mlin case eual (upd erm i u) ez of Jfonmz : ma Rje : A40n(mlet r ~ ~ (eual en. e) in q (Refr)) \nDfe : case eval env e of Ref r : Mon(~ r) An el ez : case eval env el of Refr : Mon(mlet z * r ~ eval \nenv ea in q Void) in evd (Ai. Wrong) Figure 6: Self-Interpreter for Optimal Specialisation q and relet, \nfor which the monad laws hold and so we specialisation rules is call it a quasimona~. ho. let (z, s~) \n= (A().(o, (2))) so The specialisation rules for static references are given in in (Asl. let (y, s~) \n= (A(vl). (o, (v1o))) 81 Figure 7. The rules for q and relet just generate code to in (Jsz. let (2, ss) \n= (A(vIv2).(0, (4 VZ))) pass the state around explicitly. Notice in the rule for relet in (A8~,((z, y), \n,93)) 83) S2) 81 that the residual types propagate static information about : ST () (int 3) (1OCL,10CZ) \nthe store from el to e~ (via the shared variable al). Creating a static reference (the third rule) enlarges \nthe Now so, z, y and z have void types and can so be removed static store by one element. The static \nreference may very by the void eraser, which also deletes void components from well have dynamic contents, \nwhich become part of the resid\u00ad tuples. The result of void erasure in this case is ual static store. \nThe only static information we need about the new reference itself is the location to which it refers, \nso we introduce a new form of residual type loci to rep\u00adresent the ith location. The dereferencing and \nassignment operations just generate functions that access or modify the residual store appropriately. \nNotice how the residual types We usea post-processor which contracts trivial fl-redexes record precisely \nthe static contents of every reference before (in which the actual parameter is just a variable), since \nthe and after each operation. The last three rules cannot be applied unless the shape specialisation \nrules introduce many of these. The final result after post-processing is of the static store is known; \nthe last two cannot either be applied unless the address of the static location i is also letsl=2in letsz=slinlet8~= \n4ins~ known. During specialisation this information should be propagated from the context by unification. \nIt is considered All static operations on the store have been removed, and a specialisation error if \nit is not possible to determine the the resulting program just passes around the dynamic values sise \nof the static store during specialisation. which it held. For example, suppose we specialise the term \nThe residual programs we generate with this approach are purely functional, and pass around any dynamic \ncompo\u00ad relet z * ref (lift 2) nents of the static store explicitly. An alternative approach in rnlet \ny* ref 3 which we considered was to store such dynamic components in relet z = z:= lift 4inq(z,y) in \nthe dynamic store instead. Thus every static reference : M (ref i@ ref int) (with dynamic contents) would \nbe associated with a refer\u00adence in the residual program, holding the dynamic part of its in an initially \nempty static store. The result of applying the contents. But remember that a static reference can hold \nval\u00adues with different residual types at diferent times for ex\u00ad 2This in quite different from Steele \ns paeudomonads [31]. ample, a reference of type ref Univ could hold both Num n and Fun ~ on different \noccasions. No single residual refer\u00adence can hold both dynamic parts, n and ~, because they 210 T ::= \n.llocn residual types r~el:Mrl+ej:STaoulr[ r,z:rl-z :r([ e2:Mr2+ e~:STuluzrj Z , SO,S1 fresh I ~mlet \nz ~ el inez : MT2 %AsO.let (z , .sl)=e~ so inej 81 :STUO uz r; I /-e:r+:#:# Si fresh r ~ ref e : Mr \n-A(SI . ..s~). (o, (sl . . .s~e )) : ST (u1 . ..an) (UI . . .Unr ) locn+l l?}e:ref~~e : loci si fresh \n r~!e:Mr +A(sl... s~(si,(alal . ..~m)). ST(al. ..an)(uum}uim}ui l?~el:ref~~e~:loci I ~e2:~-ej:# r~ el \n:= e:?:M void L.+ #i fresh ~(sl . . #n).(., (SL . . . si-le~gi+l . an)) : ST (al . . . Un) {111. . . \nui-lr ui+l . . . CTn)void Figure 7: Specialisation Rules for Static State have different types. We could \nperhaps associate each static reference with a tuple of references in the residual program, one for each \ntype of value it is assigned. At any dereference operation, we would of course know statically which \nof the associated residual references held the current dynamic part of the contents. But reference creation \nwould be awkward, because all but one of the corresponding residual references would be uninitialised. \nThe problems are probably soluble, but we feel that the approach we presented above is consid\u00aderably \nsimpler. 4.4 Specializing a Mixed State Now that we know how to specialise a static and a dynamic state \nseparately, it is relatively straightforward to specialise them together. The trick is to separate the \nmonad into two parts: a static part which we manipulate explicitly in the specisliser, and a dynamic \npart which we treat as a black box. We therefore interpret the monad type M r as State + M (~, State), \nwhere A4 is the monad type used in residual programs. As in the previous section, residual computations \nmay change the type of the static state; they will have types of the form STuu T=u~M (r, u ) The specification \nrules have to be modified to produce terms of th~ type. We mc]dify the rules for dynamic refer\u00adence operations \nto pass the static store on unchanged, and we modify the rules for .static reference operations to con\u00adstruct \ntrivial M computations using q. The modified rules are given in Figure 8. 4.5 Specialisation of a Lazy \nInterpreter We demonstrate the capabilities of the specialiser by spe\u00ad cializing a lazy interpreter to \na term. Laziness is imple\u00ad mented by using updatable closures. At application an ab\u00adst raction is psased \na reference to a closure, which when eval\u00aduated reduces the argument to weak-head normal form, and performs \nan update on the reference cell. Provided only a bounded number of closures are created, the reference \nop\u00aderations can be performed statically. Figure 9 shows the interpreter. Specirdising the interpreter \nto the term (~z.z + Z)(I + Z) yields the residual type ST () ( Vl (Nurn int)) ( Nurn int ). Here the \noutput store contains the Vl (Nurn int). The con\u00adstructor Vl tells us that the corresponding closure \nwas eval\u00aduated at specialisation time. The constructor Num is also manipulated at specialisation time, \nwhich means that we can eliminate tagging and untagging from residual programs. What remains to be stored \nat run time is the integer. The residual term after additional simplication (see section 6) is let f \n=A8.mlet a * s 8 in q(a+a, a) in let b = As.?(I +2) infb The resulting program, although a bit awkward, \nhas the expected structure. ~ corresponds to the abstraction, and b to the argument 1 + 2, for which \nwe have to build a closure. The closure b takes a store s, and returns the result of the addition. ~ \ntakes this closurdereference has taken place at specialisation time and applies the closure to itself-the \nclosure is the sole element in the store. The result of this application is a computation taken apart \nby the monadic let. We then perform the addition a + a, and tuple it with a, the result of updating the \nstore. The application s s of the store to itself may seem strange, but it has a perfectly admissible \nrecursive type pa.a ~ Mint. 5 Delimiting Effects The type specialiser is able, among other things, to \nderive stat ic information about the result of a dynamic conditional. For example, ~ b then Num (lift \n3) else Num (lift 4) : Univ 4 if~hen 3 else 4 : Num int which tells us statically that the result is \ntagged Num. Of course, for this to be possible and for the residual program to be well typed, the two \nbranches must have the same residual type. When the arms of the conditional are computations, this implies \nthat they must leave the static store in the same state. 211 I *e:refr-e : loci ~-!e: ~~~ J(8I ...sn).v(~i,(~~ \n. ..sn)) : ST(UI . ..un) (uI... un) Ui resh l 1-el: ref~-e~: loci l?~ez:~~e~:~ r1 el := ez :M vo]d Q \n~(~1 . . sn).q(., (al si le~si+l . . . an)) rl e:r_e :# r 1-@ e:M (~ r) C+ ~s.mlet r * I !-e:@T_e :ref# \nrF!_e:MT%h.mletv ~ I l-el:@T_e{:ref# rEe1Ee2 : Mvoid _ Au.mlet z ~ Figure 8: Specialisation It is very \nawkward to meet this restriction. It means that if any references are created in one branch, then ex\u00adactly \ncorresponding references must be created in the other, and assigned contents with the same atatic part \n! Most par\u00adtial evaluators for imperative languages handle the problem by forbidding modiiicationa to \nthe static store in the arms of a dynamic conditional, but this makes the static store much less useful. \nOur solution to the problem is to support a stack discipline for the static store. We introduce a store \nprompt # e, which causes the specialiser to deallocate static refer\u00adences created by e after its specialisation \nis complete (see Fig. 10). If we use store prompts in the arms of a dynamic conditional, then each arm \ncan refer to the previous static store, and create and use its own local references, without causing \na residual type mismatch. Of course store prompts should only be used when local references do not escape \nfrom the enclosed expression. As we have explained, operations on the static store can only be specialised \nonce the size of the store is known. The specialisation rules propagate the static store from operation \nto operation, so if the initial size of the store is known, then its size will be derivable at all subsequent \noperations. Of course we intend the initial store to be empty, but so far we have glossed over how we \nspecify this. In fact, there can be other reasons for wanting to specify the size of the store. Consider \nthe function ~t.mlet r * ref 1 in relet v ~ ! r in v (t~liftv). which uses the static store internally \nto compute t+ 1.This function s type is -M&#38;, and it cannot be specialised without knowing the sise \nof the static store at the point where it is called. That seems unfortunate since it doesn t refer to \nany non-local references. Moreover, if the function were called from two points wit h dfierent static \nstores, then we would need to make it polyvariant and construct two dif\u00adferent specialisations even \nthough it doesn t access those 8i fresh : S2 (u1 . . . un) (al . . . ui l~ ui+l . . .un) void s, r fresh \nref e in 9(7, U): ST mu (ref ~ ) f s, v fresh !erin9(v, s): STaa T r~e2:T_e~:T a, z fresh e[ := ej \nin q(z,s) :S2 u u void Rules for Mixed State different stores. This would be the case even if we inserted \na store prompt into the function s definition. The solution is to run the computation in the function \ns body in a new, empty static store. We define two new op erators to do so, which differ only in their \ntreatment of the dynamic store (see Fig. 11). runM e turns a computa\u00adtion with effects into one without, \nfrom a computational perspective. It runs e in a completely new (static and dy\u00adnamic) store, so that \ne can neither access nor modify the enclosing store it is a state thread. The residual type does not \ninvolve the monad, and so the residual code must itself include the effect delimiter runM . runM e runs \ne of type AfT in a new and empty static store, but threads the dynamic store through, so that e can access \nand mod\u00adify non-local dynamic references. The residual type is still MT because of the dynamic store. \nWe should ensure that references neither escape from runM e nor can they be im\u00adported from the outside \n(and analogously for runM e and static references). We gloss over this problem here because we can apply \nLaunchbury and Peyton Jones s solution to a related problem [20, 21]. The specialisation rule for runM \ne (see Fig. 12) applies the residual quasimonad of e to an empty initial static state, runs the resulting \ndynamic computation, and discards the final static state. The result is a value with residual type ~ \n. The rule for runM e also applies the residual quasimonad to an empty static state, and the final state \nu is again dis\u00adcarded. In contrast with runM , the resulting expression might still induce effects at \nrun-time. For this reason we thread the resulting computation into the dynamic compu\u00adtation thread. Now \nwe can specialise our initial example in two ways: relet r+ ref 1 &#38;.runM in relet v + ! r int-M \nint . _ in q (t~liftv) ( ) + At.runM (q (t+ 1)) :int -+ Mint 212 data E= data U= data V= type Env \n let upd = = int ~ U Aenv.Ai.Av.Aj.if Cnint\\Vr intl Lmint EIAp Num m I Fun (UtiU) I Ref (@ Bl int I C1(M \nvoid) \\ VZU i= j then v else letrec ewal = Aenv. Ae. case e of C% n Vr i Lmie Ap el e~ in eval (Ai. Wrong) \n( or relet r + Jt.rtrnNl in relet in ~ The only other speaaliser EEIPl EE U) I Wron9 envj in v (Num \n(lift n)) case env i of Refp : relet v ~ !pin q(l un relet relet relet case case u of Vlv :qv C lm : \nrelet a * min mletb~! pin case b of WV :qv (Jv. eval (upd env i v)e)) f * eval env el in a e ref Bl O \nin b ~ a:= Cl (relet c ~ eval em e~in a := vi c) in ~ of Fun j : fQ(Re~ a) relet a * eval em el in relet \nb ~ eval env ez in case a of Numa ; case b of Num b: q(Numa+b) Figure 9: Lazy Interpreter Figure 10: \nTyping and Specialisation Rules for Store Prompts ref 1 v * ! r : int~int (t*liftv) ) to combine first-class \nfunctions with static references is due to Dussart and Thiemann [13]. It is a more conventional partial \nevaluator, in which dynamic A-expressions cannot have static arguments. Consequently it always specialises \nthe bodies of dynamic A-expressions in an empty static store. Our runM essentially simulates tKM behaviour. \nOur effect delimiters runM and runM are rather blunt instruments. Ideally we would like to be able to \nspeaalise a computation knowing only the contents of static references that it actually refers to. Likewise \nwe would like to be able to invoke such a specialisation in diflerent static stores, pro\u00advided the references \nactually used have the same contents. To do so requires polymorphism in residual programs we want to \ninvoke the same residual function on stores with different residual types. The generation of polymorphic \npro\u00adgrams by type specialisation is so far an open problem, which must be solved before a better treatment \nof static stores is possible. 6 Postprocessing As we remarked above, A~l is a pure language in the sense \nthat isomorphisms like void + T ~ T are still valid. There\u00adfore the all techniques for void erasure in \nthe lambda calculus [17] are still valid. 6.1 Void Erasure The void eraser follows one simple principle: \nReplace every expression whose type denotes a singleton set by c. Apart from the standard situations \ninvolving products, sums, and function types described by Hughes [17], we now have two additional type \nconstructors to consider: ref r and M r. Keeping references of type ref void makes no sense unless there \nis an equality operation on references. They can be erased in the absence of such an operation. However, \nthe type M void denotes a computation returning void, but the computation may very well affect the (dynamic) \nstate. It cannot be erased. 213 l+e:kf~ I 1-e:M7 I 1-runMe:r rl runMe:Mr Figure 11: Type Rules for \nStore Delimiters I 1-e:M r-e : S2 oa 7\u00b0 r 1 runM e : r C+ ml(runM (e ())) : T r}e:MT_e :S2 ou / I } runM \ne:MT_ h.mlet (z ,s ) ~ e () Z ,LI:STuu nr~ T Figure 12: Specialisation Rules for Store Delimiters Since \nwe regard ref void as useless we can replace all operations processing them by appropriate monadic units: \nref e : M (ref void) C+ q.:itfvoid !e:bf void _ qe:Mvoid el:refvoid ~el:=ea:iU void + qe:lfvoid This \nprocess removes operations on the dynamic state. The residual operations on the static state are expressed \nby ex\u00adplicit store passing. The simplification rules for the standard types suffice to simplify them. \n6.2 Monadic Simplifications The residual programs arc written in monadic style and con\u00adtain a lot of \nredundant monadic let and unit expressions. This is partly due to the actions of the preceding void era\u00adsure. \nThese expressions present ideal targets for applying the three monad laws. Moggi [25] has proved that \nA-l is sound and complete with respect to all monadic mod\u00adels, specifically for our model that uses state \ntransformers. Therefore, the application of the monad laws is very safe in the sense that it does not \nduplicate, discard, or reorder computations. If the type of z is void then after application of the rule \nrelet z e qe ine -letz=e ine we can drop the let and e entirely if the type of z is void. In this caae \nlet z= e in e -e . The post-processor does not unfold lets in general, but we do use the simplication \nIet z = z in e 4 e because terms of this form appear quite often after applying the preceding transformations. \nFinally, simplification discards trivial computations wrapped in runM : runlVl (q e) -e. 6.3 Further \nSimplifications Due to the explicit store-passing style, the residual program cent sins many trivial \nfl-redexes like ( Az, e) z. We convert them to let-expressions and apply the same simplifications as \noutlined above. No other (serious) /3 reductions are per\u00adformed. The explicit represent ation of the \nstore as t uples in\u00adt reduces a lot of extraneous t upling and tuple operations. These are eliminated \nvia an arity raising phase [29]. Finally, explicit store-passing introduces many q-redexes on top of \ncomputations that do not affect the static store. Therefore, the void eraser also performs q-reductions. \n7 Assessment Type specialisation enhances partial evaluation with infor\u00admation propagation by unification. \nThis enables side-ways information exchange in the source program s execution tree whereaa partial evaluation \nis restricted to paths in the exe\u00adcution tree. A consequence is that residual programs cannot be constructed \nin a bottom-up manner, but rather program fragments are constructed in a demand-driven order pre\u00adscribed \nby the flow of information. Much of the power of type specialisaton stems from the refined type language \nused. The type language is able to express single element types that play the r61e of symbolic values \nand partially static values of standard partial evalu\u00adation. In contrast to partial evaluation, the two-level \ntype disci\u00adpline of type specialisation does not impose well-formedness conditions like if an abstraction \nis dynamic then its argu\u00adment and result must also be dynamic. Another instance is the fact that dynamic \nreferences may hold static values. This liberality is unique to type specialisation. It is the key to \nthe optimal specialisation results for typed functional languages reported here and in Hughes initial \nwork [17], be\u00adcause it allows information (such as static tags) to be com\u00admunicated out of dynamic functions. \nHowever, the well-formedness conditions are at the heart of traditional binding-time analyaia. A binding-time \nanrJ\u00adyser constructs the least (most static) assignment of well\u00adformed types to terms; dynamic inputs \nforce other computa\u00adtions to be dynamic also via the well-formedness conditions. The most static type \nassignment is rdways best it leads to the strongest specialisation. The type specialiaer in con\u00adtrast \ncan check that a two-level program is well-typed, but cannot infer a best sasignment of twe-level types \nto a one\u00adlevel program. In the absence of well-formedness constraints many more operations can be made \nstatic, but this does not mean that they should be. For example, in the interpreters in this paper we \nchose to make Univ a static sum type, so that type tags would be removed during specialisation. In consequence \nthese interpreters can only be specialised to well-typed programs they interpret a typed language. \n214 Had we chosen to make Uniu a dynamic sum instead, then type tags would have remained in residual \nprograms, but we would be able to specialise the interpreters to arbitrary programs such interpreters \naccept an untyped language. Neither alternative is obviously better than the other, and we do not believe \nthe choice can be made by an automated analysis. The assignment of binding-times in an interpreter is \nessentially a way to specify the static semantics of the interpreted language, and this is a creative \nactivity. Furthermore, type specialisation also performs program analysis of the residual program while \nit constructs theresid\u00adual program. An example for this is shown in Hughes work [17] where he constructs \na firstifying two-level interpreter for a type lambda calculus. Here the type specialize performs closure \nanalysis and the construction of recursive residual types on the fly. 8 Related Work 8.1 Type and Constructor \nSpecialisation Hughes [17] int reduces type specialisation for a simply-typed lambda calculus with products \nand sums. The current work is an conservative extension to Moggi s computational met\u00adalanguage [25]. \nThe type specialiser embodies constructor specialisation [24, 12] and extends it to higher-order lan\u00adguages. \nHagiya and Iino [14] consider a weaker variant of constructor specialisation that they call data type \nspeciali\u00adsation for a Lisp-like language. They have set up an exten\u00adsion of the standard partisl evaluation \nframework consisting of an extended binding-time analysis and a specialiser which is expressed as a non-standard \ninterpreter and constructs the residual program in a compositional manner. Bechet [3] gives an account \nof a very similiar technique geared towards typed languages. The other constructor specialisers and the \ntype specialiser construct the residual program out of order. 8.2 Partial Evaluation Partial evaluators \nfor traditional imperative languages like FORTRAN, C, Pascal, or Modula-2 [1, 10, 2, 8, 23] all per\u00adform \nside effects at specialization time. However, none of them incurs the problems inherent in higher-order \ncontrol flow and only some of them face the problems of dealing with pointers which are ramilar to the \nproblems with first\u00adclsss references. In the present work, type specialisation deals satisfactorily with \na language with higher-order con\u00adtrol flow and first-class references. Partial evaluators for iigher-order \nfunctional languages are quite common [5, 7, 9, 4, 22, 32]. Only some of them attempt to handle side \neffects [7, 4, 22] but all of them follow the approach of Bondorf and Danvy [6] in deferring all side \neffects like operations on references, assignments to global variables, and 1/0 to run time. Recently, \nthere have been approaches to perform partial evaluation of imperative programs with static operations \non references. Moura, Consel and Lawall [26] show how to use Schism [9] a partial evaluator for pure \nScheme with a poy\u00advariant binding-time analysis-to specialise programs in the C language by transforming \nthem to a sophisticated variant of store-passing style. This approach avoids the construc\u00adtion of new \nprogram anaiyses and new partial evaluation techniques for imperative programs. Dussart and Thiemann \n[13] have constructed an offline partial evaluator for a simply-typed lambda calculus with first-class \nreferences, Their system is traditional in that its monovariant binding-time analysis ensures that there \nare no errors at specialization time. The binding-time analysis is based on an effect system. The system \nis automatic, in con\u00adtrast to the type specialiser which only works on manually annotated programs. 9 \nConclusion Type specialisation adapts searnlesaly to the specialisation of imperative languages. Unification \nagain gives us the nec\u00adessary power to achieve optimal specialisation for a version of Moggi s computational \nmet alanguage with first-class ref\u00aderences. Our use of the computational metalanguage A~t turned out \nto be a big win. First, we only had to add rules for the new constructs of the language to the type specialize. \nNo existing rule was changed. Contrast this with the special\u00adize of Dussart and Thiemann [13] where every \nspecialisation rule had to be changed in order to thread the state through the computation. Second, our \nspecialiser is completely in\u00addependent of the evaluation order. Only the sequence of the monadic operations \nis fixed; the remaining parts of a pro\u00adgram can be specialized without committing to a specific evaluation \norder. Third, we conjecture that the monadic approach ~OWS further modular extensions to cater for ex\u00adceptions, \nnon-determinism, etc. Acknowledgement We thank the anonymous reviewers and Mads Tofte for their valuable \ncomments. References [1]L. O. Andersen. Progmm Anaiy8i8 and Specialization for the C Progmmming Language. \nPhD thesis, DIKU, University of Copenhagen, May 1994. [2] R. Baier, R. Gliick, and R. Z6chling. Partial \nevalua\u00adtion of numerical programs in Fortran. In P. Sestoft and H. S@ndergaard, editors, Proc. ACM SIGPLAN \nWorkshop on Partial Evaluation and Semantica-Based Progmm Manipulation PEPM 94, pages 119-132, Or\u00adlando, \nFla., June 1994. ACM. [3] D. Bechet. Removing value encoding using alterna\u00adtive values in partial evaluation \nof strongly-typed lan\u00adguages. In H. R. Nielson, editor, Proc. 6th European Symposium on Progmmming, pages \n77 91, Linkoping, Sweden, Apr. 1994. Springer-Verlag. LNCS 1058, [4] L. Birkedal and M. Welinder. Hand-writing \nprogram generator generators. In M. V. Hermenegildo and J. Penjam, editors, Progmmming Languages, Imple\u00admentations, \nLogics, and Progmms (PLILP 94), vol\u00adume 844 of Lecture Notes in Computer Science, pages 198-214, Madrid, \nSpain, Sept. 1994. Springer-Verlag, [5] A. Bondorf. Automatic autoprojection of higher order recursive \nequations. Science of Programming, 17:3-34, 1991. [6] A. Bondorf and O. Danvy. Automatic autoprojection \nof recursive equations with global variables and abstract data types. Science of Progmmming, 16(2):151-195, \n1991. 215 [7] A. Bondorf and J. Jorgensen. Efficient analysis for re\u00adalistic off-line partial evaluation. \nJournal of Functional Programming, 3(3):315-346, July 1993. [8] M. A. Bulyonkov and D. V. Kochetov. Practical \naspects of specialization of Algol-like programs. In Danvy et al. [11], pages 17-32. [9] C. Consel. A \ntour of Schism. In Schmidt [30], pages 134-154. [10] C. Consel and F. Noel. A general approach for run-time \nspeaaliaation and its application to C. In Proc. 23rd Annual ACM Symposium on Principle of Program\u00adming \nLanguages, pages 145-156, St. Petersburg, Fla., Jan. 1996. ACM Press. [11] O. Danvy, R. Gluck, and P. \nThiemann, editors. Partial Evaluation, volume 1110 of Lecture Notes in Computer Science, Dagstuhl, Germany, \nFeb. 1996. Springer Ver\u00adlag, Heidelberg. [12] D. Dussart, E. Bevers, and K. De Vlaminck. Poly\u00advariant \nconstructor specialization. In W. Scherlis, editor, Proc. ACM SIGPLA N Symposium on Partial Evaluation \nand Semantica-Based Program Manipula\u00adtion PEPM 95, pages 54-63, La Jolla, CA, June 1995. ACM Press. [13] \nD. Dussart and P. Thiemann. Partial evaluation for higher-order languages with state. Berichte des Wilhelm-Schickard-Instituts \nWSI-96-??, Univer\u00adsitat Tubingen, Nov. 1996. [14] M. Hagiya and K. Iino. Binding time analysis for data \ntype specialization. In M. Takeichi, editor, Fuji Work\u00adshop on Functional and Logic Progmmming, pages \n254 269, Fuji Susono, Japan, July 1995. World Scientific Press, Singapore. [15] J. Hatcliff and O. Danvy. \nA generic account of continuation-passing styles. In POPL1994 [28], pages 458-471. [16] J. Hughes, editor. \nFunctional Pwgramming Languages and Computer Architecture, Cambridge, MA, 1991. Springer-Verlag. LNCS \n523. [17] J. Hughes. Type specialisation for the J-calculus; or, a new paradigm for partial evaluation \nbaaed on type inference. In Danvy et al. [1 I], pages 183 215. [18] N. D. Jones, C. K. Gomard, and P. \nSestoft. Partial Evaluation and Automatic Program Generation. Pren\u00adtice Hall, 1993. [19] J. Launchbury. \nA strongly-typed self-applicable partial evaJuator. In Hughes [16], pages 145-164. LNCS 523. [20] J. \nLaunchbury and S. L. Peyton Jones. Lazy functional state threads. In Proc. of the ACM SIGPLAN 94 Con\u00adference \non Programming Language Design and Imple\u00admentation, pages 24 35, Orlando, Fla, USA, June 1994. ACM Press. \n[21] J. Launchbury and S. L. Peyton Jones. State in Haakell. Lisp and Symbolic Computation, 8(4):293-341, \nDec. 1995. [22] K. Malmkjmr, O. Danvy, and N. Heintze. ML par\u00adtial evaluation using set-based analysis. \nIn Record of the A CM-SIGPLAN Workshop on ML and its Applica\u00ad tions, number 2265 in INRIA Research Report, \npages 112 119, BP 105, 78153 Le Chesnay Cedex, France, June 1994. [23] U. Meyer. Techniques for partial \nevaluation of im\u00adperative languages. In P. Hudak and N. D. Jones, editors, Proc. ACM SIGPLA N Sympoeium \non Partial Evaluation and Semantics-Based Program Manipula\u00adtion PEPM 91, pages 94 105, New Haven, CT, \nJune 1991. ACM. SIGPLAN Notices 26(9). [24] T. ill. Mogensen. Constructor specialization. In Schmidt \n[30], pages 22-32, [25] E. Moggi. Computational lambda-calculus and mon\u00adads. In Proc. of the ~rd Annual \nSymposium on Logic in Computer Science, pages 14 23, Pacific Grove, CA, June 1989. IEEE Computer Society \nPress. [26] B. Moura, C. Consel, and J. Lawall. Bridging the gap between functional and imperative languages. \nPublica\u00adtion interne 1027, Irisa, Rennes, France, July 1996. [27] F. Nielson and H. R, Nielson. Two-Level \nFunctional Languageu. Cambridge University Press, 1992. [28] Proc. .21st Annual ACM Symposium on Principle. \nof Programming Languages, Portland, OGI Jan, 1994. ACM Press. [29] S. A. Romanenko. Arity raiser and \nits use in program specialization. In N. D. Jones, editor, Proc. %d Euro\u00adpean Symposium on Programming \n1990, pages 341-360, Copenhagen, Denmark, 1990. Springer-Verlag. LNCS 432. [30] D. Schmidt, editor. \nProceeding, of the ACM SIGPLAN Symposium on Partial Evaluation and Semantica-Baeed Program Manipulation \nPEPM 93, Copenhagen, Den\u00admark, June 1993. ACM Press. [31] G. L. Steele Jr. Building interpreters by composing \nmonads. In POPL1994 [28], pages 472 492. [32] D. Weise, R. Conybeare, E. Ruf, and S. Seligman. Au\u00adtomatic \nonline partial evaluation. In Hughes [16], pages 165-191. LNCS 523. 216  \n\t\t\t", "proc_id": "258948", "abstract": "We extend type specialisation to a computational lambda calculus with first-class references. The resulting specialiser has been used to specialise a self-interpreter for this typed computational lambda calculus optimally. Furthermore, this specialiser can perform operations on references at specialisation time, when possible.", "authors": [{"name": "Dirk Dussart", "author_profile_id": "81100619759", "affiliation": "Department of Computer Science, K.U.Leuven, Celestijnenlaan, 200A, B-3001 Leuven, Belgium", "person_id": "P67462", "email_address": "", "orcid_id": ""}, {"name": "John Hughes", "author_profile_id": "81100166325", "affiliation": "Chalmers Tekniska H&#214;gakola, S-41296 G&#246;teborg, Sweden", "person_id": "PP40024464", "email_address": "", "orcid_id": ""}, {"name": "Peter Thiemann", "author_profile_id": "81100458917", "affiliation": "Institut f&#252;r Informatik, Universit&#228;t, T&#252;bingen, Sand 13, D-72076 T&#252;bingen, Germany", "person_id": "PP39043747", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/258948.258968", "year": "1997", "article_id": "258968", "conference": "ICFP", "title": "Type specialisation for imperative languages", "url": "http://dl.acm.org/citation.cfm?id=258968"}