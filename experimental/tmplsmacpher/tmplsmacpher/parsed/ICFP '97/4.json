{"article_publication_date": "08-01-1997", "fulltext": "\n Functional Programming with Graphs Martin Erwig FernUniversitat Hagen, Praktische Informatik IV 58084 \nHagenj Germany erwig@fernuni-hagen.de Abstract Graph algorithms expressed in functional languages often \nsuffer from their inherited imperative, state-baaed style. In particular, this impedes formal program \nmanipulation. We show how to model persistent graphs in functional languages by graph constructors. This \nprovides a decompositional view of graphs which is very close to that of data types and leads to a more \nfictional formulation of graph algo\u00adrithms. Graph constructors enable the definition of general fold \noperations for graphs. We present a promotion theo\u00adrem for one of these folds that allows program fusion \nand the elimination of intermediate results. Fusion is not r\u00adstricted to the elimination of tr~like structures, \nand we prove another theorem that facilitates the elimination of in\u00adtermediate graphs. We describe an \nML-implementation of persistent graphs which etEciently supports the presented fold operators. For example, \ndepth-first-search expressed by a fold over a functional graph has the same complexity as the corresponding \nimperative algorithm. 1 Introduction Traditionally, most graph algorithms are formulated in an imperative \nmanner, for example, in depth-first search, nodes are marked as being visited to prevent repetitive traversal. \nMost often, this imperative style is carried over when impl~ menting graph algorithms in functional languages, \nfor ex\u00adample, a set of visited nodes is threaded through successive function calls. Although this strategy \ncan keep programs free of imperative updates, the stat-baaed, imperative algo\u00adrithm is still present, \njust in functional disguise. However, in order to find program transformations like in the unfold/fold \napproach or in the Bird/Meertens formalism, an integration in a truly functional style is needed. We \nfollow Richard Bird who recently concluded [3]: But if we rema n within a functional formalism, then \nwe need to reformulate standard algorithms [...] The treatment of graphs in functional languages has \nnow been addressed in quite different ways [4, 13, 7, 14], but Permission to make digital/hard copy of \npart or all this work for personal or classroom use is granted without fee provided that copies are not \nmade or distributed for profit or commercial advan\u00adtage, the copyright notice, the title of the publication \nand its date appear. and notice is given that copying is by permission OfACM, Inc. To copy otherwise, \nto republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or \na fee, ICFP 97 Amsterdam, ND @ 1997 ACM 0-89791 -918 -1/97 /0006 ...$3.50 there is no accepted standard \nyet. We believe that one reason for this situation is that the integration often suffers from the inherited \nimperative style. This has to be seen in contrast to tree-like structures that can be directly represented \nby data typea which present themselves rather uniformly across different functional lan\u00adguages: the notions \nof data type, constructor, pattern and pattern matching are well-known and they are present in almost \nall modern functional languages. Research on gener\u00adalized fold operations, also called catamorphisms, \n[18, 22, 10] has (among other things) produced far-reaching opportuni\u00adties for program transformations. \nIn particular, the fusion of multi-pass algorithms [17, 24, 16, 12] is a profitable opti\u00admization technique. \nNow it is challenging to reach a comparable status for graphs and graph algorithms. At this point one \nmight object that graphs are application-specific structures rather than a programming language concept, \nand they should there fore be implemented by means of language features already present. Even if this \nis true, it is nonetheless important to have a uniform comprehension of graphs together with a cor\u00adresponding \nprogramming style to facilitate program trans\u00adformation and optimization as it is known from data types. \nThis paper suggests a (de) compositional view of graphs which is very close to that of data types. This \ngives a new flavor of defining graph algorithms and clears the way for defining general fold operations \non graphs. We show how to define graph algorithms in terms of graph folds and how this facilitates program \ntransformations and optimizations. An integration of the proposed concept into a functional lam guage \nrequires as its backbone an implementation of func\u00adtional, or persistent, graphs. We have implemented \nfunc\u00adtional graphs together with graph folda in ML, providing efficient implementations for graph operations \nlike depth\u00adfirst-search. There are two ways to achieve such an integration: First, as a language extension. \nThis allows the optimizations de\u00adscribed in Section 6 to be used by a compiler, and it pro\u00advides a convenient \nway of pattern matching (Section 4) to the user. On the other hand, it is not reasonable to expect a \ncompiler to provide all the different graph representation that are needed to efficiently deal with graphs \nin specific situations. Secondj by providing a graph library. With this approach it is much easier to \nprovide (and extend) different graph implementations. We follow the latter approach since it seems to \nbe more promising, but for the convenient pre\u00adsentation of examples in this paper we assume having the \npattern matching capability available. [4] [13] [7] [14] [10] This paper generality yes yes no (yes) \nno yes efficiency no no yes yes no yes mreness ves ves ves no ves ves clearness I no I (ves) I ves I \nyes I ves I ves I reasoning no no no (yes) yes yes Figure 1: Treatment of graphs in functional languages. \n Related Work In [41the state used by graph algorithms is simulated by functiorial arrays that are threaded \nthrough function calls. It is shown how to directly transfer classical algorithms into a lazy functional \nlanguage, but no particular use of func\u00adtional languages is made in the design of the algorithms themselves. \nIn contrast, in [13] algorithms are described as fixed points of recursive equations which essentially \nrelies on lazy evaluation. Though being more functional , the algorithms become quite complex and are \nrather difficult to compre\u00adhend. Both approaches do not achieve the asymptotic run\u00adtime of imperative \nalgorithms. In [7] we have identified some classes of graph algorithms and have introduced a few corresponding \npredefine oper\u00adators. A graph algorithm is realized by simply providing an operator with some parameter \nfunctions and data struc\u00adtures. We believe that the approach reflects the structure of graph algorithms \nvery well. However, like the previous two approaches there is not much potential for forrnaJ pro\u00adgram \nmanipulation. Moreover, the operator approach lacks generality. In the proposal of [14] the focus is \non a generated data structure, the depth-first spanning forest, instead of the un\u00adderlying graph algorithm. \nThM facilitates formal reasoning, in particular, the formal development of many algorithms based on depth-first \nsearch (dfs) becomes possible. More\u00adover, Launchbury shows in [15] how phase fusion can be applied to \neliminate intermediate results of some of these algorithms. The dfs function itself is reahzed nicely \nin a generate-and-prune manner. Monads are used to implement the state maintained during dfs (that is, \nthe vertices visited) to achieve linear running time. At this point the approach is stuck with the imperative \nprogramming style. Although encapsulated and restricted to a single point, it comes up in the process \nof program fusion where transformations be\u00adcome quite complex when functions are moved across state transformers. \nAs yet the iipproach applies just to dfs. Fegaras and Sheard investigate in [10] a generalization of \nfold operations to data types with embedded functions. As one motivating example they show how to model \ngraphs. However, that approach is somewhat limited (it is not clear how to define, for example, a function \nfor reversing all edges in a graph) and it lacks efficiency since direct access to a node requires, in \ngeneral, traversaJ of the whole graph. Also related is the work of Gibbons [11] who considers the definition \nof graph fold operations within an algebraic framework. But he deals only with acyclic graphs, and an \nimplementation is not discussed, so that his approach is cur\u00adrently not usable. A summary of the preceding \ncomparison is shown in Figure 1. In the next section we define graph constructors and show their use \nin building directed graphs. In Section 4 we describe a special kind of pattern matching for graphs. \nSection 5 presents some fold operations. Two theorems are given in Section 6 to demonstrate the optimization \nof graph algorithms by simple program transformations. The imple mentation is described in Section 7, \nand some conclusions follow in Section 8. 3 A Model of Directed Graphs We propose a (de)compositional \nview of graphs in the style of algebraic data types found in languages like ML or Haakell: a graph is \neither empty, or it is constructed by a graph G and a new node v together with edges fxom v to its successors \nin G and edges from its predecessors in G leadlng to v. This view is closely related to an adjacency \nrepresen\u00adtation of graphs. The main difference to data types is that predecessors are mentioned explicitly. \nWe present our ideaa in terms of ML, but a translation to other languages is not difficult. 3.1 Graph \nConstructors There are quite different kinds of graphs, and it is almost impossible to capture all aspects \nin a single type. Therefore we focus in the following on directed, node-labeled multi\u00adgraphs. This, on \nthe one hand, includes some non-trivial aspects, such as multiple edges between two nodes, and, on the \nother hand leaves out other details, for example, edge labels, that would only make examples longer and \nmore dif\u00adficult to read. Adaption to other graphs types is straightfor\u00adward. The constructive view of \ngraphs suggests the following two constructors: Empty: a gxaph k: a context * a graph -> a gxaph The \ntype parameter a gives the type of node labels. Distin\u00adguishing between nodes and node labels is necessary \nwhen\u00adever different nodes may have the same label, see Figure 2. (This example aho shows the need for \nmultiple edges between two nodes.) The context of a node is the node itself together with its label and \nthe lists of its predecessors (first component) and its successors (last component): type a context = \nnode list x node a $a e node list The requirements on the type node are given by the signa\u00adture below. \nIn particular, we have to create node values before we can build a graph. This is done by the gen func\u00ad \ntion that generates any requested number of different nodes. sig eqtype node val gen : int -> node list \nval new : node list -> node end In the subsequent examples we will use the following nodes: val [A,B,c,DI \n= gen 4 The fimctionnev creates anode that is not conttinedin a given list of nodes. This function isuseful \nwhen extendinga graph whose construction history is not known since we need a node value not already \ncontained in the graph. However, to apply new we have to extract the nodes of a graph. This is done by \nthe function nodes: val nodes: a graph -> node list As an example, we construct a cycle of three nodes \nand extend it by a node with an edge to some node of the cycle. vti CyC = ([cI, A,l, [BI) k ([1, B,2, \n[c1) k ([I, C,3, [I) k Empty let val N = nodes cyc in ([) ,new N,4, [hd N]) &#38; cyc end The DAG for \nthe expression 1 +(2+2) and a graph expres\u00adsion for it are shown in Figure 2. + +    @i b () 2 \no datatype int.expr = CONof int .. I Op of int * int -> int Val exprDag = ([],D,OP (op+), [B, C]) k \n([I,C,CON 1,[1) &#38; ([I,B,OP (oP+),[A,AI) &#38; ([I,A,CON 2,[1) &#38; Empty Figure2: ADAGand its graph \nexpression. 3.2 Semantics of Graph Constructors A graph G = (V, E,U) of type a consists of a set of \nnodes V, a multiset (or, bag) of edges EEM(V xV), and a total mapping u : v~ff defiting the node labels. \nRepresenting the edges of a graph se a bag of node pairs accounts for multiple edges between two nodes. \nThesemantics of the above graph constructors with re\u00adspect tothisgraph model is given by inference rules \nas used in the definition of Standard ML [19], see Figure 3: an as\u00adsertion pl-e +. v says that expression \ne evaluates in the environment p to the value v. For simplicity, we assume having bags as semantic values, \nand we denote a bag by writing a sequence of its elements, that is, (Z I,. . . . Zn) or ~ for short, \ndisregarding the order of elements. The union of two bags is written like the concatenation of two lists \nL and L by L. L . The last two rules describe exceptional situations: trying to add a context for an \nalready existing node results in a Node exception. Likewise, trying to add an edge between non-existing \nnodes raises an Edge exception. (Note that adding a tuple (v, w) to the edge bag of a graph means not \nonly that w is a successor of v, but also that v is a predecessor of w.) It is not difficult to define \ntwo functions addnode and addedge for adding a single node (without predecessors amd successors) and \na single edge (between two nodes that are known to be already contained in the graph): fun addnode (v,l) \ng = ([ I,v,l, [I) &#38; g fun addedge (v,v) ((p,u,l, s) &#38; g) = if U=v then (p, u,l, w::s) &#38; g \nelse (p, u,l ,s) &#38; (addedge (v, w) g) These can be used to build any graph by first inserting all \nnodes and after that inserting all edges. Hence we know: Theorem 1 (Completeness) Any node-labeled multi-gmph \ncan be represented by a gmph expression. .  4 Pattern Matching on Graphs Regarding the free term algebra \ngenerated by Empty and &#38;, pattern matching is the same as with other data types. For example, we \ncan define a function gmap for mapping a function to all node labels of a graph: fun gmap f Empty = Empty \nI grnap f ((p,v,l, s) h g) = (p, v,f 1,s) &#38; (gmap f g) The semantics of graphs, however, suggests \nthat some dis\u00adtinct graphs should be regarded as equal. In particular, many function definitions become \nmore convenient when a kind of pattern matching could be used that abstracts away the order of node/edge \ninsertions. Consider, for example, functions suc and del for selecting the successors of node v in a \ngraph g, respectively, for deleting v from g. Function definitions get remarkably simple when node v \nis inserted laat into g, that is, g = (p, v,l, s) &#38; g : then we can sim\u00adply return s, respectively, \ng as result. It is an immediate corollary of Theorem 1 that we can always reorganize g to obtain the \nterm above, since for any graph g we can always find a term for a graph g with a specific node v (and \ninci\u00addent edges) removed, so that g can be obtained by inserting v together with its incident edges into \ng . Miranda laws and the views mechanism proposed by Wadler [23] allow pattern matching on non-free algebraic \ndata types by mapping data type terms to canonical rep\u00adresentations. For our purposes instead of mapping \nto a canonical representation we rather need to select a spe\u00adcific representation (among several equivalent) \nvia a pat\u00adtern, namely one with a certain node inserted last. With the p t- Empty * (QI, 04) d--g* WJW \n@p*P, pt-v*v pt-1=%1 pt--S*s, VW Undo {PiI c (v u (4) Uy.t~{Si) G (V u (v}) pt--(p,v,Ls) &#38; g*(vu \n,E*((pl,v),*..~(p,,v),(v,sl),*~.,(v,s,)),uU d--g=+ ww pt-v-rv VEV pt-(p,v,l,s) &#38; g+ [Node] P I- \ng * (VTE, 4 d-p*P, P t- v * v <(J-L, {Pi} lJ UT=,{s;}) g (v u {v}) pk (P,v,l,s) &#38; g* [Edge] Figure \n3: Semantics of graph constructors. above graph semantics we can easily define such a pattern as a language \nprimitive. We write an environment map-ping variables z to values K as {ICI HV~,...,Z~HV,}, and an assertion \np, v t- p + p says that pattern p matched against value v in the environment p results in the bind- ing(s) \n(that is, variable environment) p [19]. Let G = (KE ((v,S1),~~~~(v,sn),(Pl,V),~~~~~Pm,v)),uU {(v~l)})* \nThen: pl-v+v p,GC-(p,v,l,s) &#38; g* {SHSn,pHpm,lHz,gH(V-{V},E,v)} This rule says that the unbound variables \np, 1, and s of the pattern are bound to the corresponding values of v s context. If v $ V, a special \nsemantic object FAIL is re-turned [19]. FAIL is not a value, its only purpose is to direct pattern matching \nto the next case. If the last case returns FAIL, a Match exception is raised. The notation in the rule \nassumes that n and m are chosen maximally, that is, all edges incident to v are selected. Moreover, writ-ing \nsuccessors preceding predecessors in the above edge list matches self loops as successor-based, that \nis, matching the pattern (p,A,l,s) &#38; g to the expression (CA] ,A,l, Cl) &#38; Empty binds node A \nin list s even though it was placed in the predecessor list. One could think of binding A in both lists, \ns and p, but this would contradict the intuition that the rule (p,A,l,s) &#38; g => (p,A,l,s) &#38; g \ndenotes the identity on graphs. To see this consider the application to a graph con-sisting of just one \nnode with an edge to itself. If A were bound in p and s, the result expression would add two self loops \nto A. As another example, consider matching ( (p ,B, 1, s> &#38; g> to the expression exprDag. We get \n1 = op +, p = [D],s = CA,AI,andg=@-@ @. Thus we can define sue and de1 by: fun sue v ((p,v,l,s) &#38; \ng) = s fun de1 v ((p,v,l,s) &#38; g> = g Since the described pattern matching process not only com-putes \nbindings, but also performs an implicit reorganization of the matched value, we call &#38; an active \npattern. Note that this is not possible with laws/views (since computation is guided by external values, \nthat is, from the outside of the pattern); in [21] a similar feature is described for n + k-patterns \nin Haskell. The use of active patterns is actually not restricted to graphs, and it is an interesting \nlanguage concept in its own right with many subtleties (the reader might have noticed the non-linear \npatterns), for further details see [8]. We do not require &#38; as a language extension, instead we can \nalways replace a function definition fun f . . . v . . . Up,v,l,s) &#38; g> = e by using a predefined \noperation context for computing a node s context: fun f . . . v . . . g = let val ((p,v,l,s) &#38; g> \n= context (v,g > in e end The translation of active patterns in the general case when a function has \nmore than one rule can be found in [8]. In the sequel, however, we keep using the active pattern &#38; \nfor syntactic convenience. - 5 Graph Folding Whereas for data types the fold operation has a canonical \nform, reducing graphs can be done in quite different ways. 5.1 Unordered Fold A first approach is to \ndefine graph folding in strong analogy to data types, that is, given a binary function f : a context \n* b -> b, unordered fold is defined: fun ufold f u Empty =U 1 ufold f u (c &#38; g> = f (c,ufold f u \ng) Note that we do not use the active pattern &#38;. We can employ ufold to implement some basic functions, \nsuch as reversing edges, the function gmap from above, or testing node mem-bership: val grev = ufold \n(fn ((p,v,l,s),g>=>(s,v,l,p) &#38; g> Empty fun gmap f = ufold (fn ((p,v,l,s>,g>=>(p,v,f 1,s) &#38; g> \nEmpty fun gmember v = ufold (fn ((_,w,_,_),b)=>v=w orelse b) false However, the scope of ufold is somewhat \nlimited. This is mainly because we have no control about the order of graph decomposition, but this actually \nseems to be of high impor\u00adtance to many graph algorithms (already indicated by their name: depth-first, \nbreadth-first, best-jirst, and so on). 5.2 Linear Graph Fold When folding a data type value one always \nmoves forward from the current constructor (node) to the contained values (that is, successors). In contrast, \nthe graph constructor k also provides access to a node s predecessors. So we have to determine the fold \ndirection within the fold operator. We do this by a parameter function f, computing from a node s context \nthe list of nodes (1) which are to be accessed, that is, folded, next. Two such functions which will \nbe used in the sequel direct fold to the successors, respectively, prede\u00adcessors: fun fwd (p, V,l, S) \n= S fun bud (P, V,l, S) = p Now fold operates on a node v in two steps: first, fold is recursively applied \nto the list of nodes, 1, which is computed by f from v s context, yielding a list of results 1 . Since, \nin general, the length of 1 is varying, the results in 1 have to be accumulated in some way. This is \nachieved by a parameter function b which is (list-) folded along 1 , yielding a value r. A further parameter \nfunction d is finally applied to lab (the label of v) and r. Another parameter is the linearity of nodes, \nthat is, whether a node value can be used only once in a compu\u00adtation or if it might be used multiple \ntimes (when reached, for example, from different predecessors). We first consider the former option: \nonce we have matched a node context (p, v, 1,s) &#38; g we proceed with just graph g, thus forget\u00adting \nv. This is a bit dangerous since v might be tried to be matched in g later (coming from a different predecessor) \nthus causing a Match exception. Being aware of that fact, however, we can recover from exceptions by \ngiving meaning\u00adful defaults. In fact, this is done in the following definition of @old. We first define \ntwo functions for performing fold from just one node, respectively, from a list of nodes. fungfoldlfdbuv \n((c as (-, v,lab, -)) &#38; g) = let vel (r,gl) =gfoldn fdbu(f c) g in (d (lab, r) ,gl) end mdgfoldn__. \nu[] g = (U,g) I gfoldnfdbu(v::l)g= let val (x,gl) =gfoldl f db uvg val (y,g2) =gfoldn f db u1gl in (b \n(x. y) ,g2) end handle Hatch => gfoldn f d b u 1 g In addition to the accumulated value, both functions \nhave as a result the reduced graph. Performing successive fold calls always on these reduced graphs essentially \nensures that nodes are visited only once. In a sense, the graphs passed around represent the progressive \nconsumption of nodes from the original graph. The exception handling in gfoldn cap\u00adtures the following \ncase: when a node passed to gfoldn has akeady been consumed by a recursive call to gfoldl at the time \nit is to be processed, it causes a Match exception (in gfoldl). In that case gfoldn simply takes the \nnext node in the list. (Those who do not like programming with ex\u00adceptions might note that their use \nis not essential here. Al\u00adternatively, we execute the second RI-H of gfoldn only if gmeinber v g is true, \notherwise we cd gfoldn f d b u 1 g.) Now gf old performs gf oldn and drops the graph result: fungfold \nfdbulg=$ll(gfoldn fdbulg) In essence, gfold fvd performs depth-first search on graphs. As demonstrated \nin [14], many graph problems can be easily solved by first computing a depth-first spanning tree of the \ngraph. So we show how to compute it with gfold. We will represent trees of variable degree by the following \ndata type: datatype a tree z Branch of a * a tree list Now, df e is simply given by (with val Cons = \nop : :): fun dfs 1 g = gfold fud Branch Cons [] 1 g This definition for depth-first search is very different \nfrom the Haskell implementation presented in [14]. In particular, the way of maintaining the dfs-state \nis distinctive: instead of using state transformers, remembering already visited nodes is implicit in \nthe graph decomposition achieved by pattern matching. 1 Note that we have deliberately omitted a caae \nlike fungfoldl fdbuv Empty=u from the definition of gfoldl to obtain a more general typ\u00ading. As gfold \nis actually of type ( a context -> node list) -> ( a * b-> c) -> ( c * b-> b) -> b-> node list -> a graph \n-> b adding the above case would result in a unification of b with c entailing some effort to adjust \ndefinitions like that of df e. The similarity of gf old to dfs makes it the basis for many graph algorithms. \nSince we can establish general laws for gfold (see Section 6) graph algorithms become amenable to program \noptimization. Linear fold is different from fold on data types: there, multiple threads to a value are \npossible via the use of sharing variables. In a decomposition of a value containing multiple threads, \nsay, to a subvalue v, v is processed as many times as there are threads leading to it. This is not the \ncase for @old which processes just one thread.  5.3 Multiple Access Graph Fold An obvious generalization \nof gfold is to allow for multiple accesses to nodes which can be accomplished by re-inserting the currently \nmatched node v with only incoming edges (ex\u00adcept the one via which v is reached); multiple accesses to \nthe node are then possible through successor lists of other nodes that have not been processed yet. Node \nsharing and loops (edges from a node to itself) require caeful treatment within the fold operator: when \na node v is processed, that is, 1Actually, these nodesareforgottenand not remembered. a function d is \napplied to v s label lab and a value r result\u00ading from reducing the currently remaining graph, the result \nd (lab ,r) is not just returned as a value, but is also inserted as v s label into the graph to be reduced. \nThis ensures that the value is available at later stages of the reduction, and it furthermore avoids \nits recomputation. This accounts for nodes reached via more than one predecessor. When fold\u00ading a node \nv that contains an edge to itself, v is among its own successors, and eventually fold is applied to it. \nThus, v must be present in the argument graph passed to the re\u00adcursive fold call, that is, v must be \nre-inserted into g before the recursive cdl with its original label lab and without any predecessors \nand successors (this guarantees termination). Since the result type of the fold is, in general, different \nfrom the type of node labels we actually have to process a heterogeneous graph where nodes labels are \neither tagged SRC(not processed yet) or DEST(node carries a result value). We therefore use the following \nunion type: datatype ( a, b) hybrid = SRC of a I DEST of b Now we can define the function refold. In \nmfoldl we have to remove in each step exactly one edge the edge by which the current node v was reached. \nWe therefore have to pass as an additional parameter the node z from which v was accessed. Since there \nis no such node for any of the argument nodes initially passed to rnfold we use the option data type: \ndatat ype a option = SOMEof a I NONE and apply the SOMEconstructor to parent nodes and pass a nullary \nNONEto the initial call of rnfoldn. (This also hides the parameter from the interface of mfold.) For \nsimplicity we ornit the parameter f (recall the definition of gfold) and consider only a forward fold, \nthat is, we always move to successors. Thus, when reaching a node v with a DEST\u00adlabel, we can simply \nre-insert v with its current predecessors except z. fun mfoldl d b u (z, v) ((p, v,lab, s) &#38; g) = \ncase lab of DEST w => (v,(dropz p,v,DEST v,[]) &#38; g) I SRCW => let val (r, gl) = mfoldn d b u (SOMEV,S) \n(([], v,SRc w,[]) k g) val nev =d (v,r) in (nev, (drop z p,v, DEST nev, []) &#38; del v gl) end and mfoldn \n--u (-, [1) g = (U,g) I mfoldn d b U (z, v::l) g = let val (x,gl) =mfold~ db u(z,v) g val (y, g2) = MfOhiIl \nd b U (2,1) gi in (b (x, y), g2) end (drop (SOMEx) premoves oneoccurrence of theelementx from list p, \nanddxop NONEp= p.) Nowmfold first wraps upthe nodes of the graph with SRC,then reduces the graph by means \nof mfoldn, and finally drops the graph part of the result: funmfolddbulg= #l (mfoldn d b u (NONE,l) (gmap \nSRC g)) As an example, an evaluator for expression DAGs is given by the function evalDag. (The expression \nfilter p 1 selects all elements of the list 1 for which the predicate p yields true. ) fun pred v ((p, \nv,_,_) k -) = P fw roots g = filter (fn v+pred v g=[l) (nodes g) fun evalNode (CON i,_) = i I evalNode \n(OP f,[x,yl) = f (x, y) fun evalDag g = refold evalNode Cons [] (roots g) g It seems there are only few \napplications of refold: there must beaneed to fold along al ledges (folding along a span\u00adning tree can \nbe done with gfold), and the order of de\u00adcomposition must be important (otherwise ufold could be used). \nHowever, some advanced examples can be found in thetranslation ofvisual programs [9]. 5.4 Graph Backtracking \nBy passing the very same graph to all recursive fold calls of one successor list we obtain a backtracking \noperator: fun backtrack d b u v ((-,v,lab, s) &#38; g) = d (lab, backtrack d b u s (([], v,lab, []) &#38; \ng)) andbacktrack db unil g=u I backtrack d b u (v::l) g = b (backtrack d b u v g, backtrack d b u 1 g) \nWith backtrack we can compute, for example, all simple paths inagraph(letval append = op @): fun conspaths \n(v)l) = map (fn p=>v::p) 1 fun paths from s g = backtrack conspathe append [nil] [s] g fun allpaths g \n= fold append (map (fn ~=>pathsfr;rn v g) (nodes g)) [1 (Actually, the list ofpaths returned by allpaths \ncontains lV1-times the empty path.)  6 Program Fusion A popular optimization technique for functional \nlanguages is to eliminate intermediate results of multi-pass algorithms. Concerning graph algorithms, \nLaunchbury [15] gives some examples of howtofnse operations based ondfs. The following theorem shows \nthat program fusion alao applies toalgorithms specified by graph folds. (The proof is given in the Appendix.) \nTheorem2 (Promotion Theorem) lfPland Nwej?mc\u00adtions such that M (d (x, y)) =e (x,N y) N (b (X, y)) =f \n(n x,N y) Nu=u then: N(gfold hdbulg)=gfoldh efu lg As an application example consider the definition \nof topo\u00adlogic.d sorting as given in [14, 15]: fun postorder (Branch (v, f)) = postorderf f @ [vI and \npostorderf [1 = [1 I postorderf (t::f) = postorder t @ postorderf f fun tovsort E = re~ (poe~orderf (dfs \n(nodes g) g)) After unfolding the definition of dfs we can apply the pro\u00admotion theorem to obtain a version \nof topsort that does not build an intermediate tree structure. First, we match variables ofthe theorem: \nh = fud, d = Brsnch, b = Cons, u = [], and N=rev o postorderf. Nextwehaveto inventvaluesfor theremaining \nvariables: e = COBS,f = fn (x,y)=>y~x (append first argument to second), u = [l, and M=rev o postorder. \nNowwe check thepremises of the theorem: it is clear that N u= [] =u . Moreover: ki (d (X,y)) = rev (postorder \n(Brench (x,y))) = rev (postorderf y @ [xl)) = [xl @ (rev (postorderf y)) = x::(rev o postorderf) y = \ne (x,N y) N (b (X,y)) = rev (postorderf (x::y)) = rev (postorder x @ postorderf y)) = rev (postorderf \ny) ~ rev (postorder x) = (rev o postorderf) y ~ (rev o postorder) x = f (Hx,Ny) Thus weobtain the following \noptimized version oftopsort: fun topsort g = gfold fud Cons (fn (x,y)=>y@x) [1 (nodes g) g Theorem 2 \nfacilitates the elimination of intermediate tree structures which certainly haa many applications. Yet, \nit is challenging to investigate unfold/fold transformations to save intermediate graph structures, too; \naccording to Wadler [24] we could call this degraphation. As an examplewe optimize the implementation \nof Sharir s strongly-connected components algorithm wi given in [7, 14]: fun scc g = dfs (rev (postorderf \n(dfs (nodes g) g))) (grev g) The algorithm works by performing dfs on a graph with its edges reversed \n(grev g) while the argument node list of thetraversal must beareverse postorder list of thegraph s nodes \n(rev (postorderf . . . g)). Wecansave the inter\u00admediate graph resulting from the edge reversal by fusing \nthe definitionofdfs with that ofgrev. Todothisweuseadu\u00adality theorem that relates gfold fud to gfold \nbwd. (The proof csm befound in the Appendix. ) Theorem 3 (Duality Theorem) gfold fwddb u1 (grev g) = \ngfoldbwddbulg The application to the function scc gives (using the opti\u00admized versionof topsort): fun \nscc g = gfold bwd Branch Cons [] (gfold fvd Cons (fn (x,Y)=zYOX) [1 (nodes g) g) g 7 Implementation \nwe have implemented the proposed graph concept aa an extension of ML. At the core is a data structure \nfor per\u00adsistent graphs, that is, graphs that are nondestructively updated through applications of the \nk constructor and by decomposition. To our knowledge, data structures for per\u00adsistent graphs have not \nbeen investigated previously [20]. (The method of [6] cannot be used since it applies only to linked \nstructures with nodes of constant bounded in-degree.) Since, even for imperative graphs, no single graph \nrepre\u00adsentation exists that is optimal for idl kinds of applications, we initially focus on a representation \nsuited for sparse graphs and base our implementation on node-indexed arrays ofad\u00adjacency lists. Byusing \nfunctional arrays weensurethatsny update to the graph does not invalidate older graph ver\u00adsions. We use \nthe version tree implementation of functional arrays [1] in which updates take constant time and index \naccess time depends on the depth of the version tree. Ex\u00adtending version trees by an (imperatively updated) \ncache array that actuslly duplicates the array represented by the leftmost node in the version tree, \nindex access becomes 0(1) for single-threaded amays. Letusaasumefor amomentthatwe represent agraph by \nthree arrays L, S, and Pstoring node labels, successor and predecessor lists. Then adding a nodecontext \n(p,v,l, s) (of sizec) can be simply done by(i) setting the node label, (ii) adding successors, and (iii) \nadding predecessors as follows: (i) L[v]:=l (ii) S[v] :=S and Vu~p: S [u] :=v::,$[u]  (iii) P[v]:=p \nand Vw~s:P[w] :=v::P[w]. Thus, adding a node context takes O(c) steps plus the time tolocate allthelists \nS[u] and P[v]. In the worst case, this is O(cu) where udenotes thenumber ofupdates tog. (Note that u \nis generally not even bounded by the number of edges.) However, in single-threaded graphs, an adjacency \nlist is found in O(l), so k is O(c). Graph decomposition aa requested by a match (p,v,l,s) &#38; gis, \nin general, more complex: not only must we return v s label, its successors and predecessors, we also \nhave to build the reduced graph resulting from the deletion of context (p, v,l, s). Todothiswedeletev, \nand we remove v from all successor (predecessor) lists of v s predecessors (successors), that is, (i) \nL[v] :=1 (ii) Vu ~p:S[u]:= drop v S[u]  (iii) Vw6s:P[w]:=dropv P[w]. The costly operations are those \nin steps (ii) and (iii): we have to find O(c) adjacency lists, which requires O(cu) steps in general \nand O(c) steps in the singlethreaded case. The deletion ofvtakes O(c) time foreach list. Thus, &#38;is \nO(c2u) in general and 0(c2) in the single-threaded case. We can improve this implementation by exploiting \nthe following observation: the deletion of a node in any adja\u00adcency list can be noticed at the earliest \nwhen that list is requested by (another) context match. So in the implemen\u00adtation of &#38; instead of \nremoving v from adjacency lists, we just mark vaa deleted. (This isdonein an additional array V.) Butnowp \nandscannot simply be bound to S[v] and P[v], respectively, instead only those nodes are returned that \nare not marked as deleted in V. This means that building the reduced graph is 0(1) and computing p, 1, \nand s now takes O(u + c) steps (O(c) in the single-threaded case), amd this is also the complexity of&#38;. \nEven if this means a reduc\u00adtion in complexity only for non-sparse graphs, it is in any case an important \nimprovement in practice, since in addi\u00adtion to smaller constants within the big-Oh expressions, we also \nsave a lot of heap allocations. There remains one problem with the proposed approach: assume the context \nof node v is deleted and, for example, S[w] contains v. Now, if later on v is re-inserted without was \none of its predecessors, then v still must not be considered a successor of w. But this seems to be impossible \nsince we cannot mark v as deleted anymore. A solution is to equip nodes with a kind of time stamps : \nwhen a node v is in\u00adserted the first time, it gets a stamp, say 1 (that is, we set V [v] := 1) and we \nstore this stamp with each entry in an adjacency list. When v is removed, we set V [v] := V [v]. When \naccessing nodes in an adjacency list, we return only those nodes whose stamps in the list are equal to \nthat in V. So deleted nodes will be filtered out. Now when re\u00adinserting v we set V[v] := V[v] + 1 so \nthat old entries in adjacency lists still have non-matching stamps and will correctly be filtered out. \nThe importance of the structure lies in its behavior on single-threaded graph decompositions: 2 a function \nlike gf old has a running time of O(IV I + IE1), that is, is linear in the size of the graph. As an immediate \nconsequence of this, algorithms, such aa dfs, have the same complexity as in the imperative case. However, \nthe implementation of functional graphs bears a considerable overhead. To get an impression of the real \nbehavior, we compare the functional algorithms for graph reversal, dfs, and DAG evaluation with corresponding \nim\u00ad perative implementations. We also give the measures for a functional realization of the imperative \nalgorithms with functional arrays. The imperative algorithms make use of the imperative arrays of ML \nand represent a graph simply by two arrays for storing node labels and successors. The functional im\u00adplementation \nof the imperative algorithms use an efficient implementation of balanced binary search trees [2] to repre\u00adsent \nfunctional arrays. The algorithms are slightly changed to exploit the dynamic behavior of search trees \nand to ac\u00adcount for state threading. The functional algorithms are those defined in the paper, that is, \nthey are defined through ufold, gfold, and rnfold which are based on k and &#38; (or, cortt ext ) aa \nprovided by the persistent graph implement a\u00adtion described above. The source code is shown in the Ap\u00adpendix. \nAs expected, the functional algorithms are signifi\u00adcantly slower than the imperative ones. This is mainly \ndue to the intensive use of the heap caused by updates to the graph representing functional arrays. We \ncan improve the running time of the functional algo\u00ad rithms by providing predefine graph fold operations. \nCon\u00ad sider, for example, gfold: Instead of decomposing the ar\u00ad gument graph in each step, we can use \na local (imperative) array M to mark those nodes already matched during the current run of gfold: Initially, \nM is obtained by copying the stamp array V of the argument graph. Then each time the context of a node \nv is matched, M[v] is set to 1, and only those successors and predecessor of v are selected that 2NO~e~ha~the \n~OmpleXityOfthe general case cOuld be imprOved by employing advanced data structures for functional arrays, \nsuch as [5], although an implementation would require considerable effort. have a stamp equal to that \nin M. Similarly, refold can be improved by locally storing traversed edges in a haah table. The case \nfor ufold is more subtle. First, we observe that the chosen array representation of a graph forgets about \nits construction history. In particular, we do not know which node (context) waa inserted last. This \nimplies that with this implementation k cannot be used in pattern matching. So to implement uf old we \nrather need a function like mat chany that matches an arbitrary node context. Now a simple im\u00adplementation \nfor mat cheny will search for any (for example, the first) node that has a valid stamp. Used repeatedly, \nthis leads to a running time of uf old that is quadratic in the number of nodes. Thus, a predefine version \nthat scans the node array in a fixed order achieves linear complexity for uf old. The implementation \nalso uses a local imperative array M similar to the predefine @old. We ran the implementations of gxev \nand dfs on a sparse graph (with a degree of 8) with 1000, 5000, and 10000 nodes. The user time spent \nby SML/NJ 1.09 on a SPARCstation 10 is given in Figures 4 and 5. The functiordized -r ows show the times \nof a functional realization of the imperative algorithms. 1000 I 5000 I 10000 II ratios ~ Figure 4: Running \ntimes of grev. The predefine version of ufold improves the running time of gxev by an order of magnitude, \nbut it cannot com\u00adpete with the imperative implementation. However, it is recognizably faster that the \nfunctionalized implementation. 1000 5000 10000 ratios functional 0.08s 0.72s 1.58s 8..12 predefine gfold \n0.02s 0.17s 0.26s 2..3 imperative 0.01s 0.06s 0.13s 1 functionalized 0.21s 0.76s 2.28s 13..21 Figure \n5: Running times of dfs. We can see in Figure 5 that the predefine version of gfold performs quite well. \nIt is striking that gfold seems to run much faster than ufold. This is certainly because imperative dfs \nhaa to build a dfs-tree on the heap whereas imperative graph reverse only works on its imperative ar\u00adrays. \nThe function evaldag waa applied to tree-shaped DAGs where internal nodes have two successors and predecessors. \nThe results are shown in Figure 6. Againl the basic functional solution is extremely slow be\u00adcause refold \nhas to make intensive use of graph constructors. 5050 11325 20100 ratios functional 2.59s 7.41s 15.08s \n43..49 predefine rnfold 0.32s 0.82s 1.72s 5 imperative 0.06s 0.15s 0.32s 1 functionalized 0.46s 1.18s \n2.35s 7..8 Figure6: Running times ofevaldag.    Conclusions We have presented a new programming style \nfor graphs that draws much of its attraction from being baaed on pattern matching and value decomposition, \nwhich are well-known and accepted programming concepts. The most significant difference between this \naad previous approaches is the de\u00adparture from the imperative view of graph traversals, giving more opportunities \nfor program transformation and opti\u00admization. Although more work is required on functional graphs and \nefficient graph operations, experiments with an initial implementation are encouraging showing that the \npre\u00adsented approach is a reasonable and practical alternative to imperative graphs in functional languages. \nIn particular, predefine graph operations offer much potential for further efficiency improvements. References \n[1] A. Aaaa, S. Holstrom, and C. Nilsson. An Efficiency Comparison of Some Representations of Purely \nFunc\u00adtional Arrays. BIT, 28:490-503, 1988. [2] S. Adarns. Efficient Sets -A Balancing Act. Journal of \nllmctional Programming, 3:553 561, 1993. [3] R. S. Bird. Functional Algorithm Design. In Mathemat\u00adics \nof Program Construction, LNCS 947, pages 2 17, 1995. {4] F. W. Burton and H.-K. Yang. Manipulating Multi\u00adlinked \nData Structures in a Pure Functional Language. Software -Practice and Experience, 20(11):1167-1185, 1990. \n[5] P. F. Dietz. Fully Persistent Arrays. In Workshop on Algorithms and Data Structures, LNCS 382, pages \n67\u00ad74, 1989. [6] J. R. Driscoll, N. Sarnak, D. D. Sleator, and R. E. Tar\u00adjan. Making Data Structures \nPersistent. Journal oj Computer and System Sciences, 38:86-124, 1989. [7] M. Erwig. Graph Algorithms= \nIteration + Data Struc\u00adtures? The Structure of Graph Algorithms and a Cor\u00adresponding Style of Programming. \nIn 18th Int. Work\u00adshop on Graph Theoretic Concepts in Computer Sci\u00adence, LNCS 657, pagea 277 292, 1992. \n[8] M. Erwig. Active Patterns. In 8th Int. Workshop on Im\u00adplementation of Functional Languages, pages \n95 112, 1996. To appear in LNCS. [9] M. Erwig and B. Meyer. Heterogeneous Visual Lan\u00adguages Integrating \nVisual and Textual Programming. In 1Ith IEEE Symp, on Visual Languages, pages 318\u00ad325, 1995. [10] L. \nFegaras and T. Sheard. Revisiting Catamorphisms over Datatypes with Embedded Functions. In ACM Symp. \non Principles of Progmmming Languages, pages 284-294, 1996. [11] J. Gibbons. An Initial Algebra Approach \nto Directed Acyclic Graphs. In Mathematics of Program Construc\u00adtion, LNCS 947, pages 282 303, 1995. [12] \nZ. Hu, H. Iwssaki, and M. Talceichi. Deriving Structural Hylomorphisms from Recursive Definitions. In \n1stInt. Conf. on Functional Progmmming, pages 73-82, 1996. [13] Y. Kaahiwagi and D. Wise. Graph Algorithms \nin a Lazy Functional Programming Language. In dth Int. Symp. on Lucid and Intentional Pragmming, pages \n3546, 1991. [14] D. J. King and J. Launchbury. Structuring Depth-First Search Algorithms in Haakell. \nIn ACM Symp. on Prin\u00adciples of Progmmming Languages, pages 344 354, 1995. [15] J. Launchbury. Graph Algorithms \nwith a Functional Flavour. In 1st Int. Spring School on Advanced l%nc\u00adtional Progmmming, LNCS 925, pages \n308 331, 1995. [16] J. Launchbury and T. Sheard. Warm Fusion: Deriving Build-Catas from Recursive Definitions. \nIn Conj. on I%nctional Progmmming and Computer Architecture, pages 314-323, 1995. [17] G. Malcom. Homomorphisms \nand Promotability. In Mathematics of Progmm Constructio~ LNCS 375, pages 335-347, 1989. [18] E. Meijer, \nM. Fokkinga, md R. Paterson. Functional Programming with Bananaa, Lenses, Envelopes and Barbed Wire. \nIn Conj. on fictional Progmmming and Computer Architecture, pages 124-144, 1991. [19] R. Milner, M. Tofte, \nand R. Harper. The Definition of Standard ML. MIT Press, Cambridge, MA, 1990. [20] C. Okasaki. Functional \nData Structures. In Advanced l%ctional Programming, LNCS 1129, pages 131-158, 1996. [21] P. Palao Gonstanza, \nR. Peiia, and M. Ndiiez. A New Look at Pattern Matching in Abstract Data Types. In Ist Int. Conf. on \nl%nctional Pmgmmming, pages 110\u00ad121, 1996. [22] T. Sheard and L. Fegaraa. A Fold for all Seasons. In \nConf. on Rmctional Progmmming and Computer Ar\u00adchitecture, pages 233 242, 1993. [23] P. Wadler. Views: \nA Way for Pattern Matching to Cohabit with Data Abstraction. In ACM Symp. on Principles of Progmmming \nLanguages, pages 307-313, 1987. [24] P. Wadler. Deforestation: Transforming Programs to Eliminate Trees. \nTheoretical Computer Science, 73:231-284, 1990. Appendix Proof of Theorem 2. We perform an induction \non 1 and g. 1=[] From the definition of gfoldn we see immediately that gfoldn h d b u [] g = (u,g) and \nthus @old hdbu [] g=u. Likewise, gfoldhefu [1 g= u . Now the conclusion of the theorem follows directly \nfrom the third premise. g= Empty,l=v::l gfoldn causes a call gfoldl h d b u v Emptyraising a Match exception \nthat is handled to return @~ldn h-d b u 1 Empty. Since no element of 1 can be matched in the empty graph, \nwe know by induction on 1 and by the previous case that gfold h d b u 1 Empty = u. For the same reason, \n@old h e f u} 1 Empty =u , and for this case the theorem follows from the third premise. g=c&#38;g ,l=v::l \nHere gf old first causes a call gfoldl h d b u v g which either succeeds or results in a Hatch exception. \nIn the latter case handling the exception yields gfoldn h d b u 1 g, and since in that case the corresponding \nexpression gfoldl h e f u v g also raises Hatch and yields gfoldn h e f u 1 g, we can assume the theorem \nby induction. Otherwise, the gfoldl expression results in a pair (x,gl) with x = d (lab,r) where lab \nis the label of v and r is the first component of the recursive call gfoldn hdb usg (wheres=f c). Inthesame \nway we obtain a value r as the first component of the expression gfoldn h e f u s g , and we can apply \nthe induction hypothesis to obtain tJr=r (1) After the gfoldl expression has been evaluated, a pair \n(y,g2) is computed by gfoldn h d b u 1 gl. Simi\u00adlarly, a pair (y ,g2) is given by the expression gfoldn \nh e f u 1 gl. Note that the graphs gl and g2 are actually identical in the two folds since the graph \ndecomposition is only affected by h, l/1 , and g/g which are identical in the corresponding fold expres\u00adsions. \nNow we can apply the induction hypothesis agtin and get Ny=y (2) Finally,theresultof@old hdb u1gisb \n(x,y) = b (d (lab, r), y). Likewise, the result of gf old h e f u 1 gis f (e (lab, r ), y ). Now we can \nconclude: N(gfoldhdbulg) = N (b (d (lab, r), y)) = f (H (d (lab, r)),N y) (2nd premise) = f (e (lab,N \nr)), N y) (lst premise) = f (e (lab, r )), y ) (ind.hyp. (l), (2)) =gfoldhefu lg .  Proof of Theorem \n3. The proof is by induction on 1 and g. For (1 = []) and (g = Empty,l = x::l ) the result of gfold is \nalways u, and thus the theorem is true for these cases. Thus consider the caae (g # Empty,1 = v: : 1 \n). gfoldn bwd computes a pair (x ,gl) through a call to gfoldl bwd. There g can be written (due to active \npattern matching) as3 (p, v ,lab, s) &#38; g which means that the pair (r, g2) in gf oldl bud is given \nby the expression gf oldn bwd d b u p Since grev g = (s, v,lab, p) &#38; (grev g ) the corre\u00adsponding \ngfoldl expression caused by gfold fwd d b u 1 g can be written as gfoldl fwd d b u v ((s, v,lab,p) &#38; \n13 (grev g ) ) ifs does not contain v. (This restriction is nec\u00adessary because otherwise v would be moved \nthrough the matching horn s into p.) Then the pair (r, g2) in the call gfoldl fwd k given by the expression \ngfoldn fwd d b u p (grev g ). NOW the theorem follows by applying the induc\u00adtion hypothesis to gfoldn \nbwd d b u p g and gfoldrt f wd d b u p (grev g ) and to the remaining calls gfoldn fwd d b u 1 (grev \ngl) in gf oldn, respectively, gfold bud d b u 1 gl in gf oldn bwd. If, on the other hand, v is contained \nin s, the gfoldl expression caused by gfold fwd d b u 1 g can be written as gfoldi fwd d b u v ((s , \nv,lab, p ) &#38; (grev g >) ) where s } is equal to s with all occurrences of v re\u00admoved and p is p with \nall occurrences of v in s appended, say. This means the pair (r ,g2) in the call gf oldl f wd is given \nby the expression gf oldn f wd d b u p (grev g ). The difference to the expression above is some occurrences \nofvin p . Now since v is not contained in grev g the eventually caused gfoldl calls with v will all raise \na Hatch exception (which are handled by just moving in p to the next node). Thus, the presence of v in \np has actually no impact on the result compared with the corresponding com\u00adputation using p. This means \nthat gfoldn f wd d b u p (grev g ) yields the same result as gfoldn f wd d b u p (grev g ), so we can \nagain apply the induction hypothesis, and the theorem is proved. . fun id x=x fun pl (x,-) =x fun forceOpt \n(SO14S x) = x fun select -_ [1 =[1 I select f p (x: :1) = if pxthen f x::select f p1 else select f p \n1 Figure 7: Some utility functions. signature PUU-A3RAY = s ig  type a array val array : int * a -> \na array val sub a array * int -> a val size : a array -> int val update : a array * int * a -> a array \nval toIrnpArray : a array -> a Array. array val frond..ist : a list -> a array val f rornImpArray \n: Ja Array. array -> a array end Figme 8: Operations on functional arrays. 3The cme for the Match exception \nis identical to TheOrem 2 structure FunArray : FUN. ARRAY = struct datatype a array = Root of a Array.array \nI Node of int * a * a array I Cache of int * Ja * a array * bool ref * a Array.erray fun array (n,x) \n= Cache (O,x,Root (Array.enay (n,x)), ref true,Array.array (n,x)) fun search (Cache <_,_,Root a,_,_),i) \n= Array.sub (a,i) I search (Cache (j,x, tree,_,_),i) = if i=j then x else search (tree,i) I search (Node \n(_,-,Root a),i) = Array.sub (a,i) I search (Node (j,x, tree),i) = if i=j then x else search (tree,i) \n and sub (tree as Cache (_,_,-, ref cache,c),i) = if cache then Array.sub (c,i) else search (tree,i) \nI sub (tree,i) = search (tree,i) fun size (Root a) = Array.length a I size (Node (_,_,a)) = eize a I \nsize (Cache (_,-,-,-,a)) = Array.length a  fun update (a as Cache (_,_,-,cache,c),i,x) = if !Cache then \n(cache := false; Array.update (c,i,x); Cache (i,x,a,ref true,c)) else Node (i,x,a) I update (a,i,x) = \nNode (i,x,a) fun fromList 1 = Cache (O,hd l,Root (Array.fromList 1), ref true,Array.fromList 1) fun fromImpArray \na = let val b = Array.erray (Array.length a,Array.sub (a,O)) in (Array.copy {src=a,si=O,len=NONE,dst=b,di=O]; \nCache (O,Array.sub (a,O),Root a,ref true,b)) end fun toImpArray (Cache (_,_,Root a,_,_)) = (* is used \nonly on unchanged arrays *) let val b = Array.array (Array.length a,Array.sub (a,O)) in (Array.copy {src=a,si=O,len=NONE,dst=b,di=O]; \nb) end end Figure9: Implementation of functional arrays with cache. signature GRAPH= sig eqtype node \n= int type a context = node list * node * a * node list type a graph exception Node val empty : int -> \na graph val% a context * a graph -> a graph val context : node * a graph -> a context * a graph val \nmatchany : a graph -> a context * a graph val noNodes a graph -> int val graap -> a graph -> b graph \n : (Ja-_> ~b) val ufold : ( a context * b -> b) -> b -> a graph -> b val gfold : ((node * int) list \n* node * a * (node * int) list -> (node * int) list) -> ( a * b-> c) ->(1c*~b-> lb) -> b -> node list \n-> a graph -> b val refold : ( a* b -> c) -> ( c * b -> b) -> b -> node list -> a graph -> b end Figure \n10: Operations on functional graphs. functor Graph (FunArray: FUN_ARRAY) : GRAPH= struct type node = \nint type a context = node list * node * a * node list exception Node exception Edge  (* additional array \nfunctions *) open FunArray fun apply (a,i,f) = update (a,i,f (sub (a,i))) fun firstIndex (a,p) = let \nfun scan (i,p) = if p (sub (a,i)) then i else scan (i+l,p) in scan (O,p) end fun arrayToList f a = let \nfun list (f,a,i,n) = if i<n then f (sub (a,i))::list (f,a,i+l,n) else [1 in list (f,a,O,size a) end fun \nimpArrayToList a = let fun list (a,i,n) = if i<n then Array.sub (a,i)::list (a,i+l,n) else [1 in list \n(a,O,Array.length a) end (* etap type and operations *) type stamp = int fun stampTrue i = abs i+l fun \nstampFalse i = (abs i+l) fun getStamp (na,n) = sub (na,n) fun getlJegStamp (na,n) = let val s=sub (na,n) \n in if s>O then raise Node else s end fun getPosStamp (na,n) = let val s=sub (na,n) in if s<=O then \nraise Edge else s end datatype a graph = Empty of int I Full of stamp array * a array * (node * stamp) \nlist array (* pred *) * (node * stamp) list array (* Suc *)  (* basic graph operations *) fun empty \nn = Empty n  infixr 5 k fun (pred,n,l,suc)&#38;(Empty i) = (pred,n,l,suc)&#38;(Full (array (i,O),array \n(i,l),array (i,[]),array (i, []))) I (pred,n,l,suc)ft(Full (na,la,pa,sa)) = let val stsmpN = stsmpTrue \n(getNegStemp (na,n)) val stsmpedPred = raap (fn x=>(x,getPosStemp (na,x))) pred vsl stsmpedSuc = map \n(fn x=>(x,getPosStamp (na,x))) suc fun updAdj (a,[]) =a   I updAdj (a,v::l) = updAdj (apply (a,v,fn \nadj=>(n,stsmpN) ::adj),l) in Full (update (na,n,stempN),update (la,n,l), updAdj (update (pa,n,stampedPred),suc),updAdj \n(update (sa,n,stampedSuc),pred)) end fun context (n,Empty _) = raise Match I context (n,Full (na,la,pred,suc)) \n= if getStamp (na,n)>O then ((select PI (fn (v,i)=>i=getStamp (na,v)) (sub (pred,n)),n,sub (la,n), select \npl (fn (v,i)=>i=getStamp (na,v)) (sub (suc,n))), Full (apply (na,n,stampFalse),la,pred,suc)) else raise \nMatch fun matchany (Smpty _) = raise Match I matchany (g as (Full (na,la,pred,suc))) = (context (firstIndex \n(na,fn i=>i>O),g) handle Subscript => raise Match) Figurell: Functional graph implementation (Part 1). \n. fun noNodes (Empty .) 0 I noNodes (Full (na, _,_,_)) = size na fun gmap f (Empty i) = Empty i I gmap \nf (Full (na,la, pa, sa)) =  let val x = f (sub (la,O)) val n = ref (size la) val L = Array.array (!n,x) \nval -= while (!n>O) do (n := !n-l;Array.update (L,!n,f (cub (la, !n)))) in Full (na,fromImpArray L,pa,sa) \nend (. predefine fold operation *) fun ufold f U(hqky _) =U I ufold f u (Full (na,la,pred,suc)) = let \nval V = toImpArray na val n= Array.length V fun ufoldi x = if xtn then let val c = (select pl (fn (v,-)=>Array.sub \n(V,V)>O) (sub (pred,x)), x,sub (la,x), select pl (fn (v,i)=>Array.sub (V,V)>O) (sub (suc,x))) val -= \nArray.update (V,x,-l) val r = ufoldi (x+1) in f (c,r) end elee u in ufoldi O end  fungfoldfdbul(hpty \n_)=U I gfold f d bu 1 (Full (na,la,pred,suc)) = let val V = toImpArray na fun gfoldl V= (Array.update \n(V,v,-l); let val l=sub (la,v) in d (l,gfoldn (f (sub (pred,v),v,l,sub (suc,v)))) end) and gfohh [] =U \nI gfoldn ((v,i)::l} = let val j = Array.eub (V,v) in if j<O orelse i<>j then gfoldn 1 elee b (gfoldl \nv,gfoldn 1) end and @oMm [1 =U I gfoldm (V::l) = if Array.eub (V,V)<O then gfoldm 1 else b (gfoldl v,gfoldm \n1) in gfoldm 1 end Figure 12: Functional graph implementation (Part 2). fun refold d b u 1 (Empty _) \n=u I refold d b u 1 (Full (na,la, pred, sue)) =  let val V = tolm@rray na val n = Array.length V val \nU = Array.array (n,false) val L = Array.array (n,NONE) exception NotFound fun e2word (v,w) = Word.fromInt \n(v*5+w) val E = HashTable.mkTable (e2word,fn (v,v)=>v=w) (5*n+l,NotFound) fun mfoldl (pred,v) = (case \npred of SOME z > HashTable.insert E ((z,v),true) I NONE > (); if Array.sub (U,v) then forceOpt (Array.sub \n(L,v)) else (Array.update (U,v,true); let val x=d (sub (la,v),mfoldn (v,sub (suc,v))) in (Array.update \n(L,v,SONE x); x) end)) and mfohin (.,[1) =U I mfoldn (z, (v, i)::l) = let val j = Array.sub (V,v) val \ne = getOpt (HashTable.find E (z,v),false) in if j<O orelse if>j orelee e then mfoldn (z,l) else b (mfoldl \n(SOMEz,v),mfoldn (z,l)) end and mfoldm [1 I mfoldm (v::l) ~ ~f Array.sub (V,V)<O then mfoldm 1 else \nb (mfoldl (NONE,v),mfoldm 1) in mfoldm 1 end end (* functor Graph *) Figure 13: Functional graph implementation \n(Part 3). fun dfs d bu (N,SUC) = let val V = Array.array (Array.length N,fel.se) fun dfsl v = (Array.update \n(V,v,true); d (Array.sub (N,v),dfsn (Array.sub (SUC,V)))) end dfen [] =U I dfsn (v::l) = if Array.eub \n(V,v) then dfsn 1 else b (dfsl v,dfsn 1) in dfsn (List.tabulate (Array.length N-l,id)) end fun dfs g \n= dfs Branch (op ::) [1 g fun evaldag (N,SUC) = let val R = Array.erray (Array.length N,Array.sub (N,O)) \nVal -= Array.copy {src=N,si=O,len=NONE,dst=R,di=O}; fun evall v = case Array.sub (R,v) of CONi => i \nI OP f => let val result = f ((fn [x,y] => (x,y)) (evaln (Array.sub (Suc,v)))) in (Array.update (R,v,CON \nresult) ;result) end and evaln [1 = [1 I evaln (x::l) =evall x::evaln 1 in evaln (roots (N,SUC)) end \nfun grev (N,SUC) = let val R = Array.array (Array.length N,[]:int list) fun scan (i,[l) = () I scan (i,v::l) \n= (Array.update (R,v,i::Array.sub (R,v)); scan (i,l)) val _ = Array.appi scan (SUC,O,NONE) in (N,R) end \nFigure 14: Imperative graph algorithms. \n\t\t\t", "proc_id": "258948", "abstract": "Graph algorithms expressed in functional languages often suffer from their inherited imperative, state-based style. In particular, this impedes formal program manipulation. We show how to model persistent graphs in functional languages by graph constructors. This provides a decompositional view of graphs which is very close to that of data types and leads to a \"more fictional\" formulation of graph algorithms. Graph constructors enable the definition of general fold operations for graphs. We present a promotion theorem for one of these folds that allows program fusion and the elimination of intermediate results. Fusion is not restricted to the elimination of tree-like structures, and we prove another theorem that facilitates the elimination of intermediate graphs. We describe an ML-implementation of persistent graphs which efficiently supports the presented fold operators. For example, depth-first-search expressed by a fold over a functional graph has the same complexity as the corresponding imperative algorithm.", "authors": [{"name": "Martin Erwig", "author_profile_id": "81100586223", "affiliation": "FernUniversit&#228;t Hagen, Praktische Informatik IV, 58084 Hagen, Germany", "person_id": "PP31077481", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/258948.258955", "year": "1997", "article_id": "258955", "conference": "ICFP", "title": "Functional programming with graphs", "url": "http://dl.acm.org/citation.cfm?id=258955"}