{"article_publication_date": "08-01-1997", "fulltext": "\n Implementing Bit-addressing with Specialization Scott Draves School of Computer Science Carnegie Mellon \nUniversity 5000 Forbes Avenue, Pittsburgh, PA 15213, USA Abstrsct General media-processing programsare \neasily expressed with bit\u00adaddressing and variable-sized bit-fields. But the naturalimplemen\u00adtationof \nbit-addressingreliesondynamicshiftoffsetsandrepeated loads,resultinginslowexecution. Ifthecode isspecializedtothe \nalignmentof thedatsagainstwordboundaries,theoffsetsbecome staticandmanyrepeatedloads can be removed. \nWe showhow in\u00adtroducingmodulararithmeticinto anautomaticcompiler generator enables the transformationof \na program that uses bit-addressing into a synthesimx of fast specialized programs. In partiai-evrduation \njargon we say modular arithmetic is sup\u00adported by extending the binding time lattice used by the static \nanal\u00adysis in a polyvsriant compiler generator. The new binding time Cyclic functions like a partially \nstatic integer. A software cache combined with a fast, optimistic sharing anal\u00adysis built into the compilers \neliminates repeated loads and stores. The utility of the transformation is demonstrated with a collection \nof examples and benchmark data. The examples include vector arithmetic, audio synthesis, image processing, \nand abase-64 codw.  Introduction Media such as audio, images, and video are increasingly common in computer \nsystems. Such data are represented by iarge arrays of small integers known as samples. Rather than wasting \nbhs, sam\u00adples are packed into memory. Figure 1 illustrates three exampies: monaural sound stored as an \narray of 16-bit vahres, a grayscale im\u00adage stored as an array of 8-bit values, and a color image stored \nas interleaval 8-bit arrays of red, green, and blue samples. Such ar\u00adrays are caiied signals. Say we \nspecify a signal s representation with four integers: from and to are bit addresses; size and stride \nare numbers of bits. We use iittle-endian addressing so the least significant bit of each word (LSB) \nhas the ieast address of the bits in that word. .This reseamhwas sponsoredin part by ths De.ferrssAdvanced \nResearch Projects Agency CS 10 under tbs titfe lk Fox Projew AdvancedLanguage-sfor Systems Sotiwsrs, \nARPA Ordsr No. C533, issuedby ESCENS underContractNo. Fl%28\u00ad95-C-0050. The visws and conclusions contairrsdin \nthis docutmnt arc those of the aorkors and should not be kapraed as repmemting the officisl policies, \neither ex\u00adpressedor irnptied,of the Defense Advanced Research projects Agency or the U.S. Go nt. Permission \nto make digital/hard copy ot part or ail tnls worK Tor personal or classroom use is granted without fee \nprovided that copies are not made or dis~ributed for profit or commercial advan\u00ad tage, the copyright \nnotice, the title of the publication and its date appear, and notice is given that copying is by permission \nof ACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires \nprior specific permission and/or a fee. ICFP 97 Amsterdam, ND  @ 1997 ACM 0-89791 -918 -1/97 /0006 ... \n$3.50 (a) (c) Or OgOb lr CT % Fiare 1: Lavout of (a) 16-bit monaural sound. (b) an 8-bit gr~yscale image, \nand (c) a 24-bit color image. ~e heavy lines indicate 32-bh word boundaries. type signal = int * int \n* int * int ( * from to size stride *) Figure 2 gives the code to sum the elements of a signal. This \nand other examples use ML syntax extended with infix bit opera\u00adtions as found in the C programming ianguage \n(<< >> &#38; I). The loa~word primitive accesses a memory location. Thk pa\u00adper assumes 32-bit words, \nbut any other size could just as easily be substituted even at run-time. The integer division (/) rounds \ntoward minus infinity; integer remainder (%)has positive base and result. To simplify this presentation, \nload-sramp le does not han\u00addle samples that cross word boundaries. If we fix the layout by assuming stride \n= size = 8 and (from % 32) = (to % 32) = O then the implementation in Figure 3 computes the same value, \nbut runs more than five times faster (see Figure 21). There are several reasons: the loop is un\u00adrolled \nfour times, resulting in fewer conditionals and more instruc\u00ad tion level parallelism; the shift offsets \nand masks are known stati\u00adcally, allowing immediate-mode instruction selection; the division and remainder \ncomputations in load.samp 1e are avoided; redun\u00addant loads are eliminated. Different assumptions result \nin different code. For example, se\u00adquential 12-bit sampies result in rmroliing 8=lcm( 12,32)/12 times \n fun sum (from, to, size, stride) r = t if from = to then r else sum ((from+stride), to, size, stride) \n-a -b (r + (load_sample from size)) fun load_sample p b = 4$w ((l<<b) -1) &#38; ((load_word (p / 32)) \n>> (p % 32)) -c Q-P T t  Figure2:Sumrning asignalusingbitaddressing. fun sum_OOB8 from to r = if \nfrom = to then r else let val v = load_word from in sunLO088 (from + 1) to {r+ {v&#38;255) + ((v>>8) \n&#38;255) + ((V >> 16) &#38; 255)+ ((V >> 24) &#38; 255)) end Figure 3: Summing a signal assuming packed, \naligned 8-bit sam\u00adples as in Figure l(b). so that three whole words are loaded each iteration (see Figure \n4). Handling samples thatcross wordboundaries requiresaddinga conditional to load-ample that loads an \nadditional word, then doesashift-mask-shift-or sequenceofoperations. As such, the programmer is faced \nwith a familiar trade-off write one slow, easy-to-read, general-purpose routine; or write many fast spmial \ncases. We pursue analtemative write general\u00adpurposecodeand automatically derivefastspezialcases. Thetech\u00adniques \npresented here are designed to be fast enough to generate special cases lazily at run-time, thus providing \nan interface to run\u00adtime code generation (RTCG). It is not strictly necessary that spe\u00adcirdization occur \nat run-time, but because the number of special cases is exponential in the number of static arguments, \ncode space quickly becomes a problem if the specialization is all done at com\u00adpile time, as with macro \nand C++ template expansion. As a concrete example consider the screen position of a win\u00addow. The horizontal \ncoordinate affects the alignment of its pixels against the words of memory, so special-purpose graphics \nopera\u00adtions may be created each time a window is opened or moved. As another example, consider an interactive \naudio designer. A partic\u00adular voice is defined by a small progranx Figure 5 is a typical example of an \nFM synthesizer. Most systems allow the user to pick from several predefine voices and adjust their scatar \nparameters. Whh RTCG, the user may define voices with their own wiring ctia\u00adgrams. Other interfaces to \ninn-time code generation have been ex\u00adplored in a variety of places: there have been manual systems such \nas Common Lisp [Steele90] with eval, macros with back\u00adquote/comma syntax, and slow code generation. Fast \nmanual sys\u00adtems such as Synthesis [MassaJin92] and the Blit terminal ~-LoRei85] confirmed the performance \nbenefits of RTCG in operat\u00ading systems and bit-mapped graphics, respectively. C [EnHsKa95] II II I I \n Figure 4: 12-bit signal against 32-bit words shown with abbreviated vertical axis. Figure 5: Two voices. \nOn the left is a simple 2-in-1 FM synthesizer. Oscillators a and b sum to modulate c as well as feeding \nback into a. On the right is another possibility. adds a Lispstyle interface to RTCG to the C programming \nlan\u00adguage. Fabius [LeLe96] uses fast automatic specialization for mn\u00adtime code generation of a subset \nof ML, but cannot handle bit\u00adaddressing. Tempo [COHONONOV096]attempts to automate the kind of RTCG used \nby Synthesis. Self takes an automatic but less general approach to run-time code generation [ChaUng91], \nas do recent just-in-time (JIT) implementations of Java [GoJoSte96]. Past work in bit-level processing \nhas not emphasized imple\u00admentation on word-machines. VHDL [IEEE91 ] allows this level of specification, \nbut lacks an efficient compiler. Synchronous real\u00adtime languages like Signal [GuBoGaMa91] support programming \nwith streams, but not at the bit level. This paper shows how to implement bit-addressing with a par\u00adtird \nevaluator. Section 2 presents a polyvariant, direct-style specialize and briefly describes how to derive \na compiler generator from it. Sec\u00adtion 3 extends the specialize with cyclic integers, resulting in an \nanalysis similar to [Granger89]. Swtion 3.2 shows how irregular (data-dependent) layouts are handled. \nSection 4 shows how ex\u00adtending of partial evaluation allows fast elimination of redundant loads and stores. \nSection 5 describes two implementations of these ideas; Section 6 presents example source programs and \ncompares the performance of the generated code with hand-written C pro\u00adgrams. 2 Specialization We begin \nour discussion of specialization with a definition, then we introduce our notation and give a simple \npolyvariant specialize for a A-language. Section 2.1 discusses efficient implementation via compiler \ngeneraton and introduces the concept of bhiing times. Section 2 is generally a review of partial evaluation \npractic~ [Jo-GoSe93] is the standard text of the field and may be considered a reference of first resort \nif you can find it. ~eiCoRuSe91] is a more widely available description of an advanced on-line special\u00adize. \nThe system described here is a polyvariant version of type\u00addirected partial evaluation [Danvy96], much \nlike [Sheard96]. A specialize mix satisfies the following equation where italic names denote program \ntexts and Quine quotes [.] denote ordinary evrduation [t] ZY= [[mizl t z] y There are many ways to implement \n[mix]; a simple curry function suffices. Our intension is that [ miz ] will do as much work of ~ as is \npossible knowing only its first argument and return a residiuzl program that finishes the computation. \nBecause we expect to use this residual function many times, this gives us a way of factoring or staging \ncomputations as in [JoSche86]. t E Type : := atom I Type -> Type Figure 6 gives the grammar of our object \nlanguage, and de\u00ad d, eE Exp ::= Val I Var I Exp Exp fines some domains and their metavariables. The \nlanguage is the I if Exp Exp Exp A-calculus extended with explicit types on abstractions, constants, \nI lambda Var: Type . Exp primitives, a conditional, and a lift annotation. I lift Expl Expo Exp I . \n. . We say the lift is an annotation because in the ordinary se\u00admantics of the A-calculus, lift has \nno meaning; it becomes the iden-s,~EVal=Bool+Z tity function. The ordinary semantics can be useful for \ndebugging. ttEF=(M+M)x Type Figure 7 gives a specialize S. The notation [w+k 1p denotes mEM=Exp+Val+F \nupdating the environment p with a binding from the variable v to pCEnv=Var+M the vahre k; o denotes a \ngeneric, black box binary primitive oper\u00ad ation; frames mark manipulation of the terms of the A-1anguage \ns Figure 6 The Manguage, domains and metavsriables. o is a prim- Q e isp s backquote); match e pat + \ne... denotes pat\u00ad syntax (li itive Var is the set of variables. tern matching where the metavariables \nonly match the appropriate domain. Figure 8 defines the reification and reflection fimctions R and L. \nThey operate as coercions between code and data; understand\u00ading them is not essential to this work. s \n:Expx Env+M S is aparriul-svufuation functiom it assigns a meaning from M s F] P= match (S eo p, Se, \np)to a source text with environment. The difference from an ordinary (so, 81) + so o S1 semantics is \nthat M contains Exp, whose members represent com\u00ad (dO, dl) + m putations dependent on unknown values, \ni.e. are residual code. We say the specialize emits residual code. . P=Pv s WesayS is polyvariantbecause \na given piece of syntax may s  be both executed by S and emitted as residual. This happens to f in this \nexample s l?&#38;&#38;!klP= (Av . S e ([u+v IP) )t let fun fx=x+l s l-~ P = retch (S eo p, Se, p) fungsd=(fs)+(fd) \n(i, m)+fm in (g 1 (lift 1)) (do, dl) + ml end s =R(Sep) mP Creating general code and a special case \nof the same source text s lfeoele2p= ~ corresponds to the standard fast-path optimization technique. \nmatch (S en o)Note that the if clause requires that when a conditioned has SO+ if s; then (S el p) \n dynamic predicate, then both arms are also dynamic.  else (S e2 p) S is similar to the A-mix of [GoJ091], \nbut because A-mix  &#38;+letdl=Selo is monovariant, it uses a two-level input language where source \n dz = S e2 p lambda terms have been labeled either for execution or immedi\u00ad in if&#38; then dl else \nd2 ate residualiration. ISreserves judgement until the A is applied; S end depends on lift annotations \nto emit functions. Note that many cases m missing from S. We assume that all input programs are type-correct \nand lift annotations appear as Figure 7: A direct-style polyvariant specialize, necessary. Placement \nof the Iitls is crucial to successful staging too many lifts and S degenerate into the curry function; \ntoo few and S fails to terminate. ~pically binding time analysis (BTA) is combined with programmer annotations \nto insert the lifts. For example, if p= [a+6 b+~l then S requires ( (lift a) ob) R :M+Exp rather than \n(aob ). his kind of lift is obvious, and is easily han-R d=d dled by BTA. As an example of the kind of \nlift that cannot be easily R S=m automated, consider the following tail-recursive function: Rf, =yet \nv = gensyra e = R(f (Lt V )) funloopber= if (1 =e) then r in lambda v] :t .e else loop b(e-1) (b*r) \nL :Typex Exp+M fun power be=loop be1 L (t->t ) e. = (Av . let el= R v in where e is in Exp and b is \nin Val. Unless we manually lift r to dynamic, S will diverge. L u Monovariant BTA is well-understood \nand can be efficiently im\u00adplemented with type-inference [Henglein91]. Polyvariant BTA is usually implemented \nwith abstract interpretation [Conse193]. Figure 8: Reification timction R and reflection function L. \n[miz ] can be defined with S like this: [rniz] ez = R(c R([R(S e [1)1 x}] . ) but this is just a hypothetical \nand rather limited way to access S. Now we return to the sum example to see the result of special\u00adizing \nit without cyclic values. Conceptually], we specialize thetext of sum to its size and stride like this: \nS su7n [ from+~{ to+~ size+8 stride+8 r+~] In the residual code, the mask computation ( (1 << b) -1) \nbecomes constant, but all other operations are unaffected.  2,1 Compiler Generation If we use a literal \nimplementation of S to specialize programs, then every time we generate a residual program, we also traverse \nand dispatch on the source text. The standard way to avoid this repeated work is to introduce another \nstage of computation, that is, to use a compiler generator cogen instead of a specialize mix. The com\u00adpiler \ngenerator converts ~ into a synthesizer of specialized versions off: These systems are called compiler \ngenerators because if ~ is an interpreter, then [ cogen I ~ is a compilen the pm of the execution of \n~ we call interpretation overhead is only performed once. Al\u00adthough a procedure like sum is not what \nwe normally think of as an interpreter, the idea is the same: factoring-out the overhead of using a general \nrepresentation. The stsrtdard way of implementing a compiler generator begins with a static analysis \nof the program text, then produces the syn\u00adthesizer by syntax-directed traversal of the text annotated \nwith the results of the analysis. Cogen knows what will be constant but not the constants themselves. \nWe call such information binding times; they correspond to the injection tags on a members of M. We say \nmembers of Val are sfatic and members of Exp are dynumic. The binding times form a lattice because they \nrepresent partial informa\u00adtion: it is always safe for the compiler to throw away information; this is \ncalled l~fiing and is the meaning of the 1 i ft annotation in the .&#38;language, [BoDu93] shows how \nto derive a cogen from A-mix in two steps. The first step converts a specialize into a compiler gener\u00adator \nby adding an extra level of quoting to S so static statements are copied into the compiler and dynamic \nones are emitted. The second step involves adding a continuation argument to S to allow propagation of \na static context into the arms of a conditional with a dynamic test. One of the interesting results of \n[Danvy96] is how this property (the handling of sum-types) can be achieved while remaining in direct \nstyle by using the shiftheset control operators ([DaFi92] Section 5.2). Making a working implementation \nof a compiler generator in a call-by-value language requires handling of memorization, inlin\u00ading, and \ncode duplication as well. Practical systems usually supply heuristics and syntax to control these features. \nMany systems (in\u00adcluding ours) use the dynamic-conditional heuristic, which irdines calls to procedures \nthat do not contain a conditional with dynamic predicate. A remarkably pleasing though less practical \nway of imple\u00admenting [ cogen ] is by self-application of a specialize [ [mix] mix mix 1, as suggested \nin [Futamura71] and first implemented in [JoSeSo85]. ]Notformatlybecauseour A-languageisnottheML of theexample \n(bgr)~Cyclic =Zx Expx Z rn~M=Exp +Val+ Cyclic +F R (b q r) = [b q+rl Figure 9: Extending domains and \nR for cyclic values, Swlp = match (Seop, Sel p) ((b q r), S) + let r = (r+s) % b q =(r+s) / b  in (b-l \nr ) end s~ p = match (S eo p, Selp) ((b qr), s) + if s>0 then (sb q sr) else if s < 0then error else \nO Figure 10: First attemptat extending S to cyclic values; normal form is maintained.  3 Cyclic Integers \nThis section shows how adding some rules of modular arithmetic to the compiler generator can unroll loops, \nmake shift offsets static, and eliminate the division and remainder operations inside the 1oad.sarnp \n1e procedure. Figure 9 defines the Cyclic domain, redefines M to include Cyclic as a possible meaning, \nand extends R to handle cyclic val\u00adues. Whereas previously an integer vahre was either static or dy\u00adnamic \n(either known or unknown), a cyclic vahre has known base and remainder but unknown quotient. The base \nmust be positive. Initially we assume the remainder is normal , ie non-negative and less than the base. \nFigure 10gives an initird version of the addition and multiplica\u00adtion cases for Son cyclic values. Again \nwe assume cases not given are avoided by lifting, treating the primitives as unknown (rdlow\u00ading o to \nmatch any primitive), or by using the commutivity of the primitives. The multiplication rule doesn t \nhandle negative scales, A case for adding two cyclic values by taking the GCD of the bases is straightforward, \nbut has so far proven umecessary. Such multi\u00adplication is also possible, though more complicated and \nless useful, Note that this addition rule contains a dynamic addition to the quotient. But in many cases \nq is zerw so the addition may be omit\u00adted up by the backend (GCC handles this fine). But the allocation \nof a new dynamic location would confuse the sharing analysis (see Section 4). Furthermore, The multiplication \nrule has its own de\u00adfect: in order to maintain normal form we must dissallow negative scales. The rules \nused by Nitrous appear in Figure 11. They are simpler and more general because Nitrous imposes normal \nform only at memorizationpoints. Figure 12 gives rules for zero?, division, and remainder. These rules \nare interesting because the binding time of the results depends on the static vahse rather than just \nthe binding times of the arguments as in the previous rules. In the case of zero ?, if the remainder \nis non-zero, then we can statically conclude that the orig\u00adinal value is non-zero. But if the remainder \nis zero, then we need a dynamic test of the quotient, This is a conjunction short-circuiting s -1 P \n= m.t.h (S .o p, S e, p) ((bq r), s) + (b~r+s) s -] P= match (S .o p, Se, p) ((b q r), s) + if s>0then \n(sbqsr) else if ~< 0 then (-sb -q sr) else O F@re 11: Rules for addition and multiplication. s zero? \nep =match Sep (b 1qr) + if (zero? (r % b)) then let t= (r / b) in zero? q+t else false s ml P=match \n(S.. p, Se, p) ((bqr), s) + if (zero? (b %s)) then ((b is) q(r/s)) s -j P = match (S.. p, Se, p) ((bqr), \ns)+ifb=s then r Figure 12 More rules for cyclic values. across stages, and is why we require a polyvariant \nsystem. If we constrain such tests to be immediately consumed by a conditional, then one could probably \nincorporate these techniques into a mono\u00advariant system. Division and remainder could also use polyvariance, \nbut experi\u00adence indicates this is expensive and is not essential, so our systems just raise an error. \nInstead of adding rules to the spxializer, we could get some of the same fimctionrtlity by defining (in \nthe object language) a new type which is just a partially static structure with three members. The rules \nin Figures 10 and 12 become procedures operating on this type. This has the advantage of working with \nan ordinary spe\u00adcialize, but the disadvantageof not interacting well with sharing. Now we explain the \nimpact of cyclic values on the sum exam\u00ad ple. The resultof S sum [from+ (32 =] O) to+(32 ~ O) size+8 \nstride+8 r+~l appears in Figure 13. Because the loop index is cyclic three equal\u00adity tests are done \nin the compiler before it reaches an even word boundary. At this point, the specialize emits a dynamic \ntest and forms the loop. Note that f romq and toq are word-pointers. If the alignments of from and to \nhad differed, then the odd iterations would have been handled specially before entering the loop. The \ngeneration of this prelude code is a natural and automatic result of using cyclic values: normally it \nis generated by hand or by special-purpose code in a compiler. If we want to apply this optimization \nto a dynamic value, then we cart use case analysis to convert it to cyclic before the loop, resulting \nin one prelude for each possible remainder, followed by a single loop. Arbitrary arithmetic on pointers \ncould result in values with any base, but once we are in a loop like sum we want a particular base, set \n-base gives the programmer control: fun sum_O088 fromq toq r = i.f fromq = toq then r else sum_O088 (fromq \n+ 1) to (r+ ( ( (load_word fromq) >>0) 6c255) + (( (load_word fromq)>>8)&#38;255) + ( ( (load_word fromq)>>16)Ck255) \n+ ( ( (load_word fromq) >>24) &#38;255) ) Figure 13: Residual code automaticallygeneratedwith cyclic \nval\u00adues, fun binop (from, to, size, stride) (from , to , size , stride ) = if ( (from = to) andalso (from \n= to ) ) then () else (.. . ; binop( ... )) Figure 14 Looping over two signals. (set-base m b) + (b d \nr) Since m may be dynamic, set-base can be used to perform case analysis. While we currently rely on \nmanual placement of set -base, we believe automation is possible.  3.1 Multiple Signels If a loop reads \nfrom multiple signals simultaneously then it must be unrolled until all the signals returnto their original \nalignment. Theordinarywayof implementingapair-wiseoperationonsame\u00adlength signals uses one conditional \nin the loop because when one vector ends, so does the other. Since our unrolling depends on the conditional, \nthis would resultin the alignments of one of the vectors being ignored. To solve this, we perform such \noperations with what normally would be a redundant conjunction of the end-tests. In both imple\u00admentations \nthe residual loop has only one conditional, though after it exits it makes one redundant testz. F@e 14 \nillustrates this kind of loop. Because 32 has only one prime factor (2), on 32-bit machines this conjunction \namounts to taking the worst case of all of the sig\u00adnats. If the word-size were composite then more complex \ncases could occur, for example, 24-bit words with signals of stride 8 and 12 results in umolling 6 times. \n 3.2 Irregular Data Layout The sum example shows how signals represented as simple arrays can be handled. \nThe situation is more complex when the data layout depends on dynamic values. Examples of this include \nsparse ma\u00adtrix representations, run length encoding, and strings with escape sequences. Figure 15 shows \nhow 15-bit values might be encoded into an 8-bit stream while keeping the shift offsets static. It works \nbecause both sides of the conditional of v are specialized. Read-es c is a good example of the failure \nof the dynamic\u00adconditional heuristic. Unless we mark the rtxxsrsivecall as dynamic (so it is not inlined), \nspecialization would diverge because some strings are never aligned, as illustrated in F@ure 16. 2Nitmusdoes \nthisbecauseit uses continuations;Simpledoes because itscompiler to Ctmnslates while (E&#38;&#38;F)Stowhile \n(E)while(F)S. fun read_esc from to r = if from =to then r else let val v = load_sample from 8 in if \n(v<128) then read_esc (from + 8) to (next v r) else d@ read_esc (from+16) to (next (((v &#38; 127) z< \n8) I (load_sample (from + 8) 8)) r) end Figure 15: Reading a string of 8-bit characters with escape \nse\u00adquences. d@indicates adynamiccall, etc 1 Figure 16: A string with escapes illustrating need for dynamic \ncall annotation inread.esc. SharingandCaching Theremaininginefficiency ofthecodeinFigure 13stemsfromthe \nrepeatedloads.The standsrdapproachtoeliminatingthem istoap\u00adply common subexpression elimination (CSE) \nand rdiasinganaly\u00adsis (see Chapter10.8 of [ASeU186])to residualprograms. Efficient handling of stores \nis beyond traditional techniques, however. We propose fast, optimistic sharing and static caching as \nan alternative. Weimplement thecachewithamomd Wadler92]. Uses of the loacLword primitive rirereplaced \nbycalls toa cached load procedure load.word.c. The last several addresses and memory values are stored \nin a table in the monad; when load-word-c is called thetableis checked. If amatchingaddressis found,thepre\u00adviously \nloaded value is returned, otherwise memory is referenced, a new table entry is created, and the least \nrecently used table entry is discarded. Part of the implementation appears in Appendix A. In fact, any \ncache strategy could be used as long as it does not depend on the values themselves. Note that safely \neliminating loads in the presence of stores re\u00adquires negative may-alias information (knowing that values \nwill not be equal) [Deutsch94]. We have not yet implemented anything to guarantee this. Tlseprime variable \nis the size of the cache. How many previous loads should be stored? Though this is currently ieft to \na manual setting, automation appears feasible because requirements combine simply. How does the cache \nwork? Since the addresses are dynamic any kind of equality test of the addresses will be dynamic. Yet \nthese tests must be static if the cache is to be eliminated. Our solution is to use a conservative early \nequality operator for the cache-hit tests: S Iearly= eo el Ip = match (~ eo p, ~ e] PI (do, dl) + aliases? \n(do, dl) ((60 go ro), (h ql rl)) + 60 = bl and aliases? (go, ql ) and ro =rl This operator takes two \ndynamic vahres and returns a static vahre; the compiler returns true only if it can prove the values \nwill be equal, this is positive alias (sharing) information. The aliasing in\u00adformation becomes part of \nthe static information given to compil\u00aders,storedinthememo tables,etc. Detailsappearin [Draves96], In \nNitrous the generatedcompilers keep track of the names of the dynamic values; the aliases ? function \nmerely tests these names for equality. Thus at compile time a cached load operation requires only a set-membership \n(memq) operation. These names are also used for irslining without a postpass (among other things), so \nno additional work is required to support early=, Simple uses textual equality of the terms. The cache \nfunctions like a CSE routine specialized to examine only loads, so we expect a cache-based compiler to \nrun faster than a CSE-based one, But since CSE subsumes the use of a cache and is probably essential \nto good performance anyway, why do we consider the cache? Because CSE cannot handle stores, but tbe cache \ndoes, as explained below. Like the optirnizations of the previous section, these load op\u00adtirnizations \nhave been achieved by making the compiler generator more powerful (supporting ear 1y=). Even more so \nthan the previ\u00adous section, the source program had to be written to take advantage of this. Fortunately, \nwith the possible exception of cache size, the modifications can be hidden behind ordinary abstraction \nbarriers. 4.1 Store Caching So far we have only considered reading from memory, not writ\u00ading to it. \nStoring samples is more complicated than loading for two reasons: an isolated store requires a load as \nwell as a store, and optimizing stores most naturally requires information to move backwards in time. \nThis is because if we read several words from the same location, then the reads after the first are redundant. \nBut if we store several words to the same location, all writes before the last write are redundant. We \ncan implement store-word-c the same way a hardware write-back cache does (second edition of [HePa90] \npage 379): cache lines are extended with a dirty flag, stores only go to memory when a cache line is \ndiscarded. The time problem above is solved by buffering the writes. The load is unnecessary if subsequent \nstores eventually over\u00adwrite the entire word. Solving this problem requires extending the functionality \nof the cache to include not just dirty lines, but par\u00adtially dirty lines. Thus the status of a line maybe \neither clean or a mask indicating which bits are dirty and which are not present in the cache at all. \nWhen a line is flushed, if it is clean no action is required. If it is dirty and the mask is zero, then \nthe word is simply stored. Otherwise a word is fetched from memory, bit-anded with the mask, bit-ored \nwith the line contents, and written to memory.  5 Implementations We currentty have two implementations \nof bit-addressing: Ni\u00adtrous and Simple, a first-order system. Both are available from http: //www. cs \n.cmu. edu/+spot. Nitrous [Draves96] is an automatic compiler generator for a higher-order, three-address-code \nintermediate language. It han\u00addles partially-static structures (product types), moves static con\u00adtexts \npast dynamic conditionals (sum types), cyclic integers, shar\u00ading, and memorization. It uses the dynamic-conditional \nheuristic. Cache and signal libraries were implemented in a high-level hm\u00adguage and compiled to the intermediate \nkmguage3. 31nfact, this compilation was perfomed with a generated compiler as we!l; the outputof the \noutputof cogen is fedinto cogen. A number of examples were specialized, compiled to C (includ\u00ading GCC \nSindirect-goto extension), and benchmarked. At the time of [Draves96], performance was about half that \nof hand-written, specialized C code; since then the performance has been signifi\u00adcantly improved. Unfortunately \nNitrous fails to terminate when given more com\u00adplicated input. The reason is unknown, but we suspect \nexponential static code is being generated as a result of the aggressive prop\u00adagation of static data, \nparticularly in the cache and inside nested loops. In order to scale-up the examples, we built Simple, \nan on-line speciafizer that avoids using shift/reset or continuations by restrict\u00ading dynamic control \nflow to loops (ie sum and arrow types are not fully handfed). It is a straight-forward translation of \nthe formal system presented in this paper. All procedure calls in the source programs are expanded, but \nthe input language is extended with a while-loop construct that may be residualized: Exp: :=... I loop \nVar Exp Exp Exp Exp which is eqrivafent to the following simple recursive procedure: let fun GVar = if \nExp then Exp else G Exp in G Exp end The loop construct is specialized as if it were a recursive pro\u00adcedure \nwith the dynamic conditional heuristic and memorization: it is inlined until the predicate is dynamic, \nthen the loop is entered and unrolled until the predicate is dynamic again. At this point, the static \npart must match the static part at the previous dynamic conditional. Because Simple is based on symbolic \nexpansion, code is dupli\u00adcated in the output of the specialize. GCC S optimizer fixes most of these. \nThe specialize is written in SMIJNJ without concern for speed but the examples here specialize in fractions \nof a second.  Example The main example built with the simple system is an audiohctor library It provides \nthe signal type, constructors that create sig\u00adnafs from scalars or sections of memory, combinators such \nas cre\u00adating a signal that is the sum of two other signafs, and destructors such as copy and reduce. \nThe vector operations are suspended in constructed data until a destructor is called. Figure 17 contains \na graphical representation of this kind of program. Interleaved vectors are stored in the same range \nof memory; Figure l(c) is an example of three interleaved vectors. Whh an ordinary vector package, if \none were to pass interleaved vectors to a binary operation, then each input word would be read twice. \nA on\u00adchip hardware cache makes this second read relatively inexpensive. But with the software cache the \nsituation is detected once at code\u00adgeneration time; specialization replaces a cache hit with a register \nreference. Figure 18 gives the signature for part of the library. The se\u00admantics and implementation are \nmostly triviaf; some of the code appears in Appendix B. One exception is that operations on mul\u00adtiple \nsignals use a conjunction on the end test (Section 3.1). As a corollary, endp of an intinitesignaf such \nas a constantalways returns true. The delay operator returns a signal of the same length as its input, \nthus it loses the last sample of the input signal. The other possibility (that it returns a signal one \nlonger) requires sum-types because there would be a dynamic conditional in the next method.  $-iwl-iw \n Figure 17: A graphical tinker-toy DSP program. z 1 is a delay. The filter combinator is built out of \na series of delays, maps, and binops. Another combinator built from combinators is the FM oscillator. \nSimple uses first-order anafogues of the higher-order argu\u00adments. We can implement recursive filters \n(loops in the dataflow) with state, as wavrec, scan, and de 1ayl do. A higher-order system would support \na general purpose rec operator for creating any recursive program. The benchmarks were performed by translating \nthe specialized code to C and compiling with CCC v2.7.2 with the -01 option. We afso collected data with \nthe -02 option, but it was not significantly different so we do not present it. 03 is not available on \nour SGI. There are two groups of examples, the audio group (Figure 19) and the video group (Figure 20). \nThe audio group uses 2000-byte buffers and 16-bit signals; the video group uses 4000-byte buffers and \nmostly 8-bit signals. Each of the examples was run for 1000 iterations; real elapsed time was measured \nwith the get t hneof day system call. The whole suite was run five times, and the best times were taken. \nThe R4400 system is an SGI Indigoz with 150Mhz R4400 mnning IRIX 5.3. The P5 is an IBM Thinkpad 560 with \n133Mhz Pentium mn\u00adning Linux 2.0.27. The graphs show the ratio of the execution time of the code generated \nby Simple to manually written C code. In the audio group, this code was written using short * pointers \nand process\u00ading one sample per iteration. In the video group, the code was writ\u00adten using whole-word \nmemory operations and immediate-mode shifts/masks. Some of the code appears in Appendix D, Some of the \nstatic information used to create the specialized loops appears in Appendix C. These are generally arguments \nto the interpreter copy, which is used for all the audio examples. The video examples also use copy, \nexcept iota, sum, and sum 12. The audio examples operate on sequential aligmxl 16-bit data unfess noted \notherwise: inc add 10 to each sample. add two signafsto forma third. fiIter2 filter with kernel width \n2, ~ter5 filter with kernel width 5. The manual code doesn t unroll the inner loop over the kernel. fm \n1 a one oscillator IN synthesizer. sig type samp type signal type address type binop = samp * samp -> \nsamp fun get: signal -> samp fun put : signal -> samp -> unit fun next: signal -> signal fun endp: signal \n-~ bool fun memory: address . address * int * int -> signal fun constant: samp -> signal fun map: (samp \n-> samp) . signal -> signal fun map2: binop * signal * signal -> signal fun delayl: signal . samp -> \nsignal fun scan: signal * samp * binop -> signal fun lut: address * signal -> signal fun sum_tile: smnp \n* signal * int -> signal fun copy : signal * signal -> unit fun reduce: signal . samp * binop -> samp \nfun filter: signal * (samp * samp) list > signal fun fm_osc: signal * int * address * int * signal * \nint -> signal end Figure 18:Signature forsignallibrary fm2 aone-in-one oscillator FMsynthesizer. hrt \na!ook-up tableofsize 256. Theinput signalis8-bits perpixel. sum ailthesarnples inthe input wavree anFMsynthesizer \nwith feedback. Thevidco examples operate on sequential aligned 8-bit data unless noted othenvise copy \nnoopcration. gaps destinationsignal hasstride 16andsize8. ca68 convertsbinary to ASCII byreading asix-bit \nsignal andwrit\u00adingeight. ca86 ASCI1to binary byreading eight andwriting six. iota fills bytes with O, \n1,2, ... sum asinFigure2,speeializedasinFigure3 sum12 atwelve-bit signal. Figure21eontainstwo moregraphs. \nThegraph ontheleftcom\u00adpares twoways ofimplementing the sumexample. The baseline code reads whole words \nand uses explicit shifts and masks to ac\u00adcess the bytes. This iscompared tocodethat uses char* pointers, \nbut is unrolled the same number of times (four and eight). Despite its higher instruction count, the \nword-based code runs faster (all the bars are higher than 1.0). The graph on the right compares general \ncode written using bit-addressing to specialized code. All the code is handwritten. As one expects, without \nspecialization bh-addressing is very expen\u00adsive. Higher levels ofabstraction such asthesignal library \nwould incur even higher expense. 7 Conclusion We have shown how to apply partial evahration and specirdiza\u00adtion \nto problems in media-processing. The system has been im\u00adplemented and the benchmarks show it has the \npotential to allow programmers to write and type-check very general programs, and then create specialized \nversions that are comparable to hand-crafted C code. Neither implementation is yet practical, but we \nbelive both are fixable. The basic idea is to introduce linear-algebraic properties of in\u00adtegers into \npartial evahsation instead of treating them as atoms. The programmer can write high-level specifications \nof loops, and gen\u00aderate efficient implementations with the confidence that the partial evaluator will \npreserve the semantics of their cede. By making aliasing and alignment static, the operations normally \nperformed by a hardware cache at runtime can be done at code generation time. References [ASeU186] A \nV Aho, R Sethi, J D Unman. Compilers: Principles, Techniques, and Tmls. Addison-Wesley 1986. [BoDu93] \nAnders Bondorf, Dirk Dussart. Handwriting Cogen for a CPS-Based Partial Evaluator. Partial Evaluation \nand Semantics-Based Pmgrant Manrpufation, 1994. [ChaUng91] Craig Chambers, David Ungar. Iterative ~ Analysis \nand Extended Message Splitting: Optimizing Dynamically-~ped Object-oriented Programs. Conference on Programming \nLanguage Design and Implementation, 1990. [COHONONOV096] Charles Consel, Luke Homof, Francois Noi?l, \nJacque Noy4, Nicolae Volanschi. A Uniform Approach for Compile-lime and Run-Time Specialization. Dagstuhl \nWork\u00adshop on Partial Evaluation (LNCS1l 10), 1996. [Conse193] Charles ConseL Polyvariant Binding-Time \nAnaly\u00adsis For Applicative Languages. Partial Evaluation and Senrantics-Based Pmgrarn Manipulation, 1993. \n[DaH92] Olivier Danvy, Andrzej Filinksi, Representing Control, a Study of the CPS Transformation. Mathematical \nStructures in Computer Science, 2(4):361-391. [Danvy96] Olivier Danvy. ~pe-Directed Partial Evaluation. \nPri\u00adnciplesof Programming Languages, 1996. [Deutseh94] Alain Deutsch. Interprocedural May-Alias analysis \nfor pointers: Beyond k-limiting. Conference on Programming Language Design and Implementation, 1994. \n[Draves96] Scott Draves. Compiler Generation for Interactive Graphics using Intermediate Code. DagstuM \nWorkshap on Partial Evaluation (LNCS1lIO), 1996. [EnHsKa95] Dawson Engler, Wilson Hsieh, M Frans Kaashoek. \nC: A Language for High-Level, Efficient, and Machine\u00adindependent Dynamic Code Generation. Conference \non Ptv\u00adgramming Language Design and Implementation, 1995. [Futarnura71] Y Futarnura. Partird evalutaion \nof computation pro\u00adcess -an approach to a compiler-compiler. Systems, Comput\u00aders, Contrvl.r, 245-50. \n2 1.8 1.6 1.4 1.2 n . R4400 1 . P5 0.8 0.6 0.4 0.2 0 Figure 19: Audio group. Speed of automaticallygeneratedcode \nnormalized to speed of hand-specialized code. Figure20 Video group. Speed of automatically generated \ncode normalized to speed of hand-specialized code. 4 8 CS66 sum Figure 21: The graph on the left shows \nspeed of bytes normalized to words unrolled four and eight times. The graph on the right shows the speed \nof general code normalized to specialized code. [GoJ091] Carsten K Gomard, Neil Jones. A partialevaluatorfor \nA Cache Source the untyped lambda-calculus. Journal of Functional Pro\u00ad gramming, 1:21-69. A fragment \nof the LRU (least recently used) cache implementation. val W= 32 [GoJoSte96] James Gosling, Bill Joy, \nGuy Steele. The Java Lun\u00adguage Specification. Addison-Wesley 1996. fun maskb=(1<<b) -1 [Granger89] Philippe \nGranger. Static Analysis of Arithmetic Con\u00ad fun load_sample (p, b) = gruences. International Journal \nof Computer Math, ?:165\u00ad letwa= p/W199. let ba=p%W [GuBoGaMa91] P Le Guemic, M Le Borgne, T Gauthier, \nC Le let WO = ( load_word_cached wa ) Maire. Programing rerd time applications with SIGNAL. Pro-let sO \n= (mask b) &#38; (wO >> ba) ceedings of the IEEE, 79(9):1305-1320. if ((ba +b) > w) (let ub = W-ha [Henglein91] \nFritz Henglein. Efficient ~pe Inference for Higher-let W1 = load_word_cached Order Binding-Time Analysis. \nIntemationul Conference on ( (p+ub) / W) Functional Programming Languages and Computer A~hitec-let S1= \n(w1 &#38; (mask (b -ub) )) lure, 1991. so I (s1 <<ub)) so [HePa90] John L Hemessy, David A Patterson. \nComputer Archi\u00adtecture: A Quantita~ive Approach. Morgan Kaufmann 1990. fun flush_line line = let (addr, \nclean, mask, v) = line [IEEE91] IEEE. IEEE Standard 1076: VHDL Language Refer\u00ad if clean line ence Manual. \nIEEE 1991. let V2= (if (O=mask) v (v I (mask &#38; (load_word addr) ) ) ) [JoGoSe93] Neil Jones, Carsten \nK Gomard, Peter Sestoft. Par\u00ad let line2 = (addr, true, O, v2) tial Evaluation and Automatic Program Generation. \nPrentice\u00ad store_word (addr, V2 ) Hall 1993. [JoSche86] Ulric J@ring, William Scherlis. Compilers and \nStag-fun load_word_cached (addr J = ing Transformations. Principles of Programming Languages, let (effects, \ncache) = get_store 1986. if (is_pair(cache) ) (1W 1OOP (cache, ( ) , addr) ) [JoSeSo85] Neil D Jones, \nP Sestoft, H S@ndergaard.An exper-(load_word (addr ) ) iment in partial evaluation: The generation of \na compiler generator. Rewriting Techniques and Applications, D~on, fun lw_loop(cache, prev_cache, addr) \n= France, 1985. if (is_pair cache) (let (line, rest) = cache [LeLe96] Peter Lee, Mark Leone. Optimizing \nML with Run- llme let (addr2, clean, mask, v) = line Code Generation. Con~immce on Programming Language \nif (alias es(addr2, addr)) Design and Implementation, 1996. (if (clean or (mask = O)) (cache_done (prev_cache, \nrest, addr, [Massstlin92] Henry Massrslin.Eficient Implementation of Funaiz\u00ad true, O, v)) mental Operating \nSystem Services. Columbia 1992. (error cannot_cross_streams2)) [PiLoRei85] Rob Pike, Bart Locanthi, John \nReiser. Hard-(lw_looP (rest, (line, prev_cache), addr))) wareASoftwareTrade-offs for Bitmap Graphics \non the Blit. ((flush_line(left prev_cache)); Software-Practice and Experience, 15:131-151. (let w = (load_word(addr)) \n(cache_done ((right prev_cache), (), addr, [Sheard96] lim Sheard. A ~pe-directed, On-line, Partial Evalua-true, \nO, w)))) tor for a Polymorphic Language. OGI-TR-96-004.  B Signal Source [Steele90] Guy Steele. Common \nLisp ihe Lzmguage. Digital Press 1990. Afragment oftheSimple implementationof thesignallibrary: [Wadler92] \nPhilip Wadler. The Essence of Functional Program\u00ad fun rnernorY_ernpty (start; stop, size, stride) = ming. \nPrinciples of Programming Languages, 1992, (start = stop) fun memory_next (start, stop, size, stride) \n= [WeiCoRuSe91] Daniel Weise, Roland Conybeare, Erik Ruf, Scott (v_memory, ((start+stride), stop, size, \nstride)) Seligman. Automatic online program specialization. Interna\u00ad fun memory_get (start, stop, size, \nstride) = tional Conferzmce on Functional Programming Lzzngaages load_sample(start, size) and Computer \nArchitecture, 1991. fun memory_put ((start, stop, size, stride), v) = store_sample (start, size, v) fun \nconstant_empty k = true fun constant_next k = (v_constant, k) fun constant_get k = k fun constant_put \n(k, v) = (error) fun noise_empty (state, ia, ic, im) = true fun fm_osc (mod_freq, c, wav, size, fun \nnoise_next (state, ia, ic, im) = base_freq, init_phase) = (v_noise, (((lift (ia state + it)) % ire), \nlet prec = 8 ia, ic, ire)) (v_lut, fun noise_get (state, ia, ic, im) = state (wav, fun noise_put (state, \nia, ic, im) = (error) (v_map , (op_shift_right, prec,fun bin_empty (op, v, w) = (v_sum_tile, ((vec_empty \nV) and (vet empty w)) (init_phase,fun bin_next (OP, v, w) = (size * (l<<prec)), (v_bin, (op, (vec_next \nv), (vec_next w))) (v_bin, (op_plus, base_freq, fun bin_get (OP, v, w) = (v_map, (op_shift_right, prec,(do_op \n(op, (vec_get v), (vec_get w))) (v_map , fun bin~ut ((oP, v, w), q) = (error) (op_times, c, mod_freq) \n))))))))))) fun delayl_empty (h, v) = (vec_empty v) fun delayl_next (h, V) = fun rgb2m (r, g, b, m) = \n(v_delayl, ((vec_get v), (vec_next v))) ((v_maP, (op_div, 64, fun delayl_get (h, V) = h (v_bin, (oP_Plus, \n(v_bin, (op_plus,fun delayl_put ((h, v), q) = (error) (v_map, (op_times, 30, r)), (v_map, (op_times, \n25, g)))), fun scan_empty (op, h, v) = (vec_empty v) (v_maP, (op_times, 9, b)))))), fun scan_next (oP, \nh, v) = (v_scan, (oP, m) (do_op (h, (vec_get v))), (vec_next v))) fun scan_9et (OP, h, V) = h fun scan_put \n((oP, h, v), q) = (error)  C Signal Examples fun lut_empty (m, v) = (vec_empty v) Pmgramsimplemented \nwiththesignalLibrary. fun lut_next (m, v) = (v_lut, (m, (vec_next v))) va 1 add = (op_plus, sig16, sig16_l, \nsig16_2) fun lut_get (m, v) = val inc = (op~lus, sig16, (v_constant, 10), (load_word (m + ((vec_get v)))) \nsig16_l) fun lut~ut ((m, v), w) = (error) val filter2 = ((v_bin, fun sum_tile_empty (v, max, in) = (op_plus, \n(vec_empty in) (v_delayl, fun sum_tile_next (v, max, in) = (( first), sig16)), let next = ((v + (vec_get \nin)) &#38; (maX-l)) sig16) ) , (v_sum_tile, (next, max, (vec_next in))) sig16_2) fun sun_tile_get (v, \nmax, in) = v fun sum_tile_put (v, max, in) = (error) val kernel = (1, 2, 4, 2, 1, ()) val prefix= (( \na), ( b), ( c), ( d), ( e), ()) fun reduce (op. init, vet) = val filter5 = ((filter (sig16, kernel, \nprefix)), loop (v, vie) ((lift init), vet) sig16_l) (vec_empty vet) ((do_op(op, v, (vec_get vet))), \nval lutl = ((v_lut, (( buf), sig8)), sig16) (vec_next vet)) v val wavtabl = ((v_lut_feedback, (( buf), \n1024, 1, 32,fun copy (a, b) = ( prev), sig16)), loop (a, b) (a, b) sig16_l)((vec_empty a) and (vet-empty \nb)) ((vec_put (b, (vec_get a))); va 1 fml = ((fm_osc ((v_constant, O), O, ((vec_next a), (vec_next b))) \n( buf), 1024, () (v_constant, 256), ( init_phase)) ), fun filter (i, k, pre) = sig16) if (is_pair k) \n(v_binop, (oP_Plus, va 1 fm2 = ((f~osc ((OSC (( buf), 1024, (v_map, (op_times, (left k), i)), (v_constant, \n256) , (filter ((v_delayl, ((left pre)), i), ( phaseO) )), 1, (right k), (right pre) )))) ( buf), 1024, \n(v_constant, 256), ( phase))), sig16) val rgb2m_l = rgb2m (rgba_r, rgba_g, rgba_b, mono8) va 1 rgb2m_2 \n= rgb2m (rgb_r, rgb_g, rgb_b, mono8) val base64_encode = (aligned_6s, aligned_bytes) val base64_decode \n= (aligned_kWes, aliwed_6s) D Manual Code BaselineCcode. int sum16(short *start, short .stop, int sum) \n{ while (start != stop) { sum += *start++; } return sum; } void filter2(short *start, short *stop, short \n*startl, short stopl) { while {start != stop) { *startl = start[Ol + start[ll ; start++; startl++; } \n} void filter5 short *start, short *stop, short startl, short stopl) { int i, t; while (start != stop) \n{ t= o; for (i =O;i<5; i++) t += start[i]; *startl = t; start++; startl++; } } int sum8(int *start, \nint *stop, int sum) { int v; while(start != stop) { v = *start; sum += (((v>>O)&#38;255) + ((v>>8)&#38;255) \n+ ((v>>16)&#38;255) + ((v>>24)&#38;255)) ; start += 1; } return sum; } void iota(int *start, int *stop) \n{ int i =O; while(start != stop) { *start++ = i I ((i+l)<<8) I ((i+2)<<16) I ((i+3)<<24); i+=4; } } \nvoid copy(int *startO int* stopO, int *startl int stopl) { while (startO = Stopo) *startO++ = startl++; \n} void gaps(int .startO, int* stopO, int *startl, int stopl) { while (startO != stopO) { int v = *startO; \n- int bO = (v>>O)&#38;255; int bl = (v>>8)&#38;255; int b2 = (v>>16)&#38;255; int b3 = (v>>24)&#38;255; \nint mask = OxffOOffOO; startl[O] = (startl[O] &#38; mask I bO I (bl << 16); startl[l] = (startl[l] &#38; \nmask I b2 I (b3 << 16); startO++; startl+=2: } } int sum12(int *start, int *stop) { int sum=O; while \n(start != stop) { int WO = start[O] ; int W1 = start[l]; int W2 = start[2]; sum+= ( Wo&#38;Oxfff) + ((WO \n>> 12) &#38; Oxfff) + (((wO >> 24) &#38; Oxff) I ((wl&#38;oxf) <<8)) + (!$71>> 4) &#38; Oxfff) + (w1 \n>> 16) &#38; Oxfff) + (((w1 >> 28) &#38; Oxf) I ((W2 &#38; Oxff) << 4)) + ((W2 >> 8) &#38; Oxfff) + ((W2 \n>> 20) &#38; Oxfff)); start += 3; } return sum; } void fml(int *lut, int phase, short *start, short *stop) \n{ while (start != stop) { *start++ = lut[phase>>8]; phase += 256; phase = phase &#38; ((1024*256)-1); \n } } \n\t\t\t", "proc_id": "258948", "abstract": "General media-processing programs are easily expressed with bit-addressing and variable-sized bit-fields. But the natural implementation of bit-addressing relies on dynamic shift offsets and repeated loads, resulting in slow execution. If the code is specialized to the alignment of the data against word boundaries, the offsets become static and many repeated loads can be removed. We show how introducing modular arithmetic into an automatic compiler generator enables the transformation of a program that uses bit-addressing into a synthesizes of fast specialized programs.In partiai-evaluation jargon we say: modular arithmetic is supported by extending the binding time lattice used by the static analysis in a polyvariant compiler generator. The new binding time Cyclic functions like a partially static integer.A software cache combined with a fast, optimistic sharing analysis built into the compilers eliminates repeated loads and stores. The utility of the transformation is demonstrated with a collection of examples and benchmark data. The examples include vector arithmetic, audio synthesis, image processing, and a base-64 codec.", "authors": [{"name": "Scott Draves", "author_profile_id": "81332496830", "affiliation": "School of Computer Science, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA", "person_id": "PP39071297", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/258948.258971", "year": "1997", "article_id": "258971", "conference": "ICFP", "title": "Implementing bit-addressing with specialization", "url": "http://dl.acm.org/citation.cfm?id=258971"}