{"article_publication_date": "08-01-1997", "fulltext": "\n The Measured Cost of Copying Garbage Collection Mechanisms Michael W. Hicks, Jonathan T. Moore, and \nScott M. Nettles Computer and Information Science Department University of Pennsylvania Philadelphia, \nPA 19103 {mwh, jonm,nettles}@dsl. cis .upem. edu Abstract We examine the costs and benefits of a vmiety \nof copying garbage collection (GC) mechanisms across multiple archi\u00adtectures and programming languages. \nOur study covers both low-level object representation and copying issues as well as the mechanisms needed \nto support more advanced techniques such as generational collection, large object spaces, and typ~segregated \nareaa. Our experiments are made possible by a novel perfor\u00admance analysis tool, Oscar. Oscar allows us \nto capture snapshots of programming language heaps that may then be used to replay garbage collections. \nThe replay program is self-contained and written in C, which makes it easy to port to other architectures \nand to analyze with standard perfor\u00admance analysis tools. Furthermore, it is possible to study additional \nprogramming languages simply by instrumenting existing implementations to capture heap snapshots. In \ngeneral, we found that careful implementation of GC mechanisms can have a significant benefit. For a \nsimple collector, we measured improvements of as much as 95~0. We then found that while the addition \nof advanced features can have a sizeable overhead (up to 1570), the net bene\u00adfit is quite positive, resulting \nin additional gains of up to 42%. We also found that results varied depending upon the platform and language. \nMachine characteristics such as cache arrangements, instruction set (RISC/CISC), and reg\u00adister pool were \nimport ant. For different languages, average object size seemed to be most important. The results of \nour experiments demonstrate the useful\u00adness of a tool like Oscar for studying GC performance. With\u00adout \nmuch overhead, we can easily identify areaa where pro\u00adgramming language implementors could collaborate \nwith GC implementors to improve GC performance. This work was supported by DARPA under Contracts #DABT63\u00ad95-C-0073, \n#N66001-96-C-S52 and #MDA972-95-l-OO13, and by the National Science Foundation CAREER Grant #CCR-9702107, \nwith additional support from the Hewlett-Packard and Intel Corporations and the University of Pennsylvania \nResearch Foundation. The views and conclusions contained in this document are those of the authors and \nshould not be interpreted ss representing the official policies, ei\u00adther expressed or implied, of the \nU.S. Government. Permission to make digital/hard copy of part or all this work for personal or classroom \nuse is granted without fee provided that copies ara not mada or distributed for profit or commercial \nadvan\u00ad tage, the copyright notice, the tide of the publication and its date appear, and notice k given \nthat copying is hy permission of ACM, Inc. To copy otherwise, to republish. to post on servers, or to \nredistribute to lists, requires prior specific permission and/or a fee ICFP 97 Amsterdam, ND @ 1997 ACM \n0-89791 -918 -1/97 /0006 . ..$3.50 1 Introduction Garbage collection (GC) is an important feature of \nmany programming languages, functional ones in particular. De\u00adspite the importance of GC performance, \nthere is surpris\u00adingly limited information available to guide implementors in how to design their languages, \ndata representations, and runtime systems so that the collector has good performance. In practice, implementors \neither tune their systems based on their own experiments and guesses, or worse, just ignore the performance \nof the collector. In this paper, we attempt to improve this situation for copying GC. We explore the \ncosts and benefits of mecha\u00adnisms that are needed to implement a wide range of copying GC techniques, \nfocusing on how they affect the speed with which data is collected. These mechanisms include those needed \nby even the most basic copying collectors, including valid pointer determinat ion, object type and length \ndetermi\u00adnation, and copying. We also examine mechanisms that are needed to implement many of the most \nsophisticated GC techniques such aa generational collection, non-contiguous spaces, object-se~egation \nby types, and large object spaces. In our study, we examine heaps generated by the imple\u00admentations of \nmultiple languages (Standard ML (SML) and Java) and hardware platforms (SGI/MIPS and HP/Intel). The diversity \nof our study allows us to gain more general insights than a similar study that only addresses a specific \ncollector for a specific language on a specific platform. To facilitate our study, we have designed and \nimple\u00admented a GC testbed, Oscar. Oscar uses a snapshot and replay strategy to allow repeatable experiments \nto be per\u00adformed in a controlled environment. We instrument a pro\u00adgramming language s implementation \nto snapshot each heap to disk in a standard format just before it is collected by the native collector. \nThen, using a portable and highly param\u00adeterized replay program, we read in the heaps and replay the \ncollections using the GC mechanisms we wish to study. This allows us to study the effects of heap characteristics \nthat are associated with a language implementation but not its collect or, such as sizes, types, and \ndistributions of ob jects being collected. Oscar is easy to port, and since the replay program is written \nin C, we can use standard performance analysis tools. Therefore, studying additional languages is 55 \nsimple as instrumenting existing implementations to cap\u00adture snapshots, and studying additional architectures \nmerely involves porting the replay program. Due to the sheer volume of possible choices, it will come \nas no surprise that we do not touch on all aspects of copying GC performance. Instead, we focus on those \naspects of GC performance that are the least well understood in a general context and on which we believe \nwe can shed the most light. Because Oscar measures only the garbage collection time, it cannot evaluate \nthe costs that GC mechanisms impose on other parts of the language implementation. One exam\u00adple of this \nis mutation-tracking devices like remembered sets whose overhead is mostly felt during program execution. \nWe therefore concentrate on measuring GC mechanisms whose impact is felt entirely (or nearly so) by the \ncollector. Fur\u00adthermore, we focus on the costs of these mechanisms rather than on the Dolicies needed \nto control them: these Dolicies . . are very language-implementation dependent. We expect that our results \nwill help language implementors set policies in a more informed way through a greater understanding of \nthe costs involved. Our results show that ii careful implementation of some of the simple mechanisms \ncan result in significant improve\u00adments in GC speed, as much as 95 %. In addition, we found that the \nbenefits of the advanced techniques we measured result in further GC speed improvements of as much as \n42V0. Overall, we Oscar was extremely successful in enabling us to draw general conclusions about copying \nGC mechanisms, as well as to understand tradeoffs resulting from a particular language or architecture. \nWe begin by presenting an overview of basic copying garbage collection and the advanced techniques we \nshall con\u00adSIder,focusing on the mechanisms that we shall study. We then describe the design and implementation \nof Oscar, fol\u00adlowed by a description of the experimental conditions of our tests. The results from our \nexperiments form the heart of the paper. We begin by briefly assessing the impact of cache locality on \nour benchmarks. Then we examine the impact of various design and implementation choices on a simple col\u00adlector. \nWe proceed to study the costs and benefits of more advanced GC techniques. Finally, we discuss related \nworks, our plans for the future, and our conclusions. 2 Copying Garbage Collector Design In this section, \nwe provide a brief overview of the design, implementation and basic performance costs of copying GC. \nWe concentrate on the issues and constructs that we measure in our study. For a more general dkcussion \nof GC design and implementation, see Wilson [17], or Jones [9]. Throughout the paper, object simply refers \nto a datum managed by the collector; the existence of methods or other semantic features is not important. \nLikewise, type refers to the type that is relevant to the collector, rather than to the type as seen \nby the programming language. 2.1 Basic Algorithm Both copying and mark-and-sweep collectors are tracing \ncol\u00adlectors. Tracing collectors work by finding all live objects that may be used by the program in the \nfuture and then re\u00adclaiming the unused garbage objects. lkcing collectors find the live objects by starting \nwith ail objects that are dwectly referenceable by the program, the roots, and then following (or tracing) \npointers to find the transitive closure of all ob\u00adjects reachable from the roots. We refer to the total \nsize of the live objects as the hvesize. Copying collectors copy all of the live objects from their current \nlocation, from-space, into a new location, to-space. Ltrhen the algorithm terminates, from-space contains \nonly the garbage and the old versions of the live objects and can be reclaimed. Most, although not all, \ncopying collectors use a technique called the Cheney scan [2] to implement the transitive closure algorithm. \nBecause it is by far the most common, it is this specific technique that we focus on here; see the future \nwork (Section 7) for the issues involved in studying other techniques. 2.1.1 The Cheney Scan Two operations \nare used to implement the Cheney scan, copy and scsn. Copy takes a pointer to an object in from\u00adspace. \nIf the object is not yet copied then copy performs the following actions: it copies the object to to-space \nat the lo\u00adcation pointed to by the copy pointer, marks the from-space version as forwarded, stores a \nforwarding pointer in the from\u00adspace version pointing to the to-space version, advances the copy pointer, \nand returns the location to which the object was copied. If the object has already been marked as for\u00adwarded \nthen copy simply returns the forwarding pointer. Scan takes the object pointed to by the scan pointer \nand ap\u00adplies copy to each from-space pointer in the object, updating them with the new to-space locations; \nscan then advances the scan pointer to point at the next object in to-space. The algorithm is initialized \nby setting the copy and scan pointers to the beginning of to-space and then applying copy to each root, \nupdating them with the new to-space loca\u00adtions. The algorithm proceeds by applying scan until the scan \npointer equals the copy pointer, at which time all ob\u00adjects have been copied and updated and the algorithm \nter\u00adminates. At the end of a collection, to-space contains only live objects, so copying collection has \nthe additional benefit that it compacts the live data. Note that the Cheney scan results in a breadth-first \ntraversal of the pointer graph, with the region between the copy and scan pointers serving as an implicit \nqueue of objects to be processed. Consider the mechanisms needed to implement this al\u00adgorithm. Scan will \nneed to know how to find all the point\u00aders in an object, how to tell if they point into from-space, and \nhow to advance the scan pointer. It is common that some objects do not contain pointers and thus need \nnot be scanned; this requires that scan be able to determine the type of the object. Copy will need to \nknow if the object has been forwarded, how to find the forwsding pointer, how to determine the object \ns length, how to advance the copy pointer, and of course how to copy the object. Variations in how these \nmechanisms are implemented are an important part of our study, so we defer the details of how our collector \nachieves them until the experimental results in Section 5. 2.1.2 The Cost of Basic Copying Collection \nThe simplest asymptotic bound on the cost of copying col\u00adlection is that collection is O(hvesize). However, \nour study focuses on the details of the cost, so we need a more detailed picture. Each object must be \ncopied and scanned once, and each of these operations has some cost related to the size of the object \nand some cost that is incurred on a per-object basis. Since the from-space is reclaimed in a single opera\u00adtion, \nthe size of from-space does not afkct the asymptotic cost of collection and we do not consider this cost \nhere. When copy is applied to a pointer to an uncopied ob\u00adject, the following costs are per object: determining \nif the object is forwarded, finding the objects length, and ad\u00advancing the copy pointer. Of course, the \ncost of actually copying the object is dependent on the size of the object. For already-copied objects, \ndetermining if the object is for\u00adwarded and finding the forwarding pointer are per-object costs, and \nthere is not generally any size-dependent cost. For scan, determining the size and type of the object \nand advancing the scan pointer are per-oh ject costs, and in fact, for objects that do not contain pointers \nthese we the only costs. For objects that can contain pointers, the costs for determining where the pointers \nare and then applying copy to them and updating them with the to-space pointers are size-dependent. In \naddition to the costs for scanning heap objects, scanning the roots incurs the cost of applying copy \nto each root that is a pointer. 2.2 Advanced Techniques A major goal of our study is to compme the costs \nin a simple Cheney-scan collector described above to those of the mech\u00adanisms needed by more advanced \ntechniques in common use. Here we provide a brief overview of the advanced techniques covered in this \nstudy; further implementation detail can be found in the experimental results in Section 5. Space does \nnot permit a full discussion of the motivations or policies for using these techniques. We consider following \ntechniques: . Generational collection [10, 15] segregates objects by allocation age and focuses collect \nion work on the younger objects, which are more likely to become garbage. . Non-contiguous spaces occur \nwhen a space is not a contiguous range of memory. These can be used to reduce virtual memory use [8] \nand are needed by some of the advanced techniques. . Segregation by type [13] (for example, separating \nob\u00adjects that contain pointers from those that do not) is used by some collectors to take advantage of \ncommon characteristics to improve GC performance. . Separate big-object spaces [1, 16] are used to avoid \nthe inefficient copying of large objects, especially those that do not contain pointers.  Each of these \ntechniques makes implementing the basic GC mechanisms more costly, but may also have compensating benefits. \nOur goal is to gain insight into how to minimize the cost, and thus maximize the net benefit. 3 Oscar \nTo facilitate our study, we designed and implemented a GC testbed, Oscar. Oscar provides a controlled \nand portable environment in which to study particular GC techniques systematically and comparatively \nusing data derived from a variety of programming language implementations. It is important to understand \nthat our goal is not to study a programming language implementation s native collector or collection \ntechnique; in fact, the native collector can bear es\u00ad sentially no relation to the collection techniques \nbeing stud\u00ad ied. What we wish to capture from a language are the sizes, types, and dktributions of the \nobjects being collected. Oscar is based on a simple snapshot and replay tech\u00ad nique: a language implementation \nis modified to capture the pertinent GC conditions in heap snapshots, and a re\u00ad play program repeats \nthe collections corresponding to these snapshots so they may be closely observed. The replay pro\u00adgram \nis parameterized so that we may systematically vary particular aspects of the collections being studied \nin a con\u00adtrolled way. This approach mirrors those that are common in other forms of experimental computer \nscience, such as using reference traces when studying cache effects or eval\u00aduating file-system design. \nThe advantages of this approach include: . It provides a controlled and repeatable environment. . It \nallows changes to be made to garbage collection mechanisms without requiring changes to other parts of \nthe language implementation, such as the allocator or compiler. . Heaps from diiferent languages can \nbe studied with ex\u00adactly the same collector, without having to understand the details of the language \nimplementation s collector and how it interfaces to the rest of its implementation. . The replay program \nis a C program, so it is portable  and can be used with standard performance evaluation tools. Often \nlanguage runtimes are such that standard tools cannot be used. A principle disadvantage is that Oscar \ncannot measure the costs that GC mechanisms incur outside of the collector. 3.1 Heap Snapshots A heap \nsnapshot is written to dk+keach time the implemen\u00adtation s collector is called and captures all of the \ninforma\u00adtion needed to repeat the collection. The heap snapshots are taken in a canonical form similar \nto that used by Standard ML of New Jersey (SML/NJ) 1.09; this collector contains most of the features \nwe wish to model and so this repre\u00adsentation is sufficiently general for our current use. If in the future \nwe wish to study other features not supported by this canonical representation, extending it should not \nbe difficult. Furthermore, our results do not depend on this canonical form, it merely serves as a standard \nformat in which to store snapshots. One significant detail is that, for collections based on generational \nheaps, the remembered set is processed into the smne representation used for the roots in non-generational \nheaps when the snapshot is taken. This effectively factors out the remembered set implementation, which \nis not within the scope of this study. Currently we can capture heaps from SML/NJ 1.09, and from Sun \nMicrosystems Java Developer s Source Release 1.0. Because we use a similar heap representation, making \nsnap\u00adshots of SML/NJ 1.09 heaps is straightforward. On the other hand, the Java implementation hss a \nheap represen\u00adtation designed for mark-and-sweep collection with com\u00adpaction. Support for compaction \nincludes a handle-space through which object references are undirected. Further\u00admore, the location of \npointers is encoded in the class struc\u00adture for each class, unlike SML/NJ. Thus making snapshots of Java \nheaps is more challenging but still feasible. The Java snapshots still encode the basic types, lengths, \nand even rel\u00adative locations of the objects in Java s heap. Since they have no impact on this study, \nwe omit the details of exactly how SML and Java snapshots are generated. Machine Cache IMakeup Size index \nWrite-Policy Line-Size Write-Buffer SGI Challenge Primary split Instr: 16K direct-map write-back 64 bytes \n64 bytes Data:16K Secondary unified 4 MB direct-map write-back 128 bytes 128 bytes HP Netserver Primary \nsplit Instr:8K 2-way write-back 32 bytes nla Data:8K Secondarv unified 1 MB direct-maD write-back 128 \nbvtes n/a Table 1: Cache characteristics of Benchmark Machines 3.2 Heap Replay The replay program is \nsimple. It first reads in the canon\u00adical representation and converts it to the exact representa\u00adtion \nneeded for a given experiment. For example, it might convert the lengths of objects from bytes to words. \nThen the program prepares the environment for the actual replay. For example, it may flush the caches \nand then touch certain parts of the heap to make them cache resident. Finally, it calls the garbage collector \nand replays the collection. The replay program s collector contains code to imple\u00adment a hwge number \nof different options and mechanisms. In general, these options are parameterized by #if def s and a customized \nversion of the replay program is compiled to test the performance of a particular feature. This approach \navoids using runtime tests to change the behavior of the sys\u00adtem; such checks would cause the replay \ncollector to have unrealistic performance. Although it typically uses similar object representations, \nthe replay collector is not based on the SML/NJ 1.09 collector, but is instead based on a collec\u00adtor \nthat we wrote and tuned extensively using a preliminary version of Oscar. 4 Experimental Conditions Here \nwe describe the conditions of our experiments, includ\u00ading: the machine environment, the programs used \nto gen\u00aderate the heaps, the distribution of data in the heaps, and some miscellaneous details. 4.1 Machine \nEnvironment An important feature of Oscar is that is it quite portable, allowing the use of a variety \nof machines. For our current study, we have used two machines, one from Silicon Graph\u00adics (SGI) baaed \non a RISC-style processor from MIPS, and one from Hewlett Packard (HP) baaed on a CISC-style pro\u00adcmsor \nfrom Intel. These machines are quite representative of the major architectural variations that me found \nin today s market. Both madnes provide a modest level of instruction level parallelism, but do not support \naggressive instruction level parallelism or speculative execution. We do not be\u00adlieve our major conclusions \nare sensitive to this issue, but expect that as such machines become more common that we will need to \nverify this. With respect to architectural varia\u00adtion, a more significant limitation is that both machines \nused have structurally similar memory hierarchies, although our results do show some important differences. \nFortunately, as we discuss in Subsection 5.2.1, the majority of our recom\u00admendations are not sensitive \nto memory hierarchy effects, despite the fact that absolute GC performance certainly is. 4.1.1 SGI Challenge-L \nOur SGI machine is a Challenge-L. The machine is equipped with four 250 MHz MIPS R4400 processors (our \nbench\u00admarks only use one processor) and has 384 megabytes of main memory, which is two-way interleaved. \nCache charac\u00adteristics are shown in Table 1. The machine runs IRIX 6.2. Bcopy achieves a copying rate \nof 36 megabytes per second on this machine, while simply reading memory can be done at 67 megabytes per \nsecond, and writing it can be done at 74 megabytes per second. The latency for loading from the first \nlevel cache is 8 nanoseconds and from the second level cache is 64 nanoseconds, while for main memory \nit is 1150 nanoseconds, almost a factor of twenty difference from the second level cache. To perform \ntiming measurements, we used MIPS high-speed cycle counter that has an overhead of roughly 200 nmoseconds \nper access. 4.1.2 HP NetServer OurHP machine is a NetServer 5/166 LS4. The machine is equipped with four \n166 MHz Intel Pentium processors and haa 128 megabytes of main memory. Cache character\u00adistics are summarized \nin Table 1, although we were unable to find information about the write buffers for this machine. The \nmachine runs Red Hat Linux 4.0. Bcopy achieves a copying rate of 25 megabytes per second on this machine, \nwhile simply reading memory can be done at 61 megabytes per second, and writing it can be done at 37 \nmegabytes per second. The latency for loading from the first level cache is 6 nanoseconds and from the \nsecond level cache is 100 nanoseconds, while for main memory it is 500 nanosec\u00adonds, only a factor of \n5 difference from the second level cache. Thus, compared to the SGI, main memory is signif\u00adicantly closer \nto the processor. We will see that this makes memory hierarchy effects much less pronounced on the HP. \nTo perform timing measurements, we used the Pentium cy\u00adcle counter that has an overhead of about 2 microseconds \nper access. 4.2 Snapshot Generation The Java heaps were generated by compiling the Java com\u00adpiler and \nrunning a number of Java applets. These snapshots were generated on a Spare, which was necesswy because \nwe do not have access to the source for a Java implementation that runs on our benchmarking machines. \nBecause of space limitations, we only present the Java compiler heaps here. In general, the other Java \nheaps were small enough that they were collected so fast that it was difficult to gather sta\u00adtistically \nmeaningful data about them, although the results suggest that the compiler heaps were representative. \n50 o string record 40 30 20 10 o string o array record pair o 1- Object io Size in Words 15 20 0 I \no m 5 Object I 10 Size in Words I 15 I 20 Figure 1: Distribution of Objects (Java) Figure 2: Distribution \nof Objects (Major) For SML/NJ, we generated heaps both by compiling the compiler, and from a sort benchmark. \nWe collected heaps from both minor first generation collections and major higher generation collections. \nThe major collection heaps include both typed-segregated objects and separate big ob\u00adjects. Our results \nin Subsection 5.8 make it clear that the big objects should be handled separately, so in general, we \nexcluded them from the heaps used for our tests. SML S al\u00adlocation arena, which is collected by minor \ncollections, does not contain big objects and does not segregate objects by types, so the minor heapa \ndo not have these features. Again, in the interest of space, we do not present the sort bench\u00admark heaps, \nalthough the results of studying those heaps are quite similar to the compiler heaps. 4.3 Heap Object \nDistributions Here we present some information about the dktribution of object sizes and types in the \nheaps we studied. This is im\u00adportant, because these distributions determine the relative importance of \ncosts that are per-object and costs that are size-dependent and between the cost of copying and scan\u00adning. \nHere we also introduce a convention we use through\u00adout the rest of the paper, in which we label our plots \nwith Java for the Java compiler heaps, with Major for the SML compiler major collection heaps, and with \nMinor for the SML compiler minor collection heaps. Figure 1 shows the distribution of live objects in \nthe Java compiler heaps. The X-axis is the object size in worda, while the Y-axis is the percent of objects \nhaving that size. The objects labeled string do not contain pointers, while those labeled record do. \nThe classification into string and record is not one made by the Java runtime, but rather one we imposed \nwhen we snapshot the heaps by labeling non\u00adpointer containing objects as strings. Note that we have not \nshown some very large, but infrequent object sizes. Notice that only odd-sized objects occur. This is \nbecause the Java implementation requirea that all objects begin on a two word boundary, and all objects \ninclude a one word header, which is not included in these lengths. The most typical object size is five \nwords. Also note that almost all objects contain pointers, which means they must be scanned as well as \ncopied. Figure 2 shows the distribution of live objects in the SML compiler major heaps, in the same \nformat as Figure 1. Because SML haa a somewhat more complicated set of GC types, the legend uses string \nto refer to objects that do not contain pointers, array to refer to objects that are mu\u00adtable and can \ncontain pointers, record to refer to objects that are immutable and can contain pointers, and pair to \nrefer to pointer containing objects that are immutable and of exactly length two. The distribution for \nSML compiler minor heaps is similar, and is not presented here. Note that pairs are by far the most common \nobject type, making up about so~o of the heaps. Also notice that strings and mutable objects are rare. \nThe shorter object length will mean that for the SML heaps, per-object costs will be more important than \nfor Java heaps. 4.4 Other Details In all cases, when virtual memory is allocated for the heaps, it is \ntouched, forcing physical pages to be assigned to it and avoiding those overheads during replay. Because \nour bench\u00admarking machines have ample physical memory, no paging occurs during collection. Prior to running \neach collection we flush the first and second level caches; details of why we use this policy are given \nin Subsection 5.2.1. On the SGI machine, we used the vendor supplied C (Version 6.2) compiler to compile \nOscar, while of the HP, we used gcc (Version 2.7.2). We experimented with optimiza\u00adtion levels and compiler \nflags and used those that resulted in the highest performance with our basic collector. There is some \nevidence that the SGI C compiler was somewhat more aggressive in its optimizations. All me~urements are \nbaaed on elapsed time. Wkhin the limits of the coarse-grained clocks used to measure CPU time, we found \nCPU time to be well correlated with elapsed time aa we would expect since the system does no 1/0 dur\u00ading \nGC. All of the measurements presented here are the median values of at least thirteen, and often twenty-one, \nruns. Examining the quartiles of the runs showed that the measurements show only a small variance. To \nhelp interpret our measured results, we also used the SGI S tool, Pixie. Pixie reada a program, determines \nits basic blocks, then instruments the program to count how often each block is entered. After the program \nis run, Pixie then converts Instruction counts to machine cycles, typically with a CPI of around 1.58. \n0.006 j 5.2 Locality Effects A flushed cache (SGI) warm cache (SGI) A flushed cache (HP) A warm cache \n(HP) xx 0.000p -- II I o 20 40 60 Livesise(KB) Figure 3: Cache Effects (Minor) 5 Experiments We performed \nthree different kinds of experiments. First were tests designed to quantify the effect of locality on \nour basic collector and our benchmarks. Second were experi\u00adments that studied variationa of the implementation \nof our basic collector, such as how unrolling the copying loop changed the speed of the collector. Finally, \nwe were interested in how the mechanisms needed by the advanced features would af\u00adfect performance. Unfortunately, \nspace does not permit us to show all of our results. Instead we have selected representative results \nwhen all of the results lead to the same conclusion, and multiple results when they allow us to show \nsome interest\u00ading language or architectural difference. All of the results, as well aa sample code sequences, \nand other details can be found in Hicks, et al. [5], See Section 8 for how to obtain [5], as well as \nour data, heap snapshots, and Oscar itself. 5.1 Our Basic Collector Before we describe the experiments \nwe did, we must first fill in the details about how the basic collector itself and its data representation \nwork. In Section 2.1 we described the basic mechanisms needed by a copying collector. In our basic col\u00adlector, \nwe enable these mechanisms in a simple and common way. Each object has a header word that records its \nlength and type. The type is used to determine certain object char\u00adacteristics, such as whether the object \nmay contain pointers, whether the object is mutable, or the units of its length (e.g., words, bytes, \netc. ). In pointer-containing objects, the words that are pointers are tagged (using a low-bit tag) so \nthat they can be identified. From-space is a single contiguous region of memory, and so a trivial range \ncheck can establish whether a pointer points into from-space. Pointers are only allowed to point to the \nword immediately after the header, so finding the length of an object given a pointer to it is sim\u00adple. \nThis is in contrast to some collectors that allow interior pointers, thus requiring a more complicated \nheader finding scheme; SML/NJ 1.09 current Iy allows interior pointers, for example. When an object is \ncopied, the header is changed so that it identifies the object as forwarded, and the for\u00adwarding pointer \nis stored in the first word after the header. The to-space is also contiguous, and is sized so that copy \nwill never write beyond its bounds. In our basic collector, the copy loop is unrolled four times, a number \nwhich we had found to be optimal on earlier experiments on the SGI. We wanted to fist quantify the effect \nof locality on our ba\u00adsic collector and benchmarks. This information is useful for differentiating cache \nfrom instruction count effects when in\u00adterpreting th~ results of later experiments. To this end, we performed \ntwo experiments. The first measured the performance of the collector with initially hot or cold caches. \nThe second experiment involved systemat\u00adically varying the space betw~n live objects in the heap, primarily \nto learn how the caches line structures would af\u00adfect collector performance. 5.2.1 Hot versus Cold Caches \nWhen we replay a collection, it is not possible for us to recre\u00adate the cache contents present when the \nheap was captured. This is both because we are not able to observe this informa\u00adtion at the time of the \nsnapshot, and because we may well be using a completely different cache during replay. How\u00adever, we do \nknow that in the best case the cache will be hot and will contain only heap data, and in the worst case, \nit will be cold and will contain no heap data. Thus we can gain arI understanding of the range of possible \nperformance by studying these two extremes. Before looking at the results, it is important to consider \nhow locality will aHect the measured performance. For a fixed set of roots and GC traversal algorithm, \nthe pattern in which data will be referenced is fixed, and therefore the pattern in which it is entered \ninto the cache during GC is fixed. In a cold cache, the initial reference to each object will incur a \nmiss for each word that is copied. For the hot cache, all initial references will be in the cache. In \neither case, whether or not a subsequent reference will be in the cache is determined by the fixed reference \npattern. There\u00adfore, the only difference between the hot and cold case is the locality of the initial \nreferences, and then only if the initial reference is not to a location in the cache already touched \nby the collection. This implies that when comparing collectors whose mechanisms differ slightly (such \nas in the implemen\u00adtation of the copy loop) but whose traversal algorithm is the same, the effect of \ninitial cache locality will tiect both col\u00adlectors in the same way. For this reason, all the experiments \nin the remainder of the paper were run with a cold cache. To measure performance with a cold cache, we \nsimply flush the caches before replaying each collection. To create a hot cache, we first flush the cache, \nand then touch all the heap data in a linear fashion. Figure 3 shows the effect of our experiments on \nthe elapsed time of the SML compiler s minor collections, for both the SGI and HP platforms. The X-axis \nis the livesize of the heap in kilobytes, while the Y\u00adaxis is the elapsed time in seconds. As expected \nfor heaps that fit entirely inside the second\u00adlevel cache, we found a noticeable difference in the elapsed \ntimes. The minor heaps are 1 MB in total size (including garbage), so they fall into this category for \nboth machhes. The effect of flushing the cache on the HP is less pronounced than on the SGI: about 3% \nas compared to 20%. This is because the HP s memory access time relative to that of its second-level \ncache is far less than that of the SGI. In fact, this trend is present in the results throughout the \nrest of the paper: improvements tend to be less pronounced on the HP thau on the SGI. Figure 3 also clearly \ndemonstrates that the GC time incre=es Iineazly with livesize, as discussed earlier, z 20  i** 18L \n~ 20 * pixie (SGI) ********* A flushed cache (SGI) : x ; 15 x warm cache (SGI) o flushed cache (HP) \n 8 ~:~~z * wg + warm cache (HP) $$ 10 .. I  om~m~ -* o z 5\u00ad8 1 III1 0 5 10 15 20 HeapSize(MB) Figure \n4: Cache Effects (Major) Figure 4 shows the effect of cache flushing on the SML compiler s major collections, \nagain for both platforms. Here, we show the performance (on the Y-axis) in terms of the copying rate \n(megabytes copied divided by elapsed time); laxger numbers indicate better performance. This presen\u00adtation \neffectively factors out the linear increase in elapsed time due to the livesize, and we will use it throughout \nthe rest of the paper. Aleo note that the X-axis is the size of from-space, not the livesize, and that \nthe Y-axis does not begin at zero. For heaps that dld not fit in the second-level cache, the effect of \nflushing declined as heap size increased until no measurable effect was observed. For these heaps, only \nthe data touched most recently was in the cache at the time of GC. The larger the heap, the less likely \nthe data initially referenced by the collector would be in the cache. Recall for the SGI machine that \nthe second-level cache size is 4 MB, while for the HP it is 1 MB. For smaller heaps on the SGI, hot caches \nshow a slight advantage, but as the heap size increases beyond 4 MB, the flushing effect dkappems. For \nthe HP, all of the heaps shown are larger than 1 MB and so exhibit the minimal effect of flushing. There \nare two ma\u00adjor heaps where the hot case shows a significant advantage; both of these heaps have significantly \nsmaller Iivesizes than the rest (41K and 71K), thus significantly reducing confhct misses. We also used \nPixie to generate instruction counts and compared them to the measured results. As expected, Pixie indicates \nfairly constant GC speeds, the differences being largely attributed to cache effects. It is also notable \nthat Pixie presents GC speecks of up to 70% greater thau those measured. This shows that the cache can \nhave a great effect on the absolute performance of a collector. 5.2.2 Heap Respacing The previous experiment \nindicates that while initial cache contents play a relatively minor role in GC Speed, the differ\u00adence \nbetween perfect and measured locality can be signifi\u00adcant. Locality can be improved in several ways. \nOne obvious way would be to increase the size of the caches. Another, more subtle way would be to increase \nthe amount of data prefetched into the cache as a result of filling a cache lie. In GC, cache line effects \nare largely felt in from-space. When an object is being copied into to-space, its from-space data will \nbe brought into the caches. If the object doesn t completely fill the cache lines, some additional data \nwill be ~ f o 2 o a 16-;~; 04 0 g 08 ~ 14-AA 16 % Vjff % * 32 &#38; ~ 12-X 64 V + 128 u $ 10 % basic \n: *$; tII 1.0 1.5 Livesize (MB) Figure 5: Heap Respacing (SGI) (Java) prefetched into the cache. If this \ndata is part of another object to be reached by copy soon, it will already be at least partially in the \ncache when it is reached, thereby improving the time to process the object. We can expect that this effect \nwill be greater in heaps that are mostly live, because they will have more livedata per cache line. To \ndetermine the magnitude of the possible improve\u00adment, we respaced the heaps to align live objects on \nan n-word boundary, and then ran tests for various n. The results for the Java heaps on the SGI machine \nare shown in Figure 5 and for the HP machine in Figure 6. The Y-axis is GC speed in megabytes per second, \nwhile the X-axis is the livesize in megabytes. The numbers in the legend indicate the various values \nof n, while basic is the result for the true spacing of the data. Again, the overall effect on the HP \nis less than on the SGI; the dfierence from O to 128 word alignment is only a decrease of 15% as compared \nto 47% for the SGI. For both machines, the performance when using the original spac\u00ading falls almost \nexact ly between the best and worst cases, amounting to a drop from n=O of 26% on the SGI and 7~o on \nthe HP. Since the instruction counts for all of these runs are the same, the mesaured difference is entirely \nthe result of locality. The cache line effects can be more clearly seen for the SGI machine, aa indicated \nby the bands present at n val\u00adues of 16 and 32 correspondkrg to the cache line sizes of 32 and 64 bytes, \nrespectively. We believe that the severe drop born n=8 to n=16 is a result of eliminating first level \ncache 11.01 9.0  I 1:0 $: ~o 0 1 1:5 X 64 0 128 % basic I Livesize(MB) Figure 6: Heap Respacing (HP) \n(Java) 14 0 0 unrolled 0o 0 00 simple o 0 000 0 memcpy u D u 10 1 o OD 9 II1 1.0 1.5 2.0 Livesize(MB) \nFigure 7: Copy Loop Implementation (SGI) (Java) prefetching, and the second drop is due to the elimination \nof second level cache prefet thing. However, we must also consider that since the respacing procedure \nalso inflates the heapsize, there is a greater chance for conflict misses as de\u00adpendent on the size of \nthe cache. Since the HP has only a 1 MB second level cache, this effect likely dominates in the quick \ndrop from n=O to n=2 followed by much lower reduc\u00adtions from then on. This clouds the trends due to Iinesize \nfor the HP, although we can see a slight drop from n=8 to n=16. 5.3 Variations of the Basic Collector \nWe found that seemingly minor implementation and compi\u00adlation details can significantly affect the collector \ns perfor\u00admance. We first measured the effect to GC performance of varying the level of compiier optimization. \nWe then mea\u00adsured the performance of GC using various copy loop imple\u00admentations and object header formats. \nNot surprisingly, complier optimizations had a major ef\u00adfect on the speed of the collector. Our tests \nshow that dif\u00adferences of as much as 40 % in the basic copying rate are common. To achieve good performance, \nit was also impor\u00adtant to inline key GC functions and to make sure that critical values were kept in \nregisters. In general, using globals for important values like the copy pointer defeats the register \nallocator, and so we found it to be important to pass them into functions as arguments. All of the results \npresented in this paper are with the maximum possible optimization. 5.3.1 Copy-Loop Implementation At \nthe heart of copy is the copy loop that actually does the job of moving the bytes from from-space to \nto-space. We compared three implemental ions of the copy loop: the basic collector s unrolled loop, a. \nsimple wordwise loop, and the system s memcpyprocedure. The results of these options run with the Java \nheaps on the SGI machine are shown in Figure 7. The X-axis is the the Iivesize in megabytes, while the \nY-axis is the GC copying speed in megabytes per second. Note the non-zero origins, The unrolled loop \nperforms the best, about 6% better than the simple loop, and 35% better than memcpy. This can be correlated \nwith the fact that over 70yo of the ob\u00adjects in the Java heaps are length 5 or more, thus allowing the \nunrolling setup time to be absorbed. Even so, it seems o ~ 11\u00ad o g a~ o memcpy ; O simple o o unrolled \n% 0 u 000 u 00 0 I1I 1.0 1.5 2.0 Livesize(MB) Figure 8: Copy Loop Implementation (HP) (Java) odd \nthat memcpy would not also have such unrolling opti\u00admization. We believe the hand-coded loops are better \nthan the system memcpybecause memcpysupports a more general byte-wise copy and requires a procedure call. \nThe results of the same experiment on the HP are shown in Figure 8, with similar axes. Here, the results \nare exactly reversed, with memcpyimproving on the simple and unrolled loops by about 9% and 10%, respectively \n(notice that the Y-axis scale is different). While the superiority of memcpy is not surprising, it is \ncurious that the simple loop would outperform the unrolled one. We disassembled both and fou~d that in \nboth cases, performance was limited by the small supply of general-purpose registers: references to the \nstack were made during the loop, with the unrolled loop having more occurrences, since it requires more \nregisters to do the unrolling. Examining the header files shows that memcpyis translating directly into \nthe Pentium s string move instruction, which probably avoids these unneeded memory accesses since it \ncan access implementation resources not visible at the architectural level. The lesson of this experiment \nis clear. The implementa\u00adtion of the copy loop has a significant effect that is architecture\u00adand operating \nsystem-dependent. Fortunately, this aspect of collector performance is easy to tune, and so language \nimplementors should give it careful consideration to achieve the best performance. 5.3.2 Object Header \nRepresentation As described in Subsection 5.1, the purpose of the object header is twofold: it denotes \nobject type and length. Scan uses the type to determine if an object contains pointers, and both scan \nand copy use the length to determine how much to scan or copy, respectively. Optimizing header representation \nto facilitate quick access to length and type characteristics would reduce per-object costs of GC. We \ntherefore looked at a number of ways headers might be implemented. Our basic collector reserves 4 bits \nfor type and 26 bits for length (the remaining two bits are lost to low bit tags). The type-fields are \nthe same as those of SML/NJ 1.09. To determine if an object contains pointers, the type field is extracted \n(by shift and mask operations), and used to index an array of booleans. In our basic collector, lengths \nare always in words; in contrast, in SML/NJ 1.09, lengths may be in double words, words, or bytes, the \nunits are determined by the type within a long switch statement. 0 + n I + o tags=ours, Ien=words e \n00 x + tags=SML, Ien=words +3.0 A A A tags=ours, len=SML x X tags=SML, len=SML *XA 2.5 xx ~, I I I I \n, 1,0 1.5 2,0 2.5 Livesize(MB) Figure 9: Effect of Object Header Design (SGI) (Java) In addition to \nusing SML S type fields, we designed our own set so that determining if an object may contain point\u00aders \ncan be done by shift and mask alone. Our tags also facilitate using a shift/mask to determine the units \nof the length field. We experimented with each combination of tag and length representation to determine \nthe effect on the collector. The results of measuring these arrangements on the Java heaps on the SGI \nis shown in Figure 9. The Y-axis is GC speed in megabytes per second, and the X-axis is Iivesize in megabytes. \nBoth axes have non-zero origins. The results measured on the HP were similar, and so are not presented \nhere. The slowest of all arrangements is the SML/NJ 1.09 im\u00adplementation. The effect of implementing \nthese tests via shift /mask can be seen by the results of using our tags with multiple lengths. Making \nlengths in words removes the length-units test, and this has a significant effect, for both ours and \nSML S type-fields. Less significant is the cost of indexing an array to determine if the object contains \npoint\u00aders as shown by comparing SML tags to our own when using lengths in words. Overall, the best case \n(tags= ours,len=words) improves on the worst case (tags= SML,len=SML) by about 5~0. We expected that \nall of these improvements are a result of reduced instruction counts, especially in the case of the eliminated \nswitch statement for the length determination. This was confirmed by examining the results from Pixie, \nwhich closely resemble Figure 9. Again, the lesson is simple. Careful attention to the de\u00adsign of tags \nand length representations can have a positive effect on G C performance. The overhead of even a few \nextra instructions or branch penalties can be significant. Fortu\u00adnately for the implementor, these effects \nare not cache de\u00adpendent, and so simply choosing representations that mini\u00admize instruction counts is \nsufficient. 5.3.3 Object Length Representation The previous section indicated that GC performance im\u00ad \nprovements result from requiring that afl object lengths be in words. However, this requirement may be \nunacceptable when the language implementation also uses the length fields for its own operations. A more \nreasonable canonical form would be to specify lengths in bytes, which could then ac\u00ad curately represent \nthe lengths of byte-arrays and strings in the language. This would require two additional machine in\u00ad \ng~ Livesize (MB) Figure 10: Byte versus Word Length (SGI) (Major) structions to obtain the length in \nwords to be used by copy and SC-. We measured the effect of using lengths in bytes and compared it to \nour basic collector. The results for the SML compiler major heaps on the SGI are shown in Figure 10, \nwhile the effect on instruction counts (from Pixie) is shown in Figure 11, Both graphs have a Y-axis \nof GC speed in megabytes per second and a X-axis of livesize in megabytes. Both have non-zero origins \nand the Y-origin is not the same for each graph. Since object length is determined twice for each live \nob\u00adject, once by copy and once by scan, we would expect that difference between bytes and words would \nbe 4 x the num\u00adber of live objects. This is confirmed by the Pixie instruc\u00adtion counts, which differ \nby exactly this amount, resulting in about a 3% decline in speed. For the measured cases, the decline \nis about 2.5%. This clearly indicates that a canoni\u00adcal length format is a win, and while using bytes \nrather than words is less attractive to the GC, it is still more attractive than the heterogeneous case. \n5.3.4 Summary of Basic Variation Results Each of the results presented thus far analyzes a particu\u00adlar \nmechanism and the effect of its implementation on per\u00adformance. To summarize the effects in aggregate, \nwe have constructed bar graphs that show the relative improvement gained by the best of each of the implementations \nwe stud\u00ad 15~ Livesize(MB) Figure 11: (Pixie) Byte versus Word Length (SGI) (Major) 9 our lags our fags \nkm words 1 s len words Cl noip o unrolled 1 v 2(3 u N l-l o MAJOR rnin avg max JAVA min avg max Figure \n12: Overall Improvement on the SGI ied. Starting with the worst performing arrangement, we replaced \nthe worst with the best implementations one at a time, and then graphed the improvements as percent greater \nthan the starting case. The SGI graph is depicted in Fig\u00adure 12. For the SGI machine, the worst case \nused memcpy for its copy loop, allowed interior pointers, used SML length formats, and used SML tags, \nwhile the best case used the unrolled loop (denoted as unrolled in the legend), disallowed interior pointers \n(noip), required all object lengths in words (Ien words) and used our tag formats (our tags). For the \nHP machine, the worst case was identical to that of the SGI machine except that it used the unrolled \ncopy loop; the best case was also identical except for the use of memcpy in the copy loop. The HP results \nare in Figure 13. The graphs depict minimum, average, and maximum observed improve\u00adment for both the \nMajor and Java benchmarks. The order that the mechanism implementations were replaced to the worst caae \nis arbitrary, and since effects are additive a dHer\u00adent order might yield different tier sizes, even \nthough total improvement would be the same. For both machines, we see that the impact of the copy loop \n(the lowest tier of the bars) is far greater for the Java benchmark than for the Major benchmark. This \nis likely due to the larger average object size in the Java heaps, thus improving the performance of \nthe cache as well as the cost of loop unrolling. The remaining effects are all instruction count related. \nThe required disuse of interior pointers (sec\u00adond tier) can be a significant win, as shown by the MA\u00adJOR/max \ncase of Figure 12 which happens to contain many interior pointers, The Java heaps have no interior point\u00aders, \nso the measured difference is only in the support for potentially having them. In general, the instruction \ncount\u00adbased improvements have greater relative effect on the HP machine because of its fairly unwavering \ncache effects. Overall, these graphs paint a clear picture: careful im\u00adplementation can net sigmficant \ngains. The GC Speed of the SGI heaps improves by up to !%~ol while the HP heaps see up to 23 % improvement. \n5.4 The Costs and Benefits of Advanced Features The advanced features discussed in Subsection 2.2 can \nboth introduce new costs into GC and yield performance benefits. Here we explore the costs and benefits \nof many of the mech\u00adanisms needed to implement generational collection, typed areas, and big-object spaces. \nMAJOR rnin avg max JAVA min avg max Figure 13: Overall Improvement on the HP 5.5 Multiple From-Spaces \nFrom-space will, in general, be composed of severzd distinct regions of memory in collectors that implement \nmultiple gen\u00ad erations, that segregate objects by type, or that support non-contiguous spaces. One impact \nof this is that a simple range check of a pointer against two register-resident vaL ues is no longer \nsufficient for scan to determine if a pointer points into from-space. We explore two possible implementations \nof multiple from\u00ad spaces here. The fist is to use a table cent aining boolean values indicating whether \nan address is in from-space using some part of the address as an index. If the index is just the upper \nn bits of the address for some n, then determin\u00ad ing whether a pointer is in from-space requires only \na shift and an array lookup. The second implementation is to keep the bounds of the from-spaces in a \npair of arrays and search these arrays doing a series of bounds checks. We consider both linear and binary \nsearches. To study the costs of multiple from-spaces in Oscar, we evenly divided a single-area from-space \ninto n smaller areas, and then used one of the above schemes to perform the from\u00ad space determination. \nThe results of running this experiment on the minor SML compiler heaps on the SGI are shown in Figure \n14. The Y-axis is GC speed in megabytes per second and has a non-zero origin, while the X-sxis is the \nnumber of from-spaces. The basic point represents the single-mea range check used by the basic collector. \nUse of the table lookup results in a small overhead com\u00ad pared to the basic case. More importantly, \nit shows es\u00ad 14-1 1 x + +- . .+ -----+------------+  n ~ -.. -*- Table lookup 1 --~ -- Array (linear \nsearch) o Array (binary search) 1234 8 Number of from-spaces Figure 14: Multiple Rem-Spaces (SGI) (Java) \n 1_____ ____ ___ -\u00ad 1241. i 10 \\ \u00ad .\u00ad ._. _ -_ ._ ._. _ -_ ---\u00ad---\u00ad Array(pixie) Locals (pixie) Array \n(actual) ---- Array (actual) Locals (actual) 1. ------ Locals (actual) 8 s .. -.. -. . . . ---\u00ad --------. \n6  IV -w ~ Numberof to-spaces Figure 15: Multiple To-Spaces (SGI) (Major) sentially constant performance \nas the number of spaces in\u00adcreases. This suggests that accesses to the table have good locality and thus \nare inexpensive. The disadvantage is that the table itself may be large, depending upon the granu\u00adlarityy \nof the address ranges used as indices. Still, this re\u00adsult strongly suggests that GC designers need not \nshy away from using multiple from-spaces, even if the number of divi\u00adsions is large. The array schemes, \non the other hand, store only the from-space bounds as meta-data, and so are very space-efficient. However, \nboth array schemes perform poorly compared to the table scheme, with the simplicity of a lin\u00adear search \nwinning over binary search for small numbers of from-spaces, This indicates that a GC designer using \nan w\u00adray scheme should keep an expected number of from-spaces in mind when choosing an implementation. \n5.6 Multiple To-Spaces Generational-collection, typed arenas, big object spaces, and general support \nfor non-contiguous spaces all may require that to-space actually be many separate spaces. This means \nthat when an object is copied, the collector must decide into which space to copy it, as well as maintain \nseparate scan and copy pointers for each to-space area. A straightforward scheme involves keeping the \nmultiple scan and copy pointers in arrays. Alternatively, we might define local variables for each scan \nand copy pointer in an attempt to keep them in registers during GC, just as the single scan and copy \npointers are keck in rezisters for our basic collector. of course. as the > number of local variables \napproaches the number of general purpose registers on the machine, we would expect them to be spilled \ninto the stack. We explore both of these implementations in Oscar, us\u00ading a technique similar to our \nmultiple from-space study in Subsection 5.5. We divide to-space into several regions, and then systematically \ncopy each successive object to a difTer\u00adent region. This method cannot model the cost of deciding which \ncopy pointer to use (this is fundamentally a policy de\u00adcision ). but we believe it realistically models \nthe other costs ,4 . of maintaining multiple pointers. Figure 15 shows the results averaged over the \nSML Major heaps on the SGI, The Y-axis, which has a non-zero origin, is GC speed in megabytes per second, \nwhile the X-axis is the number of to-spaces modeled. To make the data easier to read, we have omitted \nmarkers for the individual points, but the tick marks on the X-axis indicates which sizes we actually \nmeasured. b~ 1816 32 Number of to-spaces Figure 16: Multiple To-Spaces (HP) (Major) The uppermost two \ncurves show the speeds for the ar\u00adray and local variable implementations, calculated based on instruction \ncounts from Pixie. Notice that the array im\u00adplementation is almost constant across different numbers \nof from-spaces, which is as one would expect. The local vari\u00adable graph drops with additional sets of \nlocal variables until most of the variables have been spilled into the stack, at which point the curve \nalso levels off. Disassembling the code showed that the substantial drops at the beginning are also due \nto more efficient compilations of the switch statement that avoid the use of a jump table. The lower \ntwo curves in Figure 15 represent the average speeds calculated from elapsed time measurements. The curve \nfor the local variables is shaped roughly like the Pixie version of the curve, although it never levels \noff completely. However, the array implementation shows some very un\u00adusual memory hierarchy effects, \nwhich we are not funda\u00admentally able to explain, The general trend is mostly flat (except for the aberrations \nat 3, 4, 13, and 14 spaces), but much slower relative to the local variable implementation when compared \nto the Pixie numbers. One possible expla\u00adnation is that our arrays of copy and scan pointers may have \nan unfortunate alignment with respect to our direct\u00admapped cache. The ability to do memory hierarchy \nsimu\u00adlations would be extraordinarily valuable here in aiding our understanding of these unusual results. \nFigure 16 shows the elapsed time-based results for the HP platform, in the same format as Figure 15. \nThis graph is much more similar to the Pixie curves in Figure 15, which is another demonstration that \ncache effects have less of an impact on the HP. This result allows us to tentatively accept the array \nimplementation as the better of the two. In comparison to our basic collector, the average speed for \nit (Pixie version) on the SGI is 17.3 MB/s, and for elapsed time, 11,0 MB/s. Both the array and local \nvariable implementations are so much slower because of to-space se\u00adlection policy we chose. After copying \neach object, we add 1 to the current (copy index and then perform a modulo operation, which was implemented \nusing a costly div in\u00adstruction in order to make the code generated not dependent upon the number of \nto-spaces. It is perfectly reasonable to expect that an actual multiple to-space policy could be less \nexpensive; in fact, the policy used for our next experiment with type segregation in the next section \nrequires minimal additional overhead because each to-space has an associated from-space, and the process \nof testing a pointer to see if it points into from-space also returns a to-space index. l   l   \n l   1   l i 2 3 4 s Livesize(MB) Figure 17: Type Segregation Schemes (HP) (Major) 5.7 Segregation \nby Type By segregating objects by type, some collectors are able to take advantage of their type-related \nproperties to im\u00adprove GC performance. For example, keeping non-pointer\u00adcontaining objects in their own \narea allows the collector to avoid scanning that mea. If we segregate objects of fixed size and type, \nwe can avoid storing headers for them (as their lengths and types are implied by their locations in memory), \nand thus avoid copying the headers during GC. Of course, creating multiple type-baaed regions incurs \nthe coats of multiple to-and liom-spaces, as already studied in Subsections 5.5 and 5.6. Here we explore \nsome of the other issues that are specific to segregation by type. We address the iaaues of multiple \nto-and from-spaces in this experiment by using table Iookups for the from-space determination and by \nkeeping our multiple scan and copy pointers in arrays-the optimal choices as suggested by our previous \nexperiments. The table used to determine whether a pointer points into from space contains an arena index, \nwhich can be used directly to look up the to-space. We examine the effects of several segregation schemes \nin the context of GC types: (a) doing no segregation (denoted as single in the leg\u00adend), (b) separating \nthe non-pointer-containing objects from the re3t ( pnp ),  (c) storing (iixed length) pairs without \nheaders separately from the other objects ( prnpr ), (d) combining schemes (b) and (c) ( ppnp ), and \n (e) extending scheme (d) by adding a third area for mu\u00adtable pinter containing typea ( ala 109 )  \nNote that only the major collection heaps from SML/NJ have sufficient information for this experiment, \nalthough we did process the Java heaps so that we could do a simpler version of this experiment, and \ngot similar results.. The results for the SML major collections on the HP compared to the basic collector \nare shown in Figure 17. The Y-axis is GC speed in megabytes per second, while the X\u00adaxis is the livesize \nin megabytes. 1The SML/NJ -1.09 [13] rrmtime uses this mutable area to aid in remembered set calculation \nin support of multiple generations. There are several trends that are important. Notice first that the \nbasic collector outperforms the single arena case by about 14% again illustrating the overhead of multiple\u00adspace \nsupport. Segregating non-pointer-cent aining objects provides a benefit, as this not only allows the \ncollector to avoid scanning them but also allows it to scan every pointer\u00adcontaining object for pointers \nwithout further regard to type. On average, this provides around a 15% gain in speed over the basic case. \nKeeping the pairs separate nets about 27~0 improvement; the savings derived from not copying and scanning \nthe header Me significant since the header repre\u00adsents 33~0 of the overall size of a pair, and this advantage \nis greatly multiplied due to the high frequency of pairs in SML heaps. We can see that the two schemes \ncombine quite well with just over 4270 overall improvement. Finally, adding an additional pointer-containing \narea (like the mutable region in the a la 109 scheme) does not adversely affect perfor\u00admance, which suggests \nthat if it derives other benefits, it can be done without penalty. In general, we can see that the benefits \nof all of these segregation schemes far outweigh the costs of the additional mechanisms needed to support \nthem. 5.8 Big Objects It is waatefid to copy very large objects that survive repeated collections. For \nthis reason, some collectors keep large ob\u00adjects in a separate area that is managed by mark-and-sweep \ncollection. For example, SML/NJ 1.09 keeps its code ob\u00adjects in such special regions. Although a complete \nstudy of big object spaces is beyond the scope of the current work, since SML/NJ 1.09 supports this feature, \nwe were curious to at least see if a more extensive study was warranted. To implement a big object space, \nour collector maintains a linked list of handles that point to big objects; during scan, big objects \nare themselves marked as they are reached, After all data has been scanned and copied, the collector \nsweeps through the linked list of handles, adding the ones that point to garbage (unmarked) objects to \nthe free list. To study the costs and benefits involved, we compared the mark-and-sweep technique for \nbig objects to the basic copy everything technique. We used heaps that contained big objects, as well \naa the same heaps with the big objects removed. These tests include only the objects classified by SML/NJ \nas big objects in the big object space (strings that contain code). A more general study would need to \ncon\u00adsider a range of other possibilities, for instance, classifying all non-pointer-containing objects \nabove a certain size as big objects. The results for the SML major compiler heaps on the SGI are shown \nin Figure 18. The Y-axis is GC speed in megabytes per seconds, and the X-axis is livesize in megabytes. \nThe following trend is clear: copying big objects is ex\u00adpensive. When compared to the basic collector \non heaps with the big objects removed, the mark-and sweep tech\u00adnique adds a non-trivial overhead. However, \nof course, the former is not really an option; big objects exist and must be collected. Given this, an \nimportant result is that once the mark-and-sweep space is in place, actually collecting big objects imposes \nonly a minimal impact on speed, and furthermore shows a significant advantage over copying the big objects. \nOne final trend is worth explaining; it appears that the advantage of these techniques diminishes as \nthe livesize increases. The reason for this is simple: ail of these 40 o copy, No Bos oo o Mark-sweep, \nNo BOS Q A Mark-sweep, BOS included m 30 g 22 o o Copy, BOS included 2 ~ 000 o &#38; 20 00 ~fif ~oo gana \n~ 008 0:8 fi8a; u 000000 u 1 0 ~ 6 789 Livesize(MB) Figure 18: Big Object Schemes (SGI) (Major) heaps \ncontain roughly the same amount of big object data, thus as the livesize increases the number of small \nobjects increase and the effect of the small objects becomes more pronounced. In summary, it appears \nthat for languages that generate big objects, special support for them can be a significant advantage. \nFurther study will be need to gain a clear idea of exactly what the design space of desirable options \nis. 6 Related Work There have been many sophisticated copying garbage col\u00ad lectors designed and implemented. \nOur understanding of what mechanisms are needed to implement such collectors draws especially on the \nlanguage independent GC toolkit of Hudson et al. [8] and on Reppy s SML/NJ collector [13]. W~lson s survey \n[17] provides pointers to many individual papers concerning copying GC implementation. Our study ignores \nGC related costs that occur while the user s code is executing. The study by Tardlti and Di\u00ad wan [14] \nexamines these costs in some detail. Our study also ignores the costs associated with maintaining and \nusing remembered sets. There have been a number of papers con\u00ad cerning this issue, but the work by Hosking \n[7, 6] is perhaps the most complete. In general, these studies complement our current work. There have \nbeen a number of studies relating cache per\u00ad formance and GC [12, 19, 4], but they are concerned with \na different set of issues than we are, Zorn has compared the cost of copying and mark-and-sweep collection \n[20] and the cost of conservative collection to malloc-and-free alloca\u00ad tion [21], but again he does \nnot provide the same multiple language and platform context that our current study does, and he also \nfocuses on different issues. Zorn [3] has used a trace-oriented approach to study collection in the context \nof database collection. We are unaware of any other general studies of the issues considered here. 7 \nFuture Work An important avenue for future work is improving Oscar it\u00adself. We consider it particularly \nimportant to extend Oscar to enable more detailed memory hierarchy studies by us\u00ading address tracing. \nWith such facilities, we should be able to better understand which effects are issues of instruction \ncounts and pipeline structure and which are due to caching, Such traces will also allow us to study the \neffects of other cache ~chit ect ures, in particular ones that we might antic\u00adipate becoming common in \nthe future. We are also interested in extending Oscar to study other GC techniques. One obvious study \nsuggested by the current work is a more extensive examination of big-object space related issues, such \nas the minimum size of big objects, whether or not they can contain pointers, and how they are allocated. \nAnother issue is how non-Cheney scanning techniques [11, 18] might tiect performance. Since the goal \nof these techniques is usually to improve the performance of the client, we expect such a study would \nalso have to include programming language dependent/client side mea\u00adsurements. Finally, in the long term \nwe hope to be able to use Oscar to directly compare copying and mark-and-sweep collection. Finally, it \nis also important that we extend the range of language implementations we can study. To this end, we \nhave begun to instrument a Smalltalk implementation, and we expect to finish this implementation soon, \nWe suspect that the distribution of object types and sizes in Smalltalk is considerably dflerent from \nour current heaps and we are eager to dkcover how this affects our current results. 8 Conclusions We \nfound Oscar to be extremely valuable in studying GC performance for a number of reasons. Since captur;ng \na heap is much simpler than making systematic changes to an entire language implementation, it is easy \nto determine the performance of GC techniques for many different languages. In addition, the portable, \ncanonical format of the snapshots allows us to study the heaps on machine types other than the one on \nwhich they were generated. The replay program also makes it easier to repeat and control experiments, \nand it avoids having to rerun client code just to generate heaps with which to study collection. The \nlanguage independent nature of Oscar does restrict our studies to issues that are not directly coupled \nto the programming language imple\u00admentation, but this restriction also aids us in separating the issues \ninto component parts, crucial when trying to under\u00adstand a complex system. Using Oscar, we have have \nbeen able to quantitatively es\u00adtablish a number of issues about copying GC performance. In particular, \nit is quite clew from our study that GC per\u00adformance can be significantly improved if careful attention \nis paid to implementation details; we measured gains of up to 95Y0. In some cases, the same implementations \nexhibited similar trenda on both architectures (such as in the choice of type representation), but for \nothers, the outcome was architecture-and language-dependent (as in the copy loop implementation). Particularly \ninteresting are some of the results about the costs and preferred techniques for imple\u00admenting certain \nadvanced techniques. These results iden\u00adtify implementations that can be used without major per\u00adformance \npenalty. We found that some of the advanced tech\u00adniques can result in significantly faster collectors, \nwith gains of as much as 4270 over a well-implemented basic collector, despite their increased complexity. \nWe have made Hicks, et al. [5], our data, the heap snap\u00adshots used here, as well as the source for Oscar \navailable via anonymous ftp at: f tp: //f tp. cis. upenn. edulpubioscar. Acknowledgments Thanks to our \nreaders, Alex Garthwaite, Scott Alexander, and Angeles Keromytis, and to the students of CIS 570, for \nthe initial demonstration that a snapshot replay program could be used to improve GC performance. Thanks \nto Eliot Moss, Amer Diwan, and Tony Hosking for help with the GC toolkit, and to the SML/NJ developers, \nespecially John Reppy, for SML/NJ 1.09. Special thanks to Alex Garthwaite for implementing the changes \nto Java and for providing the Java heap, and to Jim Henson for inspiration. References [1] P. J. Caudill \nand A. Wirfs-Brock. A Third-Generation SmaUtalk-80 Implementation. In N. Meyrowitz, edi\u00adtor, 00PSLA 86 \nACM Conference on Object-Oriented Systems, Languages and Applications, volume 21(11) of ACM SIGPLAN Notices, \npages 119-130. ACM Press, Oct. 1986. [2] C. J. Cheney. A Non-Recursive List Compacting Al\u00adgorithm. Communications \nof the ACM, 13(11):677-8, Nov. 1970. [3] J. E. Cook, A. L. Wolf, and B. G. Zorn. Partition Selec\u00adtion \nPolicies in Object Database Garbage Collection. In Proceedings of the 1994 ACM SIGMOD International Conference \non Management of Data, pages 371-382, Minneapolis, MN, May 1994. [4] A. Diwan, D. Tardlti, and J. E. \nB. Moss. Mem\u00adory Subsystem Performance of Programs using Copy\u00ading Garbage Collection. In Conference Record \nof the Twenty-first Annual ACM Symposium on Principles of Programming Languages, ACM SIGPLAN Notices. \nACM Press, Jan. 1994. [5] M. W. Hicks, J. T. Moore, and S. M. Nettles. The Measured Cost of Copying Garbage \nCollection Mecha\u00adnisms. Technical Report MS-CIS-97-06, Department of Computer and Information Science, \nUniversity of Perm\u00adsylvania, April 1997. [6] A. L. Hosking and J. E. B. Moss. Protection Traps and Alternatives \nfor Memory Management of an Object-Oriented Language. In Proceedings of the Fourteenth Symposium on Operating \nSystems Principles, volume 27(5) of Operating Systems Review, pages 106-119, Asheville, North Carolina, \nDec. 1993. ACM Press. [7] A. L. Hosking, J. E. B. Moss, and D. StefanoviL A Comparative Performance Evaluation \nof Write Bar\u00adrier Implementations. In A. Paepcke, editor, OOP-SLA 92 ACM Conference on Object-Oriented \nSystems, Languages and Applications, volume 27(10) of ACM SIGPLAN Notices, pages 92 109, Vancouver, British \nColumbia, Ott. 1992. ACM Press. [8] R. L. Hudson, J. E. B. Moss, A. Diwan, and C. F. Weight. A Language-Independent \nGarbage Collector Toolkit. Technical Report COINS 91-47, University of Massachusetts at Amherst, Dept. \nof Computer and In\u00adformation Science, Sept. 1991. [9] R. Jones. Garbage Collection: Algorithms for Auto\u00admatac \nDynamic Memory Management, Wiley, 1996, With a chapter on Distributed Garbage Collection by Rafael Lins. \n [10] H. Lieberman and C. E. Hewitt. A Real-Time Garbage Collector Based on the Lifetimes of Objects. \nCommu\u00ad nications of the ACM, 26(6):419 29, 1983. [11] D. A. Moon, Garbage collection in a large LISP \nsys\u00adtem. In G. L. Steele, editor, Conference Record of the 1984 ACM Symposium on Lisp and Functional \nPro\u00adgramming, pages 235-245, Austin, Texas, Aug. 1984. ACM Press. [12] M. B. Reinhold. Cache Performance \nof Gaxbage-Collected Programs. In Proceedings of SIGPLAN 94 Conference on Programming Languages Design \nand Im\u00adplementation, volume 29 of ACM SIGPLAN Notices, Orlando, Florida, June 1994. ACM Press. [13] J. \nH. Reppy. A High-Performance Garbage Collector for Standmd ML. Technical memorandum, AT&#38;T Bell Laboratories, \nMurray Hill, NJ, Dec. 1993. [14] D. Tarditi and A. Diwan. Measuring the Cost of Stor\u00adage Management. \nTechnical Report CMU-CS-94-201, Carnegie Mellon University, 1994. [15] D. M. Ungar. Generation Scavenging: \nA Non-Disruptive High Performance Storage reclamation al\u00adgorithm. ACM SZGPLAN Notices, 19(5):157-167, \nApr. 1984. [16] D. M. Ungar and F. Jackson. An Adaptive Tenuring Policy for Generation Scavengers. ACM \nTransactions on Programming Languages and Systems, 14(1) :1 17, 1992. [17] P. R. Wilson. Uniprocessor \nGarbage Collection Tech\u00adniques, In Y. Bekkers and J. Cohen, editors, Pro\u00adceedings of International Workshop \non Memory Man\u00adagement, volume 637 of Lecture Notes in Computer Science, University of Texas, USA, 16-18 \nSept. 1992. Springer-Verlag. [18] P. R. Wilson, M. S. Lam, and T. G. Moher. Effec\u00adtive static-graph reorganization \nto improve locality in garbage-collected systems. In Proceedings of the 1991 SIGPLAN Conference on Programming \nLanguage De\u00adsign and Implementation, pages 177 191, Toronto, On\u00adtario, June 1991. ACM Press. Published \nas SIGPLAN Notices 26(6), June 1992. [19] P. R. Wilson, M. S. Lam, and T. G. Moher. Caching Considerations \nfor Generational Garbage Collection. In Conference Record of the 1992 ACM Symposium on Lisp and Functional \nProgramming, pages 32-42, San Francisco, CA, June 1992. ACM Press. [20] B. Zorn. Barrier Methods for \nGarbage Collection. Tech\u00adnical Report CU-CS-494-90, University of Colorado, Boulder, Nov. 1990. [21] \nB. Zorn. The Measured Cost of Conservative Garbage Collection. Software Practice and Experience, 23:733\u00ad756, \n1993. posters  \n\t\t\t", "proc_id": "258948", "abstract": "We examine the costs and benefits of a variety of copying garbage collection (GC) mechanisms across multiple architectures and programming languages. Our study covers both low-level object representation and copying issues as well as the mechanisms needed to support more advanced techniques such as generational collection, large object spaces, and type segregated areas.Our experiments are made possible by a novel performance analysis tool, <i>Oscar</i>. Oscar allows us to capture snapshots of programming language heaps that may then be used to replay garbage collections. The replay program is self-contained and written in C, which makes it easy to port to other architectures and to analyze with standard performance analysis tools. Furthermore, it is possible to study additional programming languages simply by instrumenting existing implementations to capture heap snapshots.In general, we found that careful implementation of GC mechanisms can have a significant benefit. For a simple collector, we measured improvements of as much as 95%. We then found that while the addition of advanced features can have a sizeable overhead (up to 15%), the net benefit is quite positive, resulting in additional gains of up to 42%. We also found that results varied depending upon the platform and language. Machine characteristics such as cache arrangements, instruction set (RISC/CISC), and register pool were important. For different languages, average object size seemed to be most important.The results of our experiments demonstrate the usefulness of a tool like Oscar for studying GC performance. Without much overhead, we can easily identify areas where programming language implementors could collaborate with GC implementors to improve GC performance.", "authors": [{"name": "Michael W. Hicks", "author_profile_id": "81100060959", "affiliation": "Computer and Information Science Department, University of Pennsylvania, Philadelphia, PA", "person_id": "PP40035564", "email_address": "", "orcid_id": ""}, {"name": "Jonathan T. Moore", "author_profile_id": "81100045762", "affiliation": "Computer and Information Science Department, University of Pennsylvania, Philadelphia, PA", "person_id": "PP39065256", "email_address": "", "orcid_id": ""}, {"name": "Scott M. Nettles", "author_profile_id": "81100150673", "affiliation": "Computer and Information Science Department, University of Pennsylvania, Philadelphia, PA", "person_id": "PP14062809", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/258948.258976", "year": "1997", "article_id": "258976", "conference": "ICFP", "title": "The measured cost of copying garbage collection mechanisms", "url": "http://dl.acm.org/citation.cfm?id=258976"}