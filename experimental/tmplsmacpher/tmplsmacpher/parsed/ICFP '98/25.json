{"article_publication_date": "09-29-1998", "fulltext": "\n Fold and Unfold for Program Semantics Graham Hutton Languages and Programming Group Department of Computer \nScience University of Nottingham, UK http://www.cs.nott.ac.uk/-gmh Abstract In this paper we explain \nhow recursion operators can be used to structure and reason about program semantics within a functional \nlanguage. In particular, we show how the re-cursion operator fold can be used to structure denotational \nsemantics, how the dual recursion operator unfold can be used to structure operational semantics, and \nhow algebraic properties of these operators can be used to reason about program semantics. The techniques \nare explained with the aid of two main examples, the first concerning arithmetic expressions, and the \nsecond concerning MiIner s concurrent language CCS. The aim of the paper is to give functional programmers \nnew insights into recursion operators, program semantics, and the relationships between them. Introduction \nMany computations are naturally expressed as recursive pro-grams defined in terms of themselves, and \nproperties proved of such programs using some form of inductive argument. Not surprisingly, many programs \nwill have a similar recur-sive structure, and many proofs will have a similar inductive structure. To \navoid repeating the same patterns of program and proof again and again, special recursion operators and \nproof principles that abstract out the common patterns can be introduced, allowing us to concentrate \non the details that are specific to each different application. In the functional programming community, \nmuch previ-ous work in this area has focussed on a recursion operator called fold, and on its associated \nproof principle called uni-versality. Fold captures a common programming pattern in which a list of values \nis processed in a certain recursive manner, and universality captures a common pattern of in-ductive \nproof concerning programs that process lists. Fold and universality have proved useful in a variety of \napplica-tion areas, including algorithm construction [l, 11, 21, hard- ware construction [7, 61, compiler \nconstruction [12], and au-tomatic program transformation [20, 3, 81. Using ideas from category theory, \nfold has been uniformly generalised from lists to a large class of recursive datatypes [lo, 141. In this \npaper we are concerned with the application of recursion operators in the area of program semantics. \nOne of the most popular styles of semantics is the denotational approach [19], in which the meaning of \nprograms is defined using a valuation function that maps programs into values in an appropriate semantic \ndomain. The valuation function is defined using a set of recursion equations, and must be compositional \nin the sense that the meaning of a program is defined purely in terms of the meaning of its syntactic \nsubcomponents. In fact, the pattern of recursion required by compositionality is precisely the pattern \nof recursion cap- tured by fold. Hence, a denotational semantics can be char- acterised as a semantics \ndefined by folding over program syn- tax. Although widely known in certain circles, many func- tional \nprogrammers are still not aware of this connection. The recursion operator fold has a natural dual, called \nunfold, which captures a common programming pattern in which a list of values is produced (as opposed \nto processed) in a certain recursive manner. The dual proof principle, again called universality, captures \na common pattern of coinduc-tive proof concerning programs that produce lists. Unfold has also been generalised \nfrom lists to a large class of recur-sive datatypes [lo, 141. While applications of fold abound, relatively \nlittle attention has been given to unfold in the functional programming community. Another popular style \nof semantics is the operational ap-proach [17], in which the meaning of programs is defined us-ing a \ntransition relation that captures single execution steps in an appropriate abstract machine. The transition \nrelation is defined using a set of inference rules, and the meaning of a program is given by repeatedly \napplying the relation to gen-erate a transition tree that captures all possible execution paths of the \nprogram. In fact, the pattern of recursion used to construct transition trees is precisely the pattern \nof recur-sion captured by unfold. Hence, an operational semantics can he characterised as a semantics \ndefined by unfolding to transition trees. This connection has been developed using category theory [18, \n211, hut most functional programmers are not aware of this connection. In this paper we explain how recursion \noperators can he used to structure and reason about program semantics within the functional language \nHaskell [16]. In particular, we show how fold can he used to structure denotational se-mantics, how unfold \ncan he used to structure operational semantics, and how algebraic properties of these operators can he \nused to reason about program semantics. The techniques are explained with the aid of two main ex-amples, \nthe first concerning arithmetic expressions, and the second concerning Milner s concurrent language CCS \n[15]. As the paper proceeds we adopt an increasingly categorical approach to semantics, to give a deeper \nunderstanding of the issues. However, previous knowledge of category theory is not required. The aim \nof the paper is to give functional programmers new insights into recursion operators, program semantics, \nand the relationships between them. 2 Denotational semantics In denotational semantics [19], the meaning \nof terms is de- fined using a valuation function that maps terms into values in an appropriate semantic \ndomain. In this section we ex-plain how a denotational semantics can be characterised as a semantics \ndefined by folding over syntactic terms. Formally, a denotational semantics for a language T of syntactic \nterms comprises two components: a set V of se-mantic values, and a valuation function [ 1 : T -+ V that \nmaps terms to their meaning as values. The valuation func-tion must be compositional in the sense that \nthe meaning of a compound term is defined purely in terms of the meaning of its T-subterms. When the \nset of semantic values is clear, a denotational semantics is often identified with a composi- tional \nvaluation function. 2.1 Arithmetic expressions As an example, let us consider a language of simple arith-metic \nexpressions, built up from the set Z of integer values using the addition operator +. The language E \nof such ex-pressions is defined by the following grammar: E ::= Z ] E+E We assume that parentheses can \nbe used to disambiguate expressions if required. The grammar for expressions can be directly translated \ninto a Haskell datatype definition, pa-rameterised over the type of values for flexibility: data Expr \na = Val a 1 Add (Expr a) (Expr a) For example, the expression 1+ (2 + 3) is represented by the value \nAdd (Val 1) (Add (Val 2) @al 3) ). From now on, we mainly consider expressions represented in Haskell. \nArithmetic expressions have an the obvious denotational semantics, given by taking V as the Haskell type \nInt of in-tegers and [ 1 : Expr Int + Int as the evaluation function for expressions defined recursively \nas follows: [Val nl = n I[Add 2 Yn = bn + Ml This definition satisfies the compositionality requirement, \nbecause the meaning of compound expressions of the form Add 1: y is defined purely by applying + to the \nmeanings of the subexpressions z and y. The evaluation function can be translated directly into a Haskell \nfunction definition: eval :: Expr Int -> Int eval (Val r-t) =n eval (Add x y) = eval x + eval y For example, \neval (Add (Val I) (Add (Val 2) (Val 3))) = 1+(2+3) = 6, or drawing expressions as trees: eval = 1 + \n= 6 A A Val 2 Val 3 2 3 i I Looking at this example, we see that an expression is evalu- ated by removing \neach constructor Val (or equivalently, re-placing each constructor Val by the identity function id on \nintegers), and replacing each constructor Add by the addi-tion function (+) on integers. That is, even \nthough eval was defined recursively, its behaviour can be understood non-recursively as simply replacing \nthe two constructors for expressions by the functions id and (+). 2.2 Fold for expressions Abstracting \nfrom the specific case of eval, we can consider the general case of a denotational semantics deno that \ngives meaning to arithmetic expression by replacing each Val by a function f, and each Add by a function \ng. By definition, a semantics defined in this manner will be compositional, be-cause the meaning of addition \nis defined purely by applying g to the meanings of the two argument expressions: deno (Val n) =fn deno \n(Add x y) = g (den0 x) (deno y) Since the behaviour of such functions can be understood non-recursively, \nwhy don t we actually define them in this manner? This is precisely what fold allows us to do. Using \nfold for arithmetic expressions, we can define denotational semantics for expressions simply by supplying \nthe function f that replaces each Val and the function g that replaces each Add. For example, using fold \nthe denotational semantics eval can be simply defined as follows: eval = fold id (+) As another example, \nusing fold we can define an alternative semantics camp tha.t doesn t evaluate expressions directly, but \nrather compiles expressions into a list of instructions for execution using a stack. As for eval, defining \nthe semantics using fold makes it compositional by definition: data Inst = PUSH Int 1 ADD conlp :: Expr \nInt -> [Instl camp = fold f g uhere fn = [PUSH nl g xs ys = xs ++ ys ++ [ADD] For example, camp (Add \n(Val 1) (Add @al 2) (Val 3))) = [PUSH 1, PUSH 2, PUSH 3, ADD, ADD]. The fold function itself can be defined \nsimply by ab-stracting on the free variables f and g in the general defini-tion of a denotational semantics \ndeno for expressions: fold f g (Val n) =fn fold f g (Add x y) = g (fold f g x) (fold f g y) The type \nof fold is given by the following inference rule: f :: a -> b :: b -> b -> b I3 fold f g : : Expr a \n-> b  2.3 Generalising Of course, the use of fold to define denotational seman-tics is not specific \nto our language of arithmetic expressions, but can be generalised to many other languages. For exam-ple, \nconsider extending expressions with integer variables of the form Var c for any character c. Then the \nfold opera-tor would simply be generalised to take an extra argument function h to replace each constructor \nVar in an expression: fold f g h (Val n) =fn fold f g h (Add x y) = g (fold f g h x) (fold f g h y) fold \nf g h (Var c) =hc In turn, the denotational semantics eval would be gener-alised to give the meaning \nof expressions as functions from stores (containing the value of each variable) to integers. Assuming \na type Store for stores and a function find for looking up the value of a variable in a store, eval is \ndefined using the generahsed fold as follows: eval :: Expr Int -> (Store -> Int) eval = fold f g h where \nf n = \\s -> n g fx fy = \\s -> fx s + fy s h c = \\s -> find c s Again, defining the semantics using fold \nmakes it composi-tional by definition. As in this example, the set of seman-tic values for most non-trivial \nlanguages will usually involve functions in some way. For a more general discussion on the use of fold \nto return functions, see [4]. In general, we have the following simple connection be-tween denotational \nsemantics and fold operators: Denotational semantics Folding over syntax trees 3 Operational semantics \nIn operational semantics [17], the meaning of terms is de-fined using a transition relation that captures \nexecution steps in an appropriate abstract machine. In this section we ex-plain how an operational semantics \ncan be characterised as a semantics defined by unfolding to transition trees. Formally, an operational \nsemantics for a language T of syntactic terms comprises two components: a set S of states, and a transition \nrelation -+ c S x S that relates states to all the states that can be reached by performing a single \nex-ecution step. (For some applications, a more general notion of transition relation may be appropriate, \nbut this simple notion suffices here.) If (s,s ) E -, we say that there is a transition from state s \nto state s , and usually write this as s + s . When the set of states is clear, an operational semantics \nis often identified with a transition relation. 3.1 Arithmetic expressions Returning to our example \nfrom the previous section, simple arithmetic expressions have an obvious operational seman-tics, given \nby taking S as the Haskell type Expr of expres-sions, and ---) c Expr x Expr as the transition relation \ndefined by the following three inference rules: Add (Val n) (Val m) + Val (n + m) x -2 Y -Y Addxy + Addx \ny Add x y + Add 2: y The first rule states that two values can be added together to give a single value, \nand the last two rules permit the first rule to be applied to either argument of an addition expression. \nFor example, the (concrete) expression (1 + 2) + (3 + 4) has two possible transitions, because the first \ntransition rule can be applied to either argument of the top-level addition: (1+2)+(3+4) -3+(3+4) (1+2)+(3+4) \n-(1+2)+7 By repeated application of a transition relation, is is pos- sible to generate a transition \ntree that captures all possible execution paths for a syntactic term. For example, the ex-pression (1 \n+ 2) + (3 + 4) gives rise to the following transition tree, which captures the two possible execution \npaths: 10 10 The inference rules defining the transition relation for expressions can be easily translated \ninto a Haskell function definition. The relation is represented as a list-valued func-tion that maps \nexpressions to lists of expressions that can be reached by performing a single execution step: trans \n*. Expr Int -> [Expr Intl trans (Val n) *i Cl trans (Add (Val n) (Val m)) = [Val (n+m)l trans (Add x \ny) = [Add x y I x' <-tram xl ++ [Add x y I y <-trans yl In turn, we can define a Haskell datatype for \ntransition trees, and an execution function that converts expressions into trees by repeated application \nof the transition function: data Tree a = Node a [Tree al exec : : Expr Int -> Tree (Expr Int) exec e \n= Node e [exec e I e <-trans el Looking at the definition of exec, we see that an expres-sion is executed \nto yield a tree by taking the expression unchanged as the root of the tree (or equivalently, applying \nthe identity function id on expressions), and generating a list of residual expressions to be processed \nto give the sub-trees by applying the trans function. That is, even though exec was defined recursively, \nits behaviour can be under-stood non-recursively as simply applying the identity func-tion id to generate \nthe root expression, and the transition function trans to generate a list of residual expressions to \nbe processed to generate the subtrees.  3.2 Unfold for trees Abstracting from the specific case of exec, \nwe can consider the general case of an operational semantics oper that gives meaning as trees by using \na function f to generate the root of the tree, and a function g to generate a list of residual values \nto be processed to generate the subtrees: oper : : a -> Tree b oper x = Node (f x) [oper x I x <-g xl \n Since the behaviour of such functions can be understood non-recursively, why don t we actually define \nthem in this manner? This is precisely what unfold allows us to do. Using unfold for trees, we can define \noperational semantics as trees simply by supplying the function f that generates the root of the tree, \nand the function g that generates the residual values. For example, using unfold the operational semantics \nexec can be simply defined as follows: exec = unfold id trans The unfold function itself is defined \nsimply by abstract-ing on the free variables f and g in the general definition of an operational semantics \noper as trees: unfold f g x = Node (f x) [unfold f g x I x <-g xl  The type of unfold is given by the \nfollowing inference rule: f :: a -> b : : a -> [al g unfold f g : : a -> Tree b  3.3 Generalising Of \ncourse, the use of unf old to define operational semantics is not specific to our language of arithmetic \nexpressions, but can be generalised to many other languages. That is, we have the following simple connection \nbetween operational semantics and unfold operators: I 1 I Operational semantics 1 Q Unfolding to transition \ntrees This is precisely dual to the connection for denotational se-mantics given in the previous section. \nHence, taking a struc- tured approach to program semantics using recursion opera-tors has revealed a \nduality between denotational and opera-tional semantics that might otherwise have been missed. In the \nnext section we will see that using recursion operators also brings benefits when proving properties \nof sem. 4 Reasoning about semantics One of the main reasons for defining the formal semantics of programming \nlanguages is to support formal reasoning about languages and programs written in them. In this sec-tion \nwe explain how properties of semantics can be proved using the uniuer.9aZityof the recursion operator \nfold [13, 141, rather than explicit structural induction. 4.1 Arithmetic expressions Consider the following \nthree equations concerning our se-mantics for (finite) simple arithmetic expressions: (1) and [deno e \n= deno e I e <-trans e] = True (2) and [size e' < size e I e <-trans el = True (3) and Cn == deno e \nI n <-vals (oper e)] = True  The first equation states that the transition function pre-serves the \ndenotational semantics of expressions. The sec-ond equation states that the transition function decreases \nthe size of expressions, where the size is defined as the num-ber of Add constructors. The last equation \nstates that the denotational and operational semantics are equivalent, in the sense that the integer \nvalues in the transition tree gener-ated by the operational semantics are all equal to the value obtained \nfrom the denotational semantics. The auxiliary functions size and vals are easy to define. Equations \n(1) and (2) above can be proved by induction on the structure of e. In turn, by making use of these two \nauxiliary results, (3) can be proved by induction on the size of e. However, the two proofs using structural \ninduction can also be proved using the universality of fold, which avoids the need for explicit use of \ninduction. 4.2 Universality for expressions For simple arithmetic expressions, the universality of fold \nis captured by the following equivalence: h (Val n) = f n h (Add x y) = g (h x) (h y) h = fold f g \nThis equivalence states that fold f g is the unique solution to the first two equations, and can itself \nbe proved using a simple structural induction. Indeed, the two equations are precisely the assumptions \nrequired to show that h = fold f g using structural induction. For specific cases then, by verifying \nthe two assumptions (which can typically be done without the need for induction), we can then appeal \nto universality to complete the inductive proof that h = fold f g. In this manner, universality captures \na common pattern of inductive proof, just as fold itself captures a common pattern of recursive definition. \nTo prove equation (1) above using the universality of fold, it must first be expressed in the form h \n= fold f g. In this case, h can be defined simply by abstracting over e on the left-hand side of the \nequation: h e = and [deno e = deno e I e <-trans el Abstracting on the right-hand side of the equation \ngives the constant function \\e -> True, which can be expressed in the form fold f g by defining f n = \nTrue and g x y = x %% y. Hence, by appealing to the universality of fold for expres-sions, we can conclude \nthat equation (1) is equivalent to the following two equations: h (Val n) = True h (Add x y) = (h x) \n%% (h y) These equations can now be verified by routine calculations, without the need for an explicit \ninduction. Universality can also be used to prove (2) in a similar way, again without the need for an \nexplicit induction. 5 Concurrent processes in CCS Up to this point, all our examples have been concerned \nwith arithmetic expressions. For the remainder of the paper we show how our techniques apply to a real-life \nexample, Mil-ner s language CCS (Calculus of Concurrent Systems) for describing concurrent processes \n[15]. In this section we con-sider the Haskell datatypes required for the syntax and se-mantics of CCS \nprocesses, and show how they can be defined in an abstract manner as least jixpoints of functors. As \nwe shall see in subsequent sections, this approach will permit a more abstract treatment of the semantics \nof processes. Given a set N of process names, and a set o of process actions, the language P of processes \nin CCS is defined by the following grammar: P ::= N -constants -prefixing 1 5: I Pi -(finite) choice \n-parallelism I p -restriction -rela belling I prfi We assume that parentheses can be used to disambiguate \nprocesses if required. Named processes are defined by (pos-sibly recursive) equations. The set o of actions \nis assumed to comprise input actions a, b, c, . . ., the corresponding output actions a,&#38;, E, ., \nand the silent action T used to indicate synchronisation. A relabelling function f is a function from \nactions to actions that preserves their underlying structure, in the sense that f(~) = f(x) and f(r) \n= r. As a simple example of a process, consider the recursive equation A = a.A + b.A. Intuitively, this \nequation defines the process A that can either perform the action a and then continue as A again, or \nperform the action b and then con-tinue as A again. More formally, the meaning of a process can be described \nby a (possibly infinite) transition tree, in which the nodes represent the states of the process, and \nthe edges are labelled with the actions that are performed in moving between states. For example, the \nmeaning of A is given by the infinite tree pictured in Figure 1. Assuming types Nanre and Act for names \nand actions re-spectively, the grammar for processes can be directly trans-lated into a Haskell datatype \ndefinition: data Proc = Con Name I Pre Act Proc I Cho [Procl 1 Par Proc Proc I Res Proc Act I Rel Proc \n(Act -> Act) a b /\\ Figure 1: Transition tree for A = a.A + b.A (The Proc type could be parameterised \nover the type of actions, but we use a fixed type Act for simplicity.) In turn, a datatype for trees \ncan be defined as follows: data Tree = Node [(Act ,Tree)] However, there is another approach to defining \nProc and Tree that will permit the semantics of processes as trees to be defined in a more abstract manner. \nRather than defin-ing these types directly as recursive datatypes, we prefer to define them indirectly \nas least fixpoints of functors. 5.1 least fixpoints In semantics, it is common to model recursively \ndefined val-ues as least fixpoints of non-recursively defined functions [19]. For the special case of \nrecursively defined types, the least fixpoint Fix f of a type constructor f (a function from types to \ntypes) can be defined in Haskell as follows: nentype Fix f = In (f (Fix f)) For example, the recursive \ntype Proc can be expressed as the least fixpoint of a non-recursive type constructor P, where the definition \nfor P is precisely the same as for the original Proc type, except that each recursive call within the \ndefini-tion is replaced by an instance of a type parameter p: type Proc = Fix P data P p = Con Name 1 \nPre Act, p I Cho Cpl I Par p p 1 Res p Act 1 Rel p (Act -> Act) Constructors for the new Proc type are \ndefined simply by applying the tag In to the constructors for P: con n = In (Con n) pre a p = In (Pre \na p) cho ps = In (Cho ps) = In (Par p q) res p a = In (Res p a) rel p f = In (Rel p f) par P 9 In turn, \nthe recursive type Tree can be expressed as the least fixpoint of a non-recursive type constructor T: \ntype Tree = Fix T data T t = Node C(Act,t)l Given the above definitions, it can be shown that the Proc \nand Tree types defined as least fixpoints are isomorphic to the original types defined using explicit \nrecursion. That is, the types are equivalent in the sense that there is a one- to-one correspondence \nbetween their values.  5.2 Functors The next concept to be considered is that of a functor, which comes \nfrom category theory [9]. The notion of a functor is captured as a built-in class in Haskell, defined \na.s follows: class Functor f where map :: (a -> b) -> (f a -> f b) This definition states that a type \nconstructor f is a member of the class Functor if it is equipped with a map function that lifts functions \nof type a -> b to functions of type f a -> f b. Although not made explicit in the Haskell defini-tion, \na functor must also preserve the identity function and distribute over function composition, in the sense \nthat: map id = id map (g.h) = (map g).(map h) For example, the type constructor P can be made into an \ninstance of the class Functor with the following definition: instance Functor P where map f x = case \nx of Con n -7 Con n Pre a p -> Pre a (f p) Cho ps -7 Cho [f p 1 p <-psi Par p q -> Par (f p) (f q) Res \np a -> Res (f p) a Rel p g -> Rel (f p) g It is easy to verify that this definition satisfies the equations \nrequired of a functor. In turn, the type constructor T can be made into an instance of the class Functor \nas follows: instance Functor T where map f (Node xs) = Node [(a, f t) 1 (a,t) <-xsl In summary, we have \nnow expressed the recursive type Proc as the least fixpoint of a non-recursive functor P, and the recursive \ntype Tree as the least fixpoint of the non-recursive functor T. The map functions for both functors play \nno role yet, but they will in subsequent sections. 6 Operational semantics of CCS As for most languages \ninvolving some form of concurrency, the standard semantics for CCS is an operational semantics 1151. \nIn this section we show how the operational semantics for processes as trees can be defined in Haskell \nin an abstract manner using a polytypic version of fold. The operational semantics of CCS is given by \na transition relation + C P x o x P, where P is the set of processes, and o is the set of actions. If \n(P, a, P ) E -+, we say that the process P can perform the action a to become the process P , and usually \nwrite this as P 2 P . The transition relation + is defined by the following set of inference rules: z \n(A = P) a.P -% P P -5 P Q * 9 P -% P Q:Q PIQ~P IQ PIQ~PIQQ P 1 Q I, P I Q b P -+ P P3 P (a, ii # b) P\\a \n-f+ P \\a P[f] 2 P [f] For example, using these rules the named process A defined by A = a.A + b.A has \ntwo possible transitions: A:A A- bA By repeated application of the transition relation, it is pos- sible \nto generate a (possibly infinite) transition tree that captures all possible execution paths for a process. \nFor ex-ample, the process A gives rise to the tree in Figure 1. The inference rules defining the transition \nrelation for processes can be easily translated into a Haskell function definition. The relation is represented \nas a list-valued func-tion that maps processes to lists of (actionprocess) pairs that arise from single \nexecution steps: trans :: Proc -> [(Act,Proc)l trans (In x) = case x of Con n -? trans (defn n) Pre a \np -> C(a,p)l Cho ps -> concat (map trans ps) Par p q -7 [(a, par p q) I (a,p ) <-trans pl ++ C(b, par \np q') I (b,q ) <-trans ql ++ [(Tau, par p q ) 1 (a,p ) <-tram3 p, (b,q ) <-trans q, synch a bl Res p \na -7 [(b, res p a) 1 (b,p ) <-trams p, strip a /= strip bl Rel p f -7 [(f a, rel p f) I (a,p ) <-trans \npl The auxiliary function defn maps process names to their definitions and should be defined as appropriate \nby the user, while synch decides if two actions can synchronise, and strip removes any bars from an action \nto give its under-lying name. Both synch and strip are easy to define. In turn, we can define an execution \nfunction that con-verts processes into trees by repeated application of the tran-sition function using \nthe unfold function for trees: exec :: Proc -> Tree exec = unfold trans The general purpose unfold function \nfor our type Tree of transition trees can itself be defined as follows: unfold f x = In (Node [(a, unfold \nf x ) I (a.x ) <-f xl) However, by exploiting the fact that Tree is defined as the least fixpoint of \na functor, the execution function can be defined in a more abstract manner by repeated application of \na transition co-algebra using a polytypic version of unfold that is not specific to any particular recursive \ndatatype. 6.1 Co-algebras The concept of a co-algebra that we use comes from category theory, and generalises \nthe idea of a transition function. In Haskell, a co-algebra for a functor f is a function of type a -> \nf a for some specific type a. For example, the transition func-tion for processes can be converted into \na transition co-algebra for the functor T by the following simple definition: trans :: Proc -> T Proc \ntrans p = Node (trams p) A more general example of a co-algebra concerns the fixpoint type Fix f. In \nparticular, the inverse function out of the tag In is a co-algebra for any functor f: out :: Fix f -> \nf (Fix f> out (In x) = x  6.2 Polytypic unfold The co-algebra out is special among all co-algebras for \na functor f, being in fact the final co-algebra. Technically, this means that for any other co-algebra \ng : : a -> f a, there is a unique function unfold g : : a -> Fix f such that the following diagram commutes \n[13, 141: unfold g a-----------+Fixf fa *f (Fix f) map (unfold g) Using this diagram and the fact that \nout is the inverse to In, the unfold function itself can be defined as follows: unfold g = In . map (unfold \ng) . g That is, the function unfold g first applies the co-algebra g to break down an argument of type \na into a structured value of type f a, then applies the function map (unfold g) to recursively process \neach of the a components to give a value of type f (Fix f 1, and finally applies the tag In to give a \nvalue of the recursive type Fix f. In this manner, unfold is a general purpose function for producing \nvalues of a recursive type using a simple pattern of recursion. While previously we defined unfold functions \nthat were specific to particular recursive datatypes (for example, trees) the above version of unfold \nis polytypic [5], in the sense that it can be used with any recursive datatype that can be expressed \nas the least fixpoint of a functor. In the case of processes, because the datatype Tree is expressed \nas the least fixpoint of the functor T, and the tran-sition function is expressed as a co-algebra trans \n for T, the execution function that maps processes to trees can now be defined using the polytypic version \nof unfold: exec :: Proc -> Tree exec = unfold trans In summary, we have now expressed the operational \nse-mantics of processes as trees as the unique function unfold tram that makes the following diagram \ncommute: unfold trans pro= ----------+ Tree trans out I I T Proc *T Tree map (unfold trans. ) 7 Denotational \nsemantics of CCS In the previous section we defined an operational seman-tics for processes a.s trees \nby unfolding a transition func-tion expressed as a co-algebra. In this section we consider the less well-known \ndenotational semantics for processes as trees, and show how it can be defined in a dual manner by folding \na combining function expressed as an olgebru. 7.1 Algebras In the spirit of category theory, the notion \nof a co-algebra is dual to that of an algebra. In Haskell, an algebra for a functor f is a function of \ntype f a -> a for some specific type a. For example, the tag function In : : f (Fix f) -> Fix f is an \nalgebra for any functor f. A more specific example of a co-algebra concerns the semantics of processes \nas trees. In particular, it is natural to define an algebra for the functor P as follows: comb :: P Tree \n-> Tree comb x = In (Node (case x of Con n -> denode (eval (defn n)) Pre a t -> C(a,t)l Cho ts -> concat \n(map denode ts) Par t u -> [(a, comb (Par t u)) I (a,t ) <-denode tl ++ [(b, comb (Par t u )) 1 (b,u \n) <-denode ul ++ [(Tau, comb (Par t u )) 1 (a,t ) <-denode t, (b,u ) <-denode u, synch a b1 Res t a -> \nC(b, comb (Res t a)) I (b,t ) <-denode t, strip a /= strip bl Rel t f -> C(f a, comb (Rel t f)) 1 (a,t \n) <-denode t])) The auxiliary function eval will be defined shortly, while denode is the destructor function \nfor trees: denode :: Tree -> [(Act,Tree)I denode (In (Node xs)) = xs We refer to comb as a combining \nfunction, because it takes a value built by applying a CCS operator to trees rather than to processes, \nand combines the trees into a single tree by interpreting the operator in the appropriate manner for \ntrees. For example, the third case for parallel compo-sition Par t u states that if the tree t has an \na-labelled branch to a subtree t , the tree u has a b-labelled branch to a subtree u , and the actions \na and b can synchronise, then the resulting combined tree has a Tau-labelled branch to the recursively \ncomputed subtree comb (Par t u ).  7.2 Polytypic fold 8 Reasoning about CCS The algebra In is special \namong all algebras for a functor f, being in fact the initial algebra. Technically, this means that for \nany other algebra g : : f a -> a, there is a unique strict function fold g : : Fix f -> a such that the \nfollow-ing diagram commutes [13, 141: map (fold g) f (Fix f) *f a Fixf-----------+a fold g Using this \ndiagram and that fact that In is the inverse to out, the fold function itself can be defined as follows: \nfold g = g . map (fold g) . out That is, the function fold g first applies the function out to break \ndown an argument of the recursive type Fix f into a structured value of type f (Fix f), then applies \nthe func-tion map (fold g) to recursively process each of the Fix f components to give a value of type \nf a, and finally applies the algebra g combine all the a components into a single result value of type \na. In this manner, fold is a general purpose function for processing values of a recursive type using \na simple pattern of recursion. While previously we defined fold functions that were specific to particular \nrecursive datatypes (for example, ex-pressions), the above version off old is polytypic. Moreover, the \ndefinition for fold is precisely dual to that for unfold. Hence, taking an abstract approach to recursion \noperators has revealed an explicit duality between fold and unfold that might otherwise have been missed. \nReturning to processes, because the datatype Proc is ex- pressed as the least fixpoint of the functor \nP, and the com-bining function is expressed as an algebra comb for P, the denotational semantics of processes \nas trees can now be de- fined using the polytypic version of fold: eval : : Proc -> Tree eval = fold \ncomb In summary, we have now expressed the denotational se-mantics of processes as trees as the unique \nstrict function fold comb that makes the upper square in the diagram be-low commute, and dually, the \noperational semantics of pro-cesses as trees as the unique function unfold trans that makes the lower \nsquare commute. map (fold comb) P Proc +P Tree In comb fold comb --------+ hoc 1 1 _ _ -_ --_ -+ Tree \nunfold trams I I trans out I + + T Proc *T Tree map (unfold trans') We have now defined both operational \nand denotational se-mantics for CCS. It is natural to ask how the two semantics are related. In this \nsection we show that they are equal by exploiting the universality of the recursion operator fold. 8.1 \nUniversality of fold For arbitrary recursive datatypes expressed as the least fix- points of functors, \nthe universal property of fold is captured by the following equivalence (for strict h): [13, 141: map \nh = h . In ++ h = fold g g * Because fold is a polytypic operator, so this universal prop-erty is a \npolytypic proof principle [5]. Returning to our se-mantics for processes, it is easy to verify that unfold \ntrans is strict, using the definitions of the functions concerned, together with the strictness of tags \ndefined using newtype. Hence, applying the universal property gives: unfold trans' = fold comb 0 comb \n. map (unfold trans ) = unfold trans . In The final equation above can now be verified by a routine \ninduction on the size of an argument p : : P Proc, where the size is defined as the number of In tags \nin p. Hence our operational and denotational semantics for CCS are equal for all processes of finite \nsize. Further work is still required to extend the proof to processes of infinite size. The two semantics \ncan also be proved equal using the dual universal property of unfold rather than that of fold, but the \nproof works out simpler using the later. 9 Summary and future work In this paper, we have shown how fold \nand unfold can be used to structure and reason about program semantics within Haskell. The paper is based \nupon categorical work on se-mantics, but explaining the ideas using Haskell makes them simpler, accessible \nto a wider audience, and executable. In-teresting topics for future work include: l The application of \nthe techniques to further examples, including languages in different paradigms; l The use of monadic \nfold operators to structure the de-notational semantics of programming languages with imperative effects \nsuch as mutable state; s Exploring recursion operators and algebraic properties that correspond to non-structural \npatterns of induc-tion, such as induction on the size of values; s Further applications of fold and unfold. \n Acknowledgements This work is supported by Engineering and Physical Sciences Research Council (EPSRC) \nresearch grant GR/L74491 Struc-tured Recursive Programming. Thanks to colleagues in Birm-ingham, Cambridge, \nGlasgow, Nottingham, Oxford, and York for many useful comments and suggestions. References Gordon Plotkin. \nA structured approach to operational [I71 Richard Bird. Constructive functional programming. In PI Proc. \nMarktoberdorf International Summer School on Constructive Methods in Computer Science. Springer- Verlag, \n1989. PI Richard Bird and Oege de Moor. Algebra of Program- ming. Prentice Hall, 1997. Andy Gill, John \nLaunchbury, and Simon Peyton Jones. A short-cut to deforestation. In Proc. ACM Confer-ence on Functional \nProgramming and Computer Archi-tecture, 1993. [31 Graham Hutton. Fold. In preparation, 1998. [41 Johan \nJeuring and Patrik Jansson. Polytypic pro- [51 gramming. In John Launchbury, Erik Meijer, and Tim Sheard, \neditors, Advanced Functional Program-ming, LNCS 1129, pages 68-114. Springer-Verlag, 1996. [61 Geraint \nJones. Designing circuits by calculation. Tech-nical Report PRG-TR-10-90, Oxford University, April 1990. \nGeraint Jones and Mary Sheeran. Circuit design in Ruby. In Staunstrup, editor, Formal Methods for VLSI \nDesign, Amsterdam, 1990. Elsevier Science Publica-tions. [71 PI John Launchbury and Tim Sheard. Warm \nfusion: Deriving build-catas from recursive definitions. In Proc. ACM Conference on Functional Programming \nand Computer Architecture, 1995. PI Saunders MacLane. Categories for the Working Mathe-matician. Number \n5 in Graduate Texts in Mathematics. Springer-Verlag, 1971. Grant Malcolm. Algebraic data types and program \ntransformation. Science of Computer Programming, 14(2-3):255-280, September 1990. WI Lambert Meertens. \nAlgorithmics: Towards program-ming as a mathematical activity. In Proc. CWI Sympo-sium, Centre for Mathematics \nand Computer Science, Amsterdam, November 1983. [I21 Erik Meijer. Calculating Compilers. PhD thesis, \nNi-jmegen University, February 1992. P31 Erik Meijer, Maarten Fokkinga, and Ross Paterson. Functional \nprogramming with bananas, lenses, en-velopes and barbed wire. In John Hughes, editor, Proc. Conference \non Functional Programming and Computer Architecture, number 523 in LNCS. Springer-Verlag, 1991. Erik \nMeijer and Graham Hutton. Bananas in space: Extending fold and unfold to exponential types. In Proc. \n7th International Conference on Functional Pro-gramming and Computer Architecture. ACM Press, San Diego, \nCalifornia, June 1995. D41 PI Robin Milner. Communication and Concurrency. Pren-tice Hall, 1989. 1161 \nJohn Peterson et al. The Haskell language report, version 1.4. Available on the World-Wide-Web from http: \n//awn .haskell .org, April 1997. semantics. Report DAIMI-FN-19, Computer Science Department, Aarhus University, \nDenmark, 1981. [I81 Jan Rutten and Daniele Turi. Initial algebra and final coalgebra semantics for concurrency. \nIII J.W. de Bakker et al., editor, Proc. A Decade of Concur-rency -Reflections and Perspectives, LNCS. \nSpringer-Verlag, 1994. [191 David A. Schmidt. Denotational Semantics: A Method- ology for Language Development. \nAllyn and Bacon, Inc., 1986. Tim Sheard and Leonidas Fegaras. A fold for all sea- PO1 sons. In Proc. \nACM Conference on Functional Pro-gramming and Computer Architecture. Springer, 1993. Daniele Turi and \nGordon Plotkin. Towards a math- Pll ematical operational semantics. In Proc. IEEE Con-ference on Logic \nin Computer Science, pages 280-291. Computer Society Press, 1997.     \n\t\t\t", "proc_id": "289423", "abstract": "In this paper we explain how recursion operators can be used to structure and reason about <i>program semantics</i> within a functional language. In particular, we show how the recursion operator <i>fold</i> can be used to structure denotational semantics, how the dual recursion operator <i>unfold</i> can be used to structure operational semantics, and how algebraic properties of these operators can be used to reason about program semantics. The techniques are explained with the aid of two main examples, the first concerning arithmetic expressions, and the second concerning Milner's concurrent language CCS. The aim of the paper is to give functional programmers new insights into recursion operators, program semantics, and the relationships between them.", "authors": [{"name": "Graham Hutton", "author_profile_id": "81100521968", "affiliation": "Languages and Programming Group Department of Computer Science, University of Nottingham, UK", "person_id": "PP40035996", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/289423.289457", "year": "1998", "article_id": "289457", "conference": "ICFP", "title": "Fold and unfold for program semantics", "url": "http://dl.acm.org/citation.cfm?id=289457"}