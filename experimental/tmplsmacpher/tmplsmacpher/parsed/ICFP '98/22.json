{"article_publication_date": "09-29-1998", "fulltext": "\n Recycling Continuations Jonathan Sobel Daniel P. Friedman Computer Science Department Indiana University \nBloomington, Indiana 47405 {j sobel , df ried}@cs . indiana. edu Abstract If the continuations in functional \ndata-structure-generating programs are made explicit and represented as records, they can be recycled. \nOnce they have served their purpose as temporary, intermediate structures for managing program control, \nthe space they occupy can be reused for the struc-tures that the programs produce as their output. To \nef-fect this immediate memory reclamation, we use a sequence of correctness-preserving program transformations, \ndemon-strated through a series of simple examples. We then apply the transformations to general anamorphism \noperators, with the important consequence that all finite-output anamor-phisms can now be run without \nany stack- or continuation-space overhead. Introduction The runtime architecture for a language implementation \nkeeps track of the continuations of procedure calls using either a stack of call frames or a linked chain \nof heap-allocated continuation structures. One advantage that may be claimed for the stack approach is \nthat deallocation of frames is inexpensive, and it happens as soon as a proce- dure returns. Heap-allocated \ncontinuations, on the other hand, typically take up memory until the space is reclaimed by the runtime \nmemory manager (e.g., a garbage collector). We show how, for a certain class of procedures, using the \ncontinuation-based approach can lead not only to immedi-ate reclamation of the space used by the continuations, \nbut also to the elimination of most of the memory overhead in-curred by both of the aforementioned architectures. \nLink-inversion algorithms (more generally, Deutsch-Schorr-Waite algorithms) are a standard means of travers- \ning data structures with polynomial (sum-of-products) types using little or no memory overhead. The problem \nwith such algorithms is that they mutate the structures dur-ing the traversal. Thus, link inversion is \ngenerally un-safe, except in critical sections, during garbage collections (the context of the original \npublished description [14]), or over uniquely referenced objects. We demonstrate a se- ries of correctness-preserving \nprogram transformations that takes a functional structure-generating procedure (includ- ing anamorphisms \n[8, 91) and produces one that uses a safe variation of link inversion to reduce or eliminate the memory \noverhead of recursive procedure calls. Some of our algorithms for traversing and constructing data structures \nare variations on techniques first published twenty or thirty years ago; some special cases might have \nbeen in use since the 1950s. They are now standard fare in undergraduate data structures texts [6, 151. \nThe con-tribution here is to derive these algorithms through a se-quence of correctness-preserving program \ntransformations. This exercise may seem at first to be more entertaining than useful, but there are two \nreal practical benefits to the transformation-based approach. First, a programmer using this technique \ncan begin with a formal mathematical algo-rithm that parallels the recursive structure of the data and \nthen tune the program for efficiency. Second, the transfor-mations could be automated, allowing them \nto be included in a compiler or source-to-source translator/optimizer. We begin with a simple, concrete \nprocedure definition and follow it through a sequence of transformations. The first example is a procedure \nthat copies a list. After working through this example, we consider a procedure that copies binary trees. \nAt the end, we generalize from the examples. 2 History How to implement recursion (and, more generally, \nproce-dure calls) efficiently has been a subject of great scrutiny in programming language design and \ncompiler construction. Early on, the stack-based runtime architecture was devel-oped, with call frames \nbeing pushed onto the stack for each procedure call. Stack frames take up memory, though, and programmers \nhave always been on the lookout for ways to reduce memory consumption. One technique for traversing trees \nand other inductively defined structures, while using very little memory overhead, is to invert the pointers \non the way down into the structure, leaving a trail to follow on the way back out. As each recursive \ncall returns, instead of popping a stack frame, it re-inverts a pointer back to its orig- inal orientation. \nDuring such a traversal, however, if another procedure needed to start from the top of the structure, \nit would not be able to perform a correct traversal until all the pointers were returned to their original \nstates. Not sur-prisingly, then, the first publication of such a technique [14] specified a context in \nwhich it is not only essential to use little or no memory, but also safe to assume (at the time) that \nno other traversal could happen concurrently: garbage collection. Schorr and Waite s algorithm was developed \nat the same time that Deutsch developed a similar one inde- pendently; thus, link-inversion algorithms \nare now known collectively as Deutsch-Schorr-Waite algorithms1 The idea that programs could be transformed \nso that no return information needed to be saved for procedure calls is first recorded in a talk by Van \nWijngaarden and a transcript of the ensuing discussion [18]. Using explicit continuations to represent \ncomputational contexts moved center-stage be- ginning with Reynolds s seminal paper [13]. As implemen- \ntors began to explore the possibility of replacing the stack- based architecture with a continuation-based \none, a variety of tradeoffs were exposed. The trade-off of immediate interest is between the sim-plicity \nof a procedure call in continuation-based systems and the simplicity of space-reclamation upon procedure \nreturn in stack-based systems. When a program has been written in (or transformed into) continuation-passing \nstyle (CPS), ev-ery call is a tail call and may be implemented as a simple jump (or goto), but the information \nabout what still needs to be done after the call returns is encapsulated into a structure that is usually \nheap-allocated. Barring the user-level reification of continuations, each continuation could be disposed \nof after it has been used (thus, the stack archi-tecture), but if continuations are treated just like \nany other heap structures, they continue to occupy space until the run-time memory manager recognizes \nthat they are unreachable. In a directly recursive, stack-based style, each call requires some extra \nwork in the creation and setup of a new stack frame, but returning from a call is usually a simple matter \nof adjusting the value of a single register. The memory used by the frame is instantly available for \nreuse. In an attempt to eliminate recursion overhead altogether, Minamide demonstrates a new technique \nfor representing unfinished data structures [lo]. Using this technique, pro-grams that would otherwise \nhave required the growth of a stack or the construction of a chain of continuations can instead run as \nsimple loops. The style in which the pro-grams must be written in order to benefit from Minamide s representation \nis closely related to CPS, as the author men-tions. This relationship led us to question more deeply \nthe connection between the continuations that appear in cer-tain programs and the data structures that \nthe programs construct. What follows is the result of that inquiry. 3 Lists The code in the following \nexamples is in Scheme, but in order to maintain continuity between the list and tree examples, the use \nof built-in Scheme lists (e.g., car, cdr, cons) will be avoided. Instead, we explicitly define a list \ndatatype: (list clatatype)E (datatype List (Empty-List) (Pair head tail)) The datatype construct is not \na standard Scheme form. It is intended to feel similar to the homonymous form in ML. Scborr and Waite \nused link inversion during the tnark phase of a mark-sweep collector, so they had to chase arbitrary \npointers tlwougll a graph structure. Our focus is on functional, recursive structures. not arbitrary \ngraphs. *A definition for (and explication of) the datatype macro appears at http://www.cs.indiana.edu/-jsobel/. \nIn short, defining a datatype provides us with constructors for the variants, as well as a case form \nfor de-structuring objects of the datatype. Thus, the procedure for copying lists is: (list copy)= (def \nins list-copy (lambda (1) (List-case 1 ((Empty-List) (Bnpty-List)) ((Pair h t) (Pair h (list-copy +.))I))) \nUsing list-copy as an example may seem a bit odd, since it is functionally equivalent to the identity \nfunction. The intent of these examples is to convey the nature of the pro- gram transformations, though, \nand list-copy is among the simplest of procedures that construct lists. Procedures that actually perform \ninteresting work, such as subst, remove, or iota (which produces an increasing list of numbers), are \nstraightforward generalizations of list-copy, as long as their non-varying parameters are abstracted \nout of the recursion. 3.1 (Lists) Continuation-Passing Style The first transformation to be applied to \nlist-copy is the standard call-by-value conversion to continuation-passing style (CPS). All the administrative \nCPS reductions have been left out, and constructors have been treated as primi- tive, atomic operations. \n(list copy in CPS)E (define list-copy-cps (lambda (1 k) (List-case 1 ((Empty-List) (k (Empty-List))) \n((Pair h t) (list-copy-cps t (lambda (v) (k (Pair h v)))>)>)) The original one-argument version of list-copy \ncan be re-covered with a simple wrapper for list-copy-cps. (list copy in CPS)+r (define list-copy (lambda \n(1) (list-copy-cps 1 (lambda (v) VI))) The two versions of list-copy-direct and continuation- passing \nstyle-compute the same result, but the work done along the way is slightly different. In the direct-style \nversion, the language implementation maintains the control context. In the CPS version, every call is \na tail call (equivalent to a jump or goto); as far as the compiler can tell, there is nothing left to \ndo after copying the tail. Instead, the con-tinuation is maintained explicitly in the program. As list-copy \nwalks down the list, it constructs a se- quence of nested closures. With the display-closure repre-sentation \n[5, 2, 3] in mind (in which a closure is a sequence of memory locations containing a code pointer and \nthe values of the procedure s free variables), it becomes evident that the continuation is really a linear, \nlinked structure. Upon reaching the end of the list, the procedure starts a chain re-action by invoking \nthe outermost (most recently linked) of these closures. The first one constructs a new pair and then \ninvokes the next one, each constructing another pair until the identity (empty, initial) continuation \nis reached, and the copied list is returned.  3.2 (Lists) Representation Independence The explicit treatment \nof the continuations in the CPS version of list-copy is, in fact, a bit too explicit: the continuations-as-closures \nrepresentation was hard-wired into the code. The next step is to make list-copy treat the continuation \nmore abstractly [13], so that the representation of the continuation can be modified at will. The procedures \nwill retain their prior form: (list copy in representation-independent CPS)ZE (define list-copy-cps (lambda \n(1 k) (List-case 1 ((Empty-List) (invoker, k (Empty-List))) ((Pair h t) (list-copy-cps t (constmct pairing \ncontinuation))) )) 1 (define list-copy (lambda (1) (list-copy-cps 1 (construct initial continuation)))) \n Here the continuations will be constructed and invoked through a more abstract interface. For invocation, \nthe invoker. operator suffices to eliminate the assumption that continuations are procedures. If, in \nspite of the more ab-stract interface, continuations really are still represented as procedures, invokeL \ncan be defined as (invoking list continuations (procedure representation)) Z (define i.nvokeL (lambda \n(k v) (k ~1)) Having committed to a procedural representation, the con- tinuation constructors had better \nproduce the same closures as in the original CPS version (with the bodies transformed where necessary). \nOf course, this implies that the values of the free variables must be passed as arguments to the constructors. \n(list continuation constructors (procedure representation))- (define Initial-k (lambda 0 (lambda (v) \nv))) (define Pairing-k (lambda (h k) (lambda (v) CinvokeL k (Pair h v)))))  Thus, the missing part of \nlist-copy-cps is (wnstmct pair&#38;g wntinuation)G (Pairing-k h k) and the missing part of list-copy \nis just (construct initial wntinuation)E (Initial-k) 3.3 (Lists) Representing Continuations as Records \nFreed from any dependence on a particular representation, we can now switch from procedural to record-based \nrepre-sentation of continuations without changing the definition of list-copy. What changes is the construction \nand in-vocation of the continuations: instead of explicitly defined constructors, a datatype is used. \n(list continuation type)= (datatype List-k (Initial-k) (Pairing-k head k)) Thus, the constructors defined \nin the preceding section are replaced by trivial record constructors. The work is shifted to the invokeL \nprocedure. In order to preserve the seman-tics of the invocation, we can simply copy the code from the \nbodies of the old continuation procedures into the different cases of the invoker, procedure. (invoking \nlist wntinuations (record representation))= (define invokeL (lambda (k v) (List-k-case k ((Initial-k) \nv) ((Pairing-k h k) CinvokeL k (Pair h v)))))) The linked structure of continuations described in Sec-tion \n3.1 is now completely explicit. Each continuation has an explicit link (in the form of a record field) \nto the next continuation in the chain. Consider how a call to list-copy works now: First, list-copy-cps \nwalks down the list, creating a Pairing-k record for each Pair in the list. At the end of the list, the \nbase case of list-copy-cps calls invoker, with the chain of Pairing-ks and an Empty-List as arguments. \nThen, invokeL constructs a new Pair for each Pairing-k, using the saved head value for the head and the \nvalue of the v parameter for the tail. When invokeL reaches the initial continuation, the value of v \n(which is by now a complete copy of the original list) is returned. Thus, three data structures are \ninvolved in a call to list-copy: the input list, the chain of continuations, and the output list. In \naddition, the property that every call is a tail call has been preserved from the original CPS trans-formation. \n 3.4 (Lists) Link Inversion There are two crucial observations to be made about the last definition of \ninvokeL. First, we have accidentally used the name k as part of the pattern on the Pairing-k line, shadowing \nthe k parameter of invokeL. Hiding the k parameter did no harm, though; it was only used to choose a \ncase and bind the pattern variables. The continuation passed to invokeI. is neuer used on the right-hand \nside of a case clause. Since there are no free occurrences of k in the clauses, its value is no longer \nreachable (i.e., it is garbage) as soon as the pattern is matched [12]. In other terminology, invoker, \nobeys a linear type discipline, which has important implications about how the data structure bound to \nk can be treated [4]. The second observation is that the Pair constructed in the Pairing-k clause is \nsimilar in form and contents to the Pairing-k itself. Both have two fields, and both have the same value \nin their first fields. The only difference is that the k in the Pairing-k is replaced with v in the Pair. \nTaking advantage of the linearity of k, why not take the replacement of k with v more literally? The \ninput con-tinuation will not be needed again, so why not use it as the result? Suppose that, in addition \nto regular constructors and the case form, the datatype form is extended to de-fine recycling constructors \nfor each variant. For instance, when the List datatype is defined, we get not only the Pair constructor, \nbut also a recycle-as-Pair constructor, which is a new syntactic form that looks like this: (form of \nrecycle-as-Pair)= (recycle-as-Pair record-expr [fieldname expcprl . . .) Evaluating this form is roughly \nequivalent to evaluating the following pseudo-expression: (pseudo-expansion of recycle-as-Pair)% (let \n((record record-expr)) (coerce-to-Pair! record) (set-fieldname ! record expcpr) . record) where coerce-to-Pair! \nis an imagined low-level operation that requires its argument to be the same size record as a Pair and \nre-tags the record as a Pair. The also-imagined set-head! and/or set-tail! update the appropriate fields \nof the record with the values of their second arguments. If no update is specified for a field, its value \nremains unchanged after recycling. Thus, we could rewrite invoker. like this: (recycling list continuations \n(record representation))% (define invokeL (lambda (c v) (List-k-case c ((Initial-k) v) ((Pairing-k h \nk) (invokeL k (recycle-as-Pair c [tail ~3)))))) This version of invokeL does exactly what has been sug-gested. \nIt reuses the memory formerly used by the Pairing-k3 to construct a Pair whose contents are the same \nas those of the Pairing-k, except that the tail field con-tains the value of v, instead of whatever used \nto be there. In other words, the Pairing-k is re-tagged to be a Pair, and the tail field is updated to \ncontain a new value. No new memory is allocated. A call to list-copy still produces the same answer as \nbefore, but now the behavior along the way has changed quite a bit: First, list-copy-cps walks down the \nlist, creating a Pairing-k record for each Pair in the list. At the end of the list, the base case calls \ninvoker, with the chain of Pairing-ks and the Empty-List as argu- ments. Then, invokeL inverts the backward-pointing \nlinks (k) that connect each continuation to the next, making them point forward to the tail (v) instead. \nWhen invoker. reaches the initial continuation, the v (which is by now a complete copy of the original \nlist) is returned. Only two data structures are involved in a call to list-copy now: the input list and \nthe continuations/output list. No extra memory is used beyond that which is necessary for creating the \nresult.* This diagram represents the situation as invokeL is called for the third time when copying a \nlist containing the numbers 1, 2, 3, and 4. SIronically. exploiting linearity requires its violation. \n4Actually. a constant anlount of tnen~ory is used for allocating the Initial-k, so it is lore accurate \nto cl&#38;l that the nwnory use of list-copy is constant. not counting the rnenlory used for the output. \nk V The narrow box on the far right is an Empty-List. The square on the left is the initial continuation. \nThe records containing 1 and 2 are Pairing-ks. The records containing 3 and 4 are Pairs. After this last \ntransformation, list-copy works almost exactly like the standard link-inversion algorithm for stack- \nless list traversal. The difference is that, whereas most pre-sentations of link inversion treat it as \na means of traversing an existing list, list-copy uses link inversion to traverse its freshly allocated \nresult list. Since no external references to the result can exist yet, the usual danger created by link \ninversion goes away: two traversals cannot happen at once. The first version of list-copy, at the beginning \nof Sec- tion 3, had the desirable property that its recursive structure closely matched the inductive \ntype structure of the lists be-ing copied. That correspondence is now less clearly present in the code, \nwhich is why it is preferable to begin by writing the direct-style version and to convert to the more \nspace-efficient version through correctness-preserving transforma-tions. After four transformations, \nthe program still pro-duces the same answer, but it uses no extra space, whereas the original program \nrequires extra space proportional to the length of the list in order to compute its result. To complete \nthe path toward a more standard link-inversion algorithm for lists, one can observe that maintaining \nthe distinction be-tween List and List-k is not necessary. Thus, the List-k type could be eliminated \nin favor of Lists everywhere, in- cluding the representation of continuations, obviating the record coercion. \n  3.5 Other List Examples The transformation technique demonstrated in the preced-ing sections never \ndepended on the form of the input to the procedure being transformed. A certain control flow was necessary \nin order to produce the desired output, and a cer- tain form for the continuations was induced by the \ncontrol flow. To clarify that the form of the output is the determining factor, consider that from having \nseen only list-copy, it would be easy to conclude that this sequence of transforma- tions only works \nfor programs that take a list of length n and return a new list, also of length n. In fact, exactly the \nright number of continuations are allocated for procedures in which the lengths are two different numbers \nn and m. For instance, the procedure remove returns a list whose length is less than or equal to the \nlength of its input, eliminating the elements that match its first argument. (Temove)E (def ins remove \n(lambda (x 1) (letrec ((rem (lambda (I) (List-case 1 ((Empty-List) (Empty-List)) ((Pair h t) (if (equal? \nh x) (rem t) (Pair h (rem t)))))))) (rem 1)))) After completely transforming this program, it becomes: \n (remuve with recycling)r (define remove (lambda (x 1) (letrec ((rem-cps (lambda (1 k) (List-case 1 (@mpty-List) \nCinvokeL k (Empty-List))) ((Pair h t) (if (equal? h x) (rem-cps t k) (rem-cps t (Pairing-k h k)))))))) \n(rem-cps 1 (Initial-k))))) where invokeL is defined exactly as in the recycling version of list-copy. \n(In fact, invokeL will be the same for almost any program that constructs lists. A counterexample is \nthe procedure double, which creates a list with two elements for every one in the input list, so that \nthe output list is twice as long as the input list.) Here we see that a Pairing-k is created just in \ncase a Pair is needed in the output list. No space is wasted by the continuations. The input need not \neven be a list, though. A program need only construct a list as its output to be suitable for our transformation. \nFor example, the iota procedure, which takes a natural number n and produces a list that contains the \nnumbers from 0 up to (but not including) n, is also a suitable candidate for eliminating stack overhead. \n(iota)s (define iota (lambda (n) (letrec ((up (lambda (i) (wand CC= i n) (Empty-List)) (else (Pair i \n(up (t i I)))))))) (up 0)))) Since iota returns a list, it needs. to construct a pair at each recursive \nstep. After the transformations, we get: (iota with recycling)= (define iota (lambda (n) (letrec ((up-cps \n(lambda (i k) (cond cc= i n) CinvokeL k (mt.y-List))) (else (up-cps (+ i I) (Pairing-k i k))))))) (up-cps \n0 (Initial-k))))) The construction of the Pair has been replaced by the con-struction of the pairing \ncontinuation, which will be con-verted to a Pair in invokeL, after the base case is reached. Trees Lists \nare sometimes beguilingly simple. Their definition has a single recursive reference, which makes it straightforward \nto use them as their own continuations, or even to tra-verse them iteratively. This is not possible for \narbitrary, inductively-defined structures. In the following sections, we attempt to apply the same sequence \nof transformations to a tree-copying procedure that we applied to the list-copying procedure. This attempt \nreveals subtleties (clarified in Sec- tion 4.4) that did not arise in the preceding section. We begin, \nas before, with a datatype definition. For simplicity and without loss of generality, there will be \nno distinct leaf type; a leaf is a node with two empty subtrees. (tree datatype)= (datatype Tree (Empty-Tree) \n(Node datum left right.)) The direct-style copying procedure induced by this type is: (tree copy)= (define \ntree-copy (lambda (t) (Tree-case t ((Empty-Tree) (R+y-Tree)) ((Node d 1 r) (Node d (tree-copy 1) (tree-copy \nr)))))) It is the presence of two recursive calls to tree-copy-and the accompanying two input and output \nsubtrees-that will act as the crucible for our sequence of transformations, help-ing us to refine and \ngeneralize them. 4.1 (Trees) Continuation-Passing Style Even the standard CPS conversion raises issues \nthat were hidden before. CPS is inherently sequential, but the two recursive calls to tree-copy have \nno specified ordering in the direct-style programs in Scheme. We arbitrarily choose a left-to-right evaluation \nordering. (tree wpy in CPS)F (define tree-copy-cps (lambda (t k) (Tree-case t ((Empty-Tree) (k (&#38;apty-Tree))) \n((Node d 1 r) (tree-copy-cps 1 (lambda (~1) (tree-copy-cps r (lambda (vr) (k (Node d vl vr)))))))))) \n(define tree-copy (lambda (t) (tree-copy-cps t (lambda (v) v))))  As in the list examples, converting \nthe program to CPS makes explicit the still-to-be-done computation at any point. The procedure walks \ndown the leftmost path in the tree, cre-ating a sequence of nested closures formed from the (lambda (vl) \n. . . ) continuation. Upon reaching the end of the left branch, the last of these closures is invoked, \nand the proce- dure begins to walk down a right branch. When both left and right branches are completed, \nthe next continuation in the chain is invoked on a newly created node. Even though the data structure \nbeing traversed is now a tree, the contin- uation always remains a linear structure. This is precisely \nwhat makes a stack-based runtime architecture feasible. 4.2 (Trees) Representation Independence The \nlist-copy procedure creates two different kinds of con- tinuations; the tree version creates three. The \nfirst is the omnipresent empty continuation. The second is created when descending into left branches, \nand the third-for right branches-is created only when the second is invoked. Here is a version of the \nCPS tree copy expressed more abstractly: (tree copy in TepTeSentatiOn-independent CPA )- (define tree-copy-cps \n(lambda (t. k) (Tree-case t (@@y-Tree) (invokq k (Empty-Tree))) ((Node d 1 r) (tree-copy-cps 1 (construct \nleft continuation) 1) 1) 1 (define tree-copy (lambda ct.) (tree-copy-cps t (Initial-k)))) Given the \nprocedural definition of inVOkeT, (invoking tree WntinUatiOnS (pTOCedUTe TepTeSentatiOn))E (define i.nVOkeT \n(lambda (k v) (k v))) the constructor for the initial continuation is the same as the one for lists: \n(tree continuation constTuctoT3 (procedure representation))-(define Initial-k (lambda 0 (lambda (v) v))) \nConstructors for nested continuations are easiest to write starting with the innermost and working outward. \n(tree continuation constTuctoT3 (pTOcedUTe representation)) +Z (define Right-k (lambda (d vl k) (lambda \n(v) (illVOkeT k (Node d vl v))))) This clarifies that the constructor for left continuations must be: \n(tree continuation constructors (procedure representation))+% (define Left-k (lambda (d r k) (lambda \n(v) (tree-copy-cps r (Right-k d v k))))) Thus, the missing part of tree-copy can now be completed only \none way: (WnstrzLct left continuation)S (Left-k r k) d  4.3 (Trees) Representing Continuations as Records \nSection 3.3 ended with the observation that the types List-k and List were isomorphic and that the distinction \nbetween them could be dropped. This was clearly a special case, because Tree and Tree-k are dissimilar: \n(tree continuation type)= (datatype Tree-k (Initial-k) (Left-k datum right. k) (Right-k datum left k)) \n A fact that may not be readily apparent is that the right field of Left-k (the r parameter above) and \nthe left field of Right-k (the vl parameter above) point into two different structures, the old tree \nand the new tree, respectively. The right field of Left-k always refers to the old tree, the one being \ncopied. The left field of Right-k refers to the newly constructed left subtree, which will become a part \nof the output tree. As before, the burden of behavior is lifted from the con-tinuations and falls onto \nthe invokeT procedure. Copying the bodies of the continuation procedures from the preced- ing section, \nwe get: (invoking tree continuations (record representation))= (define invokq (lambda (k v) (Tree-k-case \nk ((Initial-k) v) ((Left-k d r k) (tree-copy-cps r (Right-k d v k))) ((Right-k d 1 k) (iW,OksT k (Node \nd 1 v)))))) The linked structure of the continuations is completely explicit again. If we were to observe \nthe dynamic behavior of the chain of continuations, though, we would see something that we did not observe \nin the list example. The chain of list continuations grew (monotonically) until it reached its maximum \nlength. Then it shrank (monotonically) until the initial continuation was invoked. The chain of tree \ncontin-uations alternately grows and shrinks, matching the growth pattern that a stack would exhibit \nduring a traversal of the tree being copied. The total number of continuation records created (which \nis greater than the maximum length of the chain) is the same as the number of nodes in the tree, again \ninviting us to reuse the space they occupy.  4.4 (Trees) Link Inversion Comparing the tree invokeT with \nthe list invokeL, we ob- serve that the first case clause is identical to the list version. The last \ncase clause is so similar to what we saw in the list example that it is evident what to do with it: simply \nreplace k with v. The middle clause is new, but it is still fairly ob-vious that the Right-k being constructed \nis nearly identical to the Left-k it replaces. The only difference is that the r in the Left-k is replaced \nwith v in the Right-k. Thus, it is again possible to recycle the continuations and avoid allocating new \nrecords. (recycling tree continuations (TeWTd TepTeSentatiOn))E (define invokq (lambda (c v) (Tree-k-case \nc ((Initial-k) v) ((Left-k d r k) (tree-copy-cps r (recycle-as-Right-k c Cleft VI))) ((Right-k d 1 k) \n(iIWOkeT k (recycle-as-Node c [right VI)))))) Let us walk through the sequence of events that begins \nwith a call to tree-copy on some tree. As tree-copy-cps walks down the leftmost branch of the tree, it \ncreates for each node a Left-k record. That record contains the node s datum and a pointer to its right \nsubtree, which still needs processing. At the first empty left subtree, tree-copy-cps calls invokeT with \nthe chain of Left-ks and an Empty-Tree as arguments. Next, invokeT turns the Left-k into a Right-k, saving \nthe new left subtree (an Empty-Tree, for the first in the chain of Left-ks), and redirects tree-copy-cps \nto the old node s right subtree. The whole process starts over with the right sub-tree, until a node \nwith no children is reached. Then, tree-copy-cps calls inVOkeT, and the first continua-tion in the chain \nis now a Right-k. Later, inVOkeT turns the Right-k into a Node, inverting the upward-pointing link (to \nthe next continuation in the chain) so that it points down to the right subtree instead, and moves on \nto the next continuation. When inVOkeT reaches the initial continuation, the whole new tree is returned. \nSuppose tree-copy is called on a balanced 3-node tree. This diagram represents the situation as invokeT \nis called for the second time: V The tree on the left is the tree being copied. The current continuation \nis a Right-k. In the next step, the continuation is recycled as a Node, with the right field updated \nto point to the current v (an Empty-Tree). Then, iIIVOkeT is called again, with new values for k and \nv: Now the current continuation is a Left-k, and the value of v is the Node that was just completed. \nAs with the list copying program, only two data struc-tures are involved now: the input tree and the \nchain of con- tinuations, which gradually becomes the output tree. It is not possible, however, to claim \nthat tree-copy has no mem- ory overhead. The continuations are not isomorphic to trees because it is \nnecessary to distinguish between left and right continuations, whereas trees have only one kind of node. \nIn link inversion terminology, a tag bit is necessary to deter-mine which pointers have already reached \ntheir final state, and which are being used to save data that still need to be processed. Since there \nare only two alternatives, the dis-tinction can be encoded in a single bit. The length of the chain of \ncontinuations is equal to the depth of the recursion at any moment, so the maximum amount of memory needed \nis a number of bits equal to the maximum depth (i.e., the height) of the tree. At least one undergraduate \ndata struc-tures textbook comes very close to expressing the connection between the tag bit and continuations: \nThis method . . . does not completely eliminate the memory used by the stack of the recursive al- gorithm, \nthough it does reduce it to a single bit per cell. In essence, this bit encodes the same in- formation \nas is needed in the recursive version . . to indicate, when a recursive call finishes, which of the two \nrecursive calls in the body of the program caused that invocation, and hence where in the body of the \nprogram execution should continue [6, page 1141. In comparison with the prior versions of tree-copy, \na great deal of memory overhead has been eliminated, but it is not possible to remove all the overhead. \nIn binary trees, the remaining overhead is merely one bit per level of recursion, but the situation grows \nworse for more general recursive structures. For a structure with n recursive fields, there will be n \ndifferent kinds of continuations to distinguish, leading to an overhead of Ig n bits per level of recursion. \nIn practice, record length is constrained by memory word length, so the overhead is bounded to one word \nper level of recursion.5 The preceding discussion notwithstanding, most imple- mentations of user-defined \ntypes and variants actually take an entire word to tag each variant and distinguish among them. Thus, \neven though the left and right continuations theoretically occupy more space than the tree node, they \ntypically take up exactly the same amount of space in prac- tice. Otherwise, it would not really be a \nfair trick to recycle a Right-k as a Node, as was done in the last definition of inVOkeT. 5 General Polynomial \nTypes The transformations in the preceding sections generalize to types other than lists and trees. \nIn fact, most procedures that act as generators of any polynomially typed data (sums of product or record \ntypes [ i ]) can use link inversion, instead of stack space. In particular, our sequence of transforma- \ntions works for every finite-output anamorphism [8], to be defined more precisely in Section 6. The transformations \nin the following three sections are based on those in the sources already cited in the exam-ples, but \nwe have reconstructed and extended them to suit the language of the examples and the needs of the fourth \ntransformation, which is entirely our own. 5.1 Continuation-Passing Style The CPS algorithm we use is \nstandard, with the following exception: constructors and most language-defined primi-tives are treated \nas atomic operations, like variable refer-ence. Procedures that run user code, such as map, are not treated \natomically; they need to be rewritten by the user (e.g., map-cps). Also, we insist on defining the wrapper \nprocedures that explicitly call the CPS-transformed versions with initial continuations. For example, \nit would have been difficult to discuss the complete behavior of list-copy-cps without referring to the \nlist-copy wrapper. Most complete CPS algorithms define translations for call/cc; however, in order for \nit to be possible to recy-cle the continuations, they must be treated linearly (in the 6Another way to \nimplement the algorithm is with an extra pointer in each record. which identifies the next field that \nstill needs to be processed. This also takes one memory word per record. Still another implementation \nuses plain tree nodes plus a stack of tag bits [17]. sense of linear logic). The translation for call/cc \ncreates two references to-and multiple potential uses of-the con-tinuation, and so we disallow it in \nthe programs for which recycling is desired.  5.2 Representation Independence This transformation can \nbe briefly described as follows: 1. Replace all continuation invocations (k E) with (invoke k E). 2. \nBeginning with the innermost continuation construc-tions (lambda-expressions), transfer each continuation \nto a new definition, using an outer lambda-expression to bind free variables. 3. At the site of the \noriginal continuation constructions (the ones that were just moved), insert calls to the newly defined \nconstructors, passing the values of the free variables. 4. Define invoke as (lambda (k v) (k v)).  \n The ordering of the free-variable parameters (i.e., the pa-rameters of the outer lambda of each constructor) \nprovides some room for optimization. The goal is to do as little shift-ing as possible at the recycling \nstage. The most general solutions to this problem are equivalent to register alloca-tion strategies in \ncompilers. A simple heuristic that works well, though, is based on the fact that the free variables usually \nmatch the fields of the records over which the re-cursion is defined. The value of one field is the argument \nto the continuation; the rest of the fields should be listed in the order they appear in the record. \nThus, there will be one field fewer mentioned in the free-variable parameters of the continuation constructors \nthan there are in the variant being handled by the continuation. Every continuation ex-cept the initial \none also needs to refer to exactly one other continuation. If the other continuation is passed as the \nlast argument to the constructor, the total number of arguments will be the same as the number of fields \nin the variant being handled, and the order will simplify its recycling later. 5.3 Representing Continuations \nwith Records In this transformation, we eliminate all the constructor def-initions produced in the preceding \ntransformation and re-place them with a single datatype definition. The fields of each variant should \nbe exactly the same as the free-variable parameters in the corresponding procedural con-structor. The \ninvoke procedure is rewritten to use the case form of the continuation datatype. Each clause contains \nthe body of the constructor procedure from which the vari- ant was derived. If the interpretive (dispatch) \noverhead of invoke is an important concern, the kinds of jump-table op-timization performed by Smalltalk-style \nobject systems can be used to recover most of the speed of direct procedure calls. 5.4 Link Inversion \nAfter completing the three preceding transformations, it should be the case that each clause of invoke \nconstructs a new variant. The clauses that correspond to the in-nermost continuations will construct \nresult types, and the other clauses (with the exception of the initial-continuation clause) will match \nand construct continuation types. As we demonstrated in the list and tree examples, the standard constructors \nfor these types can be replaced with recycling constructors,6 updating only one field at a time. In fact, \nwhen each continuation is first constructed (and allocated), all of its fields will refer to the input \ndata structure. The fields will be updated, one at a time, to refer to continu- ations that have been \ncompletely recycled as output data structures. Finally, when the last field (the k field) is up- dated, \nand the continuation is recycled as an output type, the next continuation in the chain is invoked. One \nof its fields is updated to point down to the newly completed node, and thus, the link is inverted. \nA potential cause for concern at this point is that the record lengths of the continuations might not \nmatch the record lengths of the appropriate output records. If there is a mismatch in the lengths, either \nthe continuation record has fewer slots than the output structure, or it has more. In or-der for the \ncontinuation to have too few slots, it must be fill- ing the output structure with data that does not \ncome from free variables. For instance, if the continuation were (lambda (v> (k (Pair 88 v> > 1, it would \nhave only one free-variable slot (for k). In truth, it is surprisingly hard to find realis-tic examples \nof this problem. (The iota procedure of Sec- tion 3.5 would at first seem likely to fit into this problem \nclass, but it does not.) If a real case does arise, the values (like 88) can be passed to the continuation \nconstructors as if they were free (as in (Pairing-k 88 k)), or an alternative translation can be used. \n(The <list of 88 s program could have been written iteratively from the start.) What about continuations \nwith more free variables than there are slots in the output structure? For example, what if a continuation \nlooks like (lambda (v) (k (Pair (+ x y) v) 1) where x, y, and k are free? Clearly, it would be possi- \nble to construct this continuation with (Pairing-k (+ x y) k) , instead of (Pairing-k x y k). More generally, \nit is al- ways possible to fold the free values into fewer slots, either by shifting computations to \nan earlier time (as in the preceding example) or by storing values in an additional record, and adding \nan indirection. This last solution is obviously less desirable, because it creates more memory overhead, \nwhich is precisely what we are trying to eliminate! Finally, we claim that in practice, the preceding \nprob-lem cases arise very rarely. In fact, we assert-but do not prove here, since the following section \nwill make it clear-that in ali procedures expressed as finite-output anamor- phisms (to be defined shortly), \nthe length of the continuation records always matches the length of the output records. 6 Anamorphisms \nWe have already argued that what enables the recycling of continuations as output structures is not the \nalgorithm being implemented, but a similarity of form between the contin-uations and the output data \nin structure-generating proce-dures. One canonical class of generators of inductively typed data is the \nset of procedures defined as anamorphisms [8]. An anamorphism is a recursive function that can be defined \nusing the lens operator K 1, a generalized version of the list unfold functional [l]. For those not acquainted \nwith 1~1 sonle settitlgs. it Inay be more appropriate for recyclitlg cow structors (e.g.; recycle-as-pair) \nto be a low-level operation that may not always be available to the end user: but Inay be available to \nthe conlpiler or to users operating in sortie privileged Inode. anamorphisms, we define them briefly \nhere, but it is not es-sential to understand these definitions in order to appreciate the remaining programs. \nDefinition 1 A functor F is a homomorphism between categories, mapping objects A to objects F(A) and \narrows A -f, B to arrows F(A) 2 F(B), such that identity and composition are preserved: F(~A) = IF(A) \nF(sof) = F(g) 0E't.f) Definition 2 An endofunctor is a functor from a category back to itself. Definition \n3 Given an endofunctor F and an arrow A 2 F(A), an anamorphism is an arrow A 3 fix(F) such that the followang \ndiagram commutes: A-?L F(A) ($1 F(l$l) 1 1 fix(F) -F(fix(F)) In other words, an anamorphism over $ is \ndefined recur-sively as %+I = FWI) 0 ?c, The preceding definitions allow ?c, to be such that no closed-form \nsolution exists for the anamorphism. In lazy languages or languages with streams, it can be useful to \nad- mit such anamorphisms. In our setting, however, where we wish to produce finite structures, we require \nanamorphisms over + to terminate (strictly), producing finite data struc-tures. (Otherwise, there would \nnever be an opportunity to invert the links. ) In programming, the category of interest is usually Type, \nwith types as objects and functions as arrows. Func-tors are a pair of a type constructor and a higher-order \nfunc-tion (usually named map in the purely functional program-ming community). For example, the type \nconstructor whose fixpoint is the lists with elements of type LY is F = Xr . (Empty-List) 1 (Pair or) \n The higher-order map function is F=Xf:r-+u. Xx : F(T) case x of (Empty-List) * (Empty-List) (Pair at) \n=+ (Pair a f(t)) or, in Scheme ( un-curried ), (list map)= (define List-map (lambda (f 1) (List-case \n1 ((Fmpty-List) (Eknpty-List)) ((Pair h t) (Pair h (f t)))))) The Yens brackets [ 1 are written out as \nana. Here is the corresponding ana for lists: (list lens)- (define List-ana (lambda (psi) (letrec ((f \n(lambda (1) (List-map f (psi 1))))) f))) These allow us to write list-copy, remove, and iota as anamorphisms. \n (list copy anamorphism)E (define list-copy (List-ana (lambda (1) 1))) (remove anam.orphism) E (define \nremove (lambda (x 1) ((List-ana (lambda (1) (List-case 1 ((Eknpty-List) 1) ((Pair h t) (if (equal? h \nx) t 1))))) 1))) (iota anamorphism) E (define iota (lambda (II) ((List-ana (lambda (i) (cond ((= i n) \n(Empty-List)) (else (Pair i (t i 1)))))) 0))) But now, instead of transforming the individual procedures, \nwhy not transform the ana operator itself (along with the appropriate datatype functors)? If we use these \ncompletely transformed versions: (list map with recycling)= (define List-map-cps (lambda (f 1 k) (List-case \n1 ((Empty-List) (invokeL k (Empty-List))) ((Pair h t) (f t (Pairing-k h k)))))) (list lens with recycling)E \n(define List-ana-cps (lambda (psi) (letrec ((f (lambda (1 k) (List-map-cps f (psi 1) k)))) f))) (define \nList-ana (lambda (psi) (lambda (1) ((List-ana-cps psi) 1 (Initial-k)))))  and use the recvcling definition \nof invoker, from Section 3.4, then the definitions of list-copy, remove, and iota can remain untouched, \nbut they now run without any stack or continuation space overhead! Since many of the code-transformation \npasses of a compiler or language preproces-sor can be written as anamorphisms, the recycling technique \nappears to be very widely applicable. In fact, the CPS trans- formation itself can be implemented this \nway, inviting self-application of the optimization. 7 Future Directions We have demonstrated a method \nfor transforming both specific structure-producing programs and general anamor-phism operators to eliminate \nthe stack-space usage of re-cursion. For lists, it is clearly possible to go further and eliminate the \nlink-inversion stage by inverting the links for-ward as the pairs/continuations are constructed. This \nmakes list construction fully iterative, as in Minamide s paper [lo]. There should be some way to extend \nthe iterative property to one spine of trees and other polynomial types, but we have not yet discovered \nan elegant transformation (to follow the link-inversion transformation) to produce this property. Wand \n!6] demonstrates an elegant sequence of trans-formations that exploits the connection between CPS and \naccumulator-passing style. He makes a different, but re-lated, set of observations about the free variables \nin contin- uation terms, enabling recursive procedures to be converted to iterative ones with accumulators. \nWhere Wand per-forms his conversion by proving that the accumulated value correctly represents the continuation, \nwe simply switch to another-obviously equivalent-representation. Our trans-formations are presented rather \ninformally, but Wand s tech-nique for formal equivalence proofs should be very easily applicable to all \nbut the last transformation. We expect the introduction of side effects to complicate somewhat the equivalence \nproof for the last transformation. Meijer and Hutton extend anamorphisms to exponential types (function \nspaces) [9]. Their work should fit naturally into our setting, but we have not explored the implications \nof this mix. Furthermore, since anamorphisms and catamor- phisms are duals we wonder whether there is \na dual to our technique, which might provide some sort of optimization for catamorphisms. The relationship \nbetween continuation-passing style and monadic style [II] invites the extension of this work to a monadic \nsetting. The CPS monad is but one monad that incurs a cost in terms of space. We plan to explore the \nexten- sion of these transformations to other monads or some gen- eral monadic framework. (For related \nwork, see Chen and Hudak s discussion of translating functional, linear abstract datatypes into monadic \nones with updatable state [4].) Also, one of the great powers of monads is their pro-vision for the reification \nand reflection of monadic meta-information. In order to retain this power, the precise im-plications \nof the presence of reflective operators like call/cc must be explored in the context of recycling. Acknowledgments \nOur thanks to Erik Hilsdale for hours of discussion and help with the datatype macro. Thanks to Erik, \nMatthias Felleisen, Mitch Wand, Jon Rossie, Steve Ganz, and an anonymous referee for thorough readings \nand insightful com-ments to guide us on our way. We also appreciate the com- ments of Michael Levin, \nAnurag Mendhekar, and the par- ticipants of the Friday Morning Programming Languages Seminar at Indiana \nUniversity. References [I] Richard Bird and Philip Wadler. An Introduction to Functional Programming. \nPrentice-Hall, 1988. [2] Luca Cardelli. The functional abstract machine. Tech-nical Report TR-107, Bell \nLabs, 1983. Bell Labs Tech-nical Memorandum TM-83-11271-1. [3] Luca Cardelli. Compiling a functional \nlanguage. In Conference Record of the 1984 ACM Symposium on LISP and Functional Programming, pages 2088217. \nACM Press, 1984. In his words. In this paradignl. one writes a clear. correct. though possibly inefficient. \nprogram. and then transforms it via correctness-preservitlg transformations into a program which is more \nefficient although probably less clear. PI Chih-Ping Chen and Paul Hudak. Rolling your own mutable ADT: \nA connection between linear types and monads. III Conference Record of POPL 97: The 24TH A CM SIGPLA \nN-SIGA CT Symposium on Principles of Programming Languages, pages 54-66, Paris, France, January 1997. \nACM Press. 151 R. Kent Dybvig. Three Implmentation Models for Scheme. PhD thesis, University of North \nCarolina at Chapel Hill, 1987. Harry R. Lewis and Larry Denenburg. Data Structures PI and Their Algorithms. \nHarperCollins, 1991. Grant Malcolm. Algebraic data types and program [71 transformation. Science of Computer \nProgramming, 14(2-3):255 -280, September 1990. Erik Meijer, Maarten Fokkinga, and Ross Paterson. PI Functional \nprogramming with bananas, lenses, en-velopes, and barbed wire. In FPCA 91: 5th Inter-national Conference \non Punctional Programming Lan-guages and Computer Architecture, number 523 in Lec- ture Notes in Computer \nScience. Springer-Verlag, 1991. Erik Meijcr and Graham Hutton. Bananas in space: PI Extending fold and \nunfold to exponential types. In FPCA 95: vh International Conference on Functional Programming Languages \nand Computer Architecture, pages 324~~333, La Jolla, June 1995. ACM Press. PO1 Yasuhiko Minamide. A \nfunctional representation of data structures with a hole. In Conference Record of POPL 98: The 25TH ACM \nSIGPLAN-SIGACT Sym-posium on Principles of Programming Languages, San Diego, January 1998. ACM Press. \nEugenio Moggi. Notions of computation and monads. P11 Information and Computation, 93(1):55-92, July \n1991. Greg Morrisett, Matthias Felleisen, and Robert Harper. P21 Abstract models of memory management. \nIn FPCA 95: ?Lh International Conference on Functional Programming Languages and Computer Architecture, \npages 66-77, La Jolla, June 1995. ACM Press. John C. Reynolds. Definitional interpreters for higher- \n[I31 order programming languages. In Proceedings of the ACM National Conference, pages 717-740. ACM \nPress, 1972. H. Schorr and W. M. Waite. An efficient machine- P4 independent procedure for garbage collection \nin var-ious list structures. Communications of the ACM, 10(8):501 506, August 1967. Jeffrey D. Smith. \nDesign and Analysis of Algorithms. PI PWS-KENT, Boston, 1989. Mitchell Wand. Continuation-based program \ntransfor- WI mation strategies. Journal of the Association for Com-puting Mwhinery, 27(1):164-180, January \n1980. Benjamin Wegbreit. A space-efficient list structure P71 tracing algorithm. IEEE !I!ransactions \non Computers, c21:1009 -1010, 1972. A. Van Wijngaarden. Recursive definition of syntax and PI semantics. \nIn Formal Language Description Languages, pages 13 24. North-Holland, 1964.  \n\t\t\t", "proc_id": "289423", "abstract": "If the continuations in functional data-structure-generating programs are made explicit and represented as records, they can be \"recycled.\" Once they have served their purpose as temporary, intermediate structures for managing program control, the space they occupy can be reused for the structures that the programs produce as their output. To effect this immediate memory reclamation, we use a sequence of correctness-preserving program transformations, demonstrated through a series of simple examples. We then apply the transformations to general anamorphism operators, with the important consequence that all finite-output anamorphisms can now be run without any stack- or continuation-space overhead.", "authors": [{"name": "Jonathan Sobel", "author_profile_id": "81332528781", "affiliation": "Computer Science Department, Indiana University, Bloomington, Indiana", "person_id": "P147474", "email_address": "", "orcid_id": ""}, {"name": "Daniel P. Friedman", "author_profile_id": "81100636522", "affiliation": "Computer Science Department, Indiana University, Bloomington, Indiana", "person_id": "PP39051860", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/289423.289452", "year": "1998", "article_id": "289452", "conference": "ICFP", "title": "Recycling continuations", "url": "http://dl.acm.org/citation.cfm?id=289452"}