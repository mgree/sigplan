{"article_publication_date": "09-29-1998", "fulltext": "\n A Framework for Type Inference with Subtyping FranCok Pottier* Francois.PottierOinria.fr Introduction \nIn type systems based on subtyping, type equality is re-placed with subtyping, which is a less restrictive \nrelation-ship. The idea is, if 71 is a subtype of ~2, then a value of type ~1 can be transparently supplied \nwherever a value of type r~ is expected. Subtyping has been used as a key concept to create formal type \nsystems for object-oriented languages. These systems often require the programs to be annotated with \nuser-supplied type information. Being able to omit this information--or at least part of it-provides \nthe program-mer with a greater degree of freedom; hence, the desire arises to do type inference in the \npresence of subtyping. This issue has been extensively studied in the past few years. Many type systems \nhave been proposed, with vary-ing degrees of richness and complexity. Possible features are the existence \nof a least type I and a greatest type T, the presence of contravariant type constructors such as +, the \nexistence of union and intersection types, the existence of recursive types, the ability for the user \nto extend the type language through generative type declarations, etc. Virtu-ally all of these systems \nhare their type inference algorithms upon the same principle. Each function application node in the program \ngenerates a subtyping constraint, which re-quires that the actual argument s type be a subtype of the \nfunct,ion parameter s t,ype. Type inference consists in gath- ering these constraints and checking that \nthey admit a so- lution. The type system we present here is quite general. It has a fixed type language, \nwhich is the set of regular types formed with I, T and --+. It is not as powerful as that of [4, 51, \nwhich has much more general union and intersection types; still, it is general enough to easily support \nthe addition of a wide class of type constructs, such as extensible records and variants. Type inference \nis done as explained above; determining whether a conjunction of constraints admits a solution is done \nthrough a closure computation [7]. In the- ory, the issue is settled. In practice, however, things only \nbegin here. The number of constraints accumulated by type inference is large (at best, linear in the \nprogram size; at worst, exponential, because let constructs duplicate them). This slows down type inference \n(the closure algorithm takes time cubic in the number of type variables) and makes types illegible (constraints \nare part of the type information given to the user). Therefore, algorithms are needed to simplify sets \nof sub- typing constraints, without affecting their meaning. In [14], we introduced two such methods. \nOne removed so-called unreachable constraints; the other used heuristics to come up with substitutions \nwhich could be applied to the inferred type schemes without lessening their power. Smith and Tri- fonov \n[18] refine the former into a concept called garbage collectron. Besides, they describe a process called \ncanoniza-tzon, which consists in rewriting a type scheme so that each variable has at most one constructed \nlower (resp. upper) bound. Other substitution methods can be found in [l]. So, at this point, various \nsimplification methods are known, some of which are very effective, such as garbage collection. However, \nthese are not sufficient to obtain an efficient, well-integrated type inference algorithm; several problems \nremain. First, substitut.ion heuristics are inher-ently ineficient.. Many possibilities have to be tried \nout, and each t.rv is costly, since it involves proving that the suhstitution is lrgal. We solve this \nproblem by eliminating heuristics altogether and replacing them with a very efficient minimizatl:on algorithm, \na close cousin to the Hopcroft algorithm introduced in [9]. Second, although garbage col-lection, as \ndescribed by Smith and Trifonov, works well, it does not preserve the closure property. This is a problem, \nsince we would like the type inference algorithm to work with closed constraint sets at all times, so \nit can do incremental closure computations. We solve it by showing that. if the type inference rules \nare properly formulated, then no bipo-lar type variables are generated, which ensures that garbage collection \npreserves closure. Third, we give a precise formal descript.iou of the canonizatl;orb algorithm, and \nwe combine it with garbage collection, which makes it more efficient. We also show that. a nat.ural generalization \nof this algorithm can be used, if desired, to eliminate bipolar variables. Fourth and finally, wp draw \na distinction between internal and ex-ternal simplific:ation methods. The former help efficiency, and \ncan be used throughout the type inference process; the latter help readability, and must. be used only \nwhen submit-ting type information to the user. The former conflict with the latter, which is why the \ndistinction is important; trying to achieve cflicienc,y and readability at the same time is a design \nmistake. To sum up, we describe a full framework for type in-ference and simplification in the presence \nof subtyping. It consists of a few key components: a set of type inference rules, designed to preserve \na couple of desirable invariants, some internal simplification methods (canonization, garbage collection \nand minimization), and some (classic) external methods. Each of them integrates smoothly into the whole. \nAs a result, the system is theoretically simple and leads to an efhcient implementation. This paper is \norganized as follows. Section 1 describes the type system, while section 2 recalls how to compute po-larities \nand use them to do garbage collection. Section 3 shows that it is enough (and best) to work with small \nterms. Section 4 introduces canonization and combines it with garbage collection to gain efficiency. \nSection 5 explains why and how to eliminate bipolar variables. Section 6 talks about the difference between \ninternal and external strate-gies. Section 7 defines the minimization algorithm. Sec-tion 8 describes \nthe implementation and gives some perfor-mance figures. Section 9 discusses the remaining problems. Finally, \nrelated work is surveyed in section 10. An appendix shows the type inference engine at work on a typical \nexam-ple. Proofs, as well as several non-essential definitions, were omitted by lack of space; all can \nbe found in [15]. Sections 3 to 7 discuss separate points and are largely independent from one another. \n1 The setting The type system described in this section is a close cousin to Smith and Trifonov s [lS]. \nOur type inference rules have been modified so as to preserve some additional invariants, discussed in \nsections 3 and 5. Also, our definition of con-straint graphs allows occurrences of U and fl at certain \npo-sitions in types, as a purely syntactic convenience. The language under study is a X-calculus with \nlet, where X-bound and let-bound identifiers form two distinct syntac-tic categories. The set of ground \ntypes is the set of regular trees built over I, T (both with arity 0) and -+ (with arity 2). A partial \norder on ground types, called subtyping, is defined in the classic manner. Definition 1.1 Subtyping is \nthe greatest relation such that r 5 r holds i;fs at least one of the following is true: 0 r=l. l r =T. \n Equipped with this ordering, the set of ground types be-comes a lattice. We denote its least upper bound \nand great- est lower bound operators by U and tl, respectively. As in any lattice, U and fl are associative \nand commutative. In addition, they are fully characterized as follows: Proposition 1.1 The following \nare identities: The definition of types resembles that of ground types; however, types are finite terms, \nand they may cantain type variables. Furthermore, we shall permit U and Il constructs to appear at specific \npositions in types. This allows encoding multiple bounds into a single one, e.g. by writing (r 5 cr A \nT < o) as (7 U 7 ) 5 Q. This decision affects only the syntax of our constraint language, not its power; \nit shall mainly allow a simpler description of canonization. Thus, we define two kinds of types, pas-types \nand neg-types: T+ ::= o 1 -L ( T IT--+ T+ 1 u{~+, ,7+} r-::= o I I I T 1 T+ --f ,r-) n{~-, . ,7-} (Note \nthat U (resp. fl) has two distinct meanings; it is an op- eration on ground types and a type constructor.) \nTypes are considered equal modulo the equations of proposition 1.1. Types have normal forms such that \nwhenever U or fl ap-pears with n arguments, then 71. is at least 2, at most one argument is not a type \nvariable, and if such an argument exists. then its head constructor is +. A type is said to be constructed \niff it is neither a variable nor a U- or n-construct. To keep track of the subtyping constraints created \nby analyzing an expression, we introduce constraint graphs. A constraint graph represents a conjunction \nof constraints, but allows only one constructed bound per type variable. (This requirement is made possible \nby the availability of U and fl constructs.) .L constraint graph C is made up of a relation between type \nvariables, denoted by SC, and two maps C and CT, which map each Q E V to a its lower bound (a constructed \npas-type) and t.o its upper bound (a constructed neg-type), respectively. A ground substitution p is \na solution of C (which we denote by p t C) iff the following conditions hold: vrr/!3 E 1 . N <c-v a =+ \np(a) I p(P) vo E 1 P(C%N 5 Pb) 5 P(C W To verify ihat a constraint graph admits a solution, one needs \nto put it in some kind of solved form, off which a solution can ire read straightforwardly. There exist \nsev-eral notions of solved form. The simplest, which is also the strongest, we call closure; it consists \nin requiring that the constraint graph be closed by transitivity on variables and by structural decomposition. \nIt was originally introduced in [7] . Its advantage is that there exists a simple procedure which turns \nanv constraint graph into an equivalent closed graph, if one exists. This allows deciding whether any \ngiven constraint graph has a solution. The weakest known notion of solved form, which we call wea,d: \nclosure, is similar in prin- ciple, but is denned usin a not.ion of provable entailment. B It was intrcduietl \nin [18] . Its advantage is that it imposes less drastic r,+clnirements on the constraint graph, which \nis useful in SOII!C cielicate proofs, like that of Smith and Tri-fonov s sub,+uml;tion algorithm. Here, \nwe need to introduce a third notion, called simple closure, which is intermediate between weakclosure \nand closure. lt is used to formalize the canonization prncess; it is introduced in detail in section \n4.1. A context A = (x : r-Z)zE~ is a finite map from X-identifiers to neg-t.ypes. A type scheme u = A \n+ t 1 C is a triple of a context il, a pos-type 7, and a constraint graph C. All type v; :-i&#38;les \nin a type scheme should be understood 1 as universally quantified. However, we do not introduce any quantifiers, \nand we deal with a-conversion explicitly. The subtyping relation on ground types is extended to a so-called \nsubsumptkvz relation on type schemes. One says that UI is subsumed by ~2, and one writes (~1 < UZ, iff \nfor any ground instance of UZ, there exists a ground instance of 01 which is smaller. This can be written \nAl + TI 1 Cl Iv AZ + ~2 1 CZ iff Vpz I- Cz 3pl I- CI pl(A~ =+ 71) 5 ~z(A2 =+ TZ) Equivalence of type \nschemes, denoted by = , is defined as subsumption in both directions. The typing rules are given in figure \n1. They are essen- tially identical to those given in [18], only slightly simplified. Note that environments \ncontain only let-identifiers, while X-identifiers appear in the inferred context,s. Introducing contexts \nin type schemes and in the typing rules is not required by the advent of subtypiug. Rather, this technique, \ncalled X-lifting, is a technical trick which al- lows us to deal solely with closed (i.e. fully quantified) \ntype schemes, and thus, to define scheme subsumption without taking environments into account. Since \nsubsumption is at the very core of the theory, the whole system is made signif- icantly simpler. Concerning \nsubsumption, the presence of a context does not add any complexity. It behaves essentially as a record \ntype to the left of an arrow type, hence the arrow notation A 3 r 1 C. These typing rules are not syntax-directed, \nbecause rule (SUB) can be applied at any time. Furthermore, rule (APP) places sharing constraints on \nits premises. In figure 2, we present a set of type inference rules, which do not have these problems, \nand thus can be directly turned into an algorithm. One shows that they are correct and complete with \nrespect to the typing rules. That is, they find a typing if and only if one exists, and in that case, \nit is a most general typing with respect to subsumption. A type inference judgment is of the form [F] \nl? t-1 e : [F ] CT. F and F are sets of type variables, used to ensure that any newly introduced type \nvariable is fresh ; thus, two unrelated branches of a type inference tree are guaranteed to produce type \nschemes which share no variables. These annotations should otherwise be ignored. Our type inference rules \ndiffer from those of [18] in sev-eral aspects. Modifications have been made to ensure that they comply \nwith the small terms invariant (see section 3) and with the mono-polarity invariant (see section 5). \nIn-cidentally, these rules do describe an algorithm, since no sharing constraints remain, and fresh variables \nare handled explicitly. Note that both sets of rules implicitly require all con-straint graphs to have \na solution. This condition is enforced in the implementation by working with closed constraint graphs \nat all times. The + operation, which adds a con- straint to a constraint graph, performs an incremental \nclo-sure computation in practice. 2 Polarities and garbage collection Trifonov and Smith [18] explain \nhow to annotate each type variable in a type scheme with its polarity, and use this no-tion to define \ngarbage collection. We recall their definitions here. [f c7 = A * 7 / C is a type scheme, where C is \na weakly closed constraint graph, then the sets dam+(a) and dome(a) of l>c&#38;ive (resp. negative) variables \nof u are the smallest sets ~;uch that for any F E {-, +}, l ff(T) c d:Vll (u); l v.2: E dom( A) fv (A(z)) \nC dam (0); l do E do&#38; (u) fv (CL(~)) C doIn ( l v tr E &#38;m-(n) fv (f?(~)) C dam (a). The polardy \nof a variable cy is defined as {E E {-, +} ; (Y E dam (u)}. Informally speaking, the + (resp. -) sign \nindi- cates that Q r;light receive a new upper (resp. lower) bound in the filtilrr. I olarities carry \ninformation about the direc-tion of the data flow: positive (rcsp. negative) variables represent an oIltput \n(resp. input) of the code being typed. Note that a y:,ri.sble can also carry both signs at once (we call \nsuch a variable bipolar), or no sign at all (we call such a varia!,lc :ze~?~~l). Garbage collect.ion \nuses this information to throw away meaningless const.raints. A constraint is only useful if it ac-tually \nconstraros the type scheme, i.e. if it can potentially cause a future closure computation to fail. If \n5~ is transi- tive, then CC>(cr) is defined as .4 + 7 1 D, where 0 a <r, iTi iii YE 5~ a, a E don.- (0) \nand ,B E dam+(u); * I?(n) is \\ :, ,n) when a E &#38;!III+(u), and I otherwise; u Z> (n) is i:; (N) when \nR E dom-(a), and T otherwise. In section 5, we show that it is possible to prohibit bipolar variables: \nthis gives better theoretical properties to garbage collection. The minimization algorithm described \nin section 7 also makes fundamental use of polarities. 3 Work with small terms We are now c).)ne recalling \ndefinit.ions, and can start dis-cussing ;hi; k)l\\i)eT s contributions. 111 this section, we show that \nit in: pos:;ii)i- to enforce quite drastic restrictions on the height and sr.apc of the type terms we \nmanipulate. In addi- tion to simplifyil:g the theory, these restrictions are actually benefit ial fro:l: \nan implementor s point of view. A type term is said to he a leafterm iff it has height 0 and it is not \nconstructed; equivalent!y, iff it is a combination using L or n of one or more tvpe variables. A type \nterm is said to be a ,s:rk:llc:ll term iff it it is constructed and its sub-terms are lea,- r+rn>s. A \ntype scheme .4 =+ r 1 C is said to verify t/it, smn/~. trrms inaariant irf every A(z) is a leaf term, \n7 is a leaf tcrrt and for each cr, /TL(~ly) and CT(a) are small terms. It. is quit* ci sy to show that \nany type scheme admits an equivalent form which verifies the small terms invariant. Whenever a type t.erm \nviolates the invariant, it suffices to break it down by introducing fresh variables (together with appropriate \nsubtype inequations) t.o stand for its sub-terms. Actually, this is not even necessary in practice, because \nour type inference, rules are designed to always produce compli-ant type schcl lcs. In the typing rules, \n(ABs) is the only rule which builds 11;) ,Lew terms. So, n:les (ABSI) and (ABs ,) in-troduce one PX :n \ntiesh variable, n;+rn~ly (Y, to avoid breaking the smaLl tryLit.:. lr~c-ar!.ant. This in~.ar lnt simplifies \nboth theory and implementa-tion of ~<l!>se<i~~~~!l~ Additionally, developments. it helps sim- plify inftarreti \n:::pe schemes. Ind~~cl, !arge terms no longer A(z) = r I- t e : (A + [x t-+ r!; =+ 7 [ C (VAR) (Am) \nrkz:A+~/C r t- xx.e : A =+ i -:-/ 7 1 c I ~Q:A=+Q+TIC !L I-Q:A~T~~C 17(.Y) = u (.APP) r i ?i :~ (LFXVAR) \n r I- el ez : A =S T 1 C u cv u rtel :ul r + [X k+ CT,] I-e2 : u2 ri-e:a _ (LET) -(SIJR) r t- let X \n= el in e2 : fl2 I t P : 0 Figure 1 Typing rules _.-. (VA%) [F] r h x : [F u {G, p}] (x : ~2) =+ p \n1 0 + (N 5 R) [F] r kI e : [F ] A 3 T 1 C A(z) = r  -.?-!!:I.-(Ass*) [F] r kI Xx.e : [F u {a}] (A \n\\ z) =s a 1 C + (T + T 5 (t) [F] r tI e : [F ] A a 7 1 C 2 e dam(A) ff,d $ F?  ----(ABs'I)[Flr~-rXz.e:[F \nu{a,P}]A~(YIC+i~j-t~ F(~ ! [F] r h el : [F ] AI + ~1 1 Cl [F ] r I-] p2 : [F ] AZ + Q I Cz a, 9 +! F \n--.-~-- (A= :) [F] I I-I el e2 : [F U {a, p}] (Al fl Az) =+ 9 1 C where C = (Cl U CZ) + (Q 5 p) + (~1 \n5 n -+ 0) r(x) = 0 p renaming of u mg(p) n F = d (LETVARI) [F] r t-1 X : [F U mg(p)] p(o) -- [F] r I-I \nel : [F ] m (F] r + [X l--i al] 11 c2 : p ~~-~ (LETI) [F] r FI let, X = el in e2 : [F\"] (~2 Figure 2: \nType inference rules exist.-informally speaking, each sub-term of a large term lower (resp. up!.~r) constructed \nbound. There are two main is labeled with its own type variable. Thus, the minimiza-reasons for thi+ \nFirst, combining several bounds into a sin- tion algorithm (see section 7), whose occupation is to merge \ngle one is a sirrT$ification per se, because some structure is variables, is able to find new opportunities--which \nessen-shared (or ev:~; eliminated). Second, this property is essen- tially means share sub-terms of large \nterms. tial in the c!:r-;:I. +.,f some simplification algorithms, such as The idea of working solely \nwith small terms is not new, minimizatioa (spc section 7). at least from a theoretical point of view. \nIt is to be found, To this end. we have introduced U and il constructs in for instance, in the theory \nof unification [12], where prob-type expressio:??. The closure algorithm can easily handle lems are presented \nas sets of multi-equations of depth 1 them, and they are a cheap way of enforcing uniqueness of at most, \nrather than as equations between arbitrary terms. the constructed bound. However, in doing so, we have \nmade Among works more closely related to ours, those of Aiken the type language more complex, and these \nnew constructs and Wimmers [2] and of Palsberg [13] use a similar conven-cannot easily be dealt with \nby some algorithms (again, min-tion. However, it is often a mere theoretical convenience. imization). \nHere, the invariant shall also help improve sharing between So, we no\\\\ wish to eliminate them, in a \nprocess called nodes, through the minimization algorithm, and is thus es-cano&#38;afion. ~ he small terms \ninvariant ensures that, once sential from an implementor s point of view. types have learn put in normal \nform, arguments of u and n ciln oc1ly iV :-!;pe variables. To do away with these con- structs, we c;+n \nti+e the same principle as in section 3: adding Canonization fresh variabkbs with appr0priat.e constraints. \nAn occurrence of, say, G! u ,Y can be replaced with a fresh variable y, pro- When dealing with arbitrary \nsets of constraints, it quickly vided (E 5 y a!ni ,u 5 y are added. appears desirable that each type \nvariable have exactly one -.-..-. r+(a) = ck when {a} $ &#38; I,-ic,j = a when {nl $J E r+ (US) = AS \nwhen S E &#38; r-(Mj = ys when S E E r+(I) = I F(I) = I v+(T) = T r--(T) = T r+(n, -4 71) = r-(m) --f \nr+(n) I.-(n1 -'. T-1) = r+(T(,) -+ r-(n) Figure 3: Definition of the rewriting -~IJ~.~.~,~>I!s D (n) \n= r+(C (n)) D (a) = r-(C (a)) DT(?.s) = r-l n CT(a)) Dqxs) = r+( u C (cr)) WES ae.3 Figure 4: Definition \nof LIL and 1) 0 <n P when KY <C R 7.S <D (1 when cr E S a ID As when (Y E S 7.-(n$ 50 -yT when ISI 2 \n1 anti flS <(-, i,7 xs SD r+(UT) when 12 1 > 1 and US <? UT r+(uS) SD r-(nT) when IS 1 2 1, I I ( > I \n;.nd US 5~ nT - Figure 5: Definition of <D, module transi:i:c c,!osure This process was first introduced \nby Smith and Tri-fonov [18]. We improve their results in several ways. First, we give a more precise \nformal description of it, and prove that it preserves simple closure. This allows running the garbage \ncollection algorithm on the output type scheme. We simulate its execution and show that many of the constraints \ngenerated by canonization are then dropped. We can thus Definition 4.2 .4 constraint grrtph C of domain \nV is sim- define an efficient combined algorithm, which avoids gen- ply closed ijfto,r all 0, p E I , \nerating any constraints which would provably be dropped immediately afterwards by garbage colktion. Second, \nwe show that the same algorithm can be used to eliminate bipo-lar variables (more about this in section \n5). * c- I,,.) _:_( * (a). 4.1 Simple closure A type S&#38;CVX o = -4 =+- r ( C as soid to be simply \nclosed iff c as. As PxqJainetl above, we wish to run the type scheme output by canonization through garbage \ncollection, which requires 4.2 Canonization it to be at least weakly closed. This notion, defined using \nprovable entailment, is rather complex, so we prefer to use Theorem 4.1 Let (T = A * r 1 C be a simply \nclosed type something coarser, but simpler. Plain closure is too crude: scheme of domain I'.. Let t be \na subset of 2 , that is, a we cannot, in general, require that the output constraint far&#38;g of sd,:~~fs \nof V. E must contaan all subsets of V of graph he closed. So, we define an intermediate notion. called \ncardinalzty str~?!~/ greater than 1, and must not contain the sample c1osu7.c. empty nc?. ( !I i. C!L \n.kngleton {a} can be left out or made part Definition 4.1 Let C bc a. constraint yraph. Tlze relation \nFor each, S 06 E, pick two fresh varrables Xs a.nd ys. The2~ Z.S extended to leaf terms by rewriting \nfun(:ii~)ns 1.+ and r-arc defined in figure 3. The output const.rair:t graph D is defined by its components \nD and U r: y'iww LPLfigure 4, and by the relation 50, which is the transitivr clos;Ire of the constraints \ngiven in figure 5. Let o- = Y ( 4) + r+(r) ) L). Then O is simply closed, contazns no u x K constructs, \nand u = CT . Figure 6: Definition of El and E Figure 7: Definition of <s This definition might seem complex \nand non-intuit.ive. In particular, one might wonder why figure 4 assigns a con- st.ructed lower (resp. \nupper) bound to ys (resp. Xs), or why the last line of figure 5 creates constraints of the form Xs < \n7~. These constraints are only necessary to guaran- tee that simple closure is preserved; they will be \ndropped immediately by garbage collection, as we shall see below. o is simply closed; this allows computing \npolarities and performing garbage collection. We now simulate these phases. It turns out that the newly \nintroduced Xs are (at most) positive, while the ys are (at most) negative. As for the existing variables \nQ, their polarities must decrease. In particular, if {o} E E, then cr becomes neutral. These re- sults \nare formalized below. Lemma 4.1 We have dom+(a ) C {X,q ; S E &#38;A S C dam+(o)} U {a ; {CY} $Z &#38;A \nn! E dam+(a)} dam-(rr ) c (7,~; S E &#38;AS c dam-(o)} U{a;(o}~&#38;AcrEdom-(cr)} A first consequence \nof this lemma is that this transfor-mation can be used to elitninate bipolar variables. It suffices to \nchoose E such that whenever N is bipolar, then it is sched- uled for elimination; i.e. {o} E E. The lemma \nshows that if this condition holds, then there are no bipolar variables in the output type scheme. The \nsecond consequence is to allow garbage collection on 17 . Because we only have an approximation of the \npolari-ties, we can only do a partial garbage collection. So, the type scheme u which we are going to \ndefine is not GC(o ); it contains more constraints than GC(a j, but fewer than g . Thus, in practice, \nwe still have to complete the job by running the garbage collection algorithm, but we have saved time \nby not generating superfluous constraints. Theorem 4.2 Let E be the constraint graph given in fig- urcs \n6 and 7. Let 0 = r-(A) + T+(T) ) E. Then is a partially gcrbagc-collect&#38; version of u . That is, \n&#38;(n ) = GC(u ). Outlaw bipolar variables We want our implementation of the type inference engine \nto work with closed constraint graphs, which allows performing the closure ciktnputations incrementally. \nAs a consequence, we must en.<::; \\ that all simplification phases preserve clo- sure. However, I: I \nJ~XS out that in general, garbage collection does not presrrve closure. For instance, consider a bipolar \nvariable 0 whose constructed bounds are CJ(n) = cs* -+ p+ c?(N) = l5* + y- Then, if the constraint graph \nis to be closed, it must contain /?+ < ?-. However, such a constraint shall be thrown away by garbage \nco!!ection, thus breaking the invariant. Fortunate,:\\.. this problem has a simple solution. One easily \nverifk:; ,l:n-I if the input t.ype scheme has no bipo-lar variables 11Ictr garbage collection does preserve \nclosure. Such d type .,, !I~\\~;le is said to vcrifv the mono-polarity in-varialkt. I~urthermore. we prove \nthat any type scheme produced by our type inference rules complies with this invariant. The principle \nof the proof is two-fold. First, no freshly introduced variables are bipolar. Rules (\\'ARl) and (APPI) \nexpressly introduce two variabies (Y _< j9,where (Y is non-positive and ,S is non-negative. If a single \nfresh variable were used, it might be bipolar. S$ ,cncl, polarities decrease with time. That is, if a \nvariable ii non-positive (resp. non-negative) when it is first intro&#38;c,, :I; then it will 71~~ become \npositive (resp. negative ! at .4 3 ;tr stage of the type inference process. This property is consistent. \nwith the intuition that a non-positive (resp nonnegative) variable will never receive new upper (resp. \nlower) bounds in the future. As shown in section 4, the canonization algorithm can be used to transform \nany type scheme into an equivalent one which verifies the invariant. It replaces each bipolar variable \nwith two fres!! variables, a negative one and a positive one. Thus, the mono-polarity invariant will \nat worst double the number of tJ.1,: \\-ariables in a type scheme. This inv>r;,. ~: also has beneficial \neffects in other areas. For instance, r: f,pens new opportunities to the minimization algoritlm: tl~~~ \nI. :jrb:l in section 7. Indeed, a bipolar variable cannot Le n:(!rg~d with any other variable; but once \nit is spli ! , e~( h hali (.a:, potentially be merged with some other variable. Another effect is that \nmy cycles in the constraint graph are now ar:t.omatically remL)ved by garbage collection. Indeed, it \nis c:n<y to verify that if a cycle survives garbage collection, then it contains a bipolar variable. \nThus, remov-ing cycles in n separate phase (as done in [6, 14, I]) is no longer rnce~:-;:-~ ;i 6 Sugar \nbefore display There is a well-known simplificat.ion strategy which we have not discussed so far: namely, \nreplacing a non-negative (resp. non-positive) variable with its unique lower (resp. upper) bound, if \nit exists. This strategy 11~ been proposed in numerous papers [6, 1, 3, 141. However, it becomes illegal \nin our setting! Indeed, sup- pose we attempt to replace a variable N with its unique bound. If this bound \nis a constructed term, then the sub- stitution violates the small terms invariant. If, on the other hand, \nit is a type variable /?, then cy and /3 must have oppo- site signs, because garbage collection would \notherwise have thrown away t.he constraint which links them. Thus, identi- fying them creates a bipolar \nvariable and violat.es the mono- polarity invariant. One should not be particularly upset about this \nfact. In our eyes, the fact that this strategy breaks some desirable in- variants only shows that it \nconflicts with the efficiency goal. So, we forbid its use during the type inference process. The strategy \nstill remains useful after the type inference process is complete, that is, immediately before displaying \nthe re- sult to the user. Indeed, though our invariants are internally useful, they make type schemes \nillegible by creating many intermediate nodes. So, we do allow replacing variables with their unique \nbounds prior to display. We believe that this distinction between internal and external representations \nis quite important, and failure to recognize it can lead a de- signer to set up conflicting simplification \nstrategies. This distinction also exists in ML typecheckers, where type terms are internall; represented \nby graphs, but dis- played as trees. Preserving sharing internally is crucial to avoid an exponential \nefficiency loss. 7 Minimization In [14], we proposed a two-step method to simplify a type scheme (T. \nFirst, come up with some substitution p; then, check whether p(o) is equivalent to U. However, the number \nof possible p is huge. ant: the entailment algorithm used in the second step is rather costly, so it \nwas necessary to devise heuristics to select appropriate substitutions. This solution was unsatisfactory, \nbecause these heuristics were rather ad hoc and still extremely inefficient. Thus, a systematic algorithm, \nwhich directly builds as powerful a substitution as possible, is needed. Such an al-gorithm was proposed \nby Felleisen and Flanagan [9] in the case of set-based analysis; the same principle can be ap-plied here. \nStart with the largest conceivable substitution, i.e. one which merges all negative (resp. positive) \nvariables together. (We cannot merge a variable with a constructed term, or a negative variable with \na positive one, as explained in section 6.) We can, eqcivalently, consider it as a partition of t.he \nvariables into t,wo classes. Then, simulate a run of the suhsumption algorithm which checks whether this \nsubstitu-tion is acceptable. If a failure occurs, determine why and refine the partition by Splitting \nan appropriate class so as to eliminate the cause of the failure. Repeat this process until no more failures \nare detected. A failure typically occurs when we try to merge two vari- ables which do not play the same \nrole in the type scheme. A variable s role is determined by the way it is linked to other variables through \nsubtyping constraints. Thus, an-other way to (informally) present the algorithm is to say that two v.zial&#38;~ \ncan be merged if each one carries links of the same !i:r,tl as the other one and these links lead to \nvariables whit?, can themselves be merged. Roth dcsc:+l.i:)ns ring a bell- --similar ideas are used to \nminiinizc fin 1 I state automata. Theorem 7. .t A u = A + r / C he a garbage collected type schtmr. :( \npartl;tl:on z oj (T S llariables is said to be compatiible ~~5 (I z /j imp&#38;s l polarity(n) = polarity(B); \nl C (tr) E IT+(d) and CT(o) E CT(p). From this r~suit, we deduce the so-called minimization algorithm. \nV!: J,:orithm computes the coarsest compatible partition. (A-explained above, this involves computing \nan initial partition, and running Hopcroft s minimization algo-rithm [I l] to r&#38;v it.) It then collapses \neach class down to a single t\\-pe variable. The whole process takes time O(dn log n), where d is the \ndegree of the graph <c, and n is the numbrr of type variables in n. We conjecture that it is also O(2Vllo,q \nN), where N is some measure of the size of g. Actual te::+> show that its running time is approximately \nlinear in the :,:i:e of its input. It is uat?r:,! t0 ask whether the algorithm is complete, i.e. whether \nti,,\\ c,oarsest compatible substitution is really the coar.+st : h tution allowed by our definition of \nscheme subsumption. 1.ae answer is negative; the problem is that our definition 01 predecessor and successor \nsets rely on con- straint.s which are syntactically present in the constraint graph. if th >:%e sets \nare defined using entailment, a more powerful defir;i):i,,n of compatibiliIy is obtained, which might \nbe complete (no (co-tinter-examples are known to us). How-ever, u~ng CX: r~ .n:ant has two i!l effects: \nfirst, the extended algori thin ::: : : ;l! not complete, because no complete entail-ment aigoritl:n! \nis known; and srcond, it is slower, because our m:oI:Ipi( I~X cut ailment algorithm is rather costly. \n How do these pieces fit together? First, the type inference engine analy7r5 the program using the rules \nof figure 2. As new c~r~strai~~r i appear, their closure is computed incre-mentally-. wli!(.i, gllarantees \nthat they have a solution. At any point. c-a:lc?r,iyation, garbage collection and minimiza-tion hi\\~: \nIIF. (27.;. our theoret!c:al development guarantees )I :,;]; that their ( (1, ..i)in;ltion preserves \nclosure. Thus, the sim-plification 1 CC; fits smoothly ;nto t,he regular inference process. V.%CY~ to \nS.QXJUI~ simplificatic,n? At least at each let no&#38;, l;(:cause tbL? erivironment would otherwise contain \nun-simplified type schemes, and the siniplification work would be needlessly dcplicated. At other nodes, \nwe have a choice; in practice, OU.Y garbage collection is performed, because it is cheap anti iti-::ct;-ce \nenough. F:risliv. n ttm a type scheme must he presented to the user, Wf <JY] ii. . external ,,miplification \nmethod to it; that is. ,+re ;:- i.. c each type variable with its unique bound, whenevt~r alit ,JJJ~(I \ni,y the occur check. Although the type language described in this paper is reduced to a bare minimum, \nour implementation has a very rich type language, featuring record and variant types with row variables \n1161, and an interesting type-based exception analysis. In particular, ir clan handle arbitrary Caml-Light \nprograms, provided thq are translated into our language. This translatio[l r~movc:: all type declarations \nand interface specifications. This yields much more precise typings, but makes the typ4 cker s task much \nheavier. Figure 8 presents a few performance measurements on existing Caml-Light libraries. The prototype, \ncompiled into machine language using Objective Cam1 1.07, was tested on a 150MHz Pentium Pro processor; \ntimings are in user time. The last column shows Caml-Light s compilation time, and roughly WprfWnts a \nuser s expectation; the previous one shows our typecheckcr s performance in the absence of minimization. \nFor Graphs and MLgraph, performance seems close to Cam-Light s, The standard library contains smaller \nfunc-tions, and is dealt with more easily; the Format library, on the other hand, contains functions \nwith large concrete types--recall that we use almost no abst.ract types-and is more difficult to handle. \nSo, performance is reasonable when dealing with small or medium types, and worsens when handling large \nones; the prototype does not bphavtx linearly in this respect. However, it is nice to note that simplification \nis not a bottleneck, since closure itself is non-linear. This is not entirely surprising, since> closure \nhas, in theory, cubic time complexity. Also, note that the hcnefit.s of minimization outweigh its cost; \nthat was not the case in ;9]. Directions To improve performance. one possibility is to investigate incremental \nsinlplification algorithms--all of our three algo-rithms work on a whole type scheme, rather than only \non the most recently added constraints. On the other hand, our performance figures suggest that our simplification \ntechniques are reasonably fast, i.e. of the same order as closnr~ itself. Thus, anot r,er possibility \nis 1-o concentrate not on the rimplification algorithms, but on the constraint creat,ion engine itself-that \nis, on the formulation of the type inference rules. Indeed, our use of X-lifting incurs a loss of sharing \nbetween the various branches of a type inference derivation. Sharing is explicitly restored by the context \nintersection operation, but this entails extra closure and simplification work. Formulating the tvping \nrules without rilaking use of X- lifting would thus pr&#38;luc:e a more intuitive, and possibly -more \neEcient. vpstcm. It would also help deal with impera-tivc cr:r::itrul,t %cc our currEnt system does not \nsupport UIICV ST r,i;cc! : -.-I\\ .:ariahles, it dc:\\r!s with expansive let defi- 1 .-. . nitions 1):: \nrib ir .:; them i2to B-rcd~xes [19]. However, top-level li?! dc. -.I r.s cannot hc rewritten in such \na way, so they art a:( \\ (L, I mlv if they tl~f:n~ i\\ monomorphic value-hen<<,>_p $71 ICC cbt: flexibi!itp \nSuch a sy:;tcm seems to requi:c(l a more complex subtyp-ing rule wi,,(:ll 1s why we adopt.4 X-lifting \nin the first place. So, it is a challenging research subject. Finally, let us mention that. better practical \nperformance would of course he obtained if the code being tested used appropriate a l,srrac:t type defimtitms \nand module interfaces. The ty:,>? g 9. uq~il in this l a;)er are the same as in [lS]. Oldrr iii?+C Y \n141 have the SX:W valid programs, but fewer Idid i~~~?~rq,s,I;ecause the;: lack a polymorphic sub-siirnl.: \n:,~. r. !-.nc, as in [18!, theme subsumpt.ion is the one mti only f,l~~t,retical basis for $11 simplification \nmethods, whic;l is sinlplc and elegant. The type sgstcm proposec 1 Ii? Sulzmann et al. [17] is fairly \n( I,:;I t, ,Tl:rs, but does n(It use X-lifting. Still, it is not the> ld~~i .,Y : ~t;m we discusbrci \nin the previous section, becaL:>s : ;; :>;,I I( ,);ug rule is tot) ,.c>ak: in particular, it does not \n:dl\\!\\x. &#38;i;i +F:.:! collection in j.l~ global constraint graph. As ; ;. i I-<,, . ::lliScation is \ncoilccrned, our current algo-rithms ay? :,I 1 - XT more powe&#38;.: than those given in [6, 1, 1s;. ,? \nL.;ll;l.i.. : .lj>,. is slightly i.:+ powerful, in theory, than the hVqlristic~:, ,): (~41, because the \nlatter are based on en-tailr;,sur; iio~?Wr, in practice, it is much more effective and effic:la t. A \nprrformar:<:c comparison with [I] or [9] is difficult, be-cause t?le analvsis performed, as weii as the \nmachine and compiler usei.. va:y widely. it Sims safe to say that our results 8rC at I,,xst as good. \npal!: ):yp i I. :mti Henglein ii. 0 i study efficient type infer-ence for ar, 0 i >r : calculus. It difrers \niargely from our system (in parn~ l:~>i~ 1 c,:gi~in takes adI-antage of the fact that ob-jfl:t :y:;w \n:: ?: vanant to inlprovcx efficiency). However, some c:f .C~P t-c~ arliques preserired here (garbage \ncollection and yltir :,m:v; t II might carry-over to it. Thii p~~;er dc: ~i!-i,os our work to.wards an \nefficient, stream-lint-c; t~;r~c~ illl.l:.q.:,ce engine in 1.1e presence of subtyping. our 3 -,iiui; \n,, <:11. -&#38;an ex+:;.g type system with sev-eri.1 :,-i .8/i, 4 .,nicati~ stratci,ic+ (canonization, \ngarbage colle( :ie :. > ,rl,,lcm.enting I: ir: im efficient way was not pWh; ,lt [i-iii .i trv Inck of \nx :j\\.~,~tt~,-~~l:icsubstitution algo-riti]IiI, II qIQ ., *'it:ISf? the isF-.;r oi data representation \nWas not :jpt, yi: ;; l:x~l trarisforma7tiin~:: had conflicting effects. We give new algorithms: a very \nefficient minimization algorithm, which eliminates the need for any substitution heuristics, and a coml)ined \ncanonization/garl)age collection algorithm. In addition, we deal with the problem of data representation \nby identifying invariants which should be pre- served during type infercncc: the small terms invariant \nand the mono-polarity invariant. They have beneficial effects in several areas. Some transformations \nwhich improve read-ability, but break :he invariants, are postponed until the type must be displayed \nto the user. The result is a full framework for type inference with sub-typing, with a clean awl simple \ntheory, leading to efficient algorithms. At present, the most promising research topic appears to be \nthe elimination of X-lifting, yielding a system closer to ML, but with greater theoretical c.omplexity. \nReferences [l] Alexander Aiken and Manuel Fihndrich. Making set-constraint based program analyses scale. \nTechnical Report CSD-96-917, Ilniversity of California, Berke-ley, September 1996. URL: http://htt?.ca.berkeley.edu/ \n-manuel/papers/scv96.ps.gz. [2] alexander Aiken ar:tl Edward L. Wnnmers. Solving systems of set conp:traints. \nIn .4ndre Scedrov, editor, Pr-oceedrngs of the :%h Annual IE;EE: Symposium on Logic in Computer Science, \npages 329- 340, Santa Crux, CA, June 1992. IEEE Computer Society Press. URL.: http://http.cs.berkeley.edu/-aiken/ftp/lics92.ps. \n[3] Alexander Aiken, Edward L. Wimmers, and Jens Pals- berg. Optim2ll representations of polymorphic \ntypes with subtyping. Technical Report CSD-96-909, Uni-versity of California Berkeley, July 1996. IJRL: \nhttp: //http.cs.berkelsy.edu~-aiken/ftp/quant.ps. [4] -4lexander S. Illl;t~l and Edward L Wimmers. Type \ninclusion constr;Gntj and type ir1ferinc.e. In Fun&#38;or&#38; ! rogramming 63 Gmputer Architecture, \npages 31-4 1. ACM Press, June 1993. URL: htt?://http.cs.berkeley. edu/aaiken/ftp/fpca93.ps. [5] Alexander \nS. Aiken, Edward L. Wirr.mers, and T. K. Lakshman Soft typing with conditional types. In Prin- ciples \nof Programming Languages, pqes 163-173, Jan- uary 199-4. UR.1,: http://http.cs.berkfGzy.ndu/-aiken/ftp/ \npap194.ps. [6] Jonathan Eifrig. Scott Smith, ar8.d Valery Trifonov. Sound polymo;%c type inferencr fin \nobjects. Tn OOPSLA 95 Conference Proceedinge, ?&#38;une 30(10) of A CM SIGPLAN Nr;li::c~s, pages 169. \n184, 1995. UR5: http://uwu.cs.jhu.edu/\"trifanav/papers/sptio.ps.gz. (71 Jonathan Eifrig, Scott Smith, \nant! Valery Trifonov. Type infcrcncra for -ecursively constrained types and its application to OOP. In \nMathemntical Foundations of Programming Scmmtics, New O&#38;cans, volume 1 of Electronic Notes in Th,eoretical \nComputer Science. El- s&#38;es, 1995. LJRT: http://uuv.elsevior.nl/locate/sntca/ volumel.html. [S] Cormac \nFlanagan ilr1.d Matt,hias Felleisen. Madu-lar and pol:,lr!orphic~ set-based analydis: Theory and pl at \nii<0 ;;>&#38;nical Rel)ol : TRSG-266, Rice Univer- sity, xc\\:. a ,; c l :396. lJRI~:i~t::p://~~~.cs.ric~.edu/CS/FLT/ \nIah: I cat .-'-96-266.ps.gz PI c s I:, ,l:ac i : 1 ?gan and Mat tllias Felleisen. Componen-t;;li set-~)awcl \nanalysis. In Proceedings of the ACM SIG-PLAN 97 Cor&#38;rencr on Programming Language De-sagn and irrkplementation, \nl)agrs 235-248, Las Vegas, NWXla , .jullC 1997. URL: http://wu.cs.rice.edu/CS/PLT/ Puolications/pldi97-ff.ps.gz. \n[lo] F rit., Hc~r gi+in. breaking through the n3 barrier: Faster lil),ji.( L i I i r :i,?:ence. I:. B>- \n:jamin Pierce, editor, IL:,- , , :c , 1 !vorli5ho;j -!I, Fo?olrndations of Ohject-(j ~1$7,.,4 .m i r, \n::!/ .,lI,l/(cg (I; ~(;/.,). i nris, France, January ;ys;, < 1 nttp://nww.cs..i!ldlana.edu/hyplan/pierce/fool/ \nI ecg-EL'. i ._A [II] .l(>h11 E. Hopc~oft. An n k,g R algorithm for minimiz-irib; stztcs i:i a finite \nautomi!ton. In Z. Kohavi, editor, Il ?;r:ory of ~lfnchines and Ccmputations, pages 189-196. .A(~ilrlettlt \n~ Prf+;s., NY, 1971. [12; ,t; i,. ,;.(; ; ;i c i?i.wlution i! : ql~,~tions dans des langages ,I . ,!.( \n: ), L . PhD t cG. riniversitC Paris VII, ir: :fl::i ,:j. i [13] .J<V .; pii I 5. Efticient infdLre:;ce \nof object types. Infor- ;,l.l,,!~:,,,l I,:'0 :hrr;p~tnti~n. 1 ??(?)I 198-209, i995. URL: !ltr3: .;/'wl;c \n:,.pnrdue.edu~~omes/palsberg/paper/ic96-p.ps.gz. [14] L+wc;ois Portier. Simplifyinq subtyping constraints. \nIn &#38;-ocedin7s of the 1996 ACM SIGPLAN International Co,~fertvxx on Funcfionol t rogmmming (ICFP 96), \npages 122---i%, January l!%F. URL: http://pauillac. .\"T fc -;ticr/publia/IC.~P!. : \":,.gz. [15] p/<~(ptc(,i. \n'.' ,- :+. Qpe irLfcw: w m the presence of sub-tpmq: j; i. rllcory to pro, 2cf. PhD thesis, IJniver-s,+.\\ \n1 ;; :'_ .li,iy 1998. 'I1<',: sttp://pauillac.inria.fr/ ~- -:-< 4.: _I :I17 'thesis-fpctti:r.F3,gz. [16] \ni)irlicr R&#38;I!-. Type inference for records in a natural ex-:-eil:;~nn of \\ I I.. !n Carl A. Gunter \nand .John C. Mitchell, f.( itors. 7 t~,~,vZicnl Aspc ct., Clj Object-Oriented Pro-grmnrr,a:~ 0. !)prs, \nSemont~ 1.~ and LnThguage Design. qff 1 I F;.,t>,. i. i : 4% IJR.L:ftp:/./ftp.inria.fr/INRIA/Projecta/ \n,y..; r-a]. '7 ,' -.Rwy/taoopl.p ,r '_ . ;ilcry l;,I ic):-:n~ and Scort Smith. Subtyping con-  Pfd \n_ ?Itcii::l; ( y!);*$. In Proccttl&#38;gs of the Third Inter- G 7~: one: .C~X.: Ann1ysa.s S~1qosium, \nvolume 1145 of < i?itY?. : IY~ 349.--365. SV. September 1996. URL: hft.3: I/w,: 7: ,' hu.ndu/-trifonov/papers/subcon.ps.gz. \n119; .i #ji j:i ,L : lt, pn)yr(, / 8 ; ;E i:.n: for imperative lan-ST : ~: ,. .,; - pero+.rc :s7:rii. \nTechnical Report 93- ! ,.I. i 1.c ( 1 -:. ;r -sity, I+!~rl::~i.,. ! 993. A A full exampIe This appentlis \nshow-s thr type inference engine at work on a sr~~all. but rather typic-al example, nanlely the classic \nmap operation. l hr type language used here has products and extensible variant types (without row variables, \nfor the sake of simplicity). The exprrssion language has corresponding constructs (pairs, data constructors \nand pattern matching). as well as a fix-point operator rec. ret map in function f -> function Nil -> \nNil I Cors (x, rest? -> Cons (f x, map f rest) The tyl)e inference rules indicate that the expression \nhas type 7122. togcthcr with the c0nstraint.s given by figure 9. Let us compute their closure; it is \ngiven by figure 10, in a wav which looks more like a constraint graph: there is one line per tyI)c variable, \nmentioning its lower and upper bountla. It turns out that the closure computation did not in-troduce \nany symbolic U or n constructs. So, in this case, the canonizwtiou algorithm has nothing to do. We can \niiow cornputc polarities. The fix-point computation starts I:y marki1.g tile cr:try point, ~2:. AS positive, \nand then propagntcls marks through the ahove constraint graph. WC find that Vo, 08, ?1,6, 7)) 7, ~20, \n~21, v~p arc positive, while ~2, v:j, v.5, ~7, 7113, f:l~( a-~ ncagative. Given this infornlation, we \ncan perform garbage collrc-tion. Positive (resp. negative) variables 10s~ all of their up- per (.-c>:-i. \nIo~z!~ i :nulds; flirt!t?r-ri0W, constraints involving two ?w:,tl~~lcs Y Bcpt only if thi M-hand one \nis negative and t11: -igo;llt-il.-l:fd one is positivP. This yields we can ~ ,OW l-i . :he minimization \nalgorithm. We compute the larp:tsst c~!i~i I~~IKC relation such that two equivalent variabli,.< h;~. \nn I !I, same polarity. the same successors and predece+<trl L. $#,:I p,.luivalent (~~1 st.ructed bounds. \nIn this case, it ifi cc.:. XC* that the (1 ,1) r-x-trivial equivalence Cl.WSc .5 : ._ t ; 1) and {i:lb,*:;Q.I). \nr4;I.c ti,; .. .?*,,,,!.ing % lg Gil, P,~~ ca11 be viewed as rec- ogmz.~;~~ 2, +~~ :.!l~ unrolled iix-l)<jint. \nIndeed, it essentially COIIS. ,-iii I l!i,i~,. 1I.g I; (@.F(l)) wttt pt.F(t), where E is the type opfmtor \nfti [ Nil I Cons of v5 *t 1 By collapsing the equivalence classes, WC obtain Type simplification is \nover; as far as the internal engine is concerned, this is t.he result. At t.his point, external simlMication \nstrategies may be applied to make the type scheme more readable. We replace each variable with its unique \nbound (except where disallowed by the occur check). Finally, we obtain that. map has type (1% --f ?~a) \n-+ ~13 --t 1116 which is optimal, giver. our rype language. If we want to go a little further, we can \nnotice that the above inequalities can bc :rplaced by equaiitirs. (The proof of correctness is trivial. \nGarbage collectilm would replace these qualities with the original inequalities; since garbage coll&#38;ian \nis correct, thr two type schemes are equivalent.) Thus, map s type could be printed as (v6 + v#) + pt. \nC Nil I Cons of WC,* f I + //t.C Nil I Cons of us *i! 1  \n\t\t\t", "proc_id": "289423", "abstract": "", "authors": [{"name": "Fran&#231;ois Pottier", "author_profile_id": "81100490085", "affiliation": "Fran&#231;ois Pottier, Project Cristal, INRIA Roquencourt, BP 105, 78153 Le Chesnay Cedex, France", "person_id": "PP39077387", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/289423.289448", "year": "1998", "article_id": "289448", "conference": "ICFP", "title": "A framework for type inference with subtyping", "url": "http://dl.acm.org/citation.cfm?id=289448"}