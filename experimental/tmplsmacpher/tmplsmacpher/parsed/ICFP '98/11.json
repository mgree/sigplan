{"article_publication_date": "09-29-1998", "fulltext": "\n Compiling Standard ML to Java Bytecodes Nick Benton Andrew Kennedy George Russell Persimmon IT, Inc. \nCambridge, U.K. {nick,andrew,george}Opersimmon.co.uk Abstract MLJ compiles SML 97 into verifier-compliant \nJava byte-codes. Its features include type-checked interlanguage work-ing extensions which allow ML and \nJava code to call each other, automatic recompilation management, compact com-piled code and runtime \nperformance which, using a just in time compiling Java virtual machine, usually exceeds that of existing \nspecialised bytecode interpreters for ML. Notable features of the compiler itself include whole-program \noptimi-sation based on rewriting, compilation of polymorphism by specialisation, a novel monadic intermediate \nlanguage which expresses effect information in the type system and some in- teresting data representation \nchoices. Introduction The success of Sun Microsystem s Java language [3] means that virtual machines \nexecuting Java s secure, multi-threaded, garbage-collected bytecode and supported by a capable collection \nof standard library classes, are now not just available for a wide range of architectures and operat- \ning systems, but are actually installed on most modern ma-chines. The idea of compiling a functional \nlanguage such as ML into Java bytecodes is thus very appealing: as well as the obvious attraction of \nbeing able to run the same compiled code on any machine with a JVM, the potential benefits of interlanguage \nworking between Java and ML are consider-able. Many existing compilers for functional languages have \nthe ability to call external functions written in another lan-guage (usually C). Unfortunately, differences \nin memory models and type systems make most of these foreign func-tion interfaces awkward to use, limited \nin functionality and even type-unsafe. Consequently, although there are, for ex-ample, good functional \ngraphics libraries which call X11, the typical functional programmer probably doesn t bother to use a \nC language interface to call everyday library func-tions to, say, calculate an MD5 checksum, manipulate \na GIF file or access a database. She thus either does more work than should be necessary, or gives up \nand uses another lan-guage. This is surely a major factor holding back the wider adoption of functional \nlanguages in real world applications, and the situation is getting worse as more software has to operate \nin a complex environment, interacting with com-ponents written in a variety of languages, possibly wrapped \nup in a distributed component architecture such as CORBA, DCOM or JavaBeans. Because the semantic gap \nbetween Java and ML is smaller than that between C and ML, and because Java uses a simple scheme for \ndynamic linking, MLJ is able to make interlanguage working safe and straightforward. MLJ code can not \nonly call external Java methods, but can also ma-nipulate Java objects and declare Java classes with \nmethods implemented in ML, which can be called from Java. Thus the MLJ programmer can not only write \napplets in ML, but also has instant access to standard libraries for 2 and 3D graphics, GUIs, database \naccess, sockets and networking, concurrency, CORBA connectivity, security, servlets, sound and so on, \nas well as to a large and rapidly-growing collec-tion of third-party code. The interesting question is \nwhether ML can be compiled into Java bytecodes which are efficient enough to be useful. Java itself is \noften criticised for being too slow, especially running code which does significant amounts of allocation, \nand its bytecodes were certainly not designed with compila-tion of other languages in mind. There is \nlittle opportunity for low-level backend trickery since we have no control over the garbage collector, \nheap layout, etc., and the requirement that compiled classes pass the Java verifier places strict type \nconstraints on the code we generate. Furthermore, current Java virtual machines not only store activation \nrecords on a fixed-size stack, but also fail to optimise tail calls. Thus the initial prospects for generating \nacceptably efficient Java bytecodes from a functional language did not look good (our first very simple-minded \nlambda-calculus to Java translator plus an early JVM ran the nfib benchmark 40 times slower than Moscow \nML), and it was clear that a practical ML to Java bytecode compiler would have to do fairly extensive \noptimisations. MLJ is still a work-in-progress, and there is scope for significant improvement in both \ncompilation speed and the generated code (in particular, the current version still only optimises simple \ntail calls), but it is already quite usable on source programs of several thousand lines and pro- duces \ncode which, with a good modern JVM, usually out-performs the popular Moscow ML bytecode interpreter. \n 2 Overview 2.1 Compiler phases MLJ is intended for writing compact, self-contained appli-cations, applets \nand software components and does not have the usual SML interactive read-eval-print top level. Instead, \nit operates much more like a traditional batch compiler. The structures which make up a project are separately \nparsed, typechecked, translated into our typed Monadic Interme-diate Language (MIL, see section 4) and \nsimplified. These separately compiled MIL terms are then linked together into one large MIL term which \nis extensively transformed before being translated into our low-level Basic Block Code (BBC, see section \n7). Finally, the backend turns the BBC into a collection of compiled Java class files, which by default \nare placed in a single zip archive. This whole-program approach to compilation is unusual, though not \nunique [26, 221. It increases recompilation times considerably, but does allow us easily to produce much \nfaster and (just as importantly for applets) smaller code. We monomorphise the whole program and can \nperform trans-formations such as inlining, dead-code elimination, arity-raising and known-function call \noptimisation with no regard for module boundaries, so there is no runtime cost associated with use of \nthe module system. Appendix A contains an example of JVM code generated by MLJ. 2.2 Compilation environment \nMLJ can be run entirely in batch mode or as an interactive recompilation environment. Top-level structures \nand signa- tures are stored one-per-file, as in Moscow ML (MLJ 0.1 doesn t implement functors). All compilation \nis driven by the need to produce one or more named Java class files: for an application this is usually \njust the class containing the main method, whilst for an applet it is usually a subclass of java. awt \n.Applet. Once these root classes and their ex-ported names have been specified, the make command com-piles, \nlinks and optimises the required structures using an automatic dependency analysis. There is a smart \nrecompi-lation manager, similar to SML/NJ s CM [7], which ensures that only the necessary structures \nare recompiled when a file is changed, though the post-link optimisation phase is always performed on \nthe whole program. Typically, the re-compile time (relinking all the translated MIL structures from memory, \noptimising and generating code) is around two thirds of the total initial compile time (which also in- \ncludes parsing, typechecking and translation into MIL). During compilation, the compiler not only typechecks \nthe ML code, but if any external Java classes are men-tioned, their compiled representations are read \n(typically from within the standard classes.zip file) to typecheck and resolve the references. 3 The \nLanguage ML3 compiles the functor-free subset of SML 97 [14] (in- cluding substructures, the new where \nconstruct, etc.), plus our non-standard extensions for interlanguage working with Whole program compiler \nsounds a bit naive, so we prefer to think of it as a post-link optimiser , which has a much more sophis-ticated \nring :-1 Java. A large subset of the new standard Basis Library has been implemented. The interlanguage \nfeatures bring Java types and val-ues into ML, whilst enforcing a separation between the two (though \na Java type and an ML type may well end up the same in the compiled code). External Java types may be \nreferred to in ML simply by using their Java names in quo- tation marks. Thus java. awt . Graphics * \nint is the type of pairs whose first component is a Java object representing a graphics context and \nwhose second compo-nent is an ML integer. An important subtlety is that, whilst in Java all pointer (reference) \ntypes implicitly include the special value null, we chose to make their quoted Java type names in ML \nrefer only to the non-null values. Where a value of Java reference type t may be passed from Java to \nML code (as the result of an external field access or method call, or as a parameter to a method implemented \nin ML), then that value is given the ML type t option with the value NONE corresponding to Java null \nand values of the form SOME(v) corresponding to non-null values. Similarly we guarantee that any ML value \nof type t option will be represented by the an element of the underlying Java class. This complication \nallows the type system to catch statically what would otherwise be dynamic NullPointerExcaptions and \nalso gives the compiler more freedom in choosing and sharing data representations (see Section 6). The \nbuiltin structure Java includes ML synonyms for common Java types and coercions between many equivalent \nML and Java types, for example Java. tostring, which con-verts a Java String into an ML string. With \none exception (fromWord8), these all generate no actual bytecodes, and are included just to separate \nthe two type systems securely in the source. MLJ code can perform basic Java operations such as field \naccess, method invocation and object creation by using a collection of new keywords, all of which start \nwith an under- score - (which fits well with the existing lexical structure of SML), and we also use \nquotation marks for Java field and method names. Thus, for example let type colour = java. awt .Color \nval grey = valOf (-getfield colour gray ) in Java. toInt (-invoke getRed (grey) ) end makes colour be \nan ML synonym for the Java class Color and then gets the static field called gray from that class. MLJ \nreads the compiled Java class file which states that the field holds an instance of the class Color and \nso in-fers the type java. awt . Color option for the -getf ield construct, because it s an external \nvalue which might be null. We then invoke the virtual method getRed on the returned colour value to get \nthe Java int value of its red component, which we convert into an ML int. The valOf is required to remove \nthe option from the type of the re-turned value (it raises an exception if its argument is NONE) because \nMLJ does not allow method invocation on possibly- null Java values. The new constructs such as -getf \nield and -invoke have essentially the same static semantics as their equivalents in Actually, version \n0.1 departs from the Definition in four places. The only significant one is that arithmetic operations \ndo not raise the Overflow exception. datatype Behave = B of unit -> Behave -classtype MLButton -extends \njava.awt.Button ( -private -field behaviour : Behave -public -method action (e: java.awt.Event option, \no : Java.Object option) : Java.boolean = let val B(b) = -getfield behaviour -this in (-putfield behaviour \n(-this, b 0) ; Java.fromBool true) end -constructor (name : string, b:Behave) {-super (Java. fromString \nname) ; behaviour = bl Figure 1: Generating a Java class from MLJ the Java language. ,4t a virtual method \ninvocation, for ex-ample, the compiler searches up the class hierarchy for a method matching the given \nname and argument types, ap-plying the same rules for finding the most specific method by possible coercions \non the arguments as Java does. (ML polymorphism isn t allowed to confuse things further as all uses of \nthese new constructs must he implicitly or explicitly monomorphic.) This makes it easy to convert existing \nJava programs or code fragments into MLJ hut is not intended to make MLJ a fully-fledged object-oriented \nextension of SML; in particular, inheritance does not induce any subtyping re-lation on ML types. MLJ \nstructures can also declare new Java classes with fields and methods having a mixture of ML and Java \ntypes and methods implemented in ML. In fact, all programs must contain at least one such class for the \nJava runtime system to call into; otherwise no ML code could ever get called. The extensions for declaring \nclasses can express most of what can he expressed in the Java language, including all the access modifiers \n(public, private, etc.), though there are some natural restrictions, such as that static fields of ML \ntype (or non-option .Java reference type) must have initialisers, since there are no default values of \nthose types, and overloading on ML types is forbidden (as two ML types may end up rep- resented as the \nsame Java type). Class definitions in struc- tures are all anonymous (there are ML type names hound to \nthem, hut no Java class names) so a class that is referenced purely by ML code will end up with an internal \nname in the compiled program, and so not he directly accessible from external Java code.3 However, as \nwe ve already mentioned, at least one top-level class has to he exported with a given Java name. Since \nexported classes can he accessed from Java, there are some not entirely trivial further restrictions \nconcerning the t,ypes and access modifiers of their fields and methods which are necessary to ensure \nthat anything which is visible, either directly or by inheritance, to the external Java world is of Java \nprimitive or optioned Java class type. Just to give the flavour of how Java classes may be gen- erated \nfrom MLJ, Figure 1 shows the definition of an ML button class, similar to one which might form part of \na functional GUI toolkit. MLButton subclasses the standard Java Button class and has an instance variable \nwhich is of a higher-order ML datatype Behave. When the button is pressed, its action method is called, \nwhich causes its he-haviour function to be called (presumably with some side-effects), returning a new \nhehaviour which is stored in the instance variable ready for the next click. The constructor (for which \nour syntax is particularly baroque) is called with an ML string and the initial behaviour. It starts \nby call-ing the superclass initialiser with a Java String and then initialises the instance variable \nwith the supplied behaviour. 4 MIL The Monadic Intermediate Language, MIL, is the heart of the compiler. \nMIL is a typed language, inspired by Moggi s comnutational lambda calculus 1151. which makes a distinc- \ntionbetween computations and ;alLes in its type system. It has impredicative System-F-style polymorphism \nand refines the computation type constructor to include effect informa- tion. MIL also includes some \nslightly lower level features so that most of the optimisations and representation choices we wish to \nmake can be expressed as MIL-to-MIL transfor-mations. These include not-quite-first-class sequence types \n(used for multiple argument/multiple result functions and flat datatype constructors), Java types (used \nnot just for interlanguage working, but also to express representations for ML types) and three kinds \nof function: local, global and closure. Typed intermediate languages are now widely accepted to he A \nGood Thing [19, 22, 111. Types aid analysis and op- timisation, provide a more secure basis for correct \ntransfor-mations, and are required in type-directed transformations such as compilation of polymorphic \nequality. They also help catch compiler bugs! In our case it seems especially natural, as the Java hytecode \nwhich we eventually produce is itself typed. The use of computational types in the intermediate lan-guage \nis more unusual, though similar systems have recently been proposed in [23, 11, 241 and the use of a \nmonadic in-termediate language to express strictness-based optimisa-tions was proposed in [4]. The separation \nof computations and values gives MIL a pleasant equational theory (full p and 11 rules plus commuting \nconversions), which makes cor-rect rewriting simpler. Order of evaluation is made explicit (much as it \nis in CPS-based intermediate representations [l]), and our refinement of computation types into differ-ent \nmonads gives a unified framework for effect analysis and associated transformations. 3Unlessit usesJava \ns introspection capabilities, which we consider to be cheating... v ::= x,y,f I ; vl,... ,Vn) iniV 1 \n1 i 1 1 in,V -IGV vi fold, V unfold V r,V M ::= ( 1 I al G let Zc e=Ml VP ref V !V in MZ 1 zi;e=v z \n1 try 2 + Ml handle y + M3 in Mz case V of inlZ1 =+ MI;. ; in,&#38; * M, caseVofin,,Zl *MI;... ;inxn&#38; \n*M,;-=+ caseVofcl *Ml;... ;cn * M,;-+-M leth~fl(~ l:i;)=M1:yl;...;f,(~~::~)=M,:y,inM M Figure 3: Terms \nin MIL value variables constant of base type tuple injection into sum injection into exception type \nabstraction type application recursive type introduction recursive type elimination projection trivial \ncomputation evaluation function application reference creation dereferencing assignment throw an exception \nevaluate and catch case on sum case on exception case on base type (recursive) function declaration int \n1 char ( base types 71 x  x Tn product ?1 + + 7:, sum l-+-y function type r ref reference type exn exception \npolymorphic type _ I ::= pt.7 (71,. ,Tn) recursive type vector of types ; ::= TE(F) computation type \nE C {I, throws, reads, writes, allots} effects Figure 2: Types in MIL 4.1 Types and terms Types in MIL \nare divided into value types (ranged over by r) and computation types (ranged over by y) as shown in \nFigure 2.4 Value types include base types (int, char, etc.), products, function types with multiple arguments \nand re-sults, sum types with multiple argument summands, refer-ence types, polymorphic types and recursive \ntypes. Compu-tation types have the form TE(?), indicating a computation with result types r and effect \nE. The subset relation on effects induces a subtyping relation on types in an unsur-prising way. Note \nthat because we are only interested in compiling a call-by-value language, we have restricted func-tion \ntypes to be from values to computations. Terms are also divided into values (ranged over by V) and 4Some \nsimplifications have been made for this presentation. In particular the implementation has mutual recursion \nover multiple types; also, lower-level features that capture Java representations are omit,ted. computations \n(ranged over by M) as shown in Figure 3. All evaluation happens in let or try. The Moggi let construct \nevaluates the computation term Ml and binds the result to x in the scope of the computation term Mz. \nThe construct try Z -+ MI handle y + M3 in M2 generalises this by providing a handler M3 in which the \nex-ception raised by Ml is bound to a variable y and the con-tinuation M3 is not evaluated. (It is interesting \nto note that this construct cannot be defined using let and the more con-ventional MI handle y + M2 without \nrecourse to a value of sum type). The construct val ? allows a value (or multiple values) to be treated \nas a trivial computation.  4.2 Translation from SML The initial translation from typed SML to MIL is \nessentially Moggi s call-by-value translation [15]. For example, a source application ezpl exp2 translates \nto letz+M1inlety+Mzinzy where Ml and M2 are the translations of expl and expcpz. The translation also \nexpands out certain features of the source language that are not present in the target: patterns are \ncompiled into flat case constructs, records are translated as ordinary tuples (with fields sorted by \nlabel) and even the structure constructs of the SML module language are compiled into ordinary tuples \nof values. (The latter relies on the impredicative polymorphism in MIL. An alternative [20] would be \nto stratify the intermediate language in a similar way to the stratification of SML into code and module \nlev-els.) Uses of polymorphic equality are also compiled away, us-ing a simple dictionary-passing style. \nOne might expect that equality would be an ideal candidate for exploiting Java s virtual methods, overriding \nthe equals method in classes representing ML types. But that would prevent us sharing representations \nbetween ML types with different equality functions, so doesn t seem worth doing. 5 Transformations Most \nof the compiler s time is spent applying a set of MIL-to- MIL transformations to the term representing \nthe whole pro- gram. Some of these are considered to be general-purpose simplification steps, and are \napplied at several stages of the compilation, whilst others perform specific transformations, such as \narity-raising functions. Most of the transformations are obviously meaning-preserving rewrites5, but \nthe tricky part is deciding when they should be applied. At the moment, most of these de-cisions are \ntaken on the basis of some simple rules, rather than as the result of any sophisticated analysis. Some \nof these rules are type-directed whereas others involve simple properties of terms, such as the size \nof a subterm or number of occurrences of a variable. The effect inference is currently rather naive, \nparticularly with regard to recursive functions and datatypes, so there are a very small number of places \nin the basis where we have annotated computations as pure so that they may be dead-coded in programs \nin which they are not referenced. 5.1 Simplification The most basic of the transformations are essentially \njust the pure p and 71 reductions and the commuting conver-sions one obtains from the proof theory of \nthe computa-tional lambda calculus [5], adapted so that large or allo-cating terms are not duplicated \n(see Figure 4). The re-ductions are genuine simplifications whilst the commuting conversions are reorganisations \nwhich tend to expose fur-ther reductions. MLJ applies the commuting conversions exhaustively to obtain \na CC-normal form from which code generation is particularly straightforward. Doing this sort of heavy \nrewriting on a large term can be expensive, partic-ularly when it is done functionally as the heap turnover \nis then very high. Our current simplifier uses a quasi-one-pass algorithm similar to that described by \nAppel and Jim in [2]: it maintains an environment of variable bindings, a cen- sus to count variable \noccurrences and a stack of evaluation contexts to perform commuting conversions efficiently. This algorithm \nis several times faster than our first version, but simplication still ends up being the most expensive \nphase because it is repeated at several stages during compilation - the total time spent in the simplifier \nis typically around half the recompile time. The validity of certain rewrites depends on effect infor-mation \nin the types. Some of these are shown in Figure 5. 5.2 Polymorphism Most implementations of SML compile \nparametric polymor-phism by boxing, that is, by ensuring that values of type t 5Not that we re claiming \nto have justified them formally with re-spect to a semantics for the full language! case-beta: -e case \niniV of inl&#38; j MI; . ; inn&#38; * M, -let Zi *val C in Mi let-eta : let2t=Minvall -bM let-let-cc \n: letZ~e=(let&#38;*M1inM2)inM3 ?~let&#38;*M~inlet&#38;GM2inM3 let-case-cc : let c+case Vof in121 =+ Ml \n; ... ; in,&#38; + M, in M u let f(y3 = M in case V of inlZ1 =+ lety +Ml infa; ... ; in,&#38;, + let \ny e M,, in f y Figure 4: Some proof-theoretic rewrites dead-let : let cZ* MI in MZ -M2 if 3c not free \nin M2 and MI : T=(l) for E c {allots, reads} dead-try : try Z S= Ml handle y =+ M2 in M3 -let Z+ Ml \nin Ms if Ml : TE(F) where throws 4 E hoist-let : lety +Min caseVofinl&#38; *Ml; ... ; in,&#38; *M, - \ncase V of inI&#38; * Ml ; ... ; ini&#38; =S let y eM in Mi ; ... ; in&#38;, + M, if y free only in Mi \nand M : TE(F) for E s {allots, reads} Figure 5: Some rewrites dependent on effect information inside \na value of type Vt.r are represented uniformly by a pointer (they are boxed ). In Java, the natural way \nto box objects is by a free cast to Object, and the natural way to box primitive types is to create a \nheap-allocated wrap-per object and cast that to Object. Unboxing then involves using Java s checkcast \nbytecode to cast back down again and in the case of primitive types, extracting a field. Done naively, \nthis kind of boxing can be extremely inefficient and there are a number of papers which address the question \nof how to place the coercions to reduce the cost of boxing (e.g.[13, 91). For an early version of our \ncompiler we imple- mented a recent and moderately sophisticated graph-based algorithm for coercion placement \n[12]. Whilst the graph-based algorithm worked fairly well, we have more recently taken a more radical \nand straightforward approach: removing polymorphism entirely. This is possible firstly because we have \nthe whole program to work with, and secondly because it is a property of Standard ML that the finite \nnumber of types at which a polymorphic function is used can be determined statically. In languages with \npoly-morphic recursion (such as recent versions of Haskell) this property does not hold: the types at \nwhich a function is used may not be known until run-time. Each polymorphic function is specialised to \nproduce a separate version for each type instance at which it is used. In the worst case, this can produce \nan exponential blowup in code size, but our experience is that this does not ac-tually happen in practice. \nThree reasons can be cited for this. First, we specialise not with respect to source types but with respect \nto the final Java representations, for which much sharing of types occurs. For example, suppose that \nthe filter function is used on lists containing elements of two different datatypes, both of which are \nrepresented by the universal sum class discussed in Section 6. Then only one version of filter is required.6 \nSecond, boxing and un-boxing coercions introduce a certain amount of code blowup of their own and this \nis avoided. Third, polymorphic func-tions tend to be small, so the cost of duplicating them is often \nnot great. Indeed, not only are many polymorphic functions inlined away prior to monomorphising, but \nafter monomorphising it is often the case that a particular in-stance of a function is used only once \nand is consequently inlined and subject to further simplification.  5.3 Arity raising Heap allocation \nis expensive. We try to avoid creating tuples and closures by replacing curried functions and functions \naccepting tuples with functions taking multiple arguments, and to flatten sums-of-products datatypes \nby using multi-ple argument constructors. We also remove the type unit (nullary product) entirely. These \ntransformations should ideally be driven by information about how values are used, but we find that fairly \nsimple-minded application of type isomorphisms such as the following N 71 x 72 -+ y -(71,T2) + y 71 + \nT0(72 + y) = (n,n) -+ y 71 x rz + 7-3 x 74 E (71,T2) + (75,T4) 7- x unit 2 T unit + y z $T; T,(unit) \nE This can itself be regarded as a kind of boxing, since we re using uniform (shared) representations \nfor certain ML types, but note that we never box primitive types. Object   I\\ /\\ I\\ F, F, SI s, El \nE, Figure 6: Java classes representing MIL types combined with the other rewrites, produces a significant \nimprovement in most programs. For example, the Boyer-Moore benchmark runs in 4 seconds with all optimisation \nen-abled but takes 6 seconds when tuple-argument arity raising is turned off. Worse, it crashes with \na stack overflow when de-currying is also disabled because MLJ is then unable to use a got0 instruction \nin place of a tail call. Observe that de-currying depends upon effect informa-tion in the types (including \ntermination), for otherwise it would be unsound. 6 Data representation 6.1 ML base types Most ML base \ntypes have close Java equivalents, so ML . __- ints are represented by Java ints, and ML strings are \nrepresented as Java Strings. There are a couple of small differences in the semantics of these types \nor their operations which have led us to diverge from the ML definition: integer arithmetic does not \nraise Overflow and for us the Char a.nd String structures are the same as WideChar and WideString since \nJava is based on Unicode. 6.2 Products Each distinct product type ri E ri x x rn is represented by a \ndifferent class Pi whose fields have types 71, , r, (see the class hierarchy in Figure 6). Because we \nmonomorphise, representations can be shared by sorting the fields by type, for example using the same \nclass for the type int x string as for stringx int, but the current version of the compiler doesn t do \nthis yet. 6.3 Sums The natural object-oriented view of a sum type ri +. .+m is to represent the n summands \nas n subclasses of a single class and then to use method lookup in place of case. For example, one would \nrepresent lists by a class List with sub-classes Nil (with no fields) and Cons (with a field for the \nhead and a field for the tail). The length function would compile to a method whose implementation in \nNil simply returns zero and whose implementation in Cons returns the successor of the result of invoking \nthe method on the tail. Whilst this technique is elegant, it is not necessarily effi-cient Typical functional \ncode would generate a large num-ber of small methods; moreover it is necessary to pass in free variables \nof the bodies of case constructs as arguments to the methods. If the JVM had a classcase bytecode then \nit would be possible to use this instead of method invocation. Unfortu-nately it does not: one must check \nthe classes one at a time (using instanceof) and then cast down (using checkcast). A variation on this \nidea is to store an integer tag in the superclass and to implement case constructs as a switch on the \ntag followed by a cast down to the appropriate subclass. We take this a stage further, using a single \nsuperclass S for all sum types with a subclass S, for each type of summand. This reduces the number of \nclasses required for summands; moreover, because only a single class is used to represent most sum types \noccurring in the source program, further sharing of representations is obtained in types constructed \nfrom sum types. This universal sum scheme is used in general, but for two special cases we use more efficient \nrepresentations as described below. Enumeration types: for datatypes whose constructors are all nullary \nwe use the primitive integer type. l+ types: In Java, variables of reference type con-tain either a valid \nreference to an object or array, or the value null. We make use of this for types of the form () + T \nwhere r can itself be represented by a non- null class or array type. For example, the SML type (int \n*int 1 opt ion is implemented using the same class as would be used for int*int, with NONE represented \nby null. The type int list is implemented by a single product class with fields for the head (an integer) \nand tail (the class itself) with nil represented by null. If 7 is primitive then a product class is used \nto first wrap the primitive value. What if 7 is itself of the form () + r ? Then we create an additional \ndummy value of appropriate class type to represent the extra value in the type. For example, a value \nSOME x with the SML type int list option is represented in the same way as would be x of type int list, \nbut NONE is an extra value created and stored in a global variable when the class is initialised. Reference \ntypes In general a reference type 7 ref is simply represented by a unary product class. For references \ncreated at top-level (i.e. not inside functions) that are not used in a first-class way (assigned to \nand dereferenced but not compared for equality or passed around), we use static fields in a distinguished \nclass G, in other words, global variables. In the future we hope to perform escape analysis in order \nto use local variables for reference types where possible. 6.5 Exceptions In SML the type exn has a \nspecial status in that it is ez-tensible. Exception declarations create fresh distinguishable exception \nconstructors; in the operational semantics this is formalised by the creation of fresh names. However, \nfor ex-ceptions declared outside of functions there will be a fixed finite set of names that can be determined \nat compile-time. We exploit this by representing each such exception con-structor by a separate class \nEi that subclasses E, the class of ML exceptions . In contrast to sums, we do not use the same class \nfor exception constructors whose argument types are the same. ML s handle construct then fits better \nwith the JVM try-catch construct where the class of the excep-tion is used to determine a block of code \nto execute. For generative exception declarations that appear inside functions, we generate a fresh integer \ncount from a global variable and store this in a field in the exception constructor object. This field \nis tested against in exception handlers and case constructs. Exceptions also give a nice anecdotal example \nof the sort of low-level Java-specific tweaking that we ve found to be necessary in addition to high-level \noptimisations. In an early version of the compiler we noticed that certain pro-grams which used exceptions \nas a control-flow mechanism ran hundreds of times more slowly than we would have ex-pected. The problem \nwas tracked down to a feature of Java: whenever an exception is created, a complete stack trace is computed \nand stored in the object. The solution was simply to override the fillInStackTrace method in the ML exception \nclass so that no stack trace is stored. 6.6 Functions As mentioned earlier, functions in MIL are divided \ninto lo-cals, globals and closures. These are used as follows: 0 Functions that only ever appear in tail \napplication po-sitions sharing a single continuation can be compiled inline as basic blocks. Function \napplication is compiled as a goto bytecode. Incidentally, this is one good rea-son for comniline: to \nJVM bvtecodes rather than Java source -the got\\ instruction is not available in Java. For example: let \nval f = fn x => some-code in if 2<3 then f(z) else f (w+z) end The body of f is simply compiled as a \nblock of code and the two calls to f are compiled as jumps. It is sometimes even possible to transform \na non-tail function application into a tail one. Consider the fol-lowing fragment of ML: let val f = \nfn x => some-code val y = if 2<3 then f(z) else f(w+z) in some-more-code end Assuming that f does not \nappear in the expression some-more-code, then this continuation can be moved into the definition of f \nand the calls to f implemented by jumps. Other functions appearing only in application positions are \ncompiled as static Java methods in a distinguished class G. Function application is implemented by the \ninvokestatic bytecode, unless it is a recursive tail call to itself, in which case goto is used. The \nremaining functions are used in a higher-order way and so must be compiled as closures. There are a num- \nber of ways that this can be achieved. The most ob-vious is to generate for each function type an abstract \nclass with an abstract app method, and then to sub-class this for each closure of that type, storing \nthe free variables as instance variables in the object. This is rather wasteful of classes, using one \nper function type and closure appearing in the program. We currently use a different (and at first sight \nrather alarming) scheme. Instead of a single app method, we use different method names for different \nfunction types. There is a single superclass F of all functions, with dummy methods for each possible \napp method. Then closures with the same types of free variables but dif-ferent function types share subclasses \nof F. It is even nossible for the actual closure obiects to be shared, if &#38;ir free variables are \nthe same but app methods are different. For example: fun f (x:string) = let val fl = fn y:int => (x,y) \nval f2 = fn z:string => (x,2) in end If closures are required for f 1 and f 2 then they can share the \nsame closure class as their free variable types are the same but function types are different. More-over, \nthe values of their free variables are the same so the same object can be used for both, saving an allo-cation. \n A simple flow analysis is used to decide how each func- tion should be compiled. A more sophisticated \nflow analysis would not only allow us to identify more known functions, but would also refine our type-based \npartitioning of appli- cation methods, allowing more sharing of classes between closures. 7 BBC The Basic \nBlock Code (BBC) is a static single-assignment representation of the operations available in the Java \nvirtual machine, which abstracts away from certain instruction se-lection details, including the distinction \nbetween stack and local variahles. Normal form MIL (with all commuting con-versions applied) is translated \ninto BBC (which also includes some information about effects and which object fields are mutable) and \nthe backend then orders and selects instruc-tions to turn that into real bytecode. Currently the hack-end \nconstructs a dependency DAG from the BBC and then works from the top down, using the stack where possible \nbut (mostly) storing intermediate results in local variables where they are used more than once, or where \nordering constraints make their immediate use on the stack impossible. After that, there is a pass in \nwhich local variable numbers are re- assigned, so that the number of copies between basic blocks is minimised, \ncombined with a standard register-colouring phase in which we try to minimise the total number of local \nvariables used. This scheme produces code which is respectable but far from optimal. Data passed between \nbasic blocks is never left on the stack but is instead passed via local variables. Within basic blocks \nthemselves, the JVM s stack is only used in a fairly simple-minded way. This causes too many local vari-ables \nto be used and leads to code being somewhat larger than we would like. Heavy optimisation of the backend \nis Benchmark 1 MLJ ) Moscow 1 SML/NJ Nfib I 3.2 0.5 0.4 Quicksort 5.7 1.3 0.7 Life 10.6 1.0 2.4 Knuth-Bendix \n20.6 1.8 5.3 Mandelbrot 4.3 0.5 0.7 Boyer-Moore 51.8 5.0 8.0 FFT 13.7 1.5 2.4 Table 1: Compile times \n(seconds) probably not justified from the point of view of execution speed, given that the bytecodes \nwill usually be recompiled by a JIT-compiling virtual machine which does its own in-dependent mapping \nof stack locations and local variables to registers and memory, but we are also keen to reduce the size \nof the bvtecodes. We are currently developing an im-proved backend which will make muchmore intelligent \nuse of the stack. 8 Current Status and Performance MLJ 0.1 currently comprises about 60,000 lines of \nSML, written using SML/NJ version 110, plus the Basis li-brary code. It is freely available over the \nweb (at http://research.persimmon.co.uk/mlj/) .~ as an SML/NJ heap image for Solaris, Win32, Linux and \nDigital Unix with the Basis code compiled in. Although there is scope for further improvement, MLJ is \nalready useful in real applications. Internal projects at Persimmon using MLJ include Writing functional \nSGML/XML stylesheets which can be downloaded into web browsers or run on servers. This involves a lot \nof interlanguage working, includ-ing with Javascript (using Netscape and Microsoft s browser-specific \nJava classes) and with third-party Java XML parsers. Implementing a graphical functional language for \nfil-tering and classifying events from web servers. This involves interworking with a third-party graph \neditor written in Java. We have a number of nice demonstrations, including Paul-son s Hal theorem prover \nfor first-order logic [18] compiled with some third-party Java terminal code to produce an ap- plet, \nseveral functional programs with graphical user inter-faces and some which access an Oracle database \nvia Java s JDBC API. The largest program which we have successfully compiled is around 12,000 lines (a \ncompiler for ASN.l, pro-ducing C++). 8.1 Compile times The compile times for a range of standard SML \nbenchmark programs are shown in Table 1. All timings were taken on a 200MHz Pentium Pro with 64MB of \nRAM running Windows NT4.0. We have compared a recent (July 1998) internal version of MLJ (O.le) with \nMoscow ML 1.42 and SML/NJ 110. Benchmark MLJ ( Moscow ( SML/NJ Nfib 3.7 3.4 310 Quicksort 7.1 3.8 346 \nLife 11.4 5.9 337 Knuth-Bendix 24.6 9.8 360 Mandelbrot 4.2 3.7 316 Boyer-Moore 25.4 39.1 439 FFT 15.4 \n6.5 374 Table 2: Code size comparisons (kilobytes)  8.2 Code size Table 2 lists the sizes of compiled \ncode produced by the three compilers. To obtain a (roughly) fair comparison, each excludes the run-time \nsystem. For MLJ, the total size of the class files is given (so this excludes the Java interpreter required \nto run it). For Moscow ML, the size of the hytecode file alone is given (so this excludes the camlrunm \ninterpreter required to run it). For SML/NJ, the size of the Windows heap image produced by exportFn \nis given (so this excludes the run. x86-win32 run-time system required to run it). 8.3 Run times Some \npreliminary benchmark times are shown in Table 3. All timings were performed on the same machine as the \ncompilation benchmarks and we again compare MLJ O.le with Moscow ML 1.42 and SML/NJ 110. The run times \ndo not include start-up time for the run-time system. Four different Java implementations were used to \nrun the code compiled by MLJ: java NT: the latest version (1.2 beta 3) of Sun s Java Development Kit \nrunning under Windows NT4.0 with Symantec s JIT (3.00.023(x)) enabled; jview NT: the latest version of \nMicrosoft s JIT compiler (build 2613) running under Windows NT4.0; kaffe Linux: the latest version (0.9.2) \nof Tim Wilkin-son s Kaffe JVM with JIT enabled, running under Red-Hat Linux 5.0; java Linux: Steve Byrne \ns 1.1.6~2 port of Sun s inter- preting JVM running under RedHat Linux 5.0. To illustrate the effect \nthat initial heap size can have on performance, Sun s JVMs were tested twice: firstly with the default \ninitial heap of 1MB and secondly with the heap starting at 30MB. 8.4 Interpretation of the results As \nusual, the details of these small-program benchmark fig-ures should be treated with some scepticism, \nbut it s possible to make some broad generalisations. The first thing to note is that MLJ compile times \nare very high (between 4 and 11 times slower than Moscow ML and between 4 and 8 times slower than SML/NJ), \nthough it s worth reiterating that the recompile times, which are the important numbers for software \ndevelopment, are typ-ically a third less than the total compile times given here. But it s hardly surprising \nthat extensive functional rewriting of the whole program turns out to be a costly compilation technique \n-if SML/NJ is given a whole program as a sin- gle file, then its compile times are often higher than \nMLJ s. Our intermediate language certainly uses more space than a more traditional untyped lambda calculus \nwould, firstly because we are carrying types around and secondly because the computational lambda calculus \ntranslations are inher-ently more verbose. This slows compilation by increasing heap turnover. The current \nparser also contributes to long compile times as it uses parser combinators rather than be-ing table-driven. \nSecondly, Java Virtual Machines vary widely in perfor- mance. A good JIT compiler produces significant \nspeedups, but the current state of the art is that the fastest JITs also have bugs. Microsoft s Win32 \nJIT is generally quite fast but has a fundamental bug that sometimes causes operations to be unsoundly \nreordered. Luckily, we have been able to identify the problem sufficiently precisely to add a compiler \noption to produce slightly less efficient code which avoids the bug. The current version of Symantec \ns JIT is some-times very good but has a number of serious bugs which often prevent it, from running our \ncode. These problems in-dicate a pragmatic (though we hope temporary!) drawback of clever compilation \nof other languages to Java bytecodes - most Java compilers produce fairly naive, stylised hyte-codes \nwhereas MLJ produces rather more contorted byte-codes which, whilst perfectly legal according to the \nJVM specification, could not have been produced by any Java compiler. This tends to uncover bugs which \nhave not have been found by JVM implementers who have only tested against the output of existing Java \ncompilers. In general, MLJ code run with a JIT compiler tends to have particularly good performance (even \nbetter than SML/NJ) on heavily numeric benchmarks such as Nfib, Mandelbrot and FFT. This is unsurprising, \nas our monomor- phisation should allow such code to be easily translated by a JIT into much the same \ncode as would be generated by a naive C compiler. More typical functional code which does a lot of heap \nallocation (e.g. Quicksort, Boyer-Moore and Life) tends to run rather more slowly, showing that storage \nman-agement in JVMs is still fairly poor (we suspect that the fact that increasing the initial heap size \nmakes a significant differ-ence to Quicksort, Knuth-Bendix and Boyer-Moore but not to Life when running \non the Sun/Symantec JVMs indicates inefficient heap expansion rather than just slow garbage col-lection \nper se). The comparison with SML/NJ on the Life benchmark is particularly interesting -taking the bench-mark \nas originally written, our whole program optimisa-tion allows us to specialise representations, including \nuses of polymorphic equality, and run up to 3 times faster than SML/NJ. However, when the program is \nconstrained with a minimal signature, SML/NJ can also specialise and runs nearly 4 times faster than \nthe best MLJ can manage. The particularly poor performance of Microsoft s JIT (and unim- pressive performance \nof the others) on the Knuth-Bendix benchmark seems to be due to the fact that as well as doing a good \ndeal of allocation, it makes heavy use of exceptions. The code produced by MLJ is impressively compact \nbut not quite as small as that produced by Moscow ML. Al-though Moscow has the advantage of a bytecode \nspecifically designed for ML, we expect to be able to narrow the gap in future. We have already mentioned \nongoing improvements to our backend, but just as significant is the non-trivial space overhead associated \nwith the fairly large number of distinct Java classes produced by MLJ. For example, the Knuth-Bendix \nbenchmark produces a total of 42 classes and over Benchmark MLJ Moscow SML/NJ java Java jview kaffe Java \nlava NT LinuxNT NT Linux Linux 30MB 30MB Nfib 0.8 0.9 0.9 1.7 5.1 5.1 8.2 1.3 Quicksort t8.5 $25.1 $17.7 \n109.1 35.7 18.2 21.8 0.9 Life 7.0 7.1 6.3 16.3 32.3 31.9 38.3 +18.9 Knuth-Bendix t82.1 37.0 10.7 426.8 \n63.0 26.9 10.0 2.4 Mandelbrot 32.7 $217.9 t217.9 167.4 217.2 217.4 322.7 41.9 Boyer-Moore 4.2 $6.8 0.9 \n37.3 8.3 3.3 2.1 0.6 FFT 15.5 11.8 12.3 28.8 71.3 71.8 441.6 k28.7 t requires compilation with MLJ s \nmicrosoftbug switch set to avoid bug in the Microsoft JIT $ program crashed due to a bug in the Symantec \nJIT; timing is with JIT disabled * SML/NJ gave incorrect results  * timing improves to 1.7 seconds when \ntop-level structure is constrained by minimal signature  Table 3: Run times (seconds) 6K of the total \nsize of 24.6K is taken up by the product, sum, exception and F classes, each of which contains essen-tially \nno real code. There is certainly scope for improving our representation choices to decrease code size \nstill further. 9 Conclusions and Further Work ML.7 is a very useful tool: a compiler for a popular func-tional \nlanguage which produces compact, highly portable code with reasonable performance and which has unusually \npowerful and straightforward access to a large collection of foreign libraries and components. But the \nreasonable per-formance has only been achieved at the price of high compile times and a limitation on \nthe size of programs which may reasonably be compiled. Our decision to do whole-program optimisation \nis certainly controversial, so it seems worth try-ing to give some explicit justification: Most importantly, \nbecause of the relative inefficiency of current JVMs and the difficulty of mapping ML to Java bytecodes, \nit was simply the only way to achieve what we considered to be adequate performance. The limitation on \nprogram size is just not a problem for many real applications. The number of SML programs which are more \nthan, say, 15,000 lines long is actually rather small, as one can do an awful lot in that much SML. We \nhave had it suggested to us that a compiler which cannot compile itself is useless, but this is clearly \nnonsense. Trends towards component architectures, interlan-guage development, dynamic linking and distributed \nsystems mean that the large monolithic application is becoming less common. Of course, component bound-aries \nreintroduce the problems of separate compilation in a worse form, but that s all the more reason to com-pile \neach component as well as possible. Completely separate compilation at the granularity of modules introduced \nfor software engineering purposes is an anachronism for which high-level languages can pay dearly, as \nthe earlier discussion of SML/NJ s per-formance on the Life benchmark indicates (more real-istically, \nwe have ourselves doubled the speed of parts of MLJ simply by manually demodularising the code). There \nis a whole range of approaches between com-pletely naive whole-program compilation and simple-minded \nseparate compilation and, whilst the optimum lies somewhere in the middle, the two extremes are much \nthe easiest for the compiler writer. As MLJ develops, caching more information about each mod-ule and \nsacrificing some rewrites to be able to handle larger programs, and other compilers add ever more complex \nintermodule optimisations, we expect them to come much closer together. It is interesting to compare \nMLJ with Wadler and Oder- sky s Pizza [17]. We have started with a standard func-tional language and \nadded extensions to support interlan-guage working with Java, whereas Pizza starts with Java and adds \nsome functional features, such as pattern match-ing and parameterised types. We are aware of two other \nattempts to compile SML into Java, one by Bertelsen [S], based on Moscow ML, and the other by Walton \nat Edin-burgh. Both of these use uniform representations and do not perform anything like the same level \nof optimisation as MLJ. Wakeling has also compiled Haskell into Java bytecode, with disappointing results \n(large code and performance consid-erably slower than the Hugs interpreter) [25]. The other main attempt \nto compile a mainly functional language into Java bytecode is Bothner s Kawa compiler for Scheme [8]. \nKawa has an interactive top-level loop and com-piles bytecodes dynamically, but has to use uniform repre-sentations. \nSome informal tests indicate that Kawa typically runs an order of magnitude more slowly than MLJ and \ntends to run out of memory or stack space much earlier than MLJ. We are currently developing concurrency \nextensions for MLJ which are built on top of Java s built-in threads and, looking further into the future, \nare thinking about the pos- sibility of taking advantage of Java s remote method invo-cation infrastructure \nto develop distributed and mobile ap-plications in ML. We have reason to believe that JVMs which do tail \ncall elimination may appear soon, which would remove one of the other significant limitations of MLJ \n-although we already compile most simple loops into jumps, the lack of more gen-eral tail call optimisation \ndoes make some programs run out of stack space on reasonable-sized inputs. If tail call opti-misation \nis not done for us, then we will reluctantly have to consider selective use of two techniques: placing \nsome func- tions in the same method, and the tiny interpreter tech-nique used in the Glasgow Haskell \ncompiler [lo]. Naive use of of these techniques would cause a significant worsening of code size and \nspeed, so we would base our decisions on a more sophisticated flow analysis, which would also allow US \nto improve most of the other transformations [21, 161. This would appear to point to even longer compilation \ntimes, but we hope that this can be avoided by improving the represen- tation of our intermediate language. \nThe compiler currently spends a vast amount of time (and memory) performing trivial rewrites on the MIL \nterm. Many of these rewrites are commuting conversions, which would simply disappear if a suitable graph \nrepresentation were used instead. If we were also to rewrite destructively, then we should be able to \nobtain further compiler speedups. A further minor im-provement which we need to make is to ensure that \nMLJ never generates methods which exceed the JVM s 64K byte limit. This has so far only happened to us \non one unusual program, and we do not anticipate any great difficulty in modifying the code generator \nto prevent it happening. Our use of a monadic intermediate language is particu- larly novel. Whilst we \nwould not claim that this allows us t,o perform any optimisations which could not be achieved by more \nad hoc methods, we have found it to be a power- ful and elegant framework for structuring the compiler. \nIn general, we would strongly advocate a principled use of type theoretic and semantic ideas in compiler \nimplementation. One of the good things about compiling to Java byte-codes is that there is a large ongoing \neffort to develop better faster JVMs which we can take advantage of for free. Early information from \nSun about their next generation of JVMs indicates that allocation and collection will be improved sig-nificantly, \npossibly by a factor of 4. That could make MLJ s runtime performance competitive with native compilers \neven if we made no further improvements ourselves. References [l] A. W. Appel. Compilzng with Contznuations. \nCam-bridge University Press, 1992. [2] A. W. Appel and T. Jim. Shrinking lambda expres-sions in linear \ntime. Journal of Functional Program-ming, 7(5), September 1997. [3] K. Arnold and J. Gosling. The Java \nProgramming Lan-guage. Addison-Wesley, second edition, 1998. [4] P. N. Benton. Strictness Analysis of \nLazy Functional Programs. PhD thesis, University of Cambridge Com-puter Laboratory, August 1993. Technical \nReport 309. [5] P. N. Benton, G. M. Bierman, and V. C. V. de Paiva. Computational types from a logical \nperspective. Jour-nal of Functional Programming, 1998. To appear. [6] P. Bertelsen. Compiling SML to \nJava bytecode. Mas-ter s thesis, Dept. of Information Technology, Technical Univ. of Denmark, January \n1998. [7] M. Blume. CM: A compilation manager for SML/NJ. Technical report, 1995. Part of SML/NJ documenta-tion. \n[8] P. Bothner. Kawa -compiling dynamic lan-guages to the Java VM. In USENIX Conference, June 1998. Compiler \nand paper available from http://www.cygnus.com/-bothner/kawa.html. [9] F. Henglein and J. Jorgensen. \nFormally optimal box-ing. In ACM Symposium on Principles of Programming Languages, pages 213-226, 1994. \n[lo] S. L. Peyton Jones. Implementing lazy functional lan-guages on stock hardware: the spineless tagless \nG-machine. Journal of Functional Programming, pages 127-202, April 1992. [II] S. L. Peyton Jones, J. \nLaunchbury, M. Shields, and A. Tolmach. Bridging the gulf: a common intermediate language for ML and \nHaskell. In ACM Symposium on Principles of Programming Languages, January 1998. [12] J. Jorgensen. A \ncalculus for boxing analysis of poly- morphically typed languages. Technical Report 96/28, DIKU, University \nof Copenhagen, May 1996. [13] X. Leroy. Unboxed objects and polymorphic typing. In 19th Annual ACM Symposium \non Principles of Pro- gramming Languages, pages 177-188, 1992. [14] R. Milner, M. Tofte, R. Harper, and \nD. MacQueen. The Definition of Standard ML (Revised). MIT Press, Cambridge, Mass., 1997. [15] E. Moggi. \nNotions of computation and monads. Infor-mation and Computation, 93(l), 1991. [16] C. Mossin. Flow analysis \nof typed higher-order pro-grams. Technical Report 97/l, DIKU, University of Copenhagen, 1997. [17] Martin \nOdersky and Philip Wadler. Pizza into Java: Translating theory into practice. In ACM Sympo-sium on Principles \nof Programming Languages, Jan-uary 1997. [18] L. C. Paulson. ML for the Working Programmer. Cam-bridge \nUniversity Press, second edition, 1996. [19] Z. Shao. An overview of the FLINT/ML compiler. In ACM SIGPLAN \nInternational Conference on Func- tional Programming, pages 85-98, June 1997. [20] Z. Shao. Typed cross-module \ncompilation. Technical Report YALEU/DCS/TR-1126, Department of Com- puter Science, Yale University, July \n1997. [21] 0. Shivers. Control-flow Analysis of Higher-Order Lan-guages. PhD thesis, Carnegie Mellon \nUniversity, May 1991. CMU-CS-91-145. [22] D. Tarditi, G. Morrisett, P. Cheng, C. Stone, R. Harper, and \nP. Lee. TIL: A type-directed opti-mizing compiler for ML. In ACM SIGPLAN Confer-ence on Programming Language \nDesign and Implemen- tation, pages 181-192, Philadelphia, PA, May 1996. A. Tolmach. Optimizing ML using \na hierarchy of P31 monadic types. In Workshop on Qpes in Compilation, March 1998. P. Wadler. The marriage \nof effects and monads. In 3rd P4 ACM SIGPLAN Conference on Fzlnctional Program-ming, September 1998. \n(this volume). D. Wakeling. VSD: A Haskell to Java virtual ma- 1251 chine code compiler. In 9th International \nWorkshop on Implementation of Functional Languages, Septem-ber 1997. S. Weeks. A whole-program optimizing \ncompiler 1261 for Standard ML. Technical report, NEC Re-search Institute, November 1997. Available from: \nhttp://uww.neci.nj.nec.com/homepages/sweeks/smlc/. A Sample output The code below implements the quicksort \nalgorithm for in- teger lists. fun quick xs = let fun quicker (xs, ys) = case xs of Cl => ys [xl => x::ys \na::bs => let fun partition (left ,right, Cl 1 = quicker (left, a: :quicker (right, ys)) I partition \n(left,right, x::xs) = if x <= a then partition (x::left, right, xs) else partition (left. x::right, xs) \nin partitionc Cl , Cl ,bs) end quicker (xs, [I) end The internal function quicker compiles to the following \nstatic method: Method Ra b(Ra, Ra) 0 goto 31 3 new x22 <Class Ra> 6 dup 7 iload-8 aload-9 invokespecial \n138 <Method Ra(int,Ra)> 12 a&#38;ore-O 13 goto 55 16 new X22 <Class Ra> 19 dup 20 iload 4 22 aload-23 \naload-24 invokestatic X39 <Method Ra b(Ra, Ra)> 27 invokespecial t38 <Method Ra(int,Ra)> 30 astore-31 \naload-32 ifnull 105 35 aload-36 getfield t35 <Field Ra b> 39 dup 40 astore 5 42 ifnull 92 45 aload- \n46 getfield #37 <Field int a> 49 istore 4 51 aconst-null 52 astore- 53 aconst-null 54 adore-0 55 aload \n5 57 ifnull 16 60 aload 5 62 getfield X37 <Field int a> 65 dup 66 istore- 67 iload 4 69 aload 5 71 getfield \nt35 <Field Ra b> 74 astore 5 76 if-icmple 3 79 new X22 <Class Ra> 82 dup 83 iload- 84 aload- 85 invokespecial \nX38 <Method Ra(int,Ra)> 88 astore- 89 goto 55 92 nsw X22 <Class Ra> 95 dup 96 aload- 97 getfield #37 \n<Field int a> 100 aload- 101 invokespecial X38 <Method Ra(int,Ra)> 104 areturn 105 aload- 106 areturn \n This program illustrates some of the code transformations performed by MLJ. Integer lists have been \nrepresented by the class Ra, with nil represented by null and x: :xs represented by an in-stance of Ra \nwith x stored in field a and xs in field b. Notice how the calls to partition and the first call to quicker \nhave been implemented by goto bytecodes. The tuples have been removed: the triple passed to partition \nhas vanished, and the function quicker expecting a pair has been transformed into a method with two arguments. \n  \n\t\t\t", "proc_id": "289423", "abstract": "MLJ compiles SML'97 into verifier-compliant Java byte-codes. Its features include type-checked interlanguage working extensions which allow ML and Java code to call each other, automatic recompilation management, compact compiled code and runtime performance which, using a 'just in time' compiling Java virtual machine, usually exceeds that of existing specialised bytecode interpreters for ML. Notable features of the compiler itself include whole-program optimisation based on rewriting, compilation of polymorphism by specialisation, a novel monadic intermediate language which expresses effect information in the type system and some interesting data representation choices.", "authors": [{"name": "Nick Benton", "author_profile_id": "81100165244", "affiliation": "Persimmon IT, Inc., Cambridge, U.K.", "person_id": "P208599", "email_address": "", "orcid_id": ""}, {"name": "Andrew Kennedy", "author_profile_id": "81100450709", "affiliation": "Persimmon IT, Inc., Cambridge, U.K.", "person_id": "PP14158369", "email_address": "", "orcid_id": ""}, {"name": "George Russell", "author_profile_id": "81332525036", "affiliation": "Persimmon IT, Inc., Cambridge, U.K.", "person_id": "PP14203434", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/289423.289435", "year": "1998", "article_id": "289435", "conference": "ICFP", "title": "Compiling standard ML to Java bytecodes", "url": "http://dl.acm.org/citation.cfm?id=289435"}