{"article_publication_date": "09-29-1998", "fulltext": "\n Functional Differentiation of Comput,er Programs Jerzy Karczmarczuk University of Caen, Fra.nce Abstract \nWe present two purely functional implementations of the computational difierentiation tools -the well \nknown numeric (not symbolic!) techniques which permit to compute point-wise derivatives of functions \ndefined by computer programs economically and ezactly. We show how the co-recursive (lazy) algorithm \nformulation permits to construct in a trans- parent and elegant manner the entire infinite tower of deriva- \ntives of higher order for any expressions present in the pro- gram, and we present a purely functional \nvariant of the re-verse (or adjoint) mode of computat,ional differentiation, us-ing a chain of delayed \nevaluations represented by closures. Some concrete applications are also discussed. Keywords: Haskell, \ndifferentiation, derivatives, arithmetic, lazy semantics. 1 Introduction There is no particular need \nto advocate the necessity of computing derivatives in the huge realm of numerical cal-culations. They \nare vital for all kind of approximations: Newton algorithm, and other gradient methods of equation solvers, \nmany sorts of asymptotic expansions, etc. We need them for optimization, and for the sensitivity analysis. \nWe compute tangents to t,rajectories, normals to parametric sur-faces, and many other geometric entities \nin 3D modelling, image synthesis and animation. In the domain of differen- tial equations t.hey are used \nnot only directly, but also as an analytic tool for evaluating the stability of a given discrete algorithm. \nBefore solving the equation of motion one has first to construct them, and this often involves differentia-tion. \nEven in the discret,e mathematics they are useful to compute some combinatorial factors from the appropriate \npartition functions [l]. There are essentially t,hree ways to compute derivatives of expressions wrt. \nsome specific variables with the aid of computers. l The approximation by finite differences. This method \nmay be eit#her very inaccurate if Ax is big, or intro-duce serious cancellation errors, so it, is not \nvery sta-ble numerically, and the computational process using 0 1998 ACM I-581 13.024.4/99/0009...t5.00 \nt,hem must be very thoroughly analysed, otherwise this technique becomes useless due to the accumulation \nof errors. Symbolic computations. This is essentially the man-ual method, with a Computer Algebra package \nsub-stituted for a human slave. The derivatives, gradi-ents, Jacobians, Hessians, etc. are exact, but \nthe tech- nique is rather costly, and the intermediate expression swell might be very cumbersome. The \ngenerated nu-merical program is usually huge, illegible, and needs a very good optimizing compiler in \norder to eliminate all t,he common sub-expressions, which tend to proliferate when symbolic computations \nare intensely used. Moreover, it is not obvious how to differentiate symbol-ically an expression which \nresults from an iterative pro-cess or other computations which use non-trivial con-trol struct,ures, \nso this technique is usually not entirely aut,omatic. The Computational Diflerentiation known also under \nt,he name of Automatic or .4lgorithmic Differentiation (which makes the bibliographic search somewhat \nirri-t,ating.) This is the main subject of this article. The Comput,acional Differentiation is a very \nwell established research and engineering domain, and the number of references is impressive. See e.g. \n[2, 3, 4, 51, and espe- cially [6]. Unfortunately relatively little has been writ- ten about t,he functional \nprogramming in this context. Perhaps it was considered to be too easy?. . . In fact it seems t,hat the \npractical aspects of the domain tend to concent.rate the implementors attention to Fortran, C , and C++ \n. The last one is a natural choice if one wants t,o exploit the arithmetic operator overloading, for \nsimpler languages some source code preprocessing is usually unavoidable. The idea is very simple, it \nrelies on standard computer arith-metic, and has nothing to do with the symbolic manipula-tions of data \nstructures representing the algebraic formulae. Even a very complicated expression is composed out of \nsim- ple arithmet,ic operat.ions and elementary, built-in functions with known differential properties. \nOf course, a program is not a numerical expression, it. has local variables, itera-tions, somet,imes \nthe goto and other specific control struc-tures, which makes it, difficult, -t,o differentiate symbolically \nand aut,omat,irally a sufficiently complicated code. A sym- bolic package would have t,o unfold the loops \nand to follow the branches in fact, in general case it would have t,o in- terpret symbolically the program. \nThe idea is to compute in parallel with the main expres-sions, all its needed derivatives taking into \naccount that for all primitive arithmetic operators the derivation is known, and that all the compositions \nobey the chain rule. The same control structures as in the main computation are used (although not necessarily \nin the same way, as we show later). We shall now restrict the presentation to the univariate case, and \nwe will analyze the so-called direct or forward mode of differentiation. The reverse mode will be treated \nin the sec-tion (5). The system is implemented in standard Haskell, tested with Hugs of Mark Jones [7], \nand heavily uses the overloading of numerical operations. z Overloading and Differentiation; First Approach \nIn this section we introduce a new extended numerical structure: a combination of a numerical value \nof an expres- sion and the value of the derivative of the same expression at the same point. We may declare \ndata Dx = D Double Double deriving (Eq,Show) where for simplicity only we restrict the base type to \nDouble, in principle we could have used any number domain rich enough for our needs. This domains should \nbe at least a Ring (a Field if we need the division). The derivation of the equality and the printed \nrepresentation is utterly trivial, what we have declared here is just a two-field record. The elementary \nobjects which will be injected into the calculations are either constants for example D 3.14159 0.0, \nor the derivation variable which will be represented as something like D 2.71828 1.0. We underline once \nmore the fact that we are not doing symbolic calculations, the vari-able does not need to have a particular \nname. Prom the above we see that const,ants are objets whose derivatives vanish, and the variable (henceforth \nalways referred to in italic) differentiates to 1. The rest is fully aut,omatic. We declare the following \nnumerical instances: instance Nun Dx where (D x a)+(D y b) = D (x+y) (a+b) (D x a)-CD y b) = D (x-y) \n(a-b) (D x a)*(D y b) = D (x*y) (x*b+a*y) negate (D x a) = D (negate x) (negate a) We define also two \nauxiliary functions specifying the con-stants and the variable, and the reciprocal function. cst z = \nD z 0.0 dVar z = D z 1.0 instance Fractional Dx where recip (D x a) = let ix=recip x in D ix (negate \na * ix*ix) fromDouble z = cst z (Furthermore we have augmented the Standard Prelude by a small add-on \nto the Fractional class where recip x is defined by default as I/x. It seems natural to demand al-ternatively \nthat by default = x * recip yX/Y because the reciprocal is sometimes easier to manipulate than the binary \noperator of division. This may be the case here.) We have yet, to implement the chain rule, which for \nev-ery unary fun&#38;on demands the knowledge of its derivative form, for exampie sin -+ cos. All elementary \nfunctions may then be easily lifted to the Dx domain. Here are some use- ful examples: dlift f f (D x \na) = D (f x) ((f x)*a) instance Floating Dx where exp = dlift exp exp sin = dlift sin cos cos = dlift \ncos (negate . sin) sqrt = dlift sqrt ((0.5 /) . sqrt) = dlift log recip1% and now t,he following program \nch z = let e=exp z in (e + i.O/e)/2.0 res = ch (dVa:- 0.5) computes the hyperbolic sine together with \nthe hyperbolic cosine for any concrete value, here for z = 0.5. The value of res is D 1.12763 0.521095. \nOf course ch (cst 0.5) will calculat,e the principal value, but its derivative will be equal to zero. \nl he expression sqrt (cos (dVar 1.0) ) computes also -sin:r/(2v/COS) for x = 1. We end this section by \nobserving that Haskell is an excel-lent implement,ation language for our algebra because of its system \nof classes. But from the general algebraic point of view the classes Nun, Floating etc. are not natural. \nWe would prefer to deal with the standard algebraic structures like Monoid, Ring, Field, etc. Putt,ing \nthe classical elemen-tary functions such as exp or cos into the class Floating is uselessly rest,rictive, \nforcing all implementors of new alge-braic datatype t,o declare them as generalized numbers , even if \nthe;,. are very complex, e. g. functional. This is clumsy not on!y for people who would like to use modern \nfunctiotlal languages for the Computer Algebra. 3 Differential Algebra and Lazy Towers of Deriva-tives \nThe only langllage att.ributes really needed in the example above were: t,he possibility to overload \nthe operators, and the construction of data structures, so it may have been implemented in almost any \nserious language, for example in C++, and of course it has been done, see the cited literature. We car, \nextract the derivatives from the expressions, and code some mixed type arithmetics as well, involving \nnormal expressions and the pairs D z z together. But this is not homogeneous, and the possible extensions \nneeded to get the second derivative, etc. are not so straightforward. We propose thus to skip all the \nintermediate stages, and to define -obviously lazily -a data structure which represent,5 an expression \nand all its derivatives: { a, a , ci , n ( ) .}, without any t runcation explicitly present within :he \ncode. We construct a complete arithmetic for these si ructures, we show how to lift the elementary func-tions \nand their compositions, and we give some simple ap-plicatior. examples of the const,ructed framework, \nbut we propose tnst an easy theoret,ical introduction. 3.1 What is the differential algebra? Irving Kaplansky \nin his book [s] defines it as the the domain developed in 99% or more by Ritt [9] and Kolchin. . . We \nbegin with a standard commutative algebra implemented as normaY arithmetic operations on an algebraic \nfield of characteristic zero: the normal , primitive numbers. (Of course, the concrete computer representation \nof numbers is necessarily truncated, but in this paper the finite machine precision, the existence of \nthe divisors of zero etc. are ir-relevant.) With the aid of some data constructors we may implement more \nelaborate structures such as fractions or polynomials, and the construction of the associated com-mutative \nalgebra is essentially straightforward. The addi-tion, multiplication etc. are always local, i. e. point-wise \noperations. In order to constSruct. a differential algebra we take the above-mentioned field of characteristics \nzero, and to the set of standard operations upon it we add one more, the deriva- tion: a mapping o + \ncl which is linear and obeys the Leibniz rule: (ab) = a b + ab . We shall need some regularity prop-erties, \nand this is all. From the linearity we see that 0 = 0, from the Leibniz rule we get immediately that \n1 = 0 as well, this implies that all int,eger numbers possess vanishing derivatives, and by extension \nwe prove easily that all ratio- nal numbers form the field of constants of the algebra. For a computer \nscientist it means that all numbers are constants. By the derivation of (Q-IQ) = 1 we get algebraicallythe \nfor-mula for the derivative of the reciprocal: (a- ) = -a /~?, but in order to obtain a non-trivial differential \nalgebra we must extend the basic field somehow. A standard extension consists in adjoining an algebraic \nindeterminate, some x which permits the lifting of any field (or ring) A into its polynomial ring A[x], \nor the field of rational expressions over A: A(x), or possibly into a tran- scendental extension cont,aining \nexponentials, etc. There is nothing symbolic in this new object. The only requirement we impose, is its \nalgebraic independence of the elements of A. The word indeterminate is misleading; in the previous sections \nthe ordinary numbers have been lifted into the domain Dx by y + D y 0, and obviously any D y p with p \n# 0 serves our purposes, provided we know how to manipulate it. But it should exist within the system \non the same footing as all t,he constant elements, i. e. we want it to be a number . All objects a, a \n, a , etc. must belong to the same domain. We want our differential algebra to be closed. The deriva- \ntion should be as algebraic as all the other operations. For any new x introduced into the domain we \nhave to provide I , x , x c3), etc. -in fact, an infinite number of algebraic indeterminates, as there \nis absolutely no reason that an E should be algebraically dependent on I. This seems to be hopeless from \na practical point of view. The standard solu-tion consists in choosing a symbolic basis, and in construct- \ning a typical Computer Algebra package, where the deriva- tion is a non-local, structwal operation on \ntrees or DAGs representing the formul=. We propose the following alternative: we will augment the basic \nnumerical field extensionally. To any expression a (a numerical value!) we adjoin expplicitly its derivative \na , and by necessity all the higher derivatives as well. The computational differentiation technique \n-as sketched in the previous section -augmented by the lazy evaluation protocol gives to the framework \na sane, simple, precise and efficient operational semantics. 3.2 The data and basic manipulations The \ndat,at,ype we shall work with belongs to an infinite co-recursive domain: data L = D !Double L There \nis no structural difference between such a datatype and a normal lazy list composed of Doubles, and we \ncould have used [#he newtype aliasing for it, but for the sake of presentation, the independent declaration \nis cleaner. A very minor point, is the declaration of the first field as strict. In t,his clat,a the \nfirst. field is the numerical expression it-self, ant1 the second is the tower of all its derivatives, \nbegin-ning wit,11 the first. Here is the definition of constants, of the variable, and of the function \ndf which computes/retrieves the derivative of/from an expression. This function is obvi- ously almost \ncompletely trivial: lzero = C 0.0 lzero --Inf. list of zeros cst x = D x lzero dVar x = D x (cst 1.0) \ndf (D _ q) = q The imtance of Eq for our data is not well defined. The inequality can be discovered after \na finite number of com-parisons, but, t,he (==) operator may loop forever, as always with infinite lists. \nReaders who are unhappy about this should realize that t.he equality of symbolic expressions is ill-defined \nas well, and the Computer Algebra has to cope with this primeval sin. The derivat,ion is obviously not \ntrivial: it forces the re-duction of the t.ail. We pass now to the standard aritmetic operat,ions, declared \nas methods of the standard Haskell classes: Nnm, i3at ional, etc. 3.3 Arithmetic The definitions below \nare legible even for high-school pupils who happen t,o know the syntax of Haskell. The instance definitions \nbelow are not complete, but the reader can com- plete them in Sve minutes. Moreover, in a more ambitious \nimplementatiou we should use more generic functionals, zips and map. (The oatatype L is obviously a Functor.) \ninstance Nun L where (D x x : + (C y y ) = D (x+y) (x +y ) (D x x ) -(D y y ) = D (x-y) (x -y ) pQ(D \nx x ) * q@(D y y ) = D (x*y) (x *q + p*y ) negate (D x x ) = D (negate x) (negate x ) instance Fractional \nL where fromDouble z = cst z recip pQ(D 1: x ) = ip where ip = D (recip x) ((negate x )*ip*ip) The multiplication \nrule proves that the operation df is a derivation. The algorit,hm for the reciprocal shows the power \nof the lazy! protJco1 -the corresponding (truncated) strict algorit,hm wok! be much longer and clumsy. \nThe generalized expressions belong thus to a Differen-tial Field. One can add, divide or multiply them, \none can calculate the derivatives (which costs nothing to the pro- grammer, !lecause they are calculated \n-lazily -anyway), and this is all. There are no symbols, and the expressions are not tree-like structures. \nThe main value (first field) of an expression is an ordinary number. The whole structure is linear. One \ncan define also the elementary functions for such expressions. We begin with a general lifting functional, \nbut then we propose some optimizations for the standard tran-scendent,al functions. dlift (f:fpq) pQ(D \nx x ) = D (f x) (x * dlift fpq p) : : L -> Double -> L Pow pQ(D x x ) pow a = D (x**a) (a*>x * p pow \n(a-1)) instance Floating L where --exp = dlift fz where fz = exp:fz exp pQ(D x x ) = r where r = D (exp \nx) (x *r) log pQ(D x x ) = D (log x) (x /p) sqrt z = pow z 0.5 sin = dlift fz where fz=cycle [sin,cos,(negate.sin),(negate.cos)] \ncos = dlift fz where fz-cycle Ccos,(negate.sin),(negate.cos),sinl The function dlift lifts any univariate \nfunction to the L domain, provided the list of all the primitive derivatives is given, for example (exp,exp, \n. . .) for the exponential, or (sin, cos, - sin, -cos, sin . . .) for the sine. In the definition above \nwe introduced the power function pow z a: ra for a double, which is needed together with the standard \n(**) for which the types of the exponent and of the base are equal. The definitions of the exponent and \nof the logarithm have been optimized for efficiency purposes, although the func-tion dlift could have \nbeen used. The auto-generating lazy sequences are coded in an extremely compact way, compare it with \ne.g. [5], where a more classical approach is presented. (In [IO] we have shown how the lazy formulation \nsimplifies the coding of infinite power series arithmetic as compared to t,he vector style found anywhere, \nfor example in [ll]. We see here a similar shortening of algorithms. In this sense the present paper \nis a continuation of our previous work discussed in [lo] or [12].) Our definition of the hyperbolic cosine \nstill works, but gives an infinit.e sequence beginning with cash and followed by all its derivatives \nat a given point. The following func-tion : lnga 2 = (z-0.5)*log z -z + 0.9189385 + 0.08333331 (z + 0.033333/(2 \n+ 0.2523809/(z + 0.525606/ (z + l.O115231/(z + 1.517474/(2 + 2.26949/z)))))) called as, say, lnga (dVar \n1.8) produces the logarithm of the Euler r function, together with the digamma $J, trigamma, etc., needed \nsometimes in the same pro-gram: -0.071084, 0.284991, 0.736975, -0.523871, 0.722494, -1.45697, 3.83453,. \n. One should not exaggerate: the orig- inal cont,inuous fraction expansion taken from [13], is an approximation \nonly, and the errors in the derivatives will accumulate, but it is incomparable to the finite difference \nmethod; here the $J(~)(z) will have still several digits of pre- cision, if the original formula applies, \nalthough this formula has not been designed to express the derivatives. 3.4 Discussion We recapitulate \n.here the basic properties of our computa-tional framework. If the definition of a function is autonomous, \nwithout ext,ernal black-box entities, the computation of deriva- tives is ,fu/ly acrtomatic, wit,hout \nany extra program-ming effort,. The derivar,ives are computed exactly, i. e. with the machine precision. \nThere is no propagation of instabil-ities, if we neglect the st,andard algebraic error prop-agation through \nnormalization, truncation after multi-plication etc. The possible difficulties are exactly the same as \nwith the main computation, perhaps a little more important, as usually more arithmetic operations are \nneeded for the derivative than for the main expres-sion. The generalization to vector or tensor objects \ndepend-ing on scalar variables is straightforward, it fact noth-iny new is needed, provided the standard \ncommutative algebra is implemented. The eficiency of the method is very good. It has been proven by Baur \nand Strassen in 1983 and by Griewank in 1989 t&#38;hat, tfhe Computat,ional Differentiation needs between \n2 and 5 more operations than the computa-tion of t,he main value, even in the multivariate case. The \nmanual analytic, highly tuned differentiation may be faster, but the automatic symbolic differentiation \ntechniques are far behind. Some cnverrt.~.&#38;!I seem appropriate. We have mentioned the fact that the \nsymbolic algebraic manipulations suffer from the int,ermediat.e expression swell which may render impos-sible \nto Doria= too high order derivatives of complicated expressions. In our framework we have just numbers, \na zero muItipiying anything gives zero, and we don t see par- ticular reasons to generate horrible expressions. \nBut. the non-st,rict eval-zation protocol is far from being a free lunch. The space Leaks induced by \nthe deferred closures might be really dangerous, the reader should not think that he might compute TOO \nderivatives of a reasonably complex ex-pression using our lazy towers. unless the program finds some \nspecific shortcuts preventing the proliferation of clo- sures. T here are no leaks leading to too long \ntowers which are intrinsic to the method, but if only ordinary algebraic operations are performed in \na long loop, the first unevalu-ated derivative thunk may explode. We propose thus the following suggestions, \nand we present some optimizations introduced to our package in order to increase its perfor-mance, and \nto delay the unpleasant reaction of the garbage collector. . If the p: )glarn does not need the derivatives \nintrin-sically, e. u if we need just to compute some first de ,ivative:. ,t the end, and there are no \nrecurrent dif-ferential identities involved, it is much more efficient to use a t,runcat)ed, strict variant. \nof the method, sketched in (2). l We have defined L as: D !Double L I C !Double, where thp non-recursive \nclause C denotes a constant, i.e. cst x = C x. The infinite chains of zeros might st,il! appe,tr through \ncancellations, but the package it-self wi!l no:, generat,e t)hem. s The processing of constants should \nbe optimized. The multiplication by a constant zero should destroy all pending closures. The multiplication \nby 1.0 should not be suspended. The user should realize that if a function is discont,inu-ous and defined \npiece-wise, the computational differentia-tion will not discover it, and it will blindly compute one \nof the possible values, by following the control structures of the program. This may be or may be not \nwhat the user wishes, in such circumstances the differentiation cannot be fully automatic. 4 Some Applications \nLazy formulat,ion of a computational problem does not make miracles, but helps sometimes to transform \nintricate equa-tions into algorithms. We show now how the formalism pre-sented may be practically used. \n4.1 Recurrently defined functions Suppose you teach Quant,um Mechanics, and you have a one-shot problem: \nyou want to plot a high-order Hermite function in order to prove that the wave-function envelope of the \noscillator corresponds to the classical distribution. But you remember only that Ho(z) = exp(-x2/2), \nand that   H,(s)= &#38; (x - $) &#38;-l(1). (1) You are not interestled by the analytical form of the \nresult, and your students even less, you just want to plot this func- tion. It suffices to code hrm \nn x = cc where D cc _ = hr n (dVar x) hr 0 x = expcnegate x * x / 2.0) hr n x = dsq/cst(sqrt (fromInteger \n(2*n)))* (x*z -df z) where z-hr (n-1) x and t,o launch. say, map (hrm 24) C-8.0, -7.9 . . 8.01 before \nplotting the obtained sequence. The example is a bit contrived, but it works in practice without, problems. \nIt is almost as efficient as other recurrent formulas. 4.2 Lambert function We will find the Taylor expansion \naround zero of the Lam- bert function [16] given by the equation w4 = z W(z)e , (2) without using any \nsymbolic manipulations. This functions is used in many branches of computational physics, and in combinatorics. \nWe see immediately that dz dW=ew(l + W) (= +(l+W)fort#O) (3) whose inverse dW eeMr W 1 -=-= --(4) dz \n1 + FY 2 1SW gives a one-line code for t,he McLaurin sequence of W, know-ing that W(0) = 0. ul = D 0.0 \n(exp (negate wl)/(l.O+wl)) producing the following numerical sequence: 0.0, 1.0, -2.0, 9.0, -64.0, 623.0, \n-7776.0, 117649.0, -2.09715e+006 ,..., which agrees wir,h the known theoretical values: WC ) (0) = (-ny-1. \nAccessorily we see that if we plug-in the formula (4) into any program which calculates W(z), (for example \nusing the Newton or Haley approximation [16]) we obtain all its derivatives at, any point. Can we use \nthe second, apparently cheaper form of (4) which does not use t,he exponential? For z # 0 obviously yes, \nprovided we knew W using some numerical technique. Lazy algorithms needs sometimes some intelligent pre-processing \nin order to t.ransform equations in algorithms, exactly as in Computer Algebra. In the papers [lo, 121 \nwe have presented a different approach to similar questions -a complete incre-mental arithmetic on lazy \npower series, the domain where the derivat,ion is also defined trivially, but differently. Some examples \nt,herein need an involved gymnastics. In the ex-ample above. there is no immediate solution, passing \nfrom exp(-W) to 12;/9 at z = 0 loses some information, we don t know any more that its value at 0 is \nequal to 1. We can add it by hand int,rocmcing a function Y defined by W = rY, and putting Y( 16) = 1. \nThen Y = exp(-zY), and we get for the derivative y = -Y2Y = -I (Y + zY ) or (5)l+zy Both forms are implement,able \nnow, and the first, recursive, is faster! We have just to introduce an innocent dirty trick, a functmn \nC which multiplies an expression f by the wariable z at t = 0. zeta f = D 0.0 (f + zeta (df f)) yl = \nD 1.0 yl where yl = negate yl*(yl + zeta ~1 ) from which we can reconstruct the derivatives of W in \none line, knowing th;.t. it, is equal to zY.  4.3 A singular equation The previous example shows also \nhow to code the Taylor expansion of nng function fulfilling a (sufficiently regular) differential equation. \nThere is nothing really new in the lazy approach, only t,be coding is of an order of magnitude shorter \nthan an approach using arrays, indices and truncations. In some cases it i:i possible to treat singular \nequations as well, see the trick used in [lo] to produce the Bessel function. Here it would be different, \nbut, based on the same principle: the function u(J,) defined by u(z: ) = z- J~(z) obeys the equality \nf (r) = --&#38; (s f (r) + ;fw) Y (6) which is pervc,rse. needing f and f to compute f , and sin- gular \nat, J == 0 ;alt,hough this singularity is not dangerous). But we may apply now our C(U)) trick. By putting \nfor sim- plicity 11 e,+aI tc zero, ancl rep!acing z ~ (z) by [(((f )) in (6), we oktzin J = 1.0, -0.25, \n0.0625, -0.140625, 0.878906, -10.7666, 2i8.C%,... for besf = D 1.0 fp where fp = negate (0.25*besf + \nzeta (zeta (df fp))) because the second derivative is protected twice from being touched by the reduction \nof the auto-referential expression f P. 4.4 WKB expansion Our next exercice present,s a way of generating \nand han-dling functions defined by intricate differential identities in the clomain of power series in \nsome small parameter (not the differentiation variable!). We will derive higher order terms for the Wentzel-Kramers-BriIIouin \napproximation, as presented in [14], and useful for some quasi-classical approx-imation to the wave function \nin Quantum Mechanics. We start with an equation CY Q(x)y. (7)2 = with c very small. The essential singularity \nat zero prevents a regular development of y in C. Within the WKB formalism y is represented as Inserting \nthis into (7) generates a chain of entangled recur-rent equalities fulfilled by S,. The lowest approximation \nis Sl, = i1/Q, (which needs an explicit int,egration irrelevant for our discussion), and exp(S1) = l/G, \nwhich has prof- ited from t,he fact, that t,he coefficients 5&#38;,+1 are trivially integrable. We propose \nthe following expansion, which separates the odd and the even powers of t. The coefficients of proportion- \nality, and the necessity to combine linearly the two solutions differing by the sign of fi are omitted. \ny M exp ;so + U(x, 2) + EV(X, 2)) (9) ( Injecting this formula into the equation (7) gives the follow- \ning differential identities: or eu = u = 2 % + t2V 2 s; +tzv &#38;&#38;qF (lo) and v' = j$ (V2 + ly\" \n+ cqP) (11) 0 These cross-referencing definitions are horrible, but despite that, they constitute an \neffective functional algorithm. Until now we never really needed all derivatives of a function, and the \nreduction of the lazy chain stopped always after a finite number of steps. But. here, in order to get \none numerical valzlr of, say L (Z), we needthe second derivative of U, which needs the second and the \nthird derivative of V, etc. Nevertheless this is a perfectly sane lazy algorithm, which can be coded \nin a straightforward manner, and the only remaining part of the algorithm is the explicit integra-tion \nof \\ . The point is that U and V should be treated as series in c2, and the higher derivatives of U and \nV appear only in higher-order t,erms. We have thus to introduce some lazy techniques of power series \nmanipulation. This topic has been extensively covered elsewhere, see [lo] and the references therein. \nWe remind here the basics. The series l](z) = uo+ulzfu~z2+~~ + (with the symbolic var.able z implicit) \nwill be represented as a lazy list cue, rtl, u2,. . . I. The linear operations: term-wise addition and \nulu,tiplication by a scalar are trivial (zip with (+), and map). The multiplication algorithm is a simple, \n one-line recursion. If we represent U(z) = uo + zis, then U . 1 = uo PIO + t (tlopl+ ~0V). For the reciprocal \nW = l/U (with 210 # 0) we have 2~0 = l/z&#38;o, and E = -woG/u. The differentiation and integration need \nonly some multiplicative zips with factorials, and eventually an integration constant. The elementary \nfunctions such as W = exp(U) may use the following technique: W = U lY, and thus W = exp(rro) + s G HJ \n~12. ? FR, This is a correct co-recursive algorithm, see [ill. Of courses thr t,erms 21k need not, be \nnumbers, they may belong to the tlomain L, or on the contrary, our Differential Field may be an extension \nof t.he series domain, i. e. the values present, within the L structure are not Doubles, but series. \nThere is no relation between the differentiation vari-able, and t,he formal variable of the series. The \nfirst variant is used here. We shall have t,hus a doubly lazy structure, and we need a t,rivial extension \nof the differentiation opera-tor over thr wklblez which should linearly propagate along the lazy list. \nrepresenting a series over E. In this domain it suffices to dehrrr df = map df, or in a more explicit \nway df (UC :uq) = df u0 : df uq. We may 1e-1 t,he algorit,hm and generate the approxima- tion to (he \n:Grp function (for one value of x), which is the solution of the p(luation 7 for Q(s) = x. We fix the \nvalue of the varinble, e. g. q = dVar 1.0, then we define sO =sqrt qands0 = df SO , and the equations \n(10) and (11) may be writt,en as U = (-0.5)*:(sO\" : df v )/(sO : v ) v = p where p=((-0.5)/sO ) *:(u \n^2 + df U +:> p*p) where a ~l?-p:, addit,ion operator a +: > b which represents a + t?E sho:Jd 1,~: \ndefined as (aO:aq) +:> c -a0 : (aq+b) and (a:> multiplies a series by a scalar. Now u is a se- ries \nwhose elements belong to our differential datatype L, but we don t n,?ed the derivatives, only the main \nvalues, and we may easily construct. a function f which returns this main value from the L sequence, \nneglecting the rest. One applicai;ion of map f to t,he series u suffices to ob- tain -0.26, -0.1,: 1375, \n-1.65527, -28.8208, -923.858, -47242.1, -3.52963eAOOr;. etc. while v producess -0.15625, -0.539551, -6.31905, \n-15?.&#38; X, -6271.45, -391094.0, -3.44924e+007, etc. The genrrat o:, nnd exponentiat)ion of u, and \nthe integration of v g;x~ for, a .$c:ficiently small c a very good numerical pre- cision, which is well \nknown. Our aim was to prove that the result, can be obtained in a very few lines of code, wit,hout any \nsymbolic manipulation machinery behind-the curtains. Other asymptoi.lc expansions, for example the saddle-point \ntechniques whicn also generate horrible formulae may be im- plemented wife equal ease. 5 Reverse Mode \nof Differentiation The augl,ieli;A arir.hmetic of the previous sections seem quite nntL ;ili, :)ut, \nit, is not the only possible choice for com- puting 1:1t derivatives of an algorithmic program. The so- \ncalled rel;crar rr-,~,:la may be more efficient, especially in t&#38;he multivi:.?rLte cast which is \nnot discussed here. The presen- tatiou b&#38;w is riot. complet,e, much more work is needed in order \nto implement the reverse method in an effident way, but we sketch the main idea. It seems that the functional \napproach to the reverse differentiation has been never at-tempted until now. We discuss only the generation \nof first derivatives, and t,he functional framework used doesn t need to be lazy. Suppose that the program \noperates upon m independent variables, for example 21, ~2, , zm. The dynamic chain of operations obtained \nby the streamlining of all the control structures, is a set of assignments for the new variables: xmt1 \n= fmSl(Xl,. ,&#38;7x), xv?+2 = fmta(~1,. . ,Gn,GnSL), (12) 51% = fn(xl,...,Zm,.. rzn-l), some of which, \nperhaps just the last one, will be the desired final result(s) of the program. The derivatives are defined \nby lhe chain rules (13) If we introduce the matrices 1 0 0 . . . ax2/ax* 1 Jv =e = dX3/dXl axgpxz Y : \n: : (J4) { > . . . ii f: i / 0 0 0 . ..\\ the equation (13) gets the form J = I + DJ. By following the \nchain, beginning wit,h x7,+1, and terminating at xn and its derivatives, we Cal&#38;ate them iteratively, \nand this is the standard forward mode. But it is possible also to start wit,h the last partial derivatives, \nand to follow the chain backwards. The general formulation goes as follows. The Jacobi matrix is the \nsolution of the equation (I -D)J = I, and it may be solved in finite number of iterative steps J=I+D+D2+...+Dn--l, \n(16) because if D,, vanishes for i 5 j, then D is nilpotent, and the series aboves truncates. If for \nsimplicity we assume that the final result is scalar, that we are interested only by x,, and its derivatives, \nand the only independent variable is x1, we need only one row: J 721 of the resulting matrix. It is calculated \nin the following order: Jn,n--l = Dn,n--l (17) Jn,n-2 = Du,n-2 + Dn,n-,Dn-,,n--2 (18) Jn,n-3 = Dn,n-3 \n+ Dn,n--1Dn--l,n--3 + (Dn,n--2 + Dn,n-I IL--l,n--2) Dn-ap-3 (19) or, if U P r~&#38;;.od~.cr the adjoint \nvariables zk E Jnkr we have Zn- 1 = 3, ?j -i . and !ifn-2 = D7zp-2 + s-n-1 Dn-1,nm-a (20) F7,-? = Dn.n--.? \n+ %--DDn--l,n-3 + ~n-2Dn-2,7~--3 (21) k+l j-, = Dnk -I-c Cd ,kT (22) which car, he coded, but not in \na straightforward way. In an impe:,cLtivc language every instruction defining xm = fm(x1,. . ( .rln-l \n) introduces also the following assignments for the ac?joint variables ??k for k < m.: !i% t zk +??,Dmk. \n(23) We have to proceed from m = n downwards, but after hav-ing comput,ed all the x. For example, if \nwe define y = sin(z), z = 2Y + x/y, (24) where c ic I :!e !i:~r~l result as a function of Z, we find \nfirst !:=2-x/y2, r = l/y, (25) and then z t E + ,cos(x). (26) To resume: if the final result is f, for \neach dependent vari-able xm we int,-educe its adjoint, a new variable f,, = af /aGn , whicll ran be calculated \nby tracking back the chain of definitions Df all x above xn. One adjoint variable is needed, even if \n.rm depends on many others. The over-loading ?f !I? ,a-i+,hmetic data types will not help us here as \ndirer1 !j. as i;l l.he forward case. The operators will be overloaded. but r,he structure of the code \nis really different, and needs some tricks, for example storing the trace of the executed forward instructions \non a linear tape which is then processed backwards, and drives the adjoint assign-ments. The program \n[15] uses the overloading (in C++ ) in a quite complex way, and the adjoints are explicitly stored in \nspecific data structures. It is possible not t,o store ev-erything, and co recompute some intermediate \nvalues, it is possible also t,cl mix the two strategies. The necessity to store evrl-I? hing might be \nquite dangerous in programs con-taining i,.op,. iL> every iteration step introduces new inter-mediate \n1rsl~l~. and thus new adjoint variable. The time is convert,ed into space, and the %ape might be very \nlong. Independent,ly of all the pract,ical aspects of this tech-nology, we find ic challenging to implement \nsuch a perverse combination of control and data flow in a pure functional language. Obviously, if our \nlazy t,echnique is used in too dif- ficult cases, it may generate subst,antial space consumption, but \nthis is une.voidable, they are not spurious leaks. The soiutioil is quite simple if every variable is \nused only once, i. c if the program reduces to a chain of unary funct,ions: I I :: j = g(h(. .p(x). . \n.)). (In order to apply this strat.e:;y io the hy[)erbolic cosine example, the form g(y) = 1/2(y + 1,/y) \nrnvst be considered as a primitive black-box, with g calculated off-line). The chain y = f(z); z = g(y); \nis completed by 2- = 1; y = f. g (y); Z = a. f (z) with no updates. We want to compute the lnst adjoint \nstatement in parallel with the first forward one, because both are related to the function f. But during \nt,he assignment to y we don t know g yet, and the const,ruction of Z is impossible. So, we construct \nits ezpplicit promise: XiJ -+ 5. f (z) and we com- pose it with the previous promise (remember that the \nchain is linear; this is a huge simplification, but we shall use it in a more complex case as well). \nThe new data structure used here is a pair containing the value of the expression, and the promise to \ncompute the adjoint of its argument, when the adjoint of the intermediate variable assigned to the expression \nis known. The promise for the principal variable itself is the identity. At the end the promise convolution \nshould be applied to 1. Here is the example of such a calculus computing both hyperbolic functions for \nz = 0.75: data L = D Double (Double->Double) lift f f (D x prm) = D (f x) (prm . (\\r -> r*(f x))) fn \nx = 0.5*(x-1.0/x) fn x = 0.5*(1+1.0/(x*x)) ch x = let y=(lift exp exp) x in (lift fn fn ) y res = (x, \nprm 1 .O) where (D x prm) = ch (D 0.75 id) We have presented this simplified example because the con-struction \nof deferred closures is a natural functional way of generating t,he adjoint code, and it will remain \nin more involved cases. However, even such simple sequence as g = f(s); z = x + y; shows already the \nfull problem: the adjoint E is assigned in two instructions, and the sum (22) is non-trivial. This sum \nin principle can be evaluated function-ally without any problem, but it depends on the forward or principal \nvalues, and we want it to reuse them. So, in the general case we shall keep a list of all the contributions \nto the computed adjoint, and at the end we compute its sum. The second field of L stores a function belonging \nto the type Double-> [Double]. The differentia-tion variable is lifted into D x (\\r -> Cr] ). Of course, \nthe promise attached to a constant delivers zero, and the opti- mization may eliminate it. The lifting \nof an unary function is the same as above, and a binary operator g (together with its partial derivatives \ngx and gy) lifts to dlift g gx gy (D xl hl) (D x2 h2) = D (g xl x2) (\\r -> (hl (r*(gx xl x2)) ++ h2 (r*(gy \nxl x2)))) The typical numeric instances (in the Num, Rational and Floating classes) are overloaded as \nexpected. Symbolically, altogether: exp = lift exp exp sin = lift sin cos (+I = dlift (+) i i where i=(\\x \ny -> 1.0) (-) = dlift (-) (\\x y -> 1.0) (\\x y -> (-1.0)) (*I = dlift (*) (\\x y -> y) (\\x y -> x) recip \n= lift recip (negate . recip . square) (/) = dlift (/) (\\x y -> recip y) (\\x y -> (negate x)/(y*y)) \nOur original program which computes the hyperbolic co-sine, applied to the lifted variable returns the \nmain value together with t,he ultimate promise of all the adjoint leaves of the comput.at,ion graph. \nIn order to get the derivative, the following function is applied: rdx (D x prm) = (x , sum (p 1.0)) \nThe presented solution is workable, but should be treated rather as a frasibility study, than as an industrial \nstrength algorithm. It needs more work. The memory requirements in complex problems might be very high \n(even without com- puting higher (ierivatives). It can [or even should) be gen- eralized to :.hc i::ultivariate \ncase, but in this paper we delib- erately wanted I.C be elementary. To summarize: the lazy implementation \nof the re-verse mrt.nod of Computational Differentiation consists in building-up t,he dynamic, delayed \ncode corresponding to the assignment,s of t,!re adjoint variables. The resulting code is linear (even \nif the expression treated are arborescent). Its execution is efficient, and reuses all the partial results \nob-tained during the forward phase of the algorithm. Its constwxtion is t.he main additional cost of \nthe technique. Despite a reasonably long history. the lazy functional tech-niques are rarely ,.rsed in \nnumerical computations for effi-ciency reasons. We do not plan to fight against brutal force, we want \nonly to show t,hat the lazy techniques provide some marvellous coding tools, simple, legible, and semantically \nvery powerful, economising thus plenty of human time. In many c;r\\-umstances they can replace the usage \nof Computer Algebra programs. In cases where a truly sym-bolic answer is rierded in order to provide \nsome insight into the mat,hcmatlcal properties of treated objects, the usage of the er:te~.ler: arithmetic \nwill not, suffice, but very often the sy-m&#38;&#38;c algebra is applied in despair, just to gener-ate \nsome horrible expressions consumed by the FORTRAN or C compiler only, and never looked upon by a (sane) \nhuman. A typical, ccmpetent real programmer : a quantum chemist or an engineer will usually try to optimize \nthe nu-merical codes, aird to avoid the symbolic manipulations, but the differentiat,ion is still often \nperceived as an analytical, or structural ope,racion, which cannot be treated algebraically by a numerical \n->rogram, although it is a mathematical truth that differerlti~+ion ds algebraic. Thanks to the Compu-tational \niMerectlation packages the situation is changing, but the implement at,ion of the differential algebra \nint,egrates badly wilh sum languages as FORTRAN or C . If we are imerested not only by the computation \nof some derivatives, but we need also to implement complex differen-tial idemities which define our data, \nthe differential algebra should be closed. in principle it is possible to implement the algehrair cierivat,ion \nin a strict language which permits overloading, b11: t.he truncation gymnastics will be very ugly, and \nerror-p-on:. IT he lazy functional formulation is straight- forward, and a:,-essible for beginners. One \nshould not be fascinate.1 IYj 1 ,,I fact that lazy sequences are infinit.e and that wc : a~, crn!prrte \ncheaply hundreds of terms. From this point of view al, equivalent. strict code may be much more efficient \nGnce ii, wiil not permit t,he creation of huge un-evaluated thunks which may clobber the heap. The gain \nis the elimina~tiou of administrat.ive charges during t,he pro- gramming process, and the simplification \nof the code. Whal, is impor ant here is t,he fact that the modern func-tional ioos uei 3: it, to manipulate \nprograms. We may dy-namical;!, CCJI,-.(,I ilct, and compose closures, either explicit, or hidden within \nthe delayed thunks, whose semantics cor-responds to t,he final desired answer, and not just to the syml)olic \nexpression t,hereof. This might be very important for practical scient,ific programming problems. Of \ncourse, the symbolic comput,at,ions may be enriched also by some lazy codes, but. the impact, of this \nphilosophy within the Computer Algebra world remains very weak. References R. E. Graham, Donald E. Knuth, \nOren Patashnik, PI Concrete Mathematics, Addison-Wesley, Reading, MA, ( 1989). Martin Bert,, (Christian \nBischof, George Corliss, An- PI dreas Griewank. eds., Computational Differentiation: Techniques, Applications \nand Tool.?, Second Interna-t,ional Workshop on Computational Differentiation, (1996), Proceedings in \nApplied Mathematics 89. P. Hovland, C. Bischof, D. Spiegelman, M. Cosella, Ef- [31 jicient Derivative \nCodes through Automatic Dieren-tiation and Interfa.ce Contraction: an, Application in Biostmtistics, \nSIAM J. on Sci. Comp. 18, (1997), pp. 105&#38;1066. Ralf Giering. Thomas Kaminski, Recipes for Ad- 141 \njoint Code Construction, Tech. Rep. 21.2, Max-Planck- Inst,itut. fiir Met,eorologie, (199G), ACM TOMS \nin press. Claus Bendtsen, Ole Stauning, TADIFF, a Flexible[51 C++ Package for Automatic Differentiation, \nTech. Rep. IMM-REP-1997-07, Dept. of Mathematical Mod-elling, Technical Universit,y of Denmark, Lyngby. \nGeorge F. Corliss, Automatic Differentiation Bibli- PI ography, originally published in the SIAM Proceed-ings \nof Auto,matic Diflerentiation of Algorithms: The-ory, Implementation and Application, ed. by G.G. Corliss \nand A. Griewank, (1991), but many times up-dated since then. 4vailable from the netlib archives (netlibQresearch.att \n.com), or by an anoymous ft,p to ftp://boris.mscs.mu.edu/pub/corliss/8utodiff The package and all documentation \ncan be obtained PI from http://www.haskell.org/hugs14 PI Irving Kaplansky, An Introduction to Differential \nbra, Hermann, Paris (1957). Alge- PI Joseph F. Ritt,, Differential Algebra, Dover, N.Y., (1950). PO1 \nJerzy Karczmarczuk, Generating Power of Lazy Se-mantics, Theoret,iral Computer Science 187, (1997), pp. \n203-219. [ill Donald E. Knuth. The Art of Computer Programming, vol. 2: Seminumerical Algorithms, Addison-Wesley, \nReading, (1981). Jerzy Karczmarczuk, Functional Programming and WI Afathenkatical Objects, Proceedings, \nFunctional Pro-gramming Languages in Education, FPLE 95, Lec-ture Notes in Comput,er Science, vol. 1022, \nSpringer, (1995), pp 1 1-137 Milton ~4bramowit8z, Irene A. St,egun, eds. Handbook of Mrr;hemaiic~al Functions, \nDover Publications, (1970). (Jar.1 M. Bender, Steven A. Orszag, ildvanced Mathe-mn;rc.al , !lci!<ods \n,for Scientists and Engineers, McGraw-Iiiil, (1 Cl < j. C:laliq Hrl:,:itPen. Ole St,auning, FADBAD, a \nFlexible C ++ i*acl.:i:qf: for Automatic Differentiation using the Fo~~card anti Backward Methods, Tech. \nRep. IMM-RF:P-1997-IT. Dept. of Mathemat,icaI Modelling, Tech-nical ITnirersity of Denmark. Lyngby. R. \nM. < :ortess. G.H. Gonnet, D.E.G. Hare, D.J. Jeffrey, D.E. Knut8h. On the Lambert W Function, Advances \nin Co~np~. la -ional Mathematics 5 (1996), pp. 329-359. See ~1s: ( 1~3 : o?-iunentation of thr Maple \nSHARE Library.  \n\t\t\t", "proc_id": "289423", "abstract": "We present two purely functional implementations of the <i>computational differentiation tools</i> -- the well known numeric (not symbolic!) techniques which permit to compute pointwise derivatives of functions defined by computer programs economically and <i>exactly</i>. We show how the co-recursive (lazy) algorithm formulation permits to construct in a transparent and elegant manner the entire infinite tower of derivatives of higher order for any expressions present in the program, and we present a purely functional variant of the <i>reverse</i> (or <i>adjoint</i>) mode of computational differentiation, using a chain of delayed evaluations represented by closures. Some concrete applications are also discussed.", "authors": [{"name": "Jerzy Karczmarczuk", "author_profile_id": "81100291722", "affiliation": "University of Caen, France", "person_id": "PP31083852", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/289423.289442", "year": "1998", "article_id": "289442", "conference": "ICFP", "title": "Functional differentiation of computer programs", "url": "http://dl.acm.org/citation.cfm?id=289442"}