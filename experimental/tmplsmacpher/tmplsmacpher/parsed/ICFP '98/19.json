{"article_publication_date": "09-29-1998", "fulltext": "\n Pragmatic Subtyping in Polymorphic Languages Johan Nordlander Department of Computing Science Chalmers \nUniversity of Technology S - 412 96 Gijteborg, Sweden Email: nordlandks. Chalmers. se Abstract We present \na subtyping extension to the Hindley/Milner type system that is based on name inequivalence. This ap-proach \nallows the subtype relation to be defined by incre-mental construction of polymorphic records and datatypes, \nin a way that subsumes the basic type systems of both lan-guages like ML and Java. As the main contribution \nof the paper, we describe a partial type inference algorithm for the extended system which favours succinctness \nover gen-erality, in the sense that it never infers types with subtype constraints. The algorithm is \nbased on an efficient approx-imating constraint solver, and is able to type a wide range of programs \nthat utilize subtyping and polymorphism in a non-trivial way. Since constrained types are not inferred, \nthe algorithm cannot be complete; however, we provide a completeness result w. r. t. the Hindley/Milner \ntype system as a form of characterizing lower bound. introduction The combination of subtyping with polymorphic \ntype infer-ence has been under intensive study for more than a decade [Mit84, FM90, FM89, Mit91, Kae92, \nSmi94, AW93, EST95, Hen96, Reh97, MW97]. From the outset, this line of re-search has focused on complete \ninference algorithms, and the related notion of principal types. This direction is not hard to justify \nconsidering the evident merits of polymor- phic languages like ML or Haskell: program fragments can be \ntyped independently of their context, and programmers may rest assured that any absent type information \nwill be filled in with types that are at least as general, and at least as succinct, as any information \nthe programmer would have come up with. Still, although complete algorithms for polymorphic sub-type \ninference exist, practical language implementations that take advantage of these are in short supply. \nThe main reason seems to be that the algorithms are inefficient and the output, even for relatively simple \ninput expressions, ap-pears excessively long and cumbersome to read [HM95]. Many attempts have been made \nat simplifying the output from these algorithms, but they have only partially suc-ceeded, since the problem \nin its generality seems to be in- tractable, both in theory and practice [HM95, Reh97]. However, even \nif type simplification were not an issue, there is an inherent conflict between generality and suc-cinctness \nin polymorphic subtyping that is not present in the original ML type system. While the principal type \nof an ML expression is also the syntactically shortest type, the existence of subtype constraints in \npolymorphic subtyping generally makes a principal type longer than its instances. In particular, the \nprincipal type for a given expression may be substantially more complex than the simplest type gen-eral \nenough to cover an intendedset of instances! Thus, type annotations, which give the programmer direct \ncontrol over the types of expressions, are likely to play a more active role in languages with polymorphic \nsubtyping than they do in ML, irrespective of advances in simplification technology. In this paper we \ntake a pragmatic standpoint and em-brace type annotations as a fact of life. This enables us to focus \non the much simpler problem of partial polymorphic subtype inference. As the main contribution of the \npaper, we present an inference algorithm that always infers types without subtype constraints, if it \nsucceeds. This is a partic- ularly interesting compromise between implicit and explicit typing, since \nsuch types possess the ML property of being syntactically shortest among their instances, even though \nthey might not be most general. We might say that the al- gorithm favours readability over generality, \nleaving it to the programmer to put in type annotations where this strategy is not appropriate. An exact \ncharacterization of which pro-grams our algorithm actually is able to accept is still an open problem, \nbut we prove, as a lower bound, that it is complete with respect to the ML type system. We will also \nprovide some evidence why the algorithm is likely to work very well in practice. A second contribution \nof the paper is presumably the subtype relation itself, which is based on name inequiua-lence, in contrast \nto the structural ditto that dominate the standard literature [CWSS, Mit84]. By structural we mean the \ncommon practice of defining special subtyping rules for functions, records, and variants, etc, assuming \na given par-tial order over a set of nullary base types. We will instead assume that type constants can \nbe of any arity, and extend the partial order on base types to a relation on fully sat-urated applications \nof these constants. Functions, records, and variants can then be seen as special cases in this general \nframework. We believe that working with named and parameterized types, whose subtype relationships are \ndefined by declara-tion, has the immediate benefit of giving the programmer full control over the type \nstructure of a program. This can be a valuable tool in the design phase of a large system, and it also \noffers greater protection from certain logical errors. Furthermore, name inequivalence seems more in \nline with both the way datatypes are treated in functional languages, and with the object-oriented notion \nof a named class. And not the least, the ability to refer to types by name has a big impact on the readability \nof the output from our inference algorithm. The rest of the paper is organized as follows. Next sec- \ntion continues with some motivating examples, before the formal type system is introduced in section \n3. In section 4 the inference algorithm is described and its theoretical prop- erties are investigated. \nIn section 5 we show how records and datatypes fit into our framework. Section 6 contains a more practical \nexploration of the algorithm, by means of some programming examples. In section 7, type checking in the \npresence of incomplete type inference is addressed. Related work is surveyed in section 8, and section \n9 concludes. Motivating examples In polymorphic subtyping, principal types generally require subtype \nconst,raints, as is easily demonstrated by the follow- ing archetypical example: twice f x = f (f x) \nAssuming Int is a subtype of Real, this function can have the type (Real->Int)->ReaI->Int, as well as \n(Int->Int)->Int->Int, which forces the principal type of twice to be as general as (a->b)->a->b 1 b < \na. However, twice is certainly not a good representative of subtyping as it is used in common object-oriented \nlan- guages. Here the type systems are mostly monomorphic, which simplifies the typing problem considerably. \nStill, the elegance of subtyping is captured perfectly well in these lan- guages: great flexibility is \nachieved without introducing any additional burden on the programmer writing flexible code. Our approach \nto polymorphic subtype inference is to let functions like twice retain their original Hindley/Milner \ntype, and, in the spirit of object-oriented languages, sup-port subtyping only when types involved are \nknown. This choice can be justified on the grounds that (a->a) ->a->a is still likely to be a sufficiently \ngeneral type for twice in most situations, and that the benefit of a consistently readable output from \nthe inference algorithm will arguably outweigh the inconvenience of having to supply a type annotation \nwhen this is not the case. We certainly do not want to pro- hibit exploration of the more elaborate areas \nof polymorphic subtyping that need constraints, but considering the cost in- volved, we think it is reasonable \nto expect the programmer to supply the type information in these cases. As an example of where the lack \nof inferred subtype con-straints might seem more unfortunate than in the typing of twice, consider the \nfunction min x y = if x < y then x else y which, assuming < is a relation on Reals, will be assigned \nthe type Real -> Real -> Real by our algorithm. A more use-ful choice would probably have been a->a->a \n1 a < Real here, but as we have indicated, such a constrained type can 'The concrete syntax used here \nseparates a type and its constraints by the symbol I. only be attained by means of an explicit type annotation \nin our system. On the other hand, note that the princi- pal type for rain, a->b->c 1 a < Real, b < Real, \na < c, b < c, is a yet more complicated type, and presumably an overkill in any realistic context. So, \nan informal characterization of our inference algo- rithm is that it, in contrast to ML-style type inference, \nal- lows subtyping steps at application nodes when the types are known, as in e. g. sin 1. In addition, \nthe algorithm com- putes least upper bounds for instantiation variables when required, so that for example \nthe list C7, 3.141 will get the type [Real]. Greatest lower bounds for anonymous function arguments will \nalso be found, resulting in the in- ferred type Int -> (BooI, Int) for the term \\x -> (x < 3.14, x + \n1). Notice, though, that the algorithm assigns constraint-free types to all subterms of an expression, \nhence a compound expression might receive a less general type even though its principal type has no constraints. \nOne ex- ample of this is let twice f x = f (f x) in twice trunc 3.14 which is assigned the type Real, \nnot the principal type Int. As a demonstration of what name inequivalence can look like in practice, \nwe define two polymorphic record types that capture a minimal set of equality and comparison opera- tions, \nrespectively. struct Eq a = ieq :: a -> a -> Boo11 struct Ord a < Eq a = {less : : a -> a -> Bool) The \nrelation declared here states that for all types a, an object of type Ord a also supports the operations \nof Eq a. Since a only occurs contravariantly in Ord, depth subtyping can furthermore be used to derive \nthat Ord Real a subtype of Ord Int (which in turn is a subtype of Eq Int). It may be worthwhile to have \nthis example in mind when the subtype relation is introduced in the next section. 3 Type system The starting \npoint of our work is the let-polymorphic type system of ML (a.k.a the Hindley/Milner system), extended \nwith subtype constraints, a subtype relation, and a sub-sumption rule. Such extensions are well represented \nin the literature [Kae92, Smi94, AW93, EST95]; in this paper we will adopt a formulation by Henglein \n[Hen96], which has the interesting merit of being generic in its subtyping the-ory. Since our system \nis obtained by instantiation with a particular form of subtyping theory, the results of [Hen961 directly \ncarry over to our case. The generic part of this type system is shown in figure 1. In our formulation, \ntype variables are ranged over by cy and p, while type constants are ranged over by t and s. The arity \nof a type constant t is denoted rat. Type constants are considered drawn from some finite set that contains \nat least the function type constructor (+) with arity 2. Substitu-tions are ranged over by a. For notational \nconvenience, we will make use of the following abbreviations: r+lJ z (+)Tp TlD E V0.TJD f T10 C7J-p; \nz cl-pr< pforallr< pED We will also assume that terms have been renamed so that all bound variables are \ndistinct and not equal to any free variables. Term language: e .-..-I I I x e e Xx.e letE=eine variable \napplicution abstraction local definition Type language: r,p ::= ff 1 tT1...Tnnr ::= m.(TIc) X,D ::= \n{T _< p} r ::= {x: a} Typing rules: C,rtpe:T -,T C,rt-pe :T ( APP) C,rtpee :r c, r u {X : T } t-p e : \nT (ABS) c,r t-p XX.e : T + T C,rtPe:u c,ru{X:c}kpe :T (LET) C,rtpletX=eine :T C, I-tp e : fi.slD c k-p \n[?/ir]D (INST) c, r tp e : [y/z]T C,rf-pe:T Ct--pTs T (SUB) C,rbpt?:T Figure 1: The basic type system \nIt can easily be verified that if C,r l-p e : VZ.T~D is derivable without the use of rule (SUB), and \nif all type schemes in r have empty constraint sets, then 0,lY !-P e : VZ.r is derivable without rule \n(SUB) and with empty con-straint sets throughout (i. e. the judgement is also deriv-able in the original \nHindley/Milner type system). Let r t-HMe : 0 denote such a derivation. In section 4 we will prove a completeness \nresult w. r. t. derivations of this limited form. 3.1 Subtype relation Figure 2 shows the inference rules \nfor our subtype relation. With the exception of rules (DEPTH) and (CONST), this def- inition directly \ncorresponds to the one in [Hen96]. The varianceof a type constant, utilized in rule (DEPTH), is defined \nas follows: Definition 1 (Variance) For a type constant t, let the sets t+,t-G {l... nt} represent the \nargument positions that the subtype relation should treat CIY co-and contravariant, re-spectively. cu \n{T 5 ,O} t-p T 5 /I (HYP) (REFL)CtpT< T CkpT<p Ctpp<T (TRANS) Ci-pT< 7 (c k p Ti 5 p~) ~+ cc kp PJ 5 \nTP-(DEPTH) cc-p tT1 . ..Tn, 5 tP1 . ../Jnt T<PP (CONST) ctp@T< @p Figure 2: Subtype relation For a built-in \ntype, these choices must of course be con- sistent with the dynamic semantics of terms of that type, \nwhich for the function symbol means that (-+)+ = {2}, and (+)-= (1) (see [CWSS]). Thus, rule (DEPTH) \nreplaces and generalizes the standard rule for functions (called (ARROW) in [Hen96]). As regards types \ndefined by the programmer, section 5 will describe how variance information can be ex-tracted from record \nand datatype declarations. Our basic subtyping theory is defined in terms of poly- morphic subtype axioms \nthat may be instantiated by substi- tution, as witnessed by rule (CONST) in figure 2. Definition 2 (Subtype \naxiom) If t 7 and sj5 are ground type expressions, t # s, and ?: and i? contain no occurrences of t and \nY, then the term (t 7 < Y 3) is a subtype axiom relating t and s. We consider subtype axioms equivalent \nup to renaming of variables. To turn a subtype axiom into a judgement, we write (T < p) E s, or more \nconveniently T <s p, where s is some given set. The interaction between multiple subtype axioms is con- \ntrolled by the following definition: Definition 3 (Subtyping theory) A subtyping theory P is a set of \nsubtype axioms such that 1. all axioms in P relate distinct pairs of type constants. 2. if t?; <p sj3 \nand ~3 <p t ?, then Q(tT <p t ?), where ip is a most general unifier of 7 and p .  The rationale for \nthese restrictions is that the constraint solving problem is greatly simplified if all derivations of \ntf < sjj (with t # Y) can be normalized to contain just one application of rule (CONST) that is not a \npremise of some (DEPTH) step. The requirements on a subtype axiom fur-thermore outrule recursive axioms, \nwhich is vital to the ter- mination property of our constraint solver, as well as axioms that would interfere \nambiguously with depth subtyping. Since the set of valid subtyping judgements depends on a particular \nsubtyping theory P, we let the inference systems in figures 1 and 2 be parametric in P. This P has role \nanalogous to the partial order on types that is assumed in [Hen96]. To see that our definition is an \ninstance of the latter, note that for each P the relation {@(T, p) I T <p p A @ is a substitution}, contains \nall relationships derivable by our (CONST) rule, and that its reflexive closure defines a partial order \non ground type expressions as required. It is also trivial to verify that our formulation preserves the \nproperty that subtyping judgements are closed under substitution. Our requirements on a subtyping theory \nare very liberal, and allow both multiple sub-and supertypes (multiyfe in- heritance in object-oriented \nterms), and a form of rank-2 polymorphism that comes from the ability to use some type variables on just \none side in a subtype axiom. In concrete programming terms, a subtyping theory expresses both the subtype \nrelationships a language might provide as built-in (e, g. Int <p Real, or Action <p Cmd (Y () ), as well \nas the relationships that result from incremental type defini- tions made by the programmer (e. g. ColorPoint \n<P Point, or Orda <p Eqcr). Incremental definitions of records and datatypes will be further described \nin section 5. 3.2 Properties of typing judgements We end this section by recapitulating some important \nresults that are proven in general in [Hen96], and thus hold for our system in particular. For this purpose, \nthe subtype relation in figure 2 is extended to type schemes as follows: 1. Cl-p u 5 T if C, {r : CT} \nt.p z : T. 2. C l-p u < u if for all D and T such that D t-p C and D kp u < r we have D kp u 5 T.  \nProposition 1 Let 6 4 fv(C, l ). Then C, F l-p e : Va.rlD iffCUD,JYtpe:r. Proposition 2 Let C kp u 5 \nu . Then: 1. IfC,rl-pe:u thenC,rt-pe:u . 2. If C, ru{z : u } t-p e : a then C, I u{x : u} I-P e : 6 \n.  Theorem 1 (Principal types) Let fv(e) C dorn(l7). Then there exists a u such that: 1. 0,rkpe:u3 \n2. for all u , if 0, r tp e : U then 0, r kp u 5 u .  Theorem 2 (Subject reduction) Let --+ stand for \npqlet-reduction, and let P be such that C l-p r + p 5 T + p only if C t-p r 5 T and C b-p p < p . Then: \nIfC,lY t.pe:u ande--+e thenC,rkpe :u. 4 The inference algorithm We will now turn to the partial inference \nalgorithm we con- sider our main result. The core of this algorithm is an ap-proximating subtype constraint \nsolver, that has the merit of being simple and efficient -in fact it is defined as a small extension \nto the very efficient unification algorithm of Martelli and Montanari [MM82]. The main characteris-tic \nof our solver is that it approximates constraints of the form LY 5 p as equality constraints, and resorts \nto unifica- 1tion in these cases. When all such constraints are The latter axiom expresses that a value \nof type Action cm tic promoted to the monad of commands that execute in a local state a and return values \nof type 0. See [NC971 for more information on this monad. 3Note that this statement does not say anything \nabout the satis- fiabdstyof the constraints that are generally contained in o. from a constraint set, \ncomputation of least upper bounds or greatest lower bounds for the remaining variables becomes straightforward, \nand the algorithm can continue (just like in ordinary unification) by solving any constraints induced \nby the arguments of the bounding type expressions. This strategy is not very refined, however, and is \nbound to fail in certain situat,ions. Consider the constraint set {o < Int,o 5 /3, Real < p> With our \nstrategy, (Y would be unified with p, resulting in the unsolvable constraint set {o _< Int,ReaI 2 o}. \nIn this case, unifying o with Int would have been a better way to proceed. So, if we want to stick to \nour simple strategy (which we really do, considering its attractively close relationship to well-known \nunification techniques), it becomes vital to feed as small constraint sets as possible into the constraint \nsolver, in order to minimize the damage that variable/variable constraints can give rise to. For example, \nif the constraint set above actually was generated as the union of the sets {o 5 Int} and {o 5 P,ReaI \n< p}, solving the first set -separately would lead to success even with our simple strat-egy. This requirement \nactually puts us closer to the standard inference algorithm W [Mi178] than what is customary in the literature \non subtype inference. As we have indicated, we will have good reasons for solving constraints as soon \nas they are generated, rather than propagating them up-wards in the syntax tree as input to some final \nsimplifica-tion/solving pass. The complication we run into is of course that constraints involving variables \nfree in the assumption environment cannot be solved immediately if we want the output of the algorithm \nto be predictable. However, we will postpone the discussion on how we address this problem until we have \npresented the constraint solver. 4.1 Solving constraints The definition of the constraint solver is given \nin figure 3. It is presented as an inference system for judgements on the form Cp bp C, which should \nhave the operational interpre-tation given a constraint set C and subtyping theory P, return substitution \nQ . If a constraint set does not match any of the five clauses, the result of the algorithm is consid- \nered to be failure. This algorithm, as well as the subsequent inference al-gorithm, depends crucially \non the ability to generate fresh type variables. Instead of burdening the presentation with unessential \ndetails concerning name supplies, we use the fol- lowing convention: the symbol v always represents a \ntype variable that is distinct from any other variable in the deriva- tion or its context, except for \nother occurrences of v in the same rule. Likewise, we let L represent a vector of zero or more unique \nvariables, equal only to the variables denoted by other occurrences of p in the same rule. Clauses (A) \nto (D) in figure 3 essentially constitute the unification algorithm of Martelli and Montanari, or more \nprecisely, their nondeterministic specification of the algo-rithm. We prefer to use this abstract. formulation \nhere be- cause of its conciseness; the reader is referred to [MM821 for concrete information on how to \nmake an efficient implemen-tal I ,)ecially on how to avoid the costly occurs-check in clause (D). The \ncomputation of least upper bounds / greatest lower bounds in clause (u) is of course added by us; the \nreader should note, though, that our formulation degenerates to t f <p SF E=fv(i ,;ij ) c! bp Cu{tr< \n[Z7/z](tr )}u {[Ti/q(sp ) i 37) (El cf, kp ckJ{tT< SF} Figure 3: The constraint solver the original one \nin case the trivial subtyping theory P = 0 2. Since a subtype axiom never relates a type constant to \nis given. These standard algorithms use a partial order on itself, clause (E) can never match. pure type \nconstants derived from P, which relates t and s 3. The case where a type constant is non-variant (that \nis,iff t = s or t 7 < p s p for some r and j5. Since neither least i 4 tS U t-for some i 5 nt) only makes \nthe algorithm upper bounds nor greatest lower bounds are guaranteed to output more general than the unifying \nsubstitution. 0 exist (for one thing, i or 2 may be empty), both algorithms might return failure. A vital \nproperty of the constraint solver is that it ter-The fatbar operator in (UPC) 0 (flpZJ picks one of minates \non all input, either with a substitution, or with the its arguments in a failure-avoiding manner. In \ncase both implicit result failure if there is no matching clause. For-arguments evaluate successfully \nthis choice is assumed to be mally, guided by the inference algorithm, so that the alternative is Proposition \n5 The relation @ kp C is decidable. taken which results in the smallest inferred type. If neither alternative \nis better than the other we somewhat arbitrarily PROOF We show that there can be no infinite derivations \nspecify that the left argument is selected. Adding an extra of Cp /=p C by considering the size ]C] of \na constraint set C, parameter to the constraint solver representing the type that as defined by should \nbe minimized is straightforward, so we leave out the details in the interest of brevity. [~sSTP;lil 1 \nGz ITi 5 P:I Clause (E) is our main extension to the unification algo-T rithm; it has no correspondence \nin the original formulation since it handles the case where two distinct type constants IiF+~ ,,l z jT!2C \nITi 5 PiI are compared. Note that this rule is only applicable if there It?< sy = 1+pL t7 l+lsji < 331 \nexists an appropriate subtype axiom in P. if t? <p 33 The following lemma states a soundness property \nof the = F* ITI1 constraint solver. CY = i l pq = 4 + 2( IF;1 + nmax + kt) Lemma 3 If@ +=p C then 0 \nl-p @C. where nmax is the maximal arity of any t, and kt is com-Since the algorithm deliberately discards \ncertain solu- puted for each t by examining all axioms in P of the form tions, completeness cannot not \nhold by definition. However, (t? < 37) or (SF < tri), and taking the sum of the sizes a distinctive characteristic \nof our approach is that it is a of each 7 and 7 thus encountered. The well-foundedness of conservative \nextension of unificution. This is made precise this definition follows from the requirement that the \ntypebelow. constants related by a subtype axioms occur just once in the axiom (see definition 2). Definition \n4 We say that a substitution CI, unifies a con- It is now straightforward to show that every premise \nin straint set C iffar = Qr for cdl 7 5 7 E C. the definition of ip b=p C involves a constraint set of \nstrictly lesser size than the constraint set of the corresponding con- Lemma 4 If Q unijies C, then Cp \nj=p C and @ = a o @ clusion. The crucial step is rule (D), where one expands the for some @ . premise \nusing rules (E) and (c), and then utilizes the fact that for every T and p, IT 5 pi is less than ITI \n+ IpI. 0PROOF By the same argument as in [MM82], noting that 1. In clause (D), all ti and sJ must be \nequal, and 4.2 Algorithm definition since at least one of c and F must be non-empty, The actual inference \nalgorithm is shown in figure 4. Again (u&#38;) 0 (flpq) trivially succeeds. we use a formulation in terms \nof an inference system, whose TTY= jw(u) 7 {ut _< ~i}a Eo-u {at 5 Ut}a Eat, C,r C=.Pe:T G,r+pe,:T, @(CUUiC,),r~pe~:Qv \n= inut (VAR ) r u {X : U} bp X : [z/KIT @+P{T<,-+V} (APP ) C,rbPe:T judgements C, r FP e : T should tion \nenvironment l?, a subtyping Figure 4: The c,ru{x*: @(cr),r vi)' bp e:T /=p XTiZ'.e:Q(Zs;-+ @ I=P C\\Cr \nT) (ABS,) U=gen(c,r,T) c',ru{X:U}+pe':T' @(CUC~),ri==pletx=eine :@ 7 be read given an assump- theory \nP, and an expression e, return type T and constraint set C . The existence of a constraint set in the \noutput from our inference algorithm might at first seem contradictory to our whole approach. However, \nthese constraint sets have a very limited form, and are there just for the same purpose as the substitutions \nreturned by Milner s algorithm W: to propa-gate requirements on the free variables of the assumption \nenvironment r [Mi178]. But instead of deciding locally on a fixed substitution that makes l? meet its \nrequirements, our algorithm will ef-fectively work on cloned copies of r (rule (vAR)), and return a constraint \nset that relates these clones to the originaL It is not until a free variable of r exits its scope that \nits con-straints are collected and a satisfying substit,ution is com- puted (last premise of rules (ABS \n) and (LET )). A key ele- ment in this step is an operation which takes a constraint set and returns \nonly those const,raints which reference variables free in r: cr = {T 5 T' 1 (T 5 T') E c Ajv(T,T')nfv(r) \n# 8) We formalize the role of these generated constraint sets as follows: Definition 5 A constraint set \nC is a r-constraint iff for all T< T EC, eitherrscr OrT _crfOrSOmecuEfW(r). Proposition 6 If C, r +=p \ne : T then C is a r-constraint. PROOF By structural induction on e, using the fact that variables free \nin r never occur in T. 0 Corollary 7 If C, r k=p e : T and fq) = 0 then C = 0. Thus, for expressions \ndefined on the top-level of a pro- gram, the inference algorithm returns just a type if it suc-ceeds. \nGeneralization and instantiation play the same role here as in Milner s W. We define these operations \nfor the full 4Here we let the symbols O+ and u-stand for the free variables of CT that occur in co-and \ncontravariant positions, respeclively. inference algorithm type scheme syntax with constraints, even \nthough we will not need this generality until section 7: gen(C, TID) = VZ.rlD where 5 = ~v(T, D)\\fv(C) \ninSt(fi.TID) = [c/??](TID) Note that gen takes the f -constraint C as a parameter in-stead of r, since \nthe free variables of TID and r will always be disjoint. The vector notations in rule (APP ) stand for \nnested application and function type construction, respectively. The possibility of letting more than \none argument influ-ence the type of an application expression can have a cru- cial impact on the result \nof the algorithm. For example, if f : Va.cr 4 cy + Q and the numeric constants have their obvious type, \nthen f 7 3.14 will be assigned the type Real, whereas f 7 applied to 3.14 will result in a type error \n(as-suming Int < Real). A similar argument, although proba-bly less important. in practice, applies to \nthe use of nested abstractions in rule (ABS ). Thus it is implicitly understood that rules (APP ) and \n(ABS ) are matched against as large expressions as possible. It is worth noting that the result of the \nalgorithm is in- dependent of the order in which subexpressions are analyzed (it is only rule (APP ) \nthat offers any degree of freedom). A related property guarantees that function arguments can be reordered \nwithout causing any other effect than a permuta- tion of the corresponding argument types. Detailed examples \nof how the algorithm works can be found in section 6. We end this section with the main tech-nical results \nabout our inference algorithm. Theorem 3 (Soundness) If C,l? kp e : T then C,r kp e : T. PROOF By structural \ninduction on e. Theorem 4 (Partial completeness) If Qr bH e : VZ.r, then C,r bp e : p and there is a \na such that @ wnifies C, WI = @I-, and Qi p = T. PROOF By induction on t,he derivation of @r l-HM e : \nVZ.r, utilizing lemma 4 and the fact that if C is a r-constraint and @ unifies C then ~u(WT ) C jw(@I \n). Cl Detailed proofs of the theorems and lemmas stated in this paper will appear in the author s forthcoming \nthesis. 221 5 Records and datatypes Subtyping in real programming languages is mostly associ-ated with \nrecord-like structures such as classes in object-oriented languages, although it is also perfectly sensible \nto define subtyping for variant types, or daIntylIes as they are commonly called in funct,ional languages \n[CW85]. In this section we present an application of our subtyping frame-work in terms of a system of \nincrementally definable record types and datatypes, in a style reminiscent, of Haskell. Be-cause we have \nchosen to work with name inequivalence, most of the work that concern extensible type hierarchies is \nalready done in the definition of a valid subtyping the-ory; therefore the material in this section becomes \nmostly straightforward. A distinguishing feature of this extension is that the treatment of records and \ndatatypes is perfectly symmetric; that is, there is a close correspondence between record selec-tors \nand dat,atype constructors, between record construction and datatype selection, and between the two forms \nof type extension, which yields subtypes for records and supertypes for datatypes. Along this line, we \nwill treat both record se-lectors and datatype construct,ors as global constants -an absolutely normal \nchoice for what datatypes are concerned, but not so for records (see e.g. [MTHSO, Gas97]). Still, we \nthink that a symmetric treatment lilce ours has some interesting merits in itself, and that the ability \nto form hi-erarchies of record types alleviates most of the problems of having a common scope for all \nselector names. We also note that overloaded names in Haskell are given very much the same treatment, \nwithout much hassle in practice. Apart from the global scope for selectors, a hierarchy of record types \nin our system subsumes the essential featlures of the class hierarchies expressible in Java and C++. \nWe extend our term language with the following con-structs: ..- e ..-k datatype constructor 1 {k-+e}* \ndatatype selection 1 record selector (1 = e}* record construction . . . The datatype selection syntax \nmight seem a little unusual, since it does not contain any expression to scrutinize. We have chosen t,he \ngiven formulation in order to emphasize the symmetry between datatype selection and record construc-tion. \nThe syntax case e of {kl + e,} normally found in functional languages can then be defined as a syntactic \nsugar for the application {k, + et} e. A similar argument applies to the common dot-notation used for \nrecord selection. We assume that datatypes are declared using the syntax datatc > {st 2,) = {k, T~}~ \nApart from the optional component > {s, p,} , this is es-sentially ordinary Haskell. Implicit in the \ndefinition is the declaration of type schemes for each constructor =E.r, + tz, -3 and a set of subtype \naxioms that the subtyping theory must contain. Since it only makes sense operationally to let datatypes \nbe subtype-related to (SEL) c, I- I- 1 : Ul (CON) c, r t- k : Ok C, r I- 1, : T t T* {lt} = ? C, r k \ne, : T, (REC) C,r t- (1, = e,) : T c,r k k, : c -+ T {I;,}' =? c,r t-e, : T + 7' (ALT: c',r' I- {k, -+ \ne,}' ; T --t T' Figure 5: Records and datatypes other datatypes (which are declared analogously), the \ntran-sitive closure prescribed by condition 2 in definition 3 can always be successfully constructed \nfrom any given set of type declarations. However, the result of this operation is not necessarily a valid \nsubtyping theory, which means that a language implementation must also perform a consistency check according \nto definitions 2 and 3 (first condition) be-fore a new set of type declarations can be accepted. Note \nthat since all constructor names k, are required to be globally unique, there is no way of modifying \nthe type of a const,ructor when a datatype is extended. Thanks to subsumption, old constructors can nonetheless \nbe used to construct values of the new type. For records, the declarations look very much the same, \nexcept that the new type now denotes a subtype instead of a supertype. struct tZi < {s, 7j,} = (1, : \nT~}~ Type schemes for the selectors are formed analogously =vz.tz-i T,, ul, as well as a set of subtype \naxioms: {t z < 3, ;ii,) The argument above regarding closure generation and va-lidity of the resulting \nsubtyping theory applies here as well. And as for datatypes, it is the subsumption rule that will allow \nus to apply old selectors to values of the new record type. Variances for user-defined types must also \nbe calculated by a language implementation. Essentially this information follows from the way argument \nvariables are used in con- structor/selector types and sub-/supertypes; however, the fact that type declarations \nmay be recursive causes a slight problem. Fortunately, a bit of abstract interpretation over a domain \nconsisting of the four subsets of {+, -} will do the job. The typing rules for records and datatypes \nare given in figure 5, while figure 6 shows the necessary algorithm ex-tensions. These definitions contain \nno surprises, the only difference subtyping makes in the typing rules is that we must generally rely \non subsumption in order to find a com- mon argument type for the selectors in rule (REC). The same holds \nfor the result type of the constructors in rule (ALT). Since a type constant contains no information \nabout its own declaration, checking that datatype selection / record 0,r k=p 1 : ind(m) lSEL ) (CON ) \n0, r /==P k : inst(ak) Figure 6: Type inference for datatypes and records, construction is exhaustive \nmust be done using an additional attribute associated with each type constant. This attribute is written \nt, and is defined as the set of constructor/selector names directly introduced in the declaration of \nt, plus all the names associated with the declared sub/super-types of t. We tacitly lift this attribute \nto a partial function on type expressions in the obvious way. We have assumed here that both kinds of \ntype declara-tions only mention variables that appear in the argument list a. As has been described by \nLaufer and Jones among oth-ers, lifting this restriction for a constructor type naturally leads to a \nsystem with support for local existential quan-tification, while the corresponding generalization for \nselec-tor types is best interpreted as local universal quantifica-tion [LBu92, Jon96, Jon97]. Taking \nthis idea further, by also letting the declared sub-/supertypes contain free vari- ables, opens up some \ninteresting possibilities to explore (re-call that the implied subtype axioms remain valid in spite of \nthis change). For one thing, a term can now be assigned an existentially quantified type simply by means \nof a type annotation, or even just by using the term in the right con-text. However, although this additional \nfeature is natural and carries no extra implementational cost, it is not clear how useful it really is \nin practice. 6 More examples In this section we will discuss the behaviour of our inference algorithm \nby means of some programming examples, that we have tested on a prototype implementation running under \nHugs 1.4. The concrete syntax we use follows the syntax of Haskell. Input to the prototype is an expression \nwritten after the prompt >. Output follows after the : : sign, and is either a type or an error message. \nWe will assume an environment where Int 5 Real, and where the following declarations are in scope: struct \nBox a = fst :: a struct Couple a < Box a = snd :: a struct Couple < Box Int = snd :: Boo1 rdict : : \nOrd Real idict : : Ord Int 01 :: Int -> Int -> Boo1 Here Ord is assumed to be defined as in section \n2. Some basic examples of how the algorithm works have also been given in section 2. Below follows some \nexamples that need slightly more involved computations, including least upper bounds, greatest lower \nbounds, and depth sub-typing with contravariance (due to type Ord). > a = {fst = 1) :: Box Int > b = \nCfst = 1; snd = 3.14) :: Couple Real > c = {fst = idict; snd = rdict) :: Couple (Ord Int) > d = {fst \n= 1; snd = False) : : Couple > e-Ifst = 3.14; snd = False) : : ### Type error These types are all inferred \nusing rule (REC'). We give the derivation used in example d as an illustration; the other derivations \nfollow the same pattern. Boxcu + cr = inst(afst) Couple + Boo1 = inst(Usnd9) [Int/cu, Couple /v] k=p \n{V 2 Couple , v < Box cr} {f st, snd } = CoGe 8, r +=p 1 : Int 8, r +P False : Boo1 Id bp {Int 5 Int, \nBoo1 < Bool} 8,r +P {fst = 1, snd = False} : Couple The first constraint solving problem above is non-trivial, \nso we write it out as well. It also illustrates most of the clauses that. const,itute the constraint \nsolver. Some obvious premises are left out for space economy reasons, though, most notably Couple <P \nBox Int in clause (E), and Couple = Couple ilp Box in the bottom application of clause (D). zig3 CA) \n[Int/o] +=p {Int 5 o} (D) (cl [Int/o] k:p {Box Int 5 Box o} [Int/o] k=p {Couple 5 Couple , Box Int 5 \nBox o} I:, [Int/o] /==p {Couple 5 Box CY} [Int/cr] bp {Couple 5 Couple , Couple I: Box cr} [:I [Int/cu, \nCouple /rj] /=P {I/ 5 Couple , v < Box cy} The next two examples illustrate the point of collecting constraints \non the assumption environment, instead of solv- ing them directly: > fx= (x.fst, x.snd) : : Couple a \n-> (a,a) gx -(x.fst, x.snd ) : : Couple -> (Int ,Bool) Part of the derivation for the lambda-abstraction \ng is shown below, where (incidentally) C is identical to the constraint set solved in detail in the previous \nexample. C, {x : u} k=p (x.fst, x.snd ) : (o,Bool) @ /=p C (ABS ) 0,0 bp Xx.(x.fst,x.snd ) : Couple + \n(Int,Bool) Had the algorithm not accumulated any constraints on v but instead attempted to solve these \nimmediately, the order in which subexpressions are visited would make a difference, and the naive order \nwould have failed at the subderivation -, {z : Box a} j=p x.snd : _ On the other hand, the following \nexample would not have been accepted if the algorithm just naively collected all con-straints and solved \nthem on the top-level: > hX if x=0 then 3.14 else x :: Int -> Real If the type variable assigned to x \nis v, our algorithm will analyze the right-hand side of h essentially as follows:5 {V 5 Int}, r kp x==o \n: Boo1 0, r bp 3.14 : Real {V 2 V/t), r k=p x : Y [Real/v , Real/v ] /=p {Real _< v , Y 5 v } {V < Int, \nv 5 Real}, P bp if x==O then 3.14 else x : Real Notice here that if the algorithm instead just had returned \n{V 5 Int, u 5 Y , Real 5 v , Y < v }, our simple con-straint solver would be bound to fa?l when applied \nto this constraint set at a later stage, for reasons discussed in the beginning of section 4. Recall \nthat the function min from section 2 was assigned a rather limited type by our inference algorithm. Interest-ingly, \nmin can be given an alternative coding, inspired by overloading in Haskell, that avoids losing information: \n> mindxy= if d.less x y then x else y :: Ord a -> a -> a -> a > i = min idict 1 2 :: Int > j = min rdict \n1 2 :: Int > k = min rdict 1 3.14 :: Real The instantiation variable that corresponds to a above will \nhave both upper and lower bounds in these examples. In ex- ample j the lower bound Int is chosen in favour \nof the upper bound Real, on the grounds that it makes the resulting type more precise (see section 4.1). \nThe effect of narrowing the context of an application can sometimes be puzzling: The rule used here can \neasily be derived if the syntax sugar if et then ea else e3 is expanded into {True + A() -t es,False \n-t A() + e3) e1. > 1 = min rdict 1 :: Int -> Int > k = min rdict :: Real -> Real -> Real Still, the algorithm \njust consequently uses every piece of local information it has -if there are no lower bounds on a variable \nit will use the upper bounds. In the next section we will see how the behaviour of the algorithm can \nbe improved by letting it take some contextual information into account as well. 7 Type checking Since \nwe are working with an incomplete inference algo-rithm, the standard method of checking type annotations \nafter a principal type has been inferred will not work. As a remedy, we develop an alternative approach \nto type anno-tations in this section, that makes type checking an integral part of the inference algorithm. \nFirst we extend the expression syntax to include terms annotated with a signature ..- e ..-. . . :: u \nI e and add a corresponding rule to the type system C,rkpe:O (TYP) c,r kp (e :: U) : u Since a signature \nis a type scheme, it can contain subtype constraints as well as explicit quantifiers, even though it \nis arguably more convenient to let the latter be implied in the concrete syntax, as is done for example \nin Haskell. We will actually require here that all variables appearing in a signa- ture be botmd in the \nsame annotation, although extending the system to deal with type variables of nested scope should present \nno specific problems. The main change we introduce compared to the previous sections is that the type \nof an expression can now also be determined by the type expected by the context in which the expression \nappears. In principle such contextual information has its origin in an explicit type annotation, but \nwe also want to make sure that expected types are propagated down to subexpressions as far as possible. \nFor an application e e this requirement means that if the result type is expected to be T, e should have \nthe type T + T, where T is the type inferred for e .6 A consequence of propagating information this way \nis that an expected type will generally contain a mixture of universally quantified variables (which \nmust be protected from instantiation and in effect be treated as unique con-stants), and free instantiation \nvariables originating from in-ferred types. This complicates the type checking algorithm slightly, but \nonce it is handled properly, two other benefits come almost for free. Firstly, the type checker and the \ninference algorithm can now be integrated, considering type inference as a special Alternatively, we \ncould have let the contextual information flow in the other direction and extract the expected type of \ne from the type inferred for e, as is done in [PT98]. But this scheme would not make any good use of \nthe parameter T, and since we moat often do not need to help the inference algorithm finding types for \nthe anonymous lambda abstractions that might appear in e (which is the motivation behind the choice in \n[PT98]), it makes more sense to collect as much information as possible before instantiating the generally \npolymorphic type of e. case of checking where the expected type is a unique instan-tiation variable. \nSecondly, it becomes possible to let the programmer exploit the use of partial type signatures, i. e. \nsignatures where the type component may contain holes in the form of wildcards (-). Such signatures may \nfor ex-ample come in handy in situations where t,he programmer needs to specify that the type of an expression \nshould be an application of a specific constant, but where the inference al-gorithm may be trusted with \ninferring the type arguments. We will not develop this idea any further here; we only want to emphasize \nthat partial signatures is a nat,ural generaliza-tion that our algorithm directly supports, provided \nthat all wildcard types are replaced with unique inst,antiation vari-ables prior t,o type checking. The \nextended inference algorithm is shown in the form of an inference system in figure 7. Judgements in this \nsys-tem are of the form C, D, r b=p e : @P(T) and should be read given an assumption environment r, a \nsubtyping theory P, an expression e, and an expected type T, return con-straint sets C and D, and substitution \na . The intuition behind this judgement form is captured by the soundness theorem for the extended algorithm, \nwhich states that, un-der certain reasonable restrictions on the variables free in T, if C, D, r f=p \ne : at)(~) then C U D, r t-p e : @ T. The rationale for returning two constraint sets is that we want \nto separate those constraints that restrict variables free in r (the r-constraint C) from those that \ndirectly or in- directly originate from type signatures supplied by the pro- grammer (the set D). The \nlatter set is a nalural part of the type schemes generated in rule (LET\"). Note that by let-ting D contain \ninstantiated constraints from rule (VA@), as well as constraints that appear directly in a signature \n(rule (TYP\")), we obtain an algorithm with the intuitive prop-erty that both z and y receive the same \ntype scheme in let X = e :: ainlety=zine . In rule (LET\") it is assumed that there exists a constraint \nsimplification algorithm simp that is applied to D before a type scheme is generated. This algorithm \ncan of course be arbitrarily sophisticated; however, since the constraints in its domain are either explicitly \ngiven in the program text, or instantiations of such constraints, it seems like the only really necessary \nrequirements on yimp are that it removes tautological constraints like Int < Real, and flags an error \nif a constraint, is obviously incons&#38;ent. Variables which are universally quantified in an enclosing \nderivation are assumed to be skolenziaed, i. e. replaced with unique type constants of arity 0, whenever \nthey appear free in a premise (see the first two premises in rule (TYP\")). For this purpose we assume \nthat the set of type constants contains a set of skolem constants that is sufficiently large for the \nprogram in question, and that fs(C) returns the set of skolem constants that occur in C. Furthermore, \nwe apply the same not,ational technique as we do for indicating unique type variables, and assume that \njj st.ands for a vector of zero or more skolem constants that are equal only to the constants denoted \nby other occurrences of ji in the same rule. The annotated expression e in rule (TYP\") is checked according \nto an extended subtyping theory P $ [jZ/Z;lD , where @ denotes the operator implicit.ly referenced in \nsec-tion 5 that adds axioms to a subtyping t.heory and validates the generated closure. Note that all \naxioms in [fc/a]D must be monomorphic, since by assumption f~( D ) C_ Cu. The ex-tended subtyping theory \nis also used to solve all direct and indirect signature constraints encountered while checking e, in \norder to est,ablish that they are all implied by D . When it has been verified that no universally quantified \nvariable escapes its scope, a fresh instance of the signature is gener- ated, arid a final check is made \nto ensure that the annotated type fits below the type expected by the context. The part,s of t.he type \nchecker that, deal with records and datatypes are straightforward and will not be shown. Type checking \ndoes nonetheless appear to be especially useful for these const,ructs in practice, since type signatures \nfor all se-lectors and constructors are already given in their respective t,ype declarat.ions. Finally \nit should be noted that this integrated type-checking/type-inference algorithm is still incomplete, in \nthe sense that adding a top-level type annotation is not nec-essarily sufficient to make a typeable program \naccepted by the algorithm. Seeing this is easy: the type checker relies on type inference for subexpressions \nin several places, and we know that the inference algorithm is incomplete. An (almost) complete type \nchecker could probably be developed for our system by following the ideas in [TS96, MW97], and would \nbe valuable for the same reason that a complete inference algorithm is. However, since the for-mer kind \nof algorithm by necessity must be based on the latter, problems regarding unreadable diagnostic messages \nwould reappear, and the programmer would experience sig-nificantly different responses from the system \ndepending on whether a term has a type amlotation or not. Despite its less ambitious goal, our approach \nto type checking fulfills a vital need in conjunction with our infer-ence strategy, in that it enables \nthe programmer to guide the inference algorithm at. points where it would otherwise have made a bad choice. \nIndeed, identifying these points on basis of error messages is likely to be facilitated by the very same \nproperties that contribute to making the algorithm in-complete: contextual information in form of expected \ntypes is explicitly propagated top-down, and readable types are assigned not, only to top-level terms, \nbut to every subex-pression as well. 8 Related work Most of t,he work on subtype inference cited in the \nin-troduction take a structural view of the subtype rela-tion, that is, types are subtype-related if \nand only if they have the same structure [a function, a pair, a list, etc) and their respective constituent \nparts are subtype-related [Mit84, FM90, FM89, Mit31, I<aeS%, Smi94, AW93, MW97]. This leads to a subtyping \ntheory where the only primitive subt,ype relationships are between nullary base types. The system described \nin [EST951 follows [CW85] and allows two records LO be subtype related if their set of selectors are \nin a SuperYet relationship. Still, it is the structure of a record that defines its position in the subtype \nhierarchy, not any in- tentions explicitly declared by the programmer. Henglein s type system [Hen961 \nis unique in that it is generic in the choice of subtyping theory, something which we have ex-ploited \nin our system based on name inequivalence. Despite the fact that type systems with name-based (in)equivalence \ndominate among both functional and object- oriented languages currently in widespread use, polymorphic \nsubtype inference for such systems have not gained much at- tention. Reppy and Riecke develop a scheme \nwhere object types are relat,ecl by declaration in their object-oriented ML variant OML [RR96], while \nJategonkar and Mitchell outline a similar (future) system for abst.ract datatypes in [JM93]. Both these \nsystems consider only parameterless type con-stant,s. Depth subtyping and variances appear in Freeman \ns ?g = ft(a) T lD = inst(a) Qr b-P {[77;/qT 5 T} (VAR\") , @[qiE]D, r u {x : 0) +P z : @p(T) cp~p{~-+uy \nT} c,u,ru{z, @ Cr, @,\"D, r c, D, r bp e : Q(U) u = yen(C, @ulsimp(D)) Q\"(C u ck), a\"#, r : @u,} b-p \nXz;.e C , D ,r bp let E c, D, r FP~B[~z/~~LY e : @([TiPI+) a I=PI~[S;IZIW D kp e : @ (Qu) Cp j=p C\\Cr \n(ABS\") : @\"V@(T) U {x : u} +p e : Q (T) = C:in e : Q @ (T) j? $ fs(@ C) a +p {@ @[F/~;]T < T} (TYP\") \n@ Q C, @ [r;/~];lo , r bp (e :: E.r lD ) : a (~) Figure 7: The integrated type-checking/type-inference \nalgorithm work on refinement types [Fre94], although in a quite differ-ent setting where the subtyping \nsystem is used to provide additional information on terms that already have an ML type. We are not aware \nof any system besides ours that provides a combination of subtype declarations resembling our polymorphic \nsubtype axioms, and variance-based depth subtyping. The choice to settle for a deliberately incomplete \ninfer-ence algorithm sets our work apart from most of the sys-tems above. Aiken and Wimmers constraint \nsolver makes some non-conservative simplifications in the interest of ef- ficiency, although for a very \nrich type language including union and intersection types [AW93]. Reppy and Riecke have implemented type \ninference for OML such .that only constraint-free type schemes are inferred [RR96], but they do not describe \ntheir algorithm, nor do they provide any technical results. Most related to our approach is perhaps Pierce \nand Turner s work on local type inference [PT98]. They start with a variant of System F with subt,yping, \nand develop an inference technique that is able to fill in missing type arguments, as well as missing \nannotations, in many cases. Their method for inferring types for anonymous abstrac-tions is similar to \nour implementation of integrated type-checking, although their algorithm switches between strict phases \nof inference and checking, which ours does not. Pierce and Turner s algorithm is not complete w. r. t. \nthe Hindley/Milner system, but they have the advantage of a declarative specification of the programs \nthat the inference algorithm accepts. Cardelli s implementation of F< [Car931 also contains a partial \ntype inference algorithm that, like ours, uses unifica- tion to solve constraints involving unbound type \nvariables. However, t,his greedy algorithm solves all constraints, in-cluding those on variables in the \nassumption environment, at the earliest stage possible, thus its behaviour can some-times appear quite \nunpredictable. Objective ML [RV97] conservatively extends ML with subtype inference and a powerful type \nabbreviation mech-anism that achieves much of the succinctness we obtain by using name inequivalence. \nThe type system of Objective ML is based on extensible records, ((bough, and does not support subsunrption. \nMany of the systems mentioned allow subtype con-straints to be recursive [I<ae92, EST95, AW93, RV97, \nMW97], thus facilitating certain object-oriented program-ming idioms not otherwise expressible [BCC+96]. \nOur system does not currently support recursively constrained types, although a generalization that would \nallow such con- straints t,o be checked (just as ordinary constraints can be checked but not automatically \ninferred by our system) would be a worthwhile future extension. 9 Conclusion and future work We have \ndescribed a subtyping extension to the Hind-ley/Milner type system, and an accompanying partial type \ninference algorithm which (1) finds principal types for all ex-pressions that do not use subtyping, (2) \nfinds succinct types for most expressions that, use subtyping but do not need con- straints, and (3) \nfails for all expressions that must be typed using constraints. We have also shown how a subtype rela-tion \nbased on name inequivalence can be used to implement a system of incrementally types, that is close in \nspirit like ML and Haskell, and object-oriented languages The inference algorithm totype interpreter. \nInitial definable datatypes and record to both datatypes of languages interface hierarchies in popular \nlike Java. has been implemented as a pro- experiments suggest that the al- gorithm works very well in \npractice, and that explicit type annotations are rarely needed. Currently, a modification of Hugs is \nunder development, that would allow practical ex-perimentation on a larger scale. Future directions for \nthis work include finding better characterizat.ions of which expressions the algorithm ac-cepls. One \npossible approach to this could be to identify a set of restrictions on terms and typing derivations \nthat would yield minimal types in the presence of subtyping, and show that under these restrictions, \nthe algorithm is complete. There are also some interesting extensions that we would like to explore, \nincluding type-checking recursive constraint,s, implement.ing higher-order polymorphism as in [Jon93], \nand investigat,ing ways to encode and eventually integrate the overloading system of Haskell. Acknowledgement \nThe author would like to thank [Jon971 M.P. Jones. First-Class Polymorphism with Thomas Hallgren, Magnus \nCarlsson, Lars Pareto, BjGrn von Type Inference. In ACM Principl&#38; of Program- Sydow, Fritz Henglein, \nand the ICFP referees for comments, ming Languages, Paris, France, January 1997. encouragement, and instructive \nfeedback. ACM Press. References [AW93] [BCC+96] [Car93] [CW85] [EST951 [FM891 [FM901 [Fre94] [Gas971 \n[Hen961 [HM95] [JM93] [Jon931 [ Jon961 A. Aiken and E. Wimmers. Type inclusion constraints and type inference. \nIn ACM Fuuc-tional Programming and Computer Architecture, Copenhagen, Denmark, June 1993. Kim B. Bruce, \nLuca Cardelli, Giuseppe Castagna, the Hopkins Objects Group (Jonathan Eifrig, Scott Smith, Valery Trifonov), \nGary T. Leavens, and Benjamin Pierce. On binary meth-ods. Theory and Practice of Object Systems, 1(3):221-242, \n1996. Luca Cardelli. An implementation of F<. Tech-nical Report Research report 97, DEC Systems Research \nCenter, February 1993. L. Cardelli and P. Wegner. On understand-ing types, data abstraction, and polymorphism. \nComputing Surveys, 17(4), 1985. J. Eifrig, S. Smith, and V. Trifonov. Sound Poly- morphic Type Inference \nfor Objects. In OOPSLA 95. ACM, 1995. Y. Fuh and P. Mishra. Polymorphic subtype inference: Closing the \ntheory-practice gap. In Theory and Practice of Software Development, Barcelona, Spain, March 1989. Springer \nVerlag. Y. Fuh and P. Mishra. Type Inference with Sub-types. Theoretical Computer Science, 73, 1990. \nTim Freeman. Refinement Types for ML. PhD thesis, School of Computer Science, Carnegie Mellon University, \nPittsburg, PA, March 1994. Benedict R. Gaster. Polymorphic Extensible Records for Haskell. In Proceedings \nof tiae Ha&#38;e11 Workshop, Amsterdam, Holland, 1997. Fritz Henglein. Syntactic properties of polymor- \nphic subtyping. TOPPS Technical Report (D-report series) D-293, DIKU, University of Copen- hagen, May \n1996. M. Hoang and J. Mitchell. Lower Bounds on Type Inference With Subtypes. In ACM Princi-ples of Programming \nLanguages, San Francisco, CA, January 1995. ACM Press. L. Jategaonkar and J.C. Mitchell. Type inference \nwith extended pattern matching and subtypes. Fund. Informaticae, 19:127-166, 1993. M.P. Jones. A System \nof Constructor Classes: Overloading and Implicit Higher-Order Polymor-phism. In ACM Functional Programming \nand Computer Architecture, Copenhagen, Denmark, June 1993. ACM Press. M.P. Jones. Using Parameterized \nSignatures to Express Modular Structure. In ACM Principles of Programming Languages, St Petersburg, FL, \nJanuary 1996. ACM Press.  [Kae92] [L;iu92] [Mi178] [Mit84] [MitSl] [MM821 [MTHSO] [MW97] [NC971 [PT98] \n[Reh97] [RR961 [RV97] [Snii94] [TS96] S. Kaes. Type inference in the presence of over- loading, subtyping \nand recursive types. In ACM Lisp and F unctional Programming, pages 193-?04, San Francisco, CA, June \n1992. K. Lgufer. Polymorphic Type Inference and Ab-stract Datu Types. PhD thesis, New York Uni-versit,y, \nJuly 1992. R. Milner. A Theory of Type Polymorphism in Programming. Journal of Computer and System Sciences, \n17(3):348-375, 1978. J. Mitchell. Coercion and type inference. In A CM Principles of Programming Languages, \n1984. J. Mitchell. Type inference with simple subtypes. Jourkkol of Flk?lctiorltk/. Programming, 1(3):245-285, \n1991. A. Martelli and U. Montanari. An efficient uni-ficat,ion algorithm. ACM Transactions on Pro-gramming \nLanguages and Systems, 4(2), 1982. R. Milner, M. Tofte, and R. Harper. The Defini-tion of Standard ML. \nMIT Press, 1990. Simon Marlow and Philip Wadler. A practical subtyping system for Erlang. In ICFP, pages \n136-149, Amsterdam,Holland, June 1997. Johan Nordlander and Magnus Carlsson. Re-active Objects in a Functional \nLanguage -An escape from the evil I . In Proceedings of the Ha&#38;e11 Workshop, Amsterdam, Holland, \n1997. Benjamin C. Pierce and David N. Turner. Local type inference. In Couzference Record of POPL 98: \nthe .Wth ACM SIGPLAN-SIGACT Sympo-sium on Principles of Programnking Languages, 1998. 3. Rehof. Minimal \ntypings in atomic subtyping. In ACM Principles of Programming Languages, Paris, France, January 1997. \nACM Press. J. Reppy and J. Riecke. Simple objects for Stan- dard ML. In SIGPLAN Conference on Pro-grammany \nLalkguuge Design and Implementation (PLDI), May 1996. Diclier Rkmy and J&#38;me Vouillon. Objective ML: \nA simple object-oriented extension of ML. In ,4CM Principles of Programming Languages, Paris, France, \nJanuary 1997. G. Smith. Principal type schemes for functional programs wit,h overloading and subtyping. \nSci-ence of Computer Programming, (23):197-226, 1994. Valery Trifonov and Scott Smith. Subtyping Constrained \nTypes. In Third International Static ,4nalysis Symposum, LNCS 1145, Aachen, Ger-many, September 1996. \n  \n\t\t\t", "proc_id": "289423", "abstract": "We present a subtyping extension to the Hindley/Milner type system that is based on <i>name inequivalence</i>. This approach allows the subtype relation to be defined by incremental construction of polymorphic records and datatypes, in a way that subsumes the basic type systems of both languages like ML and Java. As the main contribution of the paper, we describe a partial type inference algorithm for the extended system which favours succinctness over generality, in the sense that it never infers types with subtype constraints. The algorithm is based on an efficient approximating constraint solver, and is able to type a wide range of programs that utilize subtyping and polymorphism in a non-trivial way. Since constrained types are not inferred, the algorithm cannot be complete; however, we provide a completeness result w.r.t. the Hindley/Milner type system as a form of characterizing lower bound.", "authors": [{"name": "Johan Nordlander", "author_profile_id": "81100299897", "affiliation": "Department of Computing Science, Chalmers University of Technology, S - 412 96 G&#246;teborg, Sweden", "person_id": "PP39036664", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/289423.289445", "year": "1998", "article_id": "289445", "conference": "ICFP", "title": "Pragmatic subtyping in polymorphic languages", "url": "http://dl.acm.org/citation.cfm?id=289445"}