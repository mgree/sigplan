{"article_publication_date": "09-29-1998", "fulltext": "\n Implementing Typed Intermediate Languages* Zhong Shao Christopher League Stefan Monnier Dept. of Computer \nScience Yale University New Haven, CT 06520 {shao,league,monnier}@cs . Yale. edu. Abstract Recent advances \nin compiler technology have demonstrated the benefits of using strongly typed intermediate languages \nto compile richly typed source languages (e.g., ML). A type- preserving compiler can use types to guide \nadvanced opti-mizations and to help generate provably secure mobile code. Types, unfortunately, are very \nhard to represent and manip- ulate efficiently; a naive implementation can easily add expo- nential overhead \nto the compilation and execution of a pro- gram. This paper describes our experience with implement-ing \nthe FLINT typed intermediate language in the SML/NJ production compiler. We observe that a type-preserving \ncompiler will not scale to handle large types unless all of its type-preserving stages preserve the asymptotic \ntime and space usage in representing and manipulating types. We present a series of novel techniques \nfor achieving this prop-erty and give empirical evidence of their effectiveness. Introduction Compilers \nfor richly typed languages (e.g., ML [21]) have long used variants of the untyped X-calculus [2, lo] \nas their intermediate languages. An untyped compiler first type-checks the source program, and then translates \nthe program to the intermediate language, discarding all the type infor-mation. Types are used to ensure \nthat the program will not go wrong at run time, but they do not affect the rest of compilation and execution \nin any way. Recent advances in compiler technology have demon-strated many distinct advantages of using \nstrongly typed intermediate languages to compile richly typed source lan- *This research was sponsored \nin part by the Defense Advanced Research Projects Agency IT0 under the title Software Evolution using \nHOT Language Technology, DARPA Order No. D888, issued under Contract No. F30602-96-2-0232, and in part \nby au NSF CA-REER Award CCR-9501624, and NSF Grant CCR-9633390. The views and conclusions contained in \nthis document are those of the authors and should not be interpreted as representing the official poli-cies, \neither expressed or implied, of the Defense Advanced Research Projects Agency or the U.S. Government. \n guages. A type-preserving compiler type-checks the source program, but then translates both the program \nand the (inferred) type information into the intermediate language. The rest of the compiler can use \ntype8 to guide advanced op-timisations [28, 16, 22, 34, 6] and to help generate provably secure mobile \ncode [13, 26, 19, 23, 171. The compiler can also propagate the type information into the target code \nto support sophisticated run-time type dispatches and garbage collection (14, 22, 34, 431. Unfortunately, \ntype information is very hard to repre-sent and manipulate efficiently, especially when the under-lying \ntype system involves ML-like polymorphic types and module types [21]. A naive implementation can easily \nadd exponential overhead to the compilation and execution of a program. For example, in the following \nML program: fun f x = x fun toy0 s let fun g y = ((((f f) f) fl . . . fl y in g 3 end the identity function \nf has polymorphic type trcU.cr + CY. Suppose we apply f to itself n times as shown. According to the \nML type inference algorithm [5], the rightmost f has type 2 1 = (Y + CY, while the leftmost f gets instantiated \nto T,, = T,-1 + T,,-1. Clearly, representing T,, as a tree- like structure would require 0(2 ) space, \n80 a sufficiently small n (e.g., 30) would wreck the efficiency of the compiler. To avoid such exponential \nblowup, we must represent and manipulate T,, as a linear-sized dag: In fact, we must ensure that all \ntype-related operations in the compiler (including those at run time if we pass types there) would handle \nsuch large type8 in the same way. For instance, in the let body above, when g is specialized to int+int, \nwe need to apply a substitution (from cr to int) to all instance8 of Ti; clearly, we must traverse the \ndag in linear time and preserve its shape. Although the preceding example is a bit contrived, large \nIt is well known that ML type inference can take exponential time and space on certain kinds of ML programs \n[20]; the toy function defined here, however, does not belong to this category. An untyped compiler could \ncompile the toy function without any problem. Ratio CM eXene of tree size number average percentage number \naverage percentage over dag size of types tree size of total size of types tree size of total size l-3 \n12,210 42 16.39% 32,376 58 1.91% 4-15 1,304 469 19.22% 7,547 998 7.57% 16-63 102 9,224 29.54% 2,889 8,866 \n25.73% 64-255 47 21,704 32.03% 818 75,153 61.74% 256-1023 2 45,518 2.86% 8 379,315 3.05% For each type \nbuilt during the compilation, we calculate the size of its tree representation and dag representation \n(in number of nodes). The ratio of these two shows the amount of savings of the dag representations. \nWe then use the range of this ratio (2 to 2 + -1, where i = 0 ,. . . ,4) to classify all the types; for \neach category, we list the number of its members, the average size of the tree representations, and the \npercentage of its total size over that of all categories. Figure 1: A profile of compile-time type information. \ntypes are ubiquitous in real-world ML applications. For ex-ample, a 200-line ML program cm. sml in the \nSML/NJ com-pilation manager (CM) [4] contains more than 36 functor applications and more than 80 structure \nreferences; each of these modules may contain a dag of sub-structures or func-tors. Figure 1 gives a \nprofile of types built while compiling two large ML applications in our type-preserving compiler (see \nlater sections for details about the compiler). Here, CM is the compilation manager [4] and eXene is \nan ML-based X window system tool-kit [31]. If we use tree representations, a single type can contain \nmore than 45,518 nodes for CM and 379,315 nodes for eXene. These large types can be dra- matically reduced \nin size if we use dag representations. For example, under CM, 32.03% of the space occupied by types can \nbe improved by a factor of at least 64 when we use dag representations. For eXene, the savings are even \nmore dra-matic. Of course, no real compiler will use the dumb tree representations all the time, but \nthe profile does show that any loss of sharing in type representations could potentially incur huge costs \nto the compilation time and space usage. This paper describes our experience with implementing the FLINT \ntyped intermediate language [36] in the SML/NJ production compiler [39, 31. FLINT is based on a pred-icative \nvariant of the polymorphic X-calculus F, [12, 32, 151, extended with a rich set of primitive types and \nfunc-tions. FLINT supports both polymorphic types and higher- order type constructors, so the type language \nitself is a full- scale X-calculus. To support various type-directed optimiza-tions [14, 341, we perform \na large number of type-related op-erations during compilation. The main challenge is to repre- sent complex \nFLINT types (which can be arbitrary lambda terms) as compact dags so that common type-related oper-ations \n(e.g., lambda reductions, equality) can always work efficiently and yet still preserve sharing. More \ngenerally, we believe that a type-preserving com-piler will not scale to handle large types unless all \nof its type-preserving stages can preserve the asymptotic time and space usage in representing and manipulating \ntypes. To achieve this property, we present a novel and efficient rep-resentation scheme for the FLINT \ntype calculus. Our main idea is to combine hash-consing, memoization, and advanced lambda encoding [24, \n1, 9] to ensure that (1) types are al-ways represented as dags; (2) type reductions are done on a by-need \nbasis; and (3) the cost of handling types is propor- tional to the size of the dag representations. In \na companion paper [33], we have presented a new optimal type-lifting al- gorithm that lifts all run-time \ntype constructions to the top level; in fact, we can guarantee that the number of types built at run \ntime is a compile-time constant; furthermore, all of them are represented as efficiently as their compile- \ntime counterparts. The main contributions of this paper are: l As far as we know, our work is the first \ncomprehen-sive study on how to build scalable implementations of type-preserving production compilers. \nSeveral existing compilers [41, 27, 421 have also used typed intermedi-ate languages, but none of them \nhave attempted to scale their implementations to handle large types; in fact, all these compilers have \nreported extremely slow compilation times as a result of keeping types during compilation.2 l We combine \nhash-consing, memoization, and advanced lambda encoding [24,1,9] to support efficient type rep-resentation \nand manipulation. Although each of these techniques has been researched and implemented be-fore, nobody \nhas ever tried to combine them to repre- sent compiler type information. Combining these tech- niques \nis crucial yet non-trivial, as we will demonstrate in Section 5 and Section 7. l We describe several \ndifferent ways of representing type variables bound in the term languages and then com-pare their performance. \nRepresenting type variables as de Bruijn indices is faster but it also makes type ma-nipulation harder. \nWe show that using explicit names to represent type variables might be a more desirable alternative. \nl All techniques discussed in this paper have been imple- mented and incorporated into the SML/NJ production \ncompiler since version 109.24 (January 1997). The re-sulting compiler has been used and tested world-wide \non a large number of ML applications for more than 14 months. We have not received any complaints about \nthe compilation time after we switched to the type-preserving implementation. We are not aware of any \n Although GHC makes little ueo of its type information in the back end, it still rune out of memory when \ncompiling the toy benchmark. other type-preserving ML compilers that can handle large applications such \nas CM and eXene. l To verify the effectiveness of these techniques, we have measured and compared several \nversions of the SML/NJ compiler on a variety of benchmark pro-grams. The combination of these techniques \ncan re-duce the total compilation time by up to 72% on large applications (a reduction of 93% in the \ntype-preserving phases). l We also present a detailed comparison between our scheme and the lettype scheme \nused in the TIL/ML compiler (also informally described in Tarditi s the-sis [40]). 2 Related Work Typed \nintermediate languages have received much attention lately, especially in the HOT (higher-order and typed) \nlan-guage community. However, recent work [14, 22, 36, 30, 8, 29, 23) has mostly focused on the theoretical \nfoundations or other language design issues. This paper complements pre-vious work by showing that typed \nintermediate languages can indeed have pmctical and scalable implementations, but only if extreme care \nis taken. In fact, most of the tech-niques described in this paper have been incorporated into the SML/NJ \nproduction compiler since version 109.24 (Jan- uary 1997). Many results reported here are inspired by \nfeed- back from the SML/NJ user community. Several existing compilers such as TIL [41], GHC [27], and \nML-Kit [42] have also used an F,-like calculus as their typed intermediate languages. However, none of \nthem has seriously addressed the problem of how to handle large types, nor do they support efficient \nrun-time type passing. The suspension-based lambda encoding used in our im-plementation is directly borrowed \nfrom Nadathur s recent work on efficient lambda representations [25, 241. In addi-tion to doing an in-depth \ntheoretical study of the underlying encoding calculus, Nadathur [24] has also used his encoding to implement \nthe X-Prolog system. The main contribution of our work is to combine Nadathur s encoding with hash-consing \nand memoization, and then apply it to the context of typed intermediate languages. Combining these techniques \nis non-trivial because of the presence of higher-order types and the need to memoize intermediate reduction \nresults. Explicit substitutions [9, l] is another related lambda encoding scheme. Cardelli s Quest compiler \n[l] contains an implementation of this encoding; however, he did not com-bine it with other techniques \nwe used. Nor was he working in the context of type-preserving compilers. Shao and Appel [39] used hash-consing \nto enforce dag representations for types; however, their intermediate lan-guage is only monomorphically \ntyped, so it is much easier to support than FLINT-like languages. Tarditi [40] used the lettype constructs \n(in both the constructor calculus and the term language) to A-normalize [lo] all types in order to express \nsharing explicitly. But he relies on a separate com-mon sub-expression elimination phase to identify \nthe sharing information. This amounts to hash-consing with the disad-vantage that it comes too late (huge \nredundant types have already been built) and it does not guarantee that further redundancies will not \nbe introduced later in the compila-tion process. So it is not clear that the lettype scheme will (kind) \nK ::= n ( IE1 -+ /c2 (tycon) p ::= t I Int ) p1 --+p2 I At::&#38;+ I p1[p2] (type) u ::= T(p) ( u1 -+ \nCT2 1 vt :: Ic.cr (term) e ::= i 1 22 1 Xz :u.e 1 Qzlz2 I At::~.e]c[~]]letz=eiinez Figure 2: Syntax of \nthe Core-FLINT calculus. address issues such as compilation time in the face of real- world applications. \nFinally, lettype has poorly understood theoretical foundations; we do not know of any notion of nor- \nmal form 3 in the context of lettype, for example. A more detailed comparison between our scheme and \nthe lettype scheme is given in Section 8. 3 An Overview of FLINT The core language of FLINT is based \non a predicative vari-ant of the Girard-Reynolds polymorphic X-calculus F, [12, 321, with the term language \nwritten in A-normal form [lo]. It contains the following four syntactic classes: kinds (K), type constructors \n(CL), types (a), terms (e), as shown in Figure 2. Here, kinds classify type constructors, and types classify \nterms. Constructors of kind s2 name monotypes. The monotypes are generated from variables, from Int, \nand through the --t constructor. As in Fy, the application and abstraction constructors (i.e., pi[pz] \nand At :: )E.P) corre- spond to the function kind /cl + ICY. Types in Core-FLINT include the monotypes, \nand are closed under function spaces and polymorphic quantification. We use T(p) to denote the type corresponding \nto the constructor p when p is of kind R. As in F,, the term language is an explicitly typed polymor-phic \nX-calculus (but written in A-normal form); both type abstraction and type application are explicit. The \nactual FLINT language contains other familiar con-structs such as record, recursive datatype, and a rich \nset of primitive types and operators. Large types mainly come from ML-style modules (which are represented \nas FLINT records [37]) and recursive datatypes, but the challenge of implementing FLINT still lies on \nhow we handle three forms of type abstractions, i.e., constructor function (At :: )E.P), polymorphic \ntype (Vt :: K..(T), and polymorphic function (At :: /c.e). We present our solutions in Sections 5 and \n6. The structure of our type-preserving compiler is very similar to that of conventional untyped compilers. \nPrograms written in the source languages (e.g., ML) are first fed into a language-specific front end \nwhich does parsing, elaboration, type-checking, and pattern-match compilation; the source program is \nthen translated into the FLINT typed intermedi-ate format. The middle end does conventional dataflow \nop-timizations, type specializations, and X-calculus-based con-tractions and reductions, producing an \noptimized version of the FLINT code. The back end compiles FLINT into ma-chine code through the usual \nphases such as representation analysis [34], safe-for-space closure conversion [38], register allocation, \ninstruction scheduling, and machine-code gener-ation [ll]. 30f course, we could always expand out the \nlottype definitions to get to a normal form, but this eliminates the benefit of using lettype and is \nequivalent to using tree representations. edly: type-checking intermediate phases and certifying ob- \nsignature LTYEXTERN = sig (* abstract types *) type tkind (* n *I type tyc (* jL *I type 1tY (* Q *I \n(* constructors *) val tee-int : tyc (* Int *) val tee-var : tvar -> tyc (* t *I val tee-arrow : tyc \n* tyc -> tyc (* p --f p *I val tee-in : tkind l tyc -> tyc (* An+ *) val tee-app : tyc (* selectors *) \nval ted-var : tyc val ted-arrow : tyc val ted-in : tyc val ted-app : tyc (* predicates *) val tcp-int \n: tyc val tcp-var : tyc val tcp-arrow : tyc val tcp-fn : tyc val tcp-app : tyc (* utility functions *) \nval tc,eqv : tyc val tc,print : tyc . . . end (* LTYEXTERN *) + tyc -> tyc (* &#38;I *) -> tvar -> tyc \n* tyc -> tkind * tyc -> tyc l tyc -> boo1 -> boo1 -> boo1 -> boo1 -> boo1 * tyc -> boo1 -> string Figure \n3: Interface to the FLINT constructor language (p). Some constructor forms are omitted. Similar interfaces \nexist for FLINT types (u) and kinds (a). Implementation Criteria In this section, we list the goals \nthat guided the implemen-tation of the FLINT type language, and we describe its interface. We present \nthe implementation details in Sec-tion 5. The following criteria are, in our experience, impor-tant for \nan efficient implementation of a typed intermediate language. Compact space usage. As demonstrated in \nSection 1, large types are ubiquitous in real-world ML applications. For this reason, it is imperative \nthat we represent these types efficiently. Fortunately, large types are also highly redun-dant, so a \nwell-constructed dag representation can be quite compact. The representation should, however, come with \neither a guarantee that all such redundancy is exploited, or empirical evidence showing that, in practice, \ntypes remain compact even as they are manipulated and transformed. Linear-time traversal of types. Compact \nrepresentations are not enough to ensure efficiency in a type-preserving com-piler. Many operations on \ntypes (e.g., substitution and re-duction) require traversing the graph. If we are not careful, these \noperations might traverse isomorphic subgraphs mul-tiple times, even though they share the same representation. \nIn order to maintain reasonable compilation time, such op- erations must traverse the representation \nlinearly. Fast equality. Checking the equivalence of two types can be non-trivial, because there are \nmany ways to represent the same type. Equality checking is often used by type-directed optimizations. \nFor example, representation analy-sis [34] uses equality to determine where wrapping is neces- sary. \nMoreover, two compelling operations made possible by type-preserving compilation perform equality tests \nrepeat-ject code [26, 231. Thus, the implementation should support efficient equality tests. Simple Interface. \nThe software engineering benefits of hiding implementation details from clients are widely recog-nized. \nBesides concealing the tricks used to meet the other criteria, we would like clients to treat each type \nas its inten- sion; different representations of a single type should all look the same. There are two \nways to achieve such an interface. Either all types passed across the interface are in normal form (corresponding \nto eager reduction), or the top node of a type is the same as for the normalized version (ureak head \nnormal form), and the rest is normalized on demand. Our implementation meets all of these goals. We guaran- \ntee sharing by hash-consing and storing each type in a global table. Isomorphic types always share their \nrepresentation, regardless where they appear or how they were constructed. All reductions and substitutions \nare guaranteed to traverse the representation linearly (because these important oper-ations are specifically \nsupported by the implementation). For clients implementing other transformations, we provide a memoizing \nfold function that is guaranteed to traverse the representation linearly. For equality testing, thanks \nto our guarantee that iso-morphic types share the same representation, types in nor-mal form can be compared \nvery quickly using pointer equal-ity. If the types are not in normal form and pointer equality fails, \nthen we reduce the types to weak head normal form, check if the heads have the same shape, and continue \nrecur-sively on the sub-terms. In practice, this leads to very cheap equality tests. Complete type-checking \nof the intermediate code after every phase does not incur noticeable overhead. Figure 3 gives part of \nthe FLINT type language interface. All operations on FLINT types (/.J) are done through a set of basic \nprimitives: constructor functions create types from their components; predicates test for particular \nconstructs; selectors project the components, assuming an appropriate construct is given. Additionally, \nthe interface contains func-tions for equivalence testing, pretty-printing, etc. The interface behaves \nas if all types are kept in normal (fully reduced) form, even though the underlying implemen-tation uses \nlazy reduction. For example, suppose we create a type t by applying the identity function to Int. Then, \ntcp-app(t) will return false, whereas tcp-int(t) will re-turn true. 5 Representing Types Now we turn \nto a detailed explanation of our implementa-tion techniques. We show how to represent complex FLINT types \nas compact dags and make the costs of all type-related operations (e.g., substitution, equality) proportional \nto the dag size. We will focus our discussion on the FLINT con-structors (cl) only, though these techniques \napply to the FLINT types (u) and kinds (a) as well. In fact, the issues in- volved in implementing polymorphic \ntypes (Vt :: K.(T) are pre-cisely same as those for higher-order constructors (At ::K+). 5.1 Suspension-based \nlambda encoding The first challenge in representing FLINT constructors is to choose an appropriate encoding \nfor efficient manipula- 6 (t-1) (X~.pl)[pz] a Env(p1, (l,O, (p2,O) ::nil)) (r2) Envb, (O,O, nil)) =s \nP (r3) Env(p, (i,j, env)) a ~1 if p is closed, i.e., it has no free type variables) (~4) Env(#n,(i,j,env)) \n=+ #(n-i -tj) if n > i 1 n < 2 and the n-th element of enu is j (f-5) Env(#n, (i,j,env)) a #(j -j ) \nf _ ( 6) Env(#n, (i,j, env)) =+ Env(p, (0,j -j , nil)) if _ element n < i and the n-th of en2r is (j \n,~) [ri Env(Int,p) a Int Envbl + ~2, P) * Env(w, P) -+ Env(w, P) (h Env(~lb21,p) * (Env(cLl,p))[Env(~2,P)l \n(r10) Env(X/c.p,(i,j,env)) a Xn.Env(p,(i + 1,j + 1,j::env)) (rll) Env(Env(p,(i,j,env)),(O,j ,nil)) a \nEnv(p,(i,j +j ,env)) Figure 4: Type reductions under suspension-based lambda encoding. tion. Under the \nsyntax in Figure 2, testing the equality of a-convertible constructors such as ~11 = At1:: $21 -+ tl \nand ~2 = At2:: R.t2+ t2 is non-trivial. We use de Bruijn indices [7] to represent type variables, so \nthat a-equivalent constructors always have the same representation. For ex-ample, both ~1 and ~2 are \nrepresented as M.(#l + #l). The X no longer binds any named type variables (though the kind is still \nretained). Instead, we use a positive integer #n to denote the variable bound by the nth surrounding \nX-binder. Another important requirement is that type reduction should be done lazily. To achieve this, \nwe enrich the con-Figure 5: Illustration of a suspension type structor calculus to support a new suspension \nterm [25, 241 of the form Env(p,p). Intuitively, a suspension represents an unevaluated type p(p) ; it \ncorresponds to the interme- type should have zero effect. Because we memoize the set of diate result \nof some unevaluated type applications. The free variables in our type representations (see Section 5.2), \nsubstitutions involved (p) are also known as explicit sub-it is very easy to check whether a type is \nclosed. Rules (r4) stitutions [l, 91: to (r6) show how we adjust and substitute each de Bruijn index-based \ntype variable: for variables that are bound out-(constructor) p ::= #n 1 Int ( ~1 -+ Liz side the current \nbinding level (i), the new de Bruijn index I %.J 1PI[WI 1 Envh p) would be (n -i) + j; for variables \nbound in the current envi- (substitution) p ::= (i, j, env) ronment, we find out its corresponding mapping \nand adjust (environment) env ::= nil 1 j::env 1 (j,p)::env the result from its definitional level j \nto the new embedding level j. Rules (r7) to (r10) push the substitution recursivelyFollowing Nadathur \n[24, 251, we represent each such substi- into the subterm of each type; for type functions (X6+), we \ntution as a triple (i, j, env) where the first index i indicates need to add a new entry into the current \nenvironment (seethe current embedding level of bound type variables, the Rule (r10)). Rule (rll) is a \nsimple optimization to mergesecond index j indicates its new embedding level, and the two nested substitutions. \nNotice that because all intermedi- environment env contains the actual bindings of all i bound ate results \nare expressible in our calculus, the reduction rules variables. Each entry in the environment is represented \nas do not involve any external substitution machinery. More a pair (j ,$) or as an integer j (which has \nsame meaning details about the suspension-based calculus can be found in as (j , #O)); in either case, \nj denotes the definitional depth Nadathur s excellent paper [24]. of type CL . Figure 5 shows the relationship \namong these components for a type Env(p, (i, j, enw)), assuming all envi- ronment entries of form j are \nrepresented as (j ,#O), and 5.2 Hash-consing and memoization the environment enu is equal to (ji,t(i)::...::(ji,pi) \n::nil. After we choose the appropriate encoding scheme, we hash-For example, the standard ,&#38;contraction \n(XK./.Q)[~~] re-cons all FLINT kinds, type constructors (including substi-sults in a constructor of the \nform Env(pi,pe) where po = tutions), and types into three separate hash tables. Under(1, 0, (0, ~2) :: \nnil). This represents the following fact: the hash-consing, all FLINT types built during the compilation \nconstructor ~1, which was originally in the scope of 1 ab- are guaranteed to use the most compact dag \nrepresentation. straction, is now to be thought of as being in the scope of Because we are using de Bruijn \nnotation, type variables arenone; ~2, originally in the scope of 0 abstractions, is to be represented \nas integers and all o-convertible types have iden- substituted for the first free variable in ~1. tical \nrepresentations, which allow them to be collapsed via Figure 4 gives the set of type reductions used \nin our X en- hash-consing.coding. Here, Rule (rl) turns a type application into the sus- For each hash \nentry, we use weak pointers so that if anpension form. Rules (r2) and (r3) are two straightforward element \nin the hash table is no longer used anywhere else,optimizations, capturing the fact that applying an \nempty it will be garbage collected. Internally, each constructorsubstitution to a type or applying a \nsubstitution to a closed p is now accessed indirectly via an updateable hash ce11,4 and once it is done, \nthe result will be memoized for future denoted as $: use. Our measurements have shown that these techniques \nreduce the compile time of large applications by an average (hash-cell) ::= Ref (hashcode, p, auzinfo) \nof 45% (see Section 7.2). (constructor) ; ::I fy lI~t~*lL&#38; l-4 1 E%$,P~! 1 &#38;Cci,b) 6 Manipulating \nTypes Here, a hash cell is a mutable record containing the follow-ing three fields: an integer hash \ncode (hashcode), a term (cl), and a set of auxiliary information (auzinfo). The auz-info maintains two \nattributes: a flag that shows whether ~1 is already in normal form5 and if so, the set of free type variables \nin p (in de Bruijn indices, of course). Building a new constructor under this representation takes two \nsteps: (1) calculate the hash code, and (2) if the constructor is not already in the hash table, calculate \nthe avzinfo and insert the new cell. The most interesting aspect of our representation scheme is that \nwe can also memoize the result of every sequence of type reductions (e.g., those in Figure 4). Given \na constructor pS = Ref (hashcode, ~1, auzinfo), suppose p can be reduced to ~1; then, we can do an in-place \nupdate, changing the second field of ~1 to a memoization node Ind(p, &#38;): We keep the original /.J \nin the new memoization node so that all future creations of /J (which will always have the same hash \ncode) will be directly hash-consed to this new memoization node. The hashing procedure might require \nchecking syntactic equality against ~1 because of potential hashing conflicts. Note that the update is \nalways safe, because it is only done to constructors that are not in normal form, so we do not have to \nrecalculate the free variables, etc. Memoization of reduction results has very interesting consequences: \nif we do not garbage-collect any of these mem- oization nodes (we may since they are weak pointers), \nthen any redex of form ~1 can reuse the memoized result, &#38; This leads to a very practical implementation \nthat approximates optimal lambda reductions [18], with the caveat of using hash-consing, of course. The \ncombination of these techniques has proven to be very effective. With hash-consing and memoization, com-mon \noperations such as equality tests, testing if a type is in normal form, and finding out the set of free \nvariables, can all be done in constant time. With the use of suspension terms, type application is always \ndone on a by-need basis, 4Hash-consed substitutions (p ) and kinds (K ) are represented in the same way. \nActually, because substitutions are simply finite mapping from de Bruijn indices to constructors, they \nshare the same hash table with type constructors (we could simply encode them as a record constructor). \nBy normal form, we mean those constructors that do not con-tain any redexes, i.e., no sub-term matches \nthe left-hand side of the reduction rules in Figure 4. If we use named variables to represent the type \nabstraction in the term language (see Section 6), we would need to maintain two separate lists of free \nvariables, one using de-Bruijn indices, another using named variables. Although we use hash-consing, \nmemoization, and the sus-pension-based lambda encoding to support efficient type handling, none of these \nimplementation details are exposed to the clients of the type interface (see Figure 3). In fact, ma-nipulating \ntypes under our type interface is still much like manipulating simple datatype-based representations. \nThe only thing we have lost is the pattern matching capability. Our interface also treats each type as \nits intension, that is, clients never need to think whether or not a type should be represented in normal \nform (or weak-head normal form). All operations in the type interface can apply to types of any form. \nType reductions are completely hidden inside the underlying implementation and they are always done lazily. \nBecause of the various memoizations we do, our type in-terface also provides unusually fast implementations \nof sev- eral common operations. For example, we can check if a type is in normal form in constant time; \nwe can also find the set of free variables in a type in constant time as well. The only remaining issue \nis on how to represent type variables bound by polymorphic functions in the term lan-guage (i.e., At \n:: K.e). For a long time (including our most recent release), we have used the same de Bruijn indices \nto represent these type variables. This strategy requires no changes to the existing interface, but it \nhas the unfortunate effect that the representation of a type annotation with free variables is now dependent \non its lexical depth (the number of type abstractions under which it appears). The implica-tion is that \nthe client must adjust the representation when moving types from one depth to another. Although we provide \nseveral utility functions to support this operation, having de Bruijn indices exposed does com- plicate \ncertain optimization phases. Inlining, for example, requires adjusting types if the definition and call \nsite are at different lexical depths. Specialization requires particularly drastic (yet subtle) adjustments \nto the types, since type ab-stractions themselves are being inlined and even eliminated. We have experimented \nwith an alternate design which hides the de Bruijn indices by supporting two different rep-resentations. \nInside the type language, the type function (X) and the polymorphic quantifier (V) still bind de Bruijn- \nindexed type variables. In the term language, however, type abstraction (A) binds named variables. This \nway, type an-notations can be moved freely across depths because all free type variables are guaranteed \nto be named. Naturally, this simplicity has a price. First, in order to reconstruct the type of a A term, \nwe must traverse the types, converting the named variables into de Bruijn indices before placing the \nquantifier in front. Second, to memoize the set of free variables in each type representation, we now \nneed to maintain two separate lists of type variables, one using de-Bruijn indices, another using named \nvariables. Third, a-equivalent named types will not share the same represen-tation. Our intuition, however, \nis that the additional cost for A-bound type variables will be acceptable, because these represent a \nvery small portion of the total type size. In Sec- Benchmark Source Lines simple 918 vliw 3,682 sml-nj \n89,432 CM 7,703 cm1 5,966 eXene 35,662 ml-lex 1,232 toy 7 8toYP Program DescAption A spherical fluid-dynamics \nprogram A VLIW instruction scheduler SML/NJ compiler v109.32 SML/NJ Compilation Manager by Blume Concurrent \nML by Reppy An X-window system by Reppy &#38; Gansner A lexical-analyzer generator Identity function \napplied 18 times Similar, with curried application 11 Code Size 1 Tree Size I Dag Size (bytes) (nodes) \n(nodes) 114,944 34,118 1,9;3 273,836 646,215 5,682 6,779,308 20,749,395 125,044 487,048 3,186,279 27,968 \n366,684 1,203,391 17,106 2,291,628 99,567,031 78,671 103,604 112,091 3,122 22,148 30,409,149 14 30,016 \n183,463,919 743 t- Figure 6: Description of benchmarks used. The tree size expresses the number of nodes \nin the type forest, if types were represented as trees (with no sharing of any kind). The dag size is \nthe number of nodes actually created to represent the types in the compiler. The comparison between tree \nsize and dag size is only intended to demonstrate the amount of redundancy in the types. tion 7.3, we \ngive preliminary measurements indicating that the additional costs are indeed acceptable. We conclude \nthat this simpler design (using named variables) is quite feasible and will most likely be used in future \nversions of the com-piler 7 Experimental Results This section gives empirical evidence demonstrating \nthe ef-fectiveness of the techniques presented in Sections 5 and 6. All techniques have been implemented \nin the FLINT/ML compiler [35] and in the SML/NJ production compiler since version 109.24 (January 9, \n1997). All tests were performed on a Pentium Pro 200 Linux workstation with 64M physical RAM. Figure \n6 shows the set of benchmarks we used along with a summary of their salient features, including the size \nof the types. The dag size is the number of nodes when maximal sharing is realized, meaning that even \na-equivalent types share the same representation. The ratio of tree size to dag size is intended to demonstrate \nthe amount of redundancy in the types; it is not meant as a comparison of our repre-sentation to the \ncompletely naive one. 7.1 Hash-consing results This redundancy is examined for several benchmarks in \nFig- ure 7. Here, the y-axis represents some proportion of the type forest, while the x-axis shows the \nminimum reduction factor realized on that proportion of the forest thanks to hash-consing with de Bruijn \nindices. The results for VLIW are particularly interesting. VLIW is written in an algorithmic style, \nmaking little use of higher- order functions, functors, or polymorphism. Nevertheless, we still get considerable \nreduction of the types used in the in- termediate representation. This shows that hash-consing is not \nonly beneficial for heavily functorized applications such as eXene and CM. In order to get an idea of \nthe cost of hash-consing, we measured the performance of our hash table. The table is an array containing \n2,048 lists; collisions are handled by prepending new entries onto the list. Subscripting the array -a-CM \n-m- sml-nj --c exene -ml-h  + vliw 0 1 2 4 8 16 32 64 128 256 512 Ratio of tree size to deg size Figure \n7: Amount of redundancy in types. The x-axis repre-sents the minimum reduction factor realizable on some \npro- portion of the type forest. For instance, with ml-lex, 80% of the type forest can be cut at least \nin half, and 25% can be reduced by a factor of 8. is very fast, so we need only be concerned with the \ncost of traversing the lists. Figure 8 shows the dynamic distribution of the lengths of list traversals. \nMost queries are satisfied after looking at only one or two list entries. One of the reasons is locality; \nwe place new entries at the head of the list, so subsequent accesses are immediate. Furthermore, the \ntable never gets very big; the maximum length of a list is 12. It seems clear that we should not be concerned \nabout the performance of the hash table. 7.2 Memoization results Figure 9 summarizes the results of doing \nvarious combina-tions of memoizations. The y-axis represents the compila-tion time of each benchmark, \nrelative to the CPU time with-out any memoizations (the absolute time is printed above each set of bars). \nThe memoizations performed are a normal- form indicator (NF), the set of free variables (FV), interme-diate \nreduction results (RD), and combinations of these. 9.35 34.88 924.44 58.96 52.36 723.43 80.65 174.5 NF \n 0.6 N RD NF+FV NF+RD N+RD All simple vliw sml-nj CM cm1 eXene toy toYP Figure 9: Memoization results. \nThis shows the compilation times for each benchmark using various combinations of memo- izations, relative \nto the time without memoizations. The striped part of each bar represents the type-preserving phases \nof the compilation. The results for ml-lex are very similar to those 16: Length of list traversal Figure \n8: Hash table performance. This shows the dynamic number of hash table lookups (y-axis) that must search \na bucket of particular length (z-axis). Most queries are satis- fied after looking at only one or two \nlist entries. The striped part of each bar represents the type-pre-serving phases of the compilation, \nwhere our memoizations should have the most effect. Variation in the rest of the compilation time (represented \nby the solid bars) can be at- tributed to measurement error and secondary effects. Notice that, without \nmemoizations, the type-preserving phases rep- resent a significant portion of the compilation time, even \non Simple and VLIW (18 and 27%, respectively). The eXene benchmark is a large, heavily-functorized ap-for \nvliw; they were omitted due to space constraints. plication on which our techniques are particularly \neffective. They reduce the total compilation time of eXene by 72% (a reduction of 93% in the type-preserving \nphases). Without memoizations, the type-preserving phases represent a dom- inant 78% of the compilation \ntime; with them, these phases are a manageable 25%. Taking the average over the large benchmarks (sml-nj, \nCM, cml, and eXene), our techniques reduce the total compilation time by 45%. In most benchmarks, memoizing \nNF+FV+RD does not seem to win much over just NF+FV. Also, in most cases, FV+RD achieves results somewhat \nsimilar to just FV. One might be tempted to assume that the other memoizations effectively subsume memoizing \nreduction results. However, there are extreme cases (toy and toyp, for example) where RD does improve \ncompilation time when combined with other memoizations. These programs contain huge poly- morphic types \nthat are later specialized because they are only applied to integers. Without memoization of reduction \nresults, specialization blows up. 7.3 Named variable results Finally, we give preliminary measurements \nof the cost of using named type variables in the term language. As dis-cussed in Section 6, the phases \nof the current compiler that are most inconvenienced by de Bruijn indices are inlining and specialization. \nWe added support in our type interface for named vari-ables, and changed the FLINT representation to \nuse them behind type abstractions. Next, we modified all compiler phases through specialization to use \nthe named variables. The modifications were fairly painless; deleting the most variables and de Bruijn \nindices, we are able to simplify Compilation Time such manipulations outside the core modules and still \nBenchmark (seconds)\\ Ratio achieve acceptable performance. -lr deBruijn namedvar simple 8.53 8.44 0.99 \nvliw 28.78 28.37 0.99 sml-nj 566.96 565.91 1.00 CM 57.83 63.63 1.10 cm1 105.17 108.54 1.03 eXene 188.46 \n188.55 1.00 ml-lex 8.19 8.33 1.02 toy toYP 0.10 0.21 0.11 0.21 1.10 1.00 Figure 10: Named variable results. \nThis shows compilation times for each benchmark using de Bruijn indices through-out, and then using named \nvariables in the term language (and converting to de Bruijn indices after specialization). The last column \ngives the ratio of the second version over the first. subtle parts of the specialization code was downright \nenjoy-able. We have not yet modified the later phases. Instead, we temporarily inserted a phase after \nspecialization to convert all remaining named type variables into de Bruijn indices. The cost incurred \nby this extra phase is included in the mea-surements given in Figure 10. The compilation times of most \nbenchmarks are not no-ticeably affected by the change. CM, the primary excep-tion, suffered a 10% increase \nin compilation time due to the use of named variables. These results are preliminary be-cause we have \nyet to modify the later phases of the compiler to use the new mixed representation (which would obviate \nthe need for the extra conversion phase). We suspect that the remaining modifications will have no serious \nimpact on performance, and with additional profiling and tuning, we may even be able to reduce the current \noverhead. We con- clude that the simplified interface (made possible by using de Bruijn-indexed type \nvariables internally and named vari-ables externally) is quite feasible and will most likely be used \nin future versions of the compiler. 8 Comparison Returning to our implementation criteria, we can certainly \nsay that our scheme is very effective at representing types in a concise manner and provides us with \na fast type equality test. Type manipulations are also made efficient by system- atic use of memoization. \nOur experience with the interface is very positive since all the machinery is well hidden within a few \ncore modules which export simple and intuitive type operations. There are nonetheless a few weaknesses: \nl The interface hides the actual implementation behind functions which prevent the use of the pattern \nmatch-ing facilities of ML. This could be circumvented if re-ally necessary, but it turned out to be \na non-issue. Furthermore, the functional interface gives us a lot of flexibility. l The de Bruijn indices \nmake some manipulations more subtle than we would like. By using a mix of named l In order to make sure \ntype traversals are efficient, we have to use a fold function on types which encapsulates the memoization. \nHere also, our experience has shown that it is not a serious issue. Our choice of techniques to provide \nefficient type ma-nipulation should be contrasted with the lettype scheme used in the TIL compiler 1401. \nIt should be noted here that very little has been published about the lettype scheme, so this comparison \nis based on our own understanding of what lettype could look like under the ideal scenario rather than \nany existing implementation such as the one in the TIL com-piler. The basic approach is to extend the \nnotion of A-normal form to types by providing lettype (in both the term and the type languages). For \nexample, the identity function on integer pairs would look like: lettype ti = Int * Int in lettype t2 \n= tl + tl in (Xz : t1.z) : t2 This has the advantage of making the sharing explicit since all types \nare referenced through names bound in the type environment. The explicit sharing basically eliminates \nthe risk of accidentally traversing the type tree in an inefficient way. In other words, type traversals \nget memoized for free. But lettype suffers from many problems: l There is no known way to define a compact \nnormal form for such type representations. This implies that type equality tests become much more expensive. \nAll existing theoretical framework treats lettype t = ~1 in /.~e as if it is a P-reduction of form (At \n:: ~+z)bi]. This would clearly expand into a normal form, but on the other hand, this reduction is precisely \none that is banned by the lettype scheme, as otherwise, type expressions would degenerate into inefficient \ntree rep-resentations. l Similarly it is unclear how one could provide a clean interface that allows \nits clients to be oblivious to nor-malization issues while still ensuring efficient execu-tion, since \nmemoizing the normalization steps would require adding types to the environment which in turn would force \nthe rewrite of the whole term. l Expressing sharing is not enough: we still first need to find that sharing. \nWe might be able to get some sharing information straight from the type-inference phase, but this will \nrequire careful coding. Also we might not get as much sharing as we would want. TIL s solution is to \ngo through a common sub-expres-sion elimination phase. This would indeed allow us to merge all the common \ntypes, but requires precisely the same machinery as hash-consing and is done after the fact, whereas \nwe are careful to eliminate common sub-expressions as soon as they appear. Furthermore, many more common \nsub-expressions will appear dur-ing the compilation process which will require addi-tional passes through \nthe CSE phase while our scheme takes advantage of the hash-consing all along the com-pilation process \nto guarantee that sharing is constantly maintained. Another subtle difference is that lettype traverses \nits types most naturally in a bottom-up fashion which precludes (or rather reduces the effectiveness \nof) op-timizations that cut-off the traversal of types. More specifically, lettype would not let us make \nas good a use of information such as free-variables or a normal- form bit. To summarize, lettype seems \nto provide a clean way to represent types efficiently, but it ends up having to pay the cost of hash-consing \nanyway without reaping all the benefits of our more straightforward how lettype scales to real which \nour scheme handles Our approach manages plexity of type manipulation, scheme. Also it is yet to be seen \nworld situations such as eXene, easily. to hide most if not all the com-providing programmers with a \nsimple and intuitive interface. It ensures that maintenance of type information is non-intrusive, which \nis greatly ap-preciated for optimization phases that do not rely on type information. lettype on the \nother hand would most likely force every phase to maintain at the very least a type envi-ronment . Finally, \nour implementation is straightforward since it relies on well understood techniques and it does not suffer \nfrom hidden costs since all the hash-consing and memoizing is done once and for all. Conclusions Implementing \ntyped intermediate languages is not a trivial task. In this paper, we have presented a series of novel \ntechniques that make type-preserving compilers practical and scalable. We argue that a type-preserving \ncompiler will not scale to handle large types unless all of its type-preserving stages preserve the asymptotic \ntime and space usage in representing and manipulating types. We believe what we learned from our implementation \nwill be valuable to future implementations of other emerging typed interme-diate languages. Availability \nThe implementation discussed in this paper is now released with the Standard ML of New Jersey @ML/NJ) \ncompiler and the FLINT/ML compiler [35]. SML/NJ is a joint work by Lucent, Princeton, Yale and AT&#38;T. \nFLINT is a modern compiler infrastructure developed at Yale University. Both FLINT and SML/NJ are available \nfrom the following web site: http://flint.cs.yale.edu Acknowledgement We would like to thank Valery Trifonov, \nBratin Saha, and the anonymous referees for their comments and suggestions on an early version of this \npaper. Valery Trifonov helped implement the FLINT type-checker and also participated in numerous discussions \non topics covered by this paper. PI M. Abadi, L. Cardelli, P. Curien, and J. Levy. Explicit sub-stitutions. \nIn Seventeenth Annual ACM Symp. on Principles of Prog. Languages, pages 31-46, New York, Jan 1990. ACM \nPress. PI A. W. Appel. Compiling with Continuations. Cambridge University Press, 1992. [31 A. W. Appel \nand D. B. MacQueen. Standard ML of New Jersey. In M. Wirsing, editor, Third Intl Symp. on Prog. Lang. \nImplementation and Logic Programming, pages l-13, New York, August 1991. Springer-Verlag. M. Blume. A \ncompilation manager for SML/NJ. as part of SML/NJ User s Guide, 1995. [41 PI L. Damas and It. Milner. \nPrincipal type-schemes for func-tional programs. In Ninth Annual ACM Symp. on Principles of Prog. Languages, \npages 207-212, New York, Jan 1982. ACM Press. 161 0. Danvy. Type-directed partial evaluation. In Proc. \n23rd Annual ACM SIGPLAN-SIGACT Symp. on Principles of Progmmming Languages, pages 242-257. ACM Press, \n1996. N. de Bruijn. A survey of the project AUTOMATH. In To H. [71 B. Curry: Essays on Combinatory Logic, \nLambda Calculus and Formalism, pages 579-606. Edited by J. P. Seldin and J. R. Hinclley, Academic Press, \n1980. A. Dimosk, R. Muller, F. Turbak, and J. B. Wells. Strongly typed flow-directed representation transformations. \nIn Proc. 1997 ACM SIGPLAN International Conference on fine-tional Progmmming (ICFP 97), pages 11-24. \nACM Press, June 1997. 191J. Field. On laziness and optimality in lambda interpreters: Tools for specification \nand analysis. In Seventeenth Annual ACM Symp. on Principles of Prog. Languages, pages 1-15, New York, \nJan 1990. ACM Press. 181 PO1 C. Flanagan, A. Sabry, B. F. Duba, and M. Felleisen. The essence of compiling \nwith continuations. In Proc. ACM SIG-PLAN 93 Conf. on Prog. Lang. Design and Implementa-tion, pages 237-247, \nNew York, June 1993. ACM Press. WI L. George, F. Guillaume, and 3. Reppy. A portable and opti- mizing \nbackend for the SML/NJ compiler. In Proceedinas of the 1994 International Conjerence on Compiler Con&#38;uc~ \ntion, pages 83-97. Springer-Verlag, April 1994. J. Y. Girard. Interpretation Fonctionnelle et Elimination \ndee Coupures dons 1 Arithmetique d Ordre Superieur. PhD thesis, University of Paris VII, 1972. J. Gosling, \nification. [I31 1141 R. Harper intensional Symp. on New York, WI G. Huet. B. Joy, and G. Steele. The \nJava Language Spec-Addison-Wesley, 1996. and G. Morrisett. Compiling polymorphism using type analysis. \nIn Twenty-second Annual ACM Principles of Pmg. Languages, pages 130-141, Jan 1995. ACM Press. Logical \nFoundations of Functional Programming. Addison-Wesley, 1990. WI X. Leroy. Unboxed objects and polymornbic \ntwinn. In Nineteenth Annual ACM Symp. on* Phncipiee of Frog: Lan-guages, pages 177-188, New York, Jan \n1992. ACM Press. Longer version available as INRIA Tech Report. X. Leroy and F. Rouaix. Security properties \nof typed applets. Jn Twenty-fifth Annual ACM Symp. on Principles of Prog. Languages, page (to appear), \nNew York, Jan 1998. ACM Press. P71 WI J.-J. Levy. Optimal reductions in the lambda calculus. In To H. \nB. Curry: Essays on Combinatory Logic, Lambda Calculus and Formalism. Edited by J. P. Seldin and J. R. \nHindley, Academic Press, 1980. PI T. Lindholm and F. Yellin. The Java Virtual Machine Spec-ification. \nAddison-Wesley, 1997. H. G. Mairson. Deciding ML typability is complete for de-terminsitic exponential \ntime. In Proc. 17th Annual ACM SIGPLAN-SIGACT Symp. on Principles of Progmmming Languages, pages 382-401. \nACM Press, 1990. PO1 R. Mihrer, M. Tofte, Ft. Harper, and D. MacQueen. The Def- inition of Standard \nML (Revised). MIT Press, Cambridge, Massachusetts, 1997. WI G. Morrisett. Compiling with Z ypes. PhD \nthesis, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, December 1995. Tech Report \nCMU-CS-95-226. P21 G. Morrisett, D. Walker, K. Crary, and N. Glew. From sys-tem F to typed assembly \nlanguage. In Proc. 25rd Annual ACM SIGPLAN-SIGACT Symp. on Principles of Progmm- ming Languages, page \n(to appear). ACM Press, 1998. P31 G. Nadathur. A notation for lambda terms II: Refinements and applications. \nTechnical Report CS-1994-01, Duke Uni-versity, Durham, NC, January 1994. P41 G. Nadathur and D. S. Wilson. \nA representation of lambda terms suitable for operations on their intensions. In 1990 ACM Conference \non Lisp and Functional Programming, pages 341-348, New York, June 1990. ACM Press. P51 WI G. Necula. \nProof-carrying code. In Twenty-Fourth Annual ACM Symp. on Principles of Prog. Languages, New York, Jan \n1997. ACM Press. 1271 S. Peyton Jones. Implementing lazy functional languages on stock hardware: the \nSpineless Tagless G-machine. Journal of Functional Progmmming, 2(2):127-202, April 1992. S. Peyton Jones \nand J. Launchbury. Unboxed values as first class citizens in a non-strict functional language. In The \nFifth International Conference on Functional Progmmming Languages and Computer Architecture, pages 636-666, \nNew York, August 1991. ACM Press. PI WI S. Peyton Jones, J. Launchburv. M. Shields, and A. Tol-ma&#38; \nBridging the gulf: a common intermediate language for ML and Haskell. In Proc. 25rd Annual ACM SIGPLAN-SIGACT \nSymp. on Principles of Progmmming Languages, page (to appear). ACM Press, 1998. S. Peyton Jones and E. \nMeijer. Henk: a typed intermedi-ate language. In Proc. 1997 ACM SIGPLAN Workshop on Types in Compilation, \nJune 1997. i3f-v [311 J. H. Reppy and E. R. Gansner. The eXene library manual. Cornell Univ. Dept. of \nComputer Science, March 1991. I321 J. C. Reynolds. Towards a theory of type structure. In Proceedings, \nColloque sur la Programmation, Lecture Notes in Computer Science, volume 19, pages 408-425. Springer-Verlag, \nBerlin, 1974. [331 B. Saha and Z. Shao. Optimal type lifting. In Proc. 1998 International Workshop on \nTypes in Compilation, March 1998. [341 2. Shao. Flexible representation analysis. In Proc. 1997 ACM SIGPLAN \nInternational Conference on Functional Progmmming (ICFP g7), pages 85-98. ACM Press, June 1997. 2. Shao. \nAn overview of the FLINT/ML compiler. In Proc. 1997 ACM SIGPLAN Workshop on Qpes in Compilation, June \n1997. I351 [361 Z. Shao. Typed common intermediate format. In Proc. 1997 USENIX Conference on Domain \nSpecific Languages, pages 89-102, October 1997. Z. Shao. Typed cross-module compilation. In Proc. 1998 \nA CM SIGPLA N International Conference on Functional Programming (ICFP 98). ACM Press, September 1998. \n I371 [331 Z. Shao and A. W. Appel. Space-efficient closure represen-tations. In 1994 ACM Conference \non Lisp and Functional Programming, pages 150-161, New York, June 1994. ACM Press. Z. Shao and A. W. \nAppel. A type-based compiler for Stan-dard ML. In Proc. ACM SIGPLAN 95 Conf. on Prog. Lang. Design and \nImplementation, pages 116-129. ACM Press, 1995. 1391 [401 D. Tarditi. Design and Implementation of Code \nOptimita-tions for a Type-Directed Compiler for Standard ML. PhD thesis, School of Computer Science, \nCarnegie Mellon Uni-versity, Pittsburgh, PA, December 1996. Tech Report CMU-CS-97-108. [411 D. B&#38;it&#38; \nG. Morrisett, P. Cheng, C. Stone, R. Harper, and P. Lee. TIL: A type-directed optimizing compiler for \nML. In Pruc. ACM SIGPLAN 96 Conf. on Prog. Lang. Design and Implementation, pages 181-192. ACM Press, \n1996. [421 M. Tofte. Region-based memory management (invited talk). In Pmt. 1998 International Workshop \non Types in Compi-lation, March 1998. [431 A. Tolmach. Tag-free garbage collection using explicit type \nparameters. In Proc. 1994 ACM Conf. on Lisp and Func-tional Programming, pages l-11, New York, June 1994. \nACM Press.  \n\t\t\t", "proc_id": "289423", "abstract": "Recent advances in compiler technology have demonstrated the benefits of using strongly typed intermediate languages to compile richly typed source languages (e.g., ML). A type-preserving compiler can use types to guide advanced optimizations and to help generate provably secure mobile code. Types, unfortunately, are very hard to represent and manipulate efficiently; a naive implementation can easily add exponential overhead to the compilation and execution of a program. This paper describes our experience with implementing the FLINT typed intermediate language in the SML/NJ production compiler. We observe that a type-preserving compiler will not scale to handle large types unless all of its type-preserving stages preserve the asymptotic time and space usage in representing and manipulating types. We present a series of novel techniques for achieving this property and give empirical evidence of their effectiveness.", "authors": [{"name": "Zhong Shao", "author_profile_id": "81351597965", "affiliation": "Dept. of Computer Science, Yale University, New Haven, CT", "person_id": "PP14127817", "email_address": "", "orcid_id": ""}, {"name": "Christopher League", "author_profile_id": "81332511122", "affiliation": "Dept. of Computer Science, Yale University, New Haven, CT", "person_id": "PP39074878", "email_address": "", "orcid_id": ""}, {"name": "Stefan Monnier", "author_profile_id": "81100545156", "affiliation": "Dept. of Computer Science, Yale University, New Haven, CT", "person_id": "PP39080689", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/289423.289460", "year": "1998", "article_id": "289460", "conference": "ICFP", "title": "Implementing typed intermediate languages", "url": "http://dl.acm.org/citation.cfm?id=289460"}