{"article_publication_date": "09-29-1998", "fulltext": "\n A Type Based Sharing Analysis for Update Avoidance and Optimisation Jiirgen Gustavsson Chalmers University \nof Technology and Gijteborg University Abstract Sharing of evaluation is crucial for the efficiency \nof lazy fnnc- tional languages, but unfortunately the machinery to imple- ment it carries an inherent \noverhead. In abstract machines this overhead shows up as the cost of performing updates, many of them \nactually unnecessary, and also in the cost of the associated bookkeeping, that is keeping track of when \nand where to update. In spineless abstract machines, such as the STG-machine and the TIM, this bookkeeping \nconsists of pushing, checking for and popping update markers. Check-ing for update markers is a very \nfrequent operation and indeed the implementation of the STG-machine has been optimised for fast update \nmarker checks at the expense of making the pushing and popping of update markers more costly. In this \npaper we present a type based sharing analysis that can determine when updates can be safely omitted \nand marker checks bypassed. The type system is proved sound with respect to the lazy Krivine machine. \nWe have imple-mented the analysis and the preliminary benchmarks seem very promising. Most notably, virtually \nall update marker checks can be avoided. This may make the tradeoffs of cur- rent implementations obsolete \nand calls for new abstract machine designs. Introduction In the implementation of a lazy functional language \nsharing of evaluation is performed by updating. For example, the evaluation of (Xx.x + x) (1 + 2) proceeds \nas follows. First, a closure for 1 + 2 is built in the heap and a reference to the closure is passed \nto the abstraction. Second, to evaluate x + z the value of 2 is required. Thus the closure is fetched \nfrom the heap and evaluated. Third, the closure is updated (i.e., overwritten) with the result so that \nwhen the value of x is required again the expression need not be recomputed. The sharing of evaluation \nis crucial for the efficiency of lazy languages. However, it also carries a substantial over- 0 ,998 \nACM l-581 13.024.4,98,0009...85.00 head. This overhead consists of the cost of performing up-dates, many \nof them actually unnecessary, and also of the cost of the bookkeeping of updates, that is keeping track \nof when and where to update. Measurements suggest that about 20% of the execu-tion time is spent on unnecessary \nupdates [MarSS]. It is therefore no surprise that considerable effort has been put into analyses that \ncan discover unnecessary updates [SesSl, LGH+92, Mar93, TWM95, Mog]. In the design and implementation \nof abstract machines, considerable attention has been given to minimising the bookkeeping associated \nwith shared computation. See for example [PJ92]. However, comparatively little work has been done to \neliminate bookkeeping overheads by static pro-gram analysis. The only work we are aware of is an analysis \nby Sestoft (SesSl]. In this paper we present a type based sharing analysis that can determine when updates \ncan be safely omitted and also enables us to optimise the bookkeeping of updates. We will take the type \nsystem by Turner, Wadler and Mossin [TWM95] as our starting point. Our type system has a number of properties. \n0 It is more precise than the analysis by Turner et al, that is it will discover more unnecessary updates. \n. It provides information that enable us to optimise the bookkeeping of updates. Indeed, this is our \nmajor con-tribution. . It handles all features of a realistic functional language including higher order \nfunctions, data structures and mutual recursion. . It is proved sound with respect to the lazy Krivine \nma-chine, by showing that evaluation preserves typings. . Preliminary benchmarks indicates that the analysis \nis surprisingly effective. 1.1 How to optimise Consider the following program. let z = 1 + 2 in let y \n= (X.z.2) 5 in Y+Y Here, the value of y is clearly needed twice. Thus the clo-sure referred to by y needs \nto be updated with the result of (XZ.Z) x. Due to the update, (XZ.Z) z is computed only once and thus \nx will be dereferenced only once. Therefore it would be safe to omit the update of x. Our analysis provides \nthis information by annotating the expression as follows. let x =d 1 + 2 in let y =! (XZ.Z) z in Y+Y \nHere, annotating the binding of x with a d/indicates that the corresponding closure needs not be updated \nand annotating the binding of y with a ! indicates that the corresponding closure needs to be updated. \nWe can reduce the bookkeeping of updates if we can pre- dict when updates take place. Potentially, an \nupdate may be needed whenever a value has been computed. However, in our example only one update needs \nto take place, namely the update of y with the result of 1 + 2. Our analysis provides this information \nby annotating the expression as follows. let 2 =J l[ Tol +[ I 21\u00b0901in let y =! (Xl ~ola.z) x in y +PsJl \ny Here, the annotation [l, l] on 1+2 indicates that exactly one closure (namely y) needs to be updated \nwith the result of the addition. Naturally, the annotation [0, 0] indicates that no update needs to take \nplace. This information enables us to optimise the bookkeeping of updates and we will return to this \nexample in section 3 and discuss how to apply the information in the implementation of an abstract machine. \n 1.2 Outline This paper is organised as follows. Section 2 introduces the language and its semantics \nin the form of an abstract ma-chine. Section 3 discusses how to optimise the bookkeeping of updates. \nSection 4 extends the language with annotations and gives a semantics to the extended language. Section \n5 presents the type system. Section 6 argues the soundness of the type system. Section 7 gives an overview \nof the im-plementation. Section 8 presents some experimental results. Section 9 describes related work. \nSection 10 concludes. Sec-tion 11 suggests possible future work. 2 Language 2.1 Syntax The language we \nuse is a lambda calculus extended with integers, lists and recursive let-expressions. Following Launchbury \n[Lau93], we use a restricted syntax given be-low. Variables x, y, z Values 21 ::= Xx.e 1 n 1 nil 1 cons \nx y Terms e ::= wJxlexleo+elI let d in e I case e of nil -+ ee cons x y + el Declarations d ::= e(d,b \nBindings b ::= x = e The distinguishing feature of the syntax is that arguments in applications and \ncons are restricted to variables. It is straightforward to translate a term in the standard syntax into \nthe restricted form. For example, (Xx.x + x) (1 + 2) is translated into let y = 1 + 2 in (Xx.2 + x) y. \nThus the creation of a closure for 1 + 2 is made explicit via the let- expression. Making the creation \nof closures explicit greatly simplifies the abstract machine as well as the analysis pre-sented in this \npaper. Indeed, the same restriction appears in the intermediate language of the Glasgow Haskell Compiler \n[JHH+93]. 2.2 Semantics We will take the lazy Krivine machine [Ses97] as the seman-tic basis of our \nwork. The choice of an abstract machine makes the update machinery explicit and enables a sound- ness \nproof of our analysis. A correspondence between the lazy Krivine machine and Launchbury s natural semantics \nfor lazy evaluation (Lau93] has been shown in [Ses97]. The machine can also serve as a starting point \nfrom which lower level abstract machines can be derived [Ses97]. For the purpose of the abstract machine \nwe extend the set of terms to include expressions of the form add, e. We define a reduction relation \ne ++ e between terms: P-1 Y ti e[x := y] n+e e add, e add,, nl e 722 if no + ni = 122 case nil of b+ \ne0 nil + ec cons x y + ei case cons x0 xi of ++ el[yo := x0, yl := xl] nil --t ee cons YO yl --) el \nConfigurations in the abstract machine are triples (H ; e ; S), where H is a heap, e is the term currently \nbeing evaluated and S is the abstract machine stack: Configurations C ::= (H; e; S) Heaps H ::= elH,b \nStacks s ::= elR,SIU,S Reduction contexts R ::= []x][]+eladd,[]I case [ ] of nil + ee cons x y + ei Update \nmarkers u ::= #x A heap consists of a sequence of bindings. The variables bound by the heap must be \ndistinct and the order of bind- ings is irrelevant. Thus a heap can be considered as a par- tial function \nmapping variables to terms and we will write dam(H) for the set of variables bound by H and range(H) \nfor the terms bound in H. An abstract machine stack is a stack of shallow reduction contexts and update \nmarkers. The stack can be thought of as corresponding to the surrounding derivation in a natural semantics, \nwhere the role of an update marker #x is to keep track of a pending update of x. The update markers on \nthe stack will be distinct, that is there will be no more than one pending update of the same variable. \nWe will consider an update marker as a binder and we will write #S for the variables bound by the update \nmarkers in S. Consequently, we will require the variables bound by the stack to be distinct from the \nvariables bound by the heap. We will also identify configurations up to o-conversion, that Let {H; letdine; \nS) H (H,d; e; S) if dam(d) II (dam(H) U #S) = 0) {H,a: = e; 2; S) I--) (Hi e; #x,S) EFwind (H; R[e] . \nS) +-+ (H; e; R,S) Update (H; v; # z,S) H (H,x = u; v; S) Reduce (H; v; R,S) k-+ (H; e; S) if R[w] C) \ne GC WO,HI ; e; S) +-+ Wo;e;S) if dom(Hr) II (fv(range(He)) U fv(e) U fv(S)) = 8 Lookup (H,z = u; z; \nS) +-+ (H,o = v; v; S) Figure 1: Abstract machine transistion rules is renaming of the variables bound \nby the heap and the stack. An initial configuration is of the form (e ; e ; E), where e is a closed expression. \nThe transition rules of the abstract machine are given in figure 1. The rule Let creates new bindings \nin the heap. The side condition ensures that the bindings in the heap and the stack remain distinct. \nThe condition can always be met simply by o-converting the let-expression. The rule Var evaluates a variable \nz. It looks up the corresponding expression e in the heap, removes the binding, pushes an update marker \nfor z on the stack and starts the evaluation of e. Later, if e terminates, the update marker will see \nto that x gets updated with the result. The removal of the binding corresponds to so called black-holing: \nif the evaluation of e to weak head normal form depends on x (i.e., x depends directly on itself) the \ncomputation will get stuck, since x is no longer bound by the heap. Note that we still consider the configuration \nto be closed, since x is bound by the update marker on the stack. The rule Unwind allows us to get to \nthe heart of the evaluation by unwinding a shallow reduction context. When the term to be evaluated \nis a value the next tran-sition depends on whether an update marker or a reduction context is on top \nof the stack. To determine which rule to apply a so called update marker check is performed. If the top \nof the stack is an update marker the rule Update applies and the heap is updated accordingly. If it is \na reduction con-text; Reduce applies, the value is plugged into the reduction context and a reduction \ncan take place. Finally, the rule GC allows bindings to be discarded from the heap. The side condition \nensures that no live bindings are removed so that configurations remain closed. The abstract machine \npresented so far has a built in in- efficiency: When a value is looked up with the rule Var the binding \nis removed from the heap and an update marker is pushed onto the stack. Then, by the rule Update, the \nmarker is immediately popped of the stack and the binding is added to the heap again (i.e., an update \nis performed). This is in- deed a common case and the abstract machine could be op- timised for it by \nadding the synthesized rule Lookup which allows a value to be looked up without pushing a marker and \nperforming an update. Any reasonable compiler would perform this optimisation so we have included it \nto make for realistic benchmarks. The machine can terminate (modulo garbage collection) for three different \nreason: l The computation terminates successfully with a value. l A black-hole is detected. l The computation \ngoes wrong . Here, going %vrong means reaching a configuration of the form (H ; w ; R, S) where R[v] \nIft. This can not happen if we only consider well typed terms (i.e., well typed terms can not go wrong \n[Mil78]). We define Value, Blackhole and Wrong to be the sets of terminal configurations of the different \nforms. We will let V, B and W range over these sets. 3 Optimising the bookkeeping machinery We will now \ndiscuss how the bookkeeping of updates can be reduced in the context of the abstract machine. Consider \nagain the example from the introduction. letx = 1+2in let y = (Az.z) x in Y+Y Running this program in \nthe abstract machine yields the transition sequence given in figure 2. The cost of the book- keeping \nof updates shows up in a number of places. . In the applications of the rule Var where an update marker \nis pushed onto the stack in order to record that an update shall eventually take place. This happens \nin the transitions 4 H 5 and 7 c--t 8. . In the applications of the rule Update where an update marker \nis popped from the stack and an update takes place. This happens in the transitions 12 H 13 and 13 * \n14. l Whenever a value is in the second component of the configuration an update marker check has to \nbe per- formed in order to decide whether an update should take place or not. This happens in the transitions \n6 e 7, 9 I-+ 10, 11 c--) 12, 12 H 13, 13 H 14, 14 w 15 and 17 e 18. We note that update marker checks \nseem to be very com-mon. Indeed, the measurements presented in section 8 sug- gest that update marker \nchecks are typically about ten times as frequent as updates. It is therefore no surprise that im-plementations \nof abstract machines tend to be optimised for fast update marker checks at the expense of a large repre-sentation \nof update markers, making the pushing and pop- ping of them more expensive. For example, in the imple-mentation \nof the STG-machine, update marker checks are very cheap while update markers are represented using three \nwords where only one word (using 1 bit as a tag) would suf-fice [PJSL]. 41 To summarize; our example \ncan be annotated as follows. (e ; let z = 1 + 2 in let y = (XZ.z) z in y + y ; e) (1) 2% (x = 1 + 2 ; \nlet y = (X2.2) x in y + y ; e) (2) I% (cc = 1 + 2,y = (XZ.Z) x; y+ y ; e) (3) uPT (x = 1 + 2, y = (Xz.2) \n2; y ; [] + y) (4) 2% (x = 1 + 2 ; (Xz.z) x ; #y, [ ] + y) (5) ua(x = 1+2;A z.z; [lxc,#Y,[l+Y) (6) 3(x \n= 1+2;x;#y,[]+y) (7) 2s (e ; 1 + 2 ; #x7 #Y, I I + Y) (8) ue3 (c ; 1 ; [I + 2, #x:, #Y, [ I+ Y) (9) B \n(6; add1 2; #~c,#~,[l+~) (10) nu(. (e ; 2 ; add1 [I, #x:, #Y, [I + Y) (11) 25 (E ; 3 ; #x:, #Y, [ I+ \nY) (12) \"xqx = 3; 3; #y,[]+y) (13) \"F-$(x = 3,y = 3;3;[]+y) (14) w (x = 3,y = 3; adday; e) (15) &#38;(x \n= 3,y = 3; y; adda[]) 06) 3(x = 3,y = 3; 3; adda[]) (17) %(x = 3,y = 3;6;~) (18) Figure 2: A transition \nsequence However, although the cost of a single update marker check is low, an analysis that can reduce \nthe number of checks could be very worthwhile. In fact, the preliminary benchmarks of our analysis suggest \nthat update marker checks can be avoided to such an extent that they become less frequent than updates. \nAs a consequence the implemen- tations of abstract machines might be the subject of review. We can avoid \nupdate marker checks by predicting what will be on top of the stack when either of the rules Reduce or \nUpdate will be applied, that is when a value has been computed. In our example only two updates take \nplace, namely when x and y are updated with the result of 1 + 2. Thus, when 1 + 2 has been computed two \nupdate markers reside on top of the stack, one saying that x shall be updated and one saying that y shall \nbe updated (see configuration 12). However, if we bypass the update of x (as discussed in the introduction) \nthere will only be one marker there. We can express this fact by annotating 1+2 as l+I . l2, meaning \nthat exactly one update marker has to be taken care of. In our example no other value needs to take care \nof any update marker and we can convey this fact by annotating them with the annotation [O,O]. For example \nXz.z will be annotated as XI olz.z, which allows us to avoid the update marker check in the transition \n6 e 7. 1st 2 =d l[ *ol +LIT1l 21\u00b0901 in let y =! (X[\"*olz.z)x in y +lWl y In total this saves the cost \nof pushing and popping one up- date marker, performing one update and doing seven update marker checks. \n4 A language with annotations The analysis conveys information by annotating expressions. In this section \nwe present the language of annotated expres-sions and its semantics. 4.1 Syntax We will annotate bindings \nin let-expressions with a K, rang- ing over {J, !}. Here, ,/ means the binding will not be up- dated \nand ! means that it will. We define an ordering with J< !. Values (and +) are annotated with an L ranging \nover pairs [r~,[] specifying the number of update markers the value (and the result of +) can take care \nof. The first com-ponent 11 gives a lower bound on the number of markers that must reside on the stack \n(because the value will take them for granted) and the second component 4 gives an upper bound on the \nnumber of update markers that may reside on the stack (because the value will not look for more). Con-sequently, \nwe will refer to them as the lower and the upper bound. We will let 7 and < range over N U {w}. Including \nw allows us to have annotations like [O,w] meaning that an arbitrary number of update markers can be \ntaken care of, effectively serving as an escape-hatch for the analysis. We define an ordering [Q [] 5 \n[q , ( 1 iff 71 2 7 and 5 5 <. The pair can also be thought of as an interval specifying the num- ber \nof update markers that can be taken care of. Similarly the ordering can be thought of as interval inclusion. \nThus, the language of the previous section is extended as foIlows. Variables X>Y,Z Values 6 ::= Xx.dln \n[nil 1 cons zy g ..- Terms ..-liL(xIt?xJdo+ EII letJind( case Lof nil-b 20 cons x y --f 21 Declarations \nd ::= elJ,i Bindings &#38; ::= x =fi e We will sometimes use /\\ x.e and consL xy as syntactic sugar \nfor (Xx.e) and (cons x y) respectively. 4.2 Semantics The meaning of the annotations is given by modifying \nthe abstract machine of the previous section. Again, we extend the set of (annotated) terms to include \nthe expression addkd. We define a reduction relation d 4 8 between annotated Let Var-! Var-J Unwind Update \nReduce GC Lookup terms: (i?; letding; S) (fi,x =! E; 2; 3) {I?,2 =J f?;x; S) (k ; fi[Z] ; 3) (J? ; dlq9F1 \n; #x, 3) (I? ; iw ; fi2 3) (&#38;,&#38; E; S) (a, x =! tilr19E ; z ; 3)  ci (fi,f.i; E; 3) IA (a; d; \n#x,3) cj (iJ; Ii!; 3) 4 p; E; I?,$ 4 (H,x =! ~h,Sl ; +[VT1?tvll ; 3) 4 (a; E; 3) -2 (i&#38;l; (H,x 2; \n3) =! fphe1 ; ~[V-1*5-11 ; 3) if dam(d) n (dom(@ U #$ = 8 if x f2 (fv(range(H)) U fv(fZ) U fv(S)) if \n,( > 1 if 71 5 0 and fi[fjl E ] 4 6 if dom(Z?i) fl (fv(range(&#38;)) U fv(6) U k(s)) = 0 if< 1 1 Figure \n3: Abstract machine transition rules for annotated terms (XXJZ) y I4 E[x := y] d + z t-3 add; C add&#38; \nni 14 ni if 720 + 711 = n2 case nil of ;t co nil -+ to cons x y -+ 61 case cons xc 21 of ;t &#38;[yrJ \n:= xo,y1 := Xl] nil --f 6%~ cons yo y1 + 21 Configurations in the abstract machine now take the form \n(fi ; 6 ; s), where I? is a heap of annotated bindings and 3 is an annotated abstract machine stack: \nConfigurations c ::= (flj ; S) Heaps il ::== e(lq Stacks .!? ::= el R,Slfi,S Reduction contexts fi ::= \n[]xl[]+ t?]addk[]l case [ ] of nil + do cons x y * EI Update markers 0 ::= #x The transition rules of \nthe abstract machine are given in figure 3. The rules Let, Unwind and GC remain unchanged (adding only \na - everywhere). There are now two Var rules. The rule Var-! takes care of variables bound to closures \nthat shall be updated. It looks up the binding in the heap, removes it, pushes an update marker and evaluates \nthe expression. The rule Var-J takes care of variables bound to closures that shall not be updated. It \nlooks up the binding in the heap, removes it and evaluates the expression without push-ing an update \nmarker. Indeed, this means that the bind-ing will not be updated. The side condition ensures that the \nconfiguration remain closed. This is important since an open configuration would correspond to dangling \npointers in an implementation. If the side condition is not fulfilled the computation will go wrong and \nwe will consider the configuration to be ill-annotated. The rule Update has been modified to take the \nannota-tion on values into account. In order for the value to perform an update the upper bound must \nbe nonzero. This reflects that a value with a zero upper bound requires the top of the stack to contain \na reduction context. Once the update has been performed the annotation is decreased to record that an \nupdate marker has been taken care of. Here we take w -1 to be w and 0 - 1 to be 0. If the upper bound \nis zero the computation will go wrong and we will consider the configuration to be ill-annotated. The \nrule Reduce has been modified to take the annota-tion on values into account. In order for the value \nto take part in a reduction the lower bound must be zero. That is the value must not require an update \nmarker on top of the stack. If the lower bound is nonzero the computation will go %vrong and we will \nconsider the configuration to be ill-annotated. Again the rule Lookup can be added to make for a more \nefficient abstract machine. The machine can terminate (modulo garbage collection) for three different \nreason: the computation terminates suc-cessfully with a value, a black-hole is detected or the com-putation \ngoes wrong . However, now the computation can go wrong for a number of reasons: A binding is erroneously \nannotated with a J. In this case the terminal configuration has the form (I?, x =d E ; x ; 3) where x \nE (fv(range(H)) U h(E) U h(s)) An update marker is on top of the stack, but the anno-tation on the value \nerroneously specify that no update marker needs to be taken care of. Thus, the terminal configuration \nis of the form (fi ; 61 1,01 ; #z, 3). An reduction context is on top of the stack but the annotation \non the value erroneously specifies that there shall be an update marker on top of the stack. Thus, the \nterminal configuration is of the form ~h+l tl ; jj, 3). (a; The configuration @61\u00b0~c1] 4. This well typed \nterms system). We define Value-, is of the form (fi ; irIO,E1 ; fi, 3) but can not happen if we only \nconsider (that is well typed in an ordinary type Blackhole-and Wrong-to be the sets of terminal configurations \nof the different forms. We will let Q , g and fi range over these sets. We will say that a configuration \nis ill-annotated if it goes wrong . Conversely, we will say that a configuration is well-annotated if \nit do not go wrong . We will also say that a closed term Z is ill- annotated/well-annotated if (6 ; d \n; c) is ill-annotated/well- annotated. A crucial property of our analysis should be that it an-notates \nexpressions so that the computation can not go wrong . Our analysis is phrased as a type system and indeed \nour soundness result says that well-typed terms are well-annotated. An important implication is that \nif only well typed terms are considered, the cost associated with detect-ing if a computation goes wrong \ncan be avoided. This is particularly important for the rule Var-,/ where checking if the side condition \nholds would be very expensive. 5 The type system In this section we present a type system in the form \nof a type directed translation that annotates expressions. We will take the type system by Mossin, Turner \nand Wadler as our staring point and we will modify it so that it discovers more unnecessary updates and \nprovides information that allow us to bypass update marker checks. For simplicity the system is monomorphic \nand our type language does not contain type variables. 5.1 The basic idea The semantics of section 4 \nspecify that for a binding x = e to be annotated with a J it is required that when (if ever) the binding \nis used there is only one occurrence of x in the configuration, namely the one that is dereferenced. \nThe type system is based on the following idea. If, when a binding x = e is created, x occurs only once \nin the config- uration and x never gets duplicated during the computation then x will occur only once \nwhen (if ever) it is dereferenced. Thus, the type system will annotate a binding with a J, if the corresponding \nvariable occurs once when the binding is created and it can assure that it never gets duplicated. A variable \nx may get duplicated during the computation in two ways: l By the rule Reduce; when reducing an application \n(or a case-expression) the variable gets substituted into the body of the abstraction (or the cons-branch \nof the case) and if the bound variable occurs more than once in the body then z will get duplicated. \nThis is the reason why the binding of x is annotated with a ! in the following example. let x =! 1 + \n2 in 0Y.Y + Y) x l By the rule Update (or Lookup since it is synthesized from Update); when an update \n(or lookup) is per-formed the value gets duplicated and thus also its free variables. In the example \nletx =! 1+2in let f =! Xy.x + y in fl+fz the abstraction Xy.x + y gets duplicated when f is looked up. \nThus, since x is a free variable of the ab-straction, x gets duplicated. Indeed, this is the reason why \nthe binding of x is annotated with a !. There is an important exception to the above; when a vari- able \noccurs once in both branches of a case-expression. Then, since eventually only one branch will be taken, \nthe type system will consider it as occurring only once. 5.2 Type language The type language is given \nbelow and is an extension to an ordinary type language. Bare types p ::= Int 1 u -+ T 1 [u, K, L] Types \nT ::= pL Binding types u ::= rK We will sometimes use 0 + r as syntactic sugar for (a + T) . Here, p \nranges over bare types, that is types without outermost annotations, and includes integers, function \ntypes and lists. Here, r ranges over types. The type of an expression t? reflects not only the ordinary \ntype of the expression but also the number of update markers the value of the expression (if it terminates) \ncan take care of. Thus, an expression with type plqVcl must be able to handle any number of markers between \n9 and <. For example, both 51iV 1 and 51\u00b0*31 have the type Int llJl, but only 51\u00b0Y3] has the type Int1\u00b0*31. \nActu- ally every expression that has the type Int1\u00b0 31 also has the type Int 11721 We therefore say that \nIntl *31 is a subtype of . Int[L l, Here, o ranges over binding types. The type of a binding x =&#38; \nE naturally reflects the type of the expression E but it also reflects the way it is bound. Saying that \na binding has the type r! means that the binding will get updated (i.e., that K = !). On the other hand \nsaying that it has the type TJ means it might not get updated (i.e., K.= J or K = !). Thus n is a subtype \nof rd. The bare type u + r denotes functions that when ap-plied to a variable (remember expressions can \nonly be ap- plied to variables) referring to a binding of type g will yield something of type 7 . The \nbare type [o, IE, L] denotes lists whose head (if non-empty) has the binding type u, and whose tail has \nthe bind-ing type [a, 6, L];. The fact that the head and the tail are given binding types (rather than \njust types) reflects that cons can be applied to variables only. Thus the types of the head and the tail \nare actually the types of the bindings referred to from the cons-cell. An empty list can naturally be \ngiven any list-type. As suggested above there is a natural subtyping relation between types, given below. \n7 5 7 K 5 K L < L P I P TK < r;, p < p/L 6 5 u r 5 7 Int 5 Int U-bT<U --fT   u I u K 5 K L < L [u, \nK, 61 I b , K , 6 1 5.3 Contexts We use two kinds of contexts defined below. Contexts r ::= Xl : Ul, \n ) xn : un Distinct Contexts A ::= xi : 01, . . . . xn : un where xi # xj if i # j A context associates \nbinding types with variables and con-sists of a sequence of type associations of the form z : u. A context \nmay very well contain several occurrences of the same variable, and the ordering of type associations \nis irrele- vant. Several occurrences of the same variable x in a context will indicate that x occurs \nseveral times in the correspond- ing term. We will write PO, Pi to denote the concatenation of PO and \nPl. A distinct context is a context that does not contain the same variable more than once. If (and only \nif) the distinct contexts A0 and Ai have no variables in common we will write A,, Ai for their concatenation. \nA distinct context A is said to entail I (written Ab I?) if I can be obtained from A by discarding and/or \nduplicating (under some restrictions) type associations in A. Context entailment is defined below. Discard \nA D r, X : u Id- ADA ADr ADr, X:7! Duplicate ADr, x:~, x:-r! The rule Id simply says that a distinct \ncontext entails itself (when considered as a context). The rule Discard allows type associations to be \ndiscarded. The rule Duplicate allows type associations to be duplicated. Note however, that the rule \nonly applies to type associations of the form x : r!. This reflects the fact that a variable referring \nto a binding of the type rd must not get duplicated during the computation. 5.4 Typing judgements The \nanalysis is presented in the form of a type directed translation. There are three forms of typing judgements; \none for each syntactic category. Judgements for expressions take the form I l- e + d : r and shall be \nread In the context I , the expression e can be annotated as i; having type 7 . We will refer to e as \nthe source and i as the target of the translation. The context as usual keeps track of the types of the \nfree variables of d but it also plays another important role; it records the number of times each variable \noccurs in the term. Thus if x occurs n times in E it also occurs n times in I (with one important exception, \nnamely if x occurs in different branches of a case- expression). Judgements for bindings take the form \nI t-b -+ 6 : x : u. Note that the type of a binding includes the name of the bound variable, i.e., the \ntype of a binding is actually a type association. Thus, for example, x =d 511311has the type x : Int[; \n* . The context l? will contain all variables occuring in the right hand side of the binding including \nthe variable bound by the binding (if it occurs in the right hand side). Judgements for declarations \n(a group of bindings) take the form l? k d 4 2 : A. Thus, the type of a declaration is a distinct context \ncontaining the types of the bindings in 2. Insisting on a distinct context reflects that a declaration \nmay not bind the same variable more than once. Again, the context I? will contain all variables occuring \nin the right hand sides of the declaration including the variables bound by the declaration (if they \noccur in the right hand sides). 5.5 Typing rules The typing rules are given in figure 4. The rule Int \nis straightforward; it simply says that that nc has the type Int . The rule Plus, however, requires some \nexplanation. Nat- urally PO + 21 is given the type IntL, but the rule also re-quires that Ec (and ai) \ncan be given a type Intl ~~ol. This reflects the way + is evaluated: when evaluation of do starts, a \nreduction context of the form [ ] + 61 is pushed onto the stack. Thus it is crucial that 60 does not \nrequire any update markers on top of the stack. Similarly, Ei must not require any update markers either. \nThe rule Add is similar to the rule Plus. The rule App ensures that the function does not require any \nupdate markers (cf the rule Plus). Otherwise it is com- pletely standard. The rule Var is straightforward \nand simply records the fact that x occurs once in the term. There are two rules for abstractions. However, \nthis is for reasons of clarity and it is a simple matter to combine them. The rule Abs-0 applies to the \ncase when the abstraction gets annotated with [q, 01. The key feature of the rule is that if x occurs \nmore than once in i then the abstraction will be assigned a type of the form r{ + ln3 ] r indicating \nthat a variable will be duplicated if it is passed to the abstraction. This is accomplished by first \ntyping E in a context PO, Pi, where x $! dom(ro). Then, if x occurs more than once in d, x will occur \nmore than once in Pi, and thus the condition x : u D rl will force u = r/, for some 7 . The rule Abs-+1 \napplies to the case when the abstrac-tion is annotated with [r],J + 11, i.e., when the abstraction has \nto take care of possible update markers. Taking care of an update marker means updating with the value, \nthus du-plicating any free variables of the abstraction. The purpose of the extra condition is to force \nthese free variables to have a type of the form r;, indicating they might be duplicated. The rules Nil, \nCons-O, and Cons+1 are straightforward. The rule Case contains a subtle treatment of contexts. If a \nvariable occurs once in each branch of the case-expression and thus twice in the term it may still only \noccur once in the context. This is achieved by collecting the variables that occur in both branches in \na common context I i, thus effec-tively counting a variable occuring in both branches as one. The condition \ndoni fl dom(r3) = 0 just ensures that this is done in an optimal way. Finally, the last two condi-tions \ntake care of the variables bound in the cons-pattern. It works in essentially the same way as in the \nrules for ab-stractions. The rule Let ensures that if a let-bound variable occurs more than once or may \nbe duplicated then the type of the binding must be of the form r{. This is accomplished in the following \nway: First, we type 2 yielding a distinct context A containing the types of the bindings in d. We then \nsplit the context., in which d was typed, into I 0 and Ii such that any occurrence of a variable bound \nby 2 ends up in Pi. This is ensured by the condition dam(A) II dom(Po) = 0. Second, we type E and then \nsplit the context into Pz and I s. Analogously, any occurrence of a variable bound by d ends up in I \ns, ensured by dam(A) n dom(P2) = 0. Now if a let-bound variable x occurs more than once in d and E, then \nx will also occur more than once in Pi, P3 and thus the condition A D rl, r3 will force the type of x \nto be of the form r[, for some 7 . The rule Sub simply allows subtyping to be applied. The rules Declaration-c \nand Declaration are simple. They simply collect the types of the bindings in the dec-laration. plusro \nI-eo -4 Eo : Int[O'cO1 rl k el us El : Int[ C1l Int k n -4 nc : Int ro, rl I- e. + el + Co +L II : Intb \n r t- e ---+ e : D --G ~~l T ro, rl k- e u-) E : 7 Var Abs-0 APP r, x:ukexv. Ex:r x:r~l-x+-+x:r r. I-Xx.e \nr t-e -4 E : Int'\"'e'l Add r b add, e --+ addk Z : IntL x 6 dom(r0) 2:oDrl  -N~~o1x.E: g -0301 7 PO, \nrl t-e-re :r x B dom(ro) xzot>rl if y : o E r0 then 0 E 7: for some 7 Abs-+1 r. k Xx.e -4 XhL+ll~.,g \n: (T --+h~+~l 7 co 5 0 01 I [u, 6, L]1, Nil Cons-O t- nil --+ nil1 : [a, K, L ]l 5 : 00, y : u 1 b cons \nx y Q-+ cons[~~ol x y : [a, K, L][Q~Ol uo I T! 01 5 [T!, !, L]! Cons-+1 x : uo, y : g1 k cons x y * \ncons[~~~+ll X y : [r!, !, L][l) E+ll rl,r2t-eo420:T 2, Y @ dom(rl, r3) r. b e 4 E : [c, K, h][ ,cl rl, \nr3, r4 I- el or) El : 7 dom(r2) n dom(r3) = 0 % : u, y : [&#38; K, L]: D r4 Case ro, rl, r2, r3 k case \ne of {nil -+ eo; cons x y -3 el} --,casee of{nil~~0;consXy-t~~}:~ Letro, rl t-d-d: A dam(A) n dom(ro) \n= 0 rz, r3t-e-+2:7 dam(A) n dom(rz) = 0 ADI',, r3 ro, I zt-letdine++letdini:7 * rl-e+E:r r < 7 rol-d-+i:A \nl?ltb-,b:x:a Sub Declaration-e Declaration r t- e -E : T t-e-e:0 lTo, rl k d, b --+ 8, T, : A, x : u \n[n+l.E+ll ITI-e+d:r rke-+d:p Binding-Jr ,- x = Binding-! e s-ix =\\/ E : x : Td r t- x = e .+ 2 =! d \n: x : pjqTcl Figure 4: Typing rules There are two rules for bindings. However, this is for reasons of \nclarity and it is a simple matter to combine them. The rule Binding-,/ allows a binding to be annotated \nwith a J and assigns the binding a type of the form 7~. The rule Binding-! allows a binding to be annotated \nwith a ! and assigns the binding a type of the form 7!. However, it also requires the expression to be \nable to take care of, and allows it to require, an extra update marker. This re-flects the fact that \nwhen the binding gets evaluated an extra update marker will be pushed onto the stack. 6 Soundness In \nthis section we will prove the soundness of our type sys-tem, that is that well-typed terms are well-annotated. \nIt is proven by showing a subject reduction like property, i.e., that evaluation preserves typings. Since \nour notion of eval- uation involves transitions between configurations we first need to define what it \nmeans for a configuration to be trans- lated into an annotated configuration. 6.1 Typing heaps, stacks \nand configurations We will need four forms of typing judgements; for configu- rations, heaps, reduction \ncontexts and stacks. Typing judgements C ru) 6 : r. The type r of the result of evaluating we only consider \nclosed any context. Typing judgements for configurations take the form b of a configuration is simply \nthe type the configuration. Note that since configurations there is no need for for heaps take the form \nl? t- H -+ k : A. The form of judgements for heaps is similar to the form of judgements for declarations \nand can be understood in the same way. Typing judgements for reduction contexts take the form I-l--R \n+ h : [~0]rl. The type (~0171 means that if a term E of type 70 is plugged into 12 then the result @a] \nhas type 71. We will take the desired property as the definition: r0 b R *-) &#38; : [~]n iff for any \ne, E, rl such that I l k e + i2 : ~0 it is the case that I 0, rl I- R[e] -.+ fi[G] : 71. Typing judgements \nfor stacks take the form r t- 5 -+ 3 : A ; [Q]T~. Here, A corresponds to the types of the update markers \nin the stack. Since we think of them as binders they are given a type of the same form as given to a \nheap. Here, 70 is the type the stack expects the expression in the configuration to have. If so, 71 will \nbe the type of the whole configuration. The typing rules are given in figure 5. The rule Config ensures \nthat if a variable occurs several times in the configuration or may be duplicated then the I otH-+fi:A, \nrlt-e--+d:70 r2 k s --+ s : A, ; [70]71 AO, AlbrO, rl, r-2 Config Heap-e t-(H;e;S)--,(k;i2;>):~1 ke-++e:a) \nI ot-H+-+il:A rl kb-rb:x:u r+H+fi:A Heap Heap-discard Stack-e ro, rl I- H, b --+ fi, 6 : A, x : u lYH,b-&#38;:A \nk E -.+ E : 0 ; [TIT ro I- R u-) fi : [n,]n rl t- s --+ L? : A ; [ri,l~~ I- I- S -.-a 3 1A (ploVE1], \n> Stack-R Stack-# ro, rl t-R, s ---+ ii, 3 : A ; [TO]72 r t- #x, s 6 #2, s : A, x : piQ S1 ; [pl + ~~+ \nl]r I? k S u+ i? : A ; [r&#38;1 stack-sub r yk;t pc rilT: 5 To Stack-#-discard r k #x, s -w-k 3 : A \n; [ro]r1 : ;o Figure 5: Typing rules for heaps, stacks and configurations corresponding binding in the \nheap is annotated with a !. This is achieved as follows: First, we type fi in a context rs yielding a \ndistinct context Ao containing the types of the bindings in fi. Second, we type 2 in a context l?i yielding \nsome type TO. Third, we type ,? in a context r2 yielding a distinct context Ai, corresponding to the \ntypes of the update markers, and a type ri giving the type of the whole configuration. Now, if a variable \nx occurs more than once in 2, d and S then x will also occur more than once in ro, ri, r2 and thus the \ncondition A,, Ai D ro, rl, r2 will force the type of x to be of the form T!, for some T. The typing rules \nfor heaps are analogous to the rules for declarations. However, we add the rule Heap-discard that allows \nus to discard bindings as part of the translation. Note that when a binding is discarded A is not extended. \nThus the condition As, Ai D re, ri, l?2 in the rule Config will prevent us from discarding live bindings. \nThe rules Stack-c and Stack-R are straightforward. The rule Stack-# handles the case when the stack \nis of the form #x, 3. If the stack 3 requires an expression of type p[q,el then the stack #x, 3 will \nrequire an expression of type pl~+ ~+i], that is an expression that can take allowed to require, the \nextra update marker. tually gets updated, it will be updated with of type p+LE+ll , yielding a binding \nof type we extend A with x : p\\ . The rule Stack-#-discard allows update care of, and is When x even-an \nexpression x : p\\ . Thus markers to be discarded. Note that when a marker is discarded A is not extended. \nThus the condition As, Ai D I o, PI, I 2 in the rule Config will prevent us from discarding update markers \nto which there are references. Finally, the rule Stack-sub allows subtyping to be ap-plied. 6.2 Source \nand target transition The following two propositions express that evaluation pre- serves typings. Propositi_on \n6.1 (Source transition) If k C u-) C : T and C r---) C then there exist 6 such that 6 &#38; I ti and \nI- C ul) cr : 7. Proposition 6.2 (Target transition) If l- C u-) c : T and c ci r!? then there exist \nC such that C +--++ C and l- C + (? : T. The proofs are straightforward, by verifying each transition \nrule. From proposition 6.1 and 6.2 the main result is proven easily: Theorem 6.3 (Soundness of the type \nsystem) If k e H t? : T then 0 (t: ; e ; e) +-+* V iff (e; E; e) 4 ii. l (e ; e ; e) t-b* I3 iff (e; \nZ; e) A* 8. l (e ; e ; E) diverges iff (e ; E ; c) diverges l there is no I@ such that (e ; i? ; t) 4 \nl&#38; . 7 Implementation In this section we will give an overview of the implementa-tion of our type \nsystem. The interested reader is referred to [Gus981 where the implementation is described in some detail. \n7.1 An ordering on annotated terms Given an unannotated term it is the task of the implemen-tation to \nfind a corresponding well-typed annotated term. However, usually several well-typed annotated terms can \nbe obtained from a single unannotated term. In fact, for any unannotated term that is well-typed in an \nordinary type system, a well-typed annotated term can be obtained by annotating every binding with ! \nand every value with [O, w]. Naturally, the implementation should produce an an-notated term which avoids \nas many updates and update marker checks as possible. It is however not always clear which annotated \nterm to choose, as illustrated by the fol-lowing example. ~lo901x.let y =d l[ .ol +l 311 21\u00b0301 in case \nx of nil + let p =! id y inp +lo3 ] p cons z zs + let q =d id y in z +loTol q Here, 1 + 2 is annotated \nwith [O, l] because there will be the number of updates and update marker checks. Some one update marker \non top of the stack if the nil-branch is preliminary results for a few small programs (the most sub-taken \nand none if the cons-branch is taken. This means that stantial ones being a byte code interpreter and \na calculator an update marker check has to be performed when 1 + 2 that parses arithmetic expressions) \nare given below. has been computed. However, if we annotate the binding of q with ! there will always \nbe exactly one update marker and we can annotate 1 + 2 with 11, l] instead. Thus, we can avoid the marker \ncheck at the expense of performing an extra update when the cons-branch is taken. To choose the best \nof these alternatives the relative cost of updates and update marker checks as well as the relative frequency \nof the case-branches has to be taken into consideration. We will defer such choices and define an ordering \non terms which first minimise the number of updates and then minimise the number of marker checks. Now, \nlet iF and L range over vectors of K S and L S re- spectively. For vectors of length n, let Z 5 Z iff \nZ< 5 2: for all 1 5 i < n. Let L 5 Z be defined analogously and let (2, Z) 3 (3, c ) be the lexicographic \nordering on pairs induced by these two orderings. For every annotated term E we associate a pair (it, \nZ) consisting of the annotations on the term (in some order given by the structure of 6). Write -6 for \nthis pair and let Ee + di iff the underlying unanno- tated terms ohtained by stripping annotations of \nEe and 21 are equal and ee7 5 reil. If Eo i\\ El we will say that io is better than El.  7.2 A syntax \ndirected type system The type system presented in section 5 has an important shortcoming; it is not \nsyntax-directed. Thus, an algorithm that annotates terms according to the type system is not readily \nderivable. Hence, as a basis for our implementation we define a syntax directed type system which enjoys \na prin- cipal typing property and for which it is straightforward to derive an algorithm that computes \nprincipal typings. This involves extending the term and the type language with an-notation variables \nand type variables. It also necessitates the introduction of a constraint set, that is typing judge-ments \nwill take the form I ; H t-e u-) d : r where II is a set of constraints on the type variables and annotation \nvari-ables occuring in the judgement. Altogether these changes are substantial but straightforward and \nhas been omitted from this paper for reasons of space. The interested reader is referred to [Gus981 for \na complete description. The syntax directed type system is sound with respect to the type system of section \n5 in the sense that any substi- tution satisfying II yields a valid typing in that type system. It is \ncomplete in the sense that any valid typing in the type system of section 5 can be obtained in this way. \nThus, the implementation can be divided into two dis-tinct parts. One part that computes a principal \ntyping given an unannotated term and another part that finds a solution to the obtained set of constraints. \nThe goal is to find the solution to the set of constraints that yields the best anno-tated term (according \nto the ordering defined above). The existence of such a solution is proved in [Gus981 and an al-gorithm \nthat computes it can be derived easily from the proof. Without opt. With opt. Saved YO Program primes \nUpd6686 Chk 61417 Upd 640 Chk 0 Upd 90 Chk 100 substring 105 505 63 12 40 98 nqueens 732 3761 73 0 90 \n100 quicksort 39 440 0 0 100 100 interpreter 3255 27089 3250 431 0 98 zantema 73 1007 68 0 7 100 Syracuse \n167 1476 84 0 50 100 calculator 22 416 2 0 91 100 The first and second column contains the number of \nup- dates and update marker checks needed in the unoptimised and optimised abstract machine respectively. \nThe third col-umn shows the percentage of updates and update marker checks saved. The number of saved \nupdates varies greatly from pro-gram to program depending on the amount of inherent shar-ing and ranges \nfrom 0% to 100% with an average of 58%. Thus for a fair comparison with other analyses it is crucial \nto compare the results for the same programs. The mea-surements in [SesSl] are performed on very small \nprograms and furthermore he uses functions to encode lists. His re-sults ranges from 0% to 53% with an \naverage of 25%. The measurements in [Mar931 are performed on large real world programs which we can not \nmatch. His results ranges from 0% to 57% with an average of 23%. Although a direct com-parison is not \npossible, our results seem promising. The number of saved update marker checks is constantly over 98% \nwith an average of 99% and seems to be indepen- dent of the nature of the program. The measurements by \n[SesSl] show that the number of update marker checks that can be saved by his analysis ranges from 0% \nto 55% with an average of 25%. 9 Related work 9.1 Avoiding updates We are aware of a number of analyses \nthat can detect un-necessary updates: Two abstract interpretations due to Ses- toft [SesSl] and Marlow \n[Mar931 and three type systems due Launchbury et al [LGH+92], Turner, Wadler and Mossin [TWM95] and Mogensen \n[Meg]. Our type system is closely related to that by Turner et al and the analyses share a number of \nproperties. First, unlike the other analyses, both of them have been proved to be sound. Second, both \nsometimes provide more detailed informa-tion than the analyses by Sestoft, Marlow and Launchbury et al. \nThird, both type systems consider expressions like let z =J 1+2inz+(Xy.3)~ Experiments to be ill-typed \neven though x is only accessed once. In-deed, taking it to be well-typed would render our analysisWe \nhave made a prototype implementation of the analy-unsound since it goes wrong (because it leads to a \ndan- sis and an interpreter for the abstract machine that counts gling pointer which could be dereferenced \nduring garbage collection). However, it makes sense to weaken the criteria for when a binding can be \nannotated with a J (although it implies a run time cost associated with the dangling pointer) and that \nis the approach taken in the other analyses. Mo-gensen takes the type system by Turner et al as his starting \npoint and adapts it to fit this weaker criteria. We believe our analysis could be modified in the same \nway. Further experiments are needed to decide upon this tradeoff. Our analysis differs from the analysis \nby Turner et al in a number of respects. First, our analysis is more precise. The primary reason for \nthis being that we distinguish between the type of a binding and the type of the corresponding expression. \nFor example, lets =d 1+2in lety =! (Xz.L)xin Y+Y is considered to be ill-typed by their type system, \nsince y has the same type as (Xz.z) x and thus the same type as x. Second, our analysis provides information \nthat enables us to optimise the bookkeeping of updates. Indeed, this is our major contribution. Third, \ntheir analysis treats only a very restricted form of recursive let-expressions. Most notably, they cannot \nhandle mutual recursion. Fourth, they argue their analysis sound by means of the call-by-need calculus \nof Ariola et al [AFM+95], and thereby justify a number of rewrite rules to be used for program transformation. \nIndeed, program transformation is a pri- mary goal of their analysis. We, on the other hand, has proved \nour analysis sound by means of an abstract machine. This makes the bookkeeping of updates explicit and \nsuffices for our purposes. However, we believe our analysis could serve as a basis for program transformation \nas well, but we have not investigated it further. Most notably, it is not clear to us how to adapt the \nproof of Turner et al in order to handle mutual recursion. 9.2 Optimising the bookkeeping of updates \nWe are only aware of one analysis that can be used to opti- mise the bookkeeping of updates, namely an \nabstract inter-pretation by Sestoft [SesSl]. His analysis annotates values with (annotations corresponding \nto) [O,O] and [O,w], thus his analysis provides less accurate information than ours. Most notably, the \ncommon case [l, l] degenerates to [O,w]. Indeed, a direct comparison shows that this has a significant \nimpact in practice (see section 8). 9.3 Analyses based on linear logic The type system presented here \nare based on ideas taken from linear logic [Gir87] and there are a number of other type systems based \non similar ideas. This includes type systems that discover the number of times an expres-sion is used \n[WBF93, CC941 and type systems that allow update-in-place of aggregate data structures such as arrays \n[WadSO, Wad91, BS96]. Of those analyses the uniqueness type system by Barendsen and Smetsers [BSSG] used \nin the programming language Clean [PvE] is strikingly similar; just take J to be unique and ! to be non-unique. \nHowever, there is a very important difference: In our type system a function of type 7~ + r will allow \nits parameter to be non-updating and a function of type T + 7 will require its parameter to be updating. \nIn contrast, a function with uniqueness type ~,,,,i~,~~ -+ r will require its parameter to be unique \nand a function of type rn non-uniyue--f 7 will allow its parameter to be non-unique. This also shows \nup in the subtyping relation where we have r! 2 74 in contrast to rlunique 5 ~;l~~-,,,,;~,,~. Thus their \ntype system is unsound with respect to update-avoidance and our type system is unsound with respect to \nupdate-in-place. 10 Conclusions We have presented a type based sharing analysis that can determine when \nupdates and update marker checks can he avoided. We have proved our analysis sound with respect to the \nlazy Krivine machine by proving that evaluation pre-serves typings. As a consequence we get that well \ntyped expressions do not go wrong . The analysis has been im-plemented and the preliminary benchmarks \nindicate that about 58% of the updates and 99% of the update marker checks can be avoided. 11 Future \nwork 11.1 Polymorphism and polyvariance The type system presented in this paper is monomorphic and for \nthe analysis to be used in a real compiler it is crucial that the type system can be extended to handle \npolymorphism. We believe that it is straightforward to extend the syntax directed type system discussed \nin section 7.2 to deal with polymorphism. However, we have not carried it out yet. Related to polymorphism \nis polyvariance, that is to gen- erate several differently annotated terms with different types from \nthe same term. We believe that polyvariance is crucial for the experimental results in section 8 to scale \nup for large programs where common functions, such as append, is used in several different contexts. \nA naive form of polyvariance is readily available; one simply generates one version of each function \nfor every use. This approach can lead to unnec-essary code duplication but maybe worse it is not compat-ible \nwith separate compilation. A more realistic approach would be to generate a few carefully chosen versions \nof ev- ery function. For example, we could generate two versions of append, one that builds lists that \ncan be shared and one that builds non-sharable lists. An interesting topic for fu-ture work would be \nto explore how to choose which versions to generate. 11.2 The implementation of the abstract machine \nThe benchmarks presented in section 8 suggest that checking for update markers is an operation which \nis about ten times as frequent as the pushing and popping of update markers. It is therefore no surprise \nthat implementations of abstract machines tend to be optimised towards fast update marker checks at the \nexpense of the pushing and popping of update markers. However, with our analysis update marker checks \ncan be avoided to such an extent that they become far less frequent than the pushing and popping of update \nmarkers. Hence, with our analysis, the implementation of the abstract machine should instead be optimised \ntowards fast pushing and popping of update markers at the expense of the update marker checks and we \nnote this as an interesting topic to explore. 11.3 A possibility for further analysis It is often the \ncase that one optimisation opens up possibil- ities for further optimisations. Indeed, this is true for \nour analysis. Due to our analysis most update markers are never checked for (since most checks can be \navoided). A marker that is never checked for could be represented by just the pointer of where in the \nheap to update. This kind of marker would be cheap to push and pop. We are currently working on an analysis \nfor this purpose, that is an analysis which can ensure that a marker is never checked for. We believe \nthat the analysis could be very effective and would allow us to represent most markers in this cheap \nway. Indeed this would make the tradeoff discussed in section 11.2 less of an issue since it would only \nconcern the remaining markers that might need to be checked. 11.4 Incorporating the analysis into an \noptimising compiler The initial results given in section 8 seem very promising but indeed the programs \ntried out are very small. It would be interesting to incorporate the analysis into an optimising compiler \nand measure the effectiveness of the analysis in terms of the actual speedup of large real world programs. \nAcknowledgements I would like to thank my supervisors John Hughes and David Sands for their good advice. \nI am also grateful to Johan Agat, Koen Claessen, Andrei Sabelfeld and Makoto Takeyama and an anonymous \nreferee for valuable comments on this paper. References [AFM+95] Z. Ariola, M. Felleisen, J. Maraist, \nM. Odersky, and P. Wadler. A Call-By-Need Lambda Calcu-lus. In Proceedings 1995-Symposium on Princi-ples \nof Programming Languages, San Francisco, California, 1995. (BS96] E. Barer&#38;en and S. Smetsers. Uniqueness \nTyping for Functional Languages with Graph R,ewriting Semantics. Mathematical Structures in Computer \nScience, 6:579-612, 1996. [CC941 S. A. Courtenage and C. D. Clack. Analysing resource use in the X-calculus \nby type inference. In ACM SIGPLAN Workshop on Partial Evalu-ation and Semantics-Based Program Manipula-tion, \n1994. [Gir87] Jean-Yves Girard. Linear logic. Theoreticad Computer Science, 50:1-102, 1987. [Gus981 \nJGrgen Gustavsson. A Type Based Sharing Analysis for Update Avoidance and Optimi-sation. Forthcoming \nlicentiate thesis, Uni-versity of Goteborg and Chalmers University of Technology, 1998. To be available \nvia http://www.cs.chalmers.se/-gustavss. [JHH+93] S.L. Peyton Jones, C.V. Hall, K. Hammond, W.D. Partain, \nand P.L. Wadler. The Glasgow Haskell compiler: a technical overview. In Joint [Lau93] [LGH+92] [Mar931 \n[Mil78] Wgl [PJ92] IPV El [SesSl] [Ses97] [TWM95] [WadSO] [Wad911 [WBF93] Framework for Information Technology \n(JFIT), Technical Conference Digest, 1993. J. Launchbury. A Natural Semantics for Lazy Evaluation. In \nProceedings 1999 Sympo-sium on Principles of Programming Languages, Charleston, N. Carolina, 1993. J. \nLaunchbury, A. Gill, J. Hughes, S. Marlow, S. L. Peyton Jones, and P. Wadler. Avoiding Unnecessary Updates. \nIn J. Launchbury and P. M. Sansom, editors, Functional Programming, Workshops in Computing, Glasgow, \n1992. S. Marlow. Update Avoidance Analysis by Ab- stract Interpretation. In Proc. 1993 Glasgow Worlcshop \non Functional Programming, Work-shops in Computing. Springer-Verlag, 1993. Robin Milner. A Theory of \nType Polymorphism in Programming. Journal of Computer and Sys- tems Sciences, 17:348-375, 1978. T. Mogensen. \nTypes for 0, 1 or many uses. In Proceedings of IFL 97: 9th International Work-shop on Implementation \nof Functional Lan-guages, pages 112-122, St. Andrews, Scotland. Springer-Verlag, LNCS 1467. Simon L. \nPeyton Jones. Implementing lazy func-tional languages on stock hardware: the spine-less tagless g-machine. \nJournal of Functional Programming, 2(2):127-202, July 1992. M.J. Plasmeijer and M.C.J.D. van Eeke-len. \nConcurrent Clean. Available via www.cs.kun.nl/tlean/. P. Sestoft. Analysis and Eficient Implementa-tion \nof Functional Programs. PhD thesis, DIKU, University of Copenhagen, Denmark, October 1991. P. Sestoft. \nDeriving a lazy abstract machine. Journal of Functional Programming, 7(3):231-264, May 1997. D. N. Turner, \nP. Wadler, and C. Mossin. Once upon a type. In ACM Conf. on Functional Pro-gramming Languages and Computer \nArchitec-ture, La Jolla, 1995.  Philip Wadler. Linear types can change the world! In M. Broy and C. \nJones, editors, Pro-gramming Concepts and Methods, Sea of Galilee, Israel, April 1990. North Holland. \nPhilip Wadler. Is there a use for linear logic? In ACM Conference on Partial Evaluation and Semantics-Based \nProgram Manipulation, New Haven, Connecticut, June 1991. D. A. Wrigth and C. A. Baker-Finch. Usage analysis \nwith natural reduction types. In Work-shop on Semantic Analysis, 1993.    Taming Effects with Monadic \nTyping 1 Richard B. 11.ieburtz Oregon Graduate Institute Portland, Oregon, USA dick@cse.ogi.edu Abstract \nThe familiar Hindley-Milner type system of the ML language family is extended with monad annotations \nto account for possible side effects of expression evalu-ation. This also allows effects to be effectively \nencap-sulated by lexical scopesrwith enforcement provided by type checking. A type-and-effects analysis \nsup-ports type inference. Type soundness and complete-ness theorems establish the coherence of monadic \ntype inference with the reference semantics of a small ML-style language. Introduction Modern functional \nlanguages such as Haskellr Stan-dard MLrCAML and Clean have evolved into wide spectrum programming languages \nthrough the addi-tion of imperative features and foreign-language in-terfaces to a purely functional \ncore. This evolution has been necessary to make these languages practical vehicles for the design and \nimplementation of soft-ware systems but it has impaired our ability to rea-son about and formally manipulate \nprograms. When evaluation of an expression may produce a side ef-fectrthe meaning may depend not only \nupon the ex-pression itselF but upon the context in which it is evaluated. ConventionalT structural typing \nof expressions does not account for possible side effects. Some pro-gram transformation techniques such \nas type-directed partial evaluation [DGT96]r depend upon the fact that polymorphically typed expressions \nin the first-order lambda calculus have unique normal forms. 1 The research reported in this paper was \nsupported by the USAF Materiel Command. These techniques cannot be applied to SML pro-gramsr for instanter \nbecause normal forms of poly-morphic SML expressions cannot be inferred from their types when expressions \nmay have side effects. To resolve questions about the scope of possible effectsr Talpin and Jouvelot \nproposed a type-and-effects analysis for functional programming languages [TJ92r Ta19.31. The result \nof this analysisr calcu-lated in conjunction with type reconstructionr ex-presses the possible effects \nof evaluating an expres-sion. Region analysis identifies dynamically allocated variables with syntactically \nscoped regions of a pro- gramrallowing potentially mutable variables to be al- located and deallocated \nin a stack discipline [TT97]. A different approach to effects characterization is taken in Haskellr where \neffects-inducing operations are explicitly associated with monads. Operations on a state variable in \nHaskell can be restricted to a declared scope for an instance of the state monad [LP95]. Furthermorerconfusion \nof distinct state vari-ables can be prevented by an extended type system. Thus the uses of mutable state \nin Haskell can be effec- tively encapsulated. In a contemporary paper in this conference [Wad98]I Philip \nWadler shows that the ef-fects typing of Talpin and Jouvelot can be recast in a framework of monads. \nHe transposes their region analysis into a reconstruction algorithm for monadic types. This paper takes \na different approachrcon-strutting monadic types for effects when regions are associated with lexical \nscopes. Since this approach is newrwe offer proof of its soundness and completenessr relative to a semantics \nfor an imperativerfunctional language. This paper shows that monadic type inferencer based upon effects \nanalysisr can also be used as a general mechanism to enforce encapsulation of effects in a strict functional \nlanguagerwhere monads are im-plicit rather than explicit. The monadic type/effects system solves the \nproblem of incomplete specification of effects and identifies expressions that are purely functional. \nIt requires less ambitious analysis than does region analysis [BTV96]. Region analysis must infer a scope \nin which each reference defined in a pro- gram remains accessibler whereas the monadic type inference \nproposed here merely enforces a policy that requires effects to be restricted to a lexically defined \nscope. It relies upon the block structure of declara-tions to define the scopes of variablesrand thus \nof regionsr whereas region analysis makes no such as-sumption. An interesting application of monadic \ntyping for effects is described by Andrew Tolmach [To198]. The potential effects of evaluating each expression \nin an ML program are inferred in a typed intermediate lan-guagerusing monad (but not region) inference. \nThe estimates of effects given by monadic typing are then used to determine when program transformations \ncan be safely applied to optimize a program during com-pilation. Polymorphic typing in the presence \nof effects is well known to be problematic. A reference or an ex-ception constructor with a polymorphic \ntype might have bound to it (by assignment or by raising an ex-ceptionrrespectively) a value of a more \nspecific type. Since values of different types might be bound in sep-arate occurrences of a reference-typed \nvariable or of an exception constructorra type system that permits polymorphism with effects may be unsound \nunless further restrictions are imposed. One such restriction limits polymorphism to ex-pressions with \nmanifest values [TofBT?T TJ92PWri95P MTM97PWad981. The value restriction assures that evaluating a polymorphic \nexpression cannot entail ef-fects. Monadic typing allows effects-free expressions to be distinguished \nthrough the type system and thus permits relaxation of the value restriction while main-taining soundness \nof typing. This is discussed in sec-tion 5. 2 Lexical scopes for effects Lexical scopes allow logical \nassertions of program properties to be directly associated with textual blocks smaller than a whole program. \nPrograms with effects are particularly difficult to reason about when effects-sensitive expressions are \ndynamically bound in a program. Our goal is to show how a type system ex-tended with effects analysis \nsupports the enforcement of a lexical scoping discipline on effects. The reader should bear in mind that \nthe restriction of effects to lexical scopes is not imposed by any existing program-ming language known \nto the author-it represents a discipline on the use of effects that disciplined pro-grammers might find \nuseful. To illustrate the techniquef we shall apply it to two different effects mechanismsr exceptions \nand state. We consider pure exceptions (they do not carry val-ues of other types) that can be first-class \n(but scope-restricted) values. State is realized with references to typedf mutable storage cellsras in \nSML. 2.1 Mini-ML with Effects To provide a platform on which to explore type-based enforcement of effects \nencapsulation we introduce a small language in the ML family-Mini-ML/Effects. Its syntax consists of \nthree forms: expressionsrstate-ments and matches. E ::= (num) 1 (op) 1 (uar) 1 fun Ident j E 1 exnIdent+E \n( (E,E) 1 EEI(E) 1 let Ident = E in E I raise E I try E with Matches end 1 ref E 1 ! E I Stmt ; E Stmt \n:I= E := E Matches ::= (num) =+ E [ I (num) + E] In each let block (and in the global scope) there is \nimplicitly declared a single exception. Since there is exactly one exception per blockr and the typing \nsystem we shall define will not allow an exception to escape from the block in which it is definedran \nex-ception can be named by the numeral corresponding to the nesting level of the block in which the excep-tion \nis declared. This convention is not meant as a proposal for realistic language designr but rather to \nsimplify the semantic description of Mini-ML/Effects by removing the need to maintain an environment \nthat associates exception names with their scopes. Occurrences of numerals will be distinguished as ei-ther \nexception names or integer constants by their typing. Abstraction with respect to an exception is distinguished \nfrom ordinary value abstraction by the keyword exn. 2.2 A semantics for Mini-ML/Effects Because Mini-ML/Effects \nhas both state and excep-tion&#38; it is awkward to express its meaning via a reduction semantics. A \ncontinuation semantics can account for the interaction of exceptions with stater as well as other effectsrwithout \nencoding the effects mechanisms as data. HoweverPa notation in which lambda calculus expressions are \nused to express con-tinuations is not particularly easy to read. To make the semantic structure more \nevident in the notationrwe employ a suite of semantics combina-tors. Mini-ML is given semantics in a \nmonad of state and exceptionsrthus each combinator expresses either a side-effecting operation in this \nmonadf or the injec-tion of a proper f or non-side-effecting construction into the monad. Typical of \nthe proper combinators is DistLRr which expresses the distribution of the monad across products. Although \nthe construction of a pair of val-ues does not induce a side effectrwhen this operation is extended to \nthe construction of a pair of compu-tationsrthe potential effects of computing each com-ponent must be \npropagated. The combinator DistLR reflects the decision to propagate effects from left to right in evaluating \nthe components of a pan. The opposite sense of effects propagation could have been chosen if that were \nthe intended meaning of the lan-guage. The semantics domains are: threading can be made explicit by n-expanding \nthese Value = Num 1 Lot 1 Exn I Value x Value ) Num -+ Computation Env = Ident-+ Value State = Nat x \nLot + Value C-cant = State+ Result E-cant = Value -+ Env -+ State -+ Result Computation = E-cent -+ C-co&#38; \nlist + Env -+ State -+ Result The domain of control continuation&#38; C-contrmodels the continuations \nthat may be defined by an excep-tion handler. 2.2.1 Semantics functions Three semantic interpretation \nfunctions are used in defining Mini-ML/Effects: &#38; : Expr -+ Computation S : Stmt -+ C-cant --+ C-cant \nlist -+ Env + State + Result  M : Match + E-cant -+ C-cant list -+ C-cent list  These interpretation \nfunctions will be defined in terms of a set of combinators of a monad that char-acterises the semantics \ndomain we have specified. 2.2.2 A monad of semantics interpretation The semantics of expressions in \nMini-ML/Effects are given in a monad whose structure can be thought of as an abstract machinercapable \nof supporting compu-tation with state and exceptions. We call this monad Sem. This monad identifier can \nbe thought of as a type constructor that injects a Mini-ML/Effects typer rrinto a s-sorted semantic type. \nSpecificallyra T-sorted semantic type will have the structure of the domain Computationf but is specialized \nin that the value expected by the E-cant argument belongs to the specific value domain associated with \nthe source language type r. The combinators for the unit and the natural extension in the monad of state-and-exceptions \nare given below. Recall that the types of the unit and natural extension combinators of a monad are: \nUnit : cr + Sem(cr) Ext : (o -+ Sem(P)) + Sem(cu) + Sem(P) Following common practicer the definitions \nof these combinators are given in n-reduced form. State 2Some authors use bmdas an alternative to the \nnatural extension combinator [Wad92]. The difference is inversion of the order of the first two arguments. \ndefinitions. Unitvkcs=kv Ext f m k cs = m (Xv. f v k cs) cs Combinators are also needed to embed the \nmonad in a Cartesian-closed category. The combinators DistLR and S emi-dist define how computations are \ndistributed over products; the combinator App de-fines the computation of applications. DistLR : (Sem(a) \nx Sem(P)) + Sem(a x /3) DistLR(P,Q)kcs=P(Xv~.Q(Xv~.k(v~,v~))~s)~s Semi-dist : (Sem( 1) x Sem(o)) -+ Sem(cr) \nSemi-dist (S, P) k cs = S (P k cs) cs App : (Sem(cu + Sem(P)) x Sem(a)) -+ Sem(P) App = (Ext ap) o DistLR \nwhere ap is the function application combinator of a Cartesian-closed categoryr satisfying the equation \nadf, e) = f e. A combinator Abs provides a semantic definition of the abstraction of an expression with \nrespect to a variable. Abs : (Ident, x Sem(p)) -+ Sem(cu + Sem(p)) Abs (x, P) = Unit(Xv k. P (Xu t. k \nu (e $ [x H v]))) These are proper combinators of the monad Semri.e. these combinators do not specify \neffects semantics. The remaining combinators specify effects.  2.2.3 Stateful effects combinators Store \nis organized as a stack of regionsreach providing a mapping of locations to values. Three state access \ncombinators describe: allocation and initialization of a storage location in the region at the top of \nthe stackr reference to a stored valuerand assignment of a new value into a storage location. In the \nsequelr zurong is the name of an excep-tional continuation to be invoked in case an invalid block level \nindex is encountered. Whenever the wrong continuation is invokedr it is applied to the cur-rent storer \nmodeling a core dump . The variable c ranges over stacks of storage regions. The func-tion new-ref : \n(Location-+ Value)+Location delivers a location distinct from any in the domain of its argu-ment. A pairr(i, \n1) E Natx Locationis the representa-tion of a reference and belongs to the Value domain. In this representationr \nthe Nat component is the in-dex of a storage region (counting from the bottom of the stack) and the Location \ncomponent is an address within the indexed storage region. The combinator Handle applies a computation \nto a normal continuation and a list of exceptional contin- 4110~ : Computation -+ Computation uations. \nAllot P k = P (Xv (s::a). let i = lengtha -1 and 1 = newref s in k (ill) ((s u (1 I+ u})::u)) Deref : \nComputation -+ Computation Deref P k = P (X(i, 1) g. let d = length o -1 in if i > d then wrong u else \nindex 1 k (d -i) d g) where index 1 k n [sd, . , snr . , so] = k (sn[l]) Assign : (Computation x Computation) \n+ Computation Assign (P, Q) c = DistLR(P, Q) (X((i, Z), U) u. let d = lengthu -1 in if i > d then wrong \nu else c ( Update 1 u (d -i) u)) where Updatelvn[sd ,..., sn ,..., so]= [sd, . . I sn @ [I I-b u], , \nSO]  2.2.4 The block entry combinator The Restore combinator pushes a newrempty storage region onto \na stack of regionsrbut restricts the store in the result continuation to the previously existing stack. \nRestore will be used in defining the semantics of local definitions (i.e. let expressions) to provide \na new storage region for references allocated in each declaration block. The notation a&#38;, denotes \nthe re-striction of a stack to its base segment of length nP i.e. CT&#38;= u if lengthu 5 n (s :: u)&#38;~= \nuJ. otherwise Restore also pushes wrong onto the stack of excep-tional continuations. Restore D k cs \nu = D (Xv u . k v u J-lengthc) (wrong::cs) (0:: u)  2.2.5 Control combinators The control combinator \nThrow specifies how an ex-ceptional continuation is accessed. ifn<d Throw n k [cd,. . , car.. . , co] \n= cn wrong otherwise The combinator Insert replaces an indexed instance of a control combinator in a \nlist. Insert (n, c ) [cd,. . . ,&#38;I,..., co] = [Cd, . . , cn+l,c , Cn-1,). . , co] ifn<d Cl otherwise \nHandle : (Computationx (E-cant -+ C-cant list -+ C-cant list)) -+ Computation Handle (P, Q) k cs = P \nk (Q k cs)  2.2.6 Semantics definitions Semantics functions are defined in terms of the com-binators \ngiven in the preceding section. The function fofoldr : (a x b -S b) + b -+ a list -+ b is the list reduction \ncombinator. To express the semantics of matches belowrthe formula uses patterned abstrac-tions to bind \nsyntactic constructs in an argument of foldr. Patterned abstractions abbreviate the op-erations of parsing \na list of matches and extracting subexpressions. &#38;[x] = Xk cs t. k (e[x]) e where x is a variable \nE[( num)j = Unit (numeral-to&#38;t (num)) E[(op)l = Unit ( Unit o (token-to-oper (op))) &#38;[fun x \n+ e] = Abs (x, E[ej) C[exn n j e] = Abs (n, e[e]) f[(el, e2)] = DistLR (@l], E[e2]) Ef-3 -521= App(dh], \nt$dJ) E[let x = e in en = Restore (App (Abs (x, E[e]I), E[e n)) Cl[raise i] = Throw i E[try e matches] \n= Handle(L[e], M[matchesn) E[ref e] = Alloc(E[eJj) E[!e] = Deref (cUeI/) &#38;Us ; e]l = Semi-dist(S[s]l, \n&#38;[el) S[el := e2n = Assign(E[el]l, C[e2JJ) M[matchesj k cs = foldr(X([i + en, cs ). Insert (i, \nE[e]I k cs) cs ) cs matches 2.3 Level-indexed types scope exceptions and references The scopes of let \nexpressions in a program form a tree embedded in the program s abstract syntax tree. At any program pointrthe \nnested scopes containing it can be indexed by natural number&#38;with index zero designating the outermostror \nglobal scope. We as-sociate with each exception and reference definition point the (largest) index of \na nested scope that con-tains it. So that the lexical scopes of let expressions will agree with the intended \nsemantic scoping of effect&#38; assume that in an expression let x = e in er the expression e has the \nsame scope as does e. This scoping convention makes recursive definitions possi-bleralthough we are not \nusing it for this purpose and have not provided semantics for recursive definition. Exceptions and references \nin Mini-ML/Effects are first class values and could be passed out of the scope of their definition points \nif no restriction prevented it. To account for the lexical scope of a reference definitionr the argument \nof the Ref constructor for type expressions consists of a lexical nesting index paired with the type \nof a stored value. At the definition point of a ref expressionr the lexical index is bound. For exampleran \noccurrence of the expression ref 0 at block level j will be typed as Ref(j,int). Similarlyrexceptions \nare designated by the index of a block in which the exception is (im-plicitly) declared. For simplicityr \nMini-ML/Effects defines exactly one exception per let block. An exception constant is simply a numeralr \ntyped as n : Exn(n). The following definition captures the intuition that underlies lexicallv scooed \ncontainment: wher- 1 ever a reference defined in block level i is admissibler II so also is a reference \nto a value of the same type de-fined at a block level irwhere i 5 j. To compare Ref typesf we need an \nordering on the types that depend on a lexical index. The intuition underlying lexically scoped containment \nis that a ref- erence to a type T value defined in a block at level j is in scope in any block that is \nnested within this block. The partial ordering of nested blocks motivates the following definition: \nDefinition 1: least upper bound of Ref and Exn types. l.u.b.(Ref (7, i), Rej(r , i )) = Ref(mgu(7, T \n), max(i, i l.u.b.(Exn(i), Exn(i )) = Exn(max(i, i )) where mgu(r, 7 ) is the most general unifier of \nT and r rif it exists. 0 2.3.1 Level based type constraint Because the types of references and exceptions \nde-pend upon the block level of their declarationrit is possible to use type checking to constrain references \nand exceptions to the scopes of their definitions. This constraint is imposed on the types of let expressions. \nThe type system will allow no occurrence of a Refer Exn type for which the block index given in the type \nexceeds the block level of the context. Not only might a reference or an exception value leak from the \nscope of its definition by explicit occur-rencerit might also be made accessible outside this scope if \nit were embedded in the body of an exported function. For exampler the following code sequence defines \na pair of functions that export read and write access to a nested reference: letx=refOin ((fun y *!x), \n(fun y * x := y; y))  To prevent the leakage of access methods in this wayr we rely upon level-stratified \nmonad annotations in an extended type system to prohibit the export of such functions from the block \nin which the reference has been defined. 3 Monadic types for lexical scopes Mini-ML/Effects has an underlying \nHindley-Milner type system that relates expressions to the domains in which their values reside. Howeverrexpressions \nmay also induce effects which are not accounted for by or-dinary types. In the semantics of Mini-ML/Effectsr \nside effects of evaluation are represented by interpret- ing each expression in a monad. 3.1 Monadic \ntype constructors The monads in which the effects of Mini-ML/Effects expressions may be interpreted are: \nId -the identity monad; no effects are supportedr Sr -the state reader monad; only reading of state is \nsupportedr St -the state monad; supports both reading and mutation of stater Ex -the exception monad; \nexceptions may be raisedr ExSr -a composite of exceptions and the state readerr ExSt -a composite of \nexceptions and state. These symbols are used as type constructors in the Mini-ML/Effects type system. \nFor instancerif T is a typerthen Ex(r) designates the computations of type T in the monad of exceptions. \nThe effects monads form a latticerindicated by the diagram of Figure 1. The partial order which induces \nthe lattice is inclusion of effects. That isrAIl L It42 if all side-effecting operations defined in the \nmonad Mr are also defined in Mz. The 1.u.b. of a pair of monadic types is formed by taking 1.u.b. of \nthe two monadic type constructors in the latticef applied to the most general unifier of the ordinary \ntypes. Note that there is no general procedure for constructng a composite monad from two given monads. \nComposite monads are defineable in the cases we have considered herer as in many other instances of computational \nmonads [JD94]. ExSt ExSr Ex Sr \\/ Id Figure l-The lattice of effects monads  3.2 Level-stratified monad \nannotations Since effects actions are associated with lexical block indices memoized at the definition \nof a reference ex-pression or exceptionra monad annotation can be as-sociated with each block. The monad \nis determined by possible effects associated with the references and exception declared in the block. \nAn expression that accesses references or exceptions defined in several blocks is interpreted in a composite \nmonad composed from block-indexed components. Notation. 4 level-indexed monad annotationis a list of \nmonad annotations indexed from 0 through some i corresponding to nested declaration contexts. For exampler \nEzo Sri Id2 Sts is a level 3 indexed monad annotation list. If A4 is a variable ranging over level-indexed \nmonad annotationsrlet Ml, stand for the restriction of M to levels 0 through i. Indexing by a subscript1 \nas M,rdenotes the ith component of M. For instanter Ml; Mt+l = M&#38;+1. The monad denoted by a level-indexed \nmonad annotation list M Jt is given by the 1.u.b. of the individually indexed monad annotations in the \nlistr u;=, MJ. For examplerthe expression if !r > 0 then x := +(!x, !r); !x else raise 0 fi where r is \nbound to a ref expression in block lrx is bound in block 3 and the exception is defined in block Orwould \nbe interpreted in the composite monad ExStT but would be annotated as EXO Sri Id2 Sts. The 1.u.b. of \ntwo level-indexed monad annotation lists is calculated by taking the 1.u.b.s of the elements of the two \nlistsron a pointwise basis. By an abuse of notationrwe extend this convention to the 1.u.b. of an indexed \nannotation list with a scalar monad anno-tation. ThusrExoSrlIdzSts u St1 = ExoStlZdzSta. A monad annotation \nexpression is applied to a typeh-rto form a monadic type expression. The infix 1.u.b. operator (u) binds \nmore tightly than does ap-plication. The meaning of such an expression is the type gotten by applying \nthe monad denoted by the annotation list to r. Howeverrit will be convenient to represent a composite \nmonadrwhen used as a type constructorrby a level-indexed annotation listras it supports description of \neffects constraint by scoping. cl  3.3 Typing as an abstract semantics The semantics combinators introduced \nin Section 2.2 can be given an abstract interpretation as typing com-binators. The typing interpretations \nof the semantic combinators (annoted by a prime) are functions in a domain of monadic type expressions. \nSince monad annotations depend upon a lexical index valuera typing interpretation can only be given in \nthe context of a lexical scope. The relation of a typing interpretation to a level index is indicated \nby the symbol (b). Its meaning is simply that the in-dexed monad annotation list used as a type construc-tor \nis restricted to the number of levels specified by the number to the left of the relation symbol. i /= \nM(r) -M = Ml; We shall overload this relation symbol in two ways. We write i b Id(r) as an abbreviation \nfor the level-indexed monad annotation expression i k Ido.. . Id,(T). We also use the relation to qualify \nan equality between type-valued expressions. When we write i b E = M(r)I we mean to assert both i + M(T) \nand E = M(T). 3.3.1 Typing interpretations of semantic combinators The typing interpretation of semantic \ncombinators maps type expressions to conform to the signature of the combinator. The least monadic typing \nthat can be associated with the combinator is chosen. As Unit is a constant of every monadrits least \nmonadic typing is in the identity monad. In the definitions belowr 7 ranges over types and M ranges over \nthe level-indexed monad annotation lists described pre-viously. i + Unit T = Id(T) i + Ext (~1 -+ M(Tz)) \n= M(TI) -+ M(T~) 3.3.2 Monad distribution over products and arrows Monads in a Cartesian category come \nequipped with a distribution combinator that specifies how the monad distributes over products.   \n\t\t\t", "proc_id": "289423", "abstract": "Sharing of evaluation is crucial for the efficiency of lazy functional languages, but unfortunately the machinery to implement it carries an inherent overhead. In abstract machines this overhead shows up as the cost of performing updates, many of them actually unnecessary, and also in the cost of the associated bookkeeping, that is keeping track of when and where to update. In spineless abstract machines, such as the STG-machine and the TIM, this bookkeeping consists of pushing, checking for and popping update markers. Checking for update markers is a very frequent operation and indeed the implementation of the STG-machine has been optimised for fast update marker checks at the expense of making the pushing and popping of update markers more costly.In this paper we present a type based sharing analysis that can determine when updates can be safely omitted and marker checks bypassed. The type system is proved sound with respect to the lazy Krivine machine. We have implemented the analysis and the preliminary benchmarks seem very promising. Most notably, virtually all update marker checks can be avoided. This may make the tradeoffs of current implementations obsolete and calls for new abstract machine designs.", "authors": [{"name": "J&#246;rgen Gustavsson", "author_profile_id": "81100078654", "affiliation": "Chalmers University of Technology and G&#246;teborg University", "person_id": "PP31080522", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/289423.289427", "year": "1998", "article_id": "289427", "conference": "ICFP", "title": "A type based sharing analysis for update avoidance and optimisation", "url": "http://dl.acm.org/citation.cfm?id=289427"}