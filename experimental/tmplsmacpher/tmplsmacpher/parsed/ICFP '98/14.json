{"article_publication_date": "09-29-1998", "fulltext": "\n The Spineless Tagless G-machine, naturally Jon Mountjoy Department of Computer Science University of \nAmsterdam Kruislaan 403, 1098 SJ Amsterdam The Netherlands email: jon@wins.uva.nl Abstract The application \nof natural semantic specifications of lazy evaluation to areas such as usage analysis, formal profiling \nand abstract machine construction has shown it to be a use- ful formalism. This paper introduces several \nvariants and extensions of this specification. The first variant is derived from observations of the \nSpineless Tagless G-machine (STG), used in the Glasgow HaskeIl compiler. We present a modified natural \nsemantic specification which can be formally manipulated to derive an STG-like machine. The second variant \nis the development of a natural se-mantic specification which allows functions to be applied to more \nthan one argument at once. The STG and TIM ab-stract machines both allow this kind of behaviour, and \nwe illustrate a use of this semantics by again modifying this semantics following observations of the \nSTG machine. The resulting semantics can be used to formally derive the STG machine. This effectively \nproves the STG machine correct with respect to Launchbury s semantics. En route, we also show that update \nmarkers in the STG machine are necessary for termination, and show how well- known abstract machine instructions, \nsuch as the squeeze operation, appear quite naturally as optimisations of the derived abstract machine. \nIntroduction The STG machine has proved itself as being capable of ef- ficiently executing lazy functional \nlanguages. This machine lies at the heart of the Glasgow Haskell Compiler (Peyton Jones 1996), which \ncompiles by translating a program writ-ten in Haskell through intermediate, simpler, languages until \nit finally generates programs in the STG language. This fi-nal language is then compiled into code which \nmimics the operational semantics given to the language in the form of an abstract machine. However, it \nis yet to be shown that the STG abstract machine is correct. Intuitively the ma-chine does what we expect \nit to do, being a refined model of a graph reducer which, operationally, seems sound. The first natural \nsemantic specification of lazy evaluation for lazy functional languages was presented in Launchbury (1993), \nwhich was proved correct with respect to a call-by- name denotational semantics of the same language. \nWe re- gard this as the specification of what is meant by a lazy functional language. Sestoft (1997) \nhas taken the natural semantic specification and derived from it various abstract machines, proving that \nthese abstract machines are correct with respect to each other and the natural semantic speci-fication. \nIt is not quite clear, however, how these abstract machines can be related to the STG machine. This paper \nintroduces several variants of natural seman-tics specifications. The first variant is derived from obser-vations \nof the STG, which seems to treat variables pointing to lambda abstractions as values, as opposed to just \nlambda abstractions. A modified natural semantic specification is suggested which does just this, and \nwe show how it can be formally manipulated to derive an STG-like abstract ma-chine. The STG machine, \nhowever, just like the TIM abstract machine, is capable of handling multiple arguments in its applications \nand abstractions. Unfortunately, Launchbury s semantics handles only single arguments. Our second varia- \ntion extends Launchbury s semantics to multiple arguments. The STG machiue can then be formally derived \nfrom this semantics, yielding a proof that the STG machine is correct. En route, we also show that update \nmarkers in the STG machine are necessary for termination, and show how well- known abstract machine instructions, \nsuch as the squeeze operation, appear quite naturally as optimisations of the derived abstract machine. \nThe structure of this paper is as follows: l The following section recalls the natural semantics of lazy \nevaluation which we will be using as a reference semantics a Section 3 makes observations about the STG \nwhich suggest changes to the natural semantic specification. A new semantics is then developed and shown \ncorrect with respect to the reference semantics. l Section 4 illustrates how an STG-like abstract machine \ncan be derived from the semantics. l Section 5 extends the reference semantics to handle multiple arguments \nduring abstraction and application. l Section 6 parallels sections 3 and 4 and derives a seman- tics \nfor the STG machine based on multiple arguments, and subsequently the STG machine. l? : Xz.e .I) l? \n: Xx.e Lam r : e Jj A : Xy.e A : e [z/y] JJ 0 : w r:ex Jj 0:~ APP r:e 4 A:w r[zr-t e]: x JJ A[$ e W] \n:t2 Var !?[xi e e;] : e Jj A : w I : let {xi = e;}L1 in e .&#38; A : 1 Let r:cxl... Xj JJ r : c xl.. \n.zJ Cons r : e u A : Ck Xl . ..Xak A : ek[xl/ykl,. . . ,zC k/ykok] 4 @ : W r : case e of {Cj Y;: + ej}T=l \nJJ 0 : 21) Case Figure 1: Natural Semantic specification of Lazy Evaluation Unfortunately the STG machine \nis quite large and com-plex, and to describe it here would require too much space. We thus assume some \nknowledge of the STG machine, as presented in (Peyton Jones 1992). Proofs of all of the propo- sitions \nwill be made available in a technical report. 2 The Natural Semantics of Lazy Evaluation We begin by \n(briefly) reviewing the language and semantics based on Launchbury (1993). The abstract syntax of the \nlanguage, which we christen the basic language, is given by: expressions: e ::= Xx.e 1 let {x; = e;}r=l \nin e j Z CXl...Xj ) case e Of{Cj y; 4 ej}y=, This syntax guarantees that expressions are in a so-called \nnormalised form. The process of normalisation ensures that: l application of a function can only be made \nto a single variable l all constructors (c) have only variables as arguments l all constructors are saturated \n(fully applied) l all bound variables are distinct The first three properties are ensured by the syntax. \nThe last is implemented by an cY-conversion which renames all bound variables using completely fresh \nvariables, which we write as 6. Launchbury (1993) provides a simple scheme for normalising arbitrary \nexpressions into the basic language. Intuitively, the restriction of application and constructor ar-guments \nto variables makes the sharing of these arguments explicit. Note that Launchbury only allows application \nand abstraction of a single argument. A natural semantic speci-fication of this language is shown in \nFigure 1. In the above, a heap, I = [. . . ,x C) e,. . .] represents a mapping from variables x to expressions \ne. We write I [x; I-+ ei] for I [xl ti el,. . . ,xn c-) e,], dom r for the domain of l?, and mg r for \nthe range (the set of expres- sions bound in the heap) of I?. The list of alternatives of a case statement \nis written as alts(below) or {cj y: + ej}y=l, where y; represents the vector yj, . . . l/j,, of variables, \na3 being the arity of the constructor cj. A configuration r : e is an expression and a heap in which \nthe expression is to be evaluated. A judgement l? : e J,l A : w says that expression e in heap r evaluates \nto a value w and heap A. In Figure 1 we see that rules Lam and Cons say that the value of the lambda \nabstraction or constructor is itself -in-deed, these are what lazy functional languages consider to be \nvalues! Rule Let evaluates a let expression by evaluating the quali- fied expression e in a heap in which \nall of the bindings are present. Note that this is the only rule which adds bindings to the heap, and \nthat the bindings themselves are not eval-uated. Rule App says that to evaluate an application e I, we \nneed evaluate e to a lambda abstraction Xy.e , and evaluate the e with z substituted for y in the resulting \nheap to produce a final value and heap. An integral part of lazy evaluation is that when something is \nevaluated, the result is rebound in the heap so that fur-ther requests for the value yield the already \nevaluated value. Rule Var captures precisely this. This rule says that to eval- uate a variable bound \nto an expression e in the heap, we ca n evaluate this expression in a heap without this binding to produce \na value. This value can then be rebound to the variable in the final heap -thus ensuring that further \nde-mands of the variable will yield the new value. This is the only place where a heap update occurs. \nThe STG language of section 3 provides support for sometimes avoiding this expensive operarion, if it \nwill lead to no change in the final value produced. Since we duplicate value w, this may cause name clashes \n(recall that otherwise all bound variables are distinct) and we have to rename all of the bound variables \nof w to fresh variables, indicated by &#38;. See Launchbury r[z 8-b Xy.e] : 2 J. r[x t+ Xy.e] : x Lams \nr : f J, O[W t-b Xy.e] : v O[u I-+ Xy.e] : e[x/y] 1 A[u. +-+ w] : u r : f x 4 A[u I+ w] : 21 APPS I :e \n1 A[uI+w]:u w a lambda abstraction, e not I [xee]:x J- A[u~+w,r~~]:x Vars e not a lambda abstraction \n I [, X ++ e] : xrieAi X ~ ~~~. ~] . . 3 :Cyl...yj VUTCS r[xi e ei] : e J- A[u H w] : u r : let {xi = \ne<}r=r in e .j. A[ti ++ w] : u Lets rrcxl... Xj 1 r : C x1 . Xj Conss Cases Figure 2: Natural Semantic \nspecification of Lazy Evaluation for the X-normalised language (1993) and Sestoft (1997) for more on \nthis operation and its 3 The STG Machine relation to name clashes. Rule Case evaluates an expression \nto aconstructor, and then The syntax of the STG machine base language, as presented selects the appropriate \nexpression from the list of alterna- in Peyton Jones (1992), is quite different from that of the tives, \nsubstituting the arguments of the constructor for the basic language considered in the previous section. \nThese formal variables in the alternative. syntactic differences contribute significantly to the differ- \nLaunchbury proves that the natural semantic specifica-ences in the derived abstract machines. This section \nhigh-tion is sound and complete with respect to a denotational lights the key difference, that of the \nrestricted locations of semantics of the language: lambda abstractions, and introduces a new natural \nsemantic specification embodying this restriction. Proposition 1 (Denotational Correctness for JJ) To \nfocus on the key difference, and in keeping with the Zfr:e JJ A: z then for all environments p we have: \nprevious section, we use a restricted STG language where application and abstraction are only allowed \non one argu-Uebr = U4P ment. We call this language the STGS language, and its Sestoft (1997) has shown \nhow an abstract machines can be syntax is: formally derived from this natural semantics specification. \nHe thus derives an abstract machine A which simulates .l,l in the sense that the abstraction machine \nwill yield a value expressions: ::= let {Xi = Ifi}?= in e for an expression iff the natural semantics \nproves that the e I Y value of the expression e is w. Writing 4 for a step in the abstract machine A, \nand %* for its transitive closure, we / iiE2 of alts have: lambda-forms: lf ::= varsf \\x x,.e variables: \nwars ::= {Xl,. . . ) xn} (n 2 0) Proposition 2 (Correctness of Sestoft s Machine) alternatives: alts \n::= {Cj fj --t ej}yd Zfw isavalzlethene 4 1s $e+*ur. If we can use these techniques to formally derive \nthe STG machine, then we will have provided a proof of correctness We have introduced two new syntactic \ncategories, one for of the STG machine, and related it to Sestoft s machine. convenience (vars, representing \na possibly empty sequenceUnfortunately, it is not quite clear how this should be done. of variables), \nand one which is peculiar to the STG machine,The following two sections show a way -we first highlight \na lambda-form (If). A lambda-form replaces the notion ofcharacteristic features of the STG machine and \nmodify our a lambda abstraction, and as can be seen from the syntax, semantics to incorporate them, and \nwe then extend the en-restricts the position of lambda abstractions. We write {} iftire semantic framework \nto handle multiple arguments. As a x0 is empty. In a lambda-form, if xa is non-empty, then theresult, \nwe produce a semantics from which the STG machine lambda-form represents a lambda abstraction, xII being \nthe can be derived. variable which is abstracted. varsf represents the free vari- ables of the abstraction. \nEither za or warsf (or both) may be absent. These free variables play no role in the denota-tional semantics \nof the language, and are only of operational significance. In addition, a lambda-form has an update flag, \nrr, which plays a role when we consider the language oper-ationally. The flag may be set to either u \nindicating that the bound expression will be updatable, or n indicating that it will not be updatable. \nWe will discover that this is not just an optimisation but also necessary for termination, as is discussed \nin section 4.1. We may also have a lambda-form which has no argument and only free variables. This corresponds \nto an ordinary let bound expression (albeit that it cannot be a lambda ab-straction) of the basic language. \nAgain, we assume that constructors are saturated. The most notable differences enforced by the syntax \nof this language, compared to the basic language, are that: l The function body of an application must \nitself must be a simple variable, and not an arbitrary expression. l Lambda expressions cannot occur \nin arbitrary places. Instead, lambda expression can only occur let-bound. Consequences of the restricted \nsyntax The basic language allows arbitrary expressions in the body of an application: e ::= e x 1 . whereas \nthe STGS language only allows applications of variables: e::=fyI . . . where f and y are variables. A \nfunction body other than a variable is not allowed, and so has to be heap allocated by a let binding. \nThis restriction goes hand in hand with the allowed location of lambda abstractions. Since lambda ab-stractions \ncan only be let bound, and let bindings are heap allocated, this implies that all of the function bodies \nwill be heap allocated. Recall that lambda abstractions are also values -the previous observation implies \nthat if a variable is to be evaluated to a lambda abstraction (signifying a value), then it will instead \nbe evaluated to a variable pointing to the lambda abstraction. That is, we detect a value via a vari- \nable . An important intuition is that if an expression is to be evaluated to a lambda abstraction, then \nit can be evalu- ated to a variable pointing to the lambda abstraction sa nce all the lambda abstractions \nhave to be let-bound and thus will appear in the heap. This hints at the graph reduction ancestry of \nthe STG machine. We postulate then: The STG machine considers a variable bound to a lambda abstraction \nas a value, as opposed to the notion of a lambda abstraction being a value. If this concept is incorporated \ninto the natural se- mantics, an STG machine can be derived. The rest of this section shows that this \nis indeed true. The above notion detects a value sooner, in the sense that we do not wait until a value \nis rebound in the heap and start executing the value (as in rule Var), but rather stop when we see that \nwe are pointing to it. Note however, that the STG machine does not employ the same concept in its handling \nof constructors (see (Peyton Jones 1992) for details), and manipulates constructors just as in our basic \nlanguage semantics. Thus, we propose a new semantics which instead of eval- uating an expression like \nthis: Lam I- : xq.q -lj r : xq.q qx k+ Xq.q] : 2 .I) qx +b Xq.q] : x2.2 VaLet r : let 2 = Xq.q in x 4 \nr[x t-k Xq.q] : X2.2 evaluates it like this: qx ++ A~.~] : 2 4 r(32 k-k xq.q] : z Lams r : let 5 = Xq.q \nin x .&#38; r[x I+ Xq.q] : r Lets where we use the notation 4 to denote the new relation. That is, the \nnew derivation does not use a Var rule be-cause of the early detection of the lambda abstraction which \nwe have moved to the new Lam rule. We thus propose that we collapse the rules Var and Lam for the case \nwhen a vari- able is pointing to a lambda abstraction, giving us the be-haviour illustrated above. Since \nwe handle the two types of values(constructors and abstractions) differently, we propose the introduction \nof two Var rules -one for each. Figure 2 suggests a new natural semantics for the extended language based \non these observations. The rules follow those in Fig- ure 1 closely, except that in many places we halt \nwhen we have a variable bound to a value. The two ways of handling values is reflected in rules Vars \nand Vu&#38;s. Aside: We should duplicate rules Apps, Lets and Cases, so that they too may return constructors \nas results. As it stands, they can only return pointers to abstractions. This duplication just complicates \nour presentation, and presents no technical da,j%ulties(see the technical report). It also have no impact \non the derived abstract machine. The semantics presented in Figure 2 needs expressions to be in the form \ndescribed by the syntax of the extended language. We call a language which has these restrictions X-normalised. \nWe write the X-normalisation of an expression e as e*, and it is defined as follows: = x = let y = Xx.e \nin y (let {r; = ei}TLE1in e)* = let {xi = r];}T==, in e T)i = Xy.e:* if e; E Xy.e!, where li = ef otherwise \n . ex e a variable (e xl* = let y = e* in y x otherwise (c Xl.. .x,) = CXl...Xj (case e Of{Cj y; -+ ej}jn,l)* \n= case e of {Cj y< + e;}j =r Note that all introduced variables must be fresh. Values Using the natural \nsemantics, we can now state that the meaning of a closed expression e is given by either: {} : e ./, \nA[u ++ Xy.e ] : u or {} :e 4 A:cPr...Pj In the first case we can consider the lambda abstraction as \n the actual result, and in the second the constructor. Indeed, the semantics in Figure 1 will yield one \nof these (as shown below). Although the new semantics is less elegant than the old, it does hint of a \nmore operational nature and some conscious design decisions have been made: s The old semantics cannot \nearly-detect a lambda ex-pressions as a value, as illustrated in the examples above; but this is just \na tradeoff: on the one hand the new semantics does not need a lambda instruction, as we will never hit \na lambda in the control; on the other, we have introduced an extra check on the value in the heap (is \nit a lambda abstraction or not). s The old semantics needs less heap allocation as normal- ising invariably \nleads to a greater number of let bind-ings and thus heap allocations. However, the normalis- ing allows \nfor the early detection of values. A machine capable of application and abstraction of multiple ar-guments \nwould decrease the heap allocation, and moti- vates extending our semantics as presented in section 5. \nThe new semantics increases the number of times we have to check the type of a closure in the heap. For \nin- stance, rule Vars needs to check that e is not a lambda abstraction. In the rest of the paper we \nwill assume that the result of evaluating the root expression is a lambda abstraction and not a constructor. \nAll of the propositions below hold in both cases. Correctness We can prove that X-normalisation does \nnot change the meaning of an expression. That is: Proposition 3 (Correctness of A-normalisation) {} :e \n4 A:wi#{} :e* .lj O:w Finally, we can state the correctness lemma. The new semantics is correct in the \nsense that if the original semantics evaluated an expression to a value, then the new one will evaluate \nthe expression to a variable bound to the same value. For X-normalised terms then, we prove: Proposition \n4 (Correctness of 4) r:m Jj A:w ifl l?:m J. A [u~w]:u where A [u++w]~A (Note that u may point to an \no-renaming of w) We write A g A to indicate that both heaps map the same range to o-equivalent expressions. \nGiven a closed expression e, proposition 3 tells us that the value of e is unchanged by normalisation. \nProposition 4 tells us that for normalised expressions, the value of an expression is the same under \nboth reduction systems (beyond the fact that we result in a pointer to the value instead of the value \nitself). We have thus shown: X - normalisation e c e* + c 20 4 WW Since propositions 3 and 4 provides \na soundness and com-pleteness proof for normalised terms, we have indirectly a proof of correctness with \nrespect to the denotational seman-tics as well, using proposition 1. As indicated by proposition 2, Sestoft \nderives an abstract machine A which simulates 4. The following section (infor-mally) derives an abstract \nmachine N which can be proved correct in a similar manner with respect to our natural se- mantics: That \nis e 1 w iff es* w. All together, we then have that if the meaning of a closed expression e is w under \n4, then the abstract machine in the following section will derive this value (actually a pointer to it). \nThat is, our basic STGS machine is correct. 4 Deriving a STG-like machine A machine can be formally derived \nfrom a natural semantic specification by flattening the specification: operationally the natural semantics \nbuilds a derivation tree relating an expression to its value from the bottom up, whereas an ab-stract \nmachine computes the value by building a state se-quence. The technique used to make an abstract machine \nis to represent the context of subtrees by a stack. Consider the App rule: I : e Jj A : Xy.e A : e [z/y] \n.ll 0 : w l?:ex Jj 0:~ This says that to prove that e z evaluates to w, we must first find a proof that \ne evaluates to a lambda abstraction. Then, we must evaluate the expression found by substituting the \nargument in the abstraction. This final value is the value for the entire application. Flattening this \nspecification will yield two abstract ma-chine instructions. The first will start the computation of \ne - and push the argument on the stack to remember to start the computation of the substitution when \na lambda abstrac-tion occurs. The second will perform the substitution if a lambda abstraction is found \nwith an argument on the stack. We now, rather informally, use this technique to derive an abstract machine \nfrom the semantic specification in Fig- ure 2. A formal proof of correctness using this technique can \nbe found by following (Sestoft 1997) or (Sansom 1994). The first step to deriving such a machine, however, \nis the incorporation of a slight change to the semantics. In the current semantics, renaming takes place \nin the variable rule. A much more natural place for this to take place is in the Lets rule, allowing \nthe renaming to be mimicked by the al- location of fresh heap locations. This development, and the corresponding \nproof that the semantics is correct with re-spect to renaming (that is, that no name clashes occur), \ncan be found in the technical report. We begin Uy 3attening the natural semantics, as described above. \nA stack is introduced to represent the flattened tree structure, and we write l? e S to represent the \nstate of the Heap Control ,%vironinent Stack Rule let 5; = e; b1 in e E S let ===+ F(pi * (e;,E )] e \nE S I$ I-+ (e , E )] 2 E(z I-+ p; S var1 *r e E #p:S rfp ti (Xy.e , E )] ===S l [p, * (Xy.e ,E )] \nz 2 E[ * PI -e c+ PI s#,,:s vu13 l-==+r fX f E[zqb~ E[ +k psj s p, : s am I$, c-) (Xy.e, E )] x E[x e \npz:] ;:s am 3 1 e WY -PI 1 case e of alts E S case 1 =+r e E (ah, E) : S I ck xl.. . xak E (alts, \nE) : S case2 ==+r ek E[yki -I';] s Ck X1.. . Xak E #PCS =+ $I l-+ (Ck 2, E )] Ck i? E S In the let rule, \nthe variables pi that replace x; must be distinct and fresh and not environment E = E[x; ++ pi]. In the \ncase2 rule, ek is the right hand side of the k th fOI i=l,...,ak. See section 4.1 for a corrected varr \nrule. var3 occur in l? or S. The new alternative, and p; = E [z;] Figure 3: The New Abstract Machine \n(h/) derived from the new natural semantics. abstract machine in which we are evaluating expression e \nin heap I? with stack S. The rules Lams and Conss give rise to no abstract ma-chine instructions. Intuitively, \nno machine operations are needed to leave the heap and expression unchanged. The Lets rule gives rise \nto one abstract machine instruc-tion which binds the values in the heap and executes the expression e: \nr (let {z; = e;}a, in e) S * I?[pi I-+ ei] (e) S The Vars rule gives rise to two instructions, one to \ninitiate the evaluation of e (and push a marker x onto the stack) and the other to rebind the resulting \nvalue (if we find a marker on the stack): r[x * e] (x) S * r (e) (#z : S) r[x k-b Xy.e] (x) (#z : S) \n==a r[ t-b Xy.e] (2) S The VarCs rule also gives rise to two instructions. The first, to initiate the \nevaluation of e, is identical to that generated by the Vars rule. The second tells us to perform an update \nif we have a marker on the stack and constructor in the control: r (cxl... Xj) (#z : S) ==+ r[2 ct c \nxl.. . xj] (c xl.. .xj) S The Apps rule gives rise to two instructions. The first should evaluate the \non the stack), tion (when it on the stack). body of the function (and place the argument and the second \nshould perform the substitu- finds a lambda abstraction and an argument r (f x) s =+ r f (X : s) r[v \ne Xy.e] (7,) (x : S) --r. r (e[x/y]) S The Cases gives rise to two instructions. The first evaluates \nthe expression to a constructor, and the second chooses the correct alternative: r (case e of {cl Y; \n-+ eJ}) S ==+ r e ({Cj Y; + ej} : S) r (ck XI.. .G~) ({Cj 6 --f ej} : s) * r ek[zk/Yk] s The above machine \ncan be further refined by introducing an environme:?t to model the substitutions taking place. An environment, \nE, maps variables to heap locations, and can be thought of as a delayed substitution which is only applied \nwhen we meet a variable in the control. We thus replace substitutions eb,ly] by a pair (e, [y I-+ p]) \nrepresenting the fact that within e, y actually points to p. Sestoft (1997) discusses this transformation \nin more detail. We write E[x] for the value of 2 under E. The resulting machine appears in Figure 3. \nEven though we are using a restricted STG language, this machine looks much less like the machine described \nby Ses- toft, and very much more like the STG machine. The three rules for the variables (var1,uar2 and \nappg) correspond to the famous SI G Enter instruction, which enters a closure. Rules let, appI and casei \ncorrespond to the ,STG Ewal in-struction and uar2 to the ReturnCon rule. The machine described by S&#38;oft \noperated on our basic language, which has unrestricted lambda abstractions. As a consequence, his machine \nhas iustructions which fire if a lambda abstraction occurs in the iustruction stream which make it appear \nquite different to ours. We have of course, just shown that they essentially do the same thing, though \nsomewhat differently. 4.1 Tbe iYq$ected Side-Conditions Recall thaT tne STG language in section 3 annotates \nlambda-forms with ei:her u or n, indicating whether they are updat- able or not. i?rytoli Jones (1992) \nstates: It is clearly safe to 168 set the update flag of every lambda-form to u, thereby up-dating every \nclosure , and goes on further to explain that an obvious optimisation would be to set the flag to n for \nmani- fest functions (which we read here as lambda abstractions), as they are already in head normal \nform. As support for this hypothesis, the STG machine itself has no rule for an updat- able lambda abstraction. \nThis is just as well: if it wasn t for this reasoning and action the STG machine would not work. This \noptimisation is a prerequisite for correct termination! We argue this point below. The machine illustrated \nin figure 3 does not work, as we have forgotten to implement the side conditions of the nat-ural semantics \nrules V ars and VarCs. These state that action should only occur if the variable is bound to a non- lambda \nterm. The abstract machine instruction varl above does not take this into account, and as such, the machine \nstops in an erroneous state with a non-empty stack . The corrected abstract machine N should only fire \nrule vaTi if a non-lambda expression is bound. This corresponds to the STG rule described in Peyton Jones \n(1992) which fires only if the variable is bound to an updatable lambda form and lambda-abstractions \nare never marked updatable. We argue that the resulting machine is the STG machine, albeit restricted \nto one argument. Here, we refer to the STG machine as described in (Peyton Jones 1992): An apparent difference \nis the handling of the environ-ment. All closures need to hold a mapping of variables to heap addresses. \nIn our machine this is represented with the ever present E. In the STG machine, this is represented by \nthe combination of the free variables, US, and their values, UIS~. Indeed, the STG machine trims the \nenvironment to contain just those data nec-essary (the values of the free variables). The trimming is \nan optimisation, and a similar strategy can be im- plemented in our machine, as described by Sestoft. \nThe STG machine often looks up the values of the variables in the environment before using them in a \nlater instruction, as opposed to passing the environ-ment through to the later instruction. See for instance \nSTG rules 1 and 2, where pf is first looked up in the en- vironment. Semantically, rule 1 could have \njust as well pass the entire environment together with f when ex-ecuting the Enter, and look up f s value \nin the Enter rule. Thus both approaches are equivalent. The STG machine has updatable and non-updatable \nclosures. In our terminology, the updatable clo-sures correspond to expressions which are not lambda-abstractions, \nand the non-updatable to those which are. The final difference is in the tagging of the code com-ponent \nwith Enter, Eva1 or ReturnCon. This pro-vides no problems, except for the constructor case (we may need \nto Eva1 a constructor instead of just execut-ing ReturnCon). This, together with the premature lookup \nin the environment explained in (2) above, ex-plains the extra STG rule 5. We thus have that our rules \nlet, casel, case2 and vara cor-respond with STG rules 3,3,6 and 16, while rules appr, vaTI, vara and \nappa correspond with the STG rules 1,15,17 and 2 given in (Peyton Jones 1992). The full STG machine also \nhas rules for handling integers, which explains the missing Conaider executing let s = X2.2 in z numbers. \nSestoft describes a very similar way of handling numbers for ..i:; abstract machine (inspired by the \nunboxed representations which the STG machine uses), and so this development presents little difficulty \nand sheds no light on the STG machii e itself. Following Sestoft it can be shown that: Proposition 5 \n(Correctness of the N Machine) Assume 10 is a oalue. Then: (r, e, [I, IN %* (Alp c-t w],P, [I, [I) if \nr : e -1 A[u I+ w] : u That is, the single-stack STG machine is correct with re-spect to the natural \nsemantic specification given in Figure 1. 5 Allowing multiple arguments in abstractions and applications \nWe now propose an extension of Launchbury s semantics to allow the abstraction and application of multiple \nargu-ments, At the moment, if we wish to write the expression Xxy.y in our basic language, then we have \nto write instead the expression let t = Xy.y in Xx.t. As a consequence of this, any abstra;.t machine \nderived from the semantics will perform a heaI, allocation (due to the let statement) when executing \nthe above statement. In this section we propose an extension of the semantics given in figure 1 to solve \nthis. The abstract syntax of the language, which we christen the eztc:luied la<.guage, is given by: expressions: \ne ::= X2T.e I z 1 let (2; = e,}&#38; in e c 21 .,.x3 1 case e of {Cj y; --f ej}~=, -__-_-.- We use the \nr.otation, x-, to indicate a vector of n argu-ments. A few examples To guide the development of the new \nsemantics, we look at a few examples. Of course, we only expect the semantics to change for the Lam and \nApp rules, which use the extension. In the followin;;, we assume that w is some arbitrary value, and \nignore tlx contents of the heap. Having nm!:-iplc arguments has as consequence that we can now pass ;oo \nmany or too few arguments to a lambda ab-stractic;.. ::; 0)~: first example, we look at an under-applied \napplication (an application which is not given enough argu-ments), given by expression e E f w executed \nin a heap in which f is bcun 1 to the expression Xab.a. We would expect e to evaluate to a lambda abstraction \nexpecting one argument as the final value, as f will evaluate to a lambda abstraction of arity rwo, which, \nafter substituting the only argument, will end up witu the required left over abstraction. That is, we \nwant: f -I: >&#38;.a M.a[w/a] s Xb.w $ Xb.w .I -..-- f w 4 Xb.w  r : X5Y.e JJ. r : XP.e LamE r : e .l,l \nA : XF.Xlz .e A : M .e [P/y ] 4 0 : w m, 0 r:eP I) 0:~ - k ~ r : e .J- A : Xr.e A : e (Z /y L] &#38;m+l),..r \n4 0 : w n > m -  r:eI .I) 0:~ APP!E l?:e u A:w I [x I+ e] :X J,l A[z I-) w] : G VarE I [xi +-k ei] \n: e u A : w I :let{zi=ei}~=rine 4 A:w LetE r:~2~... Xj u r:cxl...xj COW r:e 4 A:c, X1...Xak A:ek[xl/ykl,...,~,~/ykc,,! \n-u @:w r : case e of {Cj Y; -k ej}~=l 4 0 : W CaSeE Note that in rule App, e may actually be a lambda \nabstraction. We just require that at least n elements can be consumed. Rule App substitutes all available \narguments, aud applies the resulting expression to the arguments not thus consumed. Figure 4: Natural \nSemantic specification of Lazy Evaluation This suggests the general rule: in Figure 1 except for the \ntwo new application rules dis-cussed above! and the LamE rule which says that a lambda r : e .(+ A: Xg.XP.e \nA : XP.e [P/F] 4 0: w abstractio:; of any number of arguments is still a value. fb m r:ei? u 0:~ Following \nsection 3, we can define an argument-normalisation which maps the extended language into the with m > \n0. basic language. Now for a case where there are just enough arguments. Executing the expression f w \nw with the same heap as be- fore, we would expect w as the resulting value as the App X# = x rule should \nsubstitute both arguments. That is, we want # (Xx1.. S,.f) = let a, = Xx,.e the following behaviour: \na,-1 = Xx,-r.a, f 4 Xab.a a[w/a, w/b] E w .I) w fww u w al = Xxl.aa in al What if there are too many \narguments? We would expect e xl.. . xn = let al = e 21 that the application rule would substitute as \nmany as are a2 = al x2 needed for the abstraction, and then apply the resulting expression to the rest \nof the arguments. That is, evaluating f wr wa ws should result in the value of applying wr to wa: an \n= an-a xn--l in a,-1 xnf 4 Xab.a a[wr/a,wz/b] wg s wr ws JJ w (let {Xi = e$e, in e)# = let {Xi =e#}~=rin \nCe# fwwz w3 u w (CXl...Xj) C Xl...Xj These suggests a new rule: (case e of {c, G -+ e,}J&#38;)# 1 case \nk#Of{Cj &#38;ii -+ e~}~zl r : e u A : Xr.e A : e [P/p] Z(m+l)...n Jl 0 : w APP E r:eiY 4 0:~ The correctness \nof the new semantics follows as in the which we want to able to apply when n 2 m. In the case X-normalisation \ncases: where we have just enough arguments, an application will not be formed. Proposition 6 (Correctness \nof argument-normalisation) The new, multiple argument, semantics {} :e !J A:w2#{} :e# 4 0:~ A natural \nsemantic specification of the extended language is shown in Figure 4. These rules are exactly the same \nas those r[x 1--f XF.e] : x 4 qx e Xp.e] : x Lamb1 r : f J. O[u I-P XF.Xz .e] : u 0 : AF.e[i?/~] 4 A[w, \n~--t w] : u nL > o r : f 2 .J A[u++w]:u &#38; PM r : f 4 A(v t+ Xr.e ] : II A : e [zi?/y ] z?(~+ ).. \n~ 4 0 : UJ n , m  -  r:f i? 4 0:~ &#38;P M l? : e J. A[u H W] : u w a lambda abstracti.on, e not r[x++e]:x \n.J A[uHw,x~$]:x VarM I :e -1 A:cyI...yJ e not a lambda abstraction r[x++ee]:x .j, A[x~cyl...?/j]:cyl...yj \nVarCn4 r[x; ++ ei] : e 4 A[u ++ w] : u r : let {xi = ei}r&#38; in e J. A[% I-+ w] : u LetM r : c x1.. \n. Xj J. r : c x1.. .x3 ConsM r:e 4 A:ckXl...Xa,, A:ek[X1/ykl,...,Xalo/ykas! 4 (i+~eWw]:U r : case e of \n{Cj 6 -+ ej}y=l .j. O[U b-+ LL] : :.. case&#38;f Figure 5: Natural Semantic specification of Lazy Evaluation \nfor the X-normalised, extended, language Deriving the Abstract Machine The following section duplicates \nthis development for We again, informally, derive the abstract machine from the above semantics modified \nfor the STG machine. We will semantics given in Figure 4. We concentrate only on those see there how \nrules 2 and 3 can be further optimised, and instructions which have changed as a result of introducing \nhow the squeeze operator can be defined. multiple arguments. As usual LamE, ConsE give rise to no instructions. \nRules 6 The STG machine, naturally VarE, CaseE, LetE are unchanged by the extension. Rule App~ gives \nrise to two rules. One to start the Repeating the argument of section 3, we can modify thecomputation \nof the body and one to do the substitution. semantics in Figure 4 to detect values via the variable. \nThisRemember that this rule fires when there are not enough results in the semantics given in Figure \n5. arguments to satisfy the bound abstraction: 1) r (e ii?) s ===+ Deriving the STG machine r e (CT \n: S) Following the previous section exactly, rule Appn,i gives rise 2) r Xy .e -:s) ==+ to the same \nt\\>,c, instructions, except now rule 2 detects ther (e[S?/F]) t lambda abstraction in the heap instead \nof in the control. Rule Apph also gives to two rules. The first is the same Remember that this rule fires \nwhen there are not enoughas the first above of course, and the second is to form the arguments to satisfy \nthe bound abstraction. application of the substituted expression with the remaining arguments: (e Z ) \nS * 1) l- (F : S) 1) r (e 2 ) S ==+ 2) $0 i--f XF.e] yu) 7 : S) e l7 e (P : S) (e[iY/pJ) !T 3) r Xp.e \n(F: S) * Rule ,?&#38;,,, follows similarly. r (e[.p/y ] ,$m+l)...n) s We only want rule 3 to fire if \nthere are too many argu-1) I-7 (e 3 ) S * ments on the stack for the abstraction. We can optimise (a? \n: S) this last rule somewhat, as we already know what do with 3) $2 k-3 X$m.e] yu) W :S) ---7 e[p/r,] \n.j$m+l)...n san application (rule 1) -the application produced by rule 3 i will just dump the rest of \nthe arguments back on the stack Again, we can optimise the application by incorporating and enter the \nnew e (see rule 1)). We could just as well rule 1: have combined it with rule 1 to produce: 3 ) r!u \ne Af+.e] (w) 3 ) r Xy-.e (P : S) ===s r e[Z /F] r e[P/y ] (~Z(m+l),..~: S) This begs the question: How \ndo we count the argu- ments? . An answer would be to just look at the arguments one by one to determine \nwhether they are update markers, end of stack markers, case markers or the variables that we want. However, \nwe cannot get a case marker following an insuficient number of arguments. To see this, we would need \nto have a: case (f g) of ah which would dump the alts on the stack and execute the scrutiniser which \nwill dump the argument g on the stack and enter f. Then we have args followed by alts. However, the above \nprogram is not well- typed, since the type of f g has to be a constructor, and not a partial application. \nWith this knowledge, we can go back to rule 2 and see that it really looks like this (remember, there \nare not enough arguments to satisfy the abstraction): 2 ) I$ H Xr.e] (v) (T:#p:S) ==+ (W/PI) #P : s But \nsince e[l /T] is actually a lambda abstraction (there weren t enough arguments), the next rule to fire \nwill be the varz (see Figure 3) which will do the heap update for p and remove it and re-execute the \nlambda abstraction. But this is the same as reentering v with the stack frame removed: 2 ) l?[v e Xp.e] \n(v) (F : #p: S) ===+ I$ I-+ e[Z /y ]] (v) (F : S) This is the famous squeeze operation, which squeezes \nout the update frame. We can go even one step further. If we consider gener-ating code for this abstract \nmachine, then we will have a slight problem with rule 2 , because it forms new code -that is, we need \nto generate code for e[p/y ] -for each partial substitution. We can get around this by replacing it with \nthe following rule: 2 ) r[v e Xp.e] (v) (Z? : #p: S) =+ qpl-b v Z ] (v) (F : S) which has the same semantics \n-instead of doing the sub- stitution we make the binding re-apply the body to the argu- ments. This too \nforces us to generate code, except this time all we have to do is generate a number of code blocks for \nan application, one for each possible number of arguments. If a uniform representation is used when mapping \nthis ma-chine to hardware, then this is not costly at all (Peyton Jones 1992). In lot. cd, the representation \nof the closure is slightly different: let f be some arbitrary variable, then bind p to the closure (f \nxs, E[f I+ v, xs ti PI). The code for this can be shared between all partial applications to the same \nnumber of arguments. All that is required is a family of such code-blocks, one for each possible number \nof arguments. The final derived machine is the STG machine. 7 Conclusions and Future Work There have \nbeen many proposals for evaluating func-tional languages, such as the G-machine (Johnsson 1984), the \nspineless tagless G-machine (Peyton Jones 1992), the CMC (Lins 1987) and TIM (Fairbairn &#38; Wray 1987). \nThe work of Lins, Thompson &#38; Peyton Jones (1994) has stressed the importance of relating different \nabstract machines, al-lowing us to examine the similarities and differences between the machines. In \nlot. tit, the TIM and CMC machines are related (albeit without sharing), and Peyton Jones &#38; Lester \n(1992) go some way in showing the relationship between the TIM and the G-machine. We believe that using \nnatural semantic specifications to compare various machines is a rewarding route to take be-cause: the \nspecification provides a common ground from which we can compare the characteristics of different ma-chines. \nIndeed, all of these machines mentioned above have in common that they attempt to reduce a language in \na manner adhering to the principles of lazy evalua-tion, and rhis is exactly what the semantics describes. \nThey only each do the same thing slightly differently. For Instance, it is quite clear from the development \nin this pap% that the hallmark of the STG machine is its characterisation of an abstraction via a variable. \nUnfor-tunately we have also seen that the common ground that we use had to be enlarged somewhat by the \nin-clusion of abstractions and applications to handle mul-tiple arguments, but this should not deter \nus as any extension should still be consistent with the original semantics. the specification can be \nformally mapped to an abstract machine. i;s we have demonstrated, this mapping ef-fortlessly handles \ncomplex notions such as shared par-tial ibatracrions, and provides a framework in which we can examine \noptimisations to the basic abstract ma-chine (suth as the squeeze operation). The process of generi&#38;g \nan abstract machine can even be auto-mated (Dieill 1996). the semantics is minimal, in the sense that \nthere is no auxiliary machinery such as stacks and environments. It should be possible to refine this \nminimal model to build each of the abstract machines mentioned above, thus rciating each of them, and \nmaking clear the de-sign de&#38;ions that were made in creating them and the difierel,ces oetween them. \nWe leave this to our future aoik. the s~:a~tics is useful, as demonstrated in (Turner, M a&#38; r 01 \nMossin 1995). Sansom &#38; Peyton-Jones (199i) have also used a similar semantics to formally prove a \nprofiling tool correct. This paper has es-rab!is!.ed the relationship between Sestoft s and the STG machine, \nand so these results can now be carried .chrough (formally) to the STG machine. Re!ated ,a<e.i:,ch includes \nthe works of Launchbury and Sestoft .nz-.tic.~td in this paper. Peyton Jones (1992) men-tions th;:t the \nSTG machine regards addresses as values, but takc3 this ,lotion no further. Lins et al. (1994) re-lates \nAI? Ti?. .iu d CMC machines, though without shar-ing. TIAelr +proach does not appear to provide a basis \nfor prov;ng other machines correct. (Sestoft 1997) shows a relatioiiship between a one-argument lazified \nTIM and his derived machine. The work of (Douence &#38; Fradet 1996) builds a very rich framework to \ncompare various implemen-tations and ~ptimisations via successive transformations of a base combinxcor-like \nlanguage. Their approach is quite different: to ours: we have a high-level semantic specification which \ncali ouly ne transformed into the STG machine and which is Ielated ro some base natural semantic specification, \nwhereas they concentrate on transforming one base language into various abstract machines. Further work \nis necessary to gauge whether their transformation technique could be used to guide us into different \nsemantic specifications.and thus different abstract machines. The work of (Meijer 1988) is similar in \nthat it advocates the successive refinement of a de- notational semantics to various abstract machines. \n(Ariola, Felleisen, Maraist, Odersky &#38; Wadler 1995) propose a re- duction semantics of lazy evaluation, \nand further work will concentrate on determining whether the techniques used for creating abstract machines \nhere carry over to this formalism. Both have their own advantages, as demonstrated in (Turner et al. \n1995). Other related works include (Seaman &#38; Pu-rushothaman Iyer 1996) and (Seaman 1993). In their \nsyn-tax, sharing is not explicitly enforced (leading to a slightly more complicated semantics). We have \nnot concentrated on the mapping between the STG machine and hardware at all, but this had a large im-pact \non the design of the original STG machine (Peyton Jones 1992). It would be interesting to identify important \nmapping principles and highlight these in the semantics: for instance, the number of stacks or taglessness. \nThis would allow us to derive an abstract machine even closer to the intended underlying architecture. \nThis would also allow us to quantify the tradeoffs made between the extra heap allo- cation in the STG \nas opposed to the lambda instruction in Se&#38;oft s machine. Following the methods of (Hannan 1991) \nit should be possible to produce machine code in a provably correct manner from the abstract machine. \nAcknowledgements The author thanks Daan Leijen, Pieter Hartel, Simon Pey-ton Jones and Peter Sestoft \nfor their many useful and in-sightful comments. The author is supported by the Nether- lands Computer \nScience Research Foundation with financial support from the Netherlands Organisation for Scientific Re-search \n(NWO). References Ariola, Z. M., Felleisen, M., Maraist, J., Odersky, M. &#38; Wadler, P. (1995), A call-by-need \nlambda calculus, in Proceedings of the 22nd ACM Symposium on Princi- ples of Programming Languages . \nDiehl! S. (1996), Semantics-directed geueration of compil- ers and abstract machines, PhD thesis, Universitst \ndes Saarlandes. Douence, R. &#38; Fradet, P. (1996), A taxonomy of functional languages implementations, \npart II: Call-by-name, call-by-need and graph reduction, Technical Report 3050, Institut National de \nrecherche en Informatique et Au-tomatique (INRIA). Fairbairn, J. &#38; Wray, S. (1987), TIM: A simple, \nlazy abstract machine to execute supercombinators, in G. Kahn, ed., 3rd Functional Programming Lan-guages \nand Computer Architecture . Vol. 274 of LNCS, Springer-Verlag, pp. 34-45. Hannan, .I. (1991), Making \nabstract machines less abstract, in J. Hughes, ed., 1991 Conference on Functional Programming Languages \nand Computer Architecture (FPCA) , i-01. 523 of LNCS, Springer-Verlag, pp. 618-cx,. Johnsson; ! . 1084), \nEfficient compilation of lazy evalua-tion; zn Fn,ceedings of the ACM SIGPLAN 1984 Sym- posium c:: Compiler \nConstruction , Vol. 19(6) of SIG- PLAN n,,trcxs, ACM Press, pp. 58.-69. Launch!>ury, J. (1993), A natural \nsemantics for lazy evalu-ation, ilt 20th Symposium on Principles of Program- ming Languages (POPL) , \nACM Press, pp. 144-154. Lins, R. D (7957), Categorical multi-combinators, in G. Kahn. ed., 3rd Functional \nProgramming Lan-gauges and Computer Architecture , Vol. 274 of LNCS, Snr;ng,xr- \\rFr!ag, pp. 60-79. Lins, \nR.. I)., Thompson, S. J. &#38; Peyton Jones, S. (1994), 011 t!re crltiivalence between CMC and TIM , \nJournal of Fu~xtw?wl Prograsmrming 4(l), 47-63. Meijer, E. (1988). A taxonomy of function evaluating \nma-chines, ill, T. Johnsson, S. Peyton Jones &#38; K. Karlsson, eds, Proceedings of the workshop on Implementation \nof Functional Languages , Chalmers University of Tech- :iology, Technical Report 53. Peyton Jonps, S. \nL. (1992), Implementing lazy functional languages on stock hardware: The STG machine , .Journd rjj \n%,ctional Programming 2(2), 127-202. Peyton Jones, 2. L. (1996), Compiling Haskell by program transformation: \na report from the trenches, in H. R. Nieison, ed., 6th European Symposium on Program-ming (ESCP 96) , \nVol. 1058 of LNCS, Springer, pp. 18- 44. Peyton Jones. S. I,. &#38; Lester, D. (1992), Implementing Fzlnc-tio:d \nLmiyuages: A Tutorial, Prentice Hall Interna-tional Series in Computer Science, Prentice Hall. Sanso:n, \nP. l\\f {1994), Execution Profiling for Non-strict Fl;uctionri Languages, PhD thesis, University of Glas- \ngo-a Sansom, P. M. &#38; Peyton-Jones, S. L. (1997), Formally-based proi;!ing for higher-order functional \nlanguages , A CIM ? !u,lsactions on Programming Languages and Systems B9(2), 334-385. Seaman, .i (1993j, \nAn Operational Semantics of Lazy Eval-uation fo Analysis, PhD thesis, Pennsylvania State University, \nDepartment of Computer Science and En-ginPeri?i,.T c Seaman. J. RL Purushothaman lyer, S. (1996), An \nopera-tional semantics of sharing in lazy evaluation , Science of Computer Programming 27, 289-322. Sestoft. \nP. (1997), Deriving a lazy abstract machine , Jour-1m1 of l%Tlctl;onal Programming 7(3), 231-264. Turner, \nD.! Wa*:ler. P. &#38; Mossin, C. (1995), Once upon a type, ~7: International Conference on Functional \nsroqrai:!r?i;lg Languages and Computer Architecture , AC:23 I+ss, pp. l-11.  \n\t\t\t", "proc_id": "289423", "abstract": "The application of natural semantic specifications of lazy evaluation to areas such as usage analysis, formal profiling and abstract machine construction has shown it to be a useful formalism. This paper introduces several variants and extensions of this specification.The first variant is derived from observations of the Spineless Tagless G-machine (STG), used in the Glasgow Haskell compiler. We present a modified natural semantic specification which can be formally manipulated to derive an STG-like machine.The second variant is the development of a natural semantic specification which allows functions to be applied to more than one argument at once. The STG and TIM abstract machines both allow this kind of behaviour, and we illustrate a use of this semantics by again modifying this semantics following observations of the STG machine. The resulting semantics can be used to formally derive the STG machine. This effectively proves the STG machine correct with respect to Launchbury's semantics.En route, we also show that update markers in the STG machine are necessary for termination, and show how well-known abstract machine instructions, such as the <i>squeeze</i> operation, appear quite naturally as optimisations of the derived abstract machine.", "authors": [{"name": "Jon Mountjoy", "author_profile_id": "81100469406", "affiliation": "Department of Computer Science, University of Amsterdam, Kruislaan 403, 1098 SJ Amsterdam", "person_id": "P146957", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/289423.289439", "year": "1998", "article_id": "289439", "conference": "ICFP", "title": "The spineless tagless G-machine, naturally", "url": "http://dl.acm.org/citation.cfm?id=289439"}