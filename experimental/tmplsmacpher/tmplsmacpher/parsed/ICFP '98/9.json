{"article_publication_date": "09-29-1998", "fulltext": "\n Programming with Variable Functions Martin Odersky University of South Australia odersky@cis.unisa.edu.au \nAbstract What is a good method to specify and derive imperative programs? This paper argues that a new \nform of func- tional programming fits the bill: where variable functions can be updated at specified \npoints in their domain. Traditional algebraic specification and functional pro-gramming are a powerful \npair of tools for specifying and implementing domains of discourse and operations on them. Recent work \non evolving algebras has introduced the function update in algebraic specifications, and has applied \nit with good success in the modelling of reactive systems. We show that similar concepts allow one to \nde- rive efficient programs in a systematic way from functional specifications. The final outcome of \nsuch a derivation can be made as efficient as a traditional imperative program with pointers, but can \nstill be reasoned about at a high level. Variable functions can also play an important role in the structuring \nof large systems. They can subsume object-oriented programming languages, without incur-ring the latter \ns problems with pointer aliasing and mod- ularity. Introduction In his paper Hints on Programming Language \nDesign [Hoa73; HJ89], which was given as a keynote ad-dress at the first ACM Symposium on Principles \nof Pro- gramming Languages in 1973: C.A.R. Hoare made the following, quite prophetic, remark: In a high-level \nlanguage: the programmer is de- prived of the dangerous power to update his own program while it is running. \nEven more valu- able, he has the power to split his machine into a number of separate variables, arrays: \nfiles, etc.; when he wishes to update any of these he must quote its name explicitly on the left of the \nassign- ment; so that the identity of the part of the ma- chine subject to change is immediately apparent; \nand: finally, a high-level language can guarantee that all variables are disjoint: and that updating \nany one of them cannot possibly have any effect on any other. . . . [In contrast,] references are like \njumps: leading wildly from one part of a data structure to an- other. Their introduction into high-level \nlan- guages has been a step backward from which we may never recover. Hoare s prophecy is still valid \ntoday. To be true, there have been programming paradigms that side-step the problem by eliminating one \nof its two constituents, ref-erence passing and mutable state. Mainstream program verification usually \ndoes not consider reference passing, and uses copying instead. On the other hand, declara-tive programming \nsidesteps the problem by having refer-ences but not considering mutable state. Without muta-ble state, \na reference is indistinguishable from the value it refers to, so references become simply an efficient \nim- plementation technique. More recently, refinements of functional programming languages have emerged \nwhich re-introduce mutable state without re-introducing references as an observable con-cept. Programming \nlanguages based on linear type dis-ciplines [Gir87, WadSOb, LM92, Bak92, Ode92, AP94, MOTW95, CH97] allow \nstate to be updated destructively as long as it can be verified that only one reference to the state \nexists. In this case, updating state is indis- tinguishable from creating and modifying a copy. Pro-gramming \nlanguages based on monads [Mog89, WadSOa, Mog91, Wad92, HPW92] allow one to partition the pro-gram into \na functional part which uses references and an imperative part which uses mutable state. While all of \nthese approaches are useful: they seem to be not flexible enough to completely replace references. On \nthe contrary; with the rise of object-oriented program- ming, references (disguised as objects) have \nbeen elevated to principal program building blocks. So it seems we are today further than ever away from \na solution to Hoare s problem. This work tries to tackle Hoare s problem by taking seriously a simple, \nwell known equivalence: 5T.f E f(z) . Seen as an equivalence between values; it states that field selection \ncorresponds to application of a selector function. Extending the equivalence to left-hand sides of assign- \nments: we get something more interesting: z.f :=y E f(z) :=f/ . Whereas the left assignment is quite \nconventional, the right assignment is not. In essence: we postulate the exis- tence of a variable function \nf i which gets updated by the assignment at one point in its domain. Most program-ming languages know \nonly a restricted form of a variable function over an integer interval, i.e. an array. The treatment \nof arrays as variable functions goes back to the early work on axiomatic program verification [Hoa71; \nDij76] and has been stated explicitly by Reynolds [Rey79]. We propose to extend this concept to variable \nfunctions over arbitrary domains. In fact, since variable functions can model mutable fields by the above \nequiv- alences: we propose to do away with mutable fields al-together and treat the left hand forms of \nselection and update as syntactic sugar for the right hand forms. In a sense this gives us a formalism \ncompletely dual to object-oriented languages and impure functional lan-guages such as Lisp or Scheme. \nIn those languages every variable or field can be mutable; whereas every function or method is immutable. \nWe propose instead a formalism in which all data are immutable but there can be functions that are mutable. \nIs this more than just a change in viewpoint? After all, every program with pointers can be transformed \nby the above equivalence into a program with variable functions, so how can the new notation give us \nbetter reasoning capabilities? Variable functions are certainly no silver bullet which makes opaque programs \nclear by a simple change in notation. However, variable functions do give the programmer increased flexibility \nwhich can be used to formulate programs that are clearer and better structured than their pointer-based \ncounterparts. The increase in flexibility comes from two sources. First, in a pointer-based program, \nthe reference z in a.f is an element of an unstructured type. Pointers are al- located from the heap \nand there is no a priory relationship between two pointers except their equality or disequality. On the \nother hand, the domain of a variable function can have structure. This is crucial for program proofs \nthat proceed by case distinction or induction. Many ex-amples of such proofs are found for programs that \nuse arrays. The same principles apply for programs with vari-able functions over other structured types, \ne.g. algebraic data types. Section 3 will present an application of such reasoning principles in a proof \nof refinement for two im- plementations of queues. Second; in a pointer-based program the field f in \nz.f needs to be declared together with the declaration of 2 s type. Sometimes: but not always, this \nis an appropriate program structuring. On the other hand, in a variable function application f(z) the \nfunction f need not be de- clared at the same place as 2. In may cases, this freedom allows more natural \nmodular decompositions. Section 4 will give examples. Variable functions have come up in a number of \ncon- texts; ranging from program logics to specification lan-guages, to programming. Dynamic logic [Har79] \nmakes extensive use of variable functions called arrays (but being not restricted to integer domains). \nEvolving algebras[Gur95]; now called abstract state machines: use a combination of algebraic data types \nand variable func-tions together with transition rules for system modelling [BBD 96] as well as programming \nlanguage semantics [GH93; BR94, BS94]. Variable functions have also been a part of the SETL programming \nlanguage [SDDS86]; where they are called maps. Maps in their full generality are more powerful than variable \nfunctions in that they support enumeration over the function s domain. Also similar is the table con-struct \nof Hermes [SBL+Sl]. Interestingly, the syntax of variable functions goes back to at least Algol-W [WH66], \nbut their use in this language is restricted, so that the concept is in fact equiv- alent to records. \nThe present work explores variable functions as a link between specification and programming. It is the \nfirst to look at variable functions in the context of a polymorphi- tally typed, higher-order language. \nThere are several other approaches to the semantics, derivation and verification of pointer-based programs. \nThe axiomatic definition of Pascal [HW73] treats the whole program heap as a collection of variable functions, \none per record type declared in a program. The domain of these functions is unstructured, so that this \napproach does not lend itself immediately to program verification without additional techniques to address \npointer alias-ing. Burstall [Bur72] uses a similar approach for pro-gram verification. He handles de-aliasing \nby the intro-duction of changed sets. Bijlsma [BijSS] and Miiller [MB93, Ma1971 refine this approach \nby using maps or re- lational algebra to express the topology of pointer struc-tures. In all these treatments: \nthe topological informa-tion about a pointer structure is external to the pro-gram be reasoned about. \nBy going from unstructured to structured domains of variable functions we can in-ternalize most of this \ninformation in the program itself. This should significantly improve documentation, remove many proof \nobligations (in particular where a pointer structure can be modelled directly as an algebraic type) and \nsimplify the proofs for those obligations that remain. An alternative way to verify programs with pointers \nworks by axiomatising a notion of program equivalence for a language with references [Mas88; MT91: ORH93: \nSF93]. It is not obvious how to extend these equational theories so that they can be applied in proofs \nof refinement. The rest of this paper is organized as follows. Sec-tion 2 sketches an example language \nto express variable functions. Section 3 exemplifies how variable functions are employed in program derivation. \nSection 4 illustrates how variable functions help in program decomposition. Section 5 presents an operational \nsemantics for a core language with variable functions. Section 6 concludes. 2 A Notation for Variable \nFunctions This section gives an overview of Fun!, a language that supports variable functions over algebraic \ntypes. The syn- tax var f (x : S) : T := E declares a variable function f : S + T whose initial value \nis E. The initial value may depend on the parameter, x. The initializer E is optional; if none is given \nthe variable function is initially everywhere undefined. The variable function can be accessed like a \nnormal function with syntax f(E). It can also be updated with syntax f(El) := E2. This will update the \nfunction at the value of El with the value of &#38;. Subsequent accesses to f(El) will yield the value \nof Ez. The value of an update state-ment is the value being assigned. For convenience; we also admit \nsingle point variables varv:T:=E. Such variables can be thought of being defined over the unit domain \n0. Every using occurrence of a single point variable is implicitly augmented with a () argument. Examples: \nvar snowfall(month: int): float var root: BinTree := Empty var prev (I: LinkedList): LinkedList := Nil \nvar h(t: BinTree): int := 0; fun height(t: BinTree): int = t. case Empty j 0 case Branch(l, x. r) + if \nh(t) = 0 then h(t) := max(h(left(t)). h(right(t))) + 1 h(t) The last definition is a bit more involved \nthan the oth-ers. It introduces a lazy memo function [Hug85]: height. The function computes the height \nof its tree parameter t from the height of t s subtrees; and updates a map h with that value. Subsequent \naccesses to height(t) then take the value directly from h instead of recomputing it. Field selection \nx.f has the same meaning as function application f(x), except that the name f is resolved in the module \nthat declares the type of x7 rather than in the current module. Hence, if both f and the type of x are \ndeclared in some module MT we may write x.f instead of M.f(x); which would be more conventional in functional \nlanguages. That way, we achieve conciseness on a par with object-oriented programming for those programs \nwhich cluster functions with their first argument. The function height is implemented by pattern match-ing \non the value of its t parameter. Pattern matching is expressed by applying the anonymous function { case \nEmpty + 0 case Branch(l. x. r) + } to the argument t. The function application is written in postfix \nsyntax x.f. Implementation How can variable functions be repre- sented? There is a range of possibilities. \nMost straight-forward would be to implement a variable function such as height as a hash table or tree. \nOr one could implement height as an array: to be indexed by integer fields that are implicitly allocated \nin BinTree nodes. Or program anal-ysis might determine that only one copy of height can exist at any \none time, in which case one might implement height by distributing it over the nodes in its domain, in \nfact going back to the standard mutable field implemen-tation. In practice one has to be careful about \navoiding storage leaks. We would expect that the storage for an associa-tion from a key x to a value \nf(x) is reclaimed when either z or f individually becomes garbage. If one used a stan- dard hashtable \nor tree; there would be a reference from the table to the key x7 so x could never become garbage as long \nas the whole table was reachable. Better storage reclamation behavior requires some cooperation from \nthe garbage collector. For instance: one can use weak refer- ences: which are not followed by the collector, \nas building blocks for weak hashtables. A Refinement The distributed implementation in terms of mutable \nnode fields incurs the overhead that every node in the domain of the variable function has to carry a \nfield, whether the function has been updated at that node or not. If the function is only updated at \na few points in its domain this might waste a lot of space. On the other hand, the distributed implementa-tion \nis clearly superior in cases where function updates are common. In general: the tradeoff between distributed \nand non-distributed implementation will be difficult to make automatically. It therefore seems reasonable \nto also give the programmer a choice between the two implemen- tations. We achieve this by introducing \na second declaration form for variable functions; where the parameter comes first: var(x:S)f:T:=E The \nnew form is intentionally analogous to the field access syntax x.f where the leading parameter section \ncontains the this parameter This form of declaration can be used only in a global context, not for a \nlocal variable in a function. Its meaning is the same as the standard variable function declaration var \nf (z : S) : T := E. However, the graph off will now be represented as a field in every S node. Since \nf and S might be defined in different program files, this means that the final layout of objects can \nbe determined only at link time. Except for their implementation aspects; the two vari- able declaration \nforms var (x : S) f and var f (x : S) are completely equivalent. In particular, they can be freely mixed \nwith the two forms of use f(x) and x.f; it is not required to use field access syntax only with field \ndeclara-tions and function application syntax only with function declarations. More details of our notation \nwill be explained in the context of the examples in the following sections. Example: Queues This section \nderives systematically an efficient imperative implementation of mutable queues from a high-level func-tional \nspecification. This problem is generally believed to be hard. In previous work [ORH93], we gave a proof \nthat a monadic queue implementation satisfies algebraic laws on queues which were derived from the functional \nspecifi-cation. Compared to this work, the present treatment is more systematic (since it uses a standard \ndata refinement approach [Hoa72]) j complete (since it directly relates to the functional specification) \nj and concise. As a first step, here is the definition of an abstract type for immutable queues. module \nQueue type Queue = case Empty case Append(prev: Queue, elem: A) fun isEmpty(Empty) = true 1 isEmpty(Append(-, \n-) = false fun first(Append(Empty, x)) = x I first(Append(Append(q, x), y) = first(Append(q, x)) fun \nrest(Append(Empty. x)) = Empty 1 first(Append(Append(q. x). y) = Append(rest(Append(q, x), y) This program \ndefines a module Queue which contains a type of queues of the same name as well as operations on queues. \nFor the time being, we let the type of queue ele- ments be an arbitrary but fixed type, A. The Queue \ntype is an algebraic data type with two constructors; Empty and Append. The module also defines functions \nisEmpty, first and rest operating on values of Queue type. All functions op-erate by pattern matching \non their parameter. They im-plement in a straightforward way the standard algebraic axioms for queues. \nSo far, we have a purely functional program. As a next step, let s define a module that encapsulates \na single mu-table queue variable. The initial definition of this module is quite simple: module QueueVar \nvar q: Queue := Empty; fun isEmpty = Queue.isEmpty(q) fun append(x: A) = q := Append(q, x) fun front0 \n= first(q) fun skip0 = q := rest(q) The module contains a mutable variable q of type Queue as well as \noperations isEmpty, append, front, and skip, which simply apply the corresponding functional opera-tions \nto the encapsulated variable. This module is fine as a specification or high-level de-sign, but it s \nnot a an efficient program since the front and skip operations take time linear in the length of the \nqueue. There exist several purely functional implementa-tions of queues with constant time cost for these \nopera- tions [HM81, Bur82; Oka97]. Rather than go down that path, we will develop a refinement for queue \nvariables that closely resembles the usual imperative algorithm. The new algorithm keeps a reference \nto an entry im-mediately before the head of the queue; so that front can be implemented efficiently. \nFurther; there is a mapping next that maps each queue element to its successor, so that skip can be implemented \nefficiently. Figure 1: Relationship module QueueVarl var next(q: Queue): Queue var head: Queue := Empty \nvar last: Queue := Empty fun isEmpty = head = last fun append(x: A) = last := next(last) := Append(last. \nx) fun front0 where not isEmpty = elem(next(head)) fun skip0 where not isEmpty = head := next(head) It \ns worth noting that the definition of front uses function elem as a selector of the elem field in Queue. \nThe relation- ship between next and Append is depicted in Figure 1. We still have to prove that the new \nefficient imple-mentation is a refinement of the original specification. This task is very hard for a \nconventional pointer-based implementation. The reason is that, in the absence of additional knowledge \nabout aliases; a pointer assignment p.x := y will invalidate any dereference q.x as long as we can t \nprove somehow that p and q are either defi-nitely the same or definitely different. This alias analysis \nis known to be difficult in general. We will show that variable functions over algebraic types give the \nproblem sufficient structure to allow an easy proof. Let s first make precise what we mean when we say \nthat QueueVarl is a refinement of QueueVar. We do so by defining an abstraction function from QueueVarl \nstates to QueueVar states. The function is defined as follows: abs(h: Queue, I: Queue): Queue = Append( \nAppend(Empty. x1). . . . . xn) where I = Append( . . Append(h. xl), .., x,) n?O The equalities in this \ndefinition are meant to be struc- tural, no pointer identity is assumed. We then need to between next \nand Append show the following refinement laws. isEmpty = isEmpty(abs(head.last)) front0 = first(abs(head.last)) \nappend(x); abs(head,last) = Append(abs(head,last), x) skip(); abs(head.last) = rest(aba(head,last)). \nThese laws show that a simulation relation exists between modules QueueVarl and QueueVar. The simulation \nrela-tion is depicted in Figure 2. State changes in QueueVarl correspond one-to-one to state changes \nin QueueVar, af-ter applying the abstraction mapping abs. Analogously, access functions in QueueVarl \nreturn the same result as access functions in QueueVar. To prove these refinement laws; we first establish \ntwo invariants for the QueueVarl module. Say a queue ql is a prefix of a queue qa if there exist elements \nxl, . . . . x, so that q2 = Append( . . . Append(ql, XI). . . . . x,) We write in this case q1 < qs. \nThen we always have: head << last and V q,q : q << last A q = Append(q , x) * next(q) = q . Both invariants \nare clearly true initially, and a simple ap-plication of predicate transformers shows that they are maintained \nby each operation in QueueVarl. With the in- variants: it is then straightforward to prove the refinement \nlaws. Two of the laws: namely isEmpty = isEmpty(ubs(head.last)) append(x); abs(head,last) = Append(ubs(head,last), \nx) follow immediately by expanding out the definitions on both sides. Instead of showing front0 = first(ubs(head,last)) \ndirectly, we prove a slightly stronger statement: 109 QueueVar.rest QueueVar.Append b 4 A abs abs abs \n(head,last) (h&#38;d&#38;t) ) (heatY,last ) QueueVarl .isEmpty QueueVar 1 skip QueueVarl .append Figure \n2: Simulation of QueueVar by QueueVarl Vq: head < q A q << last A q # head + front0 = first(aba(head,q)) \n. The proof is by an induction on the number n of elements that are in q but not in head. The case where \nn = 0 is excluded by the precondition of front. Hence the base case is n = 1. This case is shown by direct \ncomputation. q = Append(head. x) + (by the invariant) next(head) = q  + (by the two lines above) elem(next(head)) \n= x .  Hence; first(ubs(head.q)) = first(Append(Empty. x)) =x = elem(next(head)) = front0 . For the \ninductive step, assume that q = Append(Append(q . x), y) , for some q such that head < q . Then, uba \n: uba(head, q) = Append(Append(aba(head, and we have: front0 = (by the induction hypothesis) first(uba(head, \nAppend(q , x))) by definition of q ). x). y) , The proof for the connection between skip and rest is \nquite = (by definition of abs) first(Append(ubs(head, q ). x)) = (by definition of first) first(Append(Append(ubs(head. \nq ), x), y)) = (by definition of uba) first(uba(head. Append(Append(q , x), y))) . analogous, and is \nomitted here. We have thus shown that QueueVarl implements the QueueVar specification. The proof relied \ncrucially on vari- able functions over structured domains. If we had to deal with pointers directly, \nthe abstraction function uba from QueueVarl to QueueVar and with it the inductive proof of simulation \nbetween the two modules could not have been formulated in the way we presented it. However, there is \nstill something wrong with the QueueVarl implementation: The data structure refer-enced from last will \ngrow with each append, and none of the other operations will shrink its size. Hence, for pro- grams that \nperform an unbounded number of append s, we will need an unbounded amount of memory. Fortunately; this \nproblem has a simple solution. Let s postulate that all variables in QueueVarl and the Queue type itself \nare private to that module. Then the prev field of a queue: which leads from an Append node to its predecessor, \nis never accessed in the actual program code. We only need it for the correctness proof of QueueVarl. \nHence; our program is unaffected if we do away with the prev fields in queues. We still have to make \nsure that all queues we create are pairwise different. This is achieved by switching from an algebraic \ndata type with structural equality to a generative type with reference equality. The new definition of \nQueue is as follows. type Queue case Empty case new Appen d(/*prev: Queue,*/ elem: A) The keyword new \nin front of Append indicates that every application of new Append yields a fresh Append node: different \nfrom all Append nodes created before. The rest of the queue module is given below. module QueueVar2 var \nnext (q: Queue): Queue var head: Queue := Empty var last: Queue := Empty fun isEmpty = head = last fun \nappend(x: A) = last := next(last) := new Append(/*last,*/ x) fun front0 where not isEmpty = elem(next(head)) \nfun skip0 where not isEmpty = head := next(head) The only changes compared to QueueVarl are the genera- \ntive definition of Append and, analogously, the generative constructor new Append(x) in the implementation \nof ap-pend. To point out the correspondence with QueueVarl, we left the code that was replaced as a comment. \nThe correctness of QueueVar2 is easily established from the correctness of QueueVarl. It is sufficient \nto note that ev-ery application of Append in QueueVarl will create a queue which is different from any \nqueue created up to that point; hence; switching to a generative data type will not affect the topology \nof the graph spanned by head and next. As a final step in the derivation, we can change the definition \nof next to enforce a field-based representation: var (q: Queue) next: Queue The rest of the module stays \nthe same. We have thus systematically derived from a functional specification an imperative queue module \nwhich could have been written in this form by a competent programmer. The final implementation is on \na par with the best possible imperative implementations of queues -in fact it is probably more efficient \nthan what most programmers would have written! The reason is that, if we translated our algorithm to \na traditional record-or object-based lan-guage; the first queue element would look strange since it has \nan undefined elem field. Many programmers would tend to avoid this undefined field by initializing first \nand last to a null value. But then the append and rest opera-tion would have to check for the special \ncase of an empty list. These case distinctions are costly, both conceptually and in terms of run-time \nefficiency. In contrast, if we regard next as a variable function over an algebraic queue data type, \nfirst and last are sim-ply initialized to the empty queue. No special trick is involved. In fact, with \nthis approach the more efficient solution is hard to avoid. 4 Use of Variable Functions in Program (De-)Composition \nIn the last section, we have presented a case study as an argument that variable functions are the right \nway to think about programs with references to mutable state. This section shows that this concept also \nhelps in pro-gramming on a larger scale, where programs are com-posed from separate modules. The main \nbenefit to be gained here is that, unlike fields of records or objects, variable functions can be declared \nin places unrelated to their argument type. As a first example: consider the task of depth-first traversing \nan acyclic graph. The standard algorithm uses mark bits in the graph nodes to keep track of the fact \nthat a node has already been visited. This is problem- atic for several reasons. First, it violates the \nprinciple of separation of concerns because when defining the graph structure we now have to anticipate \nthe need for traver-sals when declaring the mark fields. Second: it makes the program more fragile since \na local traversal function now uses global state. We have to worry about initializing that state, about \npreventing concurrent access to it, etc. A solution with variable functions does not have these shortcomings. \nLet the graph be given by a type Node of nodes and a successor function from nodes to list of nodes. \ntype Node fun succ (x: Node): List[Node] Then the traversal function is formulated as follows. f un traverse(root: \nNode, p: Node + void) = var visited(n: Node): boolean := false fun visit(n: Node) = if not visited(n) \nthen visited(n) := true PO-4 succ(n).forall(visit) visit(root) The visited function is now declared in \nits logical place: the traversal function itself. On the other hand, accesses to visited would likely \nbe slower than if visited was imple- mented as a field in the graph nodes themselves. We can get back \nthe more efficient implementation by declaring visited as a global function: var (n: Node) visited: Boolean \n:= false traverse(root: Node, p: Node + void) = /* as before */ With the new form; we have re-gained \nalong with the efficiency of the old mark bit implementation one of its disadvantages: Mark bits are \nagain global; so we have to worry about multiple uses. Nevertheless, we still have retained some of the \nbenefits of the variable function ap-proach. First, the visited function can be defined in the same module \nas the traversal function; which is not neces-sarily the module where graph nodes are defined. Second; \nthe traverse function itself is as before; it does not need to be changed to reflect the change in implementation. \nThis simplifies the transformation process from program designs to efficient implementations. As another \nexample; consider symbol tables in a com- piler. Some attributes of symbols are universal; we would expect \nthem to be present in any program that uses the symbol table. For instance: we might want to always store \nwith a symbol its name; type, and owner. Other attributes would depend on the concrete application at \nhand. For instance: a backend might want to store an address with a variable or function symbol, whereas \na browser might want to store a list of positions where the symbol is referenced. Some of this flexibility \ncan be ob- tained by polymorphism, in either subtype or parametric form. Nevertheless such flexibility \nrequires planning -the type of symbols has to be made instantiatable, and all program points that create \nsymbols have to be supplied with a factory method [GHJV94] or something equiva-lent to generate the right \nkind of symbol. With variable functions; we can simply keep associations from symbols to additional data \nin the modules where they are main- tained and used. Example: module Symtab type Symbol = case Symbol \n( name: String, tp: Type, owner: Symbol) module Backend var (sym: Symbol) adr: int module XRef var \n(sym: Symbol) uses: List[Position] . . . This use of variable functions in program composition can be \nseen a special case of type adaptation [HB193]; restricted to the case of instance fields. An extension \nto methods seems feasible; but is not part of the current paper. 5 A Calculus for Variable Functions \nThis section gives an operational semantics for a core lan- guage with variable functions and algebraic \ndata types. Variable functions are higher-order, they can be ap-plied and updated with other functions \nas arguments. The syntax of terms is given in Figure 3. There are three alphabets, one for pattern-bound \nvariables x: one for var-bound variable functions f; and one for data constructors F. As operations we \nhave function ap-plication h/l iV: function update f 1M := N and selec-tion Ad.C; where C is a sequence \nof pattern matching clauses PI + El 0 . . . 0 P, + E, 0 fail. Pattern match-ing clauses are tried left \nto right. They always end in a fail case which represents a pattern matching failure. To avoid clutter \nwe usually omit the fail case in program examples. Functions are declared using var D in A4 where D is \na set of (possibly recursive) pattern matching function definitions f C. Function update requires an \nf variable in function po-sition. Hence: the function being updated cannot be a parameter or the result \nof a computation. However; one can always simulate these cases by a pair of setter and get- ter functions. \nAn example; which simulates a first-class reference cell, is shown below. VEU- cell () * 0 setter x * \ncell () := x getter () * cell () in (setter, getter) (For conciseness: we have omitted the 0 fail cases \nin pat- tern matches and have used tupling syntax instead of des- ignated unary and binary tuple constructors.). \nEach of the functions declared in a var declaration is updatable. There is no separate syntax to define \nim-mutable functions -there s no need for it; since the same effect can be achieved by encapsulation. \nFor instance; the term varux*Minu is operationally indistinguishable from the lambda-abstraction Xa.M. \nIn particular: the function defined by that term cannot be updated, since updates are only al-lowed for \nf-named functions, and the name a of the func- tion being defined is hidden in a local scope. Figure \n3 also defines a structural equivalence relation between terms that are taken to be identical. Besides \ncx conversion we have associativity and commutativity of declarations in a var clause. The remaining \ntwo rules allow for scope-extrusion [MPW92] and merging/splitting of scopes. These rules are subject \nto the hygiene condition that bound variables in a term are pairwise different, and Term Structure: Terms \nM;N ::= x Pattern-bound variable I f Function 1 F Ml . ..Mn Algebraic data I MN Application 1 fM:=N Assignment \n1 M.C Selection varDinM Definition Definitions D ::= f C 1 D:D Matchings c ::= fail 1 P*!kf[C Patterns \nP ::= 2 ( f 1 FPl...P, Values v; w ::= f 1 FVl...V, Evaluation Contexts: E ::= [] 1 EN IVE 1 E.C 1 fE:=N \nI fV:=E varDinE Equivalences: Q conversion for 2, f (J in D is associative and commutative E[var D in \nM] E var D in E[M] var DinvarD inM E varD,D inM All equivalences apply to hygienic terms only. Figure \n3: Term structure of core Fun! Application and Update var D7 f C in E[fV] -+ var D, f C in E[V.C] var \nD; fC in E[fV := W] + var D, f(V + W 0 C) in E[W] Pattern Matching V. (x + M 0 C) -+ M[z:=V] f . (f * \nM UC) + M (FVl...Vn).(FP~...P,+M~C) + VI.(PI* . . . v, . (P, =s- M 0 fail * (F VI . . . Vn).C) 0 fai; \n* (F VI . . . Vn).C) V.(PaMjC) -+ V.C in all other cases. Figure 4: Reductions in core Pun! 113 are also \ndifferent from free variables. We require that all terms and parts of terms we write satisfy that condition. \nFigure 4 shows core Fun!% reductions. The first two reductions in the figure have as their respective \nredex a function application and a function update. Application and update can only occur in hole position \nof an evalua- tion context relative to their function definition and their operands have to be values. \nSimilar rules for single-point variables were put forward in [CF91]. Note that the two reduction rules \nfor function ap-plication and function update are symmetrical. Func-tion application transfers a pattern \nmatching case clause from the environment where the function is defined into the evaluation context. \nFunction update takes a binding f V := W: and prefixes the corresponding pattern match-ing clause V + \nI/ to f s definition. Crucial for this technique is the fact that in core Fun! every value is a pattern. \nIn particular function names f are both values and patterns, which means that functions can be tested \nfor equality. Here s an example of a core Fun! program fragment which declares together with a function \ng an equality test for g. var g . eqg x * x. { g * true 0 y * false } It s important to note that such \nan equality test is inten- sional; it will only yield true if the value of its operand x is the function \nname g. One might be concerned that unrestricted intensional equality tests between functions would invalidate \ntoo many program equivalences. But note that it is always possible to recover these program equivalences \nin a type system, by preventing function types in patterns. In delegating the issue of function equality \nto a type system we keep the basic calculus both simple and flexible. Let + be the smallest relation \nthat contains the reduc- tion rules and is closed under arbitrary context formation. Let + denote the \ntransitive closure of -+. Then the fol- lowing propositions can be shown by arguments analogous to the \nones in [CF91]. Proposition 5.1 -+ is confluent: If M -+ Ml and M + -+ MZ then there exists a term MS \nsuch that Ml + MS and A/f, + nfs,. Let t-+ be the smallest relation that contains the re-duction rules \nand is closed under evaluation context for-mation (i.e. if M + N then also E[M] I-+ E[N]). Then we have: \nProposition 5.2 I-$ is deterministic: If M I-+ Ml and M I-+ hl, then Ml q Mz. Proposition 5.3 cs is a \nstandard evaluation relation for -+: If M -+ V then M I+ V. 6 Conclusion This paper has motivated and \nexplained variable func-tions; and has emphasized their role in program verifica-tion. It should be seen \nas the first piece of a much larger puzzle; which aims to turn variable functions into a viable programming \nmethod. If implemented, such a method should be beneficial in several areas. This paper has already presented \napplica-tions to program reasoning and decomposition. But the flexibility afforded by variable functions \npromises to also be very useful in the task of gluing together together in-dependently developed components. \nHijlzle [Ho1931 has argued that the traditional object-oriented structuring method provides insufficient \nsupport for component com-position. Variable functions promise to provide a better framework for this \ntask, and their implementation fits well with techniques for binary component adaptation [KH98]. Acknowledgements. \nThis work was motivated by Egon Borger s talks on evolving algebras given in Karl-sruhe in 1994 and 1996, \nand by John Reynold s POPL keynote address in which he brought back into memory Tony Hoare s statement \nwhich is cited in the first part of the paper. Thanks also to Gary Leavens, John Maraist; David Stoutamire; \nPhilip Wadler and the members and guests of IFIP working group 2.8 for their suggestions and constructive \ncriticism. References [AP94] P. Achten and R. Plasmeijer. Towards dis-tributed interactive programs in \nthe func-tional programming language Clean. In J. Glauert, editor, Proceedings of the 6th International \nWorkshop on the Implemen-tation of Funtional Languages: pages 28.1- 28.16. University of East Anglia, \nNorwich, UK, 1994. [Bak92] Henry G. Baker. Lively linear Lisp- look ma: no garbage! . ACM SIGPLAN Notices, \n27(8):89-98, August 1992. [BBD+96] C. Beierle, E. BSrger, I. Durdanovic, U. GlBsser, and E. Riccobene. \nRefining Ab-stract Machine Specifications of the Steam Boiler Control to Well Documented Exe-cutable \nCode. In J.-R. Abrial, E. Bijrger, and H. Langmaack, editors, Formal Methods for Industrial Applications. \nSpecifying and Pro- gramming the Steam-Boiler Control, number 1165 in LNCS, pages 62-78. Springer, 1996. \n[BijSS] [BR94] [BS94] [Bur72] [Bur82] [CF91] [CH97] [Dij76] [GH93] [GHJV94] [Gir87] [Gur95] A. Bijshna. \nCalculating with pointers. Sci-ence of Computer Programming, 12(3):191-205, September 1989. Egon D, BGrger \nand Dean Rosenzweig. A mathematical definition of full Prolog. Sci-ence of Computer Programming, 1994. \nEgon D. Barger and R. Salamone. CLAM specification for provably correct compilation of CLP(R) programs. \nIn Egon D. Bijrger: editor, Specification and Validation Methods. Oxford University Press; 1994. R. M. \nBurstall. Some techniques for prov-ing correctness of programs which alter data structures. In B. Meltzer \nand D. Michie, ed-itors; Machine Intelligence 7: pages 23-50. Edinburgh University Press; 1972. Warren \nBurton. An efficient functional im-plementation of FIFO queues. Information Processing Letters: 14:205-206, \n1982. Erik Crank and Matthias Felleisen. Parameter-passing and the lambda-calculus. In Proc. 18th ACM \nSymposium on Princi-ples of Programming Languages, Orlando, Florida, pages 233-244, January 1991. Chih-Ping \nChen and Paul Hudak. Rolling your own mutable adt -a connection be-tween linear types and monads. In \nProc. 24th ACM Symposium on Principles of Program- ming Languages: pages 54-67, January 1997. Edsger \nW. Dijkstra. A Discipline of Pro- gramming. Prentice-Hall; Englewood Cliffs, New Jersey; 1976. Yuri Gurevich \nand James K. Huggins. The semantics of the C programming language. In Computer Science Logic; Springer \nLNCS 702, pages 274-309,1993. Erich Gamma: Richard Helm, Ralph John-son, and John Vlissides. Design Patterns \n: Elements of Reusable Object-Oriented Soft-ware. Addison-Wesley, 1994. Jean-Yves Girard. Linear logic. \nTheoretical Computer Science, 50: l-102, 1987. Y. Gurevich. Evolving Algebras 1993: Lipari Guide. In \nE. Bijrger; editor, Specification and Validation Methods, pages 9-36. Oxford Uni-versity Press: 1995. \n[Har79] [HJ89] [HM81] [Hoa71] [Hoa72] [Hoa73] [HB93] [HPW92] Pug851 [HW73] [KH98] 115 David Harel. First-Order \nDynamic Logic, volume 68 of Lecture Notes in Computer Sci-ence. Springer Verlag, 1979. C. A. R. Hoare \nand Cliff B. Jones. Essays in Computing Science. Prentice-Hall Inter-national, London, 1989. Robert Hood \nand Robert Melville. Real-time queue operations in pure Lisp. Information Processing Letters, 13:50-53, \n1981. C.A.R. Hoare. Proof of a program: Find. Communications of the ACM, 14(1):39-45, 1971. C. A. R. \nHoare. Proof of correctness of data representations. Acta lnformatica: 1:271-281, 1972. C. A. R. Hoare. \nHints on programming lan-guage design. Stanford Artificial Intelligence  Laboratory Memo AIM-224 or \nSTAN-CS-- 73-403: Stanford University, Stanford, Cali- fornia: December 1973. Urs HSlzle. Integrating \nindependently- developed components in object-oriented languages. In Object-Oriented Program-ming 7th \nEuropean Conference ECOOP 93 Kaiserslautern, Germany, Proceedings, vol-ume 707 of Springer Verlag, Lecture \nNotes in Computer Science; pages 36-56; 1993. Paul Hudak, Simon Peyton Jones, and Philip Wadler. Report \non the programming lan-guage Haskell: a non-strict, purely functional language, version 1.2. Technical \nReport YALEU/DCS/RR-777, Yale University De-partment of Computer Science: March 1992. John Hughes. Lazy \nmemo-functions. In Proceedings, Functional Programming Lan-guages and Computer Architecture, Nancy, France, \npages 129-146. Springer-Verlag, September 1985. Lecture Notes in Computer Science 201. C. A. R. Hoare \nand Niklaus Wirth. An ax-iomatic definition of the programming lan-guage Pascal. Acta Informatica, 2~335-355; \n1973. Ralph Keller and Urs Hiilzle. Binary com-ponent adaptation. In Proc. European Con-ference on Object-Oriented \nProgramming, Springer Lecture Notes in Computer Science; July 1998. [LM92] [Ma&#38;81 Wg891 W@W [Mii193] \n[Mii197] [MOTW95] [MPW92] [MT911 [Ode921 [Oka97] Patrick Lincoln and John Mitchell.. Opera-tional aspects \nof linear lambda calculus. In Seventh Annual IEEE Symposium on Logic in Computer Science, Santa Crux, \nCalifor-nia: pages 235-246, Los Alamitos, California, June 1992. IEEE Computer Society Press. I. A. Mason. \nVerification of programs that destructively manipulate data. Science of Computer Programming, 10:177-210, \n1988. Eugenio Moggi . Computational lambda-calculus and monads. In Proceedings 1989 IEEE Symposium on \nLogic in Computer Sci-ence; pages 14-23. IEEE; June 1989. Eugenio Moggi. Notions of computation and monads. \nInformation and Computation, 93235-92, 1991. Bernhard Mailer. Towards pointer algebra. Science of Computer \nProgramming, 21, 57- 90 1993. Bernhard Mijller. Calculating with pointer structures. In Proc. IFIP TC2/WGd \n1 Working Conference on Algorithmic Lan-guages and Calculi, Le Bischenberg, France, 1997. John Maraist: \nMartin Odersky, David N. Turner, and Philip Wadler. Call-by-name, call-by-value, call-by-need, and the \nlinear lambda calculus. Electronic Notes in The-oretical Computer Science; l(1): 1995. Robin Milner, \nJoachim Parrow; and David Walker. A calculus of mobile processes, I + II. Information and Computation, \nlOO:l-77, 1992. Ian Mason and Carolyn Talcott. Equiva- lence in functional languages with side ef- fects. \nJournal of Functional Programming, 1(3):287-327, July 1991. Martin Odersky. Observers for linear types. \nIn B. Krieg-Briickner: editor, ESOP 92: 4th European Symposium on Programming, Rennes, France, Proceedings, \npages 390-407. Springer-Verlag, February 1992. Lecture Notes in Computer Science 582. Chris Okasaki. \nCatenable double-ended queues. In Proc. International Conference on Functional Programming, pages 66-74. \nACM, June 1997. [ORH93] PM4 [SBL+Sl] [SDDS86] [SF931 [WadSOa] [WadSOb] [Wad921 [WH66] Martin Odersky, \nDan Rabin, and Paul Hu-dak. Call-by-name, assignment, and the lambda calculus. In Proc. 20th ACM Sum-posium \non Principles of Programming Lan-guages, pages 43-56; January 1993. John C. Reynolds. Reasoning about \narrays. Communications of the ACM, 22:290-299, 1979. R. Strom, D. Bacon, A. Lowry, A. Goldberg, D. Yellin, \nand S. Yemini. Hermes: A Lan-guage for Distributed Computing. Prentice Hall, February 1991. ISBN O-13-389537-8. \nJ. Schwartz, R. Dewar, E. Dubinsky, and E. Schonberg. Programming with Sets: An Introduction to SETL. \nSpringer-Verlag, 1986. Amr Sabry and John Field. Reasoning about explicit and implicit representations \nof state. In SIPL 93 ACM SIGPLAN Workshop on State in Programming Lan-guages, Copenhagen, Denmark, pages \n17-30, June 1993. Yale University Research Report YALEU/DCS/RR-968. Philip Wadler. Comprehending monads. \nIn Proc. ACM Conf. on Lisp and Functional Programming, pages 61-78, June 1990. Philip Wadler. Linear \ntypes can change the world! In M. Broy and C. Jones, editors, IFIP TC 2 Working Conference on Program- \nming Concepts and Methods, Sea of Galilee, Israel, pages 347-359. North Holland, April 1990. Philip Wadler. \nThe essence of functional pro-gramming. In Proc.19th ACM Symposium on Principles of Programming Languages, \npages l-14, January 1992. Niklaus Wirth and C.A.R. Hoare. A contri- bution to the development of ALGOL. \nCom-munications of the ACM, 9(6):413432,1966. \n\t\t\t", "proc_id": "289423", "abstract": "What is a good method to specify and derive imperative programs? This paper argues that a new form of functional programming fits the bill: where <i>variable functions</i> can be updated at specified points in their domain.Traditional algebraic specification and functional programming are a powerful pair of tools for specifying and implementing domains of discourse and operations on them. Recent work on evolving algebras has introduced the function update in algebraic specifications, and has applied it with good success in the modelling of reactive systems. We show that similar concepts allow one to derive efficient programs in a systematic way from functional specifications. The final outcome of such a derivation can be made as efficient as a traditional imperative program with pointers, but can still be reasoned about at a high level.Variable functions can also play an important role in the structuring of large systems. They can subsume object-oriented programming languages, without incurring the latter's problems with pointer aliasing and modularity.", "authors": [{"name": "Martin Odersky", "author_profile_id": "81100056476", "affiliation": "University of South Australia", "person_id": "PP14030830", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/289423.289433", "year": "1998", "article_id": "289433", "conference": "ICFP", "title": "Programming with variable functions", "url": "http://dl.acm.org/citation.cfm?id=289433"}