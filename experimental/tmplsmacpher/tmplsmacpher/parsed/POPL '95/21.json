{"article_publication_date": "01-25-1995", "fulltext": "\n Unification Factoring for Efficient Execution of Logic Programs* S. Dawson C.R. Ramakrishnan I.V. Ramakrishnan \nK. Sagonas S. Skiena T. Swift D.S. Warren Department of SUNY at Stony Brook, Computer Science Stony Brook \nNY 11794-44oo {sdawaon, cram, ram, kostis, skiena, tsirift, warren} ~cs. srmysb. edu Abstract The efficiency \nof resolution-based logic programming g lan\u00adguages, such as Prolog, depends critically on selecting and \nexecuting sets of applicable clause heads to resolve against subgoals. Traditional approaches to this \nproblem have fo\u00adcused on using indexing to determine the smallest possible applicable set. Despite their \nusefulness, these approaches ignore the non-determinism inherent in many programming languages to the \nextent that they do not attempt to optimize execution qfter the applicable set has been determined. Unification \nfactoring seeks to rectify this omission by re\u00adgarding the indexing and unification phases of clause \nreso\u00adlution as a single process. This paper formalizes that pro\u00ad cess through the construction of factoring \nautomata. A polynomial-time algorithm is given for constructing opti\u00ad mal factoring automata which preserve \nthe clause selection strategy of Prolog. More generally, when the clause selec\u00adtion strategy is not \nfixed, constructing such an optimal au\u00adtomaton is shown to be NP-complete, solving an open trie minimization \nproblem. Unification factoring is implemented through a source code transformation that preserves the \nfull semantics of Pro\u00adlog. This transformation is specified in the paper, and using it, several well-known \nprograms show performance improve\u00adments of up to 10070 across three different systems. A prot o\u00adtype \nof unification factoring is available by anonymous ftp. 1 Introduction In logic progr amming languages, \nsuch as Prolog, a predicat e is defined by a sequence of Horn clauses. When resolving a goal, a clause \nbecomes applicable if its head unifies with the goal, and each applicable clause is invoked in textual \norder. Unification of a clause head with a goal involves two ba\u00adsic operations: elementary match operations \nand computing substitutions for variables in the two terms. When there are common parts among the clause \nheads, it should be pos\u00adsible to share the basic operations corresponding to these This work wss supported \nin part by NSF grants CCR-9102159, CCR-91029S9, CCR-9404921, CDA-9303181, INT-9314412, and ONR grszt \n400X116YIP01. Permission to copy without fee all or part of this material is granted provided that the \ncopies are not made or distributed for direct commercial advantage, the ACM copyright notice and the \ntitle of the publication and its date appear, and notice is given that copying is by permission of the \nAssociation of Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific \npermission. POPL 951/95 San Francisco CA USA @ 1995 ACM 0-89791-692-1/9510001 ....$3.50 common parts \nand do them without redundancy. Develop\u00ading such techniques is a problem of considerable importance for \nefficient evaluation of logic programs. Traditionally this optimization is viewed as a two-phase process. \nThe fist phase, known as the indexing phase, ex\u00adamines non-variable parts of the goal and clause heads \nto compute a match set which is a superset of all unifiable clauses. While indexing essentially does \nmat ch operations, the substitutions are computed in the second, or unifica\u00adtion, phase when the goal \nis nnfied with the clauses in the mat ch set. For example, indexing yields the three clauses {~(a, h \nc),~(a, h d),~(a~ C, c)} for the ca~ ~(a) x, Y) on the predicate in Figure 1a. In the unification phase \neach of the three clauses is unified in textual order. In this phase six substitutions are computed \nthree for each of the two vari\u00adables. In addition three match operations are performed requiring rescanning \nof terms already seen during indexing. But observe that it snflices to compute only two substitu\u00adtions \nfor X and three for Y. Furthermore repeating the mat ch operation is unnecessary. Thus the efficiency \nof uni\u00adfication of clause heads with the goal can be considerably enhanced by sharing the unification \noperations not only in the indexing but also in the unification phase. Although techniques for sharing \ntests needed for com\u00adputing match sets have been extensively researched (such as indexing techniques \nfor logic programming g, e.g., see [2, 5, 6, 8, 9, 14]; pattern mat thing for functional and term rewrit\u00ading \nsyst ems e.g., see [1 I]; and decision trees for concurrent logic languages e.g., [ 7, 12]), optimizing \nthe sharing of uni\u00ad fication operations has not been explored. Extant indexing techniques in logic programming \ng, on completing indexing, unify each clause head in the match set separately with the goal. That is, \nexecution after indexing is not optimized. Even rescanuing parts already seen during indexing is sel\u00addom \navoided (see [2, 5]) since in general this requires either large indexing structures (e.g., the switching \ntree of [6]) or elaborate information to be maintained and manipulated at run time. In any case since \neach clause head is unified separately with the goal, the other problem of sharing sub\u00adstitutions still \nremains. Rather than viewing head nn.i.tication as two separate and independent stages we regard it as \na single process in which both the indexing and unification phases are blended. We present a technique, \ncalled unification factoring, to unify clause heads efficiently with any arbitrary goaL In contrast to \nmatching trees (e.g., [6, 14]), the technique presented here does not rely on mode information. Unification \nfac\u00adtoring transforms the source program into another in which the basic unification operations common \nto clause heads and the goal are ~actored cwt; i.e., they can all be shared. For instance, the program \nin Figure la can be transformed into the program in Figure lb by unification factoring. Now the call \np(a, X, Y) on this transformed predicate will result in only one match operation and compute two substitutions \nfor X; the three substitutions computed for Y remain un\u00adchanged. Observe that in the transformed program, \nthe match operation is shared and only the needed substitutions are computed for X. p(a, X,Y) :-pl(x, \nY). p(b, a,c) . p(a,b, c) . pl(b, X) :-p2(x). p(a,b, d) . pl(c, x) :-p3(x). p(a, c,c). p2(c) . p(b,a,c). \np2 (d). p3(c) . (a) (b) Figurel: Original predicate (a)and transformed version(b) Unification factoring \nis a uniform technique that can en\u00adhance the overall efficiency of unifying clause heads with a goal \nby optimizing not just the matching operations (as is done by indexing techniques) but also other operations \n(such as computing substitutions). In other words, it can handle non-deterministic execution more efficiently. \nMore\u00adover, since there is no division into two separate phases, the difficult interface problem of eliminating \nrescans of terms no longer exists. In general there are several different ways to factor out unification \noperations in a program, yielding transformed programs with di.i7ering performance. The in\u00adteresting \nproblem then is the design of optimal unification factoring, i.e., one that results in a program that \nhas the best performance. We propose a solution to this problem. In contrast to existing indexing techniques, \nwhich require compiler and/or engine modifications, unification factoring is implemented as a source-to-source \ntransformation needing no engine changes. We develop the technique of unification factoring in two parts. \nIn the first part we construct a $zc\u00adtoring automaton that models the process of unifying a goal with \na set of clause heads as is done in the WAM (see Sec\u00adtion 2). Common unification operations are factored \nout by this automaton. The second part constitutes the algo\u00adrithm for transfortig this automaton into \nProlog code (see Section 4). Summary of Results 1. We describe an algorithm for constructing (at compile \ntime) an optimal factoring automaton that faithfully models Prolog s textual order-preserving clause \nselec\u00adtion strategy. We exploit this strategy for construct\u00ading an optimal automaton in polynomial time \nusing dynamic programming (see Section 3). 2. We show that, on relaxing the order-preserving strat\u00adegy, \nconstruction of an optimal automaton becomes NP-complete (see Section 3). This result solves a trie \nminimization problem left open by Comer and Sethi [4]. 3. We provide experimental evidence that our \ntransfor\u00admation can consistently improve speeds of Prolog pro\u00adgrams by factors up to 2 to 3 on widely \navailable Prolog  systems, namely, Quintus, SIC Stus, and XSB (see Sec\u00adtion 5). Our results also indicate \nthat, although the transformation can in principle increase code size by at most a constant factor, in \npractice this increase is never more than 107o, and in fact, code space decreases in some cases. 2 Unification \nFactoring The fact oring automaton decomposes the unification process into a sequence of elementary unification \noperations that model instructions in the WAM. It is structured as a tree, with the root as the start \nstate, and the edges, denoting transitions, representing elementary unification operations. Each transition \nis associated with the cost of nerformine. . the corresponding operation. Every leaf state represents \na clause, and the transitions on the path from the root to a leaf represent the set of elementary operations \nneeded to unify the head of that clause with a goal. The total cost of all these transitions is the cost \nof this unification. Common edges in the root-to-leaf paths of two leaves represent com\u00admon operations \nthat are needed to unify the goal with those two clauses. Sharing the operations associated with com\u00admon \nedges thus amounts to factoring the process of unifying the goal with the clause heads. Not e that, since \nthe tran\u00adsitions represent unification operations, all possible transi\u00adtions out of a state are attempted; \ni.e., the automaton is non-deterministic. In the following, we formalize the notion of factoring automaton. \nOur formalization is inspired by work on pattern matching tries in [1]. In the next section we describe \nthe construction of optimal automata. The Factoring Automaton We assume the standard definitions of term, \nand the notions of substitution and subsumpt ion of terms. A position in a term is either the empty string \nA that reaches the root of the term, or ~.i, where r is a position and i is an integer, that reaches \nthe ith child of the term reached by ~. By tlm we denote the symbol at position z in t.For example, p(a, \nj(X))lZ.1 = X. We denote the set of all positions by It. Terms are built horn a finite set of function \nsymbols 3 and a countable set of variables V u V, where ~ is a set of position variables. The variables \nin the set $ are of the form X=, where T is a position, and are used simply as a notational convenience \nto mark certain positions of interest in a term. The symbol t(possibly subscripted) denotes terms; a, \na , . . . denote elements of the set ZUV; -y, -y ,... denote elements of the set f U V U lI; and $, g, \nh denote function symbols. The arity of a symbol a is denoted by note that the aritg(a); arit y of variable \nsymbols is O. Simultaneous substitution of a term t at a set of positions P in term t is denoted t[F \n+ t ].For example, p(X1, f(xz.l),xs)[{z.l,s} + b] =  p(x,, f(b), b). A factoring automaton performs \nunification as a series of elementary unification operations. At each stage in the computation we need \nto capture the operations that have been performed, as well as those that remain to be done. We use \nthe notion of skeleton, which is a term over 7 U V U j, to denote this partial computation. Elements \nof 3 U V in a skeleton represent unification operations that have been performed. Position variables \ndenote portions of the goal where the remaining operations will be performed. Given a skeleton its j%inge \ndefines the positions to be explored for unification to progress. Formally, Definition 2.1 (Skeleton \nand I? Ange) A skeleton is a term over Z U V U V. The fringe of a skeleton S, denoted j%nge(S), is the \nset of all positions T such that Slm = X. and X%cy. For example, the skeleton q(Xl, g(xz.1, X2.3, X2.3)) \nfor the gord q(f(U), W) captures the fact that the substitution for W has been partially computed (to \nbe g(xz.1, X2.3, XZ.S)), and that the first argument of the goal has not yet been explored. The fringe \nof this skeleton is {1, 2.1, 2.3}. Each state in the automaton represents an intermediate stage in the \nurtiiication process. With each state is associ\u00ad ated a skeleton and a subset of clauses, called the \ncompatible set of the state. The clause heads in the compatible set share each unification operation \ndone on the path from the root to that state. Hence, each clause head in the compatible set is subsumed \nby the skeleton of that state. Recall that the fringe of a skeleton represents the positions in the par\u00adtially \nunitied goal that remain to be explored. A state then specities one such position, and each outgoing \ntransition rep\u00adresents a unification operation involving that position. We label the transition unify(r, \n-y), where -y is either a function symbol or variable in the clause head, or another position in the \n(partially unified) goal. (Positions in the label are prefixed with $ to avoid confusion with integers.) \nFor example, in Figure 2a the label on the transition from S1 to 52 specifies unifying position 1 of \nthe goal with a. The compatible set for state S2 is {p(a, b, c), p(a, b, d), p(a, c, c)}, and clause \nheads in that set share the operation wzifi($l, a). In Figure 5b the label on the transition fkom S1 \nto sz specifies unifying positions 1 and 2 in the goal, while the label on the transition from sz to \nSE specifies unifying position 2 in the goal with variable X (in the head of clause 1). A transition \nfrom a state indicates progress in the uni\u00adfication process. For a transition labeled uni.fy(r, y), the \nskeleton of the destination state is obtained by extending the skeleton S of the current state using \nthe extension op\u00aderation eaterui(S, x, y) defined below. Intuitively, S is ex\u00adtended by replacing all \noccurrences of Xx in S by the term corresponding to 7. If 7 is a function symbol, this term has 7 as \nroot and position variables representing new fringe po\u00ad sitions as its children. If 7 is a position, \nthis term is the position variable X7. Otherwise, this term is the variable 7 itself. Definition 2.2 \n(Skeleton extension) The e~tension of skeleton S at fringe position x by ~, denoted eztend(S, z, 7), \nis the skeleton S such that s =S[P+ t] where P = {n I Sl=l= Xm} 7(xm.1,....x=,=r~tv(7))(7~~) and t= X7 \n(-i~Jq {7 (7=w For example, in the skeleton q(Xl, g(xa.1, X2.3, X2.3)), the operation uni~$ 1, f/1 ) \nresults in extension of the skeleton to g(t(xl.1 ), 9(x2.1,xz.t,X2.3)). This skeleton is fmther extended \nas a result of the operation unifi($l.1, $2.1) to df(x2.1), dx2.1, X2.3, X2.3)). We now formally define \nthe factoring automaton as fol\u00adlows: Definition 2.3 (Factoring Automaton) A ~actoring au\u00ad tomaton for \na set of clauses C and a skeleton S is an or\u00addered tree whose edges are labeled with unifg(r, 7), and \nwith each node s (a state) is associated a skeleton S., a position T, E fringe(S, ) (if s is not a leaf), \nand a non-empty comp\u00adatible set C . ~ C of clause heads such that: 1. Every clause head in G , is subsumed \nby S., 2. the root state has S as the skeleton and C as the com\u00ad  patible set, 3. for each edge (s, \nd) with label uni~z,, 7), Sd = eztend(S~, T., 7), and 4. the collection of sets {cd I (s, d) is. . . \n. an edge}-. is a par\u00ad tition of C.. The partitioning of the compatible set C, at a fringe position \nr. in the above definition ensures that transitions specify unification operations involving r. in the \ngoal, and either: 1) a function symbol or variable appearing at ~, in at least one of the clause heads \nin C8; or 2) another (fkinge) position in the goal. Each set in the partition is a compatible set of \none of the next states of S,, and all the clause heads in it share the unification operation specified \nby the corresponding transition. Construction Using the programs in Figures la and 5a and the corresponding \nautomata in Figures 2a and 5b for illus\u00adtration, we informally describe the construction of a fac\u00adtoring \nautomaton. For a predicate p/n an automaton is built incrementally starting with the skeleton p(Xl,..., \nXn) for the root state S1 and the set of clauses defining p/n as its compatible set. From a given state \ns we expand the automaton as follows. We tirst choose a position r. from the flinge of its skeleton S,. \nWe then partition the compatible set c, into sets cdl, ..., C&#38; such that in each Cdi, an clause heads \nspecify the same unification opera\u00adtion, uni~x., 7;), at z.. For example, at state sz in Fig\u00adure 2a, \nthe compatible set @(a, b, c), p(a, b, d), p(a, c, c)} is partitioned into {p(a, b, c), p(a, b, d)}, \nwhich share the unifi\u00adcation operation unifi($2, b), and {p(a, c, c)}, which has op\u00aderation unifg($2, \nc). We crest e new states dl, . . . . dk such that for each state di, Cdi is its compatible set, S~i \n= entend(s,, T., 7;) is its skeleton, and the edge (~, di) is the transition into di, labeled with unify(r,, \nYi ). The process of expanding the automaton is repeated on all states that have non-empty fringes. To \npartition the clauses, we identify the set of possible unification operations for each clause head at \na given fringe position r. For a linear clause head t,the only possible uni\u00adfication operation involves \nthe symbol at position z in the clause head, i.e., uni~(z, tin). For a non-linear clause head, there \nis an additional possible operation for each friige posi\u00adtion in the head having a term identical to \nthat at position x. That is, for each fringe position x # x, such that the sub\u00adterms rooted at r and \nz are identical, unify(r, r ) is also a possible operation. Two clause heads may then be included in \nthe same partition if7 their respective sets of possible uni\u00adfication operations contain an identical \noperation. 1 Variable symbols in distinct clauses are assumed to be distinct. s,:1 P(x,.X2 .X3) S2: \n$3: p(a,x~,xj) 2 p(&#38;Y2,xj) 2  Q,1:* ) unifi($2,b) unifl($2,c) unifi($2,a) unfy($I,a) I uilifi($3,c) \nI unrfi($l,b)l S4: Sj: 6: 3 Pfa, b,xj) p(a,c, X3) 3 p(b,a,Xj) 3 unifi($3, c) unify($3,d) unlfy($3,c) \nunifi($3, c) p(a,b,c) CI p(a,b,d) c2 p(a,c,c) C3 p(b,a,c) c4  AIYi!)i (a) Figure 2: Factoring automata \nOperation Unification of a goal with the clause heads be\u00adgins at the root of the automaton. From each \nstate, a tran\u00adsition is made to the next state by performing the specified unification operation on the \npartially unified goal. If more than one transition is possible, the first such transition is made, and \nthe remaining transitions are marked as p end\u00ading. When a leaf is reached, the body of the corresponding \nclause is invoked. Whenever failure occurs, either within the automat on (due to failure of a unification \noperation), or dur\u00ading execution of a clause body, the automaton backtracks to the nearest state with \npending transitions. The process then continues wit h the next pending transition. The automaton fails \non backtracking when there are no states with pending transitions. For example, in Figure 2a for the \ngoal p(a, b, .X), only the transition labeled zmifsJ$ 1, a) is possible from state S1. Sim\u00adilarly, from \nstate sz only the transition labeled zm.ify($2, b) is possible. At state 9A, since both outgoing transitions \nfrom this state are possible, the transition labeled u7zi~y($3, d) is marked as pending, and the transition \nlabeled uni~$3, c) is made, thus unifying X with c and invoking the body of clause 1. Upon backtracking \n(to state .94), the transition labeled unifg($3, d) is taken, unifying X with d and invok\u00ading the body \nof clause 2. Observe that unifications of the goal with the heads of clauses 1 and 2 share the operations \nof unifying argument 1 with a and argument 2 with b. The above informal description of the automaton \ns operation can be formalized and its soundness and completeness can be readily established; these are \nroutine and omitted. Not e that a factoring automaton as defined above does not adhere to the order-preserving \nclause selection strat\u00ad egy of Prolog. For example, on query p(X, Y) the answers computed by the program \nin Figure 3a appear in the or\u00adder (p(a, b), p(b, c), p(a, d)), while the factoring automaton in Figure \nthe order each state clauses as lection of concept of Definition sequential of clauses 3b for the same \nprogram computes the answers in (p(a, b), p(a, d), p(b, c)). To preserve clause order, in the automaton \nmust consider its compatible a sequence and partition this sequence into a col\u00adsubsequences. For this \nwe introduce the following sequential factoring automaton (SFA). 2.4 (Sequential Factoring Automaton) \nA factoring automaton for is a factoring automaton to-right preorder traversal of A, i+l, forl~i <n. \nFigure 3C shows an SFA for the a sequence (cl, cz, . . . , cm) A such that, in a left\u00ad leaf i is visited \nbefore leaf program in Figure 3a. We for program in Figure 1a refer to factoring automata that are not \nsequential as con\u00adsequential factoring automata (NSFA). 3 Optimal Automata The cost of unifying a goal \nwith the clause heads depends on both the goal and the automaton with which the unifica\u00adtions are performed. \nFor instance, for the goal p(a, X, c), the automaton given in Figure 2a performs three matches and two \nbindings, whereas the automat on in Figure 2b performs five matches and three bindings. The unification \nof the goal P(X, b, c), on the other hand, requires three matches and two bindings in the automaton of \nFigure 2a, whereas the automaton in Figure 2b performs only two matches and one binding. Thus, the relative \ncosts of aut omata vary with the goal. We assume no knowledge of the goal at compile time, and hence, \nwe choose the worst case performance of an au\u00adtomat on as the measure of its cost. An optimal automaton, \nhence, is one with the lowest worst-case costz. We exploit the order-preserving clause selection strategy \nof SFAS for construct ing an optimal SFA in polynomial time. On the other hand, when clause order is \nnot preserved, as in an NSFA, we show that constructing an optimal automaton is NP-complete. Optimal \nSFA The following three readily established properties of an op\u00adtimal SFA are used in its construction. \nProperty 1 Each sub-automaton is optimal for the clauses in the compatible set and skeleton associated \nwith the root of the sub-automaton. Property 2 Any unification that can be shared will be shared. That \nis, in an optimal SFA, no two adj scent transitions horn any given state have the Property 3 kmsitions \nhorn a state partition ible set of clauses into subsequences. To construct an optimal automaton for a \nsequence C and initial skeleton S, we the fringe of S, the least cost structed by first inspecting that \n Whereas an optimal factoring number of unification operations decision tree minimizes the number one \nclause. consider, for each automaton that same label. its compat\u00ad of clauses position in can be con\u00ad \nposition. From Properties automaton minimizes the total over all clause heads, an optimal of tests needed \nto identify any  s,: s,: fix,,X2 1 P(X, ,X2 1 un1jjf$I,17) unifi($l, b) unifi($l,a) unify($l,b) unifv($l,a) \np(a,b). s,: sq: s,: S3: $4: p(b, c). p(a, XJ 2 Nb,X2) 2 p(ll x;) 2 P(hX2) 2 P(4XJ 2 p(a,d). unifj($2,b) \nunifi($2,c) unifl($2,d)unijj($2,b) unifi($2,d) unifi(.$2,c) P(L3,b) Ci P(c2.d) C3 p(b, C) C2 />(a, b) \nCl p(b, C) c? P(CL d) C3 7c 7fm (a) (b) (c) Figure 3: Predicate (a) and non-sequential (b) and sequential \n(c) automata and 3 it follows that the transitions out of a state and their Definition 3.2 (Common) Given \na skelet on S, a sequence order are uniquely determined by the position chosen, Since (t,,.,.,clause \nheads, and a pair of integers tn)of (i, i ), these transitions represent the partition of the compatible \n1 < i < i < n, common(S, i,i ) is a pair (E, S ), where set of clauses, to construct an optimal automaton \nfor a given E represents the set of unification operations common to start position, we first compute \nthis partition. We then con-,t;,},S extended {ti,...and is the skeleton: struct optimal automata for \neach sequence of clauses in the partition, and the corresponding extended skeletons. From common(S, i, \ni ) = Property 1 it follows that these optimal sub-automata can (1? U {(m, a)}, S ), if 3T E fringe(S) \nsuch that be combined to produce an optimal automaton for the se\u00ad tj19r=Cl,i<~< i , lected start position. \nThe lowest cost automaton among the where (E, S ) = automata constructed for each position is an optimal \nSl?A common(eztend(S, m, a), i, i ) for C and S, The recursive construction sketched above ({}, S), otherwise \nlends itself to a dynamic programming g solution. Below we formalize the construction by first considering \nlinear clause For example, given skeleton S = p(X1, X2, X3) and the heads. Construction in the presence \nof non-linear clause sequence (p(a, b, c), p(a, b, d), p(a, b, e)), cornrnon(S, 1, 3) = heads is discussed \nlater. ({(l, ~), (z, ~)},d%b, Xs)). The worst case cost of an SFA is when all transitions Linear clause \nheads We use a function part to partition are taken. Assuming that all elementary unification opera\u00adthe \nclause sequence C into the minimum number of subse-tions have unit cost, the cost of an optimal SFA for \nclause quences that share unification operations at this position. sequence (Ci, . . . . Ci, ) and skeleton \nS is expressed by Equa\u00adtion 1 in Figure 4. Note that IL?] is the number of com-Definition 3.1 (Partition) \nGiven a sequence (tl,..., i!n) mon unification operations for a subsequence in a partition. of clause \nheads corresponding to the sequence of clauses Also note that the recurrence assumes that subsequence \nC, apair ofintegers (i,i ), 1< i < # < n, and aposi\u00ad (t,,...,)has no common part with respect to skeleton \nS. tit  tion r, the partition of C by T, denoted part(i, i , z), is the Thus, the cost of an optimal \nautomaton for a predicate p/m set of triples (a, j, j ), i < j < j < i , such that j and consisting of \nn clauses is given by IEI + simpte.cost(l, n, S), j are the end points of a maximal subsequence of clause \nwhere (E, S) = common(p(Xl, . . . . X-), 1, n). heads in(ti,..., ti()having symbol a at position z. That \nis, Using Equation 1, it is straightforward to construct an (a, j,j ) 6 part(i, i ,7r) iff optimal SFA \nbased on dynamic prograrnmin g, where an op\u00adtimal automaton for each subsequence of clauses is recorded \n1. for a~j< k <j , tklm=~, in a table. A complete example of this construction is given in the appendix. \nNote that, since ....has no common (t;,tit) 2. either j = ior tj_llm# a,and  part with respect to \nthe skeleton, a skeleton S is uniquely de\u00ad 3. either j = i or tj,lm# a. termined, given i and i . Hence, \nthe cost recurrence has only two independent parameters, i and i , and the table used in Each triple \n(a, j, j ) computed by part represents a transi\u00ad the dynamic programming g algorithm will be indexed \nby i tion associated with the unification operation involving x and i . and a. For example, the partition \nof the sequence of clause Given n clauses with at most m symbols in each clause heads (p(a, b),p(b, c),p(a, \nd)) at position 1 is the set of sub\u00ad head, the number of possible subsequences is 0(n2 ), and sequences \n{(p(a, b)), (p(b, c)), (p(a, d))}. The skeleton of the hence, the number of table entries is 0(n2 ). \nThe number next state resulting from the transition is eztend(S, z, a). of partitioning positions considered \nfor each entry is O(m).The compatible clauses of this state form the subsequence For each position, part \nrequires O(n) time. Identification of (Cj,..., Cjl). common operations for all subsequences in a partition \ncan be For each subsequence in a partition there are one or more accomplished in O(m) time via a precomputed \nmatrix (that fringe positions in the skeleton with a symbol common to requires O(rnn) time and space). \nThus, the time required all clause heads in the subsequence. From Property 2 it to compute one entry \nin the table is O(m(n + m)), and the follows that the unification operations at these positions will \noverall time needed to compute an optimal automaton is be shared by all clause heads in the subsequence. \nWe use 0(n2 rn(n + m)). Therefore, the function common to identify such operations and extend the skeleton \nto record their effect: Theorem 9.3 An optimal SFA can be constructed in pollJ\u00adnomial time. sirnple-cost(i, \ni , S) = ~GfiW&#38;~)( ~ (s~wze-co~~(j,i, S )+ Ii?/, where (E, S )= cornrnon.(S,j,j ))) (1) (-!i!J )epd(i, \nt , m) rejined-cost(i, i , S) = ~G**e(~)(costcko*ce(x) + ~ (refined.mst(j, j ,S ) + ~ cogtunz~(~ , ~ \n))) (2) (m$j<i )G (+,cz )cE part(i,it,m) where (E, S ) = conwn.on(S,j,j ) Figure 4: Simple (1) and refined \n(2) recurrences for cost of optimal SFA i s,: pfx1,x2%) S2: S2: p(x,x, Y) :\u00adP(xz>xzX3 P(X28ZJ>X3 p(x,X, \na) . pl(x,Y) . p(Y,Y,b) . Unlfy($z,x) unlfy($2,X) p(u, v,w) . p(u, v,w) . pl(X, a) . ~: $4: pl(X, b). \n unifi($3,a) untfi($3,a) (a) (b) (c) (d) Figure5: Non-bear pre&#38;cate (a), automata (b)and(c), andtrasformation \n(d) In Equation 1 for the cost of an optimal SFA, it is as\u00adsumed that all unification operations have \nunit cost, and that recording pending transitions has zero cost. The re\u00adcurrence can be modiiied as follows \nto account for these costs. Let cost~~it~(~~~) be the cost of the ~cation oper\u00adation involving position \nr and a and costchoice(m) represent the cost of choosing a transition when multiple transitions are possible \nat position r. Note that the cost of recording pending transitions can be modeled using costchoice(r). \nAc\u00adcounting for these costs, an optimal SFA can be computed using Equation 2 (Fig. 4). Nonlinearity \nWith suitable modification of fimctions part and common,, Equations 1 and 2 can be used even in the presence \nof nonlinear clause heads. Consider the program in Figure 5a. The dynamic progr amming algorithm for \nlin\u00adear clause heads yields theautomaton in Figure5b. Observe that this automaton does not share the \noperation that uni\u00adfies arguments 1 and 2 that is common to clauses 1 and 2. These operations can be \nshared by considering relationships among different positions within a clause head. For this we view \neach clause head as a set of equations. For example, the three clause heads in Figure 5a correspond to \nthe following three sets of equations: p(X, X,a), a {Xl= X, Xz=X, Xs=a, X~ =X,} p(y, Y, b). + {x, =Y, \nx,= Y,x, =b, x,=x,} p(u, v, w). -+ {X, =u, x,=v, x,=w}. The unification operations involving position \n1 are Xl = X andXl =X2forclause1,Xl =YandXl =X2for clause 2, and Xl = U for clause 3. Thus, the minimum \npartition based on position 1 splits the clauses into two sets: the first consisting of clauses 1 and \n2, with the correspond\u00ading unification operation Xl = X2; and the second consist\u00ading of clause 3, with \nthe operation X1 = U. Using part and common functions that partition and identify common oper\u00adations \nbased on equation sets yields the optimal automaton in Figure 5c. Note that the size of the equation \nset corre\u00adsp onding to each clause head is quadratic in the number of symbols in the head. Thus, part, \ncommon, and the dynamic prograrmuin g algorithm still remain polynomial in the worst case. For nonlinear \nheads we need to estimate the cost of uni\u00adfying two positions in the goal. In general this cost is un\u00adbounded, \nsince the cost of unifying two positions in a term depends on the sizes of the subterrns. Hence, there \nis no suitable measure for estimating the worst-case cost of an automaton. Nevertheless, if the size \nof terms is bounded (as in Dat alog programs), Theorem 3.3 still holds. Optimal NSFA Although Properties \n1 and 2 hold in an optimal NSFA, Prop\u00adert y 3 does not. In particular, the transitions from a state can \npartition its compatible clauses into subsets. Construct\u00ading an opt irnal NS FA may require enumerating \noptimal au\u00adtomata for a large number of subsets of clauses for each position associated with a state. \nIn fact, we show: Theorem s.4 The problem of optimal NSFA construction is NP-complete. The hardness of \nfinding an optimal factoring automa\u00adton is demonstrated by viewing the automaton as an order\u00adcontaining \nfull trie (in the terminology of Comer and Sethi [4]), and showing that the corresponding trie minimization \nproblem is NP-complete. Although in [4] the hardness of constructing minimal full tries and minimal order-cent \naining pruned tries were shown, the hardness of finding a minimal order-c ontainin g full trie was left \nopen (the proof appears in the almendi?x). Note that findine an oDtirnal decision tree [10] c~~respo~ds \nto order-contair&#38;g pr-wed trie minimiza\u00adtion. Transformation Algorithm The algorithm used to translate \na factoring automaton into Prolog code is described using the programs in Figure 1 and the automaton \nin Figure 2a as an example. In an automa\u00adton, each state with multiple outgoing transitions, called a \nbranch point, is associated with a new predicate. When a stat e is reached, the computation that remains \nto be done is performed by the associated predicate. The binge variables of the state are passed as arguments \nto the predicate, since these represent the positions where remaining computations are to be performed. \nFurthermore, any head variables in the skeleton are also passed as arguments, since they may be ref\u00aderenced \nin the clause body. For example, in Figure 2a state SI is associated with predicate p with arguments \nXl, X2 and X3, and state Sz is associated with predicate pl with argu\u00adments X2 and X3. Note that the \ntransitions from a branch point represent alternative unifications to be performed. Hence, the asso\u00adciated \npredicate is defined by a set of clauses, one for each outgoing transition. Each clause performs the \nsequence of unifications associated with the transitions leading to the next branch point or leaf. In \nthe automaton in Figure 2a the fist clause of p needs to perform the unification Xl = a, and thus the \nhead of the clause is p(t.z, X2,X3). If the se\u00adquence of transitions ends in a branch point, the body \nof the clause is a call to the predicate associated with that branch point; otherwise, it is the body \nof the clause associated with the leaf. In the above example, the body of the first clause of p is pl(xz,xs). \nAn algorithm to translate any factoring automaton into Prolog code is given in Figure 6. The algorithm \nis invoked with the start state and the name of the predicate as ar\u00adguments. The clauses of the translated \nprogram are added to the set ? (initially empty). The complete translation of the automaton in Figure \n2a appears in Figure lb. Similarly, the translation of the automaton in Figure 5C appears in Figure 5d. \nObserve that choosing the appropriate transition in the automaton corremonds to an indexine oDeration \nin the re\u00adsulting program. If more than one tr%s~tion is possible at any stat e, a choice point will \nbe placed during execution of the transformed program. Thus, the operations of recording pending transitions \nand backtracking through them in the automaton correspond to placing and backtracking through choice \npoints in the execution of the resulting Prolog pro\u00adgram. Transformation in the presence of cuts In general, \nunification factoring at the source-level does not preserve the semantics of predicates containing cuts. \nFor example, on query p(X, Y) the program in Figure 7a com\u00adputes (p(a, b)), while the optimal transformed \nprogram in Figure 7b returns (p(a, b), p(b, d)). This problem arises due to implicit scoping of cuts \nin Prolog. Specifically, a cut re\u00admoves the choice point placed when the current predicate was called, \nas well as all subsequent choice points. We say that a cut cats to the most recent choice point of the \ncur\u00adrent predicate. If this choice point can be explicitly specified, then the transformation still preserves \nthe program s seman\u00adtics. In the XSB compiler, for instance, unification factoring is performed after \na transformation that makes the scope of cuts explicit. Figure 7C shows the effect of applying the cut \ntransformation used in XSB to the predicate in Figure 7a. Unification factoring is then applied to the \ncut transformed program, yielding the predicate in Figure 7d. The seman\u00ad tics of the original predicate \nis preserved. Thus, in XSB, unification factoring is uniformly applied to all progr ams, including those \nthat have cuts. 5 Implementation and Performance A direct execution of the transformed program can intro\u00adduce \nunnecessary inefficiencies mainly due to increased data movement and pro cedure calls on newly introduced \npredi\u00ad cates. Data movement In the WAM, the arguments of a predi\u00ad cate are stored in WAM registers, where \nthe @ argument is kept in register i. If the ith argument of one predicate is used as the jth argument \nin a call to another, that argument must be moved from register i to register j. Consider the two clauses \nin Figure 8a and the result of the transformation (fig. 8b). Each call to p2/2 in the body of p/3 requires \nmove\u00adment of two arguments. Such movement can be reduced in a number of ways, including the use of place-holding \nargu\u00adments (see Figure 8c). However, the potential for reducing data movement is limited by a system \ns indexing facilities. To index calls in Figure 8c, for example, would require a system, such as XSB, \nthat is able to index on an argument other than the first. Inlining By inlining the new predicates, calls \nto them are avoided, reducing execution time. Secondly, since these predicates are not user callable, \nsymbol table size is re\u00adduced. Thirdly, since these predicates have only one call site, inlining does \nnot create multiple copies, reducing code space. Finally, inlining restores the call structure of the \noriginal program, making the transformation transparent to program tracing. Performance Table 1 shows \nthe effect of performing uni\u00adfication factoring as a source transformation (that includes optimization \nfor data movement) in three ditlerent Prolog systerns3. In that table, the columns labeled Speedup list \nthe ratio of the CPU time taken by each query on the trans\u00adformed program to that on the original program. \nThe in\u00adcrease in the sizes of object files due to the program transfor\u00admation are list ed under the heading \nObject size increase . All figures were obtained using standard WAM indexing (first argument, principal \nfunctor). The columns labeled Irdine illustrate the benefits of Mining, as implemented in the XSB compiler. \nPrograms dnf (Dutch national flag), LL (k) (a parser), border (horn CHAT-80 [13]), and replace (an expression \ntranslator) and the corresponding queries were taken from [2]. Programs map (a map coloring program), \nmergesort, 3The systems used were Quintus Prolog Release 3.0, SICStus Prc\u00adlog 2.1 #9, and XSB version \n1.4.0. AU benchmarks were run on a SparcStation 2 running Sun OS 4.1.1. Benchmark programs can be obtained \nby anonymous ftp from cs.sunysb.cdu in the directory /pub/XSB/benchmarks. algorithm translate(state, \npnczme) let {ml, ..., ~~} be .r%tge(sstate )  let {Yl, . . . Yh} be the set of head variables (i.e., \nE V) in skeleton S,tate foreach edge (state, dest) let state be the first branch point or leaf on the \npath from state through dest he~d+~~~~e(yl,...,yk,tl,...  ,t )-,whereL isthe subterm of skeleton S,tate, \nrooted at ir, if state is a branch point let vname be a new medicate name bod~ t pname (yl, .. ., yk, \nXm; , . . . ,XT; ), where z{,... , r: are the fringe positions of S~tatej tran.date(state , pnamef) else \n/* state is a leaf */ body F body of clause C~tate, add (head : body) to 7J  Figure 6: Translation Algorithm \np(x, Y) :-_$savecp(Z) ,p(x,Y) :-_$savecp(Z) , p(a, X) :-pi(x). _$p(x, Y,z).p(a,b) :-!. _$p(x,Y, z) . \np(b, d). _$p(a, X,Y) :-_$pl(X, Y). p(a, c). _$p(a,b,X) :-_$cutto(X) . pi(b) :-!. _$p(b,d, _) . p(b, d). \n_$p(a, c,_) . pi(c) . _$pl(b,D :-_$cutto(X). _$p(b,d, _) . _$pl(c,_) . (a) (b) (c) (d) Figure 7: Original \n(a), unsound transformation (b), cut transformed (c), factored (d) p(a, X,Y) :-p2(x, Y). p(a,X,Y) :-p2(_,x,Y). \np(a, b,c). p2(b,c). p2(-,b,c). p(a,c,d). p2(c,d). p2(_,c,d). (a) (b) (c) Figure 8: Original(a) and \ntransformed predicate before (b) and after (c) data movement reduction Program [Query] Speedup Object \nsize increase Source Inline Source Irdine Qumtus SICS tus XSB XSB Umtus SICS tus XSB XSB dnf 51] 2.12 \n1.32 1.99 2.48 1.07 1.03 1.06 1.01 dnf 521 2.05 1.33 1.89 2.39 dnf 53] 1.83 1 !40 1.68 2.03 LL(I) p] \n0.83 0.59 1.10 1.16 1.13 1.17 1.13 1.01 LL(2) d 1,18 1.16 1.32 1.40 1.12 1.14 1.08 1.02 LL(3) r] 1.22 \n1.21 1.54 1.63 1.17 1.14 1.08 1.00 border reedit.] 2.00 1.29 2.11 2.38 1.05 1.04 0.99 0.93 border hungary] \n1.58 1.14 1,64 1.84 border alb arua 1.08 1.00 1.03 1.16 replace. sw neg] 0.82 0.86 0.97 0.97 1.00 1.05 \n0.98 0.96 replace. su <] 0.77 0.82 0.91 0.93 replace. su mul 0.96 0.96 0.98 1.00 Synchesz alcohol] 2.16 \n1.96 1.99 2.27 1.62 1.47 1.37 1,04 Synchem ether] 2.65 2.33 2.93 3,55 Synchem ,cc-dbl] 2. 71 2.53 2.67 \n2.97 maP 1.51 1.50 1.17 1.36 1.18 1.19 1.25 1.09 merge sort 0.99 0.97 1.32 1.41 1.10 1.07 1.05 1.00 mutest \n0.96 0.93 0.99 1.00 1.07 1.05 1.10 1.01 isotrees linear 0.83 0.83 1.11 1.18 1.09 1.04 1.09 0.98 isotrees \n(non-linear) 0.99 0.93 1.14 1.23 1.05 1.03 1.00 0.92 Table 1: Speedups and object size increases fornnification \nfactoring Program [Query] Speedup Source Mill e Umtus SIC Stus XSB SB Before Aft er Before After Before \nAfter Before After LL(l) ~] replace.su replace.sir replace.su neg < mul 0.86 0.82 0.77 0.96 1.30 3.60 \n2.65 1.20 0.61 0.86 0.82 0.96 1.11 3.22 2.10 1.21 1.09 0.97 0.91 0.98 1.49 4.27 2.58 1.09 1.16 0.97 0.93 \n1.00 1.57 4.35 2.65 1.13 Table2: Speedups before and after mode optimization and mutest (a theorem prover) \nare benchmarks from the Andorra system. The Synchem benchmarks arequeries ona 5,000-fact chemical database. \nThe isotrees program illus\u00adtrates the effect ofsharing non-linear unifications. Based on the performance \nresults we first summarize the strengths of unification factoring. Factoring of match oper\u00adations in \nstructures results in improved indexing and hence in performance, e.g., dnf, LL(2), LL(3), and map. Similarly \nfactoring on shared arguments asi.n border improves per\u00adformance. Speedups inmergesort andisotrees aredue \nes\u00adsentially to sharing of computed substitutions. The isotrees example also illustrates the benefits \nof factoring non-linear nnilication. Inprograms such as Synchem, where both match operations and computed \nsubstitutions are shared, the per\u00adformance gains are much more significant. Finally, unifica\u00adtion factoring \ndoes not degrade performance in the absence of sharable operations, as in the szutest example. The XSB \nspeedups are generally larger than those for Qnintus and SICStus, even when irdining is not performed. \nXSB S engine, which supports restricted SLG resolution, requires more expensive trailing, and untrailing, \nthan the WAM. The number of these operations is reduced by unifi\u00adcation factoring. Parallel Prologs such \nas Andorra have simi\u00adlar expenses so that unification factoring can be expected to provide similar speedups \nfor such systems. The transformation increases the size of the object files only by small amounts. The \nlargest size increase is for the Synchem database, and is mainly the result of the increase in symbol \ntable size due to the new predicates introduced by the transformation. But notice that by inlining even \nthis has been substantially reduced. Programs LL (1 ) and replace. sw show a slowdown pri\u00admarily due \nto factoring unifications on an output argument, resulting in loss of indexing. Recall that an optimal \nfac\u00adtoring automaton is found assuming no knowledge of the goaL When modes are known, we could first \nbuild a com\u00adplete switching tree for all the input-moded arguments (as in [61)1andthenattachoptimalSFAS \nfor the Unmoded ar\u00adguments at the leaves of the switching tree. Note, however, that the space of a switching \ntree can be exponential in the worst case. Hence we use the following simple technique of building SFAS \nin the presence of mo ales. We divide the hinge positions into input-moded positions and nnmoded positions, \nand all input-moded positions are inspected be\u00adfore any unmoded position is inspected. The effectiveness \nof this technique is indicated in Table 2. Discussion Efficient handling of non-determinism, traditionally \nignored by indexing methods, is one of the key strengths of uni\u00ad fication factoring. To reflect non-determinism, \na factoring automaton makes all possible transitions at each state. In contrast, pat tern mat thing tries \nand decision trees make at most one transition from any state. Alt bough functional progr amming languages \nsuch as ML use textual order for pattern matching just as Prolog does for unification, there are two \nnotable differences. First of all, the semantics of functional languages require that only one of potentially \nsev\u00aderal matching patterns be selected, whereas in Prolog the clauses of all heads unifying with a goal \nmay be evaluated. Secondly, pattern matching is unidirectional, whereas unifi\u00adcation is bidirectional. \nThus, the optimalit y criteria for fac\u00adtoring automata differ substantially from those of the other two \nstructures, necessitating the new techniques developed in this paper for constructing optimal automata. \nOur experimental results show that unification factor\u00ading is a practical technique that can achieve substantial \nspeedups for logic programs, while requiring no changes in the WAM and resulting in virtually no increase \nin code size. Furthermore, the speedups obtained by the source-to-source transformation on all three \nProlog systems are comparable to those obtained by the indexing technique of [2] that in\u00advolved extensive \ncompiler and WAM modifications. The speedups observed may be even more substantial when unification factoring \nis applied to programs which are themselves produced by transformations. For instance, the HiLog transformation \n[3] increases the declarativeness of programs by allowing nniiication on predicate symbols. If implemented \nnaively, however, HiLog can cause a decrease in efficiency for clause access. Experiments have shown \nthat unification factoring can lead to speedups of 3 to 4 on HiLog code. Given the demonstrable performance \nof unification factoring and its simplicity to implement, it is reasonable to expect that unification \nfactoring may become a fimdamental tool for logic program compilation. Unification factoring has been \nincorporated in the XSB logic prograrnmin g system, which is available by anonymous ftp fxom cs. sunysb. \nedu in directory /pub/XSB. References [1] Leo Bachmair, Ta Chen, and I. V. Ramakrishnan. Associative-commutative \ndiscrimination nets. In The\u00adory and Practice of Soflware Development, number 668 in LNCS, pages 61 74. \nSpringer Verlag, April 1993. [2] T. Chen, I. V. Ramakrishnan, and R. Ramesh. Mul\u00adtistage indexing algorithms \nfor speeding Prolog execu\u00adtion. In Joint International Conference/Symposium on Logic Programming, pages \n639-653, 1992. [3] W. Chen, M. Kifer, and D. S. Warren. HiLog: A foun\u00addation for higher-order logic progra \nmming. Journal of Logic Programming, 15(3):187-230, 1993. [4] D. Comer and R. Sethi. The complexity \nof trie index construction. Journal of the ACM, 24(3):428 440, July 1977, [5] W. Hans. A complete indexing \nscheme for WAM\u00adbased abstract machines. In International Symposium on Programming Language Implementation \nand Logic Programming, pages 232-244, 1992. [6] T. Hickey and S. Mudambi. Global compilation of Pro\u00adlog. \nJournal of Logic Programming, 7:193-230, 1989. [7] S. Kliger and E. Shapiro. From decision trees to deci\u00adsion \ngraphs. In North American Conference on Logic Programming, pages 97-116, 1991. [8] D. Pahner and L. Naish. \nNUA-Prolog: An extension to the WAM for parallel Andorra. In ~nternational Con\u00adference on Logic programming, \npages 429-442, 1991. [9] R. Ramesh, I. V. Rarnakrishnan, and D. S. Warren. Automata-driven indexing of \nProlog clauses. In ACM Symposium on Priciples of Programming Languages, pages 281 290. ACM Press, 1990. \n[10] R. L. Rivest and L. Hyafil. Constructing optimal binary decision trees is NP-complete. Information \nProcessing Letters, 5(1):15-17, May 1976. [11] R. C. Sekar, I. V. Rarnakrishnan, and R. Ramesh. Adaptive \npattern matching. In International Confer\u00adence on Automata, Languages, and Programming, num\u00adber 623 in \nLNCS, pages 247-260. Springer Verlag, 1992. To appear in SIAM J. Comp. [12] E. Tick and M. Korsloot. \nDeterminacy testing for nondeterminate logic programming g languages. ACM Transactions on Programming \nLanguages and Systems, 16(1):3 34, January 1994. [13] D. H. D. Warren and F. C. N. Pereira. An efficient \neas\u00adily adaptable system for interpreting natural language queries. American Journal of Computational \nLinguis\u00adtics, 8(3-4):110-122, 1982. [14] N. Zhou, T. Takagi, and K. Ushijima. A matching tree orient \ned abstract machine for Prolog. In International Conference on Logic Programming, pages 159-173, MIT \nPress, 1990. A Appendix A.1 Optimal SFA construction Construction of an optimal SFA for the sequence \nof clauses (p(a, b, c),p(a, b, d),p(a, c, c),p(b, a, c)) defining the predicate in Figure 9a begins with \nthe computation of its cost, using Equation 1. The cost and root position of the lowest cost au\u00adtomaton \ncomputed for a subsequence with end points (i, i ) at any point in the computation is stored in a table \n(Fig. 9b) at entry (i, i ), where i is the row and i the column. We begin by finding positions having \nsymbols common to all four clauses: common(p(X~, XZ, Xs), 1,4) = ({ },p(Xl, X2,X3)). There are no common \npositions, so any of positions $1, $2, and $3 might be used for the root state. We first try posi\u00adtion \n1, and compute the partition: part(l,4, $1) = {(a, 1,3), (b,4,4)}. We now need to compute optimal sub-automata \nfor subse\u00adquences (p(a, b, c),p(a, b, d), p(a, c, c)) and (p(b, a, c)) Repeating the above process for \nsubsequence (1, 3), common(p(Xl, Xz, X3), 1,3) = ({($1, a)}, p(a, X2,X3)) shows that subsequence (1, \n3) has one common position ($1) leaving positions $2 and $3 as possible root positions for the sub-automaton. \nWe fist choose position $2: part(1,3, $2) = {(b,l,2), (c,3,3)} Continuing similarly for subsequence (1, \n2) gives common(P(a, Xz, Xs), 1,2) = ({($2, b)}, p(a, b,&#38;)) part(l, 2, $3) = {(c, 1, 1), (d, 2,2)} \ncommon(p(a, b, Xs), 1, 1) = ({($3, c)}, p(a, b,c)) common(p(a, b, Xs), 2, 2) = ({($3, d)}, p(a, b,d)) \nNow, each subsequence is a single clause. Since all posi\u00adtions in a single clause are common positions, \nno positions for finther partitioning are available, and the cost for each single clause is O. Thus, \nthe cost of the sub-automaton for subsequence (1, 2) rooted at $3 is 2 (one each for transitions uni~$3, \nc) and unify($3, d)). Cost 2 and position 3 are then stored in entry (1, 2) of the table. Returning to \ncompute the cost for subsequence (3, 3), common(p(a, X2,X3), 3, 3) = ({($2, c), ($3, c)}, p(a, c,c)) \ngives the cost of the sub-automaton for subsequence (1,3) with root position $2 as 1 + 2 + 2 = 5 (one \nfor transi\u00adtion unify($2, b); one each for transitions unify($2, c) and uni~$3, c); and two as computed \nfor subsequence (1, 2). Thus, cost 5 and position 2 are stored in entry (1,3) of the table. Completing \nthe computation for sequence (1, 4) at po\u00adsition $1 yields additional costs of one (for the transition \nlabeled unify($l, a)) and three (for transitions unify($l, b), unify($2, a), and unify($3, c) for subsequence \n(4, 4)), giving a total cost of 9. Thus, cost 9 and position 1 are stored in entry (1, 4) of the table \n(highlighted). The automaton cor\u00adresponding to this cost and position is shown in Figure 9c. Verifying \nthat no other choice of position yields a better automaton is left to the interested reader. Note that \nthe shaded entries in the table are not used in computing the cost of an optimal automaton. A.2 NP-completeness \nof optimal NSFA construction A non-sequential factoring automaton can be viewed ab\u00adstractly as a trie, \nin which the clause heads of a predi\u00adcat e are viewed as strings of symbols. Optimization of an NSFA \n(when elementary unification operations are assumed to have unit cost) thus corresponds to trie minimization. \nIn the terminology of Comer and Sethi [4], an NSFA cor\u00adresponds to a full order-containing trie (full \nO-trie), where full refers to the fact any root-to-leaf path in the trie ex\u00adamines an entire string, \nand order-containing means that diRerent paths may examine characters (positions) in ditYer\u00adent orders. \n ., 1 s,: 11234 P(xj)xz ,X3) cost: o cost: 2 cost: 5 cost: 9 1 unify($l,a) unify($l,b) Pos: -Pos: 3 \nPos: 2 Pos: 1 A ,, ;p(a,b, c). cost:o : $ , ; .; p(a,b, d). 2 . ..... Pos: -, :; ~$,,; ,;. , ,,,,p(a, \nc,c). i w \u00adp(b, a,c) . ,,, ,j , cost: o cost: 2 ,,... 3 4 (a) (b) Figure 9: Optimal SFA construction: \nl heorern A.l The minimization problem jor full O-tries (FOT) is NP-complete. Proofi We show that the \ntrie minimization problem is NP-hard by reduction from the minimum set cover prob\u00adlem (SC). The minimum \nset cover problem can be stated as follows: Given a finite set U = {W,. ... un}, a collection C= {cl,..., \nCm} of subsets of U, and a positive integer k < m, do there exist k or fewer subsets in C whose union \nis S? Let 15C be an instance of SC. We construct an instance lFOT of FOT with 2n strings, each of length \n2(n + m)2 + m, as follows. Each string consists of three fields: a Test field, consisting of m characters; \na Blue field, consisting of (n+m) characters; and a lied field, consisting of (n+m)2 characters: TIT \n. .. TmBl B2 2RIR2 ~~~ ~(n+m) . . . R(m+na)~ Test Blwe Red For each element ~i E U two strings are constructed: \na Red string and a Blue string. In the Red string, Tj = O, for l~j~rn; B1=iand R1=O, forl< l~(n+m)2. \nThe ith Red str~g thus has the form Wtiw Test Blue Red In the Blue string, Tj = 1 if u; c Cj, otherwise \nO, for 1 s j<rn; Bl=iand Rl=O, forl~l ~(n+m)2. Thei* Blue string thus has the form T&#38;t Blue Red Checkiig \na character in the Test field of a Blue string can be thought of testing for membership of an element \nof U in a subset. Observe that the Red and Test fields in every Red string are identical, and that each \nBlue field is distinct. For a set consisting entirely of Red strings, a minimal trie must test all characters \nin the Red and Test fields before testing any character in the Blue field, since the first test in the \nBlue field effectively partitions the set into individual strings (see Figure 10a). The order of testing \nwithin the Red and Test fields is unimportant, as is the order of testing within the Blue field. S2: \ns~: p(a,x~,xj) 2 p(b,X2,X3) 2 unifi($2,b) unifi($2,c) unify($2,a) q: ~: ~: p(a,btX3) 3 p(a,c,xj) 3 p(b,a,X3) \n 3 unify($3,c) unifi($3,d) un1@($3,c) unifl($3,c) p(a,b,c) cl p(a,b,d) C2 p(a,c,c) C3 p(b,a,c) c~ h) \n(c) predicate (a), cost table (b), SFA (c) Also observe that the Blue field in every Blue string is \nidentical, and that each Red field is distinct. A minimal trie for a set of Blue strings must test all \ncharacters in the Blue field before testing any character in the Red field. In gen\u00aderal, testing characters \nin the Test field will incrementally partition the set. Thus, a minimal trie for Blue strings will typically \nhave the form shown in Figure 10b. The above observations lead to the following bounds on the sizes of \nminimal tries for monochromatic sets of strings. Lemma A.1.l Given a set S consisting enti~ely of Red \nstrings, such that IS[ = ?2R, the number of edges in a mini\u00admat full O-i%ie for S iS eXac@ (?2R + 1)(?2 \n+ YTZ)2+ m. Lemma A.I.2 Given a set S consisting entirelg of Blue strings, such that \\Sl = nB, the number \nof edges in a mini\u00admalfull O-trie for S is no more than (n~+l)(n+rn)2 +mnB. The main idea in constructing \na minimal full trie is to order the tests such that less partitioning occurs near the root and more occurs \ntoward the leaves. Therefore, to build a minimal trie for the 2n Blue and Red strings constructed from \n15C, testing in the Test field should precede testing in the Blue and Red fields. Observe that testing \na character in the Test field partitions a set containing both Blue and Red strings into one set containing \nonly Blue strings (on a branch labeled l ) and another containing Red and perhaps some Blue strings (on \na branch labeled O -see Figure 1la). Testing a character in the Red (Blue) field, on the other hand, \npartitions the set into one set containing all of the Red (Blue) strings and one set for each of the \nBlue (Red) strings (see Figure llb). These observations lead to the following proof that U has a cover \nof size k if and only if there exists a full O-trie for S having fewer than (2n + k + 2)(n + m)2 edges. \nLemma A.1.S ljj U has a cover of size k, then there etists a full O-trie over S having fewer than (2n \n+ k + 2)(n + m)2 edges. Proof of lemma: A trie over S can be constructed as illus\u00adtrated in Figure 1 \nla. For 1 ~ j < k, let C ii be a member of the cover of U. Now, for each node Tij in the trie, the subtrie \nat t ached to the 1 edge is a trie over Blue strings, since no Red string contains a 1 in its T field. \nSince {C il,. . . . Cik} is a cover for U, each Blue string is represented in one of the subtries attached \nto a l edge, and only the n Red strings - R+ flB (a) (b) Figure 11: Minimal trie for 2n strings (a) \nand suboptimal trie (b) f 1 are represented in the subtrie attached to the O edge of node Tik. By lemma \nA.1.2, the number of edges in the subtrie at\u00adtached to the l edge of node Tij is no more than (n.i + \nl)(rz + rn)z + (m j)nsj, where n,j is the number of strings RI represented in the subtrie. Since ~~=1 \nn.i = n, the to\u00ad 0 B, tal number of edges in the Blue subtries is no more than ( +.*) 2 o (n+k)(n+m) \n+mn. By lemma A.1.1, the number of edges 71 R(n+m)z in the subtrie attached to the O edge of node Tik \nis no more than (n+l)(n+m)z +m -k. Thus, the total number of edges I in the trie is no more than (2n+k+l)(n+m)2 \n+(n+l)m+2k, o (n+mJ2 T1 B(.+# which is less than (2n + k + z)(7z + m)z. Q o 7 ~ A II o I Lemma A.I.4 \nIf there ewists a full O-trae over S having ! fewer than (2n + k + 2)(7z + m)z edges, then U has a cover \nT. Tesf o~ size k. I o z 1 Proof of lemma: It sufiices to show that any trie over B1 R,. ..R, S having \nfewer than (2n + k + 2)(n + m) edges must be if ... in~ b i.g of the form shown in Figure 1la. Given \na set S of strings containing nR Red strings and nB Blue strings, the root of (n+nl) 2 (n+m) 2 a minimal \ntrie over S must be a T node. If, instead, the root were a R node, the trie would have at least (2nB \n+nR+ I)(n + m)z edges (see Figure 1 lb). Similarly, a trie with a ~ root node would have at least (2?2R \n+ ?zB + l)(n + m) @j? @j?l ? +1 edges. Replacement of any of the k T nodes in the trie in nR n6 Figure \n1la by a R or B node would therefore result in at least (n k)(rs + nz)z additional edges. Thus, any \ntrie having (a) (b) fewer than (2n + k + 2)(n + m)2 edges must be of the form shown in Figure 11a, which \ncan exist only if U has a cover Figure 10: Minimal tries for sets of Red (a) and Blue (b) of size k. \nstrings Lemmas A.1.3 and A.1.4 together complete the reduction horn Isc to lFO ~. The remaining details \nshowing that the transformation is polynomial in the size of 15C and that FO T G NP are standard, and \nare omitted.  \n\t\t\t", "proc_id": "199448", "abstract": "<p>The efficiency of resolution-based logic programming languages, such as Prolog, depends critically on selecting and executing sets of applicable clause heads to resolve against subgoals. Traditional approaches to this problem have focused on using indexing to determine the smallest possible applicable set. Despite their usefulness, these approaches ignore the non-determinism inherent in many programming languages to the extent that they do not attempt to optimize execution <italic>after</italic> the applicable set theory has been determined.</p><p>Unification factoring seeks to rectify this omission by regarding the indexing and unification phases of clause resolution as a single process. This paper formalizes that process through the construction of <italic>factoring automata</italic>. A polynomial-time algorithm is given for constructing optimal factoring automata which preserve the clause selection strategy of Prolog. More generally, when the clause selection strategy is not fixed, constructing such an optimal automaton is shown to be NP-complete, solving an open trie minimization problem.</p><p>Unification factoring is implemented through a source code transformation that preserves the full semantics of Prolog. This transformation is specified in the paper, and using it, several well-known programs show performance improvements of up to 100% across three different systems. A prototype of unification factoring is available by anonymous ftp.</p>", "authors": [{"name": "S. Dawson", "author_profile_id": "81100534830", "affiliation": "Department of Computer Science, SUNY at Stony Brook, Stony Brook, NY", "person_id": "PP14185718", "email_address": "", "orcid_id": ""}, {"name": "C. R. Ramakrishnan", "author_profile_id": "81332522509", "affiliation": "Department of Computer Science, SUNY at Stony Brook, Stony Brook, NY", "person_id": "PP40036402", "email_address": "", "orcid_id": ""}, {"name": "I. V. Ramakrishnan", "author_profile_id": "81452611166", "affiliation": "", "person_id": "PP39077605", "email_address": "", "orcid_id": ""}, {"name": "K. Sagonas", "author_profile_id": "81100605481", "affiliation": "Department of Computer Science, SUNY at Stony Brook, Stony Brook, NY", "person_id": "PP39078241", "email_address": "", "orcid_id": ""}, {"name": "S. Skiena", "author_profile_id": "81100248225", "affiliation": "Department of Computer Science, SUNY at Stony Brook, Stony Brook, NY", "person_id": "PP39079298", "email_address": "", "orcid_id": ""}, {"name": "T. Swift", "author_profile_id": "81100240528", "affiliation": "Department of Computer Science, SUNY at Stony Brook, Stony Brook, NY", "person_id": "P276550", "email_address": "", "orcid_id": ""}, {"name": "D. S. Warren", "author_profile_id": "81452607732", "affiliation": "Department of Computer Science, SUNY at Stony Brook, Stony Brook, NY", "person_id": "PP95035920", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/199448.199509", "year": "1995", "article_id": "199509", "conference": "POPL", "title": "Unification factoring for efficient execution of logic programs", "url": "http://dl.acm.org/citation.cfm?id=199509"}