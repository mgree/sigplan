{"article_publication_date": "01-25-1995", "fulltext": "\n Obtaining Sequential Efficiency for Concurrent Object-Oriented Languages John Plevyak, Xingbin Zhang \nand Andrew A. Chien Department of Computer Science 1304 W. Springfield Avenue Urbana, {jplevwkzhang,achien] \nAbstract Concurrent object-oriented programming (COOP) languages focus the abstraction and encapsulation \npower of abstract data types on the problem of con\u00adcurrency control. In particular, pure fine-grained \nconcurrent object-oriented languages (as opposed to hybrid or data parallel) provides the programmer \nwith a simple, uniform, and flexible model while exposing maximum concurrency. While such lan\u00adguages \npromise to greatly reduce the complexity of large-scale concurrent programming, the popularity of these \nlanguages has been hampered by efficiency which is often many orders of magnitude less than that of comparable \nsequential code. We present a sufficient set of techniques which enables the ef\u00adficiency of fine-grained \nconcurrent object-oriented languages to equal that of traditional sequential languages (like C) when \nthe required data is avail\u00adable. These techniques are empirically validated by the application to a COOP \nimplementation of the Livermore Loops. 1 Introduction The increasing use of parallel machines has exacer\u00adbated \nthe longstanding tension between high-level and low-level programming languages. Though high-level languages \nease the task of expressing a computation, advocates of low-level languages ar\u00adgue that detailed control \nis required to achieve ef\u00adficiency. Arguably, moving to parallel systems in\u00ad creases both the complexity \nthe importance of achieving determining what high level ported efficiently and how to ciently is an important \nt epic Concurrent object-oriented of programming and high efficiency. Thus, features can be sup\u00ad implement \nthem effi\u00ad of research. programming lan\u00ad guages are a promising approach to parallel pro- Permission \nto copy without fee all or part of this material is granted provided that the copies are not made or \ndistributed for direct oommeroial advantaqe, the ACM copyright notice and the title of the publication \nand Its date appear, and notice is given that copyin is by permission of the Association of Computing \nMachinery. $ o copy otherwise, or to republish, requires a fee and/or specific permission. POPL 951/95 \nSan Francisco CA USA @ 1995 ACM 0-89791 -892-1/95/0001 ....$3.50 IL 61801 @cs.uiuc.edu gramming. Recent \nyears have seen the rapid pop\u00ad ularization of object-oriented programming tech\u00ad niques for sequential \ncomputers, largely because of their benefits in managing program complexity. Concurrent object-oriented \nprogramming (COOP) languages focus the abstraction and encapsulation power of abstract data types on \nmanaging the com\u00ad plexities of concurrency and distribution. With concurrent objects, which encapsulate \ntheir own concurrency control, programmers can safely com\u00ad pose concurrent operations on distributed \ncollec\u00ad tions of objects. Unfortunately, to date the best COOP implementations have been inefficient \ncom\u00ad pared to sequential Ianguages.1 In this paper, we focus on achieving efficient se\u00adquential execution \nof COOP languages. The larger problem of achieving good parallel performance re\u00adquires both generation \nof efficient sequential code and data locality. This latter issue is beyond the scope of this paper.2 \nWe focus on the former issue, exploring the elimination of object-orientation and concurrency control \ncosts in the generated code. Concurrent object-oriented efficient largely because view of all program \ndata. mentations incur tens to for each method invocation of managing a distributed languages have been \nin\u00adthey provide a uniform Even the best imple\u00adhundreds of instructions [26, 47] due to the cost memory \n(method invoca\u00ad tions are location independent) and managing con\u00adcurrency (locks). Furthermore, the high \nprocedure call frequency typical of object-oriented programs not only magnifies the method invocation \noverhead, it also reduces the benefits of traditional optimiza\u00adtion. The overhead of method invocations \nand con\u00adcurrency control can be eliminated by aggressive inlining, access region optimizations, and state \ncaching. All of these optimizations are based on excellent (and generally precise) concrete type in\u00adformation \n[38]. With this set of optimizations, our COOP implementation equals the efficiency of the sequential \nlanguage C on the Livermore Kernels, 1We consider only languages that support object-level concurrency. \nFor a discussion of the alternatives see Sec\u00adtion 5. 2We defer to the wealth of research in thm area \n[30, 39, 22, 36, 3]. a demanding set of numerical benchmarks. While the Livermore Kernels do not benefit \ngreatly from object-orientation, all the arrays in the COOP ver\u00adsion of the kernels are implemented as \nconcurrent objects, and accessed via object met hod invocation. Thus to achieve efficiency comparable \nto C, our compiler must eliminate virtually all of the over\u00adhead of concurrency control and object orientation. \nWe believe the performance of our compiler not only exceeds that of all other concurrent object\u00adoriented \nimplementations, but even surpasses many other implement ations of sequential object-oriented implementations \nsuch as C++. The specific contributions of this work are: Identifying the critical efficiency issues \nin achieving sequential efficiency in concurrent object-oriented languages, A combination and extension \nof program op\u00adtimization which together produce sequen\u00adtially efficient COOP implementations, and A demonstration \nof these techniques on the Livermore Kernels which provides empirical evidence that COOP languages can \nbe effi\u00adcient. The remainder of this paper is organized as fol\u00adlows. Section 2 describes the COOP programming \nmodel, execution model, and compiler framework. Section 3 describes a sufficient set of transforma\u00adtions \nto construct an efficient implementation for COOP programs. In Section 4, we report the re\u00adsults of applying \nthese transformations to the Liver\u00admore Loops. Related work is discussed in Section 5, and we summarize \nthe paper in Section 6. 2 Background We describe the programming model, execution model, and the compiler \nframework. The mapping of the programming model to the execution model described here is largely conceptual; \nfurther infor\u00ad mation about our approach and actual implemen\u00ad tation of COOP can be found in [9, 30]. \n2.1 Programming Model The programming model we assume is the syner\u00adgistic union of Actors [1, 12, 21] \nand the object\u00adoriented model [17]. Each object can act concur\u00adrently to update its own state, create \nnew objects or invoke methods on other objects. An object pro\u00advides a set of abstract operations, of \nwhich only one may be active at a time. This allows objects to control updates to their internal state. \nMethods (abstract operations) may invoke methods on sev\u00aderal other objects concurrently, waiting on the \nre\u00adsponses only when required by data flow or the pro\u00adgrammer. In this way, the programmer can safely \nand conveniently cdmpose larger parallel abstrac\u00adtions and entire pro~rams. A number of languages share \nthis model [Id, 26, 33, 46]. The programmi L g model has three features which contribute fu~damentally \nto its programma\u00adbility: a shared nam~ space,  dynamic thre J d creation, and  object level ac ess \ncontrol. A shared names ) ,ace allows programmers to sep\u00adarate data layout a]- d functional correctness. \nDy\u00adnamic thread creat on allows programmers to ex\u00adpress the natural concurrency of the application, leaving \nthe system to map it to the underlying machine. Object-level access control provides a basic mutual exclu \nA Ion mechanism which can be used to construct la i ger atomic operations or other synchronization str \nL ctures. When such ezclwsioe methods are invok d on the current object, seif, they inherit any ac ess \nprivileges the caller might ! have, enabling recu~sion in exclusive methods.  2.2 Executio~ Model The \nexecution model is based on a set of single\u00adthreaded processin elements with local names- I paces. Only \nobjects local to a processing element can be accessed di~ectly. The system synthesizes the global namesp \nce of the programming model 4 by detecting and mapping operations on remote objects into comm~nication. \nThe multithreading in the programmin~ model is achieved by multi\u00ad plexing the processi~g elements in \nsoftware. Thus, each processing ele ent can be viewed as a sequen\u00ad 7 tial machine augmented with runtime \nprimitives supporting naming, locking, location, and concur\u00ad rency control. Thi model matches existing \nmas\u00ad sively parallel proce 7 sors [42, 13], and we believe it is appropriate for t e next generation \nmachines as well. I Figure 1: E~ecution Model Example Each object has! a global name, a lock to im\u00adplement \naccess con rol, and a queue for ready and suspended contexts. { Contexts are heap-allocated activation \nrecords ~hich contain a thread s state. When a message is ent, a future is created to hold the return \nvalue an J a thread is started on the tar\u00adget object. When the return value is required, the future is \ntouched ahd the thread suspended until the value is present. Thus, the logical thread within the object \nmay split then rejoin or seem to migrate from processor to processor as in Figure 1. Basic operations \nof the execution model such as locking, queuing, and context switching are ex\u00adpensive, but often can \nbe optimized away. For ex\u00adample, a naive approach would create anew thread foreach method invocation, \nbut an implementation can execute several threads within their parent to improve efficiency. Thus the \nconcurrency (relaxed serialization) supplied by the programmer can be exploited for parallel speedup, \nor discarded for se\u00adquential efficiency. The runtime exposes the fol\u00adlowing operations: LO CALJJAME \nconverts a global object name to a local name or returns a failure value.  TAKE_LOCKS, given a set of \nlocal names, at\u00adtempts to acquire locks on all the correspond\u00ading objects and returns a success or failure \nvalue.  FREE_LOCKS, given a set of local names on which locks have been acquired, releases those locks. \n INVOKE invokes the specified method, han\u00ad  dling all cases (remote objects, locked objects, etc.). \n These primitives allow the compiler to test lo\u00adcality and locks irdine, opening the door for spec\u00adulative \noptimization. They also expose the basic costs in the execution model, enabling many op\u00adtimization including \nsome described later in this paper, 2.3 Compiler Framework The optimizations described in this paper \nhave been implemented as part of the Concert compiler [9]. The intermediate form used in our compiler \nis the Program Dependence Graph (PDG) [16] in Static Single Assignment (SSA) [15] form. Using the intermediate \nform, the compiler performs con\u00adcrete type inference, global constant propagation, cloning, inlining \nand extension of access regions. Next, inst ante variables are converted to SSA, and constant folding, \ncommon sub expression elimina\u00adtion, and strength reduction are performed. In the back end, the Control \nFlow Graph (CFG) is recon\u00adstructed and the program is translated into Reg\u00adister Transfer Language (RTL). \nContext slots are allocated and cached in registers, and the RTL is converted into C++, which we use \nas a portable machine language. Properties of the intermediate form enable the optimizations described \nin this paper. Using the PDG, the compiler can determine both the partial order of execution as well \nas some total order on the contained statements. We say that two access regions (see Section 3.1) are \nadjacentwhen no other access regions appear between them in the total or\u00ad der. A set of statements are \nbetween two statements when they are required to execute so by the partial order. The SSA transformation \nchanges variables with storage locations into values. Since our mo\u00addel does not allow arbitrary pointers, \nonly instance variables are associated with storage locations and even these can be converted to SSA \nwithin access regions. We say a statement is functional when its execution cannot result in the thread \nblocking, a message being sent, a lock taken, or an update to a storage location. 3 Program Optimizations \nIn this section, we present three compiler transfor\u00admations which minimize concurrency overhead for sequential \nportions of COOP programs. Each opti\u00admization exploits information available at compile time to reduce \nand eliminate runtime overhead. In\u00ad iine substitution of methods eliminates method dis\u00ad patch overhead \nand enables intra-procedural opti\u00admization. Access region expansion reduces local\u00adity and access control \noverhead. Context and ob\u00adject state caching exploit the memory hierarchy of modern microprocessors to \nreduce multi-threading overhead during sequential execution. for (1=1 ;l<=loop ;1++ ){ for (k=O ;k<n \n;k++ ){ x[k] = y[k+l] -y[k] ; } } Figure 2: C code for Livermore Loops Kernel 12 Throughout, we use \nthe Livermore Kernels as a benchmark for sequential efficiency. Although the Livermore Kernels do not \nbenefit greatly from object-oriented structure, they are well-known to be a demanding test of a compiler \ns ability to gen\u00aderate good sequential code. Even a single extra memory reference within the innermost \nloop can cause a major drop in performance. To illustrate specific optimizations, we use Livermore Kernel \n12 shown in Figure 2. The inner loop body cent ains three array accesses. Because each array is an ob\u00adject \nin a pure object-oriented language,3 each array access involves a method invocation. Making these invocations \neach iteration, particularly in a COOP model, would incur substantial overhead compared to a C implementation. \nAs a running example, we show how this overhead can be removed as a result of the three optimizations. \n3.1 Inline Substitution Irdining is crucial for fine-grained COOP languages because methods are small \nand general method in\u00ad vocation overhead is high, including procedure call, 3Each array as a whole is \nan object. Distributed ar\u00adrays are available through aggregates a concurrent multi\u00adaccess data abstraction. \n concurrency control, and even communication over\u00adhead. Without irdining, method invocation over\u00adhead \ncan easily account for over 95% of a program s execution time. In sequential languages, the main restraint \non inlining is the increase in program size. For concurrent object-oriented languages, irdining is constrained \nby program size, access control, and locality. A method invocation can beirdined only if the target object \nis local and can be accessed (any re\u00adquired lock is available). Otherwise message pass\u00ading or queuing \nof the message is required. It is not always possible to statically determine these prop\u00aderties. As a \nresult, we speculatively irdine method invocations by testing the required properties at run time using \nthe inlining template shown in Fig\u00adure 3. The template applied to an invocation of method at on the object \nX is shown. The runtime primitives CHECK-LOCAL() and TAKE-LOCKS() check the locality and take the object \nlock, respectively. Together they define an access region under the true arm of the conditional where \nthe object X is known to be local and locked. Because the original method invocation is retained in the \nfalse arm as a fallback, the irdining template is safe for all method call sites. lf( else CHECK_LOCAL(X) \n&#38;&#38; TAKE-LOCKS(X) inlmed method FREE_LOCKS(X) body ) of at runtlme access guards region of X INVOKE(at, \nX, i) I fallback code Figure 3: Inlining Template When locality or access control information is available \nat compile time, the irdining template is specialized to eliminate the testing overhead or eliminate \nunreachable fallback code. For examde. ., no locking is required for immutable objects, and invocations \non seif require additional locking only when the callee is an exciusive method but the caller non-exclusive. \nFor other objects, the caller need only take the object lock if the target method is exclusive. Similarly, \nthe locality of the target ob\u00adject can be frequently guaranteed at compile-time, as well. Immutable objects \nand self are always lo\u00adcal, and the locality of other types oft arget objects can be estimated using \nobject creation points and the interprocedural call graph. The inlining template enables irdining at \nall method call sites where suitable type information for the target object is available ,4 To guide \ninlining decisions, we use simple heuristics based on static call frequency estimators [44], the size \nof both the caller and the callee method, and the irdine depth. Our experience shows that the simple \nheuristics combined with compile-time specialization of the inlining template reduces method invocation \nover\u00adhead significantly without excessive code size or compile time. For instance, full optimization \nof 4 We perform global concrete type analysis and customlzatlon[38, 6] to bind methods statically in \nthe pres\u00ad ence of type-dependent dmpatch and inheritance Kernel 12 results in approximately a 50% increase \nin compile time, 35% decrease in the object code size, and 50Y0 decrease in the the backend C++ compilation \ntime. /__ \\   ( G ) / f ACCESS T ~ REGION ) \\ / x 1000 () ,, ,/ . _, . - \\ / Figure 4: The PDG \nof Kernel 12 After Inlining An important result of inlining is the creation of access regions which define \na portion of the program in which the two properties, locality and access, are satisfied. Subsequent \noptimizations build on and leverage off these properties to achieve sequential efficiency. For example, \nthe PDG of the Kernel 12 after inlining (Figure 4) shows an inner loop body consisting of three access \nregions created by the irdining of the three array accesses. Within each region, a standard suite of \nsequential optimizations can be applied. 3.2 Expanding Access Regions Entering the access regions introduced \nby specula\u00adtive irdining requires runtime checks which can cost ten or more instructions. If access regions \nare small or executed frequently the overhead can be severe (as in the loop of Figure 4). In order to \nreduce this overhead we expand the dynamic extent of access regions. This not only reduces the runtime \ncheck overhead but also produces larger basic blocks for classical optimizations. In this section, we \nconsider the general problem of expanding and merging ac\u00adcess regions then describe two such optimizations, \nmerging adj scent access regions and lifting access regions above loops and conditionals. Aspects of \nthe programming and execution mo\u00addel influence these optimizations. Since control flow is structured, \nthe PDG forms a tree of prop\u00aderly nested statements. The access regions are also properly nested, with \nthe locks being acquired and released at the same nesting level. As a result, we can compose access region \nexpansion optimizations from two steps: 1) moving statements into a region and 2) creating an empty region \nwith a particu\u00adlar set of runtime tests. Note that the statements moved in may include conditionals or \nloop heads di\u00adrectly above the region, expanding the region to in\u00adclude higher levels of the statement \nnesting. Lastly, execution is non-preemptive with only the runtime cent ext switching, so we need only \nconsider local interactions between runtime primitives. 3.2.1 Correctness Access-region expanding optimizations \nmust pre\u00adserve the semantics implied by the original method invocations. This includes preserving the \nlocality and access control properties as well as mutual ex\u00adclusion of anv statements moved into a new \nre\u00adgion. Inaddit~on, wemustensure that neither mov\u00ading statements nor creating new regions introduces \ndeadlock. These properties are most conveniently discussed within the concurrent systems framework of \ncritical regions [20], monitors [5] and deadlock prevention [23]. Moving Statements into a Region When \nmoving statements into a region, we dif\u00adferentiate three cases: functional statements (Sec\u00adtion 2.3), \nstatements which access storage (non-SSA variables), and potentially blocking runtime primitives. Statements \nwhich are functional do not call the runtime nor modify storage so they cannot affect the locality or \naccess properties of a region. Hence, they can bemoved safely into any region. All exclusive storage \naccesses are conditioned by tests for locality andaccess control by the program\u00adming model. If a storage \naccess is moved into the region the tests for the destination region must sub\u00adsume the tests for the \nstorage access. Furthermore, if storage accesses for the same object from twodis\u00adtinct regions are moved \ninto the region, they must occur in whole, before or after each other, ensuring locally the mutual exclusion \nthat the programming model guarantees [20]. Together these conditions are sufficient to ensure the exclusion \nproperties of the programming model are preserved. Potentially blocking runtime primitives cannot be \nmoved into regions unless it can be moven that a resource cycle w~not result. This is b~cause block\u00ading \noperations can give rise to non-local resource deadlock [23]. Intheabsence ofglobal dependence analysis, \ncorrectness can be assured conservatively by preventing such statements from being moved into access \nregions. Crest ing New Regions Creating an access region containing no statements, but with arbitrary \nruntime tests, does not change the program providing that no deadlocks are in\u00ad troduced. New deadlocks \ncan only arise if new dependence between locks are introduced. Dead\u00adlock can be prevented by obtaining \nall required locks atomically; that is, all must be available for any to be acquired and the entire operation \nmust be executed non-preemptively. Our multi-locking runtime primitive provides this atomicity, avoiding \nany lock ordering (and thereby avoiding any new lock dependence).5 Thus, new regions can be cre\u00ad 5Note, \nthat this is not an expensive operation in our model since all objects in multi-locking operation will \nbe local. ated without introducing deadlocks. Subsequently, statements can be moved into the region subject \nto the constraints above. The fallback code used when the tests fail must be completely general. For \nany combination of tests, the fallback code must correctly handle the situation where any of the component \ntests would have succeeded. ~) T J $V I test y, x F,. ,T \\ \\ entry R \\ fallbsck J\\\\$ ready 1 Invokey \nread y I Invokexy wrmx / Invokex\\ m \\ , /1 ,, +/ Figure 5: Kernel 12 After Merge  3.2.2 Merging Adjacent \nAccess Re\u00adgions Merging adjacent access regions combines the run\u00adtime checks for two access regions and \nmerges their code bodies. Merging consists of severaJ steps. First, we create a new region with the combined \nen\u00adtrance criteria. Then, using the partial order of ex\u00adecution from the PDG, we identify the code which \nmust execute between the two regions and move it into both the entry and fallback branches of the ac\u00adcess \ntest. Finally, we move the entry and fallback code from the original regions into their respective branches \nof the new region. The combined entrance criteria represent the conjunction of the checks for the original \nregions. If the new checks attempt to acquire the lock on a single object twice, the attempt will fail, \npreserving the mutual exclusion property. However, if at com\u00adpile time we know two objects are really \nthe same, we can take out a single lock and ensure mutual ex\u00adclusion by sequencing the operations from \nthe two regions so that they do not interleave. This must\u00adtias determination need only be conservative \nsince the fallback code is completely general. Note that this approach aggressively merges ad\u00adjacent \nregions, so that the optimized path is only be executed when all locks can be acquired at once. Since \nthe cost of blocking on a failure to acquire a lock is large, this optimization extracts high effi\u00adciency \nfrom the optimized path at a relatively small increase in cost along the unoptimized path. The result \nof these transformations on the program in Figure 4 appears in Figure 5. All three of the condi\u00adtionals \nhave been merged into a single test and two branches, an optimized path andafallback path. 1, test1 \ny,.   5 % read y / Invoke y mad y ,mvoke y \\ wt. x Invoke . \\ / nn \\ / \\- Figure 6: Kernel 12 After \nHoist 3.2.3 Lift ing Access Regions Lifting access regions higher in the PDG can im\u00adprove code efficiency \nby enabling runtime testing overhead to be removed from loop bodies. In or\u00adder to ensure correctness, \nwe proceed stepwise as follows. Using a bottom up traversal of the PDG (essentially the program block \nstructure), we at\u00adtempt to merge adjacent access regions until only one remains within the control dependence \nregion. We then attempt to place the remaining statements wit hin the single access region. If this succeeds \nwe are prepared to lift the regions. There are only two types of control structures in our intermediate \nrepresentation: while loops and conditionals. For while loops, the situation is sim\u00adple. If the control \ndependence region under the loop is entirely cent abed in a single access region, the loop header can \nbe moved into the access re\u00adgion. The result is that the access region is lifted over the loop. The same \nsituation holds for single armed conditionals. if( CHECK. LOCAL(X) &#38;&#38; CHECK. LOCAL(Y) &#38;&#38; \nTAKE-LOCKS (X , y) ) for ( 1=1 ; l<=loop ; 1++ ) for (k=O ;k<n ;k++ ) x[k] = y[k+l] -y[k] ; FREE-LOCKS \n(X ,y) ; else { for ( 1=1 ; l<=loop ; 1++ ) for ( k=O ;k<n ;k++ ) { tl = IWOKE(at, y, k+l); t2 = I?fVOKE(at, \ny, k) ; INVOKE (putat, x, tl -t2) ; } } Figure 7: Example Compiler Output After Lifting on Kernel 12 \nConditionals with two arms require that the two regions be merged and lifted simultaneously. The logical \nsteps required to show correctness are: first, break the conditional into two one armed condition\u00adals, \none wit h the negation of the original condition. Then, lift the access regions above tlhese condition\u00adals \nas above. Next, merge the two resulting access regions, Finally, merge the two one armed condi\u00ad tionals \nto reconstruct the original ccmditional. After inlining and access region expansion, the code within \na function or method consists of re\u00adgions of optimized sequential code. If the program spends the majority \nof its time in these regions it will be nearly as efficient as a sequential unipro\u00adcessor implementation. \nFor example,, applying this transformations to our example produces the struc\u00adture shown in Figure 6. \nWhen both x and y are local, this first loop nest is identical to a sequen\u00adtial program. An example of \nthe code which our compiler might generate appears in Figure 7. 3.3 Caching Object andl Context States \nCaching both local temporaries (context state) and heap-allocated objects (object state) in registers \nis required to obtain sequential efficiency. We accom\u00adplish this by refining standard register allocation \ntechniques to account for the multithreaded execu\u00adtion model and object level access control. 3.3.1 \nCaching Context State Caching context state in a multithreaded execution model is complicated by the \npossibility of context switching due to synchronization. Because register values are not preserved across \ncontext switches, the register allocator must guarantee that when a context switch occurs at a touch, \nthe cached state is saved before the thread yielck control. It is also crucial to minimize unnecessary \nstate saving when the t bread does not context switch since the amount of register-cached state can be \nlarge and touches frequent. save values in S L to the context TOUCHBEGIN (Full, future=lot, . ..) save \nvalues in S n L to the context CONTEXTSHITCH Restart: load values in S n L into registers Full : TOUCHJZND \nload values in L S into registers Figure 8: Lazy State Saving at a Touch To minimize unnecessary overhead,, \nwe save and load register cached state lazily by exploiting the runtime test which determines the context \nswitch. Figure 8 shows our touch template, assuming S and L are the set of values saved and loaded respec\u00ad \ntively at a context switch. The runti]me primitive TOUCHBEGIN tests the state of the futures. If all \n futures have values, the code branches to the la\u00adbel Full without blocking; otherwise execution falls \nthrough, saving the shared values in SnL and yield\u00ading control at CONTEXTSHITCH. When the thread re\u00adsumes \nafter a context switch, control returns to the label Restart and immediately restores the shared values \nin S n L into registers. Ix= T guards save(x) restore(x) ...--x ...--x ~ Figure 9: Cent rol flow graph \nof three access regions merged, with three touches (shaded boxes) in the fallback code. The possibility \nof context switching also affects the choice of live mnges[l I] throughout which a value is either cached \nand maintained in a reg\u00adister or kept in memory. Low probabilities of context switching favor live ranges \nextending over touches; high probabilities favor live ranges delim\u00adited bv touches. treatinz touches \nas function calls in a caller-saved linkage convention. The extensive use of speculative irdining eliminates \nsuspension points inside access regions and increase the like\u00adlihood of context switching for suspension \npoints in the fallback code. Therefore, we choose to delimit live ranges by touches and apply a heuristic \nthat caches each live range whose value is accessed at least twice. For example, Figure 9 shows the re\u00adsulting \ncontrol flow graph after speculative inlining and merging of access regions at three call sites. Separate \nlive ranges of x delimited by touches al\u00adlow x to be cached throughout the access region (left) and avoid \nunnecessary reloading overhead in the fallback code (right). 3.3.2 Caching Object State We exploit access \nregions to cache object state in registers safely. Within an access region, the ob\u00adject s state is protected \nby its lock, preventing ac\u00adcesses by other threads. Thus we can safely cache this state in temporary \nvariables, eliminating mem\u00adory accesses and requiring only a single update at the end of the access region \nor before any subse\u00ad quent method invocation. For example, Figure 10 shows two possible code sequences \nfor a loop nest traversing a two\u00addimensional array. The two-dimensional array is constructed from a one-dimensional \narray with the // Code sequence without object stata caching if( CHECK_ LOCAL(a) &#38;&#38; TAKE. LOCKS(a) \n) for(i =O; i <n; i++) for(j = O; j <n; j++) . . . = a[ a.dimension * i + j] ; //a[i] [j] FREE-LOCKS \n(a) ; else ... // Code sequence with object state caching if( CHECK-LOCAL(a) kk TAKE-LOCKS(a) ) temp \n= a.dimension; for(i =O; i <n; i++) for(j = O; j <n; j++) .=a[temp*i+ j]; //a[i] [j] FREEILOCKS (a) ; \n else .. Figure 10: Comparing two output sequences, one with object state caching (bottom) and one without \n(top). inst ante variable dimension of the object a being used for index linearization. The bottom code \nuses the properties of access regions to cache dimension in a local temporary temp, potentially saving \na memory reference in the innermost loop and en\u00adabling other optimizations such as strength reduc\u00adtion. \nAnother advantage of the COOP model is that objects cannot be aliased within the region since an exclusive \nlock is acquired for each object at run time. In effect, the object level access con\u00adtrol servea as a \nform of non-aliasing declaration, enabling loads and stores to be moved freely within the access region \nand making the COOP version po\u00adtentially more efficient than a sequential language version. 4 Results \nTo demonstrate the effectiveness of these transfor\u00admation, we compare the performance of our concur\u00adrent \nobject-oriented system to a low-level sequen\u00adtial language, C [31]. For the comparison, we use the Livermore \nLoops, a set of numerical kernels [35] used to measure computation rates for CPU-limited computational \nstructures. All reported numbers are for the third workload of the Livermore ker\u00adnels at single precision \nrun on a Sparcstation II. The COOP execution times were collected with the UNIX time facility using high \niteration counts, and are accurate to within a few percent. In order to actually test a COOP programming \nstyle, we translated the FORTRAN code in a natu\u00adral object-oriented style. Multi-dimensional arrays were \ncreated by subclassing a single dimensional array and using methods to linearize the indexing operations. \nSince our COOP language does not have pointers, the programmer cannot bypass the encapsulation of the \narrays as is typically done in C++ programs to obtain efficiency. We compare 8  oxll= 7 C 6 3 2 1 0 \n Kernel Figure 11: Performance on Livermore Loops our COOP system s performance against the native remaining \nperformance gap was traced to our back- C version of the Livermore kernels compiled by the end C/C++ \ncompiler being unable in some cases to GNU C/C++ compder.6 This is the same ~om. do common optimizations \non the somewhat unnat\u00adpi.ler used by our COOP system as a backend, min-ural code output by our compiler. \nWe implemented imizing differences in low-level optimizations like these optimizations in our COOP compiler, \nand the instruction selection and scheduling. final results all include the resulting 40% increase in \nperformance, essentially matching the native C 1,6 implementation and nearly 500 times better than that \nachieved by none. 14 Figure 11 contains performance results for all 1,2 of the Livermore Loops. The \nperformance of the 1 COOP code is quite close to that of the native C ~ ~08 code. Essentially all of \nthe object-orientation over\u00ad&#38; 0,8 head and concurrency control overhead has been eliminated. Note \nthat this performance exceeds 04 that which would be delivered by most C++ com\u00ad0,2 pilers on code written \nin an object-oriented style. 0 For example, we measured the performance of two non M.. extend hold cache \nall c representative Livermore kernels in C++ using the GNU C++ compiler. Kernel 12, using virtual func- \nFigure 13: Cumulative Effect of Optimizations on tions to access elements in a one-dimensional array,Kernel \n12 achieves 0.42 MFLOPS less than a third of the COOP or the C performance. Kernel 21, which To illustrate \nthe effect of each optimization we operates on t we-dimensional arrays achieves 0.32 applied each in \nturn to Kernel 12, and present the MFLOPS and even by using non-virtual functions,performance numbers \nin Figure 13. Each addi\u00adachieves only 0.45 MFLOPS less than one fifth tional optimization produced a \nsignificant increase of the COOP or C performance. in performance. With only traditional optimiza\u00adtion, \nnone, achieved only several kiloFLOPS. Ap-In Figure 12 we report the performance of the plying speculative \nirdining produced an eighteen-COOP implementations relative to the C imple\u00adfold performance increase, \nas show by inline. Ex-mentations (( COOP-C)/C). Of the 24 kernels, our panding regions by merging increased \nperformance COOP implementation was more than 20% faster by another 60% while adding lifting access regions \non five, the C implementation was more than 2070 brought this to 44o%. Caching of context values faster \non six, and the remaining thirteen were es\u00adas in cache resulted in 4.5 times performance im-sentially \nthe same. For the codes where the C com\u00adprovement, coming close to C s performance. The piler gave superior \nperformance, these differences were traced to special purpose array manipulation 6 we Sed the hi~h~~t \nof optimization and identical level optimizations in the native C compiler and defi\u00ad compiler options \nfor all of our measurements. 100 80. 60\u00ad40\u00ad20\u00ad 0. -20\u00ad-40\u00ad-60. -80. -1OOJ Figure 12: Performance Difference \n(( COOP-C) /C) ciencies in our strength reduction optimization (it uses extra registers and does not \nwork for opera\u00adtions under conditionals in loops as in Kernel 15). In cases where the COOP system was \nfaster, the major factor was our ability to apply some opti\u00admization where the C compiler was unable \nto. The main point of these results is that the COOP mo\u00addel can be essentially as efficient at a sequential \nC programming model. Any differences that remain are purely in the purview of traditional low level optimization. \nRelated Work The fine-grained approach to COOP has been stud\u00adied extensively [33]. In particular, ABCL \n[45, 46, 47] and Concurrent Smallt alk (CST) [25, 26] were instrumental in helping define the programming \nand implementation models described here, How\u00adever, their focus was not on extensive compile\u00adtime inter-object \ntransformations. A variety of other parallel object-oriented systems pursue the approach of relying on \nan underlying sequential lan\u00adguage for efficiency [4, 8, 18, 29, 32, 40]. Our work also draws on developments \nin both the sequential and parallel compiler community. While most of our techniques are familiar ones, \nwe have adapted them significantly to the COOP mo\u00addel. Many researchers have studied inlining for se\u00adquential \nlanguages [2, 28, 34]; however, their main concern is different from our focus on concurrency and locality. \nOur irdining techniques are most sim\u00adilar to the ones used in the SELF compiler [6, 7, 24] in their requirement \nfor accurate type information and customization to enable inlining, speculative optimization, and the \ninsertion of runtime checks to condition optimized code. Our inlining heuris\u00adtics are a combination of \nstatic frequency estima\u00adtion [44] and the commonly used size constraints. One unique aspect of our inlining \ntransformations is the creation of access regions and the aggressive exploitation of access region properties \nby subse\u00adquent optimizations. The lifting of access region is conceptually sim\u00adilar to moving loops across \nprocedure boundaries and lifting and blocking of communication in par\u00adallel Fortran [19, 22]. In our \ncase, the possibility of deadlock requires atomic primitives and more ex\u00adtensive analysis. Our register \nallocation scheme is based on that of Chow and Hennessy [11], adapted for lazy state saving. The problem \nof register allo\u00adcation in the presence of synchronization points has been studied in dataflow models \n[14, 41, 43], but the model is slightly different. For instance, TAM has many t breads per cent ext, \nwhereas our execution model has only single t bread per cent ext, making local analysis around the touches \nsufficient. The non-aliasing property of an access region s objects inside the region achieves runtime \ndisambiguations of objects. Previous work [27, 37] on runtime dis\u00adambiguation focuses on memory accesses \nat the in\u00adstruction level. 6 Summary and Future Work We have shown that it is possible to produce effi\u00adcient \nimplementations from high-level COOP lan\u00adguages, dispelling the myth that such a program\u00adming model is \ninherently inefficient. Using a de\u00admanding set of numerical benchmarks, the Liv\u00adermore Kernels, we have \ndemonstrated that our concurrent object-oriented programming model can achieve good sequential performance. \nThki sequen\u00adtial efficiency forms an important basis for high ab\u00adsolute performance through hardware \nparallelism. However, it is only half of the solution. The ef\u00adfectiveness of the optimizations depends \nthe data being available (local and not currently in use). Work isunderway on both static analyses [39] \nand runtime techniques [30] to enable the system to en\u00adsure availability and thus apply the optimizations \nin a more informed manner, with the goal of freeing the programmer from the burden of data and task placement. \nWe have presented a simple programming model and implementation model for a pure concurrent object-oriented \nlanguage which includes a shared global namespace, dynamic thread creation and ob\u00adject level access control \nand shown it can be ef\u00adficient. Our continuing research is directed to\u00adward developing additional optimiz \nat ion for array and pointer based data structures through data lay\u00adout, program analysis and transformation \nand run\u00adtime migration techniques. We are optimistic that through the development of such techniques \ncon\u00adcurrent object-oriented programming can enable ef\u00adficient, portable parallel programming. Acknowledgements \n We thank Julian Dolby for his help porting the Liv\u00adermore Kernels and also Vij ay K aramcheti and Ma\u00adhesh \nSubramaniam for discussions and work on the Concert System. We also thank the reviewers for their valuable \ncomments. The research described in this paper was sup\u00adported in part by National Science Foundation \ngrant CCR-9209336, Office of Naval Research grants NOO014-92-J-1961 and NOOO14-93-1-1O86, and National \nAeronautics and Space Administra\u00ad tion grant NAG 1-613. Additional support has been provided by a generous \nspecial-purpose grant from the AT&#38;T Foundation. References [1] Gul Agha. Actors: A Model of Concurrent \nCom\u00adputation in Distributed Systems. MIT Press, Cam\u00adbridge, MA, 1986. 121 Randv Allen and Steve Johnson. \nComDilinz C for . .. vectorization, parallelization, and irdine expansion. In proceedings of the 1988 \nACM SIGPLAN Con\u00adference on Programming Language Design and Im\u00adplementation, pages 241 249, June 1988. \n[3] J. Bennett, J. B. Carter, and Winy Zwaenepoel. Munin: Distributed shared memory based on type\u00adspecific \nmemory coherence. In proceedings of the Second ACM SIGPLAN Sympos%um on the Prin\u00adciples and Practice \nof PaTallel Programming, 1990. [4] B.N. Bershad, E.D. Lazowska, and H.M. Levy. Presto: A system for object-oriented \nparallel pro\u00adgramming. Software Practice and Experience, 18(8):713 732, August 1988. [5] C. A. R. Hoare. \nMonitors: An operating system stmcturing concept. Communications of the Asso\u00adciation for Computing Machinery, \n17(10):547 557, 1974. [6] C. Chambers knd D. Ungar. Customization: Optimizing co)npiler technology for \nSELF, a dynamically-typed object-oriented programm ing language. In Pr ceedings o.f SIGPLAN Conference \n1 on Programming Language Design and Implemen\u00ad tation, pages 14b 60, 1989. [7] C. Chambers a d D. Ungar. \nIterative type analysis ~ and extended message splitting. In Pr-oceedings of the SIGPLAN conference on \nProgramming Lan\u00ad guage Design a d Implementation, pages 150 60, 1990. [8] K. Mani Chaudy 1 and Carl Kesselman. \nComposi\u00ad tional C++: Co positional parallel programming.P In proceedings f the Fifth Workshop on Compil\u00adf \ners and Languages for Parallel Computing, New Haven, Connect icut, 1992. YALEU/DCS/RR-915, Springer-Verlag \nLecture Notes in Computer Sci\u00adence, 1993. [9] Andrew Chien, Vijay Karamcheti, and John Plev yak. The \nconcert system compiler and runtime supper c for efficient fine-grained concur\u00adrent object-orie nted \nprograms. Technical Report UIUCDCS-R-93 -1815, Department of Computer Science, Univer sity of Illinois, \nUrbana, IWnois, June 1993. [10] Andrew A. C~en, Vijay Karamcheti, John Plevyak, and Xmgbin Zhaug. Concurrent \naggre\u00adgates language keport 2.0. Available via anony\u00admous ftp from ~s .uiuc .edu in /pub/c sag or from \nhttp://www-csa .cs.uiuc.edu/, September 1993. 1 [11] Frederick C. Chow and John L. Hennessy. The priority-based \nc loring approach to register allo\u00adcation. ACM T t ansactions on Programming Lan\u00adguages and Syst~ms, \n12(4):501 536, October 1990. [12] William D. Cli ger. Foundations of actor seman\u00adtics. Technical i eport \nAI-TR-633, MIT Artificial Intelligence Lab~ratory, 1981. [13] Cray Research, Inc., Eagan, Minnesota 55121. \nCRA Y T3D SoL waTe Ouervzew Technzcal Note, 1992, [14] David Culler, nurag Sah, Klaus Erik Schauser, \n{ Thorsten von Eicken, and John Wawrzynek. Fine\u00ad grain parallelism with minimal hardware support: A compiler-cent \noiled threaded abstract machine. In Proceedings f the Fourth International Con\u00ad 1 ference on Architectural \nSupport for Programming Lan~ua]es an dperating Systems, pages 164-75, 1991. [15] R. Cytron, J. Fe~ante, \nB. Rosen, M. Wegman, and F. Zadeck. An e cient method of computing static T  single assignment form \nand the control dependence graph. ACM T ansactions on Programming Lan \u00ad7 guages and Systems, 13(4):451490, \nOctober 1991. [16] Jeanne Ferrant e , Karl J. Ottenstein, and Joe D. Warren. The program dependence graph \nand its use in opti zation. ACM Transactions on T Pr-ogTamming L guages and Systems, 9(3):319 49, July \n1987. [17] Adele Goldberg and David Robertson. Smal(tak\u00ad 80: The lan 1 tiage and tts implementation. \nAddison-Wesley, 1985. [18] A. Grimshaw. Easy-to-use object-oriented par\u00adallel processing with Mentat. \nIEEE Computer, 5(26):39 51, Ma 1993. 1 [19] Mary W. Hall, Ken Kennedy, and Kathryn S. McKinley. Inter \nrocedural transformations for par\u00ad 4 allel code genera ion. In Proceedings of the 4th An\u00adnual Conference \non fIigh-Perfor-mance Computing 1 [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [30] [31] [32] [33] \n[34] (Supercomputing 91), pages 424 434, November 1991. P. Brinch Hansen. Structured multiprogramming. \nCommunications of the ACM, 15(7):574-590, July 1972, C. Hewitt and H. Baker. Actors and continuous functional. \nIn Proceedings of the IFIP Work\u00ading Conference on Formal Description of Program\u00ad ming Concepts, pages \n367 87, August 1977. Seems Hiranandani, Ken Kennedy, and Chau-Wen Tseng. Compiler optimizations for FORTRAN \nD on MIMD distributed-memory machines. Commu\u00adnications of the A CM, August 1992. Richard C, Holt. Some \ndeadlock properties of computer systems. ACM Computing Surveys, 4(3):179 196, Sept 1972. Urs H61zle and \nDavid Ungar. Optimizing dynamically-dispatched calls with run-time type feedback. In Proceedings o.f \nthe 199.4 ACM SIG-PLAIV Conference on Programming Language De\u00adsign and Implementation, pages 326 336, \nJune 1994. W. Horwat, A. Chien, and W. Dally. Experience with CST: Programming and implementation. In \nPToceed{ngs of the SIGPLAN Conference on Pro\u00ad gramming Language Design and Implementation, pages 101 \n9. ACM SIGPLAN, ACM Press, 1989. Waldemar Horwat. Concurrent Smalltalk on the message-driven processor. \nMaster s thesis, Mas\u00adsachusetts Institute of Technology, Cambridge, Massachusetts, June 1989. Andrew \nS. Huang, Gert Slavenburg, and John Paul Shen. Toward a dataOow/von nenmann hybrid architecture. In Proceedings \no.f the International Symposium on Computer Architecture, pages 200 210, April 1994. Wen-rnei W. Hwu \nand Pohua P. Chang. Inline func\u00adtion expansion for compiling C programs. In Pro\u00adceedings oj the 1989 \nACM SIGPLAN Conje?ence on Programming Language Design and Implemen\u00ad tation, pages 246 257, June 1989. \nL. V. Kale and Sanjeev Krishnan. CHARM++: A portable concurrent object oriented system based on C++. \nIn Proceedings of OOPSLA 93, 1993. Vijay Karamcheti and Andrew Chlen. Concert -efficient runtime support \nfor concurrent object\u00adoriented programming languages on stock hard\u00adware. In Proceedings oj Supercomputing \n93, 1993. Brian W. Kernighan and Dermis M. Ritchie. The C Programming Language. Prentice-Hall, Inc., \nEn\u00adglewood Cliffs, New Jersey, 1978. J. Lee and D. Gannon. Object oriented parallel progr amming. In \nProceedings o.f the A CM/IEEE Conference on Supercomputing. IEEE Computer Society Press, 1991. Henry \nLieberman. Concurrent object oriented pro\u00adgrammi ng in ACT 1. ~ Aki YOnezawa and M~io Tokoro, editors, \nObject-OTiented Concurrent PTo\u00adgmmming, pages 9 36. MIT Press, 1987. Scott McFarling. Procedure merging \nwith instruc\u00adtion caches. In Proceedings oj the 1991 ACM SIG-PLAN ConjeTence on Programming Language \nDe\u00adsign and Implementation, pages 71 79, Juue 1991. [35] F. H. McMahon. The Livermore Fortran kernels: \na computer test of the numerical performance range. Technical report UC RL-53745, Lawerence Liver\u00admore \nNational Laboratory, Livermore, California, 1986. [36] Stephan Murer, Jerome A. Feldman, Chu-Cheow Lim, \nand Martina-Maria Seidel. pSather: Layered extensions to an object-oriented language for effi\u00adcient parallel \ncomputation. Technical Report TR\u00ad93-028, International Computer Science Institute, Berkeley, Calif., \nDecember 1993. (2nd revised edi\u00adtion). [37] A. Nicolau. Run-time disambignation: Coping with statically \nunpredictable dependencies, IEEE Transactions on Computers, 38(5):663 678, May 1989. [38] John Plevyak \nand Andrew A. Chien. Precise con\u00adcrete type inference of obj ect-oriented programs. In proceedings of \n00PSLA, 1994. [39] John Plev yak, Vijay Karamcheti, and Andrew Chien. Analysis of dynamic structures \nfor effi\u00adcient pamdlel execution. In Proceedings oj the Sixth Workshop for Languages and Compilers jor \nPai-\u00adalle{ Machines, August 1993. [40] R. J. Smith, II. Experimental systems kit final project report. \nTechnical Report ACT-ESP-077-91, Microelectronics and Computer Technology Corpo\u00adration (MCC), Austin, \nTexas., 1991. [41] S. %kai, Y. Yamagnchi, K. Hiraki, Y. Kodama, and T. Yuba. An architecture of a datatlow \nsin\u00adgle chip processor. k International Symposium on Computer Architecture, 1989. [42] Thinking Machines \nCorporation, 245 First Street, Cambridge, MA 02154-1264. The Connection Ma\u00adchine CM-5 Technical SummaTy, \nOctober 1991. [43] K. Traub. Implementation of Non-strict Func\u00adtional Languages. Research Monographs \nin Parallel and Distributed Computing. MIT Press, 1991. [44] Tim A. Wagner, Vance Maverick, Susan L. \nGra\u00adham, and Michael A. Harrison. Accurate static es\u00adtimators for program optimization. In Proceedings \noj the ACM SIGPLAN Conference on Program\u00adming Language Design and Implementation, pages 85 96, Orlando, \nFlorida USA, June 1994. [45] Y. Yokote and M. Tokoro. Concurrent program\u00adming in ConcurrentSmalltalk. \nIn Aki Yonezawa and Mario Tokoro, editors, Object-OTiented Con\u00adcurrent Programming, pages 129 158. MIT \nPress, 1987. [46] Akinori Yonezawa, editor. ABCL: An Object\u00ad(hiented Concurrent System. MIT Press, 1990. \nISBN 0-262-24029-7. [47] Akinori Yonezawa, Satoshi Matsuoka, Masahiro Yasugi, and Kenjiro Tanra. Implementing \nconcur\u00adrent object-oriented languages on multicomputem. IEEE Parallel and Distributed Technology, pages \n49 61, May 1993. \n\t\t\t", "proc_id": "199448", "abstract": "<p>Concurrent object-oriented programming (COOP) languages focus the abstraction and encapsulation power of abstract data types on the problem of concurrency control. In particular, pure fine-grained concurrent object-oriented languages (as opposed to hybrid or data parallel) provides the programmer with a simple, uniform, and flexible model while exposing maximum concurrency. While such languages promise to greatly reduce the complexity of large-scale concurrent programming, the popularity of these languages has been hampered by efficiency which is often many <italic>orders of magnitude</italic> less than that of comparable sequential code. We present a sufficiency set of techniques which enables the efficiency of fine-grained concurrent object-oriented languages to equal that of traditional sequential languages (like C) when the required data is available. These techniques are empirically validated by the application to a COOP implementation of the Livermore Loops.</p>", "authors": [{"name": "John Plevyak", "author_profile_id": "81100302772", "affiliation": "Department of Computer Science, 1304 W. Springfield Avenue, Urbana, IL", "person_id": "P145562", "email_address": "", "orcid_id": ""}, {"name": "Xingbin Zhang", "author_profile_id": "81451600700", "affiliation": "Department of Computer Science, 1304 W. Springfield Avenue, Urbana, IL", "person_id": "PP31101272", "email_address": "", "orcid_id": ""}, {"name": "Andrew A. Chien", "author_profile_id": "81406600821", "affiliation": "Department of Computer Science, 1304 W. Springfield Avenue, Urbana, IL", "person_id": "PP79027166", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/199448.199524", "year": "1995", "article_id": "199524", "conference": "POPL", "title": "Obtaining sequential efficiency for concurrent object-oriented languages", "url": "http://dl.acm.org/citation.cfm?id=199524"}