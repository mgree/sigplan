{"article_publication_date": "01-25-1995", "fulltext": "\n Demand-driven Computation of Interprocedural Data Flow * Evelyn Duesterwald Rajiv Gupta Mary Lou Soffa \nDepartment of Computer Science University of Pittsburgh Pittsburgh, PA 15260 {duester,gupta,soffa} Qcs.pitt \n.edu Abstract This paper presents a general framework for deriving demand\u00addriven algorithms for interprocedural \ndata flow analysis of imperative programs. The goal of demand-driven analysis is to reduce the time and/or \nspace overhead of conventional exhaustive analysis by avoiding the collection of information that is \nnot needed. In our framework, a demand for data flow information is modeled as a set of data flow queries. \nThe derived demand-driven algorithms find responses to these queries through a partial reversal of the \nrespective data flow analysis. Depending on whether minimizing time or space is of primary concern, result \ncaching may be incorporated in the derived algorithm. Our framework is applicable to inter\u00adprocedural \ndata flow problems with a finite domain set. If the problem s flow functions are distributive, the derived \nde\u00admand algorithms provide as precise information as the corre\u00adsponding exhaustive analysis. For problems \nwith monotone but non-distributive flow functions the provided data flow solutions are only approximate. \nWe demonstrate our ap\u00adproach using the example of interprocedural copy coustant propagation, 1 Introduction \nPhrased in the traditional data flow framework [KU77], the solution to a data flow problem is expressed \nas the fixed point of a system of equations. Each equation expresses the solution at one program point \nin terms of the solution at immediately preceding (or succeeding) points. This formu\u00ad lation results \nin an inherently exhaustive solution; that is, to find the solution at one program point, the solution \nat all points must be computed. This paper presents an alternative approach to program analysis that \navoids the costly computation of exhaustive solutions through the demand-driven retrieval of data flow \ninformation. We describe a general framework for deriv\u00ad ing demand-driven algorithms that is aimed at \nreducing the Partially supported by National ScienceFoundation Presidential Young Investigator Award \nCCR-9157371 and Grant CCR-9109OS9 to the University of Pittsburgh Permissionto copy without fee aii or \npart of this material is granted provided that the copies are not made or distributed for direct commercial \nadvantage, the ACM copyright notice and the titie of the publication and its date appear, and notice \nis given that copyin is by permission of the Association of Computing Machinery.+ o copy otherwise, or \nto repubiish, requires a fee anchorspecific permission. POPL 951/95 San Francisco CA USA 0 1995 ACM 0-89791-892-1/95/0001....$3.50 \ntime and/or space consumption of conventional exhaustive analyzers. Demand-driven analysis reduces the \nanalysis cost by pre\u00adventing the over-analysis of a program that occurs if parts of the analysis effort \nare spent on the collection of superfluous information. Optimizing and parallelizing compilers that ex\u00adhaustively \nanalyze a program with respect to each data flow problem of interest are likely to over-analyze the program. \nTypically, code transformations are applied only selectively over the program and therefore require only \na subset of the exhaustive data flow solution. For example, some optimiza\u00adtion are applicable to only \ncertain structures in a program, such as loop optimizations. Even if optimizations are appli\u00adcable everywhere \nin the program, one may want to reduce the overall optimization overhead by restricting their appli\u00adcation \nto only the most frequently executed regions of the program (e.g., frequently called procedures or inner \nloops). One strategy for reducing the analysis cost in these ap\u00adplications is to simply limit the exhaustive \nanalysis to only selected code regions. However, this strategy may prevent the application of otherwise \nsafe optimizations due to the worst case assumptions that would have to be made at the entry and exit \npoints of a selected code region. For exam\u00adple, data flow information that enters a selected code region \nfrom outside the region is vital in determining the side effects of procedure calls contained in that \nregion. SimilarIy, data flow from outside a loop may be needed to simplify and/or determine the loop \nbounds or array subscripts in the loop. These applications favor a demand-driven approach that al\u00adlows \nthe reduction of the analysis cost while still providing all necessary data flow information. Another \nadvantage of demand-driven analysis is its suit\u00adability for servicing on-line data flow requests in software \ntools. Interactive software tools that aid in debugging and understanding of complex code require information \nto be gathered about various aspects of a program. Typically, the information requested by a user is \nnot exhaustive but selec\u00adtive, i.e., data flow for only a selected area of the program is needed. Moreover, \nthe data flow problems to be solved are not fixed before the software tool executes but can vary depending \non the user s requests. For example, during de\u00adbugging a user may want to know where a certain value \nis defined in the program, as well as other data flow informa\u00adtion that would help locate bugs. A demand-driven \nanalysis approach naturally provides the capabilities to service re\u00adquests whose nature and extent may \nvary depending on the user and the program. The utility of demand-driven analysis has previously been \n demonstrated for a number of specific analysis problems terprocedural copy constant propagation for \nillustrating the [CCF92, CHK92, CG93, SY93, SMHY93, Mas94]. Unlike these applications, the objective \nof our approach is to ad\u00address demand-based analysis in a general way. We present a lattice based framework \nfor the derivation of demand-driven algorithms for interprocedural data flow analysis. In this framework, \na demand for a specific subset of the exhaustive solution is formulated as a set of queries. Queries \nmay be generated automatically (e.g., by the compiler) or manually by the user (e.g., in a software tool). \nA query q=<y, n> raises the question as to whether a specific set of facts y is part of the exhaustive \nsolution at program point n. A re\u00adsponse (true or j alse) to the query q is determined by prop\u00adagating \nq from point n in reverse direction of the original analysis until all points have been encountered that \ncon\u00adtribute to the response for q. This query propagation is modeled as a partiaJ reversal of the original \ndata flow analy\u00adsis, Specifically, by reversing the information flow associated with program points, \nwe derive a system of query propaga\u00adtion rules. The response to a query is found after a finite number \nof applications of these rules. We present a generic demand algorithm that implements the query propagation \nand discuss two optimizations of the algorithm: (i) early\u00adtermination to reduce the response time for \na single query and (ii) result caching to optimize the performance over a sequence of queries. In the \nworst case, in which the amount of information demanded is equal to the exhaustive solu\u00adtion, the asymptotic \ncomplexity of the demand algorithm is no worse than the comdexitv of a standard iterative ex\u00adhaustive \nalgorithm, - The derivation of demand algorithms is based on a con\u00adventional exhaustive interprocedural \nanalysis framework. Several formal frameworks for (exhaustive) interprocedural analysis have been described \n[CC77, Ros79, JM82, SP81, KS92]. We use the framework by Sharir and Pnueli [SP81] as the basis for our \napproach. We first follow the assump\u00adtions of the Sharir-Pnueli framework and consider programs with \nparameterless (recursive) procedures and with a single global address space. We then consider extensions \nto our framework to allow non-procedure valued reference param\u00adeters and local variables. These extension \nare discussed for the example of demand-driven copy constant propagation. Our approach is applicable \nto monotone interprocedural data flow problems with a finite domain set (finite set of facts) and yields \nprecise data flow solutions if all flow func\u00adtion are distributive. This finiteness restriction does \nnot apply if the program under analysis consists of onlY a sin\u00adgle procedure (the tmbwprocedural case). \nThe distributivity of the flow functions is needed to ensure that the derived demand algorithms are as \nprecise as their exhaustive coun\u00adterparts. Conceptually, our approach may also be applied on problems \nwith monotone but non-distributive flow func\u00adtions at the cost of reduced precision. We discuss the loss \nof information that is caused by non-distributive flow func\u00adtions and show how our derived demand algorithm. \ncan still be used to provide approximate but safe query responses for non-distributive problems. The \nclass of distributive and finite data flow problems that can be handled precisely includes, among others, \nthe interprocedural versions of the classical bitvector problems, such as live variables and available \nexmessions, as well as common interprocedural problems, such as procedure side\u00adeffect analysis [C K88]. \nWe have chosen the example of in\u00addemand-driven framework in this paper. Section 2 reviews Sharir and \nPnueli s interprocedural framework. In Section 3 we derive a system of query prop\u00adagation rules from \nwhich we establish a generic demand al\u00adgorithm. We discuss optimizations of the generic algorithm which \ninclude early termination and result caching in Sec\u00adtion 4. Section 5 demonstrates the demand algorithm \nusing the example of interprocedural copy constant propagation and presents the extensions to include \nreference parameters and local variables. We discuss related work in Section 6 and conclusions are given \nin Section 7. 2 Background A program consisting of a set of (recursive) procedures is represented as \nan interprocedural J70 w graph (IFG) G = {G,,..., Gk } where Gp = (lVP, EP) is a directed flow graph \nrepresenting procedure p. Nodes in NP represent the state\u00adments in p and edges in Ep represent the transfer \nof con\u00adtrol among statements. Two distinguished nodes rP and eP represent the unique entry and exit nodes \nof p. The set E = u{E,ll < z < k} denotes the set of all edges in G, N = u{N, I1 < z < k} denotes the \nset of all nodes in G and pred(n) = {ml(rrz, n) c E,} and succ(n) = {ml(n, m) E E,} denote the set of \nimmediate predecessors and successors of node n, respectively. We assume that IEl = 0( I.NI). Finally, \nIVc.il ~ N denotes set of nodes representing procedure ca.lk (call sites) and for each node n c NCail, \ncall(n) denotes the procedure called from n. An example of an IFG is shown in Figure 1. During the analysis, \nonly vaiid interprocedural execution paths should be considered. An execution path rr is valid if ~ returns \nafter a procedure exit node eP to the matching call site that most recently occurred in r prior to eP. \nFor a node n in an interprocedural flow graph G, lP(r~~*~, n ) denotes the set of valid execution paths \nfrom the program entry node r~a,~ to node n. For example in Figure 1, the path 1,2,3,4,6,7,10, 11,4,5 \nis a valid execution path, while the path 1,2,3,4,6,7, 10, 11,9, 11,4,5 is not valid. Note that interprocedural \nexecution paths are not directly represented in an IFG since it does not contain explicit edges between \ncall and procedure entry nodes or between procedure exit nodes and return nodes. Throughout this paper \nwe assume that G is an interpro\u00adcedural flow graph representing some program P. 2.1 Interprocedural \nAnalysis Framework Data flow problems are solved by globally gathering informa\u00ad tion from a domain set \nof program facts. This information gathering process is modeled by a pair (L, F), where: -L is a complete \nlattice with a partiaJ order Q, a least el\u00adement 1 (bottom), a greatest element T (top), and a meet operator \nn (greatest lower bound) and a dual join operator U (least upper bound). L has the decreasing chain property \n(i.e., any decreasing chain ZI ~ zz ~ . . . is finite). The bot\u00adtom element L denotes null reformation \nand we assume that the top element T denotes undefined information 1. 1If ~ &#38;e~ not already contain \na top element with the meaning undefined information , it can always be extended to include an additional \nnew top element, procedure p procedure main main 6 entry declare a,b; 1 entry f6(x.,xb)=(x,,xb) begin \na:=l; read(b); call p; end procedure p begin if (cond) then a:=b else b:=l; call p; endifi end fl(x,,xb)=(x.,xb) \n2 a:zl z II f~(x=,xh)=(l ,xb) 3 read(b) f3(xa,xb)=(xa,l ) 4 call(p) A 8 J if (cond) fT(x,,xb)=(xa,xh) \nR / --\u00ad b:=l f~(xa,xb)=(x,,l) 9 call(p) 22 23 0(6,11) @(6,11)(%,xb)=if Xh=l then (1,1) else Q-,~) flO(x.>xb)=(xb,xb) \n5 exit 1 w exit Figure 1: An IFG with the local flow functions for copy constant propagation. F ~ {f \n: L ++ L} is a set of monotone functions over L ~i.e, z ~ y + j(z) ~ ~(y)). A locrd flow function fn \nE F is mapped to each node n E N iV.~LI to model the local data flow effect of exe\u00adcuting node n. A \ndata flow analysis is called fl-distt-ibutive if all local flow functions are n-distributive (i.e, $(z \nn y) = ~(z) n ~(y)). The exhaustive solution to an (interprocedural) data flow problem is the meet\u00ad over\u00ad \nall\u00ad valid-pa ths solution mop: NwL: If the analysis is n-distributive then the mop solution can be \ncomputed as the greatest fixed point of a system of data flow equations [SP81]. Data flow equations are \nevaluated in a two-phase approach. During the first phase the data flow effect of each procedure is analyzed \nindependent of its calling context. The results of this phase are procedure summary ~unctions as defined \nin equation system 1. The summary function @(.P,eP): L w L for procedure p maps data flow facts from \nthe entry node rP to the corresponding set of facts that holds upon procedure exit. Equation System 1 \nThe actual calling context of called procedures is propagated during the second phase based on the summary \nfunctions. The solution to the data flow problem is defined as the great\u00adest fixed point of equation \nsystem 2, where x(n) describes the data flow solution on entry of node n. For finite lattices, Sharir \nand Pnueli propose an iterative worklist-driven tabulation algorithm to solve the equation systems 1 \nand 2. Their algorithm requires 0( [L I x IN I) space to tabulate equations from 1 and 2 and the time \nfor O(C x Equation System 2 For each procedure p: I(?-p) = n x(m) WW..II and c~u(m)=P For each procedure \np and node n # rP: ( fm(x(m)) if m@NCau x(n) = n if mEN=~ll, m~pred(n) #{rq,eq)(~(m)) call(m) =q [ \n height(L) x ILI x INI) meet operations and/or applications of local flow functions, where C is the maximal \nnumber of call sites calling a single procedure and hetght(L) is the height of lattice L (i.e., the length \n 2.2 Example: Copy We iUustrate our approach example of copy constant of the longest chain in L). Constant \nPropagation throughout this paper using the propagation (CC P). CCP is a distributive version of the \n(non-distributive) constant prop\u00adagation analysis with expression evaluation [Ki173]. A vari\u00adable is \na copy constant if it is either assigned a constant value or it is assigned a copy of another variable \nthat is a copy constant. Since no expressions are evrduat ed, CCP is less expensive but may dkcover fewer \nconstants than con\u00adstant propagation with expression evaluation. Recent stud\u00adies on interprocedural constant \npropagation [GT93] indicate that the discovery of constants based on copies may be as effective in practice \nfor the interprocedural propagation as the more costly discovery of constants based on symbolic evaluation. \nA demand-driven algorithm for interprocedural copy con\u00adstant moDa~ation limits the analvsis effort to \nthe discoverv . of the-interesting interprocedural constants. For example in parallelization, the interesting \ninterprocedural constants in\u00adany integer undefined exhaustive solution at a selected program node n. \nA lattice A element y is a safe approximation of the solution x(n) if y . . . cl-l c, Cl+l constant \nis lower in the lattice than z(n). Figure 2: The component lattice for copy constant propa\u00adgation for \na single program variable. elude the values of variables or formal parameters that occur in array subscript \nexpressions or in loop bounds. Determining that a variable w is a copy constant requires the simultaneous \nanalysis of all programs variables. CCP is not a partttzonable [Zad84] analysis that would permit the \nseparate analysis of each variable as, for example, is possible in live variable analysis. The CCP lattice \nfor a program with k variables is the product Lk, where the component lattice L is defined as shown in \nFigure 2. Thus, each lattice element z E Lk is a k-tuple with one component (z)O E L for each variable \nW. The meet operator n and the dual join operator u are defined pointwise according to the partial order \ndepicted in Figure 2. Of particular interest are the base elements in Lk. A base element is a tuple x \nindicating that a single variable v has some constant value c and that alf other variables are not constant: \n(z). = c and (fl)~ = 1 for w # v. We use the simplified notation [V=C]C Lk for such a base element x. \nFor readabifit y, we also write [w=c, W=C] for the join of base elements: [o=c] U [w=c]. Furthermore, \nwe also use T and 1 for the top and bottom element of the product lattice Lk. We define the distributive \nflow functions j in CCP point\u00adwise for each component corresponding to one of k variables, i.e., .f(zl, \n. . ..z~)=f(zlzl ,.. .,xk)l, ..., .f(Zl,..., Zk)k). The component .f(z I, . . . . Xk)W of a local flow \nfunction .f with re\u00adspect to variable w is defined in Table 1 for various types of assignments. The local \nflow function for a conditional expression is simply the identity function locaf flow function component \n~.(z) ~,node n wherez= zl, . . ..z~ V:=c fn(z)w = ~: yt;e; :e Zfv=w IJ:= U fn(z)w = ;: otherwise u:=tor \nfn(z)w = ,: j;e; :e read(v) Table 1: The local flow functions in CCP, where u, v and u] are variables, \nc is a constant and t is an expression. In Figure 1, the local flow function associated with each node \nis shown below the node. Each lattice element is a pair (t., z,), where the first and second component \ndenote lattice values for variables a and b, respectively. The execution effect of procedure calls (nodes \n4 and 9) is modeled by the summary function @(G,l1, as defined in equation system 1. The full definition \nof fj,]I) is shown only at node 4. $4 3 Propagating Data Flow Queries A data flow query q raises the \nquestion as to whether a specific set of facts y c L is a safe approximation of the Definition 1 (Data \nflow query) Let y s L and n c N. A data flow query q is of the form q =< y, n > and denotes the truth \nvalue of the term: y L z(n). For the program in Figure 1, consider the question as to whether variable \na is a copy constant after returning from the calf to procedure p from rnazn, i.e., on entry of node \n5. The least lattice element that expresses that a has some arbitrary but fixed constant value c is the \nelement (a=c, b= J-) = [a=c] (i.e., variable 11may assume any value). Thus, the question corresponds \nto the query q =<[a=c], 5>. We now consider the problem of determining the answer (true or ~alse) for \na query q without exhaustively evaluat\u00ading the equation systems 1 and 2. Informally, the answer to q=<y, \nn> is obtained by propagating q from node n in reverse direction of the original analysis until all nodes \nhave been encountered that contribute to the answer for q. We model thk propagation process as a parttcd \nreversal of the original data flow analysis. To illustrate the reversal of the analysis we examine the \nfollowing cases in the propagation of q =< y, n >. Case (i) (Node n = r~a,~): no further propagation \nof q is possible and q evaluates to true if and only if y = J_. Case (ii) (Node n = rP for some procedure \np): q raises the question as to whether y holds on entry of every invocation of p. It follows that q \ncan be translated into the boolean conjunction of queries < y, m > for every call site m calling procedure \np. Case (iii) (Node n is some arbitrary non-entry node): For simplicity, assume for now that n has a \nsingle predecessor m. Equation system 2 shows that y G z(n) if and only if y ~ h(z(m)), where h is either \na local flow function or a summary functiou. By the monotonicity of h, the term y G h(z(m)) directly \nevaluates to true if y ~, h(l) and to false if y Q h(T). Otherwise q translates mto a new query q =< \nz, m > for node m. The lattice element z to be queried on entry of node m should be the least element \n(i.e., smallest set of facts) such that z L x(m) implies y ~ h(z(m)). To find the appropriate query element \nz for the new query g we apply the reverse function h [HL92]. Definition 2 (Reverse function) Given a \ncomplete lat\u00adtice L and a monotone function h : L ++ L, the reverse function h : L ++ L is defined as: \nh (y) =n {z EL:YE h($)} The reverse function h maps y to the smallest element z such that y ~ h(z). Note, \nthat if no such element exists h (y) = T (undefined). If the function h is n-distributive then the following \nrela\u00ad tionship holds between function h and its reverse h [COU81, HL92]: y ~h(z) + h (y) ~z (GC) The \nabove relationship uniquely determines the reverse func\u00adtion and defines a Galois connection [Bir84] \nbetween h and its reverse h . Note, that the n-distributivity is necessary for establishing this relationship. \nFor the remainder of this section we consider only distributive flow function and show how the resulting \nrelationship between flow functions and their reverse functions can be exploited during the query propagation. \n First consider the following properties of the function reversal. It can easily be shown that the fl-distributivity \nof h implies the I-1-distributivity of the reverse function h : /Z (ZUy) =lZr(X)LIh (y). Lemma lstates \nrelevant properties with respect to the composition, the meet and the join of functions. Lemma 1 Let \ng and h be two fl-distributive functions. (i) (g. h) = h ~g (ii) (gmh) = g uh  Proof: straightforward \nand omitted for brevity (see also [HL9z]). Table 2 shows the definition of the reverse flow functions \nin CCP. For all flow functions ~~ (T) = T and ~~(1-) = 1. By the LI-distributivit y of the reverse functions, \nit is ~u$icient to define ~~ for the base elements in the lattice L. node n reverse flow function f; \n( [w=c] ) ifw=v, C=c U:=CJ if w=v, c#c [w=cl otherwise if w=V ?J:=U .fl([w=cl){ {~~j] o~herwise v:+, \nif w=V f;([w=c]) [WLC] otherwise read(v) Table 2: The reverse local flow functions in CCP. The reverse \nfunction value f: (~w=cl) denotes the least lat\u00ad ---.. tice element, if any exists, that mus~ hold on \nentry of node n in order for variable w to have the constant value c on exit of n. If f~([v=c]) = J-, \nthe trivial value 1 is sufficient on entry of node n (i.e., v always has value c on exit). The value \nf: ([v=c]) = T indicates that there exists no entry value which would cause v to have the value c on \nexit. The following propagation rules result immediately from the definition of the exhaustive equation \nsystem 2 in a n\u00addistributive data flow framework and the properties of the reverse functions. The operator \nA denotes the boolean AND operator. Query Propagation (distributive functions) true ify=l < y, r~.$. \n> U false otherwise { <y, rP> * <y)m> A nzeNc.ll, call(m)=p fake if hmr(y)=T <y, n>u true if h~ (y)=l. \n771Gpred(n) < h~ (y), m > otherwise { if m$Ncau f?n where h~ = q$(rp,ep) if m 6 N.~U, call(m) = p { The \npropagation rules require the application of reverse functions. If node m is not a call site the reverse \nfunction f; can be determined by locally inspecting the (distributive) flow function ~~. Otherwise, \nif node m calls a procedure p we need to determine the reverse summary function 4[TP,.P). We first assume \nin the next section that all necessary reverse summary functions are available and then discuss their \nde\u00adtermination in detail in the following section. 3.1 Query Algorithm The demand algorithm Query that \nimplements the query propagation rules is shown in Figure 3. Query takes as in\u00adput a query q and returns \nthe answer for q after a finite number of applications of the propagation rules. Query uses a worklist \ninitialized with the input query q. The answer to q is equivalent to the boolean conjunction of the answers \nto the queries currently in the worklist. During each step a query is removed from the worklist and translated \naccording to the appropriate propagation rule associated with the node under inspection. The query resulting \nfrom this translation is merged with the previous query at the node and added to the worklist unless \nthe previous query haa not changed (lines 7-8 and 15-16). The algorithm terminates with the answer true \nif the worklist is exhausted and all queries have evaluated to true. Otherwise, false is returned. Query(y, \nn) 1. for each m ~ N do query[m] + -1\u00ad 2. cpter~[n] + y; worklisi+ {n}; 3. while wo.klist # 0 do 4. \nremove a node m from worklisfi 5. case m = Tq for some procedure q: 6. for each m ~ Nca[l s.t. call(rn \n) = q do 7. query[m ] + que~~[m ] U qtiery[m]; 8. if query[m ] changed then add ml to workks~ 9. endfor; \n 10. otherwise: 11. for each m c p~ed(m) do  f~, (werv[ml) if m @NCall 12. new + (query[m]) if m cNcall, \n?_r,,eg) { call(m )=u 13. if (new ~ T) then return( false ) 14. else if (new 3 1) then 15. query[m \n] + query[m ] U new; 16. if queTy[m ] changed then add m to worklis~ 17. endif; 18. endfor; 19. endwhile; \n 20. return(true);  Figure 3: Generic demand algorithm Query. Query(y, n) returns the response true \nor false to the query q =< y, n >. To determine the complexity of the query algorithm we count the number \nof times a join operation or a reverse function application is performed. A join/reverse function application \nis performed at a node n in lines 7, 12 and 15 only if the query at a successor of n was changed (or \nat the entry node of a procedure p if n is a call site of p) which can happen only O(height(L)) times. \nHence, algorithm Query requires in the worst case O(height(L) x INI) join operations and/or reverse function \napplications. If the program under analysis consists of only a single procedure (the intraprocedural \ncase), Query provides a com\u00ad plete procedure for demand-driven data flow analysis. For the inter-procedural \ncase, we require an efficient procedure to compute the reverse summary functions, as discussed next. \nComptitecjr(p, y) 1. if M[eP, y] = y then /8 result pr-e.aously computed */ 2. return (M[rP, y]); 3 \nwodi-W+{(ep,y)}; M[ep,Y]=Y;  4. while woTkiist #@do 5. remove a pair (n, z) from worklist and let z \n+ M[n, r]; 6. case neNCall and call(n) =q: 7. if M[eq, 2] = 2 then 8. for each m c p.cd(n) do 9. \nPropagate(m, x, M[~~, z]); 10. else /* tTiggeT computation of 4~rq,eq~(~) */ 11. M[eq, z] + z and add \n(eq, z) to woTklisti 12. case n = rq for some proc. q: /* Pvopagate z to call sites if needed */ 13. \nfor each m E N=all such that call(m) = q and M[m, z ] = z for some z do 14. for each m c pred(m) do \nPropagate(m , z , z); 15. otherwise: /*nisnot acallsite andnotan entry node */ 16. for each m C pTed(n) \ndo %-wag~tc(m, Z, jl(z)); 17. endwhile; 18. return(M[rP, y]);  Propagate(n, y, new) /* propagate new \nto M[n, y] */ 1. M[n, y] + kf[n,y] u new; 2. if M[n, y] changed then add (n, g) to worklis~ endi~  \nFigure 4: Compute#r(p, y) returns the reverse summary function value ~~r, ,ep)(y) for a procedure p and \ny 6 L. 3.2 Reverse Summary Functions This section discusses an algorithm to compute individ\u00adual reverse \nsummary function values in order to extend al\u00adgorithm Query to the interprocedural case. An obvious and \ninefficient way to compute reverse summary functions is to first determine all origim-d summary functions \nby evaluating equation system 1 and then reverse each function. We de\u00adscribe in this section a more efficient \nmethod to directly com\u00adpute the reverse functions. Our algorithm mirrors the oper\u00adations performed in \nSharir and Pnueli s worldist algorithm for evaluating equation system 1, except that we compute reverse \nsummary function values and the direction in which table entries are computed is reversed. Assuming that \nthe asymptotic cost of meet and local flow function application is the same for join and reverse flow \nfunction application, our akorithm has the same worst case comdexitv as Sharir .  and Pnueli s algorithm \nfor the original summary functions. As in Sharir and Pnueli s algorithm the tabulation strategy requires \nthe lattice L to be finite. We first derive an inductive definition of the reverse sum\u00admary functions \nfrom equation system 1. By reversing the order in which summary function are constructed and by applying \nLemma 1 we obtain the following definition of the reverse summary function #~,p,eP) for each procedure \nP: Equation System 3 Figure 4 shows an iterative work!ist algorithm Compute~r that, if invoked with a \npair (p, u), returns the value ~f., ,.P ) (Y) after a partial evaluation of the equation system 3. Individ\u00adual \nfunction values are stored in a table M : N x L + L such that ~[n, g] = 4{n,eP)(y). The table is initialized \nwith 1 and its contents are assumed to persist between subsequent calls to procedure Compute@r. Thus, \nresults of previous calls are reused and the table is incrementally computed during a se\u00adquence of calls. \nAfter calling Computexj with a pair (p, y) a worldist is initialized with the pair (eP, y). The contents \nof the worklist indicate the table entries whose values have changed but the new values have not yet \nbeen propagated. During each step a pair is removed from the worklist, its new value is determined and \nall other entries whose values might have changed as a result are added to the worklist. We next analyze \nthe cost of k calls to Computeq5r. Stor\u00ading the table A4 requires space for [N\\ x [LI lattice elements. \nTo analyze the time complexity we count the number of join operations (in procedure Propagate) and reverse \nflow function applications (at the call to Propagate in line 16). The loop in lines 4-17 is executed \nO(hezght(L) x ILI x INI) times, which is the maximal number of times the lattice value of a table entry \ncan be raised, i.e., the maximal num\u00adber of additions to the worklist. In the worst case, the cur\u00adrently \ninspected node n is a procedure entry node. Pro\u00adcessing a procedure entry node r~ results in calls to \nProp\u00adagate for each predecessor of a call site of procedure q. Thus, the k calls to Compute@ require \nin the worst case O(C x height(L) x ILI x INI) join and/or reverse function applications, where C is \nthe maximal number of call sites calling a single procedure. Assuming that each access to a reverse summary \nfunc\u00adtion in procedure Query is replaced by an appropriate call to Compute@ , the total cost of algorithm \nQuery is O(C x height(L) x l~\\ x IN[) join and reverse local flow function applications plus the space \nto store 0( INI x ILI) lattice ele\u00adments. 3.3 Queries in non-distributive Frameworks The distributivity \nof the original analysis framework is nec\u00adessary to ensure that queries are decidable, that is, to ensure \nthat the query propagation rules from the previous section yield as precise information as the original \nexhaustive analy\u00adsis does. We consider in this section the kind of approxima\u00adtion that is obtainable \nif the query propagation rules are ap\u00adplied to evaluate queries in the preseuce of non-distributive flow \nfunctions. If all flow functions in a data flow framework are i l\u00addistributive then a data flow query \n< y, n > evaluates to true if and only if element y is part of the solution at node n, i.e., if and only \nif y c x [n]. If the original analysis frame\u00adwork is monotone but not n-distributive then information \nmay be lost during the query propagating process. Specifi\u00adcally, if a flow function h is monotone but \nnot distributive, then the relation between h and its reverse h is weaker than in the distributive case; \nonly the following implication holds (see in contrast the Galois connection (GC)): y ~ h(z) =+ h (y) \ngz As a result of this weaker relationship queries are only semi\u00addecidable in the presence of non-distributive \nflow functions. Conceptually, the derived query algorithm may also be ap\u00adplied to data flow problems \nwith non-distributive functions. In the presence of non-distributivity, the above implication ensures \nthat if a query q =< y, n > evaluates to false then y ~ x [n]. However, nothing can be said if q evaluates \nto true. If appropriate e worst assumptions are made for true 4 Optimized Query Evaluation responses, \nthe query algorithm still provides approximate information in the presence of non-distributive flow func\u00ad \ntions. This section discusses two ways to improve the performance of the query evaluation. The choice \nof optimization depends on whether a fast response to (i) a single query or to (ii) a sequence of queries \nis of primary int crest. To optimize the response time for a single query, the query evaluation includes \nearlg termination. Recall that the answer to the input query is the boolean conjunction of the answers \nto the queries currently in the worklist. Thus, the evaluation can directly terminate as soon as one \nquery eval\u00aduates to fcd.se, independent of the remaining contents of the worklist. Early termination \nis included in algorithm Query in Figure 3. To process a sequence of k queries requires k invocations \nof Querg, which may result in the repeated evaluation of the same intermediate queries. Repeated query \nevaluation can be avoided by maintaining a result cache. We outline a sim\u00adple extension of algorithm \nQuery to include result caching. A global cache is maintained that contains for each node n and lattice \nelement y an entry cache [n, g] denoting the previous result, if any, of evaluating the query < g, n \n>. Before a newly generated query q is added to the worklist, the cache is first consulted. The query \nq is added to the worklist only if the answer for q is not found in the cache. Entries are added to the \ncache after each terminated query evaluation. Recall that a false answer at some node n im\u00adplies a false \nanswer for all previously generated queries at nodes that are reachable from n along some valid execution \npath. Thus, the cache can be updated in a single pass over the nodes that have been visited during the \ncurrent invoca\u00adtion of Query. First, each visited node n that is reachable from a node that evaluated \nto false is processed and the cache is updated to cache[n, guerg[n]] = false. Note, that at this point \nthe entries cache[n, z] for query [n] L z could also be set to false. For the remaining visited nodes \nn, where < query[n], n > evaluated to true, the cache is updated to cache[n, querg[n]] = true. Again, \nthe entries cache[n, z] for z L guerg[n] could also be set to true. The inclusion of result caching has \nthe effect of incre\u00admentally building the complete (exhaustive) data flow so\u00adlution during a sequence \nof calls to Quer-g. Result caching does not increase the asymptotic time or space complexity of algorithm \nQuerg. Storing the cache requires O(INI x ILI) space and updating the cache requires less than doubling \nthe amount of work performed during the query evaluation. Moreover, the asymptotic worst case complexity \nof k invo\u00adcations of Querg with result caching is the same for any number k,where1~k~ILIxINIandILIxINIisthe \nnumber of distinct queries. 5 Copy Constant Propagation This section illustrates our approach for the \nproblem of in\u00adterprocedural copy constant propagation. Using this exam\u00adple we show how the query algorithm \ncan be extended to handle programs with formal reference parameters and lo\u00adcal variables. The difficulty \nintroduced with formal reference parameters is the potential of aliasing. Ignoring aliasing refined 1ocal \nflow function ~n(z),node n wherex= zI, . . ..z~) c i.fw=v XW n c if wcalias(v, n), V:=c fn(x)w = w#v \n{ Xw otherwise Xu ifw=v xwnxu if w~alias(v, n), V:=u fn(x)w = w#v { x,,, otherwise 1 if w C alias(v, \nn)red(v)> fn(x)w = ~w V:=t otherwise Table 3: Local flow functions in CCP refined based on may\u00adalias \ninformation. ~de n refined reverse local flow function f; (1. ifw=v and c= C :=C zf wcaiias(v, n) f;([w=c]) \n= T and c# c { [w=c] otherwise [ZL=c] ifw=v f;~~w=cl~ = [w=c, u=c] ~ ;~lias(v, n), :=U [VJ=c] otherwise \n{ ad(v), T if wcaiias(v, n) i:([w=c]) = [W=c] otherwise _ t Table 4: Reverse local flow functions in \nCCP refined based on may-alias information. during the analysis may lead to unsafe (i.e., invalid) query \nresponses. We show how alias information can be incorpo\u00adrated to ensure safe answers to queries. For \nsimplicity, we assume programs with flat scoping. The address space Addr(p) of a procedure p is defined \nas: Addr(p) = Global U Local(p) U Formal(p), where Global is the set of global variables in the program, \nLocal(p) is the set of variables local to p and Formal(p) is the set of formal parameters of p. Two variables \nz and g are aliases in a procedure p if x and g may refer to the same location during some invocation \nof p. Reference parameters may introduce aliases through the binding mechanism between actual and formal \nparame\u00adters. The determination of precise ahas information is NP\u00adcomplete [M ye81]. Therefore, we assume \nthat approximate e alias information is provided for each procedure p in form of a summary relation alias(p) \nas described in [CO085]. A pair (z, y) c alias(p) if z is aliased to gin some invocation of p (ma@iases). \nWe use alias(z, p) = {g I (x, g) G alias(p)} to denote the set of mug aliases of x in p and alias(x, \nn) = aiias(z, p) if node n is contained in p. The computation of the aiias(p) sets can be expressed as \na distributive data flow problem with a finite domain set over a program s call graph [CO085]. Thus, \nwe can employ the demand-driven analysis concepts from the previous sec\u00adtion in order to compute only \nthe relevant alias pairs in procedures that are actually analyzed. However, to avoid confusion in discussing \nthe refinements in this section we make no assumption on how the alias information is com\u00adputed but assume \nthat sets aiias(p) are available to the CCP algorithm for each procedure p. Using these alias sets we \nre\u00adfine the local flow functions from Section 2 to safely account for the potential of aliasing. More \nprecise refinements are possible if, in addition to may alias information, must alias information is \navailable. The refined local flow functions and the corresponding refined reverse flow function are shown \nin Table 3 and Table 4, respectively. The equation system 3 for the determination of reverse summary \nfunctions is refined to express the name binding mechanisms of reference parameters at call sites. We \nde\u00adfine a binding function bs for each call site s that maps a lattice element x from the calling procedure \nto the corre\u00adsponding element bs(z ) in the called procedure according to the parameter passing at s. \nWe will also need to consider the reverse binding b~l to translate a lattice element from a called procedure \nto the corresponding element in the calling procedure. Let s be a call site in a procedure p that passes \nthe actual parameters (up], . . . apj ) to the formal parame\u00adters (fpI, . . . fpj ) in the called procedure \nq. Furthermore, let w c Addr(p) and w ~ Addr(q): where U = ({w} n Global) U {fp, I apt = v} [W=c] if \nWE Global b; ([w=c]) = [ap,=c] if w=fp, 1 otherwise { The functions b. and b~l are U-distributive and \nbs (T) = b:l(T) = T. Equation system 4 shows the refined definition of reverse summary functions. The \nequations in 4 are defined for base elements ~o=cl, where v c Addr(p). If v @ Addr(p) then ~tr,,ep)([i=c]j= \n[V=c]. Equation System 4 for each procedure p te.,e,)([u=cl) = [V=c] ( .f~ d{m,ep)([w=cl) ( ifrnCiVcat,, \ncall(m)= q Finally, we refine the query propagation rules using the biud\u00ad ing functions. The refinement \nis shown for queries with re\u00ad spect to the base elements in a procedure p, where n ~ NP-Ncall and uc \nAddr(p). For an arbitrary set of base elements B, the query < ~~~ b, n > is equivalent to the con\u00ad junction \n<bjn> A beB Refined query propagation in CCP < [v=c], n > U false if f~([v=cl) =T < [v=cJ, rp > * ~ < \n[b; (v) =c],m > mEN..,, , Cdl(m)=p  true if ~L([v=cl)=J\u00ad < $;([v=c]), m > otherwisern c precl(n), 1 \nm E N.. Iz false if h~([v=c])=T A true if hm([v=c])=l m ~ pred(n), < hm([v=c]), m > otherwise 1 m c \niV.. zz, can(m) = p  5.1 CCP Query Algorithm Figure 5 shows algorithm IsCCP to respond to a query of \nthe form Is variable v a copy constant at node n? . If v is a copy constant at n then Is CCP returns \nthe constant value c of v. Otherwise IsCCP returns false. Note, that this query format is more general \nthan previously discussed. More specific queries of the form Is v a copy constant at n with value c? \ncan always be answered using the response of Is CCP. Algorithm 1sCCP is an inst ante of the generic algorithm \nQuery from Section 3, except that IsCCP also includes the refinements for reference parameters. To handle \nthe more general query format, the specification of a constant value c is simply dropped; a base element \n[v=c] simplifies to [v] and a query < [v], n > raises the question as to whether variable v has some \narbitrary but fixed constant value at node n. A query < [v], n > evaluates to true at a node n if node \nn assigns any constant c to variable v, in which case the value c is remembered. If all generated queries \nevaluate to true the join over the remembered constant values is examined. If this join yields a constant \nc, then c is returned. Otherwise the response is false. The corresponding instance of the generic procedure \nComputeq$ , shown in Figure 6, partially evaluates the equa\u00adtion system 4. By the distributivity of the \nreverse summary functions, it is sufficient to maintain table entries only for base elements resulting \nin MaxAddrx N entries, where Max-Addr is the size of the maximal address space in any pro\u00adcedure. Each \nentry may contain a set of base elements and is therefore of size MaxAddr. To keep track of the actual \nconstant values encountered, the table M includes an extra field Mb, v]. val for each procedure p and \neach variable v. We now consider the cost of procedures IsCCP and Computeq$ not including the cost of \ncomputing the alias in\u00ad formation. During an invocation of IsCCP a total of 0( I!4axAcMr x INI) queries \nmay be generated resulting in 0( MaxAddr x IN [) join and reverse function applications in procedure \nIsCCP. The fixed point computation of table entries in procedure Compute#r requires in the worst case \n0( MazAddr2 x INI) table entry updates. As in the general case, each table update may trigger up to C \njoin and/or reverse function applications, where C is the maximal num\u00ad ber of call sites calling a single \nprocedure. Assuming join and reverse function applications are performed pointwise, each join or function \napplication requires 0( M ax Addr ) time resulting in the total time of O(C x MaxAddr3 x IN l). IsccP(?J, \nn) 1. for each m 6 N do guery[m] + O; 2. query [n] + [v] ; worklist 4-{n}; val = 1; 3. while worklist \n# 0 do 4. remove a node m from worklist; 5. letpbe suchthat mcNp; 6. case m = r~ for some proc. q: \n 7. for each m c NcaLt s.t. call(m ) = q do 8. quepy[m ] + query[m ] U b~l (query [m]); 9. if qaery[m \n] changed then add m to worklis~ 10. endfor; 11. otherwise: 12. for each m c pred(m) do  .f~, (~uery[ml) \nif m #NCall 13. new i-b~~ (Compufeq$r(q, bm, (z), val), if m ~NCall, { where x = alias(query[m], p) catl(m \n)=g 14. if new = L and m ~ NC.ll then vat + val Uc, where c is the constant assigned at m ; 15. if new \n= T or val = T then return( false ) 16. else if new 2 1 then 17. /* query still unresolved */ 18. \nque~y[m ] + query[m ] U new; 19. if qtie~y[m ] changed then add m to worklist; 20. endifi 21. endfor; \n 22. endwhile; 23. if val < T then return(val) else return(.fake);  Figure 5: Demand algorithm for \nCCP. IsCCP(V, n) returns the constant c if variable v is a copy constant at node n with value c, otherwise \nit returns false.  5.2 CCP Example We illustrate algorithm 1s(7CP for copy constant propaga\u00adtion using \nthe program example in Figure 7. The pro\u00adgram consists of three procedures main, p and q, where Global \n= {z}, Local(main) = {y}, Formal(p) = {f} and Formal(q) = {g, h}. The may-alias relations are: alias(p) \n= {(z, f)} and aiia~(q) = {(z, h), (X,9), (v, g), (g, h)}. The re\u00adverse local flow functions are shown \nnext to each node in the IFG. The table in Figure 7 shows the reverse summary functions. In addition \nto the reverse summary function en\u00adtries for the base elements in each procedure (rows for 4~7,9) and \nq$(lO,l,)) the table also shows intermediate entries (last two rows) collected during the computation \nof the summary functions. We consider the evaluation of the following query examples: Query 1: ?lsCC \nP(h, 10) ( Is the formal h of procedure q a copy constant on entry of each invocation of q ?): Initially, \nworldist = {10} and query[lO] = [h]. Qtiery[lO] is propagated to queries for the corresponding actual \nparame\u00adters at call sites resulting in: query [5] = [z] and qaery[8] = [f]. processing quev[8] causes \nthe propagation of [f] to node 7 and in turn to actual parameters at the call site at node 4, i.e., query \n[4] = [z]. Assume guery[5] is processed next. The global z is passed to formal ~ of procedure p at node \n4. Therefore, the reverse summary function 9$(7,9)is inspected for the two arguments [z=c] and [f =c]. \nThe new query element resulting for node 4 is determined as query [4] = ~11(4;7,9) ([~=d) u 4;7,9)([f=cl)) \n= r ([$=clf=cl) = [z=c], which is simplified to [z]. Applying the reverse func\u00adtion yields fl ([z]) = \nJ-, i.e., the query at node 4 evaluates to true and 1 is remembered as the actual constant assigned. \nCompute~r(p, y, val) 1. worklist + 0; res + 1; 2. let y = [VI, . . . . V~], where v, 6 AddT(p); 3. \nfor each v;, where 1~i~kdo 4. if M[eP, v,] = 1 then add (ep, v,) to woTklis~ 5. M[ep, v,] = [w]; endif; \n 6. while worklist # @ do 7. remove a pair (n, w) from worklisfi 8. let p be the proc. containing n; \n 9. let [wI, . . ..wj] = M[n, w]; 10. case n E Ncall and call(n) = q: 11. for each w,, where 1< i ~ \nj do 12. if w% c Global or w$ isan actual param. at n then 13. for each z 6 bn([w,]) do 14. if M[e~, \nz] = [z] then 15. for each m c pred(n) do 16. P~opagate(m, w, b~l(M[rq, z])); 17. if M[rq, z] = 1 \nthen 18. M~ , w].val + M[p , w].val U M[q, z].val; 19. else M[eq, Z] + [z]; add (eq, z) to worklist; \nendifi 20. endfor; 21. else /* skip call site if u not passed */ 22. for each m G pred(n) do Propagate \n(m, w, [w,]); 23. endfor; 24. case n = ~q for some procedure g: 25. for each m c Ncall such that \n26. call(m) = q and b~l([w]) c M[m, z] for some z do /* i.f entry M[r~, w] was .eq aested earlier */ \n 27. let p be the proc. containing m; 28. for each m G pred(m) do 29. Propagate(m , z, b~l (M[n, w]); \n 30. if M[rq, w] = L then 31. M[p , z].val i-&#38;f[p , z].val U &#38;f[q, w].vai; 32. otherwise: \n33. for each m c pred(n) do 34. Propagate(m, w, .f&#38;(M[n, w]); 35. if ~~(M[n, w]) = L then 36. \n&#38;f[p , w].val + M[p , w].val U c, where 37. c is the const. assigned at m; 38. endwhile; 39. for \neach v,, where 1~ i ~ kdo 40. res e res u M[rp, vt]; val e val u hf[p,v~].val; endfor; 41. return(res); \n Propagate(n, v, new) /* propagate new to M[n, v] / 1. M[n, v] + M[n, v] U new; 2. if M[n, v] changed \nthen add (n, v) to worklis~ endifi  Figure 6: Algorithm Compute@ (p, y, vai) in CCP returning the value \n+;,, ,eP)(v) for a procedure p and y E L. Since the worhlist is exhausted, the overall response is 1, \nindicating that the formal h of procedure q always has the value 1 on entry of procedure q. Query 2: \n?Isc7CP(Z, 6) ( Is variable z a copy constant after the call to procedure q at node 6 ?): Initially, \nworldist = {6} and quepg[6] = [x]. Processing que~y[61 results in: query[5] = b~ (@~10,12)([I=C] U [h=c])) \n= b~l ([z=c,g=c]) = [z=c,v=c] which is simplified to [x, y]. We consider the propagation of the query \nelements [z] and [y] separately. Since y is local to main, [y] directly prop\u00adagates through nodes 4 and \n3. At node 2: ~~([y]) = 1 and the value O is remembered as the actual constant as\u00adsigned. As for Query \n1, propagating the query element [z] from query [5] will eventually lead to the evaluation true at node \n4 with 1 being the actual constant assigned. Since procedure main procedure p(f) procedure q(g,h) declare \nlocal y; global x; begin begin begin call q(f,f); h:=g; read(y); end end X:=1; call p(x); call q(y,x); \n end F main procedure p(f) procedure q(g,h) fi~([X=C])=[X=C] 1 ff([x=c])=[x=c] f;([x=c])=[x=c] entry \n7 entry 10 entry ffo([g=c])=[g=c] ff([y=c])=[y=c] f+([f=c])=[f=c] fio([h=c])=[h=c] T7 f;([x=c])=[x=c] \n y:=o 1 2 8 q(f,f) fil([X=C])GIXcC, g=C] f;([y=c])=~ 11 h:=g fi~([gEC])D[gDC] fil([hmc])=[g=c]f;([x=c])=~ \n9 f;([x=c])=[x=c] 3 X:=l exit f;([y=c])=[y=c] 2 f;([f=c])=[f=c] 1, fi~([XmC])c[XEC] $ 121 fi*([gGC])z[gGC] \n fil([h=c])c[h=c]   v--%---exit Reverse summary function values 5 CI(YJ) 2 6 exit f:([x=c])=[x=c] \nf+([y=c])=[y=c] Figure7: Example for CCP the two constant values encountered differ, the response of \nIsCCP is fake. Note, the response @seis safe but imprecise. A closer inspection of the code reveals that \nz is actually a copy con\u00adstant at node 6 with value O. The imprecision is due to the loss of information \nin the alias summary sets. The alias set for a procedure is obtained by merging together the alias configurations \nfor all invocations. Avoiding this kind of in\u00adaccuracies would require a separate analysis (i. e., separate \nreverse summary function) for each different alias configu\u00adration that may hold at a procedure. However, \nthe cost of this approach is prohibitive, asitmay require an exponential number of different reverse \nsummary functions. 6 Related Work A number of variations of the notion of demand-driven anal\u00adysis and \nthe notion of a partial backward propagation of information have separately appeared in the literature. \nThe concepts of deriving data flow information by back\u00adward propagation of assertions was described using \nopera\u00adtional semantics by Cousot [COU81] and later developed and implemented in a debugging system for \nhigher-order func\u00adtions [Bou93]. The analysis for discovering linked conditions in programs described \nin [SMHY93] is also based on back\u00adward propagation of assertions starting from test sites in conditionals. \nAn important component of our demand-driven approach is the tabulation algorithm Comp uteq$ that implements \nthe lazy evaluation of only relevant (i.e., needed) equation val\u00adues. The algorithm is essentially a \nreversed version of Sharir and Pnueli s tabulation algorithm [SP81] to compute the original (unreversed) \nsummary functions. A similar lazy fixed point computation of only the relevant equations was also described \nin the chaotic iteration algorithms [CC77] and the minimal function graphs for applicative programs [JM731. \nRe~erse flow functions, which we apply in the query prop\u00adagation rules, have previously been discussed \nin [HL92] to demonstrate that an abstract interpretation may be per\u00ad formed in either a forward or a \nbackward direction. The relationship between forward and backward directions of an analysis was also \ndiscussed by Cousot [COU81]. Recently, two approaches to demand-driven interproce\u00addural analysis were \npresented by Reps [Rep94] and Reps et al. [RSH94], In the first approach [Rep94], a limited class of \ndata flow problems, the locally separable problems, are en\u00adcoded as logic programs. Demand algorithms \nare then ob\u00adtained by utilizing fast logic program evaluation techniques developed in the logic-programming \nand deductive-database communities. In a more recent work by Reps et al. [RSH94] the first approach is \ngeneralized to a larger class of problems. In this second approach a data flow problem is transformed \ninto a specialized graph-reachability problem. The graph for the reachability problem, the exploded super-graph, \nis ob\u00adtained as an expansion of a program s control flow graph by including an explicit graphical representation \nof each node s flow function. As in our approach, Reps et al. base their approach on a variant of Sharir \nand Pnueli s interprocedural analysis framework. Similar to our algorithm Compute@ , a caching tabulation \nalgorithm is used to compute the solu\u00ad tion over the graph that. However, the graph-reachability algorithm \nimposes more restrictions on the problems that can be handled than our approach. Specifically, the graph\u00adreachability \napproach is applicable to problems where the lattice is apowerset over a finite domain set and where \nall flow functions are distributive. Although we require distri\u00adbutive functions to ensure precise data \nflow solutions, our algorithms still provide approximate information in the pres\u00adence of non-distributive \nfunctions. Our approach is less re\u00adstrictive on the lattice structure in that it is applicable to any \nfinite lattice. The restriction to a finite lattice does not even apply if our approach is used for z \nntraprocedural analysis. The utility of demand-driven analysis algorithm has also been demonstrated in \na number of demand-driven algorithms developed for specific analysis problems, including the fol\u00adlowing \nproblems. Babich et al. [BJ78] presented a demand algorithm for intmprocedural live variable analysis \nbased on attribute grammars. Strom and Yellin [SY93] presented a demand based analysis for typestate \nchecking. The authors experiment ally demonstrate that their goal-oriented (and demand-driven) backward \nanalysis is more efficient than the original forward analysis for typestate checking that eagerly collects \nall available information that may or may not be of relevance. Question propagation, a phase in the algorithm \nfor global value numbering [RWZ88] performs a demand\u00adbased backward search in order to locate redundant \nexpres\u00adsions. This backward search, like our query aJgorit hm, per\u00adforms the analysis from the points \nof interest (i.e., the points where an expression is suspected to be redundant) and it also uses early \ntermination to end the search. In procedure cloning [CHK92], procedure clones are created during the \nanalysis on demand whenever it is found that an additional clone will lead to more accurate information. \nCytron and Gershbein [CG93] described an algorithm for the incremen\u00adtal incorporation of alias information \ninto SSA form. The actual optimization problem to be performed on the SSA form triggers the expansion \nof the SSA form to include only the necessary alias-information. Similar ideas have also been implemented \nin the demand-based expansion algorithm of factored def-def chains [CCF92]. Other related work addresses \nthe goal of reducing the cost of data flow analysis by avoiding the computation of irrelevant intermediate \nresults. Several sparse analysis tech\u00adniques have been presented to reduce the number of data flow equations \nby either manipulating the underlying graph\u00adical program represent ation, such as the analyses based \non the global value graph [RT82], static single assignment form [RWZ88, WZ85], the sparse evaluation \ngraph [CCF90], the dependence graph [JP93] or by direct manipulation of the equation system through partitioning \nalgorithms [D GS94]. Slotwise analysis [DRZ92] also falls into this class of sparse technique but is \nlimited to bitvector data flow problems. Sparse analysis approaches to reduce the amount of data flow \nevaluations are complementary to our demand-driven analysis algorithms in that the evaluation of dat \na flow equa\u00adtions is avoided independent of the specific information de\u00admanded; that is, the evaluation \nof those equations is avoided that are irrelevant with respect to even the exhaustive so\u00adlutions. An \ninteresting combination of the two approaches would be to use, for example, a reduced equation system \naccording to [DGS94] or a sparse evaluation graph as in [CCF90], as the basis for propagating data flow \nqueries. Incremental data flow analysis [Ros81, Zad84, RP88, PS89] has also addressed the avoidance of \nexhaustive solution re\u00ad computations. However, unlike demand-driven analysis, in\u00adcremental analysis assumes \nthat an exhaustive solution has previously been computed and is concerned with avoiding exhaustive r-e-computations \nin response to program changes. 7 Conclusions This paper described a new demand-driven approach to in\u00adterprocedural \ndata flow analysis that avoids the costly com\u00adputation of exhaustive data flow solutions. A general frame\u00adwork \nfor the derivation of demand algorithms covering the class of interprocedural data flow problems with \na finite domain set was presented. In a data flow problem with distributive flow functions, the derived \nalgorithms provide as precise information as the corresponding exhaustive al\u00adgorithm. In the presence \nof non-distributive flow functions, the derived algorithms still provide approximate solutions. We are \ncurrently exploring the utility of approximate query responses for non-distributive problems in practice. \nPar\u00adticularly efficient demand algorithms result for the classical bitvector problems. For example in \nlive variable analysis, queries about individual variables are resolved in linear time and space if the \nanalysis is intraprocedural or if the analysis is interprocedural and the programs are alias-free. The \ngoal of demand-driven program analysis is the re\u00adduction of both the time and space cost of solving data \nflow problems. When discussing result caching we implicitly as\u00adsumed that reducing the analysis time \nis of primary inter\u00adest. However, as programs grow larger, space may become as valuable a resource as \nanalysis time, in particular, if re\u00adsults for a large number of different data flow problems are to be \ncollected. Clearly, result caching should be avoided if the space consumption is the primary concern. \nSince result caching is optional in our query algorithm, it can be adjusted easily to the needs of the \ncurrent application. The option of result caching permits the computation of the complete solution with \nan asymptotic worst case complexity that is no worse than the cost of a standard exhaustive algorithm. \nTo fully evaluate the benefits of the achievable time and space reductions we are currently developing \na prototype implementation of a demand-based query algorithm for in\u00adterprocedural copy constant propagation. \nAcknowledgements We thank Tom Reps and the referees for their comments on an earlier draft of this paper. \nReferences [Bir84] G. Birkhoff. Lattice theory, volume 25. Ameri\u00ad can Mathematical Society, Colloquimn \nPublication, Washington, DC, 3rd edition, 84. [BJ78] W .A. Babich and M. Jazayeri. The method of at\u00ad \ntributes for data flow analysis: Part II . Demand anal\u00ad ysis. Acts Informuiica, 10(3), Oct. 78. [Bcm93] \nF. Bourdoncle. Abstract debugging of high-order imperative languages. In SIGPLAN 93 Con.f. on Programming \nLanguage Design and Implementation, pages 36-45, Albuquerque, NM, Jun. 93. [CC77] P. Cousot and R. Cousot. \nStatic determination of dynamic properties of recursive procedures. In E.J. Neuhold, editor, IFIP Conj. \non Formal Description of P~ogramwzing Concepts, pa~es 237 277. North- Hollan Pub. Co., 77. [CCF90] \n[CCF92] [CG93] [CHK92] [CK88] [CO085] [COU81] [DGS94] [DRZ92] [GT93] [HL92] [JM73] [JM82] [JP93] [Ki173] \n[KS92] J.D. Choi, R.K. Cytron, and J. Ferrante. Automatic construction of sparse data flow evaluation \ngraphs. In 18th ACM Symp. on Principles of Programming Languages, pages 55 66, Orlando, FL, Jan. 90. \n J.D. Choi, R. Cytron, and J. Ferrante. On the efficient engineering of ambitious program analysis. IEEE \nTrans. on Soflware Engineering, 20(2) :105-114, Feb. 92. R. Cytron and R. Gershbein. Efficient accommoda\u00adtion \nof may-alias information in SSA form. In SIG-PLAN 93 Conj. on Programming Language Deszgn and Implementation, \npages 36 45, Albuquerque, NM, Jnn. 93. K. Cooper, M. Hall, and K. Kennedy. Procedure cloning. in IEEE \n1992 Int. C on.f. on Compute T Lan\u00adguages, pages 96 105, San Francisco, CA, April 1992. K. Cooper and \nK. Kennedy. Interprocedural side\u00adeffect analysis in linear time. SIGPLA N 88 Symp. on Compiler Constmcttonj \npublished in SIGPLAN No\u00adtices, 23(7):57 66, Jun. 88. K. Cooper. Analyzing aliases of reference formal \npa\u00adrameters. In 12th ACM Symp. on Principles o.f PTo\u00adgvamming Languages, pages 281 290, 85. P. Cousot. \nSemantic foundations of program analy\u00adsis. In S. MucJmick and N.D. Jones, editors, Prog~am Flow Analysis: \nTheory and Applications, pages 303  342. Prentice-Hall, 81. E. Duesterwald, R. Gupta, and M.L. Soffa. \nReducing the cost of data flow analysis by congruence parti\u00adtioning. In 5th Int. Cont. on CompileT Construction, \npages 357 373, Edinburgh, U. K., Apr. 94. Springer Verlag, LNCS 786. D.M. Dhamdhere, B.K. Rosen, and \nF.K. Zadeck. How to analyze large programs efficiently and informa\u00adtively. In SIGPLAN 92 Conj. on Programming \nLan\u00adguage Design and Implementation, pages 212 223, San Francisco, CA, Jun. 92. D. Grove and L. Torczon. \nfnterprocednral constant propagation: a study of jump function implementa\u00adtions. In SIGPLAN 93 Con.f. \non Programming Lan\u00adguage Design and Implementation, pages 90 99, Al\u00adbuquerque, NM, Jun. 93. J. Hughes \nand J. Launchbm-y. Reversing abstract interpretations. In ~th European Symp. on Pro\u00adgramming, pages 269 \n286, Rennes, France, Feb. 92. Springer Verlag, LNCS 582.  N.D. Jones and A. Mycroft. Data flow analysis \nof ap\u00adplicative programs using minimal function graphs. In l~th Symp. on Principles of Programming Languages, \npages 194 206, Florida, 1973. N. Jones and S. Muchnick. A flexible approach to in\u00adterprocedural data \nflow analysis and programs with recursive data structures. In 9th Symp. on Pranct\u00adples o.f Programming \nLanguages, pages 66 74, Albu\u00adquerque, New Mexico, 1982. R. Johnson and K. Pingali. Dependence-based \npro\u00adgram analysis. In SIGPLAN 93 Conj. on Program\u00adming Language Design and Implementation, pages  78 \n89, Albuquerque, NM, Jun. 93. G. Kildall. A unified approach to global program op\u00adtimization. In 1st \nACM Symp. on Principles o.f Pro\u00adgramming Languages, pages 194 206, Boston, Mas\u00adsachusetts, Jan. 73. J. \nKnoop and B. Steffen. The interprocedural coinci\u00addence theorem. In Jth Int. Con.f. on Compiler Con\u00adstruction, \npages 125 140, Paderborn, Germany, Oct. 92. Springer Verlag, LNCS 641. [KU77] [Mas94] [Mye81] [PS89] \n[Rep94] [RcIs79] [RCIS81] [RP88] [RSH94] [RT82] [RWZ88] [SMHY93] [SP81] [SY93] [WZ85] [Zad84] J.B. Kam \nand J.D. Unman. Monotone data flow anal\u00adysis frameworks. Acts lnjormatica, 7(3):305 317, Jul. 177. V. \nMaslov. Lazy array data-flow dependence analy\u00adsis. In ACM Symp. on PTincaples o.f Programming Languages, \npages 1-1.5, Jan. 94. E. W. Myers. A precise inter-procedural data flow al\u00adgorithm. In 8th ACM Symp. \non Principles oj Pro\u00adgramming Languages, pages 219 230, Williamsburg, Virginia, Jan. 81. L. Pollock and \nM.L. Soffa. An incremental version of iterative data flow analysis. IEEE Trans. on Software Engineering, \n15(12):1537 1549, Dec. 89. T. Reps. Solving demand versions of interprocedu\u00adral analysis problems. In \n5th Int. Con.f. on Compiler Construction, pages 389 403, Edinburgh, U. K., Apr. 94. Springer Verlag, \nLNCS 786. B. Rosen. Data flow analysis for procedural languages. Journal of the ACM, 26(2):322-344, 79. \nB. Rosen. Linear cost is sometimes quadratic. In 8th ACM Symp. on Pmnciples of Programming Lan\u00adguages, \npages 117 124, Jun. 81. B .G. Ryder and M.C. Paull. Incremental data flow analysis algorithms. ACM TTans. \nProgramming Lan\u00adguages and Systems, 10(1):1 50, 88. T. Reps, M. Sagiv, and S. Horwitz. Interprocedu\u00adral \ndataflow analysis via graph reachability. Techni\u00adcal Report 94-14, Dat alogisk Institut, University of \nCopenhagen, Copenhagen, Denmark, 94. J. Reif and R.E. Tarjan. Symbolic program analysis in ahnost linear \ntime. SIAM Journal of Computing, 11(1):81 93, Feb. 82. B. Rosen, M. Wegman, and F.K. Zadeck. Global \nvalue numbers aud redundant computations. In f 5th ACM Symp. on Principles o.f PTogTamming Lan\u00adguages, \npages 12 27, San Diego, CA, Jan. 88.  A.D. Stoyenko, T.J. Marlowe, W.A. HaJang, and M. Younis. Enabling \nefficient schednlability analysis through conditional linking and program transforma\u00adtions. ContTol Engineemng \nPractice, 1(1):85 105, 93. M. Sharir and A. Pnueli. Two approaches to interpro\u00adcedural data flow analysis. \nIn S. Mu&#38;nick and N.D. Jones, editors, ProgTam Flow Analysis: TheoTy and Applicat~ons7 pages 189 \n234. Prentice-Hall, 81.  R.E. Strom and D.M. Yellin. Extending typestate checking using conditional \nliveness analysis. IEEE T,ans. on Software Engineenng, 19(5):478485, May 93. M. Wegman and F.K. Zadeck. \nConstant propaga\u00adtion with conditional branches. In I%h ACM Symp. on Principles of Programming Languages, \npages 291 299, New Orleans, Jan. 85. F.K. Zadeck. Incremental data flow analysis in a structured program \neditor. In SIGPLAN Symp. on Compile. Construction, pages 132 143, Jun. 84. 48 \n\t\t\t", "proc_id": "199448", "abstract": "<p>This paper presents a general framework for deriving demand-driven algorithms for interprocedural data flow analysis of imperative programs. The goal of demand-driven analysis is to reduce the time and/or space overhead of conventional exhaustive analysis by avoiding the collection of information that is not needed. In our framework, a demand for data flow information is modeled as a set of date flow queries. The derived demand-driven algorithms find responses to these queries through a partial reversal of the respective data flow analysis. Depending on whether minimizing time or space is of primary concern, result caching may be incorporated in the derived algorithm. Our framework is applicable to interprocedural data flow problems with a finite domain set. If the problem's flow functions are distributive, the derived demand algorithms provide as precise information as the corresponding exhaustive analysis. For problems with monotone but non-distributive flow functions the provided data flow solutions are only approximate. We demonstrate our approach using the example of interprocedural copy constant propagation.</p>", "authors": [{"name": "Evelyn Duesterwald", "author_profile_id": "81100028046", "affiliation": "Department of Computer Science, University of Pittsburgh, Pittsburgh, PA", "person_id": "PP39023899", "email_address": "", "orcid_id": ""}, {"name": "Rajiv Gupta", "author_profile_id": "81100027751", "affiliation": "Department of Computer Science, University of Pittsburgh, Pittsburgh, PA", "person_id": "PP39072720", "email_address": "", "orcid_id": ""}, {"name": "Mary Lou Soffa", "author_profile_id": "81452611636", "affiliation": "Department of Computer Science, University of Pittsburgh, Pittsburgh, PA", "person_id": "PP39032771", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/199448.199461", "year": "1995", "article_id": "199461", "conference": "POPL", "title": "Demand-driven computation of interprocedural data flow", "url": "http://dl.acm.org/citation.cfm?id=199461"}