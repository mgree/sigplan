{"article_publication_date": "01-25-1995", "fulltext": "\n A Linear Time Algorithm for Placing #-Nodes Vugranam C. Sreedhar Guang R. Gao School of Computer Science \nMcGill University Montreal H3A 2A7 Canada {sreedhar,gao}@acaps.cs.mcgill.ca Dataflow analysis framework \nbased on Static Single As\u00adsignment (SSA) form and Sparse Evaluation Graphs (SEGS) demand fast computation \nof program points where data flow information must be merged, the so-called #-nodes. In this paper, we \npresent a surprisingly simple algorithm for com\u00adputing ~-nodes for arbitrary flowgraphs (reducible or \nirre\u00adducible) that runs in linear time. We employ a novel program representation the DJ graph by augmenting \nthe dom\u00adinator tree of a flowgraph with edges which may lead to a potential merge of dataflow information. \nIn searching for ~-nodes we never visit an edge in the DJ-graph more than once by guiding the search \nof nodes by their levels in the dominator tree. The algorithm has been implemented and the results are \ncompared with the well known algorithm due to Cytron et al. [CFR+ 91]. A consistent and significant speedup \nhas been observed over a range of 46 Fortran procedures taken from a number of benchmark programs. We \nalso ran experiments on increasingly taller ladder graphs and confirmed the linear time complexity of \nour algorithm.  Introduction Static Single Assignment (SSA) form [CFR+ 91], Sparse Eval\u00aduation Graphs \n(SEGS) [CCF91 ], and other related intermedi\u00adate representations have been successfully used for efficient \ndata flow analysis and program transformations [RWZ88, AWZ88, WZ85, W0192, WCES94]. The algorithms for \ncom\u00adputing these intermediate representations have one common step-computing program points where data \nflow informa\u00adtion must be merged , the so called q$-nodes. Given a flow\u00adgraph, the original algorithm \nfor computing ~-nodes for an SEG consists of the following steps [CFR+ 91, CCF91]: 1. Precompute the \ndominance frontier DF(z) for each node z. A node v is in DF(z) if x dominates a pre\u00addecessor of y without \nstrictly dominating y. Permission to copy without fee all or part of this matertal is granted provided \nthat the copies are not made or distributed for direct commercial advanta~e, the ACM copyright notice \nand the title of the publication artd Its date appear, and notice is given that copyi is by permission \nof the Association of Computing Machinery.Yo copy otherwise, or to republish, requires a fee ancf/orspecific \npermission. POPL 951/95 San Francisco CA USA @ 1995 ACM 0-89791-692-1/95/0001....$3.50 2. Determine the \ninitial set of sparse nodes N. that rep\u00adresent non-identity transference in a data flow frame\u00adwork. For \nSSA, such nodes contain definitions of vari\u00adables [CFR+ 91]. 3. Compute the iterated dominance frontier \nIDF(N~) for the initial set N.. Cytron et al. have shown that the desired set of ~-nodes for an SEG is \nsame as the iterated dominance frontier lDF(N.) of the initial set [CFR+ 91].  The time complexity of \nthe original algorithm depends on the size of the dominance frontier. Although the size of the dominance \nfrontier is linear for many programs (as was noted by Cytron et al.), there are cases in which the size \nof the dominance frontier is quadratic in terms of the number of nodes in a flowgraph. This is true even \nfor some cases of structured programs, for example, nested repeat unt i 1 loops [CFR+ 91]. Note that, \neven though the size of the dom\u00adinance frontier may be quadratic in terms of the number of nodes in the \nflowgraph, the number of #-nodes needed re\u00admains linear (for a particular SEG) [CFR+ 91]. As Cytron and \nFerrante pointed out Since one reason for introducing ~-nodes is to eliminate potentially quadratic behavior \nwhen solving actual data flow problems, such worst case behav\u00adior during SEG or SSA construction could \nbe problematic. Clearly, avoiding such behavior necessitates placing ~-nodes without computing or using \ndominance frontiers [CF93]. To overcome the potential quadratic behavior of computing @\u00adnodes using dominance \nfrontiers, Cytron and Ferrante pro\u00adposed a 0(/3x a(~)) algorithm that does not use dominance frontiers.* \nTo the best of our knowledge, the problem of find\u00ading an algorithm for computing @nodes for an arbitrarySEG \nin linear time remains open.z In this paper, we present a linear time algorithm for com\u00ad puting the desired \nset of @-nodes for N. without precom\u00adputing the domtnance frontiers for all the nodes. One key feature \nof our linear time algorithm is to order the nodes in the dominator tree in such a way that when the \ncomputation of dominance frontier DF (y) is performed, the dominance frontier DF(z) of any its descendant \nnode x, if it is essential for computing the desired set of #-nodes for No, has already 1CI() is the \nslowly-growing inverse-Ackerrnann function. 2MOE recently, Johnson and Pingali ~3] have given a linear \ntime algorithm for constructing an SSA like graph, called the Depen\u00addence Flow Graph. We compare our \nwork with theirs in Section 7. been computed and is so marked. As a result for any such z, the computation \nof DF(v) does not require the traversal of the dominator sub-tree rooted at z. To perform the proper \nnode ordering and marking, our algorithm uses a novel program representation, called the DJ-graph (Section \n3). The skeleton of the DJ-graph of a program is the dominator tree of its flowgraph (whose edges are \ncalled D-edges in this paper see Section 3). The tree skeleton is augmented with join edges (called J-edges \nin this paper see Section 3) from the original flowgraph which potentially lead to join nodes where data \nflow information are merged. The levels of the nodes in the dominator tree are used to order the computation \nof dominance frontiers of those nodes which are essential to compute the final set of @-nodes in a bottom-up \nfashion. We show that our algorithm visits each edge in the DJ-graph at most once, and therefore the \ncomplexity is linear in the size of the input flowgraph? The algorithm has been implemented on the top \nof Parafrase2 compiler [Har85b]. To compare our results, we also implemented the original algorithm based \non iterating through dominance frontiers [CFR+ 91]. We experimented on a number of FORTRAN procedures \ntaken from Perfect, Eispack, and other programs. With our algorithm we were able to obtain, on average, \nmore than five-fold speedup over the original algorithm. We also tested our algorithm against the standard \nladder graph example [CF93]. Again our algo\u00adrithm exhibited a linear behavior compared to the quadratic \nbehavior of the original algorithm: The significance of the algorithm presented in this pa\u00ad per goes \nbeyond to merely computing @nodes for SEGS or SSA form. Our framework can be used for the computa\u00adtion \nof guards [Wei92]. More recently we have used iter\u00adated dominance frontiers to incrementally update domina\u00adtor \ntrees [SGL94b]. Finally, our framework is robust enough to support incremental computation of ~-nodes \n[SGL94b]. Organization. In the next section, we introduce some stan\u00addard notation and definitions that \nwe will use in the rest of the paper. In Section 3, we introduce the DJ-graph. We also discuss some of \nthe properties of this graph that are relevant to our discussion. In Section 4, we give a simple linear \ntime algorithm for computing #-nodes. We show the correctness and the complexity of our algorithm in \nSection 5. In Sec\u00adtion 6, we present an implementation of our algorithm and report results for a number \nof programs. We also report re\u00adsults for the ladder graph example. In Section 7, we compare our work \nwith related work and finally, in Section 8, we give our conclusion. 2 Background and Notation A flowgraph \nis a connected directed graph G = (N, E, START, END), where N is the set of nodes, E is the set of edges, \nSTART c N is a distinguished start node, and END g N is a distinguished end node. Figure l(a) shows an \nsAs we will show later in the paper, the number of edges in the DJ-graph is no more than IN I + IJ91, \nwhere IN I is the number of nodes in the flowgraph and IE I is the number of edges. 4Due to the complex \nnature and partial description of @mn and Ferrante s almost linear time algorithm we did not implement \nthat algorithm. example of a flowgraph. If x + y E E, then z is called the source node and y is called \nthe destination node of the edge. We will assume that every node in N is on some path from START to END. \nIf S is a set, we will use the notation ]Sl to represent the number of elements in the set. 1 Legend \n13 6 + flowgfe~edge -.-+ kminetwedge (a) (b) 14 I Iw A Figure 1: Flowgraph and its dominator tree In \na flowgraph, a node x dominates another node y iff all paths from START toy pass through z. We write \nm dom y to indicate that x dominates y, and write x !dom y if x does not dominate y. If z dom g and z \n# y, then z strictly dominates y. We write z sdom y to indicate z strictly dominates y, and write z !sdom \ng if x does not strictly dominate y. The dominance relation is reflexive and transitive, and can be represented \nby a tree, called the dominator tree. If x is a parent node of v in the dominator tree, then z immediately \ndominates y, and we write adorn(y) to denote the immediate dominator of y. Given a node x in the dominator \ntree, we define SubTree(z) to be the dominator sub-tree rooted at z. Note that the nodes in SuKFree(z \n) is simply the set of all nodes dominated by x. Figure 1(b) shows the dominator tree for the flowgraph \nshown in Figure 1(a).  For each node in the dominator tree we associate a level number that is the depth \nof the node from the root of the tree. We write z .levei to indicate the level number of a node z. For \nexample, for the dominator tree shown in Figure l(b), START.level = O, 8.level = 2, 9.level = 3, etc. \nThe dominance frontier DF(z) of a node x is the set of all Y such that z dominates a predecessor of y, \nbut z does not strictly dominate y [CFR+ 91]. We can extend the definition of dominance frontier DF(S) \nto a set of nodes S: DF(S) = u DF(Z) (1) Zes We define the iterated dominance frontier lDF(S) for a set \nof nodes S as the limit of the increasing sequence: IDFI(S) = DF(S), (2) IDF,+I(S) = DF(S u IDFt(s)) \n(3) Let N= ~ N be the initial set of sparse nodes [CF93]. Cytron et al. have shown that the desired set \nof ~-nodes for N. is exactly same as lDF(N~) [CFR+ 91]. Therefore in the rest this paper we present a \nlinear time algorithm for computing the iterated dominance frontiers for N. g N.  3 DJ-Graphs and Their \nProperties In this section we briefly introduce DJ-graphs and state some of the properties of DJ-graphs \nrelevant to our discussion. We also give a simple algorithm for computing the dominance frontiers for \na node using DJ-graphs. We will then show how to compute the dominance frontiers for a set of nodes without \nprecomputing the dominance frontiers for all nodes. In the next section we will show how to extend this \nsimple algorithm to compute the relevant set of @nodes in linear time. A DJ-graph has the same set of \nnodes as in the flowgraph, and two types of edges called D-edges and J-edges. D-edges are dominator tree \nedges, and we define J-edges as follows: Definition 3.1 (J-edge) An edgez -yin ajowgraph is named a join \nedge (or J-edge) if z !sdom y, Furthermore, y is named a join node. Therefore, in order to construct \nthe DJ-graph of a flow\u00ad graph, we first construct the dominator tree of the given flowgraph. 5 Then, \nwe insert the J-edges into the dominator tree as follows: For each join node yin the dominator tree connect \nz to y (in the dominator tree) iff z -Y is a join edge in the original flowgraph. Figure 2 shows the \nDJ-graph for the flowgraph of Fig\u00adure 1(a). To see how a J-edge is inserted in the dominator tree, consider \njoin node 2 shown in the flowgraph of Fig\u00adure 1(a). This node consists of 1 --+ 2 and 6 a 2 as its two \nincoming edges. Of these two incoming edges, node 1 strictly dominates join node 2, and so we do not \ninsert an edge from 1 to 2 in the corresponding dominator tree; how\u00adever 6 does not dominate 2, therefore \nwe insert an edge from 6 to 2 in the dominator tree. We can easily see that the time complexity for inserting \nall the J-edges in the dominator tree is O ( ~). Therefore the time complexity of constructing a DJ-graph \nis linear with respect to the size of the flowgraph.G In the rest of the section we discuss some of the \nproper\u00adties of DJ-graphs. Due to space reason we only outline the basic properties and direct interested \nreaders to the full pa\u00adper [SG94] for a more thorough discussion on this. 5Another ieW of D-edges and \nJ-edges may give a better inti\u00adition We mark each edge z + y in the flowgraph as an immediate dominance \nedge if x = don(y). The edges that are not marked are join edges. 6Note that we can construct the dominator \ntree of a flowgraph in linear time [Har85a]. Level O Level 1 Level 2 Level 3 Level 4 Level 5 Level 6 \n14 ~ join edge % Figure 2: The DJ-graph of the example flowgraph 1.The number of edges in a DJ-graph \nis less than the sum of the number of nodes and the number of edges in the corresponding flowgraph [SG94]. \nWe will use this re\u00adsult in the complexity analysis of our ~-node placement algorithm (Section 5.2). \n2. Let g G DF(z] (and thus is also in lDFI(z)). Then the level number of Y is always less than or equal \nto the level number of z [SG94]. This result is one of the key points for obtaining our linear time algorithm \nfor placing @\u00adfunctions. Intuitively what this result says is that if we want to find what nodes in a \nflowgraph could be in the dominance frontier (or the iterated dominance frontier) of node x, we only \nneed to look at those nodes whose level number is no greater than that of z. Other nodes (whose level \nnumber is strictly greater than the level number of x ) can never be in the dominance frontier of x. \n3. A node y is in DF(z) iff there is a node z c SubTree(z) and a J-edge z + y such that the level of \nv is less than or equal to the level of x (Lemma 3.1). Using this property we next give a simple algorithm \n(Algorithm 3.1) for computing dominance frontiers. Computing the Dominance Frontier for a Single Node \nOur algorithm for computing the dominance frontier of a node is based on Lemma 3.1 which establishes \na relation between a node z c DF(x ), and the nodes in the dominator sub-tree rooted at z. This relation \nis captured by a J-edge y + z, Where y is a node in the SubTree(x). Lemma 3.1 A node z c DF(x) if there \nexists a y c SubTree(x) with y -+ z as a J-edge and z.level < x.level. Using Lemma 3.1 we can easily \ndevise a simple algorithm for computing the dominance frontier of a node as follows: Algorithm 3.1 Thefollowing \nalgorithm computes the dominance frontier DF(x) of a node x using D}-graphs. DomFrontier(z) { o DFZ =@ \n1: foreach y G SubTree(z) do 2 if((y + z == Jedge) and 3: (z.level < z.ievel)) 4 DFZ = DF= UZ } For example, \nconsider the DJ-graph shown in Figure 2. Suppose we want to determine the dominance frontier of node \n3. The SubTree(3) = {3,9, 10,11,12,13, 14}. At step @wefindth e f o 11owing J-edges: {10 + 12,11 + 12,13 \n+ 3,13 + 15,14 + 12}. Of these J-edges, we see that only nodes 3 and 15 satisfy the condition at step \n~ Therefore DF(3) = {3, 15}. Computing the Dominance Frontiers for a Set of Nodes We can easily use the \nAlgorithm 3.1 to compute the domi\u00adnance frontier for a set of nodes S, by first pre-computing the dominance \nfrontiers for all the nodes, and then using Equa\u00adtion 1 to proceed with the computation To illustrate \nthis consider the computation of DF( {9, 12}). By Equation 1, we know DF({9, 12}) = DF(9) U DF(12). Let \nus therefore precompute DF(9) and DF (12). Using Algorithm 3.1 we get DF(9) = {3, 15}, and DF(12) == \n{3, 12, 15}. Therefore DF({9, 12}) = {3, 12, 15}. Notice in the above example that we visit the nodes \nin the SubTree(12) twice-once during the computation of DI (9) and once again during the computation \nof DF(12). How can we avoid this redundant visitation of the nodes in the SuWree(12)? We can avoid this \nby first computing DF(12) and marking the node 12 as being processed. Now during the computation of DF(9) \nwe avoid visiting any nodes in the SubTree(12) (since node 12 is already processed, and is so marked) \nthereby avoiding redundant visitation. Notice here that we never need to precompute DF(9) and DF(12) \nin order to compute DF({9, 12}). Therefore in order to compute DF({9, 12}), we first compute the DF(12) \nusing Algorithm 3.1, and also mark node 12 as being processed. Any candidate nodes that is generated \non-the-fly is then added to the set DF( {9, 12}). Now during the computation of DF(9) we avoid visiting \nthe nodes in the SubTree(12). Again we add any candidate nodes that is generates on-the\u00adfly to DF({9, \n12}). Based on this observation we can see that the ordering of the nodes in the dominator tree is im\u00adportant \nto avoid redundant visitation of nodes during the computation of dominance frontiers. In the next section, \nwe will show how to extend the above key observation to compute the relevant set of ~-nodes in linear \ntime without pre-computing the dominance frontier for all the nodes in the flowgraph. Notice that one \ncan still use Algorithm 3.1 for computing the relevant set of @-nodes by precomputing the dominance \nfrontiers for all the nodes. But the time complexity of the resulting algorithm will be quadratic?  \n4 Algorithm for Placing #-Nodes In this section, we give a simple linear time algorithm for computing \n@-nodes using DJ-graphs (Algorithm 4.1). Given a set of initial nodes N., the algorithm computes the \nrel\u00adevant set of @-nodes by computing the set I.DF(N.), the iterated dominance frontier of N.. As we \nindicated in Sec\u00adtion 3, a direct application of Algorithm 3.1 based on the inductive definition of iterated \ndominance frontier can lead to quadratic behavior. Instead, our linear time algorithm is based on two \nkey observations: 1. Let y be an ancestor node of a node x on the dominator tree. If DF(z) has already \nbeen computed before the computation of DF(y), DF(x) need not be recomputed when computing D F (y). However, \nthe reverse may not be true; therefore the order of the computation is crucial. In Algorithm 4.1 the \ncomputation of relevant set of nodes is ordered in such a way that at the time when the computation of \nDF(Y) is performed, DF(z) of any node z within the dominator sub-tree rooted at y has already been computed \nand is so marked, if DF(z) is essential for computing the set of desired #-nodes for N.. As a result, \nthe computation of DF(y) need not traverse the dominator sub-tree of x for all such z. 2. When computing \nDF(z) we only need to examine the J-edges y ~ z, where y is a node-in the dominator sub\u00adtree rooted at \nx and z is a node whose level is no greater than the level of z. Recall that we have previously made \nthis observation in Lemma 3.1.  We employ a data structure called the PiggyBank to keep the candidate \nnodes in the order of their respective levels. Based on the above observations, levels of the nodes in \nthe dominator tree will be used in a bottom-up fashion to or\u00adder the computation of dominance frontiers \nof those nodes, z, which are essential to compute the final set of ~-nodes. Meanwhile, during each computation \nof DF(x), the descen\u00addant nodes of z in the dominator sub-tree rooted at x are visited in a top-down \nfashion guided by the D-edges, while avoiding nodes which have already been marked. During this top-down \nvisit, the J-edges are used to identify the can\u00addidate nodes which should be added into the IDF the set \nof final ~-nodes and those to be recursively explored further. Note that each new candidate generated \non-the-fly always has a level number no greater than that of the node currently being processed, and \nwe assure that no nodes are inserted into the PiggyBank more than once. This, and the structure of the \nF iggyl?ank, are the basis of the time linearity of our algorithm as will be demonstrated later in Section \n5. TAn example of a flowg.aph which exl-dbitquadratic behavior is the ladder graph. We will discuss more \non this later in Section 6. The Piggy Bank is like a piggy-bank, where nodes are temporarily deposited \nfor later withdrawal (See Figure 3). The Piggy Bank is an array of list of nodes, with index i stor\u00ading \nnodes of level t. Associated with the Pigg yBan k are two procedures: InsertNodeo and GetNodeo. InsertNodeo \nin\u00adserts a node in the PiggyBank at the index corresponding to the level number of the node. GetNodeo \nreturns the node whose level number is the maximum of all nodes currently stored in the Piggy Bank. We \nfirst insert the initial set of nodes N. into the Piggy Bank. Then, we iteratively com\u00adpute the dominance \nfrontiers of the nodes in the PiggyBank in the order that GetNodeo returns to obtain the iterated dominance \nfrontier of the initial set of nodes N.. (A node is inserted into the PiggyBank if it is either in N~ \nor in the iterated dominance frontier of some node in N..) We for\u00admally prove the correctness the algorithm \nin Section 5.1, and analyze its complexity in Section 5.2. To simplify the presentation of the algorithm, \nwe use the following notation and data structures: N umLeve/ is the total number of levels in the domina\u00adtor \ntree embedded in the DJ-graph.  Each node z c N has the following attributes:  struct Node Structure{ \nvisited = {Visited, NotVisited} alpha = {Alpha, NotAlpha} inphi = {InPhi, NotInPhi} level = {O... NumLevel \n 1} } c Each edge x + y c E has an attribute that specifies the type of the edge: {Dedge, Jedge}. c \nThe Pigg yBan k is an array of pointers to nodes. Its structure is defined as follows: struct Piggy BankStructure{ \nNode Structure *node Piggy BankStructure *next } Piggy Bank[NumLevel] o CurrentLevel is initially NumLevel \n 1,and subse\u00adquently has a value that corresponds to the level num\u00adber of the node that GetNodeo returns. \ncurrent R.o ot always points to the node that GetN\u00ad odeo returns. CurrentRoot is equivalent to root of \nthe Sv,bTree () whose dominance frontier is currently being computed. The first step in the algorithm \nis to insert all the nodes in N. into the Piggy Bank structure. This is shown below in the Main procedure \nas steps @to @. We mark the nodes that are initially inserted into the PiggyB a n k as Alpha to indicate \nthat they belong to the initial set N.. This is needed to avoid re-inserting these nodes into the Piggy \nBan k again in the future (a condition that we check in the procedure Visitor at step @ ). We then iteratively \ninvoke the procedure Visito on the nodes that GetNode( ) returns to compute the iter\u00ad ated dominance \nfrontier. At step ~ we assign the variable CurrentRoot to point to the node x that GetNodeo returns in \norder to keep track of the current root of SubTree(z ). Be\u00ad fore Visit(z) is invoked at step ~ the node \nz is marked Visit ed at step @ This marking is crucial because we never visit a node that has been marked \nVisited. We check for this condition in the procedure Visito at step @ . Algorithm 4.1 The following \nalgorithm computes N4 = IDF(Na). ~ Inpuk A DJ graph DJ = (N, E), and the initial set N. g N of sparse \nnodes. 4 Output The set IDF = N4 = DF+(Na). ~ Initialization c IDF = {} s Vx E N (x. visited = NotVisited \n; x.inphi = NotInPhi ; x.alpha = Not Alpha ; / . Compute the level numbers. / x.level = Level(z)) ~urrentLevel \n= Num Level ~ ~ The Algorithm Maino { / Insert N. into the PiggyBank * / 1: foreach x G No do 2 x.alpha \n= Alpha 3: InsertNode(z) 4 endfor / * repeat until no more nodes in the PiggyBank * / 5: while((z = \nGetNodeo) ! = NULL) 6: CurrentRoot = z 2 x.vzstted = Visited 8: Visit(z) 9: endwhile } / EndMain / The \nprocedure Visito, called with the current root CurrentRoot, essentially traverses the dominator sub-tree \nSubTree(CurrentRoot) in a top-down fashion marking all nodes in the sub-tree as Visited if the nodes \nare not al\u00adready marked Visit ed (a condition checked at step ~. Notice that the nodes in the dominator \nsub-tree are con\u00adnected through D-edges. As it walks down the sub-tree, the procedure Visito also peeks \nat all nodes that are con\u00adnected through J-edges, but does not mark them as Vistted. It only checks the \nlevel number of these nodes, and when\u00adever it notices that the level number of a node (that it peeked \nthrough a J-edge) is less than or equal to the level number of CurrentRoot, it adds the node into the \nset IDF, if the node is not already in the set (a condition checked at step ~. It also marks the node \nas InPhi whenever the node is added to the set IDF. This marking is necessary to avoid adding the node \nagain into IDF whenever it may peek at this node through some other J-edge in the future. It also inserts \nthe node into the PiggyBank if the node is not in the set N. (a condition checked at step ~. Procedure \nVisit(x) { 10 foreach y ~ Succ(z) 11: if(s + y == Jedge) 12 if(y.ievei < currentRoot.ieve~) / Check \nif y already in N+ / 13 if(y.2nphi ! = InPhi) 14 y.inphi =InPhi /* y in N+ */ /* Compute the set N+ / \n15 IDF = IDF U{y} 16 if(y.alpha ! = Alpha) / Do not reinsert if y s N. / lZ InsertNode(y) 1s endif 19 \nendif 20 endif 21: else/* z + y is Dedge */ I* Avoid redundant visit */ 22 if(y.visited ! = Visited) \n23: y.visited = Visited 24 Visit(y) 25 endif 26 endif 2R end for } / EndVisit / InsertNodeo inserts a \nnode into the PiggyBank at an index equal to the level number of the node. Procedure InsertNode(~) { \n2&#38; z.ned = ~iggyBank[x.level] 29 Piggy Bank[z.levei] = x } / EndInsertNode / GetNodeo returns a \nnode whose level number is the max\u00adimum of all the nodes currently in the Piggy Bank. GetN\u00adodeo also \nremoves this node from the Piggy Bank, and ad\u00adjusts the (?urrentLevel accordingly. CurrentLevel keeps \ntrack of the level number of the node that GetNodeo re\u00adturns. Note that anode will never be inserted \nin PiggyBank at a level number greater than CurrentLevel. As a result, CurrentLevel monotonically decreases \nthrough the level numbers. That is, the calls to Visit(z) at step @is per\u00adformed in a bottom-up fashion, \nin contrast, with each such call, the traversal of the dominator sub-tree rooted at x is performed in \na top-down fashion. The marking of the nodes prevents any nodes from being processed more than once in \nthe algorithm. This is essential to ensure the time linearity of the algorithm. Function GetNodeo { /* \nMore nodes in the current level */ 30 if(pzggy~ank[cumentLevei] ! = NULL) 31: x = Piggy Bank[CurrentLevei] \n/ delete x from PiggyBank */ 32: .PiggyBank[CurrentLevel] = x.ned 33 return x 34 endif 35 for i = CurrentLeve~ \ndownto 1 do 36 if(PiggyBank[i] ! = NULL) / * Update the current level */ 32 CurrentLevel = i 3s z = Piggy \nBank[i] / Delete x from PiggyBank / 39 Piggy Bank[i] = z.nezt 40 return x 4k endif 42 endfor / No more \nnodes in PiggyBank */ 43: return NULL } /* EndGetNode */ Example Next we illustrate Algorithm 4.1 through \nan ex\u00adample, Consider the DJ-graph shown in Figure 2. Let N. = {5, 13}. The first step is to deposit \nthe nodes 5 and 13 into the Piggy Bank, and also mark them as Alpha. After the for loop at step ~ the \nPiggyBank would look like Fig\u00ad ure 3(a). At step ~ the function GetNodeo returns node 13. (GetNodeo also \nremoves 13 from the PiggyBank.) At step ~ CurrentRoot is set to node 13. To find the dominance frontier \nof node 13 we call Visit(13) at step ~ Prior to this, we also mark node 13 as Visited at step @. In the \nprocedure Visito, at step @ we find that the suc\u00adcessor nodes of 13 to be nodes 3, 15, and 14. Of these, \n13+ 15and 13+ 3areJ-edges, and 13+ 14isaD\u00ad edge. Since 15.level = 2 and 3.level = 2 are less than CurrentRoot.~evel \n= 13.level = 5,nodes3and15areadded to IDF (since they are not already in ID F). Also, neither 3 nor 15 \nis marked Alpha (and hence not in N.), both the nodes are inserted into the PiggyBank (Step ~. Fig\u00adure \n3(b) shows the new state of the Piggy Bank. Next, since the edge 13 + 14 is a D-edge, and node 14 is \nnot yet visited, we call Visit(14) at step @ . Again, before ca11ingVisit(14), we mark node 14 as Visited \n(step ~. The only successcr of 14 is node 12, and 12./eve/ = 4 is less than CurrentRoot.ievei = 13.level \n= 5. Ak+o, node 12 is neither in IDF nor in Nat and so is added to IDF and inserted into the PiggyBank \n(step @ and @ respectively). The call to Visit(13) terminates and returns at step @. Now the function \nGetNodeo is executed at step @and it returns node 12. Visit(12) is called at step ~ and CurrentRoot \nis set to node 12. The only successor of 12 is node 13, and 12 + 13 is a D-edge. Since node 13 is already \nmarked Visited, the call to Visit(12) terminates and returns at step H. GetNodeo is called again, and \nthis time it returns node 5. Visit(5) is called at step ~ and the process continues. Figure 3 shows \na partial trace of the PiggyBank for the possible because of the PiggyBank structure and we for\u00ad example. \nmalize this in Lemma 5.1 and Lemma 5.2. We then make an inductive argument on the decreasing level of \nthe nodes to demonstrate that all nodes in DF (v ) should already be inserted into IDF by this time. \nThe node z should also be 0 1 2 5 3 4 5 13 6 CurrentLevel = 5 N+= {] &#38; (a) L-l H 5 CurrentLevel \n= 4 No= {15,3,12) 6 (c) 0 1 2 3 4 5 c, rrr.nfl . .1 = 5 . .. . n N+= {15,3) 6 (b) 0 in IDF according \nto Lemma 3.1. From Theorem 5.1 is established. In our chain of proofs, we begin with states that a node \ncan never be inserted at an index greater than the level number this the validity of Lemma 5.1, which \nin the PiggyBank of the current root node CurrentLevel. We use this fact to prove Lemma 5.2. Lemma 5.1 \nA node is never inserted in the PiggyBank at an index that is greater than cuTTentLevei. Lemma 5,2 gives \nan order (based on the level number of nodes) in which calls to Visitor at step ~ can be performed. 1 \nThe ordering of nodes is controlled by calls to GetNodeo at 2 815 3 23 step @. Recall that GetNodeo \nalways returns a node whose CurrentLevel = 2 level number is the maximum of all nodes currently stored \n4 N+= {15,3,12,8,2) 5 in the PiggyBank structure.  a---\u00ad (d) Figure 3: Partial trace of #-node placement \nalgorithm 5 Correctness and Complexity In this section we establish the correctness of Algorithm 4.1 \nand analyze its complexity. Due to space reasons, we state the main theorems (Theorem 5.1 and Theorem \n5.2) and only give intuitive sketch of their proof structures. We also state all the supporting lemmas \nneeded and state their intuitive meaning. All proofs can be found in [SG94]. 5.1 Correctness The main \ntheorem which establishes the correctness of Al\u00adgorithm 4.1 is Theorem 5.1. The theorem states that the \nalgo\u00adrithm computes the iterated dominance frontiers of the set N.. The inductive proof of the theorem \nis based on a major lemma (Lemma 5.4), which establishes the fact that when the algorithm calls Visit(z) \nat step @and the call terminates, all nodes in the dominance frontiers DF(x) are already added into the \nset IDF (a fact used both in the induction basis and induction steps). Let z be the current root of a \ndominator SubTree(z) visited by Visit(z) at step @. Let z be in DF(z). Lemma 3.1, introduced earlier \nin Section 3, guarantees that there must exist a node g in SubTree(z) such that y ~ z is a J-edge and \nlevel.z ~ level.z. states that y will already have Visit(z) returns. There are two a node can be marked \nVisited: step @. The validity of Lemma Another lemma, Lemma 5.3, been marked Vzsited when cases in the \nalgorithm where (1) at step ~ and (2) at 5.4 for case 1 is straightfor\u00ad ward. For case 2, y must be \nmarked Vis ite~ by an earlier call of Visit(v) for a node v in SubTree(z). This fact is made Lemma 5.2 \nLet x and y be any two nodes that are inserted in the Piggy Bank and later removed (and returned) from \nthe PtggyBank by GetNodeo at step @. lf y.level > z.level, then Visit(y) will be called earlier than \nVisit(x) at step @ The next lemma establishes an important fact that when a node z is visited by a call \nof Visit(z) from step @and returned, that all nodes in the dominator SabT~ee (z) have been marked Visited. \nIntuitively this means that when such a visit returns, none of the nodes in the SubTree (z ) have been \noverlooked. Lemma 5.3 When Visit(z) returns at step ~ all nodes in subTTee(x) are marked Visited. It \nis easy to see from Lemma 5.2 and Lemma 5.3, that calls to Visito at step @are made in a bottom-up fashion \nand while each recursive call at step @ the recursive procedure Visito visits the nodes in the dominator \ntree fashion. Lemma 5.4 is the main lemma which shows cedure Visito captures the dominance frontier the \nset IDF. Intuitively the lemma states that in a top-down how the pro\u00adof anode in when Visit(z) is called \nand terminated at step S all the nodes in the dom\u00ad @ inance frontier of x are added to the set IDF. \nLemma 5.4 When Visit(x) is called with x as the current Root and returned at step@ all the nodes in DP \n(x ) are also in the set IDF. Notice that the above lemma only says that Visit(z), when it returns at \nstep ~ will have added the entire dominance frontier of x to IDF. It does not specify which of the nodes \nin the set IDF belong to DF(x). Notice that the set IDF can contain nodes that are not in the set DF(x \n). Theorem 5.1 Algorithm 4.1 correctly computes the set of ~-nodes N+ = IDF(Na). The proof of the theorem \neasily follows from the above lemmas. 68  5.2 Complexity Next we will show that the time complexity \nof Algorithm 4.1 is O(I El). Recall that the number of edges in the DJ-graph is less than INf I + IEf \n1. Therefore, the time complexity of Algorithm 4.1 is O(lNf I + lEjl). Since lEf I > lNf I 1, the time \ncomplexity of the algorithm is 0( IEf I), which is linear with respect to the number of edges in the \nflowgraph. From Algorithm 4.1, readers may have already observed that for any node x in the DJ-graph, \nthe node maybe pro\u00adcessed by a call of Visit(z) (which may happen at step @ or D24 at most once. This \nobservation is a key to the proof of linearity of the algorithm, and is stated as the following lemma. \nLemma 5.5 When Algorithm 4.1 terminates, a node x c N may beprocessedby a call to Visit(x) at most once. \nFrom the above lemma, one can see that a node can never be marked Visited more than once, and there can \nbe at most INI calls to Visito. Recall that at each node in the procedure Visito, we either visit (through \na D-edge) or peek (through a J-edge) all the successor nodes (step ~ only once. This means that we have \neffectively probed all the edges in the DJ\u00adgraph at most once. Hence one can see that the complexity \nof the algorithm is 0(1131). Theorem 5.2 The time complexity ofAlgorithm 4.1 is 0(1111). An acute reader \nmay ask the following question: What about the complexity of inserting and deleting nodes into/from the \nPiggy Bank? It is easy to see that the com\u00adplexity of inserting a node in the PiggyBank is O(l). As for \nthe complexity of getting a node from the Piggy Bank, it is again easy to see that a node will never \nbe inserted in the PiggyBank at the index greater than the CumentLevei (from Lemma 5.1). Each call of \nGetNodeo will execute the for loop with a monotonically decreasing Cuneni!Level from NumLevel 1 down-to \n1during successive calls for the entire duration of the algorithm (follows from Lemma 5.1). Hence the \noverall complexity of deleting nodes from the PiggyBank is, in the worst case, O(INI). 5.3 Discussion \nRecall that one of the key point that makes our algorithm linear is the PiggyBank structure. If one were \nto use other structures such as a linked-list, a stack or a queue, either the proof of correctness would \nfail (if we still wish to continue to mark the nodes as Visited using one color), or the complexity of \nthe algorithm would not be linear (we will need to mark the nodes as Visited using more than one color). \nThe second situation is similar to finding the iterated dominance frontier by iteratively applying Algorithm \n3.1. We can easily show that the complexity of this method will be quadratic. To fully understand the \nabove discussion, the readers are encouraged to apply the algorithm, with No = {O, 2}, to the ladder \ngraph example shown in Figure 4(a) whose DJ-graph is shown in Figure 4(c). Try to use a linked-list structure \nto replace the PiggyBank structure, and assume node O is visited before node 2. o 1 x DF(x) o {) 23 \n1 {3] 2 {3,5,7) 3 {5]  45 4 {5,71 5 {7] 6 {7) 7 () 67 J i F@qh CommmeFmitm DJ.gwh (a) (b) (c) Figure \n4: A ladder flowgraph  6 Implementation and Experimen\u00adtal Results In this section we present our experimental \nresults and their analysis. We implemented our linear time algorithm on top of the Parafrase compiler \n[Har85b] and executed on 46 FOR-TRAN routines taken from Perfect, Eispack, Lapack, Ode, Opt, and Gator!. \nWe particular chose those procedures from these suites that are large and have unstructured con\u00adtrol \nflows. We carried out our experiments on a SPARC-10 workstation. To compare the performance of our algorithm \nwith the original algorithm, we also implemented the @ node placement algorithm based on iterating through \nthe dominance frontiers [CFR+ 91]. This implementation also allowed us to doubly verify the correctness \nof our algorithm by matching the results of ~-nodes of the two algorithms. To be fair, the time measurement \nshown for the original al\u00adgorithm does not include the time for pre-computing the dominance frontiers. \nFor convenience, we will denote lDF(d~) for iterated dominance frontier algorithm based on dominance \nfrontiers, and .IDF(new) for our new linear time algorithm. Figures 5 and 6 shows the results for some \ntypical procedures taken from Perfect and Lapack programs, respectively. The X-axis gives the names of \nthe procedure we experimented on. In Figure 7 we give the performance for all the 46 FORTRAN procedures \nwe tested. The second column in the table gives the number of flowgraph nodes for the corresponding pro\u00adcedure. \nThe time measurements (averaged over 25 runs) shown for 1.DF(d~) and lDF(new) are for computing ~\u00adnodes \nfor a single SEG. We randomly chose 15 to so~o of the nodes to be N., the initial set of sparse nodes; \nand we chose the same N. for both .IDF(new) and IDF(df) algorithms. Originally we implemented the lDF(d~) \nalgorithm using bit-vectors for encoding the dominance frontiers for each node. With this implementation \nour algorithm exhibited a 8E@ck, LaPack, ode, and Opt are available from netlib.att.com. Gator is a Gas, \nAerosol, Transport and Radiation model, and is available from ftp.cs.berkeley.edu ~erfect ime (miuis~) \nr\u00ad d.dcm c,wd re.dm d.t.. +rn, e-rchk mod.h nmseq rn.sfe LY -IDF (new) ~ lDF(df) Figure 5: Performance \nof IllF(new) and lDF(d~) al\u00adgorithms on Perfect LaPack rm, WUS+ 8 ..,...., ,, 1 I cbdq .ges d .hg.qz \ncMbs .{.,-dbdw ds% ds=.d dh.wv d%.. d,-.. -lDF (new) ~ !DF(d9 Figure 6: Performance of lDF(new) and lDF(&#38;) \nal\u00adgorithms on Lapack speedup ranging from 4.2 to 19.8 over the lDF(d~) algo\u00adrithm, with average speedup \nbeing around 9.0. We then translated the bit-vector representation to a linked-list rep\u00adresentation. \nWith this representation our algorithm shows a speedup ranging from 2.2 to 8.9 over the lDF (d$) algorithm, \nwith average speedup being around 5.5. This suggest that bit-vectors may not be the best representation \nfor encoding dominance frontiers. Figure 7 shows the performance of our algorithm over the lDF(dj) algorithm \n(using linked-list structure for repre\u00adsenting the dominance frontiers). In particular, the speedup for \nroutines that we tested from the Perfect suite is ranging from 2.2 to 8.0, with average speedup being \n5.0. For rou\u00adtines from Lapack speedup range is from 3.9 to 8.1, with the average speedup being 5.7. \nFor smaller programs (less than 75 nodes) we found that both the algorithms take scant time. Clearly \nfrom these plots, we can see that our algorithm performs consistently and significantly faster even for \nreal programs we tested. Our observation here is somewhat dif\u00adferent from [CFR+ 91]: the linear complexity \nof the algorithm has demonstrated significant benefit in terms of speedup on real programs. We also measured \nthe execution time for both the al\u00ad gorithms on increasingly taller ladder graphs of the form shown \nin Figure 4. Recall that for this graph, previous alg~ rithms exhibit non-linear running time because \nthe size of the dominance frontiers of the left-spine increases quadratically as the size of the ladder \nis increased. For the ladder graphs, we tested our implementation on a SPARC 20 workstation. We measured \nthe running time of lDI (d~) and lDF (new) algorithms as the size of the graph is increased. Figure 8 \nshow the performance curve for both the algorithms on increasingly taller ladder graph. As expected, \nIDF(df ) exhibit quadratic running time, while our new algorithm shows a linear behavior. Notice that \nthe measurement shown for the IDF (df ) algorithm is in seconds, while for our algorithm it is in milliseconds. \nThis shows that our algorithm is not only linear, but is also significantly faster even for increasingly \ntaller ladder graphs. We observed sim\u00adilar trend for the nested repeat unt i 1 flowgraph. 7 Related \nWork The sparse evaluation technique is becoming popular, es\u00adpecially for analyzing large programs. To \nthis end, many intermediate representations have been proposed in the lit\u00aderature for performing sparse \nevaluation [CFR+ 91, CCF91, JP93, WCES94]. The algorithms for constructing these in\u00adtermediate representations \nhave one common step-deter\u00ad mining program points where data flow information must be merged (the so \ncalled #-nodes). The notion of d-nodes dates back to the work of Shapiro and Saint [SS70] (as noted in \n[CFR+ 91]). Subsequently, others have proposed sparse evaluation in one form or another that is related \nto work of Shapiro and Saint [RT82, CF87]. Cytron et al. [CFR+ 89] gave the first algorithm for computing \nd-nodes for arbitrary flow\u00adgraphs. Tie time comple~ity of the algorithm depended on the size of the dominance \nfrontier, which is O(N2). Recently C@on and Ferrante improved the quadratic behavior of computing ~-nodes \nto be almost linear time. The time com\u00ad 70 Proc INI IDF(new)t Perfect dcdcmp 138 0.71 card 151 0.75 readin \n407 1.18 bjt 136 0.22 dctran 310 1.23 elprnt 163 0.91 errchk 347 1.05 modchk 307 1.67 moseq2 162 0.92 \nmosfet 215 1.17 Eispack bandv 126 0.15 hqr2 177 0.27 invit 189 0.66 minfit 120 0.11 qzit 125 0.17 svd \n139 0.49 tsturm 151 0.25 Lapack cbdsqr 238 1.33 cgesvd 314 1.07 chgeqz 183 0.44 clatbs 228 1.07 clatrs \n218 0.65 dbdsqr 238 1.28 dgegv 173 0.48 dgesvd 324 0.84 dhgeqz 295 1.5 dtgevc 332 1.8 dtrevc 250 0.44 \nOde ddassi 235 1.07 svodpk 245 0.85 cntrl 228 0.38 ddastp 184 1.0 newmsh 158 1.14 * 245 1.05 dbocls \n169 0.16 dbols 129 0.4 dbolsm 318 1.81 Gator aerset 353 1.73 aqset 207 0.67 chemset 242 1.27 equilset \n350 2.06 initgas 203 1.07 jsparse 289 1.36 out 405 1.54 reader 217 0.93 smvgear 224 0.77 in milliseconds \nFigure 7 Performance of so-me typical programs. S 1.74 1.66 8.47 1.76 8.49 2.56 8.32 6.49 3.47 3.46 \n0.94 1.67 2.76 1.06 1.34 2.39 2.04 5.1.5 5.78 3.17 5.17 4.88 5.08 3.46 6.81 5.82 7.02 5.18 3.50 4.82 \n3.13 2.60 2.88 4.34 1.43 1.19 7.82 8.73 3.79 4.71 11.37 4.86 8.84 9.48 5.08 4.88 and Speedup 2.4 2.2 \n7.2 8.0 6.9 2.8 7.9 3.9 3.8 2.8 6.3 3.6 4.1 9.6 7.9 4.9 8.2 3.9 5.4 7.2 4.8 7.5 3.9 7.2 8.1 3.8 3.9 6.5 \n3.3 5.7 8.2 2.6 2.5 4.1 8.9 2.9 4.3 5.0 5.7 3.9 5.5 4.5 5.6 6.2 5.5 6.3 lDl?(d~) on lDF(new) lima (milisec) \n,23 -.. . <m m ea ,0 20 i o 0 ,C.w w Nu.bs r%? NcA.s\u00ad - ,Cca Sc.w E iDF(new) JF(df) rime pns) XcO- 25a\u00ad \nzW \u00ad ,Sm ) ~ IIKxl a xc\u00ad0 /2 1 5c.m5x07w00m  0 ,CC.2CCX.W Nun&#38;% NOdes = IDF(07J Figure 8: Performance \nof both the algorithms on ladder graph plexity of the new algorithm is O(E x a(E)), where ao is the inverse-Ackermann \nfunction [CF93]. It seems that the algorithm has not been implemented [CF93], and since the algorithm \nis not exactly linear, more experimental studies are needed to evaluate the performance of that algorithm \nwhen applied to real programs. Compared to any of the previous work, our algorithm reduces the time complexity \nof constructing a single SEG to O(E). Also, we can use our algorithm to construct SSA form or DFG in \ntime O(,E x V), where V is the number of variables. Johnson and Pingali recently proposed an algorithm \nfor constructing SSA-like representation called the Dependence Flow Graph (DFG) UP93]. To construct DFG \nthey first com\u00adpute regions of control dependence. Using this information they determine single-entry-single-exit \nregions. Then they perform, for each variable, an inside-out traversal of these regions, computing dependence \ninformation and inserting switch and merge nodes, whenever dependence cross re\u00adgions of control dependence. \nThe authors have shown that the running time of the algorithm for constructing DFG is O(E). One can easily \nconstruct the SSA form from the DFG by simply eliminating switch nodes in the DFG. Although, the method \nof Johnson and Pingali can be used for construct\u00ading the SSA form in time O (E x V) (where V is the number \nof program variables) UP93], it has the same problem as the SSA form, i.e. the DFG and the SSA form cannot \nbe used for solving arbitrary data flow problems (for example, liverress analysis), as noted in [CF93]. \nAlso, their algorithm can not be used for computing the iterated dominance frontiers for a 71 lDF(new) \n set of nodes. Iterated dominance frontiers can also used for applications other than for placing ~-nodes \n[SGL94b, Wei92]. Recently Johnson et al. use Quick Propagation Graphs (QPGs) for performing sparse evaluation \nfor arbitrary data flow problem UPP94]. They give an algorithm for construct\u00ading QPGs that runs in linear \ntime. Construction of QPG is based on first constructing regions of control dependence. A disadvantage \nof QPGs is that it is more denser than SEGS. This means that solving data flow analysis may take more \ntime on QPGs than on SEGS. We are not aware of any other algorithm for comput\u00ading ~-nodes. There is much \nrelated work that uses SSA like representation, for example, the Program Dependence Web [BM090] and the \nValue Dependence Graph [WCES94], and our algorithm could improve the complexity of con\u00adstructing these \nrelated intermediate representations. Also there are many optimizations that use SSA form for efficient \nimplementation, for example, constant propagation [WZ85], value numbering [RWZ88], register allocation \n[Bri92], code motion [CLZ86], induction variable recognition [W0192], etc. Our algorithm could improve \nthe overall running time of these optimization. In this paper we have employed a new program representation \nthe DJ-graph. Derived from a flowgraph, the DJ-graph can be viewed as a refinement representing ex\u00adplicitly \nand precisely both the dominator relation between nodes (via D-edges) and the potential program points \nwhere the dataflow information maybe merged (via J-edges). Pre\u00adviously DJ-graphs have been used indirectly \nfor capturing control flow properties of a flowgraph. DFt~~~L relation of Cytron et al. [CFR+ 91] are \nequivalent to J-edges. An edge z --A ~ is a join edge iff ~ e DFlo.az(z). In the DJ-graph we explicitly \nrepresent the DFlacal relation with join edges. CD.START and CD. END relations in [CFS90] are again re\u00adlated \nto J-edges. The Algorithm 3.1 for computing domi\u00adnance frontier is similar to the algorithm given by \nCytron et al., with one difference, we use level information for captur\u00ading the dominance frontiers of \na node, while Cytron et al. use CD.START and CD. END relations to do the job. As demonstrated in this \npaper, DJ-graphs have facilitated the development of our algorithm. Furthermore, some prop\u00aderties of \nDJ-graphs make much easier the proofs of the cor\u00adrectness and linearity of our algorithm. The DJ-graphs \ncan also be applied to program analysis other than computing $-nodes, but that is beyond the scope of \nthe present paper. In a recent work, we have used the algorithmic frame\u00adwork described in this paper \nto solve the problem of in\u00adcrementally maintaining dominator trees for an arbitrary flowgraphs [SGL94b]. \nIn the same paper we also propose a method to incrementally update the set of @-nodes of a SEG when the \nflowgraph is subjected to incremental changes. We believe the framework presented here is robust to accommo\u00addate \nincremental program analysis based on SEGS, We will further explore on this in a future paper. Conclusion \n In this paper, we have provided a positive answer to the open problem posted in the introduction: it \nis indeed possible to design an algorithm for computing &#38;nodes in linear time. This is a good news \nfor work which depends on efficient dataflow analysis as computing ~-nodes is a key step in constructing \na sparse dataflow evaluation framework. Fur\u00adthermore, the algorithm presented in this paper is very sim\u00adple. \nOur algorithm uses the properties of a new program repre\u00adsentation called the DJ-graph which facilities \nits design and analysis. We also benefit from the simplicity of the algorithm in its implementation. \nWe have constructed a prototype implementation of the algorithm on the top of Parafrase2 compiler. Our \nexperimental results indicate consistent and significant speedup even on real benchmark programs. On \nincreasingly taller ladder graphs our algorithm exhibit linear behavior. We have been using DJ-graphs \nand the algorithmic frame\u00adwork presented here to solve a number of other related flow\u00adgraph problems. \nWe direct interested readers to our compan\u00adion papers [SGL94a, SGL94b]. Acknowledgement We would like \nto thank many people for their support and encouragement during the course of our work. We owe much thanks \nto Yong-fong Lee for his insightful technical discussions and also for critically commenting on drafts \nof the paper. Erik Altrnan simplified one of the proofs and also suggested many improvements. The comments \nof Russell Olsen, Bjarne Steengard, Berry Rosen, and the POPL review\u00aders were very useful and improved \nthe quality of the presen\u00adtation. Rajiv Gupta and Ron Cytron s constant encourage\u00adments and support are \ngreatly appreciated. We thank the Na\u00adtional Sciences and Engineering Research Council (NSERC) and the \nCanadian Centers of Excellence (IRIS) for their con\u00adtinued support of this research. Lastly the first \nauthor would like to dedicate this paper in memory of his late father. References [AWZ88] Bowen Alpern, \nMark N. Wegman, and F. Ken\u00adneth Zadeck. Detecting equality of variables in programs. In Conference Record \nof the 15th An\u00adnual ACM Symposium on Principles of Programming Languages, pages 1 11, 1988. [BM090] Robert \nA. Ballance, Arthur B. Maccabe, and Karl J. Ottenstein. The program dependence web: A representation \nsupporting control-, and demand-driven interpretation of imperative lan\u00adguages. In Proceedings of the \nSIGPLAN 90 Confer\u00adence on Programming Language Design and Imple\u00admentation, pages 257 271, 1990. [Bri92] \nPreston Briggs. Register Allocation ing. PhD thesis, Rice University, April 1992. via Graph Color-Houston, \nTexas, [CCF91] Jong-Deok Choir Ron Cytron, and Jeanne Fer\u00adrante. Automatic construction of sparse data \nflow evaluation graphs. In Conference Record of the 18th Annual ACM Symposium on Principles of Program\u00adming \nLanguages, 1991. [CF87] [CF93] [CFR+89] [CFR+91] [CFS90] [CLZ86] [Har85a] [Har85b] UP93] UPP94] [RT82] \nRon Cytron and Jeanne Ferrante. What s in a name? or the value of renaming for parallelism detection \nand storage allocation. In Proceedings of the 1987 International Conference on Parallel Pro\u00adcessing, \npages 19-27, St. Charles, Illinois, August 17-21,1987. Ron Cytron and Jeanne Ferrante. Efficiently com\u00adputing \n~-nodes on-the-fly. In Languages and Com\u00adpilers for Parallel Computing, 1993. Ron Cytron, Jeanne Ferrante, \nBarry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck. An efficient method for computing static single\u00adassignment \nform. In Conference Record of the Sixteenth Annual ACM Symposium on Principles of Programming Languages, \npages 25-35, Austin, Texas, January 11-13, 1989. ACM SIGACT and SIGPLAN. Ron Cytron, Jeanne Ferrante, \nBarry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck. Effi\u00adciently computing static single assignemnt \nform and control dependence graph. ACM Transactions on Programming Languages and Systems, 13(4):452\u00ad490, \nOctober 1991. Ron Cytron, Jeame Ferrante, and Vivek Sarkar. Compact representations for control depen\u00addence. \nIn Proceedings of the SIGPLAN 90 Con\u00adference on Programming Language Design and lm\u00adplementationr pages \n337 351, White Plains, New York, June 20 22, 1990. ACM SIGPLAN. Also in SIGPLAN Notices, 25(6), June \n1990. Ron Cytron, Andy Lowry and Kenneth Zadeck. Code motion of control structures in high-level languages. \nIn Conference Record of the Thir\u00adteenth Annual ACM Symposium on Principles of Programming Languages, \npages 70-85, St. Peters\u00adburg Beach, Florida, January 13-15, 1986. ACM SIGACT and SIGPLAN. Dov Harel. \nA linear time algorithm for finding dominators inflow graphs and related problems. In Symposium on Theoyof \nComputing. ACM, May 1985. William Harrison. An overview of the struc\u00ad ture of Parafrase. Technical Report \n501, PR-85-2 UILU-ENG-85-8002, UIUC, July 1985. R. Johnson and K. Pingali. Dependence-based program ananlysis. \nIn Proceedings of the SIG-PLAN93 Conference on Programming Language De\u00adsign and Implementation, pages \n78-89,1993. R. Johnson, D. Pearson, and K. Pingali. The pro\u00adgram tree structure: Computing control regions \nin linear time. In Proceedings of the SIGPLAN94 Conference on Programming Language Design and Implementation, \npages 171-185,1994. J. H. Reif and Robert Tarjan. Symbolic program analysis in almost linear time. SIAM \nJournal of Computing, 11(1):81-93, February 1982. [RWZ88] [SG94] [SGL94a] [SGL94b] [ss70] [WCES94] [Wei92] \n[W0192] [WZ85] Barry K. Rosen, Mark N. Wegman, and F. Ken\u00adneth Zadeck. Global value numbers and redun\u00addant \ncomputations. In Conference Record of the Fifeenth Annual ACM Symposium on Principles of Programming \nLanguages, pages 12-27, San Diego, California, January 13-15, 1988. ACM SIGACT and SIGPLAN. Vugranam \nC. Sreedhar and Guang R. Gao. Com\u00adputing ~-nodes in linear time using DJ-graphs. Technical Report ACAPS \nMemo 75, School of Computer Science, McGill University, January 1994. Submitted for publication. VugranamC. \nSreedhar, Guang R. Gao, and Yong\u00adfong Lee. DJ-graphs and their applications to flowgraph analyses. Technical \nReport ACAPS Memo 70, McGill University, May 1994. Submit\u00adted for publication. VugranamC. Sreedhar, GuangR. \nGao, and Yong\u00adfong Lee. An efficient incremental algorithm for maintaining dominator trees and its application \nto @nodes update. Technical Report ACAPS Memo 77, McGill University, July 1994. Submit\u00ad ted for publication. \nR. M. Shapiro and H. Saint. The representation of algorithm. Technical Report CA-7002-1432, MCA, 1970. \nDaniel Weise, Roger F. Crew, Michael Ernst, and Bjarne Steensgaard. Value dependence graphs: Representation \nwithout taxation. In Conference Record of the 21st Annual ACM Symposium on Prin\u00adciples of Programming \nLanguages, 1994. Michael Weiss. The transitive closure of control dependence: the iterated join. ACM \nLetters on Programming Languages and Systems, 1(2), June 1992. Michael Wolfe. Beyond induction variables. \nIn Proceedings of the SIGPLAN 92 Conference on Pro\u00adgramming Language Design and Implementation, pages \n161-174,1992. Mark Wegman and Ken Zadeck. Constant prop\u00adagation with conditional branches. In Conference \nRecord of the Twelfih Annual ACM Symposium on Principles of Programming Languages, pages 291 299. ACM \nSIGACT and SIGPLAN,January 1985.  \n\t\t\t", "proc_id": "199448", "abstract": "<p>Dataflow analysis framework based on Static Single Assignment (SSA) form and Sparse Evaluation Graphs (SEGs) demand fast computation of program points where data flow information must be merged, the so-called &fgr;<bold>-nodes</bold>. In this paper, we present a surprisingly simple algorithm for computing &fgr;-nodes for arbitrary flowgraphs (reducible or irreducible) that runs in <italic>linear</italic> time. We employ a novel program representation&#8212;the <bold>DJ graph</bold>&#8212;by augmenting the dominator tree of a flowgraph with edges which may lead to a potential &#8220;merge&#8221; of dataflow information. In searching for &fgr;-nodes we never visit an edge in the DJ-graph more than once by guiding the search of nodes by their levels in the dominator  tree.</p><p>The algorithm has been implemented and the results are compared with the well known algorithm due to Cytron et al. A consistent and significant speedup has been observed over a range of 46 Fortran procedures taken from a number of benchmark programs. We also ran experiments on increasingly taller ladder graphs and confirmed the linear time complexity of our algorithm.</p>", "authors": [{"name": "Vugranam C. Sreedhar", "author_profile_id": "81100375252", "affiliation": "School of Computer Science, McGill University, Montreal H3A 2A7, Canada", "person_id": "P292332", "email_address": "", "orcid_id": ""}, {"name": "Guang R. Gao", "author_profile_id": "81100134147", "affiliation": "School of Computer Science, McGill University, Montreal H3A 2A7, Canada", "person_id": "P100128", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/199448.199464", "year": "1995", "article_id": "199464", "conference": "POPL", "title": "A linear time algorithm for placing &#966;-nodes", "url": "http://dl.acm.org/citation.cfm?id=199464"}