{"article_publication_date": "01-25-1995", "fulltext": "\n A Call-By-Need Lambda Calculus Zena M, Ariola Matthias Felleisen computer &#38; Information Science \nDepartment Department of Computer Science University of Oregon Eugene, Oregon John Maraist and Martin \nOdersky Institut fur Programmstrukturen Universitat Karlsruhe Karlsruhe, Germany Abstract The mismatch \nbetween the operational semantics of the lambda calculus and the actual behavior of implemen\u00adtations \nis a major obstacle for compiler writers. They cannot explain the behavior of their evaluator in terms \nof source level syntax, and they cannot easily com\u00adpare distinct implementations of different lazy strate\u00adgies. \nIn this paper we derive an equational characteri\u00adzation of call-by-need and prove it correct with respect \nto the original lambda calculus. The theory is a strictly smaller theory than the lambda calculus. Immediate \napplications of the theory concern the correctness proofs of a number of implementation strategies, e.g., \nthe call\u00adby-need continuation passing transformation and the re\u00adalization of sharing via assignments. \n1 Introduction Lazy functional programming languages implement the call-by-name lambda calculus. Their \nsyntax provides syntactic sugar for A terms; their evaluators map closed terms to values. The semantics \nof lazy function calls is the famous ~-axiom: every call is equivalent to the body of the function with \nthe formal parameter replaced by the actual argument. The /?-axiom gives rise to an equational theory, \nthe J-calculus, which implementors can use to reason about the outcome of programs [26]. Taken literally, \nthe ~-axiom suggests that a proce\u00addure must evaluate the argument of a specific call for each occurrence \nof the corresponding formal parameter. Realistic implementations of lazy functional languages avoid such \ncomputational overhead by memoizing the Permission to copy without fee all or part of this material \nis granted provided that the copies are not made or distributed for direct commercial advantaqe, the \nACM copyright notice and the title of the publication and Its date appear, and notice is given that copying \nis by permission of the Association of Computing Machinery. To copy otherwise, or to republish, requires \na fee and/or specific permission. POPL 95 1/95 San Francisco CA USA O 1995 ACM 0-89791-692-1/95/0001 \n....$3.50 Rice University Houston, Texas Philip Wadler Department of Computing Science University of \nGlasgow Glasgow, Scotland argument s value when it is first evaluated. More pre\u00adcisely, lazy languages \nonly reduce an argument if the value of the corresponding formal parameter is needed for the evaluation \nof the procedure body. Moreover, af\u00adter reducing the argument, the evaluator will remember the resulting \nvalue for future references to that formal parameter. This technique of evaluating procedure pa\u00adrameters \nis called call-by-need or lazyl evaluation. A simple observation justifies call-by-need: the re\u00adsult \nof reducing an expression, if any, is indistinguish\u00adable from the expression itself in all possible contexts. \nSome implementations attempt to exploit this observa\u00adtion for sharing even more computations than just \nthose of arguments. Arvind et. al. [7] provide an overview of such implementations, particularly the \nso-called fully lazy or (graph-based techniques. Unfortunately, the mismatch between the operational \nsemantics of the lambda calculus and the actual behav\u00adior of the implementation is a major obstacle for \ncom\u00adpiler writers and users. Specifically, they cannot use the calculus to reason about sharing in the \nevaluation of a program. Purushothaman and Seaman [27, 29] and Launch\u00adbury [18] recently recognized \nthis problem and devel\u00adoped two slightly different (natural semantics of the call-by-need parameter passing \nmechanism. Roughly speaking, the semantics use store-passing to describe call-by-need in terms of (the \nsemantics of) assignment statements. Due to the low-level nature of this ap\u00adproach, these semantics permit \nneither a simple ex\u00adplanation of language implementations, nor source-level reasoning about program behavior. \nWorse, given slightly different specifications based on natural or similar se\u00admantic frameworks, it is \ndifficult, if not impossible, to compare the intentions with respect to sharing in the evaluators. . \n1In this paper we write call-by-need rather than lazy to avoid a name clash with the work of Abramsky \n[2], which de\u00adscribes call-by-name reduction to values. 2 Ironically, this problem immediately showed \nup in the dif\u00ad A number of researchers [1, 13, 20, 31, 28] have stud\u00adied reductions that preserve sharing \nin calculi with ex\u00adplicit substitutions, especially in relation to optimal re\u00adduction strategies. Having \ndifferent aims, the resulting calculi are considerably more complex than those pre\u00adsented here. Closest \nto our treatment is Yoshida s weak lambda calculus [31] which introduces explicit environ\u00adments similar \nto let constructs. Her calculus subsumes several of our reduction rules as structural equivalences, even \nthough, due to a different notion of observation, reduction in this calculus is not equivalent to reduction \nto a value. In this paper, we pursue a different approach to the specification of call-by-need3. Our \nformulation is en\u00adtirely syntactic, and formulates the sharing in a call\u00adby-need evaluator as an equational \ntheory of the source language, The theory is a strict sub-theory of the call\u00adby-name ~-calculus. The \nkey technical contribution of the paper is a proof of equivalence between the call-by\u00adname and the call-by-need \nevaluator. In addition, we prove that the calculus relates to the evaluator in pre\u00adcisely the same manner \nas Plotkin s call-by-name and call-by-value calculi relate to their respective evaluators. The next two \nsections present the basic ideas of the call-by-name and call-by-need calculi. The fourth sec\u00adtion presents \nbasic syntactic results such as confluence and standardization results. The fifth and sixth sec\u00adtions \nare devoted to the correctness proof the sixth section also describes an interesting alternative formu\u00adlation \nof call-by-need. The last three sections briefly discuss extensions of the language (with constants and \nrecursive declarations), the relation of our work with natural semantics-based approaches, and applications \nof the calculus. 2 The Call-by-Name calculus In this section we briefly review the call-by-name cal\u00adculus; \nwe assume a basic familiarity of the reader with this material [8]. Figure 1 details the call-by-name \ncalculus. The set of lambda terms, called A, is generated over an infinite set of variables. The expression \nM[z := N] denotes the capture-free substitution of N for each free occurrence of z in M. The reduction \ntheory associated with the calculus is the result of taking the compatible, reflexive ferences between \nthe formulation of Purushotoman/ Se.mnan and Launchbury. 3 In fact, this approach unifies the similar, \nsimultaneous and independent work of two separate groups. While many of our re\u00adsults are indeed quite \nsimilar, there are interesting and significant differences in overall perspectives, in specific definitions \nof calculi and in proof techniques. We do not describe these differences here, and instead refer the \ninterested reader to the full technical reports of Ariola and Felleisen [4] and of Maraist, Odersky and \nWadler [19] for details. and transitive closure of ~ interpreted as an asymmetric relation: The compatible \nclosure of /3 is written as =; the reflexive and transitive closure of =, as s; =name is the symmetric \nclosure of z, or the con\u00adgruence relation generated by =. We will omit the tag ..~, when we may do so \nunambiguously. In an implementation of the calculus, closed expres\u00adsions play the role of programs. An \nexecution of a pro\u00adgram aims at returning an observable value in realistic languages. Observable values \nare basic constants like numbers and booleans. If a program s result is a proce\u00addure or a lazy tree, \nmost implementations only indicate whether or not the program has terminated and possi\u00adbly what kind \nof higher-order result it returned. Since the pure theory only contains A-abstractions, an evalu\u00adator \nonly determines whether a program terminates. Given this preliminary idea of how implementations work, \nwe can use the calculus to define a partial evalua\u00adtion function evaln.~, from programs, or closed terms, \nto the singleton consisting of the tag closure: evalname(ibf) = closure iff A t M = Ax. N That is, the \nevaluation of a program returns the tag closure if, and only if, the theory can prove the pro\u00adgram is \nequal to a value. It is a seminal result due to Plotkin that the evaluation function of a typical imple\u00admentation \nis also determined by the standard reduc\u00adtion relation [26]. Put differently, a correct implemen\u00adtation \nof the evaluator can simply reduce the standard or leftmost-outermost redex of a program until the pro\u00adgram \nbecomes a value. Evaluation contexts are a convenient way of for\u00admulating the evaluation relation [11]. \nA program M standard reduces to N, written as M _ N, ifl M = l&#38;[(~x.P)Q] and N = En[P[z := Q]]. As \nusual, M s N means M standard reduces to N via the 0/1 ~ transitive-reflexive closure of -;M W means \nM standard reduces to N in zero or one step; M -n N means M standard reduces to N in n steps. As before, \nwe will omit the tag ~a~e when no confusion arises. The following characterization of the call-by-name \nevaluator is a consequence of the conflu\u00adence property and the standardization theorem of A. Proposition \n2.1 For a program M, evalname(M) = closure zff M +-+ Ax. N This result is due to Plotkin [26]. Syntactic \nDomains Variables x,Y,~ Values v, w ::= k.h(f Terms L,M, N ::= zIVIMN Evaluation contexts E. ::= [] \nj En M Axioms (6) (~z.M) N = M[x := N] Figure 1: The call-by-name lambda calculus. Syntactic Domains \nVariables ~!Y>~ Values v, w ::= kc.M Answers A ::= V/let z= Min A Terms L,M, N ::= z[VIMNlletz=Min N \nAxioms (let-I) (/kr.M) N = letz=Nin M (let-v) let z = V in C[z] = let z = V in C[V] (let-C) (letz=Lin \nM)N = letz=Lin MN (la-Aj lety=(let z= Lin M)in N = letz=Linlety=Min N Figure 2: The calculus ~let. \n3 From Call-by-Name to Call-by-Need We augment the term syntax of the classical A-calculus with a let-construct. \nThe underlying idea is to repre\u00adsent a reference to an actual parameter in an instanti\u00adated function \ngraph by a let-bound identifier. Hence, sharing in a function graph will correspond to nam\u00ading in a term. \nFigure 2 details the calculus ~let over these augmented terms. As in the call-by-name calcu\u00adlus, we extract \nthe related reduction theory, and will fre\u00adquently make reference to its compatible closure a, the reflexive, \ntransitive closure = thereof, and fi\u00adnally the congruence =.eed which it generates, omit\u00adting tags when \nthe meaning is clear. We will also refer to reduction theories of individual axioms in the same manner, \ne.g., M ~ N, and use (for example) the ab\u00ad .-. . breviation M ~et-{v,c,A1 > N to mean that M reduces \nto N by let-V and by either let-V, let-A or let-C. As Wadsworth pointed out, one way to avoid dupli\u00adcating \nreductions is by replacing the abstracted vari\u00adables in function bodies with references to arguments, \nrather than with the arguments themselves [30]. The let-I axiom models the creation of such a let binding \nby 1The idea Of keeping pairs of procedures and their arguments in the syntax of the language is due \nto work on explicit substi\u00adtutions [1], on graph rewriting [3, 5], and on adding state to the ,1-calculus \n[10, 12, 21, 22]. Only the latter two though exploited the idea of modeling the sharing relation of the \nprogram heap inside of the source language. representing the reference with a let-bound name, e.g., (Az.zz)(ll) \n= let z = 11 inzz , where I = Az..z. The intention of this sharing of graphs is to prevent the duplication \nof work by reducing the same expression more than once. However, once a shared expression is a value \n in this core calculus, values are simply abstractions there is no harm in the duplication, In fact, \nthe duplication then becomes necessary; reduction requires an actual abstraction, not just a pointer \nto one. This dereferencing is expressed by the let-V rule, which substitutes the value bound to a name \nfor an occurrence of that name. Unfortunately, such a modeling of the sharing of val\u00adues in the source \nlevel syntax means that looking only for simple values is no longer adequate. A value may it\u00adself cent \nain shared terms, so the simple abstraction may be buried within one or more let bindings. Consider the \nexpression (Af.jI(fI)) ((AZ. AW.ZW) 11) , which let-I equates with let f=letz=ll in Aw. zw in ~1(~1) \n. Further reduction of this term clearly requires that an abstraction be substituted for the first ~ \nin~l(/1). Were we to obtain this abstraction by naively applying some relaxed form of let-V to the term \nas a whole, say letz=(let g=M inV) inC[z] = letz=(lety=Min V) inC [let y= M inV] , we would lose sharing: \nlet f = (let .z = 11 in Aw.zw) in fI(fI) = let f = (let .z = 11 in Aw.zw) (1) in (let z = 11 in Aw.zw) \n1(.fl) . The redex II and the work involved in reducing it have been duplicated. The solution is to re-associate \nthe bindings. Rather than accepting Eq. 1, re-association will allow the cur\u00adrent let-V axiom to apply \nwithout loss of sharing: let f = (let z= 11 in Aw.zw) in fI(fI) = letz= II (2) in let f = Aw.2w in fI(fl) \n= let.z= II in let f = Aw.zw in (Aw.zw)l(fl) The rearrangement of Eq. 2 is captured by the let-A axiom. \nIn the example, even though ~ does occur twice, 11 will be contracted only once5. Equation 1 points out \na second situation where let-V is inadequate in the presence of values under bindings. In the expression \n(let z =11 in Aw.zw) I , we would like to associate the abstraction Aw..zw with the argument 1, but the \ntwo are not directly adj scent as required by let-I. Again, we must rearrange the term, expecting an \nequality (let z =11 in Aw.zw) I = let z = 11 in (Aw.zw) 1 ; 5But note that we avoid duplication only \nof argument evalua\u00adtions. In the program (Af.fI(fI))(Aw. (II)w) , the redex II in the argument will be \nreduced twice. Put differ\u00adently, our calculus captures neither full laziness as described by Wadsworth \n30], nor the sharing required by optimal A-calculus interpreters 13, 15, 16, 17, 20]. However, as observed \nby Arvind, Kathail and Pingali [7], full laziness can always be obtained by extracting the maximal free \nexpressions of a function at compile\u00adtime [25]. 6 Although we rejected the liberalized let-V rule which \npro\u00adduced this particuk expression, it is perfectly reasonable to ex\u00adpect similar terms to appear. In \nfact, any left-nesting of two applications (LM)iV could produce such a term! [ this manipulation is generalized \nby the let-C axiom. The let-A and let-C rules may be viewed as allow\u00ading the scope of a bound identifier \nto be expanded over broader expressions. To avoid copying unreduced ex\u00adpressions, such an expansion is \nrequired when the argu\u00adment of a function contains a binding (the let-A rule) and when a expression which \nis itself a binding is ap\u00adplied to some argument (the let-C rule). It is precisely these two rules let-A \nand let-C which allow us to do without a separate store. Rather than creating references to new, unique \nglobal addresses, we simply extend the scope of identifiers as needed. Example 3.1 (Jz.z z) (Ay.y). let-I \nlet z = Ay.y ~ let z = Ay.y inzz in (Az.z) z 4 let-I let z = Ay.y ~ let z = Ay.y inletz=x in let z = \nAw. w in z in z ~ let z = Ay.y in let 2 = Aw. w in Av. v In the call-by-name calculus, the terms we \nidentified as observable results, or answers, were simply abstrac\u00adtions. However, in this formulation \nof call-by-need the notion of answer must reflect the possibility that such an abstraction can be under \nbindings. The appropriate definition given in Figure 2 means that answers are a syntactic representation \nof closures. 4 Basic Syntactic Properties Following Plotkin s technique, we define the call-by\u00adneed evaluator \nas a partial relation evalneed from pro\u00adgrams, or closed terms, to the singleton consisting of the tag \nclosure based on equality in our call-by-need calculus: Furthermore, from the following result, a program \neval\u00aduates to closure if and only if it reduces to an answer in the call-by-need calculus. Theorem 4.1 \nAlet is confluent: M  % % J/f, ~,, Let.&#38; >N# Proofi The system consisting of just let-I is triv\u00adially \nconfluent; then by marked and weighted redexes as in Barendregt [8] the remaining reductions let-V, let-C \nand let-A are both weakly Church-Rosser and strongly normalizing, and thus Church-Rosser as well. Since \nboth subsystems commute, the theorem follows from the Lemma of Hindley and Rosen [8, Proposition 3.3.5]. \n = This result does not imply that all reduction sequences lead to an answer in Alet. We therefore introduce \nthe notion of standard reduction, which always reaches an answer if there is one. Figure 3 details our \nnotion of standard reduction. An expression is only reduced when it appears in the hole of an evaluation \ncontext. The first two productions for evaluation contexts are those of the call-by-name calculus. Since \narguments to procedures are evaluated only when needed, the standard reduc\u00adtion system postpones the \nevaluation of the argument and instead proceeds with the evaluation of the proce\u00addure body. Once the \nformal parameter of the procedure appears in the hole of an evaluation context, and only then, the value \nof the argument is needed to complete the evaluation. In this case, the standard reduction sys\u00adtem evaluates \nthe argument before substituting it into the body of the procedure. Given a program M c Alet, M standard \nreduces to N, written asM = N, iff M s E[K] and N s E[L], where K and L are a standard redex and its \ncontractum, respectively. M -N means M and N are related via the transitive-reflexive closure of =. As \nusual, we will omit the tags from the arrows when the meaning is clear from the context. Verifying that \nthe standard relation, -, is indeed a function from programs to programs relies on the usual Unique Evaluation \nContext Lemma [11]. This lemma states that there is a unique partitioning of a non-answer into an evaluation \ncontext and a redex, which implies that there is precisely one way to make progress in the evaluation. \nLemma 4.2 Gwen a program M 6 Alet, ezther M zs an answer, or there exists a unique evaluation context \nE and a standard redex N such that M = E[N]. Proofi By structural induction on M. = Theorem 4.3 Given \na program M e Alet, eva/need(M) = closure iff 3A, M H A . Proof: The proof relies on two subsidiary results: \nthat all answers are in --normal form, and that a se\u00adquence of non-standard reductions followed by a \nse\u00adquence of standard reductions can be transformed into an equivalent sequence of first standard reductions, \nthen non-standard reductions. The proof is somewhat rem\u00adiniscent of Barendregt s standardization proof \nfor the call-by-name calculus, but without the convenience of a finiteness of developments theorem for \nall redex types [8, Lemma 11.4.3]. 1 5 Completeness of the Call-by-Need calcu\u00adlus Replacing the call-by-name \ninterpreter with the call-by\u00adneed interpreter requires an equivalence proof for the two evaluators: !5Zdneed \n= e dname . In this section, we will show the completeness direction, that is, each result obtained with \nthe call-by-name inter\u00adpreter is also produced by the call-by-need interpreter. Formally, we have for \nany program M, eval~a~~(M) = closure ~ eva/need(M) = closure . We introduce an ordering between terms \nof A and Alet. Intuitively, M < N if M can be obtained by un\u00adwinding N, that is, N contains more sharing \nthan M [3]. In other words, the ordering relation < expresses whether the terms have homomorphic graphs. \nFor ex\u00adample, the tree of M a (Ax. z)(kz.z) can be homomor\u00adphically embedded into the graph of N ~ let \ny = kr.z in yy,   to 4 -.. f+ which shows that M < N. Notation: given a term M ~ Alet, let Dag(M) \nbe the corresponding dag, and given a term N e A, let Tree(N) be the corresponding tree. Definition 5.1 \nFor M e A and N ~ Alet, M < N iff there exists an homomorphism u : Tree(M) -+ Dag(IV). If M < N, then \neach call-by-name evaluation of M can be simulated in the call-by-need calculus by evalu\u00adating N. The \nterm obtained in the call-by-need calculus is not necessarily greater (in terms of the above order\u00ading) \nthan the one obtained following the call-by-name evaluation. This is because a one-step call-by-need \nre\u00adduction may correspond to multiple call-by-name re\u00adductions. For example, the evaluation Syntactic \nDomains: Values: v ::= AX.M Answers: A ::= Vlletz=M inA Evaluation Contexts: E ::= []1 EMlletz= Min \nElletz=Ein E[~] Standard Reduction rules: (1+1) (Ax.A4)N +let%=IVin A4 (let,-V) letx=VinE[z] --+letz=VinE[V] \n(let, -C) (let z = M in A)N +letz=M inAN (let,-A) letz=(let y= Min A)in E[z]+lety =Minletz=Ain E[z] \nFigure 3: Standard call-by-need reduction. corresponds to the following call-by-need evaluation: Example \n5.2 (i) The term let z = Ax.% in .z is drawn as M ~ (h. ZZ)((~y.$!)(h.-Z)) ~ w Obviously that Ml let \nz = (Ay.y)(Az.z) in zz let~=(let y= Az..ziny)inxz~N Ml ~ N. However, there exists = Mz and Mz < N: an \nMZ . such Bz ,,, :, ,, ,.,.,,,, ,,, 2. + . . ., ... ~ .; :../ The name z associated with the root pointer \nis Ml x M2 s (k. Z)(h.Z) < N . drawn outside the shaded area. (ii) We can also have nested boxes, e,g., \nthe term Since M < M, the last point shows how to reconstruct a call-by-name evaluation in the call-by-need \ncalculus. let z = let y = let w = kr.z Hence, the completeness direction follows. in w If M < N and M \nis a ~-redex, N is not necessarily in y a let~-I redex. Suppose in z M = (XE.z)(h.z) is drawn as I N \ns let z= (let w= Az.% inw) in.ZZ l-_l.L_\u00ad then M < N, yet N does not contain a let,-I redex. The example \npoints out that our graph model of Alet terms must be able to express the let-structure of a term in \naddition to its sharing structure if we want to use it for a correctness proof. We solve this problem \nby enriching dags with boxes and labeled edges [6]. Dagn (M) is the decorated dag as\u00adsociated with an \nexpression M. A box can be thought of as a refined version of a node; the label associated with where \nthe path to the ~-node must follow the label an edge is just a sequence of let-bound variable names. \n.zyw and hit three walls. The label can be thought of as a direction to be followed In contrast, in \nthe term in order to get to a particular node. Each let induces one box, and each edge to the shared \nterm is deco-let 2 = Ax.x rated with the variable name. We pictorially represent inlety=z a term let \nx = N in M by a box divided in two parts: inletw=y the upper part corresponds to M (the unshaded area \nof inw, a box) and the lower part contains N (the shaded area drawn asof a box). Let us illustrate the \nextended dag notation and terminology with a number of examples. 1 w Y, z . ., ,ti : , ,,, ,, . . :. \nT x the path to the J-node must follow the label wy,z, and it penetrates three walls but leaves two \nencas\u00ad ings. In our runnh~ (?Xampk, ~af/D (~) k : In this decorated dag, the path from the application \n(root) node to the A-node has label .zw, and it pene\u00adtrates the wall of one box. To expose the redex \nmeans that the function pointer (of the root node) must point to the ~-node directly. In terms of our \ngraphical lan\u00adguage, we must eliminate the names Z, w, the internal box, and pull the A-node out of the \nshaded area, exactly the task of the let$-A, let, -V, and let,-C rules. Their dag-based representation \nin Table 1 reveals that let,-V pulls a value out of the shaded area, eliminating a name on an edge; let3-C \nmoves a wall; and lets-A moves a wall that is in the shaded area. The sequence let.-V, lets-A, let,-V \nsuffices to expose the redex in our example: let z= (let w= Az.z inw) in.2.2 let .2 = (let w = Ax.z in \nAx.%) in .22 let,-V let w = Az.x in (let z= Az.z in zz) let. -A> let w = Az.z in (let z = Az.z in (Az.x)z) \n. let,-V Figure 4 (without unreachable dags) illustrates these steps. From Table 1 it is clear that \nlet,-C and let,-A do not change the dag associated with a term, while letS-V causes a duplication. Lemma \n5.3 (z) Given M c Alet, # iV then ~ M let. -{C,A} Dug(M) = Dag(N).  (ii) Given M ~ Alet, if M ~et.-v> \nN then N <M. The language and notation for decorated dags is use\u00adful in proving the following three key \nlemmas. All could be formulated in plain term-based language, but at the cost of introducing more technical \ndetails. Lemma 5.4 Given M cA,N cAlet, if M < N and M s E~[(&#38;r.P)R] then there exists P , R j and \nE[ ], such that N I * E[(Xr.P )R ]. Iet,-{V,C,A} Proofi Let z be the root in Tree(M) of the @-redex \nbeing evaluated. Let Z and .z~ be the corresponding nodes in Dug(N) and Daga (N), respectively. We know \nthat the left branch of z points to a A-node, while in Dagn(N) the path from z~ to the ~-node may contain \nsome obstacles. Thus, we show that by using let$-A, let.-V, and let~-C we can remove all the obstacles \nfrom that path. We reason by induction on the number n of names associated with the path in Dagn (N) \nfrom z~ to the A-node. 0. This means that the path from z~ to the }-node is free of names, but it still \nmay penetrate inter\u00advening walls. With m walls, we need m lets-C steps to move the walls and expose the \nredex. O. By the induction hypothesis, we can remove n-1 names. Now we need to show how to eliminate \nthe last one. There are two cases: 1. The name associated with the J-node is w: Since w occurs in head \nposition, an applica\u00adtion of let,-V exposes the A-node; 2. The branch labeled w points to m boxes that \nsurround the A-node, e.g. w I ,,, Since w occurs in head position, m applica\u00adtions of let, -A followed \nby a single application of let. -V expose the A-node. let~-V : m+m let,-C : let~-A : 1/ x _,,a ,:.. \nYY Table 1: Standard call-by-need reduction rules in dag-based form. +-Zz z { x Q Figure 4: Exposing \nthe redex in let z = (let w = ~z.z in w) in ZZ. Let N , N , * N ; under the assumption let, -{ V,C,A} \nthat N = C[(AZ. P )R ] for C[] not an evaluation con\u00adtext (that is, the redex (XE.P )R is not needed) \nand hence : not the root of the leftmost-outermost redex in M, we have a contradiction. = Lemma 5.5 Given \nM GA, N e A1et, if M < N and M = Ax.P then there extsts an answer A such that N ~A. Iet,-{V,A} Proofi \nThe proof is similar to but simpler than the previous one. In fact, we need not move the walls sur\u00adrounding \nthe lambda-node; that is, no use of let~-C is required. = Lemma 5.6 Gtven a program M e A and N ~ Alet, \nzf M< Nand M - Ml, then 3M~ eA,N1 ~A1et such that Ml=M[ ,N <n N1 andM; <N1. need Pictorially: ++ M Ml \n,.-~ 3M[ + < :< . 1 <n N....@tiltil . Proofi By induction on the length n of the reduction M -n Ml. n \n= 1. Let J/l ~ En[(}Z. P) R], and let z be the root in Tree(M) of the /3-redex in the hole of En [ ]. \nFrom Lemma 5.4, there exists P , R and E[ ]: N -N and N ~ E[(Ax.P )R ] . Let Z be the root in Dug(N ) \nof the let, -I-redex. From Lemma 5.3 and the fact that M does not contain any sharing we have M < N . \nThus: M m Ml = En[P[z := R]] and N ~et-1>N1 = E[let ~ = R in P ] s If there exists a node ZI in Tree(M), \nwhere ZI # z, such that U(Z1) = z , where u is the homomor\u00adphism associated with the ordering M < N , \nthen Ml $ N1. Let Y be the set of all such nodes. Let M;, Ml GM{, by reducing all redexes in ~ and their \nresiduals. We have: M; < NI. n> 1.LetM- -1 M andM G Ml. By the induction hypothesis, 3NI ~ A1et, M ~ \nA, <(TI-l) AT1, M = N M and M <N1 . need From the Strip Lemma [8] 3M2, If M2 ~ M then we have: <(. 1) \nN1 and &#38;f D M,, N+ + need where M < N1. Otherwise, by the induction hypothesis 3N( c Alet, M; ~ A, \n= Lemma 5.7 Given a program M ~ A, M WnAx.N~M-A. Proofi Since M < M, from Lemma 5.6, 3M1 c Alet, N1 \n~ A such that: where Ax .N1 < Ml. The result then follows from Lemma 5.5. 1 With these lemmas we can \nprove the main result of this section, namely, that call-by-need can simulate a call\u00ad by-name evaluation. \nTheorem 5.8 If M e A and evalname(M) = closure, then &#38;Va/need(M) = closure. Proofi The assumption \nimplies M m ~x.N. Hence, the result follows from Lemma 5.7. = 6 A Let-Less Formulation of Call-by-Need \nIn the Alet calculus, we have treated the expression let x = M in N as a term distinct from OX.N) M. \nAn alternate treatment is also quite reasonable: that the former is merely syntactic sugar for the latter, \nIn other words, it is possible to completely eliminate let s from the call-by-need calculus and still \nhave a system with the same desired properties. By expanding let-bindings into applications, we can derive \nthe &#38; calculus shown in Figure 5 from Alet. There is of course no analogue of the let-I rule in &#38;, \nsince we must no longer convert away from plain applications. We call the evaluator for this language \neval~eed to distinguish it from the evaluator for +et . While Al is perhaps somewhat less intuitive than \n}le., its simpler syntax can make some of the basic Syntactic Domains Variables ~lY1~ Values v, w ::= \nJx.M Answers A ::= 1 / (JZ.A)M Terms L,M, N ::= .r\\VIMN Evaluation contexts E ::= [] [ EM I (kr.M)E I \n(Az.E[z])E Reduction Axioms (e-v) (lz.c[z]) v = (k.c[v]) v (e-c) (kc. L)AfN = (k. Lfv)M (LA) (k. L)((Ay.AJ)N) \n= (Ay.(kz.L)M)N Figure 5: Let-less call-by-need. (syntactic) results easier to derive. It also allows \nbet\u00adter comparison with the call-by-name calculus, since no additional syntactic constructs are introduced. \nClearly, ~let and Al are closely related. More pre\u00adcisely, the following theorem states that reduction \nin Alet can be simulated in Al, and that the converse is also true, provided we identify terms that are \nequal up to let-I introduction. Proposition 6.1 For all M c Al, M = Alet, -----+N ibf..w+N At. At. let-I \n~let-I let-I ~let-I II Ml ...... ..wj) ~;l . M let let Proposition 6.1 can be used to derive the essential \nsyn\u00adtactic properties of JL from those of Alet; in particular the confluence result for Jl follows from \nTheorem 4.1 by the proposition. JL has close relations to both the call-by-value cal\u00adculus ~v and the \ncall-by-name calculus ~. Its notion of equality =~1 t. e., the least equivalence relation gen\u00aderated \nby the reduction relation fits between those of the other two calculi, making Al an extension of ~v \nand } an extension of &#38;. Theorem 6.2 =~v C =x, C =~ Proof: (1) /3V can be expressed by a sequence \nof &#38; reductions as was shown at the beginning of this section. Therefore, =~v C =~1. (2) Each AL \nreduction rule is an equality in ~. For instance, in the case of ~-v one has: (Az.c[x]) v =6 [v/z] \n(c[g]) E [v/z] (c [v]) =@ (k.c[v]) v The other rules have equally simple translations, and so we have \n=~1 c =A. = Each of the inclusions of Theorem 6.2 is proper, e.g., (kr.z) (( Ay.y) L?) = (Ay.(lz.$) Q) \nQ where fl stands for a non-terminating computation, is an instance of rule ?-A, but it is not an equality \nin the call-by-value calculus (LI stands for a non-terminating computation). On the other hand, the following \nin\u00adstance of /3 is not an equality in AL: (Az.z)Q= Q However, one can show that the observational equiv\u00adalence \ntheories of Al and J are identical (and are in\u00adcompatible with the observational equivalence theory of \nAv): Theorem 6.3 For all programs M ~ A, eualname(M) = eval~ee,(M) Proof: FO11OWSfrom Theorem 5.8 and \n6.2. = Theorem 6.2 implies that any model of call-by-name A-calculus is also a model of At, since it \nvalidates all equalities in Al. Theorem 6.3 implies that any ade\u00adquate (respectively, fully-abstract) \nmodel of A is also adequate (fully-abstract) for &#38;, since the observational equivalence theories \nof both calculi are the same7: Corollary 6.4 For all terms A{, N e A, M Ename N iffM~need N . 7 Extensions \nMost lazy functional languages extend the pure calculus in several ways. In this section we consider \ntwo such extensions, for constructors and for recursion. ?For instance, Abramsky and Ong s model of the \nlazy lambda calculus [2] is adequate for A p. 7.1 Constructors and Primitive Operators Figure 6 extends \nAlet with data constructors kn of ar\u00adbitrary arity n and primitive operators p (of which se\u00adlectors are \na special case). There is one new form of value: W VI . . . Vn where the components VI through V. must \nbe values otherwise sharing would be lost when copying the compound value [14]. For instance, inl (1 \n+ 1) is not a legal value, since copying it would also copy the unevaluated term (1 + 1). Instead, one \nwrit es Ietz=l+lininlz. There are two new reduction rules. Rule 6-V is the usual rewrite rule for primitive \noperator application. It is defined in terms of a partial function also called 6 from operators and \nvalues to terms. This function can be arbitrary, as long as it does not look inside lambda abstractions. \nThat is, we postulate that for all operators p and contexts C there is a context D such that for all \nterms M, 6(p, C [~Z. M]) = D[Az. M] or cf(p, C[AZ .M]) is undefined. Note that rule 6-V makes all primitive \noperators unary and strict. Operators of more than one argument can still be simulated by curry\u00ading. \nRule 6-A allows let-bindings of operator arguments to be pulled out of applications of primitive operators. \nAlternatively, one could phrase these constructs in terms of constructors and case statements in reduction \nrules.  7.2 Recursion A deficiency of our treatment of call-by-need is its treat\u00adment of recursive or \ncyclic values. Traditionally one re\u00adlies on the Y combinator for recursion. In the absence of data constructors, \nthis solution is fine. However, once data constructors are included, the sharing in the source language \nno longer reflects the sharing in the evaluator. For example, the term M -Y(Ay.cons(l, y)) evaluates \nto a term containing two distinct cons cells even though an actual implementation would allocate only \none cell, representing M as a cyclic structure. To cope with recursion, we extend the call-by-need calculus \nwith a ietrec construct, where no ordering among the bindings is assumed. This extended calculus is given \nin Figure 7. Unlike the calculus Alet of Sec\u00adtion 3, we now have a restricted notion of substitution. \nIn other words, substitutions only occur when a variable appears in the hole of an evaluation context. \nOtherwise, an unrestricted notion of substitution in the presence of cycles would cause interesting non-confluence \nphenom\u00adena [5]. (0) LJ(V) Xz.N App (V, z w M) [z+/*]N J (T) V (@) LM$(T)V Figure 8: Operational semantics. \nThis extended call-by-need calculus corresponds to Ariola and Klop s call-by-name calculus with cycles \n[5], in the same way that our call-by-need calculus corre\u00adsponds to the call-by-name calculus, The correctness \nproof of the calculus with recursion can be obtained by showing its soundness and completeness with respect \nto a calculus of infinit ary graphs. 8 Relation to Natural Semantics This section presents an operational \nsemantics for call\u00adby-need in the natural semantics style of Plotkin and Kahn, similar to one given by \nLaunchbury [18]. We state a proposition that relates the natural semantics to standard reduction. A heap \nabstracts the state of the store at a point in the computation. It consists of a sequence of pairs binding \nvariables to terms, Xl++ M1, . . .. XnHMn . The order of the sequence of bindings is significant: all \nfree variables of a term must be bound to the left of its Furthermore, all variables bound by the heap \nmust be distinct. Thus the heap above is well-formed if fv(Mi) ~ {%1, ..., xi_ 1} for each i in the range \n1< i < n, and all the Zi are distinct. Let 0, Q, T range over heaps. If @ is the heap xl + Ml, .... Zm \n++ Mn, define vars(@) = {xl,..., ~n }. A configuration pairs a heap with a term, where the free variables \nof the term are bound by the heap. Thus (0) M is well-formed if@ is well-formed and fv(M) ~ vars(@). \nThe operation of evaluation takes configurations into configurations. The term of the final configuration \nis always a value. Thus evaluation judgments take the form (0) M $ (W) V. The rules defining evaluation \nare given in Figure 8. There are three rules, for identifiers, abstractions and applications. 8 So this \nmodel of the heap is incompatible with the extension for recursion given in Section 7.2; see the end \nof this discussion. Syntactic Domains Operators Constructors 1 (of arity n)  Values v, w ::= z Ik. \nitf 1P VI ...vn (n> O) Terms L,M, N ::= VI MNlletx=Min N[p Additional Axioms (6-V) p v = 6(JI, v) (ti(~, \nV) defined) (6-A) p(letx=Min N) = letz=&#38;finp N Figure 6: Data constructors and primitive operations. \nSyntactic Domains  Values v ::= z I AX.A4 Terms M, N ::= z I V I MN I (Mlzl=Nl, .,xn=Nn) Evaluation \ncontexts E ::= [] I EM I @ ID) I (E[x] I D[z,4,zn = E,D) D[z, %] ::= z = E[zl], zl = E[xz], . . ..q3_1 \n= E[zn] Axioms (~x.M)N =( Mlx=N) (lift) ((V I D))N = (VN I D) (deref) (E[z] I z = V,D) = (E[V] I z = \nV, D) (deref;) (E[z] I D[z, %], ~n = W q = (E[x] I D[x, V], $n = V, D) (assoc) ((V I D,) I Dz) = (v\\ \nD,, D,) (assoc~) (E[~] I D[z,zn],~n = (V I D), D~) = (E[z] \\ D[z, zn], zn = V, D,D1) (@need) Figure \n7: Recursion. . Abstractions are trivial. As abstractions are al-may be added (by App). The free variables \nof V ready values, the heap is left unchanged and the are bound by ~, so to ensure the heap stays well\u00adabstraction \nis returned. formed, the final heap has the form V, x t+ V, Y . e Applications are straightforward. Evaluate \nthe As one would expect, evaluation uses only well\u00adfunction to yield a lambda abstraction, extend the \nformed configurations, and evaluation only extends the heap so that the the bound variable of the ab-heap. \nstraction is bound to the argument, then evaluate Lemma 8.1 Gwen an evaluation tree with root config\u00adthe \nbody of the abstraction. In this rule, z is a uration (0) M .lJ. (W) V, zf (@) M is weil-formed then \nnew name not appearing in W or N. The renam\u00adevery configuration zn the tree is well-formed, and fur\u00ading \nguarantees that each identifier in the heap is thermore vars(@) ~ vars(~). unique. Thanks to the care \ntaken to preserve the ordering Variables are more subtle. The basic idea is of heaps, it is possible \nto draw a close correspondencestraightforward: find the term bound to the vari\u00adbetween evaluation and \nstandard reductions. If@ is the able in the heap, evaluate the term, then update heap xl ~Ml, . . ..zn~Mn \n.write IetiOin Nfor the the heap to bind the variable to the resulting value. term Iet.rl = Mlin ... \nlet Zn = M. in N. Every answerBut some care is required to ensure that the heap A can be written let \nV in V for some heap ~ and valueremains well-formed. The original heap is parti- V. Then a simple induction \non &#38;derivations yields thetioned into @, z H M, T. Since the heap is well\u00adfollowing result. formed, \nonly @ is required to evaluate M. Evalu\u00adation yields a new heap V and value V. The new  Proposition \n8.2 (@) M u (V) V zff heap V will differ from the old heap @ in two ways: binding may be updated (by \nVar) and bindings Al t-let@in M-let Vin V . 244 The semantics given here is similar to that presented \nby Launchbury [18]. An advantage of our semantics over Launchbury s is that the form of terms is stan\u00addard, \nand care is taken to preserve ordering in the heap, Launchbury uses a non-standard syntax, in order to \nachieve a closer correspondence between terms and eval\u00aduations: in an application the argument to a term \nmust be a variable, and all bound variables must be uniquely named. Here, general application is supported \ndirectly and all renaming occurs as part of the application rule. It is interesting to note that Launchbury \npresents an alternative formulation quite similar to ours, buried in one of his proofs. One advantage \nof Launchbury s semantics over ours is that his copes more neatly with recursion, by the use of multiple, \nrecursive let bindings. In particular, our heap structure is incompatible with the extension for recursion \nof Section 7.2. This extension would al\u00ad ter both the ordering property and the connection to standard \nreduction. Applications Call-by-need calculi have a number of potential applica\u00adtions. Their primary \npurpose is as a reasoning tool for the implementation of lazy languages. We sketch three ideas. Call-by-need \nand assignment Gall-by-need can be implemented using assignments. Crank [9, 10] briefly discusses a rewriting \nsemantics of call-by-need based on Felleisen and Hieb s A-calculus with assignments [12]. We believe \nthat a call-by-need calculus is the correct basis for proving this implementa\u00adtion technique correct \nwith a simple simulation theorem for the respective standard reductions. Call-by-need and cps conversion \nOkasaki et. al. [24] recently suggested a continuation\u00adpassing transformation for call-by-need languages. \nIn principle, this transformation should satisfy the same theorems as the continuation-passing transformation \nfor call-by-name and call-by-value calculi [26]. Plotkin s proof techniques should immediately apply. \nSince this transformation appears to be used in several implemen\u00adtations of lazy languages, it is important \nto explore its properties with standard tools. Garbage collection Modeling the sharing relationship of \nan evaluator s memory in the source syntax suggests that the calcu\u00ad lus can also model garbage collection, \nIndeed, garbage collection can be easily expressed in our call-by-need calculus by adapting the garbage \ncollection rule for ref\u00aderence cells of Felleisen and Hieb [10, 12]: letz=Min N=N if z # FV(N) We expect \nthat the work on garbage collection in func\u00adtional languages by Morrisett et. al. [23] will apply to \ncall-by-need languages. Such a rigorous treatment of garbage collection would strengthen the calculus \nand its utility for reasoning about the implementations of lazy languages. 10 Conclusion The calculus \nwe have presented here has several nice properties which make it suitable as a reasoning tool for lazy \nfunctional programs. With operations on the lambda-terms themselves (or perhaps a mildly sugared version) \nrather than on a separate store of bindings, and with a small set of straightforward rules, we feel that \nour approach is clearer and simpler than previous approaches. The unsugared calculus fits naturally be\u00adtween \nthe call-by-name and call-by-value versions of J. Acknowledgements. The authors would like to thank H, \nBarendregt, J. Field, J.w. Klop and D. N. Turner for numerous helpful discussions. References [1] M. \nAbadi, L. Cardelli, P.-L. Curien, and J .-J. L&#38;y. Explicit substitutions. Journal of Functional Pro\u00adgramming, \n4(l), 1991. [2] S. Abramsky. The lazy lambda calculus. In D. Turner, editor, Declarative Programming. \nAddison-Wesley, 1990. [3] Z. M. Ariola and Arvind, Properties of a first\u00adorder functional language with \nsharing. Theoretical Computer Science, September 1995. [4] Z. M. Ariola and M. Felleisen. The call-by-need \nlambda calculus. Technical Report CIS-TR-94\u00ad23, Department of computer science, University of Oregon, \nOctober 1994. [5] Z. M. Ariola and J. W. Klop. Cyclic lambda graph rewriting. In Proc. of the Eighth \nIEEE Symposium on Logic in Computer Science, Paris, 1994. [6] Z. M. Ariola and J. W. Klop. Equational \nterm graph rewriting. Acts Informataca, 1994. [7] Arvind, V. Kathail, and K. Pingali. Sharing of computation \nin functional language implementa\u00adtions. In Proc. International Workshop on High-Level Computer Architecture, \n1984. [8] H. P. Barendregt. The Lambda Calculus: Its Syn\u00adta~ and Semantics. North-Holland, Amsterdam, \n1984. [9] E. Crank. Parameter-passing and the lambda cal\u00adculus. Master s thesis, Rice University, 1990. \n [10] E. Crank and M. Felleisen. Parameter-passing and the lambda calculus. In Proc. ACM Conference on \nPrinciples of Programming Languages, 1990. [11] M. Felleisen and D. P. Friedman. Control opera\u00adtors, \nthe SECD-machine, and the lambda-calculus. In 3rd Working Conference on the Formal Descrip\u00adtion of Programming \nConcepts, Ebberup, Den\u00admark, August 1986. [12] M. Felleisen and R. Hieb. The revised report on the syntactic \ntheories of sequential control and state. Theoretical Computer Science, 102, 1992. [13] J. Field. On \nlaziness and optimality in lambda in\u00adterpreters: Tools for specification and analysis. In Proc. ACM Conference \non Principles of Program\u00adming Languages, San Francisco, 1990. [14] D. P. Friedman and D. S. Wise. Cons \nshould not evaluate its arguments. In Proc. International Conference on Automata, Languages and Program\u00adming, \n1976. [15] G. Gonthier, M. Abadi, and J.-J L&#38;y. The ge\u00adometry of optimal lambda reduction. In Proc. \nACM Conference on Principles of Programming Languages, 1992. [16] V. K. Kathail. Optimal Interpreters \nfor Lambda\u00adcalculus Based Funtional Languages. PhD thesis, Dept. of Electrical Engineering and Computer \nSci\u00adence, MIT, 1990. [17] J. Lamping. An algorithm for optimal lambda calculus reduction. In Proc. ACM \nConference on Principles of Programming Languages, San Fran\u00adcisco, January 1990. [18] J. Launchbury. \nA natural semantics for lazy eval\u00aduation. In Proc. ACM Conference on Principles of Programming Languages, \n1993. [19] J. Maraist, M. Odersky, and P. Wadler. The call\u00adby-need lambda calculus (unabridged). Technical \nReport 28/94, Universitat Karlsruhe, Fakultat fur Informatik, October 1994. [20] L. Maranget. Optimal \nderivations in weak lambda\u00adcalculi and in orthogonal term rewriting systems. In Proc. ACM Conference \non Principles of Pro\u00ad gramming Languages, Orlando, Florida, January 1991. [21] 1. A. Mason and C. Talcott. \nReasoning about programs with effects. In Proc. of Programming Language Implementation and Logic Programming, \nSpringer-Verlag LNCS 456, Berlin, 1990. [22] I. A. Mason and C. L. Talcott. Equivalence in func\u00adtional \nlanguages with effects. Journal of Functional Programming, 1(2), 1991, [23] G. Morrisett, M. Felleisen, \nand R. Harper. Mod\u00adeling memory management. Technical report, De\u00adpartment of computer science, Carnegie \nMellon University, forthcoming 1994. [24] C. Okasaki, P. Lee, and T. Tarditi. Call-by-need and continuation-passing \nstyle. In Lisp and Sym\u00adbolic Computation, 1994. [25] S. L. Peyton Jones. A fully-lazy lambda lifter \nin haskell. Soflware Practice and Experience, 21, 1991. [26] G. D. Plotkin. Call-by-name, call-by-value \nand the lambda calculus. Theoretical Computer Science, 1, 1975. [27] Purushothaman and J. Seaman. An \nadequate oper\u00adational semantics of sharing in lazy evaluation. In Proc. dth European Sympostum on Programming, \nSpringer Verlag LNCS 582, 1992. [28] K. H. Rose. Explicit cyclic substitutions. In 3rd International \nWorkshop on Conditional Term Rewriting Systems, July 1992. [29] J. M. Seaman. An Operaiionai Semantics \nof Lazy Evaluation for Analysis. PhD thesis, The Pennsyl\u00advania State University, 1993. [30] C. Wadsworth. \nSemantics And Pragmatic Of The Lambda-Calculus. PhD thesis, University of Ox\u00adford, September 1971. [31] \nN. Yoshida. Optimal reduction in weak-A-calculus with shared environments. In Proc, ACM Confer\u00adence on \nFunctional Programming Languages and Computer Architecture, Copenhagen, 1993.  \n\t\t\t", "proc_id": "199448", "abstract": "<p>The mismatch between the operational semantics of the lambda calculus and the actual behavior of implementations is a major obstacle for compiler writers. They cannot explain the behavior of their evaluator in terms of source level syntax, and they cannot easily compare distinct implementations of different lazy strategies. In this paper we derive an equational characterization of call-by-need and prove it correct with respect to the original lambda calculus. The theory is a strictly smaller theory than the lambda calculus. Immediate applications of the theory concern the correctness proofs of a number of implementation strategies, <italic>e.g.</italic>, the call-by-need continuation passing transformation and the realization of sharing via assignments.</p>", "authors": [{"name": "Zena M. Ariola", "author_profile_id": "81100218587", "affiliation": "Computer & Information Science Department, University of Oregon, Eugene, Oregon", "person_id": "P309510", "email_address": "", "orcid_id": ""}, {"name": "John Maraist", "author_profile_id": "81100024857", "affiliation": "Institut f&#252;r Programmstrukturen, Universit&#228;t Karlsruhe, Karlsruhe, Germany", "person_id": "P144952", "email_address": "", "orcid_id": ""}, {"name": "Martin Odersky", "author_profile_id": "81100056476", "affiliation": "Institut f&#252;r Programmstrukturen, Universit&#228;t Karlsruhe, Karlsruhe, Germany", "person_id": "PP14030830", "email_address": "", "orcid_id": ""}, {"name": "Matthias Felleisen", "author_profile_id": "81100323458", "affiliation": "Department of Computer Science, Rice University, Houston, Texas", "person_id": "PP39037684", "email_address": "", "orcid_id": ""}, {"name": "Philip Wadler", "author_profile_id": "81100173596", "affiliation": "Department of Computing Science, University of Glasgow, Glasgow, Scotland", "person_id": "PP39030941", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/199448.199507", "year": "1995", "article_id": "199507", "conference": "POPL", "title": "A call-by-need lambda calculus", "url": "http://dl.acm.org/citation.cfm?id=199507"}