{"article_publication_date": "01-25-1995", "fulltext": "\n Structuring Depth-First Search Algorithms in Haskell David J. King John Lau$chbury Department of Computing \nScience Department of Computer $cience and Engineering University of Glasgow Oregon Graduate Institute \nof Science &#38; Technology Glasgow G12 8QQ, UK gnik@dcs. gla. ac, uk Abstract Depth-first search is \nthe gorithms. In this paper lazy functional language, tation. Unlike traditional the structuring methods \nalgorithms from individual of algorithm construction formal proof, which we style proof of a far from \nnents algorithm. Introduction key to a wide variety of graph al\u00ad we express depth-first search in a obtaining \na linear-time implemen\u00ad imperative presentations, we use of functional languages to construct reusable \ncomponents. This style turns out to be quite amenable to exemplify through a calculational\u00adobvious strongly-connected \ncompo- The importance of depth-first search (DFS) for graph al\u00ad gorithms was established twenty years \nago by Tarjan (1972) and Hop croft and Tarjan (1973) in their seminal work. They demonstrated how depth-first \nsearch could be used to con\u00ad struct a variety of efficient graph algorithms. In practice, this is done \nby embedding code-fragments necessary for a particular algorithm into a DFS procedure skeleton in order \nto compute relevant information while the search proceeds. While this is quite elegant it has a number \nof drawbacks. Firstly, the DFS code becomes intertwined with the code for the particular algorithm, resulting \nin monolithic programs. The code is not built by re-use, and there is no separation between logically \ndistinct phases. Secondly, in order to rea\u00ad son about such DFS algorithms we have to reason about a dynamic \nproces-what happens and when and such rea\u00ad soning is complex. Occasionally, the depth-first forest is \nintroduced in order to provide a static value to aid reasoning. We build on this idea. If having an explicit \ndepth-first forest is good for rea\u00adsoning then, so long as the overheads are not unacceptable, it is \ngood for programming. In this paper, we present a wide variety of DFS algorithms as combinations of standard \ncomponents, passing explicit intermediate values from one to the other. The result is quite different \nfrom traditional presentations of these algorithms, and we obtain a greater degree of modularity than \nis usually seen. Permission to copy without fee all or part of this material is granted provided that \nthe copies are not made or distributed for direct commercial advantage, the ACM copyright notice and \nthe title of the publication and its date appear, and notice is given that copying is by permission of \nthe Association of Computing Machinery. To copy othetwise, or to republish, requires a fee andlor spade \npermission. P(3PL 951/95 San Francisco CA USA @ 1995 ACM 0-89791 -892-1/95/0001 ....$8.W Beaverton, Oregon \n~7006-1999, USA jl@cse. o~i. edu Of course, the idea of splitting algorithms into many separate phases \ncodnected by intermediate data structures is not new. To so~e extent it occurs in all programming paradigms, \nand is e~pecially common in functional languages. What is new, howeker, is applying the idea to graph \nalgo\u00adrithms. The difficul~y is always to find a sufficiently flexible intermediate value ~hich allows \na wide variety of algorithms to be expressed in ~erms of it. There is anothe challenge here, however. \nGraph algo\u00ad t rithms have long been poorly handled in functional lan\u00adguages. It has not been at all clear \nhow to express such algorithms without using side effects to achieve efficiency, and lazy languages by \ntheir nature have to prohibit side\u00adeffects. So, for ex mple, many texts provide implementa\u00ad 1 tions of \nsearch algo~ithms which are quadratic in the size of the graph (see Paulson (1991), Holyer (1991), or \nHarrison (1993)), compared ~ith the standard linear implementations given for imperativ e languages (see \nManber (1989), or Cor\u00ad man et al. (1990)). In our work the re is one place where we do destructive update \nin order to gain the same (within a constant factor) as imperative graph We make use of rece nt advances \nin lazy functional which use monads to provide updatable state, need to use complexity algorithms. languages \nas imple\u00ad mented within the Glasgow Haskell compiler. The compiler provides extensions to the language \nHaskell (Hudak et al. 1992) providing up i~atable arrays (Launchbury and Peyton Jones 1994), and allows \nthese state-based actions to be en\u00adcapsulated so that their external behaviour is purely func\u00adtional. \nConsequent ly we obtain linear algorithms and yet retain the ability to perform purely functional reasoning \non all but one fixed an~ reusable component. Most of the methods in this paper apply equally to strict \nand lazy languages. The exception is in the case when DFS is being used for a true searth rather than \nfor a complete traversal of the grdph. In this case, the co-routining be\u00adhaviour of lazy eva i uation \nallows the search to abort early without needing to add additional mechanisms like excep\u00adtions. We give \nan e ample of this in Section 6. In summary the main contributions of this paper are: r We provide in!plementations \nof DFS algorithms in lin\u00adear time in H ken. We are careful to provide real code + throughout, and avoid \nresorting to pseudo-code. We construct the algorithms using reusable compo\u00ad nents, providi ng a greater \nlevel of modularity than is typical in oth er presentations. 344 We provide examples of correctness \nproofs. Again, these are quite different from traditional proofs, largely because they are not based \nupon reasoning about the dynamic process of DFS, but rather about a static value. This paper is organised \nas follows. Section 2 introduces a data type for graphs and some standard functions which will be used \nin subsequent algorithms. Section 3 introduces depth-first search. Section 4 describes the Haskell imple\u00admentation \nof several algorithms that use depth-first search which includes: topological sorting, strongly connected \ncom\u00adponents, as well as others. Section 5 describes the linear implementation of depth-first search in \nHaskell. Section 6 describes some more complex algorithms that use depth-first search, including edge \nclassification and biconnected compo\u00adnents. Section 7 discusses the complexity of the algorithms. Finally, \nSection 8 discusses related work. Representing graphs In order to meet our goal of not resorting to pseudo-code, \nwe need to begin with some boring details. There are many ways to represent (directed) graphs. For our \npurposes, we use an array of adjacency lists. The array is indexed by vertices, and each component of \nthe array is a list of those vertices reachable along a single edge. This adjacency struc\u00adture is linear \nin the size of the graph, that is, the sum of the number of vertices and the number of edges. By us\u00ading \nan indexed structure we are able to be explicit about the sharing that occurs in the graph. Another alternative \nwould have been to use a recursive tree structure and rely on cycles within the heap. However, the sharing \nof nodes in the graph would then be implicit making a number of tasks harder. So we will just use a standard \nHaskell immutable array. This gives constant time access (but not update these ar\u00adrays may be shared \narbitrarily). We can use the same mechanism to represent undirected graphs as well, simply by ensuring \nthat we have edges in both directions. An undirected graph is a symmetric di\u00adrect ed graph. We could \nalso represent multi-edged graphs by a simple extension, but will not consider them here. Graphs, therefore, \nmay be thought of as a table indexed by vertices. type Table a = Array Vertex a type Graph = Table [Vertex] \nThe type Vertex may be any type belonging to the Haskell index class Ix, which includes lnt, Char, t \nuples of indices, and more. Haskell arrays come with indexing ( ! ) and the functions indices (returning \na list of the indices) and bounds (returning a pair of the least and greatest indices). We pro\u00advide vertices \nas an alternative for indices, which returns a list of all the vertices in a graph. vertices : : Graph \n-> [Vertex] vertices = indices Sometimes it is convenient to extract a list of edges from the graph, \nthis is done with the function edges. An edge is a pair of vertices. type Edge = (Vertex ,Vertex) edges \n:: Graph -> [Edge] edges g = [ (v,w) I v <-vertices g, w<-g!v] To manipulate tables (including graphs) \nwe provide a generic function mapT which applies its function argument to every table index fentry pair, \nand builds a new table. mapT :: (Vertex -> a -> b) -> Table a -> Table b mapT f t = array (bounds t) \n[(v, f v (t!v)) I v<-indices t] The Haskell function array takes low and high bounds and a list of index/value \npairsl, and builds the corresponding array in linear time. Because we are using an array-based implementation \nwe often need to provide a pair of vertices as array bounds. So for convenience we define, type Bounds \n= (Vertex, Vertex) Using mapT we could define, outdegree :: Graph -> Table Int outdegree g = mapT nnmEdges \ng where numEdges v us = length ws which builds a table detailing the number of edges leaving each vertex. \nTobuildupagraph fromalist ofedges wedefinebuildG. buildG :: Bounds -> [Edge] -> Graph buildG bnds es \n= accumArray (flip (:)) [] bnds es Like array the Haskell function accumArray builds an ar\u00adray from a \nlist of index/value pairs, with the difference that accumlrray accepts possibly many values for each \nindexed location, which are combined using the function provided as accunulrray s first argument. Here \nwe simply build lists of all the values associated with each index. Again, construct\u00adingthearray takes \nlinear time with respect tothe length of the adjacency list. Soinlinear time, wecanconvert a graph definedin \nterms of edges to the vertex table based graph. For example, graph =buildG ( a , j ) [( a , j ), ( a \n, g ), ( b , i ), ( b , a ),( c , h ),( c , e ), ( e>, ~j ), (>e>, >h$ ),(>e~,~d)), ( f , i ), ( g , \nf ), ( g , b )] will produce the array representation for the graph shown in Figure 1. Then, to find \nthe immediate successors to e , say, we compute: graph ! e which returns [ d , h , j ]. Combining the \nfunctions edges and buildG gives us a way to reverse all the edges in a graph giving the transpose of \nthe graph: transpose :: Graph -> Graph transpose g = buildG (bounds g) (reverseE g) reverseE :: Graph \n-> [Edge] reverseE g = [ (w,v) I (v,w) <-edges g] We extract theedges from the original graph, reverse \ntheir direction, and rebuild agraph with the new edges. Then, for example, lActually Haskell uses the \nAssoc type, which is equivalent, but introduces an unnecessary new notation. + $+ i~fd h Figure 1: A \ndirected graph. (transpose graph) ! e will return [} C ]. Now by using trensposeG we can imme\u00ad diately \ndefine an indegree table for vertices: indegree :: Graph -> l able Int indegree g = outdegree (transpose \ng) This example gives a first feel for the approach we advocate in this paper. Rather than defining indegree \nfrom scratch by, for example, building anarray incrementally as retra\u00adverse the graph, we simply reuse \npreviously defined func\u00adtions, combining them in afresh way. The result is shorter and clearer, though \npotentially more expensive (an inter\u00admediate array is constructed). There are two things to say about \nthis additional cost. Firstly, the additional cost only introduces a constant factor into thecomplexity \nmemure, so the essence of the algorithm is preserved. Secondly, recent work in the automatic removal \nof intermediate structures (deforestation) comes a long way to removing this problem. Depth-first search \n The traditional view of depth-first search is as a process which may loosely be described as follows. \nInitially, all the vertices of the graph are deemed unvisited , so we choose one and explore an edge \nleading to a new vertex. Now we start at this vertex and explore an edge leading to another new vertex. \nWe continue in this fashion until we reach a vertex which has no edges leading to unvisited vertices. \nAt this point we backtrack, and continue from the latest vertex which does lead to new unvisited vertices. \nEventually we will reach a point where every vertex reach\u00adable from the initial vertex has been visited. \nIf there are any unvisited vertices left, we choose one and begin the search again, until finally every \nvertex has been visited once, and every edge has been examined. In this paper we will concentrate on \ndepth first search as a specification for a value, namely the spanning forest defined by a depth-first \ntraversal of a graph. Such a for\u00adest for the graph in Figure 1 is depicted in Figure 2. The (solid) tree \nedges are those graph edges which lead to un\u00advisited vertices. The remaining graph edges are also shown, \nbut in dashed lines. These edges are classified according to their relationship with the tree, namely, \nforward edges (which connect ancestors in the tree to descendants), back edges (the reverse), and cross \nedges (which connect nodes across the forest, but always from right to left), This stan\u00addard classification \nis useful for thinking about a number of Figure 2: A depth-first forest of the graph. algorithms and \nlater, in Section 6, we give an algorithm for classifying edges in this way. 3.1 Specification of depth-first \nsearch As the approach to DFS algorithms which we explore in this paper is to manipulate the depth-first \nforest explicitly, the first step, therefore, is to construct the depth-first forest from a graph. To \ndo this we need an appropriate definition of trees and forests. A forest is a list of trees, and a tree \nis a node containing some value, together with a forest of sub-trees. Both trees and forests are polymorphic \nin the type of data they may contain. data Tree a = Node a (Forest a) type Forest a = [Tree a] A depth-first \nsearch of a graph takes a graph and an initial ordering of vertices. Allgraph vertices inthe initial \nordering will be in the returned forest. dfs :: Graph -> [Vertex] -> Forest Vertex This function is the \npivot of this paper. For now we re\u00adstrict ourselves to considering its properties, and will leave its \nHaskell implementation until Section 5. Sometimes the initial ordering of vertices is not impor\u00adtant. \nWhen this is the case we use the related function dff :: Graph -> Forest Vertex dff g = dfs g (vertices \ng) What aretheproperties ofdepth-fist forests? They can be completely characterised by the following \ntwo conditions. (i) The depth-first forest of agraphis a spanning sub\u00adgraph, that is, it has the same \nvertex set, but the edge set is a subset of thegraph edge set. (ii) Thegraph contains no left-right \ncross-edges with re\u00adspect to the forest.  Later on in the paper, we find it convenient to talk in terms \nofpaths rather than single edges: apath being made up of zero or more edges joined end to end. We will \nwrite v --+g w to mean that there is a path from v to w in the graph g. Where there will be no confusion \nwe will drop the graph subscript. The ban on left-right cross edges translates into paths. At the top \nlevel, it implies that there is no path from any vertex in one tree to any vertex in a tree that occurs \nlater in the forest. Thus2, Property 1 If (ts++us=dff g), then VU c ts .Vw 6 us . v ~ w 0 Deeper within \neach tree of the forest, there can be paths which traverse a tree from left to right, but the absence \nof any graph paths which cross the tree structure from left to right implies that the path has to follow \nthe tree structure. That is: Property 2 If the tree (Node x (ts++us) ) is a subtree occurring any\u00adwhere \nwithin dff g, then vlJEts. vwEus. v+lo*v +x 0 So the only way to get from v to w is via (an ancestor \nof) x, the point at which the forests that contain v and w are combined (otherwise there would be a left-right \ncross edge). Thus there is also a path from v to z. The last property we pick out focusses on dfs, and \npro\u00advides a relationship between the initial order, and the struc\u00adture of the forest3. Property 3 Let \na and b be any two vertices. Write + for paths in the graph g, and < for the ordering induced by the \nlist of vertices vs. Then 3tEdfsgvs. aCt AbEt ~ 3c. c+a Ac+b A (Vd. d+a Vd+b+-c~d) 1 This Property says \nthat: (=)-) given two vertices that occur within a single depth-first tree (taken from the forest), then \nthere is a predecessor of both (with respect to -+) which occurs earlier in vs than any other predecessor \nof either. (If this were not the case, then a and b would end up in different trees). (+) if the earliest \npredecessor of either a or b is a prede\u00adcessor of them both, then they will end up in the same tree (rooted \nby this predecessor). These three properties are certainly true of DFS span\u00adning forests, but we make \nno claim about their completeness. There are other useful properties not derivable from these. We use \nthe notation ts++us to indicate any division of the list of trees in the forest, such that the order \nof the trees is preserved. Note that either ts or us could be empty. Also, we use c to indicate forest \nmembership and not purely for set membership. 3we further overload the E notation, to mean that both \n@and b occur within the tree t. 4 Depth-first search algorithms Algorithm 1. Depth-first search numbering \n Having specified DFS (at least partly) we turn to consider how it may be used. The first algorithm is \nstraightforward. We wish to assign to each vertex a number which indicates where that vertex came in \nthe search. A number of other algorithms make use of this depth-first search number, in\u00adcluding the biconnected \ncomponents algorithm that appears later, for example. We can express depth-first ordering of a graph \ng most simply by flattening the depth-first forest in preorder. Pre\u00adorder on trees and forests places \nancestors before descen\u00addants and left subtrees before right subtrees4: preorder :: Tree a -> [a] preorder \n(Node a ts) = [a] ++ preorderF ts preorderF :: Forest a -> [a] preorderF ts = concat (map preorder ts) \n Now obtaining a list of vertices in depth-first order is easy: preOrd :: Graph -> [Vertex] preOrd g \n= preorderF (dff g) However, it is often convenient to translate such an ordered list into actual numbers. \nFor this we could use the function tabulate: tabulate :: Bounds -> [Vertex] -> Table Int tabulate bnds \nvs = array bnds (zip vs [1..1) which zips the vertices together with the positive integers 1, 2, 3,..., \nand (in linear time) builds an array of these numbers, indexed by the vertices. We can package these \nup into a function as follows: preArr :: Bounds -> Forest Vertex -> Table Int preArr bnds ts = tabulate \nbnds (preorderF ts) (it turns out to be convenient for later algorithms if such functions take the depth-first \nforest as an argument, rather than construct the forest themselves.) Algorithm. Topological sorting \n The dual to preorder is postorder, and unsurprisingly this turns out to be useful in its own right. \nPostorder places descendants before ancestors and left subtrees before right subtrees: postorder :: \nTree a -> [a] postorder (Node a ts) = postorderF ts ++ [a] postorderF :: Forest a -> [a] postorderF \nts = concat (map postorder ts) So, like with preorder, we define,  postOrd :: Graph -> [Vertex] postOrd \ng = postorderF (dff g) 4The use of repeated appends (++) caused by concat introduces an extra logarithmic \nfactor here, but this is easily removed using standard transformations. 347 The lack of left-right cross \nedges in DFS forests leads to a pleasant property when a DFS forest is flattened in pos\u00adtorder. To express \nthis we need a definition. Definition A linear ordering s on vertices is a post-ordering with re\u00adspect \nto a graph g exactly when, 0 (where v ++ u means v + u and u + v). In words, this definition states that, \nif from some vertex v there is a path to a vertex later in the ordering, then there is also a vertex \nu which occurs no earlier than w and which, like w is also reachable by a path from v. In addition, however, \nthere is also a path from u to v. This property is so-named because post order flattening of depth first \nforests have this property. Theorem 4 If vs=post Ord g, then the order in which the vertices appear in \nvs is a post-ordering with respect to g. Proof If v comes before w in a post order flattening of a forest, \nthen either w is an ancestor of v, or w is to the right of w in the forest. In the first case, take w \nas u. For the second, note that as v + w, by Property 1, v and w cannot be in different trees of the \nforest. Then by Property 2, the lowest common ancestor of v and w will do. 0 We can apply all this to \ntopological sorting. A topologi\u00adcal sort is an arrangement of the vertices of a directed acyclic graph \ninto a linear sequence V1 , . . . . vn such that there are no edgesfrom Vj to vi where z < j. This problem \narises quite frequently, where a set of tasks need to be scheduled, such that every task can only be \nperformed after the tasks it depends on are performed. We define, t opSort :: Graph -> [Vertex] topSort \ng = reverse (postOrd g) Why is this correct? If w comes before v in the result of topSort g, then v comes \nbefore w in the result of postOrd g. Thus, by Theorem 4, there exists a vertex u no earlier than w which \nis in a cycle with v. But, by assumption, the graph is acyclic, so no such path v --+ w exists. Algorithm \n3. Connected components Two vertices in an undirected graph are connected if there is a path from the \none to the other. In a directed graph, two vertices are connected if they would be connected in the graph \nmade by viewing each edge as undirected. Fi\u00adnally, with an undirected graph, each tree in the depth-first \nspanning forest will contain exactly those vertices which con\u00adstitute a single component. We can translate \nthis directly into a program. The func\u00adtion components takes a graph and produces a forest, where each \ntree represents a connected component. components :: Graph -> Forest Vertex components g = dff (undirected \ng) where a graph is made undirected by: undirected :: Graph -> Graph undirected g = buildG (bounds g) \n(edges g ++ reverseE g) The undirected graph we actually search may have dupli\u00adcate edges, but this has \nno effect on the structure of the components. Algorithm. Strongly connected components Two vertices \nin a directed graph are said to be strongly con\u00adnected if each is reachable from the other. A strongly \ncon\u00adnected component is amaximal subgraph, where all thever\u00adtices are strongly connected with each other. \nThis problem is well known to compiler writers asthe dependency analysis problem separating procedures/functions \ninto mutually re\u00adcursive groups. We implement thedouble depth-first search algorithm of Kosaraju (unpublished), \nand Sharir (1981). Scc :: Graph -> Forest Vertex scc g = dfs (transpose g) (reverse (postOrd g)) The \nvertices of a graph are ordered using postOrd. The reverse of this ordering is used as the initial vertex \norder for a depth-first traversal on the transpose of the graph. The result is a forest, where each tree \nconstitutes a single strongly connected component. The algorithm is simply stated, but its correctness \nis not at all obvious. However, it may be proved as follows. Theorem 5 Let a and bbeanytwo vertices ofg. \nThen Proof The proof proceeds by calculation. We write gT for the transpose of g. Paths + in g will \nbe paths t in gT. Further, let ~ be the post-ordering defined by postOrd g. Then its reversal induces \nthe ordering ~. Now, ~tesccg. aEt AbEt &#38; { Definition of scc } 3t G dfs gT (reverse (postOrd g)) \n. a, b G t H ~~ B;~aer~~~ b A (Qd. d+a Vdtb+-c>d) H 3c. a+c Ab+c A (Vd. a-+d Vb--+d*d~c) From here on \nwe construct a loop of implications. 3c. a--+c Ab--+c A (Vd. a-+d Vb--+d+d~c) + {Consider d=aandd=b \n} 3c. a+c Aa~c Ab--+c Ab <cA (Vd. a+dvb+d+d <c) * { < is a post-ordering } 3c. (3e. a+--+ eAc<e)A (3f. \nb+--+ fAc<f)A (Vd. a-+dvb+d+d<c) + ~j e;:; ~=bcfi: (Vd...) } + a{-}  348 which gives us one direction. \nBut to complete the loop: a-b * { There is a latest vertex reachable from a or b } a++ bA3c. (a--+ cV \nb--+ c)A (Vd. a+d Vb+d+d~c) + { Transitivity of + } 3c. a+c Ab+c A (Vd. a+d Vb+d*d<c) as required, and \nso the theorem is proved. 0 To the best of our knowledge, this is the first calcula\u00adtional proof of this \nalgorithm. Traditional proofs (see Cor\u00adman et al. (1990), for example) typically take many pages of wordy \nargument. In contrast, because we are reusing an earlier algorithm, we are able to reuse its properties \nalso, and so obtain a compact proof. Similarly, we believe that it is because we are using the DFS forest \nas the basis of our program that our proofs are simplified as they are proofs about values rather than \nabout processes. A minor variation on this algorithm is to reverse the roles of the original and transposed \ngraphs: scc~ :: Graph -> Forest Vertex See g = dfs g (reverse (postOrd (transpose g))) The advantage \nnow is that not only does the result ex\u00adpress the strongly connected components, but it is also a valid \ndepth-first forest for the original graph (rather than for the transposed graph). This alternative works \nas the strongly connected components in a graph are the same as the strongly connected components in \nthe transpose of the graph. 5 Implementing depth-first search In order to translate a graph into a \ndepth-first spanning tree we make use of a technique common in lazy functional programming: generate \nthen prune. Given a graph and a list of vertices (a root set), we first generate a (potentially infinite) \nforest consisting of all the vertices and edges in the graph, and then prune this forest in order to \nremove repeats. The choice of pruning pattern determines whether the forest ends up being depth-first \n(traverse in a left-most, top-most fashion) or breadth-first (top-most, left-most), or perhaps some combination \nof the two. 5.1 Generating We define a function generate which, given a graph g and a vertex v builds \na tree rooted at v containing all the vertices in g reachable from v. generate :: Graph -> Vertex -> \nTree Vertex generate g v = Node v (map (generate g) (g!v)) Unless g happens to be a tree anyway, the \ngenerated tree will contain repeated subtrees. Further, if g is cyclic, the generated tree will be infinite \n(though rational). Of course, as the tree is generated on demand, only a finite portion will be generated. \nThe parts that prune dk\u00adcards will never be constructed.  5.2 Pruning The goal of pruning the (infinite) \nforest is to discard sub\u00adtrees whose roots have occurred previously. Thus we need to maintain a set of \nvertices (traditionally called marks ) of those vertices to be discarded. The set-operations we require \nare initialisation (the empty set ), membership test, and ad\u00addition of a singleton. While we are prepared \nto spend linear time in generating the empty set (as it is only done once), it is essential that the \nother operations may be performed in constant time. The easiest way to achieve this is to make use of \nstate transformers, and mimic the imperative technique of main\u00adtaining an array of booleans, indexed \nby the set elements. This is what we do. We provide an explanation of state\u00adtransformers in the Appendix, \nbut as they have already been described in a number of papers (Moggi 1989, Wadler 1990, Peyton Jones \nand Wadler 1993, Launchbury 1993, Launchbury and Peyton Jones 1994), and already been im\u00adplemented in \nmore than one Haskell variant, we avoid clut\u00adtering the main text. The implementation of vertex sets \nis easy: type Set s = MutArr s Vertex Bool mkEmpt y :: Bounds -> ST s (Set s) mkEmpty bnds = newArr bnds \nFalse contains :: Set s -> Vertex -> ST s Bool contains m v = readArr m v include :: Set s -> Vertex \n-> ST s () include m v = writeArr m v True Using these, we define prune as follows. prune :: Bounds -> \nForest Vertex -> Forest Vertex prune bnds ts = runST (mkEmpty bnds thenST \\m -> chop m ts) The prune \nfunction begins by introducing a fresh state thread, then generates an empty set within that thread and \ncalls chop. The final result ofprune is the value generatedby chop, the final state being discarded. \n chop: :Set s -> Forest Vertex-> ST s (Forest Vertex) chop m [] = returnST [1 chop m (Node v ts : us) \n = contains m v thenST \\visited -> if visited then chop m us else include m v thenST \\- > chop m ts thenST \n\\as -> chop m us thenST \\bs > returnST ((Node v as) : be.) When chopping a list of trees, the root of \nthe first is exam\u00adined. If it has occurred before, the whole tree is discarded. If not, thevertex isaddedto \nthe set represented bym, and two further calls tochop aremade in sequence. The first, namely, chop m \nts, prunes the forest of de\u00adscendants of v, adding all these to the set of marked ver\u00adtices. Once this \nincomplete, thepruned subforest is named as, andtheremainder of the original forest is chopped. The \n349 result of this is, in turn, named bs, and the resulting forest is constructed from the two. All \nthis is done lazily, on demand. The state combina\u00adtors force the computation to follow a predetermined \nlinear sequence, but exactly where in that sequence the compu\u00adtation is, is determined by external demand. \nThus if only the top-most left-most vertex were demanded then that is all that would be produced. On \nthe other hand, if only the final tree of the forest is demanded, then because the set of marks is single-threaded, \nall the previous trees will be produced. However, this is demanded by the very nature of DFS anyway, \nso it is not as restrictive as it may at first seem. At this point one may wonder whether any benefit \nhaa been gained by using a functional language. After all, the code looks fairly imperative. To some \nextent such a com\u00adment would be justified, but it is important to note that this is the only place in \nthe development that destructive operations have to be used to gain efficiency. We have the flexibility \nto gain the best of both worlds: where destruc\u00ad tive update is vital we use it, where it is not vital \nwe use the powerful modularity options provided by lazy functional languages.  5.3 DFS The components \nof generate and prune are combined to pro\u00advide the definition of DFS. dfs g vs = prune (bounds g) (map \n(generate g) VS) The argument vs is a list of vertices, so the generate func\u00adtion is mapped across this \n(having been given the graph g). The resulting forest is pruned in a left-most top-most fashion by prune. \nIf paying an extra logarithmic factor is acceptable, then it is possible to dispense completely with \nthe imperative features used in prune, and to use an implementation of sets baaed upon balanced trees, \nfor example. 6 More algorithms Algorithm 5. Classifying edges We have already seen the value of classifying \nthe graph edges with respect to a given depth-first search. Here we codify the idea by buildhg subgraphs \nof the original containing all the same vertices, but only a particular kind of edge. Tree edges are \neasiest, these are just the edges that ap\u00adpear explicitly in the spanning forest. The other edges may \nbe dktinguished by comparing preorder andfor postorder numbers of the vertices of an edge. We can summarise \nthe situation in the following diagram:  /TF\\ preorder: ...... ........................w ...... <~~> \n /=-B--xw postorder: ...... ........................ ...... \\/ TFC The above diagram expresses the relationship \nbetween the four types of edge (tree edges (T), forward edges (F), back edges (B), and cross edges (C)) \nand the preorder and postorder numbers. Only back edges go from lower pos\u00ad torder numbers to higher, \nwhereas only cross edges go from higher to lower in both orderings. Forward edges, which are the composition \nof tree edges, cannot be dktinguished from tree edges by this means both tree edges and forward edges \ngo from lower preorder numbers to higher (and conversely in postorder) but as we can already determine \nwhich are tree edges there is no problem. The implementation of these principles is now immediate and \npresented in Figure 3. To classify an edge we generate the depth-first spanning forest, and use this \nto produce preorder and postorder num\u00ad bers. We then have all the information required to construct the \nappropriate subgraph. Algorithm 6. Finding reachable vertices Finding all the vertices that are reachable \nfrom a single ver\u00adtex v demonstrates that the dfs doesn t have to take all the vertices as its second \nargument. Commencing a search at v will construct a tree containing all of v s reachable vertices. We \nthen flatten this with preorder to produce the desired list. reachable :: Graph -> Vertex -> [Vertex] \nreachable g v = preorderF (dfs g [vI ) One application of this algorithm is to test for the existence \nof a path between two vertices: path :: Graph -> Vertex -> Vertex -> Bool path g v w = w elem (reachable \ng v) The elem test is lazy: it returns True as soon as a match is found. Thus the result of reachable \nis demanded lazily, and so only produced lazily. As soon as the required ver\u00adtex is found the generation \nof the DFS forest ceaaes. Thus dfs implements a true search and not merely a complete traversal. Algorithm \n7. Biconnected components We end by programming a more complex algorithm finding biconnected components. \nAn undirected graph is biconnected if the removal of any vertex leaves the remaining subgraph connected. \nThis has a bearing in the problem of reliability in communication networks. For example, if you want \nto avoid driving through a particular town, is there an alternative route? If a graph is not biconnected \nthe vertices whose removal disconnects the graph are known as articulation points. Lo\u00adcating articulation \npoints allows a graph to be partitioned into biconnected components (actually a partition of the edges). \nIn Figure 4 vertices that are articulation points are marked with an asterisk. The naive, brute force \nmethod requires O(V(V + E)) time(where the problem graph has V vertices and E edges). A more efficient \nalgorithm is de\u00adscribed by Tarjan (1972), where biconnected components are found during the course of \na depth-first search in 0( V + E) time. Here we apply the same theory as Tarjan, but express it via explicit \nintermediate values. Tarjan s method is baaed on the following theorem: 350 tree :: Bounds -> Forest \nVertex -> Graph tree bnds ts = buildG bnds (concat (map flat ts)) where flat (Node v ts) = [ (v,w) I \nNode w us <-tsl ++ concat (map flat ts) back :: Graph -> Table Int -> Graph back g post = mapT select \ng where select v us = [ w I w <-us, post!v<post!w cross :: Graph -> Table Int -> Table Int -> Graph \ncross g pre post = mapT select g -w\u00ad here eelect v us = [ w I w <-us, post!v>post!w, pre!v>pre!w] forward \n:: Graph -> Graph -> Table Int -> Graph forward g tree pre = mapT select g where select v us = [ w I \nw <-us, pre!v<pre!w] \\\\ tree!v Figure 3: Classification of graph edges. *\\ (1,1) +-. / \\ .\\  c/b*/a \nde*h i d(4,z) 9(;,5) ~(9,1) \\ Figure 5: The depth-first forest for the undirected graph. f9 Figure 4: \nAn undirected graph. Dashed lines are the important back edges used forcal\u00adculating low points. Tree \nnodes are triples, for instance, efs, I ), represents the triple (e, 5, 1),where 5 is the depth-Theorem \n6 first number and f the low point number of vertex e. Given a depth-first spanning forest of a graph, \nv is an ar- Now that we have low points for vertices we can calculate ticulation point in the graph if \nand only ifi (i) v is a root articulation points. By part (ii) of Theorem 6 if the depth\u00adwith more than \none child; or (ii) v is not a root, and for all first number of v is less than or equal to the low point \nof w proper descendants wofvthere are noedges to any proper then v is an articulation point. ancestors \nof v. The function collect coalesces each DFS tree into a bi-We apply this theorem by associating a low \npoint number connected tree, that is, a tree where the node elements are with every vertex. The low point \nnumberofv is the smallest biconnected components. At each node the DFS number is DFS numbered vertex \nthat can be reached by following zero compared with the low-point number of all the children. If or more \ntree edges, and then along a single graph edge. the child s low-point number is strictly less than the \nnode s DFSnumber, then the component involving that vertexis We calculate low point numbers by traversing \nthe DFS trees bottom-up, and associating each vertex with its low not completed. On the other hand, ifthe \nnode s DFS num\u00adpoint number. The function label (see Figure 6) annotates ber is less than or equal to \nthe child s low-point number, atreewithboth depth-first numbers andlow-point numbers. thenthat component \nis completed once the nodeis included. The function bicomps handles the special case of the root. Finally, \nbcc ties all the other functions together. At anyvertex, the low point number is theminimum ofi (i) the \nDFS number of the vertex; Coalescing the tree from Figure 5 will produce the fol\u00adlowing forest containing \ntwo trees. (ii) the DFS numbers of the vertices reached by a single While this algorithm is complex, \nagain it is made up of edge; and individual components whose correctness may (potentially (iii) ~~ &#38;ve \npoint numbers of the vertex s descendants in at least) be established independently of the other compo\u00adnents. \nThis is quite unlike typical imperative presentations where the bones of the recursive DFS procedure \nare filled For example, the result of running label on the DFS out with the other components of the algorithm, \nresulting spanning tree produced from the graph in Figure 4, gives in a single monolithic procedure. \nthe annotated tree depicted in Figure 5. 351 bcc :: Graph -> Forest [Vertex] bcc g = (concat . map bicomps \n. map (label g dnum)) forest where forest = dff g dnum = preArr (bounds g) forest label :: Graph -> \nTable Int -> Tree Vertex -> Tree (Vertex,Int,Int) label g dnum (Node v ts) = Node (v,dnum!v,lv) us where \nus = map (label E dnum) ts lV = mi~imum ([d;um!vl++[ dnum!w I w <-g!v] ++[ lU I Node (u,du,lu) xs <-USI) \n bicomps Tree (Vertex,Int,Int) -> Forest [Vertex] bicomps ~Node (v,dv,lv) ts) = [Node (V:VS) us I (1, \nNode vs us) <-map collect tsl collect :: Tree (Vertex,Int,Int) -> (Int, Tree [Vertex]) collect (Node \n(v,dv,lv) ts) = (lv, Node (V:VS) CS) where collected = map collect ts . Vs = concat [ ws I (lw, Node \nws us) <-collected, lw<dv] cs = concat [ if lw<dv then us else [Node (V:WS) USI I (lw, Node ws us) <-collected] \nFigure 6: Biconnected components algorithm. [a,b] [a, e, h, Z] / N [b,d, C] [e,f, 91 Figure 7: The \nbiconnected trees. 7 Analysis of depth-first search 7.1 Complexity Models for complexity analysis of \nimperative languages have been established for many years, and verified with respect to reality across \nmany implementations. Using these models it is possible to show that traditional implementations of the \nvarious DFS algorithms are linear in the size of the graph (thatis,runin O(V+ E)time). Corresponding \nmodels for lazy functional languages have not been developed to the same level, and where they have been \ndeveloped there has not yet been the same extensive verification. Using these models, (see for example \nSands (1993)) we believe our implementation of the DFS algo\u00adrithmsto bilinear, but because these models \nhavenot been fully tested, we also ran empirical tests ourselves. We took measurements on the strongly \nconnected com\u00adponents algorithm, which uses two depth-first searches. The results of our experiment are \nin Figure 8. Timings were taken on randomly generated graphs (with differing numbers of vertices and \nedges) and are accurate to approximately lYo. The results are quite clear. The plotted points clearly \nall lie on aplane, indicating the linearity of the algorithm. We were also curious as to the constant \nfactor that we are paying over an imperative language. We coded up Tar\u00adjan s biconnected components algorithm \nin C, and compared with our Haskell implementation. For the graphs we tested Haskell was between 10 and \n20 times slower than C. This Sec 30 25 20 15 10 5 5WJ0 cm ml , Figure 8: Measurements taken on the \nstrongly connected components algorithm. was better than we expected asthe Haskell implementation is \nmulti-pass whereas the C implementation was the mono\u00ad lithic single-pass algorithm. 8 Related work \nKashiwagi and Wise (1991) also express their graph algo\u00adrithms in Haskell. They express a graph problem \nin terms of a set of recursive equations, and the algorithm is the fixed point of these equations. The \ngraphs are represented by lists, so the algorithms have poor complexity, but are suit\u00adable for parallel \nevaluation. Unfortunately, many of their algorithm implementations are long and unreadable, giving little \ninsight into the structure of the problem. For example, their strongly connected components algorithm \nis a page of intricate Haskell. Barth et al. (1991) describe M-structures in the paral\u00adlel functional \nlanguage Id which are well suited for state based computation. For instance, an M-structure array can \nbe used for holding marks to express whether a vertex has been visited before or not during a traversal. \nThe strength of M-structures is that they are designed to support parallel evaluation: their drawback \nis that referential transparency is lost. With regard to depth-first search, Reif (1985) gives strong \nevidence that it is inherently sequential; its computa\u00adtional complexity cannot be improved upon by parallel \ncom\u00adputation. So while M-structures provide a valuable method for general graph searching in parallel, \nthey provide little help for the particular case of depth-first search. The Graph Exploration Language \n(GEL) of Erwig (1992) provides explicit extensions to a lazy functional language. These are exploration \noperators, which give a concise way of expressing many graph algorithms. However, not all graph problems \ncan be expressed in terms of a given set of prede\u00adfine high-level operations, and it seems less than \nideal to add new language concepts for every new class of problem that is tackled. Burton and Yang (1990) \nexperimented with multi-linked structures. They use arrays which are implemented using balanced trees \nto represent heaps. They give many exam\u00adples of using multi-linked structures using heaps, one exam\u00adple \nis an arbitrary depth-first search function. A drawback with their approach is that heaps have to be \npassed to and returned from each function. Another is that, by using bal\u00adanced trees a logarithmic factor \nis incurred, so their depth\u00adfirst search function is not linear in the size of the graph. Acknowledgements \n We would like to thank Kieran Clenaghan and Simon Peyton Jones for the helpful comments they gave on \na preliminary draft of this paper. We are also very grateful to Richard Bird for suggesting that DFS \ncam be expressed using the generate/prune paradigm, and to him, Geraint Jones and Theo Norvell for inspiring \nthe correctness proofs. The work was partly supported by the UK Engineering and Physical Sciences Research \nCouncil. References Barth, P. S., Nikhil, R. S. and Arvind (1991), M-structures: Extending a parallel, \nnon-strict, functional language with state, in J. Hughes, cd., Conference on Func\u00adtional Programming \nLanguages and Computer Archi\u00adtecture , LNCS 523, Springer-Verlag, Cambridge, Mas\u00adsachusetts, pp. 538 \n568. Burton, F. W. and Yang, H.-K. (1990), Manipulating mul\u00adtilinked data structures in a pure functional \nlanguage , Sofiware-Practice and Experience 20, 1167-1185. Corman, T. H., Leiserson, C. E. and Rlvest, \nR. L. (1990), in\u00adtroduction to Algorithms, The MIT Press, Cambridge, Massachusetts. Erwig, M. (1992), \nGraph algorithms = iteration + data structures? The structure of graph algorithms and a style of programming, \nin E. Mayr, ed., Graph-Theoretic Concepts in Computer Science , LNCS 657, Springer-Verlag, pp. 277 292. \nHarrison, R. (1993), Abstract data types in Standard ML, John Wiley and Sons. Holyer, L (1991), Functional \nprogramming with Miranda, Pitman, London. Hopcroft, J. E. and Tarjan, R. E. (1973), Algorithm 447: Efficient \nalgorithms for graph manipulation , Commu\u00adnications of the ACM 16(6), 372 378. Hudak, P., Peyton Jones, \nS. L., Wadler, P., Arvind, Boutel, B., Fairbairn, J., Fasel, J., Guzm5n, M. M., Hammond, K., Hughes, \nJ., Johnsson, T., Kieburtz, R., Nikhil, R. S., Partain, W. and Peterson, J. (1992), Report on the functional \nprogramming language Haskell, Version 1.2 , ACM SIGPLAN Notices 27(5). Kashiwagi, Y. and Wise, D. S. \n(1991), Graph algorithms in a lazy functional programming language, in Proceed\u00adings of the 4 th International \nSymposium on Lucid and Intensional Programming , pp. 35 46. Also available as Technical Report Number \n330, Computer Science De\u00adpartment, Indiana University. Launchbury, J. (1993), Lazy imperative programming, \nin Workshop on State in Programming Languages , ACM SIGPLAN, Copenhagen, Denmark, pp. 46-56. Launchbury, \nJ. and Peyton Jones, S. L. (1994), Lazy func\u00adtional state threads, in Conference on Programming Language \nDesign and Implementation , ACM SIG- PLAN, Orlando, Florida. Manber, U. (1989), Introduction to Algorithms \nA Creative Approach, Addison-Wesley, Reading, Massachusetts. Moggi, E. (1989), Computational lambda-calculus \nand mon\u00adads, in fSymposium on Logic in Computer Science , IEEE, Asilomar, California. Paulson, L. C. \n(1991), ML for the working programmer, Cambridge University Press, Cambridge. Peyton Jones, S. L. and \nWadler, P. (1993), Imperative func\u00adtional programming, in 20 th Symposium on Principles of Programming \nLanguages , ACM, Charleston, North Carolina. Reif, J. H. (1985), Depth-first search is inherently sequen\u00adtial \n, Information Processing Letters 20, 229 234. Sands, D. (1993), A naive time analysis and its theory \nof cost equivalence, TOPPS report D-173, DIKU, University of Copenhagen, Denmark. Sharir, M. (1981), \nA strong-connectivity algorithm and its applications in data flow analysis , Computers and mathematics \nwith applications 7(l), 67 -72. Tarjan, R. E. (1972), Depth-first search and linear graph algorithms \n, SIAM Journal of Computing 1(2), 146 160. Wadler, P. (1990), Comprehendhg monads, in Conference on Lisp \nand Functional Programming , ACM, Nice, France, pp. 61-78. Appendix Imperative features were initially \nintroduced into the Glas\u00adgow Haskell compiler to perform input and output, see Pey\u00adton Jones and Wadler \n(1993). The approach is baaed on monads (Moggi 1989, Wadler 1990), and can easily be ex\u00adtended to achieve \nin-situ array updates. Launchbury (1993) showed how the original model could be extended to allow the \nimperative actions to be delayed until their results are required. This is the model we use. We will \nuse the monad of state-transformers with type constructor ST which is defined: type STsa=s-> (a,s) So \nelements of type ST s Int, say, are functions which, when applied to the state, return a pair of an integer \nto\u00adgether with a new state. As usual we have the unit returnST and the sequencing combinator thenST: \nreturnST :: a-> STsa returnST a s = (a, s) thenST :: STsa->(a -> STsb)->STs b (m thenST k) s = k at where \n(a,t) =ms The ST monad provides three basic array operations: newArr ::Ix i=> (i, i) -> a -XT s (MutArr \ns i a) readArr ::Ix i=> MutArr s i a -> i -> ST s a writeArr::Ix i=> MutArr s i a -> i -> a -XT s () \nThe first, newArr, takes apair ofindex bounds (the type a must lie in the index class Ix) together with \nan initial value, and returns a reference to an initialised array. The time this operation takes is linear \nwith respect to the number of elements in the array. Theother twoprovide for reading and writing to an \nelement of the array, and both take constant time. Finally, the ST monad comes equipped with a function \nrunST. runST :: (Vs . STsa) -> a This takes a state-transformer function, applies it to an ini\u00adtial state, \nextracts the final value and discards the final state. The type of runST is not Hindley-Milner because \nof thenested quantifier, so it must be built-in to Haakell. The universal quantifier ensures that in \na state thread variables from other state threads are not referenced. For details of this see Launchbury \nand Peyton Jones (1994). So, for example, runST (newArr (1,8) O thenST (\\nums -> writeArr nums 5 42 thenST \n(\\. -> readArr nums 5 thenST (\\v -> returnST v))))  will return 42. This can be read as follows: run \na new state thread extracting the final value when finished; create a new array indexed from 1 to8 with \ncomponents all O; then bind this array tonums; write to array nums at index 5 the value 42; then read \nthe component in nums at index 5 and bind this value to v; finally return value v. Note that the final \nexpression returnST v is unnecessary as readArr returns a value. The parentheses immediately after thenST \nare also unnecessmy, as Haskell s grammar binds lambda expressions tighter than infix functions. \n\t\t\t", "proc_id": "199448", "abstract": "<p>Depth-first search is the key to a wide variety of graph algorithms. In this paper we express depth-first search in a lazy functional language, obtaining a linear-time implementation. Unlike traditional imperative presentations, we use the structuring methods of functional languages to construct algorithms from individual reusable components. This style of algorithm construction turns out to be quite amenable to formal proof, which we exemplify through a calculational-style proof of a far from obvious strongly-connected components algorithm.</p>", "authors": [{"name": "David J. King", "author_profile_id": "81100194254", "affiliation": "Department of Computing Science, University of Glasgow, Glasgow G12 8QQ, UK", "person_id": "PP14077306", "email_address": "", "orcid_id": ""}, {"name": "John Launchbury", "author_profile_id": "81100462557", "affiliation": "Department of Computer Science and Engineering, Oregon Graduate Institute of Science & Technology, Beaverton, Oregon", "person_id": "PP39043890", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/199448.199530", "year": "1995", "article_id": "199530", "conference": "POPL", "title": "Structuring depth-first search algorithms in Haskell", "url": "http://dl.acm.org/citation.cfm?id=199530"}