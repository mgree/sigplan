{"article_publication_date": "01-25-1995", "fulltext": "\n Structured Operational Semantics as a Specification Language Bard Bloom* Cornell University Ithaca, \nNY 14850, USA. 607/255-9211 Abstract Standard specification languages have very limited abilities to \ndefine new operations on processes. We introduce the concept of a Protean specification language, with \ngeneral definitional facilities supported by the appropriate theory. Protean languages allow elegant, \nreadable, and useful spec\u00adifications at all levels of abstraction. A good Protean spec\u00adification language \nwill admit methods of verifying that one specification is a refinement of another. Wesketch a family \nof Protean specification languages (with references to the full details) which allow a vast amount ofexpressive \npower in defining operations, but nonetheless have all the essential theoretical and specification power \nof CCS and ACP. We illustrate these techniques by presenting several spec\u00adifications of the job of protecting \nan arbitrary server by a checkpoint/backup scheme. The high-level specification of the protected server \nsimply says, It does everything it did before, and it doesn t crash. The middle-level specifica\u00adtion \ndescribes checkpointing cleanly and abstractly, with\u00adout prescribing any particular implementation. The \nlow\u00adlevel specification is fairly close to an implementation. We show the high-and medium-level specifications \nequivalent by bisimulation relation techniques, and the medium-and low\u00adlevel specifications equivalent \nby equational reasoning using automatically-generated equations. We also show that the operations expressing \ncheckpointing behavior are not defin\u00ad able in standard process algebras. *bard@ cs.cornell.edu. Supported \nby NSF grants CCR-9003441, CCR-9014363, CCR-9003440, and CCR-9223183. Permission to copy without fee \nail or part of this material is granted provided that the oopies are not made or distributed for direct \ncommercial acfvanta~e, the ACM copyright notice and the title of the publication and RSdate appear, and \nnotice is given thatmpy~bypermi~on of thesation of Computing Machinery. o copy otherwise, or to repubiwh, \nrequires a fee and/or specific permission. POPL 95 II 95 San Francisco CA USA @ 1995 ACM 0-89791-692-119510001 \n....$3.50 1 Introduction Writing specifications for programs is every bit as hard as writing programs \nthemselves. In many ways, it is harder: specification languages are generally younger and less pol\u00adished \nthan programming languages. Often, expressing im\u00adportant concepts (e.g., broadcasting and message-numbering) \nrequires nontrivial programming in the specification lan\u00adguage. In this study, we propose a powerful \nclass of process\u00adalgebraic specification languages, based on the metatheory of process algebra, allowing \nspecification at the level of pro\u00adcesses and operations. We are concerned with process-algebra-like specification \nlanguages, in the spirit of, e.g., LOTOS [BB87] and the more foundational CCS [Mi189], CSP [Hoa85], and \nACP [BW90, Bae90]. These languages are intended for specify\u00ading processes and the protocols that govern \ntheir interaction. The basic notion is p $ q: that the process (or system) p can perform the action a \nand thereafter behave like q. A specification is a process w any process with the same or approximating \nbehavior (defined suita,bly) meets that spec\u00adification. The process-algebraic approach is more linguistic \nthan most others [Luc90, AL91, LT87, MP83]. Specifications are programs in a fairly abstract programming \nlanguage. They can be combined and manipulated via a rich set of opera\u00adtions; e.g., if we have two protocol \nspecifications p and q, we may specify concepts like run p, then run q and run p and q in parallel directly, \nwithout having to modify the text of p or q at all. As programmers intuitions and skills are generally \ntuned towards programming, we may expect them to work more easily with specifications which resemble \nabstract programming. Clarity and simplicity are even more important for spec\u00adificat ions than for implement \nat ions. 1mplement ations have to make some compromises for the sake of efficiency and other pragmatic \nconcerns: ultimatel~ they have to work adequately, and a multitude of stylistlc sins can be forgiven \nif they do. Specifications exist solely for explaining what the implementations should do. When they \nare unduly obscure or complicated, they are worthless. We cannot afford to clutter our specifications \nwith complicated programming of underlying protocols at the expense c)f obscuring the high\u00adlevel task \nbeing specified. This puts stress on the language definition. The set of operations can t be too large; \na specification language the size and detail of PL/I would be unmanageable and over\u00adwhelming. On the \nother hand, the language must be large enough. Most specification languages do not include opera\u00adtions \nlike true sequencing: p; q, which runs p and when it cannot run any longer, runs q. This operation is \ngenerally excluded from programming languages for the quite good reason that it is hard to implement; \ne.g., if p deadlocks, q must be started. However, p; q is a fairly natural thing to specify; it is up \nto the implementer to figure out what kind of deadlock detection is necessary. It is, of course, possible \nto write deadlock detection pro\u00adtocols in any standard process algebra or specification lan\u00adguage. It \nrequires careful coding of the processes whose deadlock is being detected, as well as the introduction \nof monitoring processes and so forth. These are unavoidable and thereby excusable in the implementation. \nHowever, they can be disastrous for the specification: a simple in\u00adteraction of the form PI ; pZ; PS \n; pi could be transmuted to a horribly complicated specification in which the actual tasks pk are almost \nlost in a sea of deadlock detection. This vi\u00adolates the requirement of clarity, and makes a specification \nthat ought to be simple unbearably complex. We thus have two contradictory imperatives on the de\u00adsign \nof a specification language. LOTOS, CCS, CSP, and ACP have taken the standard approach to resolving this \nparadox: carefully choosing small, orthogonal sets of opera\u00adtions useful for specifying many things. \nThe choice of these operators is supported by many examples of good specifica\u00adtions. Of course, many \nuseful and sensible operations are missing. There is no better way to proceed if we must fix the set \nof operations as part of the language design. In this paper, we argue that the set of operations need \nnot be fixed as part of the specification language. Indeed, defin\u00ading new operations is as useful a component \nof specification as writing functions and subprograms is in an imperative language. We therefore introduce \na family of specification languages, based on the theory of struct ural operational se\u00admant ics (SO S) \nwhich allow the definition of new operations. SOS-based specifications are written in a somewhat dif\u00adferent \nstyle than ordinary process-algebraic specifications. Switching from ordinary to SOS-based specifications \nhas much the same feel as switching from concrete to abstract data types in programming. A program written \nin, say, C++ with with data abstraction essentially defines an ex\u00adtension of C++ which has just the right \noperations for the task at hand say, types of proofs and expressions, opera\u00ad tions of substitution and \ncut elimination and then writes the desired program in terms of the extended language. Sim\u00adilarly, a \nSOS-based specification starts off by defining the main concepts in the specification say, checkpointing \nand message numbering and then writing the specification in terms of those concepts. 1.1 Protean languages \nSplitting up a specification into concept definitions and con\u00adcept use should have the same kind of benefits \nfor specifica\u00adtion as modularity and data abstraction have for program\u00adming. o Making key concepts explicit \nand visible will help read\u00aders understand the specification, both in the gener\u00adalities and the details. \nFor example, suppose that the specification relies on a checkpointing operation. The reader can understand \nthis operation as a con\u00adcept when reading the larger specification. Further\u00admore, there are any number \nof possible variations on the theme of checkpointing; the reader may study the one that has been singled \nout as relevant for the case at hand. By contrast, in a language which does not admit the creation of \nnew operations, the checkpointing proto\u00adcols must be mixed in with the rest of the specification. That \nis, part of each process will handle checkpointing. The reader will have to figure out which subterms \nim\u00adplement checkpointing and which computation. Fur\u00adthermore, if the checkpointing protocol is intertwined \nwith the rest of the program, it cannot be isolated and studied alone. Packaging key concepts makes the \nspecification easier to maintain. If details of the checkpointing scheme change say, a scheme using \nunbounded message numbers is replaced by one using bounded numbers the specification of checkpointing \nmust of course be revised. However, unless the change affects some other aspect of the server, only the \ncheckpointing operation needs to be updated. There are some opportunities to re-use parts of speci\u00adfications. \nFor example, it is likely that more than one specification will use the concept of checkpointing. In\u00addeed, \nwe may consider the abundance of new ACP operators as the creation of a library intended for gen\u00aderal \nuse. Indeed, many ACP ,operations are presented wit h SOS rules explaining their behavior. These benefits \ncome from having a systematic way to de\u00adfine and modularize new operations, rather than from the details \nof the SOS-based methodology. The more general concept of a specification language allowing operation \ndef\u00adinition is worth defining as a separate concept. As these languages are capable of adapting their \nform to diverse cir\u00adcumst antes, we call them Protean: Informal Definition 1.1 A Protean specification \nlanguage consists of the traditional parts of a specification language in the case of process algebra, \nsets of process terms and ac\u00adtions, and some built-m basic operations upon them plus a metalanguage, \na scheme for inventing and specifying new operations as necessary, such that every invented operation \nbehaves properly and ail important methodologies apply. SO, the SOS-based languages we discuss in this \npaper are all Protean process-algebraic specification languages. Pre\u00adsumably there are other ways of \nbuilding Protean languages, both process-algebraic and otherwise, but the matter re\u00admains to be investigated. \n 1.2 SOS-based Protean Languages In this paper, we present the two SOS-based Protean spec\u00adificat ion \nlanguages: Grand and RBB Cool. These languages are in the spirit of C CS. Indeed, both are generalizations \nof CCS to Protean languages, adapted for two notions of process equivalence. They illustrate the kinds \nof tradeoffs that are available in SOS-based Protean languages. Grand is based on strong bisimulation, \nthe finest notion of process equivalence gen\u00aderally accepted in this setting. Processes are distinguished \nwhenever the distinction can be justified from process be\u00adhavior; that is, the text of a process cannot \nbe examined, but all of its actions (including actions intended to be pri\u00advate) can be. For example, \na program which idles for one step r is considered different from one which idles for two steps r; ~. \nBecause strong bisimulat ion is the finest equiva\u00adlence, Grand is the largest Protean language and admits \nthe most operations. RBB Cool is based on rooted branching bisimulation, which makes all possible distinctions \nbased on process behavior considering internal actions ~ to be invisible. This is still an extremely \nfine notion of equivalence. It is more suitable for specification often protocols need to do some inter\u00adnal \ncommunication in order to work. More generally, coarse equivalences (when correct) are generally preferable \nto fine ones for specification, as more programs will meet specifica\u00adtions and the job of programming \nwill be easier. However, it has some costs. Fewer operations are definable; in particu\u00adlar, an operation \nthat counts r moves is semantically mean\u00adingless in rooted branching bisimulation, and thus cannot be \ndefined in RBB Cool. This tradeoff can be carried further. There are a dozen process-algebraic Protean \nspecification languages, ranging from the finest equivalences in use (bisimulation) to the coarsest (part \nial trace equivalence). In this survey, we use the finest notions possible mainly because we can: we \nshow correctness to a ridiculously fine degree of equivalence.  1.3 Roadmap to this paper The goal of \nthis paper is to present and demonstrate the use of Grand and R B B Cool as specification languages. \nIn Section 2 we describe the theory and practice of CCS, much of which applies mutatis mutandis to Grand \nand RBB Cool. Section 3 introduces Grand and RB B Cool, and discusses proof methods. Section 4 describes \nthe use of these lan\u00adguages and proof methods in a case study of a reliable server. 2 Survey of t he \nt heory and practice of CCS Protean languages should be as powerful as existing specifi\u00adcation languages: \nit would be a mistake to gain the ability to define operations at the cost of losing some essential tools \nor proof methods. We give a quick survey of the essential fea\u00adtures of CCS, with an eye towards generalizing \nthem. CCS is defined by structured rules, which by no coincidence fit into most rule formats. CCS is \nparametrized by a set of actions Act. The actions are only minimally interpreted. There is a distinguished \nac\u00adtion r, the silent action, used to represent internal compu\u00adtation. Each visible (non-~) action a \nhas a complement ii, with Z = a; a and ti are used to represent a receive and a send on the same channel. \nThere is a basic operation prefixing: where a.p does an a-action and thereafter behaves like p, with \nrule: a.p Sp. (1) There is a term denoting the stopped process, O, which has no rules; thus a.b.O is \na process which performs a, then b, then is stopped. Beyond that, there are operations like + (nondeterministic \nchoice) and I (parallel composition). The process x+y can do anything that z or y can do, as indicated \nby its rules: (2) There are several other operations, notably parallel compo\u00adsit ion plq, defined by \nrules in this style. The definition of CCS is quite clear, and can be explained readily to any pro\u00adgrammer, \neven without deep mathematical background; the definition involves nothing more complicated than recursion \non the structure of terms. The mathematics of CCS is well-explored. For example, transitions are recursively \nbranching; that is, for a given p, there are at most a finite number of (a, q) pairs such that p $ q, \nand the set of all such pairs is computable in canonical index from the term p. These formal properties \nconstitute a sanity check; they are rarely used directly, but they have far-reaching consequences and \nalso suggest that we have a basic understanding of CC S comput at ion. In CCS, one generally specifies \nprocesses up to equiva\u00adlence; that is, one mat ches the specification exactly. Two CCS processes are \nconsidered equivalent iff they are bisim\u00adilar. Informally, p and q are bisimilar iff p and q make the \nsame decisions at the same times. There are several varieties of bisimulation, strong and weak and branching, \nrooted and unroot ed. The differences are technically important, but generally easy to manage in practice. \nTwo facts about all bisimulations are of vital importance: If a process is given Up to bisimulation, \nall visible as\u00adpects of its behavior are specifiecl.1  Bisimulation is a congmence; that is, all CCS \nopera\u00adtions respect it. For example, if p and p are bisimilar, and q and q are bisimilar, then p I q \nand p I q are also bisimilar.  Together, these say that bisimulation maybe used as a spec\u00adification \ntool. If p is a program and s is a specification that is, both p and s are CCS processes and p is bisimilar \nto s, then p meets s in the sense that, no matter how they are used, no difference between them can be \nseen.2 Bisim\u00adulation supports a variety of proof methods, which will be described in Section 3.3. 3 Two \nProtean Languages In this section, we present the Protean languages Grand and RBB Cool, which work with \nstrong and rooted branching bisimulation respectively. Both specification languages have the power and \nmethodology of CCS (with their respective notions of equivalence), and have rule formats which allow \nthe definition of new operations in the Protean style. 3.1 Strong Bisimulation and Grand Grand is the \nbest-developed Protean language; it is the the infinitary generalization of the GSOS rule format of [BIM88, \nB1089]. It is designed to work with strong bisimulation, the purest and finest form of bisimulation. \nFormally, p fi q if there is a symmetric relation= (thought of as is in the the l~hi~ point is ~re~ented \nin detail in [Mi181, BM92]. Scenarios n which this claim does not hold fully are described in [BK92, \nPP88]; methods for such scenarios are under development. 2Bi~imulati~n is in fact too strong [BIM88, \n1310wl.There areP= ~which require some rather extreme operations to distinguish [Abr87, BM92]. Nonetheless, \nbkimulation is adequate in the sense that, if a program is correct up to bisimulation, then it is correct. \nIt works well in practice; many programs are in fact correct up to bisimulation [Bae90, WBB92]. same \nstate of computation as ) between processes such that p=qand, whenever p~p , then~q such that q~q and \np w q . The theory of strong bisimulationis described in [Mi189]. A grand language consists of a set \nof actions, a set of operation symbols with arities, and a set of grand ruies over those actions and \noperations.3 A grand rule has the form Positive tests about z s, Negative tests aboutx s f(?) A t where~is \nan operation symbol, and; are distinct variables names; the positive tests each have the form x ~ y, \nwhere the variable z is one of the arguments of ~(Z); a is an ar\u00adbitrary action; and yisafresh variable; \nthe negative tests have the form z ~, and tis any term whose variables in\u00adclude at most the z s and y \ns. All rules appearing in this paper, all CCS rules, and most rules for pure process calculi in the Literature \nfit this format. In [BIM88, B1089, ABV92] we develop the theory of grand and GSOS languages. In brief, \nthese languages have all the mathematical and methodological properties that we men\u00adtioned about CCS, \nfrom finiteness of nondeterminism to the existence of a complete equational axiom scheme. 3.2 Rooted \nBranching Bisimulation and RBB cool Strong bisimulation is too strong for general use. When a process \nis correct up to strong bisimulation, then it is certainly correct; but there are many processes which \none wishes to consider correct, but which are not correct up to strong bisimulation. For example, Ta \nand TTa are processes which perform a visible action a after a few internal actions. They cannot be distinguished \nby observation (when one con\u00adsiders the internal actions to be invisible), not even if they are used \nin some larger CCS program. So, weakened bisimulations have been devised. Weak\u00adened bisimulations are \nmodified versions of strong bisimu\u00adlation which ignore ~ moves to the extent possible. There are a vast \nrange of weak bisimulations [vG93]. CCS is based on rooted weak bisimulation, also called ob.semation \nequiva\u00adlence [Mi189]. For this study, we use rooted bTanching bisim\u00adulation [vGW89]. Rooted branching \nbisimulation is finer than rooted weak bisimulation; indeed, it is the finest well\u00adbehaved weakened bisimulation \nin the sense of [vG93]. It is also quite well-behaved, having a slightly nicer theory than rooted weak \nbisimulation. We use rooted branching bisimulation in this study be\u00adcause our results are true in in \nit, and we are showing the strongest theorem possible. However, the results of this pa\u00adper apply equally \nwell to rooted weak bisimulation; indeed, all the work after this section would still be correct if all \nreferences to branching were replaced by weak . We sus\u00adpect that this is generally the case; the difference \nbetween branching and weak bisimulation appears only in unusual circumst antes. We defer the definitions \nto the appendix Section A. RBB Cool is a Protean specification language adapted to root ed branching \nbisimulation. It is the most powerful SOS rule format known at the time of writing to respect rooted \n31f each component is finite, the language is GSOS [BIM88]. branching bisimulation. It is a restricted \ncase of Grand, lim\u00adit ing the ways in which ~ s can appear in rules to require that the ~ s be sufficiently \ninvisible. RBB Cool has a theory comparable to that of CCS with rooted weak bisimulation. Operations \ndefined by RBB Cool rules are a fortiori defined by grand rules, so the same com\u00adputability and bounded \nnondeterminism theorems apply. The equivalence is a congruence with respect to all RBB Cool operations. \nThe scheme of [ABV92] gives sound axiomati\u00adzations for all RBB Cool operations. The axiomatizations cannot \nbe complete for computability reasons, but the stan\u00addard methods for proving equations for specific calculi \nhold in general. 3.3 Proof Methods We discuss two categories of proof methods for bisimulation. There \nare in fact other methods available for CCS; e.g., me\u00adchanical and logical ones, but they remain to be \ndeveloped in full for Grand and RBB Cool. 3.3.1 Bisimulation Relation Methods The definitions of bisimulation \nhave the form, p and q are bisimilar iff there exists a relation u such that p v q, and 0(+) , where \nO(U) formalizes the notion that whenever p v q and p does something and ends up in state p , then q can \ndo the same thing and end up in a state q with p = q . The formalization of the phrases do something \nand do the same thing vary from notion to notion. How\u00adever, they all concern a single visible action, \npossibly with r actions before and after. This definition gives rise to a method for proving pro\u00adgrams \nequivalent. One guesses a relation V, and shows that it satisfies the property @. Most of the creativity \nin the method lies in guessing =. Verifying 0(w) is generally tedious but straightforward. One must show \nthat for each pair of related states p w q and each possible action of one process, the other process \ncan perform a matching ac\u00adtion. The saving grace of the bisimulation relation method is that, in general, \nonly a path of a few actions of p need be matched against a similarly short path of q . In partic\u00adular, \nthere is no need to match an entire execution trace from p against one from q. Experience and mathematics \nboth indicate the importance of this saving grace; in many proofs using it, and the exponentially-faster \nalgorithms for computing bisimulations of finite-state processes.  3.3.2 Equational Calculations Bisimulation \n(and, indeed, most other notions of process equivalence) has a complete equational axiomatization on \nbasic finite processes. Other operations may be described by axioms indeed, the ACP school of process \nalgebra [BW90] takes the axioms as primary, and derives operational behavior for the operations. Equational \ncalculations have several advantages for prov\u00ading programs. For example, they are compositional. One \nmay verify a composite program by verifying its compo\u00adnents: to show p o p = q o q (where o is any operation), \nit suffices to show p = q and p = q . This situation arises naturally when one is refining an abstract \nspecification, as in Section 4.1.4. 110 Both Grand and RB B Cool support equational verifica\u00adtion. \nGiven a specification involving new operations, the algorithm of [ABV92] allows us to mechanically calculate \nan equational axiom system which, together with an induc\u00adtion principle, is complete for strong bisimulation, \nand as effective as any known proof technique for rooted branching bisimulation. 4 Case Study: Reliable \nServer We demonstrate the techniques of SOS-based specification languages by specifying a reliable server \nprotected by a check\u00adpoint /backup scheme, at three levels of abstraction. Spec-A is the most Abstract \nspecification, obviously correct and too simple to misinterpret, and not directly implement able. Spee-A \nis good for clients oft he server, but gives no assistance to implementers.  Spec-B is at an in-Between \nlevel of abstraction, ex\u00adplaining what checkpointing is without describing how it is done. Spec-B is \nhelpful for anyone looking for insight into the implementation without the details.  Spec-C is quite \nConcrete: it involves communicating processes for server and storage, and is about as clos~ to code as \none can expect from process algebra. Spec-C is good for implementers of the server, but gives no assistance \nto clients.  We have several specifications of the same server, in\u00adtended for different audiences. \nWe must show that the specifications are consistent. In this case, the specifica\u00adtions are actually equivalent \nup to rooted branching bisim\u00adulation. The sketches of the proofs of equivalence demon\u00adstrate the proof \nmethods: the bisimulation relation tech\u00adnique to show S pee-B implements Spec-A, and automatically generated \nequational reasoning to show Spec-C implements Spec-B. 4.1 Overview of the specifications. Let p be the \nspecification for some kind of server process, capable of either performing its proper job, or of failing \nat any time. We wish to specify a server which behaves like p except that it does not fail, using a checkpoint-backup \nscheme. For now, we leave p s behavior almost completely arbi\u00adtrary, save that it must be sufficiently \ndeterministic. We insist that all its visible actions belong to some fixed set A, except we we allow \np to fail, by performing a f @ A action. Recall that -r is used for silent actions. p$p , a~A iff p performs \nserver action a p:p iff p performs an internal action p2p iff p can fail We consider the behavior \nof p after p s failure to be unim\u00adportant. 4.1.1 Spec-A To specify a nonfailing version of p, we invent \na new operator No Fail(p) which transmits all the correct actions of p, but blocks the failing one. The \nhigh-level specification is simply: Spec-A = NoFail(p) where NoFa iI(p) is defined by the rule4 z~y, \na~(Au {r}) (3) NoFail(x) % NoFail(y) NoFai l(p) can do all of p s proper server actions, and in\u00adternal \nactions. No rules give it behavior related to f actions of p, and so NoFail(p) has no action if p can \nfail; it simply forbids the failure from occurring. Figure 1 shows a sam\u00adple p which can fail at every \nstep, and the corresponding failproof NoFail(p). Spec-A clearly has the right behavior: it behaves just \nlike p except that it avoids failure and improper actions. It cannot be directly implemented in most \nprogramming lan\u00adguages but as a specification it could hardly be simpler or clearer.  4.1.2 Spec-C \nWe achieve reliability by a checkpoint/backup scheme. At the most concrete level, the scheme includes \na running pro\u00adcess R, a stable storage S, and a communications layer CL. The running process R implements \nthe server, including its propensity to fail; it is also willing to transmit its state along a special \nchannel. The stable storage S holds a previous state of the server; when the running process fails, a \nnew copy of the running process is started from the saved state. The stable stor\u00adage is assumed not to \nfail. The running process should not get indefinitely far ahead of the saved state, so we insist that \nbackups happen at most 8 server actions apart, where 8 is an arbitrarily-chosen suitable number. The \ncommunication be\u00adtween the running process and stable storage generates some extraneous noise; the communications \nlayer CL shields the client from this noise, and also buffers actions that have not yet been committed \nto stable storage. CL, too, is assumed not to fail. The concrete specification makes all of this explicit. \nSpec-C = T. R ]S ICL (4) ( ) where R, S, and CL are terms describing the components of the system, and \n] and I are suitable parallel composition op\u00aderators describing the communication mechanism available. \nR, S, and CL are straightforward code and will be pre\u00adsented in the full version of the paper. Indeed, \nthey are as close to C code as process algebra usually gets; if p is simple enough, an implementation \ncan be read off of Spec-C. Even if p is complicated, the terms R, S, and CL explain the state transitions \nof the components of the implementation. In particular, Spec-C is likely to be useful for implementers. \nOf course, Spec-C is not suitable for explaining the sys\u00adtem to users. It is not at all clear from reading \nSpec-C that Spec-C behaves like a non-failing version of p. Indeed, p does not appear in the text of \nSpec-C (although p has been 4Strictly, our rules do not have side conditions; we simply have one such \nrule for each a G A. Also, NoFail(p) could be phrased p\\{f} in CCS at least, if f were considered a \ncommunication action but thk is irrelevant as far as the methodology goes. NoFail(p) P b a Pb o NoFail(p~) \nNoFail(pa) cd P. /1 /1 o Pbd o 0 0 NoFail(p*~)    l\\f a o0 0 Figure 1: p and NoFail(p) chopped up \nand folded into R). Spec-C is useful for imple\u00admenters, but vastly inferior to Spec-A for clients. The \noperations j and I are defined in the obvious way, to enforce the constraints in our informal specification \nof the problem. For example, R i S describes the interaction of the running process and stable storage: \nthat is, that they run in parallel, communicating by stat e transfer actions, and R may fail, stop, and \ninform S of the fact. The first rule for i allows either process to act independently except for failure \nand state transfer. The second rule says that, if z fails, it dies and y is informed of its failure. \nThe third rule says state transfers must be synchronized upon, and the synchronization results in a checkpoint \naction. x , x : ma@ {f, t.} f z + z , yty ~.lv+~ lv ylx~yix  Ziy% tt z -.$ x . II -$ d  ~[ysx iy 4.1.3 \nSpec-B We bridge the gap between Spec-A and Spec-C by present\u00ading an intermediate specification, Spec-B. \nSpec-B gives an abstract definition of the concept checkpointed process , Oh(p), but leaves the communication \nlayer concrete. Spec-B = Ch(p) I CL. (5) The definition of the Ch(p) operation is typical of how one \nprograms wit h SOS rules. We need some special-purpose actions in the protocol, shown in Figure 2. We \npresent the development of the Ch(p) operation as it happened in our research. We first invent an operator \n. was . that does unbounded checkpointing; that is, which may wait arbitrarily long before the checkpoint \nis done. We next modify it, giving an operator . wa Sn . the constraint that checkpoints happen no more \nthan our arbitrary limit of 8 steps apart. Then, we introduce a message numbering operation n umn (.), \nas the communications layer CL uses message numbers. Finally, we combine these to give Ch(p). Unbounded \ncheckpoint The essence of a checkpoint\u00adbackup scheme is that there is a running process, and a saved \nprior state of that process. So, we invent an operator . was., wit h the intent that x was y represents \na checkpoint ed pro\u00adcess with running process z and saved state y. The rules for x was y are easy to \ninvent. First, if the running process z does any proper server or internal action, then z was y does \nthe same thing, leaving the saved state unchanged: x$x , a~AU{rl xwasy~xfwasy The second rule concerns \nz failing. If z fails, the system kills z (indicated in the rule by the absence of z and x on the right-hand \nside of the transition). A new running process is started, initially in state y. The system as a whole \nperforms a r action to indicate that the restore happened. X4X xwasy~ywasy Finally, at any time, the \nsystem may perform a check\u00adpoint, copying the stat e of the running process onto the stable storage, \nand producing the action c. Neither x nor y participates in this (as indicated by the lack of antecedents); \nit is performed entirely by the system. xwasyzxwasx Bounded checkpoint To model bounded checkpoints, \nwe add a counter to the was operation. Formally, we define a family of operations . wa Sn . for O < n \n<8, where n is the number of actions remaining before a checkpoint is required. It is a straightforward \nmatter to update the previous rules to take account of the counter. We will also need to add message \nnumbers (in a later section). Let a#m be a numbered with n; for convenience, let -r# =-r. Let Ax = {a# \nla~ A,n ~ N} f p s failure c Checkpoint action r Action performed as the system restarts t, State transfer \nof state s. T Silent action (standard) aGA ordinary server action Figure 2: Special-purpose actions \nused in Spec-B and Spec-C The first rule says that the running process z can perform server actions only \nif t here the counter is > O; that is, only if actions remain before the next checkpoint must occur. \nEach visible server action decrements the counter and leaves the stable state unchanged. There is one \nsubtlety: the invisible action ~ cannot be seen, and hence was~ cannot count it. This rule is required \nby the RBB Cool format; without it (orifthe ~-rule decrementedn) theoperations would not respect rooted \nbranching bisimulation.5 ~ $$ , a ~ A# {7}, n>O (6) zwas. v4 x was._l y Z:z . x was. v -+ x was. y \nThe next rule handles failures. As before, the system kills z and spawns a copy of y. It further resets \nthe counter to its maximum value 8, because at this point it is known that the saved process matches \nthe running one. f Z+x (7) X wasm y z ywasE y As before, checkpoints may happen at any time, and x is \ncopied into the stored state, and is still the running process. The timer is reset to its limit 8 on \neach checkpoint. Xwas% Y z XW&#38;% X (8) This operation cannot be defined in general CCS, CSP, or most \nother process algebras. Numbering Server Actions Our protocol relies on message numbers. However, we \nhave not assumed that the base server p numbers its messages. Instead, we define an operator n u m~ (p), \nwhich at t aches numbers to p s messages, starting from n. If p fails, n u mfi (p) simply transmits failure \nand dies. As before, n u m~ (.) cannot observe T actions; it has to transmit them unchanged. The rules \nare easy enough: f x~x ,a~A x+x numm(z) A Onum.(~) ~ num.+l(x ) x:x num. (z) L num. (z) 5The defini~i~~ \nof the ~ule format reminds us that the ~-rule s necessary and dictates its form. No ingenuity is required. \nPutt ing it all t oget her We are now able to define the checkpointing operation we want, as a simple \nRBB Cool operation. The running process and the saved state are initially the same: both are numo (p), \nthat is, the basic server process p, with its actions numbered from O. 6 h(p) 3 numo(p) wasE numo(p) \n(9)  4.1.4 Relating the Specifications Having an intermediate specification allows us to prove that \nSpec-A z Spec-C, under suitable determinacy hypotheses. We could prove this by brute force. However, \nthe interme\u00addiate specification Spec-B allows us to make the proof easier and cleaner. First, we prove \nSpec-A = Spec-B, (lo) This is proved by the branching bisimulation relation method discussed in Section \n3.3.1. The proof is routine, easy, and boring: it involves calculating when transitions are possible, \nand then finding matching transitions in each case. The fun\u00addamental theorems of RBB Cool tell that branching \nbisim\u00adulation is a suitable notion of equivalence for this language. Then, we show that the abstract \nconcept of checkpoint\u00ading agrees with the implementation, proving that  Ch(p)= ~is. (11) For variety, \nwe prove this by equational reasoning as pro\u00advided by Grand, as discussed in Section 3.3.2. Again, the \nproof is a routine and easy use of equational methods. Note that this does not mention the communication \nlayer CL, which is the same in Spec-B and Spec-C. This equation holds up to strong bisimulation, and \na fortiori up to rooted branching All operations in this setting are compositional. Parallel\u00adcomposing \nboth sides of (11) with CL, we get ch(p)/cL=RislcL (12) which, by the definitions of Spec-B and Spec-C, \nis simply Spec-B -Spec-C, (13) From (10) and (13), we easily conclude Spec-A E Spec-C (14) 5 Conclusion \nWe have introduced the concept of a Protean specification language, which includes a definitional mechanism \nallowing the introduction of new operations as desired for expression or clarity, yet guarantees that \nthey respect the theoretical and methodological properties of the language. This concept may be applied \nin general, though building a single Protean specification language is a rather tricky matter. We have \npresented two Protean process algebras: grand and RB B COOI, suited for strong and rooted branching bisimulation \nrespect ively. We have given an extended example, of a reliable server, to illustrate Protean methods. \nWe gave three specifications: abstract, intermediate, and concrete. The abstract specifi\u00adcation is suitable \nfor clients; the concrete one is suitable for programmers. The three specifications are easily shown \nto be equivalent using proof principles validated by the grand and R B B Cool languages. Acknowledgements \nWe would like to thank Alan Fekete for patiently answering many naive questions about checkpointing and \nsuggesting several questions. Discussions with Ashvin Dsouza, Carl Gunter, Leslie Lamport, Albert Meyer, \nPravin Varaiya, Sam Weber and others have been valuable for this paper, as have the comments of anonymous \nreferees. A Definitions for Branching Bisimulation This section contains the definitions of branching \nbisimula\u00ad tion, and sufficient conditions for being in the RBB Cool format. Let Act be the set of all \nactions other than ~, and Act, = Act U {T}. Definition A. 1 w is a branching bisimulation relation zf, \nfor all actions ~ 6 Actr and for all p -p , then 1. If p 2 r, then eitheT: (a)~=rand r~p , or (b) there \nare q , r such that p ~ q ~ r , p w q , andru r : 2. Vice versa. Two processes p and p are branching \nbisimilar, p *6 p , zf there exists a branching bisimulation relation w with p w P . Definition A.2 Two \nprocesses p and p are rooted branch\u00ading bisimilar if, whenever p $ q, then p ~ ql and q -b q ; and vice \nversa. The full definition of RBB Cool languages is designed to be extremely broad, to admit operations \nwhich make sense theoretically but seem to be of little use in specifications. The details are found \nin [B1093]. For the purposes of this survery and, we suspect, for most other purposes the following \nsimplified format will suffice. The simplified for\u00ad mat for rooted branching bisimulation is based on \na format for (non-rooted) branching bisimulation. Weakened bisimulations ignore ~ moves whenever possi\u00ad \nble, and so our formats are based on rules which allow ~ s to pass wit bout comment. Such rules are called \npatience rules: that is, rules which say that an argument or a subterm of a term is able to take ~-moves \nas it wishes. Definition A.3 If t is a term and x w a varzable in t,then the patience rule for x m t \nts the (possibly derived) rule XL?J where y is a fresh varzabie. Similarly, zf f is a function symbol \nwith arity k, the patience rule for argument i (for I < i < k) of f is is the patience rule for x. in \nthe term fax,,..., Xt,Xk),Xk). For example, the inst antes of the parallel composition rule (3) with \nu =7-are patience rules for NoFail(.). Definition A.4 A process calculus .C defined by SOS rules is simply \nBB Cool if 1. All rules p for all opeTation symbols f of L have the form: z, 3 y,li 6 I(p) p= fez,,..., \nzn)Lt where I(p) is a set of numbers telling which aTguments of f take actions under rule p. 2. For all \nrules p, and all i ~ I(p), L contains a valid patience rule for aTgument i off. 3. No rules have hypotheses \nmentioning r s, except the patience rules required by clause 2.  The operation + (2) does not have \npatience rules, and does not respect branching bisimulation. All the standard operations, including +, \nrespect the somewhat finer oper\u00adation of rooted branching bisimulation. Intuitively, rooted branching \nbisimulation does one step of strong bisimulation, and then is branching bisimulation. The rule format \nRBB Cool matches that intuition: operations may use one step of grand rules (which respect the one step \nof strong bisimula\u00adtion), and then BB Cool rules. Definition A.5 A grand language L is simply RBB Cool \nifl the operation symbols can be partitioned into tame and wild operations, such that 1. The sublanguage \nof L consisting of only tame opera\u00adt~ons is szmply BB Cool, and 2. Let f(Z) 4 t be the conclusion of \nany de of L. Then t mentions only tame operations.  The CCS rules of Section 2 are all RBB cool and \nthus grand, though they barely scratch the surface of the pro\u00adgramming power of RBB coolness. All the \noperations pre\u00adsented in this paper and indeed most of the operations in the literature are RBB cool. \nB Appendix: Process Definitions B.1 Description of R and S For simplicity, we present the definitions \nof the concrete server processes in the setting without silent moves. With determinacy, the states of \nthe server may be identified with the strings of server actions: p. is the state after the actions s. \nThe first summand of the running process R acknowl\u00adedges that it may fail at any time. The second summand \nof R handles state transfer; the third summand allows R to perform any of p s enabled actions, numbering \nit, and decrementing its counter. The first summandof Sreceives state transfers. The second summand handles \nrestarts and spawning of other processes. R(s, j) = f+t..R(s,8)+ (15) + a#l sl.R(sa, j 1) (16) z c J>i)AP. \n+P.. R = R(e, 8) (17) S(s ) = ~ ts.S(.s) + r.(R(s , 8) S(s )) (18) S6P(S)) s = s(e) (19)  B.1.l The \nCommunication Laye The code for CL is in Figure 3. It has four state components: 1: The message number \nat the time of the last checkpoint. Messages numbered 1 and below are in stable storage at the server, \nand will never need to be replayed by the communications layer. m The number of the message the server \nlast remembers transmitting; if the server has not just crashed, this is the number of the last message. \nn: The largest message number received. This represents the most evolved state of the computation. Note \nthat 1 ~ m ~ 72 at all times. v: The buffer of unstable messages, which we represent as a string of (unnumbered) \nserver actions. The commu\u00adnication layer only stores actions that it might need to retransmit; that is, \nthe messages numbered 1 + 1 through n, with z@] the first element of v being message 2 + 1. If there \nare messages to be retransmit\u00adted (that is, if m < n), then the next message to be retransmitted is v[m \n1+ 1].If the server does a check\u00adpoint in a state in which it is fully caught up, then the communication \nlayer clears v. More generally, if the server does a checkpoint, then messages numbered 1 through m are \nstable, and the communications layer can drop them from the buffer. The code for our communications layer \nfollows. There are three classes of processes, each with some state information: CL(1, m, n, u) is the \nbasic communication layer; the compo\u00adnents are as above. It is ready to accept messages from either client \nor checkpointed server. CQ(a, 1, n, n, v) is the communication layer after receiving the query message \na from the client, CL will only accept messages if m = n, which is why the argument n appears twice. \nCQ S next communication action is with the server, and so CQ must handle server crashes and checkpoints; \nin both cases, the query a is added to the buffer. CR(b, 1, n, n, v) is the communication layer after \nreceiving the response message b#m+l from the checkpointed, numbered server. CR will communicate with \nthe client next. It does not need to handle server crashes and checkpoints; the asynchronous communication \nbetween communicant ions layer and server will delay server s crash and checkpoint messages until the \ncommunica\u00adtion layer has finished sending b to the client. References [Abr87] [ABV92] [AL91] [Bae90] \n[BB87] [BIM88] [BK92] CL = CL(l,l,l, C) cL(l, rn,7z,7J) = ifm=nthen~ ~<~qu~rv a. C Q(a, 1, n, n, v) --If \nserver current, accept a query from the client + if m = n then ~~,-~r,~, b#n+l.CR(b,l, n,n, v) --If server \ncurrent, accept a response from the server  + if m < n then v[m l+l]#~.CL(l, m+l, n,v) --If server recovering, \nsend/receive next message of recovery  + c. CL(m, m,n, v[(m i+ l)... (ill)])]) --Handle checkpoint, \ntrimming buffered messages  + r. CL(l,l, n,v)  (20) --Handle server crash, noting that buffered messages \nneed to be replayed CQ(a,l, n,n, v) = a#~+l.CL(l, n + I,n + l,va) --Forward query to server, add it to \nbuffer + r. CL(l,l, n + l,va) --Handle server crash, add query to buffer.  + c. C_L(rz, n,n + l,a) \n  --Handle checkpoint, add query to buffer. Cll(b,l,n,n,v) = b. CL(l, n+ l,n+ l,vb) --Forward b to client, \nadd it to the buffer. Figure 3: Code for CL [B1089] Bard Bloom. Ready Simulation, Bisimulation, and \nthe Semantics of CCS-Like Languages. PhD Samson Abramsky. Observation equivalence as a thesis, Massachusetts \nInstitute of Technology, Au\u00ad testing equivalence. Theoretical ComputeT Sci., gust 1989. 53(2/3):225-241, \n1987. [B1093] Bard Bloom. Structural operational semantics Luca Aceto, Bard Bloom, and Frits Vaandrager. \nfor weak bisimulations. Technical Report TR 93- Turning SOS rules into equations. In F%oceedings 1373, \nCornell, August 1993. To appear in Theo\u00ad of LICS 92, pages 113 124. IEEE Computer So\u00ad retical Computer \nScience. ciety Press, 1992. [BM92] Bard Bloom and Albert R. Meyer. Experimenting Martin Abadi and Leslie \nLamport. An old\u00ad with process equivalence. Theoretical Computer -fashioned recipe for real time. In de \nBakker, Sci., 102(1):223-237, November 1992. Huizing, de Roever, and Rozenberg, editors, Real- Time: \nTheory in PTactice, pages 1 27. Mook, the [BW90] J. C. M. Baeten and W. P. Weijland. Process Al- Netherlands, \n1991. gebra. Cambridge Tracts in Theoretical Computer Science 18. Cambridge University Press, 1990. J. \nC. M. Baeten. Applications of Process Alge\u00ad bra. Cambridge Tracts in Theoretical Computer [Hoa85] C. \nA. R. Hoare. Communicating Sequential Pro- Science 17. Cambridge University Press, 1990. cesses. Series \nin Computer Science. Prentice-Hall, 1985. Tommaso Bolognesi and Ed Brinksma. Introduc\u00ad tion to the 1S0 \nspecification language LOTOS. [LT87] Nancy A. Lynch and Mark R. Tuttle. Hierarchi- Computer Networks \nand ISDN Systems, 14:25-59, cal correctness proofs for distributed algorithms. 1987. In Proceedings of \nthe Sixth Annual ACM Sym\u00ad posium on Principles of Distributed Computing, Bard Bloom, Sorin Istrail, and \nAlbert R. Meyer. pages 137 151, 1987. Bisimulation can t be traced (preliminary report). In Conference \nRecord of the Fifleenth Annual [Luc90] David Luckham. Programming with Specifica- ACM Symposium on Principles \nof Programming tions: an introduction to Anna, a language for Languages, pages 229 239, 1988. Also appears \nas specifying ADA programs. Springer-Verlag, 1990. MIT Technical Memo MIT/LCS/TM-345. [Mi181] Robin Milner. \nA modal characterisation of ob- Bard Bloom and Mart a Z. Kwiatkowska. servable machine-behaviour. In \nE. Astesiano and Trade-offs in true concurrency: Pomsets and C. Bohm, editors, CAAP 81: Trees in Algebra \nMazurkiewicz traces. In S. Brookes, M. Main, and Programming, 6th Colloquium, volume 112 A. Melton, M. \nMislove, and D. Schmidt, editors, of Lect. Notes in ComputeT Sci., pages 25 34. Mathematical Foundations \nof Programming Se\u00ad Springer-Verlag, 1981. mantics, volume 598 of Lect. Notes m Computer Sci. Springer-Verlag, \n1992. [Mi189] Robin Milner. Communication and Concurrency. Prentice Hall International Series in Computer \nScience. Prentice Hall, New York, 1989. [MP83] Zohar Manna and Amir Pneuli. How to cook a temporal proof \nsystem for your pet language. In Conference Record of the Tenth ACM Symposium on Principles of Programming \nLanguages, pages 141-154, 1983. [PP88] G.D. Plotkin and V.R. Pratt. Teams can see pomsets. Manuscript \navailable as pub/pp.tex by anonymous FTP from Boole.St anford.EDU, Oc\u00adtober 1988. [vG93] Rob van Glabbeek. \nThe linear time -branching time spectrum II: The semantics of sequential pro\u00adcesses wit h silent moves. \nIn Eike Best, editor, Pro\u00adceedings of CONCUR 93, pages 66 81. Springer-Verlag, 1993. LNCS 715; paper \nalso available by anonymous ftp from Boole. stanf ord. edu. [vGW89] Rob van Glabbeek and W. P. Weijland. \nBranching time and abstraction in bisimulation semantics. In G. X. Ritter, editor, Information Processing \n89: P? oceedings of the IFIP Ilth World Computer Congress, pages 613 618. North-Holland, August 1989. \nTUM report 19052 (= SFB-Bericht Nr. 342/29/90 A) and CWI report CS-R9120. [WBB92] Sam Weber, Bard Bloom, \nand Geoffrey Brown. Compiling Joy to silicon: A verified silicon com\u00adpilation scheme. In Tom Knight and \nJohn Savage, editors, Proceedings of the Advanced ReseaTch in VLSI and Parallel Systems Conference, pages \n79 98.1992.  \n\t\t\t", "proc_id": "199448", "abstract": "<p>Standard specification languages have very limited abilities to define new operations on processes. We introduce the concept of a Protean specification language, with general definitional facilities supported by the appropriate theory. Protean languages allow elegant, readable, and useful specifications at all levels of abstraction. A good Protean specification language will admit methods of verifying that one specification is a refinement of another. We sketch a family of Protean specification languages (with references to the full details) which allow a vast amount of expressive power in defining operations, but nonetheless have all the essential theoretical and specification power of CCS and ACP.</p><p>We illustrate these techniques by presenting several specifications of the job of protecting an arbitrary server by a checkpoint/backup scheme. The high-level specification of the protected server simply says, &#8220;It does everything it did before, and it doesn't crash.&#8221; The middle-level specification describes checkpointing cleanly and abstractly, without prescribing any particular implementation. The low-level specification is fairly close to an implementation. We show the high- and medium-level specifications equivalent by bisimulation relation techniques, and the medium- and low-level specifications equivalent by equational reasoning using automatically-generated equations. We also show that the operations expressing checkpointing behavior are not definable in standard process algebras.</p>", "authors": [{"name": "Bard Bloom", "author_profile_id": "81100272990", "affiliation": "Cornell University, Ithaca, NY", "person_id": "PP14102884", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/199448.199472", "year": "1995", "article_id": "199472", "conference": "POPL", "title": "Structured operational semantics as a specification language", "url": "http://dl.acm.org/citation.cfm?id=199472"}