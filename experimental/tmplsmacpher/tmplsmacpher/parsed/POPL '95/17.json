{"article_publication_date": "01-25-1995", "fulltext": "\n The Geometry of Interaction Machine Ian Mackie* Department of Computing Imperial College of Science, \nTechnology and Medicine London SW7 2BZ England im@doc.ic.ac.uk Abstract We investigate implementation \ntechniques arising directly from Girard s Geometry of Interaction semantics for Linear Logic, specifically \nfor a simple functional programming lan\u00ad guage (PCF). This gives rise to a very simple, compact, com\u00ad \npilation schema and run-time system. We analyse various properties of this kind of computation that suggest \nsubstan\u00ad tial optimisations that could make this paradigm of imple\u00ad mentation not only practical, but \npotentially more efficient than extant paradigms. 1 Introduction There are a number of established implementation \ntech\u00adniques for functional programming languages. Broadly speak\u00ading these are based on graph rewriting \n[Wad71 ], stack ma\u00adnipulation, for example the SECD machine [Lan64], and buffered data-flow [FH88]. In \nthis paper we present the Geometry of Interaction Machine-an implementation tech\u00ad nique based on sequential, \nrather than buffered, data-flow. The significant features of this approach include: . Compact object \ncode. Small and compact run-time system. e Sound semantic foundation arising from Linear Logic [Gir87] \nand the Geometry of Interaction [Gir89b, Gir89a]. Semantics has played a crucial r61e in the past by \npro\u00adviding a tool for proving (static) properties of programs and implementations. A new paradigm in \nsemantics developed by Jean-Yves Girard under the name Geometry of interac\u00adtion can now allow the study \nof the dynamics of computa\u00adtion. His motivations were to provide mathematical tools for the study of \nthe cut-elimination process in Linear Logic proof structures; but the fundamental ideas are applicable \nto computation in general. The prominent features of his programme can be summarised as follows. Research \nsupported by an UK SERC Studentship and ESPRIT Basic Research Action 6454 (CONFER Permission to copy \nw thout fee all or part of this material is granted provided that the copies are not made or distributed \nfor direct commercial advantage, the ACM copyright notice and the title of the publication and Its date \nappear, and notice is given that oopyin is by permission of the Association of Computing Machinery. 7 \no copy otherwise, or to republish, requires a fee arrdlor specific permission. POPL 95 1/95 San Francisco \nCA USA @ 1995 ACM 0-89791 -892-1/95/0001 ....$3.50 The semantics is syntax free in that it avoids the \nbu\u00adreaucratic issues that come with the notion of substi\u00adtution (cf. the A-calculus ). o The semantics \nis chiefly a study of the dynamics of the cut-elimination process (hence ~-reduction) using denot ational \ntools. The model is termed dynamic since it captures the essential computational cent ent. The semantics \nis operational in the sense that there are a set of prescribed rules specifying how computation should \nproceed at each step. It is hoped that this new understanding of the dynamics of computation could lead \nto radically new implementation techniques for programming languages, we hope that this paper provides \na contribution towards this goal. In par\u00adticular the dynamics give a highly decomposed notion of a reduction \nstep such that the operational handling of in\u00adformation is linear and even invertible something that \nis certainly not true about standard implementations of ,f?\u00adreduction. We shall demonstrate in this paper \nthat this idea can be trivially implemented. The philosophy behind this semantics is a notion of in\u00adformation \nflow through a program, The meaning of a pro\u00adgram can be regarded as a set of information flows which \nare called paths in the program. As reduction proceeds one would expect that the information flows are \ninvariant no new information flows can be created, and none can be lost. The meaning of a program is \ngiven by a set of paths, and computation is given by a calculating the transitive closure of these paths. \nGirard haa a useful analogy of thinking of these paths as electrical circuits as reduction proceeds the \nelectrical connectivity y is preserved. We refer the reader to the work in [ADLR94, DR93] for detailed \nwork on the notion of paths in the A-calculus; and [Mac94] for the extension to cover PCF. The applications \nof this theory have been substantial. It has been the basis of the solution to a number of out\u00adstanding \nproblems in Computer Science, for example the Full abstraction for PCF [AJM94], and it has shed new in\u00adsight \ninto existing problems, for example optimal reduction for the A-calculus [GAL92], etc. The purpose of \nthis paper is to extend the Geometry of Interaction interpret ation to a simple functional program\u00adming \nlanguage (PC F) and show how simply we can imple\u00adment this semantics directly, One of the main hopes \nof the Geometry of Interaction is to obtain a deeper understanding of the dynamics of computation and, \nhopefully, to give new insights into implementation techniques and optimisations. Acknowledgement We \nsee this paper as a contribution to this work in that we: Provide an extension of the Geometry of Interaction \ninterpretation to cover a simple functional program\u00adming language (PCF).  Give a concrete implementation \nof this semantics by compiling it directly into assembly language.  Provide some optimisations of this \ncompiler technol\u00adogy arising from properties of the semantic framework.  Additionally, this compilation \ntechnique provides a very clear computational understanding of Girard s Geometry of Interaction. Related \nwork This work is about implementing Girard s Geometry of In\u00adteraction. Two related pieces of work are \nGonthier et al. for their work on Optimal reduction and Danos and Regnier for their work on Virtual reduction. \nBoth of these pieces of work are related to computing paths . . Optimal reduction: The algorithm for \nimplementing optimal reduction presented in [GAL92] uses the Ge\u00adometry of Interaction as a proof technique \nto show that all paths are preserved under reduction. One can see each local rewrite rule of there system \nas computing a set of paths. . Virtual reduction: The work of Danos and Regnier [DR93] looks at computing \npaths in a more direct way, and can be seen as a decomposition of the work on op\u00adtimal reduction but \nstill rewriting the graph, hence again their computation is using the Geometry of In\u00adteraction to show \ninvariance of reductions. In this paper we compute the execution path directly, by performing a simple \ntoken pushing semantics. In particular we do not rewrite the structure of the graph we provide a compilation \nof programs directly into assembly language. Finally we remark that there is also some recent work of \nDanos and Regnier that is in the same spirit of this work, but we are unable to comment on the relationship \nat the present time. Overview The rest of this paper is organised as follows. First, we recall the syntax \nof PCF and show an encoding into Linear Logic proof structures. Section 3 gives the theoretical foundation \noft he implementation of PCF. In Section 4 we give the main contribution by showing how we can effectively \nimplement this semantics in a very simple way in assembly language for a simple machine. We will explain \nthe concept of a path alongside the compilation that we will perform. Section 5 discusses efficiency \nand we show how we can improve the implementation by a time-space trade off. We conclude our ideas in \nSection 6 by suggesting some applications of this kind of implementation and discuss some further directions \nof this work. I am most grateful to Vhcent Danos, Mark Dawson, Simon Gay, Radha Jagadeesan and Pasquale \nMalacaria for discus\u00adsions on this work, and to the anonymous referees for their detailed comments. 2 \nPCF in Linear Logic proof structures We first recall the syntax of PCF, (Programming Language for Computable \nFunctions) [P1077], which we take to be standard. We have a set of variables x : u, a set of con\u00adst ants \nc : a, an abstraction kc. M and application MN. The constants that we shaII use are naturaJ numbers: \nn : nat, booleans: ti, ff : bool; together with arithmetic functions over these constants: SUCC, pred \n: nat + nat and iszero : nat + bool, a conditional cond : bool + a + a + C, and a fix point combinator \nY : (u + o) + a. Linear Logic provides a decomposition of the functional arrow A + B into !(A -o B): \n The type reflects the fact that functional application is a two phase process a linear (-o) part that \nsuggests a single usage of the argument, and a non-linear part reflected by the modality ! (the exponential) \nwhich al\u00adlows multiple usage of the argument,  This in turn induces a decomposition of the A-calculus \nin which both the abstraction and application are de\u00adcomposed into more primitive operations.  Information \nis made explicit in the proof structure the previously hidden structural rnles of Weakening (discarding \nan argument) and Contraction (copying an argument ) are made explicit.  Finally, it provides a new way \nof representing terms in the calculus-Proof Nets which have strong con\u00adnections with standard graph representations \nof the A-calculus .  There are several codings of the A-calculus into Linear Logic proof net structures \nin the literature [Dan90, GAL92]. Here we will very briefly recall one translation, (which is based on \nembedding the intuitionistic arrow A ~ B into !(A + B)), and give the extension of this translation to \ncover the constants of PCF. Variables The variable x is translated into an am om link, which is represented \nsimply by a piece of connecting wire) , drawn simply as: Abstraction and Application The abstraction \nkc. M and the application MN are trans\u00adlated into the following nets as shown in Figure 1. Intuitively \nwe can see these as standard graphs for the ~-calculus where we have decomposed abstraction (A) into \na par (8) and the exponential (!). The exponential of linear logic requires a notion of a box, which \nwe have drawn around the abstrac\u00adtion connecting the ! node and the ? nodes. We call the top of the box \n! the principal door, and the free wires that exit the box from the bottom are called the auxiliary doors \nwhich we draw as (?) nodes. Application (0) is decomposed into a Tensor (L3 ) and Dereliction (D). The \nDereliction ii connected to the trans\u00adlated function M by a cut link (drawn as a piece of con\u00adnecting \nwire), and the result of the application (the top of the Te-nsor) is connected to the rest of the structure \nby an axiom link. The main point of difference with standard graphs is that the bindirw variable is connected \nto its occurrence in the term. If no such variable exists then we represent this as a terminal node to \nthe net, called a Weakening node. Mul\u00adtiple occurrences of a variable are made explicit using the triangular \nContraction (sharing) node. We have also labelled some of the edges of the struc\u00adture that will be used \nfor the Geometry of Interaction inter\u00adpretation that we will present shortly. All other edges are assumed \nto have a label 1. Constants at base type Constants at base type, c ~ {E, tt, ff}, are represented as \nterminals of the net structure. For this translation we re\u00adquire that the constants are boxed values \nwhich we draw as follows: ,------..., Ci I Arithmetic functions The constant functions, ~ c {SUCC, pred, \niszero}, are rep\u00adresented in a similar way to axiom links, i.e. a piece of con\u00adnecting wire, but labelled \nwith the corresponding function: r7f The orientation of this should be thought of as taking inputs from \nthe left hand side, and the result on the right hand side. Under the chosen translation, it is necessary \nto package these up with the constructors of linear logic. Specifically, we need to do the following: \n,---------------------~ (, , / : ----------------I  !;-+ 1,[~ ,! 1, 1, (~D 1,, I Hence we can see this \nstructure as a boxed Dereliction, and then a par and a box (hence a ~) has been added to close up the \nstructure. Conditional The conditional of PCF is represented as the following graph structure: cond where \nwe label the true and false branches with Iabels tt* and H* respectively. The leftmost argument is the \nboolean test, and the result is at the top of the structure. Again we are required to package this coustant \nup in Linear Logic connective. The idea is the same as the arithmetic functions in that we close the \nconstruction up using the codings for J. Recursion We code recursion without the need to introduce anv \nnew nodes by generating a cyclic structure that explicitly ties the knot . We can now get the coding \nof the PCF constant Cl as Y1. The following diagram shows the graph represen\u00adtations of Y and fl. This \ncorresponds exactly to the coding of recursion in graph reduction, see [Pey87]. The path computation \nse\u00admantics is therefore given in terms of the rules for Tensor, Dereliction and Contraction. 3 The Geometry \nof Interaction interpretation The proof net structures have been annotated with directed labels p, q, \nr,s, t,d which are the generators of the dynamic algebra A* used for the Geometry of Interaction. We \nwill call the extension to cover PCF constants A&#38; [Mac94]. A* is a single sorted Z algebra. We write \nz and y for the variables, and alf other symbols O, l,p, q, r,s, t, d : X are constants of the theory. \nThere is a associative multiplication operator . : Z x X + X, (that we will omit), which has unit 1 and \nabsorbing element O. The theory is equipped with an involution * : X -+ X, which is used to label edges \nas we travel in the opposite direction, and an exponential operator ! : z + X, that is used to label \nall the labels that are inside a box structure. The following equations define the properties that we \nrequire. We present the algebra, which is essentially taken from [Dan90]. P Figure 1: Coding the A-calculus \no*=!(o) = o 1 =!(1) = 1 Oz=zo=o lZ=Z1=Z !(z) =!(z*) (q) = y z (Z*)* = z !(z)!(y) =!(Z?J) Annihilation: \nThere are six constants to consider: p and q are the multiplicative coefficients, r and s are the Contrac\u00adtion \ncoefficients, d is the Dereliction coefficient, and finally, t is the auxiliary door coefficient. p*p=q*q=l \nq*p =p*q = 0 d d = 1 T*T=s*s =1 s *T=T*s=(1 t t =1 Communication: The exponential coefficients interact \nwith the exponential morphism as defined by: !(Z)T = r!(z) !(Z)S = s!(z) !(z)t= t!!(z) !(z)d = dx The \nadditional constants which we need for PCF are: natural numbers n, which we shall write as E to avoid \ncon\u00adfusion with the constants O and 1; and boolean constants tt, tt* and ff, ff*, together with function \nconstants SUCC, pred and iszero. Hence we are overloading notation by using the same names for constants \nin A~C~ and PCF. We have the following annihilation equations: tt tt = fi*fi = 1 ff tt = M ff = 1) iszero \nG =tt iszero n + 1 = ff pred D =ii predn+l = E Succ ii = n+l The whole concept of this semantic interpretation \nis to use this theory to talk about which paths are the good ones in the proof net structures. Proof \nnets are graphs G = (V, E) in the usual sense. The vertices V are the logical symbols (for example @ \nand T(M) T(N) ... r into Linear Logic proof structures w in the multiplicative fragment) and the edges \nE are the connecting links that join these symbols to their premises and conclusions. There is a natural \nnotion of a walk in a graph which is nothing more than a travel around the structure. A walk in a graph \nis specified uniquely by a sequence of vertices. Here we take a slightly different perspective on these \nstandard notions. First we label the edges of a proof net rather than the vertices. The labels that \nwe use are elements of the algebra A&#38;.  Consequently the walks in a proof net are specified by a \nsequence of edges rather than vertices.  We are now in a position to define the notion of a path in \na proof net structure. Definition 3.1 A path $ in a proof net structure is a walk along the labelled \nedges such that A~C~F 4 # O. Intuitively, we can think of the algebra restricting the way that we can \nwalk over the structure. The Geometry of Interaction interpretation is nothing more than a mechanism \nfor picking out all the good paths in the term which can be characterised as being the paths that survive \nthe action of reduction. Hence the Geometry of Interaction semantics can be seen as a decomposed way \nof looking at reduction. There is a particular (unique) path that exists in a PCF program at base type \nthat starts and finishes at the root of the graph. This will be called the execution path. We will use \na model of this algebra based on simple data structures that will be the basis of the run-time system \nof our implement ation. A context is a triple (M, S, D) where &#38;t is the multiplicative stack, f is \nthe exponential tree, and V is the data value (a single element stack). These are defined by the following \ngrammar: M ::= l: Mlr:Mltl .5 ::= L:~/R:g/ <t,: >lrl D ::= ~ltt[fflo where we write 0 for the empty stack. \nWe define the carrier of the model to be the partial in\u00adjective functions on (M,&#38;, D). The operations \nare interpreted as: [ ] IS defined to be the composition of partial injective functions. * [~*] is defined \nto be the inverse partial injective func\u00adtion [f]-l. [!] is defined as: IP(.f)l(m,< el, e2 >,7J) = let \n(m , ej, U) = [~ll(rn,e~, v) in (m , < el,e~ >,cZ) The generators of the algebra are interpreted as \nfollows: Constants ~1] is the identity and Io] is the nowhere defined context transformer on (M, f, D). \nMultiplicative = (l:m, e,v) [~l(m, e,~) = (r :m, e,v) Exponential ~l(~jej~) [r-](m, < el,ez >,0) = (m, \n<L:el, e2>, v) [s](m, < e~,e~ >,w) = (m, <f/: e~, e2>, ~) [i!](m, < e~, < e.2,e~ >>, v) = (m,<< el,ez \n>,es >,v) [d](m, e,v) = (mj<U, e>, v) PCF Constants [tt](m, e, Cl) = (m, e,tt) [tt*](m, e,tt) = (m, \ne, El) [ff](m, e, Cl) = (m, e,ff) [fFJJ(m, e,ff) = (m, e, Cl) [iszero](m, e, O) = (m, e,tt] [iszeroj(m, \ne,n + 1) = (m, e,ff) [predjj(rn, e, O) = (m, e,O) [Predl(m, e,n + 1) = (m, e,n) [~l(m,e,n) = (m, e,n) \n[succ](m, e, n) = (m, e,n+l) Proposition 3.2 The context transformer. on (M, ~, D) yield a sound model \nof the algebra A~C~. 4 A sequential data-flow machine The basic notion considered here is that of data-flow. \nMore specifically, pushing a szngle token around a fixed network. The network is a proof net structure; \ngenerated by the given translation of PCF in Section 2. The token is the context data structure (M, f, \nD) that we used as a model of the dynamic algebra A~C~. More specifically, we see the token as a global \nstate of the computation where we will represent M and D as reg\u00adisters (RO and RI) and f as a dynamic \ntree data struc\u00adture. The token will compute the execution path of a pro\u00adgram of base type by traversing \nthe structure of the original network, guided by rules governed by the Geometry of In\u00adteraction. At each \nnode on the graph the token will be transformed, and directed to the next node. This process will continue \nuntil the token returns to the root of the term fif t~ term normalises), otherwise the token will travel \nan mfimte path m the term. We begin by st sting our object language which we take to be a very simple \nassembly language. We will assume a simple register machine wit h, say, 32 bit (unsigned) regis\u00adters \nRO, RI etc. The operations over these registers will be simple logical operations (such as shifts), comparisons \nand branching instructions. We propose a basic set of in\u00adst ructions which have trivial implementations \non any archi\u00adtect ure. The following table gives the intended meaning of the instructions that we require \nto code the multiplicative and constant information. We will introduce the additiomd structure for the \nexponential on demand. Additionally, we will define several macros to give greater abstraction for the \ncompilation. Macro Instruction Meaning P 1s1RO logical shift left RO c1 1s1RO logical shift left RO I \nI inc RO I in~rement RO I Isr Rn logical shift right Rn mov n RI move value n into R1 br I branch alwavs \nto 1 . be I branch to 1 if carry equal to zero cmpO Rn compare Rn with zero inc Rn increment Rn dec Rn \ndecrement Rn 4.1 Identities The compilation of the identities of Linear Logic proof struc\u00adtures produces \nno code, they just provide linking informa\u00adtion for connecting nodes. Axiom: The axiom is given by the \nidentity on contexts, hence no context transformation is required. For our implementation, the axiom \nprovides a piece of linking information to state which nodes are to be connected. Cut: Again, no context \ntransformation is required. The cut link specifies which nodes should be cut against each other, hence \nagain we just provide linking information here. 4.2 The Multiplicative The multiplicative (m = B, W) \nare straightforward to im\u00adplement, with a small run-time system. In fact a single register will suffice. \nIf we arrive from the left hypothesis (11), with context M, we pass control to the conclusion and record \nin the mul\u00adtiplicative stack the information that we came from the left: I:M. This indicates a very simple \ncompilation directly into assembly language, where we can use a single, potentially infinite register \nRO as the multiplicative register. Using O as the coding of left ) we can simply compile this as: 11: \np br 13-out Where 13-out is the other end of the edge 13, which will be an input to another node; this \nlabel is linked to the other node by the linking information provided by the identities. Recall that \nthe p macro is just a shift on a register. The token M arriving from the right hypothesis i2 re\u00adquires \nthat we record the information that we came from the right: r : III. Similarly to above, using 1 as the \ncoding of (right , we can compile this as: 12: q br 13-out Finally, arriving from the conclusion, we \nexamine the top element of the multiplicative stack and branch to either the Ieft or right premise, and \ndrop the top element of the stack M. This gets a straightforward compilation as: 13: Isr RO be Ilmut \nbr 12-out Hence, for each multiplicative node in a proof structure we have the following encoding. Each \nnode is translated in to a piece of assembly language code with three entry points. The compiler connects \nall labels which is the linking phase specified by the identities and cuts of the proof. Ill: p 12:q \nbr 13-out br 13-out 13: Isr RO I be 11.out I 1 1 br 12-out With these very simple rules we have the \npower to imple\u00adment the linear A-calculus, with just one register run time system. The size of the object \ncode (number of lines of assembly language) is just eight times the number of con\u00adst ruct ors in the \noriginal program. We suggested that RO is a potentially infinite register in the sense that in general \nwe have no idea just how large this stack will grow. However, in a typed setting, we have the following \nresult: Proposition 4.1 The size of the multiplicative stack M is bounded by the maximum size of the \ntype of any .wbterrn. Here we will assume that no program ever goes beyond a 32 bit register, which will \nallow the coding of programs at very high types. Of course, if we needed higher types, then we could \nextend the above encoding to handle larger stacks in a very simple way. Our aim here is to try to show \njust how simple we can code a PCF program.  4.3 The Exponential For the exponential we are not so lucky \nin that we require a data structure a little more complicated than a simple regis\u00adter. We will assume \na double linked tree data type, (so there are pointers to the parent node), corresponding to the ex\u00adponential \ncontext tree 2. We define operations on this data structure corresponding to the exponential context \nseman\u00adtics. Let ep (environment pointer) be a pointer to a node in the tree which indicates which part \nof the context we are currently transforming. There are some natural implemen\u00adtation techniques that \nwe use for these which we will take for granted, however we remark that there is some interesting theory \nbehind this that has recently been studied by Danos and Regnier [personal communication]. This is the \nonly connective that generates or destroys part of a cent ext tree; it crest es/removes levels of the \nenvironment. I D b 12 If we arrive from U with exponential context tree a then we add a new level to \nthe environment, hence crest e a new empty tree. We then continue downwards to the node con\u00adnect ed to \ni2. Arriving from 12 we do the reverse operation, and destroy part of the environment. The compilation \nrule for this node is given by: 11: d 12: d* br 12-out br [Lout where we can code d and d* by a sequence \nof instructions which can be seen diagrammatically as the following context tree transformation: d: a \n-/\\ * 0 a Contraction A Contraction is entirely similar to the multiplicative node, except we must place \nthe information on the current part of the exponential stack. 11 12 c I K3 If we arrive from 11 (resp. \n12) then we should record the information that we came from the left (resp. right) on the context tree, \nand continue towards the node connected to 13. Arriving from 13 will pop the current exponential information \nfrom the context tree and branch to either 11 or 12 depending on the status of the context tree. Hence \nthe compilation scheme for this node is given by: [11: r 12:s 1 Where the following explains the action \nof the code on the If the token is leaving the box then the exponential context tree: T: :pop  /\\= \n/\\ a bL:a b And similarly for s, where pop will set the carry flag accord\u00adingly. Weakening Since we \nare working on programs of ground type, and com\u00adputing a root-to-root path, no path will ever arrive \nto a Weakening node, hence there is no need to produce any code. Promotion As the token traverses the \nstructure of the proof, we will enter and leave ! boxes. This is the level of the environment that we \nare working in. There are two different cases to con\u00adsider, depending on where we enter and leave an \nexponential box. We will look at each one in turn. Principal door If we arrive at the principal door \nof a box (iI), then we in\u00adcrease the level of the environment we are working in. Du\u00adally, exiting a box \nat the principal door at (12) requires that we decrement the level of the environment. I I i2 The compilation \nof this idea is given by the following code: 11: ep = ep~right 12: ep = ep~prev br 12-out br 11-.out \nThe effect of ep=ep-+right and ep=ep+prev can be seen diagrammatically as the following waZk on the context \ntree, as shown in Figure 2. Auxiliary Doors There are two steps involved in the token passing through \nan auxiliary door of a box. 11 I 1, 12 Analogously with passing through the main door, we must increase \n(if entering the box from /2) or decrease (if leaving the box from 11) the level of the environment that \nthe token is operating at. context tree is transformed in such a way that the con\u00adtext of the box is \nsaved somewhere safe so that it can be restored when the token re-enters the box. This is achieved using \nthe t operator ( associativit y) which can be thought of as pushing the environment onto a stack for \nlater use. The dual of the above is performed if the token is en\u00adtering the box from an auxiliary door. \n Hence door of a the box: following code is compiled for each auxiliary 11: ep=ep prev t br 12-out 12: \nt* ep=ep-+right br Ii-out We can encode t and t*by a sequence of instructions which we can see diagrammatically \nas the following context tree transformation given in Figure 3. 4.4 PCF constants Here we give the compilation \nrules for each of the PCF con\u00adstants. We remark that for the translation that we are using into Linear \nLogic proof nets we require the use of exponen\u00adtial with these. Here we just give the codings of the \ncon\u00adstants and leave the reader to compose the full compilation using the (previously defined) code fragments \nfor the expo\u00adnential. Constants at base type Both natural numbers and boolean values are straightfor\u00adward \nto code we just return along the same path having pushed the value onto the data part of the context, \nwhich again we can see as being a single register, l?l. We wiU code tt as O and ff as 1, and naturzd \nnumbers are compiled trivially. We write c for a general data value. r [ The compilation is straightforward, \nyielding the follow\u00ading code module for a constant: m  Arithmetic functions The unarv functions SUCC, \npred and iszero are given by the following: r7 f 12 11 From tl we simply pass straight through without \nchange to the context. On the return up through 12 we apply the function to the data item. The compilation \nof this module can be given by: 11: br 12-out 12: f br Ii-out / ep=ep+ right: u : ep = ep + prev w \n /\\ /\\/ a bab Figure 2: Entering and leaving a box structure /\\ /\\ t:a< c: t* /\\ /\\ bca b Figure 3: Context \ntree transformations t and t* Where the different functions (f) are coded as the follow-will have a token \nstructure (0, < 0l,0 >, d) where d is the ing simple register operations: result of the computation (hence \nthe result is in Rl). Note that we set the exponential context tree to be < 0l,El >. Succ = inc RI This \nis required since the PCF constants under the chosen pred = dec RI translation are represented by boxed \nvalues. iszero = cmp O RI The correctness of this implementation comes directly from the correctness \nof the geometry of interaction. We Conditional recall a result from [Mac94] which states that the path \ncom\u00adputation is as good as reduction in PCF. We have the following module with four inputs/outputs. Proposition \n4.2 Let M be a PCF program atbase type. If11 M r-educes to a constant c then, for the execution path \nq5, we I have A~Cf~ $ = c. cond Example 4.3 We give a small example to show how things work. Space prohibits \ngiving a substantial example, but the /l\\ following at least gives some indication of how execution \npro\u00ad12 1314 ceeds. When we ask for the result of a conditional (arriving The example that we take is \nstmply (Ax.x)3. The execu\u00adalong D), we ask for the boolean value, so we travel along tion path for this \nterm is given by: the path out of /2. When we return, we either pass control to the true of false branch, \ndepending on the answer in the q*d*!(qp*)dp!(3) p*d*!(pq*)dq register .R1. Note that this data value \nis consumed by the which, using the equations m Section .2, reduces to !(3) test. The result of either \nbranch is then passed back as the as required. result of the conditional. This suggests the following \ncode The output of our compilation algorithm is given in Fig\u00ad fragment: ure ~. When run on our implementation, \nwith tracing set, produced the following output: Finished after 19 stack operations Result: 3 The implementation \nalso produced the trace of context Recursion transformations given in Figure 5, which shows all 19 oper\u00adations, \nThe trace shows the sequence of instructions (context The compilation rules for recursion are given in \nterms of the transformers) which we name using the operatiorzsfrom A~=~; codings of Contraction (C), \nDereliction (D) and Tensor (B). with the addition of using the notation ! and !* to indicate that we \nenter and leave a box respectively. 4.5 Execution The execution of a program of base type is simply given \n5 Optimisations by starting at the root of the term with the empty context (Cl, < 0 l,0 >, El) (so clearing \nRO, RI and initializing the tree The Geometry of Interaction can be regarded ss being lo\u00adstructure to \nbe the empty tree) and following the path in the cally very efficient it has provided a decomposition \nof the code generated. When the path reaches the root again we A-calculus to a level where we can directly \nimplement it on 1o11: q 1322: q d ep=ep~prev br 1221 br 1213 1112: p 1323: p d ep=ep-+prev br 1221 br \n1213 1213: d* 1131: ep=ep~right Isr RO mov 3 R1 be 1131 ep=ep~prev br 1000 br 1112 1221: ep=ep-+right \n1000: end Isr RO be 1322 br 1323 Figure 4: Object code generated Instruction M &#38; ep v 0 <U, n> \n<IJ, n> m T:n <Cl, n> <0,0> T:n <U, <n, ci>> <Cl, <n, cl>> 1  0 ?-:0 <0, <0,0>> <0,0> 0 <O, <a, n>> \n<U, D> 00 1:0 <Cl, <n, cl>> <O, n> 1 i:n <0,< 0,0>> <n)<cl,n>> 1:0 <n, n> <U, n> 1 <n, n> <U, n> 0000 \n0 <0,0> 0 0 n> <O, 0 00 <0,0> <El,n> 3 /:0 <0,0> <D, O> 3 l:U <O, <n, n>> <n, <cl, u>> 3 1:0 <0, <0,0>> \n<0,0> 3 <0, <0,0>> <Cl, cl> 3 T:n <n, <n, n>> <0,0> 3 7-:0 <13, <n, u>> <U, <u, cl>> 3 T:D <El, n> <0,0> \n3 1 <0,0> <ujn> 3  Figure 5: Example trace of the Geometry of Interaction Machine a concrete machine \nat the lowest level of detail. However, this is offset by the global performance. We have that the length \nof a path may grow exponentially in the number of cut elimination steps [Dan90]. By looking at properties \narising naturally from the un\u00adderlying semantics, we can generate some useful optimisa\u00adtions of this \nimplementation. Here we present one such op\u00adtimisation, and we refer the reader to [Mac94] for a more \ndetailed treatment of this material. Questions and answers Here we will glean some insight into implementing \npaths by recalling some properties about paths from [Mac94]. The property that we are going to make significant \nuse of is the notion that paths return back to the same place to answer the question asked, and moreover, \nthis preserves the so-called well bracketed condition. What we shall out\u00adline here is a technique to \navoid returning back along the execution path with each answer; we will jump back with the result to \nthe previous question asked, and restore the token structure. We begin with an overview, then give an \nencoding of this idea as a modification of the Geometry of Interaction Machine. A sequence of contexts \n(the trace of a path computation) gives rise to a well-bracketed sequence of questions and an\u00adswers. \nIn particular, there are critical points in the trace that ask questions of base type, for example the \nboolean test for a conditional, and the argument to a function of base type (e.g. SUCC). At each question \nq, asked, we will store the cent ext, and a return address on a stack of ques\u00adtions to be answered, The \ncomputation continues (maybe asking more questions, answering questions, etc. ) until we come to the \npoint where we are answering the question g,. At this point, rather than traversing the path backwards \nto where the question was asked, we pop the question stack, restore the token to be the state it was \nwhen it asked the question together with the answer, then return to the place where the question was \nasked. This then gives a optimisa\u00adtion on the path computation by reducing the length of the path being \ntraversed by half. We henceforth need to modify the general structure of the Geometry of Interaction \nMachine to handle this feature. The only change that we need make in fact is the encodings of the constants, \narithmetic functions and the conditional for PCF. We introduce a question stack (qs) which we define \nas: qs ::= (M, C,1) :gs \\ 0, where M and t are the mul\u00adtiplicative and exponential contexts respectively, \nand i is a label the return address. To avoid reDeatinrz the diamams for the data flow, we refer the \nreader (o the ~lagrams-given for the coding of the PCF constants given in Section 4; in particular, we \nwill use the same names for the labels. Constants of base type: A path entering a constant is answering \na unique question. We must examine the question stack to restore the context, update the context with \nthe answer, and branch to the place that the question was asked. Hence we have the following code, which \nwe will write in an informal not ation: 1: let (M, E,I ) = top qs let qs= pop qs let context = (M,E,c) \nbr 1 Constant functions: Depending on which direction we are arriving from, we are either asking a new \nquestion, or an\u00adswering a previous question. Arriving on 11 is asking a new question, so we push the \ncurrent context onto the question stack together with the return address, and continue to tra\u00adverse the \npatl.. Arriving on 12 with a context (M , E , c) is answering a question, so we update the token and \nthe question stack and branch to place that the question was asked. 11: let qs = push (M, E,12) qs 12: \nlet (M,E,I) = top qs br 12_out let qs = pop qs let token = (M,E,f c) br I Conditional: Arriving at the \ntop of a conditional requires that we ask a question for the boolean argument: 11: let qs = push (M, \nE,12) qs br 12-out Arriving on each of 12, 13 or 14 is exactly the same as the unoptimised version. \nThis optimisation to the Geometry of Interaction Ma\u00adchine of course is not obtained for free; we have \nhad to trade off space for time. Essentially, we are buildlng in features from environment machines to \navoid recomputation. The novelty of this is that the ideas arise very naturally from the underlying semantics. \n6 Conclusions In this paper we have considered a very novel idea of an im\u00adplementation of a simple functional \nprogramming language based on the notion of path computations from the Geome\u00adtry of Interaction. This \ngenerated a very compact coding of a simple functional language which has an incredibly simple (and small) \nruntime system. We propose that the basic ma\u00adchine could have applications where space is of this highest \nconcern, and efficiency is not so important. There are a substantial number of optimisations that can \nbe incorporated into this framework, and thk is our current direction of research. Preliminary investigations \nhave re\u00adsulted in simple program transformations that can reduce the length of the path computation; \nlocal (peephole) opti\u00admisations that can substantially reduce the size of the object code, and increase \nperformance; and finally, what we feel to be the most interesting direction, there are clear ways to \nincorporate not only sharing, but partial sharing aa used in the study of optimal reduction [Lev80] and \nits implementa\u00adtions [GAL92, AL91, AL93] as interaction nets [Laf90]. The extended system with the optimisations \nstarts to look like a promising alternative to extant implementation techniques. The implementation comes \ndirectly from the underlying semantics, and all optimisations come from the underlying theory of A~Cf. \nExperience with a prototype im\u00adplementation has left us with very positive feelings about this paradigm \nof compiler technology, and we hope that it may ultimately provide a new way of implementing ideas from \noptimal reduction. The compilation technique is very simple, aud the ideas seem to be general enough \nto extend to something like Stan\u00addard ML (see [Mac94] for hints on how to extend these ideas to general \nlanguages). There is still a great deal of work that we need to do to achieve this goal, but the way \nforward seems [Laf90] Yves Lafont. Interaction nets. In Proceedings of clear; we hope to be able to report \non this development in the Seventeenth ACM Symposium on Principles the not too position to References \n[ADLR941 [AJM94] [AL91] [AL93] [Dan90] [DR93] [FH88] [GAL92] [Gir87] [Gir89a] [Gir89b] distant future. \nOnly then will we really be in a talk about performance issues. Andrea Asperti, Vincent Danos, Cosimo \nLaneve, and Laurent Regnier. Paths in the J-calculus: Three years of communication without under\u00adstanding. \nIn Proceedings, Ninth Annual IEEE Symposium on Logic in Computer Science, pages 426-436. IEEE Computer \nSociety Press, 1994. Samson Abramsky, Radha Jagadeesan, and Pasquale Malacaria. Full abstraction for \nPCF (extended abstract ). In Masami Hagiya and John C. Mitchel, editors, Z heoreticai Aspects of Computer \nSoftware. International Symposium TA CS 9J, number 789 in Lecture Notes in Com\u00adputer Science, pages 1 \n15, Sendai, Japan, April 1994. Springer-Verlag. Andrea Asperti and Cosimo Laneve. Interac\u00adtion systems \nI: The theory of optimal reductions. Technical Report 1748, INRIA Rocquencourt, 1991. Andrea Asperti \nand Cosimo Laneve. Interaction systems II: The practice of optimal reductions. Technical Report UBLCS-93-12, \nLaboratory for Computer Science, University of Bologna, May 1993. Vincent Danos. Une Application de la \nLogique Lin6aire h 1 Etude des Process us de Norm alisa\u00ad tzon (principalement du A-caicul). PhD thesis, \nUniversit4 Paris VII, June 1990. Vincent Danos and Laurent Regnier. LocaJ and asynchronous beta-reduction \n(an analysis of Gi\u00adrard s execution formula). In Proceedings, Eighth Annual IEEE Sympostum on Logic tn \nC omputer Science, pages 296-306. IEEE Computer Society Press, 1993. Anthony J. Field and Peter G. Harrison. \nFunc\u00adtional Programming. Addison Wesley, 1988. Georges Gent bier, Martin Abadi, and Jean-Jacques L6vy. \nThe geometry of optimal lambda reduction. In Proceedings of ACM Symposium Principles of Programming Languages, \npages 15\u00ad26, January 1992. Jean-Yves Girard, Linear Logic. Theoretical Computer Science, 50(1):1-102, \n1987. Jean-Yves Girard. Geometry of interaction 1: Interpretation of System F. In R. Ferro et al., ed\u00aditor, \nLogic Colloquium 88. North Holland, 1989. Jean-Yves Girard. Towards a geometry of inter\u00adaction. In J. \nW. Gray and Andre Scedrov, edi\u00adtors, Categories in Computer Science and Logzc, volume 92 of Contemporary \nMathematics, pages 69-108. American Mathematical Society, 1989. of Programming Languages, pages 95 108. \nACM, ACM Press, January 1990. [Lan64] Peter J. Lanolin. The mechanical evaluation of expressions. Computer \nJournal, 6:308 320, 1964. [Lev80] Jean-Jacques Levy. Optimal reductions in the lambda CSJCUIUS. In J.P. \nHindley and J.R Seldin, editors, To H.B. Curry: Essays on Combinatory Logic, Lambda Calculus and Formalism, \npages 159 191. Academic Press, 1980. [Mac94] Ian Mackie. The Geometry of Implementation. PhD thesis, \nDepartment of Computing, Impe\u00adrial College of Science Technology and Medicine, 1994. [Pey87] Simon L. \nPeyton Jones. The Implementation of Functional Programming Languages. Prentice Hall International, 1987. \n[P1077] Gordon Plotkin. LCF considered as a program\u00adming language. Theoretical Computer Science, 5:223-255, \n1977. [Wad71] Christopher P. Wadsworth. Semantics and Prag\u00admatic of the Lambda-Calculus. PhD thesis, \nOx\u00adford University, 1971.  \n\t\t\t", "proc_id": "199448", "abstract": "<p>We investigate implementation techniques arising directly from Girard's Geometry of Interaction semantics for Linear Logic, specifically for a simple functional programming language (PCF). This gives rise to a very simple, compact, compilation schema and run-time system. We analyse various properties of this kind of computation that suggest substantial optimisations that could make this paradigm of implementation not only practical, but potentially more efficient than extant paradigms.</p>", "authors": [{"name": "Ian Mackie", "author_profile_id": "81100651343", "affiliation": "Department of Computing, Imperial College of Science, Technology and Medicine, London SW7 2BZ, England", "person_id": "PP40029491", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/199448.199483", "year": "1995", "article_id": "199483", "conference": "POPL", "title": "The geometry of interaction machine", "url": "http://dl.acm.org/citation.cfm?id=199483"}