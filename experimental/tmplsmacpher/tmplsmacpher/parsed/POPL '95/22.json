{"article_publication_date": "01-25-1995", "fulltext": "\n Separation Constraint Partitioning -A New Algorithm for Partitioning Non-strict Programs into Sequential \nThreads Klaus E. Schauser David E. Culler, Seth C. Goldstein Department of Computer Science Computer \nScience Division University of California, Santa Barbara University of California, Berkeley Santa Barbara, \nCA 93106 Berkeley, CA 94720 schauser~cs. ucsb. edu {culler,sethg}Qcs. berkeley.edu Abstract In this \npaper we present substantially improved thread par\u00adtitioning algorithms for modern implicitly parallel \nlanguages. We present a new block partitioning algorithm, separation constraint partitioning, which is \nboth more powerful and more flexible than previous algorithms. Our algorithm is guaranteed to derive \nmaximal threads. We present a the\u00ad oretical framework for proving the correctness of our par\u00ad titioning \napproach, and we show how separation constraint partitioning makes interprocedural partitioning viable. \n We have implemented the partitioning algorithms in an Id90 compiler for workstations and parallel machines. \nUsing this experimental platform, we quantify the effectiveness of different partitioning schemes on \nwhole applications. Introduction Modern implicitly parallel languages, such as the functional language \nId90, allow the elegant formulation of a broad class of problems while exposing substantial parallelism. \nHow\u00adever, their non-strict semantics require fine-grain dynamic scheduling and synchronization, making \nan efficient imple\u00admentation on conventional parallel machines challenging. In compiling these languages \nfor commodity processors, the most important step is partitioning the program into se\u00adquential threads. \nThis paper presents a new partitioning algorithm and experimentally quantifies its effectiveness. Many \nof the issues that arise in implementing non-strict languages (dynamic scheduling, synchronization, and \nheap management ) are present, independent of the source lan\u00adguage, when producing code for parallel \nmachines. Thus, the techniques developed in this paper are also applicable to par\u00adallel implementations \nof other languages. Moreover, dealing with non-strictness requires that these issues be faced even when \ncompiling for sequential processors, because non-strict programs require logical parallel execution in \norder to make forward progress. The language studied in this paper, Id90 [Nik90], is a non-d rict functional \nlanguage with eager evaluation. This combination, termed lenient evaluation [Tra91], exhibits more parallelism \nthan lazy evaluation while retaining much Permissionto copy without fee all or pati of this material \nis granted provided that the copies are not made or distributed for direct commercial advanta~e, the \nACM copyright notice and the title of the publication and Its date appear, and notice is given that copying \nis by permission of the Association of Computing Machinery. To copy otherwise, or to republish, requires \na fee and/or specific permission. POPL 951/95 San Francisco CA USA @ 1995 ACM 0-89791-692-1/95/0001....$3.50 \nof its expressive power. 1 To further increase parallelism, Id90 provides data structures that automatically \nsynchro\u00adnize between producer and consumers: I-structures and M\u00adstructures. When executing a lenient \nprogram on a parallel machine, dynamic scheduling may be required for two reasons. First, the semantics \nof the language make it impossible in gen\u00aderal to statically determine the order of operations. The order \nin which operations of a function execute may de\u00adpend on the dynamic context in which the function is \nin\u00advoked (cf., Section 2.1 ), not just on the value of its argu\u00adments. Second, long latency inter-processor \ncommunica\u00adtion and accesses to synchronizing data structures require that the computation dependent on \nthe messages be sched\u00aduled dynamically. Dynamic scheduling is expensive on com\u00admodity microprocessors, \nincurring a high cost for context switching. Therefore, these languages have been accom\u00adpanied by the \ndevelopment of specialized computer archi\u00adtectures, e.g., graph reduction machines [PCSH87, Kie87], dataflow \nmachines [ACI+83, GKW85, SYH+89, PC90], and multithreaded architectures [Jor83, NPA92]. Much research \nhas been done in compiling lenient languages for dataflow architectures [ACI+83, Tra86, AN90, GKW85, \nCU190]. As a clearer separation of language and architecture has been obtained, attention has shifted \nto compilation aspects of these languages for commodity processors [Tra91, SCVE91, Nik93]. The emphasis \nof the compilation work is to statically schedule groups of instructions into sequential threads and \nrestrict dynamic scheduling to occur only between threads. A thread forms the basic unit of work: once \nscheduled it runs until completion. The task of identifying portions of the program that can be scheduled \nstatically and ordered into threads is called partitioning [Tra91]. Partitioning the program into sequential \nthreads requires substantial com\u00adpiler analysis (dependence analysis) because, unlike in im\u00adperative \nlanguages, the evaluation order is not specified by the programmer. Care has to be taken to generate \nthreads which obey all dynamic data dependencies and do not lead to deadlock. Partitioning decisions \nimply trade-offs be\u00adtween parallelism, synchronization cost, and sequential effi\u00adciency [SCvE91]. However, \ngiven the limits on thread size imposed by the language model, the use of split-phase ac\u00adcesses, and \nthe control paradigm, our goal simply is to max\u00ad 1Usually non-strictness is combined with lazy evaluation \n(e.g., in LML and Haskell). Under lazy evaluation an expression is only eval\u00aduated if it contributes \nto the final result. Lazy evaluation decreases the parallelism because the evaluation of an expression \nis only started after ]t is known to contribute to the result. imize the length of the threads and minimize \nthe number of thread switches. 1.1 Contributions The main contribution of this paper is the development \nof a new thread partitioning algorithm that is substantially more powerful than any previously known \n[Tra91, HDGS91, SCVE91, TCS92]. A compiler for Id90 has been developed with back-ends for workstations \nand the CM-5 multipro\u00adcessor. It serves as an experimental platform for studying the effectiveness of \nthe partitioning algorithms. The parti\u00adtioning algorithms presented in [SCVE91] and [TCS92] are a starting \npoint for this work and are extended in several ways. The paper: presents a new block partitioning algorithm, \nsepara\u00adtion constraint partitioning, which is more powerful than iterated partitioning, the previously \nbest known block partitioning,  shows how separation constraint partitioning can be integrated successfully \ninto the interprocedural parti\u00adtioning algorithm to improve code at the call bound\u00adaries,  outlines \nthe theoretical framework and sketches the proof of correctness for our partitioning approach,2 b implements \nthe partitioning algorithms, resulting in a running execution vehicle for Id90 on workstations and several \nparallel machines, and a uantifies the effectiveness of the different partitioning schemes on whole applications. \nIn addition, although not documented here, we have ex\u00adtended interprocedural analysis to handle recursion \nand mu\u00adtually dependent call sites [Sch94]. Section 2 formalizes the problem of thread partitioning and \npresents an example which shows that non-strict lan\u00adguages require dynamic scheduling. In Section 3 we \npresent our new partitioning algorithm, separation constraint par\u00adtitioning. Section 4 briefly discusses \nhow separation con\u00adstraint partitioning can be integrated with interprocedural partitioning. Section \n5 presents experimental results which show the effectiveness of our partitioning algorithm. Finally, \nSection 6 contains the summary and conclusions. A short sketch of the proof of correctness for the partitioning \nalgo\u00adrithm appears in Appendix A.  1.2 Related Work Partitioning is similar in spirit to compilation \ntechniques for lazy functional languages [S NVGP91, Pey92]. Strictness analysis [Myc80, CPJ85] tries \nto determine which arguments can be evaluated before invoking the body of a function, thus avoiding the \ncreation of expensive thunks for strict ar\u00adguments. Partitioning goes a step further as it may derive \nthat arguments can be evaluated together even if a function is not strict in them [Tra91]. Path analysis \n[ElH87] detects the order in which arguments are evaluated, which may re\u00adsult in a cheaper representation \nof thunks and reduce the cost of forcing and updating them. Serial combinat ors by 2 The complete proof \ncan be found in [Sch94] Hudak and Goldberg are one of the first attempts to im\u00adprove the parallel execution \nof lazy functional programs by increasing their granularity [Go18 8]. Their approach is to group several \ncombinators into larger serial combinators. Partitioning plays a crucial role for the parallel execu\u00ad \ntion of strict functional languages, but unlike non-strict lan\u00adguages, the ordering of instructions can \nbe determined stat\u00adically. Thus, the difficulty is not what can be put into the same thread, but rather \nwhat should be placed into the same thread given communication and load balancing con\u00adstraints [SH86, \nNRB93]. Most of the partitioning research for lenient languages was inspired by Traub s seminal t heretical \nwork, which uses dependence analysis to characterize when instructions can be grouped into a thread [Tra91].3 \nTraub showed that the problem of finding a partitioning with the minimum number of threads is NP-complete. \nThus, all of the partitioning ap\u00adproaches rely on heuristics to group nodes into threads. Ian\u00adnucci developed \ndependence set partitioning, which groups nodes that depend on the same set of inputs [Ian88]. De\u00admand \nset par-titionzng, presented in [SCVE91] and [HDGS91] is analogous to dependence set partitioning, but \nit groups nodes which are demanded by the same set of outputs. Iter\u00adated partitioning combines the power \nof dependence and de\u00admand set partitioning by applying them iteratively [T CS92, HDGS91]. One of the \nalgorithms is applied, then the re\u00adduced graph is formed, and the other algorithm is applied. This process \nis repeated until no further changes occur. Schauser et al. extended the two basic partitioning schemes \nwith local merge up and merge down rules, thus achiev\u00ading essentially the same degree of grouping as \niterated par\u00adtitioning [SCvE91]. Traub et al. [TCS92] extended iterated partitioning with interprocedural \nanalysis to obtain larger threads. Recently, [CO094] and [Sch94] independently devel\u00adoped extensions \nto the interprocedural algorithm to handle recursive functions. Separation constraint partitioning iden\u00ad \ntifies all possible merges allowing the thread partitioning to be guided by high level heuristics, \nsuch as minimizing the cost of procedural boundaries. 2 Block Partitioning The partitioning algorithm \nproduces a collection of threads. The instructions of each thread are statically scheduled, and all dynamic \nscheduling required by non-strictness or poten\u00adtially long latency communication occurs between threads. \nDefinition 1 (Thread [TCS92] ) A thread is a subset of the instructions of a procedure body, such that: \n1. a compile-time instruction ordering can be determined for the thread which is valid for all contexts \nin which the containing procedure can be invoked, and 2. once the first instruction in a thread is executed, \nit is always possible to execute each of the remaining instructions in the compile-time order without \npause, interruption, or execution of instructions from other threads.  3 Traub s original framework \nallows threads to suspend; thus, his threads capture the sequential ordering which m requmed between \nin\u00adstructions, but not the dynamic scheduhng which may occur between the threads. Subsequent research \nur this area disallows thread sus\u00adpension, which has the advantage of capturing the cost of switching \nbetween threads. Our partitioning algorithms work on structured datajlow graphs [Tra86], the intermediate \nform used in the Id90 com\u00adpiler. It is similar to intermediate representations found in other optimizing \ncompilers. A structured dataflow graph consists of a collection of biocks,4 one for each function and \neach arm of a conditional, and interfaces which describe how the blocks relate to one another [TCS92]. \nEach block is represented by an acyclic dataflow graph; roughly, it cor\u00adresponds to a group of operators \nwith the same control de\u00adpendence [FOW87]. For example, all operators comprising the (then arm of a conditional, \nexcluding those in nested conditionals, are a block. Definition 2 (Dataflow Graph) A dataflow graph is \na directed acyclic graph of vertices and dependence edges, (V, E., Eq), where E. Q V x V are the certain \ndirect de\u00adpendence edges and Eq c V x V are the certain indirect dependence edges. The vertices describe \nthe instructions, including arith\u00admetic and logic operators, creation and access of data st ruc\u00adtures, \nand the sending and receiving of arguments and re\u00adsults. The edges capture certain data dependencies, \nwhich are present in every context in which the procedure can be invoked. We distinguish two kind of \ncertain dependencies: direct dependencies (represented in the examples by straight arcs see Figure 2a) \nand znckect dependencies (represented by squiggly arcs see Figure 2a), Indirect dependencies rep\u00adresent \npotentially long latency dependencies which may in\u00advolve nodes of other blocks. For example, an indirect \nedge connects the request and response nodes for a split-phase synchronizing data structure access. Such \nan access may require a long time to complete due to network latency or synchronization delay, i.e., \nit may have to wait until an other computation completes and stores the referenced value. Def\u00adinition \n1 implies that nodes connected by an indirect depen\u00addence must reside in different threads. In addition, \nwe define a potential indirect dependence (PID) as one which may exist in some but not all invoca\u00adtions \nof the block. PIDS may go through nodes of other blocks. This concept of potential indirect dependencies \nis very import ant, A Pm is a dependence which could exist in some legal execution of the program, where \na legal execu\u00adtion is defined as one which does not lead to a deadlock in the absence of partitioning. \nThe key observation is that the compiler does not have to consider PIDS which are contra\u00addicted by certain \ndependencies because such dependencies would lead to deadlock. Certain dependencies provide the mechanism \nto reduce the set of PIDS. We need to be conser\u00advative and overapproximate the PIDS. Due to the non-strict \nnature of the language the compiler initially assumes that for each function any argument may depend \non any of its results. Through the process of analysis some PIDS are ruled out. The challenge is to represent \nPIL)S as precisely and as efficiently as possible. We use inlet and outlet annotations to represent PIDS \nin the dataflow graph. Definition 3 (Annotation) An annotation for a block is a 5 tuple A = (Xi, Inlet, \nZ., Outiet, CID), where E, is the inlet alphabet, Inlet : V + ~ow(~;) maps each node to a set of inlet \nnames (the inlet annotation), XO the outlet alphabet, Outlet : V ~ ~ow(~o) maps each node to a set of \n4 In prevmus work the term bamc block was used [ I ra86, TCS92]. Since this term has a different meaning \nin the compiler literature for imperative languages we use the term b(ock. outlet names (the outlet annotation), \nand CID ~ (V x V) are the certain indirect dependencies ( CID = Eq). In the graphical representation, \nwe attach incoming cir\u00ad cles to nodes for inlet annotations and outgoing circles for outlet annotations. \nFor example, in Figure 1 the inlet an\u00ad notations are {a} and {b}. Nodes with outlet and inlet annotations \nform the end\u00ad points of PIDS. A PID may travel from a node with an outlet annotation to a node with an \ninlet annotation. An inlet name represents a set of outlet nodes that this node cer\u00ad tainly depends on. \nThis set is not known at compile time, but every node which contains the same inlet name in its an\u00ad notation \ndepends on this same set of outlet nodes, although we can not identify which outlet nodes they are. Thus, \nthe initial assumption of any inlet depending on any set of out\u00ad lets (not contradicted by certain dependencies) \nis captured by giving each inlet a unique annotation. Likewise, an out\u00ad let name represents a set of \ninlet nodes that depend on this node. The process of assigning the same or partially over\u00ad lapping inlet \n(or outlet) names to multiple nodes allows us express sharing of dependencies between nodes. As mentioned \nabove, the PLDS capture the potentiaJ de\u00adpendencies from outlet nodes to inlet nodes that do not lead \nto deadlock at runtime. We assume that a PID exists unless it is contradicted by certain dependencies. \nMore formally, we define a PID to exist from a node s to a node r if, there exists an o c Outlet(s) and \ni c Inlet(r) such that there does not exist a path over straight and squiggly edges from a node r with \nz E Inlet(r ) to a node s with o c Outlet(s ). The task of a partitioning algorithm is to take as input \na structured dataflow maDh and to Dartition the vertices of each block into non-ov&#38;l;pping reg(ons \nsuch that each sub\u00adset can be mapped into a non-suspending sequentii-d thread. Deriving the threads is \ndone in two steps. First, the nodes of each block are partitioned into disjoint subsets. Then, the instructions \nof each subset are linearized (any topologi\u00adcal ordering will do). The partitioning algorithms presented \nhere only derive the subsets of vertices and leave the actual ordering of instructions within each thread \nto a later stage of the compiler. We shall refer to each subset of vertices simply as a thread. A correct \npartitioning has no circular dependencies be\u00ad tween threads, i.e., no static cycles within blocks and \ndy\u00adnamic cycles across blocks. Without circular dependencies, it is possible to delay the scheduling \nof a thread until all of its predecessors have completed and then run the thread until completion. In \naddition, a correct partitioning must ensure that requests and responses to split-phase operations are \nin different threads. 2.1 Simple Examples We now present a simple example to illustrate the concepts \njust introduced. Figure 1 shows the dataflow graph for the function ~ which is called by the procedures \ng and h. def f uv= (u*u, V+V); def gz ={s,t =(f zs) in t}; computes (z*z)+(z*z) def hz={s,t=(f tz) in \ns}; computes (z+z)*(z+z) This example illustrates the need for dynamic scheduling even in the absence \nof conditionals. The function j takes two arguments, u and v, and returns two results, u * u and v + \nv. Within ~ there is no dependence between the multiplication and addition. Therefore, they can be scheduled \nin any or\u00adder under traditional strict evaluation. This is not true for non-strict evaluation. The function \ng feeds the first result of the function ~ back in as the second argument, while the function h feeds \nthe second result back in as the first argu\u00adment. These two dependencies are PIDS. In the context of \nfunction g the multiplication must be executed before the addition, while in function h the opposite \nis true. Thus, the multiplication and the addition have to be scheduled inde\u00adpendently, and it is impossible \nto put them together into a single non-suspensive t bread. -i--i..- Figure 1: Small example of a dataffow \ngraph for the function f ~ ~ s (U*U, V+V) ; and its partitioning into two threads. The arcs represent \ndirect dependencies while the inlet and outlet annotations represent sets of po ten tial dependencies \nas explained in the text. The shaded regions represent the threads. The two PIDS are represented by inlet \nand outlet anno\u00adtations. Without any interprocedural analysis to indicate otherwise, each node is given \na unique singleton annota\u00adtion, implying that we have to assume that each depends on (or influences) \na different set of unknown nodes. In our example, the argument receive nodes are given the inlet an\u00adnotations \n{al and {b]. while the send nodes have the outlet ., .,, annotation {.E} and {y}. The names themselves \nare not im\u00adportant; the absence of sharing between the names is what is important. By our definition \na potential dependence exists from the send node with outlet annotation {z} back to the receive node \nwith inlet annotation ~bl because there does ., not exist a certain dependence path contradicting this, \ni.e., from a node with b in its inlet annotation to a node with x in its outlet annotation. Likewisej \nthere exists a PID from Sendz to Reel. Functions g and h contain these PIDS. On the other hand, there \ncannot exist a PID from Sendl back to the Reel because this is contradicted by a certain depen\u00addence \npath. Thus, the inlet and outlet annotations correctly capture the two potential dependencies which may \narise at run time. As a result, the left and right nodes must stay in separate threads, and the partitioning \nalgorithm can at best obtain two threads, as indicated by the shaded regions. We can improve the partitioning \nby using interprocedu\u00adral analysis if we know that the function ~ is only called in the following context: \ndeffooz={s, t=(fzz) ins+t}; computes (z * 2)+(2 + z) In this case, it is valid to give both receive nodes \nof the def site the same inlet annotation, say {a}, as both argu\u00adment send nodes at the call site depend \non the same argu\u00adment of the function ~oo. Likewise, we can give the same outlet annotation, say {z}, \nto both send nodes of j. Now when partitioning ~, the compiler can determine that there cannot exist \na potential dependence from a result back to an argument, since under the new annotation there exists \na certain dependence path from a receive node with the inlet name a to a send node with the outlet annotation \nz. Thus, the compiler can group all of the nodes in ~ into a single thread. Dynamic scheduling may also \narise when accessing syn\u00adchronizing data structures. For example, assume that a function contains the \nfollowing code which manipulates I\u00adstructures, A[k] = A[m] * A[m]; A[l] = A[n] + A[n]; The corresponding \ndataflow graph is shown in Figure 2. This code fetches an element from A[rn], multiplies this with itself \nand stores the result into location A[k]. It also fetches from location A[n], adds this element with \nitself and stores it into location Alll. The declarative nature of the non\u00ad ., strict language does \nnot specify the order in which these statements are executed. Actually, that order may depend on the \ncontext in which this code is executed. If k = n, the store into location A[k] defines the value which \nis fetched from A[rt]. Therefore there exists a PID from the store to the fetch response, as indicated \nby the dashed line in Figure 2b. Thus, the multiplication has to be executed before the addition. If \n1 = m the operations would execute in the reverse order (see Figure 2c). Note that these dependencies \nare not directly present in the function, they are established through the synchronizing I-structure. \nThese potential dependencies are captured by the annotations; an inlet annotation on a fetch node represents \na dependence on some store. I-structure accesses have to be represented by split-phase operations which \nseparate the request from the response. There are two reasons why the request and the response may not \nexecute together. First, a fetch may get deferred should it occur before the corresponding store. Second, \nexecution on a parallel machine may result in a long communication latency if the accessed element resides \non another proces\u00adsor. Both forms require dynamic scheduling. Thus the re\u00adauest and resDonse have to \nreside in different threads. With s plit-phase accesses the processor can continue working after issuing \nthe request, making it possible to hide the commu\u00adnication latency with computation that is not dependent \non the requested data. The potentially long latency between the request and response is indicated by \nthe squiggly edges in the dataflow graph, which represent certain indirect de\u00adpendencies. These examples \nillustrate that potential dependencies cannot be known at compile time. They can travel through arguments, \nresults, internal call sites, and through I-struc\u00adture accesses. 2.2 Limits of Iterated Partitioning \nThe previously best known block partitioning scheme, iter\u00ad ated partitioning [TCS92], is not powerful \nenough to always find maximal threads. A slightly revised version of the first example, shown in Figure \n3, proves that separation con\u00adstraint partitioning is strictly more powerful than iterated partitioning, \nwhich fails to find maximal threads. This example consists of six nodes. Iterated partition\u00ading forms \ntwo threads. The inlet/outlet annotations are not (a) (b) (c)   m EEI F,,eceiv a Receiv b A[m] A[n] \n.+ @  Figure 3: Example where iterated partitioning fails two threads. unique singleton sets, but instead \nreflect dependence ing which could be the result of interprocedural to merge shar\u00adanalysis. The dependence \nset oft he three left nodes demand set is {z}. Iterated partitioning all into a single thread. Likewise, \nthe three grouped into a single thread because their {a, b}, and their demand set is {z, y}. Iterated \npartitioning cannot merge the nodes since their dependence and demand is {a}, and their will group them \nright nodes are dependence set is left and the right sets are different. However, they can safely be \nmerged for the following rea\u00adsons. The dependence sets represent the set of (unknown) outlets a node \ndepends on. The dependence set for the left nodes is a subset of that for the right nodes. Since the \nright nodes depend influence any PID from the reverse holds merge the two quires a more on a larger set \nof outlet nodes, they cannot of the left nodes, and thus there cannot exist a right to the left nodes. \nThe same argument in for the demand sets. It is therefore possible to threads into one. Merging these \nthreads re\u00adpowerful partitioning rule which is not based . m Elm Receiv a Receiv b A[m] $ A[n] ! , \n, , ,, * +! , ,, @&#38; k=n Figure 2: Simple example of a dataflow graph with 1-structures for the code \nA [k] The shaded regions show the four threads. Since a fetch of an I-structure element same thread as \nthe response. The threads cannot be grouped into a single thread dependencies which require dynamic scheduling, \nPart (c) for i= m. K a LT. abReel Rec2 + Sendl Send2 ~y x These PID edges are indicated by ti &#38;k3 \n I=m = A[m] * A[m] ; A[1] may defer, it cannot because there may exist the dashed arcs in Part solely \non equal dependence or demand tion is formalized by separation 3 Separation Separation constraint annotation, \nprecisely they can be merged Constraint partitioning determine or not. The Receiv b A[n] + r constraint \n= A[n] + A[n] ;, be placed into the potential indirect (b) for k= n and sets. This observa\u00adpartitioning. \nPartitioning can, with respect to any for any two nodes whether rule is simple: two nodes of a block \ncannot be merged (i. e., they must reside in different threads) if there exists either a certain indirect \ndependence (CID) or a potential indirect dependence (PID) between them. The reason is require dynamic \nGiven this tive partitioning dataflow graph, straint, merge process until this method is that both forms \nof indirect dependencies may scheduling, separation rule, we can easily devise an effec\u00ad algorithm. \nStarting with the unpartitioned we find two nodes without a separation con\u00adthem, form the reduced graph, \nand repeat this no further nodes can be merged. Although more powerful and elegant than the previous \n partitioning algorithms, unfortunately it is computationally more expensive. As discussed below, this \nproblem can be alleviated by only running it on a subset of the Separation constraint partitioning has \nfour First, it is guaranteed to derive maximal (but not optimal) threads. After it has finished, every \npair has a separation constraint between them, and is impossible to merge further. Second, it deals \nway with the partitioning constraints introduced and potential indirect require subpartitioning partitioning \n[TCS92]). bined with heuristics order which minimizes and synchronization urally integrated into rithm. \ndependencies, and therefore (as do dependence and graph. advantages. necessarily of threads therefore \nit in a unified by certain does not demand set Third, the algorithm can be com\u00adthat attempt to merge \nthe nodes in an communication, dynamic scheduling, overhead. Finally, it can also be nat\u00adthe interprocedural \npartitioning algo\u00ad 3.1 The Algorithm send to the left receive. (Sendl, Rec2 ) 6 PID exists because b \nG Inlet(Rec2) and x 6 Outlet(Sendl ), and there is no path The most complicated aspects of the algorithm \nare the initial from a node with bin its inlet set to a node with x in its out\u00ad computation of the separation \nconstraints and their update let set to contradict this. A similar argument can be made when two nodes \nare merged. Separation constraints arise for the other PID. As a result there exists a separation con\u00adfrom \nCIDS, which connect send nodes to receive nodes, and straint from any of the left nodes to any of the \nright nodes, PIDS, derived from the annotations for the block. We say and the left nodes cannot be merged \nwith the right nodes, that any two nodes that are connected through a PID or as observed earlier. a CID \nhave an indirect dependence and cannot be merged. Deriving the CIL)S is easy, as they are directly represented \nin the graph. The challenge is to efficiently determine the PIDS, which the compiler does not know and \nhas to approximate safely. Algorithm 1 (Separation constraint partitioning) Given a datafiow graph with \ninletioutlet annotations: 1.Compute the reflexive, transitive closure of the successor relation SUCC* \nover E, U CID. 2. Compute the set of potential indirect dependence edges, i.e., those edges from outlets \nto inlets which are not con\u00adtradicted by certain dependencies. PID = {(s, r)13i, 0: i~ In/e.t(~), o c \nOutlet(s), 13( T , s ) CSucc* : iCInlet(r ), oe Out/et(s )} 3. Combining PID and CID, compute the set \nof nodes with an indirect dependence between them. ID = {(u, v)l~(s, r) c PID U CID : (u, s) c SUCC*, \n(r, u) E SUCC*} 4. Find two nodes u, v without an indirect dependence be\u00adtween them, i.e., for which \n(u, v) @ ID and (TJ, u) @ ID. Merge u, v into a single thread, and update the represen\u00adtation. (a) Derive \nthe new set of nodes, use v as representative for the two merged nodes and discard u. v.,W = v {.} (b) \nCompute the new reflexive transitive closure. Succ new = {(p, s)l(p, s) e Succ , p # ajs # u} u{(p, s)\\(p, \nu) e Succ , (0, s) c SUCC*, p # U,s # a} U{(p, s)l(p, v) c SUCC*, (U, S) c SUCC*, p # U,s # a} (c) Compute \nthe new set of indirect dependencies. IDnew = {(P, S) I(P, S) ~ ID, p # U,S #rJ} U{(p, s)l(p, u) E ID, \n(V, S) c SUCC*, p#u, s #u} U{(p, s)[(p, v) e Hl, (u, s) E SUCC*, p # U,s # u} U{(P, S) I(P, U) C ,%Jcc*, \n(v, s) G ID, p # U,S # U} U{(P, s)[(p, v) e SUCC*, (u, s) E ID, p # ZL,S # U} (d) Set V = Vnew, SUCC* \n= Succ newj and ID = IDnew. 5. Repeat from Step 4 until no more nodes can be merged. Observe that existing \nseparation constraints never dis\u00adappear. Merging two nodes can only introduce new con\u00adstraints. Thus \nevery pair of nodes has to be tested at most once for merging. After merging two nodes, the transitive \nclosure and the indirect dependencies are updated. Fur\u00adthermore, the new ID can be computed from the \nold ID and SUCC*. We apply this algorithm to the example in Figure 1. Fol\u00adlowing the rule in Step 2, \nwe derive that there exists a PID from the left send to the right receive and from the right Now we apply \nthis algorithm to the example in Figure 3. Following the rule in Step 2, we derive that there are no \nPms because they are all cent radicted by certain dependencies: for every inlet/outlet name pair there \nexists a path from a node with the inlet name in its inlet set to a node with the outlet name in its \noutlet set. Therefore, PID = 0. Since CID = 0 we ascertain that ID = 0. Thus there are no sepa\u00adration \nconstraints and any two nodes can be merged. Sepa\u00adration constraint partitioning will, as expected, end \nwith a single thread. 3.2 Merge Order Heuristics The algorithm as presented so far does not specify the \norder in which pairs of nodes are visited and tested for merging. This flexibility is an important advantage, \nas it permits the algorithm to be combined with a heuristic that visits the nodes in an order that minimizes \ncommunicant ion, dynamic scheduling, and synchronization overhead. All three opera\u00adtions are expensive \non commodity processors. We decided to address communication first, since on most parallel ma\u00adchines \ncommunication has the highest overhead. Our heuris\u00adtic is first to try merging nodes belonging to the \nsame func\u00adtion call boundary (which reduces communication), then nodes at conditional boundaries (which \nreduces dynamic scheduling), and finally the remaining interior nodes of the block. After interprocedural \nanalysis (explained in Section 4), the annotations for a block may have been refined and the block can \nbe repartitioned. Repeatedly repartitioning using iterated partitioning is very expensive. However by \nusing separation constraint partitioning, we can perform the inter\u00adprocedural analysis on a restricted \ngraph consisting of the nodes at def and call site boundaries and their connectivity and then partition \nthe interior nodes after the annotations have been completely refined. Extracting this restricted graph \nfrom the original program is fairly simple and in\u00advolves only computing the transitive closure of each \nblock s dataflow graph. The saving is enormous: for our bench\u00admark programs the graph sizes are reduced \nby a factor of 10 to 20 the largest block was reduced from 619 nodes to 40 nodes. Running separation \nconstraint partitioning on the restricted graph is very fast, making interprocedural par\u00adtitioning viable. \nFinally, after obtaining the best possible partitioning at the function call boundaries, we partition \nthe interior of blocks. Our approach is to run separation constraint partitioning only on a subset of \nthe nodes of the block (the most critical nodes) and for the rest of the block use iterated partitioning, \nwhich in practice runs faster.  3.3 Complexity To compute the complexity of the above algorithm we as\u00adsume \nthat ID and SUCC* are represented by an adjacency mat rix. Assume that the problem size n is the maximum \nover the number of edges, number of inlet names, and num\u00adber of outlet names. Since the dataflow graph \nis acyclic, initially computing the transitive closure is 0(n2 ). Deter\u00admining the PID edges in Step \n2 is 0(n3 ). Computing ID is 0(n2). Testing whether two nodes can be merged takes only constant time, \nsince ID is represented as a matrix. Since merging never eliminates separation constraints, at most 0(n2) \npair of nodes have to be tested, thus this part of Step 4 is 0(n2). Merging occurs at most ~(n) times, \nand the complexity of Steps 4(a) (d)is 0(n2). Overall, the total complexity of the algorithm is 0(n3). \nIn practice the running time is too long for large blocks, For iterated partitioning the worst case \ncomplexity is also 0(n3), since the complexity of dependence and demand set partitioning is 0(n2). However \nexperimental data indicate that in practice iterated partitioning requires only a small number of iterations \nto find the final solution. Two cycles (i.e., four partitioning steps) were sufficient for partitioning \nthe blocks of the set of Id90 programs we used for the ex\u00adperimental results section. Ontheotherhand, \nit is possible to construct examples which require an arbitrary number of iterations (see [Sch94] for \ndetails),  3.4 Correctness Proving correctness of separation constraint partitioning is much harder \nthan dependence and demand set partitioning, which are quite intuitive. The appendix contains a short \ndiscussion of the correctness proof. There are two key aspects to this proof. First, we show that the \nalgorithm correctly updates the set of indirect de\u00adpendencies ID throughout the execution of the program \nev\u00ader ytime two nodes u, v are merged. This implies that certain and pot ential indirect dependencies \nare correctly taken care of. Second, we prove that when the algorithm terminates, all partitions are \nconvex, i.e., there do not exist any static cycles from a thread back to itself. This may not be the \ncase at intermediate steps of the algorithm. Thus separa\u00adtion constraint partitioning is quite different \nfrom iterated partitioning. There the partitioning is correct after every step and we could choose to \nstop at any time if so desired. Separation constraint partitioning, on the other hand, has to run until \ntermination. 4 Interprocedural Partitioning The block partitioning algorithm presented so far is limited \nin its ability to derive threads because without global anal\u00adysis it must assume that every send in a \nblock may poten\u00adtially feed back to any receive unless contradicted by certain dependencies. This is \ncaptured by the singleton inlet and outlet annotations given initially to send and receive nodes. Global \nanalysis can determine that some of these potential dependencies cannot arise and thereby improve the \nparti\u00adtioning [TCS92]. For example, the information gained while partitioning a procedure can be used \nto improve the inlet and outlet annotations of its call sites. These refined an\u00adnot ations may share \nnames, reflecting the sharing among dependence and demand sets present in the procedure. In addition, \nsquiggly edges from argument send nodes to re\u00adsult receive nodes can be introduced if the procedure has \nthe corresponding paths from the argument receives to re\u00adsult sends. Both the refined annotations and \nthe squiggly edges help to better approximate the PIDS and thereby im\u00adprove subsequent partitioning. \nThe same optimizations are possible in the reverse di\u00adrection. The annotations of the def site of a procedure \ncan be improved with the information present at its call site. Dependence and demand sets at the call \nsite determine the new sharing in inlet and outlet annotations at the def site. Squiggly edges can be \nintroduced from result send nodes back to argument receive nodes, if the corresponding paths from result \nreceive nodes to argument send nodes exist in the call site. This optimization is more complicated if \na pro\u00adcedure has more than one call site, in which case the new annotations and squiggly edges must be \ncompatible with all of the call sites. Conditionals are handled similarly to function calls. A conditional \nwith two arms can be viewed as a function call, where, depending on the result of the predicate, one \nof two blocks are called [AA89]. This representation simplifies the partitioning process, as we can use \nthe same unified mecha\u00adnism to deal with function calls and conditionals. When the analysis is applied \nto function calls it allows us to reduce communication; when applied to conditionals it allows us to \nreduce control flow overhead. 4.1 Interprocedural Partitioning Example We will not present the formal \ninterprocedural partitioning algorithm here as it already has been presented in [TCS92]. An extended \nversion which can deal with recursive function can be found in [Sch94]. However, we discuss a small exam\u00adple \nto help illustrate it. The example shown in Figure 4 consists of two blocks, a caller and callee. The \nleft part of the figure shows the dataflow graph for the caller, the function g, while the right part \nshows the dataflow graph for the callee, the function j. Both procedures receive two arguments and return \ntwo re\u00adsults. The procedure g contains a call site of the procedure ~, as indicated by the interior dashed \nrectangle, the two ar\u00adgument send nodes (AS 1 and AS2), and result receive nodes (RR1 and RR2). The corresponding \ndef site of the proce\u00addure ~ consists of the two argument receive nodes (AR1 and AR2) and two result \nsend nodes (RS1 and RS2). As shown in Part (a) of the figure, the algorithm starts by initially giving \nall receive and send nodes a unique sin\u00adgleton inlet or outlet annotation. As shown by the shaded regions \nin Part (b), partitioning the caller results in four threads, while partitioning the callee results in \ntwo threads. This is the best partitioning possible under the trivial an\u00adnotation. The top four nodes \nof the caller cannot be placed into a single thread because the partitioning algorithm has to assume \nthat a PID may exist from the node with the outlet annotation {u} back to the node with the inlet annotation \n{b}. Analogous arguments can be made for why the other threads have to stay separate. To improve the \npartitioning, we must apply interproce\u00addural analvsis which moDa~ates information across blocks. Propagati&#38; \ninvolves ~ntr~d~cing squiggly edges and refining inlet and outlet annotations. Let us first explore what \nhap\u00adpens when propagating from the caller to the callee. In this case, no squiggly edge is int reduced, \nsince the caller does not have a certain dependence path from a result receive node to an argument send \nnode. The new inlet annotations given to the argument receive nodes at the def site reflect the depen\u00addence \nsets of the argument send nodes at the call site. As shown in Part (c) of the figure, the node AR1 gets \nthe new inlet annotation {a}, while the node AR2 gets the inlet an\u00ad Callee ,----------------------------\u00ad \na) Initial Annotation m ! f ~R1 Every reeeive is annotated e AR2 f: with a uruque singleton relet name, \nand every send with a umque singleton outlet name. , -----+ . \\f RS1 RS2 z; I_._EEj Y -m . . . . . \n. ..----------------------\u00ad .............................. ...............-.... =--------~ -, b) Initial \nPartitioning f Partmomrrg of the caller AR1 e AR2 f[ results in four threads, whale the callee gets \ntwo threads. ! :.= +. -,,-~-.-.----- RR1 c RR2 d m cc! RSI Y RS2 z: -----------------\u00ad -....~.--- I \nw x -_-. -----\u00ad . ------\u00ad . ------------\u00ad -------......--...... .. --............. ............ \u00ad c) \nReannotation of Callee Annotation propagation from AR1 a the caller to the callee results in the new \ninlet and outlet annotations.  ..= f ;: ,, r m1, + d) Partitioning of Callee With the new annotation \nsepa\u00adration constraint partitioning &#38;+GM&#38; @  can obtain a single thread. ---------------------\u00ad--. \n--, e -----------------------\u00ad .-__.-. ....................... -------..................... e) Reannotation \nof Caller Annotation propagation from 9 the callee to the caller intro\u00adduces four squiggly edges and \neliminates the annotations. f) Partitioning of Caller . Partitioning now obtains two threads. Further \nreannotation and partitioning does not --------. -. -------.--------\u00adimprove this, Figure 4: E~ample \nof interprocedural partitioning with annotation propagation. not ation {a, b}. Likewise, the new outlet \nannotations given to the result send nodes reflect the demand set of the cor\u00adresponding result receive \nnodes, which are {w} and {w, z} respectively. The new annotations correspond precisely to the situation \nshown in Figure 3. Using separation constraint partitioning, we can group all nodes of the callee into \na sin\u00adgle thread, as indicated by the shaded region in Part (d) of the figure. Next we propagate annotations \nfrom the callee to the caller. This time we can introduce four squiggly edges at the call site, one from \nevery argument send node to every result receive node, since the corresponding certain dependence paths \nare present in the callee now that it consists of a single thread. These squiggly edges capture all of \nthe dependencies which can arise at this call site. Therefore, we can give the argument send and result \nreceive nodes at the call site empty inlet and outlet annotations, as shown in Part (e) of the figure. \nApplying separation constraint partitioning, the two top threads in the caller can be merged into a single \nthread, as shown in Part (f) of the figure. Likewise, the bottom two threads can be grouped into a single \nthread. Because the top and the bottom threads are connected by squiggly edges, they have to remain separate. \nThus, partitioning the caller results in two threads, the best partitioning that can be obtained for \nthis example. Further reannotation and partitioning does not improve this. Note that the resulting threads \nare the same as in a strict sequential program. 5 Experimental Results In this section we evaluate our \npartitioning scheme in the cent ext of the Berkeley Id90 compiler. Using various metrics we show how \nseparation constraint partitioning combined with interprocedural analysis approach the efficiency of \nan oracular strict partitioned. 5.1 Met hodology The Berkeley Id90 compiler uses a front-end developed \nat MIT [Tra86], which produces structured dataflow graphs for the partitioning algorithms presented here. \nThe partitioned graphs are used to generate code for TAM, a threaded ab\u00adstract machine [CGSVE93]. The \nTAM code is then trans\u00adlated to the target machine. Our translation path uses C as a portable intermediate \nform and is producing code for the CM-5, as well as for various standard sequential ma\u00adchines [Go194]. \nWe used this implementation for statistics collection and measurements. All of the programs are com\u00adpiled \nfor parallel execution. As they run, lots parallelism is exposed. However in order to factor out a broad \nfamily of issues unrelated to partitioning, such as load balancing and locality, we present data here \nfrom runs on a single pro\u00adcessor. See [CGSVE93, SGS+ 93] for data and discussion on running these programs \non parallel machines, We use six benchmark programs, shown in Table 1, rang\u00ading up to 1,100 source code \nlines. It should be noted that the code was taken as is, compiled for TAM, and executed on standard workstations \nor the CM-5 without anv modifi\u00adcations. The programs range from very fine grai~ed (e.g., Quicksort) to \nmedium grained (e.g., MMT).  5.2 Evaluation To measure the effectiveness of partitioning we compare \nfour different partitioning schemes: dataflow partitioning (lIF), iterated partitioning (IT), separation \nconstraint partitioning with interprocedural analysts (IN), and strict partitioning (ST). Dataflow partitioning \nand strict partitioning repre\u00adsent the two extremes of the spectrum. Dataflow parti\u00adtioning puts unary \nnodes into the thread of their predeces\u00adsor, reflecting the limited thread capabilities supported by \nmany dataflow machines. Strict partitioning ignores possi\u00adble non-strictness and compiles function calls \nand condition\u00adals strictly, thus representing the best possible interprocedu\u00adral partitioning algorithm. \nAlthough it is not the case for our six benchmark programs, strict partitioning produces an incorrect \npartitioning for programs which require non\u00adstrictness. Iterated and interprocedural partitioning repre\u00adsent \nthe two real partitioning schemes. With iterated par\u00adtitioning every block is partitioned in isolation. \nSeparation constraint partitioning with interprocedural analysis applies the techniques discussed in \nthis paper the interprocedural analysis uses separation constraint partitioning to first group nodes \nat def and call site boundaries, after which interior nodes are merged using iterated partitioning. Figure \n5 shows the dynamic TAM instruction distribu\u00adtion for the benchmark programs under the four partitioning \nschemes, each normalized to dataflow partitioning. Since the cost for each TAM instruction differs, this \nfigure does not necessarily reflect execution time which is presented later. Instructions are classified \ninto one of four categories: ALU operations, heap accesses, communication, and control op\u00aderations. The \nprograms toward the left of the figure ex\u00adhibit very fine-grain parallelism and are control intensive. \nThe moderate blocking (4x4) and regular structure of MMT shows a significant contrast. As expected, improved \nparti\u00adtioning substantially reduces the number of control opera\u00adtions. For most programs, iterated partitioning \nreduces the number of control operations by more than a factor of 2. For Simple and MMT the reduction \nis much larger.5 Interpro\u00adcedural partitioning further reduces the control operations for the more finely \ngrained programs, while for the coarse grained programs the improvement is insignificant. Inter\u00adprocedural \nand strict partitioning also decrease the number of instructions related to communication, as the grouping \nof arguments and results reduces the number of messages. This effect is particularly important since \ncommunication opera\u00adtions are more than ten times as expensive as any other. In order to see the effectiveness \nof separation constraint partitioning combined with interprocedural analysis we look at how boundary \nnodes are grouped into threads. In the code generation to TAM, passing of arguments and results for a \nfunction invocation requires send instructions. Simi\u00adlarly, the implement ation of conditionals is based \non switch\u00ades, which, depending on the result of the predicate, steer the control to one of two successor \nthreads. One distin\u00adguishing feature about partitioning across blocks is that it may group nodes at block \nboundaries. For example, multi\u00adple send nodes residing in the same thread can be grouped into a single \nsend node if the corresponding receive nodes also reside in a single thread. A similar optimization also \noccurs at boundaries of conditionals. Here multiple switch operations can be replaced by a single switch. \n5Just as important as the decrease of the number of control opera\u00adtions is the fact that they also become \nsimpler. For example, forks to synchromzmg thread often turn Into forks to unsynchronizing threads. \nProgram Code Size (Lines) Short Description EEEFIEl Quicksort 55 Quick sort on lists 10,000 Gamteb \n649 Monte Carlo neutron transport 40,000 Paraffins 185 Enumerate isomers of paraffins 19 Simple 1105 \nHydrodynamics and heat conduction 11100 SDeech 172 SDeech m ocessimz 1024030 I MMT 118 Matrix hultiply \n;est 500 T Table 1: Benchmark programs and their inputs. The programs are described in [CGSVE93].  \n m y . d w .  r--l=l 1\u00ad LL~z+ lL~zl-u~z +- !!73 Q o-l 0 cl-l n -m Quicksort Paraffins Speech Gamteb \nSimple MMT Figure 5: Dynamic TAM instruction distribution for the benchmark programs under various partitioning \nschemes (normalized to dataflow partitioning). ALU includes integer and floating-point arithmetic, messages \nincludes instructions execu ted to handle messages, heap includes global I-structure and M-structure \naccesses, and control represents all control-flow instructions including moves to initialize synchronization \ncoun ters. Sends per Call Partitioning Quicksort Gamteb Paraffins Simple Speech MMT DF 4.7 8.4 3.1 7.9 \n8.3 8.2 IT 4.7 8.4 3.1 7.9 8.3 8.2 IN 4.2 6.6 2.9 2.7 5.1 5.2 ST 2.0 2.0 2.0 2.0 2.0 2.0 Switches per \nConditional Partitioning Quicksort Gamteb Paraffins Simple Speech MMT DF 2.1 3.9 1.5 1.8 1.0 1.0 IT 2.1 \n3.9 1.5 1.8 1.0 1.0 IN 1.1 2.0 1.0 1.2 1.0 1.0 ST 1.0 1.0 1.0 1.0 1.0 1.0 Table 2: Dynamic ratio of sends \nper cal and the ratio of switches per conditional for the benchmark programs under various partitioning \nschemes. Table 2 shows the number of send instructions per func\u00adtion call and the number of switches \nper conditional across the different partitioning schemes. Recall that dataflow par\u00adtitioning and iterated \npartitioning alone do not work across block boundaries, and thus do not group sends at function calls \nor switch nodes at conditional boundaries. A func\u00adtion call requires at least two sends: one for sending \nin the arguments and another for returning the results. This is precisely the case under strict partitioning. \nSimilarly, every conditional requires at least one switch. Program Input Size Dl? IT IN ST Quicksort \n10,000 6.6 2.2 1.6 1.5 Gamteb 40,000 573.8 373.7 245.1 169.5 Paraffins 19 3.4 2.5 2.2 2.2 Simple 11100 \n7.1 3.9 2.9 2.7 Speech 1024030 1.4 0.6 0.6 0.6 MMT 500 130.2 66.1 61.0 61.0 Table 3: Dynamic run-time \nin seconds on a SparcSta\u00adtion 10 for the benchmark programs under various parti\u00adtioning schemes. Quicksofl \nParsffins SPsech Gamteb Simple MMT Figure 6: Dynamic run-time on a SparcStation 10 for the benchmark \nprograms under various partitioning schemes (normalized to dataflow partitioning). Finally, we see how \nthe reduction in the number of con\u00adtrol instructions leads to better performance. Table 3 shows the actual \nruntime measurements for the benchmark pro\u00adgrams on a SparcSt ation 10 under the various partition\u00ading \nschemes. Figure 6 shows the same timings graphically, normalized to dataflow partitioning. With the exception \nof Gamteb, interprocedural partitioning is very close to opti\u00admal as represented by strict partitioning. \nConclusions The main contribution of this paper is the development and implementation of substantially \nimproved thread partition\u00ading algorithms. We present a new block partitioning al\u00adgorithm, separation \nconstraint partitioning, which is more powerful and also more flexible than iterated partitioning, the \npreviously best known algorithm. Separation constraint partitioning uses a simple rule to determine whether \nor not any two nodes can be merged. Our algorithm is guaran\u00adteed to derive maximal threads. We present \na theoretical framework for proving the correctness of our partitioning approach. In addition, we show \nhow separation constraint partitioning makes interprocedural partitioning viable. We have implemented \nthe partitioning algorithms, re\u00adsulting in a running execution vehicle for Id90 on worksta\u00adtions and \nparalle~ machines. Using this experimental plat\u00adform, we were able to quantify the effectiveness of different \npartitioning schemes on whole applications. Experimental data show that sophisticated partitioning substantially \nre\u00adduces the control overhead compared to a partitioning re\u00adflecting the simple thread capabihties of \ndataflow machines. Separation constraint partitioning combined with interpro\u00adcedural analysis obtains \nnearly the same efficiency as our overly optimistic strict partitioning scheme. Thus we are able to verify \nthat the thread sizes are not limited by our partitioning algorithms, but rather by long latency opera\u00adtions \nand conventional cent rol flow. This work opens several fruitful avenues for future re\u00adsearch. (i) Partitioning \nalgorithms can be applied to other languages, especially strict and lazy functional programming languages. \nFor example, in the case of lazy languages a partitioned may group the evaluation of several arguments \nif the compiler can show that they will always be evalu\u00adated together. This can even happen if the function \nis not strict in the grouped arguments [TCS92]. (ii) The flexibil\u00adity offered by separation constraint \npartitioning should be explored more fully. Currently we only exploit this flexi\u00adbility in the context \nof interprocedural partitioning, where we group boundary nodes before any other nodes. One can develop \nmerge order heuristics for the interior nodes which minimize synchronization and dynamic scheduling. \nFor ex\u00adample, merging two connected nodes results in less synchro\u00adnization than merging two unrelated \nnodes. (iii) Using sep\u00adaration partitioning we have reduced the control overhead such that a large fraction \nof the remaining inefficiency is due to accesses to synchronizing I-structures. Better data struc\u00adture \nanalysis would reduce this overhead and further en\u00adhance the ability of interprocedural analysis to derive \nlarger t breads. Acknowledgments We are grateful to the anonymous referees for their valu\u00adable comments. \nWe would also like to thank Martin RI\u00adnard, Chris Scheiman, Deborah Weisser, and Pedro Dinez for their \ncomments. Computational support at Berkeley was provided by the NSF Infrastructure Grant number CDA\u00ad8722788. \nKlaus Erik Schauser received research support from the Department of Computer Science at UCSB. David \nCuller is supported by an NSF Presidential Faculty Fel\u00adlowship CCR-9253705 and LLNL Grant UCB-ERL-92/172. \nSeth Copen Goldstein is supported by an AT&#38;T Graduate Fellowship. References [AA89] Z. Ariola and \nArvind. P-TAC: A parallel intermedi\u00adate language. In Proceedings of the 1989 Conference on Functional \nProgramming Languages and Com\u00adputer Architecture, pages 230 242, September 1989. [ACI+ 83] Arvind, D. \nE. Culler, R. A. Iammcci, V. Kathail, K. Pingali, and R. E. Thomas. The Tagged Token [AN90] [BH87] ~cGsvE931 \n[Coo!+t] [CPJ85] [cll190] [FOW87] [GKW85] [GC1188] [G0194] [HDGS91] [Ian88] [Jor83] [Kie87] [Myc80] [Nik90] \n[Nik93] Dataflow Architecture. Technical report, MIT Lab for Comp. Sci., August 1983. Arvind and R. S. \nNikhll. Executing a Program on the MIT Tagged-Token Dataflow Architecture. IEEE Transactions on Computers, \n39(3):300-318, March 1990. A. Bless and P. Hudak. Path Semantics. In Mathe\u00admatical Foundations of PTogTamming \nLanguage Sc\u00adm antics (LNCS 298). Springer-Verlag, April 1987. D. E. Culler. S. C. Goldstein, K. E. Schauser, \nand T. von Eick&#38;. TAM A Compiler Controlled Threaded Abstract Machine. Journal of Parallel and Distributed \nComputing, 18:347-370, July 1993. S. R. Coorg. Partitioning Non-strict Languages for Multi-threaded Code \nGeneration. Master s thesis, Dept. of EECS, MIT, Cambridge, MA, May 1994. C. Clack and S. L. Peyton-Jones. \nStrictness Analysis -A Practical Approach. In Proc. Functiona[ PT-o\u00adgramming Languages and Computer Architecture, \nSept. 1985. Springer-Verlag LNCS 201. D. E. Culler. Managing Parallelism and Resources in Scientific \nDatatlow Programs. Technical Report 446, MIT Lab for Comp. Sci., March 1990. (PhD Thesis, Dept. of EECS, \nMIT). J. Ferrante, K. Ottenstein, and J. Warren. The Pro\u00adgram Dependence Graph and its Use in Optimization \nACM Transactions on Programming Languages and Systems, 9(3):319 349, July 1987. J. Gurd, C.C. Kirkham, \nand I. Watson. The Manch\u00ad ester Prototype Datatlow Computer. Communica\u00ad tions of the Association for \nComputing Machinery, 28(1):34 52, January 1985. B. Goldberg. Multiprocessor Execution of Functional Programs. \nPhD thesis, Department of Computer Sci\u00adence, Yale University, 1988. S. C. Goldstein. Implementation of \na Threaded Ab\u00adstract Machine on Sequential and Multiprocessors. Master s thesis, Computer Science Division \n EECS, U.C. Berkeley, 1994. (UCB/CSD 94-818). J. E. Hoch, D. M. Davenport, V. G. Grafe, and K. M. Steele. \nCompile-time Partitioning of a Non-strict Language into Sequential Threads. In PTOC. Symp. on Panzllel \nand Distributed PTocessang, Dec. 1991. R. A. Iannucci. A Dataflow/von Neumann Hybrid Architecture. Technical \nReport TR-418, MIT Lab for Comp. Sci., May 1988. (PhD Thesis, Dept. of  EECS, MIT). H. F. Jordan. Performance \nMeasurement on HEP A Pipelined MIMD Computer. In Proc. of the iOth Annual Sgmp. on Comp. ATch., Stockholm, \nInt. Sweden, June 1983. R. B. Kieburtz. A RISC architecture for symbolic computation. In PTOC. of 2nd \nInt. Con.f. on Architec\u00adtural Support for Programming Languages and Op \u00aderat~ng Systems, October 1987. \nA. Mycroft. The theory and practice of transform\u00adin~ call-by-need into call-by-value. In Int ernati o \nn a / Sympostum on Programming (LNCS 83). Springer-Verlag, April 1980. R. S. Nikhil. Id (Version 90.0) \nReference Manual. Technical Report CSG Memo, to appear, MIT Lab for Comp. Sci., 1990. R. S. Nikhil. \nA Multithreaded Implementation of Id using P-RISC Graphs. In Proc. Sixth Ann. Workshop on Languages and \nCompilers foT Parallel Comput\u00ading, Portland, Oregon, August 1993.  [NPA92] R. S. Nik&#38;l, G. M. Papadopoulos, \nand Arvind. *T: A Multithreaded Massively Parallel Architecture. In Proc. 19th. Annual Intl. Symp. on \nComputer Archi\u00adtecture, May 1992. [NRB93] W. A. Najjar, L. Rob, and W. A. P. B6hm. Initial Performance \nof a Bottom-Up Clustering Algorithm for Dataflow Graphs. In Proc. IFIP Conf. on Archi\u00adtectures and Compilation \nTechniques for Fine and Medium Grain Parallelism. North-Holland, January 1993. [PC90] G. M. Papadopoulos \nand D. E. Culler. Monsoon: an Explicit Token-Store Architecture. In Proc. of the 17th Annual Int. Symp. \non Comp. Arch., Seattle, Washington, May 1990. [PCSH87] S. L. Peyton Jones, C. Clack, J. Salkild, and \nM. Hardie. GRIP A High Performance Architecture for Parallel Graph Reduction. In Proc. Intl Con.f. on \nFunctional PTogTamming and Comp. Arch., Sept. 1987. [Pey92] S. L. Peyton Jones. Implementing lazy functional \nlanguages on stock hardware: the Spineless Tagless G-Machine. J. Functional Programming, April 1992. \n[Sch94] K. E. Schauser. Compiling Lenient Languages for Paral[e[ Asynchronous Ezecution. PhD thesis, \nCom\u00adputer Science Div., University of California at Berke\u00adley, May 1994. [SCVE91] K. E. Schauser, D. \nCuller, and T. von Eicken. Compiler-controlled Multithreading for Lenient Par\u00adallel Languages. In PTOC. \nConf. on Functional Pro\u00adgramming Languages and Comp. Arch., Aug. 1991. [SGS+93] E. Spertus, S. C. Goldstein, \nK. E. Schauser, T. von Eicken, D. E. Culler, and W. J. Dally. Evalua\u00adtion of Mechanisms for Fine-Grained \nParallel Pro\u00adgrams in the J-Machhe and the CM-5. In Proc. of the ,?oth Int 1 Symposium on Computer Architecture, \nSan Diego, CA, May 1993. [SH86] V. Sarkar and J. Hennessy. Compile-time Partition\u00ad ing and Scheduling \nof Parallel Programs. In Proceed\u00ad ings of the SIGPLAN 86 Symposium on Compiler Construction, June 1986. \n[SNVGP91] S. Smetsers, E. N6cker, J. van Groningen, and R. Plasmeijer. Generating Efficient Code for \nLazy Functional Languages. In Proc. Functional Program\u00adming Languages and Comp. Arch., Aug. 1991. [SYH+ \n89] S. Sakai, Y. Yamaguchi, K. Hiraki, Y. Kodama, and T. Yuba. An Architecture of a Datatlow Single Chip \nProcessor. In Proc. of the 16th Annual Int. Symp. on Comp. ATch., June 1989. [TCS92] K. R. Traub, D. \nE. Culler, and K. E. Schauser. Global Analysis for Partitioning Non-Strict Programs into Sequential Threads. \nIn PTOC. of the ACM Conf. on LISP and Functional Programming, June 1992. [Tra86] K. R. Traub. A Compiler \nfor the MIT Tagged-Token Dataflow Architecture. Technical Report TR-370, MIT Lab for Comp. Sci., August \n1986. (MS The\u00adsis, Dept. of EECS, MIT). [Tra91] K. R. Traub. Implementation of Non-stnct Func\u00adtional \nProgramming Languages. MIT Press, 1991. A Separation Constraint Partitioning Cor\u00adrectness Here we briefly \nsketch the correctness proof for separation con\u00adstraint partitioning. For convenience we introduce a \ndefinition of a legal partitioning under a particular inlet and outlet annotation. 270 which represents \nthe potential indirect dependencies, We show that a legal partitioning can be mapped into a non-suspensive \nthread satisfying Definition 1, and that separation constraint par\u00adtitioning is a legal partitioning. \nThe proof that the initial single\u00adton annotation and the annotations derived by interprocednral analysis \nare actually valid can be found in [Sch94]. Definition 4 (Potential Indirect Dependence Set) Let A = \n(Z~, Inlet, Xo, Outlet, CID) be the annotation given to a block. For a set LI ~ XO x Z, the set of edges \nPD = {(s, T)130 c Otiflef(s), % c In/et(r) such that (., i) 6 D} is called a potential Ah-ect dependence \nset if the graph (V, E8 u CID U PD ) is acyclic. We will use the notation PIDD = PD for such a potential \nindirect dependence set. The dependencies of a PD which lead to a cycle in the above graph cannot appear \nat run-time since they would lead to a dead\u00adlock of the program. Thus it is sufficient for the partitioning \nal\u00adgorithm to consider only the potential indh-ect dependence sets, PIDD. Definition 5 (Partitioning) \nA partitioning of a block (V, E. U Eq) k a set @ of non-overlapping subsets of the nodes V which cover \nit completely. For a partitioning @ and a node v G V, ~(v) denotes the partition that contains v. The \nreduced graph (V, Es U Eq) J@ contains a node for every partition of the block and the edges that cross \nthese partitions. If both a straight and squiggly edge cross the same two partitions, the squiggly edge \nis given preference. A correct partitioning has to avoid introducing circular de\u00adpendencies between threads, \nincluding both static cycles within blocks and dynamic cycles across blocks. In addition, a correct partitioning \nmust ensure that requests and responses to split\u00adphase operations are mapped to different threads. Definition \n6 (Legal Partitioning) A partitioning@ of a block is legal under an annotation A if for all PID ~ the \nfollowing three conditions are satisfied 1. the reduced graph (V, E, U CID U PIDD )J@ is acyclic, 2. \ne(s) # @(r) for all (s, ~) G PIDD, and 3. G(s) # ~(r) for all (s,,) c CID.  Lemma 1 (Correctness) A \nlegal partitioning can be mapped to threads obeying Definition 1. Proof sketch: We need to show that \na sequential or\u00addering can be found for each thread and that the thread does not suspend. The threads \ndo not have to suspend because the reduced graph is acyclic and all indirect de\u00adpendencies (certain and \npotential) cross threads. This also implies that the ordering for any thread can be determined independently \nof the ordering of other threads. Since the dataflow graph of a block is acyclic, any topological order\u00ading \nwill do. Before proving the correctness of separation constraint parti\u00adtioning we prove three auxiliary \npropositions. Proposition 1 (PID transitivity) If there exists a PIDD such that (sl , T1 ) c PIDD, (s2, \n7-2) c PIDD, and there exists a path from rl to .2 in (V, E U CID U PIDD) then there also exists a PIDD) \n~ PIDD such that (Sl, rz) c PIDDt. Proof sketch: Let PIDD be the potential indirect dependence set. Let \n01 G Outlet (r] ), il 6 Inlet(rl ), 02 G Outlet, i2 6 Inlet(r2 ), be the annotations for the two PID \nedges, i.e., (o1, ,1), (02, v) G D. For D = D U {(01, i2 )} we can show that PIDD, is also a potential \nindirect dependence set conforming to Definition 4. The next proposition may seem surprising at first \nglance but is due to our use of inlet/outlet annotations to represent poten\u00ad tial indirect dependence \nedges: we assume that potential indirect dependence edges are present uuless contradicted by certain \nde\u00ad pendencies. Proposition 2 (PID arcs) If there exists a PIDD such that (s1, T1) G PIDD and (s2, r2) \nG PIDD then there also exists a PIDD, ~ PIDD such that either (sl, 7-2) s PIDD, or (s2, rl) PIDD,. Proof \nsketch: Trying to contradict both (sl, r2 ) E PIDD{ and (s2, rl ) c PIDD J will also contradict one of \n(sl, rl) 6 PIDD or (s2, r2) s PIDD. Now we show that if partitioning a datzdlow graph resnhs in a reduced \ngraph with a cycle, then we can always find a cycle involving at most one PID edge. Proposition 3 (Single \nPID Arc) If there exists a PIDD such that (s, r) G PIDD and there exists a path from ~(r) to @(s) in \n(V, Es U CID U PID D)J@, then there exists a PID D) and (s , r ) G PIDD, such that there exists a path \nfrom @ (r ) to @ (s ) in (V, ES u CID)J.@. Proof sketch: We can show this by induction over the number \nof PID edges. Assume that (sl, r] ) and (s2, r2 ) are two neighboring PID edges in the cycle. Then, from \nProposition 2 we determine that there exists PIDDi such that (sl, r2) G PIDD, or (s2, rl) PIDD,. I.u \nthe first case, Propositions 1 and the induction hypothesis yields the result. In the second, we immediately \nobtain a cycle with only one PID edge. l There are two key aspects to the main correctness proof of \nseparation constraint partitioning. First, we need to show that the algorithm correctly updates the set \nof indirect dependencies ID throughout the execution of the program. This implies that certain and potential \nindirect dependencies are correctly taken care of. Second, we prove that when the algorithm terminates, \nall partitions are convex, i.e., there do not exist any static cycles from a thread back to itself. This \nmay not be the case at intermediate steps of the algorithm. Theorem 1 (Separation Constraint Partitioning) \nSeparation constraint partitioning is a legal partitioning. Proof sketch: We need to show that the three \nproper\u00ad ties of legal partitioning are satisfied. 1) We fist show that while the algorithm rnns, there \ndo not exist any cycles which involves certain indirect de\u00ad pendence edges or a single potential indirect \ndependence edges. This follows from the construction of ID and how ID is being updated at the merge step. \nProposition 3 then implies that there are no cycles involving any PID edges. Next we show that when the \nalgorithm fishes all par\u00ad titions are convex, i.e., do not have cycles involving only straight edges. \nThis is the case because nodes along such a cycle would not have any separation constraints between them \nand therefore could still be merged. 2) and 3) dh-ectly follows from the way ID is initially cre\u00ad ated, \nand that separation constraints never disappear. I This proof not only shows that separation constraint \nparti\u00adtioning is correct, but it also shows that at the end there is a separation constraint between \nany pair of partitions. This implies that no further merging is possible, even when other partitioning \nschemes are used. Therefore separation constraint partitioning tiuds a maximal solution.  \n\t\t\t", "proc_id": "199448", "abstract": "<p>In this paper we present substantially improved thread partitioning algorithms for modern implicitly parallel languages. We present a new block partitioning algorithm, <italic>separation constraint partitioning</italic>, which is both more powerful and more flexible than previous algorithms. Our algorithm is guaranteed to derive maximal threads. We present a theoretical framework for proving the correctness of our partitioning approach, and we show how separation constraint partitioning makes interprocedural partitioning viable.</p><p>We have implemented the partitioning algorithms in an Id90 compiler for workstations and parallel machines. Using this experimental platform, we quantify the effectiveness of different partitioning schemes on whole applications.</p>", "authors": [{"name": "Klaus E. Schauser", "author_profile_id": "81100173101", "affiliation": "Department of Computer Science, University of California, Santa Barbara, Santa Barbara, CA", "person_id": "P162375", "email_address": "", "orcid_id": ""}, {"name": "David E. Culler", "author_profile_id": "81405593136", "affiliation": "Computer Science Division, University of California, Berkeley, Berkeley, CA", "person_id": "P61148", "email_address": "", "orcid_id": ""}, {"name": "Seth C. Goldstein", "author_profile_id": "81100321807", "affiliation": "Computer Science Division, University of California, Berkeley, Berkeley, CA", "person_id": "P262218", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/199448.199511", "year": "1995", "article_id": "199511", "conference": "POPL", "title": "Separation constraint partitioning: a new algorithm for partitioning non-strict programs into sequential threads", "url": "http://dl.acm.org/citation.cfm?id=199511"}