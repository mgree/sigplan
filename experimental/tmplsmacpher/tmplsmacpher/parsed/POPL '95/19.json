{"article_publication_date": "01-25-1995", "fulltext": "\n TOTAL CORRECTNESS BY LOCAL IMPROVEMENT IN PROGRAM TRANSFORMATION David Sands University of Copenhagen* \ne-mail: dave~diku. dk Abstract The goal of program transformation is to improve efficiency while preserving \nmeaning. One of the best known transfor\u00admation techniques is Burstall and Darlington s unfold-fold method. \nUnfortunately the unfold-fold method itself guar\u00adantees neither improvement in efficiency nor total-correctness. \nThe correctness problem for unfold-fold is an instance of a strictly more general problem: transformation \nby locally equivalence-preserving steps does not necessarily preserve (global) equivalence. This paper \npresents a condition for the total correct\u00adness of transformations on recursive programs, which, for \nthe first time, deals with higher-order functional languages (both strict and non-strict) including lazy \ndata structures. The main technical result is an improvement theorem which says that if the local transformation \nsteps are guided by cer\u00ad tain optimisation concerns (a fairly natural condition for a transformation), \nthen correctness of the transformation fol\u00ad lows. The improvement theorem makes essential use of a for\u00admalised \nimprovement-theory; as a rather pleasing corollary it also guarantees that the transformed program is \na formal improvement over the original. The theorem has immediate practical consequences: It is a powerful \ntool for proving the correctness of ex\u00ad isting transformation methods for higher-order func\u00ad tional programs, \nwithout having to ignore crucial fac\u00ad tors such as mernoization or folding. We have applied the theorem \nto obtain a particularly simple proof of correctness for a higher-order variant of dejorestatzon. It \nyields a simple syntactic method for guiding and constraining the unfold/fold method in the general case \nso that total correctness (and improvement) is always guaranteed. *DIKU, Universitetsparken 1, 2100 K@benhavn \n0, DENMARK. Permission to oopy without fee all or part of this material is granted provided that the \ncopies are not made or distributed for direct commercial advantage, the ACM copyright notice and the \ntitle of the publication and its date appear, and notice is given that copying is by permission of the \nAssociation of Computing Machinery. To copy otherwise, or to republish, requires a fee andor specific \npermission. POPL 95 1/95 San Franciaco CA USA @ 1995 ACM 0-89791 -892-1/95/0001 ....$8.50 1 Introduction \nThe context of this study is transformations on functional programs. Source-to-source transformation \nmethods for re\u00adcursive programs, such as unfold-fold transformation, par\u00adtial evaluation and deforestation \n[BD77] [JGS93][Wad90], proceed by performing a sequence of equivalence preserv\u00ading steps on the definitions \nin a given program. The main goal of such methods is to improve the effi\u00adciency of programs, but not \nat the expense of their mean\u00ading. Program transformations should preserve the exten\u00adsional meaning of \nprograms in order to be of any practical value. In this case we say that the transformation is correct. \nThe Problem: Equivalence-preserving steps that do not preserve equivalence The problem is that for many \ntransformation methods which deal with recursive programs (including those methods men\u00adtioned above), \ncorrectness cannot be argued by simply show\u00ading that the basic transformation steps are meaning preserv\u00ading. \nYet this observation (clarified below) runs contrary to many informal (and some formal) arguments which \nare used in attempts to justify correctness of particular transforma\u00adtion methods. The problem arises \nthrough transformation steps for which equivalence depends critically on (ie. is local to) the func\u00adtion \nbeing transformed. A typical example of such a local equivalence would be an unfold step which replaces \na re\u00adcursive call to f by the corresponding instance of the body, or its inverse, a fold step. As a result, \nthe new definition may not be semantically equivalent to the original, in par\u00adticular since it may introduce \nnew recursive structure which leads to worse termination properties. To take a concrete (but contrived) \nexample to illustrate this point, consider the following transformation (where &#38; denotes a function \ndef\u00ad inition, and = is semantic equivalence with respect to the current definition): The problem comes \nfrom the fact that the equivalence used in a transformation step (in this example, the replacement of \n42 by the call f O) is not necessarily an equivalence with respect to the new definition. Here is another \nexample, expressed in unfold-fold steps. Given the natural law for conditionals: that it is substitutive \nan expression can be improved by improving a sub-expression. For reasoning about if x then (if x then \ny else z ) else z = if z then y else z the improvement relation a more tractable formulation we have \nthe following unfold-fold transformation: f z ~ if x then (f z) else true unfold + If z then (if x then \n(f z) else true ) else true unfold . + If z then (if x then (if x then (f z) else true ) else true) \nelse true lawxz. + If x then (f z) else true f~fz The transformed definition is thus f z A f z, and \nillus\u00adtrates the well-known fact that unfold-fold transformations do not, in general, preserve total \ncorrectness. It also serves as a counter-example to folk-law in functional programming which says that \nmore unfolds than folds is sufficient to guarantee the correctness of the unfold-fold method. Consequences \nThis problem has important consequences: (i) Many transformation methods have not been proved correct, \nand it seems that the correctness problem has received little attention because of an implicit (and incorrect) \nassumption that it is sufficient to argue the correctness at the level of the basic steps. In particular \nwe believe this to be the case for more realistic forms of partial evaluation, deforestation and supercompila\u00adtion \nof functional languages, where arguments of this form are unsatisfactory because the transformations \nperform rnemoization or foldingl. (ii) Some transformation methods simply do not preserve correctness \nin general. It is well known that this is the case for unfold-fold transformations.  The Contribution \nof this Work This paper presents a solution to the problem which deals with higher-order functional languages \n(both strict and non\u00adstrict) including lazy data structures. . The main technical result is an improvement \ntheorem which says that if the transformation steps are guided by certain optimisation concerns (a fairly \nnatural con\u00addition for a transformation), then correctness of the transformation follows. The above notion \nof optimisation is based on a formal improvement-theory. Roughly speaking, an expression e is improved \nby e if in all closing contexts C, if C[e] evaluates to some answer, C [ e ] can also evaluate to some \nanswer, but requires no more evaluation steps than C[e]. The important property of improvement from the \npoint of view of program transformation is 1A number of rigorous studies of correctness in partial evaluation \n[Gom92][Pa193][Wan93] [CK93] ignore the memorization aspects and deal with the orthogonal issue of the \ncorrectness of binding ttme and\u00adyszs, which controls where transformation occurs in a program. and some \nrelated proof techniques are used. The improvement theorem shows that if e is improved by e (in addition \nto e being operationally equivalent to e ) then a transformation which replaces e by e is totally correct; \nin addition this also guarantees that the transformed program is a formal improvement over the original. \n(Notice that in the above example, re\u00adplacement of 42 by the equivalent term f O is not an improvement \nsince the latter requires evaluation of an additional function call). The significance of the improvement \ntheorem is that it finds immediate practical application to the conse\u00adquences of the problem, namely: \n(i) Total correctness proofs for automatic transfor\u00admations based on a higher-order variant of the well-known \ndeforest ation method [Wad90]. With a new formulation of the deforestation algorithm (extended to deal \nwith higher-order functions) the proof of correctness, including the crucial folding process, becomes \nstrikingly simple. (ii) A simple syntactic method for restricting the gen\u00aderal (incorrect) unfold-fold \nmethod is provided. The method is based on a single annotation (whose meaning and algebraic properties \nare given by the improvement theory) which effectively guides and constrains transformations to guarantee \ncorrect\u00adness and improvement.  In this paper we illustrate only the second of these two ap\u00adplications. \nThe details of the first are found in [San94]. The remainder is organised as follows: Section 2 deals \nwith preliminaries including the syn\u00adtax and operational semantics of a simple higher-order func\u00adtional \nlanguage. In Section 3 we give the formal definition of a transformation, and some standard partial-correctness \nresults. In Section 4 the definition and properties of im\u00adprovement are given, and the improvement theorem \nis stated. Section 5 considers the application of the improvement the\u00adorem to the problem of ensuring \ncorrectness in unfold-fold transformations. Section 6 considers related work, and concludes with a discussion \nof further work. An appendix contains some technical details, leading up to a proof of the improvement \ntheorem. 2 Preliminaries We summarise some of the notation used in specifying the language and its operational \nsemantics. The subject of this study will be an untyped higher-order non-strict functional language with \nlazy data-constructors. Our technical results will be specific to this language (and its call-by-name \nop\u00aderational semantics), but the inclusion of a strict applica\u00adtion operator and arbitrary strict primitive \nfunctions (which could include constructors and destructors for strict data structures) should be sufficient \nto convince the reader that similar results carry over to call-by-value languages. We assume a flat set \nof mutually recursive function def\u00adinitions of the form f Z1 . . . Za f ~ q where af, the arity of function \nf, is greater than zero. (For an indexed set of functions we will sometimes refer to the arity by index, \nai, observation -ie. either they both converge, or they both rather than function name.) f, g, h . . \n. . range over func\u00ad tion names, z,y, ,z . . . over variables and e, el, e2 . . . over expressions. The \nsyntax of expressions is as follows: e=x \\f (Variable; Function name) I el ez (Application) I elQez \n(Strict application) I case e of (Case expressions) cl(zl) : el . ..cn(in) : en \\ c(?) (Constructor expressions) \n(Strict primitive ops)I P(?) We assume that each constructor c and each primitive function p has a fixed \narity, and that the constructors in\u00adclude constants (ie. constructors of arity zero). Constants will \nbe written as c rather than c ( ). The primitives and con\u00adstructors are not curried -they cannot be written \nwithout their full complement of operands. The expression written e{ ~ /~} will denote simultaneous capture-free \nsubstitution of a seqllence of expressions Z for free occurrences of a sequence of variables 2, respectively, \nin the expression e. The term FV( e) will denote the free vari\u00adables of expression e. Sometimes we will \n(informally) write substitutions of the form {~/~} to represent the replacement of occurrences of function \nsymbols ~ by expressions .?2. A context, ranged over by C, Cl, etc. is an expression with zero or more \noccurrences of a hole , [ ], in the place of some sub expressions; C [ e] is the expression produced \nby re\u00adplacing the holes with expression e. Contrasting with substi\u00adtution, occurrences of free variables \nin e may become bound in C[e]; if C[e] is closed then we say it is a clostng contezt for e. A context \nis called open if it contains free variables. 2.1 Operational Semantics The details of the operational \nsemantics of the language are given in the appendix; the important point is that evaluation relation \n(a partial function) $ is defined. If euw for some closed expression e then we say that e evaluates to \nweak head normal form w, The weak head normal forms, w, WI, W2,. . . 6 WHNF are just the constructor-expressions \nC(?), and the partially ap\u00adplied functions, f el ...ek, O< k < af. For a given closed e, if euw for some \nw then we say that e converges, and sometimes write e$. Otherwise we say that e dtver-ges. We make no \nfiner distinctions between divergent expressions, so errors and loops are identified. The operational \nsemantics is a standard call-by-name one, and J) is defined in terms of a one-step evaluation relation. \n 2.2 Approximation and Equivalence Here we define operational ordering on expressions, ~ and its associated \nequivalence E. The operational approxima\u00adtion we use is the standard Morris-style contextual ordering, \nor observational approximation [Plo75,Mi177]. The notion of observation we take is just the fact of convergence, \nas in the lazy lambda calculus [Abr90]. Observational equivalence equates two expressions if and only \nif in all closing contexts they give rise to the same 2This will only be done informally because the \nvarious equivalences and preorderings will only be closed under proper substitutions. diverge: Definition \n2.1 (i) e observationally contexts C such then C[e ].1,1. approximates that C[e], C[e ] e , e ~ are \nclosed, e , if if jor C [e].1,1 all (ti) e is observationally ande ~e. equivalent to e , e = e , if e \n~ e NB. For this language, if we choose to observe more than just convergence, such as the actual constructor \nproduced, the observational approximation and equivalence relations will be unaffected. Note that we \ncan observe the difference between -l and ~x.1 , which is the natural choice in an untyped theory of \nequivalence (for both strict and lazy languages). The distinction remains even if we impose a type discipline \non our language, and choose only to observe programs of non-function type (cf. [P1075]). This is due \nto the inclusion of a strict version of application (@I). 3 Transformation and Basic Correctness Re\u00adsults \nThe essence of the definition is that a transformation con\u00adsists of equivalence-preserving modifications \nto the bodies of a set of definitions. This is sufficiently general to describe unfold-fold transformations \nand many variants. 3.1 Transformation as Definition Construc\u00adtion For the purposes of the formal definitions \nand results, trans\u00adformation is viewed as the introduction of some new func\u00adtions from a given set of \ndefinitions; so the transformation from a program consisting of a single function f z S e, to a new version \nf x ~ e will be formally represented by the derivation of a new function g x ~ e {g/f}, rather than by \nmodification of the definition of f. Correctness of the trans\u00adformation now corresponds to validity the \nstatement: f = g. The reason why we adopt this representation of a trans\u00adformation is because observational \napproximation and equiv\u00adalence should, strictly speaking, be parameterised by the intended set of function \ndefinitions. Such explicit parame\u00adterisation is unwieldy, but we are able to avoid it by using a certain \nopen-endendness property of the language, viz. that extending the language with new function symbols \n(ie. new definitions) conservatively extends operational approxima\u00adtion and equivalence. We can similarly \ngarbage collect unreachable functions without affecting equivalences which do not pertain to those functions. \nThe implication of these properties is that we will be able to keep the set of definitions implicit in \nthe theory of opera\u00adtional approximation and equivalence, by making sure that we never change the definition \nof a given function; transfor\u00admation just adds new functions which extend the language. 3.1.1 Transformation \nSometimes it is also amrom-iate. . . to consider weaker transfor\u00admations, in which the transformation \nsteps can increase the definedness of terms. Examples are unrestricted unfolding in a strict language, \nor optimizations for strict versions of data structures such as head (cons (z, y)) ~ z. Transforma\u00adtions \nusing inequalities in this direction will be called weak transformations. Such transformations are common \nin strict languages eg. in Turchin s supercompzter[Tur86]. The following definition of transformation \n(weak trans\u00adformation) will enable us to formulate the correctness prob\u00adlem and state some relatively \nstandard partial correctness results. Definition 3.1 (Transformation) (Weak Transforma\u00adtion) A transformation \n(weak transformation, respectzvel~) o.f a set of functton definttzons, M gvoen by a set of expressions \nsuch that ef z e% (respectively, ef, ~ e,), together wath a set of new f;nctzons (the transformed program) \n{g, 01... rat ~ e,{@F}},E~ We say that there is a transformation (resp. weak trans\u00adformation) from f, \nto g%, and ~t wall be taken that the func\u00adtzon names g, are fresh. The form of the above definition of \na transformation em\u00adphasises the transformational derivation as the two phase operation of: a local transformation \n(of the bodies of the functions) followed by definition construction. We reason about transformations \nwhich introduce new auxiliary func\u00adtions by considering that all auxiliaries are introduced at the beginning \nof the transformation (cf. v~rtual transformation sequences [TS84]). A higher-order variant of the classic \nUnfold-Fold trans\u00adformation [BD77] can easily be recast as a transformation according to the above definition. \nBurstall and Darlington s original definition was for a first order functional language where functions \nare defined by pattern matching, and a call\u00adby-name evaluation is assumed. The example is considered \nin detail in Section 5, where conditions guaranteeing total correctness are studied. Definition 3.2 (Correctness) \nA transformation (weak transformation) from f% to g, M correct (weakly correct) tf f, = g, (f, &#38; \ng%. ) An alternative way of expressing a transformation would be to say that there is a transformation \nfrom fZ ~ ef to new definition g; ~ eg if and only if ef R eg{f/g}. The definition of transformation \nis complete with re\u00adspect to equivalence of definitions (cf. second order replace\u00adment [Kot80]), since \nif f 2 A e is equivalent to some new3 function gZ ~ e it follows that e ~ e , and that e ~ e {f/g}; this \nlatter equivalence defines a transformation from f to g. This shows that unfold-fold transformations \nare only a special case (of transformation), since the unfold-fold method is known to be incomplete in \nthis sense (see [BK83] [Kot80]\u00ad[Zhu94]). Of course, the problem we address in this paper is that both \nthe general transformation, and the special case of unfold-fold are not sound. 3.le. f does not depend \non g  3.2 Basic Correctness Results Here we summarise some basic correctness and partial cor\u00adrectness \nresults which relate to transformations. To sum\u00admarise: Transformation is partially correct: the derived \nfunc\u00adtions are less than the originals (g, ~ f,). Weak transformations are not partially correct: the \nderived program can be unrelated (with respect to ~) to the original. As a corollary of the partial correctness \nproperty, re\u00ad.. versible and non-recu rsive transformations are cor\u00ad rect. The first point follows \neasily from a least-fixed point property of the recursive equations, and is a fairly stan\u00addard result: \nif f is transformed to g then it is easily verified that f satisfies g s defining equation. So f is a \nfixed\u00adpoint of g s definition. But g is the least fixed-point, and hence g ~ f. This corresponds to the \n(informal) partial cor\u00ad rectness argument given by Burstall and Darlington [BD77] for unfold-fold transformation. \nA version of this result ap\u00adpears in [COU79] where it is formulated for recursive program schemes (first \norder functional programs) but in a more gen\u00aderal form. The second point, a less well-known fact, is \nillustrated with a simple example: Example 3.3 Let f x A if z then 1 else (f false). Then since f true \n~ 1 and f false ~ 2 then if z then 1 else (f false) ~ if x then (f true) else 2 so there is a weak transformation \nfrom f to g where g x A if z then (g true) else 2 , but f and g are incomparable. 4 Correctness by Improvement \nThis section presents the main technical result of the paper. It says, roughly speaking, that a transformation \nfrom f z ~ e, via equivalence e s e to g z 2 e {g/f} is totally correct if e is an improvement over e. \nThe improvement relation is expressed in terms of the number of non-primitive function calls; e is improved \nby e (written e ~ e ) if in all closing cent exts, C [ e] requires evaluation of no fewer non-primitive \nfunctions than C[e ]. 4.1 Improvement An intuition behind the use of improvement to obtain total correctness \n(at least for the case when the program computes a constant) is that improvement e ~ e represents some \nprogress towards convergence, and in the transformation to g x 42 e {g/f }, this progress is enjoyed \non every recursive call. The main problem with formulating an appropriate no\u00adtion of improvement is that \nthe language has a non-discrete data domain (ie lazy data and higher-order functions) as the results \nof programs. For this reason, as for operational ap\u00adproximation and equivalence, it is natural to define \nthis as a contextual (pre) congruence. Definition 4.1 Closed expression e converges in n-steps to weak \nhead normal form w, e.lJn w if e.l,lw, and this computa\u00adtion requires n reductions of non-primitive functions. \nThe operational semantics in the appendix makes this defi\u00adnition precise. A reduction of a non primitive \nfunction corre\u00adsponds to replacing a call instance f el . . . eaf with the cor\u00ad responding instance of \nthe body of f, ~ { el . . . ea f /s-l %f} It will be convenient to adopt the following abbreviations: \neJ,ln ~f3w. eJJnw Now improvement is defined in an analogous way to obser\u00advational approximation: Definition \n4.2 (Improvement) e is improved by e , e ~ e , if for all contewts C such that C [e], C[e ] are closed, \nif C[e]lJn then C[e ].l,lSn. Some properties (laws) of the improvement relation are stated at the end \nof the section.  4.2 The Improvement Theorem We are now able to state the main theorem, the proof of \nwhich is outlined in the appendix. Theorem 4.3 Given a set of functzon definitions, and a set of expressions \nSince improvement implies observational approximation, as a direct consequence of the Theorem we have \na condition for correctness and weak correctness: Corollary 4.4 Gwen a transformation (resp. weak trans\u00adformation) \nfrom {fi ZI . . . Xm% 4 ef, }l=~ to {g, ZI . . . Za, S ei {~/~} }icl, if qi ~ ei then the transformation \nis correct (weakly correct) and moreover, f, ~ gi,  4.3 The Theory of Improvement As is the case for \noperational approximation, to facilitate reasoning about the improvement relation it is essential to \nobtain a more tractable characterisation (than that provided by Definition 4.2) in particular one which \ndoes not quantify over all contexts. It turns out that ~ is an instance of the improvement theories previously \ninvestigated by the author [San91] (but quite independently of the transformation problem addressed in \nthis paper). This earlier study provides some crucial tech\u00adnical background for improvement orderings \nin functional languages. Most importantly, [San91] provides a definition of improvement simulation as \na generalisation of Abram\u00adsky s applicative bisimulat ion, and some results directly ex\u00adtending those \nof Howe [How89] which helps characterise when these relations are contextual congruences. We have used \nthis to show that improvement can be expressed as an im\u00adprovement simulation. In the remainder of this \nsection we describe this char\u00adacterisation, and outline the associated proof technique (for improvement) \nwhich it provides. Definition 4.5 A relation XR on closed expressions is an improvement simulation if \nfor all e, e , whenever e ?7? e , if e.$.n WI then e $~n wz for some W2 such that either: (z) WI -c(el... \ne~), Wz s c(ej... e~), and et R e~, (icl... n)jor (ii) w~ E Closures , wz E Closures, and for all closed \neo, (w1 eO) XR (WZ co).  So, intuitively, if an improvement-simulation relates e to e , then if e converges, \ne does so at least as efficiently, and yields a similar result, who s components are related by that \nimprovement-simulation. The key to reasoning about the improvement relation (and in particular, the key \nto proving the correctness of the improvement theorem) is the fact that ~, restricted to closed expressions, \nis itself an improvement simulation (and is in fact the maximal improvement simulation). Fur\u00adthermore, \nimprovement on open expressions can be charac\u00adterised in terms of improvement on all closed instances. \nThis is summarised in the following: Lemma 4.6 (Improvement Context-Lemma) For all e, e , e ~ e! if and \nonly if there exists an improvement simula\u00ad tion XR such that for all cl.osmg substitutions CT, e~ XR \nera. The lemma provides a basic proof technique, sometimes called co-znductzom to show that e ~ e it \nis sufficient to find an improvement-simulation containing each closed instance of the pair. In the appendix \nwe also use a useful variant of this proof technique which follows the idea of (bi)simulation up to from \nCCS [Mi189]. Here are some example improvement laws which follow either directly from its definition, \nor from the improvement context lemma (by exhibiting appropriate improvement sim\u00adulations): Proposition \n4.7 (ii) e~ e * C[e] ~ C[e ] (ni) elJe * e~ e (iv) fz~eiffx~e (v) e~fxiffx Aeand3e . fe JJ  4 Closures \nis the set of function-valued results, ie. partially applied functions. (vi)s R[ case z o-f ~ casexof \ne Instantiation will not be used explicitly; the role of in\u00ad CI(!A) : el cl(?l) : R[el] ... ... Cn(in) \n: en] c~(i~) : R[e~] 5 Application to Unfold-Fold Transforma\u00adt ions The improvement theorem is directly \napplicable to the veri\u00adfication of the correctness of some transformation methods; as mentioned in the \nintroduction, we have applied it to vari\u00adants of the deforestation and supercompilation transforma\u00adtions \n(see [San94]). In this section we consider a different kind of problem: transformation. The problem is \ndifferent because, in gen\u00aderal, unfold-fold is not correct. Our task is to use the im\u00adprovement theorem \nto design a simple method for constrain\u00ading the transformation process such that correctness is en\u00adsured. \nBurstall and Darlington s original definition was for a first order functional language where functions \nare defined by pattern matching, and a call-by-name evaluation is as\u00adsumed [B D77]. The following six \nrules were introduced for transforming recursion equations: (i) DefinvZon introduces a new recursion \nequation whose left-hand side is not an instance of the left-hand side of a previous definition; (ii) \nInstantiation introduces a substitution instance of an existing equation; (iii) Unfolding replaces an \ninstance of a function-call by the appropriate instance of the body of the function; (iv) Folding replaces \nan instance of a body of a function by a corresponding call; (v) Abstraction replaces occurrences of \nsome expressions by a variable bound by a where-clause; (vi) Laws rewrite according to laws about primitive \nfunc\u00adtions.  An important point, that lends significant power to the method, is that folding can make \nuse of any definition of the function being folded and this is typically an older definition. The original \ndefinition from [BD77] also allows unfolding using any earlier definition, but in practice this is never \nuseful and will not be considered here. Unfold-fold transformation carries over to higher-order functional \nlanguages essentially unchanged. To suit our par\u00adticular language but without significant loss of generality \nwe simplify the rules we will consider and the style in which they are applied. The definition rule above \nmay be applied freely since it just introduces new functions (and conservatively extends the theories \nof operational approximation and improvement). 5 R ranges over reductton contezts; these are single-holed \ncontexts whose hole is m a part of the term which will be evaluated next according to the operational \nsemantics (see appendix for details). stantiation will be played by certain distribution laws for case-branches. \nin any case, unrestricted instan\u00adtiation is problematic because it is not even locally equivalence preserving, \nsince it can force premature evaluation of expressions (a problem noted in [Bir84], and addressed in \nsome detail in [RFJ89] ) and is better suited to a typed language in which one can ensure that the set \nof instances is exhaustive. Abstraction will not be used explicitly, since this can easily be represented \nby laws using case-expressions to represent the binding (which has the advantage of enabling us to deal \nimplicitly with possible problems with bound variables which do not arise in Burstall and Darlington \ns language), or by using standard lambda\u00adlifting techniques (equivalent to definition + folding). 5.1 \nUnfold-Fold Transformation Step In our setting, an unfold-fold transformation step will consist of adding \nnew definitions and (repeatedly) transforming the right-hand sides of some set of definitions {f ~ZZ \n~ e~}lCI, according to unfolding, folding and rewriting laws, to obtain a set of expressions {e:} ~~ \n1. Using these expressions a set of new function definitions is constructed, namely {f~i, S e~{F/~}}tE~. \nHenceforth, the terms unfoldzng and folding refer to local transformations on expressions, and not to \noperations which in themselves give rise to new definitions. (It is the unfold-fold transformation step \nwhich constructs new defini\u00adtions. ) Given definitions f, Z ~ e,, the unfold and fold rewrites are just \ninstances of the congruences f, i = e, (although this law does not hold in a call-by-value language under \nar\u00adbitrary substitutions, so for strict languages further restric\u00adtions must be applied). We take the \nlaws to be operational equivalences by definition. So we see that an unfold-fold transformation step \nis now a transformation according to definition 3.1, and we obtain the standard partial correct\u00adness \nresult that the new program may be less defined than the original (see [Kot78,Kot85] [COU79] ).  5.2 \nCorrectness by Improvement in Unfold-Fold If we restrict the application of the laws so that they are \nimprovement steps (as they usually are in practice), then since the unfolding step is an improvement, \ntransformation steps not involving folds are totally correct, and the results are improvements. This \nfollows directly from Corollary 4.4. (In fact it is not too difficult to show that any transformation \nwithout folding is correct, providing the laws only involve primitive functions ([COU86] [Zhu94].) The \nproblem is that no fold step, viewed in isolation, is an improvement. The key to guaranteeing correctness \nis to ensure that we pay for each fold step at some point, thus maintaining overall improvement. Except \nfor the trivial case when the function being folded-against is everywhere undefined.  5.3 The Tick Algebra \nTo enable the required improvement condition to be main\u00adtained step-wise through the transformation, \nwe will extend the unfold-fold process with a single annotation, ~, tick . A tick represents a single \ncomputation step, so we can think of it as just an identity function ~z ~ z. From the point of view of \nobservational equivalence it is just an annotation, since e E e; but from the point of view of improvement \nit is more significant. In particular, <e ~D e but e ~ ~e (except if all closed instances of e diverge). \nThe technique for ensuring correctness of unfold-fold will be to use a tick to pay for a fold step, paid \nfor by a nearby unfold. Wit h respect to a definition f x ~ e, the unfoldlng and folding steps correspond \nto inst antes of the law f x ~ e. In terms of improvement, we have the following laws (which follow easily \nfrom the improvement context-lemma) relating a function and its body: The idea will be to use (substitution \ninstances of) these laws, in place of unfoldlng and folding, respectively. The tick algebra is a collection \nof laws for propagating ticks around an expression whilst preserving or increasing improvement. Figure \n1 gives some basic laws for ~ which will augment the transformation rules. Let d-b denote the cost -equivalence \nassociated to improvement: We also need a distribution law for nested case-expressions (a symmetric version \nof Proposition 4.7(vi). The the basic reduction steps of the operational semantics (see Appendix, Figure \n3) are also included in the improvement relation, and are therefore useful. In the laws, R ranges over \n(possibly open) reduction contexts. These are single-holed contexts whose hole is in a part of the term \nwhich will be evaluated next according to the operational semantics (see appendix for details). The laws \nare straightforward to prove using the improve\u00adment context-lemma (the details are omitted). But it is \nnot intended (or expected) that one should (need to) prove tick\u00adlaws on the fly . The method is intended \nto be viewed as completely syntactic given a reasonable set of tick laws. 5.4 Improved Unfold-Fold \nTransforma\u00adtions We show by two small examples how the tick algebra can be used to maintain improvement \nthroughout the steps of a transformation, thereby guaranteeing total correctness and improvement. Example \n5.1 In Figure 2 we give a standard unfold-fold example, but now locally maintaining the improvement re\u00adlation \nat each step of the transformation by introducing ticks at unfold steps, and propagating these, via the \ntick laws, to the fold site. The example ensures, by construction, that the derived program sumsq is \nan improvement over the original (in addition to being equivalent). Note that in this partic\u00adular case, \nsince the improvement steps above are all repre\u00adsented by cost-equivalences, we get a good picture of \nthe degree of improvement: namely, one function call is saved for each call to sumsqs , plus one more \nwhen the argument is nil. Example 5.2 To take a smaller (but less standard) exam\u00adple, consider the usual \nY-combinator of the lambda-calculus (in head normal form) given by Mdt((kh(xx))kh(2x)). A direct translation \nof this term into our language would be the expression Y, where Now we transform the definition of Y \nto obtain a direct re\u00ad cursive version, by unfolding the call to D then immediately foldlng against Y: \n Yh 4 h(Dh(Dh)) ~ h ~(h (D h(D h))) (unfold D) ~ h(Yh) (fold with Y) thus obtaining a new definition \nY h A h (Y h). This shows that Y is equivalent to Y , and Y ~ Y . (In fact since the steps of the unfold \nfold are all cost equivalences, a small extension of the main theorem gives us that Y ~wb Y .) Space \ndoes not permit an exposition of the pragmatic of the method but a few remarks concerning extensions \nand limitations are appropriate: . Transformations in which folds steps occur before any unfold steps \ncan be allowed by borrowing ticks and paying them back later. This corresponds to use of the law ~el \n~ ~ea * el ~ ez. We can incorporate this idea into a stepwise transformation (which uses only axioms, \ncongruence properties and transitivity) by introducing negative ticks. . Derived functions are improvements \non the originals, so we can allow folding against functions obtained from previous transformation steps. \nFor example, if a trans\u00adformation step from f derives a function f then f ~ f . So a subsequent transformation \nstep can include a generalised fold, in which an instance of the body off (suitably ticked ) can be replaced \nby a call to f . . Some transformations involving lazy data-structures need to be phrased slightly differently \n(from traditional unfold-fold derivations) for the method to work. Prob\u00adlems can arise because ticks \ncannot propagate across lazy constructors. These cases are easily handled by postponing the introduction \nof new definitions until the point at which a potential fold is discovered. This is analogous to the \nway transformations are performed using expression procedures [Sch81] (dk.cussed in the next section). \n 6 Related Work Although the topics of program transformation, includlng partial evaluation, have been \nactive research topics in func\u00adtional programming for the last decade and more, the prob\u00adlem of correctness \nhaa received surprisingly little attention. ~el b V ez ~ebe .W l?.[~e] d-b <R[e]el~ e2 f5&#38;e ~p(el... \nen) dwbp(el ...~et . ..e~) f ; dlb de .4 case e of dWD case e of CI(21) : el. ..cn(ii) : en Cl(il) : \nV el . ..cn(in) : en Figure 1: Tick Laws Consider the following definitions: sumzs 42 case X5 of nil \n:0 cons(y, ys) : y + sumys Sqx 42 x*x mapf X5 k case X5of nil : nil cons(y, ys) : cons((f y), mapf ys) \nNow consider the following transformation of the function which takes the sum of the squares of a list: \nsumsqs xs ~ sum(map sq m). Transforming the right-hand side we obtain: case X5 of sum(map sq m) d-b \nsum< ml : nil (unf. map) cons(y, ys) : cons((sqy), map sqys) ( ) case xs of dD V case ~ nil : nil of \n(unf. sum) cons(y, ys) : cons((sqy), map sqys) () nzl :0 cons(z, m) : z + sumzs d-b # case Z. of (J/case-laws) \nnil :0 cons(y, ys) : (sqy) + sum(mapsqys) d-b ~ case ~s of (i-laws) nzl :/0 cons (y, ys) : (sq y) + \n~sum(map sq ys) d-b d case Z. of (fold sumsqs) nil :do cons(y, ys) : (sqy) + sumsq ys Finally, after \neliminating the ticks we construct the new definition: sumsq xs + case X5 of ml :0 cons(y, ys) : (sqy) \n+ sumsq ys Figure 2: Improved sumsqs transformation Our definition of transformation and its partial \n(but not total) correctness property is what Kott named second or\u00adder replacement [Kot80] (see also [Sch80] \n[Theorem 2.7]). Kott s motivation for the definition was that the less-general unfold-fold method, even \nignoring the correctness issue, has strictly limited power (see also [BK83] [Zhu94] ). He did not however \nconsider solutions to the correctness problem for this more general class of transformations. Some principles \nfor obtaining total correctness follow from the standard par\u00adtial correctness results; for example if \none can prove the new program is total [MW79], or more generally if the new equa\u00adtions have a unique \nsolution [COU79]. Correctness also follows (from partial correctness) if the transformation is re\u00adversible. \nCourcelle [COU86] proposed simple constraints on unfold-fold transformations to guarantee reversibility. \nThis method is rather limited in power since it cannot introduce new recursive structure. Reversible \ntransformations have also been studied in logic programming (see eg. [Mah89] and [GS91]). Expression \nProcedures Considering more specific trans\u00adformation methods, of particular note is Scherlis transfor\u00admation \nmethod based on expression procedures [Sch80]. This method is less general (ie. can be simulated by) \nthe unfold\u00adfold method, but has the distinction that it preserves total correctness without need for \nany constraints. The method is proved correct (for a strict first order language) using a notion of progressiveness, \nbased on reduction orderings (re\u00adlated ideas are used in [Red89] for the synthesis of Noethe\u00adrian rewrite \nrules from equational specifications). There are some connections between progressiveness and improvement \n[San94] which suggest there is potential for applying the im\u00adprovement methods developed here to the \nproblem of gen\u00aderalizing Scherlis transformation and proving it correct for lazy and higher order languages. \nKott: Unfold-Fold With respect to total correctness of unfold-fold transformations on first-order programs, \nKott [Kot78] [Kot85] gives some technical results relating the number of unfolding steps to the number \nof folds in re\u00adstricted form of transformation of the body of a single re\u00adcursive function, using a sequence \nof unfoldings, laws and foldings (in that order). It is often stated, incorrectly, that the main result \nof the paper implies that: . . . m unfold\u00adfold derivation is correct if there are no more folds than \nthere are unfolds at any stage of a transformation. Our solution based on the tick-algebra certainly \nhas this flavour, but without further qualification this statement is false (see the example in the introduction). \nIn fact, Kott s main re\u00adsults qualify a statement of this form by modifying the very notion of correctness \nin a way which is dependent on each transformation history. From a given transformation, Kott effectively \nconstructs an additional strictness law which must hold for the above correctness statement to be true. \nIn practice this is unsatisfactory because the extra law is often not true in the intended model, even \nwhen the trans\u00adformation is correct. In comparison, in addition to the fact that we can handle higher-order \nfunctions and much less restricted transforma\u00adtions, our approach to obtaining unfold-fold transformations \nis technically simpler, and more practical since it can guide the transformation rather than being a \npost-hoc verification. Given that there are obvious relationships between strict\u00adness laws and tick-propagation \nlaws, we conjecture that it will be possible to prove that our method allows us to verify the correctness \nof strictly more transformations than Kott s (as we have observed in practice). Replacement and Unfold-fold \nin Logic Programs In logic programming there are analogous problems with the operation of replacement \nof some subgoals in a clause by a logical equivalent, see eg. [BCE92b]. In Bossi et al s work, a model-theoretic \ndefinition of semantic delay, somewhat analogous to our improvement relation, is used as part of a replacement \ncondition guaranteeing correctness for a variety of models, although to our knowledge it has not been \nused to construct simple syntactic methods to constrain unfold\u00adfold transformation (although the method \nis able to justify some fold steps in isolation [B CE92a]) or to justify other automatic transformation \nmethods. For the most part, the operation of replacement has been studied only as a subsidiary tool in \nunfold-fold transfor\u00admation [TS84,GS91,KF86, PP93]. Methods for constraining unfold-fold transformations \n(of logic programs) to guaran\u00adtee correctness under various semantic interpretations have been widely \nstudied -see Tamaki and Sate s work [TS84] and its many derivatives eg. [Sek93][Sat90][PP91]. See [Amt92] \nfor a technical framework in which many of these conditions can be expressed, and [PP93] for an overview \nof the methods. There are (informal) similarities between our method of constraining unfold-fold using \nticks and the method of counters of Kanamouri and Fujita7 [KF86] (a direct generalisation of Tamaki and \nSato s basic foldable \u00adannotations). A more technical comparison and criticism of related work is contained \nin the full version of the paper [San94]. Further Work Application of the improvement theorem to justify \nother transformation methods (eg. more realistic forms of partial evaluation), and in particular use \nof the corre\u00adsponding call-by-value improvement theorem for call\u00adby-value transformations.  Investigating \nthe use of correct unfold-fold transforma\u00adtions as a proof technique.  Investigate the completeness \nof the improvement the\u00adorem wit h respect to improvement.  Finally, a problem left open in [San91]: \nis there a tractable call-by-need theory of improvement? A reasonable call\u00adby-need theory is a prerequisite \nto answering the following natural question: does the improvement theorem hold for the corresponding \nimprovement theory? Acknowledgements This work began while the author was funded by the DART project \n(Danish Research Council), and the Department of Computer Science at Copenhagen University. The author \nis partially supported by ESPRIT BRA Coordination . The TGiven the ~aY we ww t.h~ general improvement \ntheOrem cOn\u00adstrain the unfold-fold method, and given the above analogies, it seems reasonable to ask \nwhether Bossi et al s replacement conditions can be used to justify a method for constraining unfold-fold \nsuch as Kanamouri and Fujita s. 229 author gratefully acknowledges Torben Amtoft for discus-[How89] \nD. J. Howe. Equality in lazy computation sys\u00adsions on the problem early in the development of this work, \ntems. In Fourth annual symposium on Logic In and his help in obtaining some of the references. Thanks \nComputer Science, pages 198 203. IEEE, 1989. to the Topps group at DIKU, and in particular to Robert \n[JGS93] N. D, Jones, C. Gomard, and P. Sestoft. PartialGliick, Morten Heine S@-ensen, Neil Jones, Kristian \nNielsen, Evaluation and Automatic Program Generation. Bob Paige and Tom Reps for numerous discussions \non the Prentice-Hall, 1993. subject of program transformation, and comments on earlier drafts. Thanks \nto Phil Wadler for some helpful comments, [KF86] T. Kanamori and H. Fujita. Unfold/fold trans\u00ad and to \nthe referees for suggesting a number clarifications. formation of logic programs with counters. Tech\u00adnical \nReport ICOT Tech Report TR-179, Mit\u00adsubishi Electric Corp., 1986. References [Kot78] L. Kott. About \ntransformation system: A the\u00ad [Abr90] S. Abramsky. The lazy lambda calculus. In oretical study. In B. \nRobinet, editor, ProgramD. Turner, editor, Research Topics in Functional Transformations, pages 232 247. \nDunod, 1978. Programming, pages 65-116. Addison Wesley, 1990. [Kot80] L. Kott. A system for proving equivalences \nof recursive programs, In W. Bibel and R. Kowal\u00ad [Amt92] T. Amtoft. Unfold/fold transformations preserv\u00adski, \neditors, 5th Conference on Automated Deduc\u00ading termination properties. In PLILP 92, volume tzon, volume \n87 of LNCS, pages 63 69. Springer\u00ad631 of LNCS, pages 187 201. Springer-Verlag, Verlag, 1980. 1992. [Kot85] \nL. Kott. Unfold/fold transformations. In M. Ni\u00ad [BCE92a] A. Bossi, N. Cocco, and S. Etalle. On safe folding. \nvat and J. Reynolds, editors, Algebraic Methods In PLILP 92, volume 631 of LNCS, pages 172 in Semantics, \nchapter 12, pages 412 433. CUP, 1985. [BCE92b] A. Bossi, N. Cocco, and S. Etalle. Transforming normal \nprograms by replacement. In Third Work-[Mah89] M. Maher. Correctness of a logic program trans\u00adshop on \nMets-Programming in Logic, META 92, formation system. Technical report, IBM -Wat\u00ad1992. son Research Center, \n1987 (revised 1989). 186. Springer-Verlag, 1992. [BD77] R. Burstall and J. Darlington. A transforma-[Mi177] \nR. Milner. Fully abstract models of the typed J\u00adtion system for developing recursive programs. calculus. \nTheoretical Computer Science, 4, 1977. JA CM, 24:44 67, January 1977. [Mi189] R. Milner. Communication \nand Concurrency. [Bir84] R. Bird. Using circular programs to eliminate Prentice Hall, 1989. multiple \ntraversals of data. Acts Informatzca, 21:239-250, 1984. [MW79] Z. Manna and R. Waldinger. Synthesis: \nDreams d programs. Transactions on Programming Lan\u00ad[BK83] G. Boudol and L. Kott. Recursion induction \nprin\u00ad guages and Systems, 5(4), 1979. ciple revisited. Theoretical Computer Science, 22:135-173, 1983. \n[Pa193] J. Palsberg. Correctness of binding time analysis, Journal of Functional Programming, 3(3), 1993. \n[CK93] C. Consel and S. Khoo. On-line and off-line par\u00adtial evaluation: Semantic specification and cor-[P1075] \nG. D. Plotkin. Call-by-name, Call-by-value and rectness proofs. Technical report, Yale Univer-the )-calculus. \nTheoret~cal Computer Sctence, sity, April 1993. 1(1):125 159, 1975. [COU79] B. Courcell. Infinite trees \nin normal form and re-[PP91] M Proietti and A. Pettorossi. Semantics preserv\u00adcursive equations having \na unique solution. Math-ing transformation rules for Prolog. In Proceed\u00adematical Systems Theory, 13:131 \n180, 1979. ings of the symposium on Partial Evaluation and Semantics-Based Program Manipulation. ACM \n[COU86] B. Courcelle. Equivalences and transformations press, SIGPLAN notices, 26(9), September 1991. \nof regular systems applications to recursive pro\u00adgram schemes and grammars. Theoretical Com\u00ad [PP93] \nA. Pettorossi and M Proietti. Transformation puter Science, 42:1 122, 1986. of logic programs: Foundations \nand techniques. Technical Report R 369, CNR Istituto di Anal\u00ad [FFK87] M. Felleisen, D. Friedman, and \nE. Kohlbecker. A isi dei Sistemi ed Informatica, Rome, November synt act ic theory of sequential control. \nTheoretical 1993. Computer Sctence, 52:205 237, 1987. [Red89] U. Reddy. Rewriting techniques for program \nsyn\u00ad [Gom92] C. Gomard. A self-applicable partial evaluator for thesis. In Rewriting Techniques and Applications,the \nlambda calculus: correctness and pragmatic. number 355 in LNCS, pages 388 403. Springer-ACM TOPLAS, 14(2):147-172, \n1992. Verlag, 1989. [GS91] P. Gardner and J. Shepherdson. Unfold/fold transformations of logic programs. \nIn J.-L. Lassez and G. Plotkin, editors, Computational Logic: Essays in Honor of Alan Robinson. 1991. \n 230 [RFJ89] C. Runciman, M. Firth, and N. Jagger. Trans-Operational Semantics formation in a non-strict \nlanguage: A-n approach to instantiation. In Functional Programming, Glasgow 1989: Proceedings of the \nFirst Glasgow Workshop on Functional Programming, Work\u00ad shops in Computing. Springer Verlag, August 1989. \n[sar191] D. Sands. Operational theories of improvement in functional languages (extended abstract). In \nProceedings of the Fourth Glasgow Workshop on Functional Programming, pages 298 311, Skye, August 1991. \nSpringer Workshop Series. [San93] D. Sands. A naive time analysis and its theory of cost equivalence. \nTOPPS report D-173, DIKU, 1993. To appear: Journal of Logic and Compu\u00adtation. [San94] D. Sands. Total \ncorrectness and improvement in the transformation of functional programs. DIKU, University of Copenhagen, \nUnpublished (53 pages), May 1994. [Sat90] T. Sate. An equivalence preserving first order unfold/fold \ntransformation system. In 2nd Inter\u00adnational Conference on Algebrazc and Logic Pro\u00adgramming, volume 462 \nof LNCS, pages 175-188. Springer-Verlag, 1990. [Sch80] W. Scherlis. Expression Procedures and Program \nDerivation. PhD thesis, Dept. of Computer Sci\u00adence, Stanford, 1980. Report No. STAN-CS-80\u00ad 818. [Sch81] \nW. L. Scherlis. Program improvement by internal specialisation. In 8 th Symposium on Principals of Programming \nLanguages. ACM, 1981. [Sek93] H. Seki. Unfold/fold transformation of general logic programs for the well-founded \nsemantics. J. Logic Programming, 16:5 23, 1993. [TS84] H. Tamaki and T. Sate. Unfold/fold transfor\u00admat \nion of logic programs. In S. Tarnlund, edi\u00adtor, 2nd International Logic Programming Con\u00adference, pages \n127-138, 1984. [Tur86] V. F. Turchin. The concept of a supercompiler. ToPLaS, 8:292 325, July 1986. [Wad90] \nP. Wadler. Deforestation: transforming programs to eliminate trees. Theoretical Computer Sczence, 73:231-248, \n1990. Preliminary version in ESOP 88, LNCS 300. [Wan93] M. Wand. Specifying the correctness of binding \ntime analysis. Journal of Functional Program\u00adming, 3(3), 1993. [Zhu94] Hong Zhu. How powerful are folding/unfolding \ntransformations? J. Functional Programing, 4(1):89 112, January 1994. A Appendix In this appendix we \nprovide some details not included in the main text, leadlng to an outline of the proof of the improve\u00adment \ntheorem. Omitted proofs can be found in [San94]. The semantics for the language defined in section 2 \nis pre\u00adsented in terms of a single step reduction relation using the notion of a reduction context [FFK87]. \nReduction contexts, ranged over by R, are contexts containing a single hole which is used to identify \nthe next expression to be evaluated (re\u00adduced). Definition A.1 A reduction context R is given inductmely \nby the following grammar R = []1 Re[RQelwQR I case l?of c~(;l) : e~... cn(;n) : en I P(~,~, 4 Now we \ndefine the one step reduction relation on closed ex\u00adpressions. We assume that each primitive function \np is given meaning by a partial function [p] from vectors of constants (according to the arity of p) \nto the constants (nullary con\u00adstructors). We do not need to specify the exact set of prim\u00aditive functions; \nit will suffice to note that they are strict (all operands must evaluate to weak head normal form before \nthe application of primitive-function can), and are only de\u00adfined over constants, not over arbitrary \nweak head normal forms. Definition A.2 One-step reduction + is the least relatton on closed expressions \nsatisfying the rules given in Figure 3. In each rule of the form R[e] ++ R[e ] in Figure 3, the ex\u00adpression \ne is referred to as a redez. The one step evaluation relation is deterministic; this relies on the fact \nthat if el + ez then el can be uniquely factored into a reduction context R and a redex e such that el \n= R[e ]. Recall that the weak head normal forms are constructor expressions and partially applied functions \nf el . . . ek, k < ~(f). Definition A.3 Closed expression e converges to weak head normal form w, eJJ.w, \nif and only tf e I+* w (where ++ is the transitive reflexive closure of ++). Further, if this evaluation \nrequires n rewrites using function-application rule (*) then we wrute e{nw. A Characterisation of Improvement \nWe give a more convenient definition of improvement simu\u00adlation in terms of the following operation: \nDefinition A.4 Given a relation R on closed expressions, we define R + to be the least relation on weak \nhead normal forms such that c(el . . . en) R+ c(e~ . ..e~)if e~Rej. iG{l. ..n}  (fel . . . e%) Rt(gej \n. . . e;) if for all closed e, (fel... ele)R(ge{. ..e~e)  Definition A.5 A relat~on XR on closed expressions \nis an improvement simulation if for all el, ez, whenever el D3 ez, if elvnwl then ezlJ<nwz for some wz \nsuch that W1 ZRt WZ. Lemma A.6 (Lemma ~. 6, restated) For all e, e , e ~ e if and only zf there exists \nan improve\u00adment simulation XR such that for all closing substitutions 0, eu %? e u. R[f el.. .eaf] * \nR[ef{el ea f/xl . ..zaf}l (*) R[w@w ] F+ R[w w ] R[case c,(d) of cI(iJ) : el . . . cn(in) : en ] - R[e%{z/;i}] \n(o<z<n) I?[p(z)] 1+ q. ] (If [p]z = c ) Figure 3: One-step reduction rules PROOF. The main part of the \nproof (~) follows from [San91] (Theorem 2.14), after recasting the language into the ap\u00adpropriate syntactic \nform. The (+) direction follows from the fact that the language has sufficiently many destructors although \nthe details are somewhat tedious (see the corre\u00adsponding proof about a similar relation in [San93] [Appendix \nB]). 0 Definition A. i A relationg XR on closed expressions as an improvement simulation up to improvement \n(abbreviated to an i-simulation) af for all el, ez, whenever el XR ez, tf el$nwl then ez.l,l<nwz for \nsome W2 such that W1 ~; R ; ~ W2 . Proposition A.8 To prove el ~ ez at M suficzent to find an i-simulation \nwhich contains each closed tnstance of the paw. Proof of the Improvement Theorem Now we can sketch the \nco~rectness proof of the main theo\u00adrem. In what follows let f s fl ..f~and~=gl...g~be as in the statement \nof Theorem 4.3. Under the conditions of the theorem we must show that ~ ~ ~. We do this by constructing \na relation that contains (f ~, gl ), and which we show to be an i-simulation up-to. Definition A.9 Define \nrelaihon 23 on closed expressions To show that 2S is an improvement simulation up-to im\u00adprovement we \nwill need some technical results connecting 22S and the one step reduction relation +. Recall that improvement \nis measured in terms of the number of steps of the function application rule. If e * e according to this \nrule then we write e &#38; e . If e + e by any other rule (the redexes are disjoint, so only one rule \ncan apply) then we write e &#38; e . In what follows let ++ denote the transitive reflexive closure of \n~. The following proposi\u00adtion details the interactions between the one-step reduction and the relation \n22S. Proposition A.1O (i) If el 73 ez then el 6 WHNF tf and only tf ez E Wmw?. (u) If el Z-$ ez and el \n&#38;. e; then ez&#38; e; and e; ZS e;. (ZZZ) If el ZS ez and el ~ ej then e2 ~ ej and e~(~; ZS )ej. \nIf S and T are relations then S; T denotes the relational\u00adcomposition given by: a (S; T ) b if and only \nif a S a and a T b for some a , Now we can outline the proof of Theorem 4.3: PROOF. (Theorem 4.3) Assume \nel ZS ez and elJJ wl. We show by complete induction on n that e2.1,1~n W2 for some W2 such that W1( ~; \n2S ) wz. By simple properties of ~ and LS we can see that this is sufficient to show that D is an i\u00ad \nsimulation up-to, and hence that ft kg%. Let d = {F/j} and T = {~/j}. We have, by definition of 22S, \nan expression e such that e$ z el and ey = e2. Base (n = O): Then eq$+ WI, By Proposition A. IO(ii), \necj + e for some e such that WI Z.S e . By Proposi\u00adtion A. IO(i) it follows that e E WHNF and hence e \n= W2 and we are done. Induction (n ~ 1): Since e@J.n21 then for some eo, el, e~ +++ eo, e. ~ el and elun-1WI. \nWe summarise the re\u00admaining argument with the following commuting diagram, in which we complete the squares \n(A) (D) from top left to bottom right: lx (A) m (B) en-r W2 W2 (A) by Proposition A. IO(ii); (B) by \nProposition A. 10(iii); (c) since improvement is an improvement simulation;  (D) by the induction hypothesis. \n   \n\t\t\t", "proc_id": "199448", "abstract": "<p>The goal of program transformation is to improve efficiency while preserving meaning. One of the best known transformation techniques is Burstall and Darlington's unfold-fold method. Unfortunately the unfold-fold method itself guarantees neither improvement in efficiency nor total-correctness. The correctness problem for unfold-fold is an instance of a strictly more general problem: transformation by locally equivalence-preserving steps does not necessarily preserve (global) equivalence.</p><p>This paper presents a condition for the total correctness of transformations on recursive programs, which, for the first time, deals with higher-order functional languages (both strict and non-strict) including lazy data structures. The main technical result is an <italic>improvement  theorem</italic> which says that if the local transformation steps are guided by certain optimisation concerns (a fairly natural condition for a transformation, then correctness of the transformation follows.</p><p>The improvement theorem makes essential use of a formalised improvement-theory; as a rather pleasing corollary it also guarantees that the transformed program is a formal improvement over the original. The theorem has immediate practical consequences:</p><p>&#8226; It is a powerful tool for proving the correctness of existing transformation methods for higher-order functional programs, without having to ignore crucial factors such as <italic>memoization</italic> or <italic>folding</italic>. We have applied the theorem to obtain a particularly simple proof of correctness  for a higher-order variant of <italic>deforestation</italic>.</p><p>&#8226; It yields a simple syntactic method for guiding and constraining the unfold/fold method in the general case so that total correctness (and improvement) is always guaranteed.</p>", "authors": [{"name": "David Sands", "author_profile_id": "81100313762", "affiliation": "University of Copenhagen, DIKU, Universitetsparken, 1, 2100 K&#248;benhavn &#248;, Denmark", "person_id": "PP14114603", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/199448.199485", "year": "1995", "article_id": "199485", "conference": "POPL", "title": "Total correctness by local improvement in program transformation", "url": "http://dl.acm.org/citation.cfm?id=199485"}