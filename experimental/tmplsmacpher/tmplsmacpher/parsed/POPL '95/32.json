{"article_publication_date": "01-25-1995", "fulltext": "\n PARAMETRIC PROGRAM SLICING John Field G. Ramalingam IBM T.J.WatsonResearch Center, P.O. Box 704, Yorktown \nHeights, NY, 10598, USA {j field, rama}@watson. ibm. com Frank Tip* CWI, P.O. Box 94079, 1090 GB Amsterdam, \nThe Netherlands tip@cwi. nl Abstract Progmrnslicing is a techniquefor isolating computatiorratthreadsin \npro\u00ad grams. hrtbis paper,we showhowtomecharricatly extracta family of practicalatgonthmsforcomputirrg \nslicesdirecttyfrom semanticspecif\u00ad ications.Thesealgorithms are based on combining the notion of dynamic \ndependence trackirrg interm rewriting systems [13] with aprograrrr rep\u00ad resentation whose behavior is \ndefined viarm equational logic [12]. Our approach is distinguished by the fact that changes to the behavior \nof the slicing algorithm can be accomplished through simple changes in rewriting rules that define the \nsemantics of the program representation. Thus, e.g., different notions of dependence maybe specified, \nproperties of larrguage\u00ad specific datatypes carr be exploited, arrd various time, space, and precision \ntradeoffs maybe made. This flexibility enables ustogeneralize thetradi\u00ad tiorral notions of static and \ndynamic slices to that of a constrained slice, where any subset of the inputs of a progmm may be supplied. \n1 Introduction Program slicing is an important technique for program understand\u00adingand program analysis. \nInformally, aprogram slice consists of the program parts that (potentially) affect the values of specified \nvariables at some designated program point the slicing cr-iter-br. Although originally proposed as a \nmeans for program debugging [33], it has subsequently been used for performing such diverse tasks as \nprogram integration and differencing [16], software mainte\u00adnance and testing [15, 8], compiler tuning \n[23], and parallelization of sequential code [32]. In this paper, redescribe how a family of practical \nslicing algorithms can be derived directly from semantic specifications. The title of this paper is a \ntriple entendre, in the sense that our technique is parameterized in three respects: . We generalize \nthe traditional notions of static and dynamic slices to that of a constrained slice. Static and dynamic \nslices have previously been computed by different techniques. By contrast, ourapproach provides ageneric \nalgorithm for computing constrained slices. Supportedh part by rfre European Union under ESPRIT project \n# 5399 (Compiler Gerreration for Parallel h tachines+OMPARE). Part oftlnswork wasdone while this author \nwas at IBM. Permission to copy w thout fee all or part of this material is granted provided that the \ncopies are not made or distributed for direct commercial advantage, the ACM copyright notice and the \ntitle of the publication and its date appear, and notice is given that copying is by permission of the \nAssociation of Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specMc \npermission. POPL 951/95 San Francisco CA USA @ 1995 ACM 0-89791-692-1/95/0001 ....$3.50 . Given a well-defined \nspecification of a translation from a programming language to a common intermediate represen\u00ad tation \ncalled PIM [12], we automatically extract asemantically well-founded larrguage-specz$calgorithm for computing \ncon\u00ad strained slices. An advantage of this approach is that only the PfM translation is language dependent; \nthe mechanics of slicing itself are independent of the language. PIM S semantics (and thus that of the \nsource language via translation) is defined by a set of rewriting rules. These roles implicitly carry \nout many techniques usedin optimizing com\u00adpilers, e.g., conditional constant propagation and dead code \nelimination. The slices we obtain are thus often more precise than those computed by previous algorithms. \nBy choosing different subsetsof rules or adding additional rules, the preci\u00adsion of the analysis, aswell \nasits time and spacecomplexity, may be readily varied. We illustrate the flexible nature of our approach \nby defining several extensions to PtM s core logic. These variants describe differing treatments of loop \nsemantics, and consequently define differing slice behaviors. One of the primary contributions of this \npaper is an algorithm for computing constrained slices. Despite the myriad variations on the theme of \nslicing that can be found in the literature [28], almost all existing slicing algorithms fall into one \nof two classes: static slicing algorithms, which make no assumptions about the inputs to the program, \nand consequently compute slices that are valid for all possible input instances, and dynamic slicing \nalgorithms, which accept a specific instantiation of all inputs, and compute slices valid only for that \nspecific case. A constrained slice is valid for all instantiation of the inputs that satisf y a given \nset of constraints. In the sequel, we will primarily consider constraints that specify the values of \nsome subset of the input parameters of the program. The relation between constrained slicing, static \nslicing, and dy\u00adnamic slicing is straightforward a fully constrained slice (with every input a fixed \nconstant) is a dynamic slice, and a fully unconstrained slice is a static slice. We believe that constrained \nslicing can be more useful than static or dynatnic slicing in helping programmers understand programs, \nby enabling the programmer to supply a variety of plausible input scenarios that the slicing system can \nexploit to simplify the slice obtained. While Venkatesh has defined a notion of a quasi-static slice \n[30] similar to that of a constrained slice, we know of no previous work that describes how such slices \nmay be computed. In a recent paper [24], Ning et al. describe a reverse engineering tool that permits \nusers to specify constraints on variables and extract conditional slices, but they do not specify how \nthese slices are computed or how powerful the constraints can be. One might consider combin\u00ading partial \nevaluation of programs with static slicing to compute Q = ?P; r) = ?p; p=?p; q = ?Q; q =?Q; q=?Q; if \n(p>O) if (p>O) if (p>O) ptr =&#38;y; ptr . &#38;y; ptr =&#38;y; else else else ptr =&#38;x; ptr . &#38;x; \nif(q <O){ if (q< O) { if (q < O) x = 17; y =18; y =18; } else { } else { else { x= 19: x=19: y =20; y \n=20; y =20; } result = *ptr; _l&#38;~ (a) (b) (c) p=?P; p=?P; q=?Q; q=7Q; if (p>O) lf (p>o) ptr =&#38;y; \nptr =&#38;y: else else II lf (q < o) lf(q <o){ ; else { }k- f y = 20; ;:g; }} result = *ptr; result = \n*ptr; (d) (d ) Figure 1: (a) Example program (= static slice). (h) Constrained slice with ?P :== 5. (c) \nConstrained slice with ?Q := 3. (d) Constrained slice with ?P := 5, ?Q := 3 (= dynamic slice). (d ) Non-postprocessed \nterm stice corresponding to (d). constrained slices, but, as will be explained later, this does not lead \nto satisfactory results. The feasibility of the ideas in this paper has been demonstrated by a successful \nprototype implementation of the PtM logic and trans\u00adlators for significant subsets of such disparate \nlanguages as C and Cobol using the ASF+SDF Meta-environment [19], a programming environment generator \nbased on algebraic specifications. 2 Overview In this section, we will give a brief overview of our approach \nusing examples. Details will follow in subsequent sections. 2.1 Motivating Example Fig. 1(a) shows an \nexample program written in PC, a C subset that we will use for all the examples in this paper. PC has \nthe standard C syntax and semantics, with one extension: meta-variables like ? P and ?Q are used to represent \nunknown values or inputs. All data in PC are assumed to be integers or pointers; we also assume that \nno address arithmetic is used. When we discuss loops in Section 5, we will for simplicity further restrict \nour analysis to programs containing only constant L-values. The example of Fig. l(a) is not entirely \ntrivial, due to manipu\u00adlation of pointers in a conditional statement. The static slice with respect to \nthe final value of r e su 1 t consists of the entire program. The dynamic slice with respect to the final \nvalue of re su 1 t for input p = 5, q = 3 is shown in 1(d); note thrtt it does not im\u00admediately reveal \nthe effect of each input. The effect of input p = 5 is illustrated by the constrained slice of 1(b); \nclearly it causes the aliasing of *p t r to y, and thereby makes both assignments to x obsolete. In 1(c), \nthe effect of the other input, q = 3 is shown: the statements in the first branch of the second i f statement \nbecome irrelevant. Note that in general, it is not the case that a slice with respect to multiple constraints \nconsists of the intersection of the slices with respect to each constraint. In examples in the sequel, \nwe will use the double box notation of Fig. 1 to denote a slicing criterion and the constraints, if any, \non meta-variables. We will also use the terminology slice of P at x [given C] to denote the slice of \nP with respect to the final value of variable x [given meta-variable constraints C]. Slicing with respect \nto arbitrary expressions at intermediate program points will be discussed in Section. 4.6. 2.2 Slicing \nvia Rewriting PIM [12] consists of a rooted directed acyclic graph program representation and an equational \nlogic that operates on PtM graphs, These graphs can also be interpreted (or depicted) as terms after \nflattening. A subsystem of the full PIM logic defines a rewriting semantics for a program s PIM representation. \nRewriting rules can be used not only to execute programs, but also to perform various kinds of analysis \nby simplification of a program s PtM representa\u00adtion; each simplification step consists of the application \nof a rule of PIM s logic. To compute the slice of a program with respect to the final value of a variable \nz, we begin with a term that encodes (i) the abstract syntax tree (AST) of the program, (ii) the variable \nz that represents the slicing criterion, and (iii) a (possibly empty) set of additional constraints. \nNext, we translate the AST to a graph comprising its PtM representation. This translation is assumed \nto be defined by a rewriting system (although it need not necessarily be implemented that way). The resulting \ngraph is then simplified by repeated application of sets of rewriting rules derived from the PtM logic. \nThis reduction process is carried out using the technique of term graph rewn tirrg [4]. The graph that \nresults from the reduction process represents the final value of variable z (in terms of the unconstrained \nmeta-variables). During the reduction process, we maintain dynamic dependence relations [13] that relate \nnodes of the graph being manipulated to the AST. These relations are defined in a simple way directly \nfrom the structure of each rewriting rule, and will be discussed in more detail in Section 3. By tracing \nthe dynamic dependence relations from the simplified Pm-graph back to the AST yields, we finally derive \nthe slice of the AST with respect to z. The steps involved in the slicing process are depicted in Fig. \n2. This basic slicing algorithm is unusually flexible, in that it can be adapted to new languages simply \nby providing a source-to-Pw translator for the language. In addition, simple alterations to the rules \nor rewriting strategy can be used to affect the kinds of slice produced, as well as the time or space \ncomplexity of the reduction process. The ease with which we can handle constrained slices is due principally \nto the fact that the reduction process adapts itself to the presence or absence of information represented \nby constraints. 1 Although loops and recursive procedures admit a PIM graph representation with cycles, \nwe will use a simpler DAG representation for such conshucts in this paper. ., . extract sfice , ,,.. \n,,: , ,~., : , : ,., ~@:Giyb  $,ce,,ing abstract syntax tree \\ constraints Plm graph simplified Pim \ngraph slicing criterion .., term graph rewriting A ,.,... dynamic dependence relauons prrrseable slice \nFigure 2: Overview of our approach. As more information is available, more rules are applicable that \nhave the potential to further simplify the slice. 2.3 Term Slices and Parseable Slices Formally, our \nslices are contexts derived from the program s AST, i.e., a connected set of AST nodes in which certain \nsubtrees are omitted, leaving holes behind. By interpreting these contexts as open terms, all of the \nslices we compute are executable via the PIM rewriting semantics, in the sense that any syntactically \nvalid substitution for the holes in a term slice yields a program with the same behavior with respect \nto the slicing criterion2. It is often the case, however, that one wishes obtain aparseable representation \nof the slice (i.e., a syntactically well-formed AST without missing subtrees). Therefore, term slices \nmaybe optionally postprocessed in various ways to obtain parseable programs with identical behavior. \nFig. l(d ) depicts the term slice corresponding to 1(d) before postprocessing. Certain finedetailsarepresentinthistermslicethat \ndo not appear in Fig. 1(d), e.g., the L-values but not the R-values of certain assignment statements \nappear in the term slice. The advantage of term slices is that they have a consistent semantic interpretation, \nand are oblivious to a language s syntactic quirks. This is particularly important in a language like \nC, where virtually any expression can have a side effect, and thus for which some parts of an expression \ncan be relevant to a slice while others are not. Unfortunately, term slices often introduce a certain \namount of clutter not present in more ad-hoc algorithms; thus for the sake of clarity, most of the example \nslices we use in the sequel will be minimally postprocessed, primarily by replacing assignments with \na hole in the right-hand side by empty statements. We will distinguish parseable slices from term slices \nby using boxes in the latter to represent holes. 2.4 More Examples The example in Fig. 3 illustrates \nthe flexibility of our technique by showing some of the differing treatments of loops that are possible \n(loops will be further studied in Sec. 5). Fig. 3(b) depicts what we will call a pure dynamic slice at \nre SU1 t, given ?N := 5 and ?P := 1. Note that this slice includes the whi 1 e loop though it computes \nno value relevant to the criterion. This is the case because 2 More precisely, the term encoding the \noriginal program and the slicing criterion and tie term encoding the slice (with any syntactically vatid \nsubstitution for the holes) and the sticing criterion both reduce to the same termkdue. the underlying \nslicing algorithm faithfully reflects the standard se\u00admantics, under which there is a dependence between \nthe whi 1 e loop and the subsequent assignment to result. This phenome\u00adnon is noted in Cartwright and \nFelleisen s discussion of demand and control dependence [5]. This notion of dependence is also closely \nrelated to the notion of weak control dependence discussed by Podgurski and Clarke [26]. The slice in \nFig. 3(c), similar to the kind computed by Agrawal and Horgan [1], results from adding some simple equational \nrules to be discussed later. The same vari\u00adant of the slicing algorithm produces the result in Fig. 3(d), \nthough the program is non-terminating for the constraints specified un\u00adder the standard semantics. Previous \ndynamic slicing algorithms [1, 21] will not terminate for this input constraint. In this sense,our dynamic \nslicing algorithm is more consistently lazy . As a final example, consider the program in Fig. 4. Although \nabsurdly contrived, the example illustrates several important points. By not insisting that the slice \nbe parseable, we can make distinctions between assignment statements whose R-values are included but \nwhose L-values are excluded and vice versa, as Fig. 4(b) shows. We also see that it is possible to determine \nthat the values tested in a conditional are irrelevant to the slice, even though the body is relevant. \nIn general, our approach can make a variety of fine distinctions that other algorithms cannot. Fig. 4(c) \ngives an example of a conditional constraint. Such constraints can be handled by straightforward extensions \nto our ba\u00adsic algorithm. A detailed treatment of such constraints is outside the scope of this paper, \nbut we will discuss them briefly in Section 4.7. 3 Term Rewriting and Dynamic Dependence Tracking Our \napproach to slicing is based on extending the generic notion of dynamic dependence tracking in term rewriting \nsystems [13] to realistic programming languages. In this section, we review dynamic dependence tracking \nand the basic ideas behind term and graph rewriting. For further details on term rewriting, the reader \nis referred to the excellent tutorial survey of Klop [20]. We begin by considering two PIM rewriting \nrules that define simple boolean identities: v(T, p) + T (B1O) V(V(P1 , P2) , P3) --+ V(PI , V(P2 , P3)) \n(J314) A rewriting rule is used to replace a subterrn of a term that matches the rule s left hand side \nby the rule s right hand side. Variables (here, p, p., p., and PS) match any subterrw all other symbols \nmust n=?N; n= ?N: i=l; i=l; sum =O; while (i !=n) { while (i !=n) { sum =sum +i; l=i+l; j,=i+l: }} if \n(?P) if (?P) result = n* (n-1) /2; result = n* (n-1)/2; else else result . sum; (a) n=?N; n=?DJ: if [?P) \nif (?P) result = n* (n-1) /2; result = n* (n-1) /2; else else result given ?N := 5, ?P :== 1 II III-II \n(c) (d) Figure 3: (a) An example program. (h) Pure dynamic dice at resu 1t given ?N := 5, ?P := 1. (c) \nLazy dynamic slice at result given ?N := 5, ?P := 1. (d) Lazy dynamic slice at result given ?N := O, \n?P := 1. *(u .&#38;a) . ?A; *(Ptr =&#38;a) =?A; *(O =&#38;a) =?A; b = ?B; b=m; b=n; x=a; X=a; x=~; if \n(a<3) if (a<3) if (a<3) ptr =&#38;y; ptr . &#38;y; II else else else ptr . &#38;x; t3tr = &#38;x; II \nif (b<2) if (n<n) if (u<u) X=a; x=a; x. D (*Ptr) = 20; ( ptr) = n; (*ptr) = 20; given ~A := 2 (a) (b) \n(c) Figure 4 (a) An example program. (b) Constrained slice at x given ?A := 2. (c) Condkionat constrained \nslice at x given ?A > 5. F., creationrelation ~ ;.., residuation relation ~-.,, dynamic depedencerelation \n...... Figure 5: Example of creation and residuation relations. match exactly. By applying the rules \nabove, the term A(V(V(T, F) , A(F, T)) , F) may be rewritten as follows (subterms affected by rule applications \nare underlined): TO = A(V(V(T, F) , A(F, T)) , F) .+(B14J TI = A(V(T, V(F, A(F, T))) , F) ----+ (BIO) \nTz= A(T, F) Observe in the example above that the outer context A(. , F) ( . denotes a missimz subterm) \nis not affected at all. and therefore occurs in TO, TI, and TZ. Furthermore, the occurrence of variables \nPI, P2, and P3 in both the left-hand side and the right-hand side of (B 14) causes the subterms T, F, \nand A (F, T) of the underlined subterm of TO to reappear in T1. Also note that variable p occurs only \nin the left-hand side of (B 10): consequently, the subterm (of Tl) V (F, A (F, T)) matched against p \ndoes not reappear in T2. Thus, the subterm matched against p is irrelevant for producing the constant \nT in Tz: the creation of this subterm T only requires the presence of the matched symbols V and T . This \nobservation is the keystone of our reduction-based slicing technique: We track those subterms that are \nrelevant to each reduction steps; subterms that are relevant to no reduction step can then be eliminated \nfrom the slice. The tracking process determines not only which subterrns are relevant to a given reduction \nstep, but also how subterms are com\u00adbined and propagated by the reduction as a whole. To accomplish this \ntask, we define for each reduction step that takes a term Ti and yields a new term Ti+l the notions of \ncreation and residua\u00adtion. These are binary relations between the nodes of Ti and the nodes of T,+l. \nThe creation relation relates the new symbols in T,+l produced by the rewriting step to the nodes of \nTi that matched the symbols in the left-hand side of the rewriting rule (making the rewriting step possible). \nThe residuation relation relates every other node in Ti+l to the corresponding occurrence of the same \nnode in the T,+l. The dynamic dependence relation for a multi-step reduc\u00adtion ~ then consists, roughly \nspeaking3, of the transitive closure of creation and residuation relations for the rewriting steps in \n~. Fig. 5 shows all the relations for the example reduction discussed above. For any reduction r which \ntransforms a term T into a term T , a term slice with respect to some subcontext C of T is defined as \n3 The notions of creation and residuation become more comphcated in the presence of so-called left-nonlinear \nroles and collapse rules. The exact problems posed-by rhese rules are outside the scope. of this paper, \nbut are extensively discussed in [13]. dynamic dependence relations s --+.......-. ..... . / c .... \n,, c r/ ... : ,, ;;::: ,, :;;, :: :: ,: ,, : ,, :: :: : ,, :,: : ::, : ,,,if- -)j$,,.,.. y Figure 6: \nThe concept of dynamic dependence. the subcontext S of T that is found by tracing back the dynamic dependence \nrelations from C. The term slice S satisfies the follow\u00ading properties: (i) S reduces to a term C containing \ncontext C via a reduction #, and (ii) r is a subreduction of r. These properties are rendered pictorially \nin Fig. 6, and have the important implica\u00adtion that all the slices computed by our technique are effectively \nexecutable with respect to the rewriting semantics. Our implementation maintains the transitive dependence \nrela\u00adtions between the nodes of the initial term and the nodes of the current term of the reduction by \nstoring with each node n in the current term its term slice, which is the set of nodes in the initial \nterm to which n is related. (The dependence relations associated with individual rewriting steps are \nnot stored.) The term slice with respect to a subgraph S of T is then defined as the union of term slices \nwith respect to the nodes in S. Returning to the example of Fig. 5, we can determine the term slice with \nrespect to the constant T in Tz by tracing back all cre\u00adation and residuation relations to TO. By following \nthe transitive relations in Fig. 5; the reader may verify that this slice consists of the subcontext \nV(V(T , .) , .). 3.1 Efficient Implementation of Term Rewriting We implement term rewriting using the \ntechnique of term graph rewriting [4]. This technique extends the basic idea of term rewrit\u00ading from \nlabeled trees to rooted, labeled graphs, or term graphs. A term graph may be viewed as a term by traversing \nit from its root and replacing all shared subgraphs by separatecopies of their term representations. \nFor clarity, we will frequently depict PIM term graphs or subgraphs in flattened form as terms. (The \nflat\u00adtened representation of the graph Tz in Fig. 7, for instance, is V(A(T, F) , A(T, T)).) For certain \nkinds of rewriting rules, term graph rewriting has the effect of creating shared subgraphs where none \nexisted previously. Consider following PIM boolean rule: A(PI , V(P2 , P3)) = V(A(PI , P2) , A(PI , P3)) \n(=V In rule (B22), the variable pl appears twice on the right-hand side. Although the left-hand side \ninstance of pI in (B22) matches only a single subterm, the result of the rule application must contain \ntwo in\u00adstances of the subterm matched by pl. Rather than duplicating such a term, it can be shared, as \nillustrated by the example in Fig. 7, in which rule (B22) is applied to term 7 0 R A(V(T , F) , V(F , \nT)). We see also from Fig. 7 that the result of a single application of TI T, Av o v ,/v\\ /v\\ = : Q \n Q ? / \\ Figure 7: Creation of shared subgraphs and a shared reduction step using a graph rewriting \nimplementation. reduction rule (here, rule (B 10)) inside a shared subterm can also be shared, thus giving \nthe effect of multiple reductions for the price of one. In general, graph rewriting is performed by replacing \nthe sub\u00adgraph matched by a rule with the graph corresponding to the rule s right hand side. The nodes \nin a replaced subterm that are not ac\u00adcessible from elsewhere in the graph are reclaimed by a memory \nmanager. Since the PfM representation of programs contains many shared subgraphs, a graph rewriting implementation \nis critical to acceptable performance of the algorithm in practice. 4 PIM + Dynamic Dependence Tkacking \n= Slicing PIM was designed to generalize and rationalize many of the prop\u00aderties of commonly used graphical \nrepresentations for imperative programs such as SSA-form [7] and PDGs [10], and to provide a semantically \nsound but mechanizable framework for perform\u00ading program analysis and optimization. PIM S formal progenitor \nis Cartwright and Felleisen s notion of lazy store [5], interpreted oper\u00adationally rather than denotationally. \nUnlike SSA-form and PDGs, computations on addresses required for arrays or pointers are first\u00adclass citizens, \nand procedures and functions are integral parts of the formalism. 4.1 @To-PIM lkanslation Fig. 8 depicts \na very simple PC program, P1, its corresponding PIM representation, and several slicing-related structures. \nThe graph depicted in Fig. 8, denoted by Slice (F 1, x, ()), is generated by translating P1 to its corresponding \nPIM representation and embedding the resulting graph (labeled SP1 ) in a graph corre\u00ad sponding to the \nslicing criterion x. Slice(Pl, x, ()) is simply the PrM expression denoting the final value of the variable \nx. Only a small number of graph edges, primarily those connecting shared subgraphs to multiple parents \nare shown explicitly in Fig. 8; we have flattened most other subgraphs for clarity. Parent nodes in the \ngraph are depicted below their children to emphasize the cor\u00ad respondence between program constructs \nand corresponding PIM subgraphs. SPI is generated by a simple syntax-directed translation. A representative \nsubset of the translation rules appears in Fig. 9. The translation is specified in the Natural Semantics \nstyle [17] for clar\u00ad ity; however, the translation is implemented by a pure rewriting system4, The translation \nuses several sequent forms corresponding to the principal C syntactic components. The general form for \nthese sequents is: Skc +-t 4A rewriting system can be derived from simple classes of Natural Semantics \nspecifications such as the one in Fig. 8 in a purely mechanical fashion. Program ~ PIM Translation (P) \n0S h Stint K Stint +stmt *pgm u u .. .. . . . . .. . .. . .. . . . .dY, . . . . . . . . . . . . :  \n  (s, ) s k SOuk { Stint List Stint } +stmi +Strnt u, ~ s 1\u00ad { Stint List Stint } +Stmt u o Ur ...,.,. \n) (SF., \\ ( @ iddr(i))~ ;= Slice(~, ()) ... initial trarmtlve dependence relatlons i < ; between C \nsyntsx tree and PIM graph ...; (only a subset of edges are depicted) ,.. ~ ~ denotes mot node of enclosed \ngraph Figure 8: PI and its PIM representation, Spl. Major corresponding strnctnres in PI and SPI are \nlocated side-by-side. Such a sequent maybe read as IJC construct c translates to PIM term t,given initial \n(PIM) stores. ~ is subscripted by Pgm , Exp , or LValue , depending on whether a statement, expression, \nor L-value (address), respectively, is being translated. Pure expressions (those having no side-effects) \nand unpure expressions are distinguished in the translation process; subscripts p and u are used to denote \nthe two types. The shared subgraphs in SP1 arise from repeated instances of store variables in the antecedents \nof the translation rules in Fig. 9, as illustrated in Fig. 7. The translation process establishes transitive \ndependence rela\u00adtions between nodes of the program s AST and the PINI graph SP1, as described in Section \n3. Fig. 8 depicts a representative subset of these relations for the root nodes of certain subtrees of \nthe syntax tree of PI. We have used vestigial arrows in the syntax tree to indi\u00adcate that nodes are referred \nto by some set of nodes in the PtM graph. We have also depicted statements of PI and their corresponding \nPfM subgraphs side-by-side.  4.2 Overview of PfM In this section, we briefly outline the function of \nvarious PfM sub\u00adstructures using program PI and its PIM translation, SP1. The graph SP1 as a whole is \na PtM store structure5, essentially an abstract term representation of memory. SP1 is constructed from \nthe sequential composition (using the o operator) of substores corresponding to the statements comprising \nP1. The subgraphs accessible from boxes labeled S1-S4 in Fig. 8 correspond to the four assignment statements \nin PI. The simplest form of store is a cell such as SI s {addr(p) E+ [T D PV]} 5 For clarity, Flg S does \nnot depict certsm empty stores created by the translation process; this elision will be irrelevant in \nthe sequel. s 0 UE F Stint >S+mt us(ss) 7J&#38; = -l(=(v.E , o)) s k if ( Exp ) Stint =st~t tiE O(U EDW) \n s h Exp *ExpP u (El) s F ExpP = Exp (V, 0s) (LP) s 1-Id >LV.l..P adrWd) Figure 9: Representative translation \nrules for PC. A store cell associates an address expression (here addr(p)) with a merge structure, (here \n[T D PV ] ). Constant addresses such as addr(p) represent ordinary variables. More generally, address \nexpressions are used when addresses are computed, e.g., in pointer references. 0. is used to denote the \nempty store. Merge structures are a special kind of conditional construct con\u00adtaining ordered guarded \nexpressions. The simplest form of merge expression is a merge cell such as [T D Pv], in which some boolean \npredicate (here, T) guards a value (here, the free PM variable PV representing the PC meta-variable ? \nP). The formal consequence of the presence of a free variable is that any subsequent rewriting\u00adbased \nanalysis is valid for any instantiation of the free variable. Merge expressions ml and mz maybe composed \ninto ordered lists of the form ml o~ mz, in which the rightmost guarded cell takes precedence. Such lists \ncorrespond roughly to Lisp cond ex\u00adpressions, and represent information similar to SSA-form + nodes [7], \nparticularly the gated SSA variant of [3]. Unlike normal con\u00additional expressions, however, merges cannot \nevaluate to values unless they are referred to in a special context represented by the selection operation, \n ! . Among other places, this operator is used in the translation of every variable reference. Spl contains \nno non-trivial merge structures, but such structures will arise in the simplification process. o~ denotes \nthe null merge structure. In the sequel, we will often drop subscripts distinguishing related store and \nmerge constructs when no confusion will arise. In addition to guards in merge cells, stores such as S5 \n(which correspondstothe i f statementasawhole)mayalsobeguarded. The guard expression VI corresponds to \nthe i f s predicate expres\u00adsion. Consistent with standard C semantics, the guard VI tests whether the \nvalue of the variable p is nonzero. The general form for the PIM graph constructed for a slice of program \nPat x given constraints ?Xl := Expl, ....?Xn := EXPn is Slice(F , x,(?Xl := Expl,..., ?x~ := Expn)) \n-(( Sp@addr(x))!) [XIV := v1,... ,X~v := v~] where SP is the PIM store to which P compiles, the Xlv \nare free variables corresponding to the meta-variables and the W, are PIM graphs comesponding tothevalue \nof the Expt (ignoring side ef\u00adfects). SIice(P, x,(... ))isthe P~representation of the value of x after \nexecution of P, with substitutions for free variables defined by the constraints. 4.3 PM Rewriting and \nElimination of Dependence PIM S equational logic consists of an operational subsystem, PIM-, plus a set \nof additional non-oriented equational rules for reasoning about operational equivalences in PIM+, instances \nof which can also be oriented for use in analysis. Pm-is confluent and nor\u00admalizing (assuming an appropriate \nstrategy), thus it can be viewed as defining an operational semantics or interpreter for PrM terms. An \nimportant subsystem of PM+ that defines the semantics of pro\u00adgrams without loops or procedures, PIM~, \nis canonical, that is, strongly normalizing as well as confluent. PM+ can be enriched with certain oriented \ninstances of rules in (PIM PIM-) in such a way that confluence is preserved on closed terms, and such \nthat unique normal forms for open terms exist up to certain trivial per\u00admutations. PIM S rules and subsystem \nstructure are described in detail in [12]; key subsystems are reviewed in Appendix A. Given a program \nP and a slicing criterion x, we use normalizing sets of oriented PIM equations to simplij$ Slice (P, \nx, (. ..) ) graphs by reducing them to normal (i.e., irreducible) forms. From the point of view of slicing, \nthe goal of this simplification process is to eliminate in a sound, systema~ic way, as many subgraphs \nof Slice(P, x, (. . .) ) as possible that do not aflect its behavior. 4.4 Reduction of Unconstrained \nand Constrained Slices Fig. 10 depicts key steps in the reductions of Slice(Pl, x, ( ? P := O)) and SIice(Pl, \nx, ()), the slices of PI that result from these reductions, and certain dependence relations for reduction \nsteps that are critical to producing the slices. These reductions share a common initial subsequence \nthat is independent of the substitution generated in the constrained case. We have numbered certain im\u00adportant \nintermediate graphs in the reductions. The interpretation of several of these graphs (depicted in flattened \nform) is as follows: Graph (1) is the flattened andabbreviatedformof Slice (PI, x, ()). Graph (2) results \nfrom multiple applications of the rule (s, 0. s,) (Q) rJ -(s, (!3 v) Om (s, cl v) (s4) which have the \neffect of distributing the reference to the variable x, addr(x), to the sequence of substores S1, SZ, \nSs, Ss. Graph (3) results from applications of the rule {v, w7n}@v, + =(VI , w) D~ m (S1) to all but \nthe rightmost subgraph. (S 1) has the effect of converting references to store cells into conditional \ntests comparing the cell and dereferencing addresses; these predicates guard the merge cells MI, Mz, \nM3, and Md, which are part of the original PIM graph SP1. Graph (4) results from evaluation of address \ncomparisons. The comparison fails for assignments represented by S1 and Sz (which are irrelevant to x) \nand succeeds in the case of S, and S4 (which both contain assignments to x). At (5), references to irrel\u00adevant \nassignments have been reduced to null merges. At (6), after eliminating null stores, the remaining expressions \nessentially repre\u00adsent the two definitions of x that reach its final value. Graph (7) is derived by first \nsimplifying the expression containing merge struc\u00adture Ms, yielding a merge cell containing the free \nmeta-variable ? P, then combining the PIM expression representing the predicate guarding the if statement, \nV1, with a predicate derived from the address comparison for the nested store for the assignment in store \nSA (representing the result of the assignment inside the if). The reduction thus far has the effect of \neliminating all assign\u00adments irrelevant to the final value of x. At this point, the reductions in the \nconstrained and unconstrained cases diverge: 4.4.1 Constrained Case: Slice(Pl, x, ( ? P := O)) In the \nconstrained case, PV is bound to O, i.e., is false. In step (8a), the highlighted application of the \nrule has the effect of eliminating the body of the if from the final slice. This can be seen in detail \nin the exploded (L6) rule application in Fig. 10. In this case, the only transitive dependence edges \nlinking the constructs in the body of the if statement and the identifier p in the assignment y = p; \nhave their origin in the subgraph S4. When this subgraph is eliminated by the application of (L6), the \nconstructs effectively disappear from the slice. While it may appear that the slice results entirely \nfrom the application of a single role, this rule is only the last of several rules that eliminate transitive \nedges from PIM nodes to the omitted constructs in P1. Only when the last edges are eliminated does the \nconstruct dk.appear from the slice. Other rules have the effect of combining dependence edges emanating \nfrom several intermediate nodes into a single node (as the two single-step dependence edges in the depiction \nof role (L6) illustrate).  4.4.2 Unconstrained Case: Slice(Pi, x, ()) The unconstrained case is somewhat \nmore interesting than the con\u00adstrained case: Although we do not know the value of PV and thus cannot \neffectively evaluate the i f statement in PI, we discover that the two reaching definitions for x both \nassign the same value to x, namely, Pv. Application of several rules allows us to com\u00adbine guards of \nmerge cells with the same guarded value into the disjunctive expression shown in (1 lb). The next step, \nthe reduction of (1 lb) to (12b), discovers that the predicate s value itself is irrelevant to the final \nvalue of x. As the exploded view this rewrite step illustrates, there is no transitive dependence between \nthe predicate p of the source AST and any of the nodes in the resulting term (12b) (or the final term \n(13b)). Consequently, the unconstrained slice does not contain the predicate of the i f statement, though \nit does contain assignment statement within the i f statement. (1) ((sl. s2. s3 o ,S5) @ addr(x) ) ! \n~ (2) ( (S, @addr(x)) Q (S 2 @ addr(x)) (.Sj@addr(x)) o ((lj D.S4) @addr(x)) )! ~ (3) ((=( addr(p), addr(x))D \nMl). (=(addr(y), addr(x))DM2) 0 (=(addr(x), addr(x))DM3) 0 ((lj DS4) 02addr(x)) )! i (4) ( (F DM1)  \n(F DM2) ; (T DM3) o (( V1DS4) @addr(x)) )! (5) ( d o d : (T DM3) o ((~DS4) @addr(x)) )! ~ (6) (( TDM3) \n. ~~DS4) @I addr(x) ) ! (7) ([ TDPV] o (-(=( :v, O)) D S4) @addr(x) ) ! P := o Pv unconstrained [7aJ \n([ TPO] 0(-( =(0, 0)) DS4)@addr(x) )! [7b) ( [ TD Pv] o (-(=( PV> O)) D S4 ) @ addr(x) ) ! (8.J ( [ T \nD O ] 4* ,---------7 (F D S4)j@addr(x) ) ! (8b) ( [ T D Pv] 4 : [A(=(addr(x), addr(x)), -(=( PV, O))) \nDV ] ) ! ..-.. ------------4 (Lo): ---- J (9s) ( [ T D IJ 1 O~,d @ addr(x) ) ! (9b)([TDPvl ~[A(T, -(=( \nPv>Il)))D\\l) ! i (Ioa)([TDO] ~;)! (lob) ([ TDPv] ~[-(=(Pv, O)) DPv]) ! (11a) i [T~O]! (WV{T , -(+%.-)x)D \n----- Pv ] ! .\u00ad . . . . . . , ~(BIO)L----- ------------------ -------..\\ (12a) ; (12b) [ ;_T j-~\u00ad i; \n] ! ..J..... . ....... ........ x given ?P:= O ml trmsitwe dynamic dependence relations  . (not \natl edges are depicted) v , single-step dynamic dependence relations ... ---o\u00ad dependence edge deleted \nduring reducaon (not W edgesaredepicted) : ,...... denotes root node of enclosed graph ,: ... denotes \nall ncdes ln enclosed graph Figure 10: Reduction of Slice (F I, x, ()) arrdSlice(Pl, x, (?P := O)). Stepsin \nwhich removal of dependenceedgeseliminates constructsfrom slicesare highlighted along with the resulting \nslices. 386 Slices which contain statements from the arms of a conditional statementbut not its predicate, \nareunusual enough to deservesome discussion. Such slices indicate that the vahre of the predicate it\u00adself \nis irrelevant, even though the conditional statement contains some relevant statement, e.g., an assignment \nto some relevant vari\u00adable. Such situations can arise in realistic programs. Consider, for example, the \nstatement if (P) f(foo); else f(bar); where the procedure f has some side-effect on some variable x of \ninterest, and where this side-effect itself is independent of the argu\u00adment to the procedure. Here, the \ntwo call statements are relevant to the final value of x, though the predicate itself is irrelevant. \nThis re\u00adflects a kind of reasoning that programmers do use when analyzing a program backwards, and can \nresult in substantially smaller slices because of the elimination of the statements that the predicate \nitself depends on. The possibilities for computing more precise slices in this fashion are even greater \nin the case of constrained slicing. 4.5 Slicing and Reduction Strategies AS PIM-isaconfluent rewriting \nsystem, reductions maybe per\u00adformed anywhere in a graph without affecting the final term pro\u00adduced(assuming \nthereduction terminates at all). This stateless property of reduction systems accommodates a variety \nof perfor\u00admancetradeoffs defived from va~ingtie reduction strategy. We useanoutennost or lazy strategy, \nwhlchensures that only steps thatcontribute toafinal result are performed. (Note, however, that thereduction \ndepicted in Fig. 10usesa strategy that is not strictly outermost to better illustrate the properties \nof certain intermediate terms). Alternatively, the P~representation of theentire program could be normalized \neagerly prior to the specification of any slic\u00ading criterion; those steps specific to the criterion or \nconstraints could be performed later. Reduction strategies canalso have aneffect on slices. In the constrained \ncase, both of the reductions depicted in Fig. 10 are valid, and, consequently both of the slices depicted \nare also valid. Slices are therefore not necessarily unique, even when the underly\u00adingreduction system \nis confluent. However, ourreduction strategy favors the left reduction over the right one in the constrained \ncase. Intuitively, this favors a standard execution semantics that cor\u00adresponds most closely with results \nof traditional program slicing algorithms. 4.6 Slicing at Intermediate Program Points Although our discussion \nthus far has concentrated on computing slices with respect to the final values of variables, our approach \nis capable of computing slices with respect to any expression at any programpoint. Conceptually, aslicewithrespectto \napCexpression e (assumed to be side-effect free) at some specific program point can recomputed as follows: \nFhst, introduce anew variable vand an assignment of the form v = op(v, e); at the program point of interest, \nwhere opis an abstract, uninter\u00adpreted operator. Then, compute theslice with respect tothe final value \nofv. Variable vhasthe effect of accumulating the sequence of values the expression takes on at the desired \nprogram point. In practice, it is not necessary to alter the program in order to compute slices at intermediate \npoints. Animplementation can in\u00adstead construct and maintain preference tothe Pmistore subgraph SPcorresponding \nto every program point p (note that the graphs representing these stores will generally have many nodes \nincom\u00admen). Theslice withrespect totheprogram point ofinterestis then computed by normalizing the PIM \nexpression corresponding to the translation of e in initial store 5P. 4.7 Conditional Constraints A \nslice with respecttoa conditional constraint such asthat depicted in Fig. 4(c) can be computed by constructing \na PIM graph roughly equivalent to that which would be produced by inserting the body of the program in \na conditional statement where the predicate is the conjunction of all such constraints. The effectiveness \nof our slicing algorithm in handling condi\u00adtional constraints depends primarily on its ability to reason \nabout the operations allowed in such constraints. The extensible nature of our approach makes it easy \nto augment the slicing algorithm by incorporating sophisticated reasoning capabilities about partic\u00adulardomains \ninto the slicing algorithm, asitonly involves adding rewrite rules characterizing the appropriate domains. \nFor exam\u00adple, in the case of Fig. 4(c), rudimentary rules for reasoning about arithmetic inequalities \nsuffice to compute the slice shown.  4.8 Complexity Ikadeoffs Use of different normalizing subsetsof \nPIM equations allows var\u00adiousaccuracy/time tradeoffs inthe analysis process. For instance, pointer-induced \nalias analysis is NP-complete even in the absence of loops and procedures [22], although such analysis \nis usually tractable in practice. By including or excluding appropriate PIM rules, one can effectively \nchoose more precise (but potentially slow) or more conservative (but guaranteed fast) pointer analysis. \nFor instance, eliminating rule (M3) (seeFig. 18) effectively inhibits propagation of symbolic addressesrepresenting \npointer values, thus preventing these expressions involving these addressesfrom being resolved orsimplified. \nRule(Lll) hastheeffect ofjoiningcommon results of common expression propagation (including address ex\u00adpressions)indifferent \nbranchesof aconditional, andcanbeenabled, disabled, or restricted to prevent or allow such propagation. \nThe result of more accurate pointer analysis in slicing is mani\u00adfested by elimination of more subgraphs \nof the program represen\u00adtation that are irrelevant to the slicing criterion. A similar phenom\u00adenon occurs \nwith simplification of boolean predicates involved in conditionals. 5 Variations on a Looping Theme This \nsection discusses a number of PM variants suitable for com\u00adputing slices in loops. These sets of rules \nmaybe used as building blocks for generating a variety of different slicing algorithms with\u00adout changing \nthe underlying algorithmic framework. In the sequel, we will use COREPIM-to denote those PIM rules that \nare not loop related and are common to the slicing variants we will present. While the COREPIM+ rules \nallow addresses to be stored as values and manipulated, the analysis rules presented in thk section assume \nfor simplicity that no pointers are used. The ideas in this section can be adapted easily to produce \nconservative slices in the presence of pointers; more precise pointer analysis is also possible, but \nrequires more sophisticated rules for reasoning about address equivalence. 5.1 Loop Execution Rules: \nPure Dynamic Slicing Loops are represented in PIM by terms of the form Loop(ks.body(uE , vE, US)> s) \n (where fz @ (FV(UE) U FV(U,S) U FV(VE))) (kf) g -f[z :=g] (0) (Y f) + f (Y f) (recursion) Figure 11: \nLoop execution rules LOOp(hS. bOdy(uE, WE, us), S) + project( Loop(ks .body(rsE, vE, US), s), AssignedVars(uE \n.8 us) ) (LA1) Project(s, {}) + 0S (LA2) Project(s, {v}Ur) --+ (v b+s@v)08Project( s,r) (LA3) rules for \ncomputing AssignedVms(s ), the set of variables assigned to in the stores Figure 12: ~rules Informally, \n~E is a store representing the side-effects of evaluat\u00ading the loop predicate, v~ represents the value \nof the predicate, us is a store representing the side-effects of the loop body, all as functions of the \nstore zs at the beginning of a loop itera\u00adtion. The second argument s is the incoming store. The term \nLoop(kzs. body(t@, vE, us), s) itself denotes the store repre\u00adsenting the side-effects of executing the \nloop until the predicate evaluates to false. The rewriting rules in Fig. 11, which we will refer toasloop \nexecution roles, specify this behavior fomally. The underlined subterm of the right-hand side of the \nrule (loop) may be read as: (the side-effects ofexecuting theloop consists of) the side-effects of evaluating \nthe loop predicate and, if the predicate evaluates to true, the side-effects of executing the loop body \nonce, composed with the side-effects of executing the same loop with an appropriately updated store, \nnamely, xs o, ?LE o, US. The rest of the term serves to express the recursion using the recursion combinator \nY. f[z :=g] represents theresult ofsubstitutingg for free occurrences of z in ~ (with the usual provisos \nabout variable capture and renaming), FV(T) is the set of free variables in a term ~, and S isatechnicality \na sort coercion operator that has no semantic content. [11] shows how the/3 rule and substitution can \nbe encoded as pure rewriting rules. Utilizing these rules and COREPIM+ during the simplification phase \nleads to a straightforward dynamic slicing algorithm that we call a pure dynamic slicing algorithm. Sec. \n2.4 discusses an example (Fig. 3(b)) ofthis sort of slice. 5.2 q$Rules: Lazy Dynamic Slicing Fig. 12depicts \naset of rules, the~ rules, that statically simplify P~stores generated by loops. Theeffect ofq5rulesonthe \nP~ representation isessentially to introduce an SSA-form #node [7] foreve~vatiable thattight reassigned \navalueinside theloop. In terms of slicing, these rules have the effect of permitting loops to be removed \nfrom slices if it can be determined (statically) that the loop cannot assign to any variable of interest \n. We will refer to the slicing algorithm obtained by using both the loop execution rules and the q5rules \nin conjunction with COREPIM- X = ?X; x =?X; y=x; if (x<O) if (x<O) x = .x; Z=x; 2=X; (a) (b) Figure \n13: (a) Example program. (b) Dynamic slice at z given ?X := 5. as a lazy dynamic slicing algorithm. This \nslicing algorithm com\u00adputes traditional dynamic slices, such as Fig. 3(c), as well as the somewhat more \nunusual result in Fig. 3(d). The slices produced by our lazy dynamic slicing algorithm are closer to \nthe slices produced by the Agrawal-Horgan algorithm [1] than to the slices produced by the Korel-Laski \nalgorithm [21]. The Korel-Laski slices tend to be larger than the Agrawal-Horgan slices since they, unlike \nthe Agrawal-Horgan slices, are executable. Our dynamic slices, though not executable under the standard \nsemantics , are executable with respect to the semantics specified by the rewriting rules. Fig. 13(a) \nillustrates an important difference between our al\u00adgorithm and previous dynamic slicing algorithms. Since \nthe if predicate evaluates to false, previous dynamic slicing algorithms exclude the predicate (and any \nof the statements the predicate eval\u00aduation is dependent upon) from the dynamic slice with respect to \nz. However, as has been observed before [21, 25, 2], the predicate does affect the final value of z, \nand in applications such as de\u00adbugging it is useful to include these statements in the slice. In this \nsense, our slicing algorithm produces a slice that is semantically more consistent than existing dynamic \nslicing algorithms. 5.3 Loop Splitting Rules: Static Loop Slicing Fig. 14 contains the essential subset \nof a collection of rules that we will refer to as loop splitting rules, which can be used in conjunction \nwith COREPJM+ to compute a classical static slice. The goal of these rules is quite simple. Consider \nthe PIM term Slice(while(i < 10){j = j + 2;i = i + 1; }, i, ()) This reduces to a term of the form Loop(kS.body(uE, \nvE, w),s) @ addr(i) where us, representing the loop body, is the store {addr(j) w T D. +((zs @ addr(j))! \n, 2)} O. {addr(i) -T Dti +((zs Q addr(i))! , 1)} Intuition suggeststhat this term should be reducible \nto Loop(kS.body(uE, VE, T&#38;s ),s) @ addr(i) where us is the store {addr(i) = T D. +((zs @ addr(i))! \n, 1)} Such reductions are crucial to computing static (and constrained) slices. In general, we would \nlike to reduce a term of the form Loop(k.body(sl,p, 52), 53) @ a LOOp(kS. bOdy(uE, vE, us), s)@v ---i \n(Loop(kS.body(uE, vE, us), s)@@{v})@v (SA1) (a Hrn)@l@l + (a Cz)D. (a I-+rn) (SA2) 03t2Ql + 0s (SA3) \n(.9, 0. 52) C@ 1 + (s, C@ 1)0, (s2 Q@ 1) (SA4) (g D, s) @@l1 ~ gD. (s @@i) (SA5) T ~ ( 1 U DemMd(t)E, \nzs) U Demand(uE @@ T, zs) U Demand(us Q@ r, as) ) = T LOOp(kZS. bOdy(UE, vE, Us), S) @@ 1 + LOOp(ks .body(uE \n@@ T, vE , US @@ ~), S @@ ~) (SA6) rules for computing Dernand(s, zs ), the set of addresses dereferencing \nfree instances of zs in s Figure 14 Loop splitting rules to a term Loop(kcs. body(slf, p, SZ ), s3 ) \nQ a where each Si is a restriction of the original store s, to the addresses that are relevant, given \nthat we are interested only in the final value at address a. We need to do two things here. First, we \nneed to identify the set r of relevant addresses (variables), second, we need to perform the actual restriction \nof the stores to the relevant variables. The rules in Fig. 14 show the essence of what we need to do. \nRule (SA1) simply transforms a dereference operation on a store computed by a loop into a corresponding \ndereference operation on a restn ction of the loop-computed store. This leaves the bulk of the work to \nthe operator @@,whose purpose is to restrict a store to a set of addressesof interest. Rule (SA2) is \nthe key mle defining the behavior of this operator. (The operator ~ may be interpreted asdenoting the \nusual set-theoretic member function.) The rule of primary interest is (SA6), which performs the re\u00adstriction \noperation for a store generated by a loop. Restricting a loop-computed store with respect to a set 1 \nof addressesrequires restricting the loop body store and the initial incoming store with respecttoasetofaddressesr. \nThesetrisasupersetofthesett,and effectively accounts for loop-carried dependence. The antecedent of rule \n(SA6) specifies the condition this setr hasto satisfy, namely that the setof variables r should include \nthe set1,the setof variables required to compute the loop predicate, and the set of variables nec\u00adessary \nto compute values assigned to the variables in r within the loop. Put another way, the antecedent of \n(SA6) ensures that the set of variables r is transitively closed with respect to loop-carried dependence. \nThe auxiliary function Demand(t, ZS), roughly, identifies upwards exposed variables in t.More formally, \ngiven a store or merge t, Demand (t, %s) identifies dereferences of the free store variable zs in t and \ncollects the address operands of such dereferences. Rule (SA6) is not a pure rewriting rule, since the \nvariable ~ in the antecedent of the rule is not bound in the left-hand side of the rewrite rule. Applying \nrule (SA6) thus requires computing some solution T to the constraint expressed by the antecedent. The \ncomputation of the least solution of this constraint can be performed easily using rewriting rules that \ncompute an iterative computation of the constraint s least fixed point. Rules expressed in a constraint \nstyle such as (SA6) have the advantage that they can accommodate analysis algorithms imple\u00admented by \nnon-rewriting means (and thus for which dependence tracking cannot be performed). Observe that (SA6) \nis valid for every possible instantiation of the variable r thus, one may view (SA6) as a rule schema \ndescribing infinitely many rewriting rules. One may then use any mechanism whatsoever to choose an instanti\u00adation \nfor the rule, treating the instantiation as an ordinary rewriting rule with respect to dependence tracking. \nThis approach ensures that the dependence information is computed correctly, notwith\u00adstanding the use \nof an external analysis algorithm. 5.4 Loop Invariance Rules: Invariance-Sensitive Slicing We now turn \nour attention to the final set of rules, which we will refer to as loop invariance rules. We will refer \nto the slicing al\u00adgorithm obtained by using these rules, the COWPM+ rules, and the loop splitting rules, \nas an invariance sensitive slicing algo\u00adrithm. If the loop execution rules are used as well, we obtain \na ,6 invariance-sensitive algorithm. The primary difference between the two algorithms is that the latter \nwill execute (i.e., unfold) a loop as long as its predicate evaluates to a constant. Fig. 15 illustrates \nthis behavior. The /3 invariance-sensitive slicing algorithm, by ex\u00adecuting the loop, discovers that \ntwo of the three assignments to y in the loop are irrelevant for the given input constraint ?N := 5. \nThe simpler algorithm avoids unfolding the loop; however, by ef\u00adfectively performing constant propagation, \nit discovers that one of the three assignment statements is irrelevant. We describe the goals of the \nloop invariance rules below. Con\u00adsider a store of the form Loop(Xzs.body(uE, ~E, US),s) @ a The store \nus represents the loop body, and free occurrences of xs in us denote the store at the beginning of a \nspecific loop iteration. In the presence of loop invariants, one can simplify the store us further. For \ninstance, consider the example in Fig. 15(c). The store us compiled for the loop body will contain subterms \nof the form (as @ addr(z))!, denoting the value of variable x in a particular iteration. Since the value \nof x is a loop-invariant constant 6 (given the constraint ?N := 5), we would like to replace this term \nby 6. This replacement, in turn, will allow further simplifications of the store, and ultimately lead \nto the slice depicted in the figure. Achieving this kind of simplification requires us to do two things: \nWe must identify the loop-invariant component of the store, and we must specialize the loop body (and \nthe loop predicate) with respect to the loop invariant component of the store. The second task is relatively \ntrivial. Once the loop invariant component sin. of the store has been identified, we can replace the \nfree occurrences of xs in the loop body by xs OS s,~~. The rest of Pmt will then take care of the specialization. \nChe rules in Fig. 16 formalize these intuitions. The most impor\u00adtant rule is (IA1), a conditional rule \nin the style of rule (SA6) (Fig. 14). The consequent of the rule specializes the loop body and loop predicate \nof a loop-computed store with respect to the loop-invariant part of the store, namely s;~v. The antecedent \nguarding the applic\u00adability of the rule defines what it means for a part of the store to be loop invariant. \nThis definition is stated in terms of a subsumption relationship ~ between program stores. A store S1 \nsubsumes a store SZ, if for every variable z assigned a value v in store s2, m is also assigned the same \nvalue v in store S1. The subsumption relation is concisely defined by the equational axiom (IA2). Rules \n(IA2. 1) through (IA2.3) represent a conservative approximation to the > that is more directly computable, \nsince it is defined inductively. Less conservative approximations to (IA2) can also be defined that allow \ninference of more complex loop invariants. Returning to the notion of a loop-invariant store, the store \ns,~v is considered to be loop invariant if (a) The incoming stores (the store 6.1 Properties of Graph \nReduction n=?ti; z=?Z: n=? N;z=?Z; n=? N;z=?Z; X=rl+l; x=n+l; x=n+l; i=l; y=O; i=l; y=O; i=l; y=O; while \n(i <n) { while (i <n) { while (i <n) { if (x >100) if (x >100) if (x >100) y=y+ loo; else if (y <99) \nelse if (y <99) else if (y <99) y.y +x; y.y +x; y.y +x; else else else y=y +50;y=y +50; Z=z+y; Z=z+y; \nZ=z+y; i=i+l; i=i+l; i=i+l; }}} (a) (b) (c) Figure 15: (a) Example program. (b) @invariance-sensitive \ndice at z given ?N := 5. (c) Simple invariance-sensitive SliCeat z giVen ?N := 5. s = (ss 08 stn?J ), \ns,nv os ((UE OSUS)[W := s ])&#38; simu = T Loop(kS. body(@, vE, Us), S) + LOOp(ks .body(uE [Zs := s ], \ntJE[~s:= s ], ?4s[%s:= s ]), s) (IA1) (IA2.2) =((s @a)!,m!) = T Sk ai-+m -+ T (IA2.3) Figure 16: Loop \ninvariance rules before the loop begins its first iteration) subsumes s,~v, and (b) The loop body UE \nOS us, specialized for an incoming store zs o. s,~~ that subsumes s,~u, and then composed with s,~u results \nin a store that subsumes s,nv. As with rule (SA6) discussed in the section on static slicing, rule (IA1) \ncannot be used directly by the dependence tracking system. However, we can use the rule in conjunction \nwith any algorithm for identifying loop invariants, such as the conditional constant propagation algorithm \nof Wegman et al [31]. Pragmatic A prototype implementation of our methods has been completed using the \nASF+SDF Meta-environment [19]. The results obtained from this prototype have been encouraging, and we \nare now engaged in implementing a free-standing reduction-based slicing system using the most efficient \npossible implementation techniques. In this section, we briefly touch on several pragmatic that arise \nin implementing our approach. Term graph reduction is a simple technique that can be implemented efficiently \nwhen an automaton-based matching algorithm and out\u00adermost reduction strategies are used. This leads us \nto believe that it should scale well to relatively large programs. Graph reduction also has the advantage \nthat results of reductions performed in shared subgraphs are immediately available to all supergraphs \nfrom which they are accessible. This means, e.g., that a slice can be computed in a subprogram (such \nas a procedure), and the results later used in computing the slice with respect to the entire program. \nIt also means that reduction steps that are independent of a given criterion, but dependent on the program, \ncan be shared and reused when new criteria are supplied. 6.2 Alternative Translation Algorithms As alluded \nto in Section 2.2, it is not strictly necessary to use a rewriting system to translate a source program \nto Pm. Any rslgo\u00adrithm to perform the translation suffices, provided that the dynamic dependence relations \nbetween the source AST and its Pnvt trans\u00adlation are correctly initialized. However, the correctness \nof these initial relations must be established by hand.  6.3 Chain Rules Chain rules in a language s \nabstract syntax can be used to distin\u00adguish classes of syntactically related program constructs that \nhave differing semantic properties, For instance, in our C grammar, we distinguish between pure expressions \nand those that may have side-effects. Dependence traced by CR-tracking to such nodes can be used to single \nout a particular property of a construct that causes it to be included or excluded from a slice. 7 Related \nWork PM was introduced as a semantically sound internal representa\u00adtion for program analysis in [12]. \nThe theoretical underpinnings of the notion of dynamic dependence were developed in [13] for arbitrary \nterm rewriting systems. In this paper we have augmented Pnvi s logic (particularly for loop analysis), \nand applied the notion of dynamic dependence to it to develop a family of extensible slic\u00ading algorithms \nfor standard programming languages, exploiting in particular the possibility of computing slices with \nrespect to con\u00adstraints. Some previous algorithms [6, 8, 18] combine both static and dynamic information \nto compute slices, but primarily to combine the efficiency of static slicing algorithms with the accuracy \nof dy\u00adnamic slicing algorithms. The notion of constrained slices is not studied in these papers. Constrained \nslicing and partial evaluation of programs are closely related, in a manner similar to the relationship \nbetween dynamic slicing and standard program evaluation. However, con\u00adstrained slices cannot be obtained \nsimply by partially evaluating a program, then computing a static slice from the residual program that \nresults-one must also relate slices in the partially evaluated program to the source program, this is \nnot necessarily a trivial task. Consider the example in Fig. 17. Given the input constraint ?x := 5, \nthe program in Fig. 17(a) can be simplified using constant propagation and dead code elimination to yield \nthe program shown in Fig. 17(b). However, a static slice of this optimized program at z fails to provide \nthe same information as our constrained slice of the original program with respect to the same criterion \n(Fig. 17(c)). This is due to the fact that the predicate of the if statement (which evaluates to false) \nis relevant to the computation of the final value of x = ?X; x = ?X; y = ?y; y = ?Y; y = ?y; if (x < \nO) if (x < O) y = -y; Z=y; Z=y; Z=y; (a) (b) (c) Figure 17: (a) Example program. (h) Example program \nafter optimiza\u00adtion using constant propagation and dead code elimination, given ?X := 5. The static slice \nof the optimized program at z is the optimized program itself. (c) Our constrained slice at z given ?X \n:= 5. z, and should therefore be included in the slice. Slicing is intended to indicate how the value \nof a variable or expression is computed, not merely what its value may be. For further details on the \nrelation between previous work on partial evaluation and PIM, see [12]. Ernst [9] presents an algorithm \nfor static slicing that is similar to our algorithm in certain respects. In particular, Ernst describes \nhow conventional program optimization techniques can be used to pro\u00adduce smaller and better slices. The \ninternal representation that Ernst uses, the value dependence graph (VDG), has similarities with PIM, \nand the process of optimization itself consists of transforming the VDG. Ernst refers to the problem \nof maintaining a correspondence between the VDG and the source code graph throughout the op\u00adtimization \nprocess, and mentions that this correspondence enables a history mechanism for explaining the transformations \nthat were performed. No details are presented as to how this is done, but this aspect of Ernst s work \nappears to be the analogue of the no\u00adtion of dynamic dependence used in our work to maintain a similar \ncorrespondence. Tip shows that dynamic dependence tracking can be used to compute accurate dynamic slices \nfrom a simple interpreter-style semantics for a language, and that these techniques are useful for debugging \n[27]. [29] contains a comprehensive discussion of many issues related to slicing, dynamic dependence \ntracking, and use of algebraic specifications in generating program analysis tools. While we have yet \nto undertake a formal comparison of the complexity of our approach with that of earlier methods based \non dataflow analysis or dependence graphs, informal analysis indicates that for comparable types of slices, \nour approach should be quite competitive with previous techniques. One characteristic of our approach \nthat must be kept in mind in any complexity analysis is that aspects of both intermediate representation \nconstruction and analysis using the intermediate representation are combined into reduction steps. For \ninstance, in comparing our work with PDG\u00adbased algorithms, it is apparent that there is a close correspondence \nbetween most steps involved in PDG construction and certain PIM rewriting steps. The computation of the \nslice itself in PDG-based approaches requires a graph traversal that corresponds roughly to traversing \nthe set of dynamic dependence relations in a reduced PIM term. One advantage of our approach over PDG-based \nmethods is that by using an outermost lazy graph reduction strategy, the analysis performed is effectively \ndemand-driven. Thuse only those reduc\u00ad tion steps directly relevant to the slicing criterion are performed. \nIn this respect, our approach has the potential to outperform prior techniques that may eagerly compute \ndataflow information that is never used. 8 Future Work There are several areas for future research: \nWe are currently ex\u00adploring the issues involved in extending our techniques to handle arbitrary control \nflow, arrays, address arithmetic, dynamic mem\u00adory allocation, and procedures. We also intend to study \nvarious PfM subsystems and reduction strategies in isolation to determine their worst-case complexity \nversus their ability to make slices more precise. This study can assist in designing a set of stratified \nsub\u00adsystems that let the user decide on an appropriate tradeoff between precision and speed. Finally, \nit would be would be interesting to attempt to extend the notion of dynamic dependence to more powerful \ntheorem-proving techniques, such as those incorporating higher-order or equational unification or resolution. \nReferences [1] AGRAWAL,H., AND HORGAN, J. Dynamicprogramslicing. In Proceedtrtgs of the ACM SIGPLAN 90 \nConference on Programming Language Design and Implementation (1990), pp.246-256. SIGPIAN Notices 25(6). \n[2] BALL, T. Personalcommunication. [3] BALLANCE,R. A., MACCABE,A. B., AND OTt ENSTEIN,K. J. The programde\u00adpendenceWeb: \nA representation suppordrrg control-, data., and demand-driven interpretation of imperative lrarguages. \nIn Proc. ACM S1GPL4N Conference on Prograrrrmurg .bnguage Design and Implementation (White Plains, NY, \nJune 1990), pp. 257 27 1. [4] BARENDREGT,H, VAN EEKELEN, M., GLAUERT, J., KENNAWAY, J., PLASMEUER, M., \nAND SLEEP, M. Term graph rewnturg. In Proc. PARLE Conference, Vol. II: Parallel Languages (E1rrdhoven, \nThe Netherlands, 1987), Sprirrger-Verlag, pp. 141-158. Lecture Notes in Computer Science 259. [5] CARTWRIGHT,R., \nAND FELLEISEN, M. The semantics of program dependence. hr Proc. ACM SIGPL4N Conference on Programming \nLzrnguage Design and Implementation (Portlrmd, OR, June 1989), pp. 13-27. [6] CHOI,J.-D., MILLER. B., \nANDNETZER.R. Techniques for debugging parallel pro\u00adgrams wrth flowback analysis, ACM Transactions on \nProgramming Languages and Systems 13,4 (1991), 491-530. [7] CYTRON,R., FERRANTE,J.. ROSEN,B. K., WEGMAN,M. \nN., ANDZADECK.F. K Efficiently computing static single assignment form nnd the control dependence graph. \nACM Trans. Program. Lzrng. SW 13,4 (October 1991),451-490. [s] DUESTERWALD, E., GUPTA, R., AND SOFFA, \nM, Rigorous data flow testing through output influences. In Proceedings of the Second Irvine Soj?wrre \nSymposium 1SS 92 (Cahfomia, 1992), pp. 131-145. [9] ERNST,M. Practical fine-gramed static slicing of \noptimized code. Tech. Rep. MSR-TR-94-14, Microsoft Research,Redmond, WA, 1994. [10] FERRANTE,J., OTTENSTEIN,K. \nJ., AND WARREN, J. D. The program dependence graph and lts use in optimization. ACM Trans. Program. ~ng. \nSW. 9, 3 (JuIY 19$37),319-349. [11] FIELD, J On laziness and optimality in lambda interpreters: Tools \nfor speci\u00adfication and analysis. In Proc. Seventeenth ACM Symposwm on Principles of Progranrrwng Languages \n(San Francisco, January 1990), pp. 1-15. [12] FIELD, J. A simple rewntrng semantics for realistic imperative \nprograms and its application to program analysis. In Proc, ACM S1GPL4N Workshop on Partial Evaluation \nand Semantics-Based Program Manipulatlan (San Fran\u00adcisco. June 1992), pp 9S-107 Published as Yale University \nTechnical Report YALEuiDcslRR-909. [13] FIELD, J., AND TIP, F. Dynamic dependence in term rewnthrg systems \nand its application to program slicing. Tech. Rep. RC 19?7?,IBM T.J. Watson Research Center, November \n1994. (Corrected and expanded version of [14]). [14] FIELD, J., AND TIP, F. Dynamic dependence in term \nrewriting systems aad its application to program slicing. In Proceedings of the Sixth International $vmposium \non Programming Language Implementation and Logic Programming (September 1994), M. Herrnenegddo and J. \nPerrjam, Eds., vnl. g44, Springer-Verlag, pp. 415431. [15] GALLAGHER, K., AND LYLE, J. Using program \nslicing in software maintenance. IEEE Transactions on Sofiware Engineering 17,8 (1991), 751-761. [16] \nHORWITZ,S., PRINS,J., AND REPS,T. Integrating noninterferirrg versions uf programs ACM Transactions an \nProgramming languages and Systems 11, 3 (1989), 345-387. [17] KAHN, G. Natural semaaucs. In Fourth Annual \nSymp. on Theorencal Aspects of Computer Science (1987), vol. 247 of LNCS, Springer-Verlag, pp. 22-39. \n [18] KAMKAR, M., FRITZSON, P., AND SHAHmHRI, N. Three approaches to mterpro\u00adcedural dynamic slicing. \nMicroproces.ring and Micropr-ogrammzng 38 (1993), 625-536. [19] KLINT, P. A metn-envirorrment for gerreratrrrgprogramming \nenvironments. ACM Trans. on Software Engineering and Methodology 2,2 (1993), 17&#38;201, [20] KLOP, J. \nTerm rewriting systems. In Handbook of hgtc in Computer Sctence, Volume 2. Background: Computational \nS@ctures, S. Abramsky, D, Gabbay, and T Maibaum, Eds Oxford Universny Pre;s, 1992, pp. 1-116. [21] KOREL, \nB., AND LASKI, J. Dynamic slicing of computer programs. Journal of Systems and Sojhvare 23 (1990), 187-195. \n[22] LANDI, W,, AND RYDER, B. A safe approximate algorithm for interprocedural pointer aliasmg. Jn Proceedings \nof the 1992 ACM Conference on Programming .hmguuge Design and Implementation (San Francisco, 1992), pp. \n235-248. SIG-PLAN Notices 27(7). [23] LARUS,J., AND CHANDRA, S. Using tracing and dyrranric sJicing to \ntune compil\u00aders.ComputerScienceTechnicalReport1174,University ofWkconsin-Madi son, 1993. [24] NING, J., \nENGBERTS.A., AND KOZACZYNSKI, W. Automated support for legacy code understanding. Cornrnrmicatiom of \nthe ACM 37, 5 (1994), 50-57, [25] PAN, H. So@are Debugging with Dynamic Instrumentation and Test-Based \nKnowledge. PhD thesis, Purdue University, 1993, [26] PODGURSKI, A, AND CLARKE, L, A formal model of progrrrrn \ndependence and lts imphcations for software testing, debugging, and maintenance. IEEE Transactions on \nSoftware Engzrreering 16.9 (1990), 965-979, [27] TIP, F Generic techniques for source-level debugging \nand dynamic progrmrr slicing, Report CS-R9453, Centrum voor W1skunde en Jnfor matica (CWI), 1994, [28] \nTIP, F, A survey of program slicing techniques. Report CS.R9438, Centnrm voor Wiskunde en Informatica \n(CWI), Amsterdam, 1994. [29] TIP, F, Generation of Program Analysts Tools, PhD thesis, University of \nAms\u00adterdam, 1995. Forthcoming, [30] VENKATESH,G. The semantic approach to program slicing. Jn Proc. ACM \nSIG-PLAN Conq! on Programming Language De,mgn and Implementation (Toronto, June 1991), pp. 80-91. [31] \nWEGMAN,M. N., ANDZADECK,F K. Constant propagation with conditional branches. ACM Trans. Program Lang. \nSyst. 13,2 (April 1991), 181-210. [32] WEISER, M, Reconstructing sequential behavior from parallel behavior \nprojec. lions. Information Processing Letters 17,3 (1983), 129-135. [33] WELSER,M. program shcing, IEEE \nTransactions on Software Engineering 10.4 (1984), 352-357, A PIM Details In this section, we briefly \nreview the PIM term structure and the most important subsets of PIMrules. Additional rules usedprimarily \nfor performing induction aredescribedin [12], these rules we the foundation for tJreloop analysis rules \npresented in Section 5. In general, PIM M augmented with rules defining the semantics of language-specific \ndatatypes such asintegers, PIM terms ~e constructed over an order-sorted signature. PIM sectsdistinguish \namong fundamentally incompatible syntactic sh uctures corresponding to observable values, merge structures, \nstore structures, and lambda expressions; however, sorts should not be interpreted as types in rhe usual \nsense.   A.1 PJM4 Rules The rules of PIM++are given in Fig. 18. Variables v, m, s, and f WI1lbe used \nin the rules to refer to observable values, merge suuctures, store smrctures, and kmrbda\u00adexpressions,respectively, \nEquations (L 1~L8) aregeneric to mergeor storesmrctures. Thus each of the operators labeled p is to be \ninterpreted asone of .sor m. (El ) and (E2) rweschemes for an infinite set of equatrons. Equation (Cl) \nonly applies if the argument of S(. ) is of sort S, The rules of PIM-consist of those of PIMt-, along \nwith the rules depicted in Fig. 11. A.2 PfM~ Equations The rules of PIM: a e those of PIM; along with \nthose given in f+g. 19. As before, p m rules (L9)-(L11) is assumed to be one of m or s In rule (M9), \nCV [ ] denotes an arbitrary context of value sore,this rule could also be lessperspicuously rendered \nasa frmrily of rules, one for each vaJue-sortedfunction symbol. Op 0, 1 - 1 (Ll) 1 Op O@ - 1 (L2) 11 \nOp (12 Op 13) - (Jl OP 12) O@13 (L3) P DP OP - 0, (L4) TBP1 1 (L5) FDP1 - 0, (L6) PDP (/1 O@12) PI Dp \n(P2 DP t) -- (PDP JL) OP (PDp A(PI , p~) DP ~ 12) (L7) (L8) {V* H m} @w, =(WI , 02) Dm m (s1) {v Hem} \n0s (s2) O.clv Om (s3) (3, 0s s,) @ u (s, @v) om (0, Q v) (s4) PD, {ti~rrt} - {w w (pDti m)} (s5) =(k, \n, k2) --+ T, J+ constants, ,kI ~ k.z (El) =(k, , k,) - F, k, constants, kl ~ kz (E2) [F D o] Om (Ml) \n(m O\u00ad [T D v]) - [T D rJ] (M2) [T DOv]~ -- v ? (M3) (M4) PI D\u00ad [P2 D v] - [A(P1 , P,) D VI (M5) =(T) \n- F (Bl) 7(F) - T (B2) - (1(P)) A(T, P) -- P P (B3) (B4) A(P, T) ---+ P (B5) A(F, p) - F (B6) A(P, F) \n- F (B7) A(A(PI , pz) , ps) T(A(P, , PZ)) ---+ ---+ A(PI V(7(P1 , A(P2 , P3)) ) , -l(PZ)) (B8) (B9) V(T, \np) - T (B1O) V(P, T) - T (Bll) V(F, p) - P (B12) V(P, F) - P (B13) V(V(PI , pz) , p3) - V(pl , v(p2 , \np3)) (B14) -l(v(p, , p2)) - A(=(P1 ) , =( Pz)) (B15) s(s) .s (cl) Figure 18: Equations of PIM; =(o, \ntJ)=T (E3) JzOP11OP12 ==11OP12 (L9) A /m. %)-r? ,,\\l . ,P /-\u00ad (PI Dp II) OP (P2 DP 22 =(P2 DP 12 OP (fJI \nDP 11) (L1O) (P1 DP ~) o. (P2 DP 1) = (V(P, >P,)) D, 1 (Lll) {. Hm,} 0. {v H m} = {. ++ (m, om rnz)} \n(S6) =(VI , v,) =F {., ++ m,} 0, {.2 H 777,}= {., ++ m,} 0. {., ++ Tm} (s7) (1(P) Dm m, O\u00adrn20m[p Dv]=rrz10mrn20m \n[PDV] (M6) [P Dm!]=[PD?]om(PDmm) (M7) ([TD ?] Om ~)! = ~! (M8) Cv[m! ] = (m\\(Azv. cy[zv ])) !, *V 6 FV(CV[ \n]) (M9) (ml O-m,)\\f = (m, \\f) Om (T?22\\$) (M1O) [P DOV]\\ ; : !PD (f w)] (Mll) (M12) (P DmX)\\f=PDm(TTZ\\$) \n(M13) (m\\ Ax.v)\\f = m\\ Ax.f. (M14) A(pl , pz) = A(P2 , fJI) (B16) A(P, P)=P (B17) A(P> (P)) =F (B 18) \nV(pl , p2) = V(pz , pi) (B19) V(p, p)=p (B20) V(P, -I(P)) =T (B21) A(P1 , V(pZ , p~)) = V(A(PI , p~) \n, A(P1 , p~)) (B22) V(PI , A(P2 , p~)) = A(V(PI , p~) , V(pI , p~)) (B23) [PD (A(P> q))] = [PD d (B24) \nFigure 19: Additional Equations of pJM~ 392 \n\t\t\t", "proc_id": "199448", "abstract": "<p>Program slicing is a technique for isolating computational threads in programs. In this paper, we show how to mechanically extract a family of practical algorithms for computing slices directly from semantic specifications. These algorithms are based on combining the notion of <italic>dynamic dependence tracking</italic> in term rewriting systems with a program representation whose behavior is defined via an equational logic. Our approach is distinguished by the fact that changes to the behavior of the slicing algorithm can be accomplished through simple changes in rewriting rules that define the semantics of the program representation. Thus, e.g., different notions of dependence may be specified, properties of language-specific datatypes can be exploited, and various time, space, and precision tradeoffs may be made. This flexibility enables us to generalize the traditional notions of static and dynamic slices to that of a <italic>constrained</italic> slice, where any subset of the inputs of a program may be supplied.</p>", "authors": [{"name": "John Field", "author_profile_id": "81100419562", "affiliation": "IBM T.J. Watson Research Center, P.O. Box 704, Yorktown Heights, NY", "person_id": "PP43119729", "email_address": "", "orcid_id": ""}, {"name": "G. Ramalingam", "author_profile_id": "81100519054", "affiliation": "IBM T.J. Watson Research Center, P.O. Box 704, Yorktown Heights, NY", "person_id": "PP31045870", "email_address": "", "orcid_id": ""}, {"name": "Frank Tip", "author_profile_id": "81100333471", "affiliation": "CWI, P.O. Box 94079, 1090 GB Amsterdam, The Netherlands", "person_id": "PP15029416", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/199448.199534", "year": "1995", "article_id": "199534", "conference": "POPL", "title": "Parametric program slicing", "url": "http://dl.acm.org/citation.cfm?id=199534"}