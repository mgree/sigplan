{"article_publication_date": "01-25-1995", "fulltext": "\n Time and space profiling for non-strict, higher-order functional languages Patrick M. Sansom Simon L. \nPeyton Jones Dept. of Computing Science, Dept. of Computing Science, University of Glasgow, University \nof Glasgow, Glasgow, Scotland Glasgow, Scotland sansom@dcs. glasgow. ac .uk simonpj @dcs. glasgow. ac \n.uk Abstract We present the fist proiiler for a compiled, non-strict, higher-order, purely functional \nlanguage capable of measur\u00ad ing time as well as space usage. Our profiler is implemented in a production-quality \noptimizing compiler for Haskell, has low overheads, and can successfully profile large applica\u00ad tions. \nA unique feature of our approach is that we give a formal specification of the attribution of execution \ncosts to cost cen\u00ad tres. Thk specification enables us to discuss our design de\u00ad cisions in a precise \nframework. Since it is not obvious how to map this specification onto a particular implementation, we \nalso present an implementation-oriented operational se\u00ad mantics, and prove it equivalent to the specification. \n1 Mot ivat ion and overview Everyone knows the importance of profiling tools: the best way to improve \na program s performance is to concentrate on the parts of the program which are eating the lion s share \nof the total space and time resources. One would expect profiling tools to be particularly useful for \nvery high-level languages such as non-strict, higher-order languages where the mapping from source \ncode to target machine is much less obvious to the programmer than it is for (say) C. Despite this obvious \nneed, profiling tools for such languages are very rare, Why? Because profilers can only readily mea\u00ad \nsure or count low-level execution events, whose relationship to the original high-level program is far \nfrom obvious. With this in mind, we have developed a profiler for Haskell, a higher-order non-strict, \npurely functional language (Hudak et al. [1992]), We make three main contributions, all of which relate \nto the question of mapping low-level execution events onto the programmers-eye view: Permission to copy \nwithout fee all or part of this materfal is granted provided that the copies are not made or distributed \nfor direct commercial acivantaqe, the ACM copyright notice and the title of the publication and Its date \nappear, and notice is given that copyi is by permission of the Association of Computing Machinery. $ \no oopy otherwise, or to republish, requires a fee andor specific permission. POPL 951/95 San Francisco \nCA USA @ 1995 ACM 0-89791 -892-1/95/0001 ....$3.50 1. We describe the first profiler for a compiled, \nnon-strict kmguage capable of measuring execution time as well as space usage. Non-strict languages are \nusually imple\u00admented using some form of lazy evaluation, whereby values are computed only when required. \nFor exam\u00adple, if one function produces a list which is consumed by another, execution will alternate \nbetween producer and consumer in a co-routine-like fashion. Execution of different parts of the program \nis therefore finely inter\u00adleaved, which makes it difficult accurately to measure how much time is spent \nin each part of the program, Our profiler solves this problem (Section 2.2); indeed, its results are \nentirely independent of evaluation order. 2. We support the profiling of large programs by cdlow\u00ading \nthe systematic subswnptiorz of execution costs zm\u00adder programmer-controlled headings, or cost centres \n. Most profilers implicitly attribute costs to the func\u00adtion or procedure which incurred them. This is \nun\u00adhelpful for languages like Haskell, that encourage a modular programming style based on a large number \nof small, often heavily re-used, functions, for two rea\u00adsons: firstly, there are simply too many functions \nin a large program; and secondly it does not help much to be told (say) that the program spends 20% of \nits time in the heavily-used library function map. Separating the notion of a cost centre from that of \na procedure allows us to avoid these difficulties (Section 2.1). 3. We provide a formal specification \nof the attribution of ezecution costs to cost centres, Higher-order languages make it harder to give \nthe programmer a clear model of where costs are attributed. For example, suppose a function produces \na data structure with functions embedded in it. Should the execution costs of one of these embedded functions \nbe attributed to the place where it is called, or to the function which produced the data structure? \n A unique contribution of this paper is that we back up our informal description of cost attribution \n(Sec\u00adtion 2.2) with a formal specification, which we call a cost semantics (Section 2.3). In the framework \nthus created we are able to explore the design space in a precise way (Section 4). While our approach \ncan handle non-strict languages such as Haskell, it is not restricted to them: it can also accommodate \nstrict languages such as SML (though many of the issues we address here would be simplified). From a \npractical point of implement. In Section 3 tion of the profiler in the ton Jones et al. [1993]), a piler \nfor Haskell. As well implementation, we present view, our technique is easy to we describe a full implementa- \nGlasgow Haskell Compiler (Pey\u00adstate-of-the-art optimizing com\u00adas an informal description of the a state-transition \nsystem which describes it formally, and prove it equivalent to the specifi\u00adcation The run-time overheads \nof the profiler are low (less than a factor of 2), even compared with fully-optimised compiled code, \nso that the profiler can be used to instrument even very large programs (Section 5). 2 Specifying the \nprofiler For a profiler to be useful it must be possible to explain to a programmer the exact way in \nwhich execution costs are attributed to the headings under which the profiler reports them we must give \na specification of the profiler. For a higher-order, non-strict language such as Haskell, we have found \nthat this specification is remadmbly slippery. Every time we came up with an informal specification we \nfound new examples for which the specification was inadequate! This experience eventually led us to develop \na formal speci\u00adfication of the way in which our profiler attributes costs. In this section we give an \ninformal model of cost attribution, show how it is inadequate, and then describe our formal model. An \nimportant constraint is that the profiler should do what the programmer expects in commonly occurring \ncases. The formal specification is useful for obscure or difficult cases, but our goal is that most of \nthe time programmers should not need to refer to it. In this sense, the formal system plays exactly the \nsame role as the formal semantics of a programming language: it is a guide to implementors, and the final \narbiter of obscure cases. 2.1 Cost centres In the discussion above we have repeatedly referred to the \npart of a program to which execution costs should be at\u00adtributed. Most proiilers implicitly identify \nsuch parts with functions or procedures. As we have already argued, such an identification is problematic \nfor functional languages. In\u00adstead, we introduce a separate notion of a cost centre, to which execution \ncosts are attributed. For example, consider the following function definitions: my. fun xs = scc mapper \n(map square xs) square x =X*X The scc ( set-cost-centre 1) construct explicitly annotates an expression \nwith a cost centre to which the costs of eval\u00aduating that expression should be attributed. In this case, \nthe costs of evaluating (map square xs ) will be attributed to the (arbitrarily-named) cost centre mapper \n. 1The a ony of this imperative-sounding name is not lost on us. scc is a language construct, like let \nor case, and not a function. The cost centre is a literal string, not a computed value, the scope of \nthe scc extends as far to the right as possible. The scc annotations can be added either manually by \nthe programmer, or automatically by the compiler (see Section 3.2). 2.2 Cost attribution The crux of \nthe matter is, of course, exactly which costs are attributed to which cost centre. In general, given \nan expression scc cc ezp, the costs attributed to cc are the entire costs of evaluating the expression \nexp as far as the enclosing cent ext demands it, excluding (a) the cost of evaluating the free variables \nof exp, and (b) the cost of evaluating any see-expressions within exp (or within any func;on called \nfrom ezp).  This definition has the following consequences: Costs are aggregated. function square does \nits costs are attributed in this case mapper . attributed to the cost In the example above, the not have \nan scc construct, so to the cost centre of its caller, (Other calls to square will be centre of their \ncallers, not to mapper of course. ) Similarly, the cost of executing this call to the library function \nmap will be attributed to mapper as well. In short, except where explicit sccs specify otherwise, the \ncosts of callees are automatically subsumed into the costs of the caller. Results are independent of \nevaluation order. When my_ fun is called, its argument xs may not be fully evaluated, and its further \nevaluation may ulti\u00admately be forced by map called from within my_fun. Nevertheless, the costs of evaluating \nxs will not be attributed to mapper but rather to the cost centre which encloses the producer of XS. \nSimilarly, the re\u00adsult of my_ fun is a list, which may be evaluated fully, partially, or not at all. \nThe costs of whatever evalu\u00adation is performed will be attributed to mapper , no more and no less. In \neffect, this means that the programmer does not need to understand the program s evaluation order in \norder to reason about which costs are attributed to which cost centre. The degree of evaluation performed \nis unaf\u00ad fected. The result of my_f nn will be evaluated no more and no less than would be the case \nin an un-profiled program. It follows that the costs attributed to mapper will depend on how much of \nmy_ fun s result is evaluated by its caller. To say that the results are independent of evaluation order \n(as we just noted above) is not to say that they are independent of evaluation degree! Costs are attributed \nto precisely one cost cen\u00adtre. Using the same statistical inheritance techniques as gprof (Graham, Kessler \n&#38; McKusick [1983]), it is e ::= Axe I ex lx I letxl=el,..., zn=enine I Cx,... xa I caseeof {C3zjl...x1aj \n->ej}~=l I scccce prog ::= xl=el, . . ..xn=en Figure 1: Language Syntax possible to attribute costs both \nto the current cost centre and any enclosing cost centres. However, this requires a record of the current \ncost centre and its enclosing cost centre to be maintained and attached to every heap closure (see Sansom \n[1994b]). We have not implemented this due to the added complexity and overhead. Instead we rely on the \nsimple, but accurate, mechanism for aggregating un-profiled costs described above. Despite our attempt \nat precision, our definition is still vague, especially when higher-order functions are concerned. For \nexample, what costs are attributed to tricky in the ex\u00ad pression scc tricky (\\x -> square X)? Zero cost, \nbe\u00adcause the J-abstraction is already in head normal form? Or are the costs of every application of the \nJ-abstraction at\u00adtributed to tricky ? Would the answer be different if the expression were scc tricky \n(f y), where (f y) evalu\u00adates, perhaps after many reductions, to a function? What if it were scc tricky \n(f y, 2), where the function is em\u00adbedded in a pair? In order to answer questions like these precisely \nwe developed a formal model for cost attribution, which we discuss next. 2.3 Cost semantics In order \nto speak precisely about cost attribution we need a more formal model for the costs themselves. We base \nour model on Launchbury s operational semantics for lazy graph reduction (Launchbury [1993a]; Sestoft \n[1994]), augmenting it with a notion of cost attribution. 2.3.1 Language The language we treat is given \nin Figure 12. It looks at first like no more than a slightly-sugared lambda calculus, with mutually-recumive \nlets, constructors, case, and SCC, but it has an important distinguishing feature: the argu\u00ad ment of \na function application is always a simple variable. A non-atomic argument is handled by first binding \nit to a variable using a let expression. This has a direct opera\u00adtional interpretation: a non-atomic \nfunction argument must be constructed in the heap (the let-expression) before the function is called \n(the application). 2The language is a close cousin of the STG language described in Peyton Jones [1992], \nand is a subset of the Core language used in the Glas&#38;ow Haskell compiler For notational convenience \nwe use the abbreviation {x; = et} for the set of bindings xl=el, . . . . xn=en. Similarly we write [% \n+ et] for the finite. mapping [91 * el,..., Y~ 6 e~] and e[e, /x,] for the substitution e[el/zl, . . \n. . en/z~]. We also abbreviate x I . . . X. with T. and drop the ~=1 in the case al\u00adternatives. We use \n~ for syntactical identity of expressions. 2.3.2 The judgement form We express judgments about the cost \nof evaluating an ex\u00ad pression thus: cc,I :e ye A : Z, CC. This should be read: In the context of the \nheap r and cost\u00ad centre cc, the expression e evaluates to the value z, producing a new heap A and cost \ncentre cc=; the costs of this evaluation are described by 6. To make sense of this, we need the following \nvocabulary: A value, z, is either a J-abstraction or a (saturated) constructor application. A cost centre, \ncc, is a name to which costs are at\u00adtributed. A heap, r, A, ~, is a tinite mapping from variable names \nto pairs of an expression and a cost centre. The notation r[x %e] means the heap r extended by a mapping \nfrom x to the expression e annotated with cost centre cc. In general, the cost centre attached to a binding \nis the cost centre which enclosed that binding. It is used for two purposes: to ensure correct cost attribution \nwhen a thunk is evaluated, and to give cost-centre attribution when a heap census is taken. A cost-attribution, \n19,is a finite mapping of cost centres to costs. It simply records the costs attributed to each cost \ncentre. Cost attributions are combined using v which determines the total cost attributed to each cost \ncentre. The costs themselves are denominated in arbitrary units, as follows: cost of Denoted Application \nA Case expression c Evaluating a thunk v Updating a thunk u Allocating a heap object H One should not \nthink of these costs as constants. The cost semantics specifies which cost centre is attributed with \nthe cost of (say) doing a heap allocation. The se\u00admantics makes no attempt to specify exactly how much \ncost is so attributed; it just says H . The actual im\u00adplementation attributes the actual execution costs \n(of time and space) to the cost centre specified by the se\u00admantics; it does not count A s, C S, and so \non, at all. Cc,r : Aye ~{} r : Age, cc Lambda Cc, r:cza JJ{} r: c Z., cc Constructor cc,l_ : e Uel A \n: ~y.e , cc~ %~ , A : e [xfy] lje, @ : Z,CC* I Application CC, r: e X U{CCMA}W61W82 e : Z,ccz  cc, \nr[z c~ Z]: z u{cc+v} r[$ CAZ]: Z,SUB(CC., CC) Var(whnf) where SUB( SUB I, cc) = cc SUB(CCZ , CC) = CC= \ncc,, r:e J.b A:z, ccZ e~z %r(thunk) CC,r[~ CAe] : z JJ{cctiVIM9U{ccz +U} A[x c% z] : Z, CC= cc, 17[g, \n% e,[y~/z;]] : e[g, /x,] &#38; A : Z, cc= g, fresh Let cc,r : let{~,= .%}in e ~{cc-n.H}wfj A : Z,CC= \ncc,r:e J.LglA:C~F~~,CCC CC,A : ek[X?/yk,] ~gz @ : Z,c ~z Case Cc, r : case e of {C3 ~a~ > j} U{ CCh+c}k1811J@Ze \n: Z, CC, cc.cc,17 : e JJe A : Z,CC. Scc cc,r: scc CCSCC&#38; A : Z,CCZ e I Figure 2: Formal Cost Semantics \nThe cost, 8MA1N, of evaluating the whole program is ob\u00adtained from the judgement MAIN , 17,n,t : nain \n~e~~l~ A : Z,CCZ The initial cost centre is MAIN , to which all costs are at\u00adtributed except where an \nscc construct specifies otherwise. The initial heap, 17;n,~, binds each top-level identifier to its right-hand \nside. What cost-centre should be associated with these bindings? A top-level binding defines either a \n@nc\u00adtion (if it has zu-guments) or a constant applicative form (if it does not). The costs of top-level \nfunctions should be sub\u00adsumed by their caller, so we give their bindings in r,~, ~ the special pseudo-cost-centre \nSUB to indicate this fact. The SUB cost centre is treated specially by the rules which fol\u00adlow. We discuss \nthe treatment of constant applicative forms later, in Section 4.1. 2.3.3 The rules The cost-augmented \nsemantics is given in Figure 2. The following paragraphs discuss the rules. The cost centre on the left \nhand side of the judgement is the icurrent cost centre , to which the costs of evaluating the expression \nshould be attributed. The last rule, SCC, is easy to understand: it si~ply makes the specified cost centre \ninto the current cost cent e. F It is less obvious w y we need a cost centre on the right ! hand side, \nwhich w call the returned cost centre . The 1 Lambda and Constructor rules show where it comes from: \nin both cases the expre@on to be evaluated is in head normal form, so it is returned, along with the \ncurrent cost centre. (The other rules siniply propagate it.) What use is made of the returned cost centre? \nTo see this we must look at the two rules which consume head normal forms, namely Application (where \na function is evaluated before applying it), and Case (where a data value is evaluated before taking \nit apart): In the Case rul&#38; the returned cost centre ccc is simply ignored. The a propriate alternative \nis chosen, and its right-hand side is evaluated in the original cost centre 1 enclosing the case. That \nis as one would expect: the costs of evaluating the alternatives accrue to the cost centre enclosing \nthe case expression. In the Appiicat~on rule, though, there is an interesting choice to be m lade. The \nfunction, e, is evaluated, de\u00adlivering (presu~ably) a A-abstraction Age . The ques\u00adtion now arise : should \nthe body of the abstraction 4 be evaluated in the cost centre returned by evaluat\u00ad ing the abstraction \ncc~, or in the cost centre enclosing the application, cc? This choice is represented by the box in the \nApplication rule, and is discussed further in Section 2.4. Lastly, we deal with the Let and Variable \nrules, which concern the construction and evaluation of heap-allocated thunks. The Let rule extends the \nheap with bindings for newly-allocated closures. The y, are freshly-chosen names, directly modelling \nheap addresses, and are substituted for the corresponding x, throughout. This substitution ensures that \ntwo instantiation of the same let-expression don t in\u00adterfere with each other by binding the same variable \ntwice. The current cost centre is pinned on each binding created by the Let rule. The two Variable rules \nshows how this cost centre is used:  When a variable is to be evaluated, and it is bound to a value, \nthe Var(whn.f) rule says that the value is returned, with an unchanged heap, and the returned cost-centre \nis that pinned on the binding, CCZ. Unless, that is, cc, is SUB , in which case the current cost cen\u00adtre \nis returned, which achieves the effect of subsuming the costs of top-level functions into their callers. \n The Var(thunk) rule deals with the situation when the  variable is not bound to a value. In this case, \nthe ex\u00adpression to which the variable is bound, is evaluated in the context of the cost centre pinned \non the binding, CC.. The newly calculated value is recorded in the re\u00adsulting heap, and the costs of \nentering the thunk and updating it (V and U) are recorded in the cost attri\u00adbution. Notice that the new \nbinding, of the variable to its value, has the returned cost centre cc. pinned on it, and also that the \ncost of the update is attributed to cc=. This is the second way in which the returned cost centre is \nused. The crucial point is that these rules give us a language in which to discuss cost attribution in \na precise manner. For example, should the cost of an update be attributed to cc or CCZin the Var(thunk) \nrule? One can argue the toss, but at least the rules give us a precise way to state the question and \nthe answer.  2.4 Lexical and evaluation scoping In this section we expose and discuss the major design \nde\u00adcisions we encountered while developing our cost semantics. Consider the expression let f = scc fun \n(\\x y -> x*y+l) in f2316 To which cost centre should the cost of computing (23*16+1) be attributed? \nThere are two alternatives:  Lexical scoping. Attribute the cost to fun , the cost cen\u00adtre which lexically \nencloses the \\xy-abstraction. Evaluation scoping. Attribute the cost to the cost centre active when the \nfunction f is applied. In effect, lexical scoping attributes the cost of executing a function body to \nthe cost centre which encloses the decla\u00ad ration site, while evaluation scoping attributes the cost of \nexecuting the function body to the cost centre of the func\u00adtion s application stte. The difference between \nlexical and evaluation scoping has a precise and elegant manifestation in the semantics of Figure 2: \nfor lexical semantics we choose the CCAalternative from the box in the Application rule, and for evaluation \nscoping we choose the cc alternative. The practical implications of this distinction are not imme\u00addiately \nobvious. Consider the expression let f = scc fun exp in (SCC appl f a) + (SCC app2 f b) Lexical scoping \nattributes the total cost associated with the declaration off to the cost centre fun . This includes \nboth the cost of evaluating f itself to a A-abstraction, and the cost of applying f. (The cost centres \n app 1 and app2 are OrdY attributed with the small cost of invoking the application.) In contrast, evaluation \nscoping provides a finer breakdown of costs, attributing the cost of evaluating the function f to fun \n, and the costs of applying f to the respective applica\u00adtion sites appl and app2 . We have implemented \nboth profiling schemes, and tried them out in practice. Our experience is that lexical scoping usu\u00adally \nmeasures what the programmer expected , while eval\u00aduation scoping often does not. The main reason for \nthis is that evaluation scoping is very sensitive to whether an scc construct is placed around the complete \napplication of a function or not; for example, the costs attributed to f in (SCC f f) x are very different \nto those attributed to f in (SCC f f x). Although this allows evaluation scop\u00ading to express distinctions \nwhich are inaccessible in lexical scoping, we have found that in practice such distinctions are more \nof a hindrance than a help. Section 4 discusses particular situations in which even lexical scoping has \nunexpected behaviour. 3 Implementation Perhaps surprisingly, our lexical cost-attribution model can be \nimplemented quite cheaply. We begin our description of the implementation with an informal overview of \nthe measurement technology and the output it produces (Sec\u00adtion 3.1). After this we briefly discuss program \ntransfor\u00admation (Section 3.3), before turning to the question of how we bridge the gap between the (big-step) \ncost semantics of Section 2.3 and the individual steps taken by the execution mechanism (Section 3.4). \n3.1 Implementation overview Figures 3 and 4 give an example of the output produced by the profiler. The \nformer shows what fraction of the ex\u00adecution time (Xt ime) and what fraction of the heap alloca\u00adtion \n(%alloc) was attributed to each cost centre. The latter shows the composition of the (live) heap data, \nby cost centre, plotted against time, in the style of Runciman &#38; Wakeling Tue Hay 18 17:03 1993 Time \nand Allocation Profile (Lexical scoping) hsc-O. 13 +RTS -H25M -p -RTS -C -hi . . . total time = 240.48 \nsees (12024 ticks@ 20ms) total allot = 619,779,088 bytes 51,846,277 closures COST CEETRE scc subcc Xtime \n%alloc TypeChecker 1 0 45.4 44.6 Renamer i o 25.0 27.0 built inliames i o 7.6 14.4 PrintRealC 1 0 4.5 \n4.3 C0re2C0re 1 0 3.9 2.5 MAIN 1 i 2.7 2.7 CodeGen i o 1.5 1.1 rdmodule i o 1.8 1.2 stg2stg 1 0 1.1 \n0.5 FlattenAbsC i o 0.7 0.5 cvModule i 1 0.6 0.5 C0re2Stg i o 0.6 0.3 ... Figure 3: Cost Centre Profile \nof Glasgow Haskell compiler [1993]. These measurements are collected in the following way: At any moment \nthe current cost centre is held in a special register. For example, when evaluation of an scc expression \nis begun, the register is loaded with the specified cost cent re. A cost cent re is represented by a \npointer to a block of store which holds all the counters in which the costs attributed to that cost centre \nare accumulated.  A regular clock interrupt runs a service routine which increments a tick counter in \nthe current cost centre. This enables the relative cost of the different parts of the program to be determined, \nand reported in the %t ime column of Figure 3, This statistical sam\u00adpling works in just the same way \nas any standard Unix protiler; the more clock ticks that have elapsed, the more accurate the resuks. \n(The alternative, that of sampling a clock whenever the current cost centre is changed, would have unacceptable \noverheads, even apart from any worries about the accumulation of sys\u00adtematic quantisation errors.)  \nThe compiler plants in-line code to increment an allo\u00adcation counter held in the current cost centre \nwhenever a new heap object is allocated. In this way the pro\u00adfiler is able to record how much heap was \nallocated by each cost centre; this leads to the %alloc column of Figure 3. Whenever evaluation of (an \ninstance of) an scc expres\u00adsion is begun, the entry count of the new cost centre and the sub-entrg count \nof the current cost centre are both incremented. These counts, which are displayed in Figure 3 in columns \nscc and subc c respectively, pro\u00advide information similar to the function-call-counts of conventional \nprofilers. The sub-entry count is useful mainly as a reminder that some of the cost of evaluat\u00ading the \nexpression is attributed to an inner cost centre. hs..O13 +RTS .hC .410 -RTS -C -h, 254,942,256 byte, \nFc(May 2813251993 xseconds I R.6+.6x KIIW~1 I Man Pd ,w.w ?, wok I MainTW.xlba. I Ma! c- I I Maincwezma \nI MA. Fmlwmi I mm Kw.?dJ. 1,Xa I MS. Caezs,, I M8illFhM17.4h&#38; 1,x.x I Man%2,,,   l-l I AkPm!GAF \ns.!$9 I Mum-W. m Ej MainCa%wer I OTHER m 00 ZOO 400 530 WQ p.?o 1200 1400 IWO ,mo m, S6.nti Figure 4: \nHeap ~roflle of Glasgow Haskell compiler An extra field id added to every heap object. This field is \ninitialised from the current-cost-centre register when the object is allbcated. At regular intervals \n(multiples of a clock tick) a census of all live heap objects is taken. This allows the profiler to produce \na heap pr-ojiie, an example of whit h is given in Figure 4.  The cost-centre, field in heap objects \nplays an addi\u00adtional role for ~hunks (suspensions). It records the cost centre whi ~h was current when \nthe thunk was al\u00adlocated. When the thunk is subsequently evaluated,  1 the current-cost, centre register \nis loaded from the field in the thunk, thus neatly restoring its value at the mo\u00adment the thunk was created. \nThis is just what one would expect fr~m the Var(th u nk) rule of Figure 2. All of this is quite e~sily \ndone; indeed, one of the beauties of the approach is that the runtime implementation is so simple. What \ntakes a little more care is to make sure that every pass of our opt mising compiler respects cost attribu\u00ad \n1 tion: that is, it must not move costs from one cost centre to another. We turn to @is question next. \n3.2 Automatk annotation The use of explicit sc~ annotations allows fine control of the resolution of \nthe profiler. At one extreme, the program\u00admer can simply add ~ handful of s c c annotations which en\u00adclose \nthe main distinlct computations in the program. At the other, several inchvidual sub-expressions within \na single function definition (z$ready identified as a culprit) can be scc d to determine w~ch is responsible \nfor which costs. However, many prog~ammers will prefer to proiile a com\u00adpletely unmodified plogram. We \nsupport this in a straight\u00adforward way by providing a compiler flag which annotates every top-level definit~on \nin the module being compiled with an eponymous SCC. 1A practice, compiling an entire program with this \noption is of~en the fist step in finding which part of the program is eating the lion s share of the \nresources; it can then be follow ed, if necessary, with a more selective manual placement of s cc annotations. \n 3.3 Transformation Any optimizing compiler performs many different program transformations during compilation. \nFor the profiling results to be meaningful it is important that these program trans\u00adformations maintain \nthe correct attribution of costs. Specif\u00adically, program transformations that move evaluation from the \nscope of one cost centre to another must be avoided, Since cost centre boundaries are explicitly identified \nby the scc construct, the only transformations that have to be cur\u00adtailed are those that would move costs \nacross an scc bound\u00adary. Thus the transformations which have to be curtailed depend on the actual scc \nannotations introduced by the pro\u00adgrammer. In particular, the optimisation of a large program containing \nonly a few scc annotations is largely unaffected by profiling. If instead, every function is annotated \nwith a separate cost centre, inter-procedural optimisation which is very important in languages which \nencourage small func\u00adtions may have to be curtailed. However, the scc construct also provides us with \na language that can be used to express alternative versions of trans\u00adformations that would otherwise \nmove evaluation from the scope of one cost centre to another. For example, we can move a let binding \nacross an scc boundary if we annotate the sub-expression which is moved into the scope of a differ\u00adent \ncost centre with its original cost centre. \\y -> scc y let x=exp in body - let x = sccs~b y exp in \\y \n> scc y body The Scc.ub annotation is identical to an scc annotation, except that it doesn t increment \nthe entry counts. These are only incremented when the original scc is evaluated. The formal cost semantics \nprovides us with a framework in which the correctness of these alternative transformations with respect \nto the attribution of costs can be exam\u00adined and even proved. For example, in the transformation above \nwe can reason that the transformation does not affect the cost attribution, except by moving the small \ncost of al\u00adlocating the closure for x to the enclosing cost centre. (In our implementation, we are willing \nto accept moving small constant amounts of work from one cost centre to another, to allow more transformations \nto take place than would be the case if we were completely rigorous about not moving costs. ) A full \ntreatment of transformation in the presence of scc annotations is beyond the scope of this paper. The \ninterested reader is referred to Sansom [1994b].  3.4 An operational semantics tion. Sestoft [1994] \nbridges this gap in his derivation of an abstract machine from Launchbury s natural semantics. The first \nstep in this derivation introduces a stack to record the context (or continuation) of each subtree in \nthe state. We present a version of Sestoft s stack-based state-transition semantics, augmented with the \nmanipulation of cost centres and a notion of cost attribution for lexical scoDintz. This en\u00adables us \nto highlight a number of implementat~on-~elated de\u00adsign decisions, which are applicable to a number of \ndifferent abstract machines (see Section 3.6). We also prove that the cost attribution reported by the \nstate-transition semantics is correct with respect to the cost semantics (Section 3.5). The stack-based \nstate-transition semantics, which includes the manipulation of cost centres for lexical scoping, is given \nin Figure 5. The state consists of a 5-tuple (ccc, 17,e, S, 0) where ccc is the current cost centre, \n17is the annotated heap, e is the expression currently being evaluated or the control, S is the stack \n(initially empty), and O is the cost attribution of the evaluation to date. The state-transition rules \ncorrespond directly to the rules in the cost semantics. There is one state-transition rule which begins \nthe computation for each subtree in the corresponding semantic rule. In addition the Var-(thunk) rule \ngives rise to a second state-transition rule which updates the heap with the computed result. The correspondence \nbet ween the semantic and state-transition rules is summarised below: Semantic Rule I State Transition \nRule(s) Lambda n.a. Application appI, appz Var(whnf) varl Var(thunk) uarz, vars Let let Constructor n.a. \nCase easel, casez Scc Scc The key component of the state-transition rules is the stack. It is used to \nrecord any information which is required when evaluation of a subtree is complete. There are three places \nwhere this is required: In the Application rule the argument x is pushed onto the stack while the function \nexpression e is evaluated (rule appl ). When the A-abstraction is evaluated it retrieves this argument \noff the stack and evaluates its body (rule appz ). In the Var(thunk) rule an update marker #z is pushed \nonto the stack while the thunk e is evaluated (rule varz ). When evaluation is complete the result value \nz (a J-abstraction or constructor) will encounter the update marker on the stack and update the heap \n(rule vars). Having optimised the program, being sure to maintain the appropriate cost attribution, we \nthen have to generate code to execute and protlle the program. Unfortunately there is a large gap between \nthe formal cost semrmtics of Fig\u00adure 2 and any implementation, because the cost semantics is big-step \n, specifying the final result on the right-hand\u00adside of a judgement, and the implementation is necessarily \nsmall-step , specifying intermediate steps in the computa- In the Case rule the alternatives alts is \nsaved on the stack while the case expression e is evaluated (rule case 1). When the evaluation of the \nconstructor is com\u00adplete the appropriate alternative is selected and evalu\u00adated (rule casez ). We also \nsave the current cost centre on the stack with the alternative since it has to be restored when the alternative \nis evaluated. ccc Heap Control Stack 6 ~ ccc Heap Control Stack Attribution rule cc r ex so~ccr e z:S \nO@{cc++A} qpl CCA r Aye X:s e ~ CCA r e[x/y] so ~ppz cc r[$ c%.] ~ s 6 =+ CC..b r[Z $ Z] Z s ew{cctiv} \nuarl cc r[c C*e] x so~ccer e #z:s eltJ{cc *v} varz Ccz r z $X:s 9 * Ccz r[z c%z] z s !9W{cczl-+u} vars \ncc r let {z, = e,} in e St?= =+cc r[g, # a] s S 6 w {cc I-+ rzxH} let cc r case e of alts so~ccr e (alts, \ncc): S 6 W {cc I+ C} easel ccc r Ck~.k (alts, cc): S 8 * cc r C%[Zt/?Jka] so case2 cc r scc Ccscc e s \n8 ~ cc.=. r e Sb Scc In the varl rule ccsub = SUB(CCZ, cc), where SUB is as defined in Figure 2. In \nthe varz rule e ~ z. In the let rule the introduced variables, g,, must be distinct and fresh. The notation \n; means e[g,/~;]. In the case rules alts stands for the list {Cj ~~~ -> e~} of alternatives. In the casez \nrule e~ is the right hand side of the k th alternative. Figure 5: Stack-based State Transition Rules \nfor Lexical Scoping Lest all this seem too obvious, we remark that using eval\u00aduation rather than lexical \nscoping, which involves changing only one rule of the formal cost semantics, requires signifi\u00adcant and \npervasive changes to the state-transition semantics (the details are in Sansom [1994b]). 3.5 Correctness \nof the operational semantics We have proved that the cost attribution reported by the stack-based state-transition \nsemantics for lexical scoping (Figure 5) is correct with respect to the formal cost seman\u00adtics (Figure \n2). Theorem: (cc, {}, e, [], {}) =+* (cc , A,z, [], 6 ) if and only if cc, {}:e Qe A:z, cc  The proof \nis an extension of the correctness proof of Ses\u00adtoft s original stack-based state-transition semantics \n(Sestoft [1994]). Space prohibits us from including it here, but it can be found in the full version \nof this paper (Sansom &#38; Pey\u00adton Jones [1994]). 3.6 Implementing the operational semantics The state-transition \nsemantics are easily mapped onto a number of different abstract machines, including the G\u00admachine (August \nsson &#38; Johnsson [1989]), the STG-machine (Peyton Jones [1992]) and the TIM (Fairbairn &#38; Wray \n[1987]), since these abstract machines are all based on the same form of push-enter stack-based model \nof execution which captures the behaviour of our abstract state-transition semantics. (The particular \nabstract machines differ in their organisation of lower-level details such as the representation of environments \nand the update mechanisms employed.) A flavour of the low-level execution details for our imple\u00admentation, \nwhich is based on the STG-machine, was given in Section 3.1. Observe that the state-transition semantics \nonly attributes costs to the current cost centre. This obser\u00advation justifies the implementation sketched \nin Section 3.1, in which the consumption of time and space is simply at\u00adtributed to the current cost \ncentre. Details of our STG\u00admachine implementations, including extended STG-level op\u00aderational semantics, \ncan be found in Sansom [1994b]. An important aspect of any proiiler is the overhead it im\u00adposes. Our \nmeasurements show that programs take around twice as long to run when they are profiled, which is quite \nacceptable. If the heap profile is disabled (so that the heap census does not need to be taken), the \ntime increase is re\u00adduced to a factor of only 1.7. One reason for this good perfor\u00admance is we profile \nalmost-fully-optimised code, as discussed in Section 3.3. 4 Snares and delusions It would be nice to \nreport that lexical scoping always does the Right Thing that is what the programmer probably expected \n but our experience of using the profiler is oth\u00aderwise: there are a couple of important cases when it \ndoes not. This experience has led us to make the profiler more complicated in order to make it appear \nsimpler and more intuitive to the programmer. CC, r[Z CA Z]: ~ ~{..!+v} SUBCAF(CC2! cc) Var(whnf) r[~ \nCA z]:~, Cce,r:e &#38;j A:z, ccZ e % z Var(thunk) CC, r[m c%e]: x U{cc~V}WOu{ccS+U} A[z CA z] : Z, SUBCAF(ccz, \nCC) where SUBCAF( SUB , cc) = cc SUBCAF( CAF , cc) = cc SUBCAF(CC. , CC) = CCZ ccc Heap Control Stack \n8 ~ ccc Heap Control Stack Attribution rule cc r[z CA~] ~ s 6 ~ CCwb r[LZCAZ] Z S O M {cc* V} varl cc \nr[x c%e] x se~ccer e (#x, cc) : S b M {cc I+ V} var~ Ccz r z (#z, cc), s e ~ cc.., r[z 3 z] z S 0 W {cc. \n+ U} vara In the VCM1 and oars rules cc$ub = SUBCAF(CCZ, cc), where SUBCAF is defined above. In the \nvarz rule e ~ z. Figure 6: Modified Lexical Cost Semantics and State Transition Rules 4.1 Constant applicative \nforms difference is an q-reduction. However, they will be given different cost centres in the initial \nenvironment ri.it since andl is a top level function while and2 is a CAF: Consider the top-level definition: \n x::Int rin~~ = { andl % ~xs. foldr (M) True XS, x = if <expensive> then 1 else 2 and2 % foldr (WZ) \nTrue, . . . } This sort of top-level definition, which has no arguments, is Lexical scoping will subsume \nall the costs of applying andl to called a constant applicative form or CA F. When its value is its call \nsites, butthe costs of applying and2 will be attv-ibuted first demanded, its right hand side will be \nevaluated, and the to the cost centre CAF . A major change in cost attribution CAF will be updated with \nthe resulting value. Subsequent has resulted from an innocuous change in the program. demands for its \nvalue will incur no cost. Where, though, should the cost of the first evaluation be attributed? If we \nThe following declaration highlights the problem: attach the SUB cost centre to the binding, as we do \nwith top-level functions, the one-off evaluation costs would be at-Y :: Int -> Int = if <expensive> \nthen (\\a->el) else (\\a->e2)  tributed to the unfortunate cost-centre which happens to Y first evaluate \nthe CAF. This seems wrong: if the evaluation There are two sorts of cost associated with y: the one-offorder \nchanged, then so would the cost attribution, which is costs of deciding which of the two A-abstractions \nis to be y smost undesirable. value, and the repeated costs of applying that J-abstraction. An easy fix \nis to attribute the one-off evaluation costs to Lexical scoping attributes both costs to the CAF cost \ncen\u00ada special CAF cost centre which is attached to the CAF tre, whereas the programmer would probably \nexpect the bindings in r;~,,.3 costs of applying y to be subsumed by its call site. How\u00adever, the one-off \ncost of deciding which J-abstractions is to r,ntt = { x * if <expensive> then 1 else 2, . . . be applied \nshould still be attributed to the CAF cost cen\u00ad } tre. (This problem only arises under lexical scoping, \nsince Thus, the cost of evaluating x is attributed to the cost cen-evaluation scoping always attributes \nthe cost of applying a tre CAF . Alas, this solution still gives unexpected results. A-abstraction to \nits call site.) Consider the following two definitions: How, then, can we modify the formal cost semantics \nto deal andl ,and2 :: [Bool] -> Bool with the unexpected attribution of costs to CAFS? Our solu\u00adandl \nxs = foldr (W) True xs tion is to extend the notion of subsumed cost to values with and2 = foldr (%%) \nTrue the CAF cost centre, as well as top-level functions with the !!SUB cost centre. Any red-blooded \nfunctional programmer would expect these two definitions to be entirely equivalent: after all, the only \nThe modified cost semantics are given in Figure 6. In the Var(whn/) rule the current cost centre is returned \nif the clo\u00ad 31n practice we attach a specific cost centre to each CAF, for ex\u00ad sure s cost centre cc, \nis SUB or CAF . previously we only ample CAF:x , so that the programmer can separately identify the \ncosts associated with each CAF. returned the current cost centre for the SUB II cost centre. The Var(thunk) \nrule must also be modified to deal with a CAF closure that is not in whnf (CC. = CAF ). Since the one-off \ncosts of evaluating a CAF expression to whnf must be attributed to the CAF cost centre we always evaluate \nthe bound expression e in the context of cost centre CC.. However, if the result of that evaluation z \nis returned with the cost centre q CAF (cc, = CAF ) we return the enclosing cost centre cc instead. The \nclosure is still updated with the result cost centre cc= so that subsequent references to the closure \nwill be appropriately subsumed by the Var(whnf) rule. In this way the costs of repeatedly evaluating \nthe body of a A-abstraction, declared within the scope of a CAF, are subsumed by the call sites. The \ncorresponding modifications to the state-transition rules are also given in Figure 6. Apart from the \nuse of the SUBCAF predicate, the main difference is that when evaluation of a thunk is begun, the demanding \ncost centre, cc, is saved on the stack along with the update marker #x (rule varz ). When evaluation \nof the thunk is complete, the demanding cost centre is restored if the result has the cost centre CAF \n(rule VW-,). This mechanism for saving and restoring the current cost centre is very similar to that \nused in the case rules in Fimre 5. which save current cost cent re before eval\u00ad / uating the scrutinee, \nand restore it afterwards. Is the cure worse than the disease? We believe not: before we made this change \nwe received many messages from confused users asking why a large fraction of their program s execu\u00adtion \ncosts were attributed to CAF . One reason that this fraction is sometimes very high is that our implementation \nof overloading in Haskell uses dictionaries, which are tuples of method functions. It turns out that \nall of the costs of exe\u00adcuting these methods are attributed to the CAF cost centre enclosing the dictionary \ndefinition. Indeed, even non-CAF dictionaries can give unexpected cost attribution, precisely because \nthey are introduced by the compiler and invisible to the programmer, The details are in Sansom [1994b], \nand the solution is similar to that for CAFS. 4.2 CAFS introduced by the compiler Consider the following \ndefinition: fy=letz=exp ingzy  If exp is a constant expression that is, y is not free in exp then the \ndeclaration of z can be floated to the top-level and turned into a CAF, thus: z = exp fy=gzy This transformation \navoids recomputing z every time f is evaluated, since it will now only be evaluated once when f is first \ncalled. However, the programmer is expecting the repeated evaluation of z to be subsumed by the caller \nof f, rather than a single evaluation being attributed to a CAF cost centre. Since we want to profile \noptimised execution, which only evaluates z once, we nevertheless allow this opti\u00admisation to proceed. \nThe programmer is made aware of any costs attributed to CAFS introduced by the compiler. The change in \ncost attribution only arises for constant ex\u00adpressions which are not enclosed by an scc annotation. If, \ninstead, the body of f were annotated with an SCC, the let floating transformation described in Section \n3.3 annotates the floated CAF with an e.cc~~b. f y=scc f let z=exp ingzy ==+ Z = Scc.ub f eXp fy=scc \nf gzy Now the cost of the evaluating z is still (correctly) attributed to the cost centre f which enclosed \nthe original source declaration. 4.3 Higher order functions Consider the following definitions: f x \ny = expensive x y gl = scc gi (let f x y = expensive x y inh f>) g2 = SCC g2 (h f) ht =SCC h (t 23) \n The evaluation of each of g I and g2 gives rise to a single call to expensive, one via f ) and one via \nf. Question: to which cost centres are the costs of evaluating expensive at\u00adtributed? In the first case \nthe answer is clear: the closure f will bear the cost cent re g I , and when f > is entered inside h \nthe cost centre g 1 will be re-loaded ( Var(whnf) rule), and the costs of evaluating f ~ s body will \ntherefore accrue to gI . Unfortunately, the situation is different in the case of g2, be\u00adcause the argument \nto h is a top-level function, represented in r,ntt with cost centre SUB . When f is entered in h the \ncurrent cost centre, h is left in place ( Var(whnf) rule again), and the costs of executing f S body \nwi~ accrue to h . However, one would expect the costs of f to be sub\u00adsumed by g2 since the reference \nto f is in the scope of g2 . This difference is another example of unexpected profder behaviour. It is \neasily remedied by the following transforma\u00adtion: whenever a top-level un-profiled (i.e. subsumed) func\u00ad \ntion is passed as an argument, introduce a let-binding for that argument. In this example, we would replace \nthe call (h f) with the expression (let f = f in h f ). Now the costs of f s body will accrue to g2 \nas one might expect. 5 Using the profiler in practice A version of the lexical profiler has been distributed \nwith the Glasgow Haskell compiler since Version 0.15 (June 1993) en\u00adabling other users to profile their \nHaskell applications. This has provided us with invaluable feedback about the practical use of the profiler \nfor profiling large applications. The largest applications profiled to date are the Glasgow Haskell Com\u00adpiler \n(ghc) itself and LOLITA4 a natural language system develoDed bv the Artificial Intellkence Research \nGrou~. at the U~lversity of Durham. Each of these applications con\u00adsist of over 30,000 lines of Haskell \ncode, divided into more than 100 different source modules. The initial profile of the ghc compiler, after \nexplicitly an\u00adnotating each of the main passes, highlighted two execution hot-spots: the Renamer (25%+8% \nbuilt inNames) and the Typechecker (45%) (see Figure 3). Further investigation revealed a compiler transformation \nbug which duplicated work, and a very inefficient substitution algorithm. Fixing the compiler bug and \ndeveloping a more efficient substitu\u00adtion algorithm5, improved the average compilation time by over 50% \n(Sansom [1994b]). Proiling of LOLITA initially used automatic annotation to identify the basic functions \nresponsible for a large propor\u00adtion of the execution time. Improving these functions, which were generally \nsimple operations called hundreds of thou\u00adsands of times, gave an initial performance improvement of \n8%. Further Performance imm-ovements were then obtained by developin~ more efficient =dgorithms and data \nstructures for system components, such as the word lookup table and the semantic network. which consumed \na large mo~ortion of the execution time. The overall improve~e~t ~o date is about 35% with many further \nimprovements envisaged (Jarvis [1994]). The experience gained actually using the profiler to profile \nreal programs has provided invaluable feedback. It enabled us to compare the usability of the different \ncost attribution schemes and it drew our attention to the problem attributing CAF costs under the lexical \nprofiling scheme. 6 Related work Aside from the profiler developed here, we are aware of only three execution \nprofilers for non-strict functional languages that relate the profiling data back to the program source. \nThe first two profile only space. The third profiles time as well, but has only an interpreted implementation. \nThe hbc/lml heap profiler (Runciman &#38; Wakeling [1993]) was developed concurrently with our heap profiler, \nand has very similar goals indeed, we use its post-processor to pro\u00adduce Postscript graphs. Aside from \nthe absence of time pro\u00adfiling, the main difference from our work is that the hbc/lml profiler does not \nprovide a mechanism for aggregating infor\u00admation up the call graph. For example, a profile may indi\u00adcate \nthat heap objects allocated by a certain function, say map, occupy a large amount of heap space. However \nthere is no mechanism to determine which application(s) of map were responsible for producing these cells \n(Kozato &#38; Otto [1993]). More recently the nhc heap profiler (Runciman &#38; Rojemo [1994]) extends \nthe hbc/lml heap profiling ideas. It pro\u00ad vides additional heap profiles which attempt to identify the \n4LOLITA Large-scale, Object-based, Linguistic Interact or, Trans\u00adlator and Analyser. 5 The new sub.t,tution \nd~orithm uses a mutable array (Launch\u00adbury [1993b]). It is 50 times more efficient than the orlgmal algorithm \n(Sansom [1994b]). cause, rather then the presence, of an unexpected space leak. In particular, the retainer \nprofile breaks down the heap ob\u00adjects by the (set of) objects that reference them, a most interesting \ndevelopment. Clack, Clayman &#38; Parrott [1994] have developed a time and space profiler for an interpreted \ngraph reduction sys\u00adtem. They report the time and space usage for a set of functions specified by the \nprogrammer, the profiled func\u00adtions . Though there are encouraging similarities between their profiler \nand our lexical profiler, there are also some im\u00adportant differences. First, since their system profiles \ninter\u00adpreted graph reduction, a substantial performance penalty is paid for the benefit of profiling, \nand the profiling informa\u00adtion is not necessarily faithful to a compiled version of the same program. \nSecond, and more important, if a particular function is called (directly or indirectly) by two or more \nprofiled func\u00adtions, then it implicitly becomes a separately-profiled func\u00adtion. There is no way to subsume \nits costs into those of its callers, except by duplicating the function (and the functions it calls...). \nIn contrast, our profiler subsumes the cost of all un-profiled functions even if they are shared. We \nbelieve this to be a major strength of our profiling scheme since it enables the costs of the logical \nparts of a program to be aggregated together, regardless of the sharing properties of the program. It \nis more straightforward to profile a strict language than a non-strict one, since there are no unevaluated \nclosures to worry about. The semantics in Figure 2 are easily stricti\u00adfied by: restricting the Let rule \nto only bind a set of mu\u00adtually recursive function values; introducing a second Let rule which binds \nthe value of a strictly evaluated expression; and omitting the Var(thunk) rule. The result is a profiling \nscheme very similar to the SML profiler described by Ap\u00adpel, Duba &#38; MacQueen [1988]. They introduce \nthe idea of a current function , to which regular timer interrupts are at\u00adtributed, and provide a mechanism \nwhich enables the costs of Lun-profiled functions to be subsumed by the call site. These two ideas correspond \nclosely to our notions of cur\u00adrent cost centre and subsumed toD-level functions. Their . implementation \nuses side effects to manipulate the current function at the language level; the correct attribution of \ncosts is implicitly maintained by the optimiser s treatment of the operations which manipulate the current \nfunction at the function boundaries. Our work could be viewed as a development of the SML profiler, extending \nit to non-strict languages, and providing a formal model to undergird it. 7 Conclusions We have described \na time and space profiler for a state-of\u00adthe-art compiled implementation of Haskell. A key contri\u00adbution \nis our formal model for cost attribution. Its main benefit is to provide a precise framework in which \nto discuss design decisions without becoming enmeshed in the details of a particular implementation. \nBefore we developed the formal model, we found the differences between lexical and evaluation scoping \nalmost impossible to understand and ex\u00adplain clearly, and the modified lexical scheme never occurred \nto us. The formal cost semantics is deliberately slanted towards the programmer rather than the implementation. \nWe also devel\u00adoped a small-step state-transition semantics, which directly expresses the manipulation \nof cost centres performed by the implementation, and proved it equivalent to the original cost semantics. \nThis proof greatly increases our cordidence in the correctness of the implementation. Our work on the \nsemantics is not yet complete. In partic\u00adular, we should have a proof that cost attribution is indeed \nindependent of evaluation order; and we should have proofs that a variety of program transformations \npreserve the cost attribution. It is also somewhat unsatisfactory that we lack a way to be sure that \nthe list of snares and delusions in Section 4 is complete. It is encouraging, though, that the profiler \nhas been suc\u00adcessfully applied to a number of large applications. Its use quickly focusses the programmer \ns attention on the actual execution hot-spots. The profiler has already proved invalu\u00adable in evaluating, \ncomparing and tuning the performance of different algorithmic components within large applications. Bibliography \nAW Appel, BF Duba &#38; DB MacQueen [Nov 1988], Profding in the presence of optimization and garbage \ncollec\u00adtion, Technical Report CS-TR-197-88, Princeton University. L Augustsson &#38; T Johnsson [April \n1989], The Chalmers Lazy-ML Compiler, The Computer Journal 32(2), 127 141. C Clack, S Clayman &#38; D \nParrott [March 1994], Lexical Profiling: Theory and Practice, Dept of Computer Science, University College \nLondon. To appear in Journal of Functional Programming. Jon Fairbaim &#38; Stuart Wray [Sept 1987], TIM \n-a sim\u00adple lazy abstract machine to execute supercombina\u00adtors, in Proc IFIP conference on Fictional \nPro\u00adgramming Languages and Computer Architecture, Portland, G Kahn, ed., Springer-Verlag, LNCS 274. SL \nGraham, PB Kessler &#38; MK McKusick [1983], An exe\u00adcution profiler for modular programs, Software Practice \nand Experience 13(8), 671-685. P Hudak, SL Peyton Jones, PL Wadler, Arvind, B Bou\u00adtel, J Fairbairn, J \nFasel, M Guzman, K Hammond, J Hughes, T Johnsson, R Kieburtz, RS Nikhil, W Partain &#38; J Peterson [May \n1992], Report on the functional programming language Haskell, Version 1.2, ACM .STGPLAN Notices 27(5). \nSA Jarvis [April 1994], Profiling Large Scale Lazy Func\u00adtional Systems, Artificial Intelligence Research \nGroup, University of Durham. Y Kozato &#38; GP Otto [June 1993], Benchmarking real-life image processing \nprograms in lazy functional lan\u00adguages, Functional Programming Languages and Computer Architecture, Copenhagen, \nDenmark, 18 27. J Launchbury [Jan 1993a], A natural semantics for lazy evaluation, 20th ACM Symposium \non Principles of Programming Languages, Charleston, South Car\u00adolina, 144 154. J Launchbury [June 1993b], \nLazy imperative program\u00adming, Proceedings of ACM SIGPLAN Workshop on State in Programming Languages, \nCopenhagen, Denmark, 46-56. Available as Research Report YALEU/DCS/RR-968, Yale University. SL Peyton \nJones [April 1992], Implementing lazy functional languages on stock hardware: the Spineless Tagless G-machine, \nJournal of Rmctional Programming 2(2), 127-202. SL Peyton Jones, CV Hall, K Hammond, WD Partain &#38; \nPL Wadler [March 1993], The Glasgow Haskell com\u00adpiler: a technical overview, Joint Framework for Information \nTechnology (JFIT) Technical Confer\u00adence Digest, Keele, 249-257. C Runciman &#38; N Rojemo [1994], New \ndimensions in heap profiling, Departments of Computer Science, Chalmers University and University of \nYork. C Runciman &#38; D Wakeling [April 1993], Heap profiling of lazy functional programs, Journal of \nFhnctionzd Programming 3(2), 217-245. PM Sansom [1994a], Time profiling a lazy functional com\u00adpiler, \nin Fwctional Programming, Glasgow 1993, K Hammond &#38; J O Donnell, eds., Workshops in Computing, Springer \nVerlag. PM Sansom [Sept 1994b], Execution profiling for non-strict functional languages, PhD thesis, \nResearch Report FP-1994-09, Dept of Computing Science, Univer\u00adsit y of Glasgow. (ftp: // ftp. dcs. glasgow. \nac .uk/pub/ glasgow-fp/tech~ eports\\ FP-94-09.execut ion-prof iling. ps. Z). PM Sansom &#38; SL Peyton \nJones [Nov 1994], Time and space profiling for non-strict, higher-order func\u00adtional languages, Research \nReport FP-1994-1O, Dept of Computing Science, University of Glasgow. (f tp: //f tp.dcs. glasgow. ac .uk/pub/ \nglasgow fpltechzeportsl FP-94-10-t imespace-prof iling. ps. Z). P Sestoft [April 1994], Deriving a Lazy \nAbstract Machine, Dept of Computer Science, Technical Um versity of Denmark.  \n\t\t\t", "proc_id": "199448", "abstract": "<p>We present the first profiler for a compiled, non-strict, higher-order, purely functional language capable of measuring <italic>time</italic> as well as <italic>space</italic> usage. Our profiler is implemented in a production-quality optimising compiler for Haskell, has low overheads, and can successfully profile large applications.</p><p>A unique feature of our approach is that we give a formal specification of the attribution of execution costs to cost centres. This specification enables us to discuss our design decisions in a precise framework. Since it is not obvious how to map this specification onto a particular implementation, we also present an implementation-oriented operational semantics, and prove it equivalent to the specification.</p>", "authors": [{"name": "Patrick M. Sansom", "author_profile_id": "81100173091", "affiliation": "Dept. of Computing Science, University of Glasgow, Glasgow, Scotland", "person_id": "P220614", "email_address": "", "orcid_id": ""}, {"name": "Simon L. Peyton Jones", "author_profile_id": "81100271851", "affiliation": "Dept. of Computing Science, University of Glasgow, Glasgow, Scotland", "person_id": "PP39035577", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/199448.199531", "year": "1995", "article_id": "199531", "conference": "POPL", "title": "Time and space profiling for non-strict, higher-order functional languages", "url": "http://dl.acm.org/citation.cfm?id=199531"}