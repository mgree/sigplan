{"article_publication_date": "01-25-1995", "fulltext": "\n Optimizing an ANSI C Interpreter with Superoperators Todd A. Proebsting University of Arizona Abstract \nThis paper introduces superoperators, an opti\u00ad mization technique for bytecoded interpreters. Superoperators \nare virtual machine operations automatically synthesized from smaller opera\u00ad tions to avoid costly per-operation \noverheads. Superoperators decrease executable size and can double or triple the speed of interpreted \npro\u00ad grams. The paper describes a simple and effec\u00ad tive heuristic for inferring powerful superopera\u00ad \ntors from the usage patterns of simple operators. The paper describes the design and implemen\u00ad tation \nof a hybrid translator/interpreter that em\u00ad ploys superoperators. From a specification of the superoperators \n(either automatically inferred or manually chosen), the system builds an efficient implementation of \nthe virtual machine in assem\u00ad bly language. The system is easily retargetable and currently runs on the \nMIPS R3000 and the SPARC. 1 Introduction Compilers typically translate source code into machine language. \nInterpreter systems trans\u00ad late source into code for an underlying virtual *Address: Todd A. Proebsting, \nDepartment of Com\u00adputer Science, University of Arizona, Tucson, AZ 85721. Internet: t odd@2cs. arizona. \nedu  Permission to copy without fee all or part of this material is granted provided that the copies \nare not made or distributed for direct commercial advantaqe, the ACM copyright notice and the title of \nthe publication and Its date appear, and notice is given that copying is by permission of the Association \nof Computing Machinery. To copy otherwise, or to republish, requires a fee andor specific permission. \nPOPL 951/95 San Francisco CA USA @ 1995 ACM 0-89791 -692-1/95/0001 ....$3.50 machine (VM) and then interpret \nthat code. The extra layer of indirection in an interpreter presents time/space tradeoffs. Interpreted \ncode is usually slower than compiled code, but it can be smaller if the virtual machine operations are \nproperly encoded. Interpreters are more flexible than compilers. A compiler writer cannot change the \ntarget ma\u00adchine s instruction set, but an interpreter writer can customize the virtual machine, For instance, \na virtual machine can be augmented with special\u00adized operations that will allow the interpreter to produce \nsmaller or faster code. Similarly, chang\u00ad ing the interpreter implementation to monitor program execution \n(e.g., for debugging or pro\u00ad filing information) is usually easy. This paper will describe the design \nand imple\u00admentation of ht i, a hybrid translator/interpreter system for ANSI C that has been targeted \nto both the MIPS R3000 [KH92], and the SPARC [Sun91]. ht i will introduce superoperators, a novel optimization \ntechnique for customizing in\u00adterpreted code for space and time, Superopera\u00adtors automatically fold many \natomic operations into a more efficient compound operation in a fashion similar to supercodinators in \nfunctional language implementations [FH88]. Without su\u00adperoperators ht i executable are only 8-16 times \nslower than unoptimized natively compiled code. Superoperators can lower this to a factor of 3-9. Furthermore, \nht i can generate program-specific superoperators automatically. The hybrid translator, ht i, compiles \nC func\u00adtions into a tiny amount of assembly code for function prologue and interpreted bytecode in\u00adstructions \nfor function bodies. The bytecodes 322 represent the operations of the interpreter s vir\u00adtual machine. \nBy mixing assembly code and bytecodes, hti maintains all native code calling conventions; ht i object \nfiles can be freely mixed with compiled object files. The interpreter is implemented in assembly language \nfor efficiency, Both the translator, ht i, and the interpreter are quickly retargeted with a small machine \nspecification. 2 Translator Output ht i uses lCC S front end to translate ANSI C programs into its intermediate \nrepresentation (IR) [FH91b, FH91a]. lCC S IR consists of ex\u00adpression trees over a simple 109-operator \nlan\u00adguage. For example, the tree for 2+3 would be ADD I (CNSTI, CNSTI ), where ADD I represents in\u00adteger \naddition (ADD+ I), and the GNSTI s repre\u00adsent integer constants. The actual values of the CNSTI s are \nIR node attributes. ht i s virtual machine instructions are byte\u00adcodes (with any necessary immediate \nvalues). The interpreter uses an evaluation stack to eval\u00aduate all expressions. In the simplest ht i \nvirtual machines, there is a one-to-one correspondence between VM bytecodes and lcc IR operators (superoperators \nwill change this). Translation is a left-to-right postfix emission of the bytecodes. Any necessary node \nattributes are emitted im\u00admediately after the corresponding bytecode. For this VM, the translation of \n2+3 would be similar to the following: byte 36 # CNSTI . word 2 # immediate value byte 36 # CNSTI . word \n3 # immediate value byte 8 # ADDI The interpreter implements operations via a jump-table indexed by bytecodes. \nThe inter\u00ad preter reads the first CNSTI S bytecode (36), and jumps to CNSTI s implementation. CNSTI code \nreads the attribute value (2) and pushes it on the stack. The interpreter similarly handles the 3. After \nreading the ADDI bytecode, the inter\u00adpreter pops two integers off the evaluation stack, and pushes their \nsum. The evaluation stack for each translated pro\u00adcedure exists in its own activation record. Lo\u00adcal \nstacks allow programs to behave correctly in the presence of interprocedural jumps (e.g., longjmp). ht \ni produces an assembly file. Most of the file consists of the bytecode translation of C func\u00adtion bodies, \nand data declarations. ht i does, however, produce a tiny amount of assembly language for function prologues. \nPrologue code tells the interpreter how big the activation record should be, where within it to locate \nthe evalua\u00adtion stack, where to find the bytecode instruc\u00adtions, and ultimately for transferring control \nto the interpreter. A prologue on the R3000 looks like the following: main: li $24, 192 # put act ivat \nion # record size in $24 li $8, 96 # put location of # evaluation stack in $8 la $25, $$11 # put location \nof # bytecode in $25 j .prologue.scalar # jump to interpreter (-prologue_scalar unloads scalar arguments \nonto the stack the R3000 calling conventions require a few different such prologue routines. Once the \narguments are on the stack, the inter\u00adpreter is started.) Prologue code allows natively compiled procedures \nto call interpreted proce\u00addures without modification. 3 Superoperator Optimization Compiler front ends, \nincluding ICC, produce many IR trees that are very similar in struc\u00adture. For instance, ADDP (INDIRP \n(x), CNSTI) is the most common 3-node IR pattern produced by lcc when it compiles itself. (x is a place\u00adholder \nfor a subtree.) This pattern computes a pointer value that is a constant offset from the value pointed \nto by x (i.e., the l-value of x->b in c). With only simple VM operators, translating ADDP ( INDIRP (x) \n, CNSTI ) requires emitting three bytecodes and the CNSTI S attribute. Interpret\u00ading those instructions \nrequires 1. Reading the INDIRP bytecode, popping x s value off the stack, fetching and pushing the referenced \nvalue, 2. Reading the CNSTI bytecode and attribute, and pushing the attribute, 3. Reading the ADDP \nbytecode, popping the two just-pushed values, computing and pushing their sum.  If the pattern ADDP \n(INDIRP (x) , CNSTI) were a single operation that takes a single operand, x, the interpreter avoids 2 \nbytecode reads, 2 pushes, and 2 pops. This new operator would have one attribute the value of the embedded \nCNSTI. These synthetic operators are called superopera\u00ad tors. Superoperators make interpreters faster \nby eliminating pushes, pops, and bytecode reads. Furthermore, superoperators decrease code size by eliminating \nbytecodes. The cost of a super\u00adoperator is an additional bytecode, and a cor\u00adrespondingly larger interpreter. \nExperiments in j8 show that carefully chosen superoperators re\u00adsult in smaller and significantly faster \ninterpreted code. 3.1 Inferring Superoperators Superoperators can be designed to optimize the interpreter \nover a wide range of C programs, or for a specific program. The lcc IR includes only 109 distinct operators, \nthus leaving 147 byte\u00adcodes for superoperators. Furthermore, if the in\u00adterpreter is being built for a \nspecific application, it may be possible to remove many operations from the VM if they are never generated \nin the translation of the source program (e.g., floating point operations), thereby allowing the creation \nof even more superoperators. The addition of superoperators increases the size of the interpreter, but \nthis can be offset by the corresponding reduction of emitted byte\u00adcodes. Specific superoperators may \noptimize for space or time. Unfortunately, choosing the opti\u00admal set of superoperators for space reduction \nis NP-complete External Macro Data Compres\u00adsion (SR22 [GJ79]) reduces to this problem. Sim\u00adilarly, optimizing \nfor execution time is equally complex. 3.1.1 Inference Heuristic ht i includes a heuristic method for \ninferring a good set of superoperators. The heuristic reads a file of IR trees, and then decides which \nad\u00adjacent IR nodes should be merged to form new superoperators. Each tree is weighted to guide the heuristic. \nWhen optimizing for space, the weight is the number of times each tree is emit\u00adted by the front end of \nICC. When optimizing for time, the weight is each tree s (expected) ex\u00adecution frequency. A simple greedy \nheuristic creates superopera\u00adtors. The heuristic exams all the input IR trees to isolate all pairs of \nadjacent (parent/child) nodes. Each pair s weight is the sum of the weights of the trees in which it \nappears. (If the same pair appears N times in the same tree, that tree s weight is counted IV times.) \nThe pair with the greatest cumulative weight becomes the su\u00adperoperator formed by merging that pair. \nThis new superoperator then replaces all occurrences of that pair in the input trees. For example, as\u00adsume \nthat the input trees with weights are I(A(Z, Y)) 10 A(Y, Y) 1 The original operators s frequencies of \nuse are Y 12 z 10 I 10 A 11 The frequencies of the parent/child pairs are I(A(*)) 10 A(Z, *) 10 A(*, \nY) 11 A(Y, *) 1 Therefore, A (*, Y) would become a new superop\u00aderator, B. This new unary operator will \nreplace the occurrences of A(* ,Y) in the subject trees. The resulting trees are I(B(Z)) 10 B(Y) 1 The \nnew frequencies of parent/child pairs are I(B(*)) 10 B(Z) 10 B(Y) 1 Repeating the process, a new superoperator \nwould be created for either I (B (*) ) or B(Z). Ties are broken arbitrarily, so assume that B (Z) becomes \nthe new leaf operator, C. Note that C is simply the composition of A (Z, Y). The rewritten trees are \nI(C) 10 B(Y) 1 The frequencies for the bytecodes is now Yi zo I 10 Ao B1  c 10 It is interesting to \nnote that the B superoper\u00adator is used only once now despite being present in 11 trees earlier. Underutilized \nsuperoperators inhibit the creation of subsequent superoperators by using up bytecodes and hiding constituent \npieces from being incorporated into other su\u00adperoperators. Unfortunately, attempting to take advantage \nof this observation by breaking apart previously created, but underutilized superoper\u00adators was complicated \nand ineffective. Creating the superoperators B and C elimi\u00adnated the last uses of the operators A and \nZ, re\u00adspectively. The heuristic can take advantage of this by reusing those operators s bytecodes for \nnew superoperators. The process of synthesizing superoperators repeats until exhausting all 256 bytecodes. \nThe heuristic may, of course, merge superoperators together. The heuristic implementation requires only \n204 lines of Icon [GG90]. The heuristic can be configured to eliminate obsolete operators (i.e., reuse \ntheir bytecodes), or not, as superoperators are created. Not eliminating obsolete operators allows the \nresulting translator to process all pro\u00adgrams, even though not specifically optimized for them. 4 Translator \nDesign 4.1 Bytecode Emitter ht i translates lCC S IR into bytecodes and at\u00ad tributes. Bytecodes can represent \nsimple IR op\u00aderators, or complex superoperator patterns. The optimal translation of an IR tree into bytecodes \nis automated via tree pattern matching using burg [FHP92]. burg takes a cost-augmented set of tree patterns, \nand creates an efficient pattern matcher that finds the least-cost cover of a sub\u00adject tree. Patterns \ndescribe the actions associ\u00adated with bytecodes. Some sample patterns in the burg specification, interp. \ngr, follow: stk : ADDP (INDIRP (stk), CNSTI) =5(l); stk : ADDP(stk, stk) =9(l); stk :CNSTI =36 (1) ; \nstk : INDIRP(stk) = 77 (1) ; The nonterminal stk represents a value that re\u00adsides on the stack. The integers \nafter the = s are the burg rule numbers, and, are also the actual bytecodes for each operation. Rule \n9, for exam\u00adple, is a VM instruction that pops two values from the stack, adds them, and pushes the sum \nonto the stack. The ( 1 ) s represent that each pattern has been assigned a cost of 1. The pat\u00adtern matcher \nwould choose to use rule 5 (at cost 1) over rules 9, 36, and 77 (at cost 3) whenever possible. The burg \nspecification for a given VM is gen\u00aderated automatically from a list of superoperator patterns. To change \nthe superoperators of a VM and its associated translator and interpreter one simply adds or deletes \npatterns from this list and then re-builds ht i. ht i can be built with inferred, or hand-chosen superoperators. \n 4.2 Attribute Emitter ht i must emit node attributes after appropri\u00adate bytecodes. In the previous example, \nit is necessary to emit the integer attribute of the CNSTI node immediately after emitting the bytecodes \nfor rules 5 or 36. This is sim\u00ad ple for single operators, but superoperators may need to emit many attributes. \nThe pat\u00adtern ADDI (MULI (x, CNSTI), CNSTI) requires two emitted attributes one for each CNSTI. To build \nht i, a specification associates at\u00adtributes with IR operators. A preprocessor builds an attribute emitter \nfor each superoper\u00adator. The attribute specification for CNSTI is reg: CNSTI = (1) emitsymbol (%P->syms \n[0] ->x. name, 4, 4) ; The pattern on the first line indicates that the interpreter will compute the \nvalue of the CNSTI into a register at cost 1. The second line indi\u00adcates that the translator emits a \n4-byte value that is 4-byte aligned. The preprocessor ex\u00adpands %P to point to the CNSTI node relative \nto the root of the superoperator in which it exists. W>syms [01 ->x. name is the emitted value. For the \nsimple operator, stk: CNSTI, the attribute emitter executes the following call after emitting the bytecode \nemitsymbol(p->syms [0] ->x. name, 4, 4) ; where points to the CNSTI. P For stk: ADDP (INDIRP (STK), CNSTI), \nthe at\u00adtribute emitter executes emit symbol (p->ki.ds [i] ->syms [0] ->x. name, 4, 4); where p->kids \n[1] points to the CNSTI relative to the root of the pattern, ADDP. A preprocessor creates a second burg \nspec\u00adification, math. gr, from the emitter specifica\u00adtion. The emitter specification patterns form the \nrules in math. gr. The math. gr-generated pat\u00adtern matcher processes trees that represent the VM S superoperators. \nFor every emitter pattern that matches in a superoperator tree, the asso\u00adciated emitter action must be \nincluded in the translator for that superoperator. This is done automatically from the emitter specification \nand the list of superoperator trees. (Single node VM operators are always treated as degenerate su\u00adperoperators.) \nAutomating the process of trans\u00adlating chosen superoperators to a new interpreter is key to practically \nexploiting superoperator op\u00adtimization. 5 Interpret r Generation ! The interpreter s guage. Assembly1 \nimplemented language in asenables sembly lan\u00adimportant optimizations li~e keeping the evaluation stack \npointer and inte Jpreter program counter in hard\u00adware registers. ~uch of the interpreter is auto\u00admatically \ngenera ked from a target machine spec\u00adification and the list of superoperators. The tar\u00adget machine spe \n ification maps IR nodes (or pat\u00adterns of IR nod bs) to assembly language. For instance, the m J pping \nfor ADDI on the R3000 is reg: ADDI (reglreg) = (1) addu %Or, xlr, %2r\\n This pattern i dicates that integer \naddition (ADDI) can be Jomputed into a register if the operands are in legisters. %Or, %lr, and x2r rep\u00adresent \nthe regis ers for the left-hand side non\u00adterminal, the Ie t-most right-hand side nonter\u00adminal, and the \nn xt right-hand side nonterminal, respective y. The machine pacification augments the emit\u00adter specification \ndescribed above they share the same patter s. Therefore, they can share the I same burg-generated pattern \nmatcher. The pat\u00adtern matcher processes superoperator trees to de\u00adtermine how to d est translate each \ninto machine code. Below is a small specification to illustrate n the complete tra slation for an ADDI \noperator. reg: ADDI(reg, reg) = (1) addu % r, %lr, %2r\\n reg: STK = (1) lu !lOr, XP4-4($19) \\n 1 stint: \nreg =() sw %Or, %U4($19)\\n STK is a terminal symbol representing a value on \\ the stack. The second \nrule is a pop from the eval\u00aduation stack int~ a register. ~P4 is a 4-byte pop, and $19 is the eJ aluation \nstack pointer register. The third rule is push onto the evaluation stack I from a register. ,u4 is the \n4-byte push. To generate he machine code for a sim\u00adple ADDI operati n, the interpreter-generator re\u00adduces \nthe tree I nontermi- A ~DI (STK, STK) to the nal stint using the pattern matcher. The re\u00ad and sign-extends \na l-byte value into a 4-byte sulting code requires two instances of the second register. This corresponds \nto the IR pattern, rule, and one each of the first and third rules: CVCI(INDIRC(X) ) . The specification \nfor this complex pattern follows. lW $8, 0-4($19) # pop left operand # (reg: STK) reg: CVCI (INDIRC(reg) \n) = (1) lW $9, -4-4($19) # pop right operand lb %Or, O(%lr)\\n # (reg: STK) II 11 addu $8, $8, $9 # add \nthem # (reg: ADDI(reg,reg)) The interpreter-generator may use this rule for SW $8, -8($19) # push the \nresult any superoperators that include # (stint: reg) CVCI(INDIRC(X) ) . addu $19, -4 # adjust stack \n5.1 Additional IR Operator The interpreter-generator automatically allo\u00ad cates registers $8 and $9 and, \ngenerates codeto To reduce the size of bytecode attributes, one adjust the evaluation stack pointer. \nadditional IR operator was added to 1 cc s orig- The interpreter-generator selects instructions inal \nset: ADDRb. 1CC7S ADDRLP node represents and allocates temporary registers for each super\u00ad the offset \nof a local variable relative to the frame operator. In essence, creating an interpreter is pointer. ht \ni emits a 4-byte offset attribute for traditional code generation except that it is ADDRLP. ADDRLb is \nsimply an abbreviated ver\u00ad done for astatic set ofIRtreesbejore any source sion of ADDRLP that requires \nonly a l-byte off\u00ad codeis actually translated. set. Machine-independent back end code does The emitter \nand machine specifications use the this translation. same patterns, so only one file is actually main\u00ad \ntained. The juxtaposition of the emitter code 6 Implementation Details and machine code makes their relationship \nex\u00ad plicit. Below is the complete R3000 specification Building an hti interpreter is a straightfor\u00ad for \nCNSTI. ward process. The following pieces are needed to build ht i s translator and interpreter: reg: \nCNSTI = (1) addu $17, 7; A target machine/emitter specification. srl $17, 2; Sll $17, 2; lW tiOr, -4($17)\\n \n lcc back end code to handle data layout, calling conventions, etc. emits ymbol(%P->syms[ OI->x. name, \n4, 4) ;  A library of interpreter routines for observ- Register $17 is the interpreter s program counter \ning calling conventions. (pc). The first three instructions advance the pc Machine-dependent interpreter-generator \npast the 4-byte immediate data and round the routines. address to a multiple of 4, (Because of assembler \nand linker constraints on the R3000, all 4-byte Figure 1 summarizes the sizes of the machine data must \nbe word aligned.) The lW instruction dependent and independent parts of the system loads the immediate \nvalue into a register. (lCC S front end is excluded). Machine/emitter specifications are not limited \nThe R3000-specific back end code and the to single-operator patterns. Complex IR tree interpreter library \nare much bigger than the patterns may better express the relationship be- SPARC S because of the many \nirregular argu\u00ad tween target machine instructions and lCC S IR. ment passing conventions observed by \nC code on For example, the R3000 lb instruction loads the R3000. Function Language Sizes (in] ines) \nMachine Independent R3000 SPARC Target Specification grammar 351 3.54 lcc back end c 434 244 170 interpreter \nlibrary asm 130 28 interpreter generator c 204 72 70 Figure 1: Implementation Details I 7 System Obstacles \nject code for eac~ function consists of a native\u00adcode prologue, WI ! h interpreted bytecodes for the \nUnfortunately, hti sexecutables are slower and function body. Object files are linked together bigger \nthan they ought to be because oflimi\u00ad with appropriate C libraries (e.g., libc. a) and tations of system \nsoftwareon both R3000 and executable be com\u00ad the interpreter. The may SPARC systems. The limitations \narenotintrin\u00ad pared to natively compiled code for both size sic to the architectures orhti; they are \njust the and speed. The code size includes the function results of inadequate software. prologues, byteco \nales, and one copy of the inter-Neither machine s assembler supports un\u00ad preter. Interpreter comparisons \nwill depend on aligned initialized data words or halfwords. This available superop Lraters. can cause \nwasted space between a bytecode and Comparisons w~re made for three programs: its (aligned) immediate \ndata. Consequently, the burg: A W5 000-line tree pattern matcher interpreter must execute additional \ninstructions generator, p ! ocessing a 136-rule specifica\u00ad to round its pc up to a 4-byte multiple be\u00adtion. \nfore reading immediate 4-byte data. Initial tests indicate that approximately 17 %0 of the bytes o hti: \nThe W1 3,000-line translator and sub\u00ademitted by ht i are wasted because of alignment ject of this p aper, \ntranslating a 1117-line C problems.1 file. The R3000 assembler restricts the ability to loop: An e pty \nfor loop that executes emit position-relative initialized data. For in\u00ad10,000,000 ti es. stance, the \nfollowing is illegal on the R3000: L99 : On the 33 MHz R3000, hti is compared to a 1 . word 55 production \nqualit lcc compiler. Because 1 cc s . word . -L99 SPARC code genl[ rator is not available, ht i is compared \nto ace, Sun s ANSI C compiler, on Position relative data would allow ht i to im\u00adthe 33 MHz Sun J /490. \nBecause ICC does little plement pc-relative jumps and branches. Pc\u00adglobal optimization n, acc is also \nrun without op\u00adrelative jumps can use 2-byte immediate values timization. hti is run both with and without \nrather than 4-byte absolute addresses, thus sav\u00adenabling the supe roperator optimization. Super\u00ading space. \noperators are infe red based on a static count of how many times each tree is emitted from the 8 Experimental \nResults front end for tha [ benchmark hti+so repre\u00adsents these tests. The columns labelled ht i rep\u00ad \nht i compiles C source into object code. Ob\u00adresent the interpr ter built with exactly one VM 1I understand \nthat the latest release of the R3000 as\u00ad operator for each IR operator. sembler and linker supports \nunaligned initialized data, Figures 2 and 3 ~ ummarize the sizes of the code and that the R3000 has \ninstructions for reading unaligned segments for each benchmark. code is the to\u00ad data. Unfortunately, \nI do not have access to these new tools. tal of bytecodes, 4 unction prologues, and wasted space. waste \nis the portion wasted due to tion efficiency. alignment restrictions. interp is the size of the interpreter, \n(The sizes do not include linked sys\u00adtem library routines since all executable would use the same routines.) \nThe interpreted executable are slightly larger than the corresponding native code. The inter\u00adpreted executable \nare large for a few reasons be\u00adsides the wasteful alignment restrictions already mentioned. First, no \nchanges were made to the lCC S IR except the addition of ADDRb, and lcc creates wasteful IR nodes. For \ninstance, lcc produces a CVPU node to convert a pointer to an unsigned integer, yet this a nop on both \nthe R3000 and SPARC. Removing this node from IR trees would reduce the number of emitted byte\u00adcodes. \nAdditionally, lcc produces IR nodes that require the same code sequences on most ma\u00adchines, like pointer \nand integer addition. Dis\u00adtinguishing these nodes hampers superoperator inference, and superoperators \nsave space. Unfor\u00adtunately, much of the space taken up by executa\u00adble is for immediate values, not operator \nbyte\u00adcodes. To reduce this space would require either encoding the sizes of the immediate data in new \noperators (like ADDRb) or tagging the data with size information, which would complicate fetch\u00ading the \ndata. Fortunately, ht i produces extremely fast in\u00adterpreters. Figures 4 and 5 summarize the exe\u00adcution \ntimes for each benchmark, lcc does much better than acc relative to in\u00adterpretation because it does modest \nglobal reg\u00adister allocation, which acc and ht i do not do. lCC S code is 28.2 times faster than the inter\u00adpreted \ncode on loop because of register alloca\u00adtion. Excluding the biased loop results, inter\u00adpreted code without \nsuperoperators is less than 16 times slower than native code sometimes significantly. Furthermore, superoperators \ncon\u00adsistently increase the speed of the interpreted code by 2-3 times. These results can be improved \nwith more en\u00adgineering and better software. Support for un\u00adaligned data would make all immediate data \nreads faster. Inferring superoperators based on profile information rather than static counts would make \nthem have a greater effect on execu- If space were not a consideration, the in\u00adterpreter could be implemented \nin a directly threaded fashion to decrease operator decode time [Kli81]. The implementation of each VM \noperator is unrelated to the encoding of the oper\u00adators, so changing from the current indirect table \nlookup to threading would not be difficult. 9 Limitations and Extensions Almost certainly, each additional \nsuperoperator contributes decreasing marginal returns. I made no attempt to determine what the time and \nspace tradeoffs would be if the number of superopera\u00adtors were limited to some threshold like 10 or 20. \nI would conjecture that the returns for a given application diminish very quickly and that 20 superoperators \nrealize the bulk of the poten\u00adtial optimization. The valuable superoperators for numerically intensive \nprograms probably dif\u00adfer from those for pointer intensive programs. To create a single VM capable of \nexecuting many classes of programs efficiently, the 147 additional bytecodes could be partitioned into \nsuperopera\u00adtors targeted to different representative classes of applications. This system s effectiveness \nis limited by lCC S trees. The common G expression, x ? y, cannot be expressed as a single tree by lCC. \nTherefore, ht i cannot infer superoperators to optimize its evaluation based on the IR trees generated \nby the front end. Of course, any scheme based on looking for common tree patterns will be limited by \nthe operators in the given intermediate lan\u00adguage. ht i generates bytecodes as . dat a assembler directives \nand function prologues as assembly language instructions. Nothing about the tech\u00adniques described above \nis limited to such an im\u00adplementation. The bytecodes could have been emitted into a simple array that \nwould be im\u00admediately interpreted, much like in a traditional interpreter. This would require an additional \nbytecode to represent the prologue of a function to mimic the currently executed assembly in\u00adstructions. \nTo make this work, the system would have to resolve references within the bytecode, Benchmark burg ht \ni loop R3000 lCC code 56576 230160 48 Code Size Summary (in Translator ht i code interp waste 92448 4564 \n15895 315040 4564 51868 52 4564 4 bytes) cod~ 7261P 289511 41 hti+so interp 12388 11296 600 waste 13862 \n64299 6 Figure 2: R3000 Benchmark Code Size$ Benchmark burg ht i loop SPARC Code acc code code 75248 \n84720 271736 292568 80 56 Size Summary (in Translator ht i interp waste 4080 13560 4080 41423 4080 2 \nbytes) cod ? 6399@ 25480~ 4P hti+so interp 11840 10512 312 waste 10862 37507 4 Figure 3: SPARC Benchmark \nCode Siz~s Benchmark burg ht i loop R3000 Execution Summary Times (in seconds) Ra ios lCC ht i hti+so \nhti/lcc hti+so/ lCC 1.65 14.04 7.07 8.5 4.3 2.69 42.83 23.81 15.9 8.8 1.53 43.12 13.88 28.2 9.1 hti/hti+so \n2.0 1.8 3.1 Figure 4: R3000 Benchmark Code Speeds Benchmark burg ht i loop SPAR(2 Execution Summarv I. \nTimes (in seconds) Radios acc ht i hti+so hti/acc I hti+so~acc 1.78 18.52 8.37 10.4 4.7 4.39 58.24 28.73 \n13.3 6.5 6.62 61.26 20.05 9.3 3.0 I hti/hti+so , 1 2.2 2.0 3.1 I Figure 5: SPARC Benchmark Code Spee~s \n which would require some additional machine\u00adindependent effort. (The system linker/loader resolves references \nin the currently generated as\u00adsembler.) Not emitting function prologues of ma\u00ad chine instructions would \nmake seamless calls be\u00ad tween interpreted and compiled functions very tricky, however. 10 Related Work \nMany researchers have studied interpreters for high-level languages. Some were concerned with interpretation \nefficiency, and others with the di\u00adagnostic capabilities of interpretation. Supercombinators optimize \ncombinator-based functional-language interpreters in a way sim\u00adilar to how superoperators optimize ht \ni. Su\u00adpercombinators are combinators that encompass the functionality of many smaller combinators [FH88]. \nBy combining functionality into a sin\u00adgle combinator, the number of combinators to describe an expression \nis reduced and the num\u00adber of function applications necessary to evaluate an expression is decreased. \nThis is analogous to reducing the number of bytecodes emitted and fetched through superoperator optimization. \nPittman developed a hybrid interpreter and native code system to balance the space/time tradeoff between \nthe two techniques [Pit87]. His system provided hooks for escaping interpreted code to execute time-critical \ncode in assembly language. Programmers coded directly in both interpreted operations, or assembly. Davidson \nand Gresch developed a C inter\u00adpreter, Cint, that, like ht i, maintained C call\u00ading conventions in order \nto link with native code routines [DG87]. Cint was written entirely in C for easy retargetability. Cint \ns VM is similar to ht i s it includes a small stack-based operator set. On a set of small benchmarks \nthe interpreted code was 12,4-42.6 times slower than native code on a VAX-11/780, and 20.9-42.5 times \nslower on a Sun-3/75. Executable sizes were not compared. Kaufer, et. al., developed a diagnostic C inter\u00adpreter \nenvironment, Saber-C, that performs ap\u00adproximately 70 run-time error checks [KLP88]. Saber-C s interpreted \ncode is roughly 200 times slower than native code, which the authors at\u00adtribute to the run-time checks. \nThe interpreter implements a stack-based machine, and main\u00adtains calling conventions between native and \nin\u00adterpreted code. Unlike ht i interpreted functions have two entry points: one for being called from \nother interpreted functions, and another for na\u00adtive calls, with a machine-code prologue. Similarly, \nFeuer developed a diagnostic C in\u00adterpreter, si, for debugging and diagnostic out\u00adput [Feu85]. si s primary \ndesign goals were quick translation and flexible diagnostics time and space efficiency were not reported. \nKlint compares three ways to encode a pro\u00adgram for interpretation [Kli81]. The methods are Classical, \n Direct Threaded [Be173], and Indirect Threaded. Classical employed by ht i and Cint encodes operators \nas values such that address of the corresponding inter\u00adpreter code must be looked up in a table. Direct \nThreaded encodes operations with the addresses of the corresponding interpreter code. Indirect Threaded \nencodes operations with pointers to locations that hold the actual code addresses. Klint concludes that \nthe Classical method gives the greatest compaction because it is possible to use bytes to encode values \n(or even to use Huff\u00admann encoding) to save space. However, the Classical method requires more time for \nthe table lookup. 11 Discussion ht i translates ANSI C into tight, efficient code that includes a small \namount of native code with interpreted code. This hybrid approach allows the object files to maintain \nall C calling con\u00adventions so that they may be freely mixed with natively compiled object files. The \ninterpreted object code is approximately the same size as equivalent native code, and runs only 3-16 \ntimes slower. Much of the interpreter s speed comes from being implemented in assembly language. Re\u00adtargeting \nthe interpreter is simplified using compiler-writing tools like burg and special\u00adpurpose machine specifications. \nFor the MIPS R3000 and the SPARC, each machine required fewer than 800 lines of machine-specific code \nto be retargeted, [FHP92] Christo Superoperators, which are VM represent the aggregate functioning operations \nofmany that con- Henry, BURG nected simple operators, make the interpreted lection code both smaller \nand faster. Tests indicate su- Notices peroperators can double or triple the speed of in\u00adterpreted code. \nOnce specified by the interpreter developer, new superoperators are automatically incorporated into both \nthe translator and the in\u00ad [GG90] Ralph weld . guage. I terpreter. Furthermore, heuristics can automati\u00adcally \nisolate beneficial superoperators from static or dynamic feedback information for a specific program \nor for an entire suite of programs. [GJ79] M. R. C puters , the Th( H. Free 12 Acknowledgements [KH92] \nGerry RISC I A Chris Fraser provided useful input on this work. [Kli81] Paul 1 References niques. rience, \n[Be173] James R. Bell. rnunications of 372, June 1973. Threaded the ACM, code. 16(6) Corn\u00ad:370 [KLP88] \nStephen sha Pra based I [DG87] J. W. Davidson and A RISC interpreter gramming language. J. V. Gresch. \nCint: for the C pro-In Proceedings oj the C h 1988 U; Francis the SIGPLAN terpreters and pages 189 198, \n 87 Symposium on In-Interpretive Techniques, June 1987. [Pit87] T. Pitt: preter/r bined SI [Feu85] Alan \nR, Feuer. si an interpreter for the C language. In Proceedings the 1985 Useniz Summer Conferencej Portland, \nOR, June 1985. of Proceed posium tive Tet 1987. [FH88] Anthony J. Field and Peter son. Functional Programming. \nG. Harri-Addison [Sun91] Sun Mil Archite~ Wesley, 1988. [FH91a] Christopher W, Fraser and David R, Hanson. \nA code generation interface for ANSI C. Sofiware-Practice and Experience, 21(9) :963 988, September 1991. \n[FH9. b] Christopher W. Fraser and David R. Hanson. A retargetable compiler for ANSI C. SIGPLAN Notices, \n26(10), October 1991. )her W. Fraser, Robert R. and Todd A. Proebsting.  fast optimal instruction se\u00ad \nand tree parsing. SIGPLAN 27(4):68 76, April 1992. . Griswold and Madge T. Gris- The Icon Programming \nLan\u00ad rentice Hall, 1990. arey and D. S. Johnson. Com\u00ad nd Intractability: A Guide to ory of NP-Completeness. \nW. man and Company, 1979. .ane and Joe Heinrich. MIPS ~chitecture. Prentice Hall, 1992. lint. Interpretation \ntech- Software-Practice and Ezpe\u00ad 1(10):963-973, October 1981. Kaufer, Russell Lopez, and Se\u00ad;ap. Saber-C: \nAn interpreter\u00adrogramming environment for nguage. In Proceedings of the enix Summer Conferencej San o, \nCA, June 1988. Ian. Two-level hybrid integ\u00adrative code execution for com\u00adace-time program efficiency. \nIn ngs of the SIGPLA N 87 Sym\u00adon Interpreters and Interpre\u00adbniques, pages 150 152, June rosystems, Inc. \nThe SPARC lure Manual (Version 8), 1991.  \n\t\t\t", "proc_id": "199448", "abstract": "<p>This paper introduces <italic>superoperators</italic>, an optimization technique for bytecoded interpreters. Superoperators are virtual machine operations automatically synthesized from smaller operations to avoid costly per-operation overheads. Superoperators decrease executable size and can double or triple the speed of interpreted programs. The paper describes a simple and effective heuristic for inferring powerful superoperators from the usage patterns of simple operators.</p><p>The paper describes the design and implementation of a hybrid translator/interpreter that employs superoperators. From a specification of the superoperators (either automatically inferred or manually chosen), the system builds an efficient implementation of the virtual machine in assembly language. The system is easily retargetable and currently runs on the MIPS R3000 and the SPARC.</p>", "authors": [{"name": "Todd A. Proebsting", "author_profile_id": "81100592757", "affiliation": "Department of Computer Science, University of Arizona, Tucson, AZ", "person_id": "P283229", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/199448.199526", "year": "1995", "article_id": "199526", "conference": "POPL", "title": "Optimizing an ANSI C interpreter with superoperators", "url": "http://dl.acm.org/citation.cfm?id=199526"}