{"article_publication_date": "01-25-1995", "fulltext": "\n The Semantics of Future and Its Use in Program Optimization Cormac Flanagan* Matthias Felleisen Department \nof Computer Science Rice University Houston, Texas Abstract The future annotations of MultiLisp provide \na simple method for taming the implicit parallelism of functional programs. Past research concerning \nfutures has focused on implementation issues. In this paper, we present a se\u00adries of operational semantics \nfor an idealized functional language with futures with varying degrees of inten\u00adsionality. We develop \na set-based analysis algorithm from the most intensional semantics, and use that al\u00adgorithm to perform \ntouch optimization on programs. Experiments with the Gambit compiler indicates that this optimization \nsubstantially reduces program execu\u00adtion times. 1 Implicit Parallelism via Annotations Programs in functional \nlanguages offer numerous oppor\u00ad tunities for executing program components in parallel. In a call-by-value \nlanguage, for example, the evalua\u00ad tion of every function application could spawn a par\u00ad allel t bread \nfor each sub-expression. However, if such a strategy were applied indiscriminately, the execution of \na program would generate far too many parallel t breads. The overhead of managing these threads would \nclearly outweigh any benefits from parallel execution. The future annotations of MultiLisp [1, 12] and \nits Scheme successors provide a simple method for taming the implicit parallelism of functional programs. \nIf a pro\u00ad grammer believes that the parallel evaluation of some expression outweighs the overhead of \ncreating a sepa\u00ad rate task, he may annotate the expression with the key\u00ad word future. An annotated functional \nprogram has the same observable behavior as the original program, but the run-time system may choose \nto evaluate the future expression in parallel with the rest of the program. If it does, the evaluation \nwill proceed as if the annotated ex\u00ad pression had immediately returned. Instead of a proper *Supported \nin part by NSF grant CCR 91-22518, Texas ATP grant 91-003604014 and a sabbatical at Carnegie Mellon University. \nPermission to copy without fee all or part of this material is granted provided that the copies are not \nmade or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication \nand its date appear, and notice is given that copying is by permisAon of the Association of Computing \nMachinery. To copy otherwise, or to republish, requiras a fee and/or specific permission. POPL 951/95 \nSan Francisco CA USA @ 1995 ACM 0-89791-692-1/95/0001 ....$3.50 value t hou~h, it returns a placeholder. \nWhen a ~ro~ram operation ~equires specifi~ knowledge about the ~alue of some sub-computation but finds \na placeholder in its place, the run-time system performs a touch operation, which synchronizes the appropriate \nparallel threads. Past research on futures has almost exclusively con\u00adcentrated on the efficient implementation \nof the under\u00adlying task creation mechanism [6, 17, 23, 25, 26] and on the extension of the concept to \nfirst-class continu\u00adations [19, 27]. In contrast, the driving force behind our effort is the desire to \ndevelop a semantic frame\u00ad work and semantically well-founded optimizations for languages with future. \nThe specific example we choose to consider is the development of an algorithm for re\u00admoving provably-redundant \ntouch operations from pro\u00adgrams. Our primary results are a series of semantics for a functional language \nwith futures and a program analysis. The first semantics defines future to be a semantically-transparent \nannotation. The second one validates that a future expression interpreted as process creation is correct. \nThe last one is a low-level refine\u00adment, which explicates just enough information to per\u00admit the derivation \nof a set-based program analysis [14]. The secondary result is a touch optimization algorithm (based on \nthe analysis) with its correctness proof. The algorithm was added to the Gambit Scheme compiler [6] and \nproduced significant speedups on a standard set of benchmarks. The presentation of our results proceeds \nas follows. The second section introduces an idealized functional language with futures, together with \nits definitional, sequential semantics that interprets futures as no-ops. The third section presents \nan equivalent parallel seman\u00adtics for futures and the fourth section contains the low\u00adlevel refinement \nof that semantics. The fifth section dis\u00adcusses the cost of touch operations and presents a prov\u00adably \ncorrect algorithm for eliminating unnecessary touch operations. The latter is based on the set-based \nanaly\u00adsis algorithm of the sixth section. The seventh section presents experimental results demonstrating \nthe effec\u00adtiveness of this optimization. Section eight discusses re\u00adlated work. For more details, we \nrefer the interested reader to two technical reports on this work [9, 10]. (Terms) ~let (x V) M) (let \n(z (future M)) M) (let (x (car g)) M) (let (x (cdr y)) M) (let (x (if y M M)) M) (let (x (apply g z)) \nM) V C Value ::= ~~ ; I (Ax}M) I (cons z y) (Values) x E Vars ::= ,, :,... (Variables) c E Const ::= \n{true, false, O,1,...} (Constants) Figure 1: The A-normalized Language A. 2 A Functional Language with \nFutures 2.1 Syntax Given the goal of developing a seman\u00adtics that is useful for proving the soundness \nof opti\u00admization, we develop the definitional semantics for fu\u00adtures for an intermediate representation \nof an idealized functional language. Specifically, we use the subset of A-normal forms [11] of an extended \nA-calculus-like lan\u00adguage that includes conditionals and a future construct: see Figure 1, The language \nalso includes primitives for list manipulation, which serve to illustrate the treatment of primitive \noperations, and an unspecified set of basic constants (numbers, booleans). The key property of terms \nin A-normal form is that each intermediate value is explicitly named and that the order of execution \nfollows the lexical nesting of let\u00adexpressions. The use of A-normal forms facilitates the compile-time \nanalysis of programs [29], and it simplifies the definition of abstract machines [II]. We work with the \nusual conventions and terminology of the lambda calculus when discussing syntactic issues. In particular, \nthe substitution operation lhl[c -V] re\u00adplaces all free occurrences of z within ill by V, XO de\u00adnotes \nthe set of closed terms of type X (terms, values), and ill E P denotes that the term M occurs in the \npro\u00adgram P. Also, we use the following notations through\u00adout the paper: P denotes the power-set constructor; \nf : A + B denotes that ~ is a total function from A to B; and f : A + B denotes that f is a partial function \nfrom A to B. 2.2 Definitional Semantics The semantics of A. is a function from closed programs to results. \nA re\u00adsult is either an answer, which is a closed value with all A-expressions replaced by procedure, \nor error, in\u00addicating that some program operation was misapplied, or 1, if the program does not terminate. \nWe specify the definitional semantics of the language using a sequential abstract machine called the \nC-machine (see Figure 2), whose states are either closed terms over the run-time language AC or else \nthe special state error, and whose deterministic transition rules are the typical leftmost\u00adoutermost \nreductions of the A-calculus [8]. Each transi\u00adtion rule also specifies the error semantics of a particular \nclass of expressions. For example, the transition rule for car defines that if the argument to car is \na pair, then the transition rule extracts the first element of the pair. If the argument is not a pair, \nthen the transition rule produces the state error. The rule for future pretends that future is the iden\u00ad \n tity operation. It demands that the body of a future expression is first reduced to a value, and then \nreplaces the name for the future expression with this value. The definition of the transition function \nrelies on the notion of evaluation contexts. An evaluation context E is a term with a hole [ ] in place \nof the next sub-term to be evaluated; e.g., in the term (let (x Ml) L12), the next sub-term to be evaluated \nis Ml, and thus the definition of evaluation contexts includes (let (z S) M). A machine state is a final \nstate if it is a value or the state error. No transitions are possible from a final state, and for any \nstate that is not a final state, there is a unique transition step from that state to its succes\u00adsor \nstate. This implies that the relation eval, is a total function: Either the transition sequence for a \nprogram P terminates in a final state, in which case evalC(P) is an answer or error, or else the transition \nsequence is infinite, in which case eval. (P) = 1. Since the evalua\u00adtor evalc obviously agrees with the \nsequential semantics of the underlying functional language, future is clearly nothing but an annotation. \n3 A Parallel Operational Semantics The sequential C-machine defines future as an anno\u00adt ation, and ignores \nthe intension of future as an advi\u00adsory instruction concerning parallel evaluation. To un\u00adderstand this \nintensional aspect of future, we need a semantics of future that models the concurrent evalua\u00adtion of \nfuture expressions. 3.1 The P(C) -machine The state space of the P(C)\u00admachine is defined in the first \npart of Figure 3. The set of P(C) values includes the values of the sequen\u00adtial C-machine (constants, \nvariables, closures and pairs), which we refer to as proper values. To model futures, the P(C) -machine \nalso includes a new class of values called placeholder variables. A placeholder variable p repre\u00adsents \nthe result of a computation that is in progress. Once the computation terminates, all occurrences of \nthe placeholder are replaced by the value returned by the computation. Each C-machine state represents \na single thread of control. To model parallel threads, the P(C) -machine includes additional states of \nthe form (f-let (p S1 ) S2). The primary sub-state SI is initially the body of a fu\u00adture expression, \nand the secondary sub-state S2 is ini\u00ad tially the evaluation context surrounding the future ex\u00adpression. \nThe placeholder p represents the result of S1 in S2. The usual conventions for binding constructs like \nA and let apply to f-let. The function FP returns the set of free placeholders in a state. The evaluation \nof S1 is mandatory, since it is guaranteed to contribute to the completion of the program. The evaluation \nof S2 is speculative, since such work may not be required for the termination of the program. In particular, \nif S1 raises an error signal, then the evaluator discards the state Evaluator: evalc : A: * Answers \nU {error, -L} unloadCIV] if P *: V evdc(P) = error if P -~ error L if Vi E N 3Mi E Statec such that \nP = MO and M, -c Mt+l { Data Specifications: Unload Function: S 6 Statec ::= M I error (States) unload= \n: Value: * Answers M E A. ::= (Run-time Language) unloadc[c] = c I [et (z V) M) unbadc[(k. M)] = procedure \nI (let (x (future M)) M) unloadcl(cons VI V2)1 = (cons Al A2) .. .. ., [ (let (z (car V)) K) A, = unloadc[V,] \nI (let (z (cdr V)) M) [ (M (z(if VM M)) M) I (let (x (apply V V)) M) [ (let (z M) M) V G Valuec ::= c \nI z I (k. M) I (cons V V) (Run-time Values) A E Answers ::= c I procedure \\ (cons A A) (Answers) E G \nEvalCtzt :;= (Evaluation Contexts) I ~llet (z ~) M) I (let (z (future 8)) M) Transition Rules: &#38;[ \n(let (.zV) M) ] -. 8[ M[x + V] ] (bind) &#38;[ (let (z (future V)) M) ] w, &#38;[ M[LE i-V] ] (future-id) \n&#38;[M[z + VI] ] ifV=(cons V1V?) S[ (let (z (car V)) M) ] I-+. (car) error if V # (cons VI V2) { 8[ \n(let (z (cdr V)) M) ] M. analogous to (car) (cdr) c5[(let (z MI) M) ] if V # false ~[ (let (z (if V MI \nM2)) M) ] W. (if) &#38;[(let (zM2) M) ] ifV=false ~[ (let (.zN[~ + V2]) M) ] if V1= (Ag. N) ~[ (let (z \n(apply VI V2)) M) ] -. (apply) error if V1 # (Ay. N)[ E Figure 2: The sequential C-machine S2, and any \neffort invested in the evaluation of SZ is The transition rules (join) and (join-error) merge wasted. \nThe distinction between mandatory and specu-distinct threads of evaluation. When the primary sub\u00adlative \nsteps is crucial for ensuring a sound definition of state S1 of a parallel state (f-let (p S1 ) Sz ) \nreturns a an evaluator and is incorporated into the definition of value, then the rule (join) replaces \nall occurrences of the transition relation. the placeholder p within Sz by that value. If the pri\u00ad mary \nsub-state S1 evaluates to error, then the rule (join-error) discards the secondary sub-state Sz and re- \nTransition Rules We specify the transition relation turns error as the result of the parallel state. \nof the P(C) -machine as a quadruple. If S ++~~m S The transition rule (lift) restructures nested parallel \nholds, then the index n is the number of steps involved states, and thus exposes additional parallelism \nin certain in the transition from S to S , and the index m 5 n is cases. Consider (f-let (PZ (f-let (P1 \nS1 ) V)) S3). The the number of these steps that are mandatory. rule (lift) allows the value V to be \nreturned to the sub- The transition rules (bind), (future-id), (car), (cdr), state S3 (via a subsequent \n(join) transition), without (if) and (app/y) are simply the rules of the C-machine, having to wait on \nthe termination of S1.1 appropriately modified to allow for placeholder variables. The rules (reflexive) \nand (transitive) close the rela- An application of one of these rules counts as a manda\u00adtion under reflexivity \nand transitivity. We write S wjC tory step. The transition rule (~ork) initiates parallel evalua-S if \nS ++~~ S for some n, m G N. tion. This rule may be applied whenever the current term includes a future \nexpression within an evaluation Indeterminisrn Unlike the C-machine, in which each context, i.e.: state \nhas a unique successor state, the transition rules of the P(C) -machine denote a true relation. In partic\u00ad~(let \n(z (future N)) M) ular, the definition does not specify when the transi\u00adtion rule (fork) applies. For \nexample, given the stateThe future annotation allows the expression N to be ~[ (let (x (future N)) M) \n], the machine may proceedevaluated in parallel with the enclosing context. The either by evaluating \nN sequentially, or by creating a new machine creates a new placeholder p to represent the task via a \n(fork) transition. An implementation may result of N, and initiates parallel evaluation of N and &#38;[ \n(let (z p) M) ]. 1A second reason for the inclusion of this rule is that it is necessary The transition \nrule (parallel) permits concurrent eval-for an elegant proof of the consistency of the machine using \na modified form of the diamond lemma of the lambda calculus. uation of both sub-states of a parallel \nstate. Evaluator: evalp~ : A: ~ Answers U {error, 1} u7tloadc[V] if P -jc V eualpC(F ) = error if F \nI-+jc error  1. if Vi c N 3S, E StatePC, rz,, m, ~ N with m, > O,P = SO and S, ++~; m; S,+l { Data \nSpecifications: S c Statepc ::= M I error I (f-let (p S) S) (States) M c AP. ::= Vl(let(z V) Lf)l (As \nfor A.) V E Valuepc ::= c121(kr.M) l(consVV)lp (Run-time Values) P ~ ~~-vaTs ::= {~l,~2,P3>} (Placeholders \nVariables) Transition Rules: &#38;[(let (zV) M) ] &#38;[M[z + v-]] (bind) t?[ (let (z (future V)) M) \n] S[ M[z +-v] ] (future-id) C[ M[z +-VI] ] if V = (cons V1V2)&#38;[ (let (.z (car V)) M) ] (car) error \nif V # (cons VI V2), V #p { S[ (let (x (cdr V)) M) ] analogous to (car) (cdr) &#38;[(let (zMI) M) ] ifV#false, \nV#p S[ (let (x (if V MI M2)) M) ] (if) S[ (let (z M2) M) ] if 17= false { ~[ (let (x N[y +-V2]) M) ] \nifVI=(Ag.N)8[ (let (r (apply VI V2)) M) ] (apply) error if V1 # (Ay. N), Vl #p { S[ (let (z (future N)) \nM) ] (f-let (p N) &#38;[ M[x + p] ]) p < FP(&#38;) u FP(M) (fork) (f-let (p V) S) ,qp + v] (join) (f-let \n(p error) S) error (join-error) (f-let (PZ (f-let (pI S1) S2)) S2) (f-let (PI SI) (f-let (P2 S2) S3)) \nPI e 5(S3) (lift) (f-let (p SI) S2) (f-let (p S!) Sj) if SI ++~ib S\\, S2 WP~ Sj, n=a+c (parallel) s s \n(rejlexive) s s if S+-+~:b S1, S ++~&#38;d S , n =a+c, m= b+d, n>O (transzttve) Figure 3: The parallel \nP(C) -machine consequently choose to ignore future expressions (along the lines of the C-machine), which \nyields a sequential ex\u00adecution, to execute fork as early as possible, which yields an eager task creation \nmechanism [23, 31], or to choose some strategy in between the extremes, which yields lazy task creation \n[6, 26] A second source of indeterminism is the transition rule (parallel). This rule does not specify \nthe number of steps that parallel sub-states must perform before they synchronize. An implementation \nof the machine can use almost any scheduling strategy for allocating processors to tasks, as long as \nit regularly schedules the mandatory thread. Evaluation In general, the evaluation of a program can proceed \nin many different directions. Some of these transition sequences may be infinite, even if the program \nterminates according to the sequential semantics. Con\u00adsider: P = (let (x (future error)) Q) where Q \nis some diverging sequential term, i.e., O -~j !2. The sequential evaluator never executes 0 because \nP s result is error, In contrast, P admits the following infinite parallel transition sequence: This \n(evaluation diverges because it exclusively con\u00adsists of speculative transition steps and does not include \nany mandatory transition steps that contribute to the sequential evaluation of the program. The evaluator \nfor the P(C) -machine excludes these excessively spec\u00adulative transition sequences, and only considers \ntransi\u00adtion sequences that regularly includes mandatory steps. For a terminating transition sequence, \nthe number of speculative steps performed is implicitly bounded. For non-terminating sequences, the definition \nof the evalu\u00adator explicitly requires the performance of mandatory transition steps on a regular basis. \nThis constraint im\u00adplies that an implementation of the machine must keep track of the mandatory thread \nand must ensure that this mandatory thread is regularly executed. P -if (f-let (p error) Q) via (fork) \n+-+~j (f-let (p error) Q) since Q +--+j~ Q _l,o pc . . . 3.2 Correctness The observable behavior of \nthe P(C)\u00admachine on a given program is deterministic, despite its indeterminate internal behavior. Theorem \n3.1 evalPC is a junction. We prove this consistency using a modified form of the traditional diamond \nlemma. The modified diamond lemma states that if we reduce an initial state S1 by two alternative transitions, \nproducing respectively states S2 and S3, then there is some state S4 that is reachable from both S2 and \nS3. Furthermore, the number of mandatory steps on the transition from S1 to S4 via Sz is bounded by the \ntotal number of steps on the transition from S1 to S4 via Sx, and vice-versa. This bound is necessary \nto prove that all transition sequences for a given program exhibit the same termination behavior. Since \neach sequential transition rule of the P(C)\u00admachine subsumes the corresponding transition rule of the \nC-machine, every transition of the C-machine is also a transition of the P(C) -machine, which implies \nthat the evaluators are equivalent. Theorem 3.2 evalPC = evalC Put differently, the P(C) -machine is \na correct im\u00adplementation of the C-machine in that both define the same semantics for the source language. \nHence, the in\u00adterpretation of future as a task creation construct, with implicit task coordination, is \nentirely consistent with the definitional semantics of future as an annotation. 4 A Low-Level Operational \nSemantics Since optimization heavily rely on static information about the values that variables can assume, \nthe P(C)\u00admachine is ill-suited for correctness proofs of appropri\u00adate analysis algorithms.z On one hand, \nthe states of the P(C)-machine contain no binding information relating program variables and values. \nInstead, the machine re\u00adlies on substitution for making progress. On the other hand, the representation \nof run-time values and other objects in the P(C) -machine is too coarse. For example, it does not permit \na detailed view of the synchronization operations that are required for coordinating futures. To address \nthese problems, we refine the P(C) -machine to the P(CE1[)-machine (see Figure 4) using standard techniques \n[8, 11]. 4.1 The P(CEIJ)-machine An evaluation context, which represents the control stack, is now represented \nas a sequence of activation records (which are similar to closures). A tagged activation record ((art \nz, M, E)) represents a point where the continuation can be split into separate tasks (crop. fork). The \nsubstitution opera\u00adtion is replaced by an environment in the usual manner. An environment E is a mapping \nfrom variables to run\u00adtime values. The empty environment is denoted by 0, and the operation E[x + V] \nextends the environment E to map the variable o to the value V. During the course of the refinement, \nwe also replace each placeholder p with an explicit undetermined place\u00adholder object (ph p o). The symbol \no indicates that the result of the associated computation is unknown. When the associated computation \nterminates, producing a value V, then the undetermined placeholder object is replaced by the determined \nplaceholder object (ph p V). This change of representation explicates touch opera\u00adtions in the form of \nside-conditions on the appropriate transition rules. The conditions state that an undeter\u00admined placeholder \nobject ((ph p o)) must have been re\u00adplaced by a determined placeholder object ((ph p V)) The machine \nis also far too abstract for the derivation of an implementation. This problem is also addressed by the \nfollowing development. before the program operation can take place. The con\u00additions precisely identify \nthe positions of car, cdr, if and apply that demand proper values and also show that operations like \ncons or the second position of ap\u00adply do not need to know anything about the values they process. The \ntransition relation HPC,~ reformulation of the relation +-+PC that takes into account the change of state \nrepresentation. We write S -~Cek S if S ++~;~k S for somen,m cN 4.2 Correctness The correctness proof \nfor the new machine involves two steps. The first step constructs an intermediate semantics by introducing \nplaceholder ob\u00adjects into the P(C) -machine. The second step proves the correctness of the P(CEl<)-machine \nwith respect to the intermediate semantics using standard proof tech\u00adniques [8], appropriately modified \nto account for parallel evaluation. Theorem 4.1 evalPC.~ = evalP~ 5 Touch Optimization The F (CE1[)-machine \nperforms touch operations on ar\u00adguments in placeholder-strict positions of all program operations. These \nimplicit touch operations guarantee the transparency of placeholders, which makes future\u00adbased parallelism \nso convenient to use. Unfortunately, these compiler-inserted touch operations impose a signif\u00adicant overhead \non the execution of annotated programs. For example, an annotated doubly-recursive version of jib performs \n1.2 million touch operations during the com\u00adputation of (fib 25). Due to the dynamic typing of Scheme, \nthe cost of each touch operation depends on the program operation that invoked it. If a program operation \nalready performs a type dispatch to ensure that its arguments have the ap\u00adpropriate type, e.g., car, \ncdr, apply, etc, then a touch operation is free. Put differently, an implementation of (car x) in pseudo-code \nis: (if (pair? x) (unchecked-car z) (error car Not a pair )) Extending the semantics of car to perform \na touch op\u00aderation on placeholders is simple: (if (pair? x) (unchecked-car x) (let ([y (touch x)]) (if \n(pair? y) (unchecked-car y) (error car Not a pair )))) The touching version of car incurs an additional \nover\u00adhead only in the error case or when x is a placeholder. For the interesting case when x is a pair, \nno overhead is incurred. Since the vast majority of Scheme operations already perform a type-dispatch \non their arguments,3 the overhead of performing implicit touch operations ap\u00adpears to be acceptable at \nfirst glance. 3Tw0 notable exceptions are if, which does not perform a type\u00ad disDatcb on the value of \nthe test expression. and the eaualit~ predicate ~q~, ~hich ifj typi~~lly implemented as a p ointer comparison. \nEvaluator: e~a~pc+ : A: evalPce~(P) + = Answers U {error, 1} { unload pc,, k [E(z)] error L if if if \n(P, 0, 6) -+jce~ (P, 0, 6) *jcek Vi E N 3S, 6 (2, E, 6) error StatePCek, n,, m, G N with m, > 0, So = \n(P, 0, e) and S, +.+n~ImZ~cek ,9,+1 Data Specifications: !$ ~ .!htepcek ;;= (M, E, K) I error/ (f-let \n(p S) S) (States) Mc Aa (A-nf Language) E E En7JPCek ::= Vars * ValuePCek (Environments) V e ValUepcek \n::= Pvaluepcek I Ph-ObjPCek (Run-Time Values) W C Pvaluepcek l:= c I I I Clpcek I pairpcek (Proper Values) \nClpcek :;= (( Xr. M), E) (Closures) ? U~TPcek ::= (cons V V) (Pairs) Ph-Ob~Pcek ::= (ph P O) / (ph P \nV) (Placeholder Objects) E I (ar z, M, E).K [ (art z, M, E).K K 6 ContpCek ;:= (Continuations) Auxiliary \nFunctions: touchpcek : ValuePcek a Pvahiepc.k U {o} UnlOadPCek ; Valuezcek * Answers touch o,-,~[(ph \nII 0)1 = o unload p.ik[cl = c t;;chp~j~[(ph P V)j = touchpcek [V]unload PC.k[((~Z ~), ~)] = procedure \n t@JChPd [W] = W n~oadpcek[(cons VI vz)l = (cons un~~ad cek[~] ~~~oadpcek[v,]) unload~..k[(Ph P v)] \n= ~n~oad,.d [v~ Transition Rules: ((let (.z (cons g 2)) M), E, I() I---+~~ek (M, E[z -(cOns .E(Y) E(z))], \nK) (bind-cons) transition rules for (let (z c) AI), (let (z y) M) and (let (z (Av. N)) M) are similar \nto (bind-cons) (z, E, (ar y, M, E ).K) (M;E/[~ J E(;)], K)   (return) (M, E[z + VI], K) if touchpcek[E(y)] \n= (cons VI V2) ((let (r (car g)) M), E, K) (car) error { f ouchpcek [E(Y)] @ Pat? peek U {o} ((let (x \n(cdr y)) M), E, K) analogous to (car) (cdr) (Ml, E, (ar x, M, .E).K) if touchPCek[E(y)] @ {false, o} \n((let (x (if y Ml M2)) M), E, If) (if) (M2, E, (ar z, M, E).K) if touch .e,k[~(y)] = false (N, .E [x \n+ E(z)], (ar z, M, .E).K~ if tO UChpcek[E(~)] = ((Az . N), E ) ((let (z (apply y z)) M), E, K) (apply) \n[ tOUChpcek [WV)] t? c~pcek u {o} error if ((let (z (future N)) M), E, K) (N, E, (art x, M, E).K) (future) \n(z, E, (art y, M, E ).K) (M, E [y + E(x)], K) (future-id) (M, E, A I.(art x, N, E ).A_2) (f-let (p (M, \nE, KI)) (N, .V[z + (ph p o)], 1{2)) p @ FP(E ) U F P(K2) (fork) (f-let (p (z, E, e)) S) I.-+:::h S+:= \nE(x)] (join) transition rules (~oin-err;;j; ( lift), (parallel), (refZezive) and (transitive) are as \nfor the P(C) -machine Figure 4: The P(CE1{)-machine Unfortunately, a standard technique for increasing \nexecution speed in Scheme systems is to disable type\u00adchecking typically based on informal correctness \nargu\u00adments or based on type verifiers for the underlying se\u00adquential language [33]. When type-checking \nis disabled, most program operations do not perform a type-dispatch on their arguments. Under these circumstances, \nthe source code (car z) translates to the pseudo-code: (unchecked-car z) Extending the semantics of car \nto perform a touch op\u00aderation on placeholders is now quite expensive, since it then performs an additional \ncheck on every invocation: (if (placeholder? z) (unchecked-car (touch x)) (unchecked-car z)) Performing \nthese placeholder? checks can add a signif\u00adicant overhead to the execution time. Kranz [22] and Feeley \n[6] estimated this cost at nearly 100% of the (se\u00adquential) execution time, and our experiments confirm \nthese results (see below). The classical solution for avoiding this overhead is to provide a compiler \nswitch that disables the automatic insertion of touches, and a touch primitive so that pro\u00adgrammers can \ninsert touch operations explicitly where needed [6, 20, 31]. We believe that this solution is flawed \nfor several reasons. First, it clearly destroys the trans\u00adparent character of future annot at ions. Instead \nof an annotation that only affects executions on some ma\u00adchines, future is now a task creation construct \nand touch is a synchronization tool. Second, to use this solution safely, the programmer must know where \nplaceholders can appear instead of regular values and must add touch operations at these places in the \nprogram. In contrast to the addition of future annotations, the placement of touch operations is far \nmore difficult: while the for\u00admer requires a prediction concerning computational in\u00adtensity, the latter \ndemands a full understanding of the data flow properties of the program. Since we believe that an accurate \nprediction of data flow by the program\u00admer is only possible for small programs, we reject this ((let \n(z (SFE Y)) W, E, K) -:~k (M, E[z + u], K) if E(y)= (cons VI Vz) unspecified if E(g) G Ph-ObjPCek { error \notherwise ((let (z (~ y)) M), E, K) 1,1 peek analogous to w ((let (x (M Y JfI M2)) ~), E, @ W~~ek (I/z, \nE, (ar z, M, E).1() if E(Y) = false unspecified if ~(y) 6 Ph-ObjPcek (MI, E, (ar Z, M, E) .10 otherwise \n{ ((let (.z (apply y z)) M), E, II) =1 1 peek --@, E [z + E(2)], if E(y) = ((Az . N), E ) (ar z, M, E).1() \nunspecified if E(y) G Ph-Ob~ncek \\ error otherwise [ Figure 5: Non-touching transition rules traditional \nsolution. A better approach than explicit touches is for the compiler to use information provided by \na data-fiow anal\u00adysis of the program to remove unnecessary touches wher\u00adever possible. This approach \nsubstantially reduces the overhead of touch operations without sacrificing the sim\u00adplicity or transparency \nof future annotations. 5.1 Non-touching Primitives The current language does not provide primitives that \ndo not touch arguments in placeholder-strict positions. To express and verify an algorithm that replaces \ntouching primitives by non\u00adtouching primitives, we extend the language A. with non-touching forms of \nthe placeholder-strict primitive operations, denoted w, a, M and apply, respec\u00adtively: M ::= (let (z \n(w Y)) W I (let (z (a y)) M) [ (let (x (iJ y M M)) M) I (let (z (apply y 2)) M) As their name indicates, \na non-touching operation be\u00adhaves in the same manner as the original version as long as its argument \nin the placeholder-strict position is not a placeholder. If the argument is a place-holder, the behavior \nof the non-touching variant is undefined. The extended language is called ~. We define the semantics \nof the extended language A. by extending the P(CE1{)-machine with the addition~ transition rules described \nin Figure 5. The evaluator for the extended language, evalPcek, is defined in the usual way (crop. Figure \n4). Unlike evalPc,~, the evaluator eval~,+ is no longer a function. There are programs in . .... ~ for \nwhich the ;valuator eVa~pcek can either return a value or can be uns~ecified because of the amiication \nof a non-touching o~eration to a placeholder.-Still, the two evaluators clearly agree on programs in \nA.. Lemma 5.1 For P c A., eValpc.k(P) = eValPcA(P). 5.2 The Touch Optimization Algorithm The goal of \ntouch optimization is to replace the touching opera\u00adtions car, cdr, if and apply by the corresponding \nnon\u00adtouching operation whenever possible, without changing the semantics of programs. For example, suppose \nthat a program contains (let (z (car y)) Al) and we can prove that y is never bound to a placeholder. \nThen we can replace the expression by the form (let (Z (w Y)) ~), which the machine can execute more \nefficiently without performing a test for placeholdership on y. This optimization technique relies on \na detailed data\u00adflow analysis of the program that determines a conserva\u00adtive approximation to the set \nof run-time values for each variable. More specifically, we assume that the analysis returns a valid \nset environment, which is a table map\u00adping program variables to a set of run-time values4 that subsumes \nthe set of values associated with that variable during an execution. Definition 5.2. (Set environments, \nvalidity) Let P be a program and let Varsp be the set of variables occurring in P. A mapping &#38; : \nVarsp + P( Vahepc.k) is a set en\u00advironment. A set environment &#38; is valid for P if S + &#38; holds \nfor every S such that (P, 0, 6) _jCek S. The relation S 1= &#38; holds if every environment in S maps \nevery variable z to a value in &#38;(z). The basic idea behind touch optimization is now easy to explain. \nIf a valid set environment shows that the ar\u00adgument of a touching version of car, cdr, if or apply can \nnever be a placeholder, the optimization algorithm re\u00adplaces the operation with its non-touching version. \nThe optimization algorithm 7 is defined in Figure 6. The function sba, described in the next section, \nal\u00adways ret urns a valid set environment for a program. Assuming the correctness of of set-based analysis, \nthe touch optimization algorithm preserves the meaning of programs, since each transit ion step of a \nsource program P corresponds to a transition step of the optimized pro\u00adgram. Theorem 5.3 (Correctness \nof touch-optimization) For P E A:, evalPC,~(P) = evalP~.~(Tsba[P1 [P]). Any implementation that realizes \neValPc&#38; correctly can therefore make use of our optimization technique. 6 Set-Based Analysis for \nFutures We develop the analysis that produces valid set environ\u00adments in two steps. First, we use the \ntransition rules of the P(CEl{)-machine to derive constraints on the sets of run-time values that variables \nin a program may as\u00adsume. Any set environment satisfying these constraints 40r a least a representation \nof this set that provides the appropriate information. T&#38;:Aa-+Aa T&#38;[x] = : Tf[(let (r c) hf)] \n= (let (z c) T~[M]) 7t[(let (z y) M)] = (let (z g) T2[M]) 7~[(let (z (Ay. N)) M)] = (let (z (Ay. Tt(N))) \nTz[kf]) Tt[(let (z (cons y z)) M)] = (let (x (cons y z)) 7t[Af]) T&#38;[(let (x (future N)) M)] = (let \n(z (future 7~[N])) Tz[M]) Tz[(let (z (car y)) M)] = (let (~ (~ y)) 7&#38;[~I]) if ~(y) q Pvaluepcek { \n(let (z (car y)) 7~[M]) if t(y) ~ PValuePcek T,[(let (z (cdr y)) M)] = analogous to car T&#38;[(let. \n(z (if Y MI MZ)) M)] = (let (X (H Y kfl kf2)) T&#38;[A f]) if S(v) ~ PvUIU.Pc-k (kt (Z (if Y Ml M2)) \n7C[M]) if S(!/) ~ ~VUh.Peek { Tt[(let (z (apply y z)) M)] = (let (Z (apply g 2)) ~~[ftf]) if ~(y) ~ pvah.~cek \n{ (let (2(= y ~)) Tt[Lf]) if S(y) ~ Pvah.peek Figure 6: The touch optimization algorithm T is a valid \nset environment. Second, we develop an algo\u00adrithm for finding the minimal set environment satisfying \nthese constraints. The constraints we produce are simi\u00adlar to those in Heintze s work on set based analysis \nfor ML [14], though our derivation of these constraints dif\u00adfers substantially. 6.1 Deriving Set Constraints \nWe derive constraints on valid set environments by analyzing the transition rules of the machine. Each \nconstraint we produce is of the form: A E where .4 and B are statements concerning &#38;, and A also \ndepends on the program being analyzed. A set environ\u00adment E satisfies this constraint if whenever A holds \nfor ~, then B also holds for &#38;. Let P be the program of interest, and suppose that the evaluation \nof P involves the transition S -~;~~ St, where S 1= &#38;. We derive constraints on &#38; sufficient \nto ensure that S ~ L$by case analysis on the last transition rule used for S =~;~~ S . We present three \nrepresentative cases: E Suppose s +--+;;;k s Via the rule ( bind-const): S = ((let (z c) M), E, 1{) 11 \nt+p:ek S = (M, E[z +-c], ~<) This rule binds z to the constant c, where the term (let (z c) M) occurs \nin P. To ensure that the set environment g includes c as one of the possible val\u00adues of x, we demand \nthat &#38; satisfy the constraint: (let (z c) M) CP (c:) c E :(x) E Suppose s +fek S via the rule (apply). \nIn the interesting case, y is bound, either directly or via a placeholder, to a closure ((M. N), E ): \nS = ((let (.z (apply y :)) M), E, 1{) H~#&#38; S = (N, E [z + E(z)], (ar x, M, E), A_) Then this rule \nbinds # to E(;). To ensure that S accounts for this binding, we demand that &#38; satisfy the constraint: \n(le~)g (a#y gl;)~~/ ~ P touchpc,k[v] = ((L . ~j, E) (c;) v, E :(X ) E Suppose S -~~ek S via the rule \n(return): S = (x, E , (ar y, M, E).1{) 1,1 -PC,~ S = (Al, E[y +-l? (z)], K) We need to ensure that E \nincludes E (x) as a pos\u00adsible value of y. However, when analyzing a re\u00adturn instruction x, we have no \ninformation re\u00adgarding the possible activation records that may receive the value of x during an execution. \nIn cent rast, when analyzing an application expression (let (y (apply ~ z)) M), we know both the calling \ncontext and, from ~(~), the set of closures that can possibly be invoked. Furthermore, if ((M. N), E) \nis the closure being invoked, then the result of the application will be the current binding of the vari\u00adable \nFinalVar [N], where FinalVar is the following function from expressions to variables: FinalVar : A.+ \nVars FinalVar[z] = r FinalVar[(let (z V) M)] = FinalVar[M] Fins/Var[(let (.r (future N)) M)] = Final \nVar[M] ... ... To ensure that E accounts for the binding of x to the value of FinalVar[N] for all closures \nthat may be invoked, we demand that E satisfy the constraint (let (.x (app$~;~~)lbl) C P touchPCek[V] \n= ((k . N), E) V~ ~ &#38;( F inalVar[N]) (c{) VN E 8(X) Examining each of the transition rules of the \nmachine in a similar manner results in eleven program-based set constraints C~, . . . . CL (see Figure \n7) sufficient to en\u00adsure that a set environment is valid. Put differently, if a set environment &#38; \nsatisfies the set constraints for a program P, it is easy to prove, using induction on the length of \nthe transition sequence, that ~ is valid for P. Theorem 6.1 (Soundness of Constraints) If&#38; sat\u00adisjies \nC;, ....C~~, then E is valid for P. (let (.r c) M) GF (q (c:) (let (r (Ag. N)) M) c P Vx e do?n(lq. \nE(r) e C(T) (q (( Ay. N), E) c t(T) (c:) (let (.z (car y)) M) < P l; G :(y) tOUChPCek [t ] = (cons Ii \n16) (c:) VI G t(x) (let (I (cdr ~)) M) cP 1 ct(y) touchpce-[l ] = (cons Vi W) (c;) VZ G t(r) (let (2 \n(apply y 2)) M) GP v E f(g) touchpcek[V] = ((XT . N), E) T > G t(:) (c:) 1 > E &#38;(Z ) (c:) (let (x \n(if yMI M2)) M) 6P V c S(FinalVar[Ml ] ) U Z(FZ7mtVar[M2]) (G ) 1 E s(z) (let (z (future N)) M) C P \nV G &#38;( FinalVar[N]) (CL) l; G t(x) (ph p V) GE(z) (let (x (future N)) M) G P (CL) (Ph rI o) ~ &#38;(z) \nFigure 7: Set Constraints on ~ with respect to P. 6.2 Solving Set Constraints The class of set en\u00advironments \nfor a given program P, denoted SetEnvp, forms a complete lattice under the natural pointwise partial \nordering. Smaller set environments correspond to more accurate approximations, because they include include \nfewer extraneous bindings. We define set-based analysis as the function that returns the least set envi\u00adronment \nsatisfying the set constraints. Definition 6.2. (sba) sba(P) = TI{S I ~ satisfies C~, ..., c ~ } I Since \nsba(P) maps variables to infinite sets of possi\u00adble values, we need to find a suitable finite representa\u00adtion \nfor these infinite sets. A systematic inspection of the set constraints suggests that the set of closures \nfor a A-expression can be represented by the A-expression itself, that the set of pairs for a cons-expression \ncan be represented by the cons-expression, etc. The actual sets of run-time values can easily be reconstructed \nfrom the representative terms and the set environment. In short, we can take the set of abstract values \nfor a program P to be: ~ E Abs Valaep ::= CP I (AZ. JZI)IJ [ (cons 2 Y)P I (ph XP) I (ph O) (let (r c) \nM) CP (co Cp e z(x) (q) (w (let (x (cons yI y2)) M) C P Z(yt) #o)i= 1,2  (c:) (cons g, y2)p c z(z) (~) \n (%47 (let (.T (apply y z)) M) G P v e z(y) (Ax . N)p e touch [&#38;, v] 7. c Z(z) (q 7. c Z(z ) (w \n(w (let (z (future IV)) M) G P V G Z(FtnalVar[N]) (~) i7cqx) (ph FinalVar[N]p) c ~(z) (let (x (future \nN)) M) 6 P (m) (ph O) C ~(.c) Auxiliary Function touch: touch : AbsEnvp x Abs Valuep -+ P(Abs ValueP) \ntouch (~, cpl = {CP} touch [~, (k~M)P~ = {(k. M)P} touch [~, (cons z y)p = {(cons .r y)p} touch ~, \n(ph ~P) = 1 {~ [~ <~(y) and ~ c touch [&#38;, ~] } Figure 8: Abstract Constraints on ~ with respect \nto P. where the constant CP, the A-expression (Ax. M)p, the pair (cons z y) p and the variable XP are \nall the respec\u00adtive subterms of P. The size of this set is 0( 1P]), where IP[ is the length of P. Abstract \nvalues provide finite representations for the infinite set environments encountered during set-~ased \nanalysis. Specifically, an abstract set environment f is a mapping from variables in P to sets of abstract \nvalues. The class of all abstract set environments for a program P is denoted AbsEnv p. Each abstract \nset environment represents a particular set environment according to the following function: F : A bsEnv \np--+ SetEnv P Reformulating the set constraints from Figure 7 for abstract set environments produces \nthe abstract con\u00ad straints Clp, . . . . CL in Figure 8. We define =(P) be the least abstract set environment \nsatisfying the abstract constraints with respect to P. Definition 6.3. (~) sba(P) = n{s / : satisfies \n~, ..., CL}  t The correspondence between set constraints and abstract constraints implies that sba(P) \nis a finite representation for sba(P). Theorem 6.4 sba(P) = 7 (Z(P)) Since AbsEnv p is finite, of size \n0( IP12 ), we can cal\u00ad culate sba(P) in an iterative manner, starting with the empty abstract set environment \n~(z) = 0. Since we can extend ~ at most 0( [F 12) times, this algorithm termi\u00ad nates. Furthermore, each \ntime we extend ~ with a new binding, calculating the additional bindings implied by that new binding \ntakes at most O ( IPI ) time. Hence, the algorithm runs in 0( IP13 ) time. Optimization a~orithms can \ninterpret the abstract set environment sba(P) in a straightforward manner. For example, the query on \nsba(P) from the touch op\u00adtimization algorithm: sba(P)(y) ~ PVakep~ek is equivalent to the following \nquery on sba(P): Z(P)(Y) ~ {cp, (k, LI)p, (cons o y)p} In a similar manner other queries on sba(P) can \neasily be reformulated in terms of ~(P). Experimental Results We extended the Gambit compiler [6, 7], \nwhich makes no attempt to remove touch operations from programs, with a preprocessor that implements \nthe set-based anal\u00adysis algorithm and the touch optimization algorithm. The analysis and the optimization \nalgorithm are as de\u00adscribed in the previous sections extended to a sufficiently Program Description fib \nComputes the 25*h fibonacci number using a doubly-recursive algorithm. queens Computes the number of \nsolutions to the n-queens problem, for n = 10. rantree Traverses a binary tree with 32768 nodes. mm Multiplies \ntwo 50 by 50 matrices of integers. scan Computes the parallel prefix sum of a vector of 32768 integers. \nsum Uses a chvide-and-conquer algorithm to sum a vector of 32768 integers. tri.diag Solves a tridiagonal \nsystem of 32767 equations. allpairs Computes the shortest path between all pairs in a 117 node graph \nusing Floyd s algorithm. abisort Sorts 16384 integers using adaptive bitonic sort. mst Computes the minimum \nspanning tree of a 1000 node graph. qsort Uses a parallel Quicksort algorithm to sort 1000 integers. \npoly Computes the square of a 200 term polynomial, and evaluates the result at a given value of z. Figure \n9: Description of the benchmark programs large subset of functional Scheme.5 We used the ex\u00adtended Gambit \ncompiler to test the effectiveness of touch optimization on the suite of benchmarks contained in Feeley \ns Ph.D. thesis [6] on a GP 1000 shared-memory multiprocessor [2]. Figure 9 describes these benchmarks. \nEach benchmark was tested on the original compiler (standard) and on the modified compiler (touch opti\u00admized). \nThe results of the test runs are documented in Figure 10. The first two columns present the num\u00adber of \ntouch operations performed during the execution of a benchmark using the standard compiler (column 1), \nand the sequential execution overhead of these touch operations (column 2). To determine the absolute \nover\u00adhead of touch, we also ran the programs on a single processor after removing all touch operations. \nThe next two columns contain the corresponding measurements for the touch optimizing compiler. The touch \noptimiza\u00adtion algorithm reduces the number of touch operations to a small fraction of the original number \n(column 3), thus reducing the average overhead of touch operations from approximately 90% to less than \n10% (column 4). The last three columns show the relative speedup of each benchmark for one, four, and \n16 processor config\u00adurations, respectively. The number compares the run\u00adning time of the benchmarks using \nthe standard compiler with the optimizing compiler. As expected, the relative speedup decreases as the \nnumber of processors increases, because the execution time is then dominated by other factors, such as \nmemory contention and communication costs. For most benchmarks, the benefit of our touch optimization \nis still substantial, producing an average speedup over the standard compiler of 37% on four pro\u00adcessors, \nand of 20 %0on 16 processors. The exceptions are the last three benchmarks, mst, qsort, and poly. How\u00ad \n5Five of the benchmark include a small number (one or two per benchmark) of explicit touch operations \nfor coordinating side-effects. They do not affect the validity of the analysis and touch optimization \nalgorithms. St ~dard ch optimized Benchmark touch touch speedl over dam ~ n=l n=4TE F e fib 85.0 10.2 \n40.5 39.9 36.7 queens 2116 41.2 35 1.5 28.1 30.4 28.1 rantree 327 67.5 14 2.6 38.7 37.2 26.8 nml 1828 \n121.0 3 <1 54.7 44.1 23.6 scan 1278 126.8 66 4.1 54.1 43.4 19.0 sum 525 107.3 33 6.1 48.8 37.9 20.0 tridiag \n811 110.8 7 <1 52.1 29.4 5.8 allpairs 32360 150.4 14 <1 60.0 39.6 <1 abisort 5751 106.5 9 <1 51.3 31.1 \n24.4 lnst 20422 91.4 750 5.3 45.0 17.2 <1 qsort 253 43.3 78 19.9 16.4 <1 <1 poly 526 65.3 121 16.2 29.7 \n12.5 <1 Figure 10: Benchmark Results ever, even Feeley [6] described these as poorly parallel programs, \nin which the effects of memory contention and communication costs are especially visible. It is there\u00adfore \nnot surprising that our optimizing compiler does not improve the running time in these cases. Related \nWork The literature on programming ber of semantics for parallel, only one that directly deals transparent \nannotations is Moreau studies the functional with pcall (for evaluating pressions of an application languages \ncontains a num\u00ad Scheme-like languages. The with parallelism based on Moreau s Ph.D. thesis [27]. core \nof Scheme extended function and argument ex\u00adin parallel) and first-class continuations. His primary \ngoal is to design a semantics for the language that treats pcall as a pure annotation. His correctness \nproofs is far more complicated than our techniques, due to the inclusion of continuations. Independently, \nReppy [28] and Leroy [24] define a formal operational semantics for an ML-like language with first-class \nsynchronization operations. Reppy s lan\u00adguage, Concurrent ML, can provide the future mecha\u00adnism as an \nabstraction over the given primitives. The semantics is a two-level rewriting system. Reppy uses his \nsemantics to prove a type soundness theorem. Leroy formulates a semantics for a subset of CML in the \ntradi\u00adtional natural semantics framework. He also uses his semantics to prove the type soundness of the \ncomplete language. Neither Reppy nor Leroy used their semantics for developing analyses or optimizations. \nJaganathan and Weeks [18] define an operational se\u00admantics for a simple function language extended with \nthe spawn construct by extending Deutsch s transition semantics [5]. They described an analysis for their \nlan\u00adguage that they intend to use in a forthcoming compiler, but they do not have an implementation of \ntheir analysis for a full language like functional Scheme, and they do not have optimization algorithms \nthat exploit the results of their analysis. Wand [32] recently extended his work on correctness proofs \nfor sequential compilers to parallel languages. In his prior work on the correctness of sequential compil\u00aders, \nhe derived compilers from the semantic mappings that translate syntax into A-calculus expressions. The \nextension of this work to parallel compilers starts from a semantic mapping that translates a Scheme-like \nlan\u00adguage with process creation and communication con\u00adstructs into a higher-order calculus of communication \nand computation. After separating the compiler from the machine , the correctness proof is a combination \nof the sequential correctness proof and a correctness proof for the parallel portion of the language. \nThe proof tech\u00adniques are related to the ones we used to prove the equiv\u00adalence of the P(CEK)-machine \nand the P(C) -machine. Kranz et al. [23] briefly describe a simplistic algo\u00adrithm for touch optimization \nbased on a first-order type analysis. The algorithm lowers the touch overhead to 65% from 100% in standard \nbenchmarks, that is, it is significantly less effective than our touch optimization. The paper does not \naddress the semantics of future or the well-foundedness of the optimizations. Knopp [21] reports the \nexistence of a touch optimization algorithm based on abstract interpretation. His paper presents nei\u00adther \na semantics nor the abstract interpretation. He only reports the reduction of static counts of touch \nop\u00aderations for an implementation of Common Lisp with future. Neither paper gives an indication concerning \nthe expense of the analysis algorithms. Our analysis methods most closely follows Heintze s work on set-based \nanalysis for the sequential language ML [13, 14], but the extension of his technique to paral\u00ad lel languages \nrequires a substantial reformulation of the derivation and correctness proof. Specifically, Heintze uses \nthe natural semantics framework to define a set\u00adbased natural semantics, from which he reads off safe\u00adness \nconditions on set environments. He then presents set constraints whose solution is the minimal safe set \nenvironment. We start from a parallel abstract machine and avoid these intermediate steps by deriving \nour set constraints and proving their correctness directly from the abstract machine semantics. Other \ntechniques for static analysis of sequential pro\u00adgrams include abstract interpretation [3, 4] and Shivers \nOCFA [30]. The relationship between abstract interpre\u00adtation and set-based analysis was covered by Heintze \n[13]. Sequential optimization techniques such as tagging [9] FLANAGAN, C., AND FELLEISEN, M. The semantics \nof Future. Rice University Comp. Sci. TR94-238. optimization [15] and soft-typing [33] are similar in \nchar\u00ad acter to touch optimization. Both techniques remove [10] FLANAGAN, C., AND I? ELLEISEN, M. Well-founded \ntouch optimization of Parallel Scheme. Rice University the type-dispatches required for dynamic type-checking \nComp. Sci, TR94-239. wherever possible, without changing the behavior of pro\u00ad [11]FLANAGAN, C,, SABRY, \nA., DUBA, B. F., AND grams, in the same fashion as we remove touch opera-FELLEISEN, M. The essence of \ncompiling with continu\u00ad tions. However, the analyses relies on conventional type ations, In PLDI (1993), \n237 247. inference techniques. [12] HALSTEAD, R. Multilisp: A language for concurrent symbolic computation. \nACM Transactions on Pro\u00adgrammmg Languages and Systems 7, 4 (1985), 501 538. Conclusion [13] HEINTZE, \nN. Set Based Program Analysts. PhD thesis, Carnegie Mellon University, 1992. The development of a semantics \nfor futures directly [14] HEINTZE, N. Set-based analysis of ML programs. In leads to the derivation of \na powerful program analy-LFP (1994), 306-317. sis. The analysis is computationally inexpensive but [15] \nHENGLEIN, F. Global tagging optimization by type in\u00ad yields enough information to eliminate numerous \nim\u00ad ference. In LFP (1992), 205-215. plicit touch operations. We believe that the construc\u00ad [16] ITO, \nT., AND HALSTEAD, R., Eds, Parallel Lisp: Lan\u00ad tion of this simple touch optimization algorithm clearly \nguages and Systems. Springer-Verlag Lecture Notes in illustrates how semantics can contribute to the \ndevel- Computer Science 441, 1989. opment of advanced compilers. We intend to use our [17] ITO, T., AND \nMATSUI, M. A parallel lisp language:semantic characterization for the derivation of other pro- Pailisp \nand its kernel specification. [16:58-100]. gram optimization in Gambit and for the design of truly [18] \nJAGANNATHAN, S., AND WEEKS, S. Analyzing stores transparent future annot at ions for languages with im\u00ad \nand references in a parallel symbolic language. In LFP perative constructs. (1994), 294-305. [19] KATZ, \nM., AND WEISE, D. Continuing into the fu\u00adture: on the interaction of futures and first-class contin- \nAcknowledgments We thank Marc Feeley for discus\u00aduations, In LFP (1990). sions concerning touch optimizations \nand for his assis\u00ad [20] KESSLER, R. R., AND R. SWANSON. Concurrent scheme. tance in testing the effectiveness \nof our algorithm, and [16:200-234]. Nevin Heintze for discussions on set-based analysis and [21] KNOPP, \nJ. Improving the performance of parallel lisp for access to his implementation of set based analysis \nfor by compile time analysis. [16:271-277]. ML. [22] KRANZ, D., HALSTEAD, R., AND MOHR, E. Mu1-T: A \nhigh-performance parallel lisp, [16:306-321]. References [23] KRANZ, D,, HALSTEAD, R., AND MOHR, E. Mu1-T: \nA high-performance parallel lisp. In PLDI (1989), 81-90. [1]BAKER, H., AND HEWITT, C. The incremental \ngarbage [24] LEROY, X. Typage polymorphe d un langage algorith\u00ad collection of processes. In Proceedings \nof the Symposium mlque. PhD thesis, Universit4 Paris 7, 1992. on Arttficzal Intelligence and Programming \nLanguages (1977), VO1. 12(8), 55-59. [25] MII,LER, J. MultiScheme: A Parallel Processing System. PhD \nthesis, MIT, 1987. [2] BBN ADVANCED COMPUTERS, INC., CAMBRIDGE, MA. Inszde the GPIOOO. 1989. [26] MOHR, \nE., KRANZ, R., AND HALSTEAD, R. Lazy task creation: A technique for increasing the granularity of [3] \nCOUSOT1 P., AND COUSOT, R. Abstract interpretation: parallel programs. In LFP (1990). A unified lattice \nmodel for static analyses of programs by construction or approximation of fixpoints. In POPL [27] MO \nREAU, L. Sound Evaluation of Parallel Functional (1977), 238-252. Programs with Fzrst-Class Continuations. \nPhD thesis, University de Liege, 1994. [4] COLTSOT, P., AND Couso:, R. Higer order abstract interpretation \n(and application to comportment analysis [28] REPPY, J.H. Hzgher-Order Concurrency. PhD thesis, generalizing \nstrictness, termination, projection and per Cornell University, Jan. 1992. analysis of functional languages. \nICCL (1994), 95 112. [29] SA~RY, A., AND FELLEISEN, M. Is continuation-passing [5] DEUTSCH, A. Meddles \nOpe%ationnels de Language useful for data flow analysis. In PLDI (1994), 1 12. de Program mation. et \nRepn%entations de Relations [30] SHIVERS, O. Control-flow Analysis of Higher-Order sue des Languages \nRationnels avec Application a la Languages OT Taming Lambda. PhD thesis, Carnegie\u00adD&#38;termination \nStatique de Propri&#38;es de Partages Dy- Mellon University, 1991. namiques de Donndes. PhD thesis, University \nParis VI, 1992.  [31] SWANSON. M., KESSLER, R., AND LINDSTROM, G. An implementation of portable standard \nlisp on the BBN [6] FEELEY, hl. An Ej)lcient and General Implementation butterfly. In LFP (1988), 132-142. \nof Futures on Large Scale Shored-Memory Multiproces\u00adsors. PhD thesis, Department of Computer Science, \n[32] WAND, M. Compiler correctness for parallel languages. Brandeis University, 1993. Unpublished manuscript, \n1995. [7] FEELEY, M., AND MILLER, J. S. A parallel virtual ma-[33] WRIGHT, A. AND R. CARTWRIGHT. A practical \nsoft chine for efficient scheme compilation. In LFP (1990). type system for scheme. In LFP (1994), 250 \n262. [8] FELLEISEN, M., AND FRIEDMAN, D. P. Control oper\u00adators, the SECD-machine, and the lambda-calculus. \nIn 3rd Worhng Conference on the Formal Description of Programming Concepts (Aug. 1986), 193-219. 220 \n\t\t\t", "proc_id": "199448", "abstract": "<p>The <bold>future</bold> annotations of MultiLisp provide a simple method for taming the implicit parallelism of functional programs. Past research concerning <bold>future</bold>s has focused on implementation issues. In this paper, we present a series of operational semantics for an idealized functional language with <bold>future</bold>s with varying degrees of intensionality. We develop a set-based analysis algorithm from the most intensional semantics, and use that algorithm to perform <italic>touch</italic> optimization on programs. Experiments with the Gambit compiler indicates that this optimization substantially reduces program execution times.</p>", "authors": [{"name": "Cormac Flanagan", "author_profile_id": "81100538763", "affiliation": "Department of Computer Science, Rice University, Houston, Texas", "person_id": "PP14187273", "email_address": "", "orcid_id": ""}, {"name": "Matthias Felleisen", "author_profile_id": "81100323458", "affiliation": "Department of Computer Science, Rice University, Houston, Texas", "person_id": "PP39037684", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/199448.199484", "year": "1995", "article_id": "199484", "conference": "POPL", "title": "The semantics of future and its use in program optimization", "url": "http://dl.acm.org/citation.cfm?id=199484"}