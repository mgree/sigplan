{"article_publication_date": "01-25-1995", "fulltext": "\n Sequential Algorithms, Deterministic Parallelism, and Intensional Expressiveness* Stephen Brookes Denis \nDancanet School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 {brookes,dsncenet} \n@es. emu. edu Abstract 1 Introduction We call language L1 intentionally more expressive than We are interested \nin establishing relative intensionai ex-L2 if there are functions which can be computed faster pressiveness \nresults for programming languages. Most in L1 than in L2. We study the intentional expressive-work in \nthe past has focussed on extensional expres\u00ad ness of several languages: the Berry-Curien program-siveness: \nLanguage L1 is eztensionaliy more expressive ming language of sequential algorithms, CDSO, a de-than \nL2 if there are functions that are computable in terministic parallel extension to it, named CDSP, and \nL1 but not computable in L2. We say that L1 is inten\u00advarious parallel extensions to the functional program-sionaliy \nmore expressive than L,2 if there are functions ming language PCF. The paper consists of two parts. computable \nfaster in L1 than in L2. Note that there In the first part, we show that CDSO can compute has been a \nlot of work comparing the intensional ex\u00adthe minimum of two numbers n and p in unary rep-pressiveness \nof different models of computation. For resentation in time O(rnin(n, p)). However, it cannot inst ante, \nallowing only a single tape for a Turing ma\u00adcompute a natural version of this function. CD SP chine can \nsquare the time necessary to recognize a lan\u00adallows us to compute this function, as well as functions \nguage versus a two-tape Turing machine [13]; and there like parallel-or. This work can be seen as an \nextension are certain problems for which there exist faster CRCW of the work of Colson [7, 8] with primitive \nrecursive PRAM algorithms than EREW PRAM algorithms [10]. algorithms to the setting of sequential algorithms. \nOur work compares programming languages, not their In the second part, we show that deterministic paral-underlying \ncomputation models. lelism adds intensional expressiveness, settling a folk In the first part of this \npaper, we study the express\u00adconjecture from the literature in the negative. We show ibility of the minimum \nfunction, which computes the that CDSP is more expressive intentionally than CDSO. minimum of two natural \nnumbers represented in unary We also study three parallel extensions to PCF: parallel-form (O, S(0), \n. . . . where S stands for successor). We or (per) and parallel conditionals on booleans (pifo) and look \nat various algorithms computing minimum, which integers (pzjL). The situation is more complicated there: \nagree when the inputs are fully defined (as they must, pij is more expressive than both pifo and per. \nHowever, since they all compute minimum), but may disagree on pi~ still is not as expressive as the deterministic \nquery undefined or partial inputs. construct of CDSP. Thus, we identify a hierarchy of in-A natural way \nto define minimum is by the following tentional expressiveness for deterministic parallelism. rewrite \nsystem: *This research was sponsored by the Office of Naval Research rnin(x, O) = O under Grant No. Noo014-93-1-0750. \nrnin(O, x) = O The views and conclusions contained in this document are those of the authors and should \nnot be interpreted as represent\u00ad 7nin(S(z), S(y)) = S(min(z, y)) ing the official policies, either expressed \nor implied, of the U.S. Govement. We need to distinguish between the function min (the least function \nsatisfying the rewrite rules above), Permission to copy without fee all or part of this material is and \nan algorithm for rein, which we denote min~. Intu\u00ad granted provided that the copies are not made or distributed \nfor direct commercial advantaqe, the ACM copyright notice and the itively, the algorithm based on the \nabove rewrite rules title of the publication and Its date appear, and notice is given computes its result \nin time O(min(n, p)) (it takes ex\u00ad that copyin is by permission of the Association of Computing actly \nmtn(n, p) + 1 steps). One can formalize this by Machinery. + o copy otherwise, or to republish, requires \na fee andlor specific permission. POPL 951/95 San Francko CA USA 0 1995 ACM 0-89791 -692-1 /95/0001 ....$3.50 \ngiving an operational semantics, and defining a notion SW(L) of cost (cf. Colson [8]). The questions \nwe are interested in are: Is it pos\u00adsible to write a program to compute the minimum of n, p in time O(rnin(n, \np)) in Berry and Curien s lan\u00adguage of sequential algorithms, CDSO? Is this possible in a language of \nparallel algorithms, CDSP, obtained by generalizing the valof construct of CDSO to a par\u00adallel form of \nquery? In the second part of the paper we consider the following more general questions: Does the parallel \nquery construct of CDSP give added inten\u00adtional expressiveness over CDSO? How does it relate to the expressiveness \nof parallel extensions to PCF? The rest of the paper is organized as follows: First, we describe intensional \nsemantics and the domain of lazy natural numbers. We give a brief overview of se\u00adquential algorithms, \nconcrete data structures, and the programming languages CDSO and CDSP. Then we re\u00adview Colson s results \nconcerning primitive recursive al\u00adgorithms. We describe our results with CDSO, CDSP, and parallel PCF. \nWe end with conclusions. 2 Background 2.1 Intensional Semantics Traditionally, most denotational semantic \nmodels of programming languages have been extensional, designed to express only the input/output behavior \nof a pro\u00adgram. We are interested in reasoning about intensional aspects (e. g., complexity), so we need \nsemantic mod\u00adels that contain more computational information. This can be achieved in many ways. We outline \njust a few possibilities: Q We could take the meaning of a program to be a function on a richer domain \n(e. g., [4, 7]) whose structure permits us to deduce information about computation strategy. . We could \ntake the meaning to be a pair consisting of a function and an object conveying intensional information; \nthis object could represent the cost of evaluating the function, or could be a function from inputs to \ncosts (e.g,, [12, 19]). e We could dispense with functions as meanings al\u00adtogether, and use algorithms \ninstead (e.g., [2]). An important point to note is that intensionality is rel\u00adative. A model can be more \nintentional than another one, if its elements convey extra computational detail. For a simple example \nconsider the semantics of prim\u00aditive recursive (P7Z) algorithms. W? algorithms are just syntax for expressing \n772 functions [16]. The syntax is in the form of a rewrite system (see Colson [7, 8] for Sk(o) \\ Y(l) \n (0)\\/ 0> s(l) \\/ 1 Figure 1: The lazy natural numbers a formal definition). Consider the following \ntwo algo\u00adrithms for integer addition in unary representation [8]: addl(O, y) = y addl(S(2), y) = S(acldl(c, \ny)) add2(z,0) = z add2(z, S(y)) = S(add2(x, y))  The standard extensional denotational semantics for \naddl, add2 maps them both into the addition function of type N2 -+ N, where N is the flat domain of natu\u00adral \nnumbers. A simple intensional semantics may be provided by using the lazy natural numbers [7, 8, 9]. \nThe domain LNAT is shown in Figure 1. LNAT cap\u00adtures the temporal aspect of finding out what an input \nis. At Sk(1) we don t know yet if we have the number Sk(0), or something larger (at least S k+l (1)). \nThis in\u00adtentional semantics is sufficient to distinguish between the two addition algorithms. Using the \nmeaning func\u00adtion [ ] from [8, 9] (which makes the meaning 1 when an algorithm tries to recur on 1) we \nhave: [addl](S2(l), S(l))= S (l) [add2](S2(l), S(l))= S(l) The LNAT semantics is richer than the N seman\u00adtics, \nand contains intensional information; the above equations can be interpreted as showing that at some \npoint, add2 tries to evaluate part of its second argu\u00adment before the first, whereas addl looks at its \nfirst input first. Although the LNA T semantics still repre\u00adsents the meanings of addl and add2 as functions \n(from LNAT x LNAT to LNAT), it conveys implicit informa\u00adtion about computation strategy. In the next \nsection we describe Berry-Curien sequential algorithms, which can be used to provide a more explicitly \nintensional model for TV? algorithms. 2.2 Sequential algorithms and concrete data structures Sequential \nalgorithms on concrete data structures pro\u00advide an intensional semantics for sequential program\u00adming \nlanguages. In contrast with the traditional exten\u00adsional semantics for such languages, continuous func\u00adtions \nare replaced by sequential algorithms, and Scott domains by concrete data structures. We provide a very \nbrief overview, along the lines of Berry and Curien s work. The interested reader is referred to [2, \n11] for details and definitions. An alternative description in terms of decision trees can be found in \n[14]. 2.2.1 Concrete data structures Concrete data structures and their domain-theoretic counterparts \n(concrete domains) were developed by Kahn and Plotkin [15] in order to distinguish between function domains \nand domains of the data on which they compute. They provide an abstract framework for modelling incremental \nsequential computation. A concrete data structure (cds) consists of a set of named cells, which can hold \nvalues, and an accessibi!\u00adity relation governing the order in which the cells can be filled with values. \nA cell c filled with a value v is called an event, written c = v. A set of events satisfy\u00ading certain \nconditions (no cell is filled more than once, accessibility relation is respected) is called a state. \nThe set of states of a cds Al ordered by set inclusion form a concrete domain (D(ill), ~). Example 2.Il \nWe define BOOL, the cds of booleans. There is one cell called B, which can be filled with either tt or \nfl. The set of states of this cds is: {{}>{B = tf}, {B =fl}}. Note that (D(BOOL), ~) is isomorphic .to \nthe flat do\u00admain of booleans. n Example 2.2 We define LNA T, the cds of lazy natural numbers. It has \ncells bn, for n z O, values O and 1, and the following accessibility relation: bo is initial (no pre\u00adcondition), \nand {bi = 1} 1-bi+l (filling a cell with 1 enables the next cell). Intuitively, filling a cell with 1 \nmeans there might be more to follow, whereas O means we re done. (D(LNAT), ~) is isomorphic to the domain \nLNA T from the previous section. The encoding of the lazy natural numbers is: Sri(L) = {bi = 1Ii< n}, \nSri(0)== {bi=lli<n}U{bn= O}, forn>O, S (L) =={bi = 1Ii> o}. 1 Using the cds framework, Kahn and Plotkin \ndefined a notion of sequential function. A continuous function ~ from D(M) to D(iV1 ) is sequential at \nx if for each cell c accessible in ~(z) either (i) no cell is accessible in z, or (ii) there is a cell \nc accessible at z that must be filled in any state y that is a superset of z such that c is filled in \n~(y). The cell c is called a sequentiality indez of f at x for c . A function is sequential if it is \ncontinuous and sequential at every z in its domain. Intuitively, this definition captures the notion \nthat a sequential function is at any point dependent on cme of its inputs; if that input diverges, the \nfunction willl diverge.  2.2.2 Sequential algorithms Berry and Curien noted that Kahn-Plotkin sequential \nfunctions and concrete data structures fail to form a cartesian closed category. They introduced sequential \nalgorithms on concrete data structures, and showed that the category of sequential algorithms and cds \nis carte\u00adsian closed. Sequential algorithms can be viewed two ways: ab\u00adstractly and concretely. Abstractly, \na sequential algo\u00adrithm is a pair consisting of a sequential function and a (sequential) computation \nstrategy. If there are several ways of proceeding during the computation, the compu\u00adtation strategy picks \nout a particular one. Concretely, a sequential algorithm is a state of a cds of arrow type (the exponentiation \ncds). Given two cds, M and M , the exponentiation cds M + M is definedl as follows: the cells are of \nthe form xc , where x is a state of M and c is a cell of M ; the values are of the form valof c where \nc is a cell of itf, or output v if VIis avalue of M . A state of M + M is a sequential algorithm. Example \n2.3 The state of BOOL + BOOL that cor\u00adresponds to the boolean negaticm is: {{}B = vaiof 1?, {B= tt}B \n= output &#38;,, {B= f7}B = OU@Ut tt:}. The way to read this definition is: Given no information about \nthe input and having to fill the output cell B, we ask what value the input cell B holds. If the input \nis true we output false and conversely. 0  2.3 CDSO The programming language CDSO [1, 3, 11] is a direct \nimplementation of the intentional denotational seman\u00ad 1T~~ is ~~Y p~~t Of the definition; it omits reference \ntO the accessibility conditions. The full definition can be found in [11], for instance. tics presented \nabove; hence, it is an intensional pro\u00adgramming language of sequential algorithms. The name stands for \nConcrete Data Structures. CDSO is a lazy, polymorphic, higher-order, functional language with some original \nfeatures: Uniformity of types. Everything in CDSO is a state of a cds. This can be a state-constant \nor a higher\u00adorder algorithm. The algorithm syntax is just syn\u00adtactic sugar for the state of a cds. Consequently, \nan algorithm can be evaluated without being ap\u00adplied to any argument. Operationally speaking, terms of \nnon-ground type can be observed.  Full abstraction. The denotational semantics of  CDSO, which maps \nan algorithm to a state of the cds corresponding to its type (hence a CDSO ob\u00ad ject) is fully abstract \nwith respect to two different operational semantics (CDSO1 and CDS02) [11]. . Semantics manipulation. \nSince the semantics of an algorithm is itself a CDSO state it is possible to write algorithms which manipulate \nthe semantics of other algorithms. Example 2.4 As a simple examplez, we implement the cds of booleans \nand boolean negation in CDSO. The boolean negation algorithm is given in two forms: sug\u00adared (NOT) and \nun-sugared (NO T. STATE): let BOOL = dcds cell B values tt, ff end; let NOT : BOOL -> BOOL = algo request \nB do valof B i.s tt : output ff ff : output tt end end end; let NOT_ STATE : BOOL -> BOOL = {{}B = valof \nB, {B=tt}B = output ff , {B=ff}B = output ttl; The request construct specifies which output cell we \nare computing, valof requests a value from an input cell, and output fills a cell with a value. 0 The \nuser can combine algorithms and states into ex\u00adpressions using the categorical combinators: applica\u00adtion, \ncomposition, curry, uncurry, fix, pair, and prod\u00aduct. Computation is lazy, demand-driven: the user 2The \nsyntax for the CDSO examples presented in this section is from our own CD SO interpret er and is very \nsimilar to that in [3, 11]. types in an expression and enters a request loop. At this point the user \nmay type in an output cell name and if the cell is filled in that expression its value will come back \nas a result. The lazy evaluation model enables us to compute with infinite structures. Example 2.5 As \na more advanced example, we imple\u00adment the lazy natural numbers, and the successor algo\u00adrithm, which \nare needed in what follows. letrec LNAT = dcds cell B values O, 1 graft (LNAT. s) access B = 1 end; \nLNA T is defined using recursion and grafling: a copy of LNAT is included, tagging all cells with the \nspecified tag. The first three cells and their access conditions are as follows: B values O, 1 (B. s) \nvalues O, 1 access B=l ((B. s) .s) values O, 1 access (B. s)=l Now let us define a few constants: 1, \n0, S(l), and  S J(l): let Bot : LNAT = {}; let Zero : LNAT = {B=O} ; let S_bot : LNAT = {B=l} ; let \nSrec : LNAT -> LNAT = algo request B do output 1 end request (( B.$V) .s) do valof (B. $V) is 1 : output \n1 end end end; let S_omega_bot : LNAT = fix(Srec); S (J_) is defined as the least fixpoint of the \nalgorithm which in the base case fills B with 1, and recursively, if the previous cell contains 1, puts \n1 into the current cell. The name variable $V matches any tag. The ability to use such variables for \ncell names and values is the source of polymorphism in CD SO. Now we can write the successor algorithm. \nIts struc\u00adture is just slightly more complicated than the algo\u00adrithm for SW (1), but warrants further \nexplanation be\u00adcause it is higher-order. Successor is defined as the fix\u00adpoint of a higher-order algorithm \nand it works as fol\u00adlows: If asked what 1? is, it immediately outputs 1 (the successor of anything is \nat least S(l)). In the gen\u00aderal case, if asked what value an output cell holds, it asks what value the \ninput cell immediately preceding it holds, and outputs the same value. let succ-rec :(LNAT-> r..mi r) \n-> LNAT -> LNAT= algo request {}B do output output 1 end request {}((B.$V).S) do output valof (B.$V) \nend request {(B. $V)=O}((B.$V).S) do output output o end request {(13.$ 0=ll( (B.$W.S) do output output \ni end end; let S : LNAT -> LNAT = fix(succ-ret); 0 2.4 CDS: A higher-level notation As can be seen from \nthe examples in the previous see\u00adtion, the syntaxofCDSOisvery low-level. Infact, CDSO was designed to \nserve as a machine-language for a syntactically ML-like language called CDS [I]. CDS was never fully \ndescribed or implemented. For ease of presentation we assume an SML-like notation [17] for it, glossing \nover exactly how the translation to CDSO might be accomplished (for a discussion see [3]). An important \ndifference between CDS and SML is that in CDS pattern matching is not allowed on tuples, so that the \nsequential nature ofthe computation ismorereadily apparent. 2.5 CDSP: Aparallel version ofCDS Brookesand \nGeva [5] extended Berry and Curien s work to the setting of deterministic parallel algorithms on cds. \nThey generalized the valof construct of CDSO which tests the value of one cell to a deterministic parallel \nquery construct, which, intuitively, spawns off a num\u00adberofvalofs, More precisely, aquerystarts anumberof \nparallel subcomputations and specifies conditions based on the results of the subcomputations under which \nthe maincomputation may resume. Wecallthe extension of CDSO with the query construct CDSP (for CDS Paral\u00adlel). \nA CDSP algorithm may be viewed as a continuous function paired with a (parallel) computation strategy. \nExample 2.6 Query enables us to compute new func\u00adtions. One example is parallel-or, which returns true \nif either of its arguments is true. Parallel-or, as the name implies, is not a sequential function. Here \nis how it would be implemented in CDSP, assuming the higher\u00adlevel syntax: algo por (bl, b2) = query (bl, \nb2) is (tt,-)* t t I(-,tt)% tt   l(ti).ff)+fl 0 In order to ensure determinism, all consistent (simul\u00adtaneously \nsatisfiable) branches of a query must have the same result; for example, the first two branches of the \nabove algorithm result in the same output. This re\u00adquirement is built into the syntax used in [5]. We \nuse a simpler notation in order to avoid the extra complexity. 3 Colson s Results Colson studied the \nexpressibility of the minimum func\u00adtion in the context of primitive recursive (T%?) algo\u00adrithms [7, 8]. \nHe established that ?%? algorithms are inherently sequential: like sequential algorithms, they possess \nsequentiality indices. Moreover, P7? algorithms are sequential in an even stronger sense. They suffer \nfrom ultimate obstination [8, 9]: at some point one argument must be chosen to be evaluated until the \nend. Using primarily the intentional denotational semantics based on LNAZ , Colson proved two main results: \nProposition 3.1 There M no P7? algorithm a of arity 2 satisfying: [a](Sn(l), Sp(J_)) = Smin(n p)(l). \nProposition 3.2 There is no V%? algorithm computing the minimum of two numbers n and p in unary repre\u00adsentation, \nwith time complexity O(min(n, p)). However, there are many PI? algorithms which com\u00adpute the minimum \nof two integers in unary representa\u00adtion. We define one below, using some auxiliary func\u00adtions (see [16]): \npred(0) = O pred(S(z)) = z sub(z, O) = z sub(x, S(y)) = pred(sub(z, y)) MIN(z, g) = sub(z, SUb(Z, y)). \nAgain we need to distinguish between the function AIIiV and an algorithm MIN., for MIN, Note that in \nan operational interpretation of this definition, the algorithm MINa (n, p) has a worst-case running \ntime of O(ma~(n, p)). The function MIN agrees with min from Section 1 on the totally defined elements \nof the lazy naturals. They differ on the partial elements, since in the LNAT semantics we have:  s~wwP)(~), \nrnin(s (l), sf (-1)) = Mliv(s (l),sf (1)) = J-. We can view Proposition 3.1 as an extensional ex\u00adpressiveness \nresult: 7V? algorithms can compute LlliV but not min. Note that there are many other functions between \nmin and MIN in the pointwise order. But it is the intensional aspect of Proposition 3.2 that is partic\u00adularly \ninteresting here: 7-V? algorithms cannot compute minimum efficiently. If we augment P7Z algorithms with \nfunctional argu\u00adments, we arrive at Godel s system T. System T can not only compute new functions (e. \ng., the Ackermann func\u00adtion), but can also compute minimum efficiently. Thus system T is more powerful \nthan 7Wl both extensionally and intentionally ( cf. [8]). Colson s results are the first intentional \nexpressive\u00adness results for programming languages of which we are aware. 4 Sequential, Parallel Algorithms, \nand Minimum We expected to obtain results similar to Colson s in our study of sequential algorithms. \nAfter all, CDSO is a sequential programming language by design: sequen\u00adtial algorithms compute sequential \nfunctions. It turns out, however, that sequential algorithms are sufficiently more powerful than 77? \nalgorithms to be able to com\u00ad pute minimum efficiently, but not powerful enough to compute the natural \n min function from the introduc\u00ad tion. The parallel query construct of CDSP allows us to compute that \nfunction. 4.1 CDSO We begin by showing that sequential algorithms cannot compute min. The proof follows \nstandard lines ( cf. [2, 5]). Proposition 4.1 There is no sequential algorithm com\u00adputing min. Proofi \nA sequential algorithm computes a sequential function. But min is not sequential, since it has no sequentiality \nindex at (1, 1) for output cell 60. In other words, there is no input cell which must be filled in order \nfor min to fill bO. (Actually, min has no sequentiality index at any (Sin(l), Sri(l)) for bn, n > O.) \nTherefore, no CDSO algorithm can compute min. 0 But this does not mean we cannot compute mini\u00admum efficiently \nin CDSO. Recall that the problem with P%? algorithms was that they become fixated on one input. Sequential \nalgorithms allow us to keep alternat\u00ading between the two inputs, examining one cell at a time. We reason \ninformally about the running time of se\u00adquential algorithms. It is possible to formalize these arguments \nby appealing to the operational semantics. Proposition 4.2 There M a sequential algortthm which computes \nthe mmzmum of two numbers n and p in unary representation, and w of time complexity O(min(n, p)). Proofi \nThe algorithm looks like a simple sequential version of the min function definition from the intro\u00adduction. \nWe choose the left input to evaluate first. algo left-rein (nl, n2) = case nl of 0+ 0 I S(x) * case n2 \nof 0+0 I S(y) * S(left.-rnin(x, y)) The algorithm has the following property:  S ~wLP)(()), [left-rnin](Sn(0), \nS (0))= so it does compute the minimum, and it works in time O(min(n, p)) by alternating between the \ninputs and ex\u00adamining one cell at a time. 0 Note that the algorithm also satisfies: [left-rnin](Sn( J-), \nS (1)) = S min(nJ J(l), so Colson s Proposition 3.1 fails as well in the context of sequential algorithms. \nThe key difference between leftmin and min. is il\u00adlustrated by their behavior on pairs of a totally defined \nand a partial element, such as (Sri(0), Sri(l)) (they agree on all other inputs): [leftmin](Sn(0), Sri(l)) \n= Sn(J_) [minalJ(S (0), Sri(l)) = Sri(0) [leftmin](Sn(l), Sri(0)) = S (L) [mina](Sn(l), Sri(0)) = Sri(0) \nThis comparison makes it clear that min is a parallel function: it must evaluate its inputs in parallel \nin order to be able to determine when either one is defined. Also note that [leftmin] fits between min \nand MIN in the pointwise order. 4.2 CDSP The addition of the parallel query construct enables us to \ncompute minj which is essentially a generalization of parallel-or to integer arguments. The program looks \nalmost the same as the definition of the min function from the introduction: algo min (nl, n2) = and \nn-ary addition. They can be computed in logarith\u00ad query (nl, n2) is mic time, which is an improvement \nover the linear time (o, .) s\u00ad o achievable in CD SO. The two functions are very similar 1(.,0)+0 in \nstructure. As with CDSO, we argue informally about [ (S(x), S(y))* S(min(x, y)) the running time of CDSP \npro,grams. We then obtain the following, using the semantics of  CDSP: Proposition 4.3 There is a \nCDSP program computing min. 5 Deterministic Parallelism and Inten\u00adtional Expressiveness There appears \nto be a folk conjecture that determinis\u00adtic parallelism is not useful. The claim (see [6], for instance) \nis that even though deterministic parallel fea\u00adtures may increase the extensional expressiveness of a \nlanguage, they are expensive to use and the additional expressiveness is not useful in practice, because \nit ap\u00adplies only to computations that are unbounded. In our terms, the claim is that deterministic parallelism \nmay increase extensional, but not intentional expressiveness. As we ll see in what follows, this conjecture \nis false. Deterministic parallelism does add intensional expres\u00adsiveness. The deterministic query construct \nof CDSP is sufficiently general to allow a speedup in the computa\u00adtion of many different functions. From \nour study of CDSO and CDSP we are natu\u00adrally drawn to a study of the sequential functional lan\u00adguage \nPCF and its parallel extensions. The reason is the close connection between CDSO and PCF: CDSO is an \nintentional semantics for PCF [3]. In fact, CDSO is extensionally more expressive than PCF, being able \nto express semantic-manipulation algorithms. Thus, the results we obtain with CDSO and parallel extensions \nare likely to be mirrored with PCF and similar parallel extensions. The situation is more complicated \nin the case of PCF: the different parallel extensions in the literature, although extensionally equivalent, \nare not intentionally equivalent. They do increase the computational power of PCF, but in different ways. \nThey are also less pow\u00aderful than CDSP s query. In this second part of the paper we no longer work with \nintegers in unary represent ation. WJe are interested in the running time of n-ary operations, such as \nadding n integers. Thus, our results should be more relevant to real programming languages. 5.1 CDSP \nWe give examples of two functions which can be com\u00adputed faster in CDSP than in CDSO: n-ary disjunction \nThe main idea is to construct a tree of processes. For notational simplicity, we define a separate function \nfor each value of n, and we assume n is a (fixed) power of 2. We have already defined por for two arguments. \nHere is the general case: algo porn (bl, .... bn) = por (porn/z (bl, . . . . b~p), per+ (bn/2+1, . . \n. . bn)) n-ary disjunction creates a tree of processes of depth log n. Addition for n arguments, paddn, \nworks similarly using addition on two arguments, padd, given by: algo padd (xl, X2) = query (xl, X2) \nis (VI, V2) * VI+ v~ Note that the addition of VI and V2 is performed sequen\u00adtially (this + is sequential, \nnot bitwise-parallel). This is not essential. What is important is that separate processes are started \nto evaluate the inputs. When computing porn or pa!ddn, in order to fill the output cell we query in parallel \ntwo cells. In order to fill those cells, we query two more for each. Intuitively, after a depth of log \nn queries we reach our n inputs. Therefore, we compute the result in time O(iog n). In CDSO, since we \nmust examine the inputs sequentially, we can only compute the result in time O(n). Proposition 5.1 CDSP \nis intensionuily more expres\u00adsive than CDSO. Note that the previous proposition is unaffected by the \nfact that we are no longer using integers in unary representation. The function porn would still be speed\u00aded \nup in CDSP even with una~y representation for in\u00adtegers.  5.2 P(3F and circuits PCF [18] is a paradigmatic \nsequential functional pro\u00adgramming language that has been used in many seman\u00adtic studies of sequentiality. \nIt is a typed Xcalculus with two ground types, booleans (o) and integers (L) and the following set of \ntypes: The syntax is given by the grammar: The constants traditionally included in the language are \nthe following: it:o ff:o n:L (the integers, n > 0) &#38;ztTO? : L* O +1: L~L 1: L+L >0 :0-0 ~ o -+ o \n(boolean conditional) >, : O~L*L*L (integer conditional) Y. :(a-+u)~ a (one for each u) For simplicity, \nwe blur the distinction between numerals and integers, and use n to denote both. The relevant rules of \nthe operational semantics for the constants are: >0 tt MON04M0, for o = L,O >ryff M. N.-+N., foru=l, \no YaA4 + M(Y-.M) +In=n+l, forn~O I(n+l)-+n, forn>O wZero? O * tt isZero? (n + 1)+ff, for n>0 In addition \nto the standard constants listed above, we assume the existence of a constant-time equality test for \nintegers: = : l~lbo with the obvious operational semantics. Traditionally, the equality test is implemented \nusing recursion (cf. [20]), but this would render some of the issues of inter\u00adest to us moot, since we \nare not dealing with integers in unary representation. We find it useful to view PCF programs as circuits. \nThere are several reasons for this. First, it enables us to reason based on the last gate used in the \ncircuit. Viewing a program as a circuit reduces the number of cases we need to consider. Second, the \nrunning time of the program corresp ends to the depth of the circuit. We are only interested in programs \nof ground type so we need not worry about complications caused by higher\u00adorder programs. And third, circuits \nprovide a visual and intuitive semantics. The translation from PCF to circuits is simple. Fig\u00adure 2 shows \ncircuits for function definition, application, and a constant. A function denotes a circuit some of whose \ninputs are labelled with variables. Application substitutes a value for a variable, or, if we have a \nwhole circuit, connects its output to the respective variable\u00adlabelled input. Note that higher-order \nfunctions can be treated in this framework as well, by labelling an in\u00adput with a function variable and \nusing gates labelled with the function variable inside the circuit. There are N xbxy MM >L   7+4 (a) \n(b) (c) gates for the various constants. The only interesting case is the Y combinator. It gives rise \nto a special kind of circuit, a dynamic czrcuit, which can have subparts expanded dynamically as required \nduring computation. The semantics of circuits is based on PCF S opera\u00adtional semantics. Execution is \ndemand-driven and be\u00adgins at the output. The last gate in the circuit is acti\u00advated. This gate may start \nevaluating one (or more, if it is parallel) of its inputs, leading to activity at further gates, and \nso on. If the computation terminates, the result will filter down to the output of the last gate. Definition \n5.2 A circuit is static tf tt w the translation of a non-recursive PCF program. Definition 5.3 A circuit \nis dynamzc if it is the trans\u00ad lation of a recurszve PCF program. A circuit could have several inputs, \nbut it always has just one output, so it is shaped as a tree. Definition 5.4 The depth of a static circutt \nts equal to the hetght of the underlying tree. Definition 5.5 A czrcuit is constant-depth if it is ei\u00adther \nstattc, or a dynamzc circuat which does not expand more than a fixed constant number of times (indepen\u00addent \nof the inputs). Example 5.6 To give an example of dynamic circuits, and to illustrate the difference \nbetween constant-depth and non-constant-depth dynamic circuits, consider the following PCF program: F=Afnz. \n3, (= n3)z(f (+1 n)z). Figure 3 shows the circuit denoted by the recursive PCF term YF. We enclose a \ndynamic circuit in a box with dotted lines, to represent the fact that it can be expanded. The box is \nlabelled with the name of the recursive part. The result of expanding the circuit once is shown in Figure \n4. n xnx .... . f: 3 >L i.............. ................. Figure 3: YF The program YF n for O< n ~ \n3 gives rise to a constant-depth dynamic circuit, while for n > 3 it re\u00adsults in a non-constant-depth \ndynamic circuit. 0 In the following, we are particularly interested in the constant-depth circuits. If \ntwo functions can be implemented in terms of each other with constant-depth circuits, we say that the \ntwo functions are intentionally equivalent. 5.3 Parallel extensions of PCF The parallel extensions of \nPCF studied in Plotkin s sem\u00adinal paper [18] are: per, pifo (parallel conditional on booleans), and pit \n(parallel conditional on integers). The functions are defined as follows: for a = t, o. They are known \nto be extensionally equiv\u00ad alent [11, 20]. Interestingly, they are not intentionally equivalent. Obviously, \npor can be used to implement n-ary dis\u00adjunction as in CDSP, thus providing added intensional expressiveness \nover PCF. It turns out that pifO and pij can also be used for this purpose. However, por and pifo cannot \nimplement pit efficiently and none of the constructs can speed up n-ary addition. Proposition 5.7 por \nand pifO are intentionally equiv\u00ad alent. Proof: We need constant-depth implementations of one in terms \nof the other. This can be done as follows (cf. [20]): n xnx ..... ................ .........., f: 1+13 \n+1 I = f: IW! :Ll ; !............. .............. ... n3x Figure 4: YF expimded once por = Axy. pifo \nx tt y, pifo = Abxy. por (pand b x) (panel (no~! b) y) (pand x y), where pand is the parallel conjunction \ndefined by: pand = Azy. not (per (not x) (not y)), and we have generalized por to three arguments in \nthe obvious way. El It is known that pi~ can implement pifO (cf. [20]): pzfo = )bxy. (= 1 (pit b(0, z \n1O)(0, Y 1O))). This implementation is also efficient. In view of the previous proposition, it follows \nthat pit can also imple\u00adment por efficiently. However, the converse is false. The problem is that por \ncan only start parallel subcompu\u00adtations on booleans, whereas pa~ operates in parallel on integers. The \nstandard way of encoding pifL with por uses recursion ( cf, [20] ): pi~ = YF O, where F = Afnbzy. ~, \n(per (pand (= x n) (= y n)) (pand b (= z n)) (pand (not b) (= y n))) n (f (+1 n) bz Y).  This is clearly \ninefficient, because of the way the recur\u00adsion unwinds, checking if x anc~ y are equal to O first, (a) \nThey evaluate to x, y, respectively. But this Table 1: Requirements for function B then 1, and so on. \nBut we cannot do any better. To show that, we prove first two lemmas which restrict the shape of any \nprogram computing ptf,. The point of the first lemma is that it is impossible to design boolean circuitry \nB which chooses between z and y and obeys all the requirements of pi~. Lemma 5.8 It is not possible to \nwrite a program in PCF + per, whtch computes pit b x y and is of the form ~, B x y, where B is a static \nczrcutt yielding a boolean. Proof: Without loss of generality, the issue is whether it is possible to \nwrite a PCF + por function B with the following properties: 1. Ifbis ttthen Bistt, 2. Ifb isfl then B \nisfl, 3. If(= xy)is ttthen B is tt. Table 1 shows some of the inputs and corresponding outputs for function \nB. For simplicity, we assume only b and (= z y) are used in evaluating B. The same ar\u00ad gument can be \ncarried through with additional inputs, since b and (= z y) must be used in evaluating B. The last line \nof Table 1 implies by monotonicity that B ff tt = tt. But this violates the monotonicity condi\u00adtion raised \nby the second line in the table. Therefore, no program of this form computes pi~ b x y. 0 Our second \nlemma generalizes the first one. Lemma 5.9 It is not possible to write a program in PCF + per, which \ncomputes pi~ b x y and is of the form 3, B N1 Nz, where B, N1, N~ are static circuits yielding a boolean \nand two integers respectively. Proofi Intuitively, there are two possibilities for B: either it chooses \nbetween N1 and N2, or it is hard\u00adwired to always pick one of them. More precisely, we have two cases \nfor the function computed by B: 1. B is non-constant. Since the program computes pt~ b z y, the result \nmust be either x or y. There are an infinite number of possible inputs and out\u00adputs and N1, N2 are static \ncircuits, so it is not possible to hard-code the output. B will some\u00adtimes return ttand sometimes H. \nThere are then three choices for what N1, N2 evaluate to: is impossible by Lemma 5.8. (b) They both evaluate \nto pi~ b x y. The ~, gate then does no work. Since N1, Nz both compute something of type integer, there \nare essentially two cases for the last gate used in their construction: (i) 2, or (ii) +1 ( 1 is handled \nsimilarly). In case (i) apply the same reasoning of this lemma. There cannot be an infinite sequence \nof ~, gates which do noth\u00ading, since the circuit is static. It is not possible for all 3, gates to do \nnothing since the out\u00adput would then have to be constructed out of +1, 1, and the integers, so it would \nei\u00adther be hard-coded (and it must work for an infinite number of values), or produce a fixed offset \nfrom x or y. The latter case is analogous to case (la) above, except that the branches evaluate here \nto a fixed offset of z or y; the same reasoning applies. In case (ii) there can\u00adnot only be +1 (or 1) \ngates for the reason outlined above. Also, there can only be a con\u00adstant number of +1 (or 1) in a row \nbefore some >, is reached, whereupon we can apply the lemma again. By the same reasoning we must at some \npoint encounter case (la) of the proof. (c) One evaluates to pit b x y and the other to x or y. What \nis the last gate in the one that evaluates to pifL b x y? Apply the same rea\u00adsoning here as in case (lb), \neventually reach\u00ading case (la).  2. B is constant. That means that either N1 or N2 must compute ptfL \nb x y. Again we have a >~ gate which does no work. Without loss of generality, assume B is tt, so N1 \nalways gets chosen. What is the last gate in N1 ? We can apply the same rea\u00adsoning here as in case (lb) \nof the proof, eventually reducing the problem to case (la). So our circuit cannot be filled with gates \nwhich do no work. At some point there must be a ~, which essentially attempts to choose between z and \ny. But that is impossible by Lemma 5.8. Therefore, our pit program cannot have even this more general \nform. 0 Now we are ready to prove the main result of this section. Proposition 5.10 PC F + por cannot \nzmplement pi~ with a constant-depth circuit. Proofi Assume there exists a constant-depth circuit computing \npit. There are two possibilities: 1, Static circuit. The result has type integer. There\u00adfore, there are \ntwo cases for the last gate in the circuit: (a) >,. By Lemma 5.9 this is not possible. (b) +1 or 1. \nThe circuit cannot be constructed entirely out of +1, 1, integers, z, y, because the result would be \neither hard-coded (and it must work for an infinite number of values), or a fixed offset of x or y. Also, \nsince the circuit is static, there can only be a constant number of +1 or 1 in a row before reaching \nan oc\u00adcurrence of >,. Then we have essentially the same situation as in case (la) (modulo some fixed \noffset, as in the proof of Lemma 5.9), and by the same argument the circuit cannot implement pi~,,  \n2. Dynamic circuit. We want to show that the circuit cannot be constant-depth. Assume, for a contra\u00addiction, \nthat there is a fixed maximum constant depth beyond which the recursion does not get unwound, regardless \nof the inputs b, z, y. Then there are only finitely many constant-depth cir\u00adcuits which could be the \nresult of the unwind\u00ading. But there are infinitely many possible inputs. Therefore, at least one of these \ncircuits must work for infinitely many inputs. Apply the same rea\u00adsoning on that circuit as in case (1) \nof this proof. We can assume there is no other recursion, oth\u00aderwise continue the argument on innermost \nrecur\u00adsion (there must be a constant number of them). Therefore, there is no fixed maximum depth for \nunwinding the recursion computing pi~. In conclusion, it is impossible to write a constant\u00addepth program \nusing pm to compute pit, therefore por and pi~t are not intentionally equivalent. 0 The next question \nwe are concerned with is whether pi~ is sufficient to implement n-ary addition efficiently. The answer \nis no. The problem is that even though pi~ can start parallel subcomputations to evaluate two integers, \nit must return one of them. There is no way to combine the results of the subcomputations. Only a limited \namount of communicant ion exists between the subcomputations: a check for equality of their results. \nWe assume the existence of an addition operation (+), as in CDSP, so we can write sequential addition \nwithout having to use recursion: addz = Amy. x + y, Proposition 5.11 PCF + pi~ cannot implement n\u00adary \naddition with a circuit of depth log n. Proofi We identify a property that holds for our CDSP program, \npadd, and show that it does not hold for pro\u00adgrams of PCF + pit. In padd the inputs are evaluated in \nparallel and the result is their sum. In PCF + pij, the only parallel primitive is pzfi so the inputs \nz and y must go through some pif, if tlhey are to be evaluated in parallel. Suppose z goes through pi~ \nafter passing through some constant-depth circuit computing F and similarly for y and a function G. Then \nthe output of the pi~ is either F(z) or G(y). If either F(z) = z + u or G(y) = ~ + y, then the addition \nwas performed se\u00adquentially before the pit. If the output of pit goes into some constant-depth H such \nthat H(F(z)) = z + y or H(G(y)) = x + y then the addition was also performed sequentially, this time \nafter the pif, So itis not pos\u00adsible to compute x + y using pit in such a way that z and y are evaluated \nin parallel. Therefore, a PCF + pit program for n-ary addition lmust be of depth n. 0 As a corollary \nof the previous two propositions, we have the following: Proposition 5.12 PCF + por cannot implement \nn\u00adary addition with a czrcuit of depth log n. In light of these results, we have the emergence of a picture \nof different levels of intensional expressiveness for deterministic parallel constructs: At the lowest \nlevel we have por and pifo, which seem to be able to speed up only n-ary boolean functions. At the next \nlevel we have pi~, which can be used to speed up some integer functions. Finally, at the top level we \nhave query, which can be used to speed up n-ary addition. 6 Conclusions The sequentiality of the primitive \nrecursive algorithms is manifested by their ability to recur on only one input. This makes them ultimately \nobstinate, and they are not able to express an efficient algorithm for minimum. The sequentiality of \nBerry-Curien algorithms is by design. A sequential algorithm computes a sequential function, by only \nchoosing one sequentiality index at a time, even if more than one exists. However, sequential algorithms \nare more expressive than primitive recursive algorithms: there is a sequential algorithm that com\u00adputes \na version of the minimum function efficiently, but not the natural , inherently paLrallel, minimum func\u00adtion. \nThe addition of functional arguments to primitive recursion (system T) gives more power intentionally \nas well as extensionally. It allows us not only to express new functions, but also to compute more efficiently. \nThe addition of deterministic parallelism to CDSO allowed us to compute the natural version of the min\u00adimum \nfunction, but CDSO was already able to express an efficient minimum algorithm. However, the addi\u00adtion \nof deterministic parallelism did add intensional ex\u00adpressiveness, contradicting a conjecture from the \nliter\u00adature. The computation of a number of functions can be speeded up, such as n-ary disjunction and \nn-ary ad\u00addition. A more careful study of deterministic parallel constructs reveals different intensional \npowers. CDSP s query is more powerful than three parallel extensions to PCF, which differ in power among \neach other. Thus we have the beginnings of a hierarchy of intensional ex\u00adpressiveness for deterministic \nparallelism. We have exhibited languages which are extension\u00ad ally but not intentionally equivalent. \nThe constructs per, pzfo, and pi~ are interdefinable in the continuous function model of PCF. However, \nPCF + pif, is inten\u00adtionally more expressive than PCF + por (or pifo). A natural question raised by this \nis whether there exists a language that is extensionally more expressive but intentionally less expressive \n(on the common subset of computable functions) than another language. The case of the Girard-Reynolds \nsystem F versus Godel s system T might be an example of this, but the matter is not settled yet (cf. \n[8]). The study of intensional expressiveness has been ne\u00adglected in the past, perhaps because it seems \nto be more difficult than extensional expressiveness. For instance, the problem of NC versus P can be \nphrased as a problem of relative intensional expressiveness between program\u00adming languages. Despite the \ndifficulty, we believe that it is possible to obtain interesting results concerning intensional expressiveness, \nand that the area of int en\u00adsional semantics deserves further exploration. Acknowledgments We thank Matthias \nFelleisen for many useful conversa\u00adtions on the topic of intensional expressiveness and for comments \non an earlier version of the first part of the paper. We also thank the anonymous referees, whose suggestions \nled to improvements in the paper. References [1] G. Berry, Programming with concrete data structures \nand sequential algorithms, in: Proc. ACM Conf. on Funct~onal Programming Languages and Computer Ar\u00adchddure, \n1981, 49-57. [2] G. Berry and P.-L. Curien, Sequential algorithms on concrete data structures, Tr5eoretical \nGomputer Science 20 (1985) 265-321. [3] G. Berry and P.-L. Curien, The kernel of the applicative language \nCDS: Theory and practice, in: M. Nivat and J.C. Reynolds, eds., Algebraic Methods m Semantics (Cambridge \nUniversity Press, 1985) 35-87. [4] S. Brookes and S. Geva, Computational Comonads and Intensional Semantics, \nin: M. Fourman, P. Johnstone, and A. Pitts, eds., Applkatzons of Categories in Com\u00ad puter Science, LMS \nLecture Notes 177 (Cambridge Uni\u00ad versity Press, 1992) 1-44. [5] S. Brookes and S. Geva, Towards a theory \nof paral\u00ad lel algorithms on concrete data structures, Theoretical Computer Science 101 (1992) 177-221. \n[6] R. Cartwright, P.-L. Curien, M. Felleisen, Fully ab\u00adstract semantics for observably sequential languages, \nto appear in Information and Computation. [7] L. Colson, About Primitive Recursive Algorithms, in: Proc. \nInternational Colloquium on Automata, Lan\u00adguages, and Programming, 1989, 194-206. [8] L. Colson, Repr&#38;entation \nintentionelle d algorithms clans tes systimesfonctionelles: une dude de cas, Th6se de Doctorat, Universit6 \nParis VII (1991). [9] T. Coquand, Une preuve directe du Th60r>me d Ultime Obstination, Comptes Rendus \nde 1 Acad.4mie des Sci\u00adences, March 1992. [10] T.H. Cormen, C.E. Leiserson, R.L. Rivest, Introduction \nto Algorithms (MIT Press, 1990). [11] P.-L. Curien, Categorical Combinators, Sequential Al\u00adgorithms, \nand Functional Programming (Birkhauser, 1993). [12] D.J. Gurr, Semantic Frameworks for Complexity, Doc\u00adtoral \nThesis, University of Edinburgh, Technical Report ECS-LFCS-91-130, January 1991. [13] J.E. Hopcroft and \nJ.D, Unman, Introduction to Automata Theory, Languages, and Computation (Addison-Wesley, 1979). [14] \nJ. Hughes and A. Ferguson, A loop-detecting inter\u00adpreter for lazy, higher-order programs, in: Proc. Glas\u00adgow \nWorkshop on Functional Languages, 1992. [15] G. Kahn and G.D. Plotkin, Concrete Domains, Theo\u00adrettctd \nComputer Science 121 (1993). [16] S.C. Kleene, Introduction to Metamathematics (North-Holland, 1952). \n[17] R. Milner, M. Tofte, and R. Harper, The definition of Standard ML (MIT Press, 1990). [18] G.D. Plotkin, \nLCF considered as a programming lan\u00adguage, Theoretical Computer Scaence 5 (1977) 223-56. [19] M. Rosendahl, \nAutomatic complexity analysis, in: Proc. ACM Conf. on Functional Programming Lan\u00adguages and Computer \nArchitecture, 1989, 144-156. [20] A. Stoughton, Interdefinability of parallel operations in PCF, Theoretical \nComputer Science 79 (1991) 357-8. \n\t\t\t", "proc_id": "199448", "abstract": "<p>We call language <italic>L</italic><subscrpt>1</subscrpt> <italic>intensionally more expressive</italic> than <italic>L</italic><subscrpt>2</subscrpt> if there are functions which can be computed faster in <italic>L</italic><subscrpt>1</subscrpt> than in <italic>L</italic><subscrpt>2</subscrpt>. We study the intensional expressiveness of several languages: the Berry-Curien programming language of sequential algorithms, CDS0, a deterministic parallel extension to it, named CDSP, and various parallel extensions to the functional programming language PCF. The paper consists of two parts.</p><p>In the first part, we show that CDS0 can compute the minimum of two numbers <italic>n</italic> and <italic>p</italic> in unary representation in time <italic>O(min(n,p))</italic>. However, it  cannot compute a &#8220;natural&#8221; version of this function. CDSP allows us to compute this function, as well as functions like parallel-or. This work can be seen as an extension of the work of Colson with primitive recursive algorithms to the setting of sequential algorithms.</p><p>In the second part, we show that deterministic parallelism adds intensional expressiveness, settling a &#8220;folk&#8221; conjecture from the literature in the negative. We show that CDSP is more expressive intensionally than CDS0. We also study three parallel extensions to PCF: parallel-or (<italic>por</italic>) and parallel conditionals on booleans (<italic>pif</italic><subscrpt>&ogr;</subscrpt>) and integers (<italic>pif</italic><subscrpt>&igr;</subscrpt>). The situation is more complicated there:  <italic>pif</italic><subscrpt>&igr;</subscrpt> is still more expressive than both <italic>pif</italic><subscrpt>&ogr;</subscrpt> and <italic>por</italic>. However, <italic>pif</italic><subscrpt>&igr;</subscrpt> still is not as expressive as the deterministic query construct of CDSP. Thus, we identify a hierarchy of intensional expressiveness for deterministic parallelism.</p>", "authors": [{"name": "Stephen Brookes", "author_profile_id": "81100532140", "affiliation": "School of Computer Science, Carnegie Mellon University, Pittsburgh, PA", "person_id": "P268069", "email_address": "", "orcid_id": ""}, {"name": "Denis Dancanet", "author_profile_id": "81100562024", "affiliation": "School of Computer Science, Carnegie Mellon University, Pittsburgh, PA", "person_id": "P65430", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/199448.199451", "year": "1995", "article_id": "199451", "conference": "POPL", "title": "Sequential algorithms, deterministic parallelism, and intensional expressiveness", "url": "http://dl.acm.org/citation.cfm?id=199451"}