{"article_publication_date": "01-01-2004", "fulltext": "\n Precise Interprocedural Analysis through Linear Algebra * Markus M\u00a8uller-Olm Helmut Seidl FernUniversit \n\u00a8TUM\u00a8urInformatikII at Hagen, LG Praktische Informatik 5 unchen, Lehrstuhl f\u00a858084 Hagen, Germany 80333 \nM\u00a8unchen, Germany mmo@ls5.cs.uni-dortmund.de seidl@informatik.tu-muenchen.de Abstract We apply linear \nalgebra techniques to precise interprocedural data.ow analysis. Speci.cally, we describe analyses that \ndeter\u00admine for each program point identities that are valid among the program variables whenever control \nreaches that program point. Our analyses fully interpret assignment statements with af.ne ex\u00adpressions \non the right hand side while considering other assign\u00adments as non-deterministic and ignoring conditions \nat branches. Under this abstraction, the analysis computes the set of all af.ne relations and, more generally, \nall polynomial relations of bounded degree precisely. The running time of our algorithms is linear in \nthe program size and polynomial in the number of occurring variables. We also show how to deal with af.ne \npreconditions and local vari\u00adables and indicate how to handle parameters and return values of procedures. \nCategories and Subject Descriptors: F.3.1 [Logics and Mean\u00adings of Programs]: Specifying and Verifying \nand Reasoning about Programs; D.3.3 [Programming Languages]: Language Constructs and Features procedures, \nfunctions, and subroutines; D.3.4 [Pro\u00adgramming Languages]: Processors compilers; D.3.4 [Program\u00adming \nLanguages]: Processors optimization General Terms: algorithms, theory, veri.cation. Keywords: interprocedural \nanalysis, linear algebra, weakest pre\u00adcondition, af.ne relation, polynomial relation. 1 Introduction \nThe .eld of program analysis is concerned with designing algo\u00adrithms that compute information about the \ndynamic behavior of programs by a static analysis. Such information is useful in many * On leave from \nUniversit\u00a8at Dortmund, FB 4, LS 5, 44221 Dort\u00admund, Germany. Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. POPL 04, January 14 16, 2004, Venice, Italy. \nCopyright 2004 ACM 1-58113-729-X/04/0001 ...$5.00 circumstances. Important application areas are optimizing \ncom\u00adpilers and validation or veri.cation of programs. An often used simpli.cation is to work with intraprocedural \nor context-insensitive analyses. Intraprocedural analyses treat bodies of single procedures (or methods) \nin isolation, while context-insensitive analyses assume conservatively that a procedure called at one \ncall site may return to any other call site of this procedure. The context-insensitive ap\u00adproach, though \ninterprocedural, still is limited in the quality of the computed information: if only weak information \nabout one partic\u00adular calling context of a procedure or method is available, this may affect all other \ncalling contexts. The design of context-sensitive in\u00adterprocedural analyses that mirror the actual call/return \nbehavior of programs is generally deemed to be challenging. Here, if we speak about interprocedural analyses \nwithout further quali.cation, we al\u00adways will mean context-sensitive ones. In this paper, we show how \nlinear algebra techniques can be used for interprocedural .ow analysis. Our speci.c goal is to determine \nfor each program point af.ne and, more generally, polynomial re\u00adlations that are valid among the program \nvariables whenever con\u00adtrol reaches that program point. An af.ne relation is a condition of the form \na0 +.ni+1 aixi =0, where a0,...,an are constants and x1,...,xn are program variables; a polynomial relation \nis a condi\u00adtion of the form p(x1,...,xn)=0, where p is a multi-variate poly\u00adnomial in x1,...,xn. We call \nan analysis precise (w.r.t. a given class of programs) if it computes for every program point u of a \nprogram all valid relations of the given form which are valid along every fea\u00adsible program path reaching \nu. (A program path is called feasible if it mirrors the actual call/return behavior of procedures.) Looking \nfor valid af.ne and polynomial relations is a rather general question with many applications. First of \nall, many classical data .ow analysis problems can be seen as problems about af.ne and polynomials relations. \nSome examples are: .nding de.nite equali\u00adties among variables like x =y; constant propagation, i.e. detecting \nvariables or expressions with a constant value at run-time; discov\u00adery of symbolic constants like x =5y \n+2or even x =yz2 +42; detection of complex common sub-expressions where even expres\u00adsions are sought \nwhich are syntactically different but have the same value at run-time; and discovery of loop induction \nvariables. Karr [10] also discusses applications in connection with parallelization of do-loops. Af.ne \nand polynomial relations found by an automatic analysis routine are also useful in program veri.cation \ncontexts, as they pro\u00advide non-trivial valid assertions about the program. In particular, certain loop \ninvariants can be discovered in this way fully automat\u00adically. As af.ne and, even more, polynomial relations \nexpress quite complex relationships among variables, the discovered assertions may form the backbone \nof the program proof and thus signi.cantly simplify the veri.cation task. In this paper we consider af.ne \nprograms for which our analy\u00adsis will be precise, i.e., compute not some but all af.ne relations which \nare valid at a program point. We then extend this analysis to compute all valid polynomial relations \nup to a given degree d and to take af.ne preconditions into account completely. Af.ne programs differ \nfrom ordinary programs over integers in that they have non-deterministic (instead of conditional) branching, \nand con\u00adtain only assignments where the right-hand sides either equal ? denoting an unknown value, or \nare af.ne expressions such as in x3:=x1 - 3x2 +7. Clearly, in practice our analyses can also be applied \nto arbitrary programs simply by ignoring the conditions at branchings and simulating input operations \nand non-af.ne right\u00adhand sides in assignments through assignments of unknown values. To use linear algebra \nfor program analysis is not a new idea. In his seminal paper [10], Karr presents an intraprocedural analysis \nthat determines all intraprocedurally valid af.ne relations in an af.ne program. However, the potential \nof linear algebra has never been exploited fully. We extend Karr s work in three respects. Firstly, we \ndescribe a precise interprocedural analysis of af.ne programs. Secondly, we extend our algorithm to an \nalgorithm that computes all interprocedurally valid polynomial relations of degree bounded by some .xed \nd. Thirdly, we show how to treat local variables and indicate how to handle parameters and result values \nof procedures. Our base algorithm as well as the extended algorithms run in time linear in the program \nsize and polynomial in the number of program variables. The key observation onto which our algorithms \nare based is that the weakest precondition of an af.ne or polynomial relation a along a single run of \nan af.ne program can be determined by means of a lin\u00adear transformation applied to a. The set of all \nlinear transformations of a vector space again forms a vector space and we can compute for each program \npoint u the .nite-dimensional subspace generated by the linear transformations induced by the program \nruns reaching u. A relation a turns out to be valid at u if and only if the sub\u00adspace of linear transformations \ncomputed for u transforms a into 0 (or a relation implied by the precondition, respectively). This im\u00adplies \nthat the set of all valid relations can be computed as the set of solutions of a linear equation system. \nThus, .nite-dimensional subspaces of linear transformation describe the effect of procedure calls precisely \nenough. The program in Figure 1 illustrates the kind of properties our anal\u00adyses can handle. It consists \nof two procedures Main and P. After memorizing the (unknown) initial value of variable x1 in variable \nx2 and initializing x3 by zero, Main calls P. Procedure P can either terminate without changing any variable \nor call itself recursively. In the latter case, it increments x1 by x2 +1 and x3 by 1 before the recursive \ncall and decrements x1 by x2 afterwards. Therefore, the total effect of each instance of P with a recursive \ncall is to incre\u00adment both x1 and x3 by one. Thus, upon termination of the call to P in Main (i.e., at \nprogram point 3), x3 holds the number of recur\u00adsive calls of P and x1 the value x2 +x3. Consequently, \nthe .nal assignment in Main always assigns zero to x1. More formally, this amounts to saying that the \naf.ne relation x1 - x2 - x3 =0 is valid at program point 3 and that the af.ne relation x1 =0 is valid \nat program point 4. Another interesting relationship between the variables holds when\u00adever P is called. \nAs mentioned, variable x3 counts the number of recursive calls, and, thus, how often x1 has been incremented \nby Main: P: x2:=x1 x1:=x1 +x2 +1 x3:=0 x3:=x3 +1 P x1:=x1 - x2 - x3 x1:=x1 - x2 Figure 1. An example \nprogram. x2 +1. Consequently, at any call to P variable x1 holds the value x2 +x3(x2 +1)=x2x3 +x2 +x3. \nThis amounts to saying that the polynomial relation (of degree 2) x2x3 - x1 +x2 +x3 =0 is valid at program \npoints 2, 5 and 7. Related Work Unlike our algorithm, Karr s intraprocedural algorithm [10] works with \na forward propagation of af.ne spaces and uses quite compli\u00adcated subroutines to deal with join points \nand assignments xj :=t where the af.ne right-hand side depends on the variable xj on the left-hand side. \nSimilar to our approach, it abstracts non-af.ne as\u00adsignments and general guards. Due to the forward propagation \nstrategy, however, it is able to handle positive af.ne guards pre\u00adcisely. In [13] we observe that, in \nabsence of af.ne guards, check\u00ading a given af.ne relation for validity at a program point can be per\u00adformed \nby a simpler backward propagating algorithm which in turn is generalized to a backward propagating algorithm \nfor checking ar\u00adbitrary polynomial relations for polynomial programs (where poly\u00adnomial right hand side \nof assignments are interpreted) in [15, 14]. In a recent paper, Gulwani and Necula [8] present a probabilistic \nanalysis for .nding af.ne relationships. Their algorithm is also just intraprocedurally applicable but \nasymptotically faster than Karr s at the price of a (small) probability of yielding non-valid af.ne re\u00adlations. \nA generalization of these approaches to the interprocedural case is not obvious. The functional approach \nof Sharir/Pnueli [20, 11] to designing interprocedural data .ow analyses is limited to .nite lat\u00adtices \nof data .ow informations. Accordingly, it has successfully been applied to the detection of copy constants \n[17]. In copy con\u00adstant detection only assignments of the form x :=a are treated ex\u00adactly where a is \na constant or a variable. The lattice of af.ne spaces, however, is clearly in.nite. The call string approach \nof [20], on the other hand, is applicable to more general lattices but abstracts the call/return behavior \nof procedures. Thus, it does not lead to precise interprocedural analyses. In more recent work on precise \ninterpro\u00adcedural analysis, Horwitz et al. propose a polynomial-time algo\u00adrithm for detecting linear constants \n[9] interprocedurally. In lin\u00adear constant detection only those af.ne assignments are interpreted whose \nright-hand sides contain at most one occurrence of a vari\u00adable. We strictly improve on these results \nas our analyses treat all af.ne assignments exactly and determine more general properties. x :=x - 2 \n Figure 2. Another example program. A generalization of Karr s algorithm in another direction is the \nuse of polyhedra instead of af.ne spaces for approximately represent\u00ading sets of program states; the \nclassic reference is Cousot s and Halbwachs paper [6]. Polyhedra allow to determine besides af.ne equalities \nalso af.ne inequalities like 3x1 +5x2 = 7x3. Since the lattice of polyhedra has in.nite height, widenings \nmust be used to ensure termination of the analysis (see [2] for a recent discussion) making it unsuitable \nfor precise analyses. Sets of af.ne inequal\u00adities, however, allow to relate the values of variables before \nand after a procedure call (a relational analysis in the terminology of Cousot) thus naturally allowing \nfor an interprocedural general\u00adization. A relational analysis, however, that uses af.ne spaces or polyhedra \nfor approximating the relational semantics of procedures is not precise enough to detect all valid af.ne \nrelations in a pro\u00adgram with procedures. For a simple example see Figure 2. The true relational semantics \nof procedure P is described by the formula x =x0 . x =2 \u00b7 x0 - 2)where x0 represents the initial and \nx the .\u00adnal value of the variable. The best approximation of this relation by an af.ne space or polyhedron \nis described by the formula true. It is obvious that this approximation of P s semantics is too weak \nto detect that the af.ne relation x =2 is valid at program point 2 in procedure Main. The paper is organized \nas follows. In Section 2, we formally in\u00adtroduce the programs to be analyzed together with their semantics. \nIn Section 3, we introduce af.ne relations, their weakest precondi\u00adtions along a program run and explain \nour algorithm for this special case. In Section 4, we generalize our approach to deal with arbitrary \npolynomial relations of bounded degree. In Section 5, we extend our approach to procedures with local \nvariables and in Section 6 we show how to take into account af.ne preconditions completely.  2 Af.ne \nPrograms We model programs by systems of non-deterministic .ow graphs that can recursively call each \nother as in Figure 1. Let X = {x1,...,xk} be the set of (global) variables the program operates on. We \nuse x to denote the column vector1 of variables x =(x1,...,xk)t. We assume that the variables take values \nin a .xed .eld .. In prac\u00adtice, .is the .eld of rational numbers. Then a state assigning val\u00adues to the \nvariables is conveniently modeled by a k-dimensional (column) vector x =(x1,...,xk)t . . k ; xi is the \nvalue assigned to variable xi. Note that we distinguish variables and their values by using a different \nfont. For a state x, a variable xi and a value c . ., we write x[xi . . c]for the state (x1,...,xi-1,c,xi+1,...,xk)t \nWe assume that the basic statements in the program are either af.ne 1The superscript t denotes the transpose \noperation which mir\u00adrors a matrix at the main diagonal and changes a row vector into a column vector \n(and vice versa). assignments of the form xj :=t0 +.k 1 tixi (with ti . .for i = i=0,...,k and xj . X)or \nnon-deterministic assignments of the form x j :=? (with x j . X). Assignments xj :=x j have no effect \nonto the program state. They are also called skip statements and omitted in pictures. Non-deterministic \nassignments xj :=? represent a safe abstraction of statements in a source program our analysis cannot \nhandle precisely, for example of assignments xj :=t with non-af.ne expressions t or of read statements \nread(xj). Let Stmt be the set of basic statements. A program comprises a .nite set Proc of procedure \nnames that con\u00adtains a distinguished procedure Main. Execution starts with a call to Main. Each procedure \nname p . Proc is associated with a con\u00adtrol .ow graph Gp =(Np,Ep,Ap,ep,rp)that consists of: a set Np \nof program points;  a set of edges Ep . Np \u00d7 Np;  a mapping Ap : Ep . Stmt . Proc that annotates each \nedge with a basic statement of the form described above or a pro\u00adcedure call;  a special entry (or start) \npoint ep . Np; and  a special return point rp . Np.  We assume that the program points of different \nprocedures are dis\u00adjoint: Np n Nq =0/ for p This can always be enforced by re\u00ad =q. naming program points. \nWe write N for . p.Proc Np, E for . p.Proc Ep, and A for . p.Proc Ap. We agree that Base ={e | A(e). \nStmt} is the set of base edges and Callp ={e | A(e)= p} is the set of edges that call procedure p. The \ncore part of our algorithm can be understood as a precise ab\u00adstract interpretation of a constraint system \ncharacterizing the pro\u00adgram executions that reach program points. We represent program executions or \nruns by sequences of af.ne assignments. Formally, a run r is a .nite sequence r = s1;...;sm of assignments \nsi of the form xj := t where x j . X and t = t0 +.k 1 tixi for some t0,...,tk . .. We write Runs for \nthe set i=of runs. The set of runs reaching program point u . N can be char\u00adacterized as the least solution \nof a system of subset constraints on run sets (see, e.g., [19] for a similar approach for explicitly \nparal\u00adlel programs). We start by de.ning the program executions of base edges e in isolation. If e is \nannotated by an af.ne assignment, i.e., A(e)= x j :=t, it gives rise to a single execution: S(e)={xj \n:=t}. The effect of base edges e annotated by a non-deterministic assign\u00adment xj :=? is captured by all \nruns that assign some value from . to x j: S(e)={x j :=c | c . .}. Thus, we capture the effect of non-deterministic \nassignments by collecting all constant assignments. Next, we characterize same\u00adlevel runs. Same-level \nruns of procedures capture complete runs of procedures in isolation. As auxiliary sets we consider same-level \nruns of program nodes, i.e., those runs that reach a program point u in a procedure p from a call to \np on same-level, i.e., after all pro\u00adcedures called by p have terminated. The same-level runs of proce\u00addures \nand program nodes are the smallest solution of the constraint system S: where e denotes the empty run, \nand the operator ; denotes con\u00adcatenation of run sets. By [S1], the set of same-level runs of a pro\u00adcedure \nq comprises all same-level runs reaching the return point of q.By [S2], the set of same-level runs of \nthe entry point of a pro\u00adcedure contains the empty run. By [S3] and [S4], a same-level run for a program \npoint v is obtained by considering an ingoing edge e =(u,v). In both cases, we concatenate a same-level \nrun reaching u with a run corresponding to the edge. If e is a base edge, we con\u00adcatenate with an edge \nfrom S(e).If e is a call to a procedure p,we take a same-level run of p. Next, we characterize the runs \nthat reach program points. They are the smallest solution of the constraint system R: [R1] R(Main) .{e} \n[R2] R( p) .R(u) if (u, ) .Callp [R3] R(u) .R( p) ; S(u) if u .Np By [R1], the procedure Main is reachable \nby the empty path. By [R2], every procedure p is reachable by a path reaching a call of p.By [R3], we \nobtain a run reaching a program point u in some procedure p, by composing a run reaching p with a same-level \nrun reaching u. So far, we have furnished procedural .ow graphs with a symbolic operational semantics \nonly by describing the sets of sequences of assignments possibly reaching program points. Each of these \nruns gives rise to a transformation of the underlying program state x .. k . Every assignment statement \nxi := t induces a state transfor\u00admation [[xj := t]] : . k .. k given by [[x j := t]]x = x[x j .t(x)] \n, where t(x) is the value of term t in state x. This de.nition is induc\u00adtively extended to runs: [[e]] \n= Id, where Id is the identical mapping and [[ra]] = [[a]] .[[r]]. The state transformation of an af.ne \nassignment xj := t0 + .ki=1 tixi is an af.ne transformation. Hence, it can be written in the form k\u00d7 \n[[x j := t]]x = Ax + b with a matrix A .. k and a (column) vector b .. k . More speci.cally, A and b \nhave the form indicated below: Ij-1 . 0 . . 0 . A = . t1 ... tk . b = . t0 . (1) 0 Ik-j 0 Here, Ii is \nthe unit matrix with i rows and columns and 0 denotes zero matrices and vectors of appropriate dimension. \nIn b, t0 appears as j-th component. As a composition of af.ne transformations, the state transformer \nof a run is an af.ne transformation as well. For any run r, let Ar .. k\u00d7k and br .. k be such that [[r]]x \n= Arx + br. 3 Af.ne Relations and Weakest Preconditions An af.ne relation over a vector space . k is \nan equation a0 + a1x1 + ...akxk = 0 for some ai ... Geometrically, it can be viewed as a hyper-plane \nin the k-dimensional vector space . k . Such a relation can be represented as a polynomial of degree \nat most 1 (namely, the left-hand side) or, equivalently, as a column vector a =(a0,...,ak)t. In particular, \nthe set of all af.ne relations forms an .-vector space which is isomorphic to . k+1 . The vector y .. \nk satis.es the af.ne ' relation a iff a0 + a\u00b7y = 0 where a'=(a1,...,ak)t and \u00b7 denotes scalar product. \nWe write y |= a to denote this fact. Geometrically, this means that the point y is an element of the \nhyper-plane de\u00adscribed by a. The af.ne relation a is valid after a single run r iff [[r]]x |= a for ' \nall x .. k , i.e., iff a0 + a\u00b7[[r]]x = 0 for all x .. k ; x represents ' the unknown initial state. Thus, \na0 + a\u00b7[[r]]x = 0isthe weakest precondition for validity of the af.ne relation a after run r.We have \n' a0 + a\u00b7[[r]]x = 0 iff [Choice of Ar and br] ' a0 + a\u00b7(Arx + br )= 0 iff [Linearity, rearrangement] \n'' (a0 + a\u00b7br)+ a\u00b7Arx = 0 iff [Law x \u00b7Ay = Atx \u00b7y from linear algebra] '') \u00b7 (a0 + a\u00b7br)+(At ax = 0 \nr From this characterization we see that the weakest precondition is again an af.ne relation. Even better: \nThe mapping that assigns to each af.ne relation its weakest precondition before run r is the linear map \ndescribed by the following (k + 1) \u00d7(k + 1) matrix Wr: bt 1 r Wr = (2) At 0 r In particular, we have \nproved that for every x .. k : [[r]]x |= a iff x |= Wr a . (3) Thus, the matrix Wr provides us with a \n.nite description of the weakest precondition transformer for af.ne relations of a single program execution \nr. Note that the only af.ne relation which is true for all program states is the relation 0 =(0,...,0)t. \nThus, the af.ne relation a is valid after run r iff Wr a = 0, because the initial state is arbitrary. \nAccordingly, the af.ne relation a is valid at a program point u, iff it is valid after all runs r .R(u). \nSummarizing, we have: LEMMA 1. The af.ne relation a .. k+1 is valid at program point uiffWra = 0 for \nall r .R(u). Thus, the set W = {Wr |r .R(u)}gives us a handle to solve the validity problem for af.ne \nrelations. The problem is that we do not know how to represent W in a .nitary way let alone how to compute \nit. In this place, we recall from linear algebra that the set of (k + 1) \u00d7(k + 1) matrices again forms \nan .-vector space. The dimension of this vector space equals (k + 1)2. We observe: LEMMA 2. Let M denote \na set of n \u00d7n matrices. a) For every W .M, the set {a |Wa = 0}forms a subspace of n .. b) As an intersection \nof vector spaces, the set {a |.W .M : n Wa = 0}forms a subspace of .. c) For every a . . n , the following \nthree statements are equiva\u00adlent: Wa =0 for all W . M;  Wa =0 for all W . Span(M).  Wa =0 for all \nW in a basis of Span(M).  Here, Span(M)denotes the vector space generated by the elements in M, i.e., \nthe vector space of all linear combinations of elements in M. We conclude that we can work with Span(W \n), i.e., the sub\u00adspace of . (k+1)\u00d7(k+1) generated by W without losing interesting information. As a subspace \nof the vector space . (k+1)\u00d7(k+1) of di\u00admension (k +1)2, Span(W )can be described by a basis of at most \n(k +1)2 matrices. Indeed, due to the special form of the matrices Wr in the .rst column all but the .rst \nentry are zero Span(W ) can have at most dimension k2 +k +1. Based on these observations, we can determine \nthe set of all af.ne relations at program point u from a basis of Span({Wr | r . R(u)}) and estimate \nthe complexity of the resulting algorithm. For simplic\u00adity we use here and in the following unit cost \nmeasure for arithmetic operations. THEOREM 1. Assume we are given a basis B for the set Span({Wr | r \n. R(u)}). Then we have: a) Af.ne relation a . . k+1 is valid at program point u iff Wa =0 for all W . \nB. b) A basis for the subspace of all af.ne relations valid at pro\u00adgram point u can be computed in time \nO(k5). PROOF. Statement a) follows directly from Lemma 1 and Lemma 2,c). For seeing b), consider that \nby a) the af.ne relation a is valid at u iff a is a solution of all the equations k . wija j =0 j=0 for \neach matrix W =(wij). B and i =0,...,k. The basis B contains at most O(k2) matrices each of which con\u00adtributes \nk +1 equations. Thus, we must determine, the solution of an equation system with O(k3)equations over \nk +1 variables. This can be done, e.g. by Gaussian elimination, in time O(k5). So we are left with the \ntask to compute, for every program point u, (k+1)\u00d7(k+1) (a basis of) Span({Wr | r . R(u)}). This subspace \nof . can be seen as an abstraction of the set R(u)of program executions reaching u. We are going to compute \nit by an abstract interpretation of the constraint system for R(u)from Section 2. Recall that the set \nof subspaces of a .nite-dimensional .-vector space V forms a complete lattice (w.r.t. the ordering set \ninclusion) where the least element is given by the 0-dimensional vector space consisting of the 0-vector \nonly. The least upper bound of two spaces V1,V2 is given by: V1 V2 = Span(V1 .V2) = {v1 +v2 | vi . Vi}. \nWe denote the complete lattice of subspaces of V by Sub(V ). The height of Sub(V ), i.e., the maximal \nlength of a strictly increasing chain, equals the dimension of V . The desired abstraction of run sets \nis described by the mapping a : 2Runs . Sub((k+1)\u00d7(k+1)): . a(R)=Span({Wr | r . R}). Thus, we have: a(0/)= \nSpan(0/)= {0}a({r})= Span({Wr}) for a single run r. By Equation (2) we get for the empty run, a({e})= \nSpan({Ik+1}) because Ae =Ik and be =0. The mapping a is monotonic (w.r.t. subset ordering on sets of \nruns and subspaces.) Also it is not hard to see that it commutes with arbitrary unions. In order to solve \nthe constraint system for the run sets R(u)over abstract domain Sub(. (k+1)\u00d7(k+1)), we need adequate \nabstract ver\u00ad sions of the operators and constants in this constraint system. In particular, we need \nan abstract version of the concatenation of run sets. For M1,M2 . . (k+1)\u00d7(k+1),we de.ne: M1 . M2 =Span({A1A2 \n| Ai . Mi}). First of all, we observe: LEMMA 3. For all sets of matrices M1,M2, Span(M1). Span(M2)=M1 \n. M2 . PROOF. Observe .rst that Span(Mi). Mi and therefore, Span(M1). Span(M2). M1 . M2 by monotonicity \nof . . For the reverse inclusion, consider arbitrary elements Bi =. j .(i)\u00b7 j (i)(i) Ain Span(Mi)for \nsuitable A. Mi. Then jj .(1).(2)(1)(2) B1B2 =.. mj \u00b7 Am Aj mj (1)(2) by linearity of matrix multiplication. \nSince each Am Ais con\u00ad j tained in M1 .M2, B1B2 is contained in M1 .M2 as well. Therefore, also the inclusion \n. follows. Accordingly, a generating system for M1 . M2 can be computed from generating systems G1,G2 \nfor M1 and M2 by multiplying each matrix in G1 with each matrix in G2. Secondly, we observe that . precisely \nabstracts the concatenation of run sets: LEMMA 4. Let R1,R2 . Runs. Then a(R1). a(R2)=a(R1; R2). PROOF. \nConsider the auxiliary map W mapping run sets to sets of matrices by: W (R)={Wr | r . R}. Then we have \na(R)=Span(W (R)). We observe: This suf.ces as the span construction commutes with composition by Lemma \n3. Let us now turn attention to the abstraction of base edges. Let us .rst consider a base edge e . Base \nannotated by an af.ne assignment, i.e., A(e)= xj :=t where t = t0 +.n Then i=1 tixi. S(e)={ x j :=t} \n. By (1) and (2), the corresponding abstract trans\u00ad former is given by a(S(e)) = a({ x j :=t} ) . . .. \n.. 0 . t0 . Ij . . .. .. . . = Span.. --- .. . --- . . . . 0 tk Ik- j Informally, the weakest precondition \nfor an af.ne relation a . . k+1 is computed by substituting t into xj of the corresponding af.ne combination. \nNext, consider a base edge e . Base annotated with xj :=?. In this case, S(e)={ x j :=c | c . .} implying \nthat we have to abstract an in.nite set of runs if the .eld .is in.nite. Clearly, the abstraction of \nthis set again can be .nitely represented. We obtain this repre\u00adsentation by selecting two different \nvalues from ., e.g., 0 and 1. We .nd: LEMMA 5. a(S(e)) = a({ x j :=c | c . .} ) = Span({ T0,T1} ), where \nTc =Wxj :=c is the matrix obtained from Ik+1 by replacing the t j +1-th column with (c,0,...,0). PROOF. \nOnly the second equation requires a proof. From Equa\u00adtions (1) and (2) we get a({ xj :=c | c . .} )=Span({ \nTc | c . .} ). We verify: Tc =(1 - c)\u00b7 T0 +c \u00b7 T1. Hence, Tc . Span({ T0,T1} ) and Span({ Tc | c . .} \n)=Span({ T0,T1} ). From the constraint systems S and R for run sets, we construct now the constraint \nsystems Sa and Ra by application of a. The variables (k+1)\u00d7 (k+1) in the new constraint systems take \nsubspaces of . as values. We apply a to the occurring constant sets { e} and S(e)and replace the concatenation \noperator ; with . : Sa(q) . Sa(rq) Sa(eq) . Span({ Id} ) Sa(v) . Sa(u). a(S(e)) if e =(u,v). Base Sa(v) \n. Sa(u). Sa(p) if e =(u,v). Callp Ra(Main). Span({ Id} ) Ra(p) . Ra(u) if (u, ). Callp Ra(u) . Ra(p). \nSa(u) if u . Np The resulting constraint system can be solved by computing on bases. For estimating the \ncomplexity of the resulting algorithm, we assume that the basic statements in the given program have \nsize O(1). Thus, we measure the size n of the given program by | N| +| E| . Note that program nodes typically \nhave bounded out\u00addegree, such that typically | N| +| E| =O(| N| ). THEOREM 2. For every program of size \nn with k variables the following holds: a) The values: Span({Wr | r . S(u)} ),u . N, Span({Wr | r . S(p)} \n),p . Proc, Span({Wr | r . R(u)} ),p . Proc, and Span({Wr | r . R(u)} ),u . N, are the least solutions \nof the constraint systems Sa and Ra, respectively. b) These values can be computed in time O(p \u00b7 k8). \nc) The sets of all valid af.ne relations at program point u, u . N, can be computed in time O(n \u00b7 k8). \nPROOF. Statement a) amounts to saying that the least solution of constraint systems Sa and Ra is obtained \nfrom the least solution of S and R by applying the abstraction a. This follows from the Trans\u00adfer Lemma \nknown in .xpoint theory (see, e.g., [1, 4]), which can be applied since a commutes with arbitrary unions, \nthe concatenation operator is precisely abstracted by the operator . (Lemma 4), and the constant run \nsets { e} and S(e)are replaced by their abstractions a({ e} )=Span({ Id} )and a(S(e)), respectively. \nFor b) we show that the least solution of the abstracted constraint systems can be computed in time O(n \n\u00b7 k8). For that, recall that the lattice of all subspaces of . (k+1)\u00d7 (k+1) has height (k +1)2. Thus, \na worklist-based .xpoint algorithm will evaluate at most O(n \u00b7 k2) constraints. Each constraint evaluation \nconsists of multiplying two sets of at most (k +1)2 matrices. The necessary (k +1)4 matrix multiplications \ncan be executed in time O(k7). Finally, we must compute a basis for the span of the resulting (k +1)4 \nmatrices. By Gaussian elimination, this can be done in time O(k8). Altogether, \u00b7 k10) we obtain an upper \ncomplexity bound of O(n \u00b7 k2 \u00b7 k8)=O(n . A better running time can be obtained if we use a semi-naive \n.x\u00adpoint iteration strategy [16, 3, 7]. The idea here is that when the value of a .xpoint variable changes, \nwe do not propagate the com\u00adplete new value to all uses of the variable in right-hand sides of constraints \nbut just the increment, i.e., in our case the new matri\u00adces extending the current basis (instead of the \ncomplete new basis). The total time spent with a constraint then sums up to O(k8)which overall results \nin the desired complexity O(n \u00b7 k8). Finally, for c) we recall that we know from Theorem 1 that, from \nbases of Span({Wr | r . R(u)} ) for all program points u, we can compute the sets of all valid af.ne \nrelations within the stated com\u00adplexity bounds. Let us consider the example program from Figure 1 for \nillustra\u00adtion. Due to lack of space, we cannot describe the .xpoint iteration in detail or give the full \nresult. However, we report and discuss some characteristic values. The .xpoint iteration for Sa stabilizes \nafter 3 iterations. We obtain: Sa(P)=Sa(9)=Span({ I4,W1} )and Sa(3)=Span({W1,W2} ), where W1,W2 are the \nmatrices . .. . 0101 1000 . 0000 .. 0110 . W1 = .. W2 = .. 0000 0000 0000 0000 Also, Sa(Main)=Sa(4)=Span({W3,W4}), \nwhere . .. . 1000 0001 . 0010 .. 0000 . W3 = .. W4 = .. 0000 0000 0000 0000 As there are no recursive \ncalls to Main, reaching runs and same\u00adlevel runs coincide for the program points of Main. Consequently, \nwe have, Ra(3)=Sa(3)=Span({W1,W2}). Hence, at program point 3 just the af.ne relations a =(a0,...,ak)t \nwith W1a =0 and W2a =0 are valid which reduces to the requirements a0 =0 and a2 =a3 =-a1. Therefore, \njust the af.ne relations of the form a1x1 -a1x2 -a1x3 =0 are valid at program point 3, in particular, \nx1 -x2 -x3 =0 which con.rms our informal reasoning from the introduction. For program point 4 we have \nRa(4)=Sa(4)=Span({W3,W4}). Here, the requirements W3a =0 and W4a =0 reduce to a0 =a2 = a3 =0. Thus, just \nthe af.ne relations of the form a1x1 =0 are valid at program point 4, in particular, x1 =0. Again this \ncon.rms our informal reasoning that x1 is a constant of value zero. The computation of Ra for the program \npoints of P stabilizes again after 3 iterations. For the program point 7 just before the recursive call \nto P, we obtain Ra(7)=Span({W5,W6}), where . .. . 1000 0101 . 0110 .. 0100 . W5 = .. W6 = .. 0000 0000 \n0000 0000 Here the conditions W5a =0 and W6a =0 for valid af.ne relations translate into a0 =a1 =a2 =a3 \n=0. Interestingly, this implies that no non-trivial af.ne relation is valid at every call to P. In order \nto .nd out about validity of the polynomial relation x2x3 - x1 +x2 +x3 =0 at program point 7, hinted \nupon in the introduction, we must generalize our analysis to polynomial relations which is the topic \nof the next section. 4 Polynomial Relations of Bounded Degree Polynomial relations are much more expressive \nthan af.ne rela\u00adtions. In particular, they are closed under disjunction: p =0.q =0 holds if and only \nif pq =0. For example, the relation: (x1 -1)\u00b7(x1 -x2)=0 represents the disjunction of the two af.ne relations: \nx1 -1 =0 .x1 -x2 =0. Also, the property whether a variable xj has a value in a given .nite set {c1,...,cr}..with \nr elements can be expressed by a polyno\u00admial relation: (x j -c1)\u00b7...\u00b7(x j -cr)=0. Formally, a polynomial \nrelation over a vector space . k is an equa\u00adtion p =0 where p is a polynomial over the unknowns X, i.e., \np ..[X]. The vector y .. k satis.es the polynomial relation p =0, y |=p for short, iff p[y/x]=0 where \n[y/x]denotes the substitution of the values yi for the variables xi. The set of polynomials .[X] forms \nan .-vector space. However, as the dimension of this vector space is in.nite, we cannot effec\u00adtively \ncompute with bases. One way out is to restrict attention to polynomials of bounded degree. The degree \nof a polynomial p (or the polynomial relation p =0) is the maximal sum j1 +...+jk of j1 jk exponents \nof a monomial ax1 ...xk occurring in p. We denote the set of polynomials of degree at most d by .=d [X]. \n(k+d).=d [X] is an .-vector space of dimension =O((k +d)d ): d j1 jk obviously, the monomials x1 ...xk \n..=d [X] with coef.cient 1 form a basis of .=d [X] and we prove momentarily by induction (k+d)that there \nare such monomials. For d =0or k =0 there d is just the single monomial x10 ...xk 0 or 1, respectively, \nand indeed (k)(d)0=d=1. So assume d > 0or k > 0. By induction hypothesis (k+d-1)there are d-1 monomials \nof degree less than d. The monomials (k-1+d)with degree d over k variables are obtained from the d mono\u00adj1 \njk-1 mials x ...xk-1 of degree at most d over the .rst k -1 variables 1 by multiplying with xlk where \nlk =d -.k-11 li. Altogether, there ki= (k+d-1)(k-1+d)(k+d)are thus +=monomials with coef.cient 1 d-1 \ndd of degree at most d. The polynomial relation p =0 is valid after a single run r iff for all x .. k \n, p[[[r]]x/x]=0 or, equivalently, p[(Arx +br)/x]=0 where Ar, br are de.ned as in Section 2. Thus, p[(Arx \n+br)/x]=0 is the weakest precondition for validity of p =0 after run r. We observe: LEMMA 6. 1. The polynomial \np[(Arx +br )/x]is again of de\u00adgree at most d. (d) 2. The mapping Wr which maps polynomials p of degree \nat mostdto p[(Arx +br )/x]is linear. PROOF. For a proof of the .rst statement, it suf.ces to consider \na run r =xi :=t, t =t0 +.k 1 tmxm, of a single assignment and a m=j1 jk single monomial p =x...xk . Then \n1 p[(Arx +br)/x]= p[t/xi] () ji .0 .k = ..0+...+.k =ji \u00b7t...t\u00b7 .0,...,.k 0 k j1+.1 ji-1 +.i-1 .i ji+1 \n+.i+1 jk +.k x ...x xx ...x , 1 i-1 ii+1 k ( ji )where the are the multinomial coef.cients for the ji-th \n.0,...,.k power of a sum on k +1 summands. Since in each monomial of the result, .0 +...+.k = ji, the \ndegree of p[t/xi]is bounded by j1 +...+jk, i.e., the degree of p. The second assertion follows since \nsubstitution commutes with sums and constant multiples. The only polynomial relation which is true for \nall program states is the zero relation 0 =0. As for af.ne relations, we conclude that the (d) polynomial \nrelation p =0 is valid after run r iff Wr p =0 (where 0 denotes the zero polynomial). Summarizing, we \nhave: LEMMA 7. The polynomial relation p of degree at most d is valid (d) at program point u iff Wr p \n=0 for all r .R(u). Now we can proceed analogously to Section 3. By applying (d) Lemma 2, we can safely \nreplace the set {Wr |r .R(u)} with its span. The resulting subspace of linear mappings can be de\u00adscribed \nby a basis of at most O((k +d)2d )matrices. The entries of these matrices are now indexed by pairs of \ntuples J =(j1,..., jk), .k 1 ji = d. Let I denote the set of all such tuples. We determine i=the set \nof all valid polynomial relations at program point u for poly\u00ad nomials of degree at most d as follows: \nTHEOREM 3. Assume we are given a basis B for the set (d) Span({Wr | r . R(u)}). Then we have: a) The \npolynomial relation p =0 of degree at most d is valid at program point u iff W p =0 for all W . B. b) \nA basis of the subspace of all polynomial relations of degree at most d valid at program point u can \nbe computed in time O((k +d)5d ). PROOF. Statement a) follows directly from Lemma 2 and Lemma 7. For \nthe proof of b), note that by a) the polynomial relation p =0 j1 jk is valid at u iff p = .J=(j1,...,jk \n).I aJ x ...xk , where the aJ ,J . I , 1 are a solution of the equation: . wIJ aJ =0 J.I for every matrix \nW =(wIJ ). B and every I . I . The basis B may contain at most O((k +d)2d ) matrices each of which contributes \nO((k +d)d )equations. Thus, we have to compute the solution of an equation system with O((k +d)3d )equations \nover O((k +d)d ) variables. This can be done in time O((k +d)5d ). By Theorem 3, it suf.ces to compute, \nfor every program point u, (d) the span of the set of all precondition transformers Wr , r . R(u). We \ndo so by abstracting the run sets to subspaces of linear transfor\u00admations now of polynomials of degree \nat most d. The abstraction is thus given by: (d) a(d)(R)=Span({Wr | r . R}). As in the case of af.ne \nrelations, we have: a(d)(0/)= Span(0/)= {0} (d) a(d)({r})= Span({Wr }) for a single run r. In particular, \na(d)({e})= Span({II }), where II is the diagonal matrix describing the identity. The map\u00adping a(d) is \nagain monotonic (w.r.t. subset ordering on sets of runs and subspaces) and commutes with arbitrary unions. \nAlso, Lemma 4 analogously holds for a(d). Therefore, the desired values can be computed by abstracting \nthe constraint systems for same\u00adlevel and reaching run sets. In order to obtain an effective algo\u00adrithm, \nit remains to derive explicit abstractions for the effects of base edges. Forade.nite assignment xj :=t, \nthis is obviously possible. It re\u00admains to consider a base edge e . Base annotated by xj :=? with S(e)={x \nj :=c | c . .}. In case .contains less than d +1 elements, the set S(e)is also .nite and we simply may \nenumerate it. More interesting is the case when .has at least d +1 elements, e.g., because .has characteristic \n0. i Each polynomial p . .=d [X]can be written as p = .d 0 pi \u00b7 xj for i=polynomials pi not containing \nxj. The coef.cient polynomials pi have at most degree d and are uniquely determined by p. For 0 = i = \nd let Ci be the mapping on .=d [X]that maps each p to its i-th coef.cient polynomial, i.e., Ci(p)=pi. \nIt is not hard to see that Ci is a linear map and hence can be represented by a matrix. We .nd that a(d)(S(e))can \nbe .nitely represented by C0,...,Cd : LEMMA 8. If .has more than d elements, then a(d)(S(e)) = a(d)({x \nj :=c | c . .}) = Span({Cl | l =0,...,d}). PROOF. From the de.nitions we have a(d)(S(e))=a(d)({x j :=c \n| (d) c . .})=Span({Wc | c . .}). It remains to show that this span xj :=equals Span({Cl | l =0,...,d}). \nFor this we show: (d) 1. Wc . Span({Cl | l =0,...,d})for all c . .. xj := (d) 2. Cl . Span({Wc | c . \n.})for l =0,...,d. xj := To 1: For arbitrary p . .=d [X]we have d (d) i Wxj :=c(p)= p[c/xj]= .Ci(p)c. \ni=0 (d) This means Wxj :=c =.d 0 ciCi, which implies 1. i= To 2: Since the cardinality of .is at least \nd +1, we can .nd d +1 distinct elements c0,...,cd . ..De.ning matrix A by .. 1 c0 ... c0 d . 1 c1 ... \ncd . . 1 . A = . ... . . . ... . . . 1 cd ... cdd .. . it is not hard to see, that ... . C0(p) p[c0/x \nj ] . . .. . . A. . . = . . . .. Cd (p) p[cd /x j] The determinant of A is an instance of what is known \nas Vander\u00admonde s determinant and has the value .0=i<l=d (cl -ci). As all ci are distinct, the determinant \nis different from 0. Therefore, matrix A is invertible and for the inverse matrix A-1 =(bil ),wehave \n... . C0(p) p[c0/x j ] . . .. . . . . . = A-1 . . . . . Cd (p) p[cd /x j] Thus, Ci(p)=.d 0 bil p[cl \n/x j ]=.d 0 bilWx(jd:=) cl (p). This shows l=l= (d) Ci =.d , which implies 2. l=0 bilWxj :=cl Analogously \nto the last section, we construct constraint systems Sa(d), Ra(d) which are obtained from the constraint \nsystems S and R by applying a(d). We conclude: THEOREM 4. For every program of size n with k variables \nthe following holds: a) The values: (d) Span({Wr | r . S(u)}),u . N, (d) Span({Wr | r . S(p)}),p . Proc, \n(d) Span({Wr |r .R( p)}),p .Proc, and (d) Span({Wr |r .R(u)}),u .N, are the least solutions of the constraint \nsystems Sa(d) and Ra(d) , respectively. b) The sets of all valid polynomial relations of degree at most \nd at program point u, u .N, can be computed in time O(n \u00b7(k + d)8d ). Consider again the example program \nfrom the introduction. Since it uses three program variables, the vector-space of polynomials of (3+2) \ndegree at most 2 has dimension 2 = 10. Assume we have or\u00addered the index tuples of monomials lexicographically \nas follows: (0,0,0) .(0,0,1) .....(1,1,0) .(2,0,0) . Then the pre-condition transformer, e.g., of the \nassignment x1:= x1 + x2 + 1 is given by the matrix: .. 1000001001 . 0100000100 . .. . 0010000000 . .. \n0001001012 .. .. 0000100100 .. . 0000010011 . .. . 0000001002 . .. . 0000000100 . .. 0000000012 0000000001 \nWe refrain from describing the details of the .xpoint iteration. The least .xpoint computation for analyzing \nthe valid quadratic rela\u00adtions at the entry of procedure p stabilizes after three iterations with three \nmatrices. The rows of these matrices span a vector space of dimension 9 and have the (coef.cients of) \nthe relation x2x3 -x1 + x2 + x3 = 0 as their only non-trivial solution (up to constant multiples, of \ncourse). Again, this con.rms our informal reasoning from the introduction. 5 Local Variables So far \nwe have considered programs which operate on global vari\u00adables only. In this section, we explain how \nour techniques can be extended to work on procedures with global and local variables. For notational \nconvenience, we assume that all procedures have the same set X = {x1,...,xm}of variables where the .rst \nk are global and the remaining m -k are local. For describing program execu\u00adtions, it now no longer suf.ces \nto consider execution paths. Instead, we have to take the proper nesting of calls into account. Therefore, \nsame-level runs s and reaching runs r are now .nite sequences of (unranked) trees b and, possibly, enter: \nst ::= x j := t | call(s) s ::= st1;...;stn (n =0) rt ::= x j := t | call(s)| enter r ::= rt1;...;rtn \n(n =0) Trees represent base actions or complete executions of procedures. Same-level runs represent sequences \nof such completed executions, while reaching runs may enter a procedure without ever leaving it again. \nThe set of runs reaching program point u .N can again be charac\u00adterized as the least solution of a system \nof subset constraints on run sets. If e is annotated by an af.ne assignment, i.e., A(e) =xj := t, we \nagain de.ne: S'(e)= {xj := t}. Similarly for A(e) =x j :=?, S '(e)= {xj := c |c ..}. The same-level runs \nof procedures and program nodes are the smallest solution of the following constraint system S' : [S'1] \nS'(q) .S'(rq) [S'2](eq) .{e} S' [S'3] S'(v) .S'(u) ; S'(e) if e =(u,v) .Base [S'4] S'(v) .S'(u) ; call(S'( \np)) if e =(u,v) .Callp Note that, for convenience, the application of the constructor call to all sequences \nof a set S is denoted by call(S). Constraints [S'1], [S'2] and [S'3] are as in Section 2. The new constraint \n[S'4] deals with calls. If the ingoing edge e =(u,v) is a call to a procedure p, we concatenate a same-level \nrun reaching u with a tree constructed from a same-level run of p by applying the constructor call. For \ncharacterizing the runs that reach program points and proce\u00addures, we construct the constraint system \nR' : R' [R'1](Main) .{e} [R'2] R'( p) .R'(u) , if (u, ) .Callp [R'3] R'(u) .R'( p) ; {enter}; S'(u) , \nif u .Np Constraints [R'1] and [R'2] are as in Section 3. The only modi.\u00adcation occurs in [R'3] where \nan enter is inserted between the run reaching the current procedure p and the same-level run inside p. \nEach of these runs gives rise to a transformation of the underlying m program state x ... Here, we just \nexplain how the transforma\u00adtions of enter and call(s)are obtained. The transformation [[enter]] passes \nthe values of the globals xj ( j = 1,...,k) and sets the locals x j, j >k,to0.2 Thus, ( ) 0 Ik [[enter]] \n= [[xk+1:= 0;...;xm := 0]] = . 0 0 Let us denote this m \u00d7m matrix by E ' . The transformation [[call(s)]] \nis more complicated. Like [[enter]],it must pass the values of the globals into the execution of the \ncalled procedure and initialize its local variables. In addition, it must re\u00adturn the values of the globals \nto the calling context and restore the values of the local variables. Given that [[s]] x = Asx + bs as \nin Sec\u00adtion 2, we de.ne: ' [[call(s)]] x = E '([[s]](E ' x)) + Tx ' =(E ' AsE ' + T ) x + E ' bs , () \n0 0 ' where T is the m \u00d7m matrix . The outermost appli\u00ad 0 Im-k cation of E 'in the .rst summand prohibits \npropagation of the called procedure s local variables and the second summand, T ' x bypasses the values \nof the local variables of the calling context. The above calculation shows that [[call(s)]] is an af.ne \ntransformation as well. 2By convention, local variables are initialized by 0. Other con\u00adventions could \neasily be modeled as well. Uninitialized local vari\u00adables as in C, for instance, can be handled by adding \nxj :=? state\u00adments for j = k + 1,...,m at the beginning of each procedure body. We want to determine \nfor every (reaching or same-level) run the transformation which produces the weakest precondition. For \nsim\u00adplicity, we construct the weakest precondition transformer only for af.ne relations. The weakest \nprecondition transformer for enter is given by: ( ) 0Wenter =Wxk+1:=0;...;xm:=0 = Ik+1 . 0 0 Let E denote \nthis matrix. To obtain analogous results as in Sec\u00adtion 3, we determine the weakest precondition transformation \nof call(s).We de.ne an operator . : . (m+1)\u00d7(k+1) .. (m+1)\u00d7(k+1) on (m +1)\u00d7(m +1)matrices by: .(W )=EWE \n+w \u00b7T where w is the element in the left upper corner of W , and T is the ( ) 0 0 (m +1)\u00d7(m +1)matrix \n.0 Im-k The operator . returns a linear transformation and is itself linear. This implies that . maps \nsubspaces of . (k+1)\u00d7(k+1) to subspaces of . (k+1)\u00d7(k+1) and, considered as a mapping on subspaces, com\u00ad \nmutes with arbitrary least upper bounds. Moreover, we have: LEMMA 9. Let Ws denote the precondition transformer \nfor s. mm Then for an af.ne relation a ..and a program state x .., [[call(s)]]x |=aiff x |=.(Ws)a. Thus, \nWcall(s) =.(Ws)is the weakest precondition transformer for call(s). In order to furnish the same approach \nas for global vari\u00adables, we de.ne the abstraction function a for sets R of (same-level or reaching) \nruns by: a(R)=Span({Wr |r .R}) In particular, a({enter})=Span({E}). Analogously to Lemma 4, we .nd: LEMMA \n10. For every set S of same-level runs, a(call(S))=a({call(s)|s .S})=.(a(S)). Finally, we construct constraint \nsystems S' a and R' a from S' and R' by applying a where concatenation is replaced with . and the constructor \ncall is replaced with . . Then we obtain our main theorem for programs with local variables: THEOREM \n5. For a program of size n with m global and local variables the following holds: a) The values: Span({Ws \n|s .S'(u)}),u .N, Span({Ws |s .S'(p)}),p .Proc, Span({Ws |s .R'(p)}),p .Proc, and Span({Wr |r .R'(u)}),u \n.N, are the least solutions of the constraint systems S' a and R' a, respectively. b) The sets of all \nvalid af.ne relations at program point u, u .N, can be computed in time O(n \u00b7m8). Our technique can be \nadapted to procedures with parameters. Value parameters, for instance, can be simulated via a scratch \npad of glob\u00adals through which the actual parameters are communicated from the caller to the callee. Return \nvalues can be treated similarly.  6 Af.ne Preconditions The analyses considered so far assume that we \nhave no knowledge whatsoever about the initial state in which the program is started. However, in a veri.cation \ncontext we are often in a more lucky sit\u00aduation when we are given a precondition that constrains potential \ninitial states. Of course, if less initial states are possible more rela\u00adtions may be valid at the nodes \nof a program and an analyses that ignores the precondition may be overly pessimistic. In this section \nwe extend the analyses of Section 3 and Section 4 to take into ac\u00adcount af.ne preconditions completely. \nThe analyses of this section thus compute for each program point of an af.ne program the space of all \nthose af.ne or polynomial relations that are valid whenever the program is started in a state satisfying \na given af.ne precondi\u00adtion. Assume given a .nite set Pre .Fk+1 of af.ne relations, repre\u00adsenting the \naf.ne precondition. We say that Pre is satis.able if k there is an x ..such that x |=h for all h .Pre.If \nPre is not satis.able, all relations are valid at all program points under pre\u00adcondition Pre. As we can \ncheck whether Pre is satis.able or not with the aid of Gaussian elimination, we can detect this trivial \ncase. Thus, we assume without loss of generality that Pre is sat\u00adis.able in the following. In this case, \nthe set of states satisfying Pre, Sat(Pre)={x .. k |x |=h,h .Pre},is anaf.ne subspace of . k and can \nbe represented in the form Sat(Pre)=x0 +L, where x0 .Sat(Pre) and L is a (linear) subspace of . k . Assume \nthat x1,...,xl with l =k is a basis of L. Then we have l Sat(Pre)={x0 +..rxr |.1,...,.l ..}. (4) r=1 \nVectors x0,...,xl .. k with this property can be computed from Pre with standard techniques from linear \nalgebra. Obviously, an af.ne relation a is valid at a program point u un\u00adder precondition Pre, iff its \nweakest precondition for each program path r reaching u is valid for all x .Sat(Pre), i.e., if x |=Wr \na for all r .R(u), x .Sat(Pre). By the characterization of Sat(Pre)in Equation 4, we thus have: LEMMA \n11. The af.ne relation a . . k+1 is valid at program point u under precondition Pre iff x0 +.l 1 .rxr \n|=Wra for all r=.1,...,.l ..,r .R(u). By arguing analogously to Section 3 we can equivalently require \nthis property for all matrices W in a basis of Span{Wr |r .R(u)}. Thus, we obtain the following generalization \nof Theorem 1: THEOREM 6. Assume we are given a basis B for the set Span({Wr |r .R(u)}). Then we have: \na) Af.ne relation a .. k+1 is valid at program point u under precondition Pre iff x0 +.l 1 .rxr |=Wa \nfor all .1,...,.l . r= .,W .B. b) A basis for the subspace of all af.ne relations valid at pro\u00adgram point \nu under precondition Pre can be computed in time O(k5). PROOF. As a) has already been justi.ed we prove \nonly b). By a) an af.ne relation a is valid at u if and only if for all W .B: l x0 +..rxr |=Wa for all \n.1,...,.l ... (5) r=1 By unfolding the de.nition of |= and writing W =(wij)and xi = presence of local \nvariables and parameter passing by value and re\u00ad(xi1,...,xik)t for i =0,...,l, Formula (5) means that \nsult. We also generalized our analyses to take af.ne preconditions into account. =0 . . . . . aj(w0 \nj + x0iwij)+ .r aj xriwij All our analyses run in polynomial time. More precisely, they are j=0 i=1 r=1 \nj=0 i=1 for all .1,...,.l . .. This is an af.ne equation in the .r whose linear in the program size and \npolynomial of a higher degree in the number of variables. It remains for future work to .nd out whether \n coef.cients are af.ne combinations of the aj. It is valid for all .r if this theoretical complexity \nbound is prohibitive to apply the anal\u00adand only if all these combinations are 0. Therefore, an af.ne \nrela-ysis in practice or in how far heuristic methods are necessary to tion a is valid at u under precondition \nPre if and only if it satis.es identify promising but suf.ciently small sets of variables to be in\u00adthe \nequations: cluded in the analysis. kk Instrumental for our approach is that we can capture the effect \n. a j(w0 j + . x0iwij)=0 of procedures as weakest precondition transformers for af.ne and j=0 i=1 polynomial \nrelations completely by subspaces of linear maps. This and provides us with a kind of abstract higher-order \ndenotation of kk procedures that we can compute in polynomial time and use at any . a j . xriwij =0 \nfor r =1,...,l call site. Similar in spirit is relational analysis of recursive proce\u00ad j=0 i=1 for all \nW =(wij). B. We can hence compute the subspace of all valid af.ne relations by setting up and solving \nthe linear equation system consisting of all these equations. Let us estimate the complexity of this \nprocedure. Each matrix W . B contributes l +1 =O(k) equations and there are at most O(k2) matrices in \nB. Hence the equation system has O(k3) equations. It is not hard to see, that the coef.cients of each \nequation can be computed in time O(k2). Hence the equation system can be set up in time O(k5). As a linear \nequation system with O(k3)equations in the k +1 variables a0,...,ak it can be solved, e.g., by Gaussian \nelimination in time O(k5). From Theorem 2 we know that we can compute a basis of Span({Wr | r . R(u)})in \ntime O(n \u00b7 k8). Together with Theorem 6 this implies: COROLLARY 1. The sets of all valid af.ne relations \nat program point u, u . N, under precondition Pre can be computed in time O(n \u00b7 k8). This approach for \ntreating af.ne preconditions can straightfor\u00adwardly be generalized to the setting of Section 4. Here \na polyno\u00admial relation of degree at most d turns out to be valid at a program point u under precondition \nPre if and only if for all W in a basis of (d) Span({Wr | r . R(u)}): l dures as proposed by Cousot [5, \n6] and the functional approach to interprocedural analysis [20, 11]. While these approaches rely on relations \nor functions, we capture the effects of procedures by .nitely representable sets of functions. Our results \nimprove on the analysis of linear constants by Hor\u00adwitz et al. [9, 17] and, upto the treatment of positive \naf.ne guards, also on the results obtained by Karr [10]. In a recent paper, Reps, Schwoon, and Jha [18] \nuse a library for reachability analysis of weighted pushdown systems for interprocedural data.ow analysis. \nThey report that G. Balakrishnan has created a prototype implemen\u00adtation of our interprocedural analysis \nfor af.ne relations based on a preliminary version of the current paper. The results of this paper are \nstill not strong enough to deal with positive af.ne guards as Karr s approach. Also, they do not gener\u00adalize \nour intraprocedural analysis in [15, 14] where we succeed in checking the validity of arbitrary polynomial \nrelations for polyno\u00admial programs even in presence of negative polynomial guards. It remains as challenging \nopen problems whether or not precise inter\u00adprocedural treatments of positive guards or precise interprocedural \nanalysis of polynomial programs are possible. Acknowledgments We would like to thank the anonymous POPL \nreviewers for their remarks that helped to improve our original submission. The .rst author would also \nlike to thank the members of IFIP Working Group 2.2 for their constructive comments on a talk about a \npre\u00ad .  .rxr |=Wp for all .1,...,.l . .. (6) liminary version of this paper. x0 + r=1 This time, this \ntranslates to a polynomial equation (of degree at most d)inthe .r whose coef.cients are af.ne combinations \nof the coef.\u00adcients of p. Again all these af.ne combinations must equal 0 which gives rise to a linear \nequation system that we can set up and solve. COROLLARY 2. The sets of all valid polynomial relations \nof de\u00adgree at most d at program point u, u . N, under precondition Pre can be computed in time O(n \u00b7 \n(k +d)8d ).  7 Conclusion We have presented an interprocedural analysis that determines for each program \npoint of an af.ne program the set of all valid af.ne relations. We generalized the algorithm to infer \nall polynomial rela\u00adtions of bounded degree and showed that our methods work also in 8 References [1] \nK. R. Apt and G. D. Plotkin. Countable Nondeterminism and Random Assignment. Journal of the ACM, 33(4):724 \n767, 1986. [2] R. Bagnara, P. Hill, E. Ricci, and E. Zaffanella. Precise Widening Operators for Convex \nPolyhedra. In 10th Int. Static Analysis Symposium (SAS), pages 337 354. LNCS 2694, Springer-Verlag, 2003. \n[3] I. Balbin and K. Ramamohanarao. A Generalization of the Differential Approach to Recursive Query \nEvaluation. Jour\u00adnal of Logic Programming (JLP), 4(3):259 262, 1987. [4] P. Cousot. Constructive Design \nof a Hierarchy of Seman\u00adtics of a Transition System by Abstract Interpretation. Elec\u00ad tronic Notes in \nTheoretical Computer Science, 6, 1997. URL: www.elsevier.nl/locate/entcs/volume6.html. [5] P. Cousot \nand R. Cousot. Static Determination of Dynamic Properties of Recursive Procedures. In E. Neuhold, editor, \nIFIP Conf. on For\u00admal Description of Programming Concepts, pages 237 277. North-Holland, 1977. [6] P. \nCousot and N. Halbwachs. Automatic Discovery of Linear Re\u00adstraints among Variables of a Program. In 5th \nACM SIGPLAN-SIGACT Symp. on Principles of Programming Languages (POPL), pages 84 97, 1978. [7] C. Fecht \nand H. Seidl. Propagating Differences: An Ef.cient New Fixpoint Algorithm for Distributive Constraint \nSystems. Nordic Jour\u00adnal of Computing (NJC), 5(4):304 329, 1998. [8] S. Gulwani and G. Necula. Discovering \nAf.ne Equalities Using Ran\u00addom Interpretation. In 30th Ann. ACM Symp. on Principles of Pro\u00adgramming Languages \n(POPL), pages 74 84, 2003. [9] S. Horwitz, T. Reps, and M. Sagiv. Precise Interprocedural Data.ow Analysis \nwith Applications to Constant Propagation. Theoretical Computer Science (TCS), 167(1&#38;2):131 170, \n1996. [10] M. Karr. Af.ne Relationships Among Variables of a Program. Acta Informatica, 6:133 151, 1976. \n[11] J. Knoop and B. Steffen. The Interprocedural Coincidence Theo\u00adrem. In Compiler Construction (CC), \npages 125 140. LNCS 541, Springer-Verlag, 1992. [12] S. S. Muchnick and N. D. Jones, editors. Program \nFlow Analysis: Theory and Applications. Prentice Hall, Engelwood Cliffs, New Jer\u00adsey, 1981. [13] M. M\u00a8uthing. \nThe Complexity of Constant Propa\u00ad uller-Olm and O. R\u00a8gation. In 10th European Symposium on Programming \n(ESOP), pages 190 205. LNCS 2028, Springer-Verlag, 2001. [14] M. M\u00a8 uller-Olm and H. Seidl. Computing \nPolynomial Program Invari\u00adants. Submitted for publication. [15] M. M\u00a8Polynomial Constants are Decidable. \nuller-Olm and H. Seidl. In 9th Static Analysis Symposium (SAS), pages 4 19. LNCS 2477, Springer-Verlag, \n2002. [16] B. Paige and S. Koenig. Finite Differencing of Computable Expres\u00adsions. ACM Trans. Prog. Lang. \nand Syst., 4(3):402 454, 1982. [17] T. Reps, S. Horwitz, and M. Sagiv. Precise Interprocedural Data.ow \nAnalysis via Graph Reachability. In 22nd ACM SIGPLAN-SIGACT Symp. on Principles of Programming Languages \n(POPL), pages 49 61. ACM Press, 1995. [18] T. Reps, S. Schwoon, and S. Jha. Weighted Pushdown Systems \nand their Application to Interprocedural Data.ow Analysis. In Int. Static Analysis Symposium (SAS), pages \n189 213. LNCS 2694, Springer-Verlag, 2003. [19] H. Seidl and B. Steffen. Constraint-Based Inter-Procedural \nAnalysis of Parallel Programs. Nordic Journal of Computing (NJC), 7(4):375 400, 2000. [20] M. Sharir \nand A. Pnueli. Two Approaches to Interprocedural Data Flow Analysis. In [12], chapter 7, pages 189 233. \n  \n\t\t\t", "proc_id": "964001", "abstract": "We apply linear algebra techniques to precise interprocedural dataflow analysis. Specifically, we describe analyses that determine for each program point identities that are valid among the program variables whenever control reaches that program point. Our analyses fully interpret assignment statements with affine expressions on the right hand side while considering other assignments as non-deterministic and ignoring conditions at branches. Under this abstraction, the analysis computes the set of all <i>affine relations</i> and, more generally, all <i>polynomial relations of bounded degree</i> precisely. The running time of our algorithms is linear in the program size and polynomial in the number of occurring variables. We also show how to deal with affine preconditions and local variables and indicate how to handle parameters and return values of procedures.", "authors": [{"name": "Markus M&#252;ller-Olm", "author_profile_id": "81100259808", "affiliation": "FernUniversit&#228;t Hagen, LG Praktische Informatik 5, Hagen, Germany", "person_id": "PP36026073", "email_address": "", "orcid_id": ""}, {"name": "Helmut Seidl", "author_profile_id": "81100146213", "affiliation": "TU M&#252;nchen, Lehrstuhl f&#252;r Informatik II, M&#252;nchen, Germany", "person_id": "PP14061465", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/964001.964029", "year": "2004", "article_id": "964029", "conference": "POPL", "title": "Precise interprocedural analysis through linear algebra", "url": "http://dl.acm.org/citation.cfm?id=964029"}