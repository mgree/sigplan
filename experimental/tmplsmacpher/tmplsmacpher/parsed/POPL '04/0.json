{"article_publication_date": "01-01-2004", "fulltext": "\n Symbolic Transfer Function-based Approaches to Certi.ed Compilation . Xavier Rival D\u00b4 epartement d \nInformatique, \u00b4 Ecole normale sup \u00b4 erieure, 45, rue d Ulm, 75230, Paris Cedex 5, France rival@di.ens.fr \nAbstract We present a framework for the certi.cation of compilation and of compiled programs. Our approach \nuses a symbolic transfer functions-based representation of programs, so as to check that source and compiled \nprograms present similar behaviors. This checking can be done either for a concrete semantic interpretation \n(Translation Validation) or for an abstract semantic interpretation (Invariant Translation) of the symbolic \ntransfer functions. We pro\u00adpose to design a checking procedure at the concrete level in order to validate \nboth the transformation and the translation of abstract invariants. The use of symbolic transfer functions \nmakes possible a better treatment of compiler optimizations and is adapted to the checking of precise \ninvariants at the assembly level. The approach proved successful in the implementation point of view, \nsince it ren\u00addered the translation of very precise invariants on very large assem\u00adbly programs feasible. \nCategories and Subject Descriptors: D.2.4 [Software/Program Veri.cation]: Formal methods, Validation, \nAssertion checkers, Correctness proofs; D.3.1 [Formal De.nitions and Theory]: Se\u00admantics; D.3.4 [Processors]: \nCompilers, Optimization; F.3.1 [Spec\u00adifying and Verifying and Reasoning about Programs]: Invariants, \nMechanical veri.cation; F.3.2 [Semantics of Programming Lan\u00adguages]: Operational semantics, Denotational \nsemantics, Algebraic approaches to semantics, Program analysis. General Terms: Algorithms, Design, Theory, \nVeri.cation Keywords: Static Analysis, Abstract Interpretation, Certi.cation, Compilation, Translation \nValidation 1 Introduction 1.1 Motivations Critical software (as in embedded systems) is concerned both \nwith correctness and safety. The designer of such systems is usually in\u00adterested in checking that the \n.nal program correctly implements a speci.cation and is safe in the sense that it should not yield any \nbad behavior . Hence, much work has been done for the analy\u00adsis of source programs [4, 5]. However, in \nthe same time compil\u00aders have become huge and complex. For instance, the code of gcc amounts to about \n500000 lines of code. Moreover, the semantics of *This work was supported by the ASTREE Project of the \nFrench RNTL Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL \n04, January 14 16, 2004, Venice, Italy. Copyright 2004 ACM 1-58113-729-X/04/0001 ...$5.00 the C language \nleaves some erroneous cases unspeci.ed in order to let the compiler designer free to implement various \noptimizations. Therefore, certifying compilation and compiled programs turns out to be necessary in the \ncase of highly critical applications. Given a source program Ps and a compiled program Pc, the following \nques\u00adtions are of interest: (1) does Pc yield the same behaviors as Ps ? (2) does Pc meet some safety \nrequirements; e.g. is it runtime error free ? Proving the compiler formally would be an answer for the \n.rst question, but this solution is usually too expensive and is not practi\u00adcal if the code of the compiler \nis not available. Hence, a more prac\u00adtical approach proceeds by proving the semantic equivalence of Ps \nand Pc (Translation Validation, noted TV). In practice, an external tool generates proof obligations \nand attempts to check them after each compilation. Analyzing directly assembly programs is prob\u00adlematic \nsince compilation induces a loss of structure both at the data and at the control level. Therefore, a \nmore satisfactory approach to the second problem is based on the analysis of the source program Ps and \non a phase of Invariant Translation (noted IT), which yields some sound invariant for Pc. A unifying \nframework for the certi.cation of compilation and of compiled programs would allow to compare these methods \nand to share the effort to tackle some common problems: .nding suitable representations for programs, \ncharacterizing the semantic relation between source and compiled programs, handling optimizations... \n 1.2 Related Work In Proof Carrying Code systems [16, 18, 2], an untrusted compiler is required to provide \nannotations together with the compiled pro\u00adgram, which is related to IT. Before the program is executed, \na cer\u00adti.er generates Veri.cation Conditions and attempts to prove them using the annotations; if it \nsucceeds, then the program obeys the safety policy and can be executed. Typed Assembly and Intermedi\u00adate \nLanguages (TAL/TIL) were proposed as a means to keep more semantic information in the target programs \n[15, 21], so as to im\u00adprove the level of con.dence into compiled programs ([14] relates the implementation \nof a compiler for a safe C dialect). The TAL approach was further extended by [22] with more expressive \nanno\u00adtations. Yet, all these systems require deep compiler modi.cations. Moreover, most of them aim at \nchecking type safety properties and are restricted to security or memory safety properties. As an example \nof abstract invariant translation system, we can cite our previous work in [20]: a source analyzer was \ninstrumented to translate the source invariant into an invariant which was then proved to hold for the \ncompiled program. The process does not re\u00adquire the compiler to be modi.ed. Complex invariants were trans\u00adlated \nand checked at the assembly level so as to prove the absence of runtime errors. Yet, these results can \nbe improved, so as to han\u00addle more complex and precise invariants, to scale up and to accom\u00admodate optimizations: \nindeed, our tool was not able to translate relational invariants or to certify optimized code. Translation \nValidation-based methods were pioneered in [19] and extended in [17, 23]. An external tool generates \nVeri.cation Con\u00additions and attempts to solve them, so as to prove the compilation correct. The compiler \ndoes not need to be modi.ed. As far as we know, the feasibility of an IT system based on TV has not been \nremarked before. 1.3 Contribution Our purpose is to provide a framework for the certi.cation of com\u00adpilation \nand of compiled programs and to propose an ef.cient, .ex\u00adible and scalable IT process: We provide a \nsymbolic representation of (source and assembly) programs so as to make compilation certi.cation suitable; \nthen, we formalize TV-and IT-based approaches in this framework.  A new approach to Invariant Translation \nbased on Translation Validation appears in this framework. Indeed, the equivalence checking step justi.es \nthe semantic correctness of compilation; hence, it discards the abstract invariant checking requirement \nof [20]. Furthermore, this new approach proves the semantic equiv\u00adalence of source and compiled programs \nand takes bene.t from the TV pass to improve the ef.ciency of the invariant translation.  We implemented \nthe latter approach. The certi.er was run on real examples of highly critical embedded software, for \nthe pur\u00adpose of proving the safety of assembly programs using the same criteria as for source programs \nin [5], which requires to translate very precise invariants. We are not aware of any similar existing \nwork.  At last, the symbolic representation of programs turns out to be a very adequate model to tackle \nthe dif.culties which arise when considering optimizations.  1.4 Overview Sect. 2 introduces some preliminaries \nand states the de.nition of compilation correctness used in the paper. Sect. 3 sets up a sym\u00adbolic representation \nof programs adequate for compilation certi.\u00adcation (either by TV or IT). IT and TV are integrated to \nthis frame\u00adwork in Sect. 4, together with a new approach to Invariant Trans\u00adlation, based on Translation \nValidation. Experimental results are provided and show the scalability of the latter approach. Sect. \n5 tackles the problem of optimizing compilation using the technical tools introduced in Sect. 3. Sect. \n6 concludes.  2 Correctness of Compilation This section introduces preliminaries and leads to a de.nition \nof compilation correctness in the non-optimizing case. 2.1 Programs, Transition Systems, Symbolic Transfer \nFunctions Programs are usually formalized as Labeled Transition Systems (LTS). An execution state is \nde.ned in this model by a program point l (a label) and a store ., that is a function which assigns a \nvalue to each memory location (variable, register, array cell). In the following, we write X for the \nset of memory locations, L for the set of labels, R for the set of values and S =X -R for the set of \nstores. In this setting, a program P is a tuple (LP,-P,iP) where LP is the set of labels of P, iP ELP \nis the entry point of P and (-P)S(LP xS)x(LP xS)is the transfer relation of P. Intuitively, (l0,.0)-P \n(l1,.1)if and only if an execution of P reaching point l0 with store .0 may continue at program point \nl1, with store .1. This model allows non-determinism since (-P)is a relation. Note that the notion of \nprogram point does not necessarily correspond to the notion of syntactic program point. A label may in \nparticular de.ne both a calling context and a program point. The semantics of a program P is the set \nof all the possible runs of P. In the following, if E is a set, we note E*for the set of the .nite sequences \nof elements of E and ](E)for the power-set of E.An execution trace (or run) is a sequence of states. \nThe semantics [PDof a program P =(LP,-P,iP)is the set of the .nite partial execution traces of P: [PDS(LP \nxS)*. The semantics [PD boils down to a least .xpoint (lfp) in the complete lattice (]((LP x S)*),S): \n[PD=lfp. 0/ FP where FP : ]((LP xS)*)-]((LP xS)*)is the continuous semantic function de.ned below: FP(X \n)={((x0,s0),,(xn,sn),(xn+1,sn+1))((x0,s0),,(xn,sn))EX I(xn,sn)-P (xn+1,sn+1)}{((iP,s))s ES} Source and \nAssembly languages: In the following, we choose a subset of C as a source language (Fig. 1(a)) and the \nPower-PC-like assembly language presented in Fig. 1(b). The source language features loops, conditionals, \nassignments, array and integer vari\u00adables. Source statements and assembly instructions are (implicitly) \nlabeled. The assembly language provides registers (denoted by ri), memory, indirections in the data and \nusual assembly features. The address of a variable x is denoted by x.If n is an integer, the predicate \nisaddr(n)means that n is a valid address for a memory location in the current program; then, content(n)denotes \nthe content of the memory cell of address n.If isaddr(x +v)holds, then the instruc\u00adtion load r0,x (v)loads \nthe value content(x +v)into register r0; in case x +v is not a valid address, the load instruction yields \nan error. The store instruction is similar (but copies the content of a register into a memory cell). \nThe instruction li r0,n loads the integer n into the register r0. The arithmetic instruction opr0,r1,r2 \ntakes the content of r1 and r2 as arguments and stores the result into r0. The comparison instruction \ncmp r0,r1 sets the value of the condition register according to the order of the values in r0 and r1: \nif r0 <r1, cr is assigned LT and so on. The conditional branching instruction bc(<)l directs the control \nto the instruction of label l if cr contains the value LT . The branching instruction b l re-directs \nthe control to the instruction of label l. The semantics of source and assembly programs is de.ned as \nabove since they are transitions systems. Adding procedures, non\u00addeterminism, and more complex data structures \nwould be straight\u00adforward. 2.2 De.nition of Correct Compilation Let Ps =(Ls,-s,is)be a source program \nand Pa =(La,-a,ia) be an assembly program. We assume that Pa is a compiled pro\u00adgram corresponding to \nPs, so we expect Ps and Pa to present simi\u00adlar behaviors: execution traces of both programs should be \nin cor\u00adrespondence. The relation between traces of Ps and of Pa can be de.ned by a mapping of source \nand assembly program points and memory locations. For instance, on Fig. 2, the source program points \nl0s,l1s,l2s,l3s,l4s,l5s,l6 s respectively correspond to the assembly program points l0 a,l2 a,l4 a,l8 \na,la Similarly, the source vari\u00ad 11,la 16. 15,la able t corresponds to a region of the assembly memory \nstarting at address t and of the same size as t (we suppose memory cells have size 1) and so on. Then, \nthe value of i at point l4 s should be equal to the value stored at address i at point l8 a . These map\u00ad \n    Lv ::=x (x EX)Lv[E]c E{<,:,=,,=,>,;} E ::=n (n E.)Lv v E{r0,,rN }Z E+EE-E op ::=add sub E*EE/E \nmul div C ::=true false C n EZ CICCvCI ::=load ri,n(v) E ==EE <E store ri,n(v) S ::=Lv :=E liri,n if(C)B \nelse B opri,rj,v while(C)B cmpri,rj B ::={S;; S}bc(c)l b l (a) Source language (b) Assembly language \nFigure 1. Syntax i,x : integer variables t : integer array of length n EI,where n is a parameter l0 si \n:=-1; l1 sx :=0; l2 s while(i <n){ l3 si :=i +1; l4 sx :=x +t[i] ls } 5 ls 6 (a) Source program l0 a \nli r0,-1 l9 a add r0,r0,1 l1 a store r0,i(0)la store r0,i(0) 10 l2 a li r1,0 la load r1,x (0) 11 la store \nla load r2,t (r0) 3 r1,x (0)12 la load r0,i(0) la add r1,r1,r2 4 13 la li r1,n la store r1,x(0) 5 14 \nla cmp r0,r1 la b la 6 154 la bc(;)la la 7 16 16 l8 a load r0,i(0) (b) Assembly program Figure 2. Compilation \npings are further represented by a bijection pL : Lr -Lr where sa Lr SLs and Lr SLa (the program point \nmapping) and a bijection sa pX : Xr -Xr where Xr SXs and Xr SXa (the variable mapping). sas a Note that \nLr CLa in general, since intermediate assembly program a points have no source counterpart as is the \ncase for l1 a ,l3 a ,l5 a ,In case some non reachable source program points are deleted, we also have \nLr CLs. The semantic correspondence between Ps and Pa can s be stated as a one to one relation between \nthe observational abstrac\u00adtion of the traces of both programs. Most optimizations do not .t in this simple \nframework; some are dealt with in Sect. 5. Erasure operators Fi : (Li xS)* -(Lir xS)*(for i E{s,a}) can \nbe de.ned, that abstract away program points and memory locations which do not belong to Lr and to Xir: \nFi(((l0,.0),,(ln,.n)))is the trace i ((lk0 ,..),,(lkm ,..))where k0 <<km, {k0,,km}={j k0 km lj ELir}and \nVi,..ki is the restriction of .ki to Xir. The observational abstraction operators ari : ]((Li xS)*)-]((Lri \nxS)*)can be de\u00ad.ned by ari (X )={Fi(s)s EX }for i E{s,a}. The stores .s and .a are equivalent if and \nonly if Vxs EXsr ,Vxa EXar ,pX (xs)=xa = .s(xs)=.a(xa)(we note .s 'pX .a for the store equivalence). \nThe traces ss =((l0s ,.0s ),,(lns ,.ns ))and sa =((l0 a ,.0a),,(lna ,.na)) are equivalent (noted ss 'sa) \nif and only if Vi,la =pL(ls .s ii )Ii 'pX .ai .If Vj E{s,a},Tj S]((Lrj xS)*), we write Ts 'Ta if and \nonly if 'is a bijection between Ts and Ta. DEFINITION 1(COMPILATION CORRECTNESS, [20]). The compilation \nof Ps into Pa is correct with respect to pL and pX if and only if ar([PsD)'ar ([PaD)(i.e. iff ar([PsD)and \nar ([PaD)are in sasabijection).  3 A Symbolic Representation of Programs An atomic source statement \nis usually expanded at compile time into a long and intricate sequence of assembly instructions (a source \nstatement can be compiled into many different sequences). Opti\u00admizations (Sect. 5) make the correspondence \nbetween source and assembly actions even more involved. Therefore, we design in this section a higher \nlevel symbolic representation of the semantics of programs which proves ef.cient to relate the behaviors \nof source and compiled programs. 3.1 Symbolic Transfer Functions Symbolic Transfer Functions (STFs) appeared \nin the work of Colby and Lee [7] as a means to avoid accumulated imprecisions along paths in the control \n.ow: an STF describes precisely the store trans\u00adformation between two program points. STFs turn out to \nbe also a nice setting for reasoning about program equivalence. An STF d is either a parallel assignment \nlx +e , where -x and -- e are respectively a sequence of memory locations and a sequence of expressions; \nor a conditional construct lc ? dt d f where c is a condition and dt , d f are STFs; or the void STF \nD(which stands for the absence of transition). Note that the empty assignment is the identity; it is \ndenoted by .. We write 1for the set of STFs. Semantics of Symbolic Transfer Functions: An STF is inter\u00adpreted \nby a function which maps a store . to the set of possible transformed stores in presence of non-determinism. \nIn the follow\u00ading, we forbid non-determinism for the sake of concision; hence, a transfer function is \ninterpreted by a function which maps a store to a store or to the constant .S (in case of error, there \nis no transformed store). We assume that the semantics of expressions and conditions is de.ned as follows: \nif e is an expression, then [eDES -R and if c is a condition, then [cDES -lwhere ldenotes the set of \nbooleans {T ,F }. The update of the variable x with the value v in the store . is denoted by .[x +v]. \nThe semantics of STFs is de.ned by induction on the syntax as follows: [DD(.)=.S [lx0 +e0,,xn +enD(.)=.[x0 \n+v0][xn +vn], where Vi,[eiD(.)=vi. Note that the values are all computed in the initial environment .. \n [dt D(.)if [cD(.)=T [lc ? dt d f D= [d f D(.)if [cD(.)=F Composition of Symbolic Transfer Functions: \nThe composition of two STFs can be written as an STF: THEOREM 1(COMPOSITION, [7]). There exists a (non \nunique) computable operator E: 1x1-1such that Vd0,d1 E1,[d0 E d1D=[d0D\u00c6[d1D. EXAMPLE 1. If x and y are \nnot aliased, if d0 =ly >3? lz + y +xlz +3and d1 =lx <4? ly +xD, then lx < 4? lx >3? lz +2x,y +xlz +3,y \n+xDis a correct de.nition for d0 Ed1. The composition is generally not associative; yet, we do not distin\u00adguish \nd0 E(d1 Ed2)and (d0 Ed1)Ed2, since they have the same semantics. We do not describe completely a composition \nfunction. The main problem of such an operator generally concerns the alias\u00ading problem. If d0 reads \na location l0 and d1 writes into the location l1, two cases should be considered when computing d0 Ed1: \neither l0 and l1 are aliases or they are not. Consequently d0 Ed1 should contain a conditional testing \nthe aliasing of l0 and l1. Hence, mul\u00adtiple compositions may result in huge functions (exponential blow\u00adup). \nTherefore, the use of a simpli.cation function is required to keep the STFs small . Let isalias(x,y)denote \nthe dynamic alias\u00ading test of locations x and y: the condition isalias(x,y)is true if and only if x and \ny represent the same memory location. Then, the following simpli.cation rules are acceptable: lisalias(x,x)? \ndt d f -dt lisalias(x,ri)? dt d f -d f if x is a variable lisalias(t[e0],t[e1])? dt d f -dt if V.,[e0D(.)=[e1D(.) \nDEFINITION 2(SIMPLIFICATION FUNCTION). A simpli.cation function is a computable function S : 1-1which \npreserves the semantics of STFs: Vd E1,[S(d)D=[dD 3.2 Semantics Using Symbolic Transfer Func\u00adtions In \nthe following, we always assume that the transfer relation of a program can be encoded as a table of \nSTFs. Let P =(LP,-P,iP). A table of STFs encoding the transfer relation -P is a family .P = (dlIl. )lIl.ELP \nwhere dlIl. stands for the STF between points l and l and is such that: dlIl. (.) =. .(l,.) =-P (l ,. \n) Intuitively, dlIl. (.)=. means that program point l can be reached with store . after one transition \nstep from the state (l,.)if . , = .S.If dlIl. (.)=.S, then l cannot be reached after the execution state \n(l,.). Therefore, the relation -P is completely and uniquely characterized by .P (this also holds in \nthe non-deterministic case). In practice, most of the dlIl. are D, so we use a sparse representation \nfor the transition tables. In the case of C programs, each statement de.nes a family of trans\u00adfer functions; \nin case of assembly programs, each instruction de\u00ad.nes one or two STFs (two in the case of conditional \nbranching only). As an example, we give the STF-based encoding for two instructions: The load l : load \nr0,x (v); l : succeeds if and only if x +v is a valid address: dlIl. =lisaddr(x +v)? lr0 +content(x +v)D \nThe comparison l : cmp r0,r1; l :: dlIl. =lr0 <r1? lcr +LT lr0 =r1? lcr +EQlcr +GT For a complete de.nition \nof the semantics of source (Fig. 1(a)) and assembly (Fig. 1(b)) programs using STFs, we refer the reader \nto Appendices A and B. Henceforth, we assume that the semantics of programs is given by their STFs-based \nrepresentation. The STFs-based assembly seman\u00adtics may abstract away some low level aspects of assembly \nin\u00adstructions, like the problems of alignments inherent in usual store and load instructions or like \nthe instructions devoted to the man\u00adagement of the cache memory and of the interruptions mechanism (system-level \noperations). Similarly, the semantics encoding may ignore over.ows. If so, the forthcoming proofs of \ncorrectness do not take these properties into account, because of this initial abstrac\u00adtion. The assembly \nmodel is de.ned by the translation of assembly programs in tables of STFs. 3.3 Towards Compilation Certi.cation \nWe assume that the compiler provides the program point mapping and the memory location mapping together \nwith the program Pa when compiling Ps. In the following, we suppose that Lr =Ls s and Xr =Xs without \na loss of generality; hence, Fs is the identity. s We envisage here the de.nition of a new program observationally \nequivalent to Pa and more adapted to further formal reasoning about compilation. This program (the reduced-LTS) \nwill be widely used in Sect. 4. A source reduced-LTS is usually also de.ned. Reduction of Labeled Transition \nSystems: We suppose here that Lr contains at least one point in each strongly connected compo\u00ad a nent \nof Pa: we assume that at least one point in each loop of the compiled program can be mapped to a point \nin the source, in order to be able to relate in a good way the behaviors of both. The entry point ia \nis also supposed to be in Lr Then, a reduced-LTS Pr can a. a be de.ned by considering the points in Lr \nonly and de.ning ade\u00ad a quate STFs between these points. If l,l ELr and c =l,l0,,ln,l a is a path from \nl to l such that Vi,li ,ELra, then we can de.ne the associated STF dc by composing single step STFs (Th. \n1). lIl. There is only a .nite number of paths c satisfying the above con\u00addition. The transition relation \nof the reduced-LTS is de.ned by (l,.)-=:c,dc )=. . Note that any pair of points (l ,. ).lIl. (. of the \nreduced-LTS de.nes a .nite set of STFs instead of one STF in the case of the original LTS. The traces \nof the reduced-LTS are the same as those of the initial LTS modulo deletion of intermediate states, as \nmentioned below: THEOREM 2(LTS REDUCTION). Let .r be the func\u00adtion .r : (La xS)* -(Lra xS)*; ((l0,.0),,(ln,.n)) \n-((lk0 ,.k0 ),,(lkm ,.km ))where {k0,,km}={i li ELra}and k0 <<km. Then, .r is onto between [PaDand [ParD(the \nprogram point forget operator): Vs E[PrD,:s E[PaD,s =.r(s). aIf the compilation of Ps into Pa is correct, \nthe bijection between traces of both programs maps a one-step transition in Ps to a one\u00adstep transition \nin Par . Simpli.cation: The computation of the reduced-LTS should ben\u00ade.t from the use of a simpli.cation. \nFor instance, real assembly languages compute and load addresses in several stages: the .rst half of \nthe address is loaded, then the second half is loaded and shifted and the sum yields the address. Similarly, \nthe load of a dou\u00adble .oating point constant can be split into several operations. A simpli.cation (Def. \n2) reduces such sequences of operations into atomic ones. In practice, we noted that using an ef.cient \nsimpli.\u00adcation procedure was crucial to make further certi.cation easier.  4 Certi.cation of Compilation \nand of Compiled Programs We consider the certi.cation of compilation and of compiled pro\u00adgrams in the \nframework introduced above. 4.1 Abstract Invariant Translation and Invariant Checking A .rst goal is \nto check that the assembly programs complies with some safety requirements. This can be done by inferring \nan in\u00advariant of the source program, translating it into a property of the assembly program and checking \nthe soundness of this invariant. We envisage here a class of invariants (large enough for most applica\u00adtions) \nand the way we can translate them, using the formalism of Abstract Interpretation [8]. Let D be an abstract \ndomain for representing sets of stores: . (](S),S)+--(D ,.). The domain D provides usual abstract a operations \n(.and Tconstants, an associative and commutative uoperator and n,.,. operators), a sound assignment operator \n(assign : ](X)xE xD -D inputs a set of locations, an expres\u00adsion and an abstract invariant and handles \nthe assignment; in case the set of locations is not a singleton, we are in presence of a may assign ), \na sound guard operator (guard : C xlxD -D inputs a condition, a boolean and an abstract invariant). The \nsoundness of the guard operator boils down to Vc EC,Vd ED ,Vb El,{. E .(d)[cD(.)=b}S.(guard(c,b,d)): \nintuitively .(guard(c,b,d)) contains at least all the stores in .(d)which evaluate the condition c to \nb (the soundness of assign is similar). It is a general result of the Abstract Interpretation theory \n[8] that a sound approxima\u00adtion [PsD : Ls -D of the behaviors of the source program can be computed: \nVl ELs,{. ES (,(l,.),)E[PsD}S.([PsD (l)). The issue we address here is how to derive from [PsD a property \nfor the assembly program, i.e. a local invariant Ia (la)for each point la ELa. The compilation of Ps \ninto Pa is here assumed to be correct. We write d for a \u00c6[dD\u00c6. (most precise abstract transfer function \ncorresponding to d; an upper approximation of it is usually com\u00adputed). Let ls ELs,la ELr such that \npL(ls)=la and .a ES such that a there exists sa =(,(la,.a),)E[PaD. The correctness of the compilation \nentails the existence of .s ES such that .s 'pX .a. We deduce that .a \u00c6pX E.([PsD )(ls), which de.nes \na sound local invariant Ia (la)at the assembly level at point la. Intuitively, a property of xs is inherited \nby pX (xs)and if (:xs EXsr ,xa = pX (xs)), then the source invariant [PsD (ls)does not give any information \nabout xa.  Once a local invariant has been determined for all the points of the reduced-LTS, a local \ninvariant can be computed for any other label of La under the assumption that any loop of Pa contains \nat least a point that is in Lra. Indeed, let la ELa. There exists a .nite set P of paths c =la,l0,,ln,la \nsuch that la ELr and  a l0 E,Lr ,,ln E,Lr and we can de.ne a local invariant Ia (la): aa . I (la)= d \n(I (la)) (1) acacEP Ic=l. I...Ila a where d c =(dlnIla EEdl. Il0 ) if c =la,l0,,ln,la a Moreover, Ia \n(la)is sound: .({. ES (,(la,.),)E[PaD}). Ia (la). We deduce that a sound invariant can be derived for \nPa from [PsD (see [20] for an extensive proof): THEOREM 3(INVARIANT TRANSLATION SOUNDNESS). If the compilation \nof Ps into Pa is correct, then the invariant Ia is sound, that is: Vla ELa,{. ES (,(la,.),)}S.(I (la)) \na Assuming the correctness of the compilation of Ps into Pa is gen\u00ad erally not acceptable; hence, we \nenvisage now a way to check the soundness of the translated invariant Ia independently of the way it \nis derived from the source invariant. Intuitively, Invariant Checking amounts to verifying the inductive\u00adness \nof the translated invariant at the reduced-LTS level (Th. 2). The principle of Invariant Checking (IC) \nis stated by the following theorem (we still suppose that Lr contains one point in each cycle a of La): \nTHEOREM 4(INVARIANT CHECKING). If Ia is an assembly ab\u00adstract invariant such that for all la,la ELr, \nand for all path c : ala,l0,,ln,la such that l0 E,Lr ,,ln ,ELra, d c(Ia (la)).Ia (la), athen Ia is sound. \nEXAMPLE 2. A basic interval analysis would infer the invariants i E[0,n -1]at point l3 s and i E[1,n]at \npoint l4s. The translated a(lainvariant Ia is such that Ia (l8 a)(i )=[0,n -1]and I 11)(i )=[1,n]. This \nensures local stability (for l8 a and la l8 aIla (Ia (la 11) since d 8 )). 11 Ia (la Stability is proved \nby checking similar inequalities for all 11). pairs of program points in the assembly reduced-LTS. In \npractice, this checking is performed in an abstract domain D more precise than D , since the structure \nof the assembly program can be much more complicated and require the re.nement of Ia . For instance, \nthe checking of an interval invariant at point la 15 in the example above requires knowing that the content \nof r0 and the content of the memory cell of address i are equal at point la 11. This information is provided \nby an auxiliary analysis using the abstract domain DE of the set of the partitions of the set of the \nassembly memory locations. This domain de.nes a Galois connection [8] .e (](Xs -R),S)+--- (DE ,.e), where: \nae . E.e(.)=V. E.,:v ER,Vx E.,.(x)=v Then, the invariant checking would be done in the reduced product \nof D and DE [10] (usually, only an approximation of the reduced product is effectively computed). Among \nthe other domain re.ne\u00adments which may turn out to be necessary to handle a real assembly language, we \ncan cite the reduced product with the congruence do\u00admain [11]: the pair (a,b)EIxIrepresents the set {a+kb \nk E}. Indeed, analyzing the access to struct members or array cells re\u00adquires congruence information \nabout the memory addresses. Precision and STFs: The decomposition of source statements into sequences \nof instructions induces a loss of structure which may compromise invariant checking when STFs are not \nused. Rela\u00adtional abstract domains often handle precisely complex operations (assignments and guards \nof complex expressions) when done in one step as is the case for the octagons [13] for some linear assignments \nlike y :=Si ai *xi where ai E. Similarly, precise transfer functions carry out in a better way big expressions \nin one step. For instance, the symbolic manipulations implemented in the analyzer of [5] re\u00adduce x +(y \n-x)to y (under certain conditions and with corrections due to possible .oating points rounding errors), \nwhich may not be doable anymore if an assignment is split into a sequence of state\u00ad ments. More formally, \nif . =[d0D\u00c6\u00c6[dnDthen . .d 0 \u00c6\u00c6dn since .xx .. \u00c6a. Generally, . d 0 \u00c6\u00c6d n: this strict inequal\u00adity corresponds \nto a loss of precision. However, STFs solve this problem since they rebuild expressions (Th. 1); hence, \nthey al\u00adlow to recover the precision achieved at the source level, provided the abstraction occurs after \nsymbolic composition of the STFs [7], i.e. provided we use (d0 EEdn) , which is more precise than d \u00c6d \n0 \u00c6n. REMARK 1(REFINEMENT AND STFS). The use of STFs can supersede the use of a symbolic domain or some \ndomain re.nement. For instance, the checking procedure of [20] requires a partitioning over the values \nof the condition register to handle branching. This re.nement is not necessary any more when using STFs, \nwhich is a major improvement compared to [20] (lower memory usage).  4.2 Translation Validation The \ngoal of Translation Validation is to prove each compilation sep\u00adarately by showing that the assembly \nprogram correctly implements the source program. The most common approach proceeds to this equivalence \nproof locally. As seen in Sect. 2.1, the semantics of the program Pi boils down equivalence of expressions \nand to decide implications and incom\u00adpatibilities between conditions. First, we observe that STFs can \nbe seen as trees. A branch in an STF de.nes a (possibly void) sequence of conditions c0,,cn (when an \nelse branch is taken, we get a negative condition) and either a (possibly empty) set of assignments or \nan error transition D. We propose here a typical algorithm for attempting to prove the local property \ndisplayed above (ls,ls,la,la denote program points such that pL(ls)=la and pL(ls)=la): -Assume that corresponding \nsource and assembly memory loca\u00adtions contain the same value. to the least .xpoint of an operator FPi \ncomputed in a complete lat\u00ad - Simplify the trees dlsIl . s and dlaIlby eliminating redundant con\u00ad . tice \n(for i E{s,a}). By continuity of FPi for the inclusion order, a ditions on their branches (and using \nequivalences like lc ? D DD). -Reduce the assembly STF using information about equality rela\u00ad tions provided \nby a preliminary analysis. . [PiD=nE. FPni (0/). Since the erasure operators Fi and 'are also continuous \n(for the inclusion order) and Fi(0/)=0/, we deduce the following theorem, which states the principle \nof Translation Vali\u00ad dation: -For each pair of branches cs ,,cs of leaf fs in dlsIl 0 m and . s a THEOREM \n5(TRANSLATION VALIDATION). If for all Es Ec,,ca of leaf fa in dlaIl 0 n . a , assume cs 0,,cms ,ca 0,,can \nand ]((Ls xSs)*)and Ea E]((La xSa)*), Fs(Es)'Fa(Ea )== check that: Fs(Fs(Es))'Fa(Fa(Ea)), then Fs([PsD)'Fa([PaD). \nThe similarity of Theorem 5 with a Fixpoint Transfer Theorem [9] can be noticed (the proofs are identical). \nIn practice, the implication Fs(Es)'Fa(Ea)==Fs(Fs(Es))' Fa(Fa(Ea))is proved by showing the following \nproperty over the assembly reduced-LTS, for all program points ls,ls,la,la such that pL(ls)=la and pL(ls)=la: \nV.s EXs -R,V.a EXa -R, (2) .s 'pX .a =[dlsIlD(.s)'pX [dlaIl D(.a) .. s a . either the current assumption \nis contradictory (the branches are not compatible); . or the leaves fs and fa are equivalent: either \nthey are both the DSTF or they correspond to semantically equivalent se\u00adquences of assignments. If one \nstep fails to prove the incompatibility of the branches or the equivalence of the assignments at the \nleaves, then the Translation Validation fails and yields an (possibly false) alarm. This algo\u00adrithm is \nconservative and possibly not optimal (global simpli.ca\u00adtions could be done right at the beginning by \nconsidering compati\u00ad ble branches only). This equivalence of STFs can be proved by a conservative deci\u00ad \nsion procedure at the reduced-LTS level (Sect. 3.3), by observing only the memory locations in pX . A \nslight generalization of (2) is used in practice, since there are generally several STFs between two \ncontrol points of a Reduced-LTS, as seen in Sect. 3.3. Handling non-determinism would require considering \nsets of states instead of states in (2). In case the semantics of the source language leaves some behaviors \nunde.ned (as the C semantics does), the proof of adequation of STFs outlined by (2) should restrict to \nwell-de.ned behaviors: the de.nition of STFs can be extended by an additional unde.ned behavior STF \u00ae, \nsuch that V. ES,[\u00aeD(.)=S. An\u00adother approach to this problem proceeds by specializing the source semantics, \nwhich is doable if the way the compiler handles the un\u00adde.ned cases is known. EXAMPLE 3 (STFS ADEQUATION). \nIn the case of the example programs of Fig. 2, l2 s corresponds to l4 a;l3 s tol8 a. After automatic \nsimpli.cation, dl2s Ils =li <n ? . Dand dl4 aIla =lcontent(i )< 3 8 n ? lcr +LT,r1 +n,r0 +content(i )D(since \nisaddr(i ) holds). These STFs are symbolically equivalent (in the sense of (2)), since cr, r0 and r1 \nare not mapped to any source variable and since the location of address i is mapped to the variable i. \nChecking the equivalence of dl3s Ils and dl8 aIla requires to assume that 4 11 t +content(i )is a valid \naddress if and only if i is a valid index in the array t (predicate noted isindex(t,i)) and that these \nlocations are associated by pX: this stems from pX and from the assembly memory model. As in Ex. 2, the \ncase of node l5 s requires more information: checking the equivalence of dl4s Ils and dla requires knowing \nthat r0 and i 5 11Ila 15 store the same value at point la 11, which can be proved by an auxil\u00adiary equality \nanalysis (as in Sect. 4.1). Principle of a translation veri.er: In practice, the local property (2) is \nchecked by a specialized theorem prover, able to show the Among systems proving a semantic equivalence \nof source and com\u00adpiled programs we can cite the systems of [19, 23] and of [17].  4.3 Translation Validation-based \nInvariant Translation TV proves the equivalence of the source program and of the com\u00adpiled program for \na concrete semantic interpretation of STFs. This veri.cation is done locally, by checking the semantic \nequivalence of source and assembly STFs (Th. 5). By contrast, IC (Th. 4) is based on an abstract semantic \ninterpretation of STFs and involves the effective checking of a .xpoint. In case no alarm appears, the \nproof is validated. If IC or TV succeeds, the con.dence level in the compilation correctness increased: \nboth approaches aim at showing that some property of the original program is preserved. Furthermore, \nTV and IT can be advantageously combined: indeed, Th. 3 suggests that invariant checking could be discarded \nif we can assume compilation correctness, which can be proved by TV. If two symbolic transfer functions \nd and d are semantically equiv\u00adalent, then they can be abstracted to a same abstract function d . We \ndeduce the principle of this approach from Th. 3 and Th. 5 as follows (with the notations of Sect. 4.1 \nand Sect. 4.2): THEOREM 6. If the hypotheses of Th. 5 hold (i.e. if for all Es E]((Ls xSs)*)and Ea E]((La \nxSa)*), Fs(Es)'Fa(Ea)== Fs(Fs(Es))'Fa(Fa(Ea))) and if Vl ELs,{. (,l,.),)}E [PsDS.([PsD(l)), then the \ninvariant Ia computed as in Sect. 4.1 is sound. A .rst advantage of doing TV instead of IC to justify \nthe soundness of the invariant translation is that the domain re.nement evoked in Ex. 2 is not necessary \nany more: the design of the reduced-product D xDE can be avoided. Moreover, running TV once is enough \nto prove several IT passes: only one checking procedure is required. However, the main practical advantage \nwe noticed is that the TV process described in Sect. 4.2 provides very useful information about the meaning \nof some assembly sequences. For instance, the conversion of integers into .oating point values involves \ncompli\u00adcated sequences of assembly operations: in the case of the real PPC language, such a conversion \nis commonly compiled into a se\u00adquence of bitwise operations, subtractions and multiplication. This sequence \ncan be proved (by hand) to be equivalent to the conver\u00adsion. Then, TV should recognize such sequences \nand replace them by the atomic conversion operation: TV produces a new, sim\u00adpler assembly STF equivalent \nto the original one. This latter STF makes the further invariant propagation step (following (1)) more \nef.cient and more precise: indeed, the design of a precise abstract counterpart for the original intricate \nsequence would be painful. Finally, we propose the following process for certifying both the compilation \nand the compiled code: 1. compilation of Ps and Pa to reduced-LTSs, using STFs 2. Translation Validation \n(Sect. 4.2); replacement of assembly STFs by simpler ones during the proof; 3. translation of the invariant \ncomputed at the source level as seen in Sect. 4.1 (no Invariant Checking required); 4. checking that \nthe safety requirements are ful.lled using the translated invariant  Note that the step 4 is still strictly \nnecessary to prove the soundness of the compiled code (i.e. that it yields no run-time error). Indeed, \nthe equivalence proof of step 3 holds with respect to the mappings pX and pL: it entails that part of \nthe computations done at the as\u00adsembly level implement the source program but it does not mean that any \ncomputation done at the assembly level is the counterpart of some part of the source program. Therefore, \nthe veri.cation of the soundness of the assembly program must eventually be done at the assembly level, \neven if the source analysis concludes that the source program is semantically safe. 4.4 Implementation \nand Results We implemented a certi.er following the approach introduced in Sect. 4.3 in OCaml [12]: the \nIT is preceded by a TV step which allows to deal with simpli.ed assembly STFs when translating in\u00advariants \nand to avoid coping with abstract invariant checking. Our goal was to certify automatically both the \ncompilation and the absence of Run-Time Errors (RTE) in the compiled assembly pro\u00adgrams. We also expected \nthe certi.er to scale up. The target archi\u00adtecture is a 32 bits version of the Power-PC processor; the \ncompiler is gcc 3.0.2 for Embedded ABI (cross-compiler). The source invari\u00adants are computed using the \nanalyzer presented in [5] and achieve a very low false alarms number when used for checking RTE. This \nresult is achieved thanks to a very precise domain, using octagons [13], boolean relations, ellipsoid \ndomain, control-based partitioning and other domain re.nements. The benchmarks involve three programs \n(of small, medium and large size) corresponding to typical embedded applications: the third one is a \nreal life application, running in true systems for a long time (it amounts to about 75 kLOC after constant \npropagation and features 10000 static variables) whereas the two others cor\u00adrespond to representative \ndevelopment examples. These programs are usually compiled with a low level of optimization since the \nuser generally wishes the compiler to produce regular and predictable results in the context of critical \nsystems. Hence, the certi.cation we attempt to operate here is realistic. The translation veri.er handles \nmost C features (excluding dynamic memory allocation through pointers which is not used in the fam- Program \n1 2 3 Size of the source (lines) 369 9500 74365 Size of the assembly (lines) 1932 56626 344005 (1) C \nfrontend (s) 0,04 0,53 2,97 (1) Assembly frontend (s) 0,08 0,97 13 (1) Mapping construction (s) 0,03 \n0,39 0,81 (2) Translation Validation (s) 0,14 0,62 9,45 Alarms in Trans. Validation 0 0 0 (3) Invariant \ntranslation (4) + RTE checking (s) 0,23 8,22 84,5 Alarms (Runtime Errors) 0 0 22 Source analysis (s) \n1,15 46,79 3698 Table 1. Benchmarks ily of highly critical programs under consideration): procedures \nand functions, structs, enums, arrays and basic data types and all the operations on these data-types. \nThe fragments of C and of the Power PC assembly language we considered are larger than those presented \nin Appendices A and B: In particular, a restricted form of alias is handled for the sake of passing by \nreference of some kind of function arguments like arrays. Non-determinism is also accom\u00admodated (volatile \nvariables). The mappings pX (for variables) and pL (for program points) are extracted from standard debugging \nin\u00adformation. The veri.er uses the Stabs format (hence, it inputs as\u00adsembly programs including these \ndata). The equivalence proof is carried out by an optimized .rst order Resolution-based prover which \naccepts only clauses of length 1 (details about Resolution can be found in [6]). This requires a pre\u00adnormalization \nof STFs (the conditions are all expanded into atomic ones). Clauses are all of the form e0R e1 (where \nR is a relation) or e0 (where e0 is a boolean expression); hence, in case a derivation of false exists, \nit can be found immediately. The Uni.cation procedure is mostly based on the ML pattern matching of the \nrules specifying the equivalence of expressions; it is not complete but very fast. Two auxiliary analyses \nwere necessary in order to make the translation validation successful: the equality analysis mentioned \nin Sect. 4.1;  a congruence analysis [11] since alignments should be handled carefully (in order to \naccommodate struct and enum data types).  The whole development amounts to about 33000 lines of OCaml \ncode: The various parsers and interfaces (e.g. with the source ana\u00adlyzer) are about 17000 lines; the \nkernel of the certi.er (the imple\u00admentation of the STFs and the prover) is about 6000 lines; the sym\u00adbolic \nencoding functions (i.e. the formal de.nition of the semantics of the source and assembly languages) \nare about 3000 lines; the invariant translator and the certi.er are about 5000 lines. The most critical \nand complicated part of the system corresponds to the sym\u00adbolic composition (2000 lines) and to the prover \n(1500 lines). The four steps described in the end of Sect. 4.3 were run on a 2.4 GHz Intel Xeon with \n4 Gbytes of RAM. TV succeeds on the three programs (no alarm is raised). The results of the benchmarks \nare given in table 1 (sizes are in lines, times in seconds). As can be seen in the table above, most \nof the time for the assembly certi.cation is spent in the invariant translation and the checking of RTE \nwhich is due to the size of the invariants dumped by the source analyzer (although only part of the source \ninvariant was used for the assembly certi.cation). The step (1) is longer than TV because the frontend \nperforms massive STFs simpli.cations, which make the equivalence checking more easy: there was no dynamic \naliasing tests (Sect. 3.1) in the simpli.ed STFs. The checking time is rather small compared to the source \nanalysis done to synthesize, process and serialize the source invariant (last raw). The memory require\u00adment \nfor the third program is 750 Mbytes for the source analysis; 400 Mbytes for the assembly certi.cation \n(i.e. for the step (4)). This requirements are much smaller than those of the IC algorithm used in [20], \nsince the certi.cation of the .rst program was about 20 s long and required about 80 Mbytes; the third \nprogram could not be treated. The key of scalability is to make IC redundant and to do TV instead: the \nabstract .xpoint checking turns out to be much too resource consuming. We believe that a direct analysis \nof the assem\u00adbly program would be very painful to design (complex invariants) and not scalable. TV is \nsuccessful: no false alarm is raised by the equivalence proofs, which justi.es the further IT. No possible \nruntime errors are re\u00adported in the .rst two programs. The alarms reported for the last one concern the \nsame program points as the source analysis for the RTE checking. These alarms can be disproved by hand, \nso the certi\u00ad.er proves able to certify the assembly program almost completely. The main conclusion of \nthis experimental study is that the approach to IT based on TV (Sect. 4.3, Th. 6) is realistic and scalable \nto large critical applications.  5 Optimizing Compilation The previous sections focused on non-optimizing \ncompilation, as de.ned in Def. 1. We envisage the extension of our framework to optimizing transformations \nby considering a few representative cases and generalizing the solutions. 5.1 Methodology First, we notice \nthat most optimizations do not .t in the above framework since they are not considered correct in Def. \n1. Hence, this de.nition should be generalized, so as to accept these new transformations as correct \ncompilations . The certi.cation methods presented in Sect. 4 are based on the reduced-LTS notion. Hence, \ntwo approaches for handling an opti\u00admizing transformation can be proposed: enhancing the de.nition of \nreduced-LTSs so as to extend the cer\u00adti.cation method (TV and IC algorithms change);  integrating the \nproof of correctness of the optimization into the computation of the reduced-LTS, which allows to extend \nthe cer\u00adti.cation method straightforwardly: the optimization is proved .rst and a de-optimized reduced-LTS \nis derived; then compi\u00adlation is proved in the previous sense (only IT must be adapted since pX and pL \nare modi.ed).  In the following, we address some families of optimizations; the issue of certifying \nseries of optimizations is considered afterwards. Extensive references about optimizations can be found \nin [1, 3]. We do not claim to be exhaustive (which would not be doable here); yet, the methods presented \nbelow are general and can be extended to more involved transformations than the mere compilation con\u00adsidered \nin Sect. 4. We examplify these methods on representative members of several classes of optimizations. \n 5.2 Code Simpli.cation: Dead Code/Variable Elimination Dead-Code Elimination: In case a compiler detects \nthat some piece of the source code is unreachable (typically after constant propagation), it may not \ncompile it: then, some source program points are not mapped to any assembly program point. This opti\u00admization \nis correct in the sense of Def. 1, since correctness was de.ned by the data of a bijection between subsets \nof the source and assembly program points Lr SLs and Lr SLa: eliminated program sa points should not \nappear in Lr. Non-eliminated points are expected s to appear somewhere in the mapping, therefore we may \nenvisage to check that the source program points that do not belong to Lr ac\u00ad s tually are dead-code \n(optimization checking). Dead-code analyses done by compilers are most often quite simple, so this should \nbe checked by a rather coarse dead-code analysis. Hence, no exten\u00adsion of the reduced-LTS notion is required \n(the transformation is veri.ed at the source reduced-LTS elaboration level). Dead-Variable Elimination: \nMost compilers do liveness analysis in order to avoid storing dead-variables. A slightly more compli\u00adcated \nde.nition of the variable mapping pX can accommodate this further transformation. Indeed, the abstract \ncounterpart of a source variable depends on the program point and may not exist at some points, so pX \n: Lrs xXs -Xa {0/}where pX (ls,xs)=0/ means that xs has no assembly counterpart at point ls and should \nnot be live at that point if the optimization is correct. Copy propagation and register coalescing: Compilers \nattempt to keep a variable that is used several times in a piece of code in a register and do not store \nit back to memory before using it again. For instance, the load instructions l4 a ,la 11 in the example \npro\u00ad 8 and la gram of Fig. 2(b) could be eliminated, since the variable i is stored in register r0 everywhere \nin the loop. The same approach as for dead-variable elimination works here. 5.3 Order Modifying Transformations: \nInstruction Level Parallelism We envisage now the case of transformations which compromise the correspondence \nof program points; our study focuses on in\u00adstruction scheduling. Instruction Level Parallelism (ILP or \nschedul\u00ading) aims at using the ability of executing several instructions at the same time featured by \nmodern architectures, so as to cut down the cost of several cycles long instructions. Hardware scheduling \nis not a dif.culty, since its soundness stems from the correctness of the processor (which should be \naddressed separately); hence, we con\u00adsider software scheduling only ([1], Chap. 20). Software schedul\u00ading \ndoes not .t in the correctness de.nition presented in Sect. 2.2, since the pieces of code corresponding \nto distinct source statements might be inter-wound (assembly instructions can be permuted). In\u00addeed, \nFig. 3 displays an example of software scheduling: the body of the loop of the example program of Fig. \n2 is optimized so as to reduce pipeline stalls. If load, store and arithmetic instructions all have a \nlatency of one cycle, then the execution of the code of Fig. 3(b) yields one cycle stall against four \nin the case of the (non optimized) code of Fig. 3(a). The pieces of code corresponding to the statements \nls -l4 s and ls -l5 s are inter-wound, so l4 s has no 34 counterpart in the optimized program. The mapping \npL is no longer de.ned; pX becomes a function from (Ls xX)to ](La xX). Be\u00adfore we extend the algorithms \nproposed in Sect. 4, we need to set up a new notion of assembly program points, so as to set up again \na mapping between Ps and Pa. Correctness of Compilation: Let us consider a source execution state of \nthe form (l4s ,.s)in the program of Fig. 2. This state corre\u00adsponds to a state of the form (la 11,.a)in \nthe program of Fig. 3(a). However, it does not correspond to a unique state in the optimized program \n(Fig. 3(b)): the value of i is effectively computed at point lo 14, whereas the value of t + i is read \nby the next statement at point lo 11. Therefore, a source execution state (ls ,.)corresponds to a se\u00adquence \nof states in the optimized program; hence, we propose to de.ne a new assembly observational abstraction \nar by extending a the Fa function (Sect. 2.2). The new Fa should rebuild the orig\u00ad l8 a load r0,i (0) \nlo load r0,i (0) 8 l9 a add r0,r0,1 l9 o load r1,x (0) la lo 10 store r0,i (0) add r0,r0,1 10 la lo \n11 load r1,x (0) load r2,t (r0) 11 la load r2,t (r0) lo add r1,r1,r2 12 12 la add r1,r1,r2 lo store \nr0,i (0) 13 13 la lo 14 store r1,x (0) store r1,x (0) 14 la lo 15 15 (a) Unoptimized code (b) Optimized \ncode Figure 3. Software Scheduling inal control points, by associating so-called .ctitious points to \nse\u00adquences of labels in the optimized code: DEFINITION 3(FICTITIOUS STATE). In case ls ELs corre\u00adsponds \nto the sequence of assembly points l0 a ,,lna, we introduce a .ctitious label l f representing the sequence \nand a set of .ctitious memory locations Xlf SLa xX representing the memory locations observed at that \npoint. Furthermore, we assert that pL(ls)=lf. The .ctitious state (lf ,. f )is associated to the sequence \nof states (l0 a ,.0a),,(lna ,.na)if and only if .f is de.ned by V(lia ,xa)E Xf ,. f (xa)=.ai (xa). Note \nthat the de.nition of the .ctitious states is not ambiguous: in case pX (ls ,xs)nXlf contains more than \none point, the de.nition of . f (xs)does not depend on the choice of the corresponding .ctitious location, \nsince V(lia ,xa),(lia . ,xa)EpX (ls,xs),.ia(xa)=.ia . (xa)if pX is correct (in practice, the equality \nanalysis of Sect. 4.1 subsumes the veri.cation of the correctness of pX ). The new Fa operator inputs \na trace s and builds up a .ctitious trace s f by concatenating .ctitious states corresponding to the \nsequences of states in s in the same order as they appear in s. As in Sect. 2.2, the new ar operator \ncan be de.ned by ar (E)={Fa(s)s EE} aaand the transformation is said to be correct if ar([PsD)'ar ([PaD). \nsa EXAMPLE 4. The mapping of the .rst and of the last points of the loop are unchanged: source variables \ni,x,t at point l3 s cor\u00adrespond to assembly locations of addresses i , x, t at point l8 o and the same \nfor points l5 s and lo Yet, at point l4s, (lo 15. 14,i)Ep X (l4s ,i); (l9 o ,x )EpX (l4s ,x)and (l9 o \n,t +k),,(lo 4,t[k]). This 14,t +k)Ep X (ls mapping de.nes a corresponding .ctitious point l2 f for l4s. \nFic\u00adtitious locations are (lo 9 ,x ), (lo 11,t [k])and (lo 11,ri)(the last 14,i), (l o choice is arbitrary). \nThe .ctitious point l 1 f (resp. l 3 f ) corresponds to the point l8 o (resp. lo are those of 15). Fictitious \nlocations for l f 1 l8 o and the same for l 3 f . The compilation of Ps (Fig. 2(b)) into Po (Fig. 3(b)) \nis correct according to the new compilation correctness de.nition. Computing Reduced-LTS: The computation \nof the reduced-LTS is based on the de.nition of STFs between .ctitious points, which is technical but \neasy. The TV (Th. 5) and the IC (Th. 4) algorithms are extended straightforwardly to the reduced-LTS \nbased on .ctitious points (which is a reduced-LTS in the sense of Sect. 3.3). EXAMPLE 5. The transfer \nfunction dl f displayed below can be 2 Ilf 3 proved equivalent to dl3s Ils =lisindex(t,i)? lx +x +t[i]D \n4 since the .ctitious locations (lo 14,i )contain the same 11,r0)and (lo value as would be shown by a \nsimple extension of the equality anal\u00adysis: dlf =lisaddr((lo )+(lo 2 Ilf 11,t11,r0))? 3 l(lo lo 9 ,x)+ \ncontent((lo )+(11,r0)), 11,t15,r1)+(lo (lo lo 15,r2)+content((lo )+(11,r0)), 11,t (lo  lo 9 ,x)+ content((lo \n)+(11,r0)) 15,x)+(lo 11,t D IC is similar (the same STFs are interpreted in the abstract domain instead \nof symbolically). Invariant Translation: The IT algorithm (Th. 3) should be adapted so as to produce \nresults for the assembly LTS (and not only for the reduced-LTS): a .ctitious point is a label of the \nreduced-LTS but not of the assembly LTS. Indeed, the above extension of Def. 1 allows to derive a local \ninvariant for each point of the reduced-LTS only (i.e. for each .ctitious point). The solution to this \nproblem also comes from the STFs formal\u00adism. Let la ELa; let us compute a local invariant for la.If lf \nis a .ctitious point corresponding to the sequence l0 a ,,la such n that pX (ls)=lf , a sound local invariant \nI (lf )can be derived for lf . Then, if there exists a path c =la ,,lna ,la ,,lma ,la, 0 n+1which does \nnot encounter any other .ctitious states of the reduced- LTS a sound abstract upper-approximation of \nthe set of stores {. (,(l0 a ,.0a),,(lna ,.na),(lna +1,.na +1),,(lma ,.ma ),(la,.))E[PaD} can be obtained \nby applying the abstract STF between lf and la along the path c to I (lf )(this STF can be computed in \nthe same way as an STF between .ctitious states). A sound invariant I (la) can be derived by merging \nthese abstract upper-approximations (as in Sect. 4.1). The same ideas apply to other optimizations which \nmove locally some computations, like code motion, Partial Redundancy Elimi\u00adnation (with a more general \nequality analysis than in Sect. 4.1) or Common Subexpressions Elimination. 5.4 Path Modifying Transformations: \nLoop Unrolling Most compilers carry out structure modifying optimizations such as loop unrolling and \nbranch optimizations (these transformations reduce time in branchings and interact well with the scheduling \nop\u00adtimizations considered in Sect. 5.3). These transformations break the program point mapping pL in \na different way: one source point may correspond to several assembly points (not to a sequence of points). \nIn this section, we focus on loop unrolling. This optimiza\u00adtion consists in grouping two successive iterations \nof a loop, as is the case in the example below (we use source syntax for the sake of convenience and \nconcision; i ++represents i :=i +1): l0: i :=0; l0: i :=0; l1: while(i <2n){l1Ie : while(i <2n){ l2: \nB; l2Ie : B; l3Ie : i ++; l3: i ++;}l1Iol2Io : B; l3Io : i ++;} l4: l4: We write P for the original code; \nP for the optimized one. Correctness of Compilation: The source point l2 corresponds to two points in \nP (l2Ie and l2Io); and the same for l3. We also du\u00ad plicated l1 into l1Ie and l1Io for the sake of the \nexample. The in\u00addex e (resp. o) is used for points where the value of i is even (resp. odd). As in Sect. \n5.3, extending the compilation correct\u00adness de.nition reduces to de.ning a new Fa function. Indeed, Fa \nshould collapse the pairs of duplicated points into a same point, mapped to the original point. We write \nfa : L -L for the pro\u00adgram point collapsing function. For instance, in the above example fa(l2Ie)=fa(l2Io)=l2. \nThen, the new assembly erasure operator is de.ned by: Fa(((l,.),,(l ,. )))=((fa(l),.),,(fa(l ),. )) The \nonly transformation operated on P is the unrolling; conse\u00adquently, the erasure operator Fs is the identity. \nFor instance, if we consider the run of P : s =((l0,.0),(l1Ie,.1),(l2Ie,.2),(l3Ie,.3),(l1Io,.4),(l2Io,.5)) \nthen: Fa(s)=((l0,.0),(l1,.1),(l2,.2),(l3,.3),(l1,.4),(l2,.5)) Given Fa(s)E[PD, the correspondence of \nFa(s)with a trace in Fs([PD)=[PDis trivial. Translation Validation: The above generalization of Def. \n1 cannot be taken into account at the reduced-LTS level, so the IC and TV certi.cation algorithms (Th. \n4 and Th. 5) must be adapted. More precisely, a translation veri.er should prove the following: Vls,ls \nELs,Vla EpL(ls),V.s ES, V.a E{. (,(la,.),)E[PaD}, (3).s 'pX .a ==:la EpL(ls)such that [dlsIl. D(.s)[dlaIl. \nD(.a) s 'pXa Intuitively, the formula states that for any transition on an edge ls,ls in the source LTS \nfrom a state (ls,.s)and for any state (la,.a)in the target LTS in correspondence with (ls,.s)there exists \na transition in the target LTS simulating the source transition. Note the additional assumption .a E{.a \n(,(la,.a),)E[PaD}in (3), which was not present in (2): it is due to the partitioning of source states \nfor a same source point by several different assembly points (like l2Ie and l2Io). For instance, in the \ncase of the example, the transfer function dl1Il2 =li <2n ? . Dcorresponds to two transfer functions: \ndl1.eIl2.e =li <2n ? . Dand dl1.oIl2.o =.. At point l1Io the value of i belongs to [1,2n-1]; the equivalence \nof dl1.oIl2.o (.)and of dl1Il2 (.) can be proved for the stores which achieve that property. A simple \nauxiliary analysis may be required to collect this kind of properties. The case of invariant checking \n(Th. 4) is similar. Invariant Translation: The derivation of a sound invariant for P from a sound abstract \ninvariant I for P is rather straightfor\u00adward, since the new de.nition of compilation correctness entails \nthat Vls ELs,Vla EpX (ls),{. ES (,(la,.))E[PaD}S{. ES (,(ls,.))}E[PsD. Consequently, if la EpX (ls), \nthen I (ls)is a sound invariant for la. A more precise invariant can be computed for the target program \nif the source analysis uses a control partitioning compatible with the transformation for instance a \nloop unrolling in the current ex\u00adample. Many optimizations which alter the branching structure of pro\u00adgrams \ncan be handled by doing similar generalizations. Function inlining also falls in that case. 5.5 Structure \nModifying Transformations: Loop Reversal Some optimizations focus on small and complex pieces of code \nand operate important reordering transformations, like loop rever\u00adsal, loop interchange, loop collapsing. \nFor instance, loop reversal modi.es a loop in the following way: l0: i :=0; l0: i :=n -1; l1: while(i \n<n){l1: while(i ;0){ l2: B; l2: B; l3: i :=i +1;}l3: i :=i -1;} l3: l3: We write P for the original code; \nP for the optimized one. Loop reversal may allow to use a speci.c branch if counter is null instruction \nfeatured by some architectures. Such a transformation is not always legal, so compilers generally check \nthat the trans\u00adformation does not break any loop carried data-dependence, which we assume here (in case \nthe transformation would break a data\u00addependence, it is illegal; hence, it is not performed). Correctness \nof Compilation: Loop reversal changes the order of execution of some statements, so the adaptation of \nthe compilation correctness de.nition requires new Fa and Fs functions to be de\u00ad.ned so as to forget \nthe order of execution of some pieces of code. More precisely, the assignments done inside the loop are \nthe same since the transformation does not break any data-dependence, but they may be executed in a different \norder. Therefore, the new obser\u00advational operators should forget the order the values are computed in \nat some program points. Hence, Fs should replace a sequence s of states inside the loop by a function \nf from {l2,l3}xX to the set ](R)such that f(l2,x)={.(x)s =(,(l2,.),)}and the same for f(l3,x)(the de.nition \nof Fa is similar). Furthermore, the value of the induction variable i is modi.ed before and after the \nloop, so Fa and Fs should abstract it away. The observational se\u00admantics of P and P is a set of sequences \nmade of states (outside of the reversed loop) and functions de.ned as above (representing a run inside \nthe reversed loop). The compilation is correct if and only if these semantics are in bijection, as in \nDef. 1. Translation Validation: Loop reversal radically changes the struc\u00adture of the program, so it \nis not possible to operate TV without resorting to a global rule, proving the correctness of the transfor\u00admation, \nas done in [23, 24]. The reversal is legal if and only if the permutation of two iterates is always legal \nand the data dependences are preserved. Hence, TV should check the absence of loop-carried dependences \nand the following equivalence (B[i +x]denotes the substitution of the variable i by the expression x \nin B): Vx,y E,B[i +x];B[i +y]B[i +y];B[i +x] For instance, in case the code of B is encoded into an STF \nd, the above equivalence can be established by the algorithm of Sect. 4.2 applied to the STFs dB Eli \n+yEdB Eli +xand dB Eli + xEdB Eli +y. Invariant Translation: We .rst remark that the IT algorithm of \nTh. 3 still works for the program points outside the reversed loop. Let us consider a program point inside \nthe loop for instance, l2. The correctness of compilation entails the equality {.(x) (,(l2,.),)E[PD}={.(x)(,(l2,.),)E[P \nD},so we can determine which classes of invariants can be translated: Case of non-relational invariants: \nThe invariant [PDis non rela\u00adtional if it is of the form L -(X -D )where D is a domain . for representing \nsets of values ((](R),S)+--(D ,)). Then, a the soundness of [PDboils down to Vl EL,Vx EX,{.(x) (,(l,.),)}S.([PD(l)(x)). \nHence, the above equality im\u00adplies that the non-relational invariant can be translated if the com\u00adpilation \nis correct. Case of relational invariants: Examples can show that relational properties may not be preserved \nby the translation in presence of globally reordering transformations. Similar arguments would extend \nto other reordering transforma\u00adtions like loop interchange, coalescing. 5.6 Results Accommodating optimizations: \nOur framework turns out to al\u00adlow the certi.cation of optimizing compilation, since a wide range of optimizing \ntransformations were considered with success. Han\u00addling a new optimization generally starts with the \nextension of the compilation correctness de.nition, by tuning the observational se\u00admantics operators \nFs and Fa and by generalizing the mappings pX and pL. De.ning more abstract observational operators allows \nto accept more optimizations; yet, these abstractions may render the translation of some classes of invariants \nimpossible as was the case in Sect. 5.5. The certi.cation algorithms can be extended to a wide range \nof optimizations: Code Simpli.cation optimizations (Sect. 5.2) are handled straightforwardly  Path \nmodifying transformations (Sect. 5.4) can be accommo\u00addated by generalizing the reduced-LTS notion and \nthe certifying procedures.  The locally order modifying transformations like scheduling (Sect. 5.3) \ncan be certi.ed without changing the reduced-LTS de.nition: the computation of reduced-LTS is more involved \nbut the further steps of certi.cation (TV or invariant checking) can be mostly preserved. Only IT must \nbe adapted (in a rather sys\u00adtematic way).  The case of globally reordering optimizations (Sect. 5.5) \nis not so satisfactory, since it involves global rules; yet such transforma\u00adtions should be rather localized \n(e.g. applied to some small loops only), so their certi.cation should still be amenable. Moreover, this \nlast class of optimizations is not so important as the previ\u00adous ones in the case of practical C compilers \n(GNU gcc, Diab C compiler...).  Real compilers perform series of optimizations. A de.nition of compilation \ncorrectness for a sequence of optimizations can be de\u00adrived by composing the observational abstractions \ncorresponding to each transformation. In case the compiler provides the intermedi\u00adate programs, the certi.cation \n(by TV or IT) can be carried out in several steps so as to cut down the dif.culty of mapping the source \nand target code. Implementation and practical aspects: We had not planed to ex\u00adtend the certi.er of Sect. \n4.4 to the certi.cation of optimized code when implementing it. However, the implementation of a new \nmod\u00adule in the frontend allowed to tackle part of the optimizations en\u00advisaged above and other similar \ntransformations: the code simpli\u00ad.cation transformations (Sect. 5.2), the local reordering optimiza\u00adtions \n(Sect. 5.3) and some limited forms of path modifying trans\u00adformations (Sect. 5.4) i.e. branching local \ntransformations. The new version of the certi.er is effective for the .rst two classes of optimizations. \nLoop unrolling cannot be proved due to a lack of adequate information in the compiler output; localized \nbranching transformations are certi.ed successfully. The gcc compiler does not perform any reordering \ntransformation in the sense of Sect. 5.5, so we did not attempt to certify them. Finally, the new certi.er \ncan handle code produced by gcc with the option -O2 (quite high op\u00adtimizing level; no possibly misleading \nglobal optimization is per\u00adformed; the highest level -O3 mainly adds function inlining and loop unrolling), \nwhich is rather positive given the additional im\u00adplementation effort is very reasonable: the additional \nfrontend .les amount to about 3000 lines of OCaml code (the whole compila\u00adtion and assembly certi.er \namounts to about 33000 lines). We also noted an important slowdown of the assembly frontend, mainly due \nto more complex interfaces and to the additional computations done to build the reduced-LTS (partial \ncomposition of STFs as mentioned in Sect. 5.3). The main dif.culty in extending the certi.er consists \nin .nding ad\u00adequate debug information, so as to know which transformation has been performed. The new \ncerti.er still uses the same standard de\u00adbug information format, which does not provide much data about \noptimizations. Moreover, these information turn out to be some\u00adtimes wrong in the case of optimizing \ncompilation: some scopes of variables or program point mappings were wrong, so we had to .x some debug \nentries by hand. The precise mapping of program points also had to be partially reconstructed by applying \nsome non trivial processing to the Stabs debugging information. The stan\u00addard debugging information formats \nprovided by most compilers are mostly aimed at debugging only and not at mapping source and compiled \nprograms for the sake of veri.cation. We may suggest the introduction of a more adequate debugging information \nformat for this purpose. Our new certi.er does not use any intermediate RTL (Register Transfer Language) \ndump provided by the gcc compiler and car\u00adries out the validation in one pass. However, it seems that \nhandling more complicated optimizations would require a several steps pro\u00adcess to be implemented as is \nthe case of the systems of [17] and [23, 24], which both address more elaborate optimizations.  6 Conclusion \nWe provided a framework for reasoning about the certi.cation of compilation and of compiled programs. \nIt is mainly based on a symbolic representation of the semantics of source and compiled programs and \non structures to check properties of the .xpoints of the semantic operators of source and compiled programs, \nhence of the semantics of source and compiled programs. Existing certi\u00ad.cation methods were integrated \nto the framework; furthermore, a new method for certifying assembly programs was proposed, which is based \non the ability to prove the semantic equivalence of pro\u00adgrams .rst: this equivalence checking not only \njusti.es IT, but also provides more precise information about the structure of assem\u00adbly programs, which \nrenders IT more ef.cient and accurate. In the implementation point of view, this approach proved very \nsuccess\u00adful, since strong properties about very large real examples of safety critical programs were \nproved at the assembly level. This success was made possible by a .ne description of the equivalence \nbetween source and compiled programs. Moreover, many classes of optimizations can be successfully ac\u00adcommodated \nin our framework. Part of them are handled at the reduced-LTS elaboration level (as code simpli.cation \nor locally or\u00adder modifying transformations). Path modifying or globally order modifying transformations \nrequire a stronger treatment. The invari\u00adant translation result derives from a precise de.nition of compila\u00adtion \ncorrectness, which allows to derive systematically which class of properties of the source extends to \nthe target code. Last, our approach allows to factor some problems out of the design of an adequate certi.cation \nprocess. For instance, the symbolic rep\u00adresentation of programs is shared by IT, IC and TV. In the practi\u00adcal \npoint of view, some steps are also common, as is the gathering of mapping data. Note that the approach \nis compiler independent (no instrumentation of the compiler is required; the certi.er only needs some \nstandard information) and even architecture indepen\u00addent. Indeed, accommodating a new target language \namounts to implementing a new frontend compiling programs into a symbolic representation (i.e. reduced-LTS), \nwhich formalizes the semantic model of the new assembly language. Acknowledgment: We deeply acknowledge \nB. Blanchet, J. Feret and C. Hymans for their comments on an earlier version. We thank the members of \nthe Astree project for stimulating discussions: B. Blanchet, P. Cousot, R. Cousot, J. Feret, L. Mauborgne, \nA. Min\u00b4e and D. Monniaux. 7 References [1] A. W. Appel. Modern Compiler Implementation in ML. Cam\u00adbridge \nUniversity Press, 1997. [2] A. W. Appel. Foundational Proof-Carrying Code. In 16th LICS, pages 247 256, \n2001. [3] D. F. Bacon, S. L. Graham, and O. J. Sharp. Compiler trans\u00adformations for high-performance \ncomputing. ACM Comput\u00ading Surveys, 26(4):345 420, 1994. [4] B. Blanchet, P. Cousot, R. Cousot, J. Feret, \nL. Mauborgne, A. Min\u00b4e, D. Monniaux, and X. Rival. Design and Imple\u00admentation of a Special-Purpose Static \nProgram Analyzer for Safety-Critical Real-Time Embedded Software, invited chap\u00adter. In The Essence of \nComputation: Complexity, Analysis, Transformation. Essays Dedicated to Neil D. Jones, LNCS 2566, pages \n85 108. Springer-Verlag, 2002. [5] B. Blanchet, P. Cousot, R. Cousot, J. Feret, L. Mauborgne, A. Min\u00b4e, \nD. Monniaux, and X. Rival. A Static Analyzer for Large Safety Critical Software. In PLDI 03, pages 196 \n207, 2003. [6] C.-L. Chang and R. C.-T. Lee. Symbolic Logic and Mechan\u00adical Theorem Proving. Academic \nPress, Computer Science Classics, 1973. [7] C. Colby and P. Lee. Trace-Based Program Analysis. In 23rd \nPOPL, pages 195 207, St. Petersburg Beach, (Florida USA), 1996. [8] P. Cousot and R. Cousot. Abstract \nInterpretation: a uni.ed lattice model for static analysis of programs by construction or approximation \nof .xpoints. In 4th POPL, pages 238 252, 1977. [9] P. Cousot and R. Cousot. Constructive versions of \nTarski s .xed point theorems. Paci.c Journal of Mathematics, 81(1):43 57, 1979. [10] P. Cousot and R. \nCousot. Abstract Interpretation and Appli\u00adcation to Logic Programs. Journal of Logic Programming, 13(2 \n3):103 179, 1992. [11] P. Granger. Static Analysis of Arithmetical Congruences. Int. J. Computer. Math., \n1989. [12] X. Leroy, D. Doligez, J. Garrigue, D. R\u00b4emy, and J. Vouillon. The Objective Caml system, documentation \nand user s man\u00adual (release 3.06). Technical report, INRIA, Rocquencourt, France, 2002. [13] A. Min\u00b4e. \nThe Octagon Abstract Domain. In Analysis, Slicing and Transformation 2001 (in WCRE 2001), IEEE, 2001. \n[14] G. Morrisett, K. Crary, N. Glew, D. Grossman, R. Samuels, F. Smith, and D. Walker. TALx86: A Realistic \nTyped Assem\u00adbly Language. In WCSSS, 1999. [15] G. Morrisett, D. Tarditi, P. Cheng, C. Stone, R. Harper, \nand P. Lee. The TIL/ML Compiler: Performance and Safety Through Types. In WCSSS, 1996. [16] G. C. Necula. \nProof-Carrying Code. In 24th POPL, pages 106 119, 1997. [17] G. C. Necula. Translation Validation for \nan Optimizing Com\u00adpiler. In PLDI 00. ACM Press, 2000. [18] G. C. Necula and P. Lee. The Design and Implementation \nof a Certifying Compiler. In PLDI 98. ACM Press, 1998. [19] A. Pnueli, O. Shtrichman, and M. Siegel. \nTranslation Valida\u00adtion for Synchronous Languages. In ICALP 98, pages 235 246. Springer-Verlag, 1998. \n[20] X. Rival. Abstract Interpretation-based Certi.cation of As\u00adsembly Code. In 4th VMCAI, New York (USA), \n2003. [21] D. Tarditi, G. Morrisett, P. Cheng, C. Stone, R. Harper, and P. Lee. TIL: A Type-Directed \nOptimizing Compiler for ML. In PLDI 96, pages 181 192. ACM Press, 1996. [22] H. Xi and R. Harper. A dependently \ntyped assembly lan\u00adguage. In International Conference on Functional Program\u00adming, Florence, Italy, September \n2001. [23] L. Zuck, A. Pnueli, Y. Fang, and B. Goldberg. VOC: A Translation Validator for Optimizing \nCompilers. In Electronic Notes in Theoretical Computer Science, 2002. [24] L. Zuck, A. Pnueli, Y. Fang, \nB. Goldberg, and Y. Hu. Trans\u00adlation Run-Time Validation of Optimized Code. In Electronic Notes in Theoretical \nComputer Science, 2002. A An STFs-based Semantics of a Subset of C This appendix gives an STFs-based \nsemantics for the subset of C presented on Fig. 1(a). We suppose there is a label before each statement. \nA location is either an integer variable or an array variable. Values are integers (hence, SR): over.ows \nare not taken into account. The STFs de.ne the semantics of statements; hence they describe for each \nstatement the error cases and the transformation of the store in case no error happens. Therefore, we \nintroduce in the next sub\u00adsection an operation which inputs a symbolic transfer function d and builds \nup a symbolic transfer function d such that d carries out the same transformation as d in case the evaluation \nof a given L-value lv (resp. an expression e, a condition c) succeeds and such that d fails if the evaluation \nof lv (resp. e, c) fails. Then, we give the semantics of statements and blocks. REMARK 2(ALTERNATE APPROACH). \nA valuable approach consists in modifying the semantics of STFs so as to allow fail\u00adures inside expressions. \nIn case the evaluation of e or lv fails in the environment ., then [llv +eD(.)=.S. This approach proved \nef.cient in practice; yet, it would have made the presentation of the paper more involved, so we restrict \nto the approach evoked in the previous paragraph. A.1 Errors in L-values, Expressions and Conditions \nWe de.ne three operators Clv : Lvx1-1, Ce : Ex1-1and Cc : Cx1-1. Intuitively, Clv(lv,d)tests whether \nthe evaluation of lv fails; in case lv does not fail, then it carries out the same action as d. The operators \nCe and Cc behave similarly. They are de.ned by mutual induction (as usual, x denotes a variable; e an \nexpression; c a condition; lv an l-value, n an integer and d an STF): Clv(x,d)=d Clv(x[e])=Ce(e,l(0 :e)I(e \n<n)? d D) (n is the size of the array x) Ce(n,d)=d Ce(lv,d)=Clv(lv,d) Ce(e0 Ee1,d)=Ce(e0,Ce(e1,d))(EE{+,-,*}) \nCe(e0/e1,d)=Ce(e0,Ce(e1,le1 =0? e0/e1 D)) Cc(c,d)=d (c E{true,false}) Cc(c,d)=Cc(c,d) Cc(c0 Ec1,d)=Cc(c0,Cc(c1,d))(EE{I,v}) \nCc(e0 Ee1,d)=Ce(e0,Ce(e1,d))(EE{<,==}) For instance, if t is an array of length 4, then Ce(18/t[i],d)=l(0 \n: i)I(i <4)? lt[i]=0? d DD. If t is an array of length n, the predicate isindex(t,i)is de.ned by isindex(t,i).((0 \n:i)I(i <n)). In case over.ows would be taken into account and the range for the integers would be [Nmin,Nmax], \nthen some of the case above would be modi.ed. For instance, the addition would be handled as follows: \nCe(e0 +e1,d)=l(Nmin :e0 +e1)I(e0 +e1 :Nmax)? d D A.2 Symbolic Transfer Functions for Statements We present \nhere the transfer functions for each language construc\u00adtion (the STFs that are not mentioned explicitly \nare D): Case of an assignment l : lv :=e; l :: dlIl. =Clv(lv,Ce(e,llv +e)) Case of a conditional l : \nif(c){lt : Bt ;lt }else {lf : Bf ;lf }; l :: dlIlt =Cc(c,lc ? . D) dlIlf =Cc(c,lc ? D. ) dlt .Il. =dl. \nf Il. =. Case of a loop l : while(c){lb : Bb;lb}; l :: dlIlb =dl. =Cc(c,lc ? . D) bIlb =dl. =Cc(c,lc \n? D. ) dlIl. bIl. For instance, if t is an array of length 4 and x is an integer variable, then the assignment \nl : x :=18/t[i]; l : is represented by the STF: dlIl. =l(0 :i)I(i <4)? lt[i],0? lx +18/t[i] =D D  B \nSTFs-based Semantics of an Assembly Language We keep the notations of Fig. 1(b) In the following, if \nx is an integer, then: x denotes the address of x;  isaddr(x)is the predicate x is a valid address \nin the input store ;  content(x)denotes the content of the memory cell of address x in the input store \n(addresses are integers).  The values are integers and comparison values. A comparison value is either \nLT (which means less than ), or EQ ( equal ) and GT ( greater than ). We do not take over.ows into account. \nHence, {LT,EQ,GT}SR. We list the STFs de.ning the semantics of the assembly language presented on Fig. \n1(b) (the STFs that are not presented explicitly are D): the load integer instruction l : li r0,n; l \n: loads the integer n into the register r0: dlIl. =lr0 +n the load instruction l : loadr0,x (v); l : \nloads the content of the memory cell of address x +v (v is either an integer constant or the content \nof a register) if x +v is a valid address (if not, it fails): dlIl. =lisaddr(x +v)? lr0 +content(x +v)D \nthe store instruction l : storer0,x (v); l : stores the content of the register r0 in the memory cell \nof address x +v if x +v is a valid address (if not, it fails): dlIl. =lisaddr(x +v)? lx +v +r0D the compare \ninstruction l : cmp r0,r1; l : compares the con\u00adtent v0 and v1 of the registers r0 and r1;if v0 <v1 then \nthe value of the condition register is set to LT; if v0 =v1, then the value of the condition register \nis set to EQ; if v0 >v1, then the value of the condition register is set to GT: dlIl. =lr0 <r1? lcr +LT \nlr0 =r1? lcr +EQlcr +GT the conditional branching instruction l : bc(<)l ; l : branches to l or to the \nnext instruction depending on the value stored in the condition register: dlIl. =lcr =LT ? D. dlIl.. \n=lcr =LT ? . D the branching instruction l : b l ; l : branches to label l : dlIl.. =. dlIl. =D the addition \ninstruction l : add r0,r1,v; l : adds the content of the register r1 and the value v (v is either the \ncontent of a register r2 or an integer constant n) and stores the result in the register r0 (and the \nsame for the subtract and the multiply instructions): dlIl. =lr0 +r1 +v In case the valid integer values \nwould be [Nmin,Nmax]nand if over.ows were taken into account, then the de.nition of the transfer function \nwould be: dlIl. =l(Nmin :r1 +v)I(r1 +v :Nmax)? lr0 +r1 +v D the division instruction l : divr0,r1,v; \nl : fails if the divisor is equal to 0: dlIl. =,0? lr0 +r1/vD lv =   \n\t\t\t", "proc_id": "964001", "abstract": "We present a framework for the certification of compilation and of compiled programs. Our approach uses a symbolic transfer functions-based representation of programs, so as to check that source and compiled programs present similar behaviors. This checking can be done either for a concrete semantic interpretation (Translation Validation) or for an abstract semantic interpretation (Invariant Translation) of the symbolic transfer functions. We propose to design a checking procedure at the concrete level in order to validate both the transformation and the translation of abstract invariants. The use of symbolic transfer functions makes possible a better treatment of compiler optimizations and is adapted to the checking of precise invariants at the assembly level. The approach proved successful in the implementation point of view, since it rendered the translation of very precise invariants on very large assembly programs feasible.", "authors": [{"name": "Xavier Rival", "author_profile_id": "81100659525", "affiliation": "&#201;cole normale sup&#233;rieure, Paris Cedex, France", "person_id": "P502800", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/964001.964002", "year": "2004", "article_id": "964002", "conference": "POPL", "title": "Symbolic transfer function-based approaches to certified compilation", "url": "http://dl.acm.org/citation.cfm?id=964002"}