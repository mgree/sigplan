{"article_publication_date": "01-01-2004", "fulltext": "\n Abstractions from Proofs. Thomas A. Henzinger Ranjit Jhala Rupak Majumdar Kenneth L. McMillan EECS Department, \nUniversity of California Cadence Berkeley Labs. Berkeley, CA 94720-1770, U.S.A. Berkeley, CA, U.S.A. \n{tah,jhala,rupak}@eecs.berkeley.edu mcmillan@cadence.com Abstract The success of model checking for large \nprograms depends cru\u00adcially on the ability to ef.ciently construct parsimonious abstrac\u00adtions. A predicate \nabstraction is parsimonious if at each control location, it speci.es only relationships between current \nvalues of variables, and only those which are required for proving correct\u00adness. Previous methods for \nautomatically re.ning predicate ab\u00adstractions until suf.cient precision is obtained do not systemati\u00adcally \nconstruct parsimonious abstractions: predicates usually con\u00adtain symbolic variables, and are added heuristically \nand often uni\u00adformly to many or all control locations at once. We use Craig inter\u00adpolation to ef.ciently \nconstruct, from a given abstract error trace which cannot be concretized, a parsominous abstraction that \nre\u00admoves the trace. At each location of the trace, we infer the relevant predicates as an interpolant \nbetween the two formulas that de.ne the past and the future segment of the trace. Each interpolant is \na relationship between current values of program variables, and is relevant only at that particular program \nlocation. It can be found by a linear scan of the proof of infeasibility of the trace. We develop our \nmethod for programs with arithmetic and pointer expressions, and call-by-value function calls. For function \ncalls, Craig interpolation offers a systematic way of generating relevant predicates that contain only \nthe local variables of the function and the values of the formal parameters when the function was called. \nWe have extended our model checker BLAST with predicate dis\u00adcovery by Craig interpolation, and applied \nit successfully to C pro\u00adgrams with more than 130,000 lines of code, which was not possible with approaches \nthat build less parsimonious abstractions. Categories and Subject Descriptors: D.2.4 [Software Engineer\u00ading]: \nSoftware/Program Veri.cation; F.3.1 [Logics and Meanings *This research was supported in part by the \nNSF grants CCR\u00ad0085949, CCR-0234690, and ITR-0326577, by the AFOSR grant F49620-00-1-0327, by the DARPA \ngrant F33615-00-C-1693, and by the ONR grant N00014-02-1-0671. Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. POPL 04, January 14 16, 2004, Venice, Italy. \nCopyright 2004 ACM 1-58113-729-X/04/0001 ...$5.00 of Programs]: Specifying and Verifying and Reasoning \nabout Pro\u00adgrams. General Terms: Languages, Veri.cation, Reliability. Keywords: Software model checking, \npredicate abstraction, coun\u00adterexample analysis. 1 Introduction Increasing dependency on software systems \nampli.es the need for techniques that can analyze such systems for errors and prove them safe. The two \nmost desirable features of such analyses is that they be precise and scalable. Precision is required \nso that the analy\u00adsis does not report errors where none exist, nor assert correctness when there are \nbugs. Scalability is necessary so that the method works for large software systems, where the need for \nanalysis is most acute. These two features are often mutually exclusive: .ow based interprocedural analyses \n[11, 15] achieve scalability by .xing a small domain of data.ow facts to be tracked, and compute .ow \nfunctions over the abstract semantics of the program on this .xed set. For complicated properties, if \nthe set of facts that are tracked is too small, many false positives are reported. Model checking based \napproaches [25] on the other hand offer the promise of precision as they are path-sensitive, but they \noften track too many facts, so state explosion comes in the way of scalability. To avoid the pitfalls \narising from using a .xed set of facts, much re\u00adcent interest has focused on interprocedural analyses \nthat automati\u00adcally tune the precision of the analysis using false positives, i.e., in a counterexample-guided \nmanner. These start with some coarse ab\u00adstract domain and successively re.ne the domain by adding facts \nthat make the abstraction suf.ciently precise to refute spurious counterexamples [4, 5, 8, 12, 19]. The \nfacts are predicates that relate values of programs variables. While this approach holds the promise \nof precision, there are several obstacles that must be over\u00adcome before it can be scaled to very large \nprograms. The .rst chal\u00adlenge is how to ef.ciently analyze a false positive and learn from it a small \nset of predicates such that the re.ned abstraction does not contain the spurious error trace. The second, \nclosely related prob\u00adlem is how to use the discovered predicates parsimoniously. The number of facts \nthat one needs to track grows with the size of the program being analyzed. However, most predicates are \nonly locally useful, i.e., only useful when analyzing certain parts of the program, and irrelevant in \nothers. If locality is not exploited, then the sheer number of facts may render the abstract system too \ndetailed to be amenable to analysis, as the size of the abstract system grows ex\u00adponentially with the \nnumber of predicates. We solve both problems using the following observation: the rea\u00adson why an abstract \ntrace is infeasible is succinctly encoded in a proof that the trace is infeasible, and so the appropriate \nabstraction can be culled from the proof. The dif.culty in extracting the rele\u00advant facts from the proof \nis that the proof uses the entire history of the trace, while our analysis, and hence our facts, should \nrefer at all points of the trace only to relationships between the current val\u00adues of program variables. \nInspired by the use of Craig Interpolation for image-computation in [22], we introduce a method by which \nthe proof can be sliced to yield the relevant facts at each point of the trace. Given an abstract trace, \nwe construct a trace formula (TF), which is the conjunction of several constraints, one per instruction, \nsuch that the TF is satis.able iff the trace is feasible. If the trace is infeasible, then we use Craig \ns interpolation theorem [9] to ex\u00adtract, for each point of the trace, the relevant facts from the proof \nof unsatis.ability of the TF. Given two formulas .-and .+, whose conjunction is unsatis.able, the Craig \ninterpolant of (.-).+)is a formula . such that (i) .-implies ., (ii) . /.+is unsatis.able, and (iii) \n. contains only symbols common to .-and .+.If .-is the part of the TF that represents a pre.x of an infeasible \ntrace, and .+encodes the remainder of the trace, then the Craig interpolant . consists of precisely the \nfacts, as relations between current val\u00adues of the variables, which need to be known at the cut-point \nof the trace in order to prove infeasibility. In this paper, we make the following contributions. First, \nwe show how a proof of unsatis.ability of .-/.+can be mined to build the interpolant .. The method is \nef.cient in that it uses the same theorem proving effort as is needed to produce a proof of unsatis.a\u00adbility: \nthe interpolant is generated by a linear scan of the proof. Sec\u00adond, we show how to infer from the interpolants, \nat each cut-point of an infeasible abstract trace, enough facts to rule out the trace. Moreover, the \ncut-points provide precise information at which pro\u00adgram locations the inferred facts are useful, thus \nenabling a parsi\u00admonious use of predicates. The method can be combined with on\u00adthe-.y lazy abstraction \n[19], and presents an improvement: while in pure lazy abstraction, the set of predicates increases monotoni\u00adcally \nalong a trace, the interpolant predicates may change from one control location to the next, i.e., interpolation \nprovides a procedure for deciding when a predicate becomes irrelevant, and therefore ob\u00adsolete. We show \nthat the method is both sound and complete, in the sense that if an abstract trace is infeasible, then \nthe interpolants al\u00adways provide suf.cient information for proving infeasibility. More\u00adover, when abstractions \nare used as certi.cates for program cor\u00adrectness following the proof-carrying code paradigm [17], then \nour parsimonious use of predicates yields more compact proofs. We illustrate the method on an imperative \nlanguage of arithmetic and pointer expressions with call-by-value function calls. There are two orthogonal \nsources of complexity in generating interpolants. The .rst is function calls and scoping. We want the \nanalysis of a function to be polymorphic in all callers, i.e., the inferred predicates should involve \nonly lvalues that are local to the scope of the func\u00adtion. Here, interpolation provides a procedure for \nsystematically discovering predicates that refer only to (i) the local variables of the function and \n(ii) the values of the formal parameters at the time of the function call. This allows us to keep the \nsubsequent anal\u00adysis interprocedural [1, 29]. The second issue is the presence of pointers and aliasing. \nWe want to generate predicates that soundly and completely capture the semantics of programs with pointers \nand memory allocation. As McCarthy s theory of arrays [21] does not offer suitable interpolants, we need \nto model memory locations individually. Finally we report on our experiences with this new kind of abstrac\u00adtion \nre.nement. We have implemented the method in BLAST [19]. Owing to the fact that we only track a few predicates \nat every pro\u00adgram location, we have been able to precisely model check pro\u00adgrams considerably larger \nthan have been reported before [7, 17], including a driver that consists of 138,000 lines of C code (we \nfound several behaviors that violate the speci.cation). Even though 382 predicates are required in total \nto show correctness, the reason the analysis scales is that the average number of relevant predicates \nat each program location is about 8. 2 Overview Consider the program fragment shown in Figure 1. The \nproperty we wish to check is that locking and unlocking alternate, i.e., between any two calls of lock \nthere must be a call of unlock, and between any two calls of unlock there must be a call of lock. Suppose \nthat the code not shown does not contain any calls of lock or unlock. while(*){ 1: if (p1) lock (); assumep1; \nif (p1) unlock (); lock (); assume-p1; 2: if (p2) lock (); assumep2; if (p2) unlock (); lock (); n: \nif (pn) lock (); if (pn) unlock (); } Figure 1. Program; spurious counterexample. A static analysis \nthat tracks whether or not the lock is held returns false positives, i.e., error traces that arise from \nthe imprecision of the analysis. One such spurious error trace is shown on the right in Figure 1. The \nanalysis is fooled because it does not track the predicate p1 which correlates the .rst two if statements; \neither both happen or neither happens, and either way the error cannot be reached. We would like to make \nthe analysis more precise so that this spurious counterexample is eliminated, and we would like to keep \nre.ning the analysis until we either have a real counterexam\u00adple, or, as in this case, the program is \nproved safe. Various methods can analyze this particular counterexample and learn that the analysis should \ntrack the value of p1. Similar coun\u00adterexamples show that all of the predicates p1))pn must be tracked, \nbut as a result, the analysis blows up, because it is not clear when we can merge states with different \npredicate values, and without merging there are an exponential number of states.1 Notice however that \nin this program, each predicate is only locally useful, i.e., each pi is live only at the statements \nbetween labels i and (not including) i +1. Hence, to make a precise analysis scalable we need a method \nthat infers both the predicates and where they are useful. In our experience, many large software systems \nhave the property that, while the number of relevant predicates grows with the size of the system, each \npredicate is useful only in a small part of the state space, i.e., the number of predicates that are \nrelevant at any particular program location is small. By exploiting this prop\u00aderty one can make a precise \nanalysis scale to large programs. In particular, our algorithm infers the predicates pi and also that \npi is useful only between the labels i and i +1; outside these labels, we can forget the value of pi. \nThus our analysis considers, in this example, only a linear number of distinct states. 1For this particular \nexample certain state representation methods such as BDDs would implicitly merge the states. 1: x :=ctr; \n(xy1)=(ctry0)x =ctr 2: ctr :=ctr +1; (ctry1)=(ctry0)+1 x =ctr -1 3: y :=ctr; (yy2)=(ctry1)x =y -1 4: \nassume(x =m); (xy1)=(my0)y =m +1 5: assume(y =m +1); (yy2)=(my0)+1 Figure 2. Infeasible trace; constraints; \npredicates. The problem, then, is (i) to prove that an abstract trace is infeasible, i.e., it does not \ncorrespond to a concrete program trace, and (ii) to extract predicates from the proof, together with \n(iii) information where to use each predicate, such that the re.ned abstraction no longer contains the \ninfeasible trace. This is not always as simple as in the locking example; consider the infeasible trace \nshown in Figure 2, where x, y, ctr, and i are program variables, :=denotes an assignment, and assumerepresents \nan if statement. Preliminary de.nitions. Suppose that the formula . (over the pro\u00adgram variables) describes \na set of program states, namely, the states in which the values of the variables satisfy .. The strongest \npost\u00adcondition [16] of . w.r.t. an operation opis a formula that describes the set of states reachable \nfrom some state in . by performing the operation op. For an assignment, SP. (x :=e)is (:x'.[x'Ix]/x = \ne[x'Ix]), and for an assumewe have, SP. (assumep)=(. /p). For example, (x =ctr)describes the set of states \nwhere the value of ' x equals that of ctr, and SP(x =ctr)(ctr :=ctr +1)=(:ctrx = ctr'/ctr =ctr'+1). The \noperator SPis extended to sequences of operations by SP. (t1;t2)=SP(SP. t1)t2. A trace t is feasible \nif SPtruet is satis.able. Given a set P of predicates and a formula ., the predicate abstraction of . \nw.r.t. P, writ\u00adten a P . is the strongest formula . (in the implication order\u00ading) such that (i) . is \na boolean combination of predicates in P, and (ii) . implies . . For example, if P ={a =0)b >0}, then \na P (a =b +c /b =2 /c >0)is -(a =0)/(b >0). The ab\u00adstract strongest postcondition of . w.r.t. the operation \nopand pred\u00adicates P is SPp. op=a P (SP. op). This is extended to traces by SPp. (t1;t2)=SPp(SPp. t1)t2. \nA trace t is abstractly fea\u00adsible w.r.t. P if SPptruet is satis.able. The problem is, given an infeasible \ntrace t, .nd a set P of predicates such that t is abstractly infeasible w.r.t. P. Symbolic simulation. \nOne way to solve this problem is to symbol\u00adically simulate the trace until an inconsistent state is reached; \nsuch inconsistencies can be detected by decision procedures [6]. A de\u00adpendency analysis can be used to \ncompute which events in the trace cause the inconsistency, and this set of events can then be heuristi\u00adcally \nminimized to obtain a suitable set of predicates [3, 7]. There are two problems with this approach. First, \nthe inconsistency may depend upon old values of program variables, e.g., in the trace shown, such an \nanalysis would use facts like x equals the value of ctr at line 1, and that the current value of ctr \nis one more than the value at line 1. In general there may be many such old val\u00adues, and not only must \none use heuristics to deduce which ones to keep, a problem complicated by the presence of pointers and \nproce\u00addures, but one must also modify the program appropriately in order to explicitly name these old \nvalues. Intuitively, however, since the program itself does not remember old values of variables, and \nyet cannot follow the path, it must be possible to track relationships be\u00adtween live values of variables \nonly, and still show infeasibility. Second, this approach yields no information about where a predi\u00adcate \nis useful. Example. We now demonstrate our technique on the trace of Fig\u00adure 2. First, we build a trace \nformula (TF) which is satis.able iff the trace is feasible. The TF . is a conjunction of constraints, \none per instruction in the trace. In Figure 2, the constraint for each in\u00adstruction is shown on the right \nof the instruction. Each term (.).) denotes a special constant which represents the value of some vari\u00adable \nat some point in the trace, e.g., (ctr)1)represents the value of ctr after the .rst two instructions. \nThe constraints are essentially the strongest postconditions, where we give new names to variables upon \nassignment [10, 14]. Thus, for the assignment in line 1, we generate the constraint (x)1)=(ctr)0), where \n(x)1)is a new name for the value of x after the assignment, and (ctr)0)is the name for ctr at that point. \nNotice that the latest name of a variable is used when the variable appears in an expression on the right. \nAlso note that the conjunction . of all constraints is unsatis.able. To compute the set P of relevant \npredicates, we could simply take all atomic predicates that occur in the constraints, rename the constants \nto corresponding program variables, create new names ( symbolic variables ) for old values of a variable \ne.g., for (ctr)1)=(ctr)0)+1 create a new name that denotes the value of ctr at the previous instruction, \nand add these names as new vari\u00adables to the program. However, such a set P is often too large, and in \npractice [3, 19] one must use heuristics to minimize the sets of predicates and symbolic variables by \nusing a minimally infeasible subset of the constraints. Craig interpolation. Given a pair (.-).+)of formulas, \nan in\u00adterpolant for (.-).+)is a formula . such that (i) .-implies ., (ii) . /.+is unsatis.able, and (iii) \nthe variables of . are common to both .-and .+.If .-/.+is unsatis.able, then an interpolant always exists \n[9], and can be computed from a proof of unsatis.a\u00adbility of .-/.+. We present an algorithm for extracting \nan inter\u00adpolant from an unsatis.ability proof in Section 3; if P is a proof of unsatis.ability of .-/.+, \nthen we write ITP (.-).+)(P )for the extracted interpolant for (.-).+). In our example, suppose that \nP is a proof of unsatis.ability for the TF .. Now consider the partition of . into .-2 , the conjunction \nof the .rst two constraints ((x)1)=(ctr)0)/(ctr)1)=(ctr)0)+1), and .+ 2 , the conjunction of the last \nthree constraints ((y)2)= (ctr)1)/(x)1)=(m)0)/(y)2)=(m)0)+1). The symbols com\u00admon to .-2 and .+ 2 are \n(x)1)and (ctr)1); they denote, respectively, the values of x and ctr after the .rst two operations of \nthe trace. The interpolant ITP (.-2 ).+ 2 )(P )is .2 =((x)1)=(ctr)1)-1). Let . 2 be the formula obtained \nfrom .2 by replacing each constant with the corresponding program variable, i.e., . 2 =(x =ctr -1). Since \n.2 is an interpolant, .-2 implies .2, and so x =ctr -1is an overapproximation of the set of states that \nare reachable after the .rst two instructions (as the common constants denote the val\u00adues of the variables \nafter the .rst two instructions). Moreover, by virtue of being an interpolant, .2 /.+ 2 is unsatis.able, \nmeaning that from no state satisfying . 2 can one execute the remaining three instructions, i.e., the \nsuf.x of the trace is infeasible for all states with x =ctr -1. If we partition the TF . in this way \nat each point i =1))4 of the trace, then we obtain from P four interpolants .i =ITP (.-i )(P ), where \n.\u00ad i ).+ is the conjunction of the .rst i i constraints of f, and .+ i is the conjunction of the remaining \ncon\u00adstraints. Upon renaming the constants, we arrive at the formulas . i, which are shown in the rightmost \ncolumn of Figure 2. We collect the atomic predicates that occur in the formulas . i, for i =1))4, in \nthe set P of predicates. We can prove that the trace is abstractly infeasible w.r.t. P. In\u00adtuitively, \nfor each point i =1))4 of the trace, the formula . i represents an overapproximation of the states s \nsuch that s is reach\u00adable after the .rst i instructions of the trace, and the remaining in\u00adstructions \nare infeasible from s. From Equation 1 of Section 3, it follows that SP(. i)opi+1 implies . i+1, for \neach i. For example, SP(x =ctr -1)(y :=ctr)implies x =y -1. Therefore, by adding all predicates from \nall .i to P, we have SPptrue(op1; ;opi) implies . i. Note that, as the trace is infeasible, . 5 =.5 =false. \nThus, SPptrue(op1; ;op5)implies false, i.e., the trace is ab\u00adstractly infeasible w.r.t. P. Locality. \nThe interpolants give us even more information. Con\u00adsider the naive method of looking at just the TF. \nThe predicates we get from it are such that we must track all of them all the time. If, for example, \nafter the third instruction, we forget that x equals the old value of ctr, then the subsequent assumedoes \nnot tell us that y =m +1 (dropping the fact about x breaks a long chain of reasoning), thus making the \ntrace abstractly feasible. In this example, heuristic minimization cannot rule out any predicates, so \nall predicates that occur in the proof of unsatis.ability of the TF must be used at all points in the \ntrace. Using the interpolant method, we show that for infeasible traces of length n, the for\u00admula SP>((SP>trueop1))opn \nis unsatis.able (see Theo\u00ad .... rem 1 for a precise statement of this). Thus, at each point i in the \ntrace, we need only to track the predicates in . i. For example, after executing the .rst instruction, \nall we need to know is x =ctr, after the second, all we need to know is x =ctr -1, after the third, all \nwe need to know is x =y-1, and so on. This gives us a way to localize predicate usage. Thus, instead \nof a monolithic set of predicates all of which are relevant at all points of a trace, we can deduce a \nsmall set of predicates for each point of the trace. Function calls. The method described above can be \ngeneralized to systematically infer well-scoped predicates for an interprocedural analysis [29]. To obtain \npredicates that contain only locally visi\u00adble variables, we cut the TF at each point i in a different \nway. The - .rst part .of a formula pair consists of the constraints from the instructions between and \nincluding iL and i, where iL is the .rst in\u00ad + struction of the call body to which i belongs. The second \npart .contains all remaining constraints. It can be shown that interpolants - for such pairs (.).+)contain \nonly variables that are in scope at the point i, and are suf.cient to rule out the false positive when \nthe subsequent static analysis is done in an interprocedural, polymor\u00adphic way [1]. Paper outline. Next, \nwe describe how to extract interpolants from proofs. In Section 4 we describe the syntax and semantics \nof our language. In Section 5 we show how the predicate inference al\u00adgorithm works for programs without \npointers, and in Section 6 we discuss how pointers can be handled. Finally, in Section 7 we report on \nour experimental results. 3 Interpolants from Proofs -+ We now present rules that, given a refutation \nof a formula ./. - in cnf, derives an interpolant .for the pair (.).+). Let FOL be the set of formulas \nin the .rst-order logic of linear equality. A term in the logic is a linear combination c0 +c1x1 +...cnxn, \nwhere x1))xn are individual variables and c0))cn are integer constants. An atomic predicate is either \na propositional variable or an inequality of the form 0 <x, where x is a term. A literal is ei\u00adther an \natomic predicate or its negation. A clause is a disjunction of literals. Here we consider formulas in \nthe quanti.er-free fragment of FOL. A sequent is of the form G ., where Gand .are sets of formulas. The \ninterpretation of G .is that the conjunction of the formulas in Gentails the disjunction of the formulas \nin .. HYP G1ff G G10 x G10 y COMB c1.2 >0 G10 c1x +c2y {f1yyfn}10 c CONTRA c 0 G1-f1yy-fn G1{f}UTG1{-f}UT' \nRES G1TUT' Figure 3. Proof system. We use a theorem prover that generates refutations for sets of clauses \nusing the sequent proof system of Figure 3. In particular, all boolean reasoning is done by resolution. \nThis system is complete for refutation of clause systems over the rationals. We obtain an incomplete \nsystem for the integers by systematically translating the literal -(0 <x)to 0 <-1 -x, which is valid \nfor the integers. We will use the notation f:.to indicate that all variables occur\u00adring in falso occur \nin ..An interpolated sequent is of the form --+ (.).+) .[.], where .and .are sets of clauses, .is a set \nof formulas, and .is a formula. This encodes the following three facts: -++ (1) . ., (2) .). ., and (3) \n.:.U.. Note that if (.).+) .[.], then .is an interpolant for (.).+). We now give a system of derivation \nrules for interpolated sequents corresponding to the rules of our proof system. These rules are a distillation \nof methods found in [20, 28]. They are sound, in the sense that they derive only valid interpolated sequents, \nand also complete relative to our proof system, in the sense that we can trans\u00ad -+ late the derivation \nof any sequent .U. .into the derivation - of an interpolated sequent (.).+) .[.]. We begin with the rule \nfor introduction of hypotheses. Here, we dis\u00ad - tinguish two cases, depending on whether the hypothesis \nis from .or .+: - HYP-A -f . (.).+) f[f] - HYP-B -f . (.).+) f[T] We take There to be an abbreviation \nfor 0 <0. The rule for in\u00adequalities is as follows: - (.).+) 0 <x [0 <x ' ] - COMB (.).+) 0 <y [0 <y \n' ]c1,2 >0 - (.).+) 0 <c1x +c2y [0 <c1x ' +c2y ' ] In effect, to obtain the interpolant for a linear \ncombination of in\u00ad -++ equalities from .and .we just replace the inequalities from .with 0 <0. Interpolated \nsequents derived using these rules satisfy the following invariant. INVARIANT 1. For any interpolated \nsequent of the form -+ (.).+) 0 <x [0 <x ' ], we have . 0 <y 'such that x =x ' +y ' . Further, for all \nindividual variables v such that v +, the coef.\u00ad :. cients of v inx and x'are equal. Using this invariant, \nwe can show that the above rules to generate interpolants are sound. As an example, Figure 4 shows the \nderivation of an interpolant for the case where .-is (0 <y-x)(0 <z-y)and .+is (0 <x-z-1). In the .gure, \nwe abbreviate (.-).+)f [.]to f [.]. Using the above rules, we derive the sequent 0 <-1 [0 <z -x]. Since \n0 <-1 is equivalent to , it follows that 0 <z -x is an interpolant for (.-).+)(which the reader may wish \nto con.rm). Inequality reasoning is connected to Boolean reasoning in our sys\u00adtem via the CONTRA rule. \nThe corresponding interpolation rule is as follows: ({a1))ak}){b1))bm})[.] CONTRA (.-).+)-a1))-ak)-b1))-bm \n[(-a1 V...V-ak)V.] This rule is sound because both the consequent and the interpolant it generates are \ntautologies. Moreover, we apply the side condition that all the bi are literals occurring in .+, while \nall the ai are literals not occurring in .+. This establishes the following invariant. - INVARIANT 2. \nFor any interpolated sequent (.).+)T [.], the set T is a collection of literals, and . is of the form \nf V., where f is the disjunction of those literals in T not occurring in .+ . Notice that, provided .-is \nin clause form, our two hypothesis in\u00adtroduction rules HYP-A and HYP-B also establish this invariant \n(any clause from .-can be rewritten into the form f V. required by the invariant). Now, we introduce \ntwo interpolation rules for resolution: one for resolution on a literal occurring in .+, and the other \nfor resolution on a literal not occurring in .+: (.-).+)f)T [(f V.)V.] (.-).+)-f)T' [(-f V.' )V.' ] RES-A \n(.-).+)T)T' [(. V.' )V(. V.' )] (.-).+)f)T [. V.](.-).+)-f)T' [.' V.' ] RES-B (.-).+)T)T' [(. V.' )V(. \n/.' )] In effect, when resolving a literal on the .-side (not occurring in .+) we take the disjunction \nof the interpolants, and when resolving a literal on the .+side (occurring in .+) we take the conjunction \nof the interpolants. Using Invariant 2 we can show that these rules are sound. As an example, Figure \n5 shows a derivation of an interpolant for (.-).+), where .-is (b)(-b Vc)and .+is (-c). Using the res\u00adolution \nrule, we derive the sequent (.-).+)[c]. Thus c is an interpolant for (.-).+). Using the invariants given \nabove, we can also show that for every derivation P of a sequent (.-).+)f in our original proof system, \nthere is a corresponding derivation P'of an interpolated sequent of the form (.-).+)f [.]. We will refer \nto the interpolant . thus derived as ITP (.-).+)(P ). Using the same proof but partitioning the antecedent \ndifferently, we can obtain related interpolants. For example, we can show the following fact: ITP (.-)f \nU.+)(P )/f = ITP (.-Uf).+)(P )(1) This fact will be useful later in showing that a set of interpolants \nderived from an infeasible program trace provides a suf.cient set of predicates to rule out that trace. \nWe can also give interpolation rules for treating equalities and un\u00adinterpreted functions. This is omitted \nhere due to space considera\u00adtions. We also note that some useful theories do not have the Craig interpolation \nproperty. For example, interpolants do not always ex\u00adist in the quanti.er-free theory of arrays (with \nseland updopera\u00adtors) [21]. For this reason, we avoid arrays in this work, although the use of array \noperators would simplify the theory somewhat.  4 Languages and Abstractions We illustrate our algorithm \non a small imperative language with integer variables, references, and functions with call-by-value pa\u00adrameter \npassing. Syntax. We consider a language with integer variables and point\u00aders. Lvalues (memory locations) \nin the language are declared vari\u00adables or dereferences of pointer-typed expressions. We assume for simplicity \nthat at the beginning of a function, memory is allo\u00adcated for each reference variable. Arithmetic comparison \nor pointer equality constitute boolean expressions. For any lvalue l, let typl be the type of l; typx \nis the declared type of the variable x in the current scope, and typ*l1 is t if typl1 is reft (and there \nis a type error otherwise). The operation l :=e writes the value of the expres\u00adsion e in the memory location \nl; the operation assume(p)succeeds if the boolean expression p evaluates to true, the program halts otherwise. \nAn operation f (x1))xn)corresponds to a call to func\u00adtion f with actual parameters x1 to xn, and returncorresponds \nto a return to the caller. We assume that all operations are type safe. We represent each function f \nas a control .ow automaton (CFA) Cf =(Lf )Ef )l0 f )Op f )Vf ). The CFA Cf is a rooted, directed graph \nwith a set of vertices Lf SPC which correspond to program loca\u00adtions, a set of edges Ef SLf xLf , a special \nstart location l0 Lf , f a labeling function Opf :E f -Ops that yields the operation label\u00ading each edge, \nand a set of typed local variables Vf SLvals. The set Vf of local variables has a subset Xf SVf of formal \nparameters passed to f on a call, and a variable rf Vf that stores the return value. A program is a set \nof CFAs l={Cf0 ))Cfk }, where each Cfi is the CFA for a function fi. There is a special function main \nand corresponding CFA C, program execution begins there. .... Let PC =U{Lf ICf l}be the set of program \nlocations. A com\u00admand is a pair (op)pc)Ops xPC.A trace of lis a sequence of commands (op1: pc1);;(opn \n: pcn), where (1) there is an edge (l0 )pc1)in Csuch that Op(l0 )pc1)=op1, (2) if opi is .... .... .... \na function call f (...), then pci is l0 f , the initial location of func\u00adtion f , (3) the function calls \nand returns are properly matched, so if opi is a return, then pci is the control location immediately \naf\u00adter the call in the appropriate caller, (4) otherwise (if opi is not a function call or return), there \nis an edge (pci-1)pci)Ef such that Op(pci-1)pci)=opi (where f is the CFA such that pci Lf ). For a trace \nt =(op1: pc1);;(opn : pcn), let Cl t be a function such that if opi is a function call, then opCl.t.i \nis the matching re\u00adturn, and Clt i =n otherwise. For each 1 <i <n, de.ne Lti to be max {j Ij <i)and opj \nis a function call, and Clt j ;i}, and 0 if this set is empty, and Rti =Cl t (Lti). For a trace t =(op1: \npc1);;(opn : pcn), and 1 <i <n, the position Lti has the call that begins the scope to which opi belongs, \nand Rti has the return that ends that scope. For simplicity of notation, we assume that every function \ncall returns. HYP-A HYP-A 10 y -x [0 y -xJ10 z -y [0 z -yJ COMB HYP-B 10 z -x [0 z -xJ 10 x -z -1 [00J \nCOMB 10 -1 [0 z -xJ Figure 4. Deriving an interpolant. HYP-A HYP-A 1b [bVJ1-byc [-b VcJ RES HYP-B 1c \n[V(Vc)J 1-c [V J RES 1 [V((Vc))J Figure 5. Deriving an interpolant using resolution. We .x the following \nnotation for the sequel. We use t for the trace (op1: pc1);;(opn : pcn). For formulas f)f1)f2 FOL, we \nwrite iteff1 f2 to abbreviate (f /f1)V(-f /f2). Semantics. The semantics of a program is de.ned over \nthe set v of states. The state of a program contains valuations to all lvalues, the value of the program \ncounter, as well as the call stack. For our pur\u00adposes, we only consider the data state, which is a type-preserving \nfunction from all lvalues to values. A region is a set of data states. We represent regions using .rst-order \nformulas with free variables from the set of program variables. Each operation opOps de\u00ad 6 .nes a state \ntransition relation --Sv xv in a standard way [23]. The semantics of a trace can also be given in terms \nof the strongest postcondition operator [16]. Let . be a formula in FOL represent\u00ading a region. The strongest \npostcondition of . w.r.t. an operation op, written SP. opis the set of states reachable from states in \n. after executing op. The strongest postcondition operator for our language can be computed syntactically \nas a predicate transformer. The strongest postcondition operator gives the concrete semantics of the \nprogram. Our analyses will consider abstract semantics of programs. The abstract domain will be de.ned \nby a set of predicates over the program variables. As we use decision procedures to com\u00adpute predicate \nabstractions, we require quanti.er-free predicates. For a formula . FOL and a set of atomic predicates \nP SFOL, the predicate abstraction of . w.r.t. the set P is the strongest formula . (in the implication \norder) with atomic predicates from P such that . implies .. Let .: PC -2FOL be a mapping from program \nlocations to sets of atomic predicates. The operator SPnis the ab\u00adstraction of the operator SPw.r.t. \n.. Formally, let . denote a set of states, and let (op: pc)be a command. Then SPn. (op: pc)is the predicate \nabstraction of SP. opw.r.t. . pc. Let SPbe a syntactic strongest postcondition operation, and SPn its \nabstraction w.r.t. the mapping .. For any trace t, the trace t is 6 j (1) feasible if there exist states \ns0)s1))sn v such that sj- -sj+1 for j =0))n -1, and infeasible otherwise; (2) SP-feasible if SPtruet \nis satis.able, and SP-infeasible otherwise; and (3) .\u00adfeasible if SPntruet is satis.able, and .-infeasible \notherwise. The two notions (1) and (2) coincide [23]. Subclasses of programs. A program is .at if it \nis a single\u00adton {C}, and there is no edge (pc)pc ' )in Csuch that Op(pc)pc ' )is a function call or a \nreturn. A program is pointer\u00adfree if all lvalues and expressions have type int. Speci.cally, a pointer-free \nprogram does not have any references. In the follow\u00ading, we shall consider four classes of programs: \n(Class Pi) .at and ........ pointer-free, (Class Pii) pointer-free (but not .at), (Class Piii) .at (but \nnot pointer-free), and (Class PiV) the class of all programs. For each class, we de.ne a syntactic predicate \ntransformer SPthat takes a formula . FOL and an operation opOps and returns the strongest postcondition \nSP. op. We also de.ne the predicate ab\u00adstraction SPnof SP. Finally, we present an algorithm Extratthat \ntakes a trace t and returns a mapping . from PC to sets of atomic predicates in FOL. The following theorem \nrelates the different no\u00adtions of feasibility. THEOREM 1. Let t be a trace of a program P of class PI, \nPII, PIII, or PIV. The following are equivalent: 1. t is infeasible (or equivalently, t is SP-infeasible). \n 2. tis SPn-infeasible for . =Extratt.  In particular, Theorem 1 states that our predicate discovery \nproce\u00addure Extratis complete for each class: for an infeasible trace t, the predicate map Extratt is \nprecise enough to make the trace SPn\u00adinfeasible (i.e., the infeasible trace t is not a trace of the abstraction). \nIf all integer variables are initialized to some default integer value, say 0, then all satisfying assignments \nof the SPof a trace will be integral even if the SPis interpreted over the rationals. Thus, if the trace \nis infeasible, our proof system can derive the unsatis.ability of the strongest postcondition. In the \nnext two sections, we describe in detail how we mine pred\u00adicates from proofs of unsatis.ability of spurious \nerror traces. First we consider programs in the classes PI and PII. We then generalize our results to \nthe classes PIII and PIV. For a given trace t of each class of program, we de.ne the following operators. \nFirst, we de\u00ad.ne the concrete and abstract strongest postcondition operators SP and SPn, which take a \nformula and an operation and return a for\u00admula, and we extend them to the entire trace t. Next, we de.ne \nan operator Con, which returns a constraint map. This is a function that maps each point i of the trace \nt to a constraint that corresponds to the ith operation of the trace. The conjunction of the constraints \nthat are generated at all points of t is the trace formula (TF) for t, which is satis.able iff the trace \nis feasible. Finally, we de.ne the procedure Extrat, which uses a proof of unsatis.ability of the TF \nto compute a function . that maps each program location to a set of atomic predicates such that the trace \nt is .-infeasible. t SP. t SP. . t Con(.yG)t (x :=e : pci) :x ' (.[x 'IxJx =e[x 'IxJ) a (. pc)(SP. t)(.' \nyG[i> (Sub.' x =Sub. e)J) ' where x 'is a fresh variable where .=Upd. {x} (assume(p): pci). p (.yG[i \n>Sub. pJ) t1;t2 SP(SP. t1 )t2 SP .(SP . . t1 )t2 Con(Con(.yG)t1 )t2 (y :=f (ee): pci); :y ' yef.[y 'IyJ \na (. pcj )(.' yG' ) t1; :r ef =ee[y 'IyJ(:y ' yef.[y 'IyJ where (return: pcj ) :VjSPtruet1 :r ef =ee[y \n'IyJ .' =Upd. {y} f y =r :VjSPtruet1 .I =Upd.'Vf f where y 'is fresh y =r) . GI =G[i >Sub.I ef =Sub.eeJ \nex are the formals of f (.OyGO)=Con(.I yGI )t1 ' r is the return variable of f G=GO[j y = >Sub.' Sub.OrJ \nV j =Vf \\(symf U{r}) r is the return variable of f f Figure 6. Postconditions and constraints for Piand \nPiitraces. 5 Programs without Pointers Algorithm 1 Extrat 5.1 Flat Pointer-free Programs: PI Strongest \npostconditions and constraints. We .rst de.ne the semantics of .at pointer-free programs in terms of \na syntactic strongest postcondition operator SPand its predicate abstraction SPn(w.r.t. a predicate map \n.). In Figure 6 the .rst three rows de\u00ad.ne the operators SPand SPnfor traces of PI. Each operator takes \na formula in FOL and returns a formula in FOL. The operator SPn is parameterized by a map . from PC to \nsets of atomic predicates. With this de.nition of SP, we can show that Theorem 1 holds. An lvalue map \nis a function . from Lvals to N. The operator Upd: 2Lvals (Lvals -N)--(Lvals -N)takes a map . and a set \nof lvalues L, and returns a map .'such that .'l =. l if lL, and .'l =il for a fresh integer il if lL. \nThe function Subtakes an lvalue map . and an lvalue l and returns (l). l). The function Sub. is extended \nnaturally to expressions and formulas. A new lvalue map is one whose range is disjoint from all other \nmaps. We use lvalue maps to generate trace formulas (TF); at a point in the trace, if the map is ., then \nthe the pair (l). l)is a special constant that equals the value of l at that point in the trace. Whenever \nsome lvalue l is updated, we update the map so that a fresh constant is used to denote the new value \nof l. For every such constant c =(l)i), let Cleanc =l. The operator Cleancan be naturally extended to \nexpressions and formulas of FOL. The constraints are generated by the function Con, which takes a pair \n(.)G)consisting of an lvalue map . and a constraint map G: N-FOL, and a command (pc : op)Cmd, and returns \na pair (.' )G' )consisting of a new lvalue map and constraint map. We generate one constraint per command. \nFor a trace t =(op1: pc1);;(opn : pcn),if (.' )G' )=Con(.)G)t for some initial G)., then G i is the constraint \nfor opi, and it can be shown by induction . on the length of the trace, that the TF 1<i<n G i is satis.able \niff SPtruet is satis.able. The generated constraints are a skolem\u00adized version of the strongest postcondition. \nThe function Conis de.ned in the .rst three rows of Figure 6. If the ith operation in the trace is the \nassignment x :=e, we .rst update the map so that a new constant denotes the value of x, and then we have \nthe ith constraint specify that the new constant for x has the same value as the ex\u00adpression e (with \nappropriate constants plugged in). For an assume operation assume(p), the constraint stipulates that \nthe constants at that point satisfy the formula p. The constants enable us to encode Input: an infeasible \ntrace t =(op1: pc1);;(opn : pcn). Output: a map . from the locations of t to sets of atomic predicates. \n. pci :=0/ for 1 in (yG):=Con(.0yG0) P :=derivation of .G i 1false 1<i<n for i :=1to n do . .j:=1<j<i \nG j . .+:=i+1<j<n G j . :=ITP (.jy.+)(P ) . pci :=. pci UAtoms(Clean.) return .. (assume(b >0): pc1); \n(by0)>0 (c :=2hb : pc2); (cy1)=2h(by0) (a :=b : pc3); (ay2)=(by0) (a :=a -1: pc4); (ay3)=(ay2)-1 .j (assume(ab): \npc5); (ay3)(by0).+ (assume(a =c): pc6) (ay3)=(cy1) Figure 7. Cutting a Pitrace. the entire history of \nthe trace in the constraint map. The following proposition states the correctness of constraint generation. \nPROPOSITION 1. [Equisatis.ability] For a trace t let (.)G)= . Con(.0)G0)t and let . =1<i<n G i. The trace \nt is feasible iff the . is satis.able. Moreover, the size of . is linear in the size of t. Predicates \nfrom cuts. Given an infeasible trace t, we want to learn a set of predicates that exhibit the infeasibility \nof the trace. Our method has been described in Section 2 and is made precise in Al\u00adgorithm 1. Algorithm \n1 .rst sets the map . to be the empty map. It then generates the constraint map G for the entire trace \nand con\u00adstructs the TF by conjoining all constraints in the range of G. Let P be a proof of unsatis.ability \nof the TF. Then for each point i in the trace, we cut the constraints into those from the .rst i com\u00admands \n(.-) and those from the remaining commands (.+). Using - the proof P we compute the interpolant . for \n(.).+)and add the atomic predicates that occur in . after cleaning to the predicate map for pci. The \ncorrectness of this procedure is stated in Theorem 1. EXAMPLE 1: Consider the infeasible trace from [3] \nshown on the left in Figure 7. On the right, the .gure shows the result of Con(.0)G0)t, where the initial \nlvalue map .0 maps a, b, and c to 0. To the right of each command is the corresponding con\u00ad (a :=0: pc1); \n(ay1)=0 (b :=inc(a): pc2); (fxy2)=(ay1) (assume(x =fx): pc2); (xy3)=(fxy2) (x :=x +1: pc3); (xy4)=(xy3)+1 \n(r :=x : pc4); (ry5)=(xy4) (returnr : pc5); (by6)=(ry5) (assume(a =b -1): pc6)(ay1)=(by6)-1  Figure \n8. Cutting a Piitrace. .- C i .+ C  the constraint for the return is the formula that equates y with \nthe return value r of the function. To deal with possibly recursive func\u00ad.-tions, we use a different \nlvalue map for the constraints of the func\u00ad.+tion body, because we do not want assignments to local variables \nof the called function to change the values of variables in the calling context. With these de.nitions, \nthe analogue of Proposition 1 holds for Piiprograms. straint. When we cut the trace at the fourth location, \nthe result\u00ading pair (.-).+)consists of the conjunctions of the constraints from above and below the line, \nrespectively. The interpolant in this case is (a)3)<(c)1)-2, which upon cleaning yields the predicate \na <c -2. Notice that the constants common to both sides of the cut denote the values of the respective \nvariables after the .rst four operations, and .-implies the interpolant. .  5.2 Pointer-free Programs: \nPII We now move on to programs with function calls, but no pointers. We assume that there are no global \nvariables, and that each func\u00adtion returns a single integer. When dealing with such programs, the analysis \nof the previous section may learn predicates that are not well-scoped, i.e., for some location, we may \nextract predicates that contain variables which are out-of-scope at that location, and such an analysis \nis not modular. For a modular interprocedural analy\u00adsis, we need to compute the effect of a function \nin terms of the arguments passed to it, i.e., we wish to relate the output of the function to its input. \nThe way to do this is to introduce symbolic variables that denote the values the function is called with, \nperform the analysis using the symbolic variables, and then, at the call site, plug in the actual values \nfor the constants to see the result of the function. This method is known as polymorphic predicate abstrac\u00adtion \n[1]. We assume that the program has been preprocessed so that (1) for every function and every formal \nparameter x, the func\u00adtion has a new local, so-called symbolic variable fx, which holds the value of \nthe argument when the function is called and is never written to (the set of symbolic variables of f \nis symf ), and (2) the .rst statement of every function is assume(/ixi =fxi ), where the conjunction \nranges over the set Xf of formal parameters. Finally, to avoid clutter in the de.nitions due to renaming, \nwe assume that dif\u00adferent functions have different names for their variables, and that no function calls \nitself directly (a function f can of course call another function which, in turn, calls f ). Strongest \npostconditions and constraints. Figure 6 shows the syntactic strongest postcondition operator SPand its \npredicate ab\u00adstraction SPnfor Piitraces. The fourth row shows the case of calls and returns (we process \nthe entire call-return subtrace at once). The strongest postcondition for this case is computed as follows: \nwe re\u00adplace y as for an assignment, we set the symbolic variables f s to the arguments.e[y ' Iy], we \nthen compute SPfor the body of the function t1 w.r.t. true(the .rst operation in the function will equate \nthe for\u00admal parameters .x with the f s), and we subsequently quantify out all locals except the f s, \nwhich leaves us with a formula that relates the return variable r to the f s. We then set y to r and \nquantify out the f s and r. The abstract postcondition does the same, only it uses the the abstract postcondition \nSPnof the function body instead of SP, and then abstracts the resulting formulas using the predicates \nof pcj, the location of the caller after the function returns. The gen\u00aderated constraints are again a \nskolemized version of the strongest postcondition. The constraint for the call command is the formula \nthat equates the f s with the corresponding actual parameters, and EXAMPLE 2: Consider the trace in Figure \n8. The function inc has the formal parameter x and return variable r, and returns a value one greater \nthan its input. Assume that we start with an lvalue map .0, which maps a, b, x, and fx to 0. The constraints \nCon(.0)true)t are shown on the right in Figure 8. . Predicates from cuts. To get well-scoped predicates, \nwe need only to generalize the notion of cuts. For each location i of a trace, in\u00adstead of partitioning \nthe constraints into those due to commands before i and those originating from commands after i, we use \nthe partition shown in Figure 8. Let iL (resp., iR) be the .rst (resp., last) command in the call body \nto which the ith command belongs. So iL is the .rst command after a function call, and the operation \nafter iR is a return. We consider four sets of constraints: (1) .-corre\u00adsponds to the commands between \n(and including) iL and i, which may include commands that call into and return from other func\u00adtions, \n(2) .+corresponds to the commands from i +1to iR, which may include commands that call into and return \nfrom other func\u00adtions between i +1 and iR, (3) .-corresponds to the commands C in the calling context \nof command i which occur before the call of the function to which i belongs, and which include the call \nof the function, and (4) .+ C corresponds to the commands in the call\u00ading context which occur after the \nreturn of the function to which i belongs, and which include the return. We then construct the in\u00adterpolant \nof (.-).+C /.+ /.-C ). One can check that the constants common to .-and .+C are denote the values of \nlocals (includ\u00ad /.+ ing the return variable) at location i, and that the constants common to .-and .- \nC are denote the values of symbolic variables upon en\u00adtry to the function, which are never changed by \nthe function, and hence also the values of the symbolic variables at location i; (these are also locals \nof the called function). Hence the interpolant, and thus the predicates we compute, are in terms of variables \nthat are in scope at location i, and they refer only to current values of those variables at location \ni. To see why such an interpolant suf.ces, consider .rst the parti\u00adtion (.-/.+).-C ), namely, into constraints \nthat belong to C /.+ the function body and constraints that belong to the calling con\u00adtext. The resulting \ninterpolant . contains the symbolic variables of the function to which the cut-point belongs, as well \nas the re\u00adturn variable r, i.e., the inputs and output of the called function. Moreover, . abstractly \nsummarizes the information about the func\u00ad - tion call which renders the trace infeasible, because . \n/(.C /.+) is unsatis.able. Now, at each point inside the function, we need to know what information is \nrequired to show that . holds at the end. To get this information, we could compute the interpolant of \n(.-).+/-C /.+ .), but since -. is implied by .-C , we can instead directly compute the interpolant of \n(.-).+C /.+ /(.-C )). The re\u00adsulting predicate discovery algorithm Extratfor traces of Piipro\u00adgrams is \nshown in Figure 2. The correctness of this procedure is stated in Theorem 1. EXAMPLE 3: Recall the trace \nand constraints from Figure 8. The formulas that result from cutting the trace at the fourth location \nare .-=(G 3 /G 4)=((x)3)=(fx)2)/(x)4)=(x)3)+1)and Algorithm 2 Extrat Input: an infeasible trace t =(op1: \npc1);;(opn : pcn). Output: a map . from the locations of t to sets of atomic predicates. . pci :=0/ for \n1 in (yG):=Con(.0yG0) P :=derivation of .G i 1false 1<i<n for i :=1to n do (iLyiR ):=(Lt iyRt i) . .j:=iL \n+1<j<i G j . .+:=i+1<j<iRj1 G j .. .C :=(1<j<iL G j)(iR<j<n G j) . :=ITP (.jy.+.C)(P ) . pci :=. pci \nUAtoms(Clean.) return .. (assume(x =y): pc1); (xy0)=(yy0) (hx :=0: pc2); (h(xy0)y3)=0 (y :=x : pc3); \n(yy4)=(xy0)(h(yy4)y5)=(h(xy0)y3) (assume(y =x): pc4); (yy4)=(xy0)(h(yy4)y5)=(h(xy0)y3) (hy :=hy +1: pc5); \n(h(yy4)y6)=(h(yy4)y5)+1 ite((xy0)=(yy4)) ((h(xy0)y7)=(h(yy4)y5)+1) ((h(xy0)y7)=((h(xy0)y3))) (assume(hx \n=0): pc6)(h(xy0)y7)=0 Figure 9. Cutting a Piiitrace. .+=G 5 =((r)5)=(x)4)). Furthermore, .-=(G 1 /G 2)= \nC ((a)1)=0 /(fx)2)=(a)1))and .+ =(G 6 /G 7)=((b)6)= C - (r)5)/((a)1)=(b)6)-1). The symbols common to \n.and .+/(.-C )are (x)4) C /.+ and (fx)2), which are the values of x and fx at that point. The interpolant \nis (x)4)=(fx)2)+1, which yields the predicate x =fx +1. Similarly, when we cut the trace at the .fth \nlocation, the common variables are (r)5)and (fx)2), and the cleaned interpolant which summarizes the \nfunction s behavior for all calling contexts is r =fx +1. Notice that at this point, which is the last \nlocation of the function body, the interpolant is guaran\u00adteed to only use the input and output variables \nof the function. When cutting the trace at the sixth location, we get the predicate b =a -1, which is \nagain well-scoped for pc6, where the variables x, fx, and r are not in scope. It is easy to check that \nthese predicates make the trace abstractly infeasible. .   6 Programs with Pointers We now consider \nprograms that deal with pointers. As before, we .rst consider .at programs, and then move on to programs \nwith procedures. The only difference with the previous section is in the constraint generation; the algorithm \nExtratis exactly the same as before, only it uses a different function Con. In this somewhat technical \nsection we show how the constraint generation must be extended to handle pointers in a sound and complete \nway. Stores. The classical way to model the store is to use memory expressions and the theory of arrays \n[13, 26, 27], which comes equipped with two special functions, seland upd. The function seltakes a memory \nM and an address a and returns the contents of the address a; the function updtakes a memory M, an address \na, and a value v, and returns a new memory that agrees with M except that the address a now has value \nv. The relationship be\u00adtween seland updis succinctly stated by McCarthy s axiom [21]: sel(upd(M)a)v))b)=ite(a \n=b)v sel(M)b). For a memory vari\u00adable M and a variable x, de.ne Mx =sel(M)x), and for an lvalue *l, de.ne \nM (*l)=sel(M)Ml). With some slight abuse of nota\u00adtion, we use M in this way to denote a map from lvalues \nto memory expressions over M. We naturally extend the map M to expressions and formulas of FOL. Expressions \nand formulas appearing in the program do not contain memory variables, sel,or upd. Symbolic variables. \nWe must generalize the notion of symbolic variables, which freeze the formal parameters of a function, \nbe\u00adcause now a pointer may be passed to a function and the func\u00adtion may change the value of some cell \nthat is reachable using the pointer. Hence, we have to relate the values of cells at the return of a \nfunction with the values the cells had upon the corre\u00adsponding function call. For a set X of variables, \nlet ReahX = {*kx IxX and k ;0}be the set of cells that are reachable from X by dereferences. As we do \nnot have recursive types, this set is .nite and syntactically computable (k is bounded by the type of \nx). The set of symbolic variables of f is now symf ={fl Il ReahXf }, where Xf is the set of formal parameters \nof f . As before, (1) the symbolic variables are local variables of the function which are never written \nto, and (2) the program is changed, by a syntactic preprocessing pass, so that each lvalue that is reachable \nfrom the formal parameters reads its value from the corresponding symbolic variable (i.e., x from fx, \n*x from f*x and so on) in the very .rst . statement of the function (using assume(lEReah.Xf l =fl )). \nAs before, for modularity we analyze the function using the symbolic variables, and replace their values \nwith the actual parameters at the call sites [1, 29]. For a symbolic variable fl , de.ne (fl .)=l. Constraints \nfor modeling allocation. Suppose there are two vari\u00adables x and y, each of type refint. When the program \nbegins, and the pointers are allocated, the standard semantics is that their values are not equal. For \ncompleteness, this must be explicitly mod\u00adeled by constraints. We modify the transformation, described \near\u00adlier, which inserts into every function as .rst statement an assume that copies the symbolic variables \ninto the formal parameters, to include another clause that states that every two distinct lvalues l1)l2 \nReah(Vf \\(Xf Usym f ))of reference type are not equal. Again, as the types are nonrecursive, this clause \nis quadratic in the size of the function. An example is the .rst assumein the trace of Figure 9. Constraints \nfor modeling the store with lvalue maps. Using seland updit is straightforward to generate the strongest \npost\u00adconditions for programs with pointers; see Figure 10. Unfortu\u00adnately, the theory of arrays does \nnot have the interpolant prop\u00aderty, thus we cannot get interpolants from TFs that use this the\u00adory. For \nexample, the conjunction of M' =upd(M)x)y)and (a = b)/(sel(M)a)=sel(M' )a))/(sel(M)b)=sel(M' )b))is un\u00ad \nsatis.able, but there is no quanti.er-free interpolant in the common set of variables, namely {M)M' }. \nWe surmount this hurdle by mod\u00adeling the memory axioms using (generalized) lvalue maps, and by instantiating \nthe array axioms on demand. Recall the de.nitions of lvalue maps and Updfrom Section 5. The set ChLval \nconsists of elements cl generated by the grammar cl ::=(x)i)I(cl)i), where i N. The function Cleanof \nthe previous section is extended by Clean(x)i)=x and Clean(cl)i)=*(Cleancl). Each cl ChLval is a special \nconstant that denotes the value of Cleancl at some point in the trace. The function Subof the previous \nsection is extended to all lvalues by Sub. (*kx)=(x). x)if k =0, and Sub. (*kx)= (Sub. *k-1 x). (*kx))otherwise, \nand extended naturally to ex\u00adpressions, atomic predicates, and formulas. Constraints for assume operations. \nModeling the memory with seland updgives us some relations for free, e.g., from x =y (modeled as sel(M)x)=sel(M)y)) \nthe equality *x =*y (mod\u00adeled as sel(M)sel(M)x))=sel(M)sel(M)y))) follows by con\u00adgruence. We explicitly \nstate these implied equalities when generat\u00ading constraints, by closing a predicate with the operator \nlos* true: FOL -FOL, where (los*bp1)op(los*bp2)if p =(p1 opp2),   [-(los*(-b)p1) if p =(-p1), . los \n*bp =p /0<k<N ((*kl1)=(*kl2))if p =(l1 =l2)and  b =true,  p otherwise, provided typl1 =typl2 =refN \nint. The formula los* truep explicates all equalities inferred by the memory axioms from the formula \np. When generating the constraints for assume(p), we .rst close p using los*, and then generate constraints \nfor the result. Consider, for example, the constraint for the fourth command in Figure 9. For any formula \np that can appear in an assume,we have Mp 9M (los* truep)in the theory of arrays. Using this equivalence, \nwe can show the following lemma, which tells us that the constraints have been modeled adequately. For \na program l,an N lvalue *kx is well-typed in lif typx =refintfor some N ;k, i.e., if x has type refint, \nthen *x is well-typed but not ***x.A formula p is well-typed w.r.t. lif (1) it does nor contain memory \nvariables, sel,or upd, and (2) each lvalue that occurs in p is well\u00adtyped in P. LEMMA 1. For a program \nl, two formulas p)p 'FOL that are well-typed w.r.t. l, and an lvalue map ., the condition M p implies \nMp'iff Sub. (los* truep)implies Sub. p ' . Constraints for assignments. When assigning to *l1 we must \nex\u00adplicate that for all lvalues *l2 such that l1 =l2, the value of *l2 is updated as well. Let Equatebe \na function that takes a pair of lvalue maps (.1).2)and a pair of expressions (l1)l2), and generates equal\u00adities \nbetween the names of l1 and its transitive dereferences under .1, and the names of l2 and its transitive \ndereferences under .2. Formally, . Equate(.1).2)(l1)l2)=(Sub.1 (*kl1)=Sub.2 (*kl2))) 0<k<N where typl1 \n=typl2 =refN int. De.ne the function EqAddr, which takes a pair of lvalues and returns a formula that \nis true when the lvalues have the same address, by EqAddr(*k1 x1)*k2 x2)= *k1-1 falseif k1 =0or k2 =0, \nand EqAddr(*k1 x1)*k2 x2)=(x1 = *k2-1x2)otherwise. For a function f (which is mainfor .at pro\u00adgrams), \nlet Lvals f =ReahVf . For an lvalue l and a function f , let Aliasfl ={l' Il'Lvals f and l'may be aliased \nto l}.We only require that Aliasfl overapproximates the set of lvalues that can actually alias l. . Finally, \nwe de.ne the function Asgn, which generates appropriate constraints for an assignment l :=e. The function \nAsgntakes a function name f , an lvalue map ., and a pair (l)e), where l is an lvalue and e the expression \nthat is being written into l, and returns a pair (.' ).' )of an updated lvalue map .'and a formula .'. \nDe.ne ' =Upd. S, where S ={*kl' Il' (Aliasl)U{l}and k ;0}, and de.ne . ' =Equate(.' ).)(l)e)/ ite(Sub. \n(EqAddr(l)l' ))) . ) (Equate(.' ).)(l' )e)) l.EA. aSf (Equate(.' ).)(l' )l' )) ..l The .rst conjunct \nof .'states that l gets a new value e, and all transitive dereferences of l and e are equated (i.e., \n*l gets the new value *e, and so on). The big second conjunct of .'states how the potential aliases l'of \nl are updated: if l and l'have the same address, then the new value of l'(given by Sub.'l') is equated \nwith e; otherwise the new value of l'is equated with the old value of l'(given by Sub. l'). This generalizes \nMorris de.nition for the strongest postcondition in the presence of pointers [24]. LEMMA 2. Let l :=e \nbe an assignment in a program l, let . =SPtrue(l :=e), and let (.' ).' )=Con(.0)true)t for some lvalue \nmap .0. For every formula p FOL that is well typed w.r.t. l, the formula . implies M p in the theory \nof arrays iff .'implies Sub.' p. 6.1 Flat Programs: PIII M The .rst three rows of Figure 10 give the \nde.nition of the operator SPusing the theory of arrays, as well as the generated constraints. The de.nition \nof SPnis the same as before, except that the new SP operator is used. Notice that the current memory \nis always rep\u00adresented by M. For assignments, SPstates that the current memory M is now an updated version \nof the old memory, which is renamed ' . We use Asgnto generate the appropriate constraints for deal\u00ading \nwith the possible alias scenarios. For assume operations, SP is de.ned as before, except that the constraint \ngenerated is on the closure of the predicate using los*. Constraints for sequences are obtained by composition. \nThe size of the constraints is quadratic in the size of the trace. By induction over the length of the \ntrace, split\u00adting cases on the kind of the last operation, and using Lemmas 1 and 2, we can prove the \nfollowing theorem. THEOREM 2. Given a trace t of a program l, let (.' )G)= . Con(.0)G0)t, let .r =SPtruet, \nand let . =1<i<n G i. For every formula p FOL that is well-typed w.r.t. l, the formula .r implies M p \nin the theory of arrays iff . implies Sub.'p. Hence, the trace t is feasible iff the formula . is satis.able. \nGiven the new de.nition of Con, the algorithm for predicate dis\u00adcovery is the same as Extrat(Algorithm \n1), and Theorem 1 holds. EXAMPLE 4: The right column in Figure 9 shows the constraints for the trace \non the left. For readability, we omit unsatis.able and uninteresting disjuncts (for the second and third \ncommands). At the fourth cut-point of this trace, the common variables are (*(y)4))3), (y)4), (x)0), \nand (*(x)0))3), which denote the values of *y, y, x, and *x at that point in the trace. The interpolant \nfor this cut is (*(y)2))3)=0, which gives the predicate *y =0 for the loca\u00adtion pc4. .  6.2 General \nPrograms: PIV The main dif.culty when dealing with functions and pointers is in handling the semantics \nof calls and returns, because the callee may be passed pointers into the local space of the caller. The \ncomplex\u00adity arises when we wish to abstract functions polymorphically [1], because then we have to summarize \nall effects that the callee may have had on the caller s store at the point of return. One way to do \nthis is to imagine the callee starting with a copy of the caller s store and, upon return, the caller \nrefreshing his store appropriately using the callee s store. As we shall see, the dif.culty is only in \nmodeling this appropriately with strongest postconditions. Following that, it t SP. t Con(.yG)t (l :=e \n: pci) :M' (.[M' IMJM =upd(M' yM'lyM' e))(.' yG[i >.J) where M'is a fresh store where (.' y.)=Asgnf . \n(lye) f is the function in which pci belongs (assume(p): pci). Mp (.yG[i >Sub. (los*truep)J) t1;t2 SP(SP. \nt1 )t2 Con(Con(.yG)t1 )t2 (f (ey): pci); :M' yMI yMO symf (.' yG' ) t1; .[M' IMJ where .I =Upd. (ReahVf \n) . (return: pcj ) M' =MI .ld .ld = . fEsym f Equate(.I y.)(fy(f t)[yIxJ) ' [MOIMJ GI =G[i >.ld J New(LyPyR' \n)(MOyMI )(.OyGO)=Con(.I yGI )t1 M =opy(M' yMOyR) LyPyRyR'as for SP where M' yMI yM0 are fresh stores \nG' =GO[j >New(LyPyR' )(.y.O)opy(.y.' y.O)(LyR)J . .ld =fEsym f f =MI (f t)[yIxJ .' =Upd. L . ex are \nthe formals of f R ={hf If sym f } ' =:Vf jSPtruet1 Vf j=Vf \\symf , L =lvalues in scope at pc2 P =symf \nand R' =ReahP Figure 10. Postconditions and constraints for Piiiand PiVtraces. (hy :=0: pc1); (h(yy0)y1)=0 \n(inc(y): pc2); (fxy1)=(yy0)(f*xy2)=(h(yy0)y1) (assume(x =fx (xy4)=(fxy1) hx =f*x): pc3); (h(xy4)y5)=(h(fxy1)y7) \n(h(xy4)y5)=(f*xy2) (hx :=hx +1: pc4); (h(xy4)y6)=(h(xy4)y5)+1 ite((xy4)=(fxy1)) ((h(fxy1)y8)=(h(xy4)y5)+1) \n((h(fxy1)y8)=(h(fxy1)y7) (return: pc5); ite((yy0)=(fxy1)) ((h(yy0)y9)=(h(fxy1)y8)) ((h(yy0)y9)=(h(yy0)y1)). \n (assume(hy =1): pc6)(h(yy0)y9)=1 Figure 11. Cutting a PiVtrace. is straightforward to generate the constraints, \nand the method for learning predicates is again Algorithm 2, only now using the new de.nition of Con. \nAs we allow functions to pass pointers into the local store as argu\u00adments, to keep the exposition simple, \nwe can assume w.l.o.g. that (1) there are no global variables (these can be modeled by passing references), \nand (2) there is no explicit return variable; instead, return values are passed by updating the contents \nof some cell that is passed in as a parameter. EXAMPLE 5: Consider the trace on the left in Figure 11. \nThe caller (main) passes a pointer y to the store to the callee in. The callee updates the memory address \n*y (which is called *x in in). There are two symbolic variables fx and f*x in in, corresponding to the \nformal parameter x and its dereference. The assumeat location pc3 loads the symbolic variables into the \nformal parameters. . Soundness. Every cell in the caller s store which is modi.ed by the callee must \nbe reachable from a parameter passed to the callee, e.g., the cell pointed to by y in the caller is the \nsame as the cell pointed to by x when the function inis called, hence upon re\u00adturn the value of *y should \nbe the value of *(fx)(as in the interim, the callee may have changed x). Every cell of the caller which \nis unreachable from the parameters passed to the callee remains un\u00adchanged as a result of the call. This \nis modeled in the SPseman\u00adtics by copying the contents of the callee s store, at the locations reachable \nfrom the passed parameters, into the caller s store. The locations that are reachable from the passed \nparameters are frozen in the symbolic variables of the callee. It can be shown that this is sound for \nour language. To express the copying in SP, we use the operator opy: for a destination memory Md , a \nsource memory Ms, and a dereferenced lvalue *l, the expression opy(Md )Ms)*l)is the result of updating \nMd such that all cells reachable from l have the same value as in Ms. Formally, we de.ne opy(Md )Ms)*l)as \n{ upd(Md )Md l)Ms (*l)) if typ*l =int, opy(upd(Md )Md l)Ms (*l)))Ms)**l)otherwise. We never need to copy \ninto a variable x. The function opyis ex\u00adtended to a sequence of lvalues.l =(*l :: .l')by opy(Md )Ms).l)= \nopy(opy(Md )Ms)*l))Ms).l'). It can be shown that the result is independent of the order of lvalues. Hence, \nwe can consider the operator opyto be de.ned over sets of lvalues. We can mimic copying by lvalue maps \nas follows. Given three lvalue maps .d , .' d , and .s, an lvalue *l, and a sequence R of lvalues, de.ne \nopy(.d).' d ).s)(*l)R)= Sub.' d *l =Sub.d *l if R =., [ ite(Sub.' dl =Sub.d f) (Equate(.' d ).s)(*l)*f)) \nopy(.d ).' d ).s)(*l)R' )if R =f :: R' . Finally, for a set L of lvalues, de.ne opy(.d ).' d ).s)(L)R)= \n. *lEL opy(.d ).' d ).s)(*l)R). When a function returns, we update all local variables of the caller. \nWe set .d to the lvalue map of the caller before the call, and .' d = Upd.dL is the lvalue map of the \ncaller after the call, where L = Reah{*x Ix is a local variable of the caller}is the set of lvalues of \nthe caller that can change (no local variable x can change as the result of a call; only dereferences \ncan change). We set .s to the lvalue map of the callee upon return, and R =symf is the set of cells that \nwere passed to the callee, and hence must be copied back into the caller. It can be checked that the \nformulas resulting from different permutations of R are equivalent. Completeness. For completeness, we \nmust ensure that we prop\u00aderly model the semantics of allocation. It can be shown that it suf.ces to ensure \nthat every cell that is being returned by the callee (i.e., reachable from a symbolic variable of the \ncallee) is either a cell passed to it (i.e., equal to some symbolic vari\u00adable) or is brand new (i.e., \ndifferent from ReahVf , the set of cells known to the caller). If a cell is different from those of the \ncaller, then transitivity of equality and the same check on all subsequent returns ensures that the cell \nis different from all pre\u00adviously allocated ones. The check is encoded with the op\u00aderator New. For an \nlvalue l of reference type, a set L of lvalues, and two stores Ml and ML, de.ne dif(l)L)(Ml)ML)= /l.EL-(Ml \nl =ML l' ). Given three sets L, P, and R of lvalues, and a pair MO and MI of stores, de.ne New(L)P)R)(MO)MI \n)= (/rER(dif(r)P)(MO)MI )dif(r)L)(MO)MI )). Here L = ReahVf . is the set of local variables of the caller \nf ', and P =sym f is the set of cells passed to the callee f , and R =Reahsymf is the set of cells returned \nby the callee. The store MO is the store upon return from f , and MI was the store upon en\u00adtry to f . \nThe formula says that for every cell r that is returned, if r is different from all cells passed to f \n, then r is different from all local cells L of the caller f ' . This is generalized to lvalue maps as \nfollows: dif(l)L)(.l).L)=/l.EL-(Sub.ll = Sub.Ll' )and New(L)P)R)(.).' )=(/rER(dif(r)P)(.).' )) dif(r)L)(.).' \n)). Strongest postconditions and constraints. Using these functions, we can generate the strongest postconditions \nand the constraints as shown in Figure 10. Assignments, assume operations, and sequenc\u00ading is handled \nas before; we describe here only function calls. For SP, we rename the caller s store to M'as it will \nchange as a result of the call. We pass a memory MI equal to M'to the callee, equate the actual parameters \nwith the symbolic variables, and compute SP of the callee. Then we rename the memory returned by the \ncallee to MO, and copy back the local store modi.ed by the call into M' to get the current memory M. \nAdditionally, we add distinctness constraints to model allocation. The de.nition of SPnis similar to \nthe one before: before it was the predicate abstraction of SP, using SPnto analyze the call body; now \nit is the predicate abstraction (using the predicates at the return location) of the new SP, using SPnrecursively \nfor the call body. The correctness of the constraint generation is stated in Theorem 2. The size of the \nconstraints is cubic in the size of the trace. Given the new de.nition of Con, the method for predicate \ndiscovery is Algorithm 2, and Theorem 1 holds. EXAMPLE 6: Consider the trace in Figure 11. The right \ncolumn shows the constraints that correspond to each command. The con\u00adstraint from the assignment *y \n=0is (*(y)1))1)=0. First, the con\u00adstraint for the call command is the clause .ld , which loads the actual \nparameters into to the symbolic constants for in. The .rst com\u00admand in the body loads the values from \nthe symbolic constants into the formal parameters; notice that we take the closure . We then build the \nconstraints for the increment operation. Now L ={y)*y}, P ={fx)f*x}, R ={*fx}, and R' ={fx)*fx)f*x}. \nThe constraint New(L)P)R)(.).O)simpli.es to true, because *fx is not a ref\u00aderence type, and (fx)1)=(y)0), \ni.e., it is a cell that was passed to f . Let .'be the map updating . so that *y is mapped to 9. Fi\u00adnally, \nthe copy-back constraint opy(.).' ).O)(L)R)is shown to the right of the return. At the end, the assume \noperation generates the constraint (*(y)1))3)=1. The set of generated constraints is unsatis.able. Consider \nthe fourth cut-point of this trace, i.e., up to and including the increment operation. The common variables \nare (fx)1), (*(fx)1))8), and (f*x)2); they denote the current val\u00adues of fx, *fx, and f*x, respectively. \nThe interpolant for this cut is (*(fx)1))8)=(f*x)2)+1, which upon cleaning gives the predicate *fx =f*x \n+1. This predicate asserts that the present cell pointed to by fx has a value 1 greater than the cell \n*x had upon entry to f . .  7 Experiments We have implemented the interpolation based abstraction re.ne\u00adment \nscheme in the software model checker BLAST [19]. The algorithm for generating interpolants uses the VAMPYRE \nproof\u00adgenerating theorem prover.2 For ef.ciency, we have implemented several optimizations of the basic \nprocedure described in this paper. First, we treat sequences of assignments atomically. Second, we do \nnot cut at every point of a spurious error trace. Instead, we perform a preliminary analysis which identi.es \na subset of the constraints that imply the infeasibility of the trace, and only consider the in\u00adstructions \nthat generate these constraints as cut-points. It is easy to check that the optimized procedure is still \ncomplete. For pointers, we only generate constraints between expressions of the same type. With these \noptimizations, we .nd, for example, that the two pred\u00adicates a <c -2 and b >0 suf.ce to prove the trace \nin Example 1 infeasible. These are fewer predicates than those generated by the heuristically optimized \npredicate discovery scheme of [3]. We ran interpolation based BLAST on several Windows NT de\u00advice drivers, \nchecking a property related to the handling of I/O Request packets. The property is a .nite-state automaton \nwith 22 states [2]. The results, obtained on an IBM ThinkPad T30 laptop with a 2.4 GHz Pentium processor \nand 512MB RAM, are sum\u00admarized in Table 1. We present three sets of numbers: Previ\u00adous gives the times \nfor running the previous version of BLAST, without interpolants; Craig uses interpolants to discover \npred\u00adicates, and drops predicates that are out of scope, but it does not track different sets of predicates \nat individual program loca\u00adtions; Craig+Locality uses interpolants and tracks only the rel\u00adevant predicates \nat each program location. The previous version of BLAST timed out after several hours on the drivers \nparport and parclass. We found several violations of the speci.cation in parclass. The numbers in the \ntable refer to a version of parclass where the cases that contain errors are commented out. Both Craig \nand Craig+Locality perform better than the previ\u00adous version of BLAST. When started with the empty set \nof ini\u00adtial predicates, Craig is faster than Craig+Locality , because Craig+Locality may rediscover the \nsame predicate at several dif\u00adferent program locations. However, since the predicates are tracked extremely \nprecisely (the average number of predicates at a program location is much smaller than the total number \nof predicates re\u00adquired), Craig+Locality uses considerably less memory, and sub\u00adsequent runs (for example, \nfor verifying a modi.ed version of the program [18], or for generating PCC-style proofs [17]) are faster, \nand the proof trees smaller.  8 References [1] T. Ball, T. Millstein, and S.K. Rajamani. Polymorphic \npredicate ab\u00adstraction. ACM Transactions on Programming Languages and Sys\u00adtems, 2003. [2] T. Ball and \nS.K. Rajamani. Personal communication. 2VAMPYRE is available from http://www.eecs.berkeley.edu/.rupak/Vampyre. \n Program LOC Previous Craig Craig+Locality Disc Reach Disc Reach Preds Disc Reach Preds Avg/Max kbfiltr \n12301 1m12s 0m30s 0m52s 0m22s 49 3m48s 0m10s 72 6.5/16 floppy 17707 7m10s 3m59s 7m56s 3m21s 156 25m20s \n0m46s 240 7.7/37 diskperf 14286 5m36s 3m3s 3m13s 1m18s 86 13m32s 0m27s 140 10/31 cdaudio 18209 20m18s \n4m55s 17m47s 4m12s 196 23m51s 0m52s 256 7.8/27 parport 61777 - - - - - 74m58s 2m23s 753 8.1/32 parclass \n138373 - - 42m24s 9m1s 251 77m40s 1m6s 382 7.2/28 Table 1. Experimental results using BLAST: m stands \nfor minutes, s for seconds; LOC is the number of lines of preprocessed code; Disc is the total running \ntime of the veri.cation starting with the empty set of predicates; Reach is the time to perform the reachability \nanalysis only, given all necessary predicates; Preds is the total number of predicates required, and \nAvg (Max) is the average (maximum) number of predicates tracked at a program location; the symbol - indicates \nthat the tool does not .nish in 6 hours. [3] T. Ball and S.K. Rajamani. Generating abstract explanations \nof spu\u00adrious counterexamples in C programs. Technical Report MSR-TR\u00ad2002-09, Microsoft Research, 2002. \n[4] T. Ball and S.K. Rajamani. The SLAM project: debugging system software via static analysis. In POPL \n02: Principles of Programming Languages, pages 1 3. ACM, 2002. [5] R. Bodik, R. Gupta, and M.L. Soffa. \nRe.ning data.ow information using infeasible paths. In FSE 97: Foundations of Software Engineer\u00ading, \nLNCS 1301, pages 361 377. Springer, 1997. [6] W.R. Bush, J.D. Pincus, and D.J. Sielaff. A static analyzer \nfor .nd\u00ading dynamic programming errors. Software Practice and Experience, 30:775 802, 2000. [7] S. Chaki, \nE.M. Clarke, A. Groce, and O. Strichman. Predicate abstrac\u00adtion with minimum predicates. In CHARME 03: \nCorrect Hardware Design and Veri.cation, LNCS 2860, pages 19 34. Springer, 2003. [8] E.M. Clarke, O. \nGrumberg, S. Jha, Y. Lu, and H. Veith. Counterexample-guided abstraction re.nement. In CAV 00: Computer-Aided \nVeri.cation, LNCS 1855, pages 154 169. Springer, 2000. [9] W. Craig. Linear reasoning. J. Symbolic Logic, \n22:250 268, 1957. [10] R. Cytron, J. Ferrante, B.K. Rosen, M.N. Wegman, and F.K. Zadek. Ef.ciently computing \nstatic single assignment form and the program dependence graph. ACM Transactions on Programming Languages \nand Systems, 13:451 490, 1991. [11] M. Das, S. Lerner, and M. Seigle. ESP: path-sensitive program ver\u00adi.cation \nin polynomial time. In PLDI 02: Programming Language Design and Implementation, pages 57 68. ACM, 2002. \n[12] C. Flanagan. Automatic software model checking using CLP. In ESOP 03: European Symposium on Programming, \nLNCS 2618, pages 189 203. Springer, 2003. [13] C. Flanagan, K.R.M. Leino, M. Lillibridge, G. Nelson, \nJ.B. Saxe, and R. Stata. Extended static checking for Java. In PLDI 02: Programming Language Design and \nImplementation, pages 234 245. ACM, 2002. [14] C. Flanagan and J.B. Saxe. Avoiding exponential explosion: \ngen\u00aderating compact veri.cation conditions. In POPL 01: Principles of Programming Languages, pages 193 \n205. ACM, 2001. [15] J.S. Foster, T. Terauchi, and A. Aiken. Flow-sensitive type quali.ers. In PLDI 02: \nProgramming Language Design and Implementation, pages 1 12. ACM, 2002. [16] D. Gries. The Science of \nProgramming. Springer, 1981. [17] T.A. Henzinger, R. Jhala, R. Majumdar, G.C. Necula, G. Sutre, and W. \nWeimer. Temporal-safety proofs for systems code. In CAV 02: Computer-Aided Veri.cation, LNCS 2404, pages \n526 538. Springer, 2002. [18] T.A. Henzinger, R. Jhala, R. Majumdar, and M.A.A. Sanvido. Ex\u00adtreme model \nchecking. In International Symposium on Veri.cation, LNCS. Springer, 2003. [19] T.A. Henzinger, R. Jhala, \nR. Majumdar, and G. Sutre. Lazy abstrac\u00adtion. In POPL 02: Principles of Programming Languages, pages \n58 70. ACM, 2002. [20] J. Krajicek. Interpolation theorems, lower bounds for proof systems, and independence \nresults for bounded arithmetic. J. Symbolic Logic, 62:457 486, 1997. [21] J. McCarthy and J. Painter. \nCorrectness of a compiler for arithmetic expressions. In Proc. Symposia in Applied Mathematics. American \nMathematical Society, 1967. [22] K.L. McMillan. Interpolation and SAT-based model checking. In CAV 03: \nComputer-Aided Veri.cation, LNCS 2725, pages 1 13. Springer, 2003. [23] J.C. Mitchell. Foundations for \nProgramming Languages. MIT Press, 1996. [24] J. M. Morris. A general axiom of assignment. In Theoretical \nFounda\u00adtions of Programming Methodology, Lecture Notes of an International Summer School, pages 25 34. \nD. Reidel Publishing Company, 1982. [25] M. Musuvathi, D.Y.W. Park, A. Chou, D.R. Engler, and D.L. Dill. \nCMC: A pragmatic approach to model checking real code. In OSDI 02: Operating Systems Design and Implementation. \nACM, 2002. [26] G.C. Necula. Proof carrying code. In POPL 97: Principles of Pro\u00adgramming Languages, pages \n106 119. ACM, 1997. [27] G. Nelson. Techniques for program veri.cation. Technical Report CSL81-10, Xerox \nPalo Alto Research Center, 1981. [28] P. Pudlak. Lower bounds for resolution and cutting plane proofs \nand monotone computations. J. Symbolic Logic, 62:981 998, 1997. [29] T. Reps, S. Horwitz, and M. Sagiv. \nPrecise interprocedural data.ow analysis via graph reachability. In POPL 95: Principles of Program\u00adming \nLanguages, pages 49 61. ACM, 1995.  \n\t\t\t", "proc_id": "964001", "abstract": "The success of model checking for large programs depends crucially on the ability to efficiently construct parsimonious abstractions. A predicate abstraction is parsimonious if at each control location, it specifies only relationships between <i>current</i> values of variables, and only those which are required for proving correctness. Previous methods for automatically refining predicate abstractions until sufficient precision is obtained do not systematically construct parsimonious abstractions: predicates usually contain symbolic variables, and are added heuristically and often uniformly to many or all control locations at once. We use Craig interpolation to efficiently construct, from a given abstract error trace which cannot be concretized, a parsominous abstraction that removes the trace. At each location of the trace, we infer the relevant predicates as an interpolant between the two formulas that define the past and the future segment of the trace. Each interpolant is a relationship between current values of program variables, and is relevant only at that particular program location. It can be found by a linear scan of the proof of infeasibility of the trace.We develop our method for programs with arithmetic and pointer expressions, and call-by-value function calls. For function calls, Craig interpolation offers a systematic way of generating relevant predicates that contain only the local variables of the function and the values of the formal parameters when the function was called. We have extended our model checker <sc>Blast</sc> with predicate discovery by Craig interpolation, and applied it successfully to C programs with more than 130,000 lines of code, which was not possible with approaches that build less parsimonious abstractions.", "authors": [{"name": "Thomas A. Henzinger", "author_profile_id": "81100034124", "affiliation": "University of California, Berkeley, CA", "person_id": "PP14024447", "email_address": "", "orcid_id": ""}, {"name": "Ranjit Jhala", "author_profile_id": "81100198278", "affiliation": "University of California, Berkeley, CA", "person_id": "P343132", "email_address": "", "orcid_id": ""}, {"name": "Rupak Majumdar", "author_profile_id": "81100319213", "affiliation": "University of California, Berkeley, CA", "person_id": "P335105", "email_address": "", "orcid_id": ""}, {"name": "Kenneth L. McMillan", "author_profile_id": "81100137792", "affiliation": "Cadence Berkeley Labs., Berkeley, CA", "person_id": "P160229", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/964001.964021", "year": "2004", "article_id": "964021", "conference": "POPL", "title": "Abstractions from proofs", "url": "http://dl.acm.org/citation.cfm?id=964021"}