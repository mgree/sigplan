{"article_publication_date": "01-01-2004", "fulltext": "\n Global Value Numbering using Random Interpretation Sumit Gulwani George C. Necula gulwani@cs.berkeley.edu \nnecula@cs.berkeley.edu Department of Electrical Engineering and Computer Science University of California, \nBerkeley Berkeley, CA 94720-1776 Abstract We present a polynomial time randomized algorithm for global \nvalue numbering. Our algorithm is complete when conditionals are treated as non-deterministic and all \noperators are treated as uninter\u00adpreted functions. We are not aware of any complete polynomial\u00adtime deterministic \nalgorithm for the same problem. The algorithm does not require symbolic manipulations and hence is simpler \nto implement than the deterministic symbolic algorithms. The price for these bene.ts is that there is \na probability that the algorithm can report a false equality. We prove that this probability can be made \narbitrarily small by controlling various parameters of the algorithm. Our algorithm is based on the idea \nof random interpretation, which relies on executing a program on a number of random inputs and discovering \nrelationships from the computed values. The computa\u00adtions are done by giving random linear interpretations \nto the opera\u00adtors in the program. Both branches of a conditional are executed. At join points, the program \nstates are combined using a random af.ne combination. We discuss ways in which this algorithm can be \nmade more precise by using more accurate interpretations for the linear arithmetic operators and other \nlanguage constructs. Categories and Subject Descriptors D.2.4 [Software Engineering]: Software/Program \nVeri.cation; F.3.1 [Logics and Meanings of Programs]: Specifying and Veri\u00adfying and Reasoning about Programs; \nF.3.2 [Logics and Meanings of Programs]: Semantics of Programming Languages Program analysis This research \nwas supported in part by the National Science Founda\u00adtion Career Grant No. CCR-9875171, and ITR Grants \nNo. CCR-0085949 and No. CCR-0081588, and gifts from Microsoft Research. The informa\u00adtion presented here \ndoes not necessarily re.ect the position or the policy of the Government and no of.cial endorsement should \nbe inferred. Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL \n04, January 14 16, 2004, Venice, Italy. Copyright 2004 ACM 1-58113-729-X/04/0001 ...$5.00  General Terms \nAlgorithms, Theory, Veri.cation  Keywords Global Value Numbering, Herbrand Equivalences, Random Inter\u00adpretation, \nRandomized Algorithm, Uninterpreted Functions 1 Introduction Detecting equivalence of expressions in \na program is a prerequi\u00adsite for many important optimizations like constant and copy prop\u00adagation [18], \ncommon sub-expression elimination, invariant code motion [3, 13], induction variable elimination, branch \nelimination, branch fusion, and loop jamming [10]. It is also important for dis\u00adcovering equivalent computations \nin different programs, for exam\u00adple, plagiarism detection and translation validation [12, 11], where \na program is compared with the optimized version in order to check the correctness of the optimizer. \nSince the equivalence problem is undecidable, compilers typically implement algorithms that solve a restricted \nproblem, where expressions are considered equivalent if and only if they are computed using the same \noperator applied on equivalent operands. This form of equivalence, where the operators are treated as \nuninterpreted functions, is called Herbrand equiva\u00adlence. Such analyses, which include global value numbering \n[2], are widely used in optimizing compilers. Existing algorithms for global value numbering are either \ntoo ex\u00adpensive or imprecise. The precise algorithms are based on an early algorithm by Kildall [9], where \nequivalences are discovered us\u00ading an abstract interpretation [4] on the lattice of Herbrand equiva\u00adlences. \nKildall s algorithm discovers all Herbrand equivalences in a function body but has exponential cost [15]. \nOn the other extreme, there are several polynomial time algorithms that are complete for basic blocks, \nbut are imprecise in the presence of joins and loops in a program. An example of a program that causes \ndif.culties is given in Figure 1. The popular partition re.nement algorithm proposed by Alpern, Wegman, \nand Zadeck (AWZ) [1] is particularly ef.cient, however at the price of being signi.cantly less precise \nthan the Kildall s algorithm. The novel idea in the AWZ algorithm is to represent the values of variables \nafter a join using a fresh selection func\u00adtion fm, similar to the functions used in the static single \nassignment form [5], and to treat the fm function as another uninterpreted func\u00adtion. The values of z \nand x after the join in our example can both be written as fm(a,b). The AWZ algorithm then treats the \nf functions as additional uninterpreted operators in the language and is able to Figure 1. Example of \nnon-trivial assertions detect that x and z are equivalent. The AWZ algorithm rewrites the second assertion \nas fm(F(a),F(b)) F(fm(a,b)), which cannot be veri.ed if the f functions are uninterpreted. In an attempt \nto remedy this problem, R\u00a8uthing, Knoop and Stef\u00adfen have proposed a polynomial time algorithm that alternately \nap\u00adplies the AWZ algorithm and some rewrite rules for normalization of terms involving f functions, until \nthe congruence classes reach a .xpoint [15]. Their algorithm discovers more equivalences than the AWZ \nalgorithm (including the second assertion in our example). It is complete for acyclic control-.ow graphs, \nbut is incomplete in the presence of loops. Recently, Karthik Gargi has proposed a set of balanced algorithms \nthat are ef.cient, but also incomplete [6]. In this paper, we describe a randomized algorithm that discovers \nas many Herbrand equivalences as the abstract interpretation algo\u00adrithm of Kildall, while retaining polynomial \ntime complexity. Our algorithm works by simulating the execution of a function on a small number of random \nvalues for the input variables. It executes both branches of a conditional, and combines the values of \nvari\u00adables at join points using f functions. The key idea is that each operator and each implicit f function \nat a join point in the program is given a random interpretation. These interpretations are care\u00adfully \nchosen such that they obey all the semantic properties of f functions (i.e. our algorithm does not regard \nf functions as unin\u00adterpreted unlike the AWZ algorithm). This means that the values of variables computed \nin one pass through the program re.ect all of the Herbrand equivalences that are common to all paths \nthrough the program. The algorithm is also simpler to implement than the deterministic symbolic algorithms, \nprimarily because it resembles an interpreter that uses a simple mapping of variables to values as its \nmain data structure. The price for the completeness and simplic\u00adity of the algorithm is that, in rare \nsituations, the algorithm might report an apparent Herbrand equivalence that is actually false. We prove \nthat the probability of this happening is very small. The idea of giving random af.ne interpretations \nto f functions has been used earlier in the context of a randomized algorithm for dis\u00adcovering linear \nequalities among variables in a program [7]. That algorithm, however, is limited to programs in which \nall computa\u00adtions consist of linear arithmetic. The biggest obstacle we had to overcome in trying to \nextend the linear arithmetic approach to ar\u00adbitrary operators was to .nd a suitable class of random interpreta\u00adtions \nfor the non-arithmetic operators. We show later in this paper that all straightforward interpretations \n(i.e., as some functions of Figure 2. A code fragment with four paths. Of the two equa\u00adtions asserted \nat the end the .rst one holds on all paths but the second one holds only on three paths. The numbers \nshown next to each edge represent values of variables in the random inter\u00adpretation scheme. the value \nof the operands) are either unsound or incomplete, when taken along with the af.ne interpretation of \nf functions. Our solu\u00adtion is surprising because it requires several parallel simulations of the program. \nThe result of an expression in a given simulation is not only based on its top-level operator and the \nvalues of its operands in that simulation, but also on the values of its operands in other simulations. \nWe give a proof of probabilistic soundness and com\u00adpleteness of this scheme. We also give an analytical \nformula de\u00adscribing the number of parallel simulations required to achieve a desired probability of error. \nFurthermore, we show that it is possi\u00adble to combine, in the same algorithm, the natural interpretation \nof linear arithmetic operators with our random interpretation of non\u00adarithmetic operators. In Section \n2 we review the random interpretation technique for dis\u00adcovering linear relationships. Then, in Section \n3, we describe the proposed scheme for interpreting operators, and prove its sound\u00adness. In Section 4, \nwe assemble the main ideas to construct the random interpreter for discovering Herbrand equivalences. \nIn Sec\u00adtion 5, we extend this scheme to discover more equivalences by us\u00ading more accurate interpretations \nfor the linear arithmetic operators and other language constructs. 2 Background We illustrate the random \ninterpretation scheme for discovering lin\u00adear relationships among variables in a program [7], by means \nof an example. We also show a new proof of probabilistic soundness that gives insight into how this algorithm \ncould be extended beyond linear arithmetic. Consider the program shown in Figure 2 (ignoring for the \nmoment the annotations shown on the side). Of the two assertions at the end of the program, the .rst \nis true on all four paths, and the second is true on three of them (it is false when the .rst conditional \nis false and the second is true). Regular testing would have to exercise that precise path to avoid inferring \nthat the second equality holds. In\u00adstead, we use a non-standard interpretation model. At conditionals, \nwe proceed on both true and false branches. At joins, we choose a random weight w and use it to combine \nthe values v1 and v2 of a variable on the two sides of a join as follows: f(v1,v2)w Xv1 +(1 -w)Xv2 We \ncall this operation an af.ne join of v1 and v2 with weight w, written as v1 Ewv2. In essence, we are \ninterpreting the f functions as af.ne combinations with random weights. In the example, all variables \nare dead on entry; so the random val\u00adues with which we start the interpretation are irrelevant (we show \nthem as *in the .gure). We use the random weights w1 5 for the .rst join point and w2 -3 for the second \njoin point. We perform the computations, maintaining at each step only a value for each variable. We \ncan then verify easily that the resulting state at the end of the program satis.es the .rst assertion \nbut does not satisfy the second. Thus, in one run of the program we have noticed that one of the exponentially \nmany paths breaks the invariant. Note that choosing w to be either 0 or 1 at a join point corresponds \nto execut\u00ading either the true branch or the false branch of its corresponding conditional; this is what \nnaive random testing accomplishes. How\u00adever, by choosing w (randomly) from a set that also contains non-Boolean \nvalues, we are able to capture the effect of both branches of a conditional in just one interpretation \nof the program. The completeness argument of this interpretation scheme relies on the observation that \nby performing an af.ne join of two sets of val\u00adues (all with the same weight), the resulting values satisfy \nall linear relationships that are satis.ed by both initial sets of values. For the purpose of this paper, \nit is also important to note that (unfortunately) the af.ne join operation does not preserve non-linear \nrelationships. For example, in the program in Figure 1 it is true that a Xb 0, but this non-linear relationship \nis not implied by the program state after the .rst join point. The probabilistic soundness argument given \nin [7] is complicated by an adjustment operation performed by the random interpreter. The purpose of \nthis operation is to adjust a program state such that it re.ects the additional equality fact implied \nby an equality condi\u00adtional on its true branch. If we ignore this operation, we can give a simpler proof \nof soundness in terms of polynomials. A straightline sequence of assignments, involving only linear arithmetic, \ncom\u00adputes the values of variables at the end as linear polynomials in terms of the variables live on \ninput. The overall effect of the af.ne join operation is to compute the weighted sum of these polynomi\u00adals \ncorresponding to each path. These weights themselves are non\u00adlinear polynomials in terms of the random \nweights wi. For example, the values of a, b, c and d at the end of the program shown in Fig\u00adure 2 can \nbe written as follows (there are no live input variables in this program): a w1 X0 +(1 -w1)X1 1 -w1 b \nw1 X1 +(1 -w1)X0 w1 c w2 X(b -a)+(1 -w2 )X(2a +b) w2 X(w1 -1 +w1)+(1 -w2)X(2 -2w1 +w1) 3w1w2 -w1 -3w2 \n+2 d w2 X(1 -2b)+(1 -w2 )X(b -2) w2 X(1 -2w1)+(1 -w2)X(w1 -2) -3w1w2 +w1 +3w2 -2 Correspondingly, the \ntwo assertions at the end of the program can be written, respectively, as (3w1w2 -w1 -3w2 +2)+(-3w1w2 \n+ w1 +3w2 -2)0 and 3w1w2 -w1 -3w2 +2(1 -w1)+1. Note that the .rst equality of polynomials is a tautology, \nwhile the second is not. We can prove that an assertion that is true on all paths (i.e., on all Boolean \nvalues for w1 and w2) will correspond to an equal\u00adity between two equivalent polynomials. The opposite \nis true for assertions that are false on at least one path. Note that when fully expanded, these polynomials \nare exponential in size; however, this is not a problem since our interpreter can evaluate them in linear \ntime. The signi.cance of reducing the problem to that of detecting poly\u00adnomial equivalence lies in the \nfollowing classic theorem due to Schwartz [16]. THEOREM 1(RANDOMIZED POLYNOMIAL TESTING.). Let Q1(x1,,xn)and \nQ2(x1,,xn)be two non-equivalent multivariate polynomials of degree at most d, in variables x1,,xn over \na .eld ,. Fix any .nite set L s,, and let a1,,an be chosen independently and uniformly at random from \nL. The probability that this choice is such that Q1(a1,,an)Q2(a1,,an)is at d most L]. ] Schwartz s theorem \nsays that if a random evaluation of two polyno\u00admials returns the same result then it is very likely that \nthe polyno\u00admials are equivalent. The theorem suggests that we can reduce the error probability in the \nrandom interpretation scheme by increas\u00ading the size of the set from which the random values are chosen. \nAdditionally, the error probability decreases exponentially with the number of independent trials. Random \ntesting can be thought of as an instance of this random interpretation scheme wherein the choice of weights \nw is restricted to the small set {0,1}(this corresponds to executing either the true branch or the false \nbranch of a conditional); but this gives a useless bound of d/2 for the error probability. The lack of \na known polynomial time deterministic algorithm for checking the equivalence of polynomials suggests \nthat randomiza\u00adtion has a chance to surpass deterministic algorithms in those pro\u00adgram analysis problems \nthat can be naturally reduced to checking equivalence of polynomials. Therefore it is not surprising \nthat ran\u00addom interpretation works so well for checking equivalences in pro\u00adgrams that involve only linear \narithmetic computations. We show in the rest of this paper that even non-arithmetic operators can be \nencoded using polynomials. These schemes are not as obvious as for linear arithmetic. They also sacri.ce \nprecision since the precise meaning of the operator is lost. However, these schemes are very effective \nin discovering Herbrand equivalences. 3 Random Interpretation of Operators We consider a language in \nwhich the expressions occurring in as\u00adsignments and equality assertions belong to the following simple \nlanguage of uninterpreted function terms (here x is one of the vari\u00adables): e :: x F(e1,e2) For simplicity, \nwe consider only one binary uninterpreted func\u00adtion F. However, our results can be extended easily to \nlanguages with any .nite number of uninterpreted functions of arbitrary ar\u00adity. Comparing expressions \nin this language is trivial because only identical expressions are Herbrand equivalent. The complications \narise in the presence of join points in a program as shown by the example in Figure 1. The random interpreter \ncompares expressions in this language by choosing an interpretation for F randomly from a suitable set \nof ad\u00adequate interpretations, followed by choosing random values for the variables and evaluating the \ntwo expressions given these choices. We assume that the choice of the interpretation of F is made by \nchoosing p parameters from some .eld ,. Thus, the interpretation of F, written [[F]]has the following \ntype: [[F]]: ,p -,X,-, Given an expression e with n variables, the given interpretation of F induces \nan interpretation of the expression e: [[e]]: , p -, n -, We achieve the desired probabilistic soundness \nproperty of random interpretation by ensuring that, for random choices of pE,p and w.h.p. .E, n, we have \nthe following two properties (where !, means implies with high probability ): w.h.p. [[e1]]p. [[e2]]p. \n![[e1]]p-on-o[[e2]](2) oo [[e1]]op-on-o[[e2]]!e1 e2 (3) We ensure property 2 by choosing the interpretation \nF to be a poly\u00adnomial on p +2 variables. Assume now that we choose the follow\u00ading polynomial interpretation \nfor F, with parameters r1 and r2: [[F]](r1,r2)(x,y)r1x2 +r2y2 (4) This interpretation has the desired \nprobabilistic soundness prop\u00aderty, although the degree of the polynomial [[e]]is exponential in the depth \nof the expression e. According to Schwartz s theorem this drastically increases the probability of error, \nsuggesting that perhaps we should consider only polynomials that are linear in the program variables \n(x and y). There is, in fact, another important reason to choose linear polyno\u00admials. We choose the af.ne \ninterpretation for ffunctions because it is very effective in reasoning about linear expressions in a \npro\u00adgram [7]. We do not know of any other interpretation for ffunc\u00adtions that is effective in reasoning \nabout any program properties. In order to ensure the desired completeness property of random in\u00adterpretation, \nwe require that [[F]]respects the af.ne interpretation given to the ffunctions. This means that for all \n.eld values a, b, c, and d, and all pE,p we must have: [[fm(F(a,b),F(c,d))]]p [[F(fm(a,c),fm(b,d))]]p \nor, equivalently: w[[F]]p(a,b)+(1 -w)[[F]]p(c,d) [[F]]p(wa +(1 -w)c,wb +(1 -w)d)(5) It can be veri.ed \nthat the interpretation for F in equation 4 does not satisfy completeness property 5 (except for the \ncases when w E{0,1}, which correspond exactly to the actual paths through the program). Moreover, it \nis possible to prove that if the ffunc\u00adtions are given the af.ne-join interpretation, and the completeness \nequation 5 is required to hold, then [[F] pmust be a linear polyno\u00admial in the program variables, for \nall values of pE,p. The example in Section 2 that demonstrates that the af.ne join operation does not \npreserve non-linear relationships also illustrates this fact. Figure 3. An example of two distinct uninterpreted \nfunction terms e1 and e2 which are equivalent when we model the binary uninterpreted function F as a \nlinear function of its arguments. Unfortunately, if [[F] pis a linear polynomial then the soundness equation \n3 does not hold. Consider, for example, the linear inter\u00adpretation [[F]](r1,r2)(x,y)r1x +r2y In Figure \n3 we show two distinct expressions that have the same interpretation, under this interpretation for F. \nSimilar counterex\u00adamples arise for any linear interpretation, and in the presence of functions of arity \nat least two, but not if the language contains only unary functions, or constants. It appears that we \nhave reached an impasse. If we .x the af.ne-join interpretation of f, then only linear polynomials satisfy \nthe com\u00adpleteness property. But linear polynomials are not sound interpre\u00adtations of arbitrary operators. \nIn the next section we describe a way out of this impasse. 3.1 Random k-Linear Interpretations One way \nto characterize the failure of the soundness property when using linear interpretations for binary functions \nis that we are re\u00adstricted to only three random coef.cients, which are too few to en\u00adcode a large number \nof leaves. Thus, it is possible for two distinct trees to have identical interpretations. To increase \nthe number of coef.cients while maintaining linearity, we modify the interpreter to maintain k values \nfor each variable and for each expression. This enables us to introduce more random parameters in the \ninterpretation function. k is a parameter of the random interpreter, and we are going to derive lower \nbounds for k later in this section. We need to re.ne the interpretations given in the previous section. \nBoth the function F and any expression e now have a family of k interpretations, each with p parameters: \n[[F]]: {1,,k}-, p -,X,-, [[e]]: {1,,k}-, p -, n -, For the rest of the presentation we are going to work \nwith a fam\u00adily of k linear polynomial interpretations (i.e. [[F]]i p is linear for all 1 :i :k). This \nfamily uses p 4k -2 parameters, named r1,,rk, ;;;; r,,rk, s1,,sk-1 and s,,sIn order to simplify the rest \n11k-1. of the presentation, we introduce an alternate notation P(e,i)for ([[e]]i), for an expression \ne and index i between 1 and k. The de.\u00adnition of P(e,i)is by induction on the structure of e, as follows: \nP(x,i)x ; P(F(e1,e2),1)r1P(e1,1)+r1P(e2,1) ; P(F(e1,e2),i)riP(e1,i)+riP(e2,i) +si-1P(e1,i -1) ; +si-1P(e2,i \n-1)fori >1 Note that the degree of polynomial P(e,i)is equal to the depth of expression e. Also note \nthat for any i, P(e,i)does not contain any of the variables ri+1,,rk and si,,sk. This means that the \npolyno\u00admial P(F(e1,e2),i)can be decomposed uniquely into the subpoly\u00ad ;; nomials riP(e1,i)+riP(e2,i)(which \ncontains variables ri and ri), ; si-1P(e1,i -1)(which contains variable si-1 but not ri or ri), and ;;; \nsi-1P(e2,i -1)(which contains variable si-1 but not ri, ri or si-1). This implies the following useful \nproperty. PROPERTY 6. For any integer i >1,P(F(e1,e2),i) ;; P(F(e1,e2),i)iff ;;;; (a) riP(e1,i)+riP(e2,i)riP(e1,i)+riP(e2,i), \nand ; (b) P(e1,i -1)P(e1,i -1), and ; (c) P(e2,i -1)P(e2,i -1) We now prove the soundness lemma, which \nstates that if the sym\u00adbolic polynomials associated with two expressions are equivalent, then the two \nexpressions are equal. LEMMA 7(K-LINEAR SOUNDNESS LEMMA). Let e and e;be two tree expressions such that \ne has at most 2j leaves, and P(e,i) ;; P(e,i)for some i ;j.Thene e. PROOF. Note that e and e;have the \nsame depth since P(e,i)and ; P(e,i)have the same degree (as they are equivalent). The proof ; is by induction \non the structure of expressions e and e. The base ; case is trivial since e and eare both leaves and \nP(e,i)e and ;; P(e,i)e;. Clearly, ee. ;;; For the inductive case, eF(e1,e2)and eF(e1,e2). By as\u00ad ;; sumption, \nP(F(e1,e2),i)P(F(e1,e2),i)for some i ;j. Since e has at most 2j leaves, it must be that at least one \nof e1 or e2 has at most 2j-1 leaves. Consider the case when e1 has at most 2j-1 leaves (the other case \nis symmetric). From Property 6(b) we have ; that P(e1,i -1)P(e1,i -1). Since i -1 ;j -1, we can apply \n;; the induction hypothesis for e1 and e1 to obtain that e1 e1. Con\u00ad ; sequently, P(e1,i)P(e1,i). This \nallows us to simplify the Prop\u00ad ; erty 6(a) to P(e2,i)P(e2,i). Since e2 has at most 2j leaves, we ; can \napply the induction hypothesis for e2 and e2 to conclude that ; e2 e2. This completes the proof. This \nresult means that our family of interpretations is an injective mapping from trees to polynomials, and \nallows us to compare trees by random testing of their corresponding polynomials. Note that the higher \nthe index of the polynomial, the larger the trees that it can discriminate. The number of parallel values \nthat we need to compute for each node must be at least the logarithm of the number of leaves in the tree. \nInterestingly, this value does not depend on the depth of the tree. A consequence is that trees involving \nonly unary constructors can be discriminated with k 1, independent of the depth. The expressions that \narise in programs can be represented as DAGs of size linear in the size of the program. In the worst \ncase, the number of leaves in such a DAG, when expressed as a tree, is exponential in the largest depth \nof an expression computed by the program; thus k must be chosen at least as big as the largest depth \nof an expression computed by the program. We have performed a number of experiments that suggest that \nan even tighter bound on k might be possible, but we are not able to prove any such result at the moment. \nWe also have not been able to prove stronger properties by using more complex linear polynomial interpretations. \n  4 The Random Interpreter . We now put together the ideas mentioned in the previous sections to describe \nthe random interpreter :. 4.1 Notation A state . E, n is an assignment of .eld values to the n variables \nof the program. We use the notation .(x)to denote the value of vari\u00adable x in state .. The notation .[x \n+q]denotes the state obtained from . by setting the value of variable x to q. Our algorithm performs \narithmetic over some .eld ,. For imple\u00admentation reasons it is desirable that the .eld ,should be .nite \nso that arithmetic can be performed using .nite representation for values. Hence, we choose ,Zq, where \nq is some prime number and Zq refers to the .eld containing the integers {0,,q -1}.In this .eld, all \nthe arithmetic operations are performed modulo prime q. The error probability of our algorithm is inversely \nproportional to the size of the .eld ,(as stated in Theorem 14 in Section 4.4). Hence, by choosing a \nlarger q, we can make the error probability of our algorithm smaller. Our algorithm maintains k states \nat each point in the program, where k is as described in the previous section. This set of states is \nreferred to as a sample S. We write Si to refer to the ith state of the sample S. The algorithm computes \nk values for each expression e in the program. The ith value of an expression e at some program point \na can be written as P(e,i)p Si, where P is the polynomial interpretation given in the previous section, \np refers to the values of ;; the parameters ri,ri,si-1 and si-1 chosen independently and uni\u00adformly at \nrandom from the .nite .eld ,, and S is the sample at the program point a. However, this value is computed \ndirectly (with\u00adout .rst computing the polynomial P(e,i)) by simply evaluating the expression on the given \nstate and the values chosen for the parame\u00ad ;; ters ri, ri, si-1 and si-1. Essentially, this value is \ncomputed by the function V (e,i,S), whose de.nition is given below. V (x,i,S)Si[x] ; V (F(e1,e2),1,S)r1V \n(e1,1,S)+r1V (e2,1,S) ; V (F(e1,e2),i,S)riV (e1,i,S)+riV (e2,i,S) +si-1V (e1,i -1,S) ; +si-1V (e2,i -1,S)for \ni >1 We say that a sample S satis.es a Herbrand equivalence e1 e2 when V (e1,k,S)V (e2,k,S). We write \nSe1 e2 when this is the case. Note that we use only the kth value when deciding the equivalence. This \nis motivated by Lemma 7, which says that the kth polynomial has the most discriminating power among the \npolynomials that we evaluate. Finally, we extend the af.ne join operation from individual values to states, \nin which case we perform the join with the same weight for each variable. We further extend the af.ne \njoin operation to samples, in which case we perform the af.ne join operation on each pair of corresponding \nstates with the same weight. 4.2 The Random Interpreter Algorithm The random interpreter :executes a \nprocedure like an abstract in\u00adterpreter or a data-.ow analyzer. It goes around each loop until a .xed \npoint is reached. The criterion for .xed point is de.ned in Sec\u00adtion 4.4. The random interpreter maintains \na sample of k states at each program point. These samples encode Herbrand equivalences, or relationships \namong uninterpreted function terms of a program. A sample at a program point is obtained from the sample(s) \nat the immediately preceding program point(s). The initial sample con\u00adsists of k copies of a randomly \nchosen state ., i.e. the values of all variables of the program in state . are chosen independently and \nuniformly at random from the .eld ,. We now describe the action of the random interpreter on the three \nbasic nodes of a .ow-chart, which are shown in Figure 4. Assignment Node: See Figure 4 (a). Si S ;i[x \n+V (e,i,S)] Conditional Node: See Figure 4 (b). S1 S ;and S2 S ;  Join Node: See Figure 4 (c). SS1 \nEwS2, where w is a fresh random value chosen inde\u00adpendently and uniformly at random from ,.  After .xed \npoint has been reached, the results of the random in\u00adterpreter can be used to verify or discover equivalences \namong ex\u00adpressions at any point in the program as follows. Two expressions e1 and e2 always have the \nsame value at some point P in a program if Se1 e2, where S is the sample at point P, or equivalently \nif their kth value is the same in the given sample.  4.3 Completeness and Soundness Theorems For the \npurpose of the analysis of the algorithm, we introduce two new interpreters: a symbolic random interpreter \n: , which is a sym\u00adbolic version of the random interpreter :, and an abstract inter\u00adpreter A, which is \nsound and complete. We prove that : is as com\u00adplete and as sound as A. We then show that this implies \nthat : is probabilistically sound, and is complete when all operators are uninterpreted and the conditionals \nare non-deterministic. 4.3.1 The Symbolic Random Interpreter : The symbolic random interpreter : maintains \na symbolic state and executes a program like the random interpreter :but symbolically. Instead of using \nrandom values for the initial values for variables, or the parameters w, ri and si, it uses variable \nnames and maintains symbolic expressions. We use the letter S to range over the sym\u00adbolic samples maintained \nby the symbolic random interpreter. We write V (e,i,S )to denote the ith symbolic value of expression \ne in symbolic sample S . The following property states the relationship between the samples computed \nby :and the symbolic samples computed by : . PROPERTY 8. Let :at some S be a symbolic sample computed \nby point in the program and let S be the corresponding sample com\u00adputed by :at the same point. The sample \nS can be obtained from the symbolic sample S by substituting the input variables, the weight and parameter \nvariables w, ri and si with the values that : has used for them.  4.3.2 The Abstract Interpreter A The \nabstract interpreter Acomputes the Herbrand equivalences in a program. In the following de.nition we \nuse the letter U to range over sets of Herbrand equivalences. We write U !e1 e2 to say that the conjunction \nof the Herbrand equivalences in U imply e1 e2. We write U1 nU2 for the set of Herbrand equivalences that \nare implied by both U1 and U2. Finally, we write U[e/x]for the relationships that are obtained from those \nin U by substituting e for x. With these de.nitions we can de.ne the action of Aover the nodes of a .ow-chart \nas follows: Assignment Node: See Figure 4 (a). U {xe[x;/x]}U;[x;/x], where x;is a fresh variable  Conditional \nNode: See Figure 4 (b). U1 U;and U2 U;  Join Node: See Figure 4 (c). UU1 nU2  The abstract interpreter \nstarts with the empty set of Herbrand equiv\u00adalences. Implementations of abstract interpretations such \nas Ahave been described in the literature [9]. The major concern there is the concrete representation \nof the set U and the implementation of the operation U1 nU2. In Kildall s original presentation the set \nU has an exponential-size representation, although this is not necessary [15]. Here we use Aonly to state \nand prove the soundness and complete\u00adness results of the random interpreter :. The abstract interpreter \nAis both sound and complete when all operators are uninterpreted and conditionals are non-deterministic \n[17, 15]. We now state the relationship between the sets of symbolic samples S computed by : and the \nsets of Herbrand equivalences U com\u00adputed by Ain the form of completeness and soundness theorems. THEOREM \n9(COMPLETENESS THEOREM). LetU beasetof Herbrand equivalences computed by Aat some point in the pro\u00adgram \nand let S be the corresponding symbolic sample. Let e1 and e2 be any two expressions such that U !e1 \ne2. Then, S e1 e2. The completeness theorem implies that the random interpreter : discovers all the Herbrand \nequivalences that the abstract interpreter Adiscovers. The proof of Theorem 9 is based on Lemma 10 which \nis stated and proved below. Lemma 10 states that the af.ne join of two states satis.es all the Herbrand \nequivalences that are satis\u00ad.ed by both the states. The full proof of Theorem 9 is given in Appendix \nA.1. S ; be two symbolic samples that satisfy the Herbrand equivalence e1 e2. Then, for any choice of \nweight w, the union S uS EwS ; also satis.es the same Herbrand equivalence. LEMMA 10 (UNION COMPLETENESS \nLEMMA). Let S and PROOF. Note that for any expression e, and any symbolic sample T , V (e,k,T )is a linear \nfunction of the program variables in expres\u00ad  Figure 4. Flow-chart nodes sion e. Hence, for any af.ne \ncombination of two symbolic states S EwS ;, one can easily verify that V (e,k,S EwS ;)wXV (e,k,S )+ (1 \n-w)XV (e,k,S ;). Thus, if V (e1,k,S )V (e2,k,S )and V (e1,k,S ;)V (e2,k,S ;), then V (e1,k,S EwS ;)V \n(e2,k,S EwS ;). From here the completeness statement follows immediately. It is not surprising that \nthe completeness lemma holds, since we have chosen the linear interpretations of operators speci.cally \nto satisfy this constraint. Next we state the soundness theorem. THEOREM 11 (SOUNDNESS THEOREM). LetU \nbeasetofHer\u00adbrand equivalences computed by Aat some point in the program and let S be the corresponding \nsymbolic sample of k symbolic states. Let e1 and e2 be two expressions such that S e1 e2, and k ; min(degree(V \n(e1,k,S )),degree(V (e2,k,S ))). Then, U !e1 e2. According to Theorem 11, if the symbolic polynomials \nassociated with two expressions under our random interpretation scheme are equivalent, then those two \nexpressions are also found equivalent by the abstract interpreter. The proof of Theorem 11 is based on \nLemma 7. Notice, however, that in Theorem 11 the lower bound on k is stated based on the degree of V \n(e,k,S ), which is equal to the depth of expression e, while in Lemma 7, it is based on the logarithm \non the number of leaves. The reason for this weakening of the soundness statement is two-fold: it would \nhave been more complicated to carry out the proof with leaf counts, and in the worst case these measures \nare equal. The full proof of Theorem 11 is given in Appendix A.2. We use this soundness theorem in the \nnext section to prove Theorem 14, which establishes an upper bound on the probability that :is unsound. \n  4.4 Fixed Point Computation For a program with loops, the random interpreter :goes around each loop \nuntil a .xed point is reached, like an abstract interpreter or a data.ow analysis algorithm. A .xed point \nis reached when the Herbrand equivalences inferred from the numerical results of : are stable. The main \nconcerns are then whether the .xed-point is ever reached, and how many iterations are required. The answers \nto these questions are implied by the fact that the lattice of sets of Herbrand equivalences that are \ntrue at any point in a program has .nite depth as stated in Theorem 12 below. THEOREM 12. The lattice \nof sets of Herbrand equivalences (in\u00advolving the program variables) that are true at any point in a pro\u00adgram \n(under the set union operation as described in Section 4.3.2) has depth at most n where n is the number \nof program variables. We give a brief sketch of the proof of this theorem. The complete proof is in the \nfull version of the paper which is available as a tech\u00adnical report [8]. The proof of this theorem relies \non the following lemma. LEMMA 13. Let T be the set of program variables. The Herbrand equivalences at \nany point in the program can be represented by a pair H (I,E), where I sT is a set of independent variables \nand E is a set of equivalences x e, one for each variable x ET -I, such that all variables that occur \nin expression e belong to set I. This lemma can be proved by induction on structure of the program. The \nkey observation then is that if H2 (I2,E2)is above H1 (I1,E1)in the lattice (which is to say that H1 \nis a stronger set of equivalences than H2), then I2 >I1 . This implies that the lattice under consideration \nhas depth at most n. Thus, the abstract interpreter Ais guaranteed to reach a .xed point within a number \nof iterations that is linear in the number of vari\u00adables of the program. Given the close relationship \nbetween Aand : as established by Theorem 9 and Theorem 11, : also reaches a .xed point in the same number \nof loop iterations. Furthermore, given the relationship between : and :as mentioned in Property 8, :also \nreaches a .xed point with high probability in the same num\u00adber of loop iterations. The above observations \nsuggest that :must go around each loop for n steps (this would guarantee that .xed-point has been reached), \nwhere n is the number of variables that are de.ned inside the loop. Another alternative is to detect \nwhen .xed-point has been reached by comparing the set of Herbrand equivalences implied by :in two successive \nexecutions of a loop (this can be done by building a symbolic value .ow graph of the program [14]). If \nthese sets are identical, then the .xed-point for that loop has been reached. An upper bound on the number \nof iterations required for reaching .xed-point enables us to state an upper bound on the error proba\u00adbility \nin the analysis performed by the random interpreter :. THEOREM 14 (PROBABILISTIC SOUNDNESS THEOREM). \nLet e1 and e2 be two non-equivalent expressions of depth at most t at some program point. Let S be the \nrandom sample at that program 2 22n+t point after .xed-point. If k ;2n+t, then Pr[Se1 e2]:, ]o] where \nn is an upper bound on the number of variables, function applications, and join points in the program, \nand ,denotes the size of the .eld ,from which the random values are chosen. PROOF. Let S be the corresponding \nsymbolic sample, and U be the corresponding set of Herbrand equivalences at that point. Since the abstract \ninterpreter Ais sound, U !e1 e2. There are at most n function applications and at most n join points \nin the program. Each function application and each join operation increases the de\u00adgree of the polynomial \ncorresponding to the resulting expression by 1. Hence, one loop iteration contributes 2n to the degree \nof the polynomial corresponding to an expression. The .xed point computation requires at most n iterations. \nHence, the degrees of the polynomials V (e1,k,S )and V (e2,k,S )are bounded above by 2 2n+t. It thus \nfollows from Theorem 11 that S e1 e2. The desired result now follows from Property 8 and Theorem 1. Theorem \n14 implies that by choosing ,big enough, the error prob\u00adability can be made as small as we like. In particular, \nif n <100, and if we choose ,such that , 232 (which means that the ran\u00addom interpreter can perform arithmetic \nusing 32-bit numbers), then the error probability is bounded above by 10-5. By repeating the algorithm \nb times, the error probability can be further reduced to 10-5b . 4.5 Computational Complexity The cost \nof each assignment operation performed by the random in\u00adterpreter is O(k), assuming that each assignment \noperation involves a constant number of function applications. Let n be the number of assignments in \na program. The .xed-point for any loop is reached in at most n steps. Therefore, the total number of \nassignment oper\u00adations performed by the random interpreter is at most O(n2). Thus, the total cost of \nall assignment operations is O(n2k). The cost of a single join operation is O(mk), where m is the number \nof f as\u00adsignments at the join point. The total cost of all join operations can be amortized to O(n2 Xk)(since \neach f assignment can be associ\u00adated with an ordinary assignment). Hence, the total time taken by the \nrandom interpreter is O(n2 Xk). Choosing kO(n2),in or\u00adder to satisfy the requirement for probabilistic \nsoundness, yields an overall complexity of O(n4). Our analysis for probabilistic soundness requires choosing \nk O(n2). However, we feel that our analysis is very conservative. The experiments that we have performed \nalso suggest that tighter bounds on k might be possible, but we are not able to prove any such result \nat the moment. Note that Lemma 7 requires working with only log n polynomials, where n is the size of \nthe tree expres\u00adsions. If we can prove a similar lemma for DAGs, then we can prove that choosing kO(logn)is \nsuf.cient for probabilistic soundness, which will yield an overall complexity of O(n2 log n)for our algo\u00adrithm. \n  5 Beyond Herbrand Equivalences Until now, we have discussed how to discover equivalences in a pro\u00adgram \nin which all the operators are treated as uninterpreted. More equivalences can be discovered if some \nof these operators are inter\u00adpreted. 5.1 Linear Arithmetic The random interpretation scheme described \nin our earlier paper [7] discovers all linear relationships among variables in a program in which all \nassignments compute only linear arithmetic expressions. The random interpretation scheme described in \nthis paper discov\u00aders all Herbrand equivalences in a program in which all operators are treated as uninterpreted. \nIt is interesting to consider whether by combining both these schemes, we can discover all the equivalences \nin a program that has expressions consisting of both linear arith\u00admetic as well as uninterpreted operators \nas described below (here q denotes a rational number): e :: xF(e1,e2)qq Xee1 \u00b1e2 One way to combine both \nthese schemes is to extend the description of the random interpreter :to use the natural interpretation \nfor the linear arithmetic operators as follows: V (q,i,S)q V (q Xe,i,S)q XV (e,i,S) V (e1 \u00b1e2,i,S)V (e1,i,S)\u00b1V \n(e2,i,S) Such a naive combination of the two schemes is unsound. For ex\u00adample, consider the two non-equivalent \nexpressions e1 F(a,b)+ F(c,d)and e2 F(a,d)+F(c,b). It is easy to see that for any sample S and any i, \nV (e1,i,S)V (e2,i,S). One way to .x this problem is to hash the value of an uninterpreted function term \nbefore being used in an arithmetic expression. This loses some information about the uninterpreted term, \nbut prevents the unintended interaction between the chosen linear interpretation of the operator and \nthe plus operator. For this purpose, we maintain an extra bit of information with every variable in a \nsample, namely whether the top-level operator used in computing the value of a vari\u00adable was an uninterpreted \noperator or not. The random interpreter :now maintains a tuple (Q,S)at every point in the program, where \nS refers to a sample as before, and Q is a mapping that maps every variable to some Boolean value. The \nrandom interpreter updates Q as follows. Assignment Node: See Figure 4 (a). QQ;[x +True]if e is of the \nform F(e1,e2)  Q;[x +False], otherwise where Q;refers to the mapping before the assignment node, and \nQ refers to the mapping after the assignment node.  Conditional Node: See Figure 4 (b). Q1 Q;and Q2 \nQ; where Q;refers to the mapping before the conditional node, and Q1 and Q2 refer to the mappings after \nthe conditional node.  Join Node: See Figure 4 (c). Q(x)Q1(x)VQ2(x) where Q1 and Q2 refer to the mappings \nbefore the join node, and Q refers to the mapping after the join node.  The function V now assigns values \nto linear arithmetic expressions in the following manner. V (e1 \u00b1e2,i,S)ToArith(e1,i,S)\u00b1ToArith(e2,i,S) \nV (q Xe,i,S)q XToArith(e,i,S) where ToArith is a function that hashes uninterpreted function terms as \nfollows: ToArith(e1,i,S)ifQ(e1)thenV (e1,i,S) elseHash(V (e1,i,S)) Such a random interpretation scheme \nis probabilistically sound but not complete. For example, consider the two equivalent ex\u00adpressions e1 \n(F(a,b)+c)-c and e2 F(a,b). It is easy to see that for any sample S and any i, V (e1,i,S) V (e2,i,S)(with \nhigh probability over the random choices made by the Hash func\u00adtion) since V (e1,i,S)Hash(V (F(a,b),i,S))and \nV (e2,i,S) V (F(a,b),i,S). We can increase the precision with a simple modi.\u00adcation. We can convert an \narithmetic value back to an uninterpreted function term value if the arithmetic value is equal to the \nhash of an uninterpreted function term value. This modi.cation can discover all equivalences inside basic \nblocks, but still remains incomplete for discovering equivalences across basic blocks. It is an interest\u00ading \nquestion to .gure out if there exists an ef.cient algorithm that discovers all the equivalences in the \npresence of linear arithmetic, uninterpreted operators and non-deterministic conditionals.  5.2 Bitwise \nOperations If we attempt to use the natural interpretation for non-arithmetic operators as well, we loose \nprobabilistic soundness. This is be\u00adcause non-arithmetic expressions cannot be expressed as polyno\u00admials. \nFor example, consider the following program fragment in which x, y, and z are input variables that take \nintegral values, and &#38; denotes the bitwise and operator. t:=x&#38;y &#38;z&#38;1; assert (t = 0) \n The assertion is not always true, yet if the basic block is executed with values for x, y, and z chosen \nrandomly from ,, then the proba\u00adbility that the assert statement at the end of the program is falsi.ed \nis 1 (the probability that all x, y and z are all chosen to be odd 8 integers), meaning that most likely \nthe random interpreter will er\u00adroneously validate the assert. Note that this problem is related to the \nNP-complete problem of detecting equivalences of Boolean expres\u00adsions. One should not expect that equivalence \nof Boolean expres\u00adsions can be decided simply by random testing since the problem of deciding equivalence \nof Boolean expressions is NP-complete, and it is not known whether NP-complete problems can be decided \nin randomized polynomial time (RP). One way to conservatively handle the bitwise operators is to model \nthem as uninterpreted functions. However, this may be too con\u00adservative. Another way is to interpret \nthe bitwise logical operators using the multiplication and addition operators as described below. V(e1&#38;e2,i,S)V(e1 \n*e2,i,S) V(e1 e2,i,S)V(e1 +e2,i,S) Here denotes the bitwise or operator. This interpretation cap\u00adtures \nthe commutativity and associativity properties of the bitwise and and bitwise or operators inside basic \nblocks. It also captures the distributivity of bitwise and operator over the bitwise or op\u00aderator. However, \nthis is still far from capturing all the Boolean ax\u00adioms. For example, it does not capture the distributivity \nof bitwise or operator over the bitwise and operator. It also does not cap\u00adture the axiom that e &#38;ee. \nNote that this interpretation is sound because &#38; and satisfy all the properties that are satis.ed \nby *and +respectively. The random interpretation scheme for handling arithmetic is as ex\u00adpected: V(e1 \n*e2,i,S)V(e1,i,S)*V(e2,i,S) V(e1 +e2,i,S)V(e1,i,S)+V(e2,i,S) This is a sound scheme since the random \ninterpretation scheme is sound for testing equivalence of polynomials. The above interpretation will \nbe unsound if expressions involve both the bitwise operations and the arithmetic operations. Hence, an \nexpression consisting of bitwise and and bitwise or operator must be hashed before being used in an arithmetic \nexpression, just as uninterpreted function terms are hashed before being used in an arithmetic expression \nas described in Section 5.1. 5.3 Memory Reads and Writes As another interesting example, consider the \nfollowing program fragment in which x and y are input variables that take integer val\u00adues, and Mem refers \nto some array. Note that the assert statement at the end of the program is not always true (since the \ninput variables x and y may have the same values). Mem[x] := 0; Mem[y] := 1; t := Mem[x]; assert (t = \n0) If the basic block is executed with values for x and y chosen ran\u00addomly from ,, then the probability \nthat the assert statement at the end of the program is falsi.ed (and the random interpreter detects the \npossible failure of the assert) is 1 , which is very small. This ]o] is the probability that x and y \nare chosen equal. Again, the prob\u00adlem of detecting equivalences of expressions inside a basic block involving \nmemory reads and writes is NP-hard. Hence, one should not expect to decide equivalences of such expressions \nsimply by random testing. Memory reads and memory writes can be modeled conservatively using two special \nuninterpreted functions as follows. The mem\u00adory state is represented by a special variable \u00b5. A memory \nread is modeled by the binary uninterpreted function Select that takes as arguments the state of the \nmemory (represented by the special variable \u00b5) and the address in the memory as follows. V(Mem[e],i,S)V(Selet(\u00b5,e),i,S) \nA memory write Mem[e1]e2 is modeled by updating the value of the special memory variable \u00b5 in the sample \nbefore the assignment as follows. Si Si;[\u00b5 +V(Update(\u00b5,e1,e2),i,S)] Here Updateis an uninterpreted function \nof 3 arguments. Such a select-update formalism is commonly used to model memory and has also been used \nin [11]. Treating Select and Update operators as uninterpreted helps to reason at least about the fact \nthat two reads from the same memory location with no intervening memory writes yield same values. 5.4 \nInteger Division Operator As another example, consider the following program fragment in which x is an \ninput variable that takes integral values, and /denotes the integer division operator. t:=2 7x; assert \n(t = 0) The assert statement above is not always true. If the basic block is executed with values for \nx chosen randomly from ,, then the probability that the assert statement at the end of the program is \nfalsi.ed is at most 2 , which is very small. This is the probability ]o] that x is chosen to be either \n1 or 2. Note that this problem can be easily reduced to the problem of checking whether a polynomial \nhas integral roots or not, which is an undecidable problem. Hence, one should not expect that equivalences \nof expressions involving the integer division operator can be decided by random testing. One way to conservatively \nhandle the integer division operator is to model it as an uninterpreted function. However, this may be \ntoo conservative. Another way to handle the integer division operator is to model it as a real division \noperator as follows. e1 V (e1 7e2,i,S)V (,i,S) e2 Here a1 denotes the real division of a1 by a2, while \na1 7a2 denotes a2 the integer division of a1 by a2. This modeling, though incom\u00adplete, may help in detecting \nequivalences that are not discovered by the earlier one. This is a sound modeling because if e1 e3 , \nthen e2 e4 e1 7e2 e3 7e4. The random interpretation scheme for handling real division is as follows. \ne1 V (e1,i,S) V (,i,S) e2 V (e2,i,S) Note that this is a sound scheme because the random interpretation \nscheme is sound for testing equivalence of polynomials, and if e1 X e3 e4 e3 Xe2, then it must be the \ncase that e1 . e2 e4  6 Comparison with Related Work The algorithms for global value numbering in literature \ncan be broadly classi.ed into being optimistic or pessimistic. The optimistic algorithms start with the \noptimistic assumption that the expressions not known to be unequal are equal. Hence they may maintain \nsome incorrect facts about a program at an interme\u00addiate analysis stage [9]. These assumptions get re.ned \nin successive loop iterations, and the process is repeated until the assumptions be\u00adcome consistent. \nThe precise algorithms that discover all Herbrand equivalences in a function body fall into this optimistic \ncategory. They are based on an early algorithm by Kildall [9], which discov\u00aders equivalences using an \nabstract interpretation on the lattice of Herbrand equivalences. The running time of these algorithms \nis ex\u00adponential. Our algorithm also falls under the optimistic category. It is based on random interpretation \non the lattice of Herbrand equiv\u00adalences. It is complete and discovers all the equivalences that are \ndiscovered by the Kildall s algorithm. However, our algorithm runs in polynomial time O(n4). The pessimistic \nalgorithms start with the pessimistic assumption that the expressions not known to be equal are unequal. \nThese al\u00adgorithms maintain only true facts about a program at every inter\u00admediate stage of the analysis \nand do not require .xed-point com\u00adputation. These algorithms are based on the popular partitioning algorithm \nby Alpern, Wegman, and Zadeck [1], which runs in time O(elog n)where n and e are the number of nodes \nand edges in a pro\u00adcedure s static single assignment graph. The running time of these algorithms is better \nthan our algorithm because the pessimistic as\u00adsumption does not need to be re.ned or reapplied, while \nthe opti\u00admistic assumption must be repeatedly re.ned and reapplied until it becomes consistent. But, \nthese algorithms cannot discover all Her\u00adbrand equivalences in a procedure and hence are less precise \nthan our algorithm. The preciseness of the results of our algorithm may outweigh its execution time. \nRecently, there have been proposals for a hybrid approach which tries to combine the best of both the \nabove approaches. Karthik Gargi has proposed balanced algorithms which start with opti\u00admistic assumptions \nfor the reachability of blocks and edges and the pessimistic assumption for the congruence of values \n[6]. He has demonstrated experimentally that balanced algorithms termi\u00adnate faster than the optimistic \nalgorithms and produce more precise information than the pessimistic algorithms. Oliver R\u00a8uthing, Jens \nKnoop and Bernhard Steffen have extended the partition re.nement algorithm proposed by Alpern, Wegman \nand Zadeck [1] with the concept of integrated normalization [15], wherein the partitioned value graphs \nare modi.ed according to a set of graph rewrite rules and the process (of partitioning the value graph \nand modifying it) is repeated until .xed-point is reached. The graph rewrite rules are able to discover \nsome more equivalences which further trigger de\u00adtection of equivalences by the partitioning algorithm. \nThese hybrid algorithms discover more equivalences than the pessimistic algo\u00adrithms, but they also cannot \ndiscover all Herbrand equivalences and are less precise than our algorithm. The running time of the algo\u00adrithm \nby R\u00a8uthing, Knoop and Steffen is O(n4 log n)and is compa\u00adrable to the running time of our algorithm, \nwhich is O(n4).   The random interpretation scheme described in this paper shares the idea of using \nan af.ne-join interpretation for f functions with the algorithm described in [7]. We extend that idea \nwith a way to interpret uninterpreted operators in the language in a manner that is complete and probabilistically \nsound. Furthermore, we show how to improve the precision of the algorithm by giving more re.ned interpretations \nto a few special operators. These extensions make it possible to start experimenting with random interpretation \nfor real programs, not just those restricted to linear computations. 7 Conclusion and Future Work We \nhave presented a global value numbering algorithm based on the idea of random interpretation. Our algorithm \nis perhaps the .rst polynomial time algorithm that discovers all equivalences in a program with non-deterministic \nconditionals and uninterpreted op\u00aderators. An important feature of this algorithm is the simplicity of \nits data structures and of the operations it performs. Our algo\u00adrithm does not require any symbolic manipulations \nlike other global value numbering algorithms. We are working on proving better up\u00adper bounds on the value \nof k for probabilistic soundness; this can reduce the time complexity of our algorithm. The next step \nwill be to implement this algorithm and compare it with other existing algorithms on several benchmarks. \nWe also plan to use the value numbering algorithm proposed in this paper as part of the translation validation \ninfrastructure [11]. We hope that this will reduce several of the false alarms currently generated by \nthe translation validation tool. An interesting open problem is to discover all equivalences in a program \nwith non-deterministic conditionals and expressions that involve both uninterpreted operators and linear \narithmetic. Another interesting open problem is to consider the case when some of the uninterpreted operators \nare known to be commutative (for e.g. .oat\u00ading point arithmetic operators) or associative or both. We \nfeel that randomization has much to offer to program analysis and this area is worthy of future research. \nCombining the randomized techniques with symbolic ones also seems to be a promising direction for fu\u00adture \nwork. 8 References [1] B. Alpern, M. N. Wegman, and F. K. Zadeck. Detecting equality of variables in \nprograms. In 15th Annual ACM Symposium on Principles of Programming Languages, pages 1 11. ACM, 1988. \n[2] P. Briggs, K. D. Cooper, and L. T. Simpson. Value numbering. Soft\u00adware Practice and Experience, 27(6):701 \n724, June 1997. [3] C. Click. Global code motion/global value numbering. In Proccedings of the ACM SIGPLAN \n95 Conference on Programming Language De\u00adsign and Implementation, pages 246 257, June 1995. [4] P. Cousot \nand R. Cousot. Abstract interpretation: A uni.ed lattice model for static analysis of programs by construction \nor approxima\u00adtion of .xpoints. In Proceedings of the 4th ACM Symposium on Prin\u00adciples of Programming \nLanguages, pages 234 252, 1977. [5] R. Cytron, J. Ferrante, B. K. Rosen, M. N. Wegman, and F. K. Zadeck. \nEf.ciently computing static single assignment form and the control dependence graph. ACM Transactions \non Programming Languages and Systems, 13(4):451 490, Oct. 1990. [6] K. Gargi. A sparse algorithm for \npredicated global value numbering. In Proceedings of the ACM SIGPLAN 2002 Conference on Program\u00adming \nLanguage Design and Implementation, volume 37, 5, pages 45 56. ACM Press, June 17 19 2002. [7] S. Gulwani \nand G. C. Necula. Discovering af.ne equalities using ran\u00addom interpretation. In 30th Annual ACM Symposium \non Principles of Programming Languages, pages 74 84. ACM, Jan. 2003. [8] S. Gulwani and G. C. Necula. \nGlobal value numbering using random interpretation. Technical Report UCB//CSD-03-1296, University of \nCalifornia, Berkeley, 2003. [9] G. A. Kildall. A uni.ed approach to global program optimization. In 1st \nACM Symposium on Principles of Programming Language, pages 194 206. ACM, Oct. 1973. [10] S. S. Muchnick. \nAdvanced Compiler Design and Implementation. Morgan Kaufmann, San Francisco, 2000. [11] G. C. Necula. \nTranslation validation for an optimizing compiler. In Proceedings of the ACM SIGPLAN 00 Conference on \nProgram\u00adming Language Design and Implementation, pages 83 94. ACM SIG-PLAN, 18 21 June 2000. [12] A. \nPnueli, M. Siegel, and E. Singerman. Translation validation. In B. Steffen, editor, Tools and Algorithms \nfor Construction and Analysis of Systems, 4th International Conference, TACAS 98, volume LNCS 1384, pages \n151 166. Springer, 1998. [13] B. K. Rosen, M. N. Wegman, and F. K. Zadeck. Global value numbers and redundant \ncomputations. In 15th Annual ACM Symposium on Principles of Programming Languages, pages 12 27. ACM, \n1988. [14] O. R\u00a8uthing, J. Knoop, and B. Steffen. The value .ow graph: A pro\u00adgram representation for \noptimal program transformations. In N. D. Jones, editor, Proceedings of the European Symposium on Program\u00adming, \npages 389 405. Springer-Verlag LNCS 432, 1990. [15] O. R\u00a8uthing, J. Knoop, and B. Steffen. Detecting \nequalities of vari\u00adables: Combining ef.ciency with precision. In Static Analysis Sym\u00adposium, volume 1694 \nof Lecture Notes in Computer Science, pages 232 247. Springer, 1999. [16] J. T. Schwartz. Fast probabilistic \nalgorithms for veri.cation of poly\u00adnomial identities. JACM, 27(4):701 717, Oct. 1980. [17] B. Steffen. \nOptimal run time optimization -proved by a new look at abstract interpretations. In 2nd International \nJoint Conference on Theory and Practice of Software Development (TAPSOFT 87), vol\u00adume 249 of LNCS, pages \n52 68. Springer-Verlag, March 1987. [18] M. N. Wegman and F. K. Zadeck. Constant propagation with condi\u00adtional \nbranches. ACM Transactions on Programming Languages and Systems, 13(2):181 210, Apr. 1991. A Proof of \nCompleteness and Soundness Theorems We now give the proofs for the completeness and soundness theo\u00adrems \nstated in Section 4.3.2. Both the abstract interpreter Aand the random interpreter :perform similar operations \nfor each node in the .ow-graph. The proofs are by induction on the number of op\u00aderations performed by \nthe interpreters. The computation performed by the interpreters can be viewed as going forward in the \nsense that the outputs of a .owchart node are determined by the inputs of the node. Hence, for the inductive \ncase of the proof, we prove that the required property holds for the outputs of the node given that it \nholds for the inputs of the node. A.1 Proof of Completeness (Theorem 9) The proof is by induction on \nthe number of operations performed by the interpreters. The base case is trivial since initially U 0/. \nSince 0/ !e1 e2, it must be the case that e1 e2. Hence, S e1 e2. For the inductive case, the following \nscenarios arise. Assignment Node: See Figure 4 (a). Consider the expressions e; 1 e1[e/x]and e; 2 e2[e/x]. \nSince U !e1 e2, U;!e;e; 2. It follows from the induction 1 hypothesis on U;and S ;that S ;e;e2;. Hence, \nS e1 e2. 1 Conditional Node. See Figure 4 (b). This case is trivial since U1 U2 U;and S 1 S 2 S ;. By \nusing the induction hypothesis on S ;and U;, we get the desired result.  Join Node: See Figure 4 (c). \nBy de.nition of A, U1 !e1 e2 and U2 !e1 e2.By induction hypothesis on U1 and S 1 and on U2 and S 2,we \nhave that S 1 e1 e2 and S 2 e1 e2. It now follows from Lemma 10 that S e1 e2.  A.2 Proof of Soundness \n(Theorem 11) The proof is again by induction on the number of operations per\u00adformed by the interpreters. \nFor the base case, V (e1,k,S )P(e1,k) and V (e2,k,S )P(e2,k)since S k[x]x. Since an expression of depth \nt can have at most 2t leaves when expressed as a tree, it fol\u00adlows from Lemma 7 that e1 e2. Hence U !e1 \ne2. For the inductive case, the following scenarios arise. See Figure 4 (a). Consider the expressions \ne; 1 e1[e/x]and e; 2 e2[e/x]. Note that S ;e;e;since S e1 e2. Also 12 note that degree(V (e1,k,S ))degree(V \n(e; 1,k,S ;)) and degree(V (e2,k,S ))degree(V (e2;,k,S ;)). Hence, k ;min(degree(V (e1;,k,S ;)),degree(V \n(e2;,k,S ;)))since k ;min(degree(V (e1,k,S )),degree(V (e2,k,S ))). It follows from the induction hypothesis \non U;, S;, e; 1 and e; 2 that U;!e;e2;. Thus, it follows that U !e1 e2. 1 See Figure 4 (b). This case \nis trivial since S 1 S 2 S ;and U1 U2 U;. The induction hypothesis on S ;and U;implies the desired result. \n See Figure 4 (c). By de.nition, V (e1,k,S )w XV (e1,k,S 1)+(1 -w)X V (e1,k,S 2)and V (e2,k,S )w XV \n(e2,k,S 1)+(1 -w)X V (e2,k,S 2), where w is a variable that does not oc\u00adcur in V (e1,k,S 1), V (e1,k,S \n2), V (e2,k,S 1)or V (e2,k,S 2). Since V (e1,k,S )V (e2,k,S ), it follows that V (e1,k,S 1) V (e2,k,S \n1)(by substituting w 0). Hence, S 1 e1 e2. Also, degree(V (e1,k,S 1)):degree(V (e1,k,S )and degree(V \n(e2,k,S 1)):degree(V (e2,k,S )). Thus, it follows from the induction hypothesis on U1 ,S 1 ,e1 and e2 \nthat U1 ! e1 e2. Similarly, we can prove that U2 !e1 e2.It now follows from the de.nition of the abstract \ninterpreter Athat U !e1 e2.    \n\t\t\t", "proc_id": "964001", "abstract": "We present a polynomial time randomized algorithm for global value numbering. Our algorithm is complete when conditionals are treated as non-deterministic and all operators are treated as uninterpreted functions. We are not aware of any complete polynomial-time deterministic algorithm for the same problem. The algorithm does not require symbolic manipulations and hence is simpler to implement than the deterministic symbolic algorithms. The price for these benefits is that there is a probability that the algorithm can report a false equality. We prove that this probability can be made arbitrarily small by controlling various parameters of the algorithm.Our algorithm is based on the idea of random interpretation, which relies on executing a program on a number of random inputs and discovering relationships from the computed values. The computations are done by giving random linear interpretations to the operators in the program. Both branches of a conditional are executed. At join points, the program states are combined using a random affine combination. We discuss ways in which this algorithm can be made more precise by using more accurate interpretations for the linear arithmetic operators and other language constructs.", "authors": [{"name": "Sumit Gulwani", "author_profile_id": "81100315615", "affiliation": "University of California, Berkeley, Berkeley, CA", "person_id": "PP14115174", "email_address": "", "orcid_id": ""}, {"name": "George C. Necula", "author_profile_id": "81100295630", "affiliation": "University of California, Berkeley, Berkeley, CA", "person_id": "PP14109324", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/964001.964030", "year": "2004", "article_id": "964030", "conference": "POPL", "title": "Global value numbering using random interpretation", "url": "http://dl.acm.org/citation.cfm?id=964030"}