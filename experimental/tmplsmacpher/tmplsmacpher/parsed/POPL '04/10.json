{"article_publication_date": "01-01-2004", "fulltext": "\n Asynchronous and Deterministic Objects Denis Caromel Ludovic Henrio Bernard Paul Serpette INRIA Sophia-Antipolis \n-CNRS -I3S -Univ. Nice Sophia Antipolis, 2004 route des Lucioles B.P. 93 F-06902 Sophia-Antipolis Cedex \n{caromel, henrio, serpette}@sophia.inria.fr Abstract This paper aims at providing con.uence and determinism \nproper\u00adties in concurrent processes, more speci.cally within the paradigm of object-oriented systems. \nSuch results should allow one to pro\u00adgram parallel and distributed applications that behave in a deter\u00administic \nmanner, even if they are distributed over local or wide area networks. For that purpose, an object calculus \nis proposed. Its key characteristics are asynchronous communications with futures, and sequential execution \nwithin each process. While most of previous works exhibit con.uence properties only on speci.c programs \n or patterns of programs, a general condition for con.uence is presented here. It is further put in practice \nto show the deterministic behavior of a typical example. Categories and Subject Descriptors: D.1.3: Concurrent \nProgramming F.3.2: Semantics of Programming Languages  General Terms: Languages Keywords: Object calculus, \nconcurrency, distribution, parallelism, object-oriented languages, determinism, futures.  1 Introduction \nCon.uence properties alleviate the programmer from studying the interleaving of concurrent instructions \nand communications. Very different works have been performed to ensure con.uence of calcu\u00adlus, languages, \nor programs. Linear channels in p-calculus [30, 20], non interference properties [29] or atomic type \nsystems [8] in shared memory systems are typical examples. Starting from de\u00adterministic calculi, Process \nNetworks [17], or Jones technique in po\u00df. [16] create deterministic concurrency. But none of them con\u00adcerns \na concurrent, imperative, object language with asynchronous communications. Permission to make digital \nor hard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. POPL 04, January 14 16, 2004, Venice, Italy. \nCopyright 2004 ACM 1-58113-729-X/04/0001 ...$5.00 In this paper, we propose a calculus where interference \nbetween processes are clearly identi.able thus simplifying reasoning about concurrent object-oriented \nprograms.Our con.uence property has a much more general goal: it identi.es the sources of non\u00addeterminism \nand provides a minimal characterization of program behavior. Furthermore, some programs must behave deterministi\u00adcally: \none could not imagine an undeterministic result to a binary or a prime number search but only a few works \nensure such results. Seeking determinism for parallel programming, we propose a cal\u00adculus in which such \nproperties can be veri.ed either dynamically or by static analysis. A .rst contribution of this work \nlies in the design of an appropriate concurrent object calculus (ASP, Asynchronous Sequential Processes). \nFrom a more practical point of view, we aim at a calculus model that is effective for parallel and distributed \ncomputations, both on local and wide area networks. Asynchronous communication is at the root of the \ncalculus (for the sake of decou\u00adpling processes and network latency hiding). In ASP some objects are \nactive, active objects are accessible through global (remote) references. Communications are per\u00adformed \nthrough asynchronous method calls called requests: the calling object sends a method call to an active \nobject but does not wait for the result. Instead the request sender obtains a future repre\u00adsenting the \nresult that will be calculated. The result will be updated when it will be available. Inside each activity, \nexecution is sequen\u00adtial: only one thread performs instructions. ASP is based on a purely sequential \nand classical object calcu\u00adlus: imp.-calculus [1] extended with two parallel constructors: Active and \nServe. Active turns a standard object into an active one, executing in parallel and serving requests \nin the order speci\u00ad.ed by the Serve operator. Automatic synchronization of processes comes from a data-driven \nsynchronization mechanism called wait\u00adby-necessity [5]: a wait automatically occurs upon a strict operation \n(like a method call) on a communication result not yet available (a future). The association of systematic \nasynchronous communi\u00adcations towards processes, wait-by-necessity, and automatic deep\u00adcopy of parameters \nprovides a smooth transition from sequential to concurrent computations. An important feature of ASP \nis that fu\u00adtures can be passed between processes, both as method parameters and as method results. The \nmain contributions of this paper are: The formal de.nition of an imperative and asynchronous ob\u00adject \ncalculus with futures (ASP).  Parallel programming as a smooth extension of sequential objects, mainly \ndue to data-.ow synchronizations (wait-by\u00ad  necessity) and pervasive futures with concurrent out-of-order \nupdates. The characterization of suf.cient conditions for deterministic behavior in such a highly asynchronous \nsetting. On the practical side, it represents the formalization of an exist\u00ading library that takes into \naccount the practical constraint of asyn\u00adchrony in wide-area networks; the ASP model is implemented as \nan open-source Java library (ProActive [7]), allowing parallel and distributed programming. Section 2 \npresents the ASP calculus, it starts with a sequential part based on the imp.-calculus; then ASP calculus \nand its principles are presented; the example of a parallel binary tree illustrates the calculus. Section \n3 presents the semantics of ASP, and Section 4 its main properties including con.uence. ASP is compared \nwith other calculi in section 5.  2 Calculus 2.1 Sequential calculus ASP sequential calculus is very \nsimilar to imperative .-calculus [1], [11]. Note that a few characteristics have been changed between \nimp.-calculus and ASP sequential calculus: Because arguments passed to active objects methods will play \na particular role, we added a parameter to every method like in [23]: in addition to the self argument \nof methods (noted xj), a parameter can be sent to the method (yj in our syntax).  We do not include \nthe method update in our calculus because we do not .nd it necessary and it is possible to express updatable \nmethods in our calculus anyway. Note that method update could be included in our calculus anyway.  As \nin [11], in order to simplify the semantics locations (reference to objects in a store) can appear in \nterms.  a,b .L::=x variable, =.(xj,yj)aj]i.1..n |[li =bi;mj object, j.1..m |a.li .eld access, |a.m j(b) \nmethod call, |a.li :=b .eld update, |clone(a) super.cial copy, |. location (not in source). Note that \nlet x =ainb1 and sequence a;b2 can be easily expressed in our calculus and will be used in the following. \nLambda expres\u00adsions, and methods with zero and more than one argument are also easy to encode and will \nalso be used in this paper. Semantic structures Let locs(a)be the set of locations occurring in a and \nfv(a)the set of variables occurring free in a. The source terms (initial expressions) are closed terms \n( fv(a)=0/) without any location (locs(a)=0/). Locations appear when objects are put in the store. The \nsubstitution of b by c in a is written: a{ b .c} . .i will denote substitutions. Let = be the equality \nmodulo renaming of locations (substitution of locations by locations) provided the renaming is injective \n(alpha\u00adconversion of locations). 1let x =ainb .[m =.(z,x)b].m(a) 2a;b .[m =.(z,x)b].m(a) The store is \na mapping from locations to objects where all .elds are reduced: =.(xj,yj)aj]i.1..n s ::={. .[li =.i;mj \n} j.1..m =.(xj,yj)aj]i.1..n Let o ::=[li =.i;mj be a reduced object. j.1..m Let dom(s)be the set of locations \nde.ned by s. Let s :: s' append two stores with disjoint locations. s +s' is de.ned by (s +s')(.)= s(.) \nif . .dom(s) s'(.) otherwise Like in [11] reduction contexts are expressions with a single hole ( ) that \nspeci.es the order of reduction. For example, objects are reduced by a left to right evaluation of .eld. \nReduction contexts are de.ned in Table 1 . .dom(s) (STOREALLOC) (R [o],s).S (R [.],{. .o}:: s) s(.)=[li \n=.i;mj =.(xj,yj)aj]i.Ik .1..n j.J (FIELD) (R [..lk],s).S (R [.k],s) =.(xj,yj)aj]i.1..n s(.)=[li =.i;mj \nk .1..m j.1..m (INVOKE) (R [..mk(.')],s).S (R [ak{ xk ..,yk ..'} ],s) =.(xj,yj)aj]i.1..n s(.)=[li =.i;mj \nk .1..n j.1..m i.1..k-1,k'.k+1..n =.k'; ' =li =.i;lk =.';lk' o (UPDATE) mj =.(xj,yj)aj j.1..m (R [..lk \n:=.'],s).S (R [.],{. .o'}+s) .'.dom(s)  (CLONE) (R [clone(.)],s).S (R [.'],{.'.s(.)}:: s) R ::= |R .li \n|R .mj(b)|..mj(R )|R .li :=b|..l :=R |clone(R )| i.[1...k-1],k'.[k+1...n] li =.i;lk =R ; lk' =bk';mj \n=.(xj,yj)aj j.1..m Table 1. Sequential reduction We de.ne a small step substitution-based operational \nsemantics for ASP sequential calculus (Table 1); it is similar to the one de.ned in [11]. It de.nes new \nobject creation (STOREALLOC), .eld access (FIELD), method invocation (INVOKE), .eld update (UPDATE) and \nshallow clone (CLONE).  2.2 Parallel calculus An active object is an object that can be referenced by \ndistant point\u00aders and can handle distant asynchronous method calls (requests). Informally, an activity \nis formed by a single active object, some passive (non-active) objects, an execution thread (called process) \nand an environment. When a request is received by an activity it is stored in a request queue. Later \non, this request will be served and when the result Legend:  will be calculated, it will be stored in \na future values list. Pending requests denote requests inside a request queue, current requests are being \nserved and served requests (requests whose service is .n\u00adished) have a result value. In ASP, each activity \nhas a single process and a single active object. Processes of different activities execute instructions \nconcurrently, and interact only through requests. When activity a sends a request to activity \u00df, \u00df stores \nit in its pending request queue and a continues its execution; in a a future will represent the result \nof this request until it is calculated and returned to a (updated). ASP activities do not share memory. \nMoreover, synchronization is only due to wait\u00adby-necessity on a future. Indeed, a future reference is \nnot suf.cient to perform strict operations on an object (e.g. a .eld access or a method call). Thus, \na strict operation on a future is blocked until the value associated to this future has been updated. \nASP syntax is extended in order to introduce parallelism. The Active operator creates a new activity \nby activating object a. Serve allows to specify which requests should be served. . is used to remember \nthe continuation of the current request while we serve another one; it should not be present in source \nprograms. a,b . L::=... |Active(a,s) Creates an activity s is either a service method mj or \u00f8 for a FIFO \nservice |Serve(M) Speci.es request to serve, |a . f ,ba with continuation b (not in source) Where M is \na set of method labels used to specify which request has to be served. M = m1,...,mn  2.3 Informal semantics \nFigure 1 gives a representation of a con.guration consisting of two activities. In every activity a, \na current term aa represents the cur\u00adrent computation. Every activity has its own store sa which con\u00adtains \none active and many passive objects. An activity consists of a process, a store, several pending requests \nand calculated replies (results of requests). It is able to handle re\u00adquests coming from other activities. \nThe store contains a unique active object and passive objects. Every object belongs to only one activity \n(no shared memory). Passive objects are only referenced by objects belonging to the same activity but \npassive objects can reference active objects. The Active operator (Active(a,mj)) creates a new activity \na with the object a at his root. The object a is copied as well as all its de\u00adpendencies3 (deep copy) \nin a new activity. AO(a) acts as a proxy for the active object of activity a. All subsequent calls to \nmeth\u00adods of a via AO(a) are considered as remote request sending to the active object of activity a. \nThe second argument to the Active op\u00aderator is the name of the method4 which will be called as soon as \nthe object is activated. If no service method is speci.ed, a FIFO service will be performed. Communications \nbetween activities are due to method calls on ac\u00adtive objects and returns of corresponding results. A \nmethod call on an active object (Active(o).foo()) consists in atomically adding an entry to pending requests \nof callee, and associating a future to the response. In practice, the request sender waits for an acknowledg\u00adment \nbefore continuing its execution. In Figure 1, futures f2 and 3to prevent distant references to passive \nobjects 4with no argument f3 denote pointers to not yet computed requests while f is a future pointing \nto a value computed by a request sent to \u00df. Arguments of requests and value of futures are deeply copied3 \nwhen they are transmitted between activities. Active objects and futures are trans\u00admitted with a reference \nsemantics. The primitive Serve can appear at any point in source code. Its execution stops the activity \nuntil a request matching its arguments is found in the requests queue. The .rst matching request is then \nexecuted (served). Futures are generalized references that can be manipulated classi\u00adcally while we do \nnot perform strict operations on the object they represent. Futures can be transmitted to other activities \nand sev\u00aderal objects can reference them. But, upon a strict operation (.eld or method access, .eld update, \nclone) on a future, the execution is stopped until the value of the future has been updated (wait-by\u00adnecessity). \nWhen a request is treated, the corresponding result (fu\u00adture value) becomes available; each activity \nstores the associations between future and its computed value. The moment where the value of a future \nis returned is not speci.ed in our calculus. From a theoretical point of view, every reference to a future \ncan be replaced by a copy3 of the future value (partial or complete) at any time. In Figure 1, the pending \nrequests are merged with the future list (indeed futures correspond to previously executed requests). \n 2.4 Example Figure 2 shows an example of a simple parallel binary tree with two methods: add and search. \nEach node can be turned into an ac\u00adtive object. The calculus allows us to express lambda expressions, \nintegers and comparisons (Church integers for example), booleans and conditional expressions, methods \nzero or with many parame\u00adters, and the de.nition of classes. All these de.nitions can be easily expressed \nin ASP and most of them have been previously de.ned on .-calculus. BT .[new = .(c,z)[ empty = true,lft \n=[],rgt =[], key = 0,val =[], search = .(s,k)(c.searchsk), add = .(s,k,v)(c.add skv)], search = .(c,z).sk.if \n(s.empty) then [] else if (s.key == k) then s.val else if (s.key > k) then s.lft.search(k) else s.rgt.search(k), \nadd = .(c,z).skv.if (s.empty) then (s.rgt := Factory(s); s.lft := Factory(s);s.val := v; s.key := k;s.empty \n:= false; s) else if (s.key > k) then s.lft.add(k,v) else if (s.key < k) then s.rgt.add(k,v) else (s.val \n:= v; s)] Factory(s) .s.new in the sequential case and Factory(s) .Active(s.new,\u00f8) for the concurrent \nBT. Figure 2. Example: a binary tree add stores a new key at the appropriate place and creates two empty \nnodes. Note that in the concurrent case, nodes are activated as soon as they are created. search searches \na key in the tree and returns the value associated with it or an empty object if the key is not found. \nnew is the method invoked to create a new node. We parameterize the example by a factory able to create \na sequential (sequential binary tree) or an active (parallel binary tree) node. In the case of the parallel \nfactory, the term let tree =(BT.new).add(3,4).add(2,3).add(5,6).add(7,8)in [a = tree.search(5),b = tree.search(3)].b \n:= tree.search(7) creates a new binary tree, puts in parallel four values in it and searches two in parallel. \nThen it searches another value and modi\u00ad.es the .eld b. It always reduces to : [a = 6,b = 8]. Note that \nas soon as a request is delegated to another node, a new one can be handled. Moreover, when the root \nof the tree is the only node reachable by only one activity, the result of concurrent calls is deterministic \n(cf section 4).  3 Parallel semantics There are three distinct name spaces: activities (a,\u00df,. . Act), \nlo\u00adcations (.) and futures ( fi) (in addition to the .eld, methods and variables identi.ers which already \nappear in the source code and are not created dynamically). Note that locations and future iden\u00adti.ers \nfi are local to an activity. A future is characterized by its identi.er fi, the source activity a and \nthe destination activity \u00df of the corresponding request ( fa.\u00df). i A parallel con.guration is a set of \nactivities: P,Q ::= a[a;s;.;F;R; f ]I\u00df[...]I... characterized by : current term to be reduced : a = \nb . fi ..a ,b ' (the process). a contains several terms corresponding to the requests being treated, \nseparated by .. The left part b is the term currently eval\u00aduated, the right one f ..a ,b ' is the continuation: \nfuture and term i corresponding to a request that has been stopped before the end of its execution (because \nof a Serve primitive). Of course, b ' can also contain continuations; store s containing all objects \nof the activity a;  active object location . is the location of the active object of ac\u00adtivity a (master \nobject of the activity);  future values, a list associating, for each served request, the lo\u00adcation \nof its calculated result f : F = { f . .};  request queue R = {[mj;.; f ..a]}, a list of pending requests; \n current future f , the future associated with the term currently evaluated.  i A request can be seen \nas the rei.cation of a method call(see for example [28]). Each request r ::=[mj;.; f a.\u00df] consists of \nthe name i of the target method (mj), the location of the argument passed to the request (.) and the \nfuture identi.er which will be associated to the response to this request ( f a.\u00df). i A reference to \nthe active object of activity a is denoted by AO(a) a.\u00dfa.\u00df and a reference to future fi by fut( fi ). \nDue to distant point\u00aders, the store codomain is extended with Generalized references (i.e. futures and \nactive objects references). Reduced objects be\u00adcome: = .(xj,yj)aj]i.1..n o ::=[li = .i;mj |AO(a)| fut( \nf a.\u00df) j.1..mi ' (a,s) .S (a ,s') .S does not clone a future (LOCAL) a[a;s;.;F;R; f ] I P -. a[a ' \n;s' ;.;F;R; f ] I P . fresh activity .'. ( dom(s) s' = {.'. AO(.)} :: s )s. = copy(.'' ,s) Service = \nif (mj = \u00f8) then Fi f oService else .'' .mj() (NEWACT) a[R [Active(.'' ,mj)];s;.;F;R; f ] I P -. a[R \n[.'];s' ;.;F;R; f ] I .[Service;s.;.'' ;0/;0/;0/] I P f a.\u00df sa(.)= AO(\u00df) .'' . dom(s\u00df) new future . \nf . dom(sa) i s' ; s\u00df,.'') s' \u00df = Copy&#38;Merge(sa,.' a = {. f . fut( f a.\u00df)} :: sa i (REQUEST) a[R \n[..mj(.')];sa;.a;Fa;Ra; fa] I \u00df[a\u00df;s\u00df;.\u00df;F\u00df;R\u00df; f\u00df] I P -. a.\u00df a[R [. f ];s' a;.a;Fa;Ra; fa] I \u00df[a\u00df;s' \n\u00df;.\u00df;F\u00df;R\u00df :: [mj;.'' ; f ]; f\u00df] I P i R = R ' :: [mj;.r; f '] :: R '' mj . M .m . M, m ./R ' (SERVE) \n a[R [Serve(M)];s;.;F;R; f ] I P -. a[..mj(.r) . f ,R [[]];s;.;F;R ' :: R '' ; f '] I P .'. dom(s) F \n' = F :: { f s' = Copy&#38;Merge(s,. ; s,.') . .'} (ENDSERVICE) ' a[. . f ,a;s;.;F;R; f ] I P -. a[a;s' \n;.;F ' ;R; f '] I P sa(.)= fut( f ..\u00df) F\u00df( f ..\u00df)= . f sa ' = Copy&#38;Merge(s\u00df,. f ; sa,.) ii (REPLY) \n a[aa;sa;.a;Fa;Ra; fa] I \u00df[a\u00df;s\u00df;.\u00df;F\u00df;R\u00df; f\u00df] I P -. a[aa;s' a;.a;Fa;Ra; fa] I \u00df[a\u00df;s\u00df;.\u00df;F\u00df;R\u00df; f\u00df] \nI P Table 2. Parallel reduction (used or modi.ed values are non-gray) The function Merge merges two stores \n(it merges independently s and s' except for . which is taken from s'): Merge(.,s, s')= s'. + s where \n. = { .'. .'' | .'. dom(s') n dom(s)\\{.},.'' fresh} copy(.,s) will designate the deep copy of store \ns starting at lo\u00adcation .. That is the part of store s that contains the object s(.) and, recursively, \nall (local) objects which it references. The deep copy is the smallest store satisfying the rules of \nTable 3. The deep copy stops when a generalized reference is encountered. In that case, the new store \ncontains the generalized reference. In Table 3, the .rst two rules specify which locations should be \npresent in the created store, and the last one means that the codomain is similar in the copied and the \noriginal store (copy of the objects values). A deep copy can be calculated by marking the source object \nand recur\u00adsively all objects referenced by marked objects. When a .x-point is reached, the deep copy \nis the part of store containing marked objects. . . dom(copy(.,s)) .'. dom(copy(.,s)) . locs(s(.')) . \ndom(copy(.,s)) .'. dom(copy(.,s)) . copy(.,s)(.')= s(.') Table 3. Deep copy The following operator deeply \ncopies the part of the store s starting at the location . at the location .' of the store s', except \nfor .', the deep copy is added in a new part of the store s' : Copy&#38;Merge(s,. ; s' ,.') .Merge(.' \n,s' ,copy(.,s){ . . .'} ) Reduction contexts become : R ::= ...| Active(R ,mj)| R . f ,a The rules of \nTable 2 present the formal semantics of ASP (the con\u00adcatenation of lists will be denoted by ::): LOCAL \ninside each activity, a local reduction can occur following the rules of Table 1. Note that sequential \nrules FIELD, INVOKE, UPDATE, CLONE5 are stuck (wait-by-necessity) when the target lo\u00adcation is a generalized \nreference. Only REQUEST allows to invoke an active object method, and REPLY may transform a future reference \ninto a reachable object (ending a wait-by-necessity). NEWACT creates a new activity . containing the \ndeep copy of the object and empty current future, pending requests and future values. A generalized reference \nto this activity AO(.) is stored in the source activity a. Other references to . in a are unchanged (still \npointing to a passive object). mj speci.es the service (.rst method executed). It has no argument. If \nno service method is speci.ed, a FIFO service is performed. An in.nite loop Repeat and the FIFO service \nare de.ned below (M is the set of all method labels de.ned by the activated object): Repeat(a) .[repeat \n= .(x)a;x.repeat()].repeat() Fi f oService .Repeat(Serve(M )) REQUEST sends a new request from activity \na to activity \u00df (Fig\u00ad ure 3). A new future fi a.\u00df is created to represent the result of the request, \na reference to this future is stored in a. A request con\u00adtaining the name of the method, the location \nof a deep copy of the argument stored in s\u00df, and the associated future ([mj;.'' ; f a.\u00df])is added to \nthe pending requests of \u00df (R\u00df). SERVE serves a new request (Figure 4). The current reduction is stopped \nand stored as a continuation (future f , expression R [[]]) and the oldest (.rst received) pending request \nconcerning one of the labels speci.ed in M is treated. The activity is stuck until a matching request \nis found in the pending requests queue. 5cloning future is considered as a strict operation to ensure \nde\u00adterminism. Figure 3. REQUEST ENDSERVICE applies when the current request is .nished (currently evaluated \nterm is reduced to a location). It associates, the location of the result to the future f . The response \nis (deep) copied to prevent post-service modi.cation of the value and the new current term and current \nfuture are obtained from the continuation (Figure 5). REPLY updates a future value (Figure 6). It replaces \na reference to a future by its value. Deliberately, it is not speci.ed when this rule should be applied. \nIt is only required that an activity contains a ref\u00aderence to a future, and another one has calculated \nthe corresponding result. The only constraint about the update of future values is that strict operations \n(e.g. INVOKE) need the real object value of some of their operands. Such operations may lead to wait-by-necessity, \nwhich can only be resolved by the update of the future value. Note that a future f ..\u00df can be updated \nin an activity different from the i origin of the request (a = .) because of the capability to transmit \nfutures (e.g. as method call parameters). Note that an activity may be stuck either on a wait-by-necessity \non a future (upon a strict operation), or on the service on a set of labels with no corresponding request \nin the request queue, or if it tries to access or modify a .eld on a reference to an active object. \nInitial con.guration An initial con.guration consists of a single activity, called main ac\u00adtivity, containing \nonly a current term \u00b5[a;0/;0/;0/;0/;0/]. This activity can only communicate by sending requests or receiving \nreplies. Note that the syntax of intermediate terms guarantees that there are no shared references in \nASP except future and active object refer\u00adences.  4 Properties and con.uence This section starts with \na property about object topology inside ac\u00adtivities (4.2), then it introduces a notion of compatibility \nbetween terms (4.3) and an equivalence modulo replies (4.4). Finally, suf.\u00adcient condition for con.uence \nbetween ASP reductions (4.5) and a speci.cation of a set of terms behaving deterministically is given: \nDON terms (4.6). A static approximation allows us to de.ne a sim\u00adple deterministic sub-calculus in 4.7. \nDetailed proofs of properties presented in this section can be found in [6]. 4.1 Notations and Hypothesis \nIn the following, aP denotes the activity a of con.guration P.We suppose that the freshly allocated activities \nare chosen determinis\u00adtically: the .rst activity created by a will have the same identi.er for all executions. \nWe consider that the future identi.er fi is the name of the invoked method indexed by the number of requests \nthat have already been received by \u00df. Thus if the 4th request received by \u00df comes from . and concerns \nmethod foo, its future identi.er will be foo..\u00df . 4 * T -. will denote the transitive closure of -. , \nand -. will denote the application of rule T (e.g. LOCAL, REPLY...). 4.2 Futures and parameters isolation \nThe following theorem states that the value of each future and each request parameter are situated in \nisolated parts of the store. Figure 7 illustrates the isolation of a future value (on the left) and a \nrequest parameter (on the right). Figure 7. Store Partitioning THEOREM 1(STORE PARTITIONING). Let . \nActiveStore(a)=copy(.a,sa) copy(.,sa), ..locs(aa) At any stage of computation, each activity has the \nfollowing invari\u00adant: .. .. .. sa . ActiveStore(a) copy(.f ,sa) copy(.r,sa) { f ..f }.Fa [lj ;.r ; f \n].Ra . where is the disjoint union. This invariant is proved by checking it on each reduction rule. This \nis mainly due to the deep copies performed inside REQUEST, END-SERVICE and REPLY. The part of sa that \ndoes not belong to the preceding partition may be freely garbage collected. 4.3 Con.guration Compatibility \nThe principles are the following: two con.gurations are compatible if the served, current and pending \nrequests of one is a pre.x of the same list in the other; moreover, if two requests can not interfere, \nthat is to say if no Serve(M) can concern both requests, then this requests can be safely exchanged. \nIn ASP, the order of activities sending requests to a given one fully determines the behavior of the \nprogram (Theorem 3, RSL-con.uence). Note that this means that futures updates and imperative aspects \nof ASP do not act upon the result of evaluation. Let FL(a) denote the list of futures corresponding to \nrequests adressed to activity a: DEFINITION 1(FUTURES LIST). Let FL(a)be the list of futures that have \nbeen calculated, the current futures (the one in the activ\u00adity and all those in the continuation of the \ncurrent expression) and futures corresponding to pending requests. It is depicted by the rectangles of \nFigure 1. FL(a)={ f \u00df.a|{ f \u00df.a . .}. Fa} :: { fa} :: F (aa) ii :: { f \u00df.a|[mj,., f \u00df.a]. Ra} ii F (a \n. f ,b)= f :: F (b) whereF (a)=0/ if a =a '. f ,b DEFINITION 2(REQUEST SENDER LIST). The request sender \nlist (RSL) is the list of request senders in the order the requests have been received and indexed by \nthe invoked method. The ith element of RSLa is de.ned by: (RSLa)i =\u00df f if f \u00df.a . FL(a) i The RSL list \nis obtained from futures associated to served requests, current requests and pending requests. DEFINITION \n3 (RSL COMPARISON :). RSLs are ordered by the pre.x order on activities: ff f11 m a1 ...an fn :a'' ...a' \nm '. n = m ..i . [1..n],ai =a' i 1 DEFINITION 4 (RSL COMPATIBILITY t). Two RSLs are com\u00adpatible if one \nis pre.x of the other: RSLa tRSL\u00df . RSLa U RSL\u00df exists . RSLa :RSL\u00df . RSL\u00df :RSLa Let MaP be a static \napproximation of the set of M that can appear in the Serve(M)instructions of aP. For a given source program \nP0, for each activity a created, we consider that there is a set MaP0 such that if a will be able to \nperform a Serve(M)then M . MaP0. DEFINITION 5(POTENTIAL SERVICES). Let P0 be an initial con.guration. \nMaP0 is any set verifying: * P0 -. P . aaP =R [Serve(M)]. M . MaP0  Let RSLarepresent the restriction \nof the RSL list on the set of M labels M ((af0 :: \u00df f1 :: . f2 ) =a f0 :: . f2). f0,f2 Two con.gurations, \nderiving from the same source term, are said to be compatible if all the restriction of their RSL that \ncan be served are compatible: DEFINITION 6(CONFIGURATION COMPATIBILITY: P tQ). ** If P0 is an initial \ncon.guration such that P0 -. P and P0 -. Q  P tQ ..a . P n Q, .M . MaP0 ,RSLaPtRSLaQ MM Following the \nRSL de.nition (De.nition 2) the con.guration com\u00adpatibility only relies on the arrival order of requests; \nthe future list (FL) order (De.nition 1), potentially different on served and current requests, does \nnot matter. In the general case, Serve operations can be performed while an\u00adother request is being served; \nthen the relation between RSL order and FL order can only be determined by a precise study. If no Serve \noperation is performed while another request is being served (only the service method performs Serve \noperations), then all the restric\u00adtions (to potential services) of the RSL and the FL are in the same \norder. In the FIFO case, the FL order and the RSL order are the same. 4.4 Equivalence modulo replies \nLet = denote the equivalence modulo renaming of locations and fu\u00adtures (renaming of activities is not \nnecessary as activities are created deterministically). Furthermore = allows to exchange pending requests \nthat can not interfere. Indeed, pending request can be reordered provided the compatibility of RSLs is \nmaintained: requests that can not interfere (because they can not be served by the same Serve primitive) \ncan be safely exchanged. Modulo these allowed permutations, equivalent con.gurations are composed of \nequivalent pending requests in the same order. More formally, . is a valid permutation on the request \nqueues of P if P t.(P). Equivalence modulo future replies (P =FQ) is an extension of = authorizing the \nupdate of some calculated futures. This is equiv\u00adalent to considering references to a future already \ncalculated as equivalent to the local reference to the part of store which is the (deep copy of the) \nfuture value. Or, in other words, a future is equiv\u00adalent to a part of store if this part of store is \nequivalent to the store which is the (deep copy of the) future value (provided the updated part does \nnot overlap with the remaining of the store). Two equiva\u00adlent de.nitions of equivalence modulo future \nreplies have been for\u00admalized in [6]. Note that this equivalence is decidable. As explained informally \nhere, two con.gurations only differing by some future update sare equivalent: P REPLY -. P '. P =FP ' \nMore precisely, we have the following suf.cient condition for equivalence modulo future replies: REPLY \nP1 -. P ' . P1 =FP2 REPLY P2 -. P ' But this condition is not necessary as it does not deal with mutual \nreferences between futures. Indeed, in case of a cycle of futures one can obtain con.gurations that will \nnever converge but behave identically (Figure 8). The simplest de.nition of equivalence consists in \nfollowing paths from the root object of equivalent activities. If the same paths can be followed in both \ncon.gurations then the two con.gurations are equivalent. Of course, paths are insensitive to the following \nof cal\u00adculated future references. Let T .{ LOCAL, NEWACT, REQUEST, SERVE, ENDSERVICE, REPLY} be any parallel \nreduction. Then let us denote by =. .the reduction -. preceded by some applications of the REPLY rule. \nDEFINITION 7(REDUCTION WITH FUTURE UPDATES). T ** REPLYTREPLY =.= -. -. if T = REPLY and -. if T = REPLY \n. THEOREM 2(EQUIVALENCE AND PARALLEL REDUCTION). TT P =.Q . P =FP '..Q ' ==FQ , P ' .Q '. Q ' .. This \nimportant theorem states that if one can apply a reduction rule on a con.guration then, after several \nREPLY, a reduction using the same rule can be applied on any equivalent con.guration. Idea of the proof \nTheorem 2 is a direct consequence of the following property: PROPERTY 1. T P -. Q . P =FP '..Q ' , P \n' =.=FQ T .Q '. Q ' Indeed, if a reduction can be made on a con.guration then the same one (up to equivalence) \ncan be made on an equivalent con.guration. The proof is decomposed in two parts. First, we may need to \napply one REPLY rule to be able to perform the same rule on the two terms : if we cannot apply the same \nreduc- T tion than P -. Q (same rule on the same activities ...) on P ' ,we apply REPLY'' -. enough times \nto be able to apply the reduction on P REPLY * P '' , P '' T (P ' -. -. Q ' ). It is straightforward \nto check that if two con.gurations are equivalent, the same reduction can be applied on the two con.guration \nexcept if one of them is stuck. The second part of the proof consists in verifying that the applica\u00adtion \nof the same reduction rule on equivalent terms leads to equiv\u00adalent terms. This is done by a long case \nstudy (not detailed here). D 4.5 Con.uence Two con.gurations are said to be con.uent if they can be \nreduced to equivalent con.gurations. DEFINITION 8(CONFLUENCE: P1 YP2 ). () ** P1 YP2 ..R1,R2,P1 -. R1 \n. P2 -. R2 . R1 =FR2 The principles of con.uence property can be summarized by: the only potential source \nof non-con.uence is the interference of two REQUEST rules on the same destination activity; the order \nof up\u00addates of futures does not have any in.uence on the reduction of a term. Note that even if this \nproperty is natural, it allows a lot of asynchrony, and .exibility in futures usage and updates even \nin an imperative object calculus. The fact that, for non-FIFO service, the order of requests does not \nmatter if they cannot be involved in the same Serve primitive allows us to extend the preceding principle. \nThus, two compatible con.gurations obtained from the same term are con.uent. THEOREM 3 (RSL CONFLUENCE). \n** P -. Q1 . P -. Q2 . Q1 tQ2 =. Q1 YQ2 Idea of the proof The key idea is that if two con.gurations are \ncompatible, then there is a way to perform missing sending of requests in the right order. Thus the con.gurations \ncan be reduced to a common one (modulo future replies equivalence). Let Q be the set of con.gurations \nobtained from P and compatible with Q1 and Q2 The proof of diamond property on -. is a long case study \non con\u00ad.icts between rules. Finally, we obtain: . . . -. S ' . T1. S1 T21 .S -. S1 . T1 T2=. S1 = FS2 \n.. S1' ,S2' , S2 -. S2 ' S -. S2 ..S ' .. 1 = FS ' . 2 S,S1,S2 . Q S1' ,S2 '. Q Note that the problem \nof con.icts between REQUEST rules is solved by the introduction of RSL and compatibility between RSL. \nNote also that the case where T1 or T2 is REPLY is not necessary for the proof of Theorem 3 because in \nthat case Theorem 2 is suf.cient to conclude. Also note that the determinism of previous reductions implies \nthat the pre.x order on activities is a suf.cient condition for RSL com\u00adpatibility: the fact that received \nrequests come from the same activ\u00adities implies that these requests are the same (have the same argu\u00adment \nand method invoked). The transition from the preceding property to the diamond property on =. (Property \n2) can be summarized by the diagram in Figure 9. More details on this proof can be found in [6]. PROPERTY \n2(DIAMOND). .. T1 T2 .. . . Q ' 1 . Q ' 1 = . P1 =. . R1 .. T2 T1 P2 =. Q ' =. Q ' 1 = FQ2 ' .. R1,R2, \nQ ' =. R2 22 .. . Q ' . . 1,Q2 '. Q . R1 = FR2 .. P1 = FP2 R1,R2 . Q = == = = S' S' R1 R2 S1 S2 T2 \nT1 1 T2  Figure 9. Diamond property proof Theorem 3 is a classical consequence of Property 2. D Note \nthat, if we replaced the primitive Serve(M)by a primitive al\u00adlowing to serve a request coming from a \ngiven activity Serve(a) then ASP calculus would be deterministic. Such a calculus would be more similar \nto process networks where get are performed on a given channel and a channel only have one source process. \nFur\u00adthermore, the order of return of results still would not act upon con\u00ad.uence, and futures would provide \npowerful implicit channels for results.  4.6 Deterministic Object Networks The work of Kahn and MacQueen \non process networks [18] sug\u00adgested us the following properties ensuring the determinism of some programs. \nIn process networks, determinacy is ensured by the facts that channels have only one source, and destinations \nread data independently (values are broadcasted to all destination pro\u00adcesses). And, most importantly, \nthe reading of entry in the buffer is blocking: the order of reading on different channels is .xed for \na given program. In ASP, the semantics ensures that Serve are block\u00ading primitives. Moreover, ensuring \ncompatibility implies that two activities cannot send concurrently a request on a given method (or set \nof method labels M that appears in a Serve(M)) of the same activity. In order to formalize this principle, \nDeterministic Object Networks (DON) are de.ned below. DEFINITION 9 (DON). A con.guration P, derived from \nan initial con.guration P0,is a Deterministic Object Network (DON(P))if: *. a . Q, . M . MaP0 ,. 1\u00df \n. Q,. m . M, P -. Q . a\u00df =R [..m(...)]. s\u00df(.)=AO(a) where . 1 means there is at most one A program is \na deterministic object network if at any time, for each set of label M on which a can perform a Serve \nprimitive, only one activity can send a request on methods of M. Consequently, DON terms always reduce \nto compatible con.gurations: PROPERTY 3 (DON AND COMPATIBILITY). ** DON(P). P -. Q1 . P -. Q2 . Q1 tQ2 \nDON de.nition implies that two activities can not be able to send requests that can interfere to the \nsame third activity then DON(P) ensures RSL compatibility between terms obtained from P. Indeed, suppose \ntwo requests could intefere in the same Serve(M) inside the activity .. Then there would be a reduction \nfrom P that would lead to a term where two different activities try to send these request to .. This \nwould be contradictory with DON de.nition. Thus the set of DON terms is a deterministic sub-calculus \nof ASP: THEOREM 4 (DON DETERMINISM). ** DON(P) . P -. Q1 . P -. Q2 =. Q1 YQ2 We have shown here that \nwe can easily identify a sub-calculus (DON terms) of ASP that is deterministic. 4.7 Application: tree \ntopology In this section we propose a simple static approximation of DON terms which has the advantage \nto be valid even in the highly inter\u00adleaving case of FIFO services. Let us consider the request .ow graph, \nthat is to say the graph where nodes are activities and there is an edge between two activities if one \nactivity sends requests to another one (a .R \u00df if a sends re\u00adquests to \u00df). If, at every step of the reduction, \nthe request .ow graph is a tree then for each a, RSLa contains occurrences of at most one activity. ** \nThen for all Q and R such that P -. Q . P -. R, we have Q tR. As a consequence, we can conclude: THEOREM \n5(TREE REQUEST FLOW GRAPH). If at any time the request .ow graph forms a set of trees then the reduction \nis deterministic. This theorem proves the determinism of the binary tree of Figure 2. In general, such \na property is useful to prove deterministic behavior of any tree-like part of a program. Determinism \nTheorem 5 is easy to specify but dif.cult to ensure. For example, a program that selec\u00adtively serves \ndifferent methods coming from different activities will still behave deterministically upon out of order \nreceptions between those methods. This is a direct consequence of DON property that is not directly related \nto object topology and such a program will not verify the Theorem 5. FIFO service is, to some extent, \nthe worst case with respect to de\u00adterminism, as any out of order reception of requests will lead to non-determinism. \nAt the opposite, a request service by source ac\u00adtivity (i.e. if service primitive was of the form Serve(a)) \nwould be entirely con.uent. Even if DON de.nition allows much more .ex\u00adibility in the general case, it \nseems dif.cult to .nd a more precise property for FIFO services. 4.8 A deterministic example: The binary \nTree The Binary Tree of section 2 veri.es Theorem 5 and thus behaves deterministically provided, at each \ntime, at most one client can add new nodes. Figure 10 illustrates the evaluation of the term: let tree \n=(BT.new).add(3,4).add(2,3).add(5,6).add(7,8) in [a =tree.search(5),b =tree.search(3)].b :=tree.search(7) \nThis term behaves in a deterministic manner whatever order of replies occurs. Flow of requests Now \nconsider that the result of a preceding request is used to create a new node (dotted lines in Figure \n10): let tree =(BT.new).add(3,4).add(2,3).add(5,6).add(7,8) in let Client =[a =tree.search(5),b =tree.search(3)]in \nClient.b :=tree.search(7); tree.add(1,Client.a) Then the future update that .lls the node indexed by \n1 can oc\u00adcur at any time since we do not need the value associated to this node. Consequently the future \nupdate can occur directly from node number 5 to node number 1.  5 Related works The ASP-calculus is \nbased on the untyped imperative object calcu\u00adlus of Abadi and Cardelli (imp.-calculus of [1]). ASP local \nseman\u00adtics looks like the one of [11] but we did not .nd any concurrent object calculus [12, 15, 24] \nwith a similar way of communication between asynchronous objects (no shared memory, asynchronous communication, \nfutures, ...). Obliq [4] is a language based on the .-calculus that expresses both parallelism and mobility. \nIt is based on threads communicating with a shared memory. Like in ASP, calling a method on a remote \nobject leads to a remote execution of the method but this execution is per\u00adformed by the original thread \n(or more precisely the original thread is blocked). Moreover, for a non-serialized object, many threads \ncan manipulate the same object. Whereas in ASP, the notion of ex\u00adecuting thread is linked to the activity \nand thus every object is se\u00adrialized but a remote invocation does not stop the current thread. Finally, \nin ASP data-driven synchronization is suf.cient and no no\u00adtion of thread is necessary: we have a process \nfor each activity. \u00d8jeblik [23] is a suf.ciently expressive subset of Obliq which has a formal semantics. \nThe generalized references for all mutable ob\u00adjects, the presence of threads and the principle of serialization \n(with mutexes) make the Obliq and \u00d8jeblik languages very different from ASP. Halstead de.ned Multilisp \n[13], a language with shared memory with futures. But the combination of shared memory and side ef\u00adfects \nprevents Multilisp from being determinate. ASP can be rewritten in p-calculus [22] but this would not \nhelp us to prove con.uence property directly. Under certain restrictions [20, 30], p-calculus terms can \nbe stati\u00adcally proved to be con.uent and such results could be applicable to some ASP terms. p-calculus \nterms communicate over channels then a notion of chan\u00adnels may be introduced in ASP. Let a channel be \na pair (destination activity, set of method label) and suppose every serve primitive con\u00adcerns a single \nlabel (if several methods can be served by the same primitive, then they must belong to the same channel). \nA communi\u00adcation over a channel (a, foo)is equivalent to a remote method call on the method foo of the \nactive object of a. If at any time only one activity can send a request on a given channel then the term \nveri.es the DON property and the program behaves deterministically. In p-calculus, such programs would \nbe considered as using only linearized channels and would lead to the same conclusion. Indeed la linear \nchannel is a channel on which only one input and one out\u00adput can be performed. The unicity of destination \nis ensured by the de.nition of channels and requests, and the unicity of source pro\u00adcess is ensured by \nthe DON property. Note that in ASP, updates of response along non-linearized chan\u00adnels can be performed \nwhich makes ASP con.uence property more powerful. Moreover, this de.nition of channels is more .exible \nbe\u00adcause it can contain several method labels and then, one can wait for a request on any subset of the \nlabels belonging to a channel, in other words we can perform a Serve on a part of a channel without losing \ndeterminacy. Pierce and Turner used PICT (a language derived from p-calculus) to implement object-based \nprogramming and synchronization based on channels in [25]. But, all languages derived from p\u00adcalculus \nnecessitate explicit channel based synchronization rather than the implicit data-.ow synchronization \nproposed in the current paper which accounts very much for ASP expressivity. The join-calculus [9, 10] \nis a calculus with mobility and distribu\u00adtion. Synchronization in join-calculus is based on .ltering \npatterns over channels. The differences between channel synchronization and data-driven synchronization \nalso make the join-calculus inade\u00adquate for expressing ASP principles. Process networks [17] provide \ncon.uent parallel processes but re\u00adquire that the order of service is prede.ned and two processes can\u00adnot \nsend data on the same channel which is more restrictive and less concurrent than ASP. The ASP channel \nview introduced above can also be compared to Process Networks channels. Like in the p-calculus case, \nASP chan\u00adnels seem more .exible and our property more general especially by the fact that future updates \ncan occur at any time: return channels do not have to verify any constraint and Serve can be performed \non a part of a channel . po\u00df. [16] is a concurrent object-oriented language. A suf.cient condition is \ngiven for increasing the concurrency without losing determinacy, it is based on a program transformation. \nUnder this condition, one can return result from a method before the end of its execution. Then, the \nexecution of the method continues in parallel with the caller thread. This suf.cient condition is expressed \nby an equivalence between original and transformed program. Sangiorgi [27], and Liu and Walker [21] proved \nthe correctness of transforma\u00adtions on po\u00df. described in [16]. In po\u00df., a caller always wait for the \nmethod result: synchronous method call with anticipated result. In ASP, method calls are systematically \nasynchronous, thus more instructions can be executed in parallel: the futures mechanism al\u00adlows one to \ncontinue the execution in the calling activity without having the result of the remote call. A simple \nextension to ASP could provide a way to assign a value to a future before the end of the execution of \na method. Note that in po\u00df., this characteristic is the source of parallelism whereas in ASP this would \nsimply allow an earlier future update. Relying on the active object concept, the ASP model is rather \nclosed to, and was somehow inspired by, the notion of actors [2, 3]. Both relies on asynchronous communications, \nbut actors are rather functionals, while ASP is in an imperative and object-oriented set\u00adting. While \nactors are interacting by asynchronous message pass\u00ading, ASP is based on asynchronous method calls, which \nremain strongly typed and structured, with future-based synchronizations and without explicit continuations. \nTo some extend, ASP future semantics, with the store partitioning property (isolation between future \nvalues, the active store, and the pending requests), accounts for the capacity to achieve con.uence and \ndeterminism in an im\u00adperative setting. More generally, parallel purely functional evaluators are determin\u00adistic \nand have been widely studied [19, 14]. Finally, the importance of tree topology in concurrent computa\u00adtions \nas already been underline in [26] but this model is based on a spreadsheet framework and does not ensure \ndeterminism. 6 Conclusion In this paper, we have introduced a parallel object calculus with asynchronous \ncommunications. It is based on a sequential calculus `ala Abadi-Cardelli extended with a primitive that \ncreates a new process (activity). Communications in ASP are based on asyn\u00adchronous requests sends and \nreplies. Simple conditions allow to specify parallel and distributed applications that behave determin\u00adistically. \nA compatibility condition on the Request Sender List (RSL: or\u00addered list of activities that have sent \nrequests) characterizes con.u\u00adence between terms. Then, a set of Deterministic Object Networks (DON) \nprograms which behaves deterministically have been identi\u00ad.ed. While requests can be non-deterministically \ninterleaved in the pending queue, DON programs will always serve them in a deter\u00administic manner. Finally, \na simple application to tree-like con.gu\u00adrations exhibited a deterministic sub-calculus of ASP. An interesting \nproperty of our calculus is that the order in which the replies to requests occur has no in.uence on \ndeterminism. This provides parallelism as a process can continue its activity while still expecting the \nresult of several requests. Moreover, on a practical side, this property allows to perform future updates \nwith separate threads. Even if parts of this work can be seen as an application of p-calculus linearized \nchannels or process networks, ASP calculus provides a powerful generalization of these techniques: futures \ncreate im\u00adplicit channels providing both convenient return of results and data\u00addriven synchronizations. \nMoreover, the order of replies does not act upon programs behavior. No typing of the ASP calculus has \nbeen proposed in this paper. Of course, Abadi and Cardelli typing of objects could be adapted to ASP. \nBut a more promising perspective lies in a type system for provided and required services that could \nallow us to approximate the potential services, and most importantly to statically identify a lot of \ndeterministic programs. More generally, a larger checking of the DON condition by applying static analysis \ntechniques could be performed. Acknowledgments We thank Andrew Wendelborn and Davide Sangiorgi for comments \non earlier versions of this paper. 7 References [1] M. Abadi and L. Cardelli. A Theory of Objects. Springer-Verlag \nNew York, Inc., 1996. [2] G. Agha, I. A. Mason, S. Smith, and C. Talcott. Towards a theory of actor computation \n(extended abstract). In W. R. Cleaveland, editor, CONCUR 92: Proc. of the Third Interna\u00adtional Conference \non Concurrency Theory, pages 565 579. Springer, Berlin, Heidelberg, 1992. [3] G. Agha, I. A. Mason, S. \nF. Smith, and C. L. Talcott. A foun\u00addation for actor computation. Journal of Functional Program\u00adming, \n7(1):1 72, 1997. [4] L. Cardelli. A language with distributed scope. In Confer\u00adence Record of the 22nd \nACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages (POPL 95), pages 286 297, San Francisco, \nJanuary 22 25, 1995. ACM Press. [5] D. Caromel. Toward a method of object-oriented concurrent programming. \nCommunications of the ACM, 36(9):90 102, Sept. 1993. [6] D. Caromel, L. Henrio, and B. P. Serpette. Asynchronous \nse\u00adquential processes. Technical report, INRIA Sophia Antipolis, 2003. RR-4753. [7] D. Caromel, W. Klauser, \nand J. Vayssi`ere. Towards seamless computing and metacomputing in Java. Concurrency: Prac\u00adtice and Experience, \n10(11 13):1043 1061, 1998. Proactive available at http://www.inria.fr/oasis/proactive. [8] C. Flanagan \nand S. Qadeer. A type and effect system for atom\u00adicity. In Proceedings of the ACM SIGPLAN 2003 conference \non Programming language design and implementation, pages 338 349. ACM Press, 2003. [9] C. Fournet and \nG. Gonthier. The re.exive CHAM and the join-calculus. In Conference Record of the 23rd ACM SIGPLAN-SIGACT \nSymposium on Principles of Program\u00adming Languages (POPL 96), pages 372 385, St. Petersburg, Florida, \nJanuary 21 24, 1996. ACM Press. [10] C. Fournet, G. Gonthier, J. Levy, L. Maranget, and D. Remy. A Calculus \nof Mobile Agents. In U. Montanari and V. Sas\u00adsone, editors, Proc. 7th Int. Conf. on Concurrency Theory \n(CONCUR), volume 1119 of Lecture Notes in Computer Sci\u00adence, pages 406 421, Pisa, Italy, Aug. 1996. Springer-Verlag, \nBerlin. [11] Gordon, Hankin, and Lassen. Compilation and equivalence of imperative objects. FSTTCS: Foundations \nof Software Tech\u00adnology and Theoretical Computer Science, 17, 1997. [12] A. D. Gordon and P. D. Hankin. \nA concurrent object calcu\u00adlus: Reduction and typing. In Proceedings HLCL 98. Elsevier ENTCS, 1998. [13] \nR. H. Halstead, Jr. Multilisp: a language for concurrent sym\u00adbolic computation. ACM Transactions on Programming \nLan\u00adguages and Systems (TOPLAS), 7(4):501 538, 1985. [14] K. Hammond. Parallel Functional Programming: \nAn Intro\u00adduction (invited paper). In H. Hong, editor, First International Symposium on Parallel Symbolic \nComputation (PASCO 94), Linz, Austria, pages 181 193. World Scienti.c Publishing, 1994. [15] A. Jeffrey. \nA distributed object calculus. In ACM SIGPLAN Workshop Foundations of Object Oriented Languages, 2000. \n[16] C. B. Jones and S. Hodges. Non-interference properties of a concurrent object-based language: Proofs \nbased on an opera\u00adtional semantics. In B. Freitag, C. B. Jones, C. Lengauer, and H.-J. Schek, editors, \nObject-Orientation with Parallelism and Persistence, chapter 1, pages 1 22. Kluwer Academic Pub\u00adlishers, \n1996. ISBN 0-7923-9770-3. [17] G. Kahn. The semantics of a simple language for parallel pro\u00adgramming. \nIn J. L. Rosenfeld, editor, Information Process\u00ading 74: Proceedings of the IFIP Congress, pages 471 475. \nNorth-Holland, New York, NY, 1974. [18] G. Kahn and D. MacQueen. Coroutines and Networks of Par\u00adallel \nProcesses. In B. Gilchrist, editor, Information Process\u00ading 77: Proc. IFIP Congress, pages 993 998. North-Holland, \n1977. [19] O. Kaser, S. Pawagi, C. R. Ramakrishnan, I. V. Ramakrish\u00adnan, and R. C. Sekar. Fast parallel \nimplementation of lazy languages -the EQUALS experience. In LISP and Functional Programming, pages 335 \n344, 1992. [20] N. Kobayashi, B. C. Pierce, and D. N. Turner. Linearity and the pi-calculus. In Proceedings \nof POPL 96, pages 358 371. ACM, Jan. 1996. [21] X. Liu and D. Walker. Con.uence of processes and systems \nof objects. In P. D. Mosses, M. Nielsen, and M. I. Schwarzbach, editors, TAPSOFT 95: Theory and Practice \nof Software De\u00advelopment, 6th International Joint Conference CAAP/FASE, volume 915 of LNCS, pages 217 \n231. Springer, 1995. [22] R. Milner, J. Parrow, and D. Walker. A calculus of mobile processes, part I/II. \nJournal of Information and Computation, 100:1 77, Sept. 1992. [23] U. Nestmann, H. H\u00a8uttel, J. Kleist, \nand M. Merro. Aliasing models for mobile objects. Information and Computation, 175(1):3 33, 2002. [24] \nO. Nierstrasz. Towards an object calculus. In M. Tokoro, O. Nierstrasz, and P. Wegner, editors, Proceedings \nof the ECOOP 91 Workshop on Object-Based Concurrent Comput\u00ading, volume 612 of LNCS, pages 1 20. Springer-Verlag, \n1992.  [25] B. C. Pierce and D. N. Turner. Concurrent objects in a process calculus. In T. Ito and A. \nYonezawa, editors, Proceedings The\u00adory and Practice of Parallel Programming (TPPP 94), pages 187 215, \nSendai, Japan, 1995. Springer LNCS 907. [26] Y. ri Choi, A. Garg, S. Rai, J. Misra, and H. Vin. Orches\u00adtrating \ncomputations on the world-wide web. In B. Monien and R. Feldmann, editors, Euro-Par, volume 2400 of Lecture \nNotes in Computer Science. Springer, 2002. [27] D. Sangiorgi. The typed p-calculus at work: A proof of \nJones s parallelisation theorem on concurrent objects. The\u00adory and Practice of Object-Oriented Systems, \n5(1), 1999. An early version was included in the Informal proceedings of FOOL 4, January 1997. [28] B. \nC. Smith. Re.ection and semantics in lisp. In Confer\u00adence Record of the Eleventh Annual ACM Symposium \non Prin\u00adciples of Programming Languages, pages 23 35, Salt Lake City, Utah, January 15 18, 1984. ACM \nSIGACT-SIGPLAN, ACM Press. [29] G. L. Steele, Jr. Making asynchronous parallelism safe for the world. \nIn ACM, editor, POPL 90. Proceedings of the seven\u00adteenth annual ACM symposium on Principles of programming \nlanguages, January 17 19, 1990, San Francisco, CA, pages 218 231, New York, NY, USA, 1990. ACM Press. \n[30] M. Steffen and U. Nestmann. Typing con.uence. Interner Bericht IMMD7-xx/95, Informatik VII, Universit\u00a8at \nErlangen\u00adN\u00a8urnberg, 1995.  \n\t\t\t", "proc_id": "964001", "abstract": "This paper aims at providing <i>confluence</i> and <i>determinism</i> properties in concurrent processes, more specifically within the paradigm of <i>object-oriented</i> systems. Such results should allow one to program parallel and distributed applications that behave in a deterministic manner, even if they are distributed over local or wide area networks. For that purpose, an object calculus is proposed. Its key characteristics are <i>asynchronous</i> communications with futures, and sequential execution within each process.While most of previous works exhibit confluence properties only on specific programs -- or patterns of programs, a general condition for confluence is presented here. It is further put in practice to show the deterministic behavior of a typical example.", "authors": [{"name": "Denis Caromel", "author_profile_id": "81100007208", "affiliation": "University Nice Sophia Antipolis, Sophia-Antipolis Cedex, France", "person_id": "PP39022937", "email_address": "", "orcid_id": ""}, {"name": "Ludovic Henrio", "author_profile_id": "81100660448", "affiliation": "University Nice Sophia Antipolis, Sophia-Antipolis Cedex, France", "person_id": "PP35032944", "email_address": "", "orcid_id": ""}, {"name": "Bernard Paul Serpette", "author_profile_id": "81100037529", "affiliation": "University Nice Sophia Antipolis, Sophia-Antipolis Cedex, France", "person_id": "P394758", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/964001.964012", "year": "2004", "article_id": "964012", "conference": "POPL", "title": "Asynchronous and deterministic objects", "url": "http://dl.acm.org/citation.cfm?id=964012"}