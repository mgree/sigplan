{"article_publication_date": "01-01-2004", "fulltext": "\n Parsing Expression Grammars: A Recognition-Based Syntactic Foundation Bryan Ford Massachusetts Institute \nof Technology Cambridge, MA baford@mit.edu Abstract For decades we have been using Chomsky s generative \nsystem of grammars, particularly context-free grammars (CFGs) and regu\u00adlar expressions (REs), to express \nthe syntax of programming lan\u00adguages and protocols. The power of generative grammars to ex\u00adpress ambiguity \nis crucial to their original purpose of modelling natural languages, but this very power makes it unnecessarily \ndif.\u00adcult both to express and to parse machine-oriented languages using CFGs. Parsing Expression Grammars \n(PEGs) provide an alterna\u00adtive, recognition-based formal foundation for describing machine\u00adoriented syntax, \nwhich solves the ambiguity problem by not intro\u00adducing ambiguity in the .rst place. Where CFGs express \nnondeter\u00administic choice between alternatives, PEGs instead use prioritized choice. PEGs address frequently \nfelt expressiveness limitations of CFGs and REs, simplifying syntax de.nitions and making it un\u00adnecessary \nto separate their lexical and hierarchical components. A linear-time parser can be built for any PEG, \navoiding both the com\u00adplexity and .ckleness of LR parsers and the inef.ciency of gener\u00adalized CFG parsing. \nWhile PEGs provide a rich set of operators for constructing grammars, they are reducible to two minimal \nrecogni\u00adtion schemas developed around 1970, TS/TDPL and gTS/GTDPL, which are here proven equivalent in \neffective recognition power. Categories and Subject Descriptors F.4.2 [Mathematical Logic and Formal \nLanguages]: Gram\u00admars and Other Rewriting Systems Grammar types; D.3.1 [Programming Languages]: Formal \nDe.nitions and Theory Syntax; D.3.4 [Programming Languages]: Processors Parsing General Terms Languages, \nAlgorithms, Design, Theory  Keywords Context-free grammars, regular expressions, parsing expression \ngrammars, BNF, lexical analysis, uni.ed grammars, scannerless parsing, packrat parsing, syntactic predicates, \nTDPL, GTDPL Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL \n04, January 14 16, 2004, Venice, Italy. Copyright 2004 ACM 1-58113-729-X/04/0001 ...$5.00 1 Introduction \nMost language syntax theory and practice is based on generative systems, such as regular expressions \nand context-free grammars, in which a language is de.ned formally by a set of rules applied re\u00adcursively \nto generate strings of the language. A recognition-based system, in contrast, de.nes a language in terms \nof rules or predi\u00adcates that decide whether or not a given string is in the language. Simple languages \ncan be expressed easily in either paradigm. For example, {s . a *| s =(aa)n} is a generative de.nition \nof a trivial language over a unary character set, whose strings are constructed by concatenating pairs \nof a s. In contrast, {s . a *| (|s| mod 2 =0)}is a recognition-based de.nition of the same language, \nin which a string of a s is accepted if its length is even. While most language theory adopts the generative \nparadigm, most practical language applications in computer science involve the recognition and structural \ndecomposition, or parsing, of strings. Bridging the gap from generative de.nitions to practical recogniz\u00aders \nis the purpose of our ever-expanding library of parsing algo\u00adrithms with diverse capabilities and trade-offs \n[9]. Chomsky s generative system of grammars, from which the ubiqui\u00adtous context-free grammars (CFGs) \nand regular expressions (REs) arise, was originally designed as a formal tool for modelling and analyzing \nnatural (human) languages. Due to their elegance and expressive power, computer scientists adopted generative \ngrammars for describing machine-oriented languages as well. The ability of a CFG to express ambiguous \nsyntax is an important and powerful tool for natural languages. Unfortunately, this power gets in the \nway when we use CFGs for machine-oriented languages that are intended to be precise and unambiguous. \nAmbiguity in CFGs is dif.cult to avoid even when we want to, and it makes general CFG parsing an inherently \nsuper-linear-time problem [14, 23]. This paper develops an alternative, recognition-based formal foun\u00addation \nfor language syntax, Parsing Expression Grammars or PEGs. PEGs are stylistically similar to CFGs with \nRE-like fea\u00adtures added, much like Extended Backus-Naur Form (EBNF) no\u00adtation [30, 19]. A key difference \nis that in place of the unordered choice operator | used to indicate alternative expansions for a non\u00adterminal \nin EBNF, PEGs use a prioritized choice operator / . This operator lists alternative patterns to be tested \nin order, uncondition\u00adally using the .rst successful match. The EBNF rules A . ab |a and A . a | a b \nare equivalent in a CFG, but the PEG rules A . ab / a and A . a / a b are different. The second alternative \nin the latter PEG rule will never succeed because the .rst choice is always taken if the input string \nto be recognized begins with a . A PEG may be viewed as a formal description of a top-down parser. Two \nclosely related prior systems upon which this work is based were developed primarily for the purpose \nof studying top-down parsers [4, 5]. PEGs have far more syntactic expressiveness than the LL(k) language \nclass typically associated with top-down parsers, however, and can express all deterministic LR(k) languages \nand many others, including some non-context-free languages. Despite their considerable expressive power, \nall PEGs can be parsed in lin\u00adear time using a tabular or memoizing parser [8]. These proper\u00adties strongly \nsuggest that CFGs and PEGs de.ne incomparable lan\u00adguage classes, although a formal proof that there are \ncontext-free languages not expressible via PEGs appears surprisingly elusive. Besides developing PEGs \nas a formal system, this paper presents pragmatic examples that demonstrate their suitability for describ\u00ading \nrealistic machine-oriented languages. Since these languages are generally designed to be unambiguous \nand linearly readable in the .rst place, the recognition-oriented nature of PEGs creates a natural af.nity \nin terms of syntactic expressiveness and parsing ef.ciency. The primary contribution of this work is \nto provide language and protocol designers with a new tool for describing syntax that is both practical \nand rigorously formalized. A secondary contribution is to render this formalism more amenable to further \nanalysis by prov\u00ading its equivalence to two simpler formal systems, originally named TS ( TMG recognition \nscheme ) and gTS ( generalized TS ) by Alexander Birman [4, 5], in reference to an early syntax-directed \ncompiler-compiler. These systems were later called TDPL ( Top-Down Parsing Language ) and GTDPL ( Generalized \nTDPL ) re\u00adspectively by Aho and Ullman [3]. By extension we prove that with minor caveats TS/TDPL and \ngTS/GTDPL are equivalent in recog\u00adnition power, an unexpected result contrary to prior conjectures [5]. \nThe rest of this paper is organized as follows. Section 2 .rst de.nes PEGs informally and presents examples \nof their usefulness for de\u00adscribing practical machine-oriented languages. Section 3 then de\u00ad.nes PEGs \nformally and proves some of their important properties. Section 4 presents useful transformations on \nPEGs and proves the main result regarding the reducibility of PEGs to TDPL and GT-DPL. Section 5 outlines \nopen problems for future study, Section 6 describes related work, and Section 7 concludes. 2 Parsing \nExpression Grammars Figure 1 shows an example PEG, which precisely speci.es a prac\u00adtical syntax for PEGs \nusing the ASCII character set. The example PEG describes its own complete syntax including all lexical \nchar\u00adacteristics. Most elements of the grammar should be immediately recognizable to anyone familiar \nwith CFGs and regular expressions. The grammar consists of a set of de.nitions of the form A <-e , where \nA is a nonterminal and e is a parsing expression. The opera\u00adtors for constructing parsing expressions \nare summarized in Table 1. Single or double quotes delimit string literals, and square brackets indicate \ncharacter classes. Literals and character classes can contain C-like escape codes, and character classes \ncan include ranges such as a-z . The constant . matches any single character. The sequence expression \ne1 e2 looks for a match of e1 immedi\u00adately followed by a match of e2, backtracking to the starting point \nif either pattern fails. The choice expression e1 / e2 .rst attempts pattern e1, then attempts e2 from \nthe same starting point if e1 fails. # Hierarchical syntax Grammar <-Spacing Definition+ EndOfFile Definition \n<-Identifier LEFTARROW Expression Expression <-Sequence (SLASH Sequence)* Sequence <-Prefix* Prefix \n<-(AND / NOT)? Suffix Suffix <-Primary (QUESTION / STAR / PLUS)? Primary <-Identifier !LEFTARROW / OPEN \nExpression CLOSE / Literal / Class / DOT # Lexical syntax Identifier <-IdentStart IdentCont* Spacing \nIdentStart <-[a-zA-Z_] IdentCont <-IdentStart / [0-9] Literal <-[ ] (![ ] Char)* [ ] Spacing / [\"] \n(![\"] Char)* [\"] Spacing Class <- [ (! ] Range)* ] Spacing Range <-Char - Char / Char Char <- \\\\ [nrt \n\"\\[\\]\\\\] / \\\\ [0-2][0-7][0-7] / \\\\ [0-7][0-7]? / ! \\\\ . LEFTARROW <- <- Spacing SLASH <- / Spacing \nAND <- &#38; Spacing NOT <- ! Spacing QUESTION <- ? Spacing STAR <- * Spacing PLUS <- + Spacing OPEN \n<- ( Spacing CLOSE <- ) Spacing DOT <- . Spacing Spacing <-(Space / Comment)* Comment <- # (!EndOfLine \n.)* EndOfLine Space <- / \\t / EndOfLine EndOfLine <- \\r\\n / \\n / \\r EndOfFile <-!. Figure 1. PEG formally \ndescribing its own ASCII syntax The ?, *, and + operators behave as in common regular expression syntax, \nexcept that they are greedy rather than nondeterministic. The option expression e? unconditionally consumes \nthe text matched by e if e succeeds, and the repetition expressions e* and e+ always consume as many \nsuccessive matches of e as pos\u00adsible. The expression a* a for example can never match any string. Longest-match \nparsing is almost always the desired behav\u00adior where options or repetition occur in practical machine-oriented \nlanguages. Many forms of non-greedy behavior are still available in PEGs when desired, however, through \nthe use of predicates. The operators &#38; and ! denote syntactic predicates [20], which pro\u00advide much \nof the practical expressive power of PEGs. The ex\u00adpression &#38;e attempts to match pattern e, then unconditionally \nbacktracks to the starting point, preserving only the knowledge of whether e succeeded or failed to match. \nConversely, the expres\u00adsion !e fails if e succeeds, but succeeds if e fails. For example, the subexpression \n!EndOfLine . in the de.nition for Comment in Figure 1, matches any single character as long as the nonter\u00adOperator \nType Precedence Description primary 5 Literal string \"\" primary 5 Literal string [] primary 5 Character \nclass . primary 5 Any character (e) primary 5 Grouping e? unary suf.x 4 Optional e* unary suf.x 4 Zero-or-more \ne+ unary suf.x 4 One-or-more &#38;e unary pre.x 3 And-predicate !e unary pre.x 3 Not-predicate e1 e2 \nbinary 2 Sequence e1 / e2 binary 1 Prioritized Choice Table 1. Operators for Constructing Parsing Expressions \nminal EndOfLine does not match starting at the same position. The expression Identifier !LEFTARROW in \nthe de.nition for Primary, in contrast, matches any Identifier that is not followed by a LEFTARROW. This \nlatter predicate prevents the right-hand-side Expression at the beginning of one Definition from consuming \nthe left-hand-side Identifier of the next Definition, eliminat\u00ading the need for an explicit delimiter. \nPredicates can involve arbi\u00adtrary parsing expressions requiring any amount of lookahead. 2.1 Uni.ed Language \nDe.nitions Most conventional syntax descriptions are split into two parts: a CFG to specify the hierarchical \nportion, and a set of regular ex\u00adpressions de.ning the lexical elements to serve as terminals for the \nCFG. CFGs are unsuitable for lexical syntax because they can\u00adnot directly express many common idioms, \nsuch as the greedy rule that usually applies to identi.ers and numbers, or negative syn\u00adtax such as the \nLiteral rule above, in which quoted string literals may contain any character except the quote character. \nRegular ex\u00adpressions cannot describe recursive syntax, however, such as large expressions constructed \ninductively from smaller expressions. Neither of these dif.culties exist with PEGs, as demonstrated by \nthe uni.ed example grammar. The greedy nature of the repetition operator ensures that a sequence of letters \ncan only be interpreted as a single Identifier and not as two or more immediately adjacent, shorter ones. \nNot-predicates describe the appropriate negative con\u00adstraints on the elements that can appear in literals, \ncharacter classes, and comments. The last ! \\\\ . alternative in the de.nition of Char ensures that the \nbackslash cannot be used in a literal or char\u00adacter class except as part of an escape sequence. Each \nde.nition in the example grammar that represents a distinct lexical token, such as Identifier, Literal,or \nLEFTARROW, uses the Spacing nonterminal to consume any whitespace and/or comments immediately following \nthe token. The de.nition of Grammar also starts with Spacing in order to allow whitespace at the beginning \nof the .le. Associating whitespace with each imme\u00addiately preceding token is a convenient convention \nfor PEGs, but whitespace could just as easily be associated with the following to\u00adken by referring to \nSpacing at the beginning of each token de.ni\u00adtion. Whitespace could even be treated as a separate kind \nof token, consistent with lexical traditions, but doing so in a uni.ed grammar such as this one would \nrequire many explicit references to Spacing throughout the hierarchical portion of the syntax. 2.2 New \nSyntax Design Choices Besides being able to express many existing machine-oriented lan\u00adguages in a concise \nand uni.ed grammar, PEGs also create new possibilities for language syntax design. Consider for example \na well-known problem with C++ syntax involving nested template type expressions: vector<vector<float> \n> MyMatrix; The space between the two right angle brackets is required because the C++ scanner is oblivious \nto the language s hierarchical syntax, and would otherwise interpret the >> incorrectly as a right shift \noperator. In a language described by a uni.ed PEG, however, it is easy to de.ne the language to permit \na >> sequence to be inter\u00adpreted as either one token or two depending on its context: TemplType <-PrimType \n(LANGLE TemplType RANGLE)? ShiftExpr <-PrimExpr (ShiftOper PrimExpr)* ShiftOper <-LSHIFT / RSHIFT LANGLE \n<- < Spacing RANGLE <- > Spacing LSHIFT <- << Spacing RSHIFT <- >> Spacing Such permissiveness can create \nunexpected syntactic subtleties, of course, and caution and good taste are in order: a powerful syntax \ndescription paradigm also means more rope for the careless lan\u00adguage designer to hang himself with. The \ntraditional behavior for operator tokens is still easily expressible if desired, as follows: LANGLE <-!LSHIFT \n< Spacing RANGLE <-!RSHIFT > Spacing LSHIFT <- << Spacing RSHIFT <- >> Spacing Freeing lexical syntax \nfrom the restrictions of regular expressions also enables tokens to have hierarchical characteristics, \nor even to refer back to the hierarchical portion of the language. Pascal-like nestable comments, for \nexample, cannot be described by a regular expression but are easily expressed in a PEG: Comment <- (* \n(Comment / ! *) .)* *) Character and string literals in most programming languages per\u00admit escape sequences \nof some kind, to express either special char\u00adacters or dynamic string substitutions. These escapes usually \nhave a highly restrictive syntax, however. A language described by a uni.ed PEG could permit the use \nof arbitrary expressions in such escapes, taking advantage of the full power of the language s ex\u00adpression \nsyntax: Expression <-... Primary <-Literal / ... Literal <-[\"] (![\"] Char)* [\"] Char <- \\\\( Expression \n) / ! \\\\ . In place of the Java string literal \"\\u2200\" containing the Uni\u00adcode math symbol . , for \nexample, the literal could be writ\u00adten \"\\(0x2200)\", \"\\(8704)\",or even \"\\(Unicode.FOR_ALL)\", where FOR_ALL \nis a constant de.ned in a class named Unicode. 2.3 Priorities, Not Ambiguities The speci.cation .exibility \nprovided by PEGs, and the new syntax design choices they create, are not limited to the lexical portions \nof a language. Many sensible syntactic constructs are inherently ambiguous when expressed in a CFG, commonly \nleading language designers to abandon syntactic formality and rely on informal meta\u00adrules to solve these \nproblems. The ubiquitous dangling ELSE problem is a classic example, traditionally requiring either an \nin\u00adformal meta-rule or severe expansion and obfuscation of the CFG. The correct behavior is easily expressed \nwith the prioritized choice operator in a PEG: Statement <-IF Cond THEN Statement ELSE Statement / IF \nCond THEN Statement / ... The syntax of C++ contains ambiguities that cannot be resolved with any amount \nof CFG rewriting, in which certain token se\u00adquences can be interpreted as either a statement or a de.nition. \nThe language speci.cation [25] resolves this problem with the informal meta-rule that such a sequence \nis always interpreted as a de.nition if possible. Similarly, the syntax of lambda abstractions, let ex\u00adpressions, \nand conditionals in Haskell [11] is unresolvably ambigu\u00adous in the CFG paradigm, and is handled in the \nHaskell speci.ca\u00adtion with an informal longest match meta-rule. PEGs provide the necessary tools prioritized \nchoice, greedy repetition, and syntac\u00adtic predicates to de.ne precisely how to resolve such ambiguities. \nThese tools do not make language syntax design easy, of course. In place of having to determine whether \ntwo possible alternatives in a CFG are ambiguous, PEGs present language designers with the analogous \nchallenge of determining whether two alternatives in a / expression can be reordered without affecting \nthe language. This question is often obvious, but sometimes is not, and is undecid\u00adable in general. As \nwith discovering ambiguity in CFGs, however, we have the hope of .nding automatic algorithms to identify \norder sensitivity or insensitivity conservatively in common situations. 2.4 Quirks and Limitations If \nthe de.nition of Grammar in Figure 1 did not reference EndOfFile at the end, then any ASCII .le starting \nwith at least one correct Definition would be interpreted as a correct gram\u00admar, even if the .le has \nunreadable garbage at the end. This pe\u00adculiarity arises from the fact that a parsing expression in a \nPEG can succeed without consuming all input text. We address this minor issue with the EndOfFile nonterminal, \nde.ned by the pred\u00adicate expression !. , which matches the end-of-.le by failing if any character is \navailable and succeeding otherwise. Both left and right recursion are permissible in CFGs, but as with \ntop-down parsing in general, left recursion is unavailable in PEGs because it represents a degenerate \nloop. For example, the CFG rules A . aA | a and A . Aa | a represent a series of a s in a CFG, but the \nPEG rule A . Aa / a is degenerate because it indicates that in order to recognize nonterminal A, a parser \nmust .rst recognize nonterminal A... This restriction applies not only to direct left recursion as in \nthis example, but also to indirect or mutual left recursion involving several nonterminals. Since both \nleft and right recursion in a CFG merely represent repetition, however, and repetition is easier to express \nin a PEG using repetition operators, this limitation is not a serious problem in practice. Like a CFG, \na PEG is a purely syntactic formalism, not by itself capable of expressing languages whose syntax depends \non semantic predicates [20]. Although the Java language can be described as a single uni.ed PEG [7], \nC and C++ parsers require an incrementally constructed symbol table to distinguish between ordinary identi.ers \nand typedef-de.ned type identi.ers. Haskell uses a special stage in the syntactic pipeline, inserted \nbetween the scanner and parser, to implement the language s layout-sensitive features.  3 Formal Development \nof PEGs In this section we de.ne PEGs formally and explore key properties. Many of these properties and \ntheir proofs were inspired by those of the closely related TS/TDPL and gTS/GTDPL systems [4, 5, 3], although \nthe formulation of PEGs is substantially different. 3.1 De.nition of a PEG In Figure 1 we used a concrete \nASCII-based syntax for PEGs to illustrate the characteristics of PEGs for practical language descrip\u00adtion \npurposes. For formal analysis, however, it is more convenient to use an abstract syntax for PEGs that \nrepresents only its essential structure. We begin therefore by de.ning this abstract syntax. De.nition: \nA parsing expression grammar (PEG) is a 4-tuple G = (VN ,VT ,R,eS), where VN is a .nite set of nonterminal \nsymbols, VT is a .nite set of terminal symbols, R is a .nite set of rules, eS is a parsing expression \ntermed the start expression, and VN n VT = 0/. Each rule r . R is a pair (A,e), which we write A . e, \nwhere A . VN and e is a parsing expression. For any nonterminal A, there is exactly one e such that A \n. e . R. R is therefore a function from nonterminals to expressions, and we write R(A) to denote the \nunique expression e such that A . e . R. We de.ne parsing expressions inductively as follows. If e, e1, \nand e2 are parsing expressions, then so is: 1. e, the empty string 2. a, any terminal, where a . VT \n. 3. A, any nonterminal, where A . VN . 4. e1e2, a sequence. 5. e1/e2, prioritized choice. 6. e *, \nzero-or-more repetitions. 7. !e, a not-predicate.  All subsequent use of the unquali.ed term grammar \nrefers specif\u00adically to parsing expression grammars as de.ned here, and the un\u00adquali.ed term expression \nrefers to parsing expressions. We use the variables a,b,c,d to represent terminals, A,B,C,D for nonter\u00adminals, \nx,y,z for strings of terminals, and e for parsing expressions. The structural requirement that R be a \nfunction, mapping each non\u00adterminal in VN to a unique parsing expression, precludes the pos\u00adsibility \nof expressions in the grammar containing unde.ned refer\u00adences, or subroutine failures [5]. The expression \nset E(G) of G is the set containing the start expres\u00adsion eS, the expressions used in all grammar rules, \nand all subex\u00adpressions of those expressions. A repetition-free grammar is a grammar whose expression \nset con\u00adtains only expressions constructed without using rule 6 above. A predicate-free grammar is one \nwhose expression set contains only expressions constructed without using rule 7. 3.2 Desugaring the \nConcrete Syntax The abstract syntax does not include character classes, the any character constant . \n, the option operator ? , the one-or-more\u00adrepetitions operator + , or the and-predicate operator &#38; \n, all of which appear in the concrete syntax. We treat these features of the concrete syntax as syntactic \nsugar, reducing them to abstract parsing expressions using local substitutions as follows: We consider \nthe . expression in the concrete syntax to be a character class containing all of the terminals in VT \n.  If a1,a2,...,an are all of the terminals listed in a character class expression in the concrete syntax, \nafter expanding any ranges, then we desugar this character class expression to the abstract syntax expression \na1/a2/.../an.  We desugar an option expression e? in the concrete syntax to ed /e, where ed is the desugaring \nof e.  * We desugar a one-or-more-repetitions expression e+ to eded , where ed is the desugaring of \ne.  We desugar an and-predicate &#38;e to !(!ed ), where ed is the desugaring of e.   3.3 Interpretation \nof a Grammar De.nition: To formalize the syntactic meaning of a grammar G =(VN ,VT ,R,eS), we de.ne a \nrelation .G from pairs of the form (e,x) to pairs of the form (n,o), where e is a parsing expression, \nx . VT * is an input string to be recognized, n = 0 serves as a step counter, and o . VT *.{ f } indicates \nthe result of a recognition at\u00adtempt. The output o of a successful match is the portion of the in\u00adput \nstring recognized and consumed, while a distinguished sym\u00adbol f . VT indicates failure. For ((e,x),(n,o)) \n..G we will write (e,x) . (n,o), with the reference to G being implied. We de.ne .G inductively as follows: \n1. Empty: (e,x) . (1,e) for any x . VT * . 2. Terminal (success case): (a,ax) . (1,a) if a . VT , x \n. VT * . 3. Terminal (failure case): (a,bx) . (1, f ) if b, and  a= (a,e) . (1, f ). 4. Nonterminal: \n(A,x) . (n + 1,o) if A . e . R and (e,x) . (n,o). 5. Sequence (success case): If (e1,x1x2y) . (n1,x1) \nand (e2,x2y) . (n2,x2), then (e1e2,x1x2y) . (n1 + n2 + 1,x1x2). Expressions e1 and e2 are matched in \nsequence, and if each succeeds and consumes input portions x1 and x2 respectively, then the sequence \nsucceeds and consumes the string x1x2. 6. Sequence (failure case 1): If (e1,x) . (n1, f ), then (e1e2,x) \n. (n1 + 1, f ).If e1 is tested and fails, then the se\u00adquence e1e2 fails without attempting e2, 7. Sequence \n(failure case 2): If (e1,x1y) . (n1,x1) and (e2,y) . (n2, f ), then (e1e2,x1y) . (n1 + n2 + 1, f ).If \ne1 succeeds but e2 fails, then the sequence expression fails.  8. Alternation (case 1): If (e1,xy) . \n(n1,x), then (e1/e2,xy) . (n1 + 1,x). Alternative e1 is .rst tested, and if it succeeds, the expression \ne1/e2 succeeds without testing e2. 9. Alternation (case 2): If (e1,x) . (n1, f ) and (e2,x) . (n2,o), \nthen (e1/e2,x) . (n1 + n2 + 1,o).If e1 fails, then e2 is tested and its result is used instead. 10. \nZero-or-more repetitions (repetition case): If (e,x1x2y) .  * * (n1,x1) and (e ,x2y) . (n2,x2), then \n(e ,x1x2y) . (n1 + n2 + 1,x1x2). 11. Zero-or-more repetitions (termination case): If (e,x) . * (n1, f \n), then (e ,x) . (n1 + 1,e). 12. Not-predicate (case 1): If (e,xy) . (n,x), then (!e,xy) . (n + 1, f \n). If expression e succeeds consuming input x, then the syntactic predicate !e fails. 13. Not-predicate \n(case 2): If (e,x) . (n, f ), then (!e,x) . (n + 1,e).If e fails, then !e succeeds but consumes nothing. \n We de.ne a relation .+ G from pairs (e,x) to outcomes o, such that (e,x) .+ o iff an n exists such \nthat (e,x) . (n,o). If (e,x) .+ y for y . VT *, we say that e matches x in G.If (e,x) .+ f , we say that \ne fails on x in G. The match set MG(e) of expression e in G is the set of inputs x such that e matches \nx in G. An expression e handles a string x . VT * if it either matches or fails on x in G. A grammar \nG handles string x if its start expression eS handles x. G is complete if it handles all strings x . \nVT * . Two expressions e1 and e2 are equivalent, written e1 ; e2,if (e1,x) .+ o implies (e2,x) .+ o and \nvice versa. The resulting step counts need not be the same. Theorem: If (e,x) . (n,y), then y is a pre.x \nof x: .z(x = yz). Proof: By induction on an integer variable m = 0, using as the induction hypothesis \nthe proposition that the desired property holds for all e,x,n = m, and y. Theorem: If (e,x) . (n1,o1) \nand (e,x) . (n2,o2), then n1 = n2 and o1 = o2. That is, the relation .G is a function. Proof: By induction \non a variable m = 0, using the induction hy\u00adpothesis that the proposition holds for all e,x,n1 = m,n2 \n= m,o1, and o2. This induction technique will subsequently be referred to simply as induction on step \ncounts of .G. * Theorem: A repetition expression e does not handle any input string x on which e succeeds \nwithout consuming input: for any x . * VT *,if (e,x) . (n1,e), then (e ,x). (n2,o2) for any n2,o2. We \ncall this the *-loop condition. Proof: By induction on step counts. 3.4 Language Properties This section \ndescribes properties of parsing expression languages (PELs), the class of languages that can be expressed \nby PEGs. PELs are closed under union, intersection, and complement. It is unde\u00adcidable in general whether \na PEG represents a nonempty language, or whether two PEGs represent the same language. De.nition: The \nlanguage L(G)of aPEG G =(VN ,VT ,R,eS)is the set of strings x .VT * for which the start expression eS \nmatches x. Note that the start expression eS only needs to succeed on input string x for x to be included \nin L(G); eS need not consume all of string x. For example, the trivial grammar ({},VT ,{},e)recognizes \nthe language VT * and not just the empty string, because the start expression e always succeeds even \nthough it does not examine or consume any input. This de.nition contrasts with TS and gTS, in which partially \nconsumed input strings are excluded from the lan\u00adguage and classi.ed as partial-acceptance failures [5]. \nDe.nition: A language L over an alphabet VT is a parsing expres\u00adsion language (PEL) iff there exists \na parsing expression grammar G whose language is L. Theorem: The class of parsing expression languages \nis closed un\u00adder union, intersection, and complement. 1 Proof: Suppose we have two grammars G1 =(VN 1 \n,VT ,R1 ,eS)and 2 G2 =(VN 2 ,VT ,R2 ,eS)respectively, describing languages L(G1)and L(G2). Assume without \nloss of generality, that VN 1 nV 2 =0/,by N renaming nonterminals if necessary. We can form a new grammar \n'' G' =(VN 1 .VN 2 ,VT ,R1 .R2 ,eS), where eS is one of the following: '12 If e=eS / eS, then L(G')=L(G1).L(G2). \nS '12 If e=&#38;eS, then L(G')=L(G1)nL(G2). S Se '1 If e=!eS, then L(G')=VT *-L(G1). S Theorem: The class \nof PELs includes non-context-free languages. Proof: The classic example language anbncn is not context-free, \nbut we can recognize it with a PEG G =({A,B,D},{a,b,c},R,D), where R contains the following de.nitions: \nA . aAb / e B . bBc / e D . &#38;(A !b)a * B !. Theorem: It is undecidable in general whether the language \nL(G) of an arbitrary parsing expression grammar G is empty. Proof: We .rst prove in the same way as for \nCFGs [3] that it is undecidable whether the intersection of the languages of two PEGs is empty. Since \nPELs are closed under intersection, an algorithm to test the emptiness of the language L(G)of any G could \nbe used to test whether L(G1)nL(G2)is empty, implying that emptiness is undecidable as well. Given an \ninstance C =(x1,y1),...,(xn,yn) of Post s correspon\u00addence problem over an alphabet S, it is known to \nbe undecidable whether there is a non-empty string w that can be built from el\u00adements of C such that \nw =xi1 xi2 ...xim =yi1 yi2 ...yim , where 1 = ij =n for each 1 = j =m. We build a grammar G =(VN ,VT \n,R,D)where VN ={A,B,D}, and VT =S .{a1,...,an}. The ai in VT are distinct terminals not in S, which will \nserve as markers associated with the elements of C. R contains the following three rules: A .x1Aa1/x2Aa2/.../xnAan/e \n B .y1Ba1/y2Ba2/.../ynBan/e  D .&#38;. &#38;(A !.)B !. Nonterminal A matches strings of the form xi1 \nxi2 ...ximaim ...ai2 ai1, while B matches strings of the form yi1 yi2 ...yimaim ...ai2 ai1. The nonterminal \nD uses the and-predicate operator to match only strings matching both A and B, representing solutions \nto the correspon\u00addence problem. The &#38;. at the beginning of the de.nition of D (desugared appropriately) \nensures that empty solutions are not al\u00adlowed, and the !. after the references to A and B ensure that \nthe complete input is consumed in each case. An algorithm to decide whether L(G)is nonempty could therefore \nbe used to solve the cor\u00adrespondence problem C, yielding the desired result. De.nition: Two PEGs G1 and \nG2 are equivalent if they recognize the same language: L(G1)=L(G2). Theorem: The equivalence of two arbitrary \nPEGs is undecidable. Proof: An algorithm to decide the equivalence of two PEGs could also be used to \ndecide the non-emptiness problem above, simply by comparing the grammar to be tested against a trivial \ngrammar for the empty language. 3.5 Analysis of Grammars We often would like to analyze the behavior \nof a particular gram\u00admar over arbitrary input strings. While many interesting properties of PEGs are \nundecidable in general, conservative analysis proves useful and adequate for many practical purposes. \nTheorem: It is undecidable whether an arbitrary grammar is com\u00adplete: that is, whether it either succeeds \nor fails on all input strings. Proof: Suppose we have an arbitrary grammar G =(VN ,VT ,R,eS), ' and we \nde.ne a new grammar G' =(V'N ,VT ,R',eS), where V'= N VN .{A}, A .VN , R' =R .{A .&#38;eS A}, and e'=A.If \nG s start S expression eS succeeds on any input string x, then this input will cause a degenerate loop \nin G' via nonterminal A,so G' is incom\u00adplete. If L(G)is empty, however, then G' is complete and also \nfails on all inputs. An algorithm to decide whether G' is complete would therefore allow us to decide \nwhether G is empty, which has already been shown undecidable. De.nition: We de.ne a relation -G consisting \nof pairs of the form (e,o), where e is an expression and o .{0,1, f }. We will write e -o for (e,o).-G, \nwith the reference to G being implied. This relation represents an abstract simulation of the .G relation. \nIf e -0, then e might succeed on some input string while consuming no input. If e -1, then e might succeed \nwhile consuming at least one terminal. If e -f , then e might fail on some input. We will use the variable \ns to represent a -G outcome of either 0 or 1. We de.ne the simulation relation -G inductively as follows: \n1. e -0. 2. a -1. 3. a -f . 4. A -o if RG(A)-o. 5. e1e2 -0if e1 -0 and e2 -0. e1e2 -1if e1 -1 and \ne2 -s. e1e2 -1if e1 -s and e2 -1.  6. e1e2 -f if e1 -f . 7. e1e2 -f if e1 -s and e2 -f .  8. e1/e2 \n-s if e1 -s. 9. e1/e2 -o if e1 -f and e2 -o.  * 10. e -1if e -1, * 11. e -0if e -f . 12. !e -f if \ne -s. 13. !e -0if e -f .  Because this relation does not depend on the input string, and there are \na .nite number of relevant expressions in a grammar, we can compute this relation over any grammar by \napplying the above rules iteratively until we reach a .xed point. Theorem: The relation -G summarizes \n.G as follows: If (e,x) .G (n,e), then e -0.  If (e,x) .G (n,y) and |y| > 0, then e -1.  If (e,x) \n.G (n, f ), then e -f .  Proof: By induction over the step counts of the relation .G. The de.nition \nrules for -G above correspond one-to-one to the rules for .G. The conclusion in each case follows immediately \nfrom the inductive hypothesis, except in the cases for the repetition operator, which require the *-loop \ncondition theorem from Section 3.3. 3.6 Well-Formed Grammars A well-formed grammar is a grammar that \ncontains no directly or mutually left-recursive rules, such as A . Aa / a , which could prevent the grammar \nfrom handling any input string. This check\u00adable structural property implies completeness, while being \npermis\u00adsive enough for most purposes. A grammar can have left-recursive rules but still be complete if \nits degenerate loops are actually un\u00adreachable, but we have little need for such grammars in practice. \n De.nition: We de.ne an inductive set WFG as follows. We write WF(e) for e .WFG, with the reference to \nG being implied, to mean that expression e is well-formed in G. 1. WF(e). 2. WF(a). 3. WF(A) if WF(RG(A)). \n 4. WF(e1e2) if WF(e1) and e1 -0 implies WF(e2). 5. WF(e1/e2) if WF(e1) and WF(e2). 6. WF(e *) if WF(e) \nand e -0. 7. WF(!e) if WF(e).  A grammar G is well-formed if all of the expressions in its expres\u00adsion \nset E(G) are well-formed in G. As with the -G relation, the WFG set can be computed by iteration to a \n.xed point. Lemma: Assume that grammar G is well-formed, and that all ex\u00adpressions in E(G) handle all \nstrings x . VT * of length n or less. Then the expressions in E(G) also handle all strings of length \nn + 1. Proof: By induction over the step counts of .G. The interesting cases are as follows: For a nonterminal \nA, the induction hypothesis allows us to as\u00adsume that RG(A) handles all strings of length n + 1; therefore \nso does A by the de.nition of .G.  For a sequence e1e2, we can assume that e1 handles all strings x \nof length n + 1. If (e1,x) . (n, e), then e1 -0, so WF(e2) applies and e2 also handles x.If (e1,x) . \n(n, y) for |y| > 0, then e2 only needs to handle strings of length n or less, which is given. If (e1,x) \n. (n, f ), then e2 is not used.  For e *, the WF(e) condition ensures that e1 handles inputs of length \nn+1, and the e -0 condition ensures that the recursive  * dependency on e in the success case only needs \nto handle strings of length n or less. Theorem: A well-formed grammar G is complete. Proof: By induction \nover the length of input strings, each expres\u00adsion in E(G) handles every input string. Since G s start \nexpression eS is in E(G), the conclusion follows. 3.7 Grammar Identities A number of important identities \nallow PEGs to be transformed without changing the language they represent. We will use these identities \nin subsequent results. Theorem: The sequence and alternation operators are asso\u00adciative under expression \nequivalence: e1(e2e3) ; (e1e2)e3 and e1/(e2/e3) ; (e1/e2)/e3. Proof: Trivial, from the de.nition of .G. \nTheorem: Sequence operators can be distributed into choice oper\u00adators on the left but not on the right: \ne1(e2/e3) ; (e1e2)/(e1e3), but (e1/e2)e3 ; (e1e3)/(e2e3). Proof: In the left-side case, the expression \n(e1e2)/(e1e3) invokes e1 twice from the same starting point on the same input string making its result \nthe same as the factored e1(e2/e3) expression. In the right-side case, however, suppose that e1 succeeds \nbut e3 fails. In the expression (e1/e2)e3, the failure of e3 causes the whole expression to fail. In \n(e1e3)/(e2e3), however, the .rst instance of e3 only causes the .rst alternative to fail; the second \nalternative will then be tried, in which the e3 might succeed if e2 consumes a different amount of input \nthan e1 did. Theorem: Predicates can be moved left within sequences distribu\u00adtively as follows: e1!e2 \n; !(e1e2) e1. Proof: If e1 succeeds, then e2 is tested starting at the same point in each case, resulting \nin the same overall behavior; the second case merely invokes e1 twice at the same position. If e1 fails, \nthen the predicate in e1!e2 is not tested at all. The predicate in !(e1e2)e1 is tested, and succeeds \nbecause of the .rst e1 s failure, but the overall result is still failure due to the second instance \nof e1. De.nition: Two expressions e1 and e2 are disjoint if they succeed on disjoint sets of input strings: \nMG(e1) n MG(e2)= 0/. Theorem: A choice expression e1/e2 is commutative if its subex\u00adpressions are disjoint. \nProof: If either e1 or e2 fails on a string x, it does not matter which is tested .rst. The only way \nthe language can be affected by chang\u00ading their order is if e1 and e2 both succeed on x and consume dif\u00adferent \namounts of input. Disjointness precludes this possibility. Although the results from Section 3.4 imply \nthat disjointness is un\u00addecidable in general, it is easy to force a choice expression to be disjoint \nvia the following simple transformation: Theorem: e1/e2 ;e1/!e1 e2 ;!e1 e2/e1, and the latter two equiv\u00adalent \nchoice expressions are disjoint. Proof: Trivial, by case analysis. 4 Reductions on PEGs In this section \nwe present methods of reducing PEGs to simpler forms that may be more useful for implementation or easier \nto rea\u00adson about formally. First we describe how to eliminate repetition and predicate operators, then \nwe show how PEGs can be mapped into the much more restrictive TS/TDPL and gTS/GTDPL systems. 4.1 Eliminating \nRepetition Operators As in CFGs, repetition expressions can be eliminated from a PEG by converting them \ninto recursive nonterminals. Unlike in CFGs, the substitute nonterminal in a PEG must be right-recursive. \nTheorem: Any repetition expression e * can be eliminated by re\u00adplacing it with a new nonterminal A with \nthe de.nition A .eA/e. Proof: By induction on the length of the input string. Theorem: For any PEG G, \nan equivalent repetition-free grammar G can be created. Proof: Simply eliminate all repetition expressions \nthroughout G s nonterminal de.nitions and start expression. 4.2 Eliminating Predicates In this section \nwe show how to eliminate all predicate operators from any well-formed grammar whose language does not \ninclude the empty string. The restriction to grammars that do not accept the empty string is a minor \nbut unavoidable problem: we will show later that it is impossible for a predicate-free grammar to accept \nthe empty string without accepting all input strings. Given a well-formed, repetition-free grammar G \n=(VN ,VT ,R, eS) where e . L(G), we will create an equivalent grammar G ' = '' (VN ,VT ,R ' ,eS) that \nis well-formed, repetition-free, and predicate\u00adfree. This process occurs in three normalization stages. \nIn the .rst stage, we rewrite the grammar so that sequence and predicate ex\u00adpressions only contain nonterminals \nand choice expressions are dis\u00adjoint. In the second stage, we further rewrite the grammar so that nonterminals \nnever succeed without consuming any input. In the third stage we .nally eliminate predicates. 4.2.1 Stage \n1 In this stage we rewrite the existing de.nitions in R and the original start expression eS, adding \nsome new nonterminals and correspond\u00ading de.nitions in the process, to produce VN ' , R1, and eS1. We \n.rst add three special nonterminals, T , Z, and F, with corre\u00adsponding rules as follows. The nonterminal \nT matches any single terminal, and has the de.nition T <-. in concrete PEG syntax, before desugaring. \nThe nonterminal Z matches and consumes any input string; to avoid introducing repetition operators, we \nde.ne it Z . TZ/e. The nonterminal F always fails; to avoid using predi\u00adcates we de.ne it F .ZT . We \nde.ne a function f recursively as follows, to convert expres\u00adsions in our original grammar G into our \n.rst normal form: 1. f (e)=e if e .{e}.VN .VT . 2. f (e1e2)=AB, adding A . f (e1)and B . f (e2)to R1. \n 3. f (e1/e2)=A/!Af (e2), adding A . f (e1)to R1. 4. f (!e)=!A, adding A . f (e)to R1.  ' De.nition: \nThe stage 1 grammar G1 of G is (VN ,VT ,R1,eS1), where eS1 =f (eS), R1 ={A . f (e)|A .e .R}.{new de.nitions \n' resulting from application of f }, and V =VN .{new nonterminals N resulting from application of f }. \nLemma: For any expression e, f (e);G1 e. Proof: By structural induction over e. The only interesting \ncase is for choice expressions, which uses the identity e1/e2 ;e1/!e1 e2. Theorem: G1 ; G, all sequence \nand predicate expressions in the expression set of G1 contain only nonterminals as their subexpres\u00adsions, \nand all choice expressions are disjoint. Proof: Direct from the construction of f . 4.2.2 Stage 2 We \nnow rewrite the stage 1 grammar G1 into another equiva\u00ad ' lent grammar G2 =(VN ,VT ,R2,eS2), in which \nall nonterminals ei\u00ad ther succeed and consume a nonempty input pre.x, or fail: .A . ' VN (A -G20). This \ntransformation is analogous to e-reduction on CFGs, though the details are different due to predicates. \nWe use two functions g0 and g1, to split expressions into e-only and e-free parts, respectively. The \ne-only part g0(e)of an expres\u00adsion e is an expression that yields the same result as e on all input strings \nfor which e succeeds without consuming any input, and fails otherwise. The e-free part g1(e)of e likewise \nyields the same result as e on all inputs for which e succeeds and consumes at least one terminal, and \nfails otherwise. We .rst de.ne g0 recursively as follows: 1. g0(e)=e. 2. g0(a)=F. 3. g0(A)=g0(RG(A)). \n 4. g0(AB)=g0(A)g0(B)if A -0, otherwise g0(AB)=F. 5. g0(e1/e2)=g0(e1)/g0(e2). 6. g0(!A)=!(A /g0(A)). \n Lemma: The function g0 terminates if G is well-formed. Proof: By structural induction over the WFG \nrelation. Termination relies on g0(AB)not recursively invoking g0(B)if A -0. We now de.ne the function \ng1 primitive-recursively as follows: 1. g1(e)=F. 2. g1(a)=a. 3. g1(A)=A. 4. g1(AB)=g0(A)B / Ag0(B)/ \nAB. 5. g1(e1/e2)=g1(e1)/g1(e2). 6. g1(!e)=F.  ' De.nition: The stage 2 grammar G2 is (VN ,VT ,R2,eS2), \nwhere R2 ={A .g1(e)|A .e .R1}, and eS2 =g1(eS1)/ g0(eS1).We effectively split all of the nonterminal \nde.nitions in R1, retaining only the e-free parts in the de.nitions of R2, while substituting the corresponding \ne-only parts at the points where these nonterminals are referenced in order to preserve the original \nbehavior. There are only two such points: case 6 of g0, where we rewrite the operands of predicate expressions, \nand case 4 of g1, for e-free sequences. We say that the splitting invariant holds if the following is \ntrue: If (e,x).+ e, then (g0(e),x).+ e and (g1(e),x).+ f . G1 G2 G2 If (e,x) .+ y for |y| > 0, then (g0(e),x) \n.+ f and G1 G2 (g1(e),x).+ y. G2 If (e,x).+ f , then (g0(e),x).+ f and (g1(e),x).+ f . G1 G2 G2 Lemma: \nAssume that the splitting invariant holds for all input strings of length n or less. Then the splitting \ninvariant holds for strings of length n +1. Proof: By induction over the step counts of .G1 and .G2. \nTheorem: G2 is well-formed and equivalent to G, and for all non\u00adterminals A .VN , A -G2 0. Proof: A direct \nconsequence of the splitting invariant and the fact that G is well-formed. 4.2.3 Stage 3 '' Finally we \nrewrite G2 into the .nal grammar G ' =(VN ,VT ,R ' ,eS). De.nition: We de.ne a function d, such that \nd(A,e) distributes a nonterminal A into an e-only expression e resulting from the stage 2 function g0: \n1. d(A,e)=e,if e .{e,F}. 2. d(A,e1e2)=d(A,e1)d(A,e2). 3. d(A,e1/e2)=d(A,e1)/ d(A,e2). 4. d(A,!e)=!(Ae). \n '' Lemma: If e =g0(e )and e .E(G2), then Ae ;d(A,e)A. That is, we can use d(A,e)to move e leftward \nacross a nonterminal ref\u00aderence in a sequence expression. Proof: Structural induction on e and the identities \nin Section 3.7. Now de.ne a function n(e,C)=(e (Z/e)/ e)C. Lemma: If e is an e-only expression in G2, \nthen !eC ;n(e,C). Proof: If e succeeds, then the (Z/e)also succeeds and consumes the entire remaining \ninput. (The nonterminal Z alone is not suf.\u00adcient because it was rewritten in stage 2 to be e-free.) \nSince any nonterminal C is e-free, the overall expression will therefore fail. If e fails, however, then \n(e (Z/e)/ e)succeeds without consuming anything, making the overall expression behave according to C. \nWe now de.ne a function h0 to eliminate predicates from e\u00adproducing expressions resulting from the g0 \nor d functions: 1. h0(e,C)=C. 2. h0(F,C)=F. 3. h0(e1e2,C)=n(n(h0(e1,C),C)/ n(h0(e2,C),C),C). 4. h0(e1/e2,C)=h0(e1,C)/ \nh0(e2,C). 5. h0(!(B/e),C)=n(B/h0(e,C),C). 6. h0(!(A (B/e)),C)=n(A (B/h0(e,C)),C).  '' Lemma: If e \n=g0(e ) or e =d(A,g0(e )) and e '.E(G2), then h0(e,C)is a predicate-free expression equivalent to eC. \nProof: By structural induction over e. Case 5 handles predicates resulting directly from g0, which always \nhave the form !(B/e1), where e1 is likewise an expression resulting from g0. Case 6 sim\u00ad ' ilarly handles \nthe situation e =d(A,g0(e )). Case 3 rewrites a se\u00adquence e1e2 using the not-predicate analog of DeMorgan \ns Law: if e1 and e2 are e-only expressions, then e1e2 ;!(!e1/!e2). We can\u00adnot simply use h0(e1e2,C)=h0(e1,C)h0(e2,C)because \nh0(e1,C) consumes input if C succeeds, which would cause the h0(e2,C)part to start at the wrong position. \nWe now de.ne a corresponding function h1 to eliminate predicates from the e-free expressions generated \nby the stage 2 function g1: 1. h1(e)=e,if e .{a,A}. 2. h1(AB)=AB. 3. h1(e1B)=h0(e1,B),if e1 is not a \nnonterminal. 4. h1(Ae2)=h0(d(A,e2),A),if e2 is not a nonterminal. 5. h1(e1/e2)=h1(e1)/ h1(e2).  ' \nLemma: If e =g1(e )and e '.E(G2), then h1(e)is a predicate-free expression equivalent to e in G2. Proof: \nBy structural induction over e. In case 3, we know from the de.nition of g1 that e1 is an e-only expression \nresulting from g0, so we use the function h0 to combine it with the subsequent (e-free) nonterminal and \neliminate predicates from e1. Case4is similar, except that we must .rst move e2 to the left of A using \nthe d function before applying the predicate transformation. De.nition: The predicate-reduced grammar \nG ' of G is '' ' (VN ,VT ,R ' ,eS), where V is the set of nonterminals produced N ' in stage 1, R ' = \n{A . h1(e) | A . e . R2}, and e = S h1(g1(eS1))/ h0(g0(eS1),T ). Theorem: G ' is well-formed, repetition-free, \npredicate-free, and equivalent to G. Proof: G ' is repetition-free because G is repetition-free and we \nnever introduced any repetition operators. From the previous result, ' each nonterminal A .VN is equivalent \nto the corresponding nonter\u00adminal in the stage 2 grammar. By the same result, the h1(g1(eS1)) ' part \nof the new start expression eS is equivalent to the e-free part of the original start expression eS. \nThe h0(g0(eS1),T )in the new start expression succeeds and consumes exactly one terminal whenever the \ninput string is nonempty and the e-only part of the original start expression eS succeeds. Finally, since \nwe made the assumption at the start that the original grammar does not accept the empty string, the transformed \ngrammar behaves identically for this degenerate case. Since the acceptance of a string into the language \nof a gram\u00admar only depends on the success or failure of the start expression, and not on how much of \nthe input the start expression consumes, the new grammar G ' accepts exactly the same strings as G. 4.2.4 \nThe Empty String Limitation To show that we have no hope of avoiding the restriction that the original \ngrammar cannot accept the empty input string, we prove that any predicate-free grammar cannot accept \nthe empty input string without accepting all input strings. Lemma: Assume that G is a predicate-free \ngrammar, and that for any expression e and input x of length n or less, (e,e).+ e iff (e,x). +e. Then \nthe same holds for input strings of length n +1. Proof: By induction over step counts in .G. Theorem: \nIn a repetition-free grammar G, an expression e matches the empty string iff it matches all input strings \nand produces only e results. In consequence, e . L(G)implies L(G)=VT * . Proof: By induction over string \nlength. We could work around the empty string limitation by de.ning PEGs to require all recognized strings \nto include a designated end marker terminal, as Birman does in the original TS and gTS systems [5]. 4.3 \nReduction to TS/TDPL We can reduce any predicate-free PEG to an instance of Birman s TS system [4, 5], \nrenamed Top-Down Parsing Language (TDPL) by Aho and Ullman [3]. We will use the latter term for its descrip\u00adtiveness. \nTDPL uses a set of grammar-like de.nitions, but these de.nitions have only a few .xed forms in place \nof open-ended hier\u00adarchical parsing expressions. We can view TDPL as the PEG ana\u00adlog of Chomsky Normal \nForm (CNF) for context-free grammars. Instead of de.ning TDPL from the ground up as Birman does, we simply \nde.ne it as a restricted form of PEG. De.nition: A TDPL grammar is a PEG G =(VN ,VT ,R, S)in which S \nis a nonterminal in VN and all of the de.nitions in R have one of the following forms: 1. A . e. 2. \nA . a, where a . VT . 3. A . f , where f = !e. 4. A . BC/D, where B,C,D . VN .  The third form, A \n. f , representing unconditional failure, is con\u00adsidered primitive in TDPL, although we de.ne it here \nin terms of the parsing expression !e. The fourth form, A . BC/D, combines the functions of nonterminals, \nsequencing, and choice. A TDPL grammar G is interpreted according to the usual .G relation. Theorem: \nAny predicate-free PEG G =(VN ,VT ,R, eS)can be re\u00ad ' duced to an equivalent TDPL grammar G ' =VN ,VT \n,R ' ,S). Proof: First we add a new nonterminal S with de.nition S . eS, representing the original start \nexpression. We then add two non\u00adterminals E and F with de.nitions E . e and F . f respectively. Finally, \nwe rewrite each de.nition that does not conform to one of the TDPL forms above using the following rules: \nA . B -. A . BE/F A . e1e2 -. A . BC/F B . e1 C . e2 A . e1/e2 A . BE/C -. B . e1 C . e2 * A . e A . \nBA/E -. B . e Aho and Ullman de.ne an extended TDPL notation [3] equiva\u00adlent in expressiveness to repetition-free, \npredicate-free PEGs, with reduction rules almost identical to those above. 4.4 Reduction to gTS/GTDPL \nBirman s generalized TS (gTS) system, named generalized TDPL (GTDPL) by Aho and Ullman, is similar to \nTDPL, but uses slightly different basic rule forms that effectively provide the func\u00adtionality of predicates \nin PEGs. De.nition: A GTDPL grammar is a PEG G =(VN ,VT ,R, S) in which S is a nonterminal and all of \nthe de.nitions in R have one of the following forms: 1. A . e. 2. A . a, where a . VT . 3. A . f , \nwhere f = !e. 4. A . B[C,D], where B[C,D]= BC /!BD, and B,C,D . VN .  Theorem: Any PEG G =(VN ,VT ,R, \neS) can be reduced to an ' equivalent GTDPL grammar G ' =VN ,VT ,R ' ,S). Proof: First we add the de.nitions \nS . eS, E . e, and F . f ,as above for TDPL. Then we rewrite all non-conforming de.nitions using the \nfollowing transformations: A . B A . B[E,F] -. A . e1e2 -. A . B[C,F] B . e1 C . e2 A . e1/e2 A . B[E,C] \n-. B . e1 C . e2 * A . e A . B[A,E] -. B . e A . !e -. A . B[F, E] B . e 4.4.1 Parsing PEGs Corollary: \nIt is possible to construct a linear-time parser for any PEG on a reasonable random-access memory machine. \nProof: Reduce the PEG to a GTDPL grammar and then use the tabular parsing technique described by Aho \nand Ullman [3]. In practice it is not necessary to reduce a PEG all the way to TDPL or GTDPL form, though \nit is typically necessary at least to eliminate repetition operators. Practical methods for construct\u00ading \nsuch linear-time parsers both manually and automatically, par\u00adticularly using modern functional programming \nlanguages such as Haskell [11], are discussed in prior work [8, 7]. 4.4.2 Equivalence of TDPL and GTDPL \nTheorem: Any well-formed GTDPL grammar that does not accept the empty string can be reduced to an equivalent \nTDPL grammar. Proof: Treating the original GTDPL grammar as a repetition-free PEG, .rst eliminate predicates \n(Section 4.2), then reduce the result\u00ading predicate-free grammar to TDPL (Section 4.3). 5 Open Problems \nThis section brie.y outlines some promising directions for future work on PEGs and related syntactic \nformalisms. Birman de.ned a transformation on gTS that converts loop fail\u00adures caused by grammar circularities \ninto ordinary recognition fail\u00adures [5]. By extension it is possible to convert any PEG into a com\u00adplete \nPEG. It is probably possible to transform any PEG into an equivalent well-formed PEG, but this conjecture \nis unveri.ed; Bir\u00adman did not de.ne a structural well-formedness property for gTS. Such a transformation \non PEGs is conceivable despite the unde\u00adcidability of a grammar s completeness, since the transformation \nworks essentially by building run-time circularity checks into the grammar instead of trying to decide \nstatically at compile-time whether any circular conditions are reachable. Perhaps of more practical interest, \nwe would like a useful conserva\u00adtive algorithm to determine if a choice expression e1/e2 in a gram\u00admar \nis de.nitely disjoint, and therefore commutative. Such an al\u00adgorithm would enable us to extend PEG syntax \nwith an unordered choice operator | analogous to the choice operator used in EBNF syntax for CFGs. The \n| operator would be semantically identical to / , but would express the language designer s assertion \nthat the alternatives are disjoint and therefore order-independent, and tools such PEG analyzers and \nPEG-based parser generators [7] could ver\u00adify these assertions automatically. A .nal open problem is \nthe relationship and inter-convertibility of CFGs and PEGs. Birman proved that TS and gTS can simu\u00adlate \nany deterministic pushdown automata (DPDA) [5], implying that PEGs can express any deterministic LR-class \ncontext-free lan\u00adguage. There is informal evidence, however, that a much larger class of CFGs might be \nrecognizable with PEGs, including many CFGs for which no conventional linear-time parsing algorithm is \nknown [7]. It is not even proven yet that CFLs exist that cannot be recognized by a PEG, though recent \nwork in lower bounds on the complexity of general CFG parsing [14] and matrix product [23] shows at least \nthat general CFG parsing is inherently super-linear. 6 Related Work This work is inspired by and heavily \nbased on Birman s TS/TDPL and gTS/GTDPL systems [4, 5, 3]. The .G relation and the basic properties in \nSections 3.3 and 3.4 are direct adaptations of Birman s work. The major new features of the present work \nare the extension to support general parsing expressions with repetition and predicate operators, the \nstructural analysis and identity results in Sections 3.5 through 3.7, and the predicate elimination procedure \nin Section 4.2. While parsing expressions could conceivably be treated merely as syntactic sugar for \nGTDPL grammars, it is not clear that the pred\u00adicate elimination transformation, and hence the reduction \nfrom GT-DPL to TDPL, could be accomplished without the use of more general expression-like forms in the \nintermediate stages. For this reason it appears that PEGs represent a useful formal notation in its own \nright, complementary to the minimalist TDPL and GTDPL systems. Unfortunately it appears TDPL and GTDPL \nhave not seen much practical use, perhaps in large measure because they were origi\u00adnally developed and \npresented as formal models for certain types of top-down parsers, rather than as a useful syntactic foundation \nin its own right. Adams [1] used TDPL in a modular language proto\u00adtyping framework, however. In addition, \nmany practical top-down parsing libraries and toolkits, including the popular ANTLR [21] and the PARSEC \ncombinator library for Haskell [15], provide back\u00adtracking capabilities that conform to this model in \npractice, if per\u00adhaps unintentionally. These existing systems generally use naive backtracking methods \nthat risk exponential runtime in worst-case scenarios, but the same features can be implemented in strictly \nlin\u00adear time using a memoizing packrat parser [8, 7]. The positive form of syntactic predicate (the and-predicate \n) was introduced by Parr [20] for use in ANTLR [21], and later incorpo\u00adrated into JavaCC under the name \nsyntactic lookahead [16]. The metafrontsystem includes a limited, .xed-lookahead form of syn\u00adtactic predicates \nunder the terms attractors and traps [6]. The negative form of syntactic predicate (the not-predicate \n) appears to be new, but its effect can be achieved in practical parsing systems such as ANTLR and JavaCC \nusing semantic predicates [17]. Many extensions and variations of context-free grammars have been developed, \nsuch as indexed grammars [2], W-grammars [28], af.x grammars [13], tree-adjoining grammars [12], minimalist \ngrammars [24], and conjunctive grammars [18]. Most of these ex\u00adtensions are motivated by the requirements \nof expressing natural languages, and all are at least as dif.cult to parse as CFGs. Since machine-oriented \nlanguage translators often need to process large inputs in linear or near-linear time, and there appears \nto be no hope of general CFG parsing in much better than O(n3) time [14], most parsing algorithms for \nmachine-oriented languages focus on handling subclasses of the CFGs. Classic deterministic top-down and \nbottom-up techniques [3] are widely used, but their limitations are frequently felt by language designers \nand implementors. The syntax de.nition formalism SDF increases the expressiveness of CFGs with explicit \ndisambiguation rules, and supports uni.ed language descriptions by combining lexical and context-free \nsyn\u00adtax de.nitions into a two-level formalism [10]. The nondeter\u00administic linear-time NSLR(1) parsing \nalgorithm [26] is powerful enough to generate scannerless parsers from uni.ed syntax de.\u00adnitions without \ntreating lexical analysis separately [22], but the al\u00adgorithm severely restricts the form in which such \nCFGs can be writ\u00adten. Other machine-oriented syntax formalisms and tools use CFGs extended with explicit \ndisambiguation rules to express both lexical and hierarchical syntax, supporting uni.ed syntax de.nitions \nmore cleanly while giving up strictly linear-time parsing [21, 29, 27]. These systems graft recognition-based \nfunctionality onto generative CFGs, resulting in a hybrid generative/recognition-based syntac\u00adtic model. \nPEGs provide similar features in a simpler syntactic foundation by adopting the recognition paradigm \nfrom the start. 7 Conclusion Parsing expression grammars provide a powerful, formally rigor\u00adous, and \nef.ciently implementable foundation for expressing the syntax of machine-oriented languages that are \ndesigned to be un\u00adambiguous. Because of their implicit longest-match recognition capability coupled with \nexplicit predicates, PEGs allow both the lexical and hierarchical syntax of a language to be described \nin one concise grammar. The expressiveness of PEGs also introduces new syntax design choices for future \nlanguages. Birman s GTDPL sys\u00adtem serves as a natural normal form to which any PEG can easily be reduced. \nWith minor restrictions, PEGs can be rewritten to elim\u00adinate predicates and reduced to TDPL, an even \nmore minimalist form. In consequence, we have shown TDPL and GTDPL to be essentially equivalent in recognition \npower. Finally, despite their ability to express language constructs requiring unlimited looka\u00adhead and \nbacktracking, all PEGs are parseable in linear time with a suitable tabular or memoizing algorithm. Acknowledgments \nI would like to thank my advisor Frans Kaashoek, as well as Franc\u00b8ois Pottier, Robert Grimm, Terence \nParr, Arnar Birgisson, and the POPL reviewers, for valuable feedback and discussion and for pointing \nout several errors in the original draft. 8 References [1] Stephen Robert Adams. Modular Grammars for \nProgram\u00adming Language Prototyping. PhD thesis, University of Southampton, 1991. [2] Alfred V. Aho. Indexed \ngrammars an extension of context\u00adfree grammars. Journal of the ACM, 15(4):647 671, October 1968. [3] \nAlfred V. Aho and Jeffrey D. Ullman. The Theory of Parsing, Translation and Compiling -Vol. I: Parsing. \nPrentice Hall, Englewood Cliffs, N.J., 1972. [4] Alexander Birman. The TMG Recognition Schema. PhD the\u00adsis, \nPrinceton University, February 1970. [5] Alexander Birman and Jeffrey D. Ullman. Parsing algorithms with \nbacktrack. Information and Control, 23(1):1 34, August 1973. [6] Claus Brabrand, Michael I. Schwartzbach, \nand Mads Vang\u00adgaard. The metafront system: Extensible parsing and trans\u00adformation. In Third Workshop \non Language Descriptions, Tools and Applications, Warsaw, Poland, April 2003. [7] Bryan Ford. Packrat \nparsing: a practical linear-time algorithm with backtracking. Master s thesis, Massachusetts Institute \nof Technology, Sep 2002. [8] Bryan Ford. Packrat parsing: Simple, powerful, lazy, linear time. In Proceedings \nof the 2002 International Conference on Functional Programming, Oct 2002. [9] Dick Grune and Ceriel J.H. \nJacobs. Parsing Techniques A Practical Guide. Ellis Horwood, Chichester, England, 1990. [10] J. Heering, \nP. R. H. Hendriks, P. Klint, and J. Rekers. The syntax de.nition formalism SDF reference manual . SIG-PLAN \nNotices, 24(11):43 75, 1989. [11] Simon Peyton Jones and John Hughes (editors). Haskell 98 Report, 1998. \nhttp://www.haskell.org. [12] Aravind K. Joshi and Yves Schabes. Tree-adjoining gram\u00admars. Handbook of \nFormal Languages, 3:69 124, 1997. [13] C.H.A. Koster. Af.x grammars. In J.E.L. Peck, editor, AL-GOL 68 \nImplementation, pages 95 109, Amsterdam, 1971. North-Holland Publ. Co. [14] Lillian Lee. Fast context-free \ngrammar parsing requires fast boolean matrix multiplication. Journal of the ACM, 49(1):1 15, 2002. [15] \nDaan Leijen. Parsec, a fast combinator parser. http://www.cs.uu.nl/ daan. [16] Sun Microsystems. Java \ncompiler compiler (JavaCC). https://javacc.dev.java.net/. [17] Sun Microsystems. JavaCC: LOOKAHEAD minitutorial. \nhttps://javacc.dev.java.net/doc/lookahead.html. [18] Alexander Okhotin. Conjunctive grammars. Journal \nof Au\u00adtomata, Languages and Combinatorics, 6(4):519 535, 2001. [19] International Standards Organization. \nSyntactic metalan\u00adguage Extended BNF, 1996. ISO/IEC 14977. [20] Terence J. Parr and Russell W. Quong. \nAdding semantic and syntactic predicates to LL(k) pred-LL(k).In Proceedings of the International Conference \non Compiler Construction, Ed\u00adinburgh, Scotland, April 1994. [21] Terence J. Parr and Russell W. Quong. \nANTLR: A Predicated\u00adLL(k) parser generator. Software Practice and Experience, 25(7):789 810, 1995. [22] \nDaniel J. Salomon and Gordon V. Cormack. Scannerless NSLR(1) parsing of programming languages. In Proceedings \nof the ACM SIGPLAN 89 Conference on Programming Lan\u00adguage Design and Implementation (PLDI), pages 170 \n178, Jul 1989. [23] Amir Shpilka. Lower bounds for matrix product. In IEEE Symposium on Foundations of \nComputer Science, pages 358 367, 2001. [24] Edward Stabler. Derivational minimalism. Logical Aspects \nof Computational Linguistics, pages 68 95, 1997. [25] Bjarne Stroustrup. The C++ Programming Language. \nAddison-Wesley, 3rd edition, June 1997. [26] Kuo-Chung Tai. Noncanonical SLR(1) grammars. ACM Transactions \non Programming Languages and Systems, 1(2):295 320, Oct 1979. [27] M.G.J. van den Brand, J. Scheerder, \nJ.J. Vinju, and E. Visser. Disambiguation .lters for scannerless generalized LR parsers. In Compiler \nConstruction, 2002. [28] A. van Wijngaarden, B.J. Mailloux, J.E.L. Peck, C.H.A. Koster, M. Sintzoff, \nC.H. Lindsey, L.G.L.T. Meertens, and R.G. Fisker. Report on the algorithmic language ALGOL 68. Numer. \nMath., 14:79 218, 1969. [29] Eelco Visser. A family of syntax de.nition formalisms. Tech\u00adnical Report \nP9706, Programming Research Group, Univer\u00adsity of Amsterdam, 1997. [30] Niklaus Wirth. What can we do \nabout the unnecessary diver\u00adsity of notation for syntactic descriptions. Communications of the ACM, 20(11):822 \n823, November 1977.   \n\t\t\t", "proc_id": "964001", "abstract": "For decades we have been using Chomsky's generative system of grammars, particularly context-free grammars (CFGs) and regular expressions (REs), to express the syntax of programming languages and protocols. The power of generative grammars to express ambiguity is crucial to their original purpose of modelling natural languages, but this very power makes it unnecessarily difficult both to express and to parse machine-oriented languages using CFGs. Parsing Expression Grammars (PEGs) provide an alternative, recognition-based formal foundation for describing machine-oriented syntax, which solves the ambiguity problem by not introducing ambiguity in the first place. Where CFGs express nondeterministic choice between alternatives, PEGs instead use <i>prioritized choice</i>. PEGs address frequently felt expressiveness limitations of CFGs and REs, simplifying syntax definitions and making it unnecessary to separate their lexical and hierarchical components. A linear-time parser can be built for any PEG, avoiding both the complexity and fickleness of LR parsers and the inefficiency of generalized CFG parsing. While PEGs provide a rich set of operators for constructing grammars, they are reducible to two minimal recognition schemas developed around 1970, TS/TDPL and gTS/GTDPL, which are here proven equivalent in effective recognition power.", "authors": [{"name": "Bryan Ford", "author_profile_id": "81100262251", "affiliation": "Massachusetts Institute of Technology, Cambridge, MA", "person_id": "PP14099531", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/964001.964011", "year": "2004", "article_id": "964011", "conference": "POPL", "title": "Parsing expression grammars: a recognition-based syntactic foundation", "url": "http://dl.acm.org/citation.cfm?id=964011"}