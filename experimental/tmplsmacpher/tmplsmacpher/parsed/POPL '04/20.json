{"article_publication_date": "01-01-2004", "fulltext": "\n Summarizing Procedures in Concurrent Programs Shaz Qadeer Sriram K. Rajamani Jakob Rehof Microsoft \nResearch One Microsoft Way Redmond, WA 98052 Abstract The ability to summarize procedures is fundamental \nto building scalable interprocedural analyses. For sequential programs, proce\u00addure summarization is well-understood \nand used routinely in a va\u00adriety of compiler optimizations and software defect-detection tools. However, \nthe bene.t of summarization is not available to multi\u00adthreaded programs, for which a clear notion of \nsummaries has so far remained unarticulated in the research literature. In this paper, we present an \nintuitive and novel notion of proce\u00addure summaries for multithreaded programs. We also present a model \nchecking algorithm for these programs that uses procedure summarization as an essential component. Our \nalgorithm can also be viewed as a precise interprocedural data.ow analysis for mul\u00adtithreaded programs. \nOur method for procedure summarization is based on the insight that in well-synchronized programs, any \ncom\u00adputation of a thread can be viewed as a sequence of transactions, each of which appears to execute \natomically to other threads. We summarize within each transaction; the summary of a procedure comprises \nthe summaries of all transactions within the procedure. We leverage the theory of reduction [17] to infer \nboundaries of these transactions. The procedure summaries computed by our algorithm allow reuse of analysis \nresults across different call sites in a multithreaded pro\u00adgram, a bene.t that has hitherto been available \nonly to sequential programs. Although our algorithm is not guaranteed to terminate on multithreaded programs \nthat use recursion (reachability analy\u00adsis for multithreaded programs with recursive procedures is unde\u00adcidable \n[18]), there is a large class of programs for which our al\u00adgorithm does terminate. We give a formal characterization \nof this class, which includes programs that use shared variables, synchro\u00adnization, and recursion. Categories \nand Subject Descriptors: D.1.3: Concurrent program\u00adming, parallel programming; D.2.4: Software/program \nveri.cation General Terms: Reliability, security, languages, veri.cation Keywords: Concurrent programs, \npushdown systems, model checking, interprocedural data.ow analysis, procedure summaries, transactions, \nreduction Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL \n04, January 14 16, 2004, Venice, Italy. Copyright 2004 ACM 1-58113-729-X/04/0001 ...$5.00 1 Introduction \nSequential programs with .nite-domain variables and recursive procedures are in.nite-state systems due \nto unbounded stack depth. In spite of the potentially in.nite state space of these programs, as\u00adsertion \nchecking is decidable for them. A common technique for analyzing such programs is CFL reachability [21, \n19] (or equiva\u00adlently, pushdown model checking [22, 10]), where the key idea is to build procedure summaries. \nThe summary of a procedure P con\u00adtains the state pair (s,s.)if in state s, there is an invocation of \nP that yields the state s.on termination. Summaries enable reuse if P is called from two different places \nwith the same state s, the work done in analyzing the .rst call is reused for the second. This reuse \nis the key to scalability of interprocedural analyses. Additionally, summarization avoids direct representation \nof the call stack, and guarantees termination of the analysis even if the program has re\u00adcursion. Assertion \nchecking for multithreaded programs with .nite-domain variables and recursive procedures is undecidable \n[18]. Most ap\u00adproaches to the analysis of such programs either restrict the interac\u00adtion between synchronization \nand procedure calls [1, 9, 6], or per\u00adform data.ow-style overapproximations [20], or use abstractions \nto capture the behavior of each thread [4, 12]. In this paper, we take a radically different approach \nto solve this problem. We perform model checking directly on the multithreaded program. However, in order \nto gain reuse and scalability in our analysis, we present a novel method to compute procedure summaries \neven in the pres\u00adence of multiple threads. Since the assertion checking problem is undecidable for multithreaded \nprograms with procedures, we can\u00adnot guarantee termination of our algorithm on all cases. However, our \nalgorithm terminates for a large class of multithreaded pro\u00adgrams that includes programs with shared \nvariables, synchroniza\u00adtion, and recursion. Our model checking algorithm for multithreaded programs has \ntwo levels. The .rst level performs reachability analysis and maintains an explicit stack for each thread. \nThe second level computes a sum\u00admary for each procedure. During the reachability analysis at the .rst \nlevel, whenever a thread makes a procedure call, we invoke the second level to compute a summary for \nthe procedure. This summary is returned to the .rst level, which uses it to continue the reachability \nanalysis. The most crucial aspect of this algorithm is the notion of procedure summaries in multithreaded \nprograms. A straightforward general\u00adization of a (sequential) procedure summary to the case of multi\u00adthreaded \nprograms could attempt to accumulate all state pairs (s,s.) obtained by invoking this procedure in any \nthread. But this simple\u00adminded extension is not that meaningful, since the resulting state s. for an \ninvocation of a procedure P in a thread might re.ect updates by interleaved actions of concurrently executing \nthreads. Clearly, these interleaved actions may depend on the local states of the other threads. Thus, \nif (s,s.)is an element of such a summary, and the procedure P is invoked again by some thread in state \ns, there is no guarantee that the invoking thread will be in state s.on completing execution of P. We \npresent a robust and intuitive notion of procedure summaries for multithreaded programs. This notion \nis based on the insight that in well-synchronized programs, any computation of a thread can be viewed \nas a sequence of transactions, each of which appears to exe\u00adcute atomically to other threads. While analyzing \nthe execution of a transaction by a thread, interleavings with other threads need not be considered. \nOur key idea is to summarize procedures within such transactions. Two main technical dif.culties arise \nwhile performing transaction-based summarization of procedures: Transaction boundaries may not coincide \nwith procedure boundaries. A transaction may begin in a procedure foo and end half-way inside a procedure \nbar called from foo. Con\u00adversely, a transaction may begin in a procedure foo and con\u00adtinue even after \nfoo returns. One way to summarize such transactions is to have a stack frame as part of the state in \neach summary. However, this solution not only complicates the algorithm but also makes the summaries \nunbounded even if all state variables have a .nite domain. Our summaries do not contain stack frames. \nIf a transaction begins in one proce\u00addure context and ends in another procedure context, we break up \nthe summary into smaller sub-summaries each within the context of a single procedure. Thus, our model \nchecking algo\u00adrithm uses a combination of two representations states with stacks and summaries without \nstacks.  A procedure can be called from different phases of a trans\u00adaction the pre-commit phase or the \npost-commit phase. We need to summarize the procedure differently depending on the phase of the transaction \nat the call site. We solve this problem by instrumenting the source program with a boolean variable representing \nthe transaction phase, thus making the transac\u00adtion phase part of the summaries.  We present a formal \ncharacterization of a class of multithreaded programs on which our summarization-based model checking \nal\u00adgorithm is guaranteed to terminate. We show that if every call to a recursive procedure is contained \nentirely within a transaction, our algorithm will terminate with the correct answer. To detect transactions \nin multithreaded programs, we leverage the theory of reduction [17]. Reduction views a transaction as \na se\u00adquence of actions a1,,am,x,b1,,bn such that each ai is a right mover and each bi is a left mover. \nA right mover is an action that commutes to the right of every action by another thread; a left mover \nis an action that commutes to the left of every action by an\u00adother thread. Thus, to detect transactions \nwe need to detect right and left movers. In this paper, we abstract away from the problem of detecting \nmovers and instead focus on the algorithm for sum\u00admarization assuming that right and left movers are \nprovided by a separate analysis [15, 14, 13]. To summarize (!), we present a novel two-level model checking \nalgorithm for multithreaded programs with (recursive) procedures. This algorithm uses transaction-based \nprocedure summarization as a core component. We present a formal characterization of a class of programs \non which our algorithm is guaranteed to terminate. acq E1 t=x E2 x = t+1 E3 rel s3 s4 s2 s7 s5 s6 s1 \n s8  E1 . acq t=x x=t+1 .rel . E2 . ssss3 s4s1 s8 2 s7 E3 5 6 Figure 1. Transaction This class includes \na number of realistic programs that use shared variables, synchronization, and procedures. In recent \nyears, .ow\u00adsensitive and context-sensitive analyses have been used to check .\u00adnite state properties of \nsequential programs in a scalable way using summarization as an essential component [3, 5]. Our work \nis a .rst step in bringing these analyses to bear on multithreaded programs as well. 2 Overview We illustrate \nour ideas using examples that use mutexes to protect accesses to shared variables. We .rst make some \ngeneral observa\u00adtions about such programs. The action acquire(m), where m is a mutex, is a right mover. \nOnce it happens, there is no enabled action of another thread that may access m. Hence, this action can \nbe commuted to the right of any action of another thread.  The action release(m) is a left mover. At \na point when it is enabled but has not happened, there is no enabled action of another thread that may \naccess m. Hence, this action can be commuted to the left of any action of another thread.  An action \nthat accesses only local variables is both a left mover and a right mover, since this action can be commuted \nboth to the left and the right of actions by the other threads.  An action that accesses a shared variable \nis both a left mover and right mover, as long as all threads acquire the same mutex before accessing \nthat variable.  A transaction is a sequence of right movers, followed by a single atomic action with \nno restrictions, followed by a sequence of left movers. A transaction can be in two states: pre-commit \nor post\u00adcommit. A transactions starts in the pre-commit state and stays in the pre-commit state as long \nas right movers are being executed. When the atomic action (with no restrictions) is executed, the trans\u00adaction \nmoves to the post-commit state. This atomic action is called the committing action. The transaction stays \nin the post-commit state as long as left movers are being executed until the transaction completes. We \nillustrate a transaction with an example. Figure 1 shows an ex\u00adecution of a concurrent program. A thread \nperforms the following sequence of four operations: (1) acquires a lock (the operation acq in the .rst \nexecution trace), (2) reads a variable x protected by that lock into a local variable t (t=x), (3) updates \nthat variable (x=t+1), and (4) releases the lock (rel). Suppose that the actions of this method are interleaved \nwith arbitrary actions E1, E2, E3 of other threads. We assume that the environment actions respect the \nlock\u00ading discipline of accessing x only after acquiring the lock. Since the acquire operation is a right \nmover, it is commuted to the right of the environment action E1 without changing the .nal state s3, even \nthough the intermediate state changes from s2 to s. 2. The read operation is the committing action. The \nwrite and release oper\u00adations are left movers and are commuted to the left of environment bool available[N]; \nmutex m; int getResource() { inti = 0; L0: acquire(m); L1: while(i<N){ L2: if (available[i]) { \nL3: available[i] = false; L4: release(m); L5: return i; } L6: i++; } L7: release(m); L8: return \ni; } Figure 2. Resource allocator with coarse-grained locking actions E2 and E3. Finally, after performing \na series of commute operations, we get the execution at the bottom of the diagram in which the actions \nof the .rst thread happen without interruption from the environment. Note that in this execution the \ninitial and .nal states are the same as before. Thus, the sequence of opera\u00adtions performed by the .rst \nthread constitute a transaction. This transaction is in the pre-commit state before the read of x and \nin the post-commit state afterwards. We illustrate different aspects of our summarization-based model \nchecking algorithm using four examples. The .rst example illus\u00adtrates a simple case where the transaction \nboundary and the pro\u00adcedure boundary coincide. The second example illustrates summa\u00adrization where a \nprocedure body contains several transactions inside it. The third example illustrates that our algorithm \nterminates and validates the program assertions even in the presence of unbounded recursion. The fourth \nexample illustrates our summarization tech\u00adnique when a procedure is called from different transactional \ncon\u00adtexts. Example 1 Consider the resource allocation routine shown in Figure 2. There are N shared resources \nnumbered 0,,N 1. The j-th entry in the global boolean array available is true iff the j-th resource is \nfree to be allocated. A mutex mis used to protect accesses to available. The mutex m has the value 0 \nwhen free, and 1 when locked. The body of getResource acquires the mutex m, and then searches for the \n.rst available resource. If a free resource is found at index i, it sets available[i] to false and releases \nthe mutex. If no free resource is found, then it releases the mutex. In both cases, it returns the .nal \nvalue of i. Thus, the returned value is the index of a free resource if one is available; otherwise, \nit is N. There is a companion procedure freeResource for freeing an allocated resource, but we have not \nshown that in the .gure. We assume that the multithreaded program consists of a number of threads that \nnondeterministically call getResource and freeResource. Since acquire(m) is a right mover, release(m) \nis a left mover, and all other actions in getResourceare both right and left movers, the entire procedure \nis contained in a single transaction. Sup\u00adpose N =2. Weuse (a0,a1)to denote the contents of available, \nwhere a0 and a1 respectively denote the values of available[0] and available[1]. The summary of getResource \nconsists of a .... set of edges of the form (pc,i,m,(a0,a1)) ,i. ,m,(a0,a1)), >(pc bool available[N]; \nmutex m[N]; int getResource() { inti = 0; L0: while(i<N){ L1: acquire(m[i]); L2: if (available[i]) \n{ L3: available[i] = false; L4: release(m[i]); L5: return i; } else { L6: release(m[i]); } L7: i++; \n } L8: return i; } Figure 3. Resource allocator with .ne-grained locking where the tuple (pc,i,m,(a0,a1))represents \nthe values of the pro\u00adgram counter and variables i, m, and available in the pre-store of .... the transaction \nand the tuple (pc,i. ,m,(a0,a1))denotes the corre\u00adsponding values in the post-store of the transaction. \nThe computed summary of getResource consists of the following edges: (LO,0,0,(0,0)) (L8,2,0,(0,0)) > \n(LO,0,0,(0,1)) (L5,1,0,(0,0)) > (LO,0,0,(1,0)) (L5,0,0,(0,0)) > (LO,0,0,(1,1)) (L5,0,0,(0,1)) > All \nedges in this summary begin at the label L0 and terminate at one of the two labels L8 or L5 both of which \nare labels at which getResource returns. Thus, the summary matches the intuition that the procedure body \nis just one transaction. The .rst edge sum\u00admarizes the case when no resource is allocated; the remaining \nthree edges summarize the case when some resource is allocated. There is no edge beginning in a state \nwith I 1 since from such a state, the execution of the transaction blocks. Once this summary has been \ncomputed, if any thread calls getResource, the summary can be used to compute the state at the end of \nthe transaction, without re-analyzing the body of getResource, thus providing reuse and scalability. \nExample 2 Let us consider a modi.cation to the resource allocator. The new program is shown in Figure \n3. We have made the locking more .ne-grained by using an array m of mutexes and protecting the j\u00adth entry \nin available with the j-th entry in m. Now, the body of the procedure getResource is no longer contained \nentirely in a single transaction. In fact, there is one transaction corresponding to each iteration of \nthe loop inside it. Again, suppose N =2. Now, the summary contains edges of the form (pc,i,(m0,m1),(a0,a1)) \n> . .... (pc,i. ,(m0,m),(a0,a)), where (m0,m1)denotes the contents of 11min the pre-store and (m0. ,m1.)denotes \nthe contents of m in the post\u00adstore. The computed summary of getResource consists of the fol\u00adlowing edges: \n(LO,0,(0,0),(0,0)) (L1,1,(0,0),(0,0)) > (LO,0,(0,0),(0,1)) (L1,1,(0,0),(0,1)) > (LO,0,(0,0),(1,0)) (L5,0,(0,0),(0,0)) \n > (LO,0,(0,0),(1,1)) (L5,0,(0,0),(0,1)) > intg = 0; mutex m; void foo(int r) { void main() { L0: \nif (r == 0) { int q = choose({0,1}); L1: foo(r); M0: foo(q); } else { M1: acquire(m) L2: acquire(m); \nM2: assert(g >= 1); L3: g++; M3: release(m); L4: release(m); M4: return; }} L5: return; } P= {main() \n}|| {main() } Figure 4. Summarization enables termination (L1,1,(0,0),(0,0))>(L8,2,(0,0),(0,0)) (L1,1,(0,0),(0,1))>(L5,1,(0,0),(0,0)) \n(L1,1,(0,0),(1,0))>(L8,2,(0,0),(1,0)) (L1,1,(0,0),(1,1))>(L5,1,(0,0),(1,0)) This summary contains three \nkinds of edges. The .rst two edges correspond to the transaction that starts at the beginning of the \npro\u00adcedure, i.e., at label L0 with i0, goes through the loop once, and ends at L1 with i1. The next two \nedges correspond to the trans\u00adaction that again starts at the beginning of the procedure, but ends with \nthe return at label L5 during the .rst iteration through the loop. The last four edges correspond to \nthe transaction that starts in the middle of the procedure at label L1 with i1, and returns either at \nlabel L5 or L8. Note that edges of the .rst and third kind did not exist in the summary of the previous \nversion of getResource, where all edges went from entry to exit. Example 3 The previous two examples \nillustrated summaries and the reuse afforded by them. In a number of programs, summaries enable our model \nchecking algorithm to terminate where the naive model checking algorithm without summaries will not terminate. \nCon\u00adsider the example in Figure 4. The program P consists of two threads, each of which starts execution \nby calling the main pro\u00adcedure. The main procedure has a local variable q which is initial\u00adized nondeterministically. \nThen main calls foo with q as the actual parameter. The procedure foo has an in.nite recursion if the \npa\u00adrameter r is 0. Otherwise, it increments global g and returns. After returning from foo, the main \nprocedure asserts that (g >= 1). All accesses to the shared global g are protected by a mutex m. The \ninitial value of g is 0. The stack can grow without bound due to the recursion in procedure foo. Hence, \nnaive model checking does not terminate on this ex\u00adample. However, the body of foo consists of one transaction, \nsince all action sequences in foo consist of a sequence of right movers followed by a sequence of left \nmovers. A summary edge for foo is ... of the form (pc,r,m,g)>(pc,r,m,g.), whose meaning is similar to \nthat of a summary edge in the previous examples. The summary for foo consists of the following edges: \n(LO,1,0,0)>(L5,1,0,1) (LO,1,0,1)>(L5,1,0,2) There is no edge beginning in a state with r0 since from \nsuch a state, the execution of the transaction never terminates. Using summaries, we avoid reasoning \nabout the stack explicitly inside foo and also avoid exploring the unbounded recursion in foo. intgm=0,gn=0; \n mutex m, n; void bar() { N0: acquire(m); N1: gm++; N2: release(m); } void foo1() { void foo2() \n{ L0: acquire(n); M0: acquire(n); L1: gn++; M1: gn++; L2: bar(); M2: release(n); L3: release(n); M3: \nbar(); L4: return; M4: return; }} P= {foo1() }|| {foo2() } Figure 5. Summarization from different transactional \ncontexts The body of main has two transactions. The .rst transaction be\u00adgins at label M0 and ends at \nlabel M1, consisting of essentially the call to foo. The second transaction begins at label M1 and ends \nat label M4. A summary edge for main has the form (pc,q,m,g)> (pc,q. ,m. ,g.). The summary for main consists \nof the following edges: (MO,1,0,0)>(M1,1,0,1) (MO,1,0,1)>(M1,1,0,2) (M1,1,0,1)>(M4,1,0,1) (M1,1,0,2)>(M4,1,0,2) \nUsing these summaries for procedures foo and main, our model checking algorithm is able to terminate \nand correctly conclude that P is free of assertion violations. The algorithm begins with an empty stack \nfor each thread. When a thread calls main, since the body of main is not contained within one transaction, \nthe algorithm pushes a frame for mainon the stack of the calling thread. However, when a thread calls \nfoo, no frame corresponding to foo is pushed since the entire body of foo is contained within a transaction. \nIn\u00adstead, foo is summarized and its summary is used to make progress in the model checking. Example 4 \nConsider the example in Figure 5. Here, two shared variables gm and gn are protected by mutexes m and \nn respectively. Procedure baraccesses the variable gm, and is called from two different proce\u00addures foo1 \nand foo2.In foo1, the procedure baris called from the pre-commit state of the transaction, since no mutexes \nare released prior to calling bar.In foo2, the procedure bar is called from the post-commit state of \nthe transaction, since mutex n is released prior to calling bar. The summary for bar needs to distinguish \nthese two calling contexts. In the case of the call from foo1, the entire body of foo1 including the \ncall to bar() is part of the same trans\u00adaction. In the case of the call from foo2, there are two transactions, \none from label M0to M3, and another from label M3 to M4. We distin\u00adguish these two by instrumenting the \nsemantics of the program with an extra bit of information that records the phase of the transaction. \nThen, each summary edge provides the pre and post values not only of program variables but also of the \ntransaction phase. More details are given Section 4. 3 Multithreaded programs The store of a multithreaded \nprogram is partitioned into the global store Global and the local store Local of each thread. The set \nLocal of local stores has a special store called wrong. The local store of a thread moves to wrong on \nfailing an assertion and thereafter the failed thread does not make any other transitions. Domains  \nt,u E Tid {1,...,n} g E Global l E Local ls E Locals Tid -Local f E Frame s E Stack Frame. ss E Stacks \n Tid -Stack State Global xLocals xStacks  A multithreaded program (g0,ls0,T,T+,T.)consists of .ve \ncom\u00adponents. g0 is the initial value of the global store. ls0 maps each thread id t ETid to the initial \nlocal store ls0(t)of thread t. We model the behavior of the individual threads using three relations: \nT C Tid X(Global XLocal)X(Global XLocal) T+ C Tid XLocal X(Local XFrame) T. C Tid X(Local XFrame)XLocal \n The relation T models thread steps that do not manipulate the stack. The relation T(t,g,l,g. ,l.)holds \nif thread t can take a step from a state with global store g and local store l, yielding (possibly mod\u00adi.ed) \nstores g.and l. . The stack is not accessed or updated during this step. The relation T+(t,l,l. ,f )models \nsteps of thread t that push a frame onto the stack. This step does not access the global store, is enabled \nwhen the local store is l, updates the local store to l., and pushes the frame f onto the stack. Similarly, \nthe rela\u00adtion T.(t,l,f ,l.)models steps of thread t that pop a frame from the stack. This step also does \nnot access the global store, is enabled when the local store is l and the frame at the top of the stack \nis f , updates the local store to l., and pops the frame f from the stack. The program starts execution \nfrom the state (g0,ls0,ss0)where ss0(t)e for all t ETid. At each step, any thread may make a transition. \nThe transition relation >t CState XState of thread t is de.ned below. For any function h from A to B, \na EA and b EB, we write h[a : b]to denote a new function such that h[a : b](x) evaluates to h(x)if x \na, and to b if xa. Transition relation >t  T(t,g,ls(t),g.,l.) (g,ls,ss)-t (g.,ls t : l.J,ss) T (t,ls(t),l.,f \n) (g,ls,ss)-t (g,ls t : l.J,ss t : ss(t).f J) ss(t)s.fT.(t,ls(t),f ,l.) (g,ls,ss)-t (g,ls t : l.J,ss \nt : sJ)  The transition relation >CState XState of the program is the dis\u00adjunction of the transition \nrelations of the various threads. >]t >t 4 Model checking with reduction Transactions occur in multithreaded \nprograms because of the pres\u00adence of right and left movers. Inferring which actions of a program are \nright and left movers is a problem that is important but orthogo\u00adnal to the contribution of this paper. \nIn this section, we assume that right and left movers are available to us as the result of a previous \nanalysis (see, e.g. [15, 14]). We use this information to derive a model checking algorithm that uses \ntransactions but does not per\u00adform any summarization of procedures. We will use the intuition developed \nin this section to derive a second model checking algo\u00adrithm in Section 5 that performs procedure summarization \nas well. Let RM,LM CT be subsets of the transition relation T with the following two properties for all \nt u: 1. If RM(t,g1,l1,g2,l2)and T(u,g2,l3,g3,l4), there is g4 such that T(u,g1,l3,g4,l4)and RM(t,g4,l1,g3,l2). \nFurther, RM(u,g2,l3,g3,l4)iff RM(u,g1,l3,g4,l4), and LM(u,g2,l3,g3,l4)iff LM(u,g1,l3,g4,l4). 2. If T(u,g1,l1,g2,l2)and \nLM(t,g2,l3,g3,l4), there is g4 such that LM(t,g1,l3,g4,l4)and T(u,g4,l1,g3,l2). Fur\u00adther, RM(u,g1,l1,g2,l2)iff \nRM(u,g4,l1,g3,l2), and LM(u,g1,l1,g2,l2)iff LM(u,g4,l1,g3,l2).  The .rst property states that a right \nmover action in thread t com\u00admutes to the right of an action of a different thread u. Moreover, the action \nby thread u is a right mover (resp. left mover) before the commute operation iff it is a right mover \n(resp. left mover) after the commute operation. Similarly, the second property states the re\u00adquirement \non a left mover in thread t. Our analysis is parameterized by the values of RM and LM and only requires \nthat they satisfy these two properties. The larger the relations RM and LM, the longer the transactions \nour analysis infers. Therefore, these relations should be as large as possible in practice. As mentioned \nbefore, a transaction is a sequence of right movers followed by a single action followed by a sequence \nof left movers. In order to minimize the number of explored interleavings and to maximize reuse, we would \nlike to infer transactions that are as long as possible. In order to implement this inference, we introduce \nin each thread a boolean local variable to keep track of the phase of that thread s transaction. Note \nthat this instrumentation is done automatically by our analysis, and not by the programmer. The phase \nvariable of thread t is true if thread t is in the right mover (or pre-commit) part of the transaction; \notherwise the phase variable is false. We say that the transaction commits when the phase variable moves \nfrom true to false. The initial value of the phase variable for each thread is true. p,p.EBoolean \u00a3,\u00a3.ELocal# \nLocal XBoolean . \u00a3s,\u00a3sELocals# Tid >Local# The initial value of the global store of the instrumented \nprogram re\u00admains g0. The initial value of the local stores changes to \u00a3s0, where \u00a3s0(t)(ls0(t),true)for \nall t ETid. We instrument the transition relation T, T+, and T.to generate new transition relations U, \nU+, and U.that update the phase appropriately. U CTid X(Global XLocal#)X(Global XLocal#) U+CTid XLocal# \nX(Local# XFrame) U.CTid X(Local# XFrame)XLocal# Ruleset 1: Model checking with reduction (INIT) S(g0,ts0,ss0) \n(STEP) \\u t.N (u,g,ts(u)) S(g,ls,ss)U(t,g,ts(t),g. ,t.) S(g. ,tst : t.J,ss) (PUSH) \\u t.N (u,g,ts(u)) \nS(g,ts,ss)U (t,ts(t),t. ,f ) S(g,tst : t.J,ss t : ss(t).f J) (POP) \\u t.N (u,g,ts(u)) S(g,ts,ss)U.(t,ts(t),f \n,t.)ss(t)s.f S(g,tst : t.J,ss t : sJ) U(t,g,(l,p),g. ,(l. ,p.)) def /T(t,g,l,g. ,l.) /p.(RM(t,g,l,g. \n,l.)/(pLM(t,g,l,g. ,l.))) U+(t,(l,p),(l. ,p.),f ) def /T+(t,l,l. ,f ) /p.p U.(t,(l,p),f ,(l. ,p.)) def \n/T.(t,l,f ,l.) /p.p In the de.nition of U, the relation between p.and p re.ects the intuition that if \np is true, then p.continues to be true as long as it executes right mover actions. The phase changes \nto false as soon as the thread executes an action that is not a right mover. Thereafter, it remains false \nas long as the thread executes left movers. Then, it becomes true again as soon as the thread executes \nan action that is a right mover and not a left mover. For each thread t, we de.ne three sets: R (t),L(t),N \n(t)CGlobal XLocal# These sets respectively de.ne when a thread is executing in the right mover part of \na transaction, the left mover part of a trans\u00adaction, and outside any transaction. For example, in the \n.rst exe\u00adcution of Figure 1, let t be the identi.er of the thread executing the transaction. Then, the \nstates {s2,s3gER (t), {s4,s5,s6,s7gEL(t), and {s1,s8gEN (t). These three sets can be any partition of \n(Global XLocal#)satisfying the following two conditions: C1. R (t)C{(g,(l,p))Il E{ls0(t),wrongg/p g. \n {l E{ls0(t),wrongg/p /}C2. L(t)C(g,(l,p))Vg. ,l.T(t,g,l,g. ,l.) 'LM(t,g,l,g. ,l.) Condition C1 says \nthat thread t is in the right mover part of a transaction only if the local store of t is neither its \ninitial value nor wrong and the phase variable is true. Condition C2 says that thread t is in the left \nmover part of a transaction only if the local store of t is neither its initial value nor wrong, the \nphase variable is false, and all possible enabled transitions are left movers. Since (R (t),L(t),N (t))is \na partition of (Global XLocal#), once R (t) and L(t)have been picked according to C1 and C2, the set \nN (t)is implicitly de.ned. We write R (t,g,\u00a3)whenever (g,\u00a3)ER (t), L(t,g,\u00a3)whenever (g,\u00a3)EL(t), and N \n(t,g,\u00a3)whenever (g,\u00a3)EN (t). Finally, using the values of N (t)for all t ETid, we model check the multithreaded \nprogram by computing the least .xpoint of the set of rules in Rule\u00adset 1. This model checking algorithm \nschedules a thread only when no other thread is executing inside a transaction. Conditions C1 and C2 \nare not quite enough for the model checking algorithm to be sound. The reason is the following. If a \ntransaction in thread t commits but never .nishes, the shared variables modi\u00ad.ed by this transaction \nbecome visible to other threads. However, the algorithm does not explore transitions of other threads \nfrom any state after the transaction commits. Therefore, we add a third con\u00addition C3 which states that \nevery committed transaction must .nish. In order to state this condition formally, we extend the transition \nre\u00adlation >t from Section 3 to the program store augmented with the phase variable in the natural way. \nC3. Suppose (g,\u00a3)EL(t), S(g,\u00a3s,ss), and \u00a3s(t)\u00a3. Then, there is g. , \u00a3s., and ss.such that (g. ,\u00a3s.(t))EN \n(t)and (g,\u00a3s,ss)>. t (g. ,\u00a3s. ,ss.). Our analysis is correct for any partition (R (t),L(t),N (t))of (Global \nXLocal#)satisfying conditions C1, C2, and C3. The smaller the value of N (t), the larger the transactions \ninferred by the analysis. Therefore, an implementation of our analysis should pick a value for N (t)that \nis as small as possible. We can now state our soundness theorem for the model checking algorithm presented \nabove. THEOREM 1. Let (g0,\u00a3s0,U,U+ ,U.)be the instrumented mul\u00adtithreaded program. Let S be the least \n.xpoint of the rules in Ruleset 1. Let the conditions C1, C2, and C3 be satis\u00ad.ed. If (g0,ls0,ss0)>.(g,ls,ss)and \nls(t)wrong, then there is .. .. (g,\u00a3s,ss.)and p such that S(g,\u00a3s,ss.)and \u00a3s.(t)(wrong,p). Proof (Sketch) \nSuppose (g0,ls0,ss0)>.(g,ls,ss)through some sequence of actions of various threads and ls(t)wrong. First, \nwe extend this sequence to complete all committed but un.nished transactions using condition C3. Then, \none by one, we commute each action in an uncommitted transaction to the right and drop it. Eventually, \nwe will get an execution sequence s with only com\u00adpleted transactions and with the property that s goes \nwrong if the original sequence goes wrong. Therefore s goes wrong as well. In s, the transactions of \na thread could have interleaved actions of an\u00adother thread. The order in which transactions commit is \na total order on the transactions in s. We denote this total order by <. We can transform s into an equivalent \nexecution s.(by appropriately right\u00adcommuting right movers, and left-commuting left movers), such that \ns.has the following properties: (1) for every thread t, no ac\u00adtion of a different thread t.occurs in \nthe middle of a transaction of thread t, (2) the transactions in s.commit in the order <. From the properties \nof right and left movers, we get that s.also goes wrong. Since s.schedules each transaction to completion, \nthe states along s.will be explored by the rules in Ruleset 1. A similar proof has been carried out with \nmore detail in an earlier paper [15]. . Although this algorithm is sound, it might not terminate if a \nthread calls a recursive procedure (even if all variables take values from a .nite domain). In the next \nsection, we use the concepts developed in this section to derive a model checking algorithm that uses \nproce\u00ad Ruleset 2: Level I Reachability (INIT) .(g0,ts0,ss0) (STEP) .(g,ts,ss)\\ut.N (u,g,ts(u)) Sum(t,g,s(t),g. \n,t.) t .(g. ,tst : t.J,ss,ps t : p.J) (PUSH) .(g,ts,ss)\\ut.N (u,g,ts(u)) Sum (t,g,ts(t),g. ,t. ,f ) .(g. \n,st : t.J,ss t : ss(t).f J) t (POP) .(g,ts,ss)\\ut.N (u,g,ts(u)) ss(t)s.f Sum.(t,g,s(t),f ,g. ,.)tt .(g. \n,tst : t.J,ss t : sJ) (CFL START) .(g,ts,ss)\\ut.N (u,g,ts(u)) P(t,g,ts(t),g,ts(t))  dure summarization \nand may thus terminate in a lot of cases where the algorithm of this section might not.  5 Model checking \nwith summarization In this section, we describe our two-level model checking al\u00adgorithm. The algorithm \noperates on the instrumented multi\u00adthreaded program de.ned in Section 4. It also uses the partitions \n{(R (t),L(t),N (t))It ETidgde.ned in Section 4. The algorithm maintains the following relations for performing \nsummarization. Relations  P <Tid x(Global xLocal#) x(Global xLocal#) Sum <Tid x(Global xLocal#) x(Global \nxLocal#) Sum <Tid x(Global xLocal#) x(Global xLocal#) xFrame Sum.<Tid x(Global xLocal#) xFrame x(Global \nxLocal#) Mark <Tid x(Global xLocal#)  5.1 Algorithm Our model checking algorithm operates in two levels. \nThe .rst\u00adlevel reachability algorithm is similar to the algorithm in the previ\u00adous section and maintains \na set of reachable states .. But it does not use U, U+and U.directly. Instead, it calls into a second\u00adlevel \nsummarization algorithm that uses U, U+and U.to compute four relations P, Sum, Sum+and Sum. . Of these \nfour relations, the last three play roles similar to U, U+and U.and are used to communicate results back \nto the .rst-level algorithm. Ruleset 2 gives the rules for the .rst-level reachability algorithm and \nRule\u00adset 3 gives the rules for the second-level summarization algorithm.  Ruleset 3: Level II Summarization \n(CFL STEP) P(t,g1,t1,g2 ,t2)U(t,g2,t2,g3,t3),N (t,g2,t2) P(t,g1,t1,g3,t3) (CFL PUSH) P(t,g1,t1,g2,t2)U \n(t,t2,t3,f ),N (t,g2,t2) P(t,g2,t3,g2,t3) (CFL POP) P(t,g1,t1,g2,t2)U (t,t2,t3,f ),N (t,g2,t2) Sum.(t,g2,t3,f \n,g3 ,t4) P(t,g1,t1,g3,t4) (CFL SUM.) P(t,g1,t1,g2,t2)U.(t,t2,f ,t3 ),N (t,g2 ,t2) Sum.(t,g1,t1,f ,g2 \n,t3) (CFL SUM) P(t,g1,t1,g2,t2)N (t,g2,t2) Sum(t,g1,t1,g2,2)Mark(t,g1,t1) t (CFL SUM ) P(t,g1,t1,g2,t2)U \n(t,t2,t3,f ),N (t,g2,t2) Mark(t,g2,3)t Sum (t,g1,t1,g2,t3,f )Mark(t,g1 ,1) t  Let us refer to elements \nof (Global XLocal#)as nodes. Then, the relations P, Sum, Sum+and Sum.are all edges since they connect \na pair of nodes. The relation Mark is a subset of nodes. We refer to the relations Sum, Sum+and Sum.as \nsummary edges. These summary edges are computed by the summarization rules (Rule\u00adset 3). The reachability \nrules and the summarization rules commu\u00adnicate with each other in the following way. The rule (CFL START) \ncreates an edge in P for a thread t when every other thread is out\u00adside a transaction. Once summarization \nhas been initiated via (CFL START) from the .rst level, it continues for as long as a transac\u00adtion lasts, \nthat is, until the condition N (t,g2,\u00a32)becomes true of a target state (g2,\u00a32). The summary edges, Sum, \nSum+, and Sum. , generated by summarization are used by the reachability rules to do model checking, \nvia the rules STEP, PUSH and POP in Ruleset 2. The edges in P correspond to both path edges and summary \nedges in the CFL reachability algorithm for single-threaded pro\u00adgrams [19]. The rule (CFL STEP) is used \nto propagate path edges within a single procedure, and the rules, (CFL PUSH) and (CFL POP) are used to \npropagate path edges across procedure boundaries. These rules have analogs in the CFL reachability algorithm. \nFigure 6 and Figure 7 illustrate how the rules work in two situations involving function calls. In these \n.gures we assume a .xed thread identi.er t, and nodes of the form (g,\u00a3)describe the global and lo- P \ncal stores of thread t. A path edge (g,\u00a3)(g. ,\u00a3.)indicates U+(f ) . ..) that P(t,g,\u00a3,g,\u00a3.)is true; at \na call the edge (g,\u00a3)(g,\u00a3 indicates that U+(t,\u00a3,\u00a3. ,f )is true, and at a return the edge U.f ) (. (g,\u00a3)(g,\u00a3.)indicates \nthat U.(t,\u00a3,f ,\u00a3.)is true; edges la\u00ad beled with Sum, Sum+and Sum.are interpreted similarly. We ex\u00adplain \nhow to infer some edges from other edges in the .gures; the inferred edges are dashed. caller callee \n(g1,\u00a31) P . . . .(g2,\u00a32) U+(f ) (g2,\u00a33) P  P P  . . P Sum.(f ). P /  U.(f ) .  (g3,\u00a32)(g3,\u00a34) \nwhere P(t,g2,\u00a33,g2,\u00a33)is inferred by (CFL PUSH) P(t,g2,\u00a33,g3,\u00a34)is inferred by (CFL STEP) Sum.(t,g2,\u00a33,f \n,g3,\u00a32.)is inferred by (CFL SUM.) P(t,g1,\u00a31,g3,\u00a32.)is inferred by (CFL POP) Figure 6. Application of \n(CFL-POP) In Figure 6 a caller is being summarized from state (g1,\u00a31)to the point of call at state (g2,\u00a32)by \nthe path edge P(t,g1,\u00a31,g2,\u00a32). At the call, indicated by the edge U+(t,\u00a32,\u00a33,f ), a self-loop P(t,g2,\u00a33,g2,\u00a33)on \nthe entry state (g2,\u00a33)of the callee is inferred to start off a new procedure summary. After some computation \nsteps in the callee a return point (g3,\u00a34)is reached, and the sum\u00admary edge Sum.(t,g2,\u00a33,g3,\u00a32.)is inferred, \nwhich connects the en\u00adtry state of the callee to the state immediately following the return in the caller. \nFinally, the path edge P(t,g1,\u00a31,g3,\u00a32.)is inferred to connect the original state (g1,\u00a31)to the state \nfollowing the return. It is important to note that the rules are designed to handle the com\u00adplications \nthat arise from a transaction terminating inside a func\u00adtion. Due to such a transaction, a summary edge \nmay end before the return point or begin after the entry point of a callee. The rules ensure that, if \na summary for a function is only partial (i.e., it does not span across both call and return), then the \nreachability level will execute both the call and return actions, via the PUSH and POP rules. These situations \ncould involve scheduling other threads before, dur\u00ading, or after the call, as de.ned by the reachability \nrelation .. Figure 7 illustrates inference of a partial summary for a function in which a transaction \nbegins at the entry state (at (g2,\u00a33))but ends (at (g3,\u00a34)) before a return point. The end of the transac\u00adtion \nis indicated by the fact that N (t,g3,\u00a34)is true. A partial summary up to the transaction boundary is \ncached in the edge Sum(t,g2,\u00a33,g3,\u00a34), which is inferred by rule (CFL SUM). Because this summary does \nnot span across the entire call, the reachability algorithm must execute the call. This is ensured by \nthe inference of Mark(g2,\u00a33)at the same time that the fact Sum(t,g2,\u00a33,g3,\u00a34)is inferred by (CFL SUM). \nThe fact Mark(g2,\u00a33)allows the inference of Sum+(t,g1,\u00a31,g2,\u00a33,f ), which in turn is used by the reachability \nrule (PUSH) to execute the call. After executing the call, the partial summary edge Sum(t,g2,\u00a33,g3,\u00a34)is \navailable to the reachability level, via rule (STEP). At state (g3,\u00a34)a new transaction summary begins \nvia (CFL caller callee (g1,\u00a31). ...Sum+(f ) P . P U+(f ) ...  (g2,\u00a32)(g2,\u00a33)  P   . Mark . Sum \n. P /  (g3,\u00a34) N P  . Sum . . P /   (g4,\u00a35) N / / / P / / / / . / . P / Sum.(f ) . / / / P // \n/ U.(f ) .  (g5,\u00a32)(g5,\u00a36) where P(t,g2,\u00a33,g2,\u00a33)is inferred by (CFL PUSH)  Sum(t,g2,\u00a33,g3,\u00a34)is inferred \nby (CFL SUM)  Mark(t,g2,\u00a33)is inferred by (CFL SUM)  Sum+(t,g1,\u00a31,g2,\u00a33,f )is inferred by (CFL SUM+) \n Sum(t,g3,\u00a34,g4,\u00a35)is inferred by (CFL SUM)  Sum.(t,g4,\u00a35,f ,g5,\u00a32.)is inferred by (CFL SUM.)  Figure \n7. Partial procedure summaries STEP) (there is a self-loop P(t,g3,\u00a34,g3,\u00a34)which is not shown for simplicity). \nThe summary continues until the new trans\u00adaction ends at (g4,\u00a35). At that point, the summary edge Sum(t,g3,\u00a34,g4,\u00a35)is \ninferred by rule (CFL SUM). Finally, the summary Sum.(t,g4,\u00a35,f ,g5,\u00a32.)is inferred for the transition \nfrom (g4,\u00a35)across the return point at (g5,\u00a36), via rule (CFL SUM.). To summarize (!), a summary edge \n(either Sum, Sum+,or Sum.) is computed by the summarization algorithm under any one of the following \nthree conditions: When a transaction ends at an edge P(t,g1,\u00a31,g2,\u00a32)(indi\u00adcated by N (t,g2,\u00a32)), the \nrule (CFL SUM) is used to generate a Sum edge. In addition, we mark the start-state of the call using \nMark(t,g1,\u00a31).  Whenever a start state of a call is marked, the rule (CFL SUM+) generates a Sum+edge \nat every corresponding call site, and also propagates the marking to the caller. This mark\u00ad  ing can \nresult in additional Sum+edges being generated by iterated application of the rule (CFL SUM+). When a \nprocedure return is encountered, a Sum.edge is gen\u00aderated by rule (CFL SUM.).  5.2 Correctness The correctness \nof our algorithm depends on conditions C1 and C2 from Section 4. However, since this algorithm computes \nthe least .xpoint over a different set of equations, the condition C3 is mod\u00adi.ed to the following condition \nC3 . In order to state condition C3 , we de.ne the relation P(t,g1,\u00a31,g2,\u00a32).Sum(t,g1,\u00a31,g3,\u00a33) to hold \nif and only if there exists a proof tree using Ruleset 3 at whose root is an application of (CFL STEP) \nwith P(t,g1,\u00a31,g2,\u00a32) among its premises and at one of whose leaves is an applica\u00adtion of (CFL SUM) with \nSum(t,g1,\u00a31,g3,\u00a33)among its conclu\u00adsions. We de.ne P(t,g1,\u00a31,g2,\u00a32).Sum.(t,g1,\u00a31,f ,g3,\u00a33)and P(t,g1,\u00a31,g2,\u00a32).Sum+(t,g1,\u00a31,g3,\u00a33,f \n)analogously (with ap\u00adplications of (CFL SUM.) and (CFL SUM+) at the leaves, respec\u00adtively). C3 . If \nP(t,g1,\u00a31,g2,\u00a32)and L(t,g2,\u00a32), then one of the following conditions must hold: 1. P(t,g1,\u00a31,g2,\u00a32).Sum(t,g1,\u00a31,g3,\u00a33) \nfor some g3,\u00a33. 2. P(t,g1,\u00a31,g2,\u00a32).Sum.(t,g1,\u00a31,f ,g3,\u00a33) for some g3,\u00a33,f . 3. P(t,g1,\u00a31,g2,\u00a32).Sum+(t,g1,\u00a31,g3,\u00a33,f \n) for some g3,\u00a33,f .  THEOREM 2. Let (g0,\u00a3s0,U,U+ ,U.)be the instrumented mul\u00adtithreaded program. Let \n. be the least .xpoint of the rules in Rulesets 2 and 3. Let the conditions C1, C2, and C3 be satis\u00ad.ed. \nIf (g0,ls0,ss0)>.(g,ls,ss)and ls(t)wrong, then there is . . .. (g,\u00a3s,ss.)and p such that .(g,\u00a3s,ss.)and \n\u00a3s.(t)(wrong,p). Proof (Sketch) The proof of this theorem depends on the fol\u00adlowing lemmas. Lemma 1: \nIf S(g,\u00a3s,ss)and \u00a3s(t)(wrong,p), then there . . .. is (g,\u00a3s,ss.)such that S(g,\u00a3s,ss.), \u00a3s.(t)(wrong,p), \nand (g. ,\u00a3s.(u))EN (u)for all u ETid. Lemma 2: If S(g,\u00a3s,ss)and (g,\u00a3s(u))EN (u)for all u ETid, then .(g,\u00a3s,ss). \nLemma 3: If S(g,\u00a3s,ss)and (g,\u00a3s(t))EN (t), then there are g.and \u00a3.such that P(t,g. ,l. ,g,\u00a3s(t)). Lemma \n4: The condition C3 implies the condition C3. Lemma 3 is used to prove Lemma 4. Due to Lemma 4 and the \npreconditions of our theorem, the preconditions of Theorem 1 are satis.ed. If (g0,ls0,ss0)>.(g,ls,ss)and \nls(t)wrong, then from . . .. Theorem 1, we get (g,\u00a3s,ss.)and p such that S(g,\u00a3s,ss.)and .... \u00a3s.(t)(wrong,p). \nFrom Lemma 1, we get (g,\u00a3s,ss..)such that ......), \u00a3s..(t.. S(g,\u00a3s,ss)(wrong,p), and (g,\u00a3s..(u))EN (u)for \nall ......). . u ETid. From Lemma 2, we get .(g,\u00a3s,ss 5.3 Termination In this section, we present suf.cient \nconditions for the least .x\u00adpoint . of the rules in Rulesets 2 and 3 to be .nite, in which case intg=0,x=0,y=0; \nmutex m; void foo (int r) { void main() { L0: if (r == 0) { int q = choose({0,1}); L1: x = 1; M0: \nfoo(q); L2: y = 1; M1: acquire(m) L3: foo(r); M2: assert(g >= 1); } else { M3: release(m); L4: acquire(m); \nM4: return; L5: g++; } L6: release(m); } L7: return; } P= {main() }|| {main() } Figure 8. Nonterminating \nexample our summarization-based model checking algorithm will terminate. These conditions are satis.ed \nby a variety of realistic programs that use shared variables, synchronization, and recursion. In our \nnotation, a frame f EFrame corresponds to a procedure call and essentially encodes the values to which \nthe local variables (e.g., the program counter) should be set, once the procedure call returns. A frame \nf is recursive if and only if there is a transition sequence (g0,\u00a3s0,ss0)>.(g,\u00a3s,ss)and a thread t such \nthat f occurs more than once in the stack ss(t). A frame f is transactional if and only if for all transition \nsequences (g0,\u00a3s0,ss0)>.(g,\u00a3s,ss)and for all t ETid,if f occurs on the stack ss(t), then (g,\u00a3s(t))EN \n(t).If f is transactional, then in any execution, after a thread t pushes f on the stack, execution continues \nwith all states outside N (t)until f is popped. THEOREM 3. Suppose the domains Tid, Global, Local, and \nFrame are all .nite. If every recursive frame f EFrame is trans\u00adactional, then the set of reachable states \n. is .nite, and the model checking algorithm based on Rulesets 2 and 3 terminates. Proof (Sketch) Because \nthe sets Global, Local, Tid and Frame are .nite, it immediately follows that the relations P, Sum, Sum+ \n, Sum., and Mark computed by the summarization rules in Rule\u00adset 3 are .nite. Consider .rst the summarization \nrules acting on a sequence of transitions following the push of a recursive frame f . The rule (CFL SUM) \ncannot be applied to any premise of the form P(t,g,\u00a3,g. ,\u00a3.). The reason is that f is transactional, \nand there\u00adfore (g. ,\u00a3.)EN (t). Hence, no facts of the form Mark(t,g,\u00a3)or Sum(t,g,\u00a3,g. ,\u00a3.)can be deduced \ndue to any pair (g. ,\u00a3.). Because no such fact Mark(t,g,\u00a3)can be deduced, the rule (CFL SUM+), in turn, \ncannot be applied to any premise of the form P(t,g,\u00a3,g. ,\u00a3.), and hence no fact of the form Sum+(t,g,\u00a3,g. \n,\u00a3. ,f )can be deduced. Consider next the reachability rules for the set . acting on a se\u00adquence of states \nfollowing the push of a recursive frame f . By the argument above, no facts of the form Sum(t,g,\u00a3,g. \n,\u00a3.)and no facts of the form Sum+(t,g,\u00a3,g. ,\u00a3. ,f )can be deduced. It follows that only the reachability \nrules (INIT), (STEP), and (POP) can be ap\u00adplied. Because none of these rules push frames on the stacks, \nonly .nitely many facts of the form .(g,\u00a3s,ss)can be deduced from the push of a recursive frame. Since \nonly the set Stacks can be in.nite on a program and non-recursive frames can generate only .nitely many \ndistinct stacks, it follows that only .nitely many facts .(g,\u00a3s,ss)can be deduced from a program all \nof whose recursive frames are transactional. . Figure 8 shows an example program on which our two-level \nalgo\u00adrithm does not terminate. Here, the procedure foo is both recursive and not transactional due to \nthe accesses it makes to the global vari\u00adables x and y. As a result, Sum+edges are returned by the summa\u00adrization \nalgorithm for the recursive call inside procedure foo, and consequently the reachability algorithm does \nnot terminate. 5.4 Single-threaded programs In a single threaded program, we can make the set N (1)for \nthe sin\u00adgle thread contain just the initial state of the program, and the states in which the thread \nhas gone wrong. Suppose the program does not reach an error state. Then, the rule (CFL SUM) can never \nbe ap\u00adplied and the summarization rules will never generate Sum or Sum+ edges. Consequently, the reachability \nrules will never explore states in which the stack is non-empty, and the model checking algorithm with \nsummarization specializes to CFL reachability. The summary of a procedure contains only Sum.edges and \nis identical to the summary produced by the CFL reachability algorithm.  6 Related work Recently, several \npapers have used the idea of reduction to develop analyses for concurrent programs. Flanagan and Qadeer \ndevel\u00adoped a type system leveraging the ideas of reduction and transac\u00adtions to verify atomicity in multithreaded \nprograms [15, 14]. Their type system was inspired by the Calvin-R static checking tool [16]. Calvin-R \nsupports modular veri.cation of multithreaded programs by annotating each procedure with a speci.cation; \nthis speci.ca\u00adtion is related to the procedure implementation via an abstraction relation that combines \nthe notions of simulation and reduction. Re\u00adcently, Flanagan and Freund have also developed a dynamic \natom\u00adicity checker called Atomizer for multithreaded Java programs [11]. A number of data.ow techniques \nhave been devised to analyze pro\u00adgrams with both concurrency and procedure calls. Duesterwald and Soffa \nuse a system of data.ow equations to check if two statements in a concurrent program can potentially \nexecute in parallel [7]. Their analysis is conservative and restricted to Ada rendezvous con\u00adstructs. \nDwyer and Clarke check properties of concurrent programs by data.ow analysis, but use inlining to .atten \nprocedure calls [8]. Flow-insenstive analyses are independent of the ordering between program statements \nand can be generalized easily to multithreaded programs with procedure calls. Rinard presents a survey \nof tech\u00adniques for analysis of concurrent programs [20]. Ramalingam proved the undecidability of assertion \nchecking with both concurrency and procedure calls [18]. The proof is by re\u00adduction from the undecidable \nproblem of checking the emptiness of the intersection of two context-free languages. Bouajjani, Es\u00adparza, \nand Touili present an analysis that constructs abstractions of context-free languages [4]. The abstractions \nare chosen so that the emptiness of the intersection of the abstractions is decidable. Their analysis \nis sound but incomplete due to overapproximation in the abstractions. In contrast, our work operates \non the concrete mul\u00adtithreaded program and uses summaries to gain reuse, scalability, and termination \nin a number of cases. Alur and Grosu have studied the interaction between concurrency and procedure calls \nin the context of re.nement between STATE-CHART programs [1]. At each step of the re.nement process, \ntheir system allows either the use of nesting (the equivalent of proce\u00addures) or parallelism, but not \nboth. Also, recursively nested modes are not allowed. In contrast, we place no restrictions on how par\u00adallelism \ninteracts with procedure calls, and allow recursive proce\u00addures. For restricted models of synchronization, \nsuch as fork-join synchro\u00adnization, assertion checking is decidable even with both concur\u00adrency and procedure \ncalls. Esparza and Podelski present an algo\u00adrithm for this restricted class of programs [9]. Counter \nmachines and variants of Petri nets have been used to check assertions on concurrent programs with unbounded \nnumber of threads [6, 2]. However, these methods handle procedure calls by inlining. 7 Conclusions We \nhave presented a novel model checking algorithm to check as\u00adsertions on multithreaded programs with procedure \ncalls. Inspired by procedure summarization in sequential programs, our algorithm attempts to use summaries \nto obtain reuse and scalability. Our algo\u00adrithm functions in two levels. The .rst level performs reachability \nanalysis and maintains an explicit stack for each thread. The sec\u00adond level computes a summary for each \nprocedure. Under certain conditions (stated precisely in Theorem 3), we guarantee that our two-level \nalgorithm will terminate even in the presence of recursion and concurrency. We are currently implementing \nour algorithm in a new model checker called ZING being developed at Microsoft Re\u00adsearch. 8 References \n[1] R. Alur and R. Grosu. Modular re.nement of hierarchic re\u00adactive machines. In POPL 00: Principles \nof Programming Languages, pages 390 402. ACM, 2000. [2] T. Ball, S. Chaki, and S. K. Rajamani. Parameterized \nver\u00adi.cation of multithreaded software libraries. In TACAS 01: Tools and Algorithms for Construction \nand Analysis of Sys\u00adtems, LNCS 2031, pages 158 173. Springer-Verlag, 2001. [3] T. Ball and S. K. Rajamani. \nBebop: A symbolic model checker for Boolean programs. In SPIN 00: SPIN Workshop, LNCS 1885, pages 113 \n130. Springer-Verlag, 2000. [4] A. Bouajjani, J. Esparza, and T. Touili. A generic approach to the static \nanalysis of concurrent programs with procedures. In POPL 03: Principles of Programming Languages, pages \n62 73. ACM, 2003. [5] M. Das, S. Lerner, and M. Seigle. ESP: Path-sensitive pro\u00adgram veri.cation in polynomial \ntime. In PLDI 02: Program\u00adming Language Design and Implementation, pages 57 69. ACM, 2002. [6] G. Delzanno, \nJ-F. Raskin, and L. V. Begin. Towards the auto\u00admated veri.cation of multithreaded Java programs. In TACAS \n02: Tools and Algorithms for Construction and Analysis of Systems, LNCS 2280, pages 173 187. Springer-Verlag, \n2002. [7] E. Duesterwald and M. L. Soffa. Concurrency analysis in the presence of procedures using a \ndata-.ow framework. In TAV 91: Testing, Analysis and Veri.cation, pages 36 48. ACM, 1991. [8] M. Dwyer \nand L. Clarke. Data .ow analysis for verifying properties of concurrent programs. In FSE 94: Foundations \nof Software Engineering, pages 62 75. ACM, 1994. [9] J. Esparza and A. Podelski. Ef.cient algorithms \nfor pre* and post* on interprocedural parallel .ow graphs. In POPL 00: Principles of Programming Languages, \npages 1 11. ACM, 2000. [10] J. Esparza and S. Schwoon. A BDD-based model checker for recursive programs. \nIn CAV 01: Computer Aided Ver.ciation, LNCS 2102, pages 324 336. Springer-Verlag, 2001. [11] C. Flanagan \nand S. N. Freund. Atomizer: A dynamic atomic\u00adity checker for multithreaded programs. In POPL 04: Princi\u00adples \nof Programming Languages. ACM, 2004. [12] C. Flanagan and S. Qadeer. Thread-modular model checking. In \nSPIN 03: SPIN Workshop, LNCS 2648, pages 213 225. Springer-Verlag, 2003. [13] C. Flanagan and S. Qadeer. \nTransactions for software model checking. In SoftMC 03: Software Model Checking Work\u00adshop, 2003. [14] \nC. Flanagan and S. Qadeer. A type and effect system for atom\u00adicity. In PLDI 03: Programming Language \nDesign and Im\u00adplementation, pages 338 349. ACM, 2003. [15] C. Flanagan and S. Qadeer. Types for atomicity. \nIn TLDI 03: Types in Language Design and Implementation, pages 1 12. ACM, 2003. [16] S. N. Freund and \nS. Qadeer. Checking concise speci.cations for multithreaded software. In FTfJP 03: Formal Techniques \nfor Java-like Programs, 2003. [17] R. J. Lipton. Reduction: A method of proving properties of parallel \nprograms. In Communications of the ACM, volume 18:12, pages 717 721, 1975. [18] G. Ramalingam. Context \nsensitive synchronization sensitive analysis is undecidable. ACM Trans. on Programming Lan\u00adguages and \nSystems, 22:416 430, 2000. [19] T. Reps, S. Horwitz, and M. Sagiv. Precise interprocedural data.ow analysis \nvia graph reachability. In POPL 95: Princi\u00adples of Programming Languages, pages 49 61. ACM, 1995. [20] \nM. C. Rinard. Analysis of multithreaded programs. In SAS 01: Static Analysis, LNCS 2126, pages 1 19. \nSpringer-Verlag, 2001. [21] M. Sharir and A. Pnueli. Two approaches to interprocedural data .ow analysis. \nIn Program Flow Analysis: Theory and Applications, pages 189 233. Prentice-Hall, 1981. [22] B. Steffen \nand O. Burkart. Composition, decomposition and model checking optimal of pushdown processes. Nordic Jour\u00adnal \nof Computing, 2(2):89 125, 1995.  \n\t\t\t", "proc_id": "964001", "abstract": "The ability to summarize procedures is fundamental to building scalable interprocedural analyses. For sequential programs, procedure summarization is well-understood and used routinely in a variety of compiler optimizations and software defect-detection tools. However, the benefit of summarization is not available to multithreaded programs, for which a clear notion of summaries has so far remained unarticulated in the research literature.In this paper, we present an intuitive and novel notion of procedure summaries for multithreaded programs. We also present a model checking algorithm for these programs that uses procedure summarization as an essential component. Our algorithm can also be viewed as a precise interprocedural dataflow analysis for multithreaded programs. Our method for procedure summarization is based on the insight that in well-synchronized programs, any computation of a thread can be viewed as a sequence of transactions, each of which appears to execute atomically to other threads. We summarize within each transaction; the summary of a procedure comprises the summaries of all transactions within the procedure. We leverage the theory of reduction [17] to infer boundaries of these transactions.The procedure summaries computed by our algorithm allow reuse of analysis results across different call sites in a multithreaded program, a benefit that has hitherto been available only to sequential programs. Although our algorithm is not guaranteed to terminate on multithreaded programs that use recursion (reachability analysis for multithreaded programs with recursive procedures is undecidable [18]), there is a large class of programs for which our algorithm does terminate. We give a formal characterization of this class, which includes programs that use shared variables, synchronization, and recursion.", "authors": [{"name": "Shaz Qadeer", "author_profile_id": "81100286660", "affiliation": "Microsoft Research, Redmond, WA", "person_id": "PP14106781", "email_address": "", "orcid_id": ""}, {"name": "Sriram K. Rajamani", "author_profile_id": "81100468626", "affiliation": "Microsoft Research, Redmond, WA", "person_id": "P266638", "email_address": "", "orcid_id": ""}, {"name": "Jakob Rehof", "author_profile_id": "81100614854", "affiliation": "Microsoft Research, Redmond, WA", "person_id": "P131488", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/964001.964022", "year": "2004", "article_id": "964022", "conference": "POPL", "title": "Summarizing procedures in concurrent programs", "url": "http://dl.acm.org/citation.cfm?id=964022"}