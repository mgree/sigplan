{"article_publication_date": "01-01-2004", "fulltext": "\n Incremental Execution of Transformation Speci.cations Ganesh Sittampalam Oege de Moor Ken Friis Larsen \nOxford University Computing Oxford University Computing IT University of Copenhagen Laboratory Laboratory \nken@friislarsen.net ganesh@comlab.ox.ac.uk oege@comlab.ox.ac.uk Abstract We aim to specify program transformations \nin a declarative style, and then to generate executable program transformers from such speci.cations. \nMany transformations require non-trivial program analysis to check their applicability, and it is prohibitively \nexpen\u00adsive to re-run such analyses after each transformation. It is desir\u00adable, therefore, that the analysis \ninformation is incrementally up\u00addated. We achieve this by drawing on two pieces of previous work: .rst, \nBernhard Steffen s proposal to use model checking for certain anal\u00adysis problems, and second, John Conway \ns theory of language fac\u00adtors. The .rst allows the neat speci.cation of transformations, while the second \nopens the way for an incremental implementa\u00adtion. The two ideas are linked by using regular patterns \ninstead of Steffen s modal logic: these patterns can be viewed as queries on the set of program paths. \nCategories and Subject Descriptors: D.3.4 [Processors]: Opti\u00admization, Incremental compilers, Translator \nwriting systems and compiler generators, D.3.3 [Language Constructs and Features]: Patterns, D.3.2 [Language \nClassi.cations]: Constraint and logic languages General Terms: Algorithms, Languages, Theory. Keywords: \nprogram transformation, transformation speci.cation, incremental algorithm, program analysis, logic programming, \ncon\u00adstraints, language factors, residuation operators. 1 Background Our aim is to create a toolkit for \neasy speci.cation of program trans\u00adformations, and for experimenting with their use. The motivating application \nof such a toolkit are domain-speci.c optimisations, typ\u00adically written by the author of a software library \n[14, 34]. Further- Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed more, we hope \nto use the same framework for the formal study of transformations: proofs of correctness, prediction \nof pro.tabil\u00adity, and interaction between different transformations. Work in this area was pioneered \nby Whit.eld and Soffa [38], and we shall adopt a style of speci.cation that is close to theirs. The contribution \nof this paper is to show how the speci.cations can be implemented via an incremental interprocedural \nanalysis algorithm. Our approach to incremental analysis is to proceed in a composi\u00adtional manner. For \nexample, to analyse the program P1; P2(i.e. the program P1 followed by the program P2), one would compose \nthe results of analysing P1 and P2 separately. This leads to incremental analysis in the sense that if \nP2 were changed then the analysis of P1 does not need to be re-computed. To illustrate the type of analysis \nbeing considered, take common subexpression elimination. It transforms a use of the expression E into \nX if it is known that on all paths to E, the value of E has been assigned to X, and neither E nor X has \nbeen changed. This requirement can be formalised by saying that all paths to the use of E from program \nentry satisfy the regular pattern: {}.; {assign(X,E),pure(E),not(triv(E)),not(occurs(X,E))}; {not(def \n(X)),transparent(E)}.; {use(E)} To wit, any path from program entry to the transformation point consists \nof four parts. First, there are zero or more statements that we make no conditions on, indicated by {}. \nNext, we encounter an assignment of the form X :=E, where E is pure (free of side effects), not trivial \nand X is not used in E. This assignment is fol\u00adlowed by zero or more statements that do not rede.ne X \nand that also leave the value of E unchanged. Finally, we encounter a use of E, which is now a candidate \nfor replacement by X. As another example transformation, consider conditional constant propagation. Here \nwe aim to replace a use of X by a constant C. Naturally this is only permitted if X has the value C on \nall paths from program entry to the transformed use of X. As a .rst attempt, we might write the pattern: \n{}.; {assign(X,C),const(C)}; {not(def (X))}.; {use(X)} for pro.t or commercial advantage and that copies \nbear this notice and the full citation The paths that we consider are statically computed, so it is in\u00adon \nthe .rst page. To copy otherwise, to republish, to post on servers or to redistribute evitable that there \nare some infeasible paths amongst them that can\u00ad to lists, requires prior speci.c permission and/or a \nfee. not occur during actual program runs. A well-known problem are POPL 04, January 14 16, 2004, Venice, \nItaly. Copyright 2004 ACM 1-58113-729-X/04/0001 ...$5.00 programs where X :=C is followed by a test whether \nX =C: in that case the else branch can be safely disregarded, but the above pat\u00adtern would fail if there \nwas an assignment to X in the unreachable else branch. Constant propagation might be more often applicable \nif we rule out such infeasible paths, by taking the disjunction of the above pattern and {}.; {assign(X,C),const(C)}; \n{not(def (X))}.; (({test(X =C)}; {else branch})1 ({test(X C)}; {then branch})); = {}. The choice between \ntwo patterns is written 1. The predicate else branch matches a dummy statement at the beginning of each \nelse branch; then branch is de.ned similarly. Note that this tech\u00adnique of ruling out infeasible paths \nis in fact quite generally ap\u00adplicable, not just to constant propagation; other common causes of infeasibility \nare also easily expressed. The structure of the paper is as follows. Section 2 gives some fur\u00adther details \nof how we specify transformations. We then turn to the problem of incremental evaluation of regular path \nqueries. In Section 3, we outline how a simpli.ed version of the problem can be solved incrementally \nvia Conway s theory of language factors. It is then shown how this partial solution can be generalised \nto the full problem in Section 4. Next, the implementation is discussed in some detail in Section 5, \nand we report preliminary performance experiments. Finally, the results are evaluated in the context \nof re\u00adlated work in Section 6.  2 Detailed examples Our system transforms programs in a subset of the \n.NET interme\u00addiate language (IL), which is an intermediate representation similar to Java bytecode [23]. \nFollowing the example of GRIMP in the SOOT toolkit [33] our transformations operate on a higher-level \nrepresentation of IL, namely syntax trees where program structure (conditionals and loops) has been made \nexplicit. The transforma\u00adtion engine itself is implemented in Standard ML. The user speci.es an individual \ntransformation named trans as a relation trans(In,Out)in Prolog, where In is the original program fragment, \nand Out represents its transformed counterpart. To facil\u00aditate the task of de.ning transformations, we \nhave augmented Pro\u00adlog with facilities for incremental evaluation of regular path queries. Furthermore \nthe Prolog programmer can query the results of other analyses that were directly implemented in ML. The \ntransformations can then be combined through a strategy script, which speci.es how the source program \nis traversed, and how the transformations are sequenced. One can freely combine transfor\u00admations speci.ed \nin Prolog and primitive transformations written in Standard ML. Our strategy language has been borrowed \nfrom Stratego [36]. The difference is that in Stratego, the transformation relation is speci.ed by pure \nrewrite rules. The .rst two examples below are exactly as implemented in our system. In the third example, \nwe have suppressed some awkward details of working with IL. 2.1 Atomic propagation Atomic propagation \nis a generalisation of constant propagation that also propagates assignments of the form X :=Y, where \nY is a vari\u00adable. In our extended version of Prolog, it is expressed as follows: atomic prop(E,F) : base \ninstr(E), fromentry ( {}.; {.assign atomic(V,A,T)}; {cnot(.def var(V))}.; {.use var type(V,T)} ), subst(A,expr \ntype(localvar(V),T),E,F). The argument E is the statement that we wish to transform; F is its transformed \ncounterpart. This optimisation will only be ap\u00adplied to atomic statements, and this is checked with the \npredicate base instr(E). The keyword fromentry introduces a pattern that must be satis\u00ad.ed by all paths \nfrom program entry to the program point where atomic prop is being evaluated. This path query states \nthat E is dominated by (possibly multiple occurrences of) the assignment V :=A, where A is a variable \nor a constant. Because the repre\u00adsentation of IL is explicitly typed, we also record the type T of A. \nThe tick marks (.)in front of the constituent predicates represent a subtlety that we chose to ignore \nin the introduction. A predicate such as use var type(V,T,S)is in fact a property of a statement S. The \ntick mark is a device to make the parameter S implicit, be\u00adcause in the path query there is nothing to \nbind it to: it only comes into play when the predicate is matched to a concrete atomic state\u00adment, during \nthe evaluation of the path query. Predicates such as def var(X,S)can be de.ned by the user. A naive implementation \nwould only match assignments S where X occurs on the left-hand side of an assignment. A more sophisticated \nde.nition queries the result of a may-alias analysis implemented in ML. The negation op\u00aderator cnot negates \na constraint, and it is thus subtly different from the not operator normally found in Prolog. The use \nof constraints is fully explained in Section 4. Finally, if all the above conditions are satis.ed, we \ncreate the new statement, using subst(S,T,E,F). This predicate replaces all occur\u00adrences of the subexpression \nT in E by S, yielding the result F. Note that we have not compromised the declarative nature of Prolog. \nThe new program fragment F is created here, but the actual modi.cation of the source program is done \nvia a strategy script. 2.2 Dead assignment elimination So far we have only considered conditions on \nall paths from pro\u00adgram entry to the point of transformation. The converse direction, paths to the program \nexit, is also useful. An example in point is dead assignment elimination: dead code(instr(Labs,E,Annot), \ninstr(Labs, exp(expr type(apply(nop,nil,void))), Annot)) : not(nonterminal(E)), toexit({.assign var(V,Exp, \n),pure(Exp),atnode(N)}; ({cnot(.use var(V))cor atnode(N)}).; (e 1 ({.def var(V),cnot(.use var(V))}; {}.))). \nThis speci.es when an assignment E =(V :=Exp)at program point N can be replaced by a skip statement \nin IL, this is represented by the nop instruction. As an aside, note that the Annot part of an instruction \nis typically used for storing results of ML analyses that can then be accessed by Prolog predicates. \nAgain this transformation only applies at the leaves of the abstract syntax tree. The toexit pattern \nsays that there are no uses of the vari\u00adable V until we encounter the program exit (the e case) or another \nde.nition of V that does not use V itself. Because the assignment may occur in a loop, a use of V at \nN should be disregarded: other\u00adwise the assignment could keep itself alive. This is checked using the \nprimitive predicate atnode. The primitive connective cor com\u00adputes the logical disjunction of its argument \nconstraints. 2.3 Strength reduction Consider a loop L =while(Cond,Body). Furthermore, assume that each \nexecution of the Body increments I by one at least once, and that there are no other assignments to I \nexcept these increments. The key step of strength reduction is to identify an expression I *C in the \nbody, where C is not changed in the loop. If these conditions are satis.ed, we can replace the original \npro\u00adgram fragment by (X :=I *C;while(Cond,Body..)). The new ver\u00adsion Body..of the loop body is the same \nas the original Body, except that each occurrence of I *C has been replaced by X. Furthermore, after \nevery increment of I we insert the corresponding assignment X :=X +C. One might argue that this transformation \nwould still be correct even if I is not changed at all in the loop. Without this condi\u00adtion, however, \nstrength reduction is likely to interfere with invariant hoisting, which seeks to hoist maximal invariant \nexpressions, and I *C may be non-maximal. Our intuitive understanding of strength reduction can be translated \ninto the speci.cation language as follows: strength red(while(Cond,Body), seq(Init,while(Cond,Body..))) \n : paths(Body,{cnot(.def var(I))cor .incr(I)}.; {.incr(I)}; {cnot(.def var(I))cor .incr(I)}.), paths(Cond,{cnot(.def \nvar(I))}.), times(E,I,C), occurs(E,Body),pure(C), paths(Body,{transparent(C)}.), paths(Cond,{transparent(C)}.), \nfresh var(int,X),subst(X,E,Body,Body.), incr(I,S),plus(PC,X,C),assign var(X,PC,int,XPC), subst(seq(S,XPC),S,Body. \n,Body..), assign var(X,E,int,Init). This transformation relation maps a while statement to an initial\u00adisation \nfollowed by a transformed while statement. The primitive paths(B,P)states that all paths through B satisfy \nthe pattern P. Here we check that I is incremented by one on every path through the body, and that such \nincrements are the only assignments to I. Next, the predicate times(E,I,C)binds E to an expression that \nis the mul\u00adtiplication of I and C. Given such an E, we look for occurrences of E in the Body. At this \nstage in the query, C is bound to a ground term, and we can verify that it is pure (free of side effects). \nNext it is checked that all paths through the loop are transparent to C, in other words that the value \nof C does not change throughout the loop.    When all these conditions are satis.ed, a new variable \nname X is generated, and all occurrences of E =I *C in the Body are replaced by X. Furthermore all increments \nto I are followed by an update of the value of X, so that the invariant X =I *C is maintained through\u00adout \nthe loop. Finally, we construct the initialisation, which sets X to I *C at the beginning of the loop. \nThe construction of the new code in is admittedly somewhat clumsy in Prolog, and we plan to rem\u00adedy that \nusing the mechanism of [35], which focuses on merging the concrete syntax of an object language into \nProlog in a natural way. Further example speci.cations in notations very close to this paper can be found \nin [12, 17, 18, 19, 21].  3 Chips and chops for incremental checking We regard our transformations as \nrewrite rules that operate on the abstract syntax tree of a program. This deviates from the accepted \npractice in optimising compilers, where one transforms basic blocks in the .ow graph. Each rewrite rule \nhas an applicability condition, speci.ed as a regular pattern. The problem of checking the ap\u00adplicability \ncondition of a transformation boils down to checking a language containment, namely: program paths Cpattern \nThe pattern may contain free meta-variables (as opposed to pro\u00adgram variables) that we wish to solve \nfor. The program paths in\u00adclude at least all sequences of atomic statements that may occur during program \nexecution. In order to obtain an accurate analysis, it is desirable to constrain the language of program \npaths as much as possible. We stipulate, therefore, that it is given by a context-free grammar, so that \nrecur\u00adsive procedures can be modelled accurately. The pattern, by con\u00adtrast, is a regular expression. \nTo demonstrate the main ideas of our approach, we outline how the above problem can be solved incrementally. \nIn doing so, we make three temporary simplifying assumptions. First, the alpha\u00adbet of the pattern is \nthe same as that of the program, consisting of atomic statements. Second, the pattern should not contain \nmeta\u00advariables. Finally, the object language is restricted to structured programs without procedures. \nIn Section 4 it is shown how to pro\u00adcess queries where the alphabet consists of propositions that may \ncontain free variables: each such proposition describes a property of an atomic statement. The generalisation \nto interprocedural anal\u00adysis is considered in Section 5. 3.1 Chip and chop Recall from the introduction \nthat our approach to incrementality is to devise a compositional analysis, so that when P1; P2 is analysed, \nand either of P1 or P2 is changed, the other one does not need to be re-analysed. More formally, we would \nlike to have an algorithm B that satis.es the speci.cation B pat prog =prog Cpat, and that is compositional \nin the following sense: B pat (prog1; prog2)=B pat prog1 0B patprog2 B pat (prog1 1prog2)=B pat prog2 \nEB patprog2 B pat (prog.)=(B pat prog). for appropriate operators 0, Eand .. A moment s re.ection shows, \nhowever, that no such algorithm B can exist: the Boolean result does not carry enough information to \nvalidate the above equations. R R\\S  S Figure 1. The chip R\\S. S/R R  S Figure 2. The chop SIR. For \nexample, let us suppose that pat is a ; b. Then, if prog1 is a and prog2 is b then false 0false would \nbe true. However, if prog1 is a and prog2 is c then false 0false would be false. To attain compositionality, \nit will be necessary to generalise from just a single pattern to all the parts of that pattern. If we \nhave solved the problem for the parts, it would be possible to paste par\u00adtial solutions together after \neach transformation, to obtain a solu\u00adtion for the whole pattern. We shall need a set of results, namely \nB pat.prog for each part pat. such strengthening of the induc\u00adtion hypothesis is very common when trying \nto achieve a compo\u00adsitional algorithm. But what is the appropriate notion of part for a regular language? \nTo answer this question, we .rst introduce the chip and chop operators, and use these to give a de.nition \nof part . We de.ne the chip operator \\on languages R and S as follows: VT : T CR\\S R ; T CS Informally, \none could think of the chip R\\S as S with R chipped off from the front. This is illustrated by the diagram \nin Figure 1. Throughout this section, we shall use such diagrams to aid intu\u00adition, but the reader is \nwarned not to use them as a formal basis for reasoning. For example, from the pictorial interpretation, \none might think that R\\R =e (the empty word), but in fact we have .\\a.. a=a. There is a dual operator \nto chip, which is called chop and denoted I. Chop is de.ned by the equivalence VT : T CSIR T ; R CS One \ncould think of the chop SIR as S with R chopped off from the back, as shown in Figure 2. These operators \nwere studied in the context of regular algebra by John Conway, who named them factors [9]. More generally \nthey are known as residuation operators and they feature in many differ\u00adent areas of theoretical computer \nscience, ranging from software speci.cation, through non-commutative linear logic to Lambek grammars \nin computational linguistics. The authors .rst learned of these operators and their theory from Roland \nBackhouse [2, 3, 4]. For the present purposes, there are certain important facts about chip and chop. \nFirst, a language S is regular if and only if its number of chops (that is the cardinality of {SIR 1R \nis any language})is .nite. Second, chips and chops naturally lead to the desired notion of part of a \nlanguage. A part of S is something that could occur X X\\S/Y Y Figure 3. X\\SIY is a part of S. R0 X=S/R0 \nX\\S/Y Y=R1  Figure 4. Any part as the chip of two chops. as a contiguous segment in the middle of S, \nthat is a language of the form (X\\S)IY. As suggested by the diagram in Figure 3, chip and chop associate \nwith each other: (X\\S)IY =X\\(SIY) To illustrate our de.nition of parts, the parts of the single word \nS =abc are given by the set {0/,e,a,ab,abc,b,bc,c,(a1b1c).} The reader may wish to check for herself \nthat the regular language S =((a.; b)1(b ; b.; a)).has four chops, and eight distinct parts. The following \nequation says that any such part of S can be obtained as the chip of two chops: VX,S,Y : CR0,R1: X\\SIY \n=(SIR0)\\(SIR1) This equation is illustrated in Figure 4 again, this is merely to build intuition, and \na formal proof uses the fact that the operators (SI )and ( \\S)are Galois adjoints. Since we know the \npattern has a .nite number of chops, there are a .nite number of parts of the pattern. In fact, if we \nwere to list all pairs of chops, the chips of these pairs would be all the parts of the pattern. Inspired \nby these facts, Conway de.nes the chip-chop matrix of a .xed regular language S. This matrix M(S)is indexed \nby the .nite set of chops of S, and the individual entries are given by M(S)X=X\\Y xY One could think \nof M(S)as a systematic organisation of all the parts of S. In particular S itself occurs as at least \none of the entries of M(S). We are now ready to generalise from the original problem (which was to determine \nwhether all program paths are in S) to a problem that is easier to solve compositionally, namely whether \nall program paths are in each part of S. That is, for a set of program paths P,we de.ne a Boolean matching \nmatrix B(P), indexed by the chops of S: B(P)xXY =P CM(S)XxY Since M(S)contains all the parts of S, this \nmatrix answers the ques\u00adtion whether P CS.for any part S.of S, in particular, for S itself. It turns \nout that B can be computed as a homomorphism of gram\u00admars, that is the compositional form that we set \nout to achieve: B(P0; P1)=B(P0).B(P1) Stat0 ::=if Expr then Stat1 else Stat2 Stat0.paths =Expr.paths \n; (Stat1.paths 1Stat2.paths) Expr.to =Stat0.to Expr.from =(Stat1.paths 1Stat2.paths); Stat0.from Stat1.to \n=Stat0.to ; Expr.paths Stat2.to =Stat0.to ; Expr.paths Stat1.from =Stat0.from Stat2.from =Stat0.from \nStat0 ::=while Expr do Stat1 Stat0.paths =Expr.paths ; (Stat1.paths ; Expr.paths). Expr.to =Stat0.to \n; (Expr.paths ; Stat1.paths). Expr.from =(Stat1.paths ; Expr.paths).; Stat0.from Stat1.to =Stat0.to ; \nStat0.paths Stat1.from =Stat0.paths ; Stat0.from Figure 5. Fragment of attribute grammar B(P0 1P1)=B(P0)!B(P1) \nwhere .designates multiplication of Boolean matrices (taking Vfor addition and !for multiplication), \nand !is pointwise conjunction. Boolean matrices are partially ordered by the pointwise implication order, \ni.e. A <B Vi,j : Aixj Bixj Since least upper bounds of regular expressions are mapped to greatest lower \nbounds of matrices, it follows that least .xpoints of grammars are mapped to greatest .xpoints on Boolean \nmatrices. In particular, we have that B(P.)=P. where P.is the greatest solution X =P.of X =B(e)!B(P).X \nWe conclude that B can be regarded as a homomorphism of regu\u00adlar algebras. Of course the laws of regular \nalgebra do not hold on arbitrary Boolean matrices, but they do on the range of B. Proofs of the above \nresults are straightforward, and omitted for reasons of space. The interested reader is referred to Conway \ns monograph [9]; in particular the above equation for B(P0; P1)can be proved via Theorem 6 (iii) on Page \n50. 3.2 Attributes How does this help with incremental program analysis? We identify program points \nwith corresponding nodes in the syntax tree. At the potential application point for a transformation, \nwe would like to compute three attributes representing a language: paths all paths that can be taken \nby the tree rooted at this node to all paths to this point from program entry from all paths from this \npoint to program exit In the terminology of attribute grammars [16], paths is a synthe\u00adsised attribute, \nwhile to and from are inherited. A fragment of the relevant attribute grammar is shown in Figure 5. The \nfragment is much simpli.ed in that it does not deal with break and continue statements our implementation \ndoes cope with these. Now consider how these attributes could be used in discharging a fromentry predicate. \nSuppose that S is a pattern that describes paths from program entry, and that we wish to check if S applies \nat a statement P. The paths from program entry up to and including P can be calculated by taking the \npaths from program entry up to P and attaching the paths that P itself can take. Thus, the containment \nthat needs to be checked is (P.to ; P.paths)CS In the preceding paragraphs, we outlined an algorithm \nwhere this containment can be checked while building up the left-hand side regular expression. Hence, \nfollowing the shape of the attribute grammar in Figure 5, we can introduce three attributes S paths, \nS to and S from for checking the relevant containments. Each of these attributes is a Boolean matrix. \n  To compute this information incrementally, we make one pass over the syntax tree prior to the transformation \nprocess, labelling each node with its S paths attribute. Next, we apply the transformations. To attempt \na transformation, we walk from the root to the node that we wish to transform, com\u00adputing the inherited \nattributes S to and S from as we go along. Such tree walks are a natural part of a transformation system \nthat oper\u00adates on abstract syntax trees. In our system, the precise nature of the walk is speci.ed in \nthe strategy language (which is borrowed from Stratego [36]). After each subtree replacement, we naturally \nhave to re-compute the S paths information on any new nodes. Furthermore, it is nec\u00adessary to recompute \nthis attribute on all nodes to the root of the tree, but nowhere else. The new S paths values can then \nbe used to compute new S to and S from information at a new potential trans\u00adformation point, by walking \ndown from the root. 3.3 Ef.ciency analysis We now analyse the ef.ciency of the above procedures, assuming \nwe are applying a simple transformation (such as constant propaga\u00adtion or dead code elimination) that \nhas a single path query S as the side condition. There are three stages to consider: the preprocess\u00ading, \nwhere the tree is decorated with S paths attributes; the search for a point to apply the transformation; \nand .nally, the subtree re\u00adplacement. Typically such a search is made by one sweep over the tree, applying \nthe transformation wherever possible. Let N be the size of the program under consideration. The ini\u00adtial \npreprocessing pass over the syntax tree takes time O(c(S).N), where c(S)is the cost of an operation (multiplication, \naddition or closure) on Boolean matrices. This cost depends on S because the dimension of the chip-chop \nmatrix is the number of chops (which equals the number of chips). We shall consider an upperbound on \nc(S)shortly. Next one needs to walk the tree in search of places to apply the transformation. Each move \n(up, down or to a sibling) takes O(c(S)) Boolean operations, and there are O(N)such moves in a complete \nsweep. It follows that the total time spent in searching for subtrees to transform is bounded by O(c(S).N). \nWhen the transformation is applied, one subtree is replaced by an\u00adother. The new subtree needs to be \nattributed with fresh S paths at\u00adtributes, and in the worst case this will take time O(c(S).Q), where \nQ is the number of new nodes in the subtree. All the S paths at\u00adtributes on the path from the subtree \nto the root need to be recom\u00adputed as well, for a total of O(c(S).(Q +D))where D is the depth at which \nthe subtree replacement occurred. Note that it is not nec\u00adessary to re-start the tree walk to identify \nthe location of the next transformation: we can continue from the point of the last rewrite. In this \ncase one needs to walk down again, along the path from the root, to the new subtree, computing S to and \nS from attributes along the way. The cost of this is O(c(S).D), so there is no increase in asymptotic \ncomplexity.      It remains to estimate an upperbound on c(S). Note that c(S)is a quantity that \ndepends on the pattern S, in fact on the number of chops C. The chip-chop matrix has C2 entries. It is \nclear that the implementation of (.)is O(C3)(using naive matrix multiplication) and (!)takes O(C2)steps. \nLet P be a program (a set of sequences of atomic statements). To establish a bound on the closure operator \nP.,de.ne F(T)=B(e)!B(P).T We compute the greatest .xpoint of F by iterating from B(e).The question is \nhow many iterations suf.ce to reach that .xpoint. To answer that, consider a part X of S. After n steps \nof the above iteration, the matrix entry corresponding to X will tell us whether e 1P 1P2 1P3 1...1Pn \nCX Another way of thinking about this is that we determined what states of the deterministic automaton \nA for X are reachable via pow\u00aders of P up to n +1, and then whether all such reachable states are .nal \nin A. In the worst case, each iteration of the .xpoint computa\u00adtion adds only one more state to those \nthat were reachable before: the number of iterations to reach the correct entry at the position corresponding \nto part X in P.is thus bounded by the number of states in A. Because each such state corresponds to a \nchip w\\X of S where w is a word (via the well-known derivatives of Brzozowski [7]), the number of states \nin A is at most the number of chips of X, say CX . Since each part of X is also a part of S, we have \nCX <C. It thus follows that C iterations suf.ce to reach a .xpoint. As each iteration takes O(C3)steps, \nwe have c(S)=O(C4) One may now wonder whether there is a bound on C given S.It is rather hard to give \na tight bound. Let k be the size of the minimal deterministic automaton A for S. Above we remarked that \nk <C. Conway proves that in the worst case C =2k, but this only hap\u00adpens in pathological examples. In \npractice the patterns that arise in program transformation seem to have C =.(k). Summarising the above \ndiscussion, the number of Boolean opera\u00adtions for P applications of a transformation (each of which intro\u00adduces \nat most Q new nodes in the tree), during one sweep of a program of size N whose syntax tree has depth \nD is O(C4 .(N +P .(Q +D))) Here C is the number of chops of the regular pattern. With regard to space \ncomplexity, every node in the syntax tree needs to be dec\u00adorated with a matrix, so the space requirements \nare O(C2 .N). It is interesting to compare the above time complexity bound with the best known non-incremental \nalgorithm [22]. That algorithm takes O(k .E +x .N)steps, where k is the number of states in deter\u00administic \nautomaton for the pattern, E the number of .ow edges in the program, x the number of transitions in the \npattern, and N the number of program statements. An estimate for the application of transformations via \nthe non-incremental method is therefore O(P .(k .E +x .N)) We conclude that the incremental algorithm \nwill outperform the straightforward method on all but the smallest instances.  4 Solving for propositions \nand variables Thus far, we have developed an incremental method for getting a true or false answer to \nthe language containment problem: paths Cpattern While this is closely related to the problem we started \noff with, it is a simpli.cation in at least two ways. First, in our program anal\u00adysis examples, the alphabet \nof the pattern consists of propositions. Unlike traditional symbols in formal language theory, two of \nthese propositions can match a program statement simultaneously. Sec\u00adond, the pattern may contain free \nvariables, and thus in general we wish to compute a constraint on the values of these variables, in\u00adstead \nof just a Boolean answer. In this section, we shall show how this more general problem can be formalised. \nNext, we shall discover that the generality is only super.cial: in fact, the general problem can be reduced \nto an in\u00adstance of the simple containment, by suitable manipulation of the pattern. 4.1 Formal speci.cation \nTo formalise the general problem, we give symbolic names to the propositions in the pattern. For example, \nrecall the pattern for (the simplest form of) constant propagation: {}.; {assign(X,C),const(C)}; {not(def \n(X))}.; {use(X)} There are four propositions in this pattern: p0 =true p1 =assign(X,C)!const(C) p2 =not(def \n(X)) p3 =use(X) The set of names of propositions is called prop. For this particular pattern, we have \nprop ={p0,p1,p2,p3}. In what follows the pattern will be interpreted as a set of strings over prop, that \nis (using ML notation for types) an element of proplistset. The validity of a proposition can be expressed \nas a constraint.A constraint is an equation X =t, where X is a meta-variable and t is a program term, \nor a negation of such an equation. Furthermore con\u00adstraints may be combined with logical conjunction \nand disjunction. The set of all constraints is denoted cons. An atomic statement is modelled by a function \nof type prop -cons, which returns the weakest constraint that validates the proposition at that statement. \nFor instance, the assignment a :=3 is modelled by the function p0 true - p1 (X =a !C =3) - p2 X = -a \np3 false - We thus make the type de.nition stat =prop -cons According to this de.nition, a set of program \npaths is a set of lists of such statements. The program itself can now be modelled as an element of stat \nlist set. The natural way to compare a program path and a string in the orig\u00adinal pattern is to use pointwise \napplication followed by conjunction. We therefore de.ne an operator (8): statlist -proplist -cons by \n[s1,s2,...,sn]8[p1,p2,...,pm] = s1(p1)!s2(p2)!...!sn(pn),if n =m false , otherwise Given a program (a \nset of lists of statements) prog and a pattern (a set of lists of propositions) pat, we wish to .nd the \nweakest con\u00adstraint that implies that each path in the program validates some string in the pattern. \nTo wit, our task is to compute the constraint C(pat,prog)=!s : s Eprog : (Vp : p Epat : s 8p) where prog \n: stat list set,and pat : proplist set as before. (Given a bi\u00adnary operator E, we write (Ex : R : T)instead \nof the more common . x:RT.) It is worthwhile to pause at this point, and check that the above is indeed \na generalisation of our earlier language containment prob\u00adlem. Given a set of symbols S, and a symbol \ns ES,de.ne fun(s): S -bool by fun(s)(p)=(s =p) This operation is lifted to sets of strings over S in \nthe obvious way. Nowwehave X CYC(fun(X),Y) In other words, the new speci.cation is a generalisation of \nthe one we considered before. Our task is now to show that this generality is super.cial, and that in \nfact the new problem is an instance of the old one. 4.2 Reduction to containment Intuitively, the main \nproblem is to deal with the fact that multiple propositions can be true of a single statement simultaneously: \nun\u00adlike the usual case, the symbols that are propositions can overlap, when multiple propositions are \ntrue of the same statement. Our solution is to change the alphabet of the pattern from single propo\u00adsitions \nto sets of propositions. Formally, this lifted pattern may be constructed as follows. Given pat,of type \nproplistset, we construct a new pattern pat.(which has type propsetlistset)by: pat. ={xs 1Cx Epat : xE\u00afxs} \nwhere E\u00afis pointwise membership on lists of sets, i.e.: [x1,x2,...,xn]E\u00af[xs1,xs2,...,xsm] = x1 Exs1 \n!x2 Exs2 !...!xn Exsn ,if n =m false , otherwise In other words, a list of proposition sets is in the \nlifted pattern if a list of propositions in the original pattern can be made by taking one element from \neach set. To parallel the above de.nition of lifted patterns, we de.ne 8\u00af,an analogue to 8, to compare \na program path and a lifted pattern: [s1,s2,...,sn]8\u00af[ps1,ps2,...,psm] = s1(ps1)!s2(ps2)!...!sn(psn),if \nn =m false , otherwise The application si(psi), where si is a statement and psi is a propset, is de.ned \nby: s({p1,...,pn})=s(p1)!...!s(pn) In other words, a path in a lifted pattern matches a path in the pro\u00adgram \nif each statement in the program path satis.es all the proposi\u00adtions in the corresponding set in the \npattern path. Some elementary reasoning now shows that Vp : p Epat : s 8p Vps : ps Epat.: s8\u00afps Therefore, \nwe may conclude that the constraint we aim to compute can be rewritten as follows: C(pat,prog)=!s : s \nEprog : (Vps : ps Epat.: s8\u00afps) As expected, to perform this computation, we shall use the chip\u00adchop \nmatrix of the lifted pattern, M(pat.), and de.ne the matrix A(pat,prog)by: A(pat,prog)ixj =!s : s Eprog \n: (Vps : ps EM(pat.)ixj : s8\u00afps) The element of A(pat,prog)corresponding to the appearance of pat.in \nM(pat.)will be precisely C(pat,prog). We now proceed to outline how A(pat,prog)is very closely related \nto the matching matrix B(prog)(where pat.is the implicit pattern) that was introduced in Section 2. In \nfact, modulo the use of free variables (and hence of substitutions), the two are identical. It is thus \nthe case that the construction of the lifted pattern pat.reduces the general problem to an instance of \nthe simple case that we have solved already. To make this conjecture precise, we .rst discuss substitutions. \nA constraint can be viewed as a set of substitutions, one for each possible valuation of the constraint. \nThe above equality for C(pat,prog)is thus equivalent to saying that f(C(pat,prog))} = f(!s : s Eprog \n: (Vps : ps Epat.: s8\u00afps)) for any substitution f. Here we use f()to denote the application of f to the \nconstraint C (i.e. f(C)is true iff f is a possible valuation of C). Note that f()distributes over conjunction \nand disjunction. We can lift the application of substitutions to statements, thus turn\u00ading a function \nfrom propositions to constraints into one from propo\u00adsitions to Booleans, which we shall choose to instead \nview as sets of propositions: f(s)={p1f(s(p))} This can be extended pointwise to lists of statements: \nf([s1,...,sn])=[f(s1),...,f(sn)] and from there to programs: f(prog)={f(s)1s Eprog} Note that since this \nde.nition is pointwise, it distributes through sequential composition: f(prog1;prog2)=f(prog1);f(prog2) \nOne can now show that f(A(pat,prog)ixj)=f(prog)CM(pat.)ixj This exposes the correspondence between A(pat, \n )and the match\u00ading matrix B of Section 2, and we can use the same algorithm to compute A(pat, ). The \nonly difference is that the logical operations manipulate constraints rather than Boolean values. 4.3 \nEf.ciency analysis Above it was established that the algorithm of Section 3 works for patterns over propositions, \nand in the presence of free variables. The complexity analysis that we conducted earlier therefore ap\u00adplies, \nbut there are a number of additional considerations. First, the factor c(S.)now refers to the lifted \nversion of S.It is easy to construct S.from an automaton for S, but the resulting automaton is non-deterministic; \nin general the number of states in the minimal automaton for S.is higher than that for S. This is not \na problem in practice, however: for atomic propagation, dead code elimina\u00adtion and common subexpression \nelimination the size of the lifted automaton is 5. Furthermore, the number of chips is also 5. Second, \nall the logical operations are now applied to constraints, rather than to Boolean values. There is thus \nan additional factor to consider, which is the cost of these logical operations they can no longer be \nregarded as constant-time. Below we shall dis\u00adcuss how the constraints can be represented via binary \ndecision di\u00adagrams, which leads to a worst-case bound of O(FV ), where F is the size of the .nite domain, \nand V the number of variables. This is the same as the additional factor to handle free variables in \nthe non-incremental algorithm of [22]. In particular, our previous con\u00adclusion that the incremental algorithm \nis asymptotically better re\u00admains valid for patterns that consist of propositions containing free variables. \n 5 Implementation Having established the theoretical foundations, we now turn to the details of implementing \nthese ideas for a real programming lan\u00adguage one with procedures, expressions and local variables. 5.1 \nBDDs to represent constraints The usual problem with machine representation of constraints is that any \nrepresentation that can be ef.ciently queried for solutions is subject to exponential blowup in the size \nof the representation if an unfortunate series of operations is applied. Binary decision dia\u00adgrams (BDDs) \n[6] are the standard representation (they are used for a wide range of applications) that tend to minimise \nthe occurrence of such blowups in practice. In its purest form, a BDD is a logical formula on Boolean \nvariables. Each BDD is conceptually a decision tree with Boolean variables for nodes, true and false \nfor edges, and 0 and 1 for the leaves. If fol\u00adlowing the tree for a particular valuation for the Boolean \nvariables leads to a 1, then the formula is true for that valuation, and other\u00adwise it is false. Each \ndecision tree is actually stored as a pointer into a single global directed acyclic graph; this graph \nis subject to the in\u00advariants that variables must always occur in the same order no mat\u00adter what path \nis taken through the graph, and that no node can have the same child for both its false and its true \nedges. Amongst other things, these invariants guarantee canonicity of representation, so that logical \nequivalence of BDDs can be tested just by comparing the pointers. Of course, our meta-variables are not \nBoolean; however, we can treat them as having a .nite domain and make a conservative es\u00adtimate of the \nmaximum number of elements that domain will con\u00adtain. Suppose this maximum is F; then we number the elements \nfrom 0upto F 1, and represent each meta-variable by a vector of log2(F)Boolean variables. The atomic \nconstraint X =t can then be represented by matching up the vector of Boolean variables for X against \nthe binary representation of the number corresponding to t. It follows that for V variables, the maximum \nsize of a BDD representing a constraint is 2(log2(F).V)=FV . Since all operations take time at most proportional \nto this maximum size, we obtain the O(FV )bound promised in Section 4.3. Although we do not know all \nthe elements of the domain in ad\u00advance, since transformations might cause new elements to be cre\u00adated, \nwe can simply give elements numbers as they appear. In fact, since distinct meta-variables may have distinct \ndomains (some may refer to literal values only, some only to program variables and some to expressions, \nfor example), it makes sense to maintain sepa\u00adrate mappings ( value tables ) for each meta-variable. \nThis allows the size of the vector of Boolean variables to be reduced, which im\u00adproves the performance \nof the BDD operations. Even better would be to allow explicit type declarations for meta-variables and \nto use one value table per type. We shall return to this point in Section 6. As we have already mentioned, \nour transformations are speci.ed as Prolog relations, and it may seem that there is a clash between Pro\u00adlog \ns use of substitutions and our use of more general constraints. Our implementation integrates the two \nby switching back and forth where necessary; moving from the constraint world to Prolog sub\u00adstitutions \nmay of course cause backtracking over the possible valu\u00adations of a constraint (in practice this does \nnot affect performance the cost of the BDD operations dominates the time spent in the Prolog interpreter). \nOf course, we must also be careful to write Pro\u00adlog programs that ensure that the appropriate variables \nare bound to ground terms before they will be needed in the constraint world. 5.2 A special case: transparency \nOne problem with our compositional approach is that general so\u00adlutions for each atomic proposition must \nbe computed indepen\u00addently for each program statement. So for example, the proposi\u00adtion not(def (X))at \nthe statement a :=3 is solved by the constraint X =a, and we must use this solution whether or not the \nprogram variable a currently occurs elsewhere in the program, since a future transformation may introduce \nit. This requirement becomes problematic when we move on to check\u00ading transparency conditions. At a particular \nprogram statement s, the predicate transparent(E)should be true iff the value of the pro\u00adgram expression \nin the meta-variable E cannot be affected by s.We take the obvious (but slightly conservative) approach \nof de.ning this to mean that none of the program variables occurring in the expression are changed by \ns, so for example a :=3 is transparent to b +c, but not to a +c (ignoring aliasing effects). Thus, for \nthe predicate transparent(E)and the statement a :=3, we need to gen\u00aderate a constraint for E that allows \nit to take on all possible program expressions that do not contain a. Clearly this is not (directly) \npos\u00adsible in a constraints language whose atomic conditions only allow us to express equality (and hence \ninequality) over .nite domains. However, although the set of all possible expressions is in.nite, the \nset of all program expressions that have so far been seen when we come to apply a transformation is .nite; \nindeed, it can be found simply by inspecting the value table for E. If we can somehow delay the computation \nof the constraint until the point at which we actually apply a transformation, our problem is solved. \nWe achieve this as follows. For each pair (E,a)of a meta-variable E and a program variable a, we dynamically \nallocate a single Boolean variable that indicates a occurs in E (we write this as E a). This Boolean \nvariable is used at constraint generation time (usually in negated form). When it comes to applying a \ntransformation, the set of expressions that have been seen so far is inspected and the set {e1,e2,...,en}of \nthose containing a is generated. We then compute the constraint E a (E =e1 VE =e2 V...VE =en) We repeat \nthis procedure for each pair (E,a), and compute the con\u00adjunction of these constraints with the constraint \nresulting from the path query in question. Note that the constraints decorating the syntax tree are left \nunchanged, so that an accurate result can still be obtained if they are used in future transformations. \nIt might seem that this procedure is quite expensive. In fact, we can maintain the constraint E =e1 VE \n=e2 V...VE =en incrementally, adding a new disjunct each time an expression containing a is added to \nthe value table for E, so the required constraint can be computed cheaply. 5.3 Local variables If we \nhave a block containing a local variable declaration, then anal\u00adysis information that propagates outside \nthat block should not con\u00adtain mention of that variable; if it were to do so then we might apply a transformation \nthat made a reference to the variable out\u00adside its scope, or we might .nd that a transformation involving \na different variable of the same name was incorrectly disabled. The latter problem will particularly \nbe an issue for recursive procedures, where we will not be able to circumvent it by renaming variables. \nOne option would be to analyse each block twice; once to produce analysis results that can be used inside \nthat block, and once to pro\u00adduce analysis results for outside the block. However, this solution doubles \nthe cost of our analysis, and does not sit very well with our compositional approach. It would be preferable \nto .nd a means of transforming the analysis information obtained from inside the block to remove mention \nof local variables. More precisely, if C is an entry of the matching matrix A(pat,B)for the entire block \nB,and X is a meta-variable with an expression e in its domain that contains an out-of-scope local variable, \nthen we would like to transform C such that all occurrences of the atomic constraint X =e in C are replaced \nby false. Some thought suggests that this is unlikely to be straightforward; the BDD representation does \nnot give us direct access to the atomic equality conditions from which a constraint was built up. Indeed, \nsuppose our domain consists of just two variables, p and q, where p is local to a block and q is not \nlocal to the block. Then the con\u00adstraints X =p and X =q will be represented by exactly the same BDD; \nbut an analysis that wished to ignore this p should generate the differently represented constraints \nfalse and X =q instead. However, this problem can be overcome by adding a special value, which we shall \nlabel *, to our domain, and by then making use of a certain operation that acts on the internal structure \nof BDDs. The signi.cance of this special value is that it will never be mentioned in the constraints \narising from the solutions of atomic propositions at statements, which ensures that the constraints for \nX =p and X =q will always have different representations. In a sense, the *value acts as a placeholder \nfor the set of all variables (or other expres\u00adsions) not visible within the scope of the code a constraint \napplies to; if a given constraint allows X to take on the value *, then it should also allow X to take \non any variable not visible in that scope, and vice-versa. The special operation on BDDs which we referred \nto is a slightly peculiar form of existential quanti.cation. If the constraint C is sat\u00adis.ed by a substitution \nS,then ExQ(X,C )is satis.ed by S overrid\u00adden with any possible assignment for X. In other words, all \nmention of X has been removed; if C was true for a certain value of X and the other meta-variables it \nmentions, then ExQ(X,C )is true for any value of X with the same values for the other meta-variables. \nThis operation is particularly natural because of the structure of BDDs described above. To remove any \nbindings X =p from the constraint C ,we de.ne C . =if X =p then ExQ(X,C !X =*)else C To see how this \nworks, consider a language of constraints induc\u00adtively built up from equality conditions (of the form \nX =p, X =q, Y =p, etc), conjunction, disjunction and negation. If this was our representation, then the \nrequired translation could be achieved by syntactically replacing occurrences of X =p with false. It \ncan be shown (by induction on the structure of this language) that if C is the equivalent BDD for such \na constraint, then C.as de.ned above is the equivalent BDD for the translated version of that constraint, \nprovided that none of the equality conditions in the original con\u00adstraint were of the form X =*. Note \nthat since C.is equivalent to the translated constraint, it also satis.es the requirement of being equivalent \nto a constraint that does not have X =*as an equality condition, and thus we can safely apply the same \nprocedure at an\u00adother block boundary without requiring a different *value for that block. Of course we \ndo not only want to hide constraints of the form X =p; we need to remove all X =e where e contains p. \nThis is achieved by replacing X =p in the above de.nition of C.by (X =e1 VX =e2 ...VX =en)where {e1,e2,...,en}is \nthe set of expressions containing p; this constraint can be maintained incre\u00admentally as we described \nin the previous section. Finally, we also need to remove occurrences of E p; to do this we simply de.ne \nC . =ExQ(E p,C !(E p)) Repetition of these two procedures for each (X,p)pair of meta\u00advariable and local \nvariable and for each E p where p is a local variable now gives us the required translation. 5.4 Procedures \nA major advantage of our compositional technique is that it can easily be extended to interprocedural \nanalysis. In particular, we can compute the analysis matrix just once for each procedure body, and reuse \nit at each call site without loss of precision in our analysis. Note that this in fact yields a context-sensitive \nanalysis, since it is equivalent to inlining the procedure body at each call site. If a procedure P has \na value parameter f and body Pbody,then we can model a call site P(a)(where a is any expression) by the \nfrag\u00adment {var f ; f :=a; Pbody } Similarly result parameters can be modelled by the fragment (here a \nmust of course be an lvalue): {var f ; Pbody; a :=f } Finally, the obvious combination can be used to \nmodel value-result parameters. Return values can be handled by adding an extra re\u00adsult parameter to the \nprocedure and treating return statements inside the procedure body as assignments to that parameter; \nof course, a knowledge of the expression evaluation semantics of the language being transformed will \nbe required to ensure that the analysis matrix is correctly inserted into the calling procedure. To compute \nsolutions for recursive procedures, we simply compute a greatest .xed point by iteration (using the matrix \nin which every element is the constraint true as the starting value). This can be extended in the usual \nfashion to mutually recursive groups. We have not yet generalised our complexity bounds to the interpro\u00adcedural \ncase. 5.5 Performance experiments The key concern with regard to our ef.ciency analysis is that in\u00addividual \noperations on constraints may prove to be very expensive, thus causing the time per transformation to \ngrow out of control. In particular, if the theoretical upper limit of O(FV )(F the size of the domain, \nV the number of meta-variables) on the time per operation were to be reached in practice, then our algorithm \nwould scale ex\u00adtremely badly, since in principle F could be in proportion to the size of the program. \nWe have implemented our algorithm, currently only for intraproce\u00addural analyses. Our implementation is \nin Standard ML, and makes use of BuDDy, a C implementation of BDDs (the interface between C and ML is \nis handled by the MuDDy package). Our object lan\u00adguage is the veri.able subset of Microsoft s .NET intermediate \nlan\u00adguage (which is roughly equivalent to the Java bytecode language). We are working towards covering \nthe whole of veri.able IL, but have not yet tackled its object-oriented features. Our implementa\u00adtion \nalso does not yet handle alias analysis; one possibility is to in\u00adtegrate the BDD-based .ow-insensitive \nanalysis of Berndl et al [5]. As well as being convenient to use because of our existing use of BDDs, \nthis analysis is incremental with respect to the addition of new points-to relations; if one is removed \nthen we must either accept an inaccurate (but safe) result or restart the analysis from scratch. We constructed \na series of test cases in which the number of pro\u00adgram variables grows linearly with the size of the \nprogram (in order to ensure that F really is in proportion to the size of the program), and used these \nto measure the time taken for each transformation as the program size grows. We also constructed a series \nof test cases in which the number of program variables remains constant as the program size grows.  \nSince we do not yet handle objects or alias analysis, our test cases were constructed by gluing together \nshort fragments of imperative code compiled from C# we wrote ourselves, rather than from real\u00adworld benchmarks. \nThe transformations we applied were atomic propagation, dead code elimination and unique uses propagation \n(a transformation that propagates V :=E where E is any expression to a use site of V so long as this \nis safe and there is precisely one such use site; the path query for this transformation requires verifying \ntransparency with respect to E in the same way as for common subexpression elimination). Our experiments \nwere performed using the MLton optimising com\u00adpiler (we ported MuDDy to it for this purpose) on a 1.8GHz \nAMD Athlon machine with 2GB of RAM. In all cases the program was just a single method, since we have \nnot yet implemented the inter\u00adprocedural aspects of our analysis. We observed that the time taken per \ntransformation applied grew approximately linearly when the domain grew with the program, but remained \napproximately constant as the program grew if the domain did not grow see Figures 6 and 7. Recall that \nthe asymp\u00adtotic analysis shows that there should be a logarithmic factor due to the need to recompute \nattributes on the path to the root; however in practice much of this computation is likely to be repeated \nwork, and caching of the results of BDD operations inside BuDDy may there\u00adfore mask this factor. (In \nfact, the domain of certain meta-variables, such as that of node identi.ers, did grow with the size of \nthe pro\u00adgram, but the way in which these meta-variables are used meant that this had no impact on the \ncost of transformations.) The linear growth in relation to F is better than the potential worst\u00adcase \nbehaviour of O(FV )(V is typically 2 or 3 for these transfor\u00admations), but still slightly disappointing, \nas we had hoped that the sharing in the BDD representation would result in something closer to logarithmic \nbehaviour. In contrast, some experiments with an implementation of a non-incremental solver for our queries \nsuggest that the cost of the BDD operations in this case grows much more slowly. However, in practice \nwe expect that domains will grow only slowly with program size; in particular, it is not generally the \ncase with real programs that a new local variable is introduced every few lines. It has been observed \nin other applications that use BDDs that their performance is very sensitive to the variable ordering \nchosen; how\u00adever we have not observed such sensitivity for our analysis.  6 Related work The relation \nof our work to that of Conway was discussed in Sec\u00adtion 3. To our knowledge, the only other applications \nof Conway s chip-chop matrices are those of Backhouse [2, 4]; in particular [2] describes an algorithm \nfor computing these matrices. Apart from this theoretical background, we have also taken inspiration \nfrom many previous works on the speci.cation of program transforma\u00adtions, and the most relevant of these \nsources are further described below. 6.1 Gospel and Genesis Gospel was put forward by Whit.eld and Soffa \nas a notation for the formal study of program transformations [38]. Furthermore, they also provided a \ntool called Genesis for implementing speci.cations that are expressed in Gospel. A speci.cation in Gospel \nconsists of three parts: a declaration sec\u00adtion where the meta-variables are declared; a precondition \nsection that typically describes a code pattern, as well as a number of .ow dependencies; and .nally \nan action, where the transformation is applied to the program, provided the preconditions have been sat\u00adis.ed. \nA similar style of speci.cation is also found in the APTS system of Paige [25]. In comparison to Gospel, \nour speci.cations do not declare the meta\u00advariables and their domains. In retrospect, this was a mistake, \nand we plan to rectify it: knowing the precise domain of a variable (statement, expression, constant, \n...) not only enlightens the spec\u00adi.cation, it also allows for a more ef.cient implementation of con\u00adstraints \nvia BDDs. We express the .ow dependencies of Gospel via regular path queries. This is more formal, and \nit allows for more rigorous analysis of the transformations, for instance when proving their correctness \n[19]. Of course the use of regular patterns is also an essential prerequisite for the incremental implementation \ntech\u00adnique that we have presented here. A .nal difference is the fact that we use Prolog as a meta-language, \nallowing the user to de.ne new primitives of her own with a mini\u00admum of effort. Any non-syntactic processing, \nsuch as comparison of integers (for example for loop unrolling) or constant folding is done within the \nProlog program. A number of other researchers have observed that logic programming is suitable for this \nkind of application, e.g. [27]. 6.2 Metaframe Our .rst attempt to put the use of Gospel on a more formal \nbasis was inspired by the work of Rus and Van Wyk on parallelizing opti\u00admisations [29]. They use modal \nlogic as the speci.cation language, an idea pioneered by Steffen and his coworkers [32]. The attraction \nof using modal logic lies both in its expressivity and in the fact that applicability can be veri.ed \nvia a model checker. In .rst instance, it might appear that the use of a model checker is prohibitively \nexpensive, but in fact it is possible to partially evaluate the model checker with respect to speci.cations, \nand hence gener\u00adate bit-vector analyses. This is implemented in the Metaframe sys\u00adtem. Steffen reports \nthat the resulting analyses are extremely ef.\u00adcient, and in fact competitive with those written by hand. \nSteffen s speci.cations do not contain free meta-variables, however, which simpli.es the implementation \nconsiderably. A major difference of our approach with Metaframe is the incre\u00admental solving algorithm. \nSokolsky and Smolka have proposed an incremental model checker for the modal mu-calculus [31], and it \nwould be interesting to see whether this can be used in the context of program analysis and transformation. \nTheir algorithm is quite similar to traditional incremental data.ow algorithms that employ the technique \nof restarting iteration [26]. At present there seem to be no model checkers that employ the other dominant \nparadigm in incremental data.ow analysis, namely incremental elimination methods [8, 30]. For now it \nappears to us that the incremental al\u00adgorithm we have presented here bears little relation to any previous \nincremental data.ow analyses. 6.3 Cobalt Cobalt [21] is also a language for specifying program transforma\u00adtions, \nin fact inspired by earlier work in our research team to em\u00adploy modal logic as a speci.cation language \n[18, 19]. The purpose of Cobalt is to enable automatic proofs that the transformations are sound, and \nthat work thus complements the present paper on ef.\u00adcient execution. To enable such automatic proofs \nof transformations, speci.cations are restricted to the form fromEntry({}.;{P};{Q}.; {R})or the dual \ntoExit({P};{Q}.;{R};{}.). (The syntax of Cobalt is of course different.) Quite a wide class of transformations \ncan be expressed this way, including complex examples such as partial redundancy elimination. An example \nof a transformation that is not readily described in this format is strength reduction. Furthermore, \nit is dif.cult to .lter out the infeasible paths as in our formulation of conditional constant propagation. \nThis is admittedly less of a problem in the presence of the com\u00adposition mechanisms of [20]. These composition \nmechanisms, which combine multiple transfor\u00admations by speculatively applying them to loop bodies and \nthen iterating until a .xed point is reached, allow transformations which are mutually dependent with \nthe results of other transformations to be applied. For example, Wegman and Zadeck s conditional con\u00adstant \npropagation [37] can be obtained by using ordinary constant propagation and dead branch elimination as \nbuilding blocks. Sim\u00adilarly, it would be possible to eliminate a chain of mutually depen\u00addent assignments \nsuch as x :=y; y :=z; z :=x inside a loop using a standard formulation of dead assignment elimination. \nWe expect that these compositions mechanisms can be easily ap\u00adplied within our framework (in [21] the \nveri.cation that this is pos\u00adsible for Cobalt is also left to future work). Indeed, our strategy language \nshould make the speci.cation very natural, in a similar manner to current work in Stratego [24]. Of course, \nspeculatively applying transformations and then iterat\u00ading is likely to be relatively expensive. A cheaper, \nbut less general, option might be to write patterns tailored for speci.c cases for example, the elimination \nof precisely three mutually dependent as\u00adsignments. We hope that it would be possible to generate such \npat\u00adterns automatically from the basic patterns for the transformations in question. 6.4 Optimix An \nalternative to a transformation system based on term rewriting is one that employs the formal notion \nof graph rewriting. This is the basis of the Optimix system, designed by A\u00dfmann [1]. It supports both \nedge addition rewrite systems (EARS, which have nice formal properties, in particular unique normal forms), \nand more general exhaustive graph rewrite systems (XRGS, which can be checked for termination). Typically \nthe EARS are used to perform analysis, augmenting the original .ow graph representing the program, and \nthen the XRGS comes into play for actually modifying the .ow graph. This is a signi.cant improvement \nover Gospel in the sense that the preconditions and the modi.cation part of the speci.cation are both \nput on a solid semantic basis. To encode data.ow analyses in Optimix, they are expressed as reachability \nproblems. Many analyses can be so expressed, as .rst shown in a ground-breaking paper by Reps, Horwitz \nand Sagiv [28]. Ef.cient implementations are generated by the application of Dat\u00adalog techniques. The \nrecent work of Liu and Stoller [22] on gener\u00adating ef.cient analyses from Datalog speci.cations is very \nclosely related to that of A\u00dfmann. We have discussed their algorithms for solving regular path queries \nearlier in this paper. This lays bare the connection to our work, as essentially we are also computing \nthe solution to a reachability problem, applied to the product of the .ow graph and the deterministic \nautomaton that represents the regular path query. The ESP system [10] extends the reachability algorithm \nof [28] in a similar way, to solve regular path queries that encode safety properties of software. A \nless principled but similar approach is taken by the designers of Metal, which was designed for the dual \npurpose of specifying optimisations and bug identi.\u00adcation [15]. The close similarity between our queries \nand those in ESP and Metal suggests that the incremental algorithm presented here might be applied to \ncheck safety properties on the .y in an interactive development environment. In [27], Reps shows how \ndemand-driven versions of certain inter\u00adprocedural program analyses (expressed as reachability problems) \ncan be obtained via the magic-sets transformation. He also notes that the same result can be obtained \nby directly executing the reach\u00adability de.nitions via tabled SLD resolution this was the tech\u00adnique \nwe employed in [11], and at least with that prototype imple\u00admentation, the performance was not adequate \nin practice. Previous work of Duesterwald et al. [13] has indicated that demand-driven analysis often \noutperforms incremental implementations, but it ap\u00adpears that for the problem considered here, the situation \nis reversed.  7 Acknowledgements We would like to thank the Programming Tools Group at Oxford for many \nenjoyable discussions about this work, especially David Lacey who suggested many improvements. Roland \nBackhouse, Jens Knoop, David Lacey, Sorin Lerner, Annie Liu, and Eric van Wyk provided valuable feedback \non a draft of this paper, as did the anonymous POPL referees. Stephen Drape did some initial exper\u00adiments \nwith the implementation. Some of the early parts of this work were funded by a generous gift from Microsoft \nResearch. 8 References [1] U. A\u00dfmann. OPTIMIX a tool for rewriting and optimiz\u00ading programs. In H. \nEhrig, G. Engels, H. J. Kreowski, and G. Rozenberg, editors, Handbook of Graph Grammars and Computing \nby Graph Transformation. Volume 2: Applica\u00adtions, Languages and Tools, pages 307 318. World Scienti.c, \n1998. [2] R. C. Backhouse. Closure algorithms and the star-height problem of regular languages. PhD thesis, \nUniversity of Lon\u00addon, 1975. [3] R. C. Backhouse. Fusion on languages. In 10th European Symposium on \nProgramming, ESOP 2001, volume 2028 of Lecture Notes in Computer Science, pages 107 121. Springer, 2001. \n[4] R. C. Backhouse and R. K. Lutz. Factor graphs, failure func\u00adtions and bi-trees. In A. Salomaa and \nM. Steinby, editors, Fourth Colloquium on Automata, Languages and Program\u00adming, volume 52 of Lecture \nNotes in Computer Science, pages 61 75. Springer Verlag, July 1977. [5] M. Berndl, O. Lhot\u00b4 ak, F. Qian, \nL. Hendren, and N. Umanee. Points-to analysis using BDDs. In ACM Conference on Pro\u00adgramming Language \nDesign and Implementation, pages 103 114, 2003. [6] R. E. Bryant. Graph-based algorithms for Boolean \nfunction manipulation. IEEE Transactions on Computers, 35(8):677 691, 1986. [7] J. A. Brzozowski. Derivatives \nof regular expressions. Journal of the ACM, 11(4):481 494, 1964. [8] M. D. Carroll and B. G. Ryder. Incremental \ndata .ow analysis via dominator and attribute updates. In ACM Symposium on Principles of Programming \nLanguages, pages 274 284, 1988. [9] J. H. Conway. Regular Algebra and Finite Machines. Chap\u00adman and Hall, \n1971. [10] M. Das, S. Lerner, and M. Seigle. ESP: Path-sensitive pro\u00adgram veri.cation in polynomial time. \nIn ACM Conference on Programming Language Design and Implementation, pages 57 68, 2002. [11] O. de Moor, \nD. Lacey, and E. Van Wyk. Universal regular path queries. Higher-Order and Symbolic Computation, 16:15 \n35, 2003. [12] S. J. Drape, O. de Moor, and G. Sittampalam. Path logic pro\u00adgramming. In ACM Conference \non Principles and Practice of Declarative Programming, pages 133 144, 2002. [13] E. Duesterwald, R. Gupta, \nand M. L. Soffa. A practical frame\u00adwork for demand-driven interprocedural data .ow analysis. ACM Transactions \non Programming Languages and Systems, 19(6):992 1030, 1997. [14] S. Z. Guyer and C. Lin. An annotation \nlanguage for opti\u00admizing software libraries. In Second conference on Domain-Speci.c Languages, pages \n39 52. USENIX, 1999. [15] S. Hallem, B. Chelf, Y. Xie, and D. Engler. A system and lan\u00adguage for building \nsystem-speci.c, static analyses. In ACM Conference on Programming Language Design and Imple\u00admentation, \npages 69 82, 2002. [16] D. E. Knuth. Semantics of context-free languages. Mathe\u00admatical Systems Theory, \n2(2):127 145, 1968. Corrections in 5(2):95-96, 1971. [17] D. Lacey. Specifying Compiler Optimisations \nin Temporal Logic. DPhil thesis, Oxford University Computing Labora\u00adtory, 2003. [18] D. Lacey and O. \nde Moor. Imperative program transformation by rewriting. In R. Wilhelm, editor, Compiler Construction, \nvolume 2027 of Lecture Notes in Computer Science, pages 52 68. Springer Verlag, 2001. [19] D. Lacey, \nN. D. Jones, E. Van Wyk, and C. C. Frederiksen. Proving correctness of compiler optimizations by temporal \nlogic. In ACM Symposium on Principles of Programming Languages, pages 283 294, 2002. [20] S. Lerner, \nD. Grove, and C. Chambers. Composing data.ow analyses and transformations. In ACM Symposium on Princi\u00adples \nof Programming Languages, pages 270 282, 2002. [21] S. Lerner, T. Millstein, and C. Chambers. Automatically \nprov\u00ading the correctness of compiler optimizations. In ACM Con\u00adference on Programming Language Design \nand Implementa\u00adtion, pages 220 231, 2003. [22] Y. A. Liu and S. D. Stoller. From Datalog rules to ef.cient \nprograms with time and space guarantees. In ACM Confer\u00adence on Principles and Practice of Declarative \nProgramming, pages 172 183, 2003. [23] E. Meijer and J. Gough. A technical overview of the com\u00admon language \ninfrastructure. Available from URL: http:// research.microsoft.com/ emeijer/Papers/CLR.pdf. [24] K. Olmos \nand E. Visser. Strategies for source-to-source constant propagation. Technical Report UU-CS-2002-042, \nUtrecht University, 2002. Available from URL: http://www. cs.uu.nl/research/techreps/UU-CS-2002-042.html. \n[25] R. Paige. Viewing a program transformation system at work. In Manuel Hermenegildo and Jaan Penjam, \neditors, Proceed\u00adings of the Sixth International Symposium on Programming Language Implementation and \nLogic Programming, pages 5 24. Springer Verlag, 1994. [26] L. Pollock and M. L. Soffa. An incremental \nversion of it\u00aderative data .ow analysis. IEEE Transactions on Software Engineering, 15(12):1537 1549, \n1989. [27] T. W. Reps. Demand interprocedural program analysis using logic databases. In R. Ramakrishnan, \neditor, Applications of Logic Databases, pages 163 196. Kluwer Academic Publish\u00aders, 1994. [28] T. W. \nReps, S. Horwitz, and M. Sagiv. Precise interprocedu\u00adral data.ow analysis via graph reachability. In \nACM Sympo\u00adsium on Principles of Programming Languages, pages 49 61, 1995. [29] T. Rus and E. Van Wyk. \nUsing model checking in a paral\u00adlelizing compiler. Parallel processing letters, 8(4):459 471, 1998. [30] \nB. G. Ryder and M. C. Paull. Incremental data .ow analysis algorithms. ACM Transactions on Programming \nLanguages and Systems, 10(1):1 50, 1988. [31] O. V. Sokolsky and S. A. Smolka. Incremental model check\u00ading \nin the modal mu-calculus. In D. L. Dill, editor, Proceed\u00adings of the 6th International Conference on \nComputer-aided veri.cation, volume 818 of Lecture Notes in Computer Sci\u00adence, pages 351 363. Springer \nVerlag, 1994. [32] B. Steffen. Data .ow analysis as model checking. In Pro\u00adceedings of Theoretical Aspects \nof Computer Science, volume 526 of Lecture Notes in Computer Science, pages 346 364. Springer Verlag, \n1991. [33] R. Vall\u00b4ee-Rai, L. Hendren, V. Sundaresan, P. Lam, E. Gagnon, and P. Co. Soot a Java optimization \nframework. In Pro\u00adceedings of CASCON 1999, pages 125 135, 1999. [34] T. L. Veldhuizen and D. Gannon. \nActive libraries: Rethink\u00ading the roles of compilers and libraries. In Proceedings of the SIAM Workshop \non Object Oriented Methods for Inter\u00adoperable Scienti.c and Engineering Computing. SIAM Press, 1998. \n[35] E. Visser. Meta-programming with concrete object syntax. In D. Batory, C. Consel, and W. Taha, editors, \nGenerative Programming and Component Engineering, volume 2487 of Lecture Notes in Computer Science, pages \n299 315. Springer-Verlag, 2002. [36] E. Visser, Z. Benaissa, and A. Tolmach. Building program optimizers \nwith rewriting strategies. In ACM Conference on Functional Programming, pages 13 26, 1998. [37] M. N. \nWegman and F. K. Zadeck. Constant propagation with conditional branches. ACM Transactions on Programming \nLanguages and Systems, 13(2):181 210, 1991. [38] D. Whit.eld and M. L. Soffa. An approach for exploring \ncode-improving transformations. ACM Transactions on Pro\u00adgramming Languages and Systems, 19(6):1053 1084, \n1997.  \n\t\t\t", "proc_id": "964001", "abstract": "We aim to specify program transformations in a declarative style, and then to generate executable <i>program transformers</i> from such specifications. Many transformations require non-trivial program analysis to check their applicability, and it is prohibitively expensive to re-run such analyses after each transformation. It is desirable, therefore, that the analysis information is incrementally updated.We achieve this by drawing on two pieces of previous work: first, Bernhard Steffen's proposal to use <i>model checking</i> for certain analysis problems, and second, John Conway's theory of <i>language factors</i>. The first allows the neat specification of transformations, while the second opens the way for an incremental implementation. The two ideas are linked by using regular patterns instead of Steffen's modal logic: these patterns can be viewed as queries on the set of program paths.", "authors": [{"name": "Ganesh Sittampalam", "author_profile_id": "81100259191", "affiliation": "Oxford University Computing Laboratory", "person_id": "PP14098545", "email_address": "", "orcid_id": ""}, {"name": "Oege de Moor", "author_profile_id": "81100198102", "affiliation": "Oxford University Computing Laboratory", "person_id": "PP14078760", "email_address": "", "orcid_id": ""}, {"name": "Ken Friis Larsen", "author_profile_id": "81100222999", "affiliation": "IT University of Copenhagen", "person_id": "P653510", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/964001.964004", "year": "2004", "article_id": "964004", "conference": "POPL", "title": "Incremental execution of transformation specifications", "url": "http://dl.acm.org/citation.cfm?id=964004"}