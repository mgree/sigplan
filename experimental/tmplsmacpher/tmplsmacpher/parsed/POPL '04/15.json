{"article_publication_date": "01-01-2004", "fulltext": "\n Abstract Non-Interference Parameterizing Non-Interference by Abstract Interpretation Roberto Giacobazzi \nIsabella Mastroeni Dipartimento di Informatica Dipartimento di Informatica Universit`Universita di Verona \n a di Verona ` Strada Le Grazie 15, I-37134 Verona, Italy Strada Le Grazie 15, I-37134 Verona, Italy \n roberto.giacobazzi@univr.it mastroeni@sci.univr.it Abstract In this paper we generalize the notion of \nnon-interference making it parametric relatively to what an attacker can analyze about the input/output \ninformation .ow. The idea is to consider attackers as data-.ow analyzers, whose task is to reveal properties \nof con.den\u00adtial resources by analyzing public ones. This means that no unau\u00adthorized .ow of information \nis possible from con.dential to public data, relatively to the degree of precision of an attacker. We \nprove that this notion can be fully speci.ed in standard abstract interpre\u00adtation framework, making the \ndegree of security of a program a property of its semantics. This provides a comprehensive account of \nnon-interference features for language-based security. We intro\u00adduce systematic methods for extracting \nattackers from programs, providing domain-theoretic characterizations of the most precise at\u00adtackers \nwhich cannot violate the security of a given program. These methods allow us both to compare attackers \nand program secrecy by comparing the corresponding abstractions in the lattice of abstract interpretations, \nand to design automatic program certi.cation tools for language-based security by abstract interpretation. \nCategories and Subject Descriptors: D.3.1 [Programming Lan\u00adguages]: Formal De.nitions and Theory Semantics; \nF.3.2 [Log\u00adics and Meanings of Programs]: Semantics of Programming Languages program analysis; K.6.5 \n[Management of Computing and Information Systems]: Security and Protection. General Terms: Languages, \nSecurity. Keywords: Abstract interpretation, non-interference, language\u00adbased security, abstract domains. \n1 Introduction A typical problem in language-based security is protecting data con.dentiality from erroneous \nor malicious attacks while data are processed by programs. The standard way to protect con.dential Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL 04, January \n14 16, 2004, Venice, Italy. Copyright 2004 ACM 1-58113-729-X/04/0001 ...$5.00 data is access control \n[30]: Higher privileges are required in order to access .les containing con.dential data. It is well \nknown that ac\u00adcess control checks do not put constraints on how the information is propagated. Once information \nis released from its container, the accessing program may, either by mistake or by purpose, improp\u00aderly \ntransmit the information in some form. In this case, in order to ensure that information can be used \nonly according to speci.c security rules, also known as information .ow policies, it is nec\u00adessary to \nanalyze how information .ows in program s semantics. Goguen and Meseguer [21] .rstly introduced non-interference \nas a key feature for specifying security policies: One group of users [...] is noninterfering with another \ngroup of users if what the .rst group does [...] has no effect on what the second group of users can \nsee . Clearly, when computations on public data are non-interfering with those on private resources, \nno leakage of con.dential information is possible by observing public input/output behavior. This prin\u00adciple \nis the pattern on which most security polices are speci.ed in language-based security (see [30] for an \nexcellent survey).  The problem The problem of re.ning security policies has been addressed by many \nauthors as a major challenge in language-based information .ow security [30]. Re.ning security policies \nmeans weakening standard non-interference checks in such a way that these restric\u00adtions can be used in \npractice. In order to adapt security policies to practical cases, it would be essential to know how much \nan attacker may learn from a program by (statically) analyzing its input/output behavior. The goal is \nto automatically generate, from security po\u00adlices, a certi.cate specifying that the given program has \nonly se\u00adcure information .ows. This is statically achieved to provide pro\u00adgrams with their appropriate \ncerti.cates. In this paper we address the problem of characterizing the degree of secrecy of a program \nrelatively to what an attacker can analyze about the input/output information .ow. Consider the following \nprogram written in a simple imperative lan\u00adguage, where the while-statement iterates until x1 is 0. Suppose \nx1: H is a secret variable and x2: L is a public variable: while x1 do (x2:= x2 + 2; x1:= x1 - 1) Clearly, \nin the standard sense of non-interference, there is an im\u00adplicit .ow from x2 to x1, since, due to the \nwhile-statement, x2 changes depending on the initial value of x1. This represents the case where no restriction \nis considered on the power of an attacker. However, suppose that the attacker can observe only the parity \nof a value (0 is even). It is worth noting that if x2 is initially even, then it is still even independently \nfrom the execution of the while, and therefore from the initial value of x1. Similarly if x2 is initially \nodd then it remains odd independently from the execution of the while, i.e., from the value of x1. This \nmeans that there s no information .ow concerning parity. In order to model these situations we need to \nweaken standard non-interference relatively to what an attacker can observe on program information .ows. \n The idea: Attackers as abstract interpretations In this paper we introduce the notion of abstract non-interference \nby parameterizing standard non-interference relatively to what an attacker can observe. The idea is to \nconsider attackers as static pro\u00adgram analyzers whose task is to reveal properties of secret data by \nstatically analyzing public resources. This idea allows us to intro\u00adduce a notion of secrecy1 relatively \nto the attacker s observation. Hence, a program ensures secrecy with respect to a given property, which \ncan be statically analyzed by the attacker, if that property on con.dential data cannot be disclosed \nby analyzing public data. For instance, in the example above, any attacker looking at parity is unable \nto disclose secrets from con.dential data. In this sense the program is secret for parity, while it is \nnot secret relatively to stronger attackers, able to observe more concrete properties of data such as \nhow much a variable growth (e.g. by interval analysis [7]). Because static program analysis can be fully \nspeci.ed as the ab\u00adstract interpretation of the semantics of the program [7], we can model attackers \nas abstract interpretations. Main contribution In this paper we apply standard techniques from abstract \ninterpre\u00adtation in order to compare programs according to their relative de\u00adgree of secrecy and in order \nto systematically extract attackers from programs by transforming abstractions. We .rst introduce the \nno\u00adtion of abstract non-interference relatively to what an attacker can observe on the input/output of \nprogram s behavior. Abstract non\u00adinterference is obtained by a step-by-step weakening of standard Goguen \nand Meseguer s non-interference by making it parametric relatively input/output abstractions. These abstractions \nmodel the power of an attacker which analyzes the public input/output be\u00adhavior of programs. The abstraction \nplays here the role of observ\u00adable properties and proving that a program satis.es abstract non\u00adinterference \nrelatively to a given abstraction means proving that relatively to that observable property, the program \nis secure. We characterize the abstract non-interference semantics of a determin\u00adistic program as an \nabstract interpretation of its maximal trace se\u00admantics in the corresponding transition system. Based \non this ob\u00adservation we give a method for checking abstract non-interference by abstract interpretation \nof the concrete semantics of the program and we prove that checking abstract non-interference boils down \ninto a standard static program analysis problem. Viewing attackers as abstract interpretations is the \nkey point in order to both make se\u00adcrecy parametric on what an attacker can observe, and to apply to \nsecrecy the techniques known in abstract interpretation to system\u00adatically transform abstract domains. \nIn this view, because attackers are modeled by abstractions, transforming abstract domains means transforming \nattackers. These can be either re.ned or simpli.ed in power, following the standard methods in abstract \ndomain de\u00adsign [16, 18]. The main result concerns the possibility of associ\u00adating programs with attackers \nin the lattice of abstract interpreta\u00ad 1In the rest of this paper we abuse by considering secrecy and \nnon-interference as synonyms. tions. We give systematic methods for extracting, when possible, the most \nprecise (viz. most concrete) attacker for which a program ensures secrecy. This is achieved by a .x-point \nconstruction which simpli.es abstract domains towards the most concrete domain for which the program \nis secret as regards as the corresponding prop\u00aderty. This canonical attacker represents, in the lattice \nof abstract interpretations, the relative degree of security of a program: Any other strictly stronger \nattacker will violate secrecy. Practical meth\u00adods for deriving approximate secure attackers are discussed. \nThe paper ends with a discussion on further research in this direction.  Related work There is a widespread \nliterature on methods and techniques for checking secure information .ows in software, ranging from stan\u00addard \ndata-.ow/control-.ow analysis techniques to type inference. All of these approaches are devoted to prove \nthat a system as a whole, or parts of it, does not allow con.dential data to .ow towards public variables. \nType-based approaches are designed in such a way that well-typed programs do not leak secrets. In a security-typed \nlanguage, a type is inductively associated at compile-time with pro\u00adgram statements in such a way that \nany statement showing a poten\u00adtial .ow disclosing secrets is rejected [33, 36, 39]. Similarly, data\u00ad.ow/control-.ow \nanalysis techniques are devoted to statically dis\u00adcover .ows of secret data into public variables [3, \n4, 23, 25, 31]. All these approaches are characterized by the way they model attackers (or unauthorized \nusers). In standard non-interference, the attacker is able to fully analyze concrete computations. In \nthis case, a con\u00adservative type/data-.ow/control-.ow analysis of information .ows would discard all the \nprograms which may provide any explicit or implicit concrete .ow from con.dential to public resources. \nThe problem of re.ning security policies has been recently addressed by many authors (see Sec. IV-D in \n[30]). The most related papers are [25, 26, 15, 35]. In these papers some restrictions have been consid\u00adered \non the power of attackers, basically related with complexity is\u00adsues [25, 35] or with precise quantitative \nestimation of unauthorized information leakage [26, 15]. While in the second case programs may have an \nabsolute degree of security, corresponding to whether they leak or not secrets under some metric, in \nthe .rst case, differ\u00adent degrees of security can be associated with programs, e.g. allow\u00ading us to accept \nprograms for which leaking secrets would be too complex for any attacker. A lattice-theoretic and property-driven \nreasoning about the information that violates non-interference is in [24, 40]. In [24] the authors introduce \na lattice-theoretic model of information .ow based on equivalence relations. This provides nec\u00adessary \nand suf.cient conditions for standard non-interference of a state machine. In [40] the authors introduce \na method for authoriz\u00ading declassi.cation when con.dential information is released, for both passive \nand active attackers. This approach, even though dif\u00adferent, has a strong connection with our work. Robust \ndeclassi\u00ad.cation can be viewed as the dual problem of characterizing the program property (in our case \nan abstract domain) which violates non-interference. We will show later in Section 6 that this notion \ncan be adapted to abstract non-interference providing a parametric characterization for downgradable \ninformation. In [31] the authors use partial equivalence relations (PERs) to model dependencies and therefore \ninformation .ows. We believe that the PER model can be easily adapted to cope with the weaker notion \nof abstract non\u00adinterference. The use of abstract interpretation in language-based security is not new, \neven though to the best of our knowledge, no author used the lattice of abstract interpretations for \nevaluating the degree of secrecy of programs. Abstract interpretation has been ba\u00adsically considered \nfor designing .exible data/control .ow analyzes [28] or enhanced security-type systems [39]. 2 Preliminaries \n2.1 Basic notions Sets are usually denoted with capital letters. If S and T are sets, then P(S)denotes \nthe powerset of S, S\"T denotes the set-difference be\u00adtween S and T , S . T denotes strict inclusion, \nand for a function f : S . T and X . S, f (X)def ={ f (x)| x . X}. We will often denote f ({x})as f (x).By \ng . f we denote the composition of the functions . id def f and g, i.e., g . f def =.x. x. (P, =) denotes \na poset =.x.g(f (x)) P with ordering relation =, while (P,=,.,.,T,.) denotes a com\u00adplete lattice P, with \nordering =, lub ., glb . , greatest element (top) T, and least element (bottom) .. Often, =P will be \nused to de\u00adnote the underlying ordering of a poset P, and .P, .P, TP and .P denote the basic operations \nand elements if P is a complete lattice. C ~ =A denotes that C and A are isomorphic ordered structures. \nThe downward closure of S is de.ned as . S def ={x . P |.y . S. x =Py}, and for x . P, . x is a shorthand \nfor .{x}, while the upward clo\u00adsure . is dually de.ned. S -. T denotes the set of all functions from \nS to T . We use the symbol C to denote point-wise order\u00ading between functions: If S is any set, P a poset, \nand f ,g : S . P then f C g if for all x . S, f (x)=Pg(x). Let C and A be com\u00ad mc plete lattices, then, \nC -. A and C -. A, denote, respectively, the set of all monotone and (Scott-)continuous functions from \nC to A. c Recall that f . C -. A iff f preserves lub s of (nonempty) chains iff f preserves lub s of \ndirected subsets, and f : C . A is (com\u00adpletely) additive if f preserves lub s of all subsets of C (emptyset \nincluded). We denote by lfp= f and gfp=f , respectively, the least .T and greatest .x-point, when they \nexist, of an operator f on a poset. c If f . C -. C then lfp=cf =.i.Nfi(.C), where, for any i . Nand \n.c x . C, the i-th power of f in x is inductively de.ned as follows: f 0(x)=x; fi+1(x)= f (fi(x)). Dually, \nif f is co-continuous then gfp=TCC f =.i.Nfi(TC). { fi(.C)}i.Nand { fi(TC)}i.Nare, respec\u00ad tively, the \nupper and lower iteration sequences of f (see [8]). 2.2 Abstract interpretation Abstract domains can \nbe equivalently formulated either in terms of Galois connections or closure operators [9]. An upper closure \noperator on a poset P is an operator . : P . P monotone, idem\u00adpotent and extensive (. x . P. x =P .(x)). \nLower closure operators are de.ned dually. The set of all upper (lower) closure operators on P is denoted \nby uco(P)(lco(C)). Let (C,=,.,.,T,.) be a com\u00adplete lattice. A basic property of closure operators is \nthat they are uniquely determined by the set of their .x-points .(C). For upper closures: X . C is the \nset of .x-points of an upper closure on C =M (X)def iff X is a Moore-family of C, i.e., X ={. S | S . \nX} where . 0=T. M (X). For any X . C, M (X) is called the Moore-closure of X in C, i.e., M (X)is the \nleast (w.r.t. set-inclusion) subset of C which contains X and it is a Moore-family of C.It turns out \nthat (.(C),=) is a complete meet sub-semilattice of C (i.e., . is its glb), but, in general, it is not \na complete sub-lattice of C, since the lub in .(C) de.ned by .Y . .(C)..(.Y) might be different from \nthat in C. Indeed, .(C)is a complete sub\u00adlattice of C iff . is additive. Often, we will .nd particularly \ncon\u00advenient to identify closure operators with their sets of .x-points. If C is a complete lattice then \nuco(C) ordered point-wise is also a complete lattice, denoted by (uco(C),C,U,n,.x.T, .x.x), where for \nevery .,. . uco(C), {.i}i.I . uco(C) and x . C: . C . iff .y . C. .(y)= .(y)iff .(C). .(C); (n i.I.i)(x)=.i.I.i(x); \nand (Ui.I.i)(x)=x ..i . I. .i(x)=x. The standard abstract interpre\u00adtation framework is based on the adjoint \nrelation between abstrac\u00ad mm tion and concretization functions [7]. If a : C-. A and . : A-. C are monotone \nfunctions such that .x.x C . . a and a. . C .x.x, then (A, a, .,C) is called a Galois connection (GC) \nbetween C and A. If in addition a . . =.x.x, then (A,a,.,C) is a Galois insertion (GI) of A in C. Note \nthat A ~ =C iff both (A,a,.,C)and (C,.,a,A) are GI. The concrete and abstract domains, C and A, are assumed \nto be complete lattices and are related by abstraction a and con\u00adcretization . forming a GC (A,a, .,C). \nFollowing a standard ter\u00adminology, A is called an abstraction of C, and C is a concretiza\u00adtion of A.If \n(A,a,.,C) is a GI, then each value of the abstract domain A is useful in representing C, because all \nthe elements of A represent distinct members of C, being . 1-1. Any GC may be lifted to a GI by identifying \nin an equivalence class those values of the abstract domain with the same concretization. This process \nis known as reduction of the abstract domain. If (A,a,.,C)isaGIand cc fC . C -. C, fA . A-. A, then fA \nis a sound approximation of fC f bca def if a. fC =A fA. a or equivalently a. fC. . =A fA. A =a. fC. \n. is known as the best correct approximation (bca) of fC in A. Sound\u00adness naturally implies that a(lfp=C \nfC)=A lfp=A fA.If a. fC = fA. a .C .A then we say that fA is a complete approximation of fC [9, 20]. \nIn this case a(lfp=C fC)=lfp=A fA. .C .A Note that any Galois insertion (A,a,.,C) uniquely determines \nan upper closure operator . . a . uco(C)and conversely, any closure operator . . uco(C) uniquely determines \na GI (.(C),.,id,C),up to isomorphic representation of domain s objects. Hence, we will identify uco(C) \nwith the so-called lattice .C of abstract interpre\u00adtations of C (cf. [7, Section 7] and [9, Section 8]), \ni.e., the com\u00adplete lattice of all possible abstract domains (modulo isomorphic representation of their \nobjects) of the concrete domain C. The point-wise ordering on uco(C) corresponds precisely to the stan\u00addard \nordering used to compare abstract domains with regard to their precision: A1 is more precise than A2 \n(i.e., A2 is an abstrac\u00adtion of A1)iff A1 C A2 in uco(C) iff (A2,a,.,A1) is a GI. Let {Ai}i.I . uco(C): \nUi.IAi is the most concrete among the domains in .C which are abstractions of all Ai s, i.e., U i.IAi \nis the least (w.r.t. C) common abstraction of all Ai s; and ni.IAi is (isomor\u00adphic to) the well-known \nreduced product (basically cartesian prod\u00aduct plus reduction) of all Ai s, or, equivalently, it is the \nmost abstract among the domains in .C which are more concrete than every Ai: ni.IAi =M (. i.IAi). The \ndisjunctive completion of a domain is the most abstract domain able to represent the concrete disjunction \ny of its objects: y(.)=U{. . uco(L)|. C . and . is additive}. . is disjunctive iff (.)=. (cf. [9, 19]). \n 2.3 The deterministic language In the following we consider a simple imperative language, IMP [38], \nwhere programs are commands with the following syntax: c ::=nil | x :=e | c;c | while x do (c) with e \ndenoting expressions evaluated in the set of values vwith standard operations, i.e., if v=Nthen e can \nbe any arithmetical expression. As usual, vcan be structured as a .at domains whose bottom element, ., \ndenotes the value of not initialized variables. In the following we will denote by Var(P) the set of \nvariables of the program P . IMP. Let s consider the well-known operational semantics of IMP in Table \n1 [38]. The operational semantics natu\u00adrally induces a transition relation on a set of states S, denoted \n., specifying the relation between a state and its possible successors. (S,.) is a transition system. \nIn our case, if |Var(P)| =n then S is a set of n-tuples of values, i.e., S =vn. We follow Cousot s construc\u00adtion \n[6, 10], de.ning semantics, at different levels of abstractions, as the abstract interpretation of the \nmaximal trace semantics of a transition system associated with each well-formed program. In the (e,s)-.n \n.vx (c0,s)-.s0, (c1,s0)-.s1 (nil,s)-.s  (x := e,s)-.s[n/x] (c0;c1,s)-.s1 (x,s)-.1, (c,s)-.s0, (while \nx do (c),s0)-.s1 (while x do (c),s)-.s1 (x,s)-.0 (while x do (c),s)-.s Table 1. Operational semantics \nof IMP following, S+ and S. def = N-.S denote respectively the set of .nite nonempty and in.nite sequences \nof symbols in S. Given a sequence s .S8 def = S+ .S., its length is denoted |s|.N.{.} and its i-th element \nis denoted si. A non-empty .nite (in.nite) trace s .S8 is a .nite (in.nite) sequence of program states \nwhere two consec\u00adutive elements are in the transition relation ., i.e., for all i < |s|: si .si+1. The \nmaximal trace semantics [10] of a transition sys\u00adtem associated with a program P is [PD8 def = [PD+ .[PD., \nwhere if n T .S is a set of .nal/blocking states then [PD.= {s .S+||s|= n,.i . [1,n) . si-1 .si}, [PD. \n= {s .S.|.i .N. si .si+1}, . [PD+= .n|sn-1 .T }, and [PDn = [PD. n>0{s .[PDn n[PD+.If s .[PD+, then s-and \ns1 denote respectively the .nal and ini\u00adtial state of s. The semantics [PD8 has been obtained in [10] \nas a .x-point of the monotone operator F8 : P(S8) .P(S8) de- P .ned on traces as FP 8(X)= [PD1 .[PD2.. \nX, where . is sequence concatenation. This operator provides a bi-induction (induction and co-induction) \non the complete lattice of the maximal trace se\u00ad 88 mantics (P(S8),C8 ,n,U8 ,n,S+ ,S.), where X C8 Y \nif and only if X nS+ .Y nS+ and Y nS. .X nS. . In this case: 8 [PD= lfpSC.8 F8 (see [6, 10] for details). \nThe angelic denota- P tional semantics associates input/output functions with programs, by ignoring non-termination. \nThis semantics is derived in [6] by abstract interpretation from the maximal trace semantics with ab\u00adstraction \naD (X) def . s = s1}, such that = .s .S. {s-|s .X nS+ ((S -.P(S),C),aD ,.D ,(P(S8),.)) is a GI, for some \n.D . Note that, since our programs are deterministic, aD (X)(s) is always a singleton. It is well known \nthat we can associate, inductively on its syntax, with each program P .IMP a function [PDdenoting its \ndef input/output relation, such that [PD= aD ([PD+).  3 Abstract non-interference A typical problem \nin language-based security is checking non\u00adinterference. If a user wishes to keep some data con.dential, \nhe or she might state a policy stipulating that no data visible to other users is affected by modifying \ncon.dential data. This policy allows programs to manipulate and modify private data, as long as visi\u00adble \noutputs of those programs do not reveal information about the data. A policy of this sort is a non-interference \npolicy [21], since it states that con.dential data may not interfere with public data. Non-interference \nis also referred as secrecy [34], since con.dential data are considered private, labeled with H (high-level \nof secrecy), while all other data are public, labeled with L (low-level of secrecy) [14]. Secrecy is \nusually formulated saying that the .nal value of a low variable does not depend on the initial value \nof high-variables [34]. An attacker (or unauthorized user) is assumed to be allowed to view only information \nthat is not secret. The usual method for showing that secrecy holds is to verify that the attacker cannot \nob\u00adserve any difference between two executions that differ only in their secret input [22, 34]. In literature, \nwhen a variation of secret input does not cause a variation of public output, we say that the program \nhas only secure information .ows [1, 2, 5, 12, 13, 14, 23, 34]. Let [PD+ be the set of terminating traces \ns of values for the variables in a program P. In order to analyze the variables of a program as regards \nthe given set of security classes {H, L}, we consider a typing function t .Var -.{H,L}, which associates \nwith each variable in a program its security class. In the following, if x .Var(P) then we denote x : \nt(x) the corresponding security typing. Moreover, if s .[PD+, we consider sH and sL as the projections \nof s on respec\u00adtively the values of H and L variables and we assume that any state s .S is such that \nH values come .rst. In this case, standard non\u00adinterference can be formulated as follows: A program P \nis secure if .s, d .[PD+ . sL1 = d1L. sL-= d-L. We can reformulate the de.nition above by using the denotational \nsemantics of P. In the following, if T .{H,L}, v .vn, and n = |{x .Var(P)|t(x)= T}|, T we abuse notation \nby denoting v .vthe fact that v is a possible value for the vector of variables with security type T. \nA program P is secure if LH .v .v,.v1,v2 .v. ([PD(v1,v))L =([PD(v2,v))L In order to model secrecy relatively \nto some .xed observable prop\u00aderty, we assume that any program variable can preserve secrecy only as regards \na particular amount of information, the one that the attacker can observe. We introduce a weaker notion \nof infor\u00admation .ow, which models an attacker that can observe only some properties of public (i.e., \nL) concrete values. As usual in abstract in\u00adterpretation, a property is an upper closure operator on \nthe concrete domain of computation [9]. We distinguish between the attacker s observational capability \non the inputs and on the outputs of pro\u00adgrams, by introducing two distinct properties for respectively \ninput and output on L-variables: .,. .uco(P(vL)). This leads us to the .rst generalization of standard \nnon-interference, called narrow (ab\u00adstract) non-interference. The idea is that a program satis.es narrow \nabstract non-interference relatively to a typing on security classes and a pair of closures . and ., \ndenoted (.,.)-NSecrecy, if when\u00adever the L input values have the same property . then the L output values \nhave the same . property. This capture precisely the intuition that .-indistinguishable input values \nprovide .-indistinguishable results. The following de.nition introduces the notion of narrow abstract \nnon-interference as a generalization of the standard one given by Goguen and Meseguer. DEFINITION 3.1. \nLet .,. .uco(P(vL)). A program P .IMP is (.,.)-NSecret if HL .h1,h2 .v,.l1,l2 .v. .(l1)= .(l2) . .([PD(h1,l1)L \n)= .([PD(h2,l2)L ). We write [.]P(.) to say that a program P is (., .)-NSecret. In this case, if [.]P(.) \ndoes not hold, written P |=[.]P(.), then the attacker may observe an interference due to con.dential \ndata-.ow. EXAMPLE 3.2. Consider the property Sign and Par represented in Figure 1 and the program: def \nP = l := 2 *l *h2; with security typing: t = (h : H,l : L)and v= 1. Clearly in the stan\u00ad 11 / / ///// \n ///// 0- 0+ 21+ 1 V 21 /V ///// V V V 0 VV V 00 Figure 1. The Sign and Par domains. dard notion of \nsecrecy there is a .ow of information from variable h to variable l, since l depends on the value of \nh, i.e., the statement is not secure. Let s see what happens for the (Sign, Par)-NSecrecy. Clearly we \nhave that if the input is such that Sign(l)= 0+, then the possible outputs, depending on h, are always \nin 21. The same holds if Sign(l)= 0-. Therefore any possible output value, with a .xed input l, has the \nsame abstraction in Par, which is 21. Hence we have that [Sign]P(Par) holds. It is worth noting that \nP |=[.]P(.) does not necessarily imply an information .ow from H to L values. In this case it is possible \na deceptive .ow due to the .-undistinguished public values. This means that there may be no information \n.ow from H to L as regards as the properties . for the input and . for the output. Clearly the more . \nis precise, the less deceptive .ows may appear. EXAMPLE 3.3. Consider the property Sign and Par represented \nin Figure 1 and the program in Example 3.2. Let us consider (Par,Sign)-NSecrecy. In this case note that \nPar(-2)= Par(4)= 21, but Sign([PD(h,-2)L )= 0- = 0+= Sign([PD(h, 4)L ). This means that [Par]P(Sign) \ndoesn t hold due to a deceptive .ow gener\u00adated by a variation of low inputs having the same property \nin Sign. In order to avoid the presence of deceptive .ows we consider the possibility of passing abstract \nL values as input for program s se\u00admantics. The idea is to model only information .ows generated by the \nvariation of H values. This is obtained by computing the concrete semantics with abstract values for \nL inputs, denoting .\u00adundistinguished data. The same idea can be applied to con.den\u00adtial data, in order \nto model properties of con.dential resources that does not .ow into public variables. In this case we \nmodel when the change of a particular property in the private input has effects in what the attacker \ncan observe concerning the properties of public output. We introduce a weaker notion of non-interference \nhaving no deceptive .ows, such that, when the attacker is able to observe the property . of public input \nand the property . of public output, then no information .ow concerning the property f of private input \ninterferes in the observable property . of the public output, under the assumption that the public input \nproperty . doesn t change. In this case the abstraction f represents the property of private data that \nmay .ow into the public variables, given an attacker that can observe . on public input and . on public \noutput. We call this no\u00adtion abstract non-interference, denoted (.,f,.)-Secrecy, as regards as the closures \n.,. .uco(P(vL)), and f .uco(P(vH)). DEFINITION 3.4. Let .,. .uco(P(vL)) and f .uco(P(vH)). P .IMP is \n(.,f,.)-Secret if HL .h1,h2 .v,.l1,l2 .v. .(l1)= .(l2) ..([PD(f(h1),.(l1))L )= .([PD(f(h2),.(l2))L ). \nIn the following when a program P is (.,f,.)-Secrecy we will write (.)P(f .[].). Next example shows the \ndifference between narrow and abstract non-interference. EXAMPLE 3.5. Consider the property Sign and \nPar repre\u00adsented in Figure 1 and the program in Example 3.2. Con\u00adsider (Par, id, Sign)-Secrecy. Then \nSign([PD(h, Par(-2))L )= Sign([PD(h,Par(4))L )= Sign(21*h2)= 1. In general we can prove that (Par)P(id.[]Sign) \nholds. Hence no more deceptive .ows can be revealed. It is clear that standard secrecy [34] based on \nGoguen and Meseguer s non-interference is (id,id)-NSecrecy, which is equiva\u00adlent to (id,id,id)-Secrecy, \ni.e., [id]P(id)=(id)P(id.[]id). More in general it is straightforward to prove the following implications \nfor any P .IMP, where the second implication is consequence of deceptive .ows in [.]P(.). [id]P(id) .[.]P(.) \n.(.)P(id.[].) .(.)P(f .[].) EXAMPLE 3.6. Consider the properties Sign and Par described above and the \nprogram fragment: def P = l := l *h2; with security typing: t = (h : H,l : L) and v= 1. Con\u00adsider (id,id,Par)-Secrecy. \nNote that Par([PD(2,1)L )= Par(4)= 21while Par([PD(3,1)L )= Par(9)= 21+ 1, which are clearly different, \ntherefore in this case (id)P(id.[]Par) doesn t hold. On the other hand consider (id,Sign,Par)-Secrecy. \nNote that Par([PD(Sign(2),1)L )= Par([PD(Sign(3),1)L )= Par(0+) = 1.In this case it is simple to check \nthat (id)P(Sign .[]Par) holds. Abstract non-interference is parametric on program properties spec\u00adi.ed \nas closure operators. In order to better understand the mean\u00ading of input/output abstractions in the \nde.nitions above, we observe that the property (.)P(f .[]{T}) always holds. Indeed if a closure identi.es \nsome object, then every more abstract closure will iden\u00adtify at least the same objects. From these simple \nobservations we derive the following basic properties of narrow and abstract non\u00adinterference. PROPOSITION \n3.7. Let .,. .uco(P(vL)), f .uco(P(vH)), and P .IMP. 1. [.]P(.) ..\u00df ;. . [.]P(\u00df); 2. [.]P(.) ..\u00df C. \n. [\u00df]P(.);  . 3. .i. [.]P(.i) .[.]P(i .i) and [.]P(. i .i); 4. [id]P(.) .(id)P(id.[].); 5. (.)P(f \n.[].) ..\u00df ;. . (.)P(f .[]\u00df);  . . 6. .i. (.)P(f .[].i) .(.)P(f .[] i .i) and (.)P(f .[]i .i). 4 Checking \nabstract non-interference In this section we derive the abstract non-interference semantics of a programming \nlanguage by abstract interpretation of its the max\u00adimal trace semantics. The abstract non-interference \nsemantics of a program is the set of all the denotations, viz. functions, repre\u00adsenting computations \nfor P that satisfy (.)P(f .[].). Formally we de.ne the domain \u00a7(.,f .[].) parametric on the abstract \ndomains .,. . uco(P(vL)) and f . uco(P(vH)). This domain is an abstrac\u00adtion of the concrete domain of \nthe angelic denotational semantics n S -. P(S) as introduced in [6], where S = v. . . HL .h1,h2 . v,.l1,l2 \n. v. . . . . def .(l1)= .(l2) . \u00a7(.,f .[].)= f . S -. P(S) . .( f (f(h1),.(l1))L )= . . . .( f (f(h2),.(l2))L \n) The key point in order to show that \u00a7(.,f .[].) is an abstraction of aD (P(S8)), is to note that if \nf doesn t satisfy abstract non\u00adinterference, i.e., f ./\u00a7(.,f .[].), then there is no way to make it satisfy \nthat property by adding traces, i.e., .g ; f . g ./\u00a7(.,f .[].). On the other hand if f . \u00a7(.,f .[].), \nthen .g C f . g . \u00a7(.,f .[].). PROPOSITION 4.1. Let .,. . uco(P(vL)) and f . uco(P(vH)). 1. \u00a7(.,f .[].) \nis a Moore-family; 2. (\u00a7(.,f .[].),a,id,aD (P(S8))) is a GI where if we consider f . aD (P(S8)) then \n . .x. S if . d1 ,s1. S ..(sL )= .(dL ). . 11 . .( f (f(sH ),.(sL ))L )= a( f )= 11 . .( f (f(sH ),.(dL \n))L ) . 11 f otherwise An analogous result holds in the narrow case. Therefore, checking for (.)P(f .[].) \nmeans checking whether a([PD)= [PD. This check boils down to a standard static program analysis. Let \nP . IMP, .,. . uco(P(vL)) and f . uco(P(vH)) be the observable proper\u00adties that characterize the attacker. \nWe observe that it is possible to monitor the possible .ows of information from variables of type H to \nvariables of type L, by considering the best correct approximation of [PDgiven by ., ., and f. THEOREM \n4.2. Let ., . . uco(P(vL)) and f . uco(P(vH)). H (.)P(f .[].) iff .Y . .(P(vL)). .x . v. .([PD(f(x),Y)L \n) is constant. H [.]P(.) iff .Y . .(P(vL)). .x . v,y . Y. .([PD(x, y)L ) is constant. Hence checking \nabstract non-interference corresponds to check whether the best correct approximation of the concrete \nsemantics of P, restricted to H variables in input and L variables in output, is con\u00adstant. A similar \nresult holds for narrow abstract non-interference, even though checking for the narrow case would not \ninvolve the best correct approximation of the concrete semantics but rather the concrete semantics itself. \nIt is clear that, if f and . are complete ab\u00adstractions for . and [PD(see [20]), then [.]P(.) iff (.)P(f \n.[].). Fi\u00adnally, note that abstract non-interference, as well as secrecy [34], is not in general a safety \nproperty unless no possible change is observ\u00adable on L variables, i.e., in general only .D (\u00a7(.,f .[]T)) \nis safety, i.e., it is a pre.x-closed set of traces. 5 Deriving attackers In this section we de.ne systematic \nmethods for deriving attack\u00aders from programs by abstract interpretation. In particular we are interested \nin characterizing the most concrete, viz. most precise, attacker for which a given program is secure. \nThis is useful both in automatic program certi.cation for abstract non-interference and in order to classify \nprograms in terms of the properties that make them secure. Since attackers are characterized as pairs \nof abstract domains, the idea is to de.ne an abstract domain transformer, de\u00adpending on the program to \nbe analyzed, which is able to trans\u00adform any non-secret abstraction . into the closest abstraction for \nwhich the program is secure. This corresponds to characterize the most concrete, viz. most precise, attacker \nfor which a given program is secure. This clearly has sense, in both narrow and abstract non-interference, \nif we mean to simplify domains: By Proposition 3.7, any re.nement of non-secret output abstraction is \nstill non-secret. Let P be a program, ., . . uco(P(vL)), and f . uco(P(vH)). Assume that the program \nP is not (.,.)-NSecret [resp. (.,f,.)-Secret]. We assume .xed the input-abstraction . and the private \nproperty f. We know by Proposition 3.7 that the most concrete \u00df ; . such that [.]P(\u00df) [resp. (.)P(f .[]\u00df)], \nalways ex\u00adists unique. We call this domain the narrow [resp. abstract] secret kernel of . for P. As usual \nin systematic abstract domain design [9, 18], we specify secret kernels by corresponding abstract domain \ntransformers, K P, [.],K P, (.), f : uco(P(vL))-. uco(P(vL)): def { } K P, [.]= ... n \u00df. C \u00df . [.]P(\u00df) \ndef { } K P, (.), f = ... n \u00df. C \u00df . (.)P(f .[]\u00df) In order to characterize these abstract domain transformers, \nwe have to identify when a program property, viewed as a collection of val\u00adues, is secure. Program properties \nare closure operators on sets of possible values. This means that we have to characterize which set of \nvalues are allowed in an abstract domain . such that [.]P(.) [resp. (.)P(f .[].)]. For any l . vL we \nconsider the following sets: ..{ H } (l) def lPI= [PD(h,y)L .(y)= .(l), h . vI(l) def H .l., P f= { \n[PD(f(h),.(l))L h . v} These sets represent the collections of the sets of values that any secure property \n. has not to distinguish, i.e., which have to be ab\u00adstracted by . into the same object. This means that \nany secure . has not to distinguish elements in .l.PI[resp. ..l, P f I]. Let e = . or e = .,f. We de.ne \nthe predicate Secre(X) for any X . v lPIL: SecrelPI(X) iff L .l . v. (.Z . .e(l) . Z . X ..W . .e(l) \n.W . X) lPI lPI Then Secre(X) holds if X does not brake any .e(l), namely lPI lPI X contains all the \nsets in .e(l) or none of them. In the follow\u00ad lPI ing we denote SlePI: P(P(vL))-. P(P(vL)) the predicate \ntrans\u00ad { } former SlePI(X)= X . XSecrlePI(X) . The following result proves that Secr precisely determines \nthe secret kernel. THEOREM 5.1. Let . . uco(P(vL)) and f . uco(P(vH)). { } K P, [.](id)= X . P(vL) Secr.PI; \nl(X)  K P, (.), f(id)=X . P(vL) Secr.,f(X). lPI Therefore the secret kernels of a generic domain . can \nbe de\u00ad.ned respectively as: K P, [.]= ... K P, [.](id) U . and K P, (.), f = ... K P, (.), f(id) U .. \nK is an abstract domain simpli.cation [18], namely K .uco(uco(P(C))), mapping insecure abstract domains \nto the most concrete of their secure abstractions. As observed above, any secure abstraction has not \nto distinguish the sets in .. (l) [resp. ..,f (l)]. Hence, for any . . uco(P(vL)), we con\u00ad lPIlPI sider \nthe following family of sets of possible values: (.) def {} HL l= [PD(v,.(y))L y .v lPI Note that l(.) \nis not in general a Moore-family. lPI EXAMPLE 5.2. Consider the program fragment def P = while h do (l \n:= l *2; h := 0) Its single-step standard denotational semantics is: { (h,l) if h = 0 [PD(h,l)= (0,l \n*2) otherwise Then for each l . 1we have [PD(1,l)L = {l,2l}, hence l(id)= {{0},{1,2},{2,4},{3,6},{4,8},...}, \nwhich is clearly lPI not a Moore-family. In order to make lan abstract domain we have to add to lPI .(l(.)) \nall the sets of possible values that don t violate the pred\u00ad lPI icate Secr. In particular, it is worth \nnoting that the principal .lter of any set in lis a possible candidate for being in the secret kernel, \nlPI unless it violates the predicate Secr. Consider Sle PI(.(l(.))). lPI Note that also this set may \nnot be in general a Moore-family. EXAMPLE 5.3. Let us consider the Example 5.2 above. We build Slid PI(.(l(id))) \nby checking those X ..(l(id)) such lPIlPI that Secrid P (X) holds. Note that the elements that have not \nto be distinguished by the abstraction are collected in the sets: .id 2n def {} (l)= {l,2l}. If we denote \nby {2}N= n .N, then lPI y{} S e (.(llPI(id)))= (n{2}Nn .2N+ 1 .{{0}}).It is lPI worth noting that this \nis a partition of N. lPIlPI lPI(l3) Figure 2. The construction of S e (.(l(.))). lPIlPI In Figure 2 \nwe have an example on how Sle PI(.(l(.))) is con\u00ad lPIstructed. Assume lP (.)= {d1,d2,d3,d4}. We note \nthat d2 brakes the set .e (l1) and d3 cause the brake of .e (l4), there\u00ad lPIlPI fore Secr(d2) and Secr(d3) \ndon t hold. The operation Sle PIerases d2 and d3 from .(l(.)). With similar argument we erase lPI from \n.(l(.)) all the points that do not satisfy Secr. The re\u00ad lPI S maining points are circled in the picture, \nand they form the set e (.(l(.))). We call these points relevant elements of the llPIIlPI secret kernel. \nThis is not suf.cient in order to get the whole secret kernel. Indeed there are elements of the concrete \ndomain of com\u00adputation that are not involved in the veri.cation of secrecy, namely in the observed output \nof the program. We call these elements ir\u00adrelevant: { HL def .h .v, l .v. Irrf,. = X .P(vL) lPI X / ..([PD(f(h),.(l))L \n) Irrelevant elements are also important in order to characterize the elements that reveal deceptive \n.ows in narrow abstract non\u00adinterference. Suppose the output closure we are checking for se- H crecy \ncontains an element X . [PD(h,.(l))L , for some h . v, l . vL, and X = 0, then clearly there exists y1 \n. .(l) such that [PD(h,y1)L . X and y2 . .(l) such that [PD(h,y2)L ./X. In this situation the revealed \n.ow for [PD(h,y1)L and [PD(h,y2)L , due to the presence of X in the closure, is clearly a deceptive .ow. \nThese sets, revealing deceptive .ows, are in general irrelevant elements in the abstract non-interference \ncase. This because in the abstract non-interference case we are interested in abstracting [PD(h,.(l))L \n. Since X .[PD(h,.(l))L .[PD(f(h),.(l))L , X falls in the set of irrelevant elements for abstract non-interference. \nIn particular, the sets X which reveal deceptive .ows are contained in Irrid,. \"Irrid,id . Note also \nthat for the narrow case we have to consider the more concrete closure that induces the same partition \nlPIlPI y{} L =[x].of values as . .uco(P(vL)), i.e., P (.) def (x .v), def {} where [x]. = y .(x)= .(y) \n. This is essential in order get the secret kernel, i.e., the most concrete domain \u00df such that [.]P(\u00df). \nThe idea is that P (.) is the most concrete closure such that for any y .P (.(x)): P (.(x)) = P (.(y)), \nwhile in general .(y) ..(x). EXAMPLE 5.4. Consider the following program fragment: def P = while h do \n(l := l + 6;h := h -1) with security typing t = (h : H,l : L) and v= 1. Let us con\u00adsider [.][PD(id), \nwhere .(P(1)) = {1,31,61,0}. We show that if we consider . instead of P (.) we don t .nd the secret ker\u00adnel \nof the program. First of all note that .l . 61lPI(l)= ... ... 61and .l. .(l)= 31lPI(l)= 61+ 3. Finally \nif l ./31, {} lPIl /then .. (l)= .61+ l .31which is 1\"31. At this point l(.)= {61,31,1}, and it is simple \nto verify that S. (.(l(.))) = {1,1\"(3(21+ 1)),31,61}. Let us consider lPI lPIlPI y now P (.)= ({1,1\"31,3(21+ \n1),61,0}). Then we have that y l(P (.))= ({61,3(21+ 1),1\"31}). At this point it is sim\u00adple to show that \nS. (.(l(P (.)))) = l(P (.)) which strictly lPI lPIlPIlPI contains the set Sl. PI(.(l(.))) obtained above. \nlPI We are now in the position to specify the secret kernel for both the narrow and abstract non-interference. \ndef [.][PD(id)= S. (.(l(P (.)))) .Irrid,id lPIlPIlPI def (.)[PD(f .[]id)= S.,f (.(l(.))) .Irrf,. lPIlPIlPI \nNote that these de.nitions introduce a slight abuse of nota\u00adtion: While (.)[PD(f .[]id) . uco(P(vL)) \nis an abstract domain, (.)P(f .[]id) is a program property. The same holds for the narrow case. These \nnotions have a strong relation as speci.ed by the fol\u00adlowing result which provides a domain-theoretic \ncharacterization of the objects in the secret kernels. THEOREM 5.5. Let .,. .uco(P(vL)) and f .uco(P(vH)). \n1. K P,[.](id)=[.][PD(id); 2. K P,(.),f(id)=(.)[PD(f .[]id). The following examples show the complete \nconstruction for both narrow and abstract non-interference. EXAMPLE 5.6. Consider the program fragment: \ndef P = while h do (l := l * 2; h := h -1) with security typing (h : H,l : L) and v= N. We noted in the \nintro\u00adduction that this fragment is secure as regards as the property Sign, while it is not secure as \nregards as Parity. We .nd here the most concrete property (that clearly has to contain Sign) that makes \nP secure. The denotational semantics is: { (h,l) if h = 0 [PD(h,l)= (0,l * 2h) otherwise We compute the \nclosure [id][PD(id). If l = 1 we have HH [PD(v,1)L = {2}N , if l = 2 then [PD(v,2)L = 2{2}N , LH and \nfor each l . vwe have [PD(v,l)L = l{2}N , i.e., {} llPI(id)= n{2}Nn . N. Moreover for each n we have \nthat .id (n)= n{2}N. Clearly not all these elements are secret, indeed lPI for each n we have n{2}N. \n2n{2}Nand this implies that n . .(n) but n ./2n{2}N , while 2n . .(n) and 2n . 2n{2}N . Hence \u00acSecrid \n(2n{2}N). We apply ySlid PIand we obtain the abstract do\u00ad lPI {} main Sid (.(llPI(id)))= (n{2}Nn . 2N+ \n1 .{{0}}). lPI Note that in this case the set of irrelevants is {0}. The resulting set, by Theorem 5.5, \nis the most concrete domain making P secure. EXAMPLE 5.7. Let us consider the program fragment: def P \n= l := l +(h mod 3); with security typing (h : H,l : L) and v= 1. Let us con\u00adsider (.)P(id.[]id) where \n. is the abstract domain .(P(1)) = {1,[2,4],[5,8],{5},0}. We have to compute [PD(1,{5})L = [5,7], [PD(1,[2,4])L \n=[2,6], [PD(1,[5,8])L =[5,10], and also [PD(1,1)L = 1. Therefore (.)= {[5,7],[2,6],[5,10],1}. On the \nother hand we have ..,id([2,4]) = {[2,4],[3,5],[4,6]}, llPI lPI ..,id ..,id ([5,8]) = {[5,8],[6,9],[7,10]}, \n(5)= {5,6,7} and, for all other possible low input l, we have ..,id(l)= {1} that can\u00ad lPIlPIlPI not create \nproblems to secrecy. It is possible to verify that S.,id (.(llPI(.))) is the following collection of \nobjects: lPI . . Y . [2,10] . . . . . . . . . . Y . P(v) Y \"[2,10], Y . [2,7], Y n [5,10] /.{[5,9],[5,8],[5,8] \n.{10}} . Y \"[2,10], Y . [5,10], Y n [2,7] /.{[4,7],[3,7],[4,7] .{2}} . . . . . . . An approximation \ncan be introduced in the derivation of the secret kernel by separately analyzing program fragments. In \nthe follow\u00ading we show two methods for approximating the secret kernel of a closure . for a program P, \nby inductively deriving the kernels of program components. Bounded iterations Let [PD(n) represents the \npartial semantics of the program at the n\u00adth step of evaluation, supposing that all while s are unfolded: \nIf P = c0;c1;...;cm, then for any n de.ne: def [PD(0) [c0D = def [PD(n+1) [cn+1D. [PD(n) = For instance, \n[PD(2) is the semantics of c0;c1;c2, that is [PD(2) = [c2D. [c1D. [c0D. We de.ne an abstract domain transformer \nK par P,[.] [resp. K par P,(.),f] denoting the common abstraction among all the domains .i such that \nthe .rst i-steps are (.,.i)-NSecret, [resp. T K sbs P,[.](.) Secret P( = uco(P(VL)) L) Figure 3. Deriving \nsecret kernels (.,f,.i)-Secret] i.e., for any i = n: .i =[.][PD(i)(id) [resp. (.)[PD(n)(f .[].i)], it \nis de.ned in the following way: . par def = .... U [.][PD(i)(id) P,[.] i=n . par def = .... U (.)[PD(i)(f \n.[]id) P,(.),f i=n It is clear that K P,[.] C K par P,[.](.)) still P,[.]. By Proposition 3.7, [.]P(K \npar holds. The same holds in the abstract non-interference case. Independent composition An even coarser \napproximation of the secret kernel of . can be ob\u00adtained by considering the most concrete abstraction \nmaking all pro\u00adgram statements in P (.,.)-NSecret [resp. (.,f,.)-Secret]. Sup\u00adpose that for each statement \nc in P, [cDis the denotational semantics of c. Let P = c0;c1;...;cm and de.ne: . sbs def = .... U [.][ciD(id) \nP,[.] i=m . sbs def = .... U (.)[ciD(f .[]id) P,(.),f i=n Intuitively K sbs [resp. K sbs ] is the property \nwhich is not dis\u00ad P,[.] P,(.),f closed with respect to each slice of the program P relatively to se\u00adquential \ncomposition. Note that, in this case, if there is at least one statement c such that [.][cD(id)= {T} \nthen K sbs = {T}. Also in P,[.] this case we have K P,[.] C K sbs It is clear that this is an upper P,[.]. \napproximation of both K P,[.] and K par P,[.], i.e., as shown in Figure 3: P,[.] C K par P,[.] C K sbs \nP,[.] The same holds for the abstract non-interference case. By Proposi\u00adtion 3.7, [.]P(K sbs P,[.](.)) \nstill hold. We now P,[.](.)) and (.)P(f .[]K sbs show a simple example for the narrow case, where the \nrelations P,[.] C K par P,[.] C K sbs are strict inclusions. P,[.] EXAMPLE 5.8. Consider the program \nfragment def P = l := 2 * h; l := l *h with typing (h : H,l : L) and v= 1and [P{D(h,l)=(h,2 * }h2).We \n2consider [id]P(id). We have llPI(id)= {2nn . 1}. Note that in this case the operator Slid PIis the identity \non .(llPI(id)). Hence we obtain the secret kernel by computing the irrelevant ele\u00ad y{ .{ }} ments which \nare ({m} m . 1\"2n2 n . 1). In this case, the secret kernel of P is the closure . = {}y.{} 22 .({2nn . \n1}. ({{m}|m . 1\"2nn . 1})). K par Consider now P, [id](id). In order to obtain this domain we have to \n.nd the closure .1 such that [id]l := 2h(.1). This domain {} is .1 = .({21}) .{n} n . 21+ 1 . Clearly \n[PD(2) = [PD, therefore K par (id)= .1 U .. It is worth noting that .1 .., there- P, [id] fore K parP, \n[id](id)= .1 ... Finally consider K sbs (id). Clearly, in this case, the secret kernel for the .rst statement \nis exactly .1 as de.ned in the previous case. We have to compute .2 such that [id]l := l *h(.2 ). It \nis worth noting that llPI(id) is the set of con\u00adgruences of the kind n1with n . 1. At this point the \noperator Sle PI determines the domain .2 = {1}. It is worth noting that .2 ..1, P, [id] (id)= .1 U .2 \n= .2 .K par therefore we have that K sbs P, [id](id). P, [id] Fix-point attackers We .nally consider \nthe problem of .nding the most concrete do\u00admain . . uco(P(vL)) such that [.]P(.) [resp. (.)P(f .[].)]. \nThis problem is slightly more complex and requires an iterative solution. Indeed, while simplifying the \noutput observation we do not brake secrecy, the same simpli.ed abstraction applied to the input may re\u00adveal \nsecrets. An abstract domain . such that [.]P(.) [(.)P(f .[].)] represents a possible attacker which is \nunable to disclose con.den\u00adtial data by analyzing the same property on input/output relations. We are \ninterested in characterizing the strongest of such attackers if it exists unique, later called the (canonical) \nsecure attacker for P. THEOREM 5.9. Let . . uco(P(vL)) and f . uco(P(vH)). [.]P(.) iff . =[.][PD(id); \n (.)P(f .[].) iff . =(.)[PD(f .[]id).  In order to constructively characterize the secure attachers \nof a program, we consider standard domain-theoretic arguments. It is worth noting that in the narrow \ncase [.][PD(id), the set of irrele\u00advants does not depend on the input observation .. Therefore, the change \nof the input property does not have any effect on the irrel\u00adevants Irrid, id . monoton\u00ad lPIMoreover, \nby construction, the set llPIically depends upon the input property .. Therefore we have the following \nresult. PROPOSITION 5.10. Let P be a program. .X.[X][PD(id) is mono\u00adtone on (uco(P(vL)),C). Therefore, \nby Tarski .x-point theorems, an iterative solution to the problem of deriving the most concrete secure \nattacker for a pro\u00adgram P in the narrow case can be found such that, at each step n, we .nd the most \nconcrete domain .n that satis.es [.n-1]P(.n), which is K P, [.n-1](id). By Proposition 5.10 we have that, \ngiven a program P, then .X. ([X][PD(id))= .X. K P, [X](id) is monotone on uco(P(vL)). The most concrete \nsecure attacker for the narrow case is therefore the least .x-point of .X. K P, [X](id). COROLLARY 5.11. \nlfpC .X. ([X][PD(id)) is the (unique) most id concrete narrow secure attacker for P. def In the following \nwe denote FP = lfpC .X. ([X][PD(id)). id EXAMPLE 5.12. Consider the fragment given in Example 5.6 to\u00adgether \nwith its denotational semantics. In Example 5.6 we de\u00ad def rived the most concrete domain .1 = Sle PI(.(llPI(id))) \nsuch that [id]P(.1). Consider the closure [.1][PD(id), namely the most con\u00adcrete closure that makes P \nsecure with input observation .1. Note H that .1 = P (.1). We have to compute [PD(v,Y ) for each Y . \n.1. H Namely, consider for example Y = {2}N, then [PD(v,{2}N)L = H {2}N,ifY = 3{2}Nthen [PD(v,3{2}N)L \n= 3{2}N, and so on. It is clear that we reached the .x-point. Therefore, in this example y{} FP =(n{2}Nn \n. 2N+ 1 .{{0}}). We now apply bounded iterations to approximate the closure FP, denoted FPpar. The single-step \nsemantics is: { (h,l) if h = 0 [PD(h,l)= (h - 1, l *2) otherwise LH Then, for each value l . v, we have \n[PD(v,l)L = {l,2l}. S id Hence it is possible to verify that lPI(.(llPI1 (id))) = y{} (n{2}Nn . 2N+ 1 \n.{{0}}). As for the second step is H concerned for each l . vL we have that [PD(2)(v,l)L = {l,2l,4l}. \nAgain it is possible to verify that the resulting domain is y{} (n{2}Nn . 2N+ 1 .{{0}}) which is the \nclosure F par.In P this case the irrelevants are {0}, since the range of [PDcovers the whole domain of \nvalues. In the example above we reached the .x-point in one step. Let us see an example where an in.nite \niteration is necessary. EXAMPLE 5.13. Consider the program fragment: def P = l := l2 +(h mod l) where \nmod is the rest of the integer division, the security typing is t = (h : H,l : L), and v= N. The denotational \nsemantics [PD of P is immediate from its de.nition. Then let us consider the clo\u00adsure [id]P(id), in particular \nlet us derive llPI(id). Consider for H example l = 1, then [PD(v,1)L = {1}.Ifl = 2 then we obtain H the \ninteger interval [PD(v,2)L =[4,5], and so on. The result\u00ading intervals are all disjoint, therefore Slid \nPIis the identity in this y case. The resulting closure contains (llPI(id)) together with all the elements \nthat are not contained in any of these intervals, i.e., the irrelevants. Let .1 be this closure. We consider \nnow the closure [.1][PD(id). We show only the abstraction of the single\u00adton {4} since the .rst three \nnatural numbers behave exactly as in the previous case. If we consider l = 4 we have, for example, that \nH [PD(v,[4,5])L =[16, 19] . [25,29], and so on for the other val\u00adues. In this way we may continue until \nthe .x-point characterizing the canonical secure attacker. Note that Slid PIon these domains is always \nthe identity, since all the generated intervals are disjoint. For the weaker case of abstract non-interference, \nthere are two facts that avoid in general monotonicity. First the set of irrelevants grows by abstracting \n., namely it is anti-monotone; second the set of rel\u00adevants is not comparable with the set of relevants \nobtained by ab\u00adstracting .. Therefore, in general, .X. (X)[PD(f .[]id) is not mono\u00adtone on (uco(P(vL)),C). \nEXAMPLE 5.14. Let us consider the program fragment P in Example 5.7. Let us consider two closures . C \n\u00df with sets of .x-points: .(1)= {1,[2, 4],[5,8], {5},0} and \u00df(1)= {1,[2,4], [5,8],0}. Consider .rst ., \nthen in Example 5.7 we obtained that (.)= {[5,7],[2,6],[5,10], 1}. We can llPI note that \u00acSecr.,id([2,6]) \nsince [2, 6] contains some elements of lPI .., id lPI(5) (see Example 5.7), but not all of them. This \nmeans that surely [2,6] is not included in the .nal domain, as seen in Example 5.7. Consider now \u00df, then \nwe have to compute [PD(1, [2,4])L =[2,6], [PD(1,[5,8])L =[5,10] and [PD(1,1)L = 1. Therefore llPI(.)= \n{[2,6],[5,10],1}. While .., id([2,4]) = lPI {[2,4], [3,5],[4,6]}, .., id ([5,8]) = {[5,8],[6,9],[7, 10]} \nand for all other possible low input y we have .., id(y)= {1} that clearly can\u00ad lPIlPI not create problems \nto secrecy. In this case Secr.,id([2,6]), which lPI implies that [2,6] is now left in the domain. This \nexample shows that in abstract non-interference, in gen\u00aderal, by further abstracting the input observation \nwe don t nec\u00adessarily further abstract the output. Hence, in the abstract non\u00adinterference case, the \nfact that .X. (X)[PD(f .[]id) is not mono\u00adtone, avoids us to characterize the canonical secure attacker \nby means of Tarski .x-point theorems. In order to constructively char\u00adacterize a secure attacker as the \nlimit of a possible trans.nite up\u00adper iteration sequence of .X. (X)[PD(f .[]id), we make this func\u00adtion \nextensive on (uco(P(vL)),C), i.e., we consider the function: .X. (X)[PD(f .[]id) UX. By a well known \nresult in lattice-theory we know that any extensive function has a .x-point [11]. The fol\u00adlowing result \nproves that any .x-point of .X. (X)[PD(f .[]id) UX is a secure attacker for P. THEOREM 5.15. Let . . \nuco(P(vL)) and f . uco(P(vH)).If . =(.)[PD(f .[]id) U. then (.)P(f .[].). 6 Abstract robust declassi.cation \nDeclassifying information means downgrading the sensitivity of data in order to accommodate with (intentional) \ninformation leak\u00adage. Robust declassi.cation has been introduced in [40] as a sys\u00adtematic method to drive \ndeclassi.cation by characterizing what in\u00adformation .ows from con.dential to public variables. In this \nsec\u00adtion we generalize the robust declassi.cation to security properties based on abstract non-interference. \nLet P . IMP be a program and .,. . uco(P(vL)) be abstract domains. Recall that the set of par\u00adtitions \nof a set S is isomorphic to the set of closures P (uco(P(S))) [29]. In particular if p is a partition \nof S and for any d .uco(P(S)) d is the equivalence relation such that v1 d v2 iff d(v1)= d(v2), then \nP (.), where . = p.{S, 0}= M (p) is the .at domain ordered by set-inclusion, is the (unique) most concrete \nabstract domain such that P (.) induces the same partition as p. We are interested in characterizing \nan abstraction f . uco(P(vH)) such that any pair of values which can be distinguished by f violates (., \nf, .)-Secrecy. Among these abstract domains we are interested in the most con\u00adcrete one. This domain \nprovides precisely the maximal amount of information concerning H-values that .ows in P when the attacker \nis given by the pair of input/output abstractions: (.,.). Note that P (f) is the most concrete domain \nthat induces the same partition as f, and because in abstract non-interference we abstract sin\u00adgle values, \nit is clear that we can consider f . P (uco(P(vH))).A closure f .P (uco(P(vH))) is called .ow-irredundant \nwith respect to a pair of input/output abstractions (.,.) if .X1,X2 .f(P(vH)), .Y . .(P(vL)) such that: \n.([PD(X1,Y )L )= .([PD(X2,Y )L ). The most concrete .ow-irredundant domain f .P (uco(P(vH))), when it \nexists relatively to (.,.), de.nes the maximal abstract interfer\u00adence of P, denoted (.)P(f ..). The idea \nfor deriving the most concrete .ow-irredundant domain f, such that (.)P(f ..) relies upon a similar construction \nas the one used in Section 5. Let: {{ .(.,.) def .([PD(h,.(l))L)= h .v = .(h,.(l)) H L .([PD(h',.(l))L \n) y .v where (X,Y ).(X',Y ')= (X .X',Y .Y '). It is worth noting that for def any Y ..(P(vL)), the set \npY = {X .P(vH) |(X,Y )..(.,.)}is H a partition of v. THEOREM 6.1. If P |=(.)P(id.[].) then: (.)P(P (. \nL)) M (pY )) ..). Y ..(P(V EXAMPLE 6.2. Consider the program fragment: P = l := l *h2; {} We can compute \n.(id,Par) which is the set (1,l) l .21. {}{ } (21,l) l .21+ 1 .(21+ 1, l) l .21+ 1 . Therefore by using \nthe notation above we have that if l . 21then pl = 1and ifl . 21+ 1 then pl = {21,21+ 1}. This means \nthat P (nl.zM (pl )) = Par. In other words we have that by looking at the low variables the only information \nthat leaks about the high variables is its parity. In order to adapt robust declassi.cation in [40] to \nthe abstract non\u00adinterference case, we consider a semantic variation of what pro\u00adposed in [40], which \nconsiders passive attackers only and a seman\u00adtics observing only the initial and the .nal states of computations. \nWe follow [40] in de.ning the information leaked by an equivalence relation transformer S[.,.] on S for \neach .,. .uco(P(vL)): LL s1S[.,.]s2 iff s1 . sand 2 (.s,d .S+ . s1 = s1 . d1 = s2 . sL .dL )) where s1,s2 \n.S. It is simple to verify that s1S[.,.]s2 iff LL .(s1)= .(s2 ) and .([PD(s1)L )= .([PD(s2)L ). This \nmeans that ab\u00adstract robust declassi.cation ala [40] is based on a formulation of non-interference which \nis stronger then abstract non-interference, since it may allow also deceptive .ows, as we have in the \nnar\u00adrow abstract non-interference case. This means that abstract robust declassi.cation characterizes \nthe information leaked in narrow ab\u00adstract non-interference. 7 Discussion In this paper we have weakened \nthe standard notion of non\u00adinterference, making it parametric relatively to input/output obser\u00advations \nmade by abstract interpretations. This provides a number of systematic methods for mapping programs to \nproperties according to their secure information .ow. There are many points that de\u00adserves further discussion \nand research. (1) Possible applications of abstract non-interference are security program analysis, when \nwe are interested in analyzing how much of information .ows from con.dential to public variables, and \nin automatic program certi.\u00adcation, where we are interested in releasing certi.cates on secure information \n.ows. In the latter case we prove that if P |=[.]P(.) [resp. P |=(.)P(f .[].)] then for all \u00df ; .: P \n|=[.]P(\u00df) [resp. P |=(.)P(f .[]\u00df)]. As shown in Section 5, certi.cates can be sys\u00adtematically derived \nby transforming abstract domains in the stan\u00addard abstract interpretation framework. In particular the \nderivation of certi.cates for secure information .ows strictly depends upon the way we model secure attackers. \nThe canonical secure attacker (when it exists) represents therefore the most precise property certi\u00adfying \na program as secure. While this attacker always exists unique in the narrow case, we are only able to \nprovide a .x-point repre\u00adsentative attacker for the weaker abstract case. It would be inter\u00adesting here \nto consider quantitative methods and metrics in order to enforce convergence towards an optimal .x-point \nunder some esti\u00admation of abstract domain s structure and complexity. (2) Check\u00ading abstract non-interference \ncorresponds to check whether an at\u00adtribute independent analysis between H and L values is equivalent \nto its relational counterpart. This observation makes stronger the relation between static program analysis \nand language-based secu\u00adrity. Abstract model checking can be used to check or derive cer\u00adti.cates. The \nfact that in the narrow case the secret kernel of an abstract domain is mostly a partition is not a chance. \nWe know from [17] and [29] that partitioning abstract domains, i.e., . such that . = P (.), have the \nsame precision as abstract model-checking. Therefore, abstract model checking can be used to check parametric \nnon-interference without any additional loss of precision in the nar\u00adrow case. (3) The standard notion \nof non-interference is based on a strict independence between con.dential and public data-.ows. This \nmeans that it is possible to isolate the program s con.den\u00adtial data-.ow by program slicing [37]. The \nde.nition of abstract non-interference weaken this independence, providing the base for weaker notions \nof program slicing relatively to observations spec\u00adi.ed by abstract domains. (4) Both notions of narrow \nand abstract non-interference can easily be generalized to multi-level, ala Den\u00adning, security policies. \nWe remind that the purpose of a secu\u00adrity policy is to declare which information .ows are not permitted \n[21, 32]. For example in standard secrecy, downward .ows are not allowed (H . L). If \u00a7is the set of security \nclasses for a set of vari\u00adables, then a security policy ]is a set of .ow constraints of the form S1 . \nS2, with S1,S2 . \u00a7, which states that any information stored in variables of type S1 cannot .ow into \nvariables of type S2. In par\u00adticular, if (\u00a7,=S) is a lattice as in Denning s model [13], then the security \npolicy is implicitly determined by avoiding upwards infor\u00ad {} mation .ows: ]= S1 . S2 S1,S2 . \u00a7, S1 =SS2 \n. Given a typing t . Var -. \u00a7, if for each S . \u00a7, we denote by vS the projec\u00adtion of the state v . S \non the values of variables of type S, then we can generalize abstract non-interference to arbitrary multi-level \nse\u00adcurity policies ]as follows: Let {.S }S.Sbe a family of abstract do\u00admains such that .S . uco(P(vS)). \nWe say that a program P . IMP is ({.S }S.S)-Secret if for each S1 . S2 . ], v1,v2 . S: SS .S = S1. .S \n(v1)= .S (v2 ) . .S2 ([PD((S.S.S )(v1))S2 )= .S2 ([PD((S.S.S )(v2))S2 ) S ' where for any S '. \u00a7: (S.S.S \n)(v)S ' = .S ' (v). (5) In this pa\u00adper we have been mostly interested in checking program security relatively \nto the observation made by an attacker. This is justi.ed by our interest in releasing security certi.cates \nfor programs. The dual problem of deriving the most abstract attacker . such that P is not (.,.)-NSecret \n[resp. (.,f,.)-Secret] has not been consid\u00adered here. This is an interesting problem for hackers, which \nmay be interested in the least efforts (viz. the most abstract domain) neces\u00adsary to disclose secrets. \nUnfortunately this problem may not have a solution. In general the concrete domain id can be secure, \nand therefore there are no closures that can make the program insecure. Moreover, while for making closures \nsecure we merge sets of pos\u00adsible values, in this case we would have to split sets. This is a major problem \nwhen proving that a most abstract output domain exists making a given program insecure (the dual of secret \nkernels). In\u00addeed, in most of the cases, any possible partition of a secure domain may reveal interference. \nConsider for instance the program in Ex\u00adample 5.12. In this example we proved that a closure that makes \ny{} the program secure is . = def (n{2}Nn . 2N+ 1 .{{0}}). Then by the construction it is worth noting \nthat if we distinguish the elementy{1}, the program is no more secure, namely by considering .' def ({n{2}N|n \n. 2N+ 1 = \"{1}} .{{2}N'{0}}. {{0},{1}}). The same situation happens if we distinguish the element .''{2} \ndef instead, namely if we consider the abstract domain = y ({n{2}N|n . 2N+ 1 \"{1}} .{{2}N'{1}}. {{0},{2}}). \nThe problem here is that .' and .'' are unrelated, and their common abstraction still makes the program \nsecure: . = .'U .'' . (6) Our construction captures precisely the idea of modeling attackers as ab\u00adstract \ninterpretations, namely as processes acting as static program analyzers whose aim is to disclose con.dential \ndata. The language we consider is rather restricted, even though the idea of making non-interference \nparametric on input/output abstractions can be in principle extended to all programming languages having \na formally de.ned semantics and for which abstract interpretation applies. In this respect we believe \nthat (narrow) abstract non-interference and secrecy can be extended to non-deterministic and concurrent \n(syn\u00adchronous or asynchronous) programs, as well as to programs with probabilistic choice. The latter \ncase is more delicate, as covert channels may in.uence secrecy. Probabilistic, termination, timing, and \nresource exhaustion channels require an adequate semantics modeling stochastic properties [27], termination \n[6], time proper\u00adties or resource usage. This is particularly evident when we are in\u00adterested in extending \nattackers, viewed as abstract interpretations, to other programming languages where the attacker can \nbene.t from speci.c control features to improve its precision. In this case more concrete semantics, \nsuch as the natural semantics modeling termi\u00adnating and non-terminating computations of a transition \nsystem [6], would provide a more adequate semantics-base. Acknowledgments The interest in weakening non-interference \nby abstract interpreta\u00adtion came to our attention during the .nal meeting of the Italian-MURST Project \nCOFIN99: Automatic Program Certi.cation by Abstract Interpretation. We wish to thank the anonymous referees, \nSamir Genaim, and Massimo Merro for their helpful comments, and all the colleagues at the Dagstuhl seminar \n03411 on Language-Based Security for pointing us relevant literature and comments on an early version \nof this paper. This work is partly supported by the Italian-MIUR Projects COFIN02-CoVer: Constraint-based \nveri.\u00adcation of reactive systems and FIRB: Abstract interpretation and model checking for the veri.cation \nof embedded systems. 8 References [1] BELL, D. E., AND BURKE, E. A software validation tech\u00adnique for \ncerti.cation: The methodology. Tech. Rep. MTR\u00ad2932, MITRE Corp. Badford, MA, 1974. [2] BELL, D. E., AND \nLAPADULA, L. J. Secure computer sys\u00adtems: Mathematical foundations and model. Tech. Rep. M74\u00ad244, MITRE \nCorp. Badford, MA, 1973. [3] BODEI, C., DEGANO,P., NIELSON,F., AND NIELSON,H. Static analysis for secrecy \nand non-interference in networks of processes. In Proc. of 6th Internat. Conference on Paral\u00adlel Computing \nTechnologies : (PaCT 01) (2001), vol. 2127 of Lecture Notes in Computer Science, Springer-Verlag, pp. \n27 41. [4] CLARK, D., HANKIN, C., AND HUNT, S. Information .ow for algol-like languages. Computer Languages \n28, 1 (2002), 3 28. [5] COHEN, E. S. Information transmission in computational systems. ACM SIGOPS Operating \nSystem Review 11,5 (1977), 133 139. [6] COUSOT, P. Constructive design of a hierarchy of semantics of \na transition system by abstract interpretation. Theor. Com\u00adput. Sci. 277, 1-2 (2002), 47,103. [7] COUSOT,P., \nAND COUSOT, R. Abstract interpretation: A uni.ed lattice model for static analysis of programs by con\u00adstruction \nor approximation of .xpoints. In Conference Record of the 4th ACM Symp. on Principles of Programming \nLan\u00adguages (POPL 77) (1977), ACM Press, New York, pp. 238 252. [8] COUSOT,P., AND COUSOT, R. Constructive \nversions of Tarski s .xed point theorems. Paci.c J. Math. 82, 1 (1979), 43 57. [9] COUSOT,P., AND COUSOT, \nR. Systematic design of program analysis frameworks. In Conference Record of the 6th ACM Symp. on Principles \nof Programming Languages (POPL 79) (1979), ACM Press, New York, pp. 269 282. [10] COUSOT,P., AND COUSOT, \nR. Inductive de.nitions, seman\u00adtics and abstract interpretation. In Conference Record of the 19th ACM \nSymp. on Principles of Programming Languages (POPL 92) (1992), ACM Press, New York, pp. 83 94. [11] DAVEY, \nB. A., AND PRIESTLEY,H. A. Introduction to Lat\u00adtices and Order. Cambridge University Press, Cambridge, \nU.K., 1990. [12] DENNING,D. E. Secure Information Flow in Computer Sys\u00adtems. PhD thesis, Purdue University, \n1975. [13] DENNING, D. E. A lattice model of secure information .ow. Communications of the ACM 19, 5 \n(1976), 236 242. [14] DENNING,D.E., AND DENNING, P. Certi.cation of pro\u00adgrams for secure information \n.ow. Communications of the ACM 20, 7 (1977), 504 513. [15] DI PIERRO, A., HANKIN, C., AND WIKLICKY, H. \nApprox\u00adimate non-interference. In Proc. of the IEEE Computer Secu\u00adrity Foundations Workshop (2002), IEEE \nComputer Society Press, pp. 1 17. [16] FILE\u00b4, G., GIACOBAZZI, R., AND RANZATO, F. A unifying view of \nabstract domain design. ACM Comput. Surv. 28,2 (1996), 333 336. [17] GIACOBAZZI, R., AND QUINTARELLI, \nE. Incompleteness, counterexamples and re.nements in abstract model-checking. In Proc. of The 8th Internat. \nStatic Analysis Symposium, (SAS 01) (2001), P. Cousot, Ed., vol. 2126 of Lecture Notes in Computer Science, \nSpringer-Verlag, pp. 356 373. [18] GIACOBAZZI, R., AND RANZATO, F. Re.ning and com\u00adpressing abstract \ndomains. In Proc. of the 24th Internat. Col\u00adloq. on Automata, Languages and Programming (ICALP 97) (1997), \nP. Degano, R. Gorrieri, and A. Marchetti-Spaccamela, Eds., vol. 1256 of Lecture Notes in Computer Science, \nSpringer-Verlag, Berlin, pp. 771 781. [19] GIACOBAZZI, R., AND RANZATO, F. Optimal domains for disjunctive \nabstract interpretation. Sci. Comput. Program. 32, 1-3 (1998), 177 210. [20] GIACOBAZZI, R., RANZATO,F., \nAND SCOZZARI, F. Mak\u00ading abstract interpretations complete. J. of the ACM. 47,2 (2000), 361 416. [21] \nGOGUEN,J.A., AND MESEGUER, J. Security policies and security models. In Proc. IEEE Symp. on Security \nand Privacy (1982), IEEE Computer Society Press, pp. 11 20. [22] GOGUEN,J.A., AND MESEGUER, J. Unwinding \nand infer\u00adence control. In Proc. IEEE Symp. on Security and Privacy (1984), IEEE Computer Society Press, \npp. 75 86. [23] JOSHI, R., AND LEINO, K. R. M. A semantic approach to secure information .ow. Sci. Comput. \nProgram. 37 (2000), 113 138. [24] LANDAUER, J., AND REDMOND, T. A lattice of informa\u00adtion. In Proc. of \nthe IEEE Computer Security Foundations Workshop (1993), IEEE Computer Society Press, pp. 65 70. [25] \nLAUD, P. Semantics and program analysis of computation\u00adally secure information .ow. In In Programming \nLanguages and Systems, 10th European Symposium On Programming, (ESOP 01) (2001), vol. 2028 of Lecture \nNotes in Computer Science, Springer-Verlag, pp. 77 91. [26] LOWE, G. Quantifying information .ow. In \nProc. of the IEEE Computer Security Foundations Workshop (2002), IEEE Computer Society Press, pp. 18 \n31. [27] MONNIAUX, D. An abstract analysis of the probabilistic ter\u00admination programs. In Proc. of The \n8th Internat. Static Anal\u00adysis Symposium, (SAS 01) (2001), vol. 2126 of Lecture Notes in Computer Science, \nSpringer-Verlag, pp. 111 127. [28] \u00d8RB\u00c6K, P. Can you trust your data? In Proc. of the 6th Inter\u00adnat. \nJoint Conf. CAAP/FASE, Theory and Practice of Software Devolpment (TAPSOFT 95) (1995), P. Mosses, M. \nNielsen, and M. Schwartzbach, Eds., vol. 915 of Lecture Notes in Com\u00adputer Science, Springer-Verlag, \npp. 575 589. [29] RANZATO,F., AND TAPPARO, F. Making abstract model checking strongly preserving. In \nProc. of The 9th Internat. Static Analysis Symposium, (SAS 02) (2002), M. Hermenegildo and G. Puebla, \nEds., vol. 2477 of Lecture Notes in Computer Science, Springer-Verlag, pp. 411 427. [30] SABELFELD, A., \nAND MYERS, A. Language-based information-.ow security. IEEE J. on selected ares in com\u00admunications 21, \n1 (2003), 5 19. [31] SABELFELD, A., AND SANDS, D. A PER model of secure information .ow in sequential \nprograms. Higher-Order and Symbolic Computation 14, 1 (2001), 59 91. [32] SCHNEIDER,F., MORRISETT, G., \nAND HARPER,R. A language-based approach to security. In In Informatics: 10 Years Back, 10 Years Ahead \n(2000), vol. 2000 of Lecture Notes in Computer Science, Springer-Verlag, pp. 86 101. [33] SKALKA, C., \nAND SMITH, S. Static enforcement of se\u00adcurity with types. In Proc. Internat. Conference on Func\u00adtional \nProgramming (ICFP 00) (2000), ACM Press, New York, pp. 254 267. [34] VOLPANO, D. Safety versus secrecy. \nIn Proc. of The 6th Internat. Static Analysis Symposium (SAS 99) (1999), A. Cortesi and G. Fil\u00b4e, Eds., \nvol. 1694 of Lecture Notes in Computer Science, Springer-Verlag, pp. 303 311. [35] VOLPANO, D., AND SMITH, \nG. Verifying secrets and relative secrecy. In Conference Record of the 27th ACM Symp. on Principles of \nProgramming Languages (POPL 00) (2000), ACM Press, New York, pp. 268 276. [36] VOLPANO, D., SMITH, G., \nAND IRVINE, C. A sound type system for secure .ow analysis. J. of Computer Security 4, 2,3 (1996), 167 \n187. [37] WEISER, M. Program slicing. IEEE Transactions on software engineering 10, 4 (1984), 352 357. \n[38] WINSKEL,G. The formal semantics of programming lan\u00adguages: an introduction. MIT Press, 1993. [39] \nZANOTTI, M. Security typings by abstract interpretation. In Proc. of The 9th Internat. Static Analysis \nSymposium, (SAS 02) (2002), M. Hermenegildo and H. Puebla, Eds., vol. 2477 of Lecture Notes in Computer \nScience, Springer-Verlag, pp. 360 375. [40] ZDANCEWIC, S., AND MYERS, A. C. Robust declassi.ca\u00adtion. \nIn Proc. of the IEEE Computer Security Foundations Workshop (2001), IEEE Computer Society Press, pp. \n15 23.  \n\t\t\t", "proc_id": "964001", "abstract": "In this paper we generalize the notion of non-interference making it parametric relatively to what an attacker can analyze about the input/output information flow. The idea is to consider attackers as data-flow analyzers, whose task is to reveal properties of confidential resources by analyzing public ones. This means that no unauthorized flow of information is possible from confidential to public data, relatively to the degree of precision of an attacker. We prove that this notion can be fully specified in standard abstract interpretation framework, making the degree of security of a program a property of its semantics. This provides a comprehensive account of non-interference features for language-based security. We introduce systematic methods for extracting attackers from programs, providing domain-theoretic characterizations of the most precise attackers which cannot violate the security of a given program. These methods allow us both to compare attackers and program secrecy by comparing the corresponding abstractions in the lattice of abstract interpretations, and to design automatic program certification tools for language-based security by abstract interpretation.", "authors": [{"name": "Roberto Giacobazzi", "author_profile_id": "81100572239", "affiliation": "Universit&#224; di Verona, Verona, Italy", "person_id": "PP14198144", "email_address": "", "orcid_id": ""}, {"name": "Isabella Mastroeni", "author_profile_id": "81100583303", "affiliation": "Universit&#224; di Verona, Verona, Italy", "person_id": "P117043", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/964001.964017", "year": "2004", "article_id": "964017", "conference": "POPL", "title": "Abstract non-interference: parameterizing non-interference by abstract interpretation", "url": "http://dl.acm.org/citation.cfm?id=964017"}