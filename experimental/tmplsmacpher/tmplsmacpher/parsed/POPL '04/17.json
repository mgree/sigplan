{"article_publication_date": "01-01-2004", "fulltext": "\n The Space Cost of Lazy Reference Counting Hans-J. Boehm HP Laboratories 1501 Page Mill Rd. Palo Alto, \nCA 94304 Hans.Boehm@hp.com Abstract Reference counting memory management is often advocated as a technique \nfor reducing or avoiding the pauses associated with trac\u00ading garbage collection. We present some measurements \nto remind the reader that classic reference count implementations may in fact exhibit longer pauses than \ntracing collectors. We then analyze reference counting with lazy deletion, the stan\u00addard technique for \navoiding long pauses by deferring deletions and associated reference count decrements, usually to allocation \ntime. Our principal result is that if each reference count operation is con\u00adstrained to take constant \ntime, then the overall space requirements can be increased by a factor of .(R)in the worst case, where \nR is the ratio between the size of the largest and smallest allocated ob\u00adject. This bound is achievable, \nbut probably large enough to render this design point useless for most real-time applications. We show \nthat this space cost can largely be avoided if allocating an n byte object is allowed to additionally \nperform O(n)reference counting work. Categories and Subject Descriptors D.3.3 [Programming Languages]: \nLanguage Constructs and Features Dynamic storage management; D.4.2 [OperatingSystems]: Storage Management \n Garbage col\u00adlection General Terms Languages, Performance.  Keywords Garbage collection, reference \ncounting, memory allocation, space complexity. Permission to make digital or hard copies of all or part \nof this work for personal or classroom use is granted without fee provided that copies are not made or \ndistributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. POPL 04, January 14 16, 2004, Venice, Italy. Copyright 2004 ACM \n1-58113-729-X/04/0001 ...$5.00 1 Introduction Reference counting[8, 14] is a commonly used technique \nfor au\u00adtomatically, or semi-automatically deallocating unreferenced mem\u00adory and other resources. It has \nbeen used with great success in some applications, e.g. in Unix/Linux .le systems. It has also often \nbeen suggested and used as a general purpose memory management dis\u00adcipline for programming language objects \n(cf. [11, 10, 1, 9]). The fundamental idea behind reference counting memory manage\u00adment is to associate \na count of incoming pointers count(p)with each object p in the heap. We maintain the invariant that count(p) \nis the number of variables or other heap locations that point to p. When count(p)becomes zero, we know \nthat p can no longer be accessed, and hence it is safe to deallocate it. This approach is completely \ndifferent from tracing garbage col\u00adlectors, which periodically traverse the entire heap to identify all \nreachable memory, and then reclaim the rest.[14] 1.1 Some pros and cons of reference counting Reference \ncounting claims a number of well-known advantages over other automatic garbage collection techniques, \nincluding[14]: It allows memory to be promptly reused. Hence newly allo\u00adcated objects may still be resident \nin the processor cache from the last use of that memory, potentially giving a signi.cant performance \nbene.t.  The technique works well with a nearly full heap. Hence, if the overhead introduced by the \nreference count is small, e.g. because the average object size is large, it can be a very space\u00adef.cient \ntechnique.  It may allow the client code to use destructive updates instead of copies, since it can \ndetermine whether an object is uniquely referenced.  Basic implementations tend to be simpler than other \ntech\u00adniques.  It is often claimed to distribute storage management overhead more evenly throughout the \napplication.  We will concentrate on the last point, both since it is often consid\u00adered decisive, and \nsince, as we will see, the above statement is a gross oversimpli.cation of the truth. On the negative \nside, reference counting potentially makes pointer updates much more expensive, in some cases prohibitively \nso. The best-known de.ciency of reference counting is that it fails to reclaim cyclic garbage : If object \np points to q and q points to p, with no other pointers to p and q, clearly the program can no longer \nrefer to either object. But count(p)=count(q)=1, and neither will be reclaimed. There are several ways \nto address this: Run a tracing collector occasionally as a backup.  Require the programmer to avoid \ncycles. This becomes more feasible in a general purpose context if we introduce weak pointers[9, 24] \nthat are not included in reference counts, but are invalidated when an object is deallocated.  Add a \nfacility to the reference count implementation for re\u00adclaiming cycles[3].  Here we concern ourselves \nwith the fundamental performance char\u00adacteristics of basic reference counting algorithms. Hence we con\u00adsider \nonly acyclic data structures. Our conclusions still apply for the core algorithm in the presence of any \nof the extensions to ac\u00adcommodate or prevent cycles. 1.2 Some Terminology For the purpose of this paper, \nwe will assume we are provided a set of underlying memory allocation and deallocation routines, which \nwe will refer to as malloc and free. We will use C programming language syntax for all program frag\u00adments. \nA reference count implementation can then be viewed as providing implementations for the following operations: \nalloc The operation alloc(size)allocates an object of size bytes by calling malloc(size). We distinguish \nit from the underlying malloc operation primarily because a reference count imple\u00admentation may wish \nto do other work at allocation time. The newly allocated object has an associated reference count of \n1.1 incr The operation incr(p)increments the reference count associ\u00adated with p. This is called whenever \nan additional pointer to p is created. (Since the initial reference count is one, in our formulation, \nthis should not be done the .rst time a reference to a newly allocated object is saved.) decr The operation \ndecr(p)decrements the reference count asso\u00adciated with p and arranges for p to be reclaimed (with free) \nif it has no further references. If p is reclaimed, we must fur\u00adther ensure that counts associated with \nobjects referenced by p are suitably adjusted. This is invoked whenever an existing pointer goes out \nof scope, or is overwritten as the result of a pointer assignment. These may require adapting the representation \nof pointers (e.g. if a second object is used to store the reference count, as for Boost shared ptr[9]) \nor the representation of heap objects (e.g. if the reference count is explicitly stored in each object). \nA reference count implementation is used by a reference-counted program. Such a program allocates heap \nobjects with alloc, and is\u00adsues incr and decr operations as pointers to heap objects are created 1It \nis perhaps more natural to allocate objects with a reference count of zero. This formulation is slightly \nmore convenient for us and typically more ef.cient. If the reference returned by alloc is not preserved, \nan explicit decr call is needed. and destroyed. A pointer assignment typically requires an incr op\u00aderation \non the new value of the pointer, followed by a decr operation on the old one.2 Typically the incr and \ndecr calls will not be explicitly present in the source. They will either be added by the compiler (as \nin [11]), or they may be provided by a reimplementation of the assignment operator etc. in the language \nitself (as with C++ smart pointer implementations such as [9]). 1.3 Towards Constant Time Traditionally \none claimed advantage for reference counting has been that it more evenly distributes storage management \noverhead. In this paper we explore the consequences of limiting alloc, incr, and decr to (near) constant \ntime, at least in the sense that each will execute at most a (near) constant number of free operations. \nThis is closely related to making a reference-count implementation usable in real-time applications, \nthough there seems to be some dis\u00adagreement on the precise requirements for real-time memory man\u00adagement \n(cf. [2, 17]). Certainly a reference count implementation cannot guarantee a minimum processor utilization \nfor a given sec\u00adtion of client code unless it is possible to bound the number of free calls made on its \nbehalf from the reference count implementation during that section of code. The principal result presented \nin this paper is that although constant-time reference-count operations are possible in a bounded amount \nof heap space, in the presence of variable object sizes the worst-case space bound is impractically large. \nThus reference counting with variable-sized objects should generally not be viewed as a real-time technique, \nunless we are willing to accept more re\u00adlaxed time bounds on the alloc operation.  2 Classic reference \ncounting The most straightforward and traditional implementations of refer\u00adence counting use the following \napproach: The alloc operation simply calls malloc and sets the associ\u00adated count to one.  The incr(p)operation \nis implemented as  void incr(counted_obj *p) { if (p != null) count(p)++; } In the multi-threaded case \nthe increment operation must be atomic. This is normally accomplished with either an atomic hardware \noperation or by acquiring a lock. 2This order ensures that an assignment such as p = p does not result \nin a premature deallocation. The decr(p)operation is implemented as3 void decr(counted_obj *p) { if \n(p != null &#38;&#38; --count(p) == 0) { for each pointer field f of *p { decr(p->f); } free(p); \n } } In the multi-threaded case the count decrement and test must be performed as a single atomic operation.4 \nImplementations along these lines are quite common. For example [9] uses such an implementation. One \ndisadvantage of this straightforward approach is that it is often expensive to implement in the presence \nof threads. At a minimum, each decr operation requires an atomic fetch-and-add instruction, and incr \nrequires an atomic add in order to protect against con\u00adcurrent updates of the same count .eld. On modern \narchitectures, these atomic operations usually require a few dozen, or even a few hundred, processor \ncycles even in the absence of cache misses.5 In addition, we ve added memory references which will require \nspace in the processor cache.6 2.1 Pause times for classic reference counting Here we will concentrate \non a second issue: The decr operation re\u00adcursively traverses as much of the data structure as has become \nun\u00adreferenced as a result of the assignment. Hence it, and thus pointer assignments, may take an unbounded \namount of time. For example, assume our program consists of the following: q = make_huge_linked_tree(); \np = make_little_tree(); q=p; The last assignment will be expanded to something like: incr(p); decr(q); \nq=p; Assuming make huge linked tree() builds a large linked acyclic data structure, and returns the \nonly reference to it, the decr opera\u00adtion, which is implicitly part of the .nal assignment, will traverse \nand invoke free on every node in the large data structure. 3For brevity, we refer to all pointer .elds \nf within p as p -> f ,even though some of these .elds may be array elements, or be contained in nested \nstructures. 4If pointer assignments as a whole are to be atomic, or if we need to guarantee type-safety \nin the presence of data races, we need further syn\u00adchronization in the assignment operation to ensure \nthat we perform the decr operation on the same pointer we are replacing. 5Atomic updates can be reduced \nor eliminated through the use of more sophisticated algorithms which store reference count updates in \nbuffers, and process them in a single thread. Cf. [10, 15, 1]. However, the added com\u00adplexity seems to \nmake such implementations relatively rare. 6Other more sophisticated techniques reduce this need by storing \na one bit reference count in the pointer itself[22, 14, 20].   C expl. free shared_ptr custom tracing \nHotSpot Figure 1. GCBench max pause times (msecs). If we contrast this with the often-maligned pauses \nof non\u00adincremental tracing garbage collectors[14], the pauses introduced by hidden decr operations directly \naffect only a single thread7 but as we will see, they may certainly be at least comparable in length \nto a full tracing garbage collection. A simple tracing collector must examine at least all pointer\u00adcontaining \nobjects. But a decr operation may also free nearly all allocated memory. Since, unlike most tracing collectors, \nit deallo\u00adcates one object at a time and, in our current version, does not defer any work to allocation, \nthe actual time spent on each object may be appreciably greater. This is especially true if locking is \nrequired to support threads. To illustrate this point, we measured the pause times associated with a \nrun of the GCBench8 small object allocation benchmark with complete manual explicit deallocation ( C \nexpl. free ), two variants of reference counting ( shared ptr , custom ), and two variants of tracing \ngarbage collection ( tracing , HotSpot ). Here we de.ne the pause time associated with explicit dealloca\u00adtion \nto be the time taken to explicitly walk and deallocate the data structures used in the benchmark. The \n.rst reference-counting variant ( shared ptr ) represents a straightforward application of the shared \nptr class in the Boost C++ library[9]. This facility makes it relatively convenient to use reference \ncounting without compiler support. The second variant ( custom ) represents a much more aggressively \ntuned use of other Boost library facilities for reference counting, and it incorporates a custom memory \nallocator. Both are implementations of classic reference counting, in that they do not use the technique \nwe review in the next section. We included two very different tracing collectors: Our stop-the\u00adworld \nnon-moving collector[5] ( tracing ), and Sun s HotSpot Client Java Virtual machine copying generational \ncollector ( HotSpot ). Both collectors are run in their default mode. They could be, but were not, adjusted \nfor signi.cantly reduced pause times on this benchmark. 7Of course that thread may happen to hold a crucial \nlock, and hence block others. 8The C, C++ and Java variants we used for this test are available at //www.hpl.hp.com/personal/Hans \nBoehm/gc/gc bench/refcnt tests.    The resulting maximum pause times, measured in milliseconds, are \ngiven in .gure 1. In most cases, we measured both thread\u00adunsafe and thread-safe variants. The former \nperform enough lock\u00ading to ensure that multiple copies of the benchmark could correctly have been run \nconcurrently in the same address space. For the thread-unsafe case, this is not true, and no locking \nor other syn\u00adchronization is performed. Substantial additional measurement details are given in the ap\u00adpendix. \nIt is clear from the measurements that reference counting pause times can easily be on the same order \nas pause times for tracing collectors. In the thread-safe case, they may well be larger. The reference \ncount pause times are spent deallocating objects and recursively decrementing reference counts, and hence \nare in\u00addependent of many optimization issues. Although C++ smart\u00adpointer -based implementations do not \naggressively try to reduce reference count updates, this has no affect on the work performed during the \npause , and hence these results should apply equally to other languages and implementation styles. They \nwill of course vary much more signi.cantly with the allocation and object reten\u00adtion characteristics \nof the benchmark.  3 Lazy deletion The standard method for avoiding the long delays introduced by cascading \ndecr operations is to defer the recursive invocations[23, 14, 24, 16] until a more opportune time. When \nan object s refer\u00adence count decreases to zero, just the top level object is added to a to-be-freed set. \nThe to-be-freed objects are then processed later incrementally, e.g. during later allocations or possibly \nin a separate thread.9 If we process one element of the to-be-freed set during each alloca\u00adtion, the \nalloc and decr implementations become: void * alloc(size_t size) { void *result; if (<to-be-freed \nis not empty>) { p = <get and remove element of to-be-freed>; for each pointer field f of *p { decr(p->f); \n } free(p); } result = malloc(size); count(result) = 1; return result; } 9Note that reference counting \nallows two somewhat distinct kinds of laziness . A number of papers starting with [11] have advocated \ndefer\u00adring incr and decr operations required directly by the client, e.g. to greatly reduce reference \ncounting costs for stack variables, or, in later papers, to reduce synchronization cost. This is different \nfrom our kind of laziness, which defers only deletion and otherwise recursive decr operations. Both techniques \nmay of course be combined, as is mentioned in [11]. In this paper, we use lazy reference counting to \nmean reference counting with lazy free and recursive decr invocations , which is termed lazy deletion \nin [14]. Figure 2. Large hidden to-be-freed objects. void decr(counted_obj *p) { if (p != null &#38;&#38; \n--count(p) == 0) <add p to to-be-freed> } If we assume that the number of pointers in each object is \nbounded by a constant (i.e. avoid pointer arrays) this approach will ensure that incr and decr run in \nconstant time, and that alloc s running time is a constant times that of malloc and free. Techniques \nfor process\u00ading pointer arrays incrementally to deal with varying numbers of pointers are discussed brie.y \nbelow. If all objects are the same size, as in [23] or the reference counting scheme described in [4], \nthe above can be simpli.ed by letting alloc simply return the block p which it removed from the to-be-freed \nset without actually deallocating and reallocating it. In this case, we are guaranteed that an allocation \nrequest can be satis.ed in this way whenever an unreferenced object is available. Unfortunately, this \nlast property does not hold in the general case. In general, we may try to allocate a large object when \nthere are many suitably-sized objects on the to-be-freed set, but none of them have yet been freed, e.g. \nif the to-be-freed set appears as in .gure 2. Thus the heap may need to be grown, or we may need to reintro\u00adduce \npauses for recursive decr calls, even when suf.cient objects are unreferenced. The rest of this paper \nquanti.es the amount of space that may be lost to this effect. 4 Fragmentation By separating out the \nunderlying memory allocation algorithm from reference counting in our presentation, we have largely dodged \nthe issue of memory fragmentation, i.e. space overhead introduced when enough unallocated space is available \nin the heap to allocate an object, but that space is not contiguous. This separation is es\u00adsential in \nsome of the proofs below, but it requires some care in interpreting the results. We will present results \nabout the amount of allocated heap space, which is the amount of memory malloced, but not freed. This \nusu\u00adally does not re.ect the worst-case space requirements of the appli\u00adcation. There may be additional \nfragmentation overhead. The reader should recall the following three results about fragmen\u00adtation overhead: \n Published measurements [13] suggest that typical fragmen\u00adtation overhead for applications with explicit \nmemory man\u00adagement and well-designed allocators is very low, at least for short-running applications. \nHere we are primarily concerned with worst-case overhead, which is a different situation.10  Any memory \nallocator that does not move objects may re\u00adquire heap space a factor of O(log(smax ))larger than allocated \n smin space, where smin and smax are the smallest and largest possi\u00adble object sizes[18, 19]. It is \neasy to design an allocator that achieves this bound, to within a constant factor. Consider an allocator \nthat rounds up all requested sizes to the next power of 2, and keeps separate free lists for each object \nsize, never coalescing objects. There are O(log(smax ))size classes, and certainly allocating as much \nsmin space as the total size of allocated objects to each one is suf.\u00adcient.  5 A Lower Bound on Space \nUsage We consider an object p to be unreferenced if one plus the number of incr operations performed \nby the client on the object is equal to the sum of The number of decr operations performed directly \non the ob\u00adject, and  The number of pointers to this object from other objects that are unreferenced \nby this de.nition.  (The latter result in recursive calls to decr(p)by the classic refer\u00adence count \nimplementation. Thus the sum is just the total number of decr(p)operations in that case.) We consider \nan object to be allocated if it has been allocated with malloc, but not yet deallocated with free. We \nassume that all allocation requests are for object sizes between smin and smax words. For simplicity, \nwe make the following convenience assumptions: smin divides smax.  smin is large enough for every object \nto hold two pointers.  smax is no more than half the maximum referenced memory.  (Any of these assumptions \ncan clearly be dropped at a cost of de\u00adcreasing the lower bound by a small constant.) We emphasize that \nwe are uniformly concerned with worst-case overhead here. We suspect that the worst-case lazy reference\u00ad \n10Our own experience with less space-ef.cient allocators, longer-running programs, and deallocation delayed \nby a non-moving tracing collector, sug\u00adgests that typical fragmentation overhead remains well below a \nfactor of two. This is far better than the theoretical worst case, but signi.cantly worse than [13]. \nWe conjecture that delaying object reclamation is a signi.cant factor in the increase, although it does \nnot affect the worst case. counting space overhead is highly atypical in practice, as is the case for \nfragmentation overhead. But worst-case space behavior must normally be considered for real-time embedded \napplications, which seems to be an important potential application for lazy refer\u00adence counting. We de.ne \nan object to be immediately reclaimable if it is unrefer\u00adenced, and any other (unreferenced) objects \npointing to it have been freed. Standard reference count implementations base deallocation deci\u00adsions \non directly executed reference count operations and on ex\u00adamination of previously freed objects. The \nfollowing de.nitions capture this notion. We say that two programs running with the same reference count \nimplementation generate isomorphic free graphs at particular points in time if we can establish 1-to-1 \ncorrespondences between the free objects11 and between the immediately reclaimable objects, such that12 \n The nth objects to become immediately reclaimable corre\u00adspond, i.e. corresponding objects became reclaimable \nin the same order.  Corresponding free objects have the same size and contain the same number of pointer \n.elds.  At the time of deallocation, corresponding pointer .elds in free objects that pointed to free \nor immediately reclaimable objects pointed to corresponding objects.  We de.ne a reference count implementation \nto be lookahead-free if it obeys the following properties: It is sequential, i.e. single-threaded. \n An object is freed only if it is immediately reclaimable. Thus no object is ever deallocated while it \nis referenced from an allocated object. (Among other things, this precludes deallo\u00adcation of cyclic data \nstructures.)  The decision about which object will be freed next depends only on the order in which \nreference counts were explicitly reduced to zero and on pointers from free objects. More pre\u00adcisely, \nif two programs produce isomorphic free graphs with the given reference count implementation, then corresponding \nobjects will be freed next.  Deallocations (with free) may be performed in response to al\u00adloc, incr \nand decr calls. The number of deallocations per\u00adformed for each call depends at most on the size argument \nto alloc, and the sizes of other objects previously freed during the same call.  All sequential reference \ncount implementations of which we are aware are lookahead-free. THEOREM 5.1. Assume a lookahead-free \nreference count imple\u00admentation deallocates (with free) at most m objects for any se\u00adquence containing \nat most one alloc, two incr, and two decr calls. 11If the memory for free objects s reused, the old and \nnew objects are treated as distinct objects. Equivalently, we identify objections by their po\u00adsition \nin the sequence of allocations and not object addresses. 12This de.nition is a bit arbitrary, particularly \nin that we do not insist that corresponding immediately reclaimable objects have the same size. This \nagain affects only the constants in our result. List 1 List 2 List 3 List 1 List 2 List 3 start   \n . . . . . .   ... . ... . ... . end   Figure 3. Initial Pexperiment data structure. Then for every \nN there exists a reference-counted program such that No cyclic data structures are ever constructed. \n No more than N bytes are referenced at any point.  At some point at least Nsmax bytes will be allocated. \n 2msmin Proof First consider a program Pexperiment which we use only to learn about the order of deallocations \nperformed by the reference count implementation. Pexperiment allocates N singly linked lists 2msmin each \nconsisting of N objects13 of size smin. We use the second word in the .rst and last nodes of each list \nto link together the .rst nodes of all lists, and similarly for the last node. This gets us the data \nstructure in .gure 3. Note that the only horizontal links are at the top and bottom of the .gure. Pexperiment \nthen continues by repeatedly performing the following operations for N iterations, i.e. once for each \nlist: 2msmin 1. Allocate a large object of size smax bytes. 2. Replace the null pointer at the end of \nthe .rst remaining list with a pointer to the newly allocated object. This requires no incr or decr operations, \nsince by convention alloc returns an object with a reference count of one. 3. Replace the two references \nto the start and end of the .rst remaining list by references to the next list, using two incr and two \ndecr operations.14  After the .rst iteration of this loop, we get the data structure in .gure 4. For \neach of the N iterations of the loop one alloc,two incr, and 2msmin two decr operations are performed. \nThus at most N free calls 2smin may be made during the loop. (None can be freed earlier, since all objects \nare referenced before this point.) Since objects may not be freed before the objects referring to them, \nnone of the large objects 13The length N here is clearly suf.cient but largely arbitrary. 14We can strengthen \nthe result slightly by observing that the reference count updates for the pointers to the end of the \nlists are not strictly neces\u00adsary, and could be omitted. Hence we really only need one incr and one decr \noperation per iteration, and we could strengthen the de.nition of m correspondingly. start . . .  . \n. .   ... . ... . ... . end Figure 4. Pexperiment data structure after iteration 1. can be deallocated. \nLet ki be the number of small objects freed from N the ith list. We have .i ki <. 2smin We then construct \nPproo f to be identical to Pexperiment , with one important difference: We arrange that the ith linked \nlist has length exactly ki.15 If any of the ki are less than 2, we also need to adjust Pproo f so that \nwe still maintain the horizontally linked start and end lists correctly.16 Since none of the large objects \nPexperiment are ever freed, inductively Pexperiment and Pproo f generate isomorphic free graphs after \neach main loop iteration.17 Speci.cally, Pproo f must still free exactly ki elements from the ith list, \nand Pproo f still fails to free any of the large objects. Since at any given point, at most one of the \nlarge objects is refer\u00adenced, the total amount of referenced memory never exceeds that in all of the \nlists allocated at the beginning, plus the size of one large object. for Pproo f , the former includes \nat most N smin or N 2 bytes. 2smin The latter consists of smax bytes, which we assumed to be at most \nN 2 . Thus at most N bytes are referenced at any point. 15For all realistic reference counting implementations, \nit will be easy to compute ki without the use of additional heap space and Pproo f can be con\u00adstructed \nout of loops, as we implied for Pexperiment .If ki is not easily com\u00adputable, it suf.ces for our stated \ntheorem to fully unroll the outer loop for constructing the lists, and include the ki as constants in \na program con\u00adstructed for a speci.c N. Or we could possibly arrange to read the ki from a .le. 16We \narrange for the start list to contain the .rst nodes of all nonempty lists, and for the end list to contains \nthe last nodes of all lists of length at least two. Thus for small ki only the start list or neither \nlist may need to be adjusted in the ith iteration. We also omit appending the newly allocated large block \nto empty lists. The large blocks corresponding to an empty list are immediately dropped. 17Note that \nan immediately reclaimable small object in Pprogrammable may correspond to an immediately reclaimable \nlarge object in Pproo f , but neither will be freed and hence, by our assumptions, the difference will \nnot affect the deallocation sequence.  Since none of the large objects are freed, at the end we have \none large allocated block for for each of the N loop iterations, for a 2msmin total of Nsmax bytes. 2msmin \nOBSERVATION 5.1. Theorem 5.1 implies that if alloc, incr and decr each perform a constant number of frees, \nand with no con\u00adstraints on the size of allocated objects, the amount of allocated memory can be quadratic \nin the amount of referenced memory. This is signi.cantly worse than the logarithmic worst-case overhead \nof fragmentation with a good memory allocator. Conversely, the above theorem also tells us that in order \nto reduce the space overhead to a constant factor, we need m =.(smax ).Up smin to this value we have \na real tradeoff between latency and space; beyond this value the result becomes vacuous. In particular, \nfor the .xed size, real-time case, i.e. with smax = smin and m =1, this is a vacuous result. (The factor \nof 2 imprecision comes from the fact that we reserved N/2 bytes in our referenced memory quota for the \nsingle large object.) This is of course ex\u00adpected, since we know that lazy reference counting works well \nfor .xed object size. Perhaps more surprisingly, the theorem suggests that there is noth\u00ading special \nabout the .xed size case; allowing a small amount of variation in allocation sizes, results in a (relatively) \nsmall amount of space overhead. The next section con.rms this.  6 An Upper Bound on Space Usage LEMMA \n6.1. Lazy reference counting preserves the property that all allocated unreferenced objects are in the \nto-be-freed set or reachable from a to-be-freed object via unreferenced objects. Proof Whenever an object \ns reference count reaches zero, we put it in the to-be-freed list. Hence any unreferenced object not \nin the set must be pointed to by an unreferenced object, which must either itself be in the to-be-freed \nset, or must be reachable from another object in the set. THEOREM 6.2. Consider a reference count implementation \nin which each alloc call calls free on exactly one unreferenced object whenever there are unreferenced \nobjects, as in the lazy reference count implementation we presented above. Let smin and smax be the smallest \nand largest requested object size as before. Assume that at most N bytes in the heap are referenced at \nany one time. Then at most smax N bytes will be allocated at any point. smin Proof The number of allocated \n(malloced but not freed) objects is equal to the largest number of objects referenced at a previous point \nin the computation. This is easily shown by induction: Initially it is true since no objects are referenced, \nmalloced, or freed. Neither incr nor decr affects either the number of allocated objects, or the largest \nnumber ever referenced. Hence it suf.ces to show that each alloc operation preserves the property. When \na new object is alloced we consider two cases: No allocated objects are unreferenced. Hence the to-be-freed \nlist is empty. By induction hypothesis, the number of allo\u00adcated objects is equal to the maximum number \never refer\u00adenced. The alloc call increases both quantities by one. There are unreferenced allocated objects. \nBy induction hy\u00adpothesis more objects were referenced at some point in the past. Thus the maximum number \nof referenced objects is not affected by the alloc invocation. From the preceding lemma, the to-be-freed \nlist is nonempty. Hence alloc both frees and mallocs exactly one object. Hence the number of allocated \nobjects is also unaffected. Let K be the maximum number of objects ever referenced. The size of any K \nobjects is at least Ksmin. Thus the total size of referenced objects N must have been at least Ksmin \nat some point, and thus N K <. smin The number of allocated objects never decreases with this particular \nalgorithm. The .nal (and largest) number of allocated objects is also K. The total size of those K objects \nis at most Ksmax <smax N. smin Note that the upper and lower bounds differ only by a factor of 2 for \nthe standard lazy reference count algorithm, at least with our con\u00advenience assumptions. (The reason \nfor the factor of 2 is described at the end of section 5.) In that case the m in the lower bound result \nis 1, since only alloc calls free, and it calls it at most once. In spite of the unpleasant space bound, \nthe above algorithm doesn t quite give us constant time reference counting operations, since al\u00adloc may \nhave to traverse an unbounded number of pointers in a large object, though it only invokes free once. \nThis can be reduced to con\u00adstant time plus a constant number of free operations per reference count operation \nwith a more complex algorithm. There appear to be at least two ways to accomplish this: We arrange to \nbe able to .nd all n non-null pointers inside an object in O(n)time, e.g. by building a doubly-linked \nlist inside the object when necessary. We then scan large pointer arrays incrementally. To compensate \nfor the fact that the aver\u00adage alloc will thus do less than one free, we arrange for incr to also process \ndeferred decrements. Since allocation and initial\u00adization of objects with many pointers require many \nincr calls, this ensures that free calls will keep pace with allocations, al\u00adthough the bound on the \nnumber of allocated objects will be higher.  David Wise18 has pointed out that reference count decrements \nduring a deferred deallocation can be deferred even further, until the pointers are overwritten in the \nnewly allocated ob\u00adject. This also avoids the .urry of decrement operations dur\u00ading allocation.  It \nis possible to obtain much more reasonable space bounds if we relax the assumption on the number of frees \nperformed by each alloc call: THEOREM 6.3. Consider a reference count implementation in which each alloc(n)call \ncalls free on unreferenced objects of to\u00adtal size at least n, whenever there are suf.cient allocated \nbut un\u00adreferenced objects available. Assume that at most N bytes in the heap are referenced at any one \ntime. Then at most N bytes will be allocated at any point. 18See http://lists.tunes.org/archives/gclist/2000-September/001835.html. \nThey previously built a hardware implementation of this approach for .xed size heap cells.[24] Proof \nWe prove the hypothesis directly by induction. Clearly it is true at program start with no allocated \nobjects. When a new object is allocated with alloc(n)we again consider two cases: Fewer than n bytes \nare allocated but not referenced. Immedi\u00adately after the alloc call all allocated unreferenced objects \nwill have been freed, and all allocated bytes will be referenced. Thus clearly the number of allocated \nbytes will be less than N.  At least n bytes are allocated but not referenced. In that case, the alloc \noperation will free at least as many bytes as are newly allocated, and the number of allocated bytes \ncannot in\u00adcrease.  Again neither incr nor decr operations affect the validity of the in\u00adduction hypothesis. \nAgain, this nicely matches the lower bound result; to be able to free n bytes when allocating n bytes, \nin the worst case we will need to make smax free calls in response to a single alloc call. Thus we are \nsmin exactly in the m = smax case, which we know is within a constant smin factor of optimal for constant \nspace overhead. This does not imply that lazy reference counting costs no space at all in this case; \nwhen we try to allocate an object with fewer than N referenced bytes, the to-be-freed list may be nonempty. \nThese allocated but not referenced objects may cause or compound frag\u00admentation in the underlying allocator. \nBut this is already accounted for by the potential O(smax )fragmentation space overhead for the smin \nunderlying allocator. Also note that in spite of the preceding result, if the program has at most n objects \nof a given size s referenced at one point, it may still be the case that more than n objects of size \ns will need to be allocated; the theorem is purely a statement about the total size of all allocated \nobjects. Thus the objects on the to-be-freed list may still impact the amount of space the underlying \nallocator needs to reserve for a given object size, assuming it segregates objects by size. The preceding \ntheorem does tell us that we can get worst-case space bounds for lazy reference counting very similar \nto those for manual memory management by letting alloc take time in proportion to the size of the allocated \nobject. This is basically the same requirement as for real-time tracing collection[4, 2].19 7 Implications \nfor Concurrency In the above, we considered only sequential or single-threaded ref\u00aderence count operations, \nand we assumed deterministic execution. But especially the lower bound result also has some implications \nfor nondeterministic or concurrent algorithms. OBSERVATION 7.1. The lower bound result assumed determinis\u00adtic \nexecution to compute the ki values. If we are interested in the worst case among several possible nondeterministic \nexecutions, it suf.ces to choose a possible ki sequence, and otherwise the same proof applies. 19For \ntime-based collection as recommended in [2], this is implicit in that a bound on the allocation rate \nis assumed. Nondeterministic or random behavior can t help the worst case. OBSERVATION 7.2. The above \nstill applies to concurrent imple\u00admentations, such as [11], if the total number of frees performed by \nany thread during a loop iteration of Pproo f is bounded by m. With a bounded number of processors, the \n.nal loop in Pproo f either can\u00adnot run in near constant time per iteration (even if the large objects \nare uninitialized), or we again risk a large increase in heap size. 8 Related work and history There \nhas been much work on tracing collectors that simultaneously satisfy space and pause-time constraints \n(cf. [4, 2]). The earliest work on lazy reference counting was by Weizenbaum[23]. It was limited to .xed \nsize objects, but that restriction has usually received minimal attention. The fact that large objects \nmay be hidden from reallocation by smaller ones is mentioned brie.y by [14], but no attempt is made to \nquantify the cost. We are not aware of much discussion of the space cost of lazy ref\u00aderence counting \nwith variable-sized objects, in spite of the fact that the technique was frequently suggested. The issue \ncame up in dis\u00adcussion on the gclist mailing list20, where we reminded readers of the issue, and asked \nfor a bound on the space cost. No answer was posted. Ritzau s later thesis[16] dismisses lazy reference \ncounting for real\u00adtime applications, insisting instead on a .xed object size as in [21]. He states, referring \nto [23]: However this technique can not be used directly in systems where objects have varying sizes. \nIf differ\u00adently size objects are used, the worst case of allocation becomes to free all objects on the \nheap (just to .nd an object of the right size). We have effectively re.ned this claim in two ways: We \nconsider the possibility of simply using more space in\u00adstead of freeing all objects on the heap . We \nquantify the cost of this approach. Since malloc/free users routinely sacri\u00ad.ce a worst-case factor on \nthe order of log(smax )in space us\u00ad smin age from fragmentation overhead, we believe that something like \nour lower bound result is needed to support his argument. Theorem 6.3 points out an alternative design \npoint: Rather than breaking up an alloc(n)call into O(n)small allocations, we could let it make O(n)free \ncalls, and use an allocator that guarantees bounded fragmentation.21 This would probably require more \nspace but improve performance of data structure accesses. 9 Acknowledgements The tuned Boost-reference-counting \nversion of GCBench used for the measurements was contributed by Peter Dimov. He also pointed out some \nproblems (since repaired) in the untuned C++ benchmark. 20See http://lists.tunes.org/archives/gclist/2000-September/001836.html \n21Given a hard a priori bound of N referenced bytes, it would suf.ce to only allow allocation of power-of-two \nobject sizes, and allocate separate heap regions of size N for each of the log(smax 1 possible size classes, \nsmin as we suggested earlier. Note that if the program is known to have only n referenced objects of \nsize s, it is not necessarily safe to reduce the heap size associated with s to sn. David Wise enlightened \nme about some of the relevant history. Both he and the anonymous reviewers suggested many improve\u00adments \nto the paper. 10 References [1] D. F. Bacon, C. R. Attanasio, H. B. Lee, V. T. Rajan, and S. Smith. Java \nwithout the coffee breaks: A nonintrusive multiprocessor garbage collector. In Proceedings of the ACM \nSIGPLAN 01 Conference on Programming Language Design and Implementation, pages 92 103. ACM, 2001. [2] \nD. F. Bacon, P. Cheng, and V. T. Rajan. A realtime garbage collector with low overhead and consistent \nutilization. In Con\u00adference Record of the Thirtieth Annual ACM Symposium on Principles of Programming \nLanguages, pages 285 298, Jan\u00aduary 2003. [3] D. F. Bacon and V. T. Rajan. Concurrent cycle collection \nin reference counted systems. In Proceedings of the Fif\u00adteenth European Conference on Object-Oriented \nProgram\u00adming, LNCS 2072, pages 207 235. Springer, 2001. [4] H. Baker. List processing in real time on \na serial computer. Communications of the ACM, pages 280 294, April 1978. [5] H.-J. Boehm. A garbage collector \nfor C and C++. http://www.hpl.hp.com/personal/Hans Boehm/gc/. [6] H.-J. Boehm. Fast multiprocessor memory \nallocation and garbage collection. Technical Report HPL-2000-165, HP Laboratories, December 2000. [7] \nH.-J. Boehm. Reducing garbage collector cache misses. In Proceedings of the 2000 International Symposium \non Memory Management, pages 59 64, 2000. [8] G. E. Collins. A method for overlapping and erasure of lists. \nCACM, 13(12):655 657, December 1960. [9] G. Colvin, B. Dawes, P. Dimov, and D. Adler. Boost smart pointer \nlibrary. http://www.boost.org/libs/smart ptr/. [10] J. DeTreville. Experience with concurrent garbage \ncollectors for modula-2+. Technical Report 64, Digital Systems Re\u00adsearch Center, August 1990. [11] L. \nP. Deutsch and D. G. Bobrow. An ef.cient incremental automatic garbage collector. Communications of the \nACM, 19(9):522 526, September 1976. [12] S. Dieckman and U. H\u00a8olzle. A study of the allocation behav\u00adior \nof the SPECjvm98 Java benchmarks. Technical Report TRCS98-33, University of California at Santa Barbara, \nDe\u00adcember 1998. [13] M. S. Johnstone and P. R. Wilson. The memory fragmentation problem: solved? In Proceedings \nof the International Sym\u00adposium on Memory Management 1998, pages 26 36, October 1998. [14] R. Jones and \nR. Lins. Garbage Collection. John Wiley and Sons, 1996. [15] Y. Levanoni and E. Petrank. An on-the-.y \nreference count\u00ading garbage collector for Java. In Conference on Object-Oriented Programming Systems \nand Languages (OOPSLA), pages 367 380, 2001. [16] T. Ritzau. Memory Ef.cient Hard Real-Time Garbage Collec\u00adtion. \nPhD thesis, Department of Computer and Information-Science, Link\u00a8oping University, April 2003.  [17] \nS. G. Robertz and R. Henriksson. Time triggered garbage collection. In LCTES 03, pages 93 102. ACM, 2003. \n[18] J. M. Robson. An estimate of the store size necessary for dynamic storage allocation. Journal of \nthe ACM, 18(3):416 423, 1971. [19] J. M. Robson. Bounds for some functions concerning dy\u00adnamic storage \nallocation. Journal of the ACM, 21(3):491 499, 1974. [20] D. J. Roth and D. S. Wise. One bit counts between \nunique and sticky. In Proceedings of the International Symposium on Memory Management 1998, pages 49 \n56, October 1998. [21] F. Siebert and A. Walter. Deterministic execution of Java s primitive bytecode \noperations. In Java Virtual Machine Re\u00adsearch and Technology Symposium, April 2001. [22] W. R. Stoye, \nT. J. W. Clarke, and A. C. Norman. Some prac\u00adtical methods for rapid combinator reduction. In Conference \non Lisp and Functional Programming, pages 159 166, 1984. [23] J. Weizenbaum. Symmetric list processor. \nCommunications of the ACM, 6(9):524 544, 1963. [24] D. S. Wise, B. Heck, C. Hess, W. Hunt, and E. Ost. \nResearch demonstration of a hardware reference-counting heap. LISP Symb. Computat., 10(2):159 181, July \n1997. A Appendix: Measurement details All measurements were obtained on a machine with two 2 GHz Pentium \n4 Xeon processors running RedHat 7.2. The second pro\u00adcessor was basically unused for any of the experiments, \nsince even the thread-safe code was single-threaded for nearly the complete program execution. C and \nC++ executables were compiled with gcc 3.2 and statically linked.22 The GCBench benchmark primarily allocates \nand drops complete binary trees of various heights up to 16, while keeping some other permanent data \nstructures live during program execution. The max\u00adimum pause times were obtained allocating and dropping \nheight 16 trees, i.e. trees containing 217 .1 vertices. (It also includes some initialization code that \nallocates a single tree occupying the entire heap to ensure a reasonable heap size. The deallocation \ntime for that was not considered as a pause for explicit deallocation/reference counting.) GCBench has \noften been correctly criticized as being an unrealistic garbage collection benchmark. Among other issues, \nit is clearly too small and too regular to be fully representative of real programs. But precisely that \nfeature makes it relatively easy to translate be\u00adtween languages, and to use it as a sanity test for \ncomparing dif\u00adferent garbage collectors and memory management approaches. In our experience, it is not \nunrealistic for programs to drop a large fraction of referenced memory with a single pointer assignment, \nthe benchmark characteristic that is most relevant here. This is con\u00ad.rmed by live object measurements \nsuch as those in [12] for several of the SPECjvm benchmarks. Each tree node allocated by GCBench contains \ntwo pointers and 22Note that atomic memory update instructions, and hence locks, are rel\u00adatively more \nexpensive on a Pentium 4 than for many other processors. The difference between thread-unsafe and thread-safe \nvariants is somewhat higher than on other architectures, but the analogous graphs for an Itanium machine \nlook qualitatively similar. two integers, which total 16 bytes on this hardware.23 This object size is \nprobably more representative of Scheme than C or C++ pro\u00adgrams, a property that favors tracing collection. \nThe benchmark does essentially nothing with its data structures other than allocat\u00ading and deallocating \nthem, which favors reference counting for total execution time, though it has minimal impact on pause \ntime mea\u00adsurements. The different memory management approaches were measured as follows: C expl. free \nThe C version of the benchmark, using explicit deal\u00adlocation with free. Deallocation requires an explicit \ntree traversal. We used the standard RedHat 8.0 malloc/free implementation. Pause times were measured \nas the largest amount of time taken to deallocate one of the height 16 trees. This is intended to be \nrepresentative of standard C coding practice. In the thread-safe case, a second thread is forked at the \nbegin\u00adning of program execution. It exits immediately, but memory allocation primitives continue to acquire \nand release locks as a result. shared ptr Straightforward use of the Boost shared ptr reference-counted \nsmart pointer class[9]. The implemen\u00adtation can be used for pointers to arbitrary classes. Hence reference \ncounts have to be allocated in separate objects, which require separate allocation calls. The thread-safe \nversion uses the default pthread locks to protect against concurrent reference-count updates. custom \nMuch more heavily tuned use of Boost reference count\u00ading using Boost intrusive ptrs embedded pointers. \nUses a custom spin-lock implementation to protect reference count updates and memory allocation. The \nlock implementation ex\u00adecutes only one atomic instruction per lock/release cycle, and is presumably only \nslight slower than using a fetch-and-add instruction for reference count updates. Although some effort \nis invested in reducing the cost of each synchronization oper\u00adation, no attempt is made to reduce their \nnumber, unlike [15] or [1]. Memory is allocated using Boost quick allocator, which keeps separate regions \nfor different sizes and alignment re\u00adquirements. It never coalesces free objects. This may possibly result \nin signi.cantly worse space utilization than the standard allocator on real applications, but it works \nvery well for this benchmark, since basically all objects have the same size. The fact that the allocator \ncan be inlined also doesn t hurt. This implementation is still based on pointer operations re\u00adde.ned \nby the client code, and does not involve any under\u00adstanding of reference counting by the compiler. No \nreference\u00adcount-speci.c compiler optimization is performed, and stacks are not traced as in [11]. We \nwould expect such optimizations to have a noticeable impact on overall execution time, but not pause \ntime. tracing This uses version 6.2 of our tracing garbage collector[5] with the C version of the benchmark, \ncon.gured without ex\u00adplicit deallocation. The thread-safe version uses thread-local allocation buffers[6], \nwhich accounts for the small overhead 23Standard allocators will typically allocate 24 bytes, since they \nneed at least a small header and 8 byte alignment. Our tracing collector allocates 24 bytes so that it \ncan correctly handle C pointers just past the end of an object. Java implementations are likely to allocate \nat least 20 bytes to accommodate a method table pointer. Thus height 16 trees actually contain about \n3 MB.   30000 25000 20000 15000 10000 5000 0 Figure 5. GCBench total execution times. 45 40 35 30 \n25 20 15 10 5 0 C expl. free shared_ptr custom tracing HotSpot  Figure 6. GCBench Heap space use. for \nthread-support. (These are also used in all common Java implementations.) For this experiment, the collector \nwas not con.gured for either parallel or incremental collection, ei\u00adther of which would have signi.cantly \nreduced pause times.24 The intent was to compare to a straightforward stop-the-world tracing collector. \nPause time measurements were obtained from measurements made by the collector itself. These are probably \nslightly lower than what is actually observed by the client. HotSpot We used the HotSpot 1.4.1 client \nVM without perfor\u00admance tuning arguments. Pause times were obtained from -verbosegc output. It is probably \npossible to reduce pause times to the minor collection pause time of 17-19 msecs by tuning heap sizes, \nbut including the full collection is probably more representative of typical use. The total execution \ntime (again in milliseconds) and space (in Megabytes) for each benchmark run are given in .gures 5 and \n6 respectively. 24We also did not con.gure the collector for explicit memory prefetching as in [7], both \nsince this currently results in non-portable X86 code, and because it has minimal impact in this case. \nWe expect the latter is due to the simplicity of the benchmark, coupled with the aggressive hardware \nprefetch engine in a Pentium 4.  \n\t\t\t", "proc_id": "964001", "abstract": "Reference counting memory management is often advocated as a technique for reducing or avoiding the pauses associated with tracing garbage collection. We present some measurements to remind the reader that classic reference count implementations may in fact exhibit longer pauses than tracing collectors.We then analyze reference counting with lazy deletion, the standard technique for avoiding long pauses by deferring deletions and associated reference count decrements, usually to allocation time. Our principal result is that if each reference count operation is constrained to take constant time, then the overall space requirements can be increased by a factor of &#937;(<i>R</i>) in the worst case, where <i>R</i> is the ratio between the size of the largest and smallest allocated object. This bound is achievable, but probably large enough to render this design point useless for most real-time applications.We show that this space cost can largely be avoided if allocating an $n$ byte object is allowed to additionally perform <i>O</i>(<i>n</i>) reference counting work.", "authors": [{"name": "Hans-J. Boehm", "author_profile_id": "81423595101", "affiliation": "HP Laboratories, Palo Alto, CA", "person_id": "PP39052937", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/964001.964019", "year": "2004", "article_id": "964019", "conference": "POPL", "title": "The space cost of lazy reference counting", "url": "http://dl.acm.org/citation.cfm?id=964019"}