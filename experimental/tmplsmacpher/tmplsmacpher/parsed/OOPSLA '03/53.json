{"article_publication_date": "10-26-2003", "fulltext": "\n No Name: Just Notes on Software Reuse Robert Biddle, Angela Martin, James Noble Victoria University \nof Wellington, New Zealand. {robert,angela,kjx}@mcs.vuw.ac.nz ABSTRACT In the beginning, so our myths \nand stories tell us, the programmer created the program from the eternal nothingness of the void. In \nthis essay, we recognise that programs these days are like any other assemblage, and suggest that in \nfact programming has always been about reuse. We also explore the nature of reuse, and claim that Components \nthemselves are not the most important consideration for reuse; it is the end product, the composition. \nThe issues still in\u00advolve value, investment, and return. But pervasive reuse promotes a change in the \nmethod of construction of the program, and in the program itself. Categories and Subject Descriptors \nD.2.13 [Reusable Software]: Reuse models  General Terms Design Keywords Software Reuse, Components, \nObject-Oriented Programming 1. PROGRAMMING In the beginning, so our myths and stories tell us, the program\u00admer \ncreated the program from the eternal nothingness of the void. Whether it is Stallman typing teco macros \nand wearing out the shift keys; Chuck Moore typing backwards at Kitt Peak; Gold\u00adberg, Deutsch, Robson \net. al. in the parclands of California; billg hunched over Allen s Altair emulator; Bill Joy s VAX crashing \nand deleting vi multibuffer support for the next ten years; Gabriel do\u00ading The Right Thing at Lucid; \nor Larry Wall doing whatever: tsall good. All of our subtribes have programmer heros: wearing tramping \nboots in the machine room. Mythic .gures who bestrode the earth in the beginning. But we are no longer \nin the beginning, and pro\u00adgrammers no longer write programs like the Don Knuth of our fa\u00adbles typing \nall the source of TEX from memory into a single Copyright is held by the author/owner. OOPSLA 03, October \n26 30, 2003, Anaheim, California, USA. ACM 1-58113-751-6/03/0010.  Figure 1: Name Story This essay \nconcerns components, reuse, value, and beauty. All photographs were taken at No Name Building Recyclers, \nWellington, New Zealand, with their kind permission. teletype, compiling without any syntax errors, printing \nThe Art of Computer Programming from that .rst binary. Programs these days are like any other assemblage \n .lms, lan\u00adguage, music, art, architecture, writing, academic papers even a careful collection of preexisting \nand new components (if we had the money and could afford them) pieced together not to tell some overwhelming \nstory, but to get the job done, meet the deadline, get paid and go home. The goals, the stepwise re.nements, \nthe structure charts, the Uni\u00ad.ed Modelling Language diagrams, the problems, the solutions although \nthey may be obvious at the start soon splinter and frag\u00adment because the program is no longer one grand \ndesign, but a whole series of little stories, user stories even, little parts, pieces, components, objects, \nclasses, .les, modules, load-time objects, run\u00adtime checks, aspects, subjects, packages, assemblies, \nbat .les, dot .les, app .les, some large, some small, something old, something new, something borrowed, \nsomething blue, some typed, some gen\u00aderated, some stolen, some bought, some crufted together automat\u00adically \nby emacs macros, some downloaded from Google, some from an expensive framework library given away free \nwith a box of breakfast cereal, some larger, some smaller, and the only common\u00adality being their lack \nof commonality, lack of anything that marks them as being part of the program except the indisputable \nfact that if you delete any one of them the program might stop working. Number One: Every Number One \nsong ever written is only made up from bits from other songs. There is no lost chord. No changes untried. \nNo extra notes to the scale or hidden beats to the bar. There is no point in searching for originality. \nJimmy Caulty and Bill Drummond, The Manual [5] 2. KNUTH The most important lesson I learned during that \npast nine years is that software is hard; and it takes a long time. From now on I shall have signi.cantly \ngreater respect for every successful software tool that I en\u00adcounter. My original plan was to spend one \nyear work\u00ading on algorithms for typography; but that was in the spring of 1977, and I soon found that \nmuch more work would be needed to .nish the job. Even so, if my health continues to be good, I think \nit will turn out that those nine years were not wasted, because they will have improved my ef.ciency \nenough that I ll be able to recover the time during the next decade or so. Most importantly, those nine \nyears were surely not wasted, because I learned an enormous number of things that will feed the theoretical \nwork that I do in the future. ... The amount of technical detail in a large system is one thing that \nmakes programming more demanding than book-writing. Another is that programming demands a signi.cantly \nhigher standard of accuracy. Programs don t simply have to make sense to another human be\u00ading, they must \nmake sense to a computer. For example, I write the entire TEX compiler and desk-checked it be\u00adfore I \ndid any debugging on a machine. At that point I had in my hands a document of some 500 pages, containing \nthe program and an informal proof of its correctness. If I had submitted the program to human referees \nfor veri.cation, they would presumably have found a few problems that could readily be .xed, after which \nI might have published my program as a theo\u00adretical demonstration that there exists a way to com\u00adpile \nTEX. But of course the computer was a much sterner taskmaster than any human referees would be; therefore \nI had to spend another .ve months of intense activity before my program actually ran well enough for \nme to believe that it did the right things. (Chapter 9, Theory and Practice III) One of the main reasons \nI ve chosen to speak about Theory and Practice this morning is that I ve spent the past 12 years working \non a project that has given me an unusual opportunity to observe how theory and prac\u00adtice support each \nother. ... What were the lessons I learned from so many years of intensive work on the practical problem \nof setting type by computer? One of the most important lessons, perhaps, is the fact that SOFTWARE IS \nHARD. From now on I shall have signi.cantly greater respect for every successful software tool that I \nencounter. During the past decade I was surprised to .nd that the writing of programs for TEX and for \nMETAFONT proved to be much more dif.cult than all the other things I had done (like proving theorems \nor writing books). (Chapter 10, Theory and Practice IV) Donald E. Knuth, Selected Papers on Computer \nScience [16]  Figure 2: Sink Story These components are easily recog\u00adnisable, but note they are kept \nin pairs because users typically want to acquire matched pairs. Figure 3: Sink Story Other components \nare less recognis\u00adable, but play important roles in creating larger assemblies, and knowledgeable users \nmay combine them as appropriate. 101-ISM: The tendency to pick apart, often in minute detail, all as\u00adpects \nof life using half-understood pop psychology as a tool. Douglas Coupland, Generation X [9]  3. CONSTANTINE \nWe recycle so many things, from grocery bags to toner cartridges, why not recycle code? Why not reuse \nde\u00adsigns and models rather than always starting from scratch? The rewards of reuse seem to be enormous. \nWhat code is cheaper to write than code you don t have to write at all? With higher levels of reuse supported \nby larger component libraries, we might double or triple effective productivity. All we have to do is \nchange the whole culture of software development and maybe the personalities of programmers. Reuse is \nhardly a new idea. The lowly subroutine was conceived so that the same instructions did not have to be \nwritten out each time a particular calculation was needed. Then, what is the problem? Unfortunately, \nmost pro\u00adgrammers like to program. Some of them would rather program than eat or bathe. Most of them \nwould much rather cut code than chase documentation or search catalogs or try to .gure out some other \nstupid pro\u00adgrammer s idiotic work. Software developers develop things; users use them. Other things being \nequal, pro\u00adgrammers design and build from scratch rather than recycle. ... Reusable component libraries \nhave been around for al\u00admost as long as people have been programing. The .rst to yield to reuse were \nmath routines, followed soon by input-output. Except for the sheer joy or perversity of doing it, no \napplications or tool developer writes their own sine-cosine routines anymore. ... If it takes the typical \nprogrammer more than 2 minutes and 27 seconds to .nd something, they will conclude it does not exist \nand therefore will reinvent it. ... The payoff is .nding the component you need in a rea\u00adsonable time \nand then .nding that it can be readily used or adapted for your use. Every time this happens, you are \nbeing reinforced for it. The habit of initially consulting the library or repository becomes ingrained \nwithout being tied to increasing the number of green stamps you get or your quarterly code bonus. Larry \nL. Constantine, The Peopleware Papers: Notes on the Human Side of Software [6](pp. 141 145)  4. COOPER \nThe primary side effect of code reuse is that large por\u00adtions of most programs exist not because some \ninterac\u00adtion designer wanted them to exist, but because some other programmer already did the work on \nsomeone else s budget. Much of the software that we interact with exists for the sole reason that it \nexisted before. ... A fascinating aspect of the imperative to reuse code is the willingness with which \nprogrammers will adopt code with a questionable pedigree. Some inexperi\u00adenced programmer will hack out \nthe .rst interaction idea that pops into his head, but once it is written, that piece of code becomes \nthe basis for all subsequent ef\u00adforts because it is so aggressively reused. In Windows, for example, \nthe really experienced pro\u00adgrammers built the internal processing of the operating system, but the .rst \nsample applications that showed third-party developers how to communicate with the user were written \nby a succession of summer interns and junior coders at Microsoft. The Windows inter\u00adnal code has been \nupgraded and rewritten over six ma\u00adjor releases, and it has steadily improved. However, an embarrassingly \nlarge number of popular applica\u00adtions have in their hearts long passages of program code written by 21-year-old \nundergraduates spending a summer in Redmond. The same is true for the Web. Amateur experimenters hacked \nout the .rst Web sites, but those who followed cloned those .rst sites, and their sites were cloned in \nturn. As you can see, there is a clear con.ict of interest be\u00adtween what the user needs and what the \nprogrammer needs. We anticipate con.ict of interest in countless activities and professions, and we have \nbuilt in safe\u00adguards to curb its in.uence. While judges and lawyers have skills in common, we never let \nlawyers adjudi\u00adcate their own cases. We never let basketball play\u00aders referee their own basketball games. \nThe con.ict\u00ading interests are clearly visible, yet we consistently let  Figure 4: Sink Story Large \ncomponents may be acquired separately without any related components included. programmers make design \ndecisions based on personal implementation considerations. Alan Cooper, The Inmates are Running the Asylum \n[7](pp.106 108)  5. GATES INTERVIEWER: In a company like Microsoft, where you have 160 programmers, \nhow do you go about cre\u00adating an environment where you can develop success\u00adful programs? GATES: One way \nis to have small project teams, typ\u00adically four or .ve people, and one of those people has to have the \nproven ability to really absorb a program. And when that lead person is uncertain about some\u00adthing, he \nor she should be able to discuss it with even more experienced programmers. Part of our strategy is getting \nthe programmers to think everything through before they go to the coding phase. Writing the design documents \nis crucial, because a lot of simpli.cation comes when you see problems ex\u00adpressed as algorithms. They \nre kind of in the smallest form then, where you can see what the overlap is. Another important element \nis code review, making sure you look through the code and see if senior people can provide hints about \nhow to do something better. And you have to review similar projects that have gone su\u00adper, super, well; \nprogrammers can look at how those other people performed previously, and get ideas from other projects \nabout how to improve their own pro\u00adgram. Bill Gates interviewed by Susan Lammers, Pro\u00adgrammers at Work \n[18](p. 74)  Figure 5: Sink Story Alternatively, large components may be acquired with additional matching \ncomponents included and already assembled. Figure 6: Sink Story Entire packages of components are also \navailable with all related components included and assembled, ready for installation. CANNIBALIZE 1. \nTo take salvageable parts from (as a disabled machine) for use in building or repairing another machine. \n...3. To use or draw on material of (as another writer or an earlier work) [a volume . . . that not only \n~ previous pub\u00adlications but is intended itself to be cannibalized R.M. Adams]. 4. To make use of (a \npart take from one thing) in building or repairing something else. Webster s Ninth New Collegiate Dictionary \n(Spring.eld, Mass.: Merriam-Webster, 1990). Quoted by Rem Koolhaas and Bruce Mau, in S,M,L,XL [17]  \n6. LANGUAGE Programming language has always been about reuse. A computer programme, like a concert programme, \nmight con\u00adsist of a single sequence. But from the earliest experience of pro\u00adgramming, we made up language \nto make programming easier by supporting reuse. Selection and iteration allow instructions to be reused, \nexecuted again without re-speci.cation, subject to varying conditions. A stored program computer allows \na computer program to be spec\u00adi.ed once, and then stored for later use and reuse. Procedures allow a \nbody of instructions to be named and then reused from different contexts. Procedure parameters allow \na body of instructions to be customised to suit the needs of differing calling contexts. Procedural encapsulation \nallows procedures to be reused without regard to their implementation. Records allow data structures \nto be named and then reused to specify different data with the same structure. Classes and objects allow \ndata structures to be bound with proce\u00addures that use them, so that both data and procedures can be reused \ntogether. Encapsulation of classes and objects means they can be reused according to their speci.cation, \nand without regard to their implementation. Figure 7: Window Story Large composite components are available \nready to install, though some adjustments may be nec\u00adessary to achieve the desired result. Types allow \nchecking the speci.cation of names and their later use and reuse to determine if the use or reuse is \nappropriate. Late binding of procedure names allows calling contexts to be reused by calling different \nbodies of instructions in different cir\u00adcumstances. Late binding of class or object names allows the \ncon\u00adtextual frameworks to be reused by working data structure with dif\u00adferent implementation and behaviour. \nAnd macros, and generators, and aspects, and so on. Program\u00adming language has always been about reuse; \nprograms work in this way. Software wants to be reused. 7. LOVELACE Those who may desire to study the \nprinciples of the Jacquard-loom in the most effectual manner, viz. that of practical observation, have \nonly to step into the Ade\u00adlaide Gallery or the Polytechnic Institution. In each of these valuable repositories \nof scienti.c illustration, a weaver is constantly working at a Jacquard-loom, and is ready to give any \ninformation that may be desired as to the construction and modes of acting of his appara\u00adtus. The volume \non the manufacture of silk, in Lard\u00adner s Cyclop\u00e6dia, contains a chapter on the Jacquard\u00adloom, which \nmay also be consulted with advantage. The mode of application of the cards, as hitherto used in the art \nof weaving, was not found, however, to be suf.ciently powerful for all the simpli.cations which it was \ndesirable to attain in such varied and compli\u00adcated processes as those required in order to ful.l the \n Figure 8: Window Story Also, smaller sub-components are available for assembly into composite structures. \nNote that these components are presented in a way so as to assist the user in locating and selecting \nthe correct item. purposes of an Analytical Engine. A method was de\u00advised of what was technically designated \nbacking the cards in certain groups according to certain laws. The object of this extension is to secure \nthe possibility of bringing any particular card or set of cards into use any number of times successively \nin the solution of one problem. Whether this power shall be taken advantage of or not, in each particular \ninstance, will depend on the nature of the operations which the problem under consideration may require. \nThe process is alluded to by M. Menabrea, and it is a very important simpli.ca\u00adtion. It has been proposed \nto use it for the reciprocal bene.t of that art, which, while it has itself no appar\u00adent connexion with \nthe domains of abstract science, has yet proved so valuable to the latter, in suggesting the principles \nwhich, in their new and singular .eld of application, seem likely to place algebraical combina\u00adtions \nnot less completely within the province of mech\u00adanism, than are all those varied intricacies of which \nin\u00adtersecting threads are susceptible. By the introduction of the system of backing into the Jacquard-loom \nitself, patterns which should possess symmetry, and follow regular laws of any extent, might be woven \nby means of comparatively few cards. Those who understand the mechanism of this loom will perceive that \nthe above improvement is easily ef\u00adfected in practice, by causing the prism over which the train of pattern-cards \nis suspended to revolve back\u00adwards instead of forwards, at pleasure, under the req\u00aduisite circumstances; \nuntil, by so doing, any partic\u00adular card, or set of cards, that has done duty once, and passed on in \nthe ordinary regular succession, is brought back to the position it occupied just before it was used \nthe preceding time. The prism then resumes its forward rotation, and thus brings the card or set of cards \nin question into play a second time. This process may obviously be repeated any number of times. Augusta \nAda King, Countess of Lovelace, Anno\u00adtated translation of Sketch of the Analytical Engine Invented by \nCharles Babbage, by L. F. Menabrea of Turin, Of.cer of the Military Engineers [8](Note C)  NOW DENIAL: \nTo tell oneself that the only time worth living in is the past, and that the only time that may ever \nbe interesting again is the future. Douglas Coupland, Generation X [9]  8. TURING Outline of Logical \nControl We also wish to be able to arrange for the splitting up of operations into subsidiary operations. \nThis should be done in such a way that once we have written down how an operation is done we can use \nit as a subsidiary to any other operation. ... When we wish to start on a subsidiary operation we need \nonly make a note of where we left off the ma\u00adjor operation and then apply the .rst instruction of the \nsubsidiary. When the subsidiary is over we look up the note and continue with the major operation. Each \nsubsidiary operation can end with instructions for this recovery of the note. How is the burying and \ndisinter\u00adring of the note to be done? There are of course many ways. One is to keep a list of these notes \nin one or more standard size delay lines (1024), with the most recent last. The position of the most \nrecent of these will be kept in a .xed TS, and this reference will be modi.ed every time a subsidiary \nis started or .nished. The bury\u00ading and disinterring processes are fairly elaborate, but there is fortunately \nno need to repeat the instructions involved, each time, the burying being done through a standard instruction \ntable BURY, and the disinterring by the table UNBURY. Alan Turing, Proposed Electronic Calculator [34] \n 9. WILKES, WHEELER, AND GILL Closed Subroutines A closed subroutine is one which is called into use \nby a special group of orders incorporated in the master routine or main program. It is designed so that \nwhen its task is .nished it returns control to the master routine at a point immediately following from \nthat which it was called in. ... The library catalog used in the Laboratory is drawn up in two sections. \nOne gives a concise speci.cation of the purpose of each subroutine together with suf.cient information \nto enable a programmer to make use of it; this includes information about the operating time and storage \nspace occupied. The second section gives the orders of each subroutine in full. The catalog is contained \nin loose leaf books so that new sheets can be inserted as new subroutines are added to the library. Maurice \nV. Wilkes, David J. Wheeler and Stanley Gill, The Preparations of Programs for An Electronic Computer: \nWith special reference to the EDSAC and the use of a library of subroutines [37] As soon as the EDSAC \nbegan to work I called a meet\u00ading of those interested in development of programming methods it would \nhave been premature to call them programmers and we constituted ourselves into a committee to establish \na library of such subroutines in order that every user should not have to start from the bottom. At .rst \nwe thought of the library as containing subroutines of the type just mentioned and subroutines for the \ncomputation of elementary functions; later it became clear that it could be expanded in various di\u00adrections, \nnotably by inclusion of subroutines for per\u00adforming some of the standard operations of numerical analysis. \nIt was so clear to me that we should base our system of programming on a library of subroutines that \nI was somewhat surprised a few years later to .nd that not everyone had gone this way. Maurice V. Wilkes, \nMemoirs of a Computer Pio\u00adneer [36] 10. NYGAARD AND DAHL The syntax for this new language feature was \neasy to .nd. The links could be declared separately, without any information about the other process \nclasses which use link instances as a pre.x layer. Since the pro\u00adcesses of these other process classes \nwere at the same time both links and something more, it was natural to indicate this by textually pre.xing \ntheir declarations with the process class identi.er of this common prop\u00aderty, namely link . These process \nclasses would the be subclasses of link . It was evident that when pre.xing was introduced, it could \nbe extended to multiple pre.xing, establishing hierarchies of process classes. (In the example, car would \nbe a subclass of link , truck and bus sub\u00adclasses of car .) It was also evident that this con\u00adcatenation \nof a sequence of pre.xes with a main part could well be applied to the action parts of processes as well. \nUsually a new idea was subjected to rather violent at\u00adtacks in order to test its strength. The pre.x \nidea was the only exception. We immediately realised that we now had the necessary foundation for a completely \nnew language approach, and in the days which fol\u00adlowed the discovery we decided that: 1. We would design \na new general programming language, in terms of which an improved SIM-ULA I could be expressed. 2. The \nbasic concept should be classes of objects. 3. The pre.x feature, and thus the subclass concept, should \nbe a part of the language. 4. Direct, quali.ed references should be introduced.  Kristen Nygaard and \nOle-Johan Dahl, The develop\u00adment of the SIMULA languages [25]  Figure 11: Window Story Complete composite \ncomponents are offered ready for users to install. Note that such compo\u00adnents may have important and \ndistinctive features, and so they are displayed prominently to make these features evident to po\u00adtential \nusers. 11. PARNAS In discussions of system structure it is easy to confuse the bene.ts of a good decomposition \nwith those of a hierarchical structure. We have a hierarchical structure if a certain relation may be \nde.ned between the mod\u00adules or programs and that relation is a partial ordering. The relation we are \nconcerned with is uses or de\u00adpends upon. It is better to use a relation between pro\u00adgrams since in many \ncases one module depends upon only part of another module (...). It is conceivable that we could obtain \nthe bene.ts that we have been discussing without such a partial ordering, e.g. if all the modules were \non the same level. The partial or\u00addering gives us two additional bene.ts. First, parts of the system \nare bene.ted (simpli.ed) because they use the services of lower levels. Second, we are able to cut off \nthe upper levels and still have a usable and useful product. For example, the symbol table can be used \nin other applications; the line holder could be the basis of a question answering system. The existence \nof the hierarchical structure assures us that we can prune off the upper levels of the tree and start \na new tree on the old trunk. If we had designed a system in which the low level modules made some use \nof the high level modules, we would not have the hierarchy, we would .nd it much harder to remove portions \nof the system, and level would not have much meaning in the system. ... We have tried to demonstrate \nby these examples that it is almost always incorrect to begin the decomposition of a system into modules \non the basis of a .owchart. We propose instead that one begins with a list of dif\u00ad.cult design decisions \nor design decisions which are likely to change. Each module is then designed to 13. JOHNSON AND FOOTE \nhide such a decision from the other. Since, in most cases, design decisions transcend time of execution, \nmodules will not correspond to steps in the processing. To achieve an ef.cient implementation we must \naban\u00addon the assumption that a module is one or more sub\u00adroutines, and instead allow subroutines and \nprograms to be assembled collections of code from various mod\u00adules. David Lorge Parnas, On the criteria \nto be used in decomposing systems into modules [26] 12. LISKOV AND ZILLES What we desire from an abstraction \nis a mechanism which permits the expression of relevant details and the suppression of irrelevant details. \nIn the case of pro\u00adgramming, the use which may be made of an abstrac\u00adtion is relevant; the way in which \nthe abstraction is implemented is irrelevant. If we consider conventional programming languages, we discover \nthat they offer a powerful aid to abstraction: the function or procedure. When a programmer makes use \nof a procedure, he is (or should be) concerned only with what it does what function it provides for \nhim. He is not concerned with the algorithm executed by the procedure. In ad\u00addition, procedures provide \na means of decomposing a problem performing part of the programming task inside a procedure, and another \npart in the program which calls the procedure. Thus, the existence of pro\u00adcedures goes quite far toward \ncapturing the meaning of abstraction. Unfortunately, procedures alone do not provide a suf\u00ad.ciently rich \nvocabulary of abstractions. The abstract data objects and control structures of the abstract ma\u00adchine \nmentioned above are not accurately represented by independent procedures. Because we are consider\u00ading \nabstraction in the context of structured program\u00adming, we will omit discussion of control abstractions. \nThis leads us to the concept of abstract data type which is central of the design of the language. An \nabstract data type de.nes a class of abstract objects which is completely characterized by the operations \navailable on those objects. This means that an abstract data type can be de.ned by de.ning the characterizing \nopera\u00adtions for that type. We believe that the above concept captures the fun\u00addamental properties of \nabstract objects. When a pro\u00adgrammer makes use of an abstract data object, he is concerned only with \nthe behaviour which that object exhibits but not with any details of how that behaviour is achieved by \nmeans of an implementation. The be\u00adhaviour of an object is captured by the set of character\u00adizing operations. \nImplementation information, such as how the object is represented in storage, is only needed when de.ning \nhow the characterizing operations are to be implemented. The user of the object is not required to know \nor supply this information. Barbara Liskov and Stephen Zilles, Programming with abstract data types, \n[19] One of the most important kinds of reuse is reuse of designs. A collection of abstract classes can \nbe used to express an abstract design. The design of a pro\u00adgram is usually described in terms of the \nprogram s components and the way they interact. For example, a compiler can be described as consisting \nof a lexer, a parser, a symbol table, a type checker, and a code gen\u00aderator. An object-oriented abstract \ndesign, also called a framework, consists of an abstract class for each major component. The interfaces \nbetween the components of the design are de.ned in terms of sets of messages. There will usually be a \nlibrary of subclasses that can be used as components in the design. ... Frameworks are useful for reusing \nmore than just main\u00adline application code. They can also describe the ab\u00adstract designs of library components. \nThe ability of frameworks to allow the extension of existing library components is one of their principal \nstrengths. Frame\u00adworks are more than well written class libraries. A good example of a set of library \nutility class de.nitions is the Smalltalk Collection hierarchy. These classes provide ways of manipulating \ncollections of objects such as Arrays, Dictionaries, Sets, Bags, and the like. In a sense, these tools \ncorrespond to the sorts of tools one might .nd in the support library for a conventional programming \nsystem. Each component in such a li\u00adbrary can serve as a discrete, stand-alone, context in\u00addependent \npart of a solution to a large range of differ\u00adent problems. Such components are largely applica\u00adtion \nindependent. A framework, on the other hand, is an abstract design for a particular kind of application, \nand usually consists of a number of classes. These classes can be taken from a class library, or can \nbe application-speci.c. Frameworks can be built on top of other frameworks by sharing abstract classes. \n... Frameworks provide a way of reusing code that is re\u00adsistant to more conventional reuse attempts. \nAppli\u00adcation independent components can be reused rather easily, but reusing the edi.ce that ties the \ncomponents together is usually possible only by copying and edit\u00ading it. Unlike skeleton programs, which \nis the conven\u00adtional approach to reusing this kind of code, frame\u00adworks make it easy to ensure the consistency \nof all components under changing requirements. Since frame\u00adworks provide for reuse at the largest granularity, \nit is no surprise that a good framework is more dif.cult to design than a good abstract class. Frameworks \ntend to be application speci.c, to interlock with other frame\u00adworks by sharing abstract classes, and \nto contain some abstract classes that are specialized for the framework. Designing a framework requires \na great deal of experi\u00adence and experimentation, just like designing its com\u00adponent abstract classes. \nRalph E. Johnson and Brian Foote, Designing reusable classes [14]  14. COMPOSED SOFTWARE One thing can \nbe stated with certainty: components are for composition. Clemens Szyperski, Introduction to Component \nSoft\u00adware Beyond Object-Oriented Programming [32] This is true: components are certainly for composition; \nthey may even be the best things to compose; but they are not the only things that can be composed; and \nthey are not the only things to consider when thinking about software reuse. Components themselves are \nnot the most important considera\u00adtion for reuse; nor is the process of composing things together the \nmost important consideration: rather, it is that the end product is a composition, that is, something \nthat is made up of many things, not just one thing. But Components are not the only things that are composed. \nBy Components (with the capital C and the inverted commas) we mean modern, object-oriented/component-oriented \nunits of soft\u00adware that have been specially designed to be composed, that ideally are speci.ed, abstracted, \nreliable, trustable, binary, marketable, and above all, substitutable, fungible. Focusing on software \ncompositions on the Composed Soft\u00adware that is the end product of a composition process shows that \nit is made up of more than just these idealised Components . Some of the software will be old software \nmade new, born again, changed and broken to become part of a new creation. Some of the software will \nbe freshly written to provide important new behaviour or meet critical quality requirements. Some of \nthe software will be mundane, egoless, glue code, written or stolen or grown to tie other pieces together \nbut providing no behaviour or qualities of its own. And, yes, some of the software will be shiny, new, \nmodern, en\u00adcapsulated components, purchased from a perfect-bound web page catalogue, printed on glossy \npaper decorated with happy pictures of imaginary programmers from central casting, cavorting on their \ndays off. In the traditions of Saussure, structural linguistics, semiology, semiotics, we can consider \nartifacts to be constructed along two dimensions or axes: the axis of combination (technically the syn\u00adtagm), \nand the axis of selection or substitution (technically the paradigm). The axis of selection gives a menu \nof choices com\u00adponents that can be made visible in catalogues that can be con\u00adsulted during the composition \nprocess. The axis of combination gives the structure of the end product the composed software in this \ncase but is visible only when the composition process is complete. Traditional software componentry \nfalls on the paradigmatic axis: programmers select components to compose into their programs. In Visual \nBasic, for example, the VBX market included many small vendors selling slightly different versions of \nscrolling table con\u00adtrols: a programmer could choose one, incorporate it into the pro\u00adgram, and substitute \na different component should the .rst prove inappropriate. On the other hand, less traditional (but more \nsuccessful) forms of software components, such as software frameworks, program\u00adming languages, and middleware \ninfrastructures, fall on the syntag\u00admatic axis. Considering Visual Basic itself, the Microsoft Founda\u00adtion \nClasses, J2EE, each of these frameworks establishes the form of a single syntagm all the programs built \nusing these frame\u00adworks have the same underlying structure. This kind of reuse has been classi.ed as \ncontext reuse, to distinguish it from the tradi\u00adtional component reuse technologies [3]. Within such \na framework, developers can paradigmatically substitute their own components, Figure 13: Door Story \n Some components are distinguished because they are superior in quality and distinctive in design. These \nare presented separately, .rstly to display their quality, and secondly because their design often features \nunique sub\u00adcomponents which are best kept assembled together. Note that the special display assists users \nto locate distinctive compo\u00adnents, and assists the repository to encourage adoption of the high value \ncomponents. choose components supplied free with the frameworks, or obtain from outside suppliers but, \nthe framework itself sets the overall shape and tone of the software. In software, successful syntagmatic \ncomposition context reuse resulting in Composed Software provides much more value than paradigmatic \nselection component reuse of software Compo\u00adnents . Does this mean Components are worthless? Of course \nnot! You cannot compose software without anything to compose: you must have some assets before you can \nleverage them to produce something new. But Composed Software is composed from many things more than \njust Components . The surplus value of a compo\u00adsition lies in the end product the value of the whole \ncomposition must surpass the cost of the parts or there is no economic bene.t to composition or reuse. \nThe focus on Components , then, from McIlroy [21] via [10] to Szyperski [32], is fundamentally mistaken. \nWhat we are interested in, where the money is, is Composed Soft\u00adware, not software components. Personality: \nSo why don t all songs sound the same? Why are some artists great, write dozens of classics that move \nyou to tears, say it like it s never been said before, make you laugh, dance, blow your mind, fall in \nlove, take to the streets and riot? Well, it s because although the chords, notes, har\u00admonies, beats \nand words have all been used before their own soul shines through; their personality demands attention. \nJimmy Caulty and Bill Drummond, The Manual [5] 15. DEVELOPMENT In traditional software development, \nthe customer is absent after initial determination of requirements. Accordingly, the developers as suppliers \nare left having to make many decisions for themselves. The customer waits for the result. This can create \nall kinds of problems, as the developers may mis\u00adinterpret the customer intentions and supply functionality \nthat is not needed, or not supply functionality that is needed. Beck s ideas for Extreme Programming \n(XP) [2] address issues in software development, and in particular concentrates on the re\u00adlationship \nbetween a software development team and the customer. As part of the process in XP, Beck emphasises an \non-site cus\u00adtomer , where a customer representative is directly available for consultation and negotiation \nwith the development team. In software reuse, the issues involve the relationship between two software \ndevelopers: one acting as supplier and the other acting as customer. In reuse, it is the supplier who \nis typically absent. Ac\u00adcordingly, it is the customer developer who is left having to make many decisions \nfor themselves. This also can create problems, as the customer developer may misinterpret the supplier \nintentions, and may decide functionality is present when it is not, or may de\u00adcide functionality is not \npresent when it is. The problem is the inverse of the one addressed in XP: the prob\u00adlem is the off-site \nsupplier, and there has been much consideration of what to do. It turns out there are many reasons for \nfailure. And as our technology has improved, we have succeeded in some mea\u00adsure, but then de.ned the \nproblem in an ever narrower way. As we have succeeded in software reuse, so the phrase software reuse \nhas come to refer to those even smaller ways in which we fail. Many of the issues that remain are economic \nand organisational. Wider economic practice regards bringing together suppliers and customers as a serious \nconcern, and offers us a variety of strategies to consider. For example, we might adopt centralised planning \nand management. Alternatively, we might facilitate a marketplace. For suppliers and customers, the issues \nstill involve value, investment, and return. 16. SCHMIDT Why Software Reuse has Failed Historically \n... In theory, organizations recognize the value of system\u00adatic reuse and reward internal reuse efforts. \nIn prac\u00adtice, many factors conspire to make systematic soft\u00adware reuse hard, particularly in companies \nwith a large installed base of legacy software and developers. In my experience, non-technical impediments \nto success\u00adful reuse commonly include the following: Organizational impediments e.g., developing, deploying, \nand supporting systematically reusable software assets requires a deep understanding of application developer \nneeds and business require\u00adments. As the number of developers and projects employing reusable assets \nincreases, it becomes hard to structure an organization to provide ef\u00adfective feedback loops between \nthese constituen\u00adcies.  Economic impediments e.g., supporting corporate\u00adwide reusable assets requires \nan economic invest\u00adment, particularly if reuse groups operate as cost\u00adcenters. Many organizations .nd \nit hard to insti\u00adtute appropriate taxation or charge-back schemes to fund their reuse groups.  Administrative \nimpediments e.g., it s hard to catalog, archive, and retrieve reusable assests across  multiple business \nunits within large organizations. Although it s common to scavenge small classes or functions opportunistically \nfrom existing pro\u00adgrams, developers often .nd it hard to locate suit\u00adable reusable assets outside of \ntheir immediate workgroups. Political impediments e.g., groups that develop reusable middleware platforms \nare often viewed with suspicion by application developers, who resent the fact that they may no longer \nbe em\u00adpowered to make key architectural decisions. Like\u00adwise, internecine rivalries among business units \nmay sti.e reuse of assests developed by other in\u00adternal product groups, which are perceived as a threat \nto job security or corporate in.uence.  Psychological impediments e.g., application de\u00advelopers may \nalso perceive top down reuse ef\u00adforts as an indication that management lacks con\u00ad.dence in their technical \nabilities. In addition, the not invented here syndrome is ubiquitous in many organizations, particularly \namong highly talented programmers.  Douglas C. Schmidt, Why Software Reuse has Failed and How to Make \nIt Work for You [31] 17. POULIN What to Measure As Reuse We will now look at various categories of \nsoftware, discuss why or why not each might count as reuse, and then give a recommend solution. Most \nof the conclu\u00adsions hinge on whether or not we expect someone to write that software. As we discuss \nthese issues and reach our conclusions, we will de.ne what it means when we refer to a Reused Source \nInstruction (RSI). Some of the choices may seem clear, but none have gone without controversy. For each \ncode category, someone has published a re\u00adport claiming the category represents reuse! Product Maintenance: \nNew Versions ...We do not count product maintenance as reuse. Use of Operating System Services ...We \ndo not count the use of the operating system as reuse. Use of High-Level Languages ...We do not count \nuse of high-level languages as reuse. Use of Tools ...We do not count the use of tools as reuse. Use \nVersus Reuse of Components ...We only count the .rst use of a component as reuse. Use of Commercial \nOff-The-Shelf Software ...We do not count COTS products as reuse. Ported Software ...We do not count \nporting as reuse. Application Generators ...We do not count generated code as reuse. Code Libraries \nUse of utility libraries ...We do not count software from utility libraries as reuse. Use of local \nutility libraries ...We may count software from utility libraries as re\u00adsue. Project and domain speci.c \nlibraries ...We count software from project and domain spe\u00adci.c libraries as reuse. Corporate resue \nlibraries ...We count software from reuse libraries as reuse. Use of Modi.ed Software ...We do not count \nmodi.ed components as reuse. 1 Applying the Counting Rules to Object-Oriented Soft\u00adware Recent articles \nand publications cite reuse as a prin\u00adciple bene.t of object-oriented technology; however, object-oriented \nprojects face the same issues as any other development project when it comes to de.ning what to count \nas reused software. Experience reports from OO projects routinely state impressive reuse lev\u00adels and \nbene.ts due to reuse. However, we cannot trust these reports without understanding what the reports counted, \ne.g., inheritance or polymorphism. As with most experience reports, recent work in OO metrics fails to \naddress this issue. Part of the problem comes from counting internal reuse in OO reuse metrics: in other \nwords, using your own code .... This common and unfortunate misconception within the OO commu\u00adnity really \nconfuses many unwary practitioners. Jeffrey S. Poulin, Measuring Software Reuse: prin\u00adciples, practices, \nand economics models [29] OPINION PARALYSIS: The tendency, when given unlimited choices, to make none. \nDouglas Coupland, Generation X [9] 18. JACOBSON, GRISS, JONSSON For example, the transition from no \nreuse to informal code reuse (sometimes called leverage or cloning), in which chunks of code are copied, \nadapted slightly, and then incorporated into the new system, occurs when developers: are familiar with \neach other s code, and trust each other  feel the need to reduce time to market, even though they would \nprefer to rewrite the software  This strategy works for a while. Development time is reduced, and testing \nis often less tedious than with totally new code. But as more products are developed using this approach, \nmaintenance problems increase. Multiple copies of the software, each slightly differ\u00adent, have to managed. \nDefects found in one copy have 1Throughout this paper we use ellipses to mark where we have left out \nwords or passages to improve clarity: in this case we wish to note that Poulin does present arguments \nfor each of these points, but it is the points themselves that we wish to highlight. Figure 15: Door \nStory Surprisingly, the repository also in\u00adcludes unused components. In this case, the components had \nbeen obtained for a new system but later found to be surplus to requirements. Rather than being discarded, \nthey are of\u00adfered to other users. In fact, the repository also offers custom\u00adbuilt components if users \n.nd no already-assembled compo\u00adnents that meet their requirements. to be found and .xed multiple times. \nThis often leads to a black box code reuse strategy, in which a care\u00adfully chosen instance of code is \nreengineered, tested, and documented for reuse. All projects are then en\u00adcouraged or required to reuse \njust this copy without modi.cation. This works well for a while, and then the issue of deal\u00ading with \nchanges to satisfy an increasing number of reusers arises. Should everyone be forced to use only the \nstandard version? Should multiple versions be maintained? Should adaptation be allowed? Who decides? \nShould test .les and designs also be reused? Who will train and educate component reusers? All of these \nissues lead to the creation of a managed workprod\u00aduct reuse process, in which the creation and reuse \nof components is explicitly managed and supported by a distinct organization. Beyond this point, to get \nhigher levels of reuse and more coverage of the lifecycle, it is important to move to architected reuse, \nto explicitly architect the com\u00adponents and the systems that will use them. This is the only way to insure \nthat components .t together. The development and use of a common architecture in\u00advolves even more organizational \ncommitment and struc\u00adture. Ivar Jacobson, Martin Griss, and Patrik Jonsson, Software Reuse: Architecture, \nProcess and Organiza\u00adtion for Business Success, [13](p. 22) 19. MCCLURE Selecting Reusable Components \nRationale The system development and maintenance processes can be speeded up and aided by reusing various \ntypes of reusable components. Other reasons for using reusable components are: improved system quality, \nreduced sys\u00adtem development and maintenance costs, and reduced risk of project failure. This technique \nensures that sys\u00adtem builders will search for all types of reusable com\u00adponents in all the likely sources. \nCritical Issues Buidling application systems from reusable components is based on the assumptions that \nreusable components exisit somewhere, they are reasonably easy to .nd and understand, and they are of \ngood quality. If the time to search for candidate reusable components is too long in the system builder \ns eyes, the system builder will opt for building the component from scratch rather than reusing an existing \nocomponent regardless of how well the reusable component .ts the current need. A Reuse Library and a \nReuse Catalog that are well organized with a classi.cation scheme and supported by search and retrieval \ntools are essential to ensuring that reusable components are actually used in application system de\u00advelopment. \nCarma McClure, Software Reuse Techniques: Adding Reuse to the System Development Process [20](pp.201 \n202) Professional: The professionals of the city are like chess players who lost to computers. A perverse \nautomatic pilot constantly outwits all attempts at capturing the city, exhausts all ambi\u00adtions of its \nde.nition, ridicules the most passionate assertions of its present failure and future impossibility, \nsteers it impla\u00adcably further on its .ight foward. Each disaster foretold is somehow absorbed under the \nin.nite blanketing for the ur\u00adban. Rem Koolhaas and Bruce Mau, S, M, L,XL [17] 20. TRACZ Have you every \ntripped down the primrose path of least resistance, commending yourself for building a new program by \nsalvaging someone else s software, only to be startled by the harsh reality that things were not as great \nas you planned? True, you thought you were building on someone else s successes, but you had not counted \non inheriting their mistakes or .nding out, too late, that what you thought you could reuse as is required \na lot more effort that you had planned. The software you were trying to sal\u00advage might be good, but for \nwhat? You budgeted time and staff to salvage or carry over code from the last project, only to .nd that \nit didn t work as advertised, if the fact that it worked was advertised at all. Ignoring blatant errors \nof commission, the subtle er\u00adrors of omission are the ones that really require the most effort to overcome \n(e.g., failure to document im\u00adplementation decisions or failure to test for certain patho\u00adlogical conditions). \nThe software might work well in the narrow context for which it was designed, but taken out of its speci.c \ndomain the software suddenly be\u00adcomes brittle in other words reuseless. ... Sometimes, programmers \nshould let old code die a nat\u00ad ural death rather than spend any effort trying to revive it. As many \nof us have learned from experience, plenty of reuseless code is lying around (one might argue that \na lot of it was useless code in the .rst place). Not that most code is reuseless (or more important, \nneeds to be created as reuseless), but software not speci.cally designed for reuse is simply more dif.cult \nand costly to reuse. Similarly code designed for reuse (reuseful code) might cost 30% to 200% more \nto develop, docu\u00ad ment, and test, but subsequent reuse costs 20% to 40% less than rewriting. Making software \nreusable exacts a cost in experience and effort. Creating reusable interfaces requires in\u00adsight in seeing \nhow software has been used in the past and envisioning how it might be used in the future. Further-more, \nbecause the most important quality of reusable software, is that it be quality software, em\u00adphasis should \nbe placed on thoroughly specifying, test\u00ading, and certifying that the software has achieved a certain \nlevel of operational and documentation qual\u00adity. Only then will programmers be willing to invest their \ntime to consider its reuse. Will Tracz, Confessions of a Used Program Sales\u00adman: Institutionalizing \nSoftware Reuse [33](pp. 73 74) 21. FOWLER This tension between builders and designers happens in building \ntoo, but it s more intense in software. It s intense because there is a key difference. In building there \nis a clearer division in skills between those who design and those who build, but in software that s \nless the case. Any programmer working in high design en\u00advironments needs to be very skilled. Skilled \nenough to question the designer s designs, especially when the designer is less knowledgeable about the \nday-to-day realities of the development platform. ... One way to deal with changing requirements is to \nbuild .exibility into the design so that you can easily change it as the requirements change. However, \nthis requires insight into what kind of changes you expect. A de\u00adsign can be planned to deal with areas \nof volatility, but while that will help for foreseen requirements changes, it won t help (and can hurt) \nfor unforeseen changes. So you have to understand the requirements well enough to separate the volatile \nareas, and my observation is that this is very hard. Now some of these requirements problems are due \nto not understanding requirements clearly enough. So a lot of people focus on requirements engineering \npro\u00adcesses to get better requirements in the hope that this will prevent the need to change the design \nlater on. But even this direction is one that may not lead to a cure. Many unforeseen requirements changes \noccur due to changes in the business. Those can t be pre\u00advented, however careful your requirements engineer\u00ading \nprocess. So all this makes planned design sound impossible. Certainly they are big challenges. ... Two \nof the greatest rallying cries in XP are the slogans Do the Simplest Thing That Could Possibly Work and \nYou Aren t Going to Need It (known as YAGNI). Both are manifestations of the XP practice of simple design. \nThe way YAGNI is usually described, it says that you shouldn t add any code that will only be used by \na fea\u00adture that is needed tomorrow. On the face of it this sounds simple. The issue comes up with such \nthings are frameworks, reusable components, and .exible de\u00adsign. Such things are complicated to build. \nYou pay an extra up-front cost to build them, in the expecta\u00adtion that you will gain back that cost later. \nThis idea of .exibility up-front is seen as a key part of effective software design. However, XP s advice \nis that you not build .exible components and frameworks for the .rst case that needs that functionality. \nLet these structures grow as they are needed. If I want a money class today that handles ad\u00addition but \nnot multiplication, then I build only addition into the Money class. Even if I m sure I ll need mul\u00adtiplication \nin the next iteration, and understand how to do it easily, and think it ll be really quick to do, I ll \nstill leave it till that next iteration.  One reason for this is economic. If I have to do any work \nthat s only used for a feature that s needed to\u00admorrow, that means I lose effort on features that need \nto be done for this iteration. The release plan says what needs to be worked on now. Working on other \nthings for the future is contrary to the developers agreement with the customer. There is a risk that \nthis iteration s stories might not get done. Even if this iteration s sto\u00adries are not at risk, it s \nup to the customer to decide what extra work should be done and that might still not involve multiplication. \nMartin Fowler, Is Design Dead? [12](pp.5 9) 22. BROOKS The development of the mass market is, I believe, \nthe most profound long-run trend in software engineer\u00ading. The cost of software has always been develop\u00adment \ncost, not replication cost. Sharing that cost among even a few users radically cuts the per-user cost. \nAn\u00adother way of looking at it is that the use of n copies of a software system effectively multiplies \nthe productivity of its developers by n. That is an enhancement of the productivity of the discipline \nand of the nation. The key issue, of course, is applicability. Can I use an available off-the-shelf package \nto perform my task? A surprising thing has happened here. During the 1950 s and 1960 s, study after study \nshowed that users would not use off-the shelf packages for payroll, inventory control, accounts receivable, \netc. The requirements were too specialized, the case-to-case variation too high. During the 1980 s, we \n.nd such packages in high de\u00admand and widespread use. What has changed? Not really the packages. They \nmay be somewhat more generalized and somewhat more customizable than for\u00admerly, but not much. Not really \nthe applications, ei\u00adther. If anything, the business and scienti.c needs of today are more diverse and \ncomplicated than those of 20 years ago. The big change has been in the hardware/software cost ratio. \nThe buyer of a $2-million machine in 1960 felt that he could afford $250,000 more for a customized payroll \nprogram, one that slipped easily and nondis\u00adruptively into the computer-hostile social environment. Buyers \nof $50,000 of.ce machines today cannot con\u00adceivably afford customized payroll programs; so they adapt \ntheir payroll procedures to the packages avail\u00adable. Computers are now so commonplace, if not yet so \nbeloved, that the adaptations are accepted as a mat\u00adter of course. ... I still remember the jolt I felt \nin 1958 when I .rst heard a friend talk about building a program, as opposed to writing one. In a .ash \nhe broadened my whole view of the software process. The metaphor shift was power\u00adful, and accurate. Today \nwe understand how like other building processes the construction of software is, and we freely use other \nelements of the metaphor, such as speci.cations, assembly of components, and scaffold\u00ading. The building \nmetaphor has outlived its usefulness. It is time to change again. If, as I believe, the conceptual structures \nwe construct today are too complicated to be speci.ed accurately in advance, and too complex to be built \nfaultlessly, then we must take a radically different approach. Let us turn nature and study complexity \nin living things, instead of just the dead works of man. Here we .nd constructs whose complexities thrill \nus with awe. The brain alone is intricate beyond mapping, powerful be\u00adyond imitation, rich in diversity, \nand self-renewing. The secret is that it is grown, not built. Frederick P. Brooks Jr., No Silver Bullet \n[15](pp.197 198) 23. ENVIRONMENT On the consequences of composing modules into systems: Given a program \nto be written ; to be built ; to be grown ; how should this be undertaken when much bespoke software \nis too expensive? Equally as important: what will the .nal program look like? The process of composing \nmodules into systems is qualitatively different from the structured approach of decomposing systems into \nmodules. In a step-wise decomposition process, we begin with an idea of what the system should do, a \nproblem to be decomposed; and we then make our decomposition, proceeding in a rational man\u00adner, isolating \nthe important design decisions in their own modules, removing upward dependencies, following the grain \nof the domain. This is best described as proceeding top-down: from a problem to a program, in contrast \nto the bottom-up method of just coding a program without thinking .rst [27]. All this is well understood, \nand underlies many modern software development methods, from Dijk\u00adstra and Wirth, through Constantine \nand Mills, Dahl and Nygaard, Wirfs-Brock and Booch. The process of composing modules into systems is \nalso qualita\u00adtively different from that of building systems step by step. In the extreme, agile, or lean \napproaches we begin with a Person, (rather than a Problem) and mututally explore the domain until that \nPer\u00adson s patience or prosperity runs out, building a program along the way. Agile aproaches build programs \na small slice (or experimen\u00adtal spike) at a time: the .rst story, task, use case is implemented running \nvertically all the way from the top to the bottom of the sys\u00adtem, iterating to widen this slice out to \nthe whole system. Rather than trusting in a Big Design Up Front , we write code but then revise the program \ns design, refactoring the program to achieve ex\u00adactly the same kind of design qualities (compression, \nconsiseness, clarity, lack of dependency, resonance with the domain) promoted by the structured approaches \nto programming, design, and analy\u00adsis. All this is also well understood, as practiced and promoted by \nBeck [2], Fowler [12], and other advocates of the agile approach. Reuse is not one of the Twelve Practices \nof Extreme Programming, nor one of the Values from the Glowing Whiteboard of Agile Soft\u00adware Development \n[1]. The process of composing modules into systems, then, is quite different from either of these approaches. \nWe care about Assets Components, Software, Libraries, Modules, Frameworks, Con.gu\u00adration, Media, Images, \nQuotations, Music, Stuff rather than Per\u00adsons or Problems [4]. We proceed by aglomerating these Assets, \nremembering for you wholesale, justaposing, hammering, chuck\u00ading them in, .lling the spaces with smaller \nthings, with glue, with strings and sealing wax, and other fancy stuff. We can take exist\u00ading programs \nas components , or perhaps begin our program by copying one or more existing programs and then adding \nmore com\u00adponents, adjusting their con.gurations, or modifying them only where absolutely necessary (or \nwhere it looks like fun). There can be a complete absense of any traditionally-recognised forms of cod\u00ading, \ndesign, or analysis, no refactoring, no stories, not top down, not bottom up, not horizontal or vertical, \nbut random splodges of function based only upon whatever comes to hand. One irony of object-oriented \nprogramming, extreme program\u00adming, agile development, lean coding, aspect-oriented separation of advanced \nmeta-concerns, et cetera, is that the qualities and kinds of programs they aim to create are all exactly \nthe same disre\u00adgarding minor differences of fashion in underlying programming languages. High cohesion, \nlow coupling, low redundancy, high functionality. The topology and structure of great modern programs \nis remarkably similar the THE Operating System, Sketchpad, Emacs, Unix, Smalltalk, the Wiki, TEX. What \nwe now think of as a program E\u00a8a: the program that is, all-the-program-there-was, what we have put together \nand cho\u00adsen to take as program, what we separate from all that is arbitrarily not-program is this agglomeration, \nthis collage, this text. Some components are bigger than others; some components are more im\u00adportant \nthan others; and yet there is no big story, there is no main component , there is no main routine or \nif there is some\u00adthing claiming to be a main component, there will be many staking that claim. Early \nprograms were written by one programmer. All kinds of software engineering whether the standards and \ndocumentation of heavyweight processes, or the panopticon of agile development attempt to keep up this \n.ction, the program should look as if it was written by one programmer. What we now think of as a pro\u00adgram \nlooks as if it was written by many people, at many different times, in many different languages, as if \nthese people were com\u00adpetitors, as if they hated each other, as if they are didn t care, and were not \nunashamed. In other words: pervasive reuse promotes a change in the method of construction of the program, \nand in the program itself. After all, the etymology of Hacker is: someone who makes furniture with an \naxe [11]. OBSCURISM: The practice of peppering daily life with obscure references (forgotten .lms, dead \nTV stars, unpopular books, defunct countries, etc.) as a subliminal means of showcasing both one s education \nand one s wish to disassociate from the world of mass culture. Douglas Coupland, Generation X [9] 24. \nMCILROY The Market Coming from one of the larger sophisticated users of machines, I have ample opportunity \nto see the tragic waste of current software writing techniques. At Bell Telephone Laboratories we have \nabout 100 general pur\u00adpose machines from a dozen manufacturers. Even though many are dedicated to special \napplications, a tremen\u00addous amount of similar software must be written for each. All need input-output \nconversion, sometimes only single alphabetic characters and octal numbers, some full-blown Fortran style \nI/O. All need assemblers and could use macro-processors, though not necessar\u00adily compiling on the same \nhardware. Many need basic numerical routines or sequence generators. Most want speed at all costs, a \nfew want considerable robustness. Needless to say much of this support programming is done sub-optimally, \nand at a severe scienti.c penalty of diverting the machine s owners from their central investigations. \nTo construct these systems of high\u00adclass componentry we would have to surround each of some 50 machines \nwith a permanent coterie of soft\u00adware specialists. Were it possible quickly and con.\u00addently to avail \nourselves of the best there is in support algorithms, a team of software consultants would be able to \nguide scientists towards rapid and improved so\u00adlutions to the more mundane support problems of their \npersonal systems. In describing the way Bell Laboratories might use soft\u00adware components, I have intended \nto described the mar\u00adket in microcosm. Bell Laboratories is not typical of  Figure 17: Balustrade Story \n Alternatively, sometimes com\u00adponents must be disassembled. In this case, the larger compo\u00adnent no longer \ncomplied with current legal requirements, and so the repository is forbidden to make the composite compo\u00adnent \navailable to users. The component was dismantled, and the smaller sub-components offered for reuse. computer \nusers. As a research and development es\u00adtablishment, it must perforce spend more of its time sharpening \nits tools, and less using them than does a production computing shop. But it is exactly such a systems-oriented \nmarket toward which a components industry would be directed. The market would consist of specialists \nin system build\u00ading, who would be able to use tried parts for all the more commonplace parts of their \nsystems. The biggest customers of all would be the manufacturers. (Were they not it would be a sure sign \nthat the offered prod\u00aducts weren t good enough.) The ultimate consumer of systems based on components \nought to see consider\u00adably improved reliability and performance, as it would become possible to expend \nproportionally more effort on critical parts of systems, and also to avoid the now prevalent failings \nof the more mundane parts of sys\u00adtems, which have been speci.ed by experts, and have then been written \nby hacks. M. Douglas McIlroy, Mass Produced Software Com\u00adponents [21] 25. COX Software crisis The gunsmith \nshop in colonial Williamsburg, Va., is a fascinating place to watch gunsmiths build guns as we build \nsoftware: by fabricating each part from raw materials and hand-.tting each part to each assembly. When \nI was last there, the gunsmith was .ling a beau\u00adtifully proportioned wood screw from a wrought iron rod \nthat he d forged on the anvil behind his shop, cut\u00adting its threads entirely by hand and by eye. I was \nfas\u00adcinated by how he tested a newly forged gun barrel charging it with four times the normal load, \nstrapping it to a log, and letting er rip from behind a sturdy shel\u00adter not the least hindered by academia \ns paralyzing obsession that such testing only reveals the presence of defects, not their absence. The \ncottage-industry approach to gunsmithing was in harmony with the economic, technological, and cul\u00adtural \nrealities of colonial America. It made sense to ex\u00adpend cheap labor as long as steel was imported at \ngreat cost from Europe. But as industrialization drove mate\u00adrials costs down and demand exceeded what \nthe gun\u00adsmiths could produce, they began to experience pres\u00adsure to replace the cottage-industry gunsmith \ns process\u00adcentered approach with a product-centered approach; high-precision interchangeable parts to \naddress the con\u00adsumer s demand for less costly, easily repairable prod\u00aducts. The same inexorable pressure \nis happening today as the cost of hardware plummets and demand for soft\u00adware exceeds our ability to supply \nit. As irresistible force meets immovable object, we experience the pres\u00adsure as the software crisis: \nthe awareness that software is too costly and of insuf.cient quality, and its devel\u00adopment nearly impossible \nto manage. Insofar as this pressure is truly inexorable, nothing we think or do will stand in its path. \nThe software in\u00addustrial revolution will occur, sometime, somewhere, whether our value system is for \nit or against it, because it is our consumers values that govern the outcome. It is only a question of \nhow quickly, and of whether we or our competitors will service the inexorable pressure for change. Brad \nJ. Cox, Planning the Software Industrial Rev\u00adolution [10] 26. NORMAN Two Kinds of Market Economies: \nSubstitutable and Nonsubstitutable The importance of a proper infrastructure goes beyond its impact upon \nusefulness and intrusiveness. It can determine the entire success and failure of a technol\u00adogy. The lesson \nof Thomas Edison and his choice of an incompatible infrastructure, both for electricity and the phonograph, \nleads to a more general lesson about the marketplace. There are two kinds of economic markets: substitutable \nand nonsubstitutable. Substitutable goods are prod\u00aducts like groceries, clothes, and furniture. Nonsubsti\u00adtutable \ngoods are invariably infrastructures. The two have very different properties. Most recent books about \nthe business and marketing side of technology miss this distinction, but a company that provides a substi\u00adtutable \ngood must function very differently than one that provides a nonsubstitutable one. In a substitutable \nmarketplace, goods of one manufac\u00adturer can be substituted for goods of another. This is the classic \nmarket-driven economy, where competition prevails. This applies to food and newspapers, to auto\u00admobiles \nand television sets. In this marketplace, stan\u00addard market forces are at work and the market can be shared \namong competitors. Usually, one company has a substantial lead, but the others can coexist in relative \nstability. This is a classic case of free market com\u00adpetition. The choice of one substitutable good makes \nno commitment for the future. The consumer can buy a Pepsi today and a Coke tomorrow: the .rst choice \ndoes not constrain the second. In a nonsubstitutable market, the required infrastruc\u00adture means that \ngoods from one manufacturer cannot be substituted for goods of another. This is the market\u00adplace that \nEdison found himself in with his use of DC electricity over the competition s use of AC. It was the same \nstory with his use of vertically cut cylinders and discs when the competition used laterally cut discs. \nIt is what happened with Beta videocassette recorders. Once there is a nonsubstitutable market is doesn \nt mat\u00adter how good the product is. Donald A. Norman, The Invisible Computer [24])(pp.116 117) 27. ROBINSON, \nHOVENDEN, HALL, AND RACHEL Fordism and postFordism When Brad Cox [1990] wrote his riposte to Fred Brooks, \nhe proposed as a silver bullet a technical solution based on components, taking a manufacturing viewpoint \nlean\u00ading heavily on the experience of mass-production. This approach is known as Fordism, or Taylorism, \nand much of software development has been grounded in this view of human industrial activity. Robin Murray \n[1989] has given a very readable ac\u00adcount of Fordism and post-Fordism. Fordism began at the start of \nthe twentieth century, and has dominated industrial processes since. It has four basic principles: standardised \nproducts  repeated tasks having potential for automation  unautomated tasks analysed using work study \nmeth\u00adods, to enable the easy training of workers and their easy replacement this is the scienti.c management \nof Fred Taylor, also known as Tay\u00adlorism.  production lines with the work moving to the work\u00aders.  \nThis method of production had high initial costs and relied upon large volumes of sales, and thus heavy \nmarketing and even the manipulation of the market. Managements were very hierarchical. Labour relations \nwere poor, and turnover of employees was large due to the mechanical nature of the employment. Fordist \nideas were not restricted to Europe and America, and were also keenly adopted within the industrialisation \nprograms of the former Communist block. We see Fordism and Taylorism in the software lifecy\u00adcle and software \ndevelopment methodologies. While we do not have production lines in computing with de\u00adtailed specialisation, \nwe do identify different skills of analysis and programming, testing and management, and decompose the \nwork into discrete steps where spe\u00adcialists are applied using their speci.c skills. The spe\u00adcialisation \nmay further breakdown into programmers skilled with particular programming languages, design\u00aders specialised \nin particular types of systems like com\u00admunications, database and interfaces, and analysts spe\u00adcialised \nin particular application areas, from command and control to banking to .ight control. The Fordist view \nis further seen in the preoccupation with met\u00adrics and process improvement, the emphasis on control ( \nyou cannot control what you cannot measure ). Alternatives to Fordism arose within retailing and from \nthere moved into manufacturing. Information technol\u00adogy is seen as critically important in these develop\u00adments, \nenabling retailers like Sainsburys to track the stocks in their stores and the sales during the day to \nar\u00adrange delivery of just the correct quantities from their warehouses overnight. From there it was a \nsmall step to by-pass the warehouses and order directly from the suppliers, so that the suppliers tuned \ntheir production to the retailing needs on a daily cycle. The need for the holding of large stocks was \nremoved, this is just\u00adin-time manufacturing. At the manufacturing end, .exibility was also achieved through \ninformation technology, with the lead coming from Japan, from Toyota, reputedly following a visit to \nthe US by Toyota who saw the post-Fordist retailing there. The move was to reskill workers, using methods \nlike quality circles to tap the knowledge of the work\u00aders. In post-Fordism, the worker is designed to \nact as a computer as well as a machine (Murray 1989 p272) And the specialist machinery itself was also \n.exible, being general purpose and capable of being set up for some other job very quickly indeed. From \nthere the next step was to subcontract wherever possible. The retailing equivalent has been franchis\u00ading. \nThe practices of Benetton in the textiles and cloth\u00ading industry is used as an example of this trend, \ndi\u00adrectly employing a few thousand people but indirectly employing tens of thousands through subcontracting \nof manufacture, and franchising sales. Information technology has been critical in monitoirng sales world\u00adwide, \nnoticing trends, and switching production to meet the orders from shops. Hugh Robinson, Fiona Hovenden, \nPat Hall and Janet Rachel, Postmodern Software Development, [30], also citing Cox [10] and Murray [22]. \n Modernism: Modernism s alchemistic promise to transform quantity into quality through abstraction and \nrepetition has been a failure, a hoax: magic that didn t work. Its ideas, aesthetics, strategies are \n.nished. Together all attempts to make a new beginning have only discredited the idea of a new beginning. \nA collective shame in the wake of this .asco has left a massive crater in our understanding of modernity \nand modernization. Rem Koolhaas and Bruce Mau, S, M, L,XL [17] 28. NAUR AND RANDELL There was a considerable \namount of debate on what some members chose to call the software crisis or the software gap . As will \nbe seen from the quotations below, the conference members had widely differing views on the seriousness, \nor otherwise, of the situation, and on the extent of the problem areas. David and Fraser: (from their \nPosition paper) There is a widening gap between ambitions and achievements in software engineering. This \ngap appears in several dimensions: between promises to users and perfor\u00admance achieved by software, between \nwhat seems to be ultimately possible and what is achievable now and between estimates of software costs \nand expenditures. The gap is arising at a time when the consequences of software failure in all its aspects \nare becoming increas\u00adingly serious. Particularly alarming is the seemingly unavoidable fallibility of \nlarge software, since a mal\u00adfunction in an advanced hardware-software system can be a matter of life \nand death, not only for individuals, but also for vehicles carrying hundreds of people and ultimately \nfor nations as well. Hastings: I am very disturbed that an aura of gloom has fallen over this assembly. \nI work in an environ\u00adment of many large installations using OS/360. These are complex systems, being \nused for many very so\u00adphisticated applications. People are doing what they need to do, at a much lower \ncost than ever before; and they seem to be reasonably satis.ed. Perhaps their sys\u00adtems do not meet everybody \ns need, they don t meet the time sharing people s demands for example, but I don t think software engineering \nshould be confused with time sharing system engineering. Areas like traf\u00ad.c control, hospital patient \nmonitoring, etc. are very explosive, but are very distinct from general purpose computing. Gillette: \nWe are in many ways in an analogous posi\u00adtion to the aircraft industry, which also has problems producing \nsystems on schedule and to speci.cation. We perhaps have more examples of bad large systems than good, \nbut we are a young industry and are learn\u00ading how to do better. Randell: There are of course many good \nsystems, but are any of these good enough to have human life tied online to them, in the sense that if \nthey fail for more than a few seconds, there is a fair chance of one or more people being killed? Graham: \nI do not believe that the problems are related solely to online systems. It is my understanding that \nan uncritical belief in the validity of computer-produced results (from a batch-processing computer) \nwas at least a contributory cause of a faulty aircraft design that lead to several serious air crashes. \nPerlis: Many of us would agree that Multics and TSS/360 have taken a lot longer to develop than we would \nhave wished, and that OS/360 is disappointing. However, perhaps we are exaggerating the importance of \nthese facts. Is bad software that important to society? Are we too worried that society will lose its \ncon.dence in us? Randell: Most of my concern stems from a perhaps over-pessimistic view of what might \nhappen directly as a result of failure in an automated air traf.c control system, for example. I am worried \nthat our abilities as software designers and producers have been oversold. Opler: As someone who .ies \nin airplanes and banks in a bank I m concerned personally about the possibility of a calamity, but I \nm more concerned about the effects of software .ascos on the overall health of the industry. Kolence: \nI do not like the use of the word crisis. It s a very emotional word. The basic problem is that certain \nclasses of systems are placing demands on us which are beyond our capabilities and our theories and meth\u00adods \nof design and production at this time. There are many areas where there is no such thing as a crisis \n sort routines, payroll applications, for example. It is large systems that are encountering great dif.culties. \nWe should not expect the production of such systems to be easy. Ross: It makes no difference if my legs, \narms, brain and digestive tract are in .ne working condition if I am at the moment suffering from a heart \nattack. I am still very much in a crisis. Fraser: We are making great progress, but neverthe\u00adless the \ndemands in the industry as a whole seem to be going ahead a good deal faster than our progress. We must \nadmit this, even though such an admission is dif.cult. Dijkstra: The general admission of the existence \nof the software failure in this group of responsible people is the most refreshing experience I have \nhad in a number of years, because the admission of shortcomings is the primary condition for improvement. \nP. Naur and B. Randell, Software Engineering: Re\u00adport of a conference sponsored by the NATO Science Committee \n[23]  1968: Sous le pav\u00b4e, la plage (under the pavement, beach): initially. May 68 launched the idea \nof a new beginning for the city. Since then, we have been engaged in two parallel operations: documenting \nour overwhelming awe for the existing city, de\u00adveloping philosophies, projects, prototypes for a preserved \nand reconstituted city and, at the same time, laughing the professional .eld of urbanism out of existence, \ndismantling it in our contempt for those who planned (and made huge mis\u00adtakes in planning) airports, \nNew Towns, satellite cities, high\u00adways, high-rise buildings, infrastructures, and all the other fallout \nfrom modernization. After sabotaging urbanism, we have ridiculed it to the point where entire university \ndepart\u00adments are closed, of.ces bankrupted, bureaucracies .red or privatized. Rem Koolhaas and Bruce \nMau, S, M, L,XL [17] 29. THE END Is Software Engineering, as a discipline, a fraud founded upon a lie? \nThe Lie: There is a software crisis in truth there never was. The Fraud: You must to spend lots of money \nto bridge a software gap in truth there is no gap. The Consolation: There is no silver bullet but there \nare no werewolves. So there have been minor problems from time to time the phone network crashes, a \nrocket explodes, every desktop computer in the world succumbs to a virus but we ignore them the way \nwe ignore .at batteries, the inevitably of traf.c accidents as we walk across a roadway, or dif.culties \nwe have in opening plastic bottles of ketchup. The dot-com bubble didn t end because of a software gap \n, be\u00adcause programmers couldn t write proper software fast enough. Rather the reverse: programmers were \nable to write software so easily that businesses began to pay for software that was pointless, and therefore \nworthless. Software engineering has failed, but software is a success. What is now becoming clear is \nthat software irrespective of how it is written, built, or grown has a structure more like a natural \nsystem than a mathematical construction. The relationships between the objects in a program is more like \nthat of the words in a novel, the cells in an organism, or the leaves on a tree, than a series of propositions \nlinked by modus ponens [35, 28]. Programs will more easily yield their secrets to critical theory, biology, \nor botany, than to modal logic or complexity theory. No program is an island: every program contains \na trace of every program that has ever been written, or that will ever be. So where does this leave us, \nat the end of Software Engineering? We are im\u00admeasurably richer due to all the software in the world. \nWe draw upon this software whenever we need to create something new : programming is and always has \nbeen re-use, re-programming. We have the makings of a new discipline: the natural science of composed \nsoftware; the normal science of the study of all the soft\u00adware in the world. Crisis: What if we simply \ndeclare that there is no crisis rede.ne our relationship with the city not as its makers but as its \nmere subjects, as its supporters? More than ever, the city is all we have . . . Rem Koolhaas and Bruce \nMau, S,M, L,XL [17]  30. REFERENCES [1] Agile Alliance. Manifesto for agile software development. Available \nat http://agilemanifesto.org, 2003. [2] Kent Beck. Extreme Programming Explained: Embrace Change. Addison-Wesley, \n1999. [3] Robert Biddle and Ewan Tempero. Understanding the impact of language features on reusability. \nIn Murali Sitaraman, editor, Proceedings of the Fourth International Conference on Software Reuse, pages \n52 61, Orlando, USA, April 1996. IEEE Computer Society. [4] Robert Biddle and Ewan Tempero. Towards asset \nbased software engineering. In Ninth Workshop on Institutionalizing Software Reuse (WISR9), 1999. [5] \nJimmy Caulty and Bill Drummond. The manual: How to Have a Number One the Easy Way. KLF Publications, \n1988. [6] Larry L. Constantine. The Peopleware Papers: Notes on the Human Side of Software. Prentice-Hall \nInc., 2001. [7] Alan Cooper. The Inmates are Running the Asylum. Sams Publishing, 1999. [8] Augusta \nAda King Countess of Lovelace. Annotated translation of sketch of the Analytical Engine invented by Charles \nBabbage, by L. F. Menabrea, of Turin, Of.cer of the Military Engineers. (Available at http: //www.fourmilab.ch/babbage/sketch.html), \n1843. [9] Douglas Coupland. Generation X: Tales for an Accelerated Culture. St. Martins Press, 1992. \n[10] Brad J. Cox. Planning the software industrial revolution. IEEE Software, November 1990. [11] Eric \nS. Raymond (Editor). The jargon .le. Available at http://www.jargon.org, 2003. [12] Martin Fowler. Is \ndesign dead? In Extreme Programming Examined, Giancarlo Succi and Michele Marchesi, Editors, pages 3 \n18. Addison-Wesley, 2001. [13] Ivar Jacobson, Martin Griss, and Patrik Jonsson. Software Reuse: Architecture, \nProcess and Organization for Business Success. Addison-Wesley, 1997. [14] Ralph E. Johnson and Brian \nFoote. Designing reusable classes. Journal of Object-Oriented Programming, June/July 1988. [15] Frederick \nP. Brooks Jr. The Mythical Man-Month: Essays on Software Engineering. Addison-Wesley, anniversary edition \nedition, 1995. [16] Donald E. Knuth. Selected Papers on Computer Science. Cambridge University Press, \n1996. [17] Rem Koolhaas and Bruce Mau. S,M,L,XL, O.M.A., Small, Medium, Large, Extra-Large, Of.ce of \nMetropolitan Architecture. The Monacelli Press, second edition, 1998. [18] Susan Lammers. Programmers \nat Work. Microsoft Press, 1986. [19] Barbara Liskov and Stephen Zilles. Programming with abstract data \ntypes. In Proceedings of the ACM SIGPLAN symposium on very high level languages, pages 50 59, 1974. [20] \nCarma McClure. Software Reuse Techniques: Adding Reuse to the System Development Process. Prenctice-Hall \nInc., 1997. [21] M. Douglas McIlroy. Mass produced software components. In P. Naur and B. Randell, editors, \nReport on a conference sponsored by the NATO Science Committee on Software Engineering, pages 138 155. \nScienti.c Affairs Division, NATO, Brussels, 1969. [22] Robin Murray. Fordism and Post-Fordism, pages \n167 276. Academy Editions, 1992. [23] P. Naur and B. Randell, editors. Software Engineering: Report of \na conference sponsored by the NATO Science Committee. NATO Scienti.c Affairs Division, Brussels, 1969. \n[24] Donald A. Norman. The Invisible Computer: Why Good Products Fail, The Personal Computer is so Complex, \nand Information Appliances are the Solution. MIT Press, 1999. [25] Kristen Nygaard and Ole-Johan Dahl. \nThe development of the simula languages. In The .rst ACM SIGPLAN conference on History of programming \nlanguages, pages 245 272, 1978. [26] David Lorge Parnas. On the criteria to be used in decomposing systems \ninto modules. Communications of the ACM, 15:1053 1058, December 1972. [27] D.L. Parnas and P.C. Clements. \nA rational design process: How and why to fake it. IEEE Transactions on Software Engineering, 12:251 \n257, 1986. [28] Alex Potanin, James Noble, Marcus Frean, and Robert Biddle. Scale-free geometry of object-oriented \nprograms. Communications of the ACM, 2004. To appear, draft available at: http://www.mcs.vuw.ac.nz/comp/ \nPublications/CS-TR-02-30.abs.html. [29] Jeffrey S. Poulin. Measuring Software Reuse: principles, practices, \nand economic models. Addison-Wesley Longman Inc., 1997. [30] Hugh Robinson, Fiona Hovenden, Pat Hall, \nand Janet Rachel. Postmodern software development. Computer Journal, 41(6), 1998. [31] Douglas C. Schmidt. \nWhy software reuse has failed and how to make it work for you. The C++ Report, 1999. [32] Clemens Szyperski. \nComponent Software: Beyond Object-Oriented Programming. Addison-Wesley Longman Inc., 1997. [33] Will \nTracz. Confessions of a Used Program Salesman: Institutionalizing Software Reuse. Addison-Wesley Publishing \nInc., 1995. [34] Alan Turing. Proposed electronic calculator. (Available at http://www.alanturing.net/turing_ \narchive/archive/index/aceindex.html), 1946. [35] S. Valverde, R. Ferrer Cancho, and R. V. Sole. Scale-free \nnetworks from optimal design. Available at http://www.santafe.edu/sfi/publications/ wpabstract/200204019. \n[36] Maurice V. Wilkes. Memoirs of a Computer Pioneer. MIT Press, 1985. [37] Maurice V. Wilkes, David \nJ. Wheeler, and Stanley Gill. The Preparations of Programs for An Electronic Computer: With special reference \nto the EDSAC and the use of a library of subroutines. Addison-Wesley Press, 1951. PARASITE What happens \nwhen a critical essay extracts a pas\u00adsage and cites it? Is this different from a citation, echo, or allusion \nwithin a poem? Is a citation an alien parasite within the body of its host, the main text, or is it the \nother way around, the interpretative text the parasite which surrounds and strangles the citation which \nis its host? J. Hillis Miller, as quoted in Gregory L. Ulmer, The Ob\u00adject of Post-Criticism , in The \nAnti-Aesthetic: essays of Post\u00admodern Culture, ed. Hal Foster (Seattle: Bay Press, 1989), as quoted in \nRem Koolhaas and Bruce Mau, S,M,L,XL [17]   \n\t\t\t", "proc_id": "949344", "abstract": "In the beginning, so our myths and stories tell us, the programmer created the program from the eternal nothingness of the void. In this essay, we recognise that programs these days are like any other assemblage, and suggest that in fact programming has always been about reuse. We also explore the nature of reuse, and claim that Components themselves are not the most important consideration for reuse; it is the end product, the composition. The issues still involve value, investment, and return. But pervasive reuse promotes a change in the method of construction of the program, and in the program itself.", "authors": [{"name": "Robert Biddle", "author_profile_id": "81100135223", "affiliation": "Victoria University of Wellington, New Zealand", "person_id": "PP14058058", "email_address": "", "orcid_id": ""}, {"name": "Angela Martin", "author_profile_id": "81100391536", "affiliation": "Victoria University of Wellington, New Zealand", "person_id": "PP39040730", "email_address": "", "orcid_id": ""}, {"name": "James Noble", "author_profile_id": "81100588708", "affiliation": "Victoria University of Wellington, New Zealand", "person_id": "PP15036977", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/949344.949403", "year": "2003", "article_id": "949403", "conference": "OOPSLA", "title": "No name: just notes on software reuse", "url": "http://dl.acm.org/citation.cfm?id=949403"}