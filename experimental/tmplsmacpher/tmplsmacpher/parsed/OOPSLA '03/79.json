{"article_publication_date": "10-26-2003", "fulltext": "\n Teaching Software Testing: Automatic Grading Meets Test-first Coding Stephen H. Edwards Virginia Tech, \nDept. of Computer Science 660 McBryde Hall, Mail Stop 0106 Blacksburg, VA 24061 USA +1 540 231 5723 \nedwards@cs.vt.edu ABSTRACT A new approach to teaching software testing is proposed: students use test-driven \ndevelopment on programming assignments, and an automated grading tool assesses their testing performance \nand provides feedback. The basics of the approach, screenshots of the sytem, and a discussion of industrial \ntool use for grading Java programs are discussed.  Categories and Subject Descriptors K.3.2 [Computers \nand Education]: Computer and Information Science Education; D.1.5 [Programming Techniques]: Object\u00adoriented \nProgramming; D.2.5 [Software Engineering]: Testing and Debugging testing tools. General Terms Languages, \nVerification.. Keywords Test-driven development, laboratory-based teaching, CS1, extreme programming, \nJava. 1. INTRODUCTION Virginia Tech has been seeking to improve the coverage of software testing skills \nin our undergraduate program. Rather than introducing a new course, we are attempting to apply an active\u00adlearning \napproach to introducing testing concepts across the entire CS curriculum [6]. Testing techniques for \nobject-oriented software are of particular interest, since our introductory sequence teaches objects-first \nusing Java. The goal is to teach testing in a way that will encourage students to practice testing skills \nin many classes and give them concrete feedback on their testing performance, without requiring a new \ncourse, any new faculty resources, or a significant number of additional lecture hours. The resulting \nstrategy is founded on two ideas: have students use test-driven development on their programming assignments \nfrom the beginning, and then use an automated grading tool to Copyright is held by the author/owner(s). \nOOPSLA 03, October 26 30, 2003, Anaheim, California, USA. ACM 1-58113-751-6/03/0010. meaningfully assess \ntheir testing performance while also providing rapid, concrete feedback on how to improve. This strategy \nhas been piloted to positive student reactions; an analysis of student programs revealed that students \nproduced 45% fewer bugs per thousand lines of code using this approach [4]. 2. TEST-DRIVEN DEVELOPMENT \nUnfortunately, in most undergraduate programs, students get little practical training in how to test \ntheir own code and often have poor skills (and even poorer expectations) in this area. In order to produce \na cultural shift in the in the way our students acquire and apply testing skills, a new approach is needed. \nThe core idea underlying this approach is that students should always practice test-first coding, also \nknown as test-driven development (TDD), on their programming assignments from the beginning, across all \nof their core courses. TDD has been popularized by extreme programming. In TDD [1], one always writes \na test case (or more) before adding new code. In fact, new code is only written in response to existing \ntest cases that fail. TDD is attractive for educational use. It is easier for students to understand \nand relate to than more traditional testing approaches. It promotes incremental development, promotes \nthe concept of always having a running (if incomplete) version of the program at hand, and promotes early \ndetection of errors introduced by coding changes. It directly combats the big bang integration problems \nthat many students see when they begin to write larger programs, where testing is saved until all the \ncode writing is complete. It dramatically increases a student s confidence in the portion of the code \nthey have finished, and allows them to make changes and additions with greater confidence because of \ncontinuous regression testing. Most importantly, students begin to see these benefits for themselves \nafter using TDD on just a few assignments. 3. AUTOMATED GRADING The key to implementing TDD across the \nboard is a powerful strategy for assessing student performance. The assessment approach should: Require \na student test suite as part of every submission.  Encourage students to write thorough tests.  Encourage \nstudents to write tests as they code (in the spirit of TDD), rather than postponing testing until after \nthe code is complete.  Support the rapid cycling of write a little test, write a little code that is \nthe hallmark of TDD.  Provide timely, useful feedback on the quality of the tests in addition to the \nquality of the solution.  Employ a grading/reward system that fosters the behavior we want students \nto have.  Unfortunately, instructors and teaching assistants are already overburdened with work while \nteaching computer science courses and have little time to devote to additional assessment activities. \nAs a result, an automated tool for grading student programs is desirable. Many educators have used automated \nsystems to assess and provide rapid feedback on large volumes of student programming assignments [5, \n8]. Such systems typically focus on compilation and execution of student programs against some form of \ninstructor-provided test data. This approach ignores any testing the student has performed and fails \nto provide the both the assessment and the feedback necessary to properly facilitate TDD. As a result, \nwe have designed and implemented a general-purpose automated grading tool and incorporated it into Web-CAT, \nthe Web-based Center for Automated Testing. Instead of automating an assessment approach that focuses \non the output of a student s program, instead we must focus on what is most valuable: the student s testing \nperformance. To provide a meaningful assessment of how correctly and thoroughly the tests conform to \nthe problem, the Web-CAT Grader examines three facets of the student s submission. First, Web-CAT assesses \nthe validity of the student s tests in terms of how correctly they reflect the problem. This can be done \nby running student tests against a (correct) reference implementation, and providing feedback on which \ntests are incorrect. Second, Web-CAT assesses the completeness of the student s tests. This can be done \nby measuring the code coverage achieved by the student s tests on their code, as well as by using a reference \ntest suite intended to capture the full space of the problem. Feedback on which portions of the code \nwere not properly covered is returned to the student. Third, the style and quality of the student s code \nis assessed using static analysis tools that point out specific problems. Web-CAT is a web-based application \nimplemented using Apple s WebObjects framework. It is designed to be language independent, but this poster \nfocuses on grading object-oriented programs written in Java. For Java programs, students write JUnit-compatible \ntest cases and submit them along with the other classes in their assignment. Web-CAT uses Clover [3] \nto instrument code for coverage analysis, and uses Checkstyle [2] and PMD [7] to perform static analysis \nof coding and commenting style and to spot potential coding issues. The reports produced by these tools \nare merged into one seamless source code markup viewable on the web by the student. To support the rapid \ncycling between writing individual tests and adding small pieces of code, the Web-CAT Curator will allow \nunlimited submissions from students up until the assignment deadline. Students can get feedback any time, \nas often as they wish. However, their score is based in part on the tests they have written, and their \nprogram performance is only assessed by the tests they have written. As a result, to find out more about \nerrors in their own programs, it will be necessary for the student to write the test cases. The feedback \nreport will graphically highlight the portions of the student code that are not tested so that the student \ncan see how to improve. Other coding or stylistic issues will also be graphically highlighted. 4. EXPERIENCE \nAND CONCLUSION This technique has been piloted in a junior-level undergraduate class of 59 students using \nan earlier version of the Web-CAT Grader. Students preferred this approach over that used in prior classes, \nand tested their programs more thoroughly [4]. As a result, using TDD in class holds great promise for \nimproving testing skills. Providing a system for rapid assessment of student work, including both the \ncode and the tests they write, and ensuring concrete, useful, and timely feedback, is critical. In addition \nto assessing student performance, students can get real benefits from using the approach, and these benefits \nare important for students to internalize and use the approach being advocated. 5. ACKNOWLEDGMENTS This \nwork is supported in part by the Virginia Tech Institute for Distance and Distributed Learning and by \nthe National Science Foundation under grant DUE-0127225. Any opinions, conclusions or recommendations \nexpressed in this paper are those of the author and do not necessarily reflect the views of the NSF. \nThe author acknowledges the contributions of the students who have implemented parts of the system: Anuj \nShah, Amit Kulkarni, and Gaurav Bhandari. 6. REFERENCES [1] Beck, K. Test-Driven Development: By Example. \nAddison-Wesley, Boston, MA. 2003. [2] Checkstyle home page. http://checkstyle.sourceforge.net/ [3] Clover: \na code coverage tool for Java. http://www.thecortex.net/clover/ [4] Edwards, S.H. Using test-driven development \nin the classroom: providing students with automatic, concrete feedback on performance. In Proc. Int l \nConf. Education and Information Systems: Technologies and Applications, 2003, pp. 421-426. [5] Jackson, \nD., and Usher, M. Grading student programs using ASSYST. In Proc. 28th SIGCSE Technical Symp. Computer \nScience Education, ACM, 1997, pp. 335-339. [6] Jones, E.L. Software testing in the computer science curriculum \na holistic approach. In Proc. Australasian Computing Education Conf., ACM, 2000, pp. 153-157. [7] PMD \nhome page. http://pmd.sourceforge.net/ [8] Reek, K.A. A software infrastructure to support introductory \ncomputer science courses. In Proc. 27th SIGCSE Technical Symp. Computer Science Education, ACM, 1996, \npp. 125\u00ad 129.  \n\t\t\t", "proc_id": "949344", "abstract": "A new approach to teaching software testing is proposed: students use test-driven development on programming assignments, and an automated grading tool assesses their testing performance and provides feedback. The basics of the approach, screenshots of the sytem, and a discussion of industrial tool use for grading Java programs are discussed.", "authors": [{"name": "Stephen H. Edwards", "author_profile_id": "81100595678", "affiliation": "Virginia Tech, Blacksburg, VA", "person_id": "PP15037134", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/949344.949431", "year": "2003", "article_id": "949431", "conference": "OOPSLA", "title": "Teaching software testing: automatic grading meets test-first coding", "url": "http://dl.acm.org/citation.cfm?id=949431"}