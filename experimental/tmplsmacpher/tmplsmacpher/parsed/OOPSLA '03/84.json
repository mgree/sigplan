{"article_publication_date": "10-26-2003", "fulltext": "\n Multicodes: Optimizing Virtual Machines using Bytecode Sequences Ben Stephenson Wade Holst University \nof Western Ontario University of Western Ontario London, Ontario, Canada London, Ontario, Canada ben@csd.uwo.ca \nwade@csd.uwo.ca ABSTRACT A virtual machine optimization technique that makes use of bytecode sequences \nis introduced. The process of determin\u00ading candidate sequences is discussed and performance gains achieved \nwhen applied to a Java interpreter are presented. The suitability of this optimization for JVMs that \nperform just-in-time compilation is also discussed.  Categories and Subject Descriptors D.3.4 [Programming \nLanguages]: Optimization  General Terms Performance, Languages Keywords virtual machine, bytecode, \noptimization, Java, JVM, inter\u00adpreter 1. INTRODUCTION Languages like Java [1] that use a virtual machine \nprovide a valuable advantage over languages that compile down to native machine code; a virtual machine \nadds a level of ab\u00adstraction that allows the same object .le to be used across multiple platforms. However, \nlike all abstractions, the intro\u00adduction of a virtual machine incurs e.ciency penalties. This paper introduces \na new technique to help lessen the ine.ciencies incurred by a virtual machine which we refer to as multicode \nsubstitution. It is related to superoperators [5], bytecode idioms [7], and selective inlining [4] but \nhas distinct qualities of its own. 2. TERMINOLOGY We will use the terms bytecode and instruction interchange\u00adably \nin this paper. These terms refer to one of the atomic Java Virtual Machine instructions de.ned by the \nJVM Spec\u00adi.cation [3]. Each bytecode consists of an opcode,which uniquely identi.es the instruction, \nand zero or more instruct\u00adion-speci.c operands. Each bytecode has a collection of statements associated \nwith it that implement the bytecode in question. These will be referred to as Java Micro Instruc\u00adtions \nand will be abbreviated JMI. Between each instruction, the virtual machine must per\u00adform a transfer to \nthe next instruction. Previous research Copyright is held by the author/owner. OOPSLA 03, October 26 \n30, 2003, Anaheim, California, USA. iload 1 istore 1 dup ++top; vars[1] = st[top]; ++top; st[top] = \nvars[1]; top; st[top] = st[top-1]; Table 1: Example JMI for simple bytecodes istore 1/iload 1 dup/istore \n1 vars[1] = st[top]; ++top; top; st[top] = st[top-1]; ++top; vars[1] = st[top]; st[top] = vars[1]; top; \nOptimized JMI Optimized JMI vars[1] = st[top]; vars[1] = st[top]; Table 2: Example unoptimized (top) \nand optimized (bottom) JMI for multicodes has shown that for simple bytecodes, almost all of the ex\u00adecution \ntime is spent performing transfers as opposed to executing the bytecode itself [4]. 3. MULTICODES A \nmulticode is a sequence of bytecodes that are treated as an atomic unit. No transfer operations are performed \nwithin a multicode. However, transfers of control still exist between multicodes. The arity of a multicode \nis de.ned to be the number of bytecodes in the sequence. An unoptimized multicode is one whose JMI is \nsimply the concatenation of the JMI for each participating bytecodes. Note that the use of an unoptimized \nmulticode still results in a performance gain because the number of transfers exe\u00adcuted is reduced. When \nseveral bytecode s JMI are concatenated some op\u00aderations may become redundant. For example, if one byte\u00adcode \nends with an operation such as incrementing the stack pointer and the next bytecode immediately undoes \nthis op\u00aderation both operations can be removed. An optimized mul\u00adticode is one whose JMI has been optimized \nto remove these kinds of unnecessary operations. Table 1 shows the JMI for three commonly occurring bytecodes. \nTable 2 shows the op\u00adtimized and unoptimized JMI for 2 multicodes constructed from the bytecodes shown \nin Table 1. 3.1 Multicode Identi.cation The number of possible k-arity multicodes quickly be\u00adcomes excessive \nas k increases. For example, a rough approx\u00adimation of the number of 2-arity multicodes is the number \nof bytecodes squared, or 2012 = 40401. However, this is an upper bound, because many bytecodes cannot \nlegally follow each other because the types of the stack values provided ACM 1-58113-751-6/03/0010. \nand required are incompatible. For example, less than 28000 2-arity sequences are type correct (approximately \n70 per\u00adcent) and less than half of the 8.1 million 3-arity sequences are type correct. Furthermore, testing \nhas also shown that the number of distinct multicodes that actually occur in any application is relatively \nsmall. For example, no application executed more than 1430 distinct 2-arity multicodes or 2736 3-arity \nmulticodes. The existing Java Virtual Machine speci.cation leaves 52 bytecodes unde.ned. However, speci.c \nimplementations of the virtual machine may make use of these bytecodes sug\u00adgesting that fewer than 52 \nbytecodes are still available. Sev\u00aderal other techniques exist for increasing the number of avail\u00adable \nbytecodes including increasing the size of each opcode from 1 byte to 2 bytes (either at class load time \nor by stor\u00ading 2-byte opcodes in a class .le), despecializing the class .le to remove uses of specialized \nbytecodes such as iload 0 and allowing the wide bytecode followed by a bytecode not normally permitted \nto follow wide to represent a multicode. Regardless of the approach taken to increase the number of bytecodes \navailable for use by multicodes, there will still generally be more multicodes used than can be implemented. \nThis suggests that it is necessary to determine which mul\u00adticodes are best to implement. As an initial \napproximation, one can simply count the number of times each multicode executes across a variety of (hopefully \nrepresentative) appli\u00adcations and conclude that the multicodes that occur with greatest frequency should \nbe implemented. A more care\u00adful analysis must consider the amount of optimization that can be performed \non each candidate sequence. An algo\u00adrithm is presented which allows the performance bene.ts of candidate \nsequences to be compared based on both their frequency of occurrence and optimization potential. 3.2 \nMulticode Optimization As shown in Table 2, some bytecode sequences can provide a signi.cant amount of \nroom for optimization when transfers are removed. While few commonly occurring bytecode se\u00adquences o.er \nsuch substantial optimization potential, most sequences o.er some optimization potential when the byte\u00adcodes \nin the sequence are related to each other in some way (for example, the .rst bytecode places a value \non the stack which is used by the second). Several optimization rules are presented which can be used \nto optimize multicodes. Furthermore, these rules take the semantics of the operand stack into consideration. \nThis allows optimizations to be performed which could not be performed by a general pur\u00adpose optimizer \nthat is not aware of the stack semantics being applied to the array used to represent the operand stack. \n 3.3 JIT Compilers and Multicodes Most performance oriented JVMs rely on JIT technology, which replaces \nthe software fetch-decode-execute loop used to execute the bytecodes with chip-speci.c, optimized, low\u00adlevel \ncode for the most computational intensive areas of the program. All optimizations that are identi.ed \nfor use in multicodes are also available for use in JIT optimizers. Since much of the multicode analysis \ncan take place o.-line, JIT optimizers will bene.t from the identi.cation of sequences of bytecodes that \ncan be e.ciently optimized. Since JITing occurs at run-time, their optimizations are often less com\u00adplete \nthan those in a more traditional optimizing compiler. By identifying and optimizing sequences o.-line, \nJIT opti\u00admizers can spend more time on other forms of optimization. Some JIT compilers, such as that \nincluded with IBM s Jikes Research Virtual Machine, make use of bytecode id\u00adioms during their compilation \nprocess. By performing multi\u00adcode substitution ahead of time the need to perform match\u00ading of the idioms \nto the code stream at runtime is removed, improving performance. 3.4 Multicode Results In order to prove \nthe utility of multicodes, we have ob\u00adtained two kinds of results. First, we establish numerous multicode \nmetrics, including an identi.cation of the most common k-arity multicodes, and, more generally, a com\u00adplete \nordering of multicodes based on frequency of occur\u00adrence for arities from 1 to 7. Additional metrics \ninclude the percentage of multicodes common to all benchmarks, an identi.cation of multicodes dominant \nacross all bench\u00admarks, and statistics on average and maximum multicode block size. This work expands \non previous studies which have computed bytecode metrics [6, 2], without considering the order in which \nthe bytecodes were executed. Second, we present timing results demonstrating the rel\u00adative improvement \nof a JVM with multicode support com\u00adpared with one without multicode support. It was found that adding \nonly .ve commonly occurring multicodes (al\u00adoad 0/getfield, iconst 1/iadd, aload 0/dup, dup/getfi\u00adeld \nand getfield/arraylength) to the Ka.e virtual ma\u00adchine resulted in speedups for Java applications ranging \nbe\u00adtween 4.3% (javac) and 7.5% (compress) depending on the application in question. Further results, \nas well as the full text of the poster, can be found on the web at http://www. csd.uwo.ca/ wade/Meta/Multicodes. \n 4. ACKNOWLEDGMENTS The authors would like to thank NSERC for their generous support of this research. \n 5. REFERENCES [1] J. Gosling, B. Joy, G. Steele, and G. Bracha. The Java Language Speci.cation. Addison \nWesley, Boston, Massachusetts, second edition, 2000. [2] D. Gregg, J. Power, et al. Platform independent \ndynamic java virtual machine analysis: the java grande forum benchmark suite. [3] T. Lindholm and F. \nYellin. The Java Virtual Machine Speci.cation, 2nd Edition. Addison-Wesley Publishing Company, Reading, \nMassachusetts, 1999. [4] I. Piumarta and F. Riccardi. Optimizing direct-threaded code by selective inlining. \nIn SIGPLAN Conference on Programming Language Design and Implementation, pages 291 300, 1998. [5] T. \nA. Proebsting. Optimizing an ANSI C interpreter with superoperators. In Conference Record of POPL 95: \n22nd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, pages 322 332, San Francisco, \nCalifornia, 1995. [6] R. Radhakrishnan, J. Rubio, and L. John. Characterization of java applications \nat bytecode and ultra-SPARC machine code levels. pages 281 284. [7] T. Suganuma, T. Ogasawara, et al. \nOverview of the IBM java just-in-time compiler. IBM Systems Journal, 39(1):175 193, 2000.    \n\t\t\t", "proc_id": "949344", "abstract": "A virtual machine optimization technique that makes use of bytecode sequences is introduced. The process of determining candidate sequences is discussed and performance gains achieved when applied to a Java interpreter are presented. The suitability of this optimization for JVMs that perform just-in-time compilation is also discussed.", "authors": [{"name": "Ben Stephenson", "author_profile_id": "81100072563", "affiliation": "University of Western Ontario, London, ON, Canada", "person_id": "P643430", "email_address": "", "orcid_id": ""}, {"name": "Wade Holst", "author_profile_id": "81100018329", "affiliation": "University of Western Ontario, London, ON, Canada", "person_id": "P295902", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/949344.949436", "year": "2003", "article_id": "949436", "conference": "OOPSLA", "title": "Multicodes: optimizing virtual machines using bytecode sequences", "url": "http://dl.acm.org/citation.cfm?id=949436"}