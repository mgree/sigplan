{"article_publication_date": "10-26-2003", "fulltext": "\n The Power of Symmetry: Unifying Inheritance and Generative Programming DeLesley Hutchins CISA, School \nof Informatics, University of Edinburgh Appleton Tower, 11 Crichton Street, Edinburgh, EH8 9LE, UK +44 \n(0) 131 650 2732 hutchins@mza.com ABSTRACT I present the Ohmu language, a unified object model which \nallows a number of advanced techniques such as aspects, mixin layers, parametric polymorphism, and generative \ncomponents to be implemented cleanly using two basic concepts: block structure and inheritance. I argue \nthat conventional ways of defining classes and objects have created artificial distinctions which limit \ntheir expressiveness. The Ohmu model unifies functions, classes, instances, templates, and even aspects \ninto a single construct the structure. Function calls, instantiation, aspect-weaving, and inheritance \nare likewise unified into a single operation the structure transformation. This simplification eliminates \nthe distinction between classes and instances, and between compile\u00adtime and run-time code. Instead of \nbeing compiled, programs are reduced using partial evaluation, during which the interpreter is invoked \nat compile-time. Within this architecture, standard OO inheritance becomes a natural vehicle for creating \nmeta-programs and automatic code generators the key to a number of recent domain-driven programming \nmethodologies.  Categories and Subject Descriptors D.3.3 [Programming Languages] Language Constructs \nand Features -- aspects, classes and objects, frameworks, inheritance, mixins, patterns, polymorphism \nF.3.3 [Logic and Meanings of Programs] Studies of Program Constructs -- object-oriented constructs F.3.2 \n[Logic and Meanings of Programs] Semantics of Programming Languages -- partial evaluation D.1.2 [Programming \nTechniques] Automatic Programming program transformation, meta-programming D.3.4 [Programming Languages] \nProcessors compilers, interpreters, code generation, pre-processors General Terms Design, Languages, \nTheory Keywords aspects, aspect-oriented programming, code generation, covariant specialization, generative \ncomponents, generative programming, generic types, join points, meta-programming, mixins, mixin layers, \nmultiple inheritance, parametric polymorphism, partial evaluation, prototypes, transformation systems, \nvirtual classes, virtual types  1.INTRODUCTION The general premise behind domain-driven development \nis that object-oriented languages and tools have somehow failed to live up to expectations. There are \nmany concepts and behaviors which cannot be easily encapsulated within standard models of classes, methods, \nand objects. Clean interfaces which hide implement\u00adation details often produce inefficient code. Cross-cutting \nconcerns result in code which is tangled throughout class hierarchies. Perhaps most importantly, specialized \nsoftware domains, such as data-flow systems and language parsers, have domain-specific notations that \nbear little resemblance to standard object models. A number of different approaches have recently been \nproposed for dealing with these issues. Instead of using good old fashioned classes and objects, these \napproaches propose new programming language constructs with clever names, such as aspects, intentions, \nsubjects, features, and components. If nothing else, we are beginning to face a crisis of terminology \n there are, after all, only so many generic-sounding words in the English language. The proliferation \nof post-object-oriented tools and techniques raises a larger question: How do these ideas fit in with \nthe principles of object-oriented design? Most tools, such as Genvoca and AspectJ, are layered on top \nof an existing OO language.1,13 The idea is that aspects and components do not replace objects, they \nmerely exist at a different level of abstraction, just as objects themselves are constructed by combining \nsubroutines with abstract data types. Unfortunately, a technique must do more than merely improve expressiveness \nand power; it must also fit in with the general conceptual framework that programmers use on other parts \nof the code. A programming environment in which features and aspects are layered on top of classes and \nobjects, which in turn are based on functional decomposition, does not present a clear and consistent \nmental model to the programmer. Throw in macros, templates, mixins, and interfaces, and the resulting \nmish-mash of tools is enough to confuse even the most dedicated software engineer. Transformation systems \nsuch as the intentional programming environment18,8 may allow different language constructs to be mixed \nseamlessly together, but they do not address this underlying conceptual confusion. The Ohmu* language \nwas designed to unify these disparate techniques into a form which should be familiar to any developer \nwho understands basic OO principles. *The name Ohmu was whimsically taken from an old Japanese animation. \nIt refers to giant telepathic insects which rule over a Copyright is held by the author/owner(s). jungle \nfilled with genetically-engineered creatures that are taking OOPSLA 03, October 26 30, 2003, Anaheim, \nCalifornia, USA. over the world. Only time will tell whether the Ohmu language ACM 1-58113-751-6/03/0010. \nsucceeds in restoring order, or just becomes a giant bug. </humor> Object-oriented design is fundamentally \nconcerned with only two operations: aggregation and specialization. These two operations provide the \nunifying factor for the Ohmu model. Aggregation combines simple objects to form more complex ones, and \ngroups sets of interacting objects behind a well-defined interface. This principle is also known as separation \nof concerns. Specialization allows specific implementations to be increment\u00adally created from more general \ndescriptions. In OO terms, aggregates are usually classes, and specialization is implemented through \ninheritance, but these principles also describe templates, aspects, features, and components. Each of \nthese constructs provides a specific mechanism to either group concepts together, or specialize existing \nconcepts by modifying them or instantiating them in a particular way. The similarities between these \nconstructs may not be immediately evident, not least because they are usually implemented in very different \nways. However, a high level language should properly hide implementation details from the programmer. \nAs I shall attempt to illustrate in this paper, all these constructs can be unified once the implementation \ndetails are stripped away. Compiled languages such as C++ and Java expose the operation of the compiler \nby creating artificial distinctions which restrict the set of legal operations to those which can be \neasily compiled. It is no accident that C++ templates are both the most powerful feature of the language \nfrom a generative programming standpoint, and by far the most difficult mechanism for compilers to handle. \nDynamic languages such as Smalltalk and CLOS are somewhat more flexible in this regard, but they achieve \nthat flexibility at the cost of considerable run-time overhead. The Ohmu language implements aggregation \nand specialization in a more general fashion, and it does so in a compiler-friendly manner. With a unified \nobject model, many domain-driven technologies are not quite as revolutionary as they might first appear. \nIf templates, aspects, and mixin-layers can be handled in the same way as classes and methods, then standard \nOO inheritance becomes powerful enough that there is no need to replace it with more esoteric constructs. \nThe failure of object-oriented languages is not so much due to limitations of the class/inheritance model \nas it is due to limitations in the way that model has been implemented in current languages. The design \nof a truly general purpose language is analogous to the principle of symmetry in modern physics. When \nfaced with a number of seemingly different items which display certain puzzling similarities, one must \nattempt to find some general unifying framework which describes all of them. This approach necessarily \ninvolves discarding any superficial distinctions which are not absolutely critical to the definitions \nof the underlying concepts. In the pursuit of symmetry, the Ohmu object model eliminates three major \ndistinctions which are a part of most other languages: 1. It eliminates the distinction between compile-time \nand run-time code. 2. It eliminates the distinction between classes and instances. 3. It eliminates \ndistinctions of scale. Class hierarchies and frameworks can be manipulated using the same operations \nas individual classes and methods.  I will refer to Ohmu as both a language and an object model throughout \nthis paper. Ohmu is not a language in the traditional sense, because it has no set syntax. I do provide \na native syntax which has been designed specifically for Ohmu, but the language also provides an API \nfor plugging alternative parsers into the compiler and interpreter. I will use the Ohmu native syntax \nin the rest of this paper. Like most variants of Lisp, Ohmu programs are data structures, and the language \nitself is defined as a core library which is glued together with a meta-object protocol. Like the intentional \nprogramming system, Ohmu thus describes a family of languages which are related by program transformations \nand a common object model. Unlike the intentional programming system, however, Ohmu does not rely on \nan integrated development environment.18,8 One of the prime benefits of unification and symmetry is that \nthe Ohmu object model is flexible enough that it can emulate other languages simply by defining an appropriate \nparser for the syntax along with an emulation library to handle the language's more arcane features. \nBoth Java and C++ can be emulated to some extent with this mechanism, as I shall describe in section \n7. This paper is divided into several parts. Section 2 provides an analysis of existing approaches to \ngenerative programming. Sections 3 and 4 describe the Ohmu prototype model, section 5 introduces the \nrun-time/compile-time model, and section 6 discusses topics of scale. Section 7 covers language emulation, \nand section 8 concludes. 2. GENERATIVE PROGRAMMING Domain-driven architectures use layers of software \nto bridge the gap between problem space and solution space. Each layer takes concepts at one level of \nabstraction, and translates them into lower-level concepts which are one step closer to an actual implementation.8 \nSoftware layers are nothing new; standard OO and procedural decomposition also advocate the use of layers \nto manage systems of high complexity. Procedural and OO methods, however, use forward refinement at the \nlevel of classes and subroutines to move from one layer to the next. Unfortunately, this mechanism is \nnot powerful enough to cross large gaps between the problem space and the solution space. High-level \nconcepts must often be translated into forms in which the original modular structure becomes tangled, \nor crosses hierarchical boundaries. Software layers of this nature can be implemented using transformational \ngenerators or meta-programs. The challenge of meta-programming, however, is not simply making it possible. \nIf writing computer code is hard, then writing code generators is doubly hard. The most difficult issue \nwhich currently faces the domain-engineering community is discovering tools and methods which make it \neasier to design, implement, and compose software generators. Software generators can come in many different \nforms. Macros are the oldest and most basic generators, and the one familiar to most programmers. At \nthe other end of the spectrum are complete domain-specific languages, such as LEX and YACC, which essentially \nact as compilers in their own right. One basic difference which influences the usability of code generation \nsystems in general is the level of integration they have with the host language. Code generators are \nseldom full-featured languages in their own right, and this means that they must rely on the host language \nin order to perform certain tasks. It is here that most current code generation technologies fall down \non the job. Many macro systems, for instance, are capable of sophisticated syntactic transformations, \nbut they may not be lexically scoped, and they do not actually understand the semantics of the code they \ntransform. Unexpected changes to the source tree, such as those produced by another generator, can confuse \na macro system. One might think that C++ templates would be well integrated, since they are part of the \nactual language specification. Nevertheless, although templates can perform basic integer arithmetic \nat compile time, they cannot deal with arbitrary C++ objects. Basic program control such as if statements \nand for loops must be implemented in a completely different way from normal code. Although templates \ncan manipulate C++ types, a template does not exist until it is instantiated, and therefore the template \ndeclaration itself does not have a type associated with it. This contrasts strongly with the standard \nOO inheritance model, in which a base class defines not only an interface, but a type name which can \nbe used to declare polymorphic objects that obey that interface. Parametric polymorphism, as implemented \nby C++ templates, thus follows a different set of type rules than subtype polymorphism, which is implemented \nthrough classes. In an ideal world, code generators would be perfectly integrated with the base language. \nThe authors of such generators would no longer have to worry about duplicating functionality, and would \nbe assured that their extensions will work properly with other third party generators and tools. Such \ntight integration can be achieved by means of a reflective meta-object protocol, or MOP.14 A MOP exposes \nthe underlying structure of source code in the base language, and provides facilities for third-party \nextensions to query object interfaces and modify object definitions. Most dynamic languages do provide \nsome support for reflection. Smalltalk allows classes to be queried and methods to be added at run-time, \nwhereas Lisp exports the entire program structure as an abstract syntax tree. CLOS provides a high-level \nMOP that exports the entire OO machinery as a set of APIs which can be manipulated by class meta-objects.14 \nJava's reflection API provides more limited support for these types of operations. MOPs can also be provided \nfor compiled languages; OpenC++ is perhaps the best such example.7 A compile-time MOP compiles, dynamically \nlinks, and evaluates user code at compile-time. Meta-object protocols are a valuable implementation tool \nfor creating code generators which are integrated with the host language. Nevertheless, MOPs and meta-programs \nhave been around for quite a while, and they have not yet ushered in a new era of domain-driven development. \nCzarnecki and Eisenecker argue that writing code generators is a task best left to specialists in the \nart of library construction.8 This is certainly true to some extent. Dr. Batory's experience with domain-specific \ngenerators reveals that acquiring enough domain knowledge and expertise is often more difficult than \nactually implementing the generator.2 However, library authors are not some elite crowd of specialists. \nEvery application of any size requires its own libraries in addition to third-party tools. Moreover, \ngeneral-purpose tools such as data structure generators must be integrated with specific domain knowledge \nrelated to the application itself. One problem with meta-object protocols is that it is not immediately \nobvious how to write generators that can be composed in an arbitrary manner. A typical procedural generator \nwhich acts through a MOP first issues query commands against the code base to determine structure and \nfunction. The results of those queries are then analyzed to determine what modifications should be made, \nand a series of imperative calls then make the actual changes. A generator written in this fashion may \nbecome confused if the code that it is attempting to modify is first transformed by another generator \n perhaps one supplied by a different vendor. This is a hard problem: scheduling and eliminating conflicts \nbetween generators is one of the most difficult tasks that a transformation system must perform.8 A more \nsubtle issue is the fact that meta-object-protocols can operate at several layers of abstraction. The \nmost popular are those that operate on abstract syntax trees, or ASTs. The benefit of ASTs is that they \nare easy to build, traverse, and modify, and they provide a generic representation for any arbitrary \npiece of code. The downside is that they are a very low-level representation; only one step above text \nitself. ASTs describe the syntax of programs, rather than the semantics, and that means that code generators \nmust do a great deal of work to interpret the meaning of program fragments. More sophisticated systems, \njust as CLOS, Smalltalk and the Java reflective API, operate at the level of classes and methods. This \nis a better approach, but the code generator must still explicitly query, analyze, and modify classes. \nThis is an unavoidable consequence of going through an API; no matter how good the API, specifying generators \nprocedurally will always be more difficult than specifying them declaratively. The AspectJ language is \nessentially a declarative MOP.13 An aspect in AspectJ is defined by declaring join points, which are \nclasses, methods or other locations in the code which need to be modified. The modifications are described \nas operations on those join points, and the actual code traversals are then performed by the aspect system, \nrather than a purpose-built procedural generator. The Ohmu language takes a somewhat different approach \nto declarative meta-programming. Instead of creating a separate meta-language such as macros, templates, \nor AspectJ, Ohmu extends the set of modifications that can be performed with standard OO inheritance. \nOne of the primary benefits of OOP is that it presents a clear and coherent mental model, so that even \nnovice programmers can get applications up and running quickly. Domain-driven tools should operate in \nthe same manner. The Ohmu language does include several procedural meta-object protocols: one for creating \nand manipulating abstract syntax trees, and another for manipulating classes and methods. However, these \nare provided as low-level tools only, a mechanism of last resort for library authors who cannot accomplish \nwhat they need to do in any other way. My goal for the Ohmu language is to integrate code generation \ninto standard OO constructs to the extent that it becomes completely transparent. If I have done my job \nproperly, the average programmer may not even realize that the classes and objects she is creating are, \nin fact, generators. 3. PARAMETERIZATION Object-oriented inheritance is founded upon the idea that general \nor abstract specifications can be refined to more specific imple\u00admentations. A general solution is one \nwhich can be specialized to create a variety of different concrete instances. An abstract specification \nis one which avoids expressing implementation details as much as possible. The two normally go together; \nthe act of specializing a general solution will fill in the missing details. General solutions express \npossible variations by means of parameters. C++ templates and generic programming have brought parameterized \nalgorithms into limelight, but they are hardly alone. In fact, parameterization is common to nearly all \nprogramming language constructs, and it thus forms the basis for unifying those constructs together. \nThe arguments to a function or subroutine are parameters of that function. Those parameters are bound \nto specific values when the function is evaluated, a process which is formally defined by the lambda \ncalculus. Classes, in turn, have two kinds of parameters data members and methods. Each instance of \na class specializes that class by binding the data members to actual values. The methods of a class are \nlikewise bound to new definitions during inheritance. A Java interface, like a C++ template, also represents \na generic, parameterized solution. The methods of the interface must be overridden (i.e. bound) to concrete \ndefinitions in order to be useful. Another property which functions, classes, and templates have in common \nis that the act of binding new values is non-destructive; the original object is used as an archetype* \nto create a new object with the same interface. For example, the arguments of a function are bound to \ncreate an activation record for that function. If activation records are treated as first-class objects, \nthen they become closures, an important element of most functional languages. The constructor of a class \ngenerates instances of that class which have the same interface as the class definition. The inheritance \noperation likewise creates new derived classes which are subtypes of the originals. Templates are similarly \ninstantiated to form concrete classes. There are only two real differences between instantiation, function \ncalls, and inheritance. These operations have different binding times, and they bind different kinds \nof objects. Functions and class instantiations are almost identical. They both bind data values at run \ntime. Templates and inheritance, on the other hand, operate at compile time. Templates bind types and \nconstants, whereas inheritance binds method definitions. Functions do differ from classes in three ways. \nActivation records are usually allocated on the stack rather than the heap, functions have return values, \nand functions evaluate a function body. These last two are not really differences, since they can be \neasily emulated using class constructors. For example, here's the factorial function written as a C++ \nclass: class Factorial { int n, result; Factorial(int n_) : n(n_) { if (n <= 1) result = 1; else \nresult = n*Factorial(n-1).result; } }; In other words, we have three different constructs (templates, \nfunctions, and classes), with four different operations (function calls, inheritance, instantiation, \nand template instantiation), all of which do essentially the same thing. If this was merely a matter \nof terminology it wouldn't be a problem, but the choice of which construct and operation to use dramatically \naffects both the interface of the objects we construct, and the uses to which those objects can be put. \n* By archetype I mean a general pattern, template, or prototype. Unfortunately, those are no longer general-purpose \nwords. Consider the problem of defining a simple array class. Arrays are the most basic of all data structures, \nso this should be a trivial problem for any OO language. An array has three main parameters: 1. The type \nof object held by the array. 2. The number of dimensions. 3. The dimensions themselves.  The use of \na type parameter means that Java is already out of the picture without an extension like GJ, although \nversion 1.5 will support generics as well.5 With C++, we have several choices of representation. Declaring \nthe number of dimensions as a template parameter allows us to simplify memory layout and optimize the \nelement accessor methods. Using constant values for the dimensions themselves allows further optimization; \nif the dimensions are a power of two then the stride calculations can use fast bit shifts instead of \nmultiplication. Unfortunately, if we specify either of these as template parameters then arrays can only \nbe allocated using compile-time constants, which is not always possible. We are faced with a lose-lose \nsituation either declare all array sizes at compile time, or define multiple classes which are essentially \nidentical except for different binding times. To add insult to injury, the syntax for each kind of declaration \nis completely different: ArrayRT<int>(2, 256, 256); // run-time ArrayCTDim<int, 2>(256, 256); // compile-time \n // fully compile-time, with traits list ArrayCT<int, 2, Cons<256, Cons<256, End> > > While it is possible \nto present a somewhat more unified interface to Array using further template meta-programming techniques, \nthis solution is clearly not scalable: a class with n parameters requires 2n implementations just to \nhandle different binding times. The solution to this dilemma is obvious. There should be only one construct, \nand that construct should not place any restrictions on either the binding times of parameters, or the \nkinds of parameters which can be bound. Here is the equivalent code in Ohmu: Array: Struct { DataType: \nObject; rank: Integer; dimensions: List.of(Integer); of: bind(DataType); ofRank: bind(rank); ofSize: \nbind(rank, dimensions); }; //compile-time: { } declares a constant list Array.ofSize(2, {256 256}); \n// run-time Array.ofSize(dim_list.length, dim_list); // partially specialized Array.ofRank(2); This \nexample introduces the Ohmu structure, which is declared with the Struct keyword. Structures support \naggregation, and they are the only aggregate construct provided by the language. Functions, classes, \nmethods, templates, interfaces, modules, and so on are all simply variations on the structure. Structures \nprovide a name space for parameters, which are declared with a name: declaration; syntax. Parameters \nand methods within the structure are accessed through the familiar dot notation. The bind keyword declares \na named constructor which allows parameters to be bound to more specialized values. Calling any of the \nofX methods here will thus instantiate a new structure with the specified parameters. The process of \nparameter binding in Ohmu is referred to as a structure transformation, for reasons which will become \nevident in later sections. Note that parameters may be bound to either constants or variables, and the \ncompiler can easily detect the difference between the two. Constant binding is performed immediately \nat compile time, while variable binding is deferred until run-time. This binding mechanism is controlled \nby a partial evaluation engine, which I will describe in section 5. With a flexible parameter binding \nscheme, declaring functions and methods is no different from declaring classes: factorial: Struct { \nn: Integer; result: if (n <= 1) then 1 else n*factorial(n-1); implicit call: bind(n) = result; }; factorial(3); \n// returns 6 There are two differences which make this structure into a function rather than a class, \nboth of which amount to little more than syntactic sugar. First, the bind declaration now specifies a \nreturn value. This eliminates the need to put a .result after each call. Second, the implicit keyword \nworks like the operator() declaration in C++, and eliminates the need to type in .call. Since the above \ndefinition is somewhat verbose for everyday use, Ohmu also provides a Function macro which automates \nthe process: factorial: Function((n: Integer), Integer) { if (n <=1) then return 1 else return n*factorial(n-1); \n }; The Functionmacro specifies both the argument list and return type, and implements a more conventional \nimperative statement list for the function body. In terms of functionality though, it's just syntactic \nsugar. 3.1 Prototypes The array example demonstrated how a generic class can be specialized to a more \nconcrete version by binding parameters to values. A structure transformation in this case maps from one \nclass to another class. The factorial function likewise maps from a function to a closure. Class instantiation, \nwhich maps a class to an instance, can be done similarly: Point: Struct { x,y: Float; new: bind(x,y); \n }; origin: Point.new(0,0); I have not yet introduced inheritance, but methods can be overridden in \nmuch the same way: by binding abstract declarations to specific definitions. Such a transformation maps \nfrom a class to a class. This model has an important consequence. My initial objective was merely to \neliminate the distinction between run-time and compile-time code. However, eliminating that distinction \nalso eliminates the distinction between types and objects, and between classes and instances. The same \noperation can either specialize one class to create another class, or specialize a class down to an instance. \nThe difference lies in usage, not semantics; the Ohmu compiler has no real way to tell classes and instances \napart. In order to distinguish between classes and instances, it would be necessary to use two separate \noperations one which binds abstract type parameters to concrete values to create an instance, and another \nwhich specializes abstract parameters to more specific (but still abstract) versions. This is, of course, \nwhat almost all other typed languages do, but it introduces needless complexity when one operation will \nsuffice, and thus violates the symmetry principle of the Ohmu language design. If instantiation and inheritance \nare the same operation, then an instance is a subtype of a class. There is no longer any difference between \nthe is-a and instance-of relationships. Compiled OO languages like C++ and Java exist in a Platonic universe. \nPrograms are declared in terms of classes, which are idealized definitions with no state or run-time \nrepresentation. It is impossible to call methods on the class meta-object itself (even if it exists at \nrun-time), because without a this pointer to the current instance, such calls have no meaning. Real objects \nare quite different. They exist at run-time, have state, and can be used in computations. However, this \nPlatonic model is largely a consequence of the separation between run-time and compile-time. The statelessness \nand hence abstract quality of classes is due to the fact that run\u00adtime state does not yet exist during \ncompilation. By eliminating that distinction, classes and instances become essentially the same. All \ntypes are first-class objects in Ohmu, but all objects are also first-class types. So what does it mean \nto have a type of 5 ? Abstract types are based on set theory. A type represents the set of all instances \nwhich have that type. The type 5 is merely a set with one element; it represents the set of all integers \nequal to 5. Let's do an informal test, the is-a test. Since traditional OO languages distinguish between \nclasses and instances, in C++ or Java we would would be forced to write: Fluffy is-an-instance-of Dog. \nA Dog is-a Mammal. A Mammal is-a Living_Thing. The Ohmu version is somewhat more natural: Fluffy is-a \nDog. A Dog is-a Mammal. A Mammal is-a Living_Thing. Or with more formal mathematical concepts: 5 is-an \nInteger An Integer is-a Number A Number is-an Object I argue that this is a simpler conceptual model \nthan the more popular class/instance dichotomy. For one thing, the Prototype and Singleton design patterns \nare no longer necessary.10 Instead of trying to define behavior in terms of classes, which can be a somewhat \nnebulous concept, behavior is defined in terms of real prototypical structures, which can be refined, \nspecialized, and copied as needed. This gives the Ohmu language a somewhat more physical or hands-on \nflavor. Note that while other dynamic OO languages like Smalltalk also represent classes as run-time \nobjects, they still maintain the same class/instance dichotomy. In Smalltalk an object is an instance \nof a class, a class is an instance of a meta-class, and so on. Since a na\u00efve implementation would lead \nto the it's turtles all the way down phenomenon, Smalltalk is forced to stop at the meta-class, which \nis an instance of itself. All behavior in Smalltalk is specified in terms of classes, which preserves \nthe Platonic model. Ohmu objects, in contrast, are self descriptive. The behavior of an Ohmu structure \nis determined entirely by the objects it contains; classes are only used to establish type relationships. \nAn Ohmu class can be more properly described as an abstract prototype. It is abstract because certain \nparameters have not been fully specialized. In the previous example, Point.x and Point.y are declared \nonly as floating point numbers, without any values attached. This abstract quality does not make the \ninterface of Point any different from that of origin, however; the method x can still be called on both \nof them. Point.x returns Float an abstract object while origin.x returns 0 a concrete object. Since \nboth Float and 0 are themselves first-class objects which also have the same interface, there is no need \nto treat Pointand origindifferently. 3.2 Abstract Prototypes The prototype model does raise a few conceptual \nquestions. If types are the same as objects, then abstract types such as Integer and Float must behave \nlike actual numbers. So what does it mean to call Integer+1 or sqrt(Float) ? An abstract type such as \nInteger or Float essentially represents a don't know value. The compiler doesn't know what the precise \nvalue of Integer+1 is, but it does know that the result will be an Integer. The expression sqrt(Float) \nwill likewise return Complex. This system has two important advantages. First of all, it is possible \nto perform computations with objects that are not fully defined. This property is a requirement of the \npartial evaluation engine, which I will describe in section 5. Secondly, type inference and type checking \nbecome trivial operations. The types of computations can be calculated merely by evaluating expressions \nwith abstract prototypes. This facility makes it much easier for intelligent libraries and applications \nto reason about the types of data they must process. Because there is no distinction between run-time \nor compile-time, Ohmu programs can be either statically or dynamically typed. A certain amount of type \ninformation will be present in the code at compile time. How much information is entirely up to the programmer. \nOhmu supports complex type declarations, such as PositiveInteger: SuchThat(x: Integer, x >= 0); so a \ngreat deal of knowledge can be encoded into the type system. (The SuchThatfunction is part of the Ohmu \nstandard library.) Any type checks which cannot be determined at compile-time will be deferred until \nrun-time. During rapid prototyping, a programmer might declare all variables to be of type Object, and \nthen fill in more detailed information later on as the project matures. The Ohmu compiler generates warnings \nwhen statements require run-time type checks, but these warnings can be turned off. 3.3 Prototypes and \nShared Behavior The difference between a traditional type system and a prototype model is that traditional \ntype systems use instantiation to create objects, whereas prototype models rely on cloning. Ohmu implements \nspecialization via structure transformations, which encompasses both. A full instantiation binds all \nabstract parameters to concrete values. A cloning operation does not bind anything; it simply creates \na new structure in which all parameters have been duplicated. Specialization typically operates somewhere \nbetween the two. A structure transformation may bind a few parameters, but not all of them; any parameters \nwhich are not bound will be copied over unchanged. For example, the declaration XAxisPoint: Point.new(Float, \n0); creates a partially specialized point prototype in which y has been fixed at zero, while xremains \nabstract. The Ohmu prototype system should not be confused with the prototype models used by other languages \nlike Self. In Self, an instance and a class are different objects, with completely different interfaces. \nA class in Self is a repository for shared methods, while an instance contains data members, and then \ndelegates messages to the class.21 An Ohmu class and an instance of that class have the exact same interface. \nThere is no explicit division into shared and local behavior. Like biological organisms, a child object \ninherits duplicate copies of all code from its parent. Methods and data members in Ohmu are conceptually \nthe same they are both parameters which are copied during a structure transformation. In reality, the \nOhmu compiler will do its best to share method code between instances, but such sharing is regarded as \na compiler optimization rather than a behavior which must be deliberately specified by the programmer. \n 3.4 Virtual Types The unification of types and objects also provides a compelling model for virtual \ntypes. Here is the simple array declaration from before: Array: Struct { DataType: Object; rank: Integer; \n dimensions: List.of(Integer); of: bind(DataType); ofRank: bind(rank); ofSize: bind(rank, dimensions); \n }; Notice that the DataType and rank parameters are both declared and used in identical ways. DataType \ncan be specialized to any subtype of Object, just as rank can be specialized to any subtype of Integer. \nThere really is no reason to distinguish between the two. In fact, the type/object unification can actually \nbe handy; Array.of(0) declares an array in which all elements are constrained to be of type 0 , a declaration \nthat can be easily optimized. There has been something of a debate in recent years over whether virtual \ntypes, which use subtype polymorphism (i.e. inheritance), or parametric polymorphism (such as that used \nin Java generic classes and C++ templates) is the best way to implement generic classes. In one sense \nit is something of a moot point, because the two are largely equivalent.11,20 Virtual types can emulated \nusing f-bounded parametric polymorphism by passing the derived class as a parameter to the base class. \nSee [11,20] for more details. This is analogous to the approach currently used to implement OO methods, \nwhere an instance passes a pointer to itself (the this pointer) back to the class as the first argument \nof a method call. In Ohmu this implementation would not be merely analogous, it would be exactly the \nsame, since the instance of a class is the most specialized version of that class. On the other hand, \ngeneric classes can also be implemented with virtual types simply by embedding the type parameters as \nnormal members of the class. There is one small hurdle, and that is that two instances of a generic class \nmay be type equivalent, even though they do not inherit from each other. For example, we would like Array.of(Integer) \nto be treated as a subtype of Array.of(Number). Both structures inherit directly from Array, however, \nso the two structures are siblings rather than parent and child, and there is not automatically any subtype \nrelationship between the two. Ohmu resolves this problem by introducing a =~ subtype comparison operator. \nIt is similar to the == operator, except that it tests for type equality rather than value equality. \nInteger=~ 0 returns true, while Integer == 0 returns Boolean. Ohmu automatically defines == and =~ operators \nthat do an element-by-element comparison between two structures, so long as both structures derive from \na common base type. Sibling structures can thus be subtypes of one another. With this extended type checking \nin place, the difference between parametric polymorphism and virtual types boils down to the difference \nbetween functional and object-oriented programming. A functional style is useful when the number of parameters \nis small, and it is easy to deal with them as an ordered list arguments. As the number of parameters \ngrows, however, it becomes more useful to organize them into records, where they can be individually \nselected and specialized by name. Ohmu supports both styles; the bind keyword provides a functional or \nparametric syntax, while extends and transform (described below) provide a syntax that resembles OO inheritance. \n 4. INHERITANCE AND MIXINS In the previous section I introduced the Ohmu structure transformation. I \nalso claimed somewhat glibly that inheritance could be implemented with transformations, without actually \ndemonstrating how this is accomplished. OO inheritance is actually a fairly complex operation, and the \nOhmu model breaks it down into several steps which are for the most part orthogonal. 1. Existing methods \nof a class may by bound to more specialized values. (Specialization) 2. New methods may be added to \nthe class. (Aggregation) 3. An invisible delegation link must be established between the old and new \nparts of the class. 4. A subtype relationship must be established between the base class and the derived \nclass.  Ohmu classes and methods are structures, so this list should more properly be written out as \na set of operations on structures and parameters rather than classes and methods. For the sake of clarity, \nhowever, I will stick to the standard OO terms. Whenever I use the terms class, method, or instance when \ndiscussing Ohmu programs, I am really referring to a structure that is being used to emulate that particular \nOO concept. Although I have mentioned specialization several times, I have not yet defined its precise \nmeaning within the Ohmu language. If object A specializes object B, then A must be a strict subtype (i.e. \nsubset) of B. Moreover, the interface of A and B must be exactly the same. Every message which can be \nhandled by A must also be handled by B, and vice versa. This strict definition of specialization is required \nto ensure plug compatibility between A and B. Ohmu is strongly typed, and plug compatibility is a requirement \nfor performing type-safe structure transformations. Since specialization is a subtype relation, it involves \na narrowing of behavior. Declarations that were once general become very specific. As more details are \nfilled in, the behavior of a class becomes more rigidly defined. Aggregation, in contrast, is a broadening \nof behavior. The behavior of several simpler objects is merged together to form a single complex object. \nThe Ohmu language implements these two principles as separate operations. Aggregation is performed by \ngrouping objects into structures. Specialization is done via structure transformations, which implement \nparameter binding. Structure transformations cannot add new methods; they can only modify existing ones. \nIn order for a derived class to inherit from a base class, it must first transform the base class, and \nthen embed that transformation as a parameter of the new derived class. Here's a simple example: Point: \nStruct { x,y: Number; }; Pixel: Struct { // create structure point: transform Point { x,y: Integer; \n// specialize x and y }; // from Number to Integer color: Integer; // add color }; Structure transformations \ncan be done using either the bind keyword or the transform keyword. The former provides a functional \nnotation for binding specific parameters, while the latter allows a list of parameters to be specified \nby name. Other than notation, the two operations are identical. Here we define a simple Point class, \nwhich declares x and y as abstract numbers. The Pixel class derives from Point. It specializes x and \ny to be screen coordinates, i.e. integers, and then adds a new parameter, color. The use of prototypes \nmeans that the Pixel class is free to redefine the types of xand y, even though they are data members \nof the class, so long as the new types are subtypes of the old. As it stands, however, while Pixel.point \nis a subtype of Point, Pixel itself is not. To complete the inheritance relation, a delegation link must \nbe established so that Pixel can forward messages, such as calls to x and y , to Pixel.point. This delegation \nlink will automatically define a subtype relationship and complete the inheritance operation. Pixel: \nStruct { extends Point { x,y: Integer; }; color: Integer; }; The extends keyword operates exactly like \ntransform, except that it establishes a delegation link with its enclosing structure. Since we are using \ndelegation, the parameter name becomes superfluous. 4.1 Multiple Inheritance and Software Layers Multiple \ninheritance has always been the Achilles heel of mainstream object-oriented languages. The main show-stopper \nis the infamous diamond problem, which is illustrated by figure 1. If a class inherits from two unrelated \nclasses, then there there is seldom a problem. But if it inherits from a common base class along two \ndifferent paths, than an ambiguity arises. The following example is adapted from [9]. Suppose we have \na base class Door, with two derived classes: ShortDoor and LockedDoor. A person must stoop to go through \na short door, and supply an appropriate key for the other. Now suppose that we want to combine some of \nthese features for instance to create a door that is both short and locked. In this case, the open() \nand pass() methods are ambiguous because they are inherited from two difference sources. Figure 1: the \ninfamous diamond problem Different languages resolve this ambiguity in different ways. C++ punts on \nthe issue and requires that all disambiguation be done by hand, which introduces code tangling at least \nas complicated as the problem that inheritance was being used to solve in the first place. Java and Ada \nsimply disallow multiple inheritance except for certain constructs (namely Java interfaces), which limits \nthe power of the approach. A more useful way to perform multiple inheritance was first introduced by \nCLOS. CLOS linearizes the inheritance tree so that it looks like the following: Door   ShortLockedDoor \n Linearization transforms a multiple inheritance tree into a single\u00adinheritance tree.4 In order for it \nto effectively resolve ambiguities, however, methods must be written in such a way that the method calls \ncan be chained together. For example, an implementation of LockedDoormight look like the following: LockedDoor: \nStruct { extends Door { pass: Function((p: Person), Boolean) { return person.hasKey(key) &#38;&#38; parent.pass(p); \n}; }; key: Key; }; Notice that the pass method does not completely replace the previous definition. \nInstead, it checks for a key, and then forwards the rest of the call to its base class using the parent \nkeyword. LockedDoor cannot call Door.pass directly, as it might in C++. The actual base class that it \nderives from may change when the transformations are linearized, so the parent keyword provides a generic \nhandle for forwarding the call to the next layer. Classes which are written in this manner are known \nas mixin classes, because they are not necessarily intended to be used as stand-alone objects.9 Instead, \neach class is designed to add a particular feature to its parent. Multiple mixins are then composed together \nto build a concrete implementation. The most interesting thing about mixins from the standpoint of domain-driven \ndevelopment is that once a set of classes have been linearized, they form a stack of software layers. \nEach new layer modifies the layers beneath it via forward refinement. The process of mixin composition \nthus resembles compositional code generators such as Genvoca. In particular, mixin classes are equivalent \nto Genvoca's symmetric components, which Batory has identified as one of the key organizational principles \nwhich allow the development of reconfigurable component libraries.2 Genvoca uses parametric polymorphism \nrather than subtype polymorphism to compose layers, but the end result is essentially the same. Layers \nin Genvoca have types (called realms) associated with them, which determine the interface of the layer. \nEach layer is treated as a function which maps from the types of its parameters onto the type of the \nlayer. If a layer maps from one type onto the same type, it is symmetric, and such layers can be composed \nin an arbitrary number of ways. Since each layer generally adds a single independent feature, arbitrary \nlayer composition is one way to implement feature-based programming. Mixin classes are automatically \nsymmetric. The base class of a mixin constitutes a hidden parameter because it may change during linearization. \nA mixin class is also a subtype of its base class, so a set of mixins which derive from a common base \nclass can likewise be composed in any number of ways. Mixin layers formalize this notion, and extend \nthe concept of a mixin class to cover large-scale systems of classes, much like Genvoca.19 Both Genvoca \nand mixin layers are compile-time systems. However, the process of method chaining used by mixin classes \nalso resembles composition filters, which are applied at run-time.3 Each mixin can be viewed as a filter \nwhich traps incoming messages and then dispatches them to the layer beneath it. Osterman has developed \na related mechanism which he dubs delegation layers.17 Delegation layers use parameterized inheritance, \nwhich combines the semantics of subtype polymorphism with explicit linearization. What is most interesting \nabout Osterman's approach is that although he uses compile-time classes to define types, the actual layers \nare composed at run-time using instances and delegation. Moreover, delegation layers provide complete \nsupport for virtual types at run-time, thus going well beyond the method-chaining capabilities of composition \nfilters. Like Ohmu, Osterman's design thus mixes the roles of classes and instances, and run-time versus \ncompile-time code. Genvoca components, mixin layers, composition filters, and delegation layers all provide \nsimilar capabilities, even though they use radically different implementations. Like my earlier comparison \nof functions, classes, and templates, there are only two main differences between these systems: they \nbind different kinds of parameters, and they perform the bindings at different times. All four systems \nperform feature composition by stacking symmetric components into layers, where each layer refines the \nlayers below it. Standard OO inheritance provides an excellent mechanism for refining objects, so long \nas it is extended to handle virtual types as well as virtual methods. Linearization, in turn, automatically \nLockedDoor ShortDoor  Each transformation creates a separate layer, and the layers are linked together \nwith parent and child pointers. The delegation pointer is then set to the topmost layer. Calls will be \ndelegated from Derived to layer3, where they will propagate backwards until they find a layer which can \nhandle them. All three layers, including the original clone of Base, are contained within the Derivedstructure. \nThis picture of transformation is a little bit more complicated than the one I painted before. Structure \ntransformations do not immediately replace the parameters they bind. The original prototype is merely \ncloned, and a transformation layer is applied which will trap incoming messages. The original parameters \ncannot be replaced immediately, because they may be accessed indirectly via calls to parent. Once the \nentire structure has been constructed, an optimizer goes through and analyzes which objects are actually \nvisible, and which have been completely obscured by the layers on top of them. Those which are not visible \nare culled, and the layers are compressed into a single structure. In this example, the initial objects \nin Base have all been obscured, so they will replaced with the new values for a, b, and c. The advantage \nof this system is that it offers a fast and flexible way to load and unload layers, while still allowing \nthe compiler to generate efficient transformations. The current Ohmu specification does not yet allow \na transformation to be undone once it has been applied, but such an operation is not impossible in principle. \nThe fact that transformation and aggregation have been split into separate operations is one of the keys \nto making this system work well. I must point out that a na\u00efve implementation of this mechanism will \ngenerate unacceptable run-time overhead for function calls and class instantiations, which are also handled \nas transformations. Both of these operations typically bind simple data values with a single layer, however, \nand they can thus can be analyzed and optimized at compile time. Complex layer generation and optimization, \nsuch as transformations that involve virtual types, will typically be compile-time operations only, and \nthe compiler will generate performance warnings in situations where this is not the case. The partial \nevaluation engine is capable of recognizing such situations because they will fail to evaluate at compile \ntime. 4.3 Covariance versus Contravariance The use of transformation layers resolves a long-standing \nproblem with virtual methods and static typing. In order for the new definition of a virtual method to \nbe a type-safe replacement for the old one, the argument types of the new method must be either the same, \nor less specialized than the original. This type of replacement is known as contravariant method replacement.6 \nUnfortunately, contravariant replacement seldom makes sense in the real-world. Several object-oriented \nlanguages, including Eiffel, advocate the use of covariant method overriding, in which function arguments \nbecome more specialized, because it seems to be a more natural model for class specialization. This argument \nis best illustrated with an example: Animal: Struct { mate: Function((a: Animal), nil) { ... }; }; \n Bird: transform Animal { mate: Function((b: Bird), nil) { ... }; }; The covariant rule, shown above, \nsays that animals mate with other animals, and birds in particular mate with birds. The contravariant \nrule would require birds to maintain the ability to mate with all other animals, although it would allow \nthem to add additional mating partners, such as rocks, cars, or shrubbery. Castagna argues that this \nconflict arises because method replacement is not appropriate for specialization. Instead, class specialization \nshould use covariant method overloading, which is similar to multiple-dispatch in functional languages.6 \nFunction overloading is easily implemented with transformation layers, since the top layers do not necessarily \nobscure the layers below. Overloading also provides an elegant solution to certain outstanding issues \nof object-oriented design, such as the binary method problem. Binary methods are those, which like the \nmate method above or the standard arithmetic operations, accept a single argument which is the same type \nas the class which contains the method. (They are called binary because the receiver of the message \nthethis pointer --constitutes an implicit second argument.) Binary methods are covariant by definition. \nMoreover, it is often the case that a single method in a base class may need to be overridden with several \noverloaded methods in a derived class. Here is how the arithmetic operations could be defined in Ohmu: \nNumber: Struct { + : Function((n: Number), Number) { return Number; // don't know value }; }; Integer: \nStruct { extends Number { + : Function((n: Integer), Integer) { ... }; + : Function((n: Float), Float) \n{ ... }; }; }; The basic operations on abstract numbers simply return Number, indicating a don't know \nvalue. The Integer class overloads this definition with not one, but two concrete methods for integer \nand floating point arithmetic. If some other kind of number is added to an Integer, then the call will \nfall back to the original definition. In this case, there is not really any good way to handle unsupported \nnumeric types the best the system can do is return a don't know value if it is asked to handle anything \nother than an integer or a float. This is not a true multiple-dispatch solution because the methods are \nstill contained within a class, and the class of the receiver of a message is still examined before the \ntypes of any arguments are considered. It does, however, alleviate some of the limitations of single-dispatch. \nNote that this solution also preserves type safety because the original method is not entirely replaced \nduring the structure transformation. All messages handled by Number are still handled by Integer, and \nwill not create run-time errors.  5. PARTIAL EVALUATION Although I have claimed that Ohmu eliminates \nthe distinction between compile-time and run-time code, that claim is not entirely true. The compiler \nat least must be able to distinguish between the two, even if the programmer does not. Ohmu uses a partial \nevaluation engine to shift operations from run-time to compile-time.12 Partial evaluation is usually \nregarded as an optimization technique, but it is actually an elegant mechanism for performing general \npurpose code generation. Partial evaluation works by locating invariant pieces of data, and then invoking \nthe interpreter to perform computations with that data at compile-time. Many languages provide some support \nfor partial evaluation. Most C++ compilers, for instance, will pre\u00adcompute constant expressions involving \nintegers, a feature which is required for many template meta-programming techniques. Before delving into \nthe details of invariant data, however, I should first describe how Ohmu deals with variables. Ohmu objects \nare not variable by default. Since parameters may be bound to either constants or variables, a parameter \ndeclaration such as x: Integer; declares only the interface of x; it does not specify whether or not \nx can be changed at run-time. A variable is actually a composite object, which is declared as range => \ninitial_value;The range is an abstract type which defines the set of all possible values for the variable. \nHere's an example: x: Integer => 0; p: Point => Point.new(1,1); Invariant data in Ohmu is a bit more \ncomplex, and it can be declared in several ways. Literal constants such as 5 and hello are automatically \ninvariant. Ohmu also provides four additional modifiers: static: Declares that an object will not change \nover the course of its lifetime, and cannot access other variables.mutable: Declares that an object may \nmake changes to other variables. abstract: Declares a structure to be an abstract prototype which has \nno run-time state. final: Declares that a parameter cannot be modified during a structure transformation. \nThe static modifier is used to declare constant data. It is also used to declare static functions functions \nwhich do not rely on external state. A static object can only access other static or abstractobjects; \nit can neither read from variables nor write to them. The mutable keyword is the opposite of static; \nit gives an object permission to modify variables. Objects which are neithermutable nor static may read \nvariables but cannot write to them. The abstract keyword is similar to static, but it is not quite so \nharsh. An abstract object can access variables, but it can only read the range it cannot read or modify \nthe value. Any attempt to modify the value is simply ignored. The key difference between staticand abstractobjects \nis that when a static object is transformed, its children will also be static. The abstract property, \non the other hand, affects only the parent object, it is not passed on to any children. This allows abstract \nprototypes to be labeled as invariant objects so that the compiler can perform compile-time inheritance \nand type checking, without otherwise affecting instances of those prototypes. Abstract parameters can \nalso be overridden with variables during a structure transformation, whereas static parameters can only \nbe overridden with staticreplacements. So in the end, the Ohmu compiler does have a way to distinguish \nclasses from instances. Classes are abstract, while instances are not. Nevertheless, the abstract property \ndoes not affect the interface of a class. It is merely a hint that the compiler should regard the class \nprototype as invariant. The final keyword is used for function inlining. All Ohmu parameters are polymorphic \nby default, so all run-time access to an object must go through a virtual interface. The final keyword \ndisables polymorphism for a particular parameter, and thus enables fast inline access. These four modifiers \ntell the partial evaluation engine not only which objects change, but how they change. Any expression \nwhich involves static or abstract objects will be evaluated at compile time. For example, the first two \nexpressions below are compile-time declarations, while the latter three are run-time expressions: dist: \nsqrt(100); // static IntArray: Array.of(Integer); // abstract x,y: Integer => 0; x := 5; // write to \nvariable xd: sqrt(x*x + y*y); // read from variable Note that Ohmu declarations are merely expressions \nwhich can be evaluated at compile time. The Ohmu compiler includes a full\u00adblown interpreter, so declarations \nare not limited to if statements or simple integer arithmetic; they can construct objects of arbitrary \ncomplexity. This simple system gives the Ohmu language immense expressive power. Instead of relying on \na fixed set of language constructs, new concepts or even complete domain-specific extensions can be added \nto the language merely by declaring them as staticor abstractoperations. As I mentioned earlier, Ohmu \ndoes include a standard reflective meta-object protocol as a system of last resort for performing code \ngeneration. I will not describe the details here, but it includes the standard complement of functions \nto query structures and traverse expression trees. It is thus perfectly possible for a class to query \nitself or other classes, and generate part of its interface at compile time depending on what it finds. \nOne distinct limitation of this approach is that compile-time code cannot actually modify anything, because \nsuch code can only read invariant data. It is possible to create new objects, but only by declaring them \nwith an invariant expression. Furthermore, when one expression attempts to read from another, it will \nforce an immediate reduction of the other expression. If that expression in turn reads from the original, \nit will create an infinite loop. In other words, two classes cannot query each other to determine their \ninterfaces unless special care is taken. The partial evaluator will detect any loops as an attempt to \nreduce the same expression twice and abort with an error. To put it another way, partially evaluated \ncode does not run in any particular order, so any set of declarations that requires a specific order \nwill fail. In practice such loops are easy to spot; the following is obviously an error: a: b+1; b: \na+1; The fact that partial evaluation is independent of the order of execution means that there is no \nneed to resolve transformation ordering conflicts. When there is a pressing need for an ordered set of \ntransformations, the mixin layers approach described earlier can be used to specify a correct order. \nAnother limitation is that unlike some other partial evaluation systems, Ohmu does not attempt to automatically \ndetermine what is and is not invariant data. Determining whether an object is static or abstract would \nrequire searching through the entire code base to determine whether or not it is ever modified. Moreover, \nit is important in an industrial setting to be able to guarantee that a particular object will be regarded \nas invariant. Unfortunately, this shifts the burden of locating invariance from the compiler to the programmer. \nThe good news is that with the exception of mutable, it is never an error to fail to specify the appropriate \nmodifier. The code will simply be less efficient, since computations will be deferred until run-time. \nIt is also relatively straightforward to declare all classes as abstract, which will ensure that inheritance, \nat least, will remain a compile-time operation. The worst-case scenario is that only inheritance is partially \nevaluated, in which case the performance of Ohmu should still be on a par with other compiled OO languages. \n5.1 Internal Dependencies The most exciting possibilities of this system arise when partial evaluation \nis combined with structure transformations. Here is another version of the multidimensional array example \nfrom earlier, which has been extended to take advantage of partial evaluation: Array: abstract Struct \n{ // classes are abstract DataType: Object; rank: Integer; dimensions: DimensionType; DimensionType: \nif (isStatic(rank)) then List.of(Integer).ofLength(rank) else List.of(Integer); strides: if (isStatic(dimensions)) \n then calculateStrides(dimensions) else DimensionType => // variable calculateStrides(dimensions); \n of: bind(DataType); ofRank: bind(rank); ofSize: bind(dimensions); calculateStrides: static // compile-time \n Function((DimensionType), DimensionType) { ... }; // omitted for brevity }; n: Integer => 2; Array2D: \nArray.ofRank(2); // fixed dimtype ArrayND: Array.ofRank(n); // var dimtype // auto-generates strides \nmyArray: Array2D.ofSize({256 256}); This example is not necessarily the best way to go about doing things, \nbut it illustrates the flexibility that the partial evaluation system can provide. When the compiler \nfirst encounters this code, it will evaluate pretty much everything, including the if statements, because \nall of the relevant parameters are initially declared as abstract prototypes. The if statements will \nreduce to either the then branch or the else branch, thus selecting an appropriate implementation. When \nthe structure is transformed, however, it may invalidate some of those computations. When rank is specialized \nfrom a generic Integer to the number 2 in Array2D, the partial evaluation engine must back up and re-evaluate \nDimension-Type, which then changes to a fixed-length list. That, in turn, changes the signature of calculateStrides. \nLater, when the dimensions are specialized down to static values in myArray, the partial evaluator must \ngo back again. When the dimensions change to a constant, the stride calculations also change from a variable \nto a constant. Static dimensions will cause the strides to be calculated at compile time, just like a \nnative array, while variable dimensions will automatically defer the stride calculations until run-time, \nand store them in a run-time variable. This ability of the partial evaluator to back up and re-evaluate \ncode is the reason why I call structure transformations transformations instead of refinements. Even \nseemingly simple inheritance may trigger a great deal of code generation. Any class of even moderate \ncomplexity will have a number of internal dependencies. The dependencies in the Array class are quite \nevident, as they are clearly labeled with if statements. In general, however, any parameter declarations \nwhich involve constants or abstract prototypes will create compile-time dependencies. When the partial \nevaluator reduces an expression, it stores a pointer to every named parameter that it must access in \norder to evaluate the expression. That way, whenever a parameter is specialized during a transformation, \nthe partial evaluator knows exactly which declarations need to be re-compiled and which are unaffected \nby the change. This system is an improvement over the C++ template approach, which typically re-compiles \nall the code in a class every time a template is instantiated. Since all transformations must be done \nusing transform, extends, or bind, which are declared with constant arguments, the compiler also knows \nthe names (although not necessarily the values) of all transformed parameters at compile time. The extra \npointers can thus be discarded after compilation, and incur no run-time overhead. 5.2 Inlining Other \nuseful optimizations can be done with the final keyword. If you recall, final disables virtual access, \nand thus allows the parameter it modifies to be inlined. Good support for inline functions is critical \nto writing efficient code, and remains one of the major strengths of C++ over other OO languages. In \nC++, however, once a method has been declared to be virtual or inline, that decision can never be changed. \nThis means that programmers must decide at the outset whether to sacrifice flexibility for efficiency \nin their code. The Ohmu model for inlining delays that decision by allowing normal parameters to be specialized \nto final parameters. In other words, programmers can start out with an abstract base class, apply several \nstructure transformations to create a concrete derived class, and then optimize that class by declaring \nkey methods as finalparameters. The Integer and Float classes in Ohmu use this principle to allow fast \narithmetic operations while still maintaining polymorphism. Both Integer and Float derive from Number, \nwhich is an abstract base class with no finalmethods. Variables of type Number are fully polymorphic \n they can store integers, floats, complex numbers, and a few other data types. The Integer and Float \nclasses themselves, however, are fully finalized, so all of their methods are inline. Since integer and \nfloat variables have no virtual methods, the compiler can even strip away the virtual method table entirely, \nso they are stored as ordinary 32-bit numbers with no performance penalty at all. When an Integer or \nFloat is cast back to a Number, the compiler will tack on a virtual method table again. The following \nexample demonstrates how a similar approach can be used to perform loop fusion on vectors: ExpressionNode: \nabstract Struct { DataType: Object; VectorType: Vector.of(DataType); length: Integer; implicit fuseLoop: \n final Function((), VectorType) { temp: VectorType.new(length); for (i: Integer, 0..(temp.length-1)) \n{ temp[i] := operation(i); }; return temp; }; operation: Function((i: Integer), DataType) { }; // \nempty virtual function + : static Function((v: VectorType), ExpressionNode) { return AddNode.new(this, \nTermNode.new(v)); } }; TermNode: abstract Struct { vector: VectorType; new: bind(vector); extends ExpressionNode \n{ operation: // specialize as final final Function((i: Integer), DataType) { return vector[i]; }; }; \n}; AddNode: abstract Struct { a, b: ExpressionNode; new: bind(a, b); extends ExpressionNode { operation: \n// specialize as final final Function((i: Integer), DataType) { return a.operation(i) + b.operation(i); \n}; }; }; Vector: abstract Struct { + : static Function((v: Vector), ExpressionNode) { return AddNode.new(TermNode.new(this), \nTermNode.new(v)); }; }; // other methods omitted for brevity This approach to loop fusion is essentially \nthe same as the expression templates technique used by the Blitz++ library.22 The main difference is \nthat Ohmu does it using a clean syntax and standard OO inheritance, rather than resorting to a separate \nmeta\u00adlanguage like C++ templates. The ExpressionNode class has a final method named fuseLoop which actually \nallocates a new vector of the appropriate length and calls operation(i) on each element. Since fuseLoop \nis implicit, it acts as a type conversion operator. AddNode and TermNode specialize operation to a final \nmethod that adds numbers together and reads elements from the input vectors. Both Vector and ExpressionNodealso \nhave a static + operator which creates appropriate AddNodes and TermNodes. With this machinery in place, \nan expression such as A := B + C + D; will be evaluated at compile time to create a tree of AddNodes \nand TermNodes. The assignment operator will invoke a type conversion, thus calling fuseLoop. The entire \nloop will be inlined in place, and dead code elimination can strip all of the node structures completely \nout of the run-time executable. 5.3 Transformational Code Generation The loop fusion example does more \nthan illustrate inlining. Loop fusion is an example of a true transformational code generator: one that \ncrosses hierarchical boundaries. Standard OO inherit\u00adance, even with mixin layers, can only perform compositional \ncode generation, otherwise known as forward refinement.8 A structure transformation can replace any parameter \nwith a more specialized version, but it cannot merge parameters, split parameters, or otherwise alter \nthe modular structure of an object. The partial evaluation engine knows no such boundaries. The strategy \nused here for loop fusion is actually a general strategy for performing most kinds of transformational \ncode generation. Instead of trying to compute the result of an expression immediately, the expression \nreduces to an intermediate representation an object structure that can be analyzed and manipulated. \nThis intermediate representation exists only at compile time, and is further reduced or transformed to \nanother object structure that can be evaluated at run-time. Since the partial evaluation engine will \ncontinue to reduce expressions until there is nothing left to reduce, there may even be several layers \nof intermediate representations. The only restriction is that the transformation code must use a functional \nprogramming style, because it cannot modify run-time state. Most other transformation architectures, \nsuch as the intentional programming system, utilize a dedicated transformation engine. These engines \ncan be configured to perform partial evaluation as well as many other operations, but they are not necessarily \nintuitive or easy to use. Ohmu takes the opposite path, and uses partial evaluation to perform general \npurpose transformations. The advantage of this scheme is that partial evaluation is completely transparent. \nPartially evaluated code consists of the same sort of expressions and declarations that make up ordinary \ncode. Code generators use classes and inheritance as normal. Partially evaluated code is not subject \nto ordering conflicts. There is no need to work with a dedicated transformation system that requires \na different set of programming skills, and uses a different set of primitives and APIs. The disadvantage, \nof course, is partial evaluation does not have the kind of expressive power that dedicated transformation \nsystems enjoy. Nevertheless, I feel that the ability to write transformational generators using simple \nOO code is far more important than raw power when it comes to bringing domain\u00addriven development to the \nmasses.  6. ASPECTS AND FRAMEWORKS I have now covered two of the three symmetries provided by the Ohmu \nlanguage: the symmetry between run-time and compile\u00adtime, and the symmetry between types and objects. \nThe final symmetry is that of scale. Most languages use different aggregate structures at different scales. \nSimple behaviors are encapsulated within methods, more complex behaviors within classes, and even more \ncomplex behaviors within frameworks, libraries, and namespaces. Unfortunately, only classes are polymorphic, \nbecause only methods can be stored in virtual method tables. In contrast, Ohmu programs are completely \nscale independent. Methods, classes, and libraries are all defined with the same construct the structure. \nStructure transformations can be applied to structures of any size, and all objects within a structure \nare considered to be parameters that can be overridden. It is at this large scale where the Ohmu partial \nevaluation system really comes into its own. The partial evaluator will track the dependencies between \na set of interacting objects, and perform code generation as needed when they are transformed. For example, \nif the root node of a complex class hierarchy is modified, that change will cascade down through all \nof the classes in the hierarchy. Using virtual classes and dependency tracking, it becomes easy to specialize \nwhole class frameworks.15 Real-world problems can seldom be solved with just a few independent classes. \nInstead, most OO solutions involve many classes which must interact closely with one another. Unfortunately, \nwhile traditional OO languages allow classes to be specialized individually, it is generally not possible \nto specialize the entire framework without resorting to template meta-programming or similar tricks. \nSee [16] for a good discussion of this problem. My final example is an implementation of the Model/View \ndesign pattern. The @ syntax here declares a reference: ModelViewFW: Struct { Model: abstract Struct \n{ private viewList: List.of(@View) => { }; registerView: mutable Function((v: @View), nil) { viewList.append(v); \n}; mutable changed: Function((), nil) { for (v: @View, viewList) { v.update(); }; }; }; View: abstract \nStruct { mutable update: Function((m: @Model), nil) { }; // empty virtual function }; }; There are \ntwo classes in this framework: Model and View. Model keeps a list of Views, and calls update on all of \nthem whenever the model changes. The only thing that makes this design pattern difficult is that when \nwe specialize View, we want the type signature for registerView to be changed appropri\u00adately otherwise \nit could be accidentally used to store views that should not be associated with that particular model. \nLikewise, when we specialize Model, we want to change the type signature of View.update. Each view will \notherwise have to perform a type cast in order to get a model reference that it can query. These issues \nare not show-stoppers by any means, but this is a short example. Real world frameworks have many classes, \nwith thousands of relationships and dependencies between them. What is a minor inconvenience with two \ndependencies can become an utter impossibility in a larger system. This problem is magnified by the fact \nthat existing languages not only fail to update dependencies automatically, they often fail to offer \nany mechanism to update them by hand. It is not possible in C++ to specialize the types of member variables, \nthe signatures of methods, or to modify private members of a class. Refining this framework in Ohmu, \non the other hand, is quite straightforward. Here we add an image to Model, and specialize Viewso that \nit draws the image to screen: DrawableFW: transform ModelViewFW { Model: abstract Struct {  extends \nparent.Model; image: Image => Image.load( im.jpg ); }; View: transform parent.View { update: mutable \nFunction((m: @Model), nil) { graphicsSystem.drawImage(m.image); }; }; }; In order to preserve the dependencies \nbetween Model and View, we need only transform them together as a complete framework. If classes are \nspecialized individually, then the relationships between them cannot be preserved. If they are specialized \nas a unit, however, then the new Model and the new View will override the old Model and the old View. \nSince the names are the same, the partial evaluation system will automatically update the dependencies \nby revisiting all of the declarations which reference Modeland View. Notice also that Model and View \ninherit from parent.Model and parent.View. The parent keyword is not restricted to chaining method calls. \nIt can be used whenever a new definition needs to specialize or refine an old one instead of replacing \nit completely. Moreover, the use of parent means that this framework can be used as a mixin layer.19 \nMixin layers are exactly the same as mixin classes, except that they operate on the scale of class frameworks \nand components rather than individual classes. As others have noted, mixin layers provide an excellent \nmechanism for implementing aspects and cross-cutting concerns.17 An aspect cross-cuts a number of classes \nand thus cannot be localized within any one class or method. However, with a scale\u00adindependent model, \nit is a simple matter to encapsulate the aspect as a transformation on the entire framework of classes \nthat it affects. Multiple aspects can then be composed together by layering them into a stack of mixin \nlayers. Ohmu structure transformations are capable of performing most of the common operations provided \nby dedicated aspect languages such as AspectJ. They can wrap methods with before and after code, add \nnew methods to classes, change the types of member variables and method signatures, and even modify the \ninheritance hierarchy. AspectJ is more flexible mainly in the variety of mechanisms that it provides \nto specify join points. In particular, AspectJ allows the use of wild cards and pattern matching, and \nallows methods to be targeted only if they are called from within a certain scope. Both of these can \nbe accomplished by means of the Ohmu reflection protocol, but the declarations are not nearly so elegant. \nThe advantage of the Ohmu approach is that aspects can be encapsulated using nothing more than block \nstructure and standard OO inheritance.   7.CURRENT DEVELOPMENT Ohmu was initially designed at the University \nof Texas, and has been further developed and implemented at MZA Associates Corporation over the course \nof several years. Future work on the language will be conducted at the University of Edinburgh, but MZA \nis still the primary source of funding for the project, and currently holds commercial rights to the \ncode. MZA can be contacted at: MZA Associates Corporation 2021 Girard SE Suite 150 Albuquerque, NM 87106-3140 \n1-505-245-9970 A prototype of the Ohmu interpreter is currently up and running, but we do not yet have \na version which is ready for release. The interpreter implements structure transformations and the partial \nevaluation system, so most of the concepts presented in this paper have been tested. We do not yet have \na compiler back-end, however, so specific performance benchmarks with C++ and Java are not available. \nOne of the most exciting possibilities for future work on the Ohmu language is the fact that the object \nmodel is flexible enough to emulate other language constructs. Java methods, classes, and interfaces \ncan all be translated into Ohmu structures in a fairly straightforward manner, using little more than \na parser which understands the appropriate syntax. C++ classes and templates can likewise be easily translated. \nIn fact, we have a prototype parser which handles a subset of C++. Unfortunately, every language has \na number of features beyond the core language constructs which require special handling. Ohmu provides \nenough meta-programming support that these features, by and large, can be handled with a domain-specific \nemulation library. The more complex the language, however, the larger this emulation library needs to \nbe. C++ in particular presents a number of challenges, such as pointer arithmetic, unions, object memory \nlayout, template pattern matching, etc. Java is considerably cleaner in this regard. Nevertheless, a \npartial emulation capability can still be useful. So long as a parser can create an accurate description \nof the interfaces to a foreign library, meta-programs can generate wrapper code that at least allows \nOhmu programs to link against the library. We have done this for simple C code, and plan to extend it \nto Java and C++ in the near future. Since Ohmu is a new language, the ability to link against existing \nsoftware libraries is crucial to its success. We also plan to develop a full emulation layer for C++ \nand Java in the future. With a full emulation layer in place, Ohmu could do far more than simply link \nagainst existing libraries. Once the source code for another language has been translated into Ohmu structures, \nthat code can be partially evaluated, refined, or transformed just like any other Ohmu program. It would \nbecome entirely possible to apply layered refinements or meta-programs to an existing code base, thus \nproviding a clear migration path for developers to experiment with domain-driven technologies or incorporate \nthem into existing projects. While the syntax of Java and C++ is not quite as well suited to meta-programming, \nminor extensions such as virtual classes and typedefs are easy to add. 8.CONCLUSION The Ohmu language \ndemonstrates that domain-driven technolo\u00adgies do not necessarily require new constructs and meta-languages \nthat are separate from existing object-oriented constructs. The basic OO concepts of objects and inheritance \ncan be quite powerful, so long as they are implemented in a flexible manner. The Ohmu language achieves \nthis flexibility by means of three symmetries. First, it eliminates the distinction between compile\u00adtime \nand run-time code by using partial evaluation. The programmer is still required to specify which things \ncan be evaluated at compile time, but there is no difference in the interface of run-time versus compile-time \nobjects. This technique allows meta-level code to be shifted to compile-time without requiring a separate \nmeta-language. Second, Ohmu eliminates the distinction between classes and instances. This mechanism \nextends standard OO inheritance by allowing types and classes to be bound to new definitions in the same \nfashion as ordinary methods and data members. This feature directly enables the third symmetry, which \nis the ability to apply OO operations such as inheritance to large-scale frameworks of classes and objects. \nIf classes can be overridden in the same way as methods, then refining a complete class hierarchy is \nno different from refining a single class. These three symmetries allow a number of domain-driven technologies \nto be implemented in a straightforward manner. Compositional code generators which use forward refinement \ncan be implemented with standard OO inheritance. Multiple features can be composed together by using \nmultiple inheritance and mixins to construct software layers. It is also possible to build transformational \ncode generators by using partial evaluation to reduce domain-specific expressions to object structures \nthat can be analyzed and manipulated at compile-time. Notice also that these three symmetries interlock \nwith one another; each one is far more useful in the context of the others than it is on its own. Partial \nevaluation, for instance, can stand alone as an optimization technique. What makes it really powerful, \nhowever, is the fact that it can be combined with inheritance and used as a mechanism to track and update \ndependencies. Virtual types and classes, in turn, rely on partial evaluation for their implementation. \nChanging a type definition will generally change the interface of a class, and in a statically typed \nlanguage that requires code generation. Most of the ideas in Ohmu have been taken from other sources. \nVirtual classes, mixins, covariant specialization, linearization, partial evaluation, and so forth have \nall appeared in other languages. What is unique about Ohmu is the way it weaves them together into a \nconsistent whole. The one truly innovative feature of the Ohmu design is its abstract prototype model. \nWhile it might be possible to design the language with a more traditional class/instance dichotomy, such \na design would fragment the symmetry of the language. The fact that a single operation can bind data \nvalues, override methods, or refine classes means that functions, classes, templates, and aspects can \nbe treated in a completely uniform manner. Such uniformity is more than just conceptual elegance. It \nallows both programmers and automatic code generators to refine and manipulate programs in a more consistent \nfashion. Most importantly, the Ohmu language accomplishes what other domain-driven and generative programming \ntechnologies have not: it provides a system in which domain-specific solutions can be encoded transparently \nwithin the context of standard object\u00adoriented programming. 9.REFERENCES [1] D. Batory, B. Lofaso, and \nY. Smaragdakis, JTS: Tools for Implementing Domain-Specific Languages. 5th International Conference on \nSoftware Reuse, Victoria, Canada, June 1998. [2] Don Batory and Sean O'Malley. The Design and Implementation \nof Hierarchical Software Systems with Reusable Components. ACM Transactions on Software Engineering and \nMethodology, 1(4):355-398, October 1992. [3] L.M.J. Bergmans. Composing Concurrent Objects Applying \nComposition Filters for the development and Reuse of Concurrent Object-Oriented Programs. Ph.D. Thesis, \nDepartment of Computer Science, University of Twente, The Netherlands, 1994. [4] G. Bracha and W. Cook. \nMixin-Based Inheritance. Joint ACM Conference on OOPSLA and ECOOP, 1990. [5] Gilad Bracha, Martin Odersky, \nDavid Stoutamire, and Philip Wadler. Making the Future Safe for the Past, Adding Genericity to the Java \nProgramming Language. Proceedings of OOPSLA '98. [6] Guiseppe Castagna. Covariance and Contravariance: \nConflict Without a Cause. ACM Transactions on Programming Languages and Systems, 1995. [7] Shigeru Chiba. \nA Meta-Object Protocol for C++. Proceedings of OOPSLA '95. [8] K. Czarnecki and U. Eisenecker. Generative \nProgramming: Methods, Techniques, and Applications. Addison-Wesley, 2000. [9] M. Flatt, S. Krishnamurthi, \nand M. Felleisen. Classes and Mixins. ACM Symposium on Principles of Programming Languages, pages 171-183, \n1998. [10]Erich Gamma, Richard Helm, Ralph Johnson, John Vlissides, Grady Booch. Design Patterns: Elements \nof Reusable Object-Oriented Software. Addison Wesley, Mass. 1995. [11]Atsushi Igarashi and Benjamin Pierce. \nFoundations for virtual types. Technical report, University of Pennsylvania, 1998. [12]Neil Jones, Carsten \nGomard, and Peter Sestoft. Partial Evaluation and Automatic Program Generation. Prentice Hall, 1993. \n[13]G. Kiczales, E. Hilsdale, J. Hugunin, M. Kersten, J. Palm, and W. G. Griswold. An overview of AspectJ. \nProceedings of ECOOP, 2001. [14]G. Kiczales, J. des Rivieres, and D. G. Bobrow. The Art of the Metaobject \nProtocol. The MIT Press, Cambridge, MA, 1991. [15]O.L. Madsen and B. M\u00f8ller-Pedersen. Virtual classes: \nA powerful mechanism in object-oriented programming. Proceedings of OOPSLA '89. [16]Gail Murphy and David \nNotkin. The Interaction Between Static Typing and Frameworks. Technical Report TR-93-09\u00ad02, University \nof Washington, 1993. See also: The Use of Static Typing to Support Operations on Frameworks. Object-Oriented \nSystems 3, 1996, pp. 197-213. [17]Klaus Ostermann. Dynamically composable collaborations with delegation \nlayers. Proceedings of ECOOP 2002. [18]C. Simonyi, .The Death of Computer Languages, the Birth of Intentional \nProgramming. NATO Science Committee Conference, 1995. [19]Yannis Smaragdakis and Don Batory. Implementing \nLayered Designs with Mixin Layers. Proceedings of ECOOP, 1998. [20]K. K. Thorup and M. Torgersen. Unifying \nGenericity Combining the Benefits of Virtual Types and Parameterized Classes. Proceedings of ECOOP, \n1999. [21]David Ungar and Randall B. Smith. Self, The Power of Simplicity. Proceedings of OOPSLA 1987 \n[22]T.L Veldhuizen. Arrays in Blitz++. Proceedings of the 2nd International Scientific Computing in Object \nOriented Parallel Environments (ISCOPE 98). See also http://www. oonumerics.org/blitz.  \n\t\t\t", "proc_id": "949344", "abstract": "I present the Ohmu language, a unified object model which allows a number of \"advanced\" techniques such as aspects, mixin layers, parametric polymorphism, and generative components to be implemented cleanly using two basic concepts: block structure and inheritance. I argue that conventional ways of defining classes and objects have created artificial distinctions which limit their expressiveness. The Ohmu model unifies functions, classes, instances, templates, and even aspects into a single construct - the structure. Function calls, instantiation, aspect-weaving, and inheritance are likewise unified into a single operation - the structure transformation. This simplification eliminates the distinction between classes and instances, and between compile-time and run-time code. Instead of being compiled, programs are reduced using partial evaluation, during which the interpreter is invoked at compile-time. Within this architecture, standard OO inheritance becomes a natural vehicle for creating meta-programs and automatic code generators - the key to a number of recent domain-driven programming methodologies.", "authors": [{"name": "DeLesley Hutchins", "author_profile_id": "81100209382", "affiliation": "University of Edinburgh, Edinburgh, UK", "person_id": "P643437", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/949344.949350", "year": "2003", "article_id": "949350", "conference": "OOPSLA", "title": "The power of symmetry: unifying inheritance and generative programming", "url": "http://dl.acm.org/citation.cfm?id=949350"}