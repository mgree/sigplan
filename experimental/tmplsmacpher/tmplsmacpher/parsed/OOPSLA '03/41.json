{"article_publication_date": "10-26-2003", "fulltext": "\n Rethinking Computer Science Education from a Test-first Perspective Stephen H. Edwards Virginia Tech, \nDept. of Computer Science 660 McBryde Hall, Mail Stop 0106 Blacksburg, VA 24061 USA +1 540 231 5723 \nedwards@cs.vt.edu ABSTRACT Despite our best efforts and intentions as educators, student programmers \ncontinue to struggle in acquiring comprehension and analysis skills. Students believe that once a program \nruns on sample data, it is correct; most programming errors are reported by the compiler; when a program \nmisbehaves, shuffling statements and tweaking expressions to see what happens is the best debugging approach. \nThis paper presents a new vision for computer science education centered around the use of test-driven \ndevelopment in all programming assignments, from the beginning of CS1. A key element to the strategy \nis comprehensive, automated evaluation of student work, in terms of correctness, the thoroughness and \nvalidity of the student s tests, and an automatic coding style assessment performed using industrial-strength \ntools. By systematically applying the strategy across the curriculum as part of a student s regular programming \nactivities, and by providing rapid, concrete, useful feedback that students find valuable, it is possible \nto induce a cultural shift in how students behave.  Categories and Subject Descriptors K.3.2 [Computers \nand Education]: Computer and Information Science Education; D.1.5 [Programming Techniques]: Object\u00adoriented \nProgramming; D.2.5 [Software Engineering]: Testing and Debugging testing tools. General Terms Verification. \n Keywords Pedagogy, test-driven development, laboratory-based teaching, CS1, extreme programming. 1. \nINTRODUCTION Many educational institutions are undergoing significant curriculum changes as they embrace \nobject orientation, often opting for an aggressive objects-first strategy for its pedagogical value [25, \n32, 26, 6]. Yet, while such changes offer the promise of eliminating the paradigm shift that would face \nstudents who receive initial training in procedural programming, other age old difficulties remain [28, \n15]. Particularly during freshman and sophomore courses, and occasionally much later, a student may believe \nthat once the code she has written compiles successfully, the errors are gone. If the program runs correctly \non the first few runs she tries, it must be correct. If there is a problem, maybe by switching a few \nlines around or tweaking the code by trial and error, it can be fixed. Once it runs on the instructor-provided \nsample data, her program is correct and the assignment is complete. Even worse, students are often able \nto succeed at simpler CS1 and CS2 assignments without developing a broader view, which only reinforces \napproaches that will handicap their performance in more advanced courses. The reason for this, as described \nby Buck and Stucki [9, 10], is that most undergraduate curricula focus on developing program application \nand synthesis skills (i.e., writing code), primarily acquired through hands-on activities. In addition, \nstudents must master basic comprehension and analysis skills [8]. Students must be able to read and comprehend \nsource code, envision how a sequence of statements will behave, and predict how a change to the code \nwill result in a change in behavior. Students need explicit, continually reinforced practice in hypothesizing \nabout the behavior of their programs and then experimentally verifying (or invalidating) their hypotheses. \nFurther, students need frequent, useful, and immediate feedback about their performance, both in forming \nhypotheses and in experimentally testing them. To this end, I propose a new vision for laboratory and \nprogramming assignments across the entire CS curriculum inspired by test-first development [4, 3]. From \nthe very first programming activities in CS1, a student should be given the responsibility of demonstrating \nthe correctness of his or her own code. Such a student is expected and required to submit test cases \nfor this purpose along with the code, and assessing student performance includes a meaningful assessment \nof how correctly and thoroughly the tests conform to the problem. The key to providing rapid, concrete, \nand immediate feedback is an automated assessment tool to which students can submit their code. Such \na tool should do more than just give some sort of correctness score for the student s code. In addition, \nit should: Assess the validity of the student s tests, giving feedback about which tests are incorrect. \nCopyright is held by the author/owner(s). Assess the completeness of the student s tests, giving an OOPSLA \n03, October 26 30, 2003, Anaheim, California, USA. indication of how to improve. ACM 1-58113-751-6/03/0010. \n Assess the style of the student s code, giving feedback about where improvements can be made.  Assess \nthe quality of the student s code, giving suggestions for improvement or drawing attention to potential \nproblems.  This paper describes a vision of a test-first-inspired educational strategy: systematically \nsupporting test-first programming from the beginning to ensure students acquire the necessary comprehension \nand analysis skills needed to support effective programming. It also describes a practical, feasible \napproach to providing automated feedback that students can really use. This approach will work even for \nvery early programming assignments in CS1 classes, and naturally meshes with existing tools for teaching \nin an objects-first style. By systematically adopting such an assessment approach across the curriculum, \nit will be possible to induce a cultural shift in how students behave when completing programming assignments \nand what they expect to get out of the process. Section 2 lays out the details of test-first assignments \nand their assessment, while Section 3 uses this foundation to describe a new vision for CS education. \nRelated work is described in Section 4, with conclusions appearing in Section 5.  2. TEST-FIRST ASSIGNMENTS \nOthers have suggested that more software engineering concepts in general [29, 30] and software testing \nskills in particular [38, 20, 21, 22, 16] should be integrated across the undergraduate CS curriculum. \nProviding upper-division elective courses on such topics is helpful, but has little influence on the \nbehaviors students practice throughout their academic endeavors. Instead, a student can easily view the \nsoftware engineering practices in most student\u00adoriented texts as something that professional programmers \ndo out in the real world but that has little bearing on and provides little benefit for the day-to-day \ntasks required of a student. Practicing test-driven development (TDD) across the curriculum is an interesting \nalternative. In TDD, one always writes a test case (or more) before adding new code. New code is only \nwritten in response to existing test cases that fail. By constantly running all existing tests against \na unit after each change, and always phrasing operational definitions of desired behavior in terms of \nnew test cases, TDD promotes incremental development and gives a programmer a great degree of confidence \nin the correctness of their code. While TDD is a practical, concrete technique that students can practice \non their own assignments. The idea of using TDD in the classroom is not revolutionary [2]. Instead, the \nreal issue is how to overcome its potential pitfalls: the approach must be systematically applied across \nthe curriculum in a way that makes it an inherent part of the programming activities in which students \nparticipate, and students must receive frequent, directed feedback on their performance that provides \nthe student with clear benefits. The key to resolving these issues is a powerful strategy for assessing \nstudent performance. 2.1 Automated Grading Providing appropriate feedback and assessment of student performance \nis the critical factor in the success of this vision. Instructors and teaching assistants are already \noverburdened with work. Doubling their workload by requiring them to assess test data as well as program \ncode will never work. This issue is even more critical for a curriculum-wide transformation. The only \npractical answer is automation. Many educators have used automated systems to assess and provide rapid \nfeedback on large volumes of student programming assignments [19, 23, 31, 35, 18]. While these systems \nvary, they typically focus on compilation and execution of student programs against some form of instructor-provided \ntest data. Indeed, Virginia Tech uses its own automated grading system for student programs and has seen \npowerful results. In spite of its classroom utility, an automatic grading strategy like the one embodied \nin the Curator also has a number of shortcomings. Most importantly, students focus on output correctness \nfirst and foremost; all other considerations are a distant second at best (design, commenting, appropriate \nuse of abstraction, testing one's own code, etc.). This is due to the fact that the most immediate feedback \nstudents receive is on output correctness, and also that the Curator will assign a score of zero for \nsubmissions that do not compile, do not produce output, or do not terminate. In addition, students are \nnot encouraged or rewarded for performing testing on their own. In practice, students do less testing \non their own, often relying solely on instructor-provided sample data and the automated grading system. \nClearly, existing approaches to automatic grading of student programs will not work. 2.2 TDD-oriented \nAssessment Instead of automating an assessment approach that focuses on the output of a student s program, \ninstead we must focus on what is most valuable: the student s testing performance. The assessment approach \nshould require a student test suite as part of every submission, and encourage students to write thorough \ntests. It should also support TDD by encouraging the rapid cycling of write a little test, write a little \ncode. Virginia Tech has developed a prototype grading system to explore the possibilities in this direction, \nand has experimented with these techniques in the classroom with positive results. The prototype is a \nservice provided by Web-CAT, the Web-based Center for Automated Testing. Suppose a student is developing \na programming assignment in Java. The student can prepare test cases in JUnit format [24]. The source \nfiles for the program and tests can be submitted to the Web-CAT Grader. Upon receipt, the student s submission \nis compiled and then assessed along four dimensions: correctness, test completeness, test validity, and \ncode quality. Assessing correctness is entirely the student s responsibility, and the percentage of student-written \ntests passed by the student s code is used for this measure. Student code is also instrumented to gather \ncode coverage instrumentation, using a tool such as Clover [14]. The instructor can choose an appropriate \ncoverage metric for the difficulty level of the course, and code coverage can be used as a measure of \nhow thoroughly the student as tested the submitted code. Further, the instructor may wish to provide \na separate reference test set the percentage of tests in this reference set that are passed by the student \nsubmission can be used as an indicator of how thoroughly the student has tested all the behavior required \nin the problem. Test validity is assessed by running the student tests against an instructor-provided \nreference implementation. In cases where the class design for the student s submission is tightly constrained, \nthis may include unit-level test cases. As students move on to more comprehensive assignments, the test \ncases can be partitioned into those that test top-level program-wide behavior and those that test purely \ninternal concerns. Only top-level test cases that capture end\u00adto-end functionality are validated against \nthe instructor s reference implementation. Finally, industrial quality static analysis tools such as \nCheckstyle [11] and PMD [34] can assess how well the student has conformed to the local coding style \nconventions as well as spot potentially error-prone coding issues. Together, Checkstyle and PMD provide \nmany dozens of fully automated checks for everything from indentation, brace usage, and presence of JavaDoc \ncomments to flagging unused code, inappropriate object instantiations, and inadvisable coding idioms \nlike using assignment operators in sub\u00adexpressions. The instructor has full control over which checks \nare enabled, which checks result in scoring deductions, and more. To support the rapid cycling between \nwriting individual tests and adding small pieces of code, the Web-CAT Grader will allow unlimited submissions \nfrom students up until the assignment deadline. Students can get feedback any time, as often as they \nwish. However, their score is based in part on the tests they have written, and their program performance \nis only assessed by the tests they have written. As a result, to find out more about errors in their \nown programs, it will be necessary for the student to write the test cases. The feedback report will \ngraphically highlight the portions of the student code that are not tested so that the student can see \nhow to improve. Other coding or stylistic issues will also be graphically highlighted. 2.3 But Can It \nBe Used Across the Board? While the idea of automatically assessing TDD assignments is exciting, it also \nraises questions when one proposes to apply it curriculum-wide. The two biggest questions are: can beginning \nstudents use it from the start of their first class, and will it work on graphically-oriented programs? \nFirst, consider beginning students. Most automated grading systems, including the current system in use \nat Virginia Tech, were designed to help cope with the large volumes of students in introductory-level \nclasses. The previous Curator system has been in use in our CS1 course for many years and has not caused \nissues in that regard. So the real question is whether or not students can write test cases from the \nstart of CS1. Interestingly, DrJava [1], which is designed specifically as a pedagogical tool for teaching \nintroductory programming, provides built-in support to help students write JUnit-style test cases for \nthe classes they write. Similarly, BlueJ [25, 26, 27], another introductory Java environment designed \nspecifically for teaching CS1, also supports JUnit-style tests. BlueJ allows students to interactively \ninstantiate objects directly in the environment without requiring a separate main program to be written. \nMessages can be sent to such objects using pop-up menus. BlueJ s JUnit support allows students to record \nsimple object creation and interaction sequences as JUnit-style test cases. Such tools make it easy for \nstudents to write tests from the beginning. Further, the scoring formula used to grade introductory assignments \nby beginners will most likely be different than that used for more advanced students. To start, the instructor \nmay wish to only require method-level coverage of beginning students (i.e., each method is import cs1705.*; \n/** * MyRobot adds three basic capabilities to a * robot: the ability to turn right, turn com- * pletely \naround, and pick up a row of beepers. */  public class MyRobotextends VPIRobot { //--------------------------------------------/** \nConstruct a new MyRobot object. */ public MyRobot() { } public void turnRight() { turnLeft(); turnLeft(); \nturnLeft(); } //--------------------------------------------/** Reverse direction with a 180-degree \nturn */ public void turnAround() { turnLeft(); turnLeft(); } //--------------------------------------------/** \nMarch along a line of beepers, picking up * each in turn. */ public void collectBeepers() { while ( \nnextToABeeper() ); { pickBeeper();if ( frontIsClear() ) { move(); } } } } Figure 1. A simple student \nprogram. executed at least once). As students grasp the concept and develop experience applying the feedback \nthey receive, grading stringency can be gradually increased. But will this technique work for graphically-oriented \nprograms? As long as a batch-oriented test execution scheme can be devised, the solution is appropriate. \nBuck and Stucki describe a simple approach for achieving the same end with graphically-oriented student \nprograms [9]. By fixing the interface between the GUI and the underlying code, the GUI can be replaced \nby an alternate driver during testing. Instructors who use custom GUI libraries designed for educational \nuse can augment them with additional support for test automation if needed. We have successfully applied \nautomated grading techniques to a variety of courses from the freshman through the junior level with \nsuccess, including some courses that use graphically-oriented projects.  2.4 An Example To show how \nTDD assignments work, consider a case that pushes the boundaries: a freshman in CS1 is learning the basics \nof programming on a graphically oriented assignment. Many institutions use variations of Karel the Robot \nbecause of the consistent and intuitive metaphor it provides to introductory students. There are several \nJava versions of Karel the Robot [5, 7, 10], some of which allow student to program Karel by writing \npure Java. Karel is a simple mobile robot that navigates in a two-dimensional grid-based world. Karel \nsupports a simple set of messages to move forward, detect walls directly in front of him, turn left, \nand pick up or put down small beepers in his environment. Students can easily grasp the concept of Karel \nas well as the basic operations he provides, and their programs are easily animated in a graphical window \nto visualize the robot s actions. Figure 1 shows the source code for a hypothetical Karel assignment: \ncreate a robot that provides three new capabilities: turning right (the base robot only knows how to \nturn left!), reversing direction, and picking up a sequence of beepers. A student completing this assignment \nmay begin with a sample robot class in a text book or provided by the instructor. What kind of test case \nmight a CS1 student write for this assignment? Suppose the student is working on gathering beepers first. \nFigure 2 shows a simple JUnit-style test case that might be created as a student works on collectBeepers(). \nThe student might even create this sequence interactively and record it as a test case using their educational \nIDE. The student could then submit code and test case together for assessment. The student could continue \nto develop test cases for each new feature or change, using repeated submissions to get feedback on his \nor her progress. Figure 3 depicts the feedback report the student would receive from the Web-CAT Grader. \nThis report is for a submission where all of the student s tests pass. It shows a summary of the correctness \nand testing assessment, which in this example is taken from the Clover code coverage measure the number \nof methods executed in this case, since students for this assignment are not yet ready for more stringent \nrequirements. The bar graphs in the report were inspired in part by JUnit's GUI TestRunner: when the \nbar is green the code is clean. Figure 3 also shows a summary of the stylistic assessment, where points \nhave been deduced for stylistic or coding errors. There is also room for a design and readability score \nfrom the TA or instructor. In this example, the code has not yet been manually assessed. Further, a more \ndetailed breakdown lists each class in the submission separately, showing the number of comments or remarks \non the corresponding source file, the points lost attributable to that class, and a summary of how thoroughly \nthat particular class has been tested. By showing the basic testing coverage achieved for each component \nin this way, the top-level summary indicates to the student where more effort can be productively spent \nto improve their understanding of the code and to ensure it operates correctly. This list is initially \nsorted by the number of comments received, although the student can resort the list using other criteria \nif desired. import cs1705.*; public class MyRobotTests extends junit.framework.TestCase { MyRobot karel; \nWorld world; protected void setUp() { // Read in a world config containing // a line of beepers at karl \ns start loc World.startFromFile( \"beeperTest.kwld\" ); karel = new MyRobot(); world = karel.getWorldAsObject(); \n} //--------------------------------------------/** Check that after calling collectBeepers(), * there \nare no more beepers left. */ public void testCollectBeepers() { karel.collectBeepers(); karel.turnAround(); \nkarel.turnOff(); karel.assertBeepersInBeeperBag(); world.assertNoBeepersInWorld(); } } Figure 2. A simple \ntest case for MyRobot. The student can click on a class name to view the suggestions and comments on \nthat portion of his or her code. Figure 4 shows an example screen shot of marked up source code that \nthe student will see. The basic form of the report is produced by Clover, and each source file is viewble \nin pretty-printed form with color\u00adhighlighted markup and embedded comments or remarks. This top\u00adlevel \nsummary shows the basic testing coverage achieved for each component, indicating to the student where \nmore effort can be productively spent to improve their understanding of the code and ensure it operates \ncorrectly. From this summary, individual reports for each file in the submission can be obtained, as \nexemplified in Figure 4. Clover automatically highlights lines that have not been executed during testing \nin pink to graphically indicate where more testing needs to be performed. In addition, an execution count \nfor each line is listed next to the line number on the left. Hovering the mouse over such lines pops \nup more detailed information about the amount of full or partial coverage achieved on troublesome lines. \nIn addition, comments from static checking tools (e.g., Checkstyle and PMD) have been folded into this \nunified report. Lines highlighted in red indicate stylistic or coding issues resulting in point deductions. \nIn Figure 4, line 18 is so marked, and the corresponding message is shown immediately below the line, \nin this case indicating that the method is missing a descriptive comment. Alternate colors and icons \nare used to denote warnings, suggestions, good comments from the TA or instructor, and extra credit items. \n Figure 3. The score summary a student receives for a submission. In Figure 4, line 40 is also highlighted \nas an error, with two associated messages. The execution count next to the line number indicates that \nlots of processing time was spent here the accidental infinite loop was terminated by the execution time \nlimit imposed for this assignment. The messages draw attention to the misplaced semicolon, helping to \nsolve the issue in this case. The Web-CAT Grader also provides an interface for TAs to review assignments. \nUsing a direct manipulation interface, comments resulting from manual grading can be directly entered \nvia a web browser. TA comments entered this way will be visible to the student just as tool-generated \ncomments.  2.5 How Are Students Affected? TDD is attractive for use in education for many reasons. It \nis easier for students to understand and relate to than more traditional testing approaches. It promotes \nincremental development, promotes the concept of always having a running (if incomplete) version of the \nprogram on hand, and promotes early detection of errors introduced by coding changes. It directly combats \nthe big bang integration problems that many students see when they begin to write larger programs, when \ntesting is saved until all the code writing is complete. It increases a student s confidence in the portion \nof the code they have finished, and allows them to make changes and additions with greater confidence \nbecause of continuous regression testing. It increases the student s understanding of the assignment \nrequirements, by forcing them to explore the gray areas in order to completely test their own solution. \nIt also provides a lively sense of progress, because the student is always clearly aware of the growing \nsize of their test suite and how much of the required behavior is already in the bag and verified. Most \nimportantly, students begin to see these benefits for themselves after using TDD on just a few assignments. \nThe Web-CAT Grader prototype and TDD have been used in a junior-level class. Compared to prior offerings \nof the class using a more traditional automated grading approach, students using TDD are more likely \nto complete assignments, are less likely to turn assignments in late, and receive higher grades. Empirically, \nit also appears that student programs are more thoroughly tested (in terms of the branch coverage their \ntest suites achieve on a reference implementation) than when using the previous automated grading system. \n 3. A NEW VISION FOR CS EDUCATION Given the example in Section 2.4, it is clear that TDD-based assignments \nwith comprehensive, automated assessment are feasible, even for introductory students. In addition, this \nstrategy can be combined easily with many recent advances in CS pedagogy. Students can be taught using \nan objects-first style [5, 6, 12, 13, 37,  Figure 4. Style and coding suggestions for one student source \nfile. 32], and introduced to programming using metaphorical systems topics are introduced and the manner \nin which programming tasks like Karel the Robot [7, 13, 37]. Role-playing activities [7] can be are framed \nas students progress in their abilities [9, 10]. used to introduce OO concepts and act out testing tasks. \nClosed As students gain more skill from early courses, requirements for test laboratory sessions can \nbe used to provide more hands-on learning. thoroughness can be increased. Unlike prior automated grading \nPair programming can be used in closed labs to increase peer-to\u00adsystems that tend to inhibit student \ncreativity and enforce strict peer learning and also to foster comprehension and analysis skills conformance \nto an unwavering assignment specification, the TDD [33, 39]. Bloom s taxonomy can be used to plan the \norder in which approach more readily allows open-ended assignments such as those suggested by Roberts \n[36]. If a student wishes to do more work or implement more features, they can still write their own \ninternal tests. As long as they also implement the minimum requirements for the assignment as embodied \nin the instructor s reference test suite, their submission will be graded on the thoroughness of their \nown testing against their enhanced solution. After students have used TDD techniques across several classes, \nit will become the cultural norm for behavior, not just an extra requirement that one instructor imposes \nand that can be thrown away after his or her class has been passed. The goal is to foster this cultural \nshift for pedagogical ends. By continually requiring students to test in the small, every time they add \nor change a piece of code, they are also continually practicing and increasing their skills at hypothesizing \nwhat the behavior should be and then operationally testing those hypotheses. This will truly bring the \nlaboratory nature of computer science training to the fore if this vision is adopted across an institution \ns curriculum.  4. RELATED WORK The vision described here builds on a large body of prior work. Infusing \nsoftware engineering issues and concerns across the undergraduate curriculum has been discussed at SIGCSE \non several occasions [17, 29, 30]. TDD and other extreme programming ideas have even been used in the \nclassroom [2]. This idea is complementary to the test-first assignment strategy described here. The main \ndifference is that the TDD strategy focuses on operational techniques that provide clear benefits to \nstudents in a way that is natural part of the programming process and that can be applied across the \ncurriculum. The idea of including software testing activities across the curriculum has also been proposed \nby others [16, 20]. Jones has described some experiences in this direction [21, 22]. While Jones has \nused a traditional automated grading system for assessing student work [23], his system is similar to \nothers in that it focuses on assessing program correctness first and foremost. This paper proposes TDD \nrather than more traditional testing techniques and focuses specifically on the unique assessment issues \nnecessary for fostering a positive cultural change in student behavior. Automated grading has also been \ndiscussed in the educational literature [19, 35, 18]. Unfortunately, most such systems are of the home \nbrew variety and see little or no use outside their originating institution. Further, virtually all focus \non output correctness as the sole assessment criterion. Mengel describes experiments in using metrics-based \ntechniques to assess style [31]. Here, the intent is to use of industrially proven tools. By installing \nand configuring these tools on a server and combining them with a unified feedback format, students can \nreadily take advantage of the information they provide without being exposed to the hassles of installing \nand learning to use the tools. 5. CONCLUSION Despite the best efforts of computer science educators, \nCS students often do not acquire the desired analytical skills that they need to be successful until \nlater than we would like, if at all. Reassessing typical computer science education practices from a \ntest-first perspective leads one to focus on programming activities and how they are carried out. It \nis possible to infuse continual practice and development of comprehension and analysis skills across \nthe programming assignments in a typical CS curriculum using TDD activities. Providing a system for rapid \nassessment of student work, including both the code and the tests they write, and ensuring concrete, \nuseful, and timely feedback, is critical. In addition to assessing student performance, students can \nget real benefits from using the approach, and these benefits are important for students to internalize \nand use the approach being advocated. Using TDD across the board can serve as the core for a broader \nvision of re-engineering programming practices across the CS curriculum. The goal is to develop a culture \nwhere students are expected to test their own code (that is, apply analytical and code understanding \nskills on a daily basis), and where it is an accepted part of life across all of a student's courses. \nInstead of being the exception i.e., testing is something students do in one class focused on the topic \ntesting one's own code will become the norm. As students become inculcated with this expectation, it \nis possible to emphasize testing across the curriculum as a natural part of existing classes, without \nrequiring extra class time or lecture materials. The hope captured in this vision is that students will \nacquire better skills for a variety of programming tasks, that instructors and TAs will be able to devote \nmore attention to design assessment (because simple stylistic, correctness, and testing issues are automatically \nassessed), and thus more teaching time and effort can go into the deeper issues that all students must \nmaster once they conquer their programming fundamentals. 6. ACKNOWLEDGMENTS This work is supported in \npart by the Virginia Tech Institute for Distance and Distributed Learning and by the National Science \nFoundation under grant DUE-0127225. Any opinions, conclusions or recommendations expressed in this paper \nare those of the author and do not necessarily reflect the views of the NSF. I wish to acknowledge the \nfeedback provided by Manuel P\u00e9rez-Qui\u00f1ones on these ideas, and the students who have worked on the project: \nAnuj Shah, Amit Kulkarni, and Gaurav Bhandari. 7. REFERENCES [1] Allen, E., Cartwright, R., and Stoler, \nB. DrJava: a lightweight pedagogic environment for Java. In Proc. 33rd SIGCSE Technical Symp. Computer \nScience Education, ACM, 2002, pp. 137-141. [2] Allen, E., Cartwright, R., and Reis, C. Production programming \nin the classroom. In Proc. 34th SIGCSE Technical Symp. Computer Science Education, ACM, 2003, pp. 89-93. \n[3] Beck, K. Aim, fire (test-first coding). IEEE Software, 18(5): 87-89, Sept./Oct. 2001. [4] Beck, K. \nTest-Driven Development: By Example. Addison-Wesley, Boston, MA. 2003. [5] Becker, B.W. Teaching CS1 \nwith Karel the Robot in Java. In Proc. 32nd SIGCSE Technical Symp. Computer Science Education, ACM, 2001, \npp. 50-54. [6] Bergin, J., et al. Resources for next generation introductory CS courses: report of the \nITiCSE 99 working group on resources for the next generation CS 1 course. ACM SIGCSE Bulletin, 31(4): \n101-105. [7] Bergin, J., Stehlik, M., Roberts, J., Pattis, R. Karel J. Robot: A Gentle Introduction to \nthe Art of Object-Oriented Programming in Java. http://csis.pace.edu/~bergin/KarelJava2ed/ [8] Bloom, \nB.S., et al. Taxonomy of Educational Objectives: Handbook I: Cognitive Domain. Longmans, Green and Co., \n1956. [9] Buck, D., and Stucki, D.J. Design early considered harmful: graduated exposure to complexity \nand structure based on levels of cognitive development. In Proc. 31st SIGCSE Technical Symp. Computer \nScience Education, ACM, 2000, pp. 75-79. [10] Buck, D., and Stucki, D.J. JKarelRobot: a case study in \nsupporting levels of cognitive development in the computer science curriculum. In Proc. 32nd SIGCSE Technical \nSymp. Computer Science Education, ACM, 2001, pp. 16-20. [11] Checkstyle home page. http://checkstyle.sourceforge.net/. \n[12] Comer, J., and Roggio, R. Teaching a Java-based CS1 course in an academically-diverse environment. \nIn Proc. 33rd SIGCSE Technical Symp. Computer Science Education, ACM, 2002, pp. 142-146. [13] Cooper, \nS., Dann, W., and Pausch, R. Teaching objects-first in introductory computer science. In Proc. 34th SIGCSE \nTechnical Symp. Computer Science Education, ACM, 2003, pp. 191-195. [14] Clover: a code coverage tool \nfor Java. http://www.thecortex.net/clover/. [15] Decker, R. and Hirshfield, S. The top 10 reasons why \nobject\u00adoriented programming can t be taught in CS 1. In Proc. 25th Annual SIGCSE Symp. Computer Science \nEducation, ACM, 1994, pp. 51-55. [16] Goldwasser, M.H. A gimmick to integrate software testing throughout \nthe curriculum. . In Proc. 33rd SIGCSE Technical Symp. Computer Science Education, ACM, 2002, pp. 271-275. \n[17] Hilburn, T.B., and Towhidnejad, M. Software quality: A curriculum postscript? In Proc. 31st SIGCSE \nTechnical Symp. Computer Science Education, ACM, 2000, pp. 167-171. [18] Isong, J. Developing an automated \nprogram checker. J. Computing in Small Colleges, 16(3): 218-224. [19] Jackson, D., and Usher, M. Grading \nstudent programs using ASSYST. In Proc. 28th SIGCSE Technical Symp. Computer Science Education, ACM, \n1997, pp. 335-339. [20] Jones, E.L. Software testing in the computer science curriculum a holistic approach. \nIn Proc. Australasian Computing Education Conf., ACM, 2000, pp. 153-157. [21] Jones, E.L. Integrating \ntesting into the curriculum arsenic in small doses. In Proc. 32nd SIGCSE Technical Symp. Computer Science \nEducation, ACM, 2001, pp. 337-341. [22] Jones, E.L. An experiential approach to incorporating software \ntesting into the computer science curriculum. In Proc. 2001 Frontiers in Education Conf. (FiE 2001), \n2001, pp. F3D7-F3D11. [23] Jones, E.L. Grading student programs a software testing approach. J. Computing \nin Small Colleges, 16(2): 185-192. [24] JUnit home page. http://www.junit.org/. [25] K\u00f6lling, M. and \nRosenberg, J. Guidelines for teaching object orientation with Java. In Proc. 6th Annual Conf. Innovation \nand Technology in Computer Science Education, ACM, 2001, pp. 33-36. [26] K\u00f6lling, M. and Rosenberg, J. \nBlueJ the hitchhiker s guide to object orientation. Maersk Mc-Kinney Moller Institute for Production \nTechnology, Univ. Southern Denmark, Tech. Report 2002, No. 2, ISSN No. 1601-4219. http://www.mip.sdu.dk/~mik/papers/hitch-hiker.pdf. \n[27] K\u00f6lling, M. BlueJ The Interactive Java Environment. http://www.bluej.org/. [28] Krause, K.L. Computer \nscience in the Air Force Academy core curriculum. In Proc.13th SIGCSE Technical Symp. Computer Science \nEducation, ACM, 1982, pp. 144-146. [29] McCauley, R., Archer, C., Dale, N., Mili, R., Roberg\u00e9, J., and \nTaylor, H. The effective integration of the software engineering principles throughout the undergraduate \ncomputer science curriculum. In Proc. 26th SIGCSE Technical Symp. Computer Science Education, ACM, 1995, \npp. 364-365. [30] McCauley, R., Dale, N., Hilburn, T., Mengel, S., and Murrill, B.W. The assimilation \nof software engineering into the undergraduate computer science curriculum. In Proc. 31st SIGCSE Technical \nSymp. Computer Science Education, ACM, 2000, pp. 423-424. [31] Mengel, S.A., Yerramilli, V. A case study \nof the static analysis of the quality of novice student programs. In Proc. 30th SIGCSE Technical Symp. \nComputer Science Education, ACM, 1999, pp. 78-82. [32] Mitchell, W. A paradigm shift to OOP has occurred \n implementation to follow. J. Computing in Small Colleges, 16(2): 95-106. [33] Nagappan, N., Williams, \nL., Ferzli, M., Wiebe, E., Yang, K., Miller, C., and Balik, S. Improving the CS1 experience with pair \nprogramming. In Proc. 34th SIGCSE Technical Symp. Computer Science Education, ACM, 2003, pp. 359-362. \n[34] PMD home page. http://pmd.sourceforge.net/. [35] Reek, K.A. A software infrastructure to support \nintroductory computer science courses. In Proc. 27th SIGCSE Technical Symp. Computer Science Education, \nACM, 1996, pp. 125-129. [36] Roberts, E. Strategies for encouraging individual achievement in introductory \ncomputer science courses. In Proc. 31st SIGCSE Technical Symp. Computer Science Education, ACM, 2000, \npp. 295-299 [37] Sanders, D., and Dorn, B. Jeroo: a tool for introducing object\u00adoriented programming. \nIn Proc. 34th SIGCSE Technical Symp. Computer Science Education, ACM, 2003, pp. 201-204. [38] Shepard, \nT., Lamb, M., and Kelly, D. More testing should be taught. Communications of the ACM, 44(6): 103 108, \nJune 2001. [39] Williams, L., Upchurch, R.L. In support of student pair\u00adprogramming. In Proc. 32nd SIGCSE \nTechnical Symp. Computer Science Education, ACM, 2001, pp. 327-331.  \n\t\t\t", "proc_id": "949344", "abstract": "Despite our best efforts and intentions as educators, student programmers continue to struggle in acquiring comprehension and analysis skills. Students believe that once a program runs on sample data, it is correct; most programming errors are reported by the compiler; when a program misbehaves, shuffling statements and tweaking expressions to see what happens is the best debugging approach. This paper presents a new vision for computer science education centered around the use of test-driven development in all programming assignments, from the beginning of CS1. A key element to the strategy is comprehensive, automated evaluation of student work, in terms of correctness, the thoroughness and validity of the student's tests, and an automatic coding style assessment performed using industrial-strength tools. By systematically applying the strategy across the curriculum as part of a student's regular programming activities, and by providing rapid, concrete, useful feedback that students find valuable, it is possible to induce a cultural shift in how students behave.", "authors": [{"name": "Stephen H. Edwards", "author_profile_id": "81100595678", "affiliation": "Virginia Tech, Blacksburg, VA", "person_id": "PP15037134", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/949344.949390", "year": "2003", "article_id": "949390", "conference": "OOPSLA", "title": "Rethinking computer science education from a test-first perspective", "url": "http://dl.acm.org/citation.cfm?id=949390"}