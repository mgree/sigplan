{"article_publication_date": "01-01-1975", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee.&#38;#169; \n1975 ACM 0-12345-678-9 $5.00 PROGRAM SCHEMAS WITH CONCURRENCY: EXECUTION TIME AND HANGUPS+ Bruce P. \nLester Department of Electrical Engineering Princeton University Princeton, N.J. 08540 Abstract A class \nof program schemas with con-the best parallel algorithm for a given currency is defined as a natural \nextension problem is usually quite different from the of the standard notion of sequential flow-best \nsequential algorithm [2]. For examp].e, chart-like schemas. The question is there is a parallel sorting \nalgorithm [3] considered as to whether such a program which requires time 0(logN)2. But theschema may \nreach a premature termination sequential form of this same algorithm is(or hangup ) for some interpretation. \nIt actually quite inefficient and requires is shown that in general this question is undecidable: however, \nit is shown to be time O(N(~ogN) 2). So a programmer using decidable for the class of free program a \nsequential language would never choose schemas. And an algorithm for testing this this fast parallel \nmethod. property is presented with an upper time bound that grows linearly with the size of In general, \nthe only way to take full. the schema. advantage of concurrency in programs is to permit the explicit \nuse of concurrency at Several structural properties are the highest level. For this purpose we shown \nto be equivalent to the hangup-free need programming languages with control. conditon for free schemas. \nAnd we give a primitives that express concurrency. But method for computing the expected execu-we will \nsee that the use of such primitives tion time of a program schema if the by a programmer introduces the \npossibility expected frequencies of choices at the of improperly formed control structures branch points \nare known. that cause hangups in the flow of control. 1. Introduction In this paper we will analyze this \nproblem from a theoretical viewpoint using There are two basic approaches to the program schemas with \nconcurrency. We will introduction of concurrency into programs. see that for the general case, it is \nThe first is to write the program with a undecidable whether a schema is hangup-e:reesequential programming \nlanguage and then for all interpretations. However, for the utilize some mechanical procedure to find \ncase of free schemas, the problem is potential concurrency rl], In this way decidable in linear time. \nAnd this decision the programmer need not be aware of con\u00adprocedure will suggest a sufficient condi\u00adcurrency. \nHowever, it has been found that tion which can be used in practice to guarantee that real programs do \nnot hangup, t This research was supported by the National Science Foundation under research 2. Proqram \nSchemas grant GJ-35570. we will extend the definition of program schemas found in [4,5] by perrnit\u00adting \nthe use of Fork and Join primitives for concurrency. These primitives are similar to the fork and join \ninstructions first suggested by Conway [6]. Let us consider an alphabet for pro\u00adgram schemas consisting \nof 1. variable symbols  1 X2 2. (n-ary) function symbols -f: n 3. (n-ary) predicate symbols p. 3 A \nproqram schema is a finite, connect\u00aded digraph with one of the following types of statements associated \nwith each vertex, such that there is exactly one Start and one Stop statement: J x -f;(xk ,Xk ). . . \n$x Assignment i k; 12 w Decider The fork and join statements are used to permit concurrency in the schema. \nThe fork causes execution of both output arcs and the join waits for flow of control on both inputs. \nOtherwise this definition of program schema is equivalent to the stand\u00adard definitions found in [4] and \n[5]. (The. Merge i. just to explictly represent the point where two arcs come together). The standard \ndefinitions of interpretation and execution can be used with only slight modification. An interpretation \nI for a schema over a nonempty domain D has the properties: 1. For each variable x,, 1(x,) ~ D 11 2. \nFor each function symbol f;, I(f;): Dn+D 3. For each predicate symbol p;,  I(Pj)m:Dm * {T,F] Execution \nSince the schema has concurrency there may be multiple execution points. The state of a schema is a pair \n(w,V) , such that w associates active or inactive with each arc, and V associates a member of D with \neach variable x. . Intuitively, the active arcs are the ;urrent execution points: and for each variable \nxi, V(xi) is the value of x.. The initial state associates in\u00ad acti$e with all arcs: and the initial \nvalue of each variable x i is I(xi). For all statement kypeS eXCept Start and Merge, a statement is executable \nif all its input arcs are active. Start is execu\u00adtable if it has newer been executed (i.e. Start executes \nonly once), and a Merge is executable if exactly one of its input arcs is active. The execution of a \nstate\u00adment (except Decider) causes all input arcs to become inactive and all output arcs active. The \nexecution of a Decider selects one output arc to become active by applying the predicate to the current \nvalue of the variables. (Also, the execution of assign\u00adment statements has the obvious effect of changing \nthe value of variables appropriate\u00adly) . A computation of a schema under inter\u00adpretation I is any execution \nsequence from the initial state which (1) ends with the first execution of Stop or (2) ends with no statements \nexecutable: A computation results in a premature termination if it ends with one or more arcs in the \nactive state. Hanqups A program schema is hanqup -free if for all interpretations, no computation results \nin a premature termination (i.e. if termina\u00ad tion occurs, then all execution in the schema has been properly \ncompleted) .t T BY considering all possible computation, we have avoided the question of determinacy. \nThis problem has been studied by others (see ref. [7]). Our results hold for de\u00ad terminate or indeterminate \nschemas. In figure 1 we see an example of a schema which is not hangup-free. If the deciders make different \nchoices, a hangup results. However, if we modify the schema slightly by changing p2(x1) to pl(xl), then \nit is hangup-free. Now we will show that hangup\u00adfreeness is in general an undecidable property. F Join \ns top Y Figure 1 Schema with Hangup A program schema is nonparallel if it has no Fork or Join statements. \nA non\u00adparallel schema S with an interpretation I has only one possible computation. Let Val(S,I) denote \nthe final value of all the variables if the computation is finite; otherwise, Val(S,I) is undefined. \nFrom [4], we have the following definition and lemma for nonparallel schemas. Schemas S andS are weakly \nequivalent if for all interpretations I, Val(S,I) = Val(S ,1) whenever both are defined. Lemma 1: (Luckham, \npark, Paterson) Weak equivalence is not partially decidable for nonparallel schemas. Theorem 2: It is \nnot partially decidable whether an arbitrary program schema is hangup-free. Show that for any nonparallel \nX?Q2E: schemas S, S , we can construct a pro\u00adgram schema P which is hangup-free if and only if S and \nS are weakly equiva\u00adlent. Lemma 1 will then give us our result. Let us rename the variables in S as And \nlet k-l be the maximum ; x;  xn index of any predicate symbol in S or S . Then the needed schema P is \nshown in figure 2. If both Val(S,I) and Val(S ,1) are defined, then the only way to avoid hangup is for \nx1,x2, . . ..xn and xi>x~,...,x n to have the same values (i.e. Val(S,I) = Val(S ,1)). QED . Figure \n2 -Program Schema P . 3. Block-Reducible Schemas Although hangup-freeness is undecid\u00adable, we have a \n-simple structural condi\u00adtion to guarantee this property (i.e. a sufficient condition) . We will state \nthis condition below and show that it can be tested in linear time. A block is a connected sub-schema \n(with exactly one input and one output arc) which is one of the following types: 1. Parallel block -- \nA digraph with no directed circuits and an association of fork, join, or block with each vertex. 2. \nDecision block --A digraph with an association of decider, merge, or block with each vertex. 3. Assignment \nblock --An assignment statement.  A program schema is block-reducible if it is of the following form: \nStart Block 2 stop A Notice that block-reducibility is purely a structural property independent of the \npar\u00adticular assignments or decisions. In figure 3, we have an example of a block\u00adreducible schema. The \nparsing of the statements into blocks is shown by broken lines. There are two decision blocks and one \nparallel block plus numerous assignment blocks . The echema of figure 1 is not block-reducible. Theorem \n3: A program schema is block-re\u00adducible -it is hangup-free. proof: The proof is by induction on the nested \nblock structure of the schema. Clearly, every assignment block is hangup-free . This forms a basis for \nthe induction. The inductive step is to prove that a parallel block or decision block is hangup-free \nif its component blocks are hangup-free. QED. So we have shown block-reducibility is sufficient for \nhangup-freeness. Now we will show that a seeming unrelated condition is equivalent to block-reducibility. \nAnd this new condition will suggest a method of test\u00ading block-reducibility in linear time. For this \nwe need the concept of a current for each arc which is a nonzero linear combina\u00adtion of the independent \ncurrents (il,i2, . . ..ik) such that 1. For each decider, merge, or assignment, the sum of the input \ncurrents equals the sum of the output currents. 2. For each fork or join, all arcs have the same current. \n 3. The output of the Start has the same current as the input to the Stop.  Rules 1 and 3 are identical \nto Kirch\u00adhoff s Current Law at those vertices. This type of application of KCL to sequential programs \nis well-known and is discussed in [8] and [9]. Rule 2 is not part of KCL and is unique to programs with \nconcurrency. The role of these current laws in a more general frame-work of parallel computations is \npresented in [10]. A program schema with d decider state\u00adments is balanced if there exists a current \nassignment with 1 +-d independent currents. The schema of figure 3 is balanced. A balanced current assignment \nfor this schema is shown in figure 4. Notice that there are 3 deciders and so 4 independent currents \n(il,i2,i3,i4). A sequential schema is a program schema in which all fork and join statements have exactly \none input arc and one output arc. A program schema is sequentially-coverable if it coverable by sequential \nschemas (i.e. it is the union of sequential schemas) . Theorem 4: A program schema is block-re\u00adducible \ne if it is balanced and sequen\u00adtially coverable. proof: b) Let us assign a unique independent current \nto the output arc of Start and the T output arc of every decider. Applying rules 1, 2, and 3 for currents, \nthis gives us a balanced current assignment. Start o / a . \u00ad xl+f1(x2) / A \\ ~ \\lxl-f3 (x3) 2-f2 (X2 \nI x) x +f \\ 44(4 Ip/ +f x ) \\ \\ 2(X2:+* ,~ I x5ef1 (x4) \\ . // Join \\ // \\  I\\f ) \\ \\-. / / /  , \\ \n\\ 12 ~ (X2 \\F / T \\ \\/+ m \\ w I-I / / \\ / \\\\ - stop () Figure 3 Block-Reducible Schema tart 1 8 11 \n.. 11-12 1 13I * ., 11-12  Figure 4 Balanced Current Assignment From theorem 6 of Hack [11] and our \nthe domain D of I contains all stringsH theorem 2, we have sequential coverability. over the alphabet \nZ = (x ,x ,. . ,Xn, 12ff ~, 23..., fml . (=) Using the properties of current flow in a schema, we can \nestablish that 2. For every variable symbol xi, I xi) = for each Fork, there is a pair of sequen\u00ad x, \n. tial schemas whose union is as shown in 1 figure 5. The subschemas S.,S1,S2,S3 3. For every function \nsymbol f?, 11 must be sequential subschemas. And I(f~)alu2. ..an from properties of currents, we can \n= fj ala2 an verify that S1 and S2 are disjoint from (i.e. the concatenation of f. with its eachother \nand from SO andS ~ (i.e. s1ns2= arguments) . 3 sons1us2) =s3n(s1us2) = o). Similar definitions can be \nfound in [4] and r51. A ~roaram schema is free if for Considering all Forks of. the schema, all Herbrand \ninterpretations, the same we can verify block-reducibility from this predicate symbol is never applied \nto the property. QED . same term more than once. This definition is in analogy with the original definition \nnof free sequential schemas found in [12]. Y Start It is interesting that our sufficient.con\u00addition \nfrom section 3 for general schemas becomes necessary and sufficient for free schemas. Theorem 5: For \nthe class of free program schemas, the following properties are equivalent: 1. Hangup-free. 2. Block-reducible. \n 3. Balanced and Sequentially-coverable.  proof: Show (1) = (3) -Let w(a) de\u00ad . note the current for \narc a. For a schema with arcs ~ = (a a .. an, , the three current laws defi~e2a homogeneous system of \nlinear equations of the form Bwa = ~. If we count the number of times each arc is executed during a \nterminating execution sequence, then these counts will also satisfy the three current laws. So we have \nat least one 3 solution to B@a = O. The general solu\u00ad tion to these ~qua~ions will give us a current \nassignment. By the properties s top of free schemas, the currents on the T outputs of all deciders must \nbe inde\u00ad 8 pendent. So we have a balanced current assignment. Sequentially-coverableFigure 5 -Union of \nSequential Schemas follows from theorem 6 of Hack [11]. 4. Free Schemas From theorem 3, we have (2) \n@ (3) ; and from theorem 2, we have (2) * (1) . QED. Intuitively, a free schema is one in which the choice \nat every decider is al-Linear Time Algorithms ways independent of any previous choices made by deciders. \nTo define this formally From theorem 4 we can deve@P an al90\u00adwe need the concept of a Herbrand interpre\u00adrithm \nfor testing block reducibility thattation IH of a schema s as follows: has an upper time bound which \nis linear in the number of statements in the schema. The1. If S has variables x ,x ,...,x main difficulty \nin testing is to know wherefunction symbols fl, 12, ?.., fm,nt;~ one block ends and the other begins. \nThis problem is resolves by the fact that all fozk-join statements of the same parallel block have the \nsame current. A balanced current assignment can easily b~ found in linear time. So we know exactly how \nthe forks and the joins are divided into blocks. Using this information, a simple linear search through \nthe schema can test for block reducibility. This algorithm is much better than any known techniques for \nana\u00adlyzing parallel computations, most of which require exponential time [1,7,11] . 5. Execution Time \nIn order to compute the expected ex\u00adecution time of any program, we must have some a priori measure of \nthe expected prob\u00adabilities of choices at the conditional branch points. These probabilities can be determined \nby simulation on test data or by a probabilistic analysis of the algorithm itself (see pp. 95-102 of \n18]). From references [9] and [13] , there is a tech\u00ad nique of calculating the expected execution frequency \nfor each statement in a sequential program if the probabilities are known: Let p. , denote the probability \nof 13 executing statement j following statement i. And let fi denote the expected execution frequency \nof statement i. Then clearly, fi = 2 ptiif~+bi where statement 1 is the i entry]point to the program \nand tii=l if i = 1 and O otherwise. Letting f denote the vector of fre\u00ad quencies (fl,f2, . . ..fn). \nwe have a stochas tic matrix P, from the p.. such that, ]1 sf =Pf + ~. -1 ThuS, f = (I-P) ~. So we have \na method of computing the expected execution frequencies of every statement in a sequen\u00ad tial program. \nAnd the expected execution time is clearly Zf i.ti where ti is the exe\u00ad tcution time of statement i. \n For parallel schemas, the problem is more complex bee se we have concurrency. In fact an exact analytic \nsolution will be quite difficult as illustrated by some of the results in [14]. Instead we will pre\u00adsent \na simple method of estimating the execution time of block-reducible schemas. -t If we replace the word \nstatement by program block in the above discussion, then the method is still valid. This method will \nintroduce a slight negative bias over the exact probabilistic solution, and thus gives a lower bound \non the expect\u00aded execution time. Let us associate a time with each assignment and decider which is its \nexecu\u00adtion time. The time for Fork, Join, or Merge is zero. And the time for each block is defined recursively \nas follows: 1. Assignment block -Time is execution time of that assignment statement.  2. Decision block \n-Time is Zfi.t. , where  1 fi are frequencies calculated using method for sequential pr~grams and ti \nare times of vertices, 3. Parallel block -Time is maximum path length from input to output arc (where \npath length is sum of times along the path). Theorem 6: For a block-reducible parallel schema, the time \n(defined above) of its outermost block is a lower bound on the expected execution time. The proof is \nby induction on the nested block-structure. The time for assignment blocks is the basis and the inductive \nstep can be verified from the definition of time for each block type. QED . Fl?22sE: Summary In this \npaper we have shown that in general the property ?hangap-free for parallel schemas is undecidable. However, \nblock-reducibility is sufficient for hangup\u00ad freeness. And for free parallel schemas, block-reducibility \nis necessary and suffi\u00adcient. We have outlined a method of testing this condition in linear time and \nwe have given a technique for computing a lower bound on the expected execution time of any block-reducible \nprogram from the decision probabilities. t If we have times for all subblocks of a decision block, \nwe can use the same method as for sequential programs. References 1. 2. 3. 4. 5. 6. 7. 8. 9. \n 10.  R.M. Keller, parallel program schemata 11. M. Hack, Analysis of production and maximal parallelism \nII: construc-Schema ~ Petri-Nets, Technical Report tion of closures. Journal of the ACM MAC-TR-94, Project \nMAC, Massachusetts Vol. 20, No. 4, pp 696-710 (October Institute of Technology, Cambridge, 1973) . MaSS. \n(February 1972). H.S. Stone, Problems of parallel 12. M. Paterson, Equivalence Problems in computation. \nComplexity-of Sequential a Model of Computation, Ph.D. Thes%,  and Parallel Numerical Algorithms. Cambridge \nUniversity (1967). Ed. J.F. Traub, Academic Press, New York, 1973. 13. M. Schaefer, &#38; Mathematical \nTheory , of Global Program Optimization, B.S. Stone, Parallel processing with Prentice-Hal~, Englewood \nCliffs, N.J. , perfect shuffle. IEEE Transactions z pp 148-149 (1973). on Computers, VO~. C-20, No. 2, \npp 153\u00ad~1 (February 1971). 14. D.E. Martin and G. Estrin, Models of computational systems -cyclic to \nD.C. Luckham, D.M.R. Park, M.S. Pater-acylic graph transformations, ~ son, On formalized computation \npro- Transactions on Electronic Computers, grams. Journal ~ Computer and System Vol. EC-16, N~ 1, pp \n70-79 (February Sciences: 4, Pp 220-249 (1970). 1967) . Z. Manna, Program schemas. Currents in the Theory \nof Computing, Ed. A.V. Aho , Prentice-~11, Englewood Cliffs, N.J., Pp 90-142 (1973). M.E. Conway, A \nmultiprocessor system design. Proceedings of AFIPS Confer\u00adence 24, pp 139-146 (=6= R.M. Karp and R.E. \nMiller, Parallel program schemata. Journal of Computer and System Sciences: ~ pp =7-195, ~y 1969). D.E. \nKnuth, The Art of Computer Pro\u00ad gramming, ~. ~, Fundamental ~~ ri thins, Addison-Wesley, Inc., Read\u00ading, \nMass., pp 364-369 (1968). N. Dee, Graph Theory with Ap placations to Engineering and Computer Science, \n~entice-Hall, ~lewood Cliffs, N.J., Pp 439-448 (1974). B.P. Lester, ~ Balance Property of Parallel Computations, \nph.D. Thesi~ PrOjeCt MAC, Dept. of Electrical Engineering, Massachusetts Institute of Technology, Cambridge, \nMass. (February 1974).  \n\t\t\t", "proc_id": "512976", "abstract": "A class of program schemas with concurrency is defined as a natural extension of the standard notion of sequential flow-chart-like schemas. The question is considered as to whether such a program schema may reach a premature termination (or \"hangup\") for some interpretation. It is shown that in general this question is undecidable; however, it is shown to be decidable for the class of free program schemas. And an algorithm for testing this property is presented with an upper time bound that grows linearly with the size of the schema.Several structural properties are shown to be equivalent to the hangup-free conditon for free schemas. And we give a method for computing the expected execution time of a program schema if the expected frequencies of choices at the branch points are known.", "authors": [{"name": "Bruce P. Lester", "author_profile_id": "81100441991", "affiliation": "Princeton University, Princeton, N.J.", "person_id": "P33553", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/512976.512995", "year": "1975", "article_id": "512995", "conference": "POPL", "title": "Program schemas with concurrency: execution time and hangups", "url": "http://dl.acm.org/citation.cfm?id=512995"}