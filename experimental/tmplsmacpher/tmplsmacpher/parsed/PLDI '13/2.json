{"article_publication_date": "06-16-2013", "fulltext": "\n Automated Feedback Generation for Introductory Programming Assignments Rishabh Singh Sumit Gulwani \nArmando Solar-Lezama MIT CSAIL, Cambridge, MA Microsoft Research, Redmond, WA MIT CSAIL, Cambridge, MA \nrishabh&#38;#169;csail.mit.edu sumitg&#38;#169;microsoft.com asolar&#38;#169;csail.mit.edu Abstract \nWe present a new method for automatically providing feedback for introductory programming problems. In \norder to use this method, we need a reference implementation of the assignment, and an er\u00adror model consisting \nof potential corrections to errors that students might make. Using this information, the system automatically \nde\u00adrives minimal corrections to student s incorrect solutions, providing them with a measure of exactly \nhow incorrect a given solution was, as well as feedback about what they did wrong. We introduce a simple \nlanguage for describing error models in terms of correction rules, and formally de.ne a rule-directed \ntranslation strategy that reduces the problem of .nding minimal corrections in an incorrect program to \nthe problem of synthesizing a correct program from a sketch. We have evaluated our system on thousands \nof real student attempts obtained from the Introduction to Programming course at MIT (6.00) and MITx \n(6.00x). Our results show that relatively simple error models can correct on average 64% of all incorrect \nsubmissions in our benchmark set. Categories and Subject Descriptors D.1.2 [Programming Tech\u00adniques]: \nAutomatic Programming; I.2.2 [Arti.cial Intelligence]: Program Synthesis Keywords Automated Grading; \nComputer-Aided Education; Pro\u00adgram Synthesis 1. Introduction There has been a lot of interest recently \nin making quality educa\u00adtion more accessible to students worldwide using information tech\u00adnology. Several \neducation initiatives such as EdX, Coursera, and Udacity are racing to provide online courses on various \ncollege\u00adlevel subjects ranging from computer science to psychology. These courses, also called massive \nopen online courses (MOOC), are typ\u00adically taken by thousands of students worldwide, and present many \ninteresting scalability challenges. Speci.cally, this paper addresses the challenge of providing personalized \nfeedback for programming assignments in introductory programming courses. The two methods most commonly \nused by MOOCs to provide feedback on programming problems are: (i) test-case based feed\u00adback and (ii) \npeer-feedback [12]. In test-case based feedback, the student program is run on a set of test cases and \nthe failing test cases Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. PLDI 13, June 16 19, 2013, Seattle, WA, USA. Copyright c &#38;#169; 2013 ACM 978-1-4503-2014-6/13/06. \n. . $15.00 are reported back to the student. This is also how the 6.00x course (Introduction to Computer \nScience and Programming) offered by MITx currently provides feedback for Python programming exer\u00adcises. \nThe feedback of failing test cases is however not ideal; es\u00adpecially for beginner programmers who .nd \nit dif.cult to map the failing test cases to errors in their code. This is re.ected by the num\u00adber of \nstudents who post their submissions on the discussion board to seek help from instructors and other students \nafter struggling for hours to correct the mistakes themselves. In fact, for the classroom version of \nthe Introduction to Programming course (6.00) taught at MIT, the teaching assistants are required to \nmanually go through each student submission and provide qualitative feedback describ\u00ading exactly what \nis wrong with the submission and how to correct it. This manual feedback by teaching assistants is simply \nprohibitive for the number of students in the online class setting. The second approach of peer-feedback \nis being suggested as a potential solution to this problem [43]. For example in 6.00x, stu\u00addents routinely \nanswer each other s questions on the discussion fo\u00adrums. This kind of peer-feedback is helpful, but it \nis not without problems. For example, we observed several instances where stu\u00addents had to wait for hours \nto get any feedback, and in some cases the feedback provided was too general or incomplete, and even \nwrong. Some courses have experimented with more sophisticated peer evaluation techniques [28] and there \nis an emerging research area that builds on recent results in crowd-powered systems [7, 30] to provide \nmore structure and better incentives for improving the feedback quality. However, peer-feedback has some \ninherent limi\u00adtations, such as the time it takes to receive quality feedback and the potential for inaccuracies \nin feedback, especially when a majority of the students are themselves struggling to learn the material. \nIn this paper, we present an automated technique to provide feedback for introductory programming assignments. \nThe approach leverages program synthesis technology to automatically determine minimal .xes to the student \ns solution that will make it match the behavior of a reference solution written by the instructor. This \ntech\u00adnology makes it possible to provide students with precise feedback about what they did wrong and \nhow to correct their mistakes. The problem of providing automatic feedback appears to be related to the \nproblem of automated bug .xing, but it differs from it in fol\u00adlowing two signi.cant respects: The complete \nspeci.cation is known. An important challenge in automatic debugging is that there is no way to know \nwhether a .x is addressing the root cause of a problem, or simply masking it and potentially introducing \nnew errors. Usually the best one can do is check a candidate .x against a test suite or a partial speci.cation \n[14]. While providing feedback on the other hand, the solution to the problem is known, and it is safe \nto assume that the instructor already wrote a correct reference implementation for the problem.  Errors \nare predictable. In a homework assignment, everyone is solving the same problem after having attended \nthe same lec\u00adtures, so errors tend to follow predictable patterns. This makes it possible to use a model-based \nfeedback approach, where the potential .xes are guided by a model of the kinds of errors stu\u00addents typically \nmake for a given problem. These simplifying assumptions, however, introduce their own set of challenges. \nFor example, since the complete speci.cation is known, the tool now needs to reason about the equivalence \nof the student solution with the reference implementation. Also, in order to take advantage of the predictability \nof errors, the tool needs to be parameterized with models that describe the classes of errors. And .nally, \nthese programs can be expected to have higher density of errors than production code, so techniques which \nattempts to correct bugs one path at a time [25] will not work for many of these problems that require \ncoordinated .xes in multiple places. Our feedback generation technique handles all of these chal\u00adlenges. \nThe tool can reason about the semantic equivalence of stu\u00addent programs with reference implementations \nwritten in a fairly large subset of Python, so the instructor does not need to learn a new formalism \nto write speci.cations. The tool also provides an er\u00adror model language that can be used to write an \nerror model: a very high level description of potential corrections to errors that students might make \nin the solution. When the system encounters an incor\u00adrect solution by a student, it symbolically explores \nthe space of all possible combinations of corrections allowed by the error model and .nds a correct solution \nrequiring a minimal set of corrections. We have evaluated our approach on thousands of student solu\u00adtions \non programming problems obtained from the 6.00x submis\u00adsions and discussion boards, and from the 6.00 \nclass submissions. These problems constitute a major portion of .rst month of assign\u00adment problems. Our \ntool can successfully provide feedback on over 64% of the incorrect solutions. This paper makes the following \nkey contributions: We show that the problem of providing automated feedback for introductory programming \nassignments can be framed as a syn\u00adthesis problem. Our reduction uses a constraint-based mecha\u00adnism to \nmodel Python s dynamic typing and supports complex Python constructs such as closures, higher-order functions, \nand list comprehensions.  We de.ne a high-level error model language EM L that can be used to provide \ncorrection rules to be used for providing feed\u00adback. We also show that a small set of such rules is suf.cient \nto correct thousands of incorrect solutions written by students.  We report the successful evaluation \nof our technique on thou\u00adsands of real student attempts obtained from 6.00 and 6.00x classes, as well \nas from PE X 4FU N website. Our tool can pro\u00advide feedback on 64% of all submitted solutions that are \nincor\u00adrect in about 10 seconds on average.  2. Overview of the approach In order to illustrate the key \nideas behind our approach, consider the problem of computing the derivative of a polynomial whose coef.cients \nare represented as a list of integers. This problem is taken from week 3 problem set of 6.00x (PS3: Derivatives). \nGiven the input list poly, the problem asks students to write the function computeDeriv that computes \na list poly such that {i \u00d7 poly[i] | 0 < i < len(poly)} if len(poly) > 1 poly =[0] if len(poly) = 1 \nFor example, if the input list poly is [2, -3, 1, 4] (denoting f(x) = 4x 3 + x 2 - 3x + 2), the computeDeriv \nfunction should return [-3, 2, 12] (denoting the derivative f'(x) = 12x 2 + 2x - 3). The reference implementation \nfor the computeDeriv function is shown 1 def computeDeriv_list_int(poly_list_int): 2 result = [] 3 for \ni in range(len(poly_list_int)): 4 result += [i * poly_list_int[i]] 5 if len(poly_list_int) == 1: 6 return \nresult # return [0] 7 else: 8 return result[1:] # remove the leading 0 Figure 1. The reference implementation \nfor computeDeriv. in Figure 1. This problem teaches concepts of conditionals and iteration over lists. \nFor this problem, students struggled with many low-level Python semantics issues such as the list indexing \nand iteration bounds. In addition, they also struggled with conceptual issues such as missing the corner \ncase of handling lists consisting of single element (denoting constant function). One challenge in providing \nfeedback for student submissions is that a given problem can be solved by using many different algo\u00adrithms. \nFigure 2 shows three very different student submissions for the computeDeriv problem, together with the \nfeedback generated by our tool for each submission. The student submission shown in Figure 2(a) is taken \nfrom the 6.00x discussion forum1. The stu\u00addent posted the code in the forum seeking help and received \ntwo responses. The .rst response asked the student to look for the .rst if-block return value, and the \nsecond response said that the code should return [0] instead of empty list for the .rst if statement. \nThere are many different ways to modify the code to return [0] for the case len(poly)=1. The student \nchose to change the initializa\u00adtion of the deriv variable from [ ] to the list [0]. The problem with \nthis modi.cation is that the result will now have an additional 0 in front of the output list for all \ninput lists (which is undesirable for lists of length greater than 1). The student then posted the query \nagain on the forum on how to remove the leading 0 from result, but unfortunately this time did not get \nany more response. Our tool generates the feedback shown in Figure 2(d) for the student program in about \n40 seconds. During these 40 seconds, the tool searches over more than 107 candidate .xes and .nds the \n.x that requires minimum number of corrections. There are three problems with the student code: .rst \nit should return [0] in line 5 as was suggested in the forum but wasn t speci.ed how to make the change, \nsecond the if block should be removed in line 7, and third that the loop iteration should start from \nindex 1 instead of 0 in line 6. The generated feedback consists of four pieces of information (shown \nin bold in the .gure for emphasis): the location of the error denoted by the line number.  the problematic \nexpression in the line.  the sub-expression which needs to be modi.ed.  the new modi.ed value of the \nsub-expression.  The feedback generator is parameterized with a feedback-level parameter to generate \nfeedback consisting of different combina\u00adtions of the four kinds of information, depending on how much \ninformation the instructor is willing to provide to the student. 2.1 Work.ow In order to provide the \nlevel of feedback described above, the tool needs some information from the instructor. First, the tool \nneeds to know what the problem is that the students are supposed to solve. The instructor provides this \ninformation by writing a reference im\u00ad 1 https://www.edx.org/courses/MITx/6.00x/2012_Fall/discussion/ \nforum/600x_ps3_q2/threads/5085f3a27d1d422500000040  Three different student submissions for computeDeriv \n1 def computeDeriv(poly): 2 deriv = [] 3 zero = 0 4 if (len(poly) == 1): 5 return deriv 6 for e in range(0,len(poly)): \n7 if (poly[e] == 0): 8 zero += 1 9 else: 10 deriv.append(poly[e]*e) 11 return deriv (a) The program \nrequires 3 changes: In the return statement return deriv in line 5, replace deriv by [0].  In the comparison \nexpression (poly[e] == 0) in line 7, change (poly[e] == 0) to False.  In the expression range(0, len(poly)) \nin line 6, increment 0 by 1.  (d) 1 def computeDeriv(poly): 2 idx = 1 3 deriv = list([]) 4 plen = len(poly) \n5 while idx <= plen: 6 coeff = poly.pop(1) 7 deriv += [coeff idx] 8 idx = idx + 1 9 if len(poly) < 2: \n 10 return deriv * (b) Feedback generated by our Tool The program requires 1 change: In the function \ncomputeDeriv, add the base case at the top to return [0] for len(poly)=1. (e) 1 def computeDeriv(poly): \n2 length = int(len(poly)-1) 3 i = length 4 deriv = range(1,length) 5 if len(poly) == 1: 6 deriv = [0] \n7 else: 8 while i >= 0: 9 new = poly[i] i 10 i -= 1 11 deriv[i] = new 12 return deriv * (c) The program \nrequires 2 changes:  In the expression range(1, length) in line 4, increment length by 1.  In the comparison \nexpression (i >= 0) in line 8, change operator >= to !=.  (f) Figure 2. Three very different student \nsubmissions ((a), (b), and (c)) for the computeDeriv problem and the corresponding feedback generated \nby our tool ((d), (e), and (f)) for each one of them using the same reference implementation. plementation \nsuch as the one in Figure 1. Since Python is dynam\u00adically typed, the instructor also provides the types \nof function ar\u00adguments and return value. In Figure 1, the instructor speci.es the type of input argument \nto be list of integers (poly_list_int) by appending the type to the name. In addition to the reference \nimplementation, the tool needs a description of the kinds of errors students might make. We have designed \nan error model language EM L, which can describe a set of correction rules that denote the potential \ncorrections to errors that students might make. For example, in the student attempt in Figure 2(a), we \nobserve that corrections often involve modifying the return value and the range iteration values. We \ncan specify this information with the following three correction rules: return a . return [0] range(a1, \na2) . range(a1 + 1, a2) a0 == a1 . False The correction rule return a . return [0] states that the expres\u00adsion \nof a return statement can be optionally replaced by [0]. The error model for this problem that we use \nfor our experiments is shown in Figure 8, but we will use this simple error model for sim\u00adplifying the \npresentation in this section. In later experiments, we also show how only a few tens of incorrect solutions \ncan provide enough information to create an error model that can automatically provide feedback for thousands \nof incorrect solutions. The rules de.ne a space of candidate programs which the tool needs to search \nin order to .nd one that is equivalent to the ref\u00aderence implementation and that requires minimum number \nof cor\u00adrections. We use constraint-based synthesis technology [16, 37, 40] to ef.ciently search over \nthis large space of programs. Speci.cally, we use the SK E T CH synthesizer that uses a SAT-based algorithm \nto complete program sketches (programs with holes) so that they meet a given speci.cation. We extend \nthe SK ET C H synthesizer with sup\u00adport for minimize hole expressions whose values are computed ef.\u00adciently \nby using incremental constraint solving. To simplify the pre\u00adsentation, we use a simpler language MPY \n(miniPython) in place of Python to explain the details of our algorithm. In practice, our tool supports \na fairly large subset of Python including closures, higher order functions, and list comprehensions. \n2.2 Solution Strategy Figure 3. The architecture of our feedback generation tool. The architecture of \nour tool is shown in Figure 3. The solu\u00adtion strategy to .nd minimal corrections to a student s solution \nis based on a two-phase translation to the Sketch synthesis language. In the .rst phase, the Program \nRewriter uses the correction rules to translate the solution into a language we call M MPY; this language \nprovides us with a concise notation to describe sets of MPY candi\u00addate programs, together with a cost \nmodel to re.ect the number of corrections associated with each program in this set. In the second phase, \nthis M MPY program is translated into a sketch program by the Sketch Translator.  1 def computeDeriv(poly): \n2 deriv = [] 3 zero = 0 4 if 5 6 for 7 8  9 else: 10 deriv.append(poly[e]*e) 11 return { deriv ,[0]} \nFigure 4. The resulting M MPY program after applying correction rules to program in Figure 2(a). In the \ncase of example in Figure 2(a), the Program Rewriter produces the M MPY program shown in Figure 4 using \nthe correc\u00adtion rules from Section 2.1. This program includes all the possi\u00adble corrections induced by \nthe correction rules in the model. The M MPY language extends the imperative language MPY with expres\u00adsion \nchoices, where the choices are denoted with squiggly brack\u00adets. Whenever there are multiple choices for \nan expression or a statement, the zero-cost choice, the one that will leave the ex\u00adpression unchanged, \nis boxed. For example, the expression choice { a0 , a1, \u00b7 \u00b7 \u00b7 , an} denotes a choice between expressions \na0, \u00b7 \u00b7 \u00b7, an where a0 denotes the zero-cost default choice. For this simple program, the three correction \nrules induce a space of 32 different candidate programs. This candidate space is fairly small, but the \nnumber of candidate programs grow exponen\u00adtially with the number of correction places in the program \nand with the number of correction choices in the rules. The error model that we use in our experiments \ninduces a space of more than 1012 can\u00addidate programs for some of the benchmark problems. In order to \nsearch this large space ef.ciently, the program is translated to a sketch by the Sketch Translator. \n 2.3 Synthesizing Corrections with Sketch The SK E T C H [37] synthesis system allows programmers to \nwrite programs while leaving fragments of it unspeci.ed as holes; the contents of these holes are .lled \nup automatically by the synthe\u00adsizer such that the program conforms to a speci.cation provided in terms \nof a reference implementation. The synthesizer uses the CEGIS algorithm [38] to ef.ciently compute the \nvalues for holes and uses bounded symbolic veri.cation techniques for performing equivalence check of \nthe two implementations. There are two key aspects in the translation of an M MPY program to a SKET CH \nprogram. The .rst aspect is speci.c to the Python language. SK ETC H supports high-level features such \nas closures and higher-order functions which simpli.es the translation, but it is statically typed whereas \nMPY programs (like Python) are dynamically typed. The translation models the dynamically typed variables \nand operations over them using struct types in SK E T C H in a way similar to the union types. The second \naspect of the translation is the modeling of set-expressions in MPY using ?? M (holes) in SK ETC H, which \nis language independent. The dynamic variable types in the MPY language are modeled using the MultiType \nstruct de.ned in Figure 5. The MultiType struct consists of a flag .eld that denotes the dynamic type \nof a variable and currently supports the following set of types: {INTEGER, BOOL, TYPE, LIST, TUPLE, STRING, \nDICTIONARY}. The val and bval .elds store the value of an integer and a Boolean struct MultiType{ int \nval, flag; struct MTList{ bit bval; int len; MTString str; MTTuple tup; MultiType[len] lVals; MTDict \ndict; MTList lst; } } Figure 5. The MultiType struct for encoding Python types. variable respectively, \nwhereas the str, tup, dict, and lst .elds store the value of string, tuple, dictionary, and list variables \nre\u00adspectively. The MTList struct consists of a .eld len that denotes the length of the list and a .eld \nlVals of type array of MultiType that stores the list elements. For example, the integer value 5 is represented \nas the value MultiType(val=5, flag=INTEGER) and the list [1,2] is represented as the value MultiType(lst=new \nMTList(len=2,lVals={new MultiType(val=1,flag=INTEGER), new MultiType(val=2,flag=INTEGER)}), flag=LIST). \nThe second key aspect of this translation is the translation of ex\u00adpression choices in M MPY. The SK \nETC H construct ?? denotes an un\u00adknown integer hole that can be assigned any constant integer value by \nthe synthesizer. The expression choices in M MPY are translated to functions in SK ETC H that based on \nthe unknown hole values return either the default expression or one of the other expression choices. \nEach such function is associated with a unique Boolean choice variable, which is set by the function \nwhenever it returns a non-default expression choice. For example, the set-statement return { deriv ,[0]}; \n(line 5 in Figure 4) is translated to return modRetVal0(deriv), where the modRetVal0 function is de.ned \nas: MultiType modRetVal0(MultiType a){ if(??) return a; // default choice choiceRetVal0 = True; // non-default \nchoice MTList list = new MTList(lVals={new MultiType(val=0, flag=INTEGER)}, len=1); return new MultiType(lst=list, \ntype = LIST); } The translation phase also generates a SK ET C H harness that compares the outputs of \nthe translated student and reference im\u00adplementations on all inputs of a bounded size. For example in \ncase of the computeDeriv function, with bounds of n = 4 for both the number of integer bits and the maximum \nlength of input list, the harness matches the output of the two implementations for more than 216 different \ninput values as opposed to 10 test-cases used in 6.00x. The harness also de.nes a variable totalCost \nas a function of choice variables that computes the total number of corrections performed in the original \nprogram, and asserts that the value of totalCost should be minimized. The synthesizer then solves this \nminimization problem ef.ciently using an incremental solving al\u00adgorithm CEGISMIN described in Section \n4.2. After the synthesizer .nds a solution, the Feedback Generator extracts the choices made by the synthesizer \nand uses them to generate the corresponding feedback in natural language. For this example, the tool \ngenerates the feedback shown in Figure 2(d) in less than 40 seconds. 3. EM L: Error Model Language In \nthis section, we describe the syntax and semantics of the error model language EML. An EM L error model \nconsists of a set of rewrite rules that captures the potential corrections for mistakes that students \nmight make in their solutions. We de.ne the rewrite rules over a simple Python-like imperative language \nMPY. A rewrite rule transforms a program element in MPY to a set of weighted MPY program elements. This \nweighted set of MPY program elements is  [ a] = {(a, 0)} [ { a0 , \u00b7 \u00b7 \u00b7 , an}] = [ a0] . {(a, c + 1) \n| (a, c) . [ ai] 0<i=n} [ a0[ a1]]] = {(a0[a1], c0 + c1) | (ai, ci) . [ ai] i.{0,1}} [ while b : s] = \n{(while b : s, cb + cs) | (b, cb) . [ b] , (s, cs) . [ s] } Figure 7. The [ ] function (shown partially) \nthat translates an M MPY program to a weighted set of MPY programs. represented succinctly as an MMPY \nMPY program element, where Mextends the MPY language with set-exprs (sets of expressions) and set-stmts \n(sets of statements). The weight associated with a pro\u00adgram element in this set denotes the cost of performing \nthe corre\u00adsponding correction. An error model transforms an MPY program to an M MPY program by recursively \napplying the rewrite rules. We show that this transformation is deterministic and is guaranteed to terminate \non well-formed error models. 3.1 MPY and MMPY languages The syntax of MPY and M MPY languages is shown \nin Figure 6(a) and Figure 6(b) respectively. The purpose of M MPY language is to represent a large collection \nof MPY programs succinctly. The M MPY language consists of set-expressions (a and b) and set-statements \n(s ) that represent a weighted set of corresponding MPY expres\u00adsions and statements respectively. For \nexample, the set expres\u00adsion { n0 , \u00b7 \u00b7 \u00b7 , nk} represents a weighted set of constant inte\u00adgers where \nn0 denotes the default integer value associated with cost 0 and all other integer constants (n1, \u00b7 \u00b7 \n\u00b7 , nk) are associated with cost 1. The sets of composite expressions are represented suc\u00adcinctly in \nterms of sets of their constituent sub-expressions. For example, the composite expression { a0 , a0 + \n1}{ < , =, >, = , ==,= }{ a1 , a1 + 1, a1 - 1} represents 36 MPY expressions. Each MPY program in the \nset of programs represented by an M MPY program is associated with a cost (weight) that denotes the number \nof modi.cations performed in the original program to obtain the transformed program. This cost allows \nthe tool to search for corrections that require minimum number of modi.cations. The weighted set of MPY \nprograms is de.ned using the [ ] function shown partially in Figure 7, the complete function de.nition \ncan be found in [36]. The [ ] function on MPY expressions such as a returns a singleton set {(a, 0)} \nconsisting of the corresponding expression associated with cost 0. On set-expressions of the form { a \n0 , \u00b7 \u00b7 \u00b7 , a n}, the function returns the union of the weighted set of MPY expressions corresponding \nto the default set-expression ([ a0] ) and the weighted set of expressions corresponding to other set-expressions \n(a 1, \u00b7 \u00b7 \u00b7 , a n), where each expression in [ ai)]] is associated with an additional cost of 1. On composite \nexpressions, the function computes the weighted set recursively by taking the cross-product of weighted \nsets of its constituent sub-expressions and adding their corresponding costs. For example, the weighted \nset for composite expression x [ y] consists of an expression xi[yj ] associated with cost cxi + cyj \nfor each (xi, cxi ) . [ x] and (yj , cyj ) . [ y] .  3.2 Syntax of EML An EML error model consists of \na set of correction rules that are used to transform an MPY program to an M MPY program. A correc\u00adtion \nrule C is written as a rewrite rule L . R, where L and R de\u00adnote a program element in MPY and M MPY respectively. \nA program element can either be a term, an expression, a statement, a method or the program itself. The \nleft hand side (L) denotes an MPY pro\u00adgram element that is pattern matched to be transformed to an M \nMPY program element denoted by the right hand side (R). The left hand side of the rule can use free variables \nwhereas the right hand side can only refer to the variables present in the left hand side. The language \nalso supports a special ' (prime) operator that can be used to tag sub-expressions in R that are further \ntransformed recursively using the error model. The rules use a shorthand notation ?a (in the right hand \nside) to denote the set of all variables that are of the same type as the type of expression a and are \nin scope at the corresponding program location. We assume each correction rule is associated with cost \n1, but it can be easily extended to different costs to account for different severity of mistakes. INDR: \nv[a] . v[{a + 1, a - 1, ?a}] INITR: v = n . v = {n + 1, n - 1, 0}RANR: range(a0, a1) . range({0, 1, a0 \n- 1, a0 + 1}, {a1 + 1, a1 - 1}) COMPR: a0 opc a1 . {{a ' 0 - 1, ?a0} Dopc {a ' 1 - 1, 0, 1, ?a1}, True, \nFalse} where Dopc = {<, >, =, =, ==, =} RETR: return a . return{[0] if len(a) == 1 else a, a[1 :] if \n(len(a) > 1) else a} Figure 8. The error model E for the computeDeriv problem. Example 1. The error \nmodel for the computeDeriv problem is shown in Figure 8. The INDR rewrite rule transforms the list access \nindices. The INITR rule transforms the right hand size of constant initializations. The RANR rule transforms \nthe arguments for the range function; similar rules are de.ned in the model for other range functions \nthat take one and three arguments. The COMPR rule transforms the operands and operator of the comparisons. \nThe RETR rule adds the two common corner cases of returning [0] when the length of input list is 1, and \nthe case of deleting the .rst list element before returning the list. Note that these rewrite rules de.ne \nthe corrections that can be performed optionally; the zero cost (default) case of not correcting a program \nelement is added automatically as described in Section 3.3. De.nition 1. Well-formed Rewrite Rule : A \nrewrite rule C : L . R is de.ned to be well-formed if all tagged sub-terms t ' in R have a smaller size \nsyntax tree than that of L. The rewrite rule C1 : v[a] . {(v[a]) ' + 1} is not a well-formed rewrite \nrule as the size of the tagged sub-term (v[a]) of R is the same as that of the left hand side L. On the \nother hand, the rewrite rule C2 : v[a] . {v ' [a ' ] + 1} is well-formed. De.nition 2. Well-formed Error \nModel : An error model E is de.ned to be well-formed if all of its constituent rewrite rules Ci . E are \nwell-formed.  3.3 Transformation with EML An error model E is syntactically translated to a function \nTE that transforms an MPY program to an M MPY program. The TE function .rst traverses the program element \nw in the default way, i.e. no transformation happens at this level of the syntax tree, and the function \nis called recursively on all of its top-level sub-terms t to obtain the transformed element w0 . MPY. \nFor each correction M rule Ci : Li . Ri in the error model E, the function contains a  Arith Expr a \n:= n | [ ] | v | a[a] | a0 opa a1 | [a1, \u00b7 \u00b7 \u00b7 , an] | f(a0, \u00b7 \u00b7 \u00b7 , an) | a0 if b else a1 Arith Op opa \n:= + | - | \u00d7 | / | * * Bool Expr b := not b | a0 opc a1 | b0 opb b1 Comp Op opc := == | < | > | = | = \nBool Op opb := and | or Stmt Expr s := v = a | s0; s1 | while b : s | if b : s0 else: s1 | for a0 in \na1 : s | return a Func Def. p := def f(a1, \u00b7 \u00b7 \u00b7 , an) : s (a) MPY  Arith set-expr a := | a | { a0 , \n\u00b7 \u00b7 \u00b7 , an} | a[ a] | a0 Dopa a1 [ a0, \u00b7 \u00b7 \u00b7 , an] | f ( a0, \u00b7 \u00b7 \u00b7 , an) set-op Dopx := opa | { Dopx0 \n, \u00b7 \u00b7 \u00b7 , Dopxn } Bool set-expr b := b | { b0 , \u00b7 \u00b7 \u00b7 , bn} | not b | a0 Dopc a1 Stmt set-expr s := s \n| { s0 , \u00b7 \u00b7 \u00b7 , sn} | v := a | s0; s1 Func Def p | | := while b : s | for a0 in a1 : s if b : s0 else \n: s1 | return a def f (a1, \u00b7 \u00b7 \u00b7 , an) s (b) MMPY | b0 D b1 opb Figure 6. The syntax for (a) MPY and \n(b) M MPY languages. Match expression that matches the term w with the left hand side of the rule Li \n(with appropriate uni.cation of the free variables in Li). If the match succeeds, it is transformed to \na term wi . M MPY as de.ned by the right hand side Ri of the rule after calling the TE function on each \nof its tagged sub-terms t '. Finally, the method returns the set of all transformed terms { w0 , \u00b7 \u00b7 \n\u00b7 , wn}. Example 2. Consider an error model E1 consisting of the follow\u00ading three correction rules: C1 \n: v[a] . v[{a - 1, a + 1}] C2 : a0 opc a1 . {a0 ' - 1, 0} opc {a1 ' - 1, 0} C3 : v[a] . ?v[a] The transformation \nfunction TE1 for the error model E1 is shown in Figure 9. TE1 (w : MPY) : M= MPY let w0 = w[t . TE1 (t)] \nin (* t : a sub-term of w *) let w1 = Match w with v[a] . v[{a + 1, a - 1}] in let w2 = Match w with \na0 opc a1 . {TE1 (a0) - 1, 0} opc {TE1 (a1) - 1, 0} in { w0 , w1, w2} Figure 9. The TE1 method for error \nmodel E1. The recursive steps of application of TE1 function on expression (x[i] < y[j]) are shown in \nFigure 10. This example illustrates two interesting features of the transformation function: Nested Transformations \n: Once a rewrite rule L . R is ap\u00adplied to transform a program element matching L to R, the in\u00adstructor \nmay want to apply another rewrite rule on only a few sub-terms of R. For example, she may want to avoid \ntrans\u00adforming the sub-terms which have already been transformed by some other correction rule. The EM \nL language facilitates making such distinction between the sub-terms for performing nested corrections \nusing the ' (prime) operator. Only the sub\u00adterms in R that are tagged with the prime operator are visited \nfor applying further transformations (using the TE function re\u00adcursively on its tagged sub-terms t '), \nwhereas the non-tagged sub-terms are not transformed any further. After applying the rewrite rule C2 \nin the example, the sub-terms x[i] and y[j] are further transformed by applying rewrite rules C1 and \nC3. Ambiguous Transformations : While transforming a program using an error model, it may happen that \nthere are multiple rewrite rules that pattern match the program element w. After applying rewrite rule \nC2 in the example, there are two rewrite rules C1 and C3 that pattern match the terms x[i] and y[j]. \nAfter applying one of these rules (C1 or C3) to an expression v[a], we cannot apply the other rule to \nthe transformed expression. In such ambiguous cases, the TE function creates a separate copy of the transformed \nprogram element (wi) for each ambiguous choice and then performs the set union of all such elements to \nobtain the transformed program element. This semantics of handling ambiguity of rewrite rules also matches \nnaturally with the intent of the instructor. If the instructor wanted to perform both transformations \ntogether on array accesses, she could have provided a combined rewrite rule such as v[a] . ?v[{a + 1, \na - 1}]. Theorem 1. Given a well-formed error model E, the transforma\u00adtion function TE always terminates. \nProof. From the de.nition of well-formed error model, each of its constituent rewrite rule is also well-formed. \nHence, each applica\u00adtion of a rewrite rule reduces the size of the syntax tree of terms that are required \nto be visited further for transformation by TE . There\u00adfore, the TE function terminates in a .nite number \nof steps. M 4. Constraint-based Solving of MPY programs In the previous section, we saw the transformation \nof an MPY pro\u00adgram to an M MPY program based on an error model. We now present the translation of an \nM MPY program into a SKE T CH program [37]. 4.1 Translation of MMPY programs to SK E T C H The M MPY \nprograms are translated to SK E T C H programs for ef.cient constraint-based solving for minimal corrections \nto the student solutions. The two main aspects of the translation include : (i) the translation of Python-like \nconstructs in M MPY to SK ET C H, and (ii) the translation of set-expr choices in M MPY to SK ET C H \nfunctions.  T (x[i] < y[j]) = { T (x[i]) < T (y[j]) , {T (x[i]) - 1, 0} < {T (y[j]) - 1, 0}} T (x[i]) \n= { T (x)[T (i)] , x[{i + 1, i - 1}], y[i]} T (y[j]) = { T (y)[T (j)] , y[{j + 1, j - 1}], x[j]} T (x) \n= { x } T (i) = { i } T (y) = { y } T (j) = { j } Therefore, after substitution the result is: T (x[i] \n< y[j]) = { { x [ i ] , x[{i + 1, i - 1}], y[i]} < { y [ j ] , y[{j + 1, j - 1}], x[j]} , {{ x [ i \n] , x[{i + 1, i - 1}], y[i]} - 1, 0} < {{ y [ j ] , y[{j + 1, j - 1}], x[j]} - 1, 0}} Figure 10. Application \nof TE1 (abbreviated T ) on expression (x[i] < y[j]). Handling dynamic typing of MThe dynamic typ-MPY \nvariables ing in M MPY is handled using a MultiType variable as described in Section 2.3. The M MPY expressions \nand statements are transformed to SK E TC H functions that perform the corresponding transforma\u00adtions \nover MultiType. For example, the Python statement (a = b) is translated to assignMT(a, b), where the \nassignMT function as\u00adsigns MultiType b to a. Similarly, the binary add expression (a + b) is translated \nto binOpMT(a, b, ADD_OP) that in turn calls the function addMT(a,b) to add a and b as shown in Figure \n11. 1 MultiType addMT(MultiType a, MultiType b){ 2 assert a.flag == b.flag; // same types can be added \n3 if(a.flag == INTEGER) // add for integers 4 return new MultiType(val=a.val+b.val, flag = INTEGER); \n5 if(a.flag == LIST){ // add for lists 6 int newLen = a.lst.len + b.lst.len; 7 MultiType[newLen] newLVals \n= a.lst.lVals; 8 for(int i=0; i<b.lst.len; i++) 9 newLVals[i+a.lst.len] = b.lst.lVals[i]; 10 return \nnew MultiType(lst = new MTList(lVals=newLVals, len=newLen), flag=LIST);} 11 \u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7 12 } Figure 11. The \naddMT function for adding two MultiType a and b. Translation of MMPY set-expressions The set-expressions \nin M MPY are translated to functions in SK E TC H. The function bodies ob\u00adtained by the application of \ntranslation function (F) on some of the interesting M MPY constructs are shown in Figure 12. The SK E \nT C H construct ?? (called hole) is a placeholder for a constant value, which is .lled up by the SKE \nT CH synthesizer while solving the constraints to satisfy the given speci.cation. The singleton sets \nconsisting of an MPY expression such as {a} are translated simply to the corresponding expression itself. \nA set-expression of the form { a 0 , \u00b7 \u00b7 \u00b7 , a n} is translated recur\u00adsively to the if expression :if \n(??) F( a0) else F({a 1, \u00b7 \u00b7 \u00b7 , a n}), which means that the synthesizer can optionally select the default \nset-expression F( a0) (by choosing ?? to be true) or select one of the other choices (a 1, \u00b7 \u00b7 \u00b7 , a \nn). The set-expressions of the form Figure 12. The translation rules (shown partially) for converting \nF({a}) = a F({ a0 , \u00b7 \u00b7 \u00b7 , an}) = if (??) F( a0) else F({ a1, \u00b7 \u00b7 \u00b7 , an}) F({ a0, \u00b7 \u00b7 \u00b7 , an}) = if \n(??) {choicek = True; F( a0)} else F({ a1, \u00b7 \u00b7 \u00b7 , an}) F( a0[ a1]) = F( a0)[F( a1)] F( a0 = a1) = F( \na0) := F( a1) M MPY set-exprs to corresponding SK E T C H function bodies. {a 0, \u00b7 \u00b7 \u00b7 , a n} are similarly \ntranslated but with an additional state\u00adment for setting a fresh variable choicek if the synthesizer \nselects the non-default choice a 0. The translation rules for the assignment statements (a 0 := a 1) \nresults in if expressions on both left and right sides of the assignment. The if expression choices occurring \non the left hand side are desugared to individual assignments. For example, the left hand side expression \nif (??) x else y := 10 is desugared to if (??) x := 10 else y := 10. The in.x operators in M MPY are \n.rst translated to function calls and are then translated to sketch using the translation for set-function \nexpressions. The remaining M MPY expressions are similarly translated recursively and the translation \ncan be found in more detail in [36]. Translating function calls The translation of function calls for \nrecursive problems and for problems that require writing a function that uses other sub-functions is \nparmeterized by three options: 1) use the student s implementation of sub-functions, 2) use the teacher \ns implementation of sub-functions, and 3) treat the sub\u00adfunctions as uninterpreted functions. Generating \nthe driver functions The SK ETC H synthesizer sup\u00adports the equivalence checking of functions whose input \narguments and return values are over SK E TC H primitive types such as int, bit and arrays. Therefore, \nafter the translation of M MPY programs to SK E T C H programs, we need additional driver functions to \nintegrate the functions over MultiType input arguments and return value to the corresponding functions \nover SK E T CH primitive types. The driver functions .rst converts the input arguments over primitive \ntypes to corresponding MultiType variables using library functions such as computeMTFromInt, and then \ncalls the translated M  MPY func\u00adtion with the MultiType variables. The returned MultiType value is \ntranslated back to primitive types using library functions such as computeIntFromMT. The driver function \nfor student s programs also consists of additional statements of the form if(choicek ) totalCost++; and \nthe statement minimize(totalCost), which tells the synthesizer to compute a solution to the Boolean variables \nchoicek that minimizes the totalCost variable.  4.2 CEGISMIN: Incremental Solving for the Minimize holes \nAlgorithm 1 CEGISMIN Algorithm for Minimize expression i . 0, F0 . F, fp . null 1: s0 . srandom, 2: while \n(True) 3: i . i + 1 4: Fi . Synth(si-1, Fi-1) C Synthesis Phase 5: if (Fi = UNSAT) C Synthesis Fails \n6: if (Fprev = null) return UNSAT_SKETCH 7: else return PE(P,fp) 8: choose f . Fi 9: si . Verify(f) C \nVeri.cation Phase 10: if (si = null) C Veri.cation Succeeds 11: (minHole, minHoleValue) . getMinHoleValue(f) \n12: fp . f 13: Fi . Fi . {encode(minHole < minHoleVal)} We extend the CEGIS algorithm in SKE T CH [37] \nto obtain the CEGISMIN algorithm shown in Algorithm 1 for ef.ciently solving sketches that include a \nminimize hole expression. The input state of the sketch program is denoted by s and the sketch constraint \nstore is denoted by F. Initially, the input state s0 is assigned a random input state value and the constraint \nstore F0 is assigned the constraint set obtained from the sketch program. The variable fp stores the \nprevious satis.able hole values and is initialized to null. In each iteration of the loop, the synthesizer \n.rst performs the inductive synthesis phase where it shrinks the constraints set Fi-1 to Fi by removing \nbehaviors from Fi-1 that do not conform to the input state si-1. If the constraint set becomes unsatis.able, \nit either returns the sketch completed with hole values from the previous solution if one exists, otherwise \nit returns UNSAT. On the other hand, if the constraint set is satis.able, then it .rst chooses a conforming \nassignment to the hole values and goes into the veri.cation phase where it tries to verify the completed \nsketch. If the veri.er fails, it returns a counter-example input state si and the synthesis-veri.cation \nloop is repeated. If the veri.cation phase succeeds, instead of returning the result as is done in the \nCEGIS algorithm, the CEGISMIN algorithm computes the value of minHole from the constraint set f, stores \nthe current satis.able hole solution f in fp, and adds an additional constraint {minHole<minHoleVal} \nto the constraint set Fi. The synthesis-veri.cation loop is then repeated with this additional constraint \nto .nd a conforming value for the minHole variable that is smaller than the current value in f.  4.3 \nMapping SK ET C H solution to generate feedback Each correction rule in the error model is associated \nwith a feed\u00adback message, e.g. the correction rule for variable initialization v = n . v = {n + 1} in \nthe computeDeriv error model is associated with the message Increment the right hand side of the initialization \nby 1 . After the SK ETC H synthesizer .nds a solution to the constraints, the tool maps back the values \nof unknown integer holes to their corresponding expression choices. These expression choices are then \nmapped to natural language feedback using the messages associated with the corresponding correction rules, \nto\u00adgether with the line numbers. If the synthesizer returns UNSAT, the tool reports that the student \nsolution can not be .xed. 5. Implementation and Experiments We now brie.y describe some of the implementation \ndetails of the tool, and then describe the experiments we performed to evaluate our tool over the benchmark \nproblems. 5.1 Implementation The tool s frontend is implemented in Python itself and uses the Python \nast module to convert a Python program to a SKE T CH program. The backend system that solves the sketch \nis imple\u00admented as a wrapper over the SK ET C H system that is extended with the CEGISMIN algorithm. \nThe feedback generator, implemented in Python, parses the output generated by the backend system and \ntranslates it to corresponding high level feedback in natural lan\u00adguage. Error models in our tool are \ncurrently written in terms of rewrite rules over the Python AST. In addition to the Python tool, we also \nhave a prototype for the C# language, which we built on top of the Microsoft Roslyn compiler framework. \nThe C# prototype supports a smaller subset of the language relative to the Python tool but nevertheless \nit was useful in helping us evaluate the potential of our technique on a different language.  5.2 Benchmarks \nWe created our benchmark set with problems taken from the Intro\u00adduction to Programming course at MIT \n(6.00) and the EdX version of the class (6.00x) offered in 2012. Our benchmark set includes most problems \nfrom the .rst four weeks of the course. We only ex\u00adcluded (i) a problem that required more detailed .oating \npoint rea\u00adsoning than what we currently handle, (ii) a problem that required .le i/o which we currently \ndo not model, and (iii) a handful of triv\u00adial .nger exercises. To evaluate the applicability to C#, we \ncreated a few programming exercises2 on PEX 4FUN that were based on loop\u00adover-arrays and dynamic programming \nfrom an AP level exam3. A brief description of each benchmark problem follows: prodBySum-6.00 : Compute \nthe product of two numbers m and n using only the sum operator. oddTuples-6.00 : Given a tuple l, return \na tuple consisting of every other element of l.  compDeriv-6.00 : Compute the derivative of a polynomial \npoly, where the coef.cients of poly are represented as a list.  evalPoly-6.00 : Compute the value of \na polynomial (repre\u00adsented as a list) at a given value x.  compBal-stdin-6.00 : Print the values of \nmonthly installment necessary to purchase a car in one year, where the inputs car price and interest \nrate (compounded monthly) are provided from stdin.  compDeriv-6.00x : compDeriv problem from the EdX \nclass.  evalPoly-6.00x : evalPoly problem from the EdX class. oddTuples-6.00x : oddTuples problem from \nthe EdX class. n  iterPower-6.00x : Compute the value m using only the mul\u00adtiplication operator, where \nm and n are integers.  recurPower-6.00x : Compute the value m n using recursion.  iterGCD-6.00x : Compute \nthe greatest common divisor (gcd) of two integers m and n using an iterative algorithm.  hangman1-str-6.00x \n: Given a string secretWord and a list of guessed letters lettersGuessed, return True if all letters \nof secretWord are in lettersGuessed, and False otherwise.  2 http://pexforfun.com/learnbeginningprogramming \n3 AP exams allow high school students in the US to earn college level credit.  hangman2-str-6.00x : \nGiven a string secretWord and a list of guessed letters lettersGuessed, return a string where all letters \nof secretWord that have not been guessed yet (i.e. not present in lettersGuessed) are replaced by the \nletter _ .  stock-market-I(C#) : Given a list of stock prices, check if the stock is stable, i.e. if \nthe price of stock has changed by more than $10 in consecutive days on less than 3 occasions over the \nduration.  stock-market-II(C#) : Given a list of stock prices and a start and end day, check if the \nmaximum and minimum stock prices over the duration from start and end day is less than $20.  restaurant \nrush (C#) : A variant of maximum contiguous subset sum problem.  5.3 Experiments We now present various \nexperiments we performed to evaluate our tool on the benchmark problems. Performance Table 1 shows the \nnumber of student attempts cor\u00adrected for each benchmark problem as well as the time taken by the tool \nto provide the feedback. The experiments were performed on a 2.4GHz Intel Xeon CPU with 16 cores and \n16GB RAM. The experiments were performed with bounds of 4 bits for input integer values and maximum length \n4 for input lists. For each benchmark problem, we .rst removed the student attempts with syntax errors \nto get the Test Set on which we ran our tool. We then separated the attempts which were correct to measure \nthe effectiveness of the tool on the incorrect attempts. The tool was able to provide appropriate corrections \nas feedback for 64% of all incorrect student attempts in around 10 seconds on average. The remaining \n36% of incorrect student attempts on which the tool could not provide feedback fall in one of the following \ncategories: Completely incorrect solutions: We observed many student attempts that were empty or performing \ntrivial computations such as printing strings and variables.  Big conceptual errors: A common error \nwe found in the case of eval-poly-6.00x was that a large fraction of incorrect at\u00adtempts (260/541) were \nusing the list function index to get the index of a value in the list (e.g. see Figure 13(a)), whereas \nthe index function returns the index of .rst occurrence of the value in the list. Another example of \nthis class of error for the hangman2-str problem in shown in Figure 13(b), where the so\u00adlution replaces \nthe guessed letters in the secretWord by _ in\u00adstead of replacing the letters that are not yet guessed. \nThe cor\u00adrection of some other errors in this class involves introducing new program statements or moving \nstatements from one pro\u00adgram location to another. These errors can not be corrected with the application \nof a set of local correction rules.  Unimplemented features: Our implementation currently lacks a few \nof the complex Python features such as pattern matching on list enumerate function and lambda functions. \n Timeout: In our experiments, we found less than 5% of the student attempts timed out (set as 4 minutes). \n Number of Corrections The number of student submissions that require different number of corrections \nare shown in Figure 14(a) (on a logarithmic scale). We observe from the .gure that a signif\u00adicant fraction \nof the problems require 3 and 4 coordinated correc\u00adtions, and to provide feedback on such attempts, we \nneed a technol\u00adogy like ours that can symbolically encode the outcome of different corrections on all \ninput values. Repetitive Mistakes In this experiment, we check our hypothesis that students make similar \nmistakes while solving a given problem. The graph in Figure 14(b) shows the number of student attempts \ncorrected as more rules are added to the error models of the bench\u00admark problems. As can be seen from \nthe .gure, adding a single rule to the error model can lead to correction of hundreds of attempts. This \nvalidates our hypothesis that different students indeed make similar mistakes when solving a given problem. \nGeneralization of Error Models In this experiment, we check the hypothesis that the correction rules \ngeneralize across problems of similar kind. The result of running the compute-deriv error model on other \nbenchmark problems is shown in Figure 14(c). As expected, it does not perform as well as the problem-speci.c \nerror models, but it still .xes a fraction of the incorrect attempts and can be useful as a good starting \npoint to specialize the error model further by adding more problem-speci.c rules. 6. Capabilities and \nLimitations Our tool supports a fairly large subset of Python types and language features, and can currently \nprovide feedback on a large fraction (64%) of student submissions in our benchmark set. In compari\u00adson \nto the traditional test-cases based feedback techniques that test the programs over a few dozens of test-cases, \nour tool typically per\u00adforms the equivalence check over more than 106 inputs. Programs that print the \noutput to console (e.g. compBal-stdin) pose an inter\u00adesting challenge for test-cases based feedback tools. \nSince beginner students typically print some extra text and values in addition to the desired outputs, \nthe traditional tools need to employ various heuris\u00adtics to discard some of the output text to match \nthe desired output. Our tool lets instructors provide correction rules that can optionally drop some \nof the print expressions in the program, and then the tool .nds the required print expressions to eliminate \nso that a student is not penalized much for printing additional values. Now we brie.y describe some of \nthe limitations of our tool. One limitation of the tool is in providing feedback on student attempts \nthat have big conceptual errors (see Section 5.3), which can not be .xed by application of a set of local \nrewrite rules. Correcting such programs typically requires a large global rewrite of the student solution, \nand providing feedback in such cases is an open question. Another limitation of our tool is that it does \nnot take into account structural requirements in the problem statement since it focuses only on functional \nequivalence. For example, some of the assignments explicitly ask students to use bisection search or \nrecursion, but our tool can not distinguish between two functionally equivalent solutions, e.g. it can \nnot distinguish between a bubble sort and a merge sort implementation of the sorting problem. For some \nproblems, the feedback generated by the tool is too low-level. For example, a suggestion provided by \nthe tool in Fig\u00adure 2(d) is to replace the expression poly[e]==0 by False, whereas a higher level feedback \nwould be a suggestion to remove the cor\u00adresponding block inside the comparison. Deriving the high-level \nfeedback from the low-level suggestions is mostly an engineering problem as it requires specializing \nthe message based on the con\u00adtext of the correction. The scalability of the technique also presents a \nlimitation. For some problems that use large constant values, the tool currently replaces them with smaller \nteacher-provided constant values such that the correct program behavior is maintained. We also currently \nneed to specify bounds for the input size, the number of loop un\u00adrollings and recursion depth as well \nas manually provide special\u00adized error models for each problem. The problem of discovering these optimizations \nautomatically by mining them from the large corpus of datasets is also an interesting research question. \nOur tool also currently does not support some of the Python language fea\u00adtures, most notably classes \nand objects, which are required for pro\u00adviding feedback on problems from later weeks of the class.  \n1 def evaluatePoly(poly, x): 2 result = 0 3 for i in list(poly): 3 secretWord = secretWord.replace(letter, \n _ ) 4 result += i*x**poly.index(i) 4 return secretWord 5 return result (a) an evalPoly solution (b) \na hangman2-str solution (a) (b) (c) Figure 14. (a) The number of incorrect problem that require different \nnumber of corrections (in log scale), (b) the number of problems corrected with adding rules to the error \nmodels, and (c) the performance of compute-deriv error model on other problems. Benchmark Median (LOC) \nTotal Attempts Syntax Errors Test Set Correct Incorrect Attempts Generated Feedback Average Time(in s) \nMedian Time(in s) prodBySum-6.00 5 1056 16 1040 772 268 218 (81.3%) 2.49s 2.53s oddTuples-6.00 6 2386 \n1040 1346 1002 344 185 (53.8%) 2.65s 2.54s compDeriv-6.00 12 144 20 124 21 103 88 (85.4%) 12.95s 4.9s \nevalPoly-6.00 10 144 23 121 108 13 6 (46.1%) 3.35s 3.01s compBal-stdin-6.00 18 170 32 138 86 52 17 (32.7%) \n29.57s 14.30s compDeriv-6.00x 13 4146 1134 3012 2094 918 753 (82.1%) 12.42s 6.32s evalPoly-6.00x 15 4698 \n1004 3694 3153 541 167 (30.9%) 4.78s 4.19s oddTuples-6.00x 10 10985 5047 5938 4182 1756 860 (48.9%) 4.14s \n3.77s iterPower-6.00x 11 8982 3792 5190 2315 2875 1693 (58.9%) 3.58s 3.46s recurPower-6.00x 10 8879 3395 \n5484 2546 2938 2271 (77.3%) 10.59s 5.88s iterGCD-6.00x 12 6934 3732 3202 214 2988 2052 (68.7%) 17.13s \n9.52s hangman1-str-6.00x 13 2148 942 1206 855 351 171 (48.7%) 9.08s 6.43s hangman2-str-6.00x 14 1746 \n410 1336 1118 218 98 (44.9%) 22.09s 18.98s stock-market-I(C#) 20 52 11 41 19 22 16 (72.3%) 7.54s 5.23s \nstock-market-II(C#) 24 51 8 43 19 24 14 (58.3%) 11.16s 10.28s restaurant rush (C#) 15 124 38 86 20 66 \n41 (62.1%) 8.78s 8.19s Table 1. The percentage of student attempts corrected and the time taken for \ncorrection for the benchmark problems. 7. Related Work In this section, we describe several related work \nto our technique from the areas of automated programming tutors, automated pro\u00adgram repair, fault localization, \nautomated debugging, automated grading, and program synthesis.    7.1 AI based programming tutors \nThere has been a lot of work done in the AI community for building automated tutors for helping novice \nprogrammers learn program\u00adming by providing feedback about semantic errors. These tutoring systems can \nbe categorized into the following two major classes: Code-based matching approaches: LAURA [1] converts \nteacher s and student s program into a graph based representation and compares them heuristically by \napplying program transforma\u00adtions while reporting mismatches as potential bugs. TALUS [31] matches a \nstudent s attempt with a collection of teacher s algo\u00adrithms. It .rst tries to recognize the algorithm \nused and then ten\u00adtatively replaces the top-level expressions in the student s attempt with the recognized \nalgorithm for generating correction feedback. The problem with these approach is that the enumeration \nof all possible algorithms (with its variants) for covering all corrections is very large and tedious \non part of the teacher. Intention-based matching approaches: LISP tutor [13] cre\u00adates a model of the \nstudent goals and updates it dynamically as the student makes edits. The drawback of this approach is \nthat it forces students to write code in a certain pre-de.ned structure and limits their freedom. MENO-II \n[39] parses student programs into a deep syntax tree whose nodes are annotated with plan tags. This anno\u00adtated \ntree is then matched with the plans obtained from teacher s solution. PROUST [24], on the other hand, \nuses a knowledge base of goals and their corresponding plans for implementing them for each programming \nproblem. It .rst tries to .nd correspondence of these plans in the student s code and then performs matching \nto .nd discrepancies. CHIRON [32] is its improved version in which the goals and plans in the knowledge \nbase are organized in a hierar\u00adchical manner based on their generality and uses machine learning techniques \nfor plan identi.cation in the student code. These ap\u00adproaches require teacher to provide all possible \nplans a student can use to solve the goals of a given problem and do not perform well if the student \ns attempt uses a plan not present in the knowledge base.  Our approach performs semantic equivalence \nof student s at\u00adtempt and teacher s solution based on exhaustive bounded sym\u00adbolic veri.cation techniques \nand makes no assumptions on the al\u00adgorithms or plans that students can use for solving the problem. Moreover, \nour approach is modular with respect to error models; the local correction rules are provided in a declarative \nmanner and their complex interactions are handled by the solver itself.  7.2 Automated Program Repair \nKo\u00a8nighofer et. al. [27] present an approach for automated error lo\u00adcalization and correction of imperative \nprograms. They use model\u00adbased diagnosis to localize components that need to be replaced and then use \na template-based approach for providing corrections using SMT reasoning. Their fault model only considers \nthe right hand side (RHS) of assignment statements as replaceable compo\u00adnents. The approaches in [23, \n41] frame the problem of program repair as a game between an environment that provides the inputs and \na system that provides correct values for the buggy expressions such that the speci.cation is satis.ed. \nThese approaches only sup\u00adport simple corrections (e.g. correcting RHS side of expressions) in the fault \nmodel as they aim to repair large programs with arbi\u00adtrary errors. In our setting, we exploit the fact \nthat we have access to the dataset of previous student mistakes that we can use to con\u00adstruct a concise \nand precise error model. This enables us to model more sophisticated transformations such as introducing \nnew pro\u00adgram statements, replacing LHS of assignments etc. in our error model. Our approach also supports \nminimal cost changes to stu\u00addent s programs where each error in the model is associated with a certain \ncost, unlike the earlier mentioned approaches. Mutation-based program repair [10] performs mutations \nrepeat\u00adedly to statements in a buggy program in order of their suspi\u00adciousness until the program becomes \ncorrect. The large state space of mutants (1012) makes this approach infeasible. Our approach uses a \nsymbolic search for exploring correct solutions over this large set. There are also some genetic programming \napproaches that exploit redundancy present in other parts of the code for .xing faults [5, 14]. These \ntechniques are not applicable in our setting as such redundancy is not present in introductory programming \nprob\u00adlems. 7.3 Automated Debugging and Fault localization Techniques like Delta Debugging [44] and QuickXplain \n[26] aim to simplify a failing test case to a minimal test case that still exhibits the same failure. \nOur approach can be complemented with these techniques to restrict the application of rewrite rules to \ncertain failing parts of the program only. There are many algorithms for fault localization [6, 15] that \nuse the difference between faulty and successful executions of the system to identify potential faulty \nlocations. Jose et. al. [25] recently suggested an approach that uses a MAX-SAT solver to satisfy maximum \nnumber of clauses in a formula obtained from a failing test case to compute potential error locations. \nThese approaches, however, only localize faults for a single failing test case and the suggested error \nlocation might not be the desired error location, since we are looking for common error locations that \ncause failure of multiple test cases. Moreover, these techniques provide only a limited set of suggestions \n(if any) for repairing these faults. 7.4 Computer-aided Education We believe that formal methods technology \ncan play a big role in revolutionizing education. Recently, it has been applied to multiple aspects of \nEducation including problem generation [2, 4, 35] and solution generation [17]. In this paper, we push \nthe frontier forward to cover another aspect namely automated grading. Recently [3] also applied automated \ngrading to automata constructions and used syntactic edit distance like ours as one of the metrics. Our \nwork differs from theirs in two regards: (a) our corrections for programs (which are much more sophisticated \nthan automata) are teacher\u00adde.ned, while [3] considers a small pre-de.ned set of corrections over graphs, \nand (b) we use the Sketch synthesizer to ef.ciently navigate the huge search space, while [3] uses brute-force \nsearch. 7.5 Automated Grading Approaches The survey by Douce et al. [11] presents a nice overview of \nthe systems developed for automated grading of programming assign\u00adments over the last forty years. Based \non the age of these systems, they classify them into three generations. The .rst generation sys\u00adtems \n[21] graded programs by comparing the stored data with the data obtained from program execution, and \nkept track of running times and grade books. The second generation systems [22] also checked for programming \nstyles such as modularity, complexity, and ef.ciency in addition to checking for correctness. The third \ngeneration tools such as RoboProf [9] combine web technology with more sophisticated testing approaches. \nAll of these approaches are a form of test-cases based grading approach and can produce feedback in terms \nof failing test inputs, whereas our technique uses program synthesis for generating tailored feedback \nabout the changes required in the student submission to make it correct.  7.6 Program Synthesis Program \nsynthesis has been used recently for many applications such as synthesis of ef.cient low-level code [29, \n38], data struc\u00adture manipulations [34], inference of ef.cient synchronization in concurrent programs \n[42], snippets of excel macros [18, 33], rela\u00adtional data representations [19, 20] and angelic programming \n[8]. The SKET C H tool [37, 38] takes a partial program and a refer\u00adence implementation as input and \nuses constraint-based reasoning to synthesize a complete program that is equivalent to the reference \nimplementation. In general cases, the template of the desired pro\u00adgram as well as the reference speci.cation \nis unknown and puts an additional burden on the users to provide them; in our case we use the student \ns solution as the template program and teacher s solu\u00adtion as the reference implementation. A recent \nwork by Gulwani et al. [17] also uses program synthesis techniques for automatically synthesizing solutions \nto ruler/compass based geometry construc\u00adtion problems. Their focus is primarily on .nding a solution \nto a given geometry problem whereas we aim to provide feedback on a given programming exercise solution. \n8. Conclusions In this paper, we presented a new technique of automatically pro\u00adviding feedback for introductory \nprogramming assignments that can complement manual and test-cases based techniques. The tech\u00adnique uses \nan error model describing the potential corrections and constraint-based synthesis to compute minimal \ncorrections to stu\u00addent s incorrect solutions. We have evaluated our technique on a large set of benchmarks \nand it can correct 64% of incorrect solu\u00adtions in our benchmark set. We believe this technique can provide \na basis for providing automated feedback to hundreds of thousands of students learning from online introductory \nprogramming courses that are being taught by MITx, Coursera, and Udacity.  9. Acknowledgements This \nwork is supported in part by National Science Foundation grants Expeditions in Computer Augmented Program \nEngineering (ExCAPE NSF-1139056) and NSF-1139056, MIT EECS Super UROP program, Microsoft Research and \na Microsoft Research PhD fellowship. We thank Ned Batchelder, Sarina Canelake, Piotr Mitros, and Victor \nShnayder from the EdX team for their support and for pro\u00adviding us the 6.00x data. We thank the anonymous \nreviewers for their valuable feedback. We also thank the UROP students Tuan Andrew Nguyen, Xola Ntumy, \nand Eugene Oh for their contri\u00adbutions to this project. Finally, we thank Nikolai Tillmann and Jonathan \nde Halleux from the Pex4Fun team at Microsoft Research for providing us the C# data for the initial version \nof our tool. References [1] A. Adam and J.-P. H. Laurent. LAURA, A System to Debug Student Programs. \nArtif. Intell., 15(1-2):75 122, 1980. [2] U. Ahmed, S. Gulwani, and A. Karkare. Automatically generating \nproblems and solutions for natural deduction. In IJCAI, 2013. [3] R. Alur, L. D Antoni, S. Gulwani, D. \nKini, and M. Viswanathan. Automated grading of dfa constructions. In IJCAI, 2013. [4] E. Andersen, S. \nGulwani, and Z. Popovic. A trace-based framework for analyzing and synthesizing educational progressions. \nIn CHI, 2013. [5] A. Arcuri. On the automation of .xing software bugs. In ICSE Companion, 2008. [6] T. \nBall, M. Naik, and S. K. Rajamani. From symptom to cause: localizing errors in counterexample traces. \nIn POPL, 2003. [7] M. S. Bernstein, G. Little, R. C. Miller, B. Hartmann, M. S. Ackerman, D. R. Karger, \nD. Crowell, and K. Panovich. Soylent: a word processor with a crowd inside. In UIST, 2010.  [8] R. Bod\u00edk, \nS. Chandra, J. Galenson, D. Kimelman, N. Tung, S. Barman, and C. Rodarmor. Programming with angelic nondeterminism. \nIn POPL, 2010. [9] C. Daly. Roboprof and an introductory computer programming course. ITiCSE, 1999. [10] \nV. Debroy and W. Wong. Using mutation to automatically suggest .xes for faulty programs. In ICST, 2010. \n[11] C. Douce, D. Livingstone, and J. Orwell. Automatic test-based assess\u00adment of programming: A review. \nJ. Educ. Resour. Comput., 5(3), Sept. 2005. [12] P. Ertmer, J. Richardson, B. Belland, D. Camin, P. Connolly, \n G. Coulthard, K. Lei, and C. Mong. Using peer feedback to enhance the quality of student online postings: \nAn exploratory study. Journal of Computer-Mediated Communication, 12(2):412 433, 2007.  [13] R. G. Farrell, \nJ. R. Anderson, and B. J. Reiser. An interactive computer-based tutor for lisp. In AAAI, 1984. [14] S. \nForrest, T. Nguyen, W. Weimer, and C. L. Goues. A genetic programming approach to automated software \nrepair. In GECCO, 2009. [15] A. Groce, S. Chaki, D. Kroening, and O. Strichman. Error explanation with \ndistance metrics. STTT, 8(3):229 247, 2006. [16] S. Gulwani, S. Srivastava, and R. Venkatesan. Program \nanalysis as constraint solving. In PLDI, 2008. [17] S. Gulwani, V. A. Korthikanti, and A. Tiwari. Synthesizing \ngeometry constructions. In PLDI, 2011. [18] S. Gulwani, W. R. Harris, and R. Singh. Spreadsheet data \nmanipula\u00adtion using examples. In CACM, 2012. [19] P. Hawkins, A. Aiken, K. Fisher, M. C. Rinard, and \nM. Sagiv. Data representation synthesis. In PLDI, 2011. [20] P. Hawkins, A. Aiken, K. Fisher, M. C. Rinard, \nand M. Sagiv. Concur\u00adrent data representation synthesis. In PLDI, 2012. [21] J. B. Hext and J. W. Winings. \nAn automatic grading scheme for simple programming exercises. Commun. ACM, 12(5), May 1969. [22] D. Jackson \nand M. Usher. Grading student programs using assyst. SIGCSE, 1997. [23] B. Jobstmann, A. Griesmayer, \nand R. Bloem. Program repair as a game. In CAV, pages 226 238, 2005. [24] W. L. Johnson and E. Soloway. \nProust: Knowledge-based program understanding. IEEE Trans. Software Eng., 11(3):267 275, 1985. [25] M. \nJose and R. Majumdar. Cause clue clauses: error localization using maximum satis.ability. In PLDI, 2011. \n[26] U. Junker. QUICKXPLAIN: preferred explanations and relaxations for over-constrained problems. In \nAAAI, 2004. [27] R. K\u00f6nighofer and R. P. Bloem. Automated error localization and correction for imperative \nprograms. In FMCAD, 2011. [28] C. Kulkarni and S. R. Klemmer. Learning design wisdom by augment\u00ading physical \nstudio critique with online self-assessment. Technical re\u00adport, Stanford University, 2012. [29] V. Kuncak, \nM. Mayer, R. Piskac, and P. Suter. Complete functional synthesis. PLDI, 2010. [30] G. Little, L. B. Chilton, \nM. Goldman, and R. C. Miller. Turkit: human computation algorithms on mechanical turk. In UIST, 2010. \n[31] W. R. Murray. Automatic program debugging for intelligent tutoring systems. Computational Intelligence, \n3:1 16, 1987. [32] W. Sack, E. Soloway, and P. Weingrad. From PROUST to CHIRON: Its design as iterative \nengineering: Intermediate results are important! In In J.H. Larkin and R.W. Chabay (Eds.), Computer-Assisted \nInstruc\u00adtion and Intelligent Tutoring Systems: Shared Goals and Complemen\u00adtary Approaches., pages 239 \n274, 1992. [33] R. Singh and S. Gulwani. Learning semantic string transformations from examples. PVLDB, \n5, 2012. [34] R. Singh and A. Solar-Lezama. Synthesizing data structure manipula\u00adtions from storyboards. \nIn SIGSOFT FSE, 2011. [35] R. Singh, S. Gulwani, and S. K. Rajamani. Automatically generating algebra \nproblems. In AAAI, 2012. [36] R. Singh, S. Gulwani, and A. Solar-Lezama. Automated semantic grading of \nprograms. CoRR, abs/1204.1751, 2012. [37] A. Solar-Lezama. Program Synthesis By Sketching. PhD thesis, \nEECS Dept., UC Berkeley, 2008. [38] A. Solar-Lezama, R. Rabbah, R. Bodik, and K. Ebcioglu. Program\u00adming \nby sketching for bit-streaming programs. In PLDI, 2005. [39] E. Soloway, B. P. Woolf, E. Rubin, and P. \nBarth. Meno-II: An Intelli\u00adgent Tutoring System for Novice Programmers. In IJCAI, 1981. [40] S. Srivastava, \nS. Gulwani, and J. Foster. From program veri.cation to program synthesis. POPL, 2010. [41] S. S. Staber, \nB. Jobstmann, and R. P. Bloem. Finding and .xing faults. In Correct Hardware Design and Veri.cation Methods, \nLecture notes in computer science, pages 35 49, 2005. [42] M. Vechev, E. Yahav, and G. Yorsh. Abstraction-guided \nsynthesis of synchronization. In POPL, 2010. [43] D. S. Weld, E. Adar, L. Chilton, R. Hoffmann, and E. \nHorvitz. Person\u00adalized online education -a crowdsourcing challenge. In Workshops at the Twenty-Sixth \nAAAI Conference on Arti.cial Intelligence, 2012. [44] A. Zeller and R. Hildebrandt. Simplifying and isolating \nfailure\u00adinducing input. IEEE Transactions on Software Engineering, 28:183 200, 2002.  \n\t\t\t", "proc_id": "2491956", "abstract": "<p>We present a new method for automatically providing feedback for introductory programming problems. In order to use this method, we need a reference implementation of the assignment, and an error model consisting of potential corrections to errors that students might make. Using this information, the system automatically derives minimal corrections to student's incorrect solutions, providing them with a measure of exactly how incorrect a given solution was, as well as feedback about what they did wrong.</p> <p>We introduce a simple language for describing error models in terms of correction rules, and formally define a rule-directed translation strategy that reduces the problem of finding minimal corrections in an incorrect program to the problem of synthesizing a correct program from a sketch. We have evaluated our system on thousands of real student attempts obtained from the Introduction to Programming course at MIT (6.00) and MITx (6.00x). Our results show that relatively simple error models can correct on average 64% of all incorrect submissions in our benchmark set.</p>", "authors": [{"name": "Rishabh Singh", "author_profile_id": "81435592617", "affiliation": "MIT CSAIL, Cambridge, USA", "person_id": "P4148922", "email_address": "rishabh@csail.mit.edu", "orcid_id": ""}, {"name": "Sumit Gulwani", "author_profile_id": "81100315615", "affiliation": "Microsoft Research, Redmond, USA", "person_id": "P4148923", "email_address": "sumitg@microsoft.com", "orcid_id": ""}, {"name": "Armando Solar-Lezama", "author_profile_id": "81100173160", "affiliation": "MIT CSAIL, Cambridge, USA", "person_id": "P4148924", "email_address": "asolar@csail.mit.edu", "orcid_id": ""}], "doi_number": "10.1145/2491956.2462195", "year": "2013", "article_id": "2462195", "conference": "PLDI", "title": "Automated feedback generation for introductory programming assignments", "url": "http://dl.acm.org/citation.cfm?id=2462195"}