{"article_publication_date": "06-16-2013", "fulltext": "\n A General Constraint-centric Scheduling Framework for Spatial Architectures Tony Nowatzki Michael Sartin-Tarm \nLorenzo De Carli Karthikeyan Sankaralingam Cristian Estan* Behnam Robatmili 1 University of Wisconsin-Madison \n*Broadcom Qualcomm Research Silicon Valley (tjn@cs.wisc.edu, msartintarm@wisc.edu, lorenzo@cs.wisc.edu, \nkaru@cs.wisc.edu, cristian@estan.org, behnamr@qti.qualcomm.com) Abstract Specialized execution using \nspatial architectures provides energy ef.cient computation, but requires effective algorithms for spatially \nscheduling the computation. Generally, this has been solved with architecture-speci.c heuristics, an \napproach which suffers from poor compiler/architect productivity, lack of insight on optimality, and \ninhibits migration of techniques between architectures. Our goal is to develop a scheduling framework \nusable for all spatial architectures. To this end, we expresses spatial scheduling as a constraint satisfaction \nproblem using Integer Linear Program\u00adming (ILP). We observe that architecture primitives and scheduler \nresponsibilities can be related through .ve abstractions: placement of computation, routing of data, \nmanaging event timing, managing resource utilization, and forming the optimization objectives. We encode \nthese responsibilities as 20 general ILP constraints, which are used to create schedulers for the disparate \nTRIPS, DySER, and PLUG architectures. Our results show that a general declar\u00adative approach using ILP \nis implementable, practical, and typically matches or outperforms specialized schedulers. Categories \nand Subject Descriptors D.3.4 [Processors]: opti\u00admization, retargetable compilers; G.1.6 [Optimization]: \nInteger programming, Linear programming Keywords Spatial Architectures; Spatial Architecture Schedul\u00ading; \nInteger Linear Programming 1. Introduction Hardware specialization has emerged as an important way to \nsus\u00adtain microprocessor performance improvements to address tran\u00adsistor energy ef.ciency challenges and \ngeneral purpose process\u00ading s inef.ciencies [6, 8, 19, 28]. The fundamental insight of many specialization \ntechniques is to map large regions of com\u00adputation to the hardware, breaking away from instruction-by\u00adinstruction \npipelined execution and instead adopting a spatial architecture paradigm. Pioneering examples include \nRAW [50], Wavescalar [46] and TRIPS [9], motivated primarily by perfor\u00admance, and recent energy-focused \nproposals include Tartan [39], CCA [10], PLUG [13, 35], FlexCore [47], SoftHV [15], MESCAL [31], SPL \n[51], C-Cores [48], DySER [25, 26], BERET [27], and NPU [20]. A fundamental problem in all spatial architectures \nis the 1 Majority of work completed while author was a PhD student at UT-Austin Permission to make digital \nor hard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. PLDI 13, June 16 19, 2013, Seattle, WA, USA. \nCopyright c &#38;#169; 2013 ACM 978-1-4503-2014-6/13/06. . . $15.00 scheduling of computation to the \nhardware resources. Speci.cally, .ve intuitive abstractions in terms of graph-matching describe the scheduling \nproblem: i) placement of computation on the hardware substrate, ii) routing of data on the substrate \nto re.ect and carry out the computation semantics -including interconnection network assignment, network \ncontention, and network path assignment, iii) managing the timing of events in the hardware, iv) managing \nuti\u00adlization to orchestrate concurrent usage of hardware resources, and v) forming the optimization objectives \nto meet the architectural performance goals. Thus far, these abstractions have not been modeled directly, \nand the typically NP-complete (depending on the hardware archi\u00adtecture) spatial architecture scheduling \nproblem is side-stepped. Instead, the focus of architecture-speci.c schedulers has typi\u00adcally been on \ndeveloping polynomial-time algorithms that approx\u00adimate the optimal solution using knowledge about the \narchitecture. Chronologically, this body of work includes the BUG scheduler for VLIW proposed in 1985 \n[17], UAS scheduler for clustered VLIW [41], synchronous data-.ow graph scheduling [7], RAW scheduler \n[36], CARS VLIW code-generation and scheduler [33], TRIPS scheduler [12, 40], Wavescalar scheduler [37], \nand CCA scheduler proposed in 2008 [43]. While heuristic-based approaches are popular and effective, \nthey have three problems: i) poor com\u00adpiler developer/architect productivity since new algorithms, heuris\u00adtics, \nand implementations are required for each architecture, ii) lack of insight on optimality of solution, \nand iii) sandboxing of heuris\u00adtics to speci.c architectures understanding and using techniques developed \nfor one spatial architecture in another is very hard. Considering these problems, others have looked \nat exact mathe\u00admatical and constraint-theory based formulations of the scheduling problem. Table 1 classi.es \nthese prior efforts, which are based on integer linear programming (ILP) or Satis.ability Modulo Theory \n(SMT). They lack in some prominent ways -which perhaps ex\u00adplains why the heuristic-based approaches continue \nto be preferred. In particular, Feautrier [22] is the most related -but it lacks three of .ve abstractions \nrequired, and the static-placement/static-issue of VLIW restrict its applicability to the general problem. \nThese tech\u00adniques and their associated problems serve as the goal and inspi\u00adration for our work, which \nis to develop a declarative, constraint\u00adtheory based universal spatial architecture scheduler. By unifying \nmultiple facets of the related work above, speci.\u00adcally the past experience of architecture-speci.c spatial \nschedulers, the principal of attaining architectural generality, and the mathe\u00admatical power of integer \nlinear programming, we seek to create a solution which allows high developer productivity, provides prov\u00adable \nproperties on results, and enables true architectural generality. Achieving architectural generality \nthrough the .ve scheduling ab\u00adstractions mentioned above is the key novelty of our work. Implementation: \nIn this paper, we use Integer Linear Program\u00adming (ILP) because it allows a high degree of constraint \nexpress\u00adability, can provide strong bounds on the solution s optimality, and has fast commercial solvers \nlike CPLEX, GUROBI, and XPRESS.  Table 1. Related work Legend: i) computation placement ii) data routing \niii) event timing iv) utilization v) optimization objective Speci.cally, we use the GAMS modeling language. \nWe show that a total of 20 constraints specify the problem. We implement these constraints and report \non results for three architectures picked to stress our ILP scheduler in various ways. To stress the \nperformance deliverable by our general ILP approach, we consider TRIPS be\u00adcause it is a mature architecture \nwith sophisticated specialized schedulers resulting from multi-year efforts [1, 9, 12, 40]. To rep\u00adresent \nthe emerging class of energy-centric specialized spatial ar\u00adchitectures, we consider DySER [26]. Finally, \nto demonstrate the generality of our technique, we consider PLUG [13, 35], which uses a radically different \norganization. Respectively, only 3, 1, and 10 additional (and quite straightforward) constraints are \nrequired to handle architecture-speci.c details. We show that standard ILP solvers allows succinct implementation \n(our code is less than 50 lines in GAMS), provide solutions in tractable run-times, and the mappings \nproduced are either competitive with or signi.cantly bet\u00adter than those of specialized schedulers. The \ngeneral and declarative approach allows schedulers to be speci.ed, implemented, evaluated rapidly. Our \nimplementation is provided as an open-source down\u00adload, allowing the community to build upon our work. \nPaper Organization: The next section presents background on the three architectures and an ILP primer. \nSection 3 presents an overview of our approach, Section 4 presents the detailed ILP for\u00admulation, Section \n5 discusses architecture-speci.c modeling con\u00adstraints. Section 6 presents evaluation and Section 7 concludes. \nRe\u00adlated work was covered in the introduction. 2. Spatial Architectures and ILP Primer 2.1 Spatial Architectures \nWe use the term spatial architecture to refer to an architecture in which some subset of the hardware \nresources, namely functional units, interconnection network, or storage, are exposed to the com\u00adpiler, \nwhose job, as part of the scheduling phase, is to map compu\u00adtation and communication primitives in the \ninstruction set architec\u00adture to these hardware resources. VLIW architectures, data.ow ma\u00adchines likes \nTRIPS and Wavescalar, tiled architectures like RAW and PLUG, and accelerators like CCA, SoftHV, and DySER \nall .t this de.nition. We now brie.y describe the three spatial architec\u00adtures we consider in detail, \nand a short primer on ILP. A detailed diagram of all three architectures is in Figure 8 (page 7). The \nTRIPS architecture is organized into 16 tiles, with each tile containing 64 slots, with these slots grouped \ninto sets of eight. The slots from one group are available for mapping one block of code, with different \ngroups used for concurrently executing blocks. The tiles are interconnected using a 2-D mesh network, \nwhich implements dimension-ordered routing and provides support for .ow-control and network contention. \nThe scheduler must perform computation mapping: it takes a block of instructions (which can be no more \nthan 128 instructions long) and assigns each instruction to one of the 16 tiles and within them, to one \nof the 8 slots. The DySER architecture consists of functional units (FUs) and switches, and is integrated \ninto the execution stage of a con\u00adventional processor. Each FU is connected to four neighboring switches \nfrom where it gets input values and injects outputs. The switches allow datapaths to be dynamically specialized. \nUsing a compiler, applications are pro.led to extract the most commonly executed regions, called path-trees, \nwhich are then mapped to the DySER array. The role of the scheduler is to map nodes in the path\u00adtrees \nto tiles in the DySER array and to determine switch con.gu\u00adrations to assign a path for the data-.ow \ngraph edges. There is no hardware support for contention, and some mappings may result in unroutable \npaths. Hence, the scheduler must ensure the mappings are correct, have low latencies and have high throughput. \n The PLUG architecture is designed to work as an accelera\u00adtor for data-structure lookups in network processing. \nEach PLUG tile consists of a set of SRAM banks, a set of no-buffering routers, and an array of statically \nscheduled in-order cores. The only mem\u00adory access allowed by a core is to its local SRAM, which makes \nall delays statically determinable. Applications are expressed as data.ow graphs with code-snippets (the \nPLUG literature refers to them as code-blocks) and memory associated with each node of the graph. Execution \nof programs is data-.ow driven by messages sent from tile to tile -the ISA provides a send instruction. \nThe scheduler must perform computation mapping and network map\u00adping (data.ow edges . networks). It must \nensure there is no con\u00adtention for any network link, which it can do by scheduling when send instructions \nexecute in a code-snippet or adjusting the map\u00adping of graph nodes to tiles. It must also handle .ow-control. \nIn all three architectures, multiple instances of a block, region, or data.ow graph are executing concurrently \non the same hardware, resulting in additional contention and .ow-control. 2.2 ILP Primer Integer Linear \nPrograms (ILP) are algebraic models of systems used for optimization [52]. They are composed of three \nparts: 1) decision variables describing the possible outcomes, 2) linear equations on these variables \ndescribing the set of valid solutions, 3) an objective function which orders solutions by desirability. \nA short tutorial on ILP modeling and solving techniques is here: http://wpweb2.tepper.cmu.edu/fmargot/introILP.html. \n 2.3 Non goals Graph abstractions and ILP (Integer Linear Programming) tech\u00adniques are common in architecture \nand programming languages, and are used for a variety of applications unrelated to spatial scheduling. \nOur goal is not to unify this wide and diverse domain. In the following we discuss a few examples of \nnon-related work and non-goals , highlighting the difference from our techniques. Notably, uses of ILP \nin register allocation, code-generation, and optimization ordering for conventional architectures [32, \n42] are unrelated to the primitives of spatial architecture scheduling. Af.ne loop analysis and resulting \ninstruction scheduling/code-generation for superscalar processors is a popular use of mathematical mod\u00adels \n[2, 5, 44], and since it falls within the data-dependence analysis role of the compiler, not its scheduler, \nis a non-goal for us. In gen\u00aderal, modeling loops or any form of back-edges is meaningless for  Table \n2. Relationship between architectural primitives and scheduler responsibilities. our work because of \nthe nature of our scheduler s role (scheduling happens at a .ner granularity than loops, so we always \ndeal with Directed Acyclic Graphs by design). On the architecture side, ILP and similar optimization \ntheo\u00adries have been used for hardware synthesis and VLSI CAD prob\u00adlems [4, 11, 31]. These techniques \nfocus on taking a .xed com\u00adputational kernel and generating a specialized hardware implemen\u00adtation. This \nis generally accomplished by extending/customizing a well-de.ned hardware pipeline structure. Therefore, \neven if in principle they share some responsibilities with our scheduler, the concrete approach they \ntake differs signi.cantly and cannot be ap\u00adplied to our case. For example, in [4] contention and timing \nissues are avoided by design, by provisioning enough functional units to meet the target latency, and \nby statically adjusting the system to timing constraints. In [31], ILP is used as a sub-step to synthesize \npipelined hardware implementations of loops. Their formulation is speci.c to the problem and system, \nand does not apply to the more general scheduling problem. Finally, performance modeling frame\u00adworks, \nsuch as [38], are also orthogonal to our work. We emphasize that we have cited only a small subset of \nrepresentative literature in the interest of space. 3. Overview We present below the main insights of \nour approach in using constraint-solving for specifying the scheduling problem for spatial architectures. \nWe distill the formulation into .ve responsibilities, each corresponding to one architectural primitive \nof the hardware. For a more general discussion of limitations and concerns related to our approach, see \nthe comments in section 7 (Conclusions). The scheduler for a spatial architecture works at the granularity \nof blocks of code, which could be basic-blocks, hyper-blocks, code-regions, or other more sophisticated \npartitions of program code. These blocks, which we represent as directed acyclic graphs (DAGs) consist \nof computation instructions, control-.ow instruc\u00adtions, and memory access instructions that must be mapped \nto the hardware. We formulate the scheduling problem as spatially map\u00adping a typed computation DAG G \nto a hardware graph H under certain constraints as shown by Figure 1 on page 4. For ease of ex\u00adplanation, \nwe describe G as comprised of vertices and edges, while H is comprised of nodes, routers and links(formal \nde.nitions and details follow in Section 4). To design and implement a general scheduler applicable to \nmany spatial architectures, we observe that .ve fundamental archi\u00adtectural primitives, each with a corresponding \nscheduler responsi\u00adbility, capture the problem as outlined in Table 2 (columns 2 and 3). Implementing \nthese responsibilities mathematically is a mat\u00adter of constraint and objective formulas involving integer \nvariables, which form an ILP model, covered in depth in Section 4. Below we describe the insight connecting \nthe primitives and responsibilities and highlight the mathematical approach. Table 2 summarizes this \ncorrespondence (in columns 2 and 3), and describes these primi\u00adtives for three different architectures. \n Computation HW organization . Placement of computation: The spatial organization of the computational \nresources, which could have a homogeneous or heterogeneous mix of computational units, requires the scheduler \nto provide an assignment of individual operations to hardware locations. As part of this responsibility, \nvertices in G are mapped to nodes in the H graph. Network HW organization . Routing of data: The capabilities \nand organization of the network dictate how the scheduler must handle the mapping of communication between \noperations to the hardware substrate, i.e. the scheduler must create a mapping from edges in G to the \nlinks represented in H. As shown in the 2nd row of Table 2, the network organization consists of the \nspatial layout of the network, the number of networks, and the network routing algorithm. The .ow of \ndata required by the computation block and the placement of operations de.nes the required communication. \nDepending on the architecture, the scheduler may have to select a network for each message, or even select \nthe exact path it takes. Hardware timing/synchronization . Manage timing of events: The scheduler must \ntake into consideration the timing properties of computation and network together with architectural \nrestrictions, as shown in the 3rd row of table 2. In some architectures, the scheduler cannot determine \nthe exact timing of events because it is affected by dynamic factors (e.g. memory latency through the \ncaching hierarchy). For all architectures, the scheduler must have at least a partial view of timing \nof individual operations and individual messages to be able to minimize the latency of the computation \nblock. In some architectures, the scheduler must exert extensive .ne-grained control over timing to achieve \nstatic synchronization of certain events. Concurrent hardware resource usage . Managing Utilization: \nCentral to the dif.culties of the scheduling problem is the concur\u00adrent usage of hardware resources by \nmultiple vertices/edges in G of one node/link in H. We formalize this concurrent usage with a notion \nof utilization, which represents the amount of work a sin\u00adgle hardware resource performs. Such concurrent \nusage (and hence > 1 utilization) can occur within a DAG and across concurrently executing DAGs. Overall, \nthe scheduler must be aware of resource  Figure 1. Example of computation G mapped to hardware H. limits \nin H and which resources can be shared as shown in Table 2 row 4. For example, in TRIPS, within a single \nDAG, 8 instruction\u00adslots share a single ALU (node in H), and across concurrent DAGs, 64 slots share a \nsingle ALU in TRIPS. In both cases, this node\u00adsharing leads to contention on the links as well. Performance \ngoal . Formulate ILP objective: The performance goals of an architecture generally fall into two categories: \nthose which are enforced by certain architectural limitations or abilities, and those which can be in.uenced \nby the schedule. For instance, both PLUG and DySER are throughput engines that try to perform one computation \nper cycle, and any legal schedule will naturally enforce this behavior. For this type of performance \ngoal, the sched\u00aduler relies on the ILP constraints already present in the model. On the other hand, the \nscheduler generally has control over multiple quantities which can improve the performance. This often \nmeans deciding between the con.icting goals of minimizing the latency of individual blocks and managing \nthe utilization among the available hardware resources to avoid creating bottlenecks, which it manages \nby prioritizing optimization quantities. 4. General ILP framework This section presents our general ILP \nformulation in detail. Our formal notation closely follows our ILP formulation in GAMS instead of the \nmore conventional notation often used for graphs in literature. We represent the computation graph as \na set of vertices V , and a set of edges E. The computation DAG, represented by the adjacency matrix \nG(V .E, V .E), explicitly represents edges as the connections between vertices. For example, for some \nv . V and e . E, G(v, e) = 1 means that edge e is an output edge from vertex v. Likewise, G(e, v) = 1 \nsigni.es that e is an input to vertex v. For convenience, lowercase letters represents elements of the \ncorresponding uppercase letters set. We similarly represent the hardware graph as a set of hardware computational \nresource nodes N, a set of routers R which serve as intermediate points in the routing network, and a \nset of L unidi\u00adrectional links which connect the routers and resource nodes. The graph which describes \nthe network organization is given by the ad\u00adjacency matrix H(N.R.L, N .R.L). To clarify, for some l . \nL and n . N, if the parameter H(l, n) was 0, link l would not be an input of node n. Hardware graphs \nare allowed to take any shape, and typically do contain cycles. Terms vertex/edge refer to mem\u00adbers in \nG, and node/link to members in H. Some of the vertices and nodes represent not only computation, but \nalso inputs and outputs. To accommodate this, vertices and nodes are typed by the operations they can \nperform, which also enables the support of general heterogeneity in the architecture. For the treatment \nhere, we abstract the details of the types into a compatibility matrix C(V, N ), indicating whether a \nparticular vertex is compatible with a particular node. When equations depend on speci.c types of vertices, \nwe will refer this set as Vtype . Figure 1 shows an example G graph, representing the compu\u00adtation z \n= (x + y)2, and an H graph corresponding to a sim\u00adpli.ed version of the DySER architecture. Here, triangles \nrepre- Inputs: Computation DAG Description (G) V Set of computation vertices. E Set of Edges representing \ndata .ow of vertices G(V .E , V .E) The computation DAG .(E) Delay between vertex activation and edge \nactivation. .(V ) Duration of vertex. G(E) (PLUG) Delay between vertex activation and edge reception. \nBe Set of bundles which can be overlapped in network. Bv (PLUG only) Set of mutually exclusive vertex \nbundles. B(E.V , Be.Bv) Parameter for edge/vertex bundle membership. P (TRIPS only) Set of control .ow \npaths the computation can take Av(P, V ), Ae(P, E) (TRIPS) Activation matrices de.ning which vertices \nand edges get activated by given path Inputs: Hardware Graph Description (H) N Set of hardware resource \nNodes. R Routers which form the network L Set of unidirectional point-to-point hardware Links H(N.R.L, \nN.R.L) Directed graph describing the Hardware I(L, L) Link pairs incompatible with Dim. Order Routing. \nInputs: Relationship between G/H C(V , N ) Vertex-Node Compatibility Matrix M AX N , M AX L Maximum degree \nof mapping for nodes and links. Variables: Final Outputs Mvn (V , N ) Mapping of computation vertices \nto hardware nodes. Mel(E , L) Mapping of edges to paths of hardware links Mbl(Be, L) Mapping of edge \nbundles to links Mbn(Bv, N ) (PLUG only) Mapping of vertex bundles to nodes d(E) (PLUG) Padding cycles \nbefore message sent. .(E) (PLUG) Padding cycles before message received. Variables: Intermediates O(L) \nThe order a link is traversed in. U(L.N) Utilization of links and nodes. Up(P ) (TRIPS) Max Utilization \nfor each path P . T (V ) Time when a vertex is activated X(E) Extra cycles message is buffered. .(b, \ne) (PLUG) Cycle when e is activated for bundle b LAT Total latency for scheduled computation S V C Service \ninterval for computation. M I S Largest Latency Mismatch. Table 3. Summary of formal notation used. \n sent input/output nodes and vertices, and circles represent compu\u00adtation nodes and vertices. Squares \nrepresent elements of R, which are routers composing the communication network. Elements of E are shown \nas unidirectional arrows in the computation DAG, and elements of L as bidirectional arrows in H representing \ntwo unidi\u00adrectional links in either direction. The scheduler s job is to use the description of the typed \ncom\u00adputation DAG and hardware graph to .nd a mapping from com\u00adputation vertices to computation resource \nnodes and determine the hardware paths along which individual edges .ow. Figure 1 also shows a correct \nmapping of the computation graph to the hardware graph. This mapping is de.ned by a series of constraints \nand vari\u00adables described in the remainder of this Section, and these variables and scheduler inputs are \nsummarized in Table 3. We now describe the ILP constraints which pertain to each scheduler responsibility, \nthen show a diagram capturing this re\u00adsponsibility pictorially for our running example in Figure 1. Responsibility \n1: Placement of computation. The .rst responsibility of the scheduler is to map vertices from the computation \nDAG to nodes from the hardware graph. For\u00admally, the scheduler must compute a mapping from V to N, which \nwe represent with the matrix of binary variables Mvn (V, N ). If Mvn (v, n) = 1, then vertex v is mapped \nto node n, while Mvn (v, n) = 0 means that v is not mapped to n. Each vertex v . V must be mapped to \nexactly one compatible hardware node n . N in accordance with C(v, n). The mapping for incompatible nodes \nmust also be disallowed. This gives us: .v Sn|C(v,n)=1Mvn (v, n) = 1 (1) .v, n|C(v, n) = 0, Mvn (v, \nn) = 0 (2) An example mapping with corresponding assignments to Mvn is shown in Figure 2. Figure 2. \nPlacement of computation Responsibility 2: Routing of data The second responsibility of the scheduler \nis to map the required .ow of data to the communication paths in the hardware. We use a matrix of binary \nvariables Mel(E , L) to encode the mapping of edges to links. Each edge e must be mapped to a sequence \nof one or more links l. This sequence must start from and end at the correct hardware nodes. We constrain \nthe mappings such that if a vertex v is mapped to a node n, every edge e leaving from v must be mapped \nto one link leaving from n. Similarly, every edge arriving to v must be mapped to a link arriving to \nn. .v, e, n|G(v, e), Sl|H(n,l), Mel(e, l) = Mvn (v, n) (3) .v, e, n|G(e, v), Sl|H(l,n), Mel(e, l) = Mvn \n(v, n) (4) In addition, the scheduler must ensure that each edge is mapped to a contiguous path of links. \nWe achieve this by enforcing that for each router, either we have no incoming or outgoing links mapped \nto a given edge, or we have exactly one incoming and exactly one outgoing link mapped to the edge. .e \n. E, r . R Sl|H(l,r), Mel(e, l) = Sl|H(r,l)Mel(e, l) (5) .e . E, r . R Sl|H(l,r), Mel(e, l) = 1 (6) Figure \n3 shows these constraints applied to the example. Figure 3. Routing of data. Some architectures require \ndimension order routing: a message propagating along the X direction may continue on a link along the \nY direction, but a message propagating along the Y direction cannot continue on a link along the X direction. \nTo enforce this re\u00adstriction, we expand the description of the hardware with I(L, L), the set of link \npairs that cannot be mapped to the same edge (i.e. an edge cannot be assigned to a path containing any \nlink pair in this set). '' ' .l, l|I(l, l), e . E , Mel(e, l) + Mel(e, l) = 1 (7) Responsibility 3: \nManage timing of events We capture the timing through a set of variables T (V ) which rep\u00adresents the \ntime at which a vertex v . V starts executing. For each edge connecting the vertices vsrc and vdst, we \ncompute the T (vdst) based on T (vsrc ). This time is affected by three compo\u00adnents. First, we must take \ninto account the .(E), which is the num\u00adber of clock cycles between the start time of the vertex and \nwhen the data is ready. Next is the total routing delay, which is the sum of the number of mapped links \nbetween vsrc and vdest. Since the data carried by all input edges for a vertex might not all arrive at \nthe same time, the variable X(E) describes this mismatch. .vsrc , e, vdest|G(vsrc , e)&#38;G(e, vdest), \nT (vsrc ) + .(e) + Sl.LMel(e, l) + X(e) = T (vdest) (8) The equation above cannot fully capture dynamic \nevents like cache misses. Rather than consider all possibilities, the scheduler simply assumes best-case \nvalues for unknown latencies (alterna\u00adtively, these could be attained through pro.ling or similar means). \nNote that this is an issue for specialized schedulers as well. With the constraints thus far, it is possible \nfor the scheduler to overestimate edge latency be\u00adcause the link mapping allows .c\u00adtitious cycles. As \nshown by the cy\u00adcle in the bottom-left quadrant of Figure 4, the links in this cycle  falsely contribute \nto the time be-Figure 4. Fictitious cycles. tween input x and vertex + . This does not violate constraint \n5 because each router involved con\u00adtains the correct number of incoming / outgoing links. In many architectures, \nrouting constraints (see constraint 7) make such loops impossible, but when this is not the case we elim\u00adinate \ncycles through a new constraint. We add a new set of vari\u00adables O(L), indicating the partial order in \nwhich links activated. If an edge is mapped to two connected links, this constraint enforces that the \nsecond link must be of later order. '' '' .l, l, e . E|H(l, l), O(l) + Mel(e, l) + Mel(e, l) - 1 = O(l) \n(9) Figure 5 shows the intermediate variable assignments that the constraints for timing provide. Figure \n5. Timing of computation and communication. Responsibility 4: Managing Utilization The utilization of \na hardware resource is simply the number of cy\u00adcles for which it can not accept a new unit of work (computation \nor communication) because it is handling work corresponding to an\u00adother computation. We .rst discuss \nthe modeling of link utilization U(L), then discuss node utilization U(N). .l . L, U (l) = Se.E Mel(e, \nl) (10) The equation above models a link s utilization as the sum of its mapped edges and is effective \nwhen each edge takes up a resource. On the other hand, some architectures allow for edges to be overlapped, \nas in the case of multicast, or if it is known that sets of messages are mutually exclusive (will never \nactivate at the same time). This requires us to extend our notion of utilization with the concept of \nedge-bundles, which represent edges that can be mapped to the same link at no cost. The set Be denotes \nedge-bundles, and B(E, Be) de.nes its relationship to edges. The following three constraints ensure the \ncorrect correspondence between the mapping of edges to links and bundles to links, and compute the link \ns utilization based on the edge-bundles. .e, be|B(e, be), l . L, Mbl(be, l) = Mel(e, l) (11) .be . Be, \nl . L, Se.B(e,be)Mel(e, l) = Mbl(be, l) (12) .l . L, U(l) = Sbe.B Mbl(b, l) (13) To compute the vertices \nutilization, we must additionally con\u00adsider the amount of time that a vertex fully occupies a node. This \ntime, .(V ), is always 1 when the architecture is fully pipelined, but increases when the lack of pipelining \nlimits the use of a node n in subsequent cycles. To compute utilization, we simply sum .(V ) over vertices \nmapped to a node: .n . N U(n) = Sv.V .(v)Mvn (v, l) (14) For many spatial architectures we use utilization-limiting \ncon\u00adstraints such as those below. One application of these constraints are hardware limitations in the \nnumber of registers available, in\u00adstruction slots, etc. Also, they ensure lack of contention with op\u00aderations \nor messages from within the same block or other blocks executing concurrently. .l . L, U(l) = M AXL (15) \n.n . N, U(n) = M AXN (16) As shown in the running DySER example below in Figure 6, we limit the utilization \nof each link U(l) to M AXL = 1. This ensures that only a single message per block traverses the link, \nallowing the DySER s arbitration-free routers to operate correctly. Figure 6. Utilization Management. \nResponsibility 5: Optimizing performance The constraints governing the previous sections model the quanti\u00adties \nwhich capture only individual components for correctness and performance. However, the .nal responsibility \nof the scheduler is to manage the overall correctness while providing performance in the context of the \noverall system. In practice, this means that the scheduler must balance notions of latency and throughput. \nHav\u00ading multiple con.icting targets requires strategic resolution, since there is not necessarily a single \nsolution which optimizes both. The strategy we take is to supply to the scheduler a set of variables \nto optimize for with their associated priority. To calculate the critical path latency, we .rst initialize \nthe input vertices to zero (or some known value) then .nd the maximum latency of an output vertex LAT \n. This represents the scheduler s estimate of how long the block would take to complete. .v . Vin, T \n(v) = 0 (17) .v . Vout, T (v) = LAT (18) To model the throughput aspects, we utilize the concept of \nthe service interval SV C , which is de.ned as the minimum number of cycles between successive invocations \nwhen no data dependen\u00adcies between invocations exists. We compute SV C by .nding the maximum utilization \non any resource. .n . N, U(n) = S V C (19) .l . L, U(l) = S V C (20) For fully pipelined architectures, \nS V C is naturally forced to 1, so it is not an optimization target. Other notions of throughput are \npossible, as in the case of DySER, where minimizing the latency mismatch M I S is the throughput objective \n(see Section 5.2). For our running example, the .nal solution is shown in Figure 7, where the critical \npath latency LAT and the latency mismatch M I S (mentioned above), are both optimized by the scheduler. \n Figure 7. Optimizing performance. 5. Architecture-speci.c modeling In this section, we describe how \nthe general formulation pre\u00adsented above is used by three diverse architectures. Figure 8 shows schematics \nand H graphs for the three architectures. 5.1 Architecture-speci.c details for TRIPS Computation organization \n. Placement of computation: Fig\u00adure 8 depicts the graph H we use to describe a 4-tile TRIPS archi\u00adtecture. \nA tile in TRIPS is comprised of nodes n . N denoting a functional unit in the tile and r . R representing \nits router -the two are connected with one link in either direction. The router also connects to the \nrouters in the neighboring tiles. The functional unit has a self-loop that denotes the bypass of the \ntile s router to move results into input slots for operations scheduled on the same tile. Network organization \n. Routing data: Since messages are ded\u00adicated and point-to-point (as opposed to multicast), we use con\u00adstraints \nmodeling each edge as consuming a resource and con\u00adtributing to the total utilization. The TRIPS routers \nimplement dimension-order routing, i.e. messages .rst travel along the X axis, then along the Y axis. \nTRIPS uses the I(L, L) parameter, which disallows the mapping of certain link pairs, to invalidate any \npaths which are not compatible with dimension-order. HW timing . Managing timing of events: We can calculate \nnetwork timing without any additions to the general formulation. Concurrent HW usage . Utilization: TRIPS \nallows signi.cant level of concurrent hardware usage which affects both the latency and throughput of \nblocks. Speci.cally, the maximum number of vertices per node is M AXN = 8. The utilization on links is \nused to .nally formulate the objective function. Extensions: For TRIPS, the scheduler must also account \nfor control .ow when computing the utilization and ultimately the service interval for throughput. Simple \nextensions, as explained below, can in general handle control .ow for any architecture and could  Figure \n8. Three candidate architectures and corresponding H graphs, considering 4 tiles for each architecture. \n Table 4. Description of ILP model implementation for PLUG belong in the general ILP formulation as well. \nLet P be the set of control .ow paths that the computation can take through G. Note that p . P is not \nactually a path through G, but the subset of its vertices and edges activated in a given execution. Let \nAv(P, V ) and Ae(P, E) be the activation matrices de.ning, for each vertex and edge of the computation, \nwhether they are activated when a given path is taken or not. For each path we de.ne the maximum utilization \non this path Wp(P ). These constraints are similar to the original utilization constraints (10, 14), \nbut also take control .ow activation matrices into account. .l . L, p . P, Se.E Mel(e, l)Ae(p, e) = Wp(p) \n(21) .n . N, p . P, Sv.V Mvn (v, n).(v)Av(p, v) = Wp(p)(22) And an additional constraint for calculating \noverall service interval: SV C = Sp.P Wp(p) (23) Note that this heuristic provides the same importance \nto all control-.ow paths. With pro.ling or other analysis, differential weighting can be implemented. \nObjective formulation: For the TRIPS architecture, we empiri\u00adcally found that optimizing for throughput \nis of higher importance, in most cases, then for latency. Therefore, our strategy is to .rst minimize \nthe SV C , add the lowest value as a constraint, and then optimize for LAT . The following is our solution \nprocedure, where numbers refer to constraints from the formulation: min SV C s.t. [ 1, 2, 3, 4, 5, 6, \n7, 8, 10, 14, 15, 16, 17, 18, 21, 22, 23] min LAT s.t. [ 1, 2, 3, 4, 5, 6, 7, 8, 10, 14, 15, 16, 17, \n18, 21, 22, 23]  5.2 Architecture-speci.c details for DySER Computation organization . Placement of \ncomputation: We model DySER with the hardware graph H shown in Figure 8; heterogeneity is captured with \nthe C(V, N ) compatibility matrix. Network organization . Routing data: We use bundle-link map\u00adping constraints \nto model multicast, and constraint 9 to prevent .c\u00adtitious cycles. Since the network has the ability \nto perform multi\u00adcast messages and can route multiple edges on the same link, we use the bundle-link \nmapping constraints. Since there is no ordering constraint on the network, we need to prevent .ctitious \ncycles. HW timing . Managing timing of events: No additions to the general formulation are required. \nConcurrent HW usage . Utilization: Since DySER can only route one message per link, and max one vertex \nto a node, both M AXL and M AXN are set to 1. Objective Formulation: DySER throughput can be as much \nas one computation G per cycle, since the functional units themselves are pipelined. However, throughput \ndegradation can occur because of the limited buffering available for messages. The utilization de.ned \nin the general framework does not capture this problem because it only measures the usage of functional \nunits and links, not of buffers. Unlike TRIPS, where all operands are buffered as long as needed in named \nregisters, DySER buffers messages at routers and at most one message per edge is buffered at each router. \nand SV C = SV Coptimal Thus, two paths that diverge and then converge, but have different lengths, will \nalso have different amounts of buffering. Combined with backpressure, this can reduce throughput. Computing \nthe exact throughput achievable by a DySER sched\u00adule is dif.cult, as multiple such pairs of converging \npaths may exist -even paths that converge after passing through functional units affect throughput. Instead \nwe note that latency mismatches always manifest themselves as extra buffering delays X(e) for some edges, \nso we model latency mismatch as M I S : .e . E , X (e) = M I S (24) Empirically, we found that external \nlimitations on the through\u00adput of inputs is greater than that of computation. For this reason, the DySER \nscheduler .rst optimizes for latency, adds the latency of the solution as a constraint, then optimizes \nfor throughput by minimizing latency mismatch M I S , as below: min LAT s.t. [ 1, 2, 3, 4, 5, 6, 8, 9, \n11, 12, 13, 14, 15, 16, 17, 18, 24] min M I S s.t. [ 1, 2, 3, 4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, \n17, 18, 24] and LAT = LAToptimal  5.3 Architecture-speci.c details for PLUG The PLUG architecture is \nradically different from the previous two architectures since all decisions are static. Our formulation \nis gen\u00aderal enough that it works for PLUG with only 10 predominantly simple additional constraints. In \nthe interest of clarity, we summa\u00adrize the key concepts of the PLUG architecture, corresponding ILP model, \nand additional equations in Table 4. The grayed rows sum\u00admarize the extensions, and this section s text \ndescribes them. Computation organization . Placement of computation: See Table 4, row 1. No additional \nconstraints required. Network organization . Routing data: See Table 4, row 2. Additional constraints: \nMulticast handled with edge-bundles: Let Bmulti . Be be the subset of edge-bundles that involve multicast \nedges. The following constraint, which considers links through a router to a node, then enforces that \nthe bundle mapped to the router link must also be mapped to the node s incoming link. ' '' .b . Bmulti, \nl, r, l , n|H(l, r)&#38;H(r, l )&#38;H(l , n), Mbl(b, l) = Mbl(b, l ' ) (25) HW timing . Managing timing \nof events: See Table 4, row 3. Additional constraints: We need to handle the timing of send in\u00adstructions. \nWe use .(E) and the newly-introduced G(E) to re\u00adspectively indicate the relative cycle number of the \ncorresponding send instruction and use instruction. Network contention is avoided by code-scheduling \nthe send instructions with NOP padding to create appropriate delays and equalize all delay mismatch. \nd(E) denotes sending delay added, and .(E) denotes receiving delay added. To model the timing for PLUG, \nwe augment equation 8 as follows: .vsrc , e, vdst|G(vsrc , e)&#38;G(e, vdst), T (vsrc )+Sl.LMel(e, l)+.(e)+d(e)=T \n(vdst)+G(e)+.(e) (26) Because the insertion of no-ops can only change timing in speci.c ways, we use \ntwo constraints to further link d(E) and .(E) to .(E) and G(E). The .rst ensures that the scheduler never \nattempts to pad a negative number of NOPs. The second ensures that sending delay d(E) is the same for \nall multicast edges carrying the same message. To implement these constraints we use the following 4 \nsets con\u00adcerning distinct edges e, e ' : S I (e, e ' ) has the set of pairs of edges arriving to the \nsame vertex such that G(e) < G(e ' ), LI F O(e, e ' ) has, for each vertex with both input and output \nedges, the last in\u00adput edge e and the .rst output edge e ' , SO(e, e ' ) has the pairs of output edges \nwith the same source vertex such that .(e) < .(e ' ), and E QO(e, e ' ) has the pairs of output edges \nleaving the same node concurrently. '' ' .e, e |SI (e, e ),.(e) = .(e ) (27a) '' ' .e, e |LI F O(e, \ne ),.(e) = d(e ) (27b) '' ' .e, e |SO(e, e ),d(e) = d(e ) (27c) '' ' .e, e |E QO(e, e ),d(e) = d(e ) \n(28) Concurrent HW usage . Utilization: See Table 4, row 4. Additional constraints: PLUG groups nodes \nin G into super\u00adnodes (logical-page), and programmatically only a single node executes in every super-node. \nThis mutual exclusion behavior is modeled by partitioning V into a set of vertex bundles Bv with B(V \n, Bv) indicating to which bundle a vertex v . V belongs. We introduce Mbn(b, n) to model the mapping \nof bundles to nodes, enforced by the following constraints: .v, bv|B(v, bv), n . N, Mbn(bv, n)=Mvn (v, \nn) (29) .bv . Bv, n . N, Sv.B(v,bv)Mvn (v, n)=Mbn(bv, n) (30) We then de.ne the utilization based on \nthe number of vertex bundles mapped to a node. We also instantiate edge bundles be for all the set of \nedges coming from the same vertex bundle and going to the same destination. Since all the edges in such \na bundle are logically a single message source, the schedule must equalize the receiving times of the \nmessage they send. Let Bmutex . Be be the set of edge-bundles described above. Then we add the following \ntiming constraint: .e, e ' , bx . Bmutex|B(e, bx)&#38;B(e ' , bx), .(e) = .(e ' ) (31) Additionally, \narchitectural constraints require the total length in instructions of the vertex bundles mapped to the \nsame node to be = 32. This requires de.ning, for each bundle, the maximum bundle length .(bv) as a function \nof the last send message of the vertex. This length can then be constrained to be = 32. To achieve this, \nwe .rst de.ne the set LAST (Bv, Be), which pairs each vertex bundle with its last edge bundle, corresponding \nto the last send message of the vertex. This enables to de.ne the maximum bundle length .(bv) as: .e, \nbe, bv|LAST (bv, be)&#38;B(e, be), .(e) + d(e) = .(bv) (32) We .nally de.ne Q(Bv, N ) as the required \nnumber of instruc\u00adtions on node n from vertex bundle bv and limit it to 32 (the code\u00adsnippet length). \n.bv, n . N, Q(bv, n)-32 * Mbn(bv, n)=.(bv)-32 (33) .n, Sbv.Bv Q(bv, n) = 32 (34) Objective Formulation: \nFor PLUG, the smallest service interval is achieved and enforced for any legal schedule, and we optimize \nsolely for latency LAT . min LAT s.t. [1, 2, 3, 4, 5, 6, 7, 11, 12, 13, 14, 15, 16, 17, 18, 25, 26, 27, \n28, 29, 30, 31, 32, 33, 34]  6. Implementation and Evaluation In this section, we describe our implementation \nof the constraints in an off-the-shelf ILP solver and evaluate its performance compared to native specialized \nschedulers for the three architectures. 6.1 Implementation We use the GAMS modeling language to specify \nour constraints as mixed integer linear programs, and we use the commercial CPLEX solver to obtain the \nschedules. Our implementation strategy for  Table 5. Tools and methodology for quantitative evaluation \n frontend : passes in the compiler that produce pre-scheduled code; backend : passes that convert scheduled \ncode into binary. Figure 9. Implementation of our ILP scheduler. Dotted boxes indicate the new components \nadded. prioritizing multiple variables follows a standard approach: we de.ne an allowable percentage \noptimization gap (of between 2% to 10%, depending on the architecture), and optimize for each variable \nin prioritized succession, .nishing the solver when the percent gap is within the speci.ed bounds. After \n.nding the optimal value for each variable, we add a constraint which restricts that variable to be no \nworse in future iterations. Figure 9 shows our implementation and how we integrated with the compiler/simulator \ntoolchains [1, 13, 25]. For all three architectures, we use their intermediate output converted into \nour standard directed acyclic graph (DAG) for G and fed to our GAMS ILP program. We speci.ed H for each \narchitecture. To evaluate our approach, we compare the performance of the .nal binaries on the architectures \nvarying only the scheduler. Table 5 summarizes the methodology and infrastructure used.  6.2 Results \nIs this ILP-based approach implementable? Yes, it is possible to express the scheduling problem as an \nILP problem and implement it for real architectures. Considering the ILP constraint formulation for the \ngeneral framework, our GAMS implementation is around 50 lines of code. Result-1: A declarative and general \napproach to expressing and implementing spatial-architecture schedulers is possible. Is the execution \ntime of standard ILP-solvers fast enough to be practical? Table 6 (page 10) summarizes the mathematical \nchar\u00adacteristics of the workloads and corresponding scheduling behav\u00adior. The three right-hand columns \nrespectively show the number of software nodes to schedule, the amount of single ILP equations cre\u00adated, \nand the solver time.2 There is a rough correlation between the workload size and scheduling time, but \nit is still highly variable. The solver time of the specialized schedulers in comparison is typically \non the order of seconds or less. Although some blocks may take minutes to solve, these times are still \ntractable, demon\u00adstrating the practicality of ILP as a scheduling technique. Result-2: Our general ILP \nscheduler runs in tractable time. 2 For TRIPS, the per-benchmark number of DAGs can range from 50 to \n5000, and the metrics provided are average per DAG. For DySER, #DAGs is 1 to 4 per benchmark, and PLUG \nis always 1. Are the output solutions produced good? How do they compare against the output of specialized \nschedulers? Figure 10 (page 10) shows the performance of our ILP scheduler. It shows the cycle\u00adcount \nreduction for the executed programs as a normalized percent\u00adage of the program produced by the specialized \ncompiler (higher is better, negative numbers mean execution time was increased). We discuss these results \nin terms of each architecture. Compared to the TRIPS SPS specialized scheduler (a cumulated multi-year \neffort spanning several publications [9, 12, 40]), our ILP scheduler performs competitively as summarized \nbelow.3 Compared to SPS (a)Better on 22 of 43 benchmarks up to 21% GM +2.9% (b)Worse on 18 of 43 benchmarks \nwithin 4.9% GM -1.9% (typically 2%) (c)5.4%, 6.04%, and 13.2% worse on ONLY 3 benchmarks Compared to \nGRST Consistently better, up to 59% better; GM +30%  Groups (a) and (b) show the ILP scheduler is capturing \nthe architecture/scheduler interactions well. The small slowdown\u00ads/speedups compared to SPS are due to \ndynamic events which disrupt the scheduler s view of event timing, making its node/link assignments sub-optimal, \ntypically by only 2%. After detailed anal\u00adysis, we discovered the reason for the performance gap of group \n(c) is the lack of information that could be easily integrated in our model. First, the SPS scheduler \ntook advantage of information regarding the speci.c cache banks of loads and stores, which is not available \nin the modular scheduling interface exposed by the TRIPS compiler. This knowledge would improve the ILP \nsched\u00aduler s performance and would only require changes to the compat\u00adibility matrix C(V , N ). Second, \nknowledge of limited resources was available to SPS, allowing it to defer decisions and interact with \ncode-generation to map movement-related instructions. What these results show overall is that our .rst-principles \nbased approach is capturing all the architecture behavior in a general fashion and ar\u00adguably aesthetically \ncleaner fashion than SPS s indirect heuristics. Our ILP scheduler consistently exceeds by appreciable \namounts a previous generation TRIPS scheduler, GRST, that did not model contention [40], as shown by \nthe hatched bars in the .gure. On DySER, the ILP scheduler outperforms the specialized scheduler on all \nbenchmarks, as shown in Figure 10, for a 64\u00adunit DySER. Across the benchmarks, the ILP scheduler reduces \nindividual block latencies by 38% on average. When the latency of DySER execution is the bottleneck, \nespecially when there are dependencies between instances of the computation (like the nee\u00addle benchmark), \nthis leads to signi.cant speedup of up to 15%. We 3 We did not run on SPEC benchmarks for three reasons: \nprior TRIPS scheduler work uses this set; TRIPS simulator does not have sim-point etc. to meaningfully \nsimulate TRIPS benchmarks; TRIPS compiler does not produce good enough code on SPEC (10-15 inst blocks \nonly) to make scheduler a factor [12, 23]. Using TRIPS hardware was impractical for us.  Table 6. Benchmark \ncharacteristics and ILP scheduler behavior. also implemented an extra DySER benchmark, which elucidates \nthe importance of latency mismatch and is described in Table 5. The specialized scheduler tries to minimize \nthe extra path length at each step, exacerbating the latency mismatch of the short and long paths in \nthe program. The ILP scheduler, on the other hand, pads the length of the shorter path to reduce latency \nmismatch, increasing the potential throughput and achieving a 4.2\u00d7 improvement over the specialized scheduler. \nFinally, we also compared to manually scheduled code on an 16-unit DySER (since hand-scheduling for 64unit \nDySER is exceedingly tedious). The ILP scheduler always matched or out-performed it by a small (< 2%) \npercentage. The ILP scheduler matches or out-performs the PLUG hand\u00admapped schedules. It is able to both \n.nd schedules that force SV C = 1 and provide latency improvements of a few percent. Of particular note \nis solver time, because PLUG s DFGs are more complex. In fact, each DFG represents an entire application. \nThe most complex benchmark, IPV4, contains 74 edges (24 more than any others) split between 30 mutually \nexclusive or multicast groups. Despite these dif.culties, it completes in tractable time. Result-3: Our \nILP scheduler outperforms or matches the perfor\u00admance of specialized schedulers.  7. Discussion and \nConclusions Scheduling is a fundamental problem for spatial architectures, which are increasingly used \nto address energy ef.ciency. Com\u00adpared to the architecture-speci.c schedulers, which are the current \nstate-of-the-art, this paper provides a general formulation of spatial scheduling as a constraint-solving \nproblem. We applied this for\u00admulation to three diverse architectures, ran them on a standard ILP solver, \nand demonstrated such a general scheduler outperforms or matches the respective specialized schedulers. \nSome potential lim\u00aditations and concerns about our work are outlined in Table 7, but do not detract from \nits central contributions. We conclude with a discussion of some broader extensions and implications \nof our work. Speci.cally, we discuss the possibility of improving the scheduling time through algorithmic \nspecializa\u00adtion, and how our scheduler delivers on its promises of compiler\u00addeveloper productivity/extensibility, \ncross-architecture applicabil\u00adity, and insights on optimality. Specializing ILP Solvers: While the bene.ts \nof using Integer Lin\u00adear Programming come at the cost of additional scheduler execu\u00adtion time, we suspect \nthat there may be further opportunities for improvement. One strategy is to specialize the solver s algorithms \nto the problem domain. The Network Simplex algorithm for the minimum cost .ow problem is a widely known \nexample. To create a specialized algorithm for our problem, a detailed investigation of the constraints \nfrom a solver design perspective would be required, as well as the modi.cation of existing ILP solvers. \nThis is one crit\u00adical direction for future work.  Table 8. Applicability to other Spatial Architectures \nFormulation Extensibility: In our experience, our model formula\u00adtion was easily adaptable and extensible \nfor modeling various prob\u00adlem variations or optimizations. For example, we improved upon our TRIPS scheduler \ns performance by identifying blocks with carried-loop cache dependencies (commonly the most frequently \nexecuted), and extended our formulation to only optimize for rele\u00advant paths. Application to Example \nArchitectures: Table 8 shows how our framework could be applied to three other systems. For both WaveScalar \nand RAW, we can attain optimal solutions by refraining from making early decisions, essentially avoiding \nthe drawbacks of multi-stage solutions. For WaveScalar, our scheduler would con\u00adsider all levels of the \nnetwork hierarchy at once, using different latencies for links in different networks. For RAW, our scheduler \nwould consider both the partitioning of instructions into streams, and the spatial placement of these \ninstructions simultaneously. As a more recent example, NPU [20] is a statically-scheduled architecture \nlike PLUG, but uses a broadcast network instead of a point-to-point, tiled network. Instead of using \nthe routing equations for communication, the NPU bus is more aptly modeled as a com\u00adputation type. Timing \nwould be modeled similarly to PLUG, where no-ops prevent bus contention, allowing a fully static schedule. \nInsights on Optimality: Since our approach provides insights on optimality, it has potentially broader \nuses as well. For instance, in the context of a dynamic compilation framework, even though the compilation \ntime of seconds is impractical, the ILP scheduler still has signi.cant practical value it enables developers \nto easily for\u00admulate and evaluate objectives that can guide the implementation of specialized heuristic \nschedulers. Revisiting NPU scheduling, we can observe another potential use of ILP models, speci.cally \nin designing the hardware itself. For the NPU, the .fo depth of each processing element is expensive \nin terms of hardware, so we could easily extend the model to calculate the .fo depth as a function of \nthe schedule. One strategy would be to .rst optimize for performance, then .x the performance and optimize \nfor lowest maximum .fo depth. Doing this across a set of benchmarks would give the best lower-bound .fo \ndepth which does not sacri.ce performance. Finally, while our approach is general, in that we have demon\u00adstrated \nimplementations across three disparate architectures and shown extensions to others, a somewhat open \nquestion remains on universality : what spatial architecture organization could render our framework \nineffective? This is subject to debate and is future work. Overall, our general scheduler can form an \nimportant com\u00adponent for future spatial architectures. Acknowledgments We thank the anonymous reviewers \nfor comments. Thanks to Mark Hill and Daniel Luchaup for their valuable insights and comments on the \npaper. Support for this research was provided by NSF un\u00adder the following grants: CCF-0845751, CNS-0917213, \nand CNS\u00ad0917238. References [1] Trips toolchain, http://www.cs.utexas.edu/ trips/dist/. [2] A. V. Aho, \nM. S. Lam, R. Sethi, and J. D. Ullman. Compilers: Principles, Techniques, and Tools. [3] S. Amarasinghe, \nD. R. Karger, W. Lee, and V. S. Mirrokni. A theoret\u00adical and practical approach to instruction scheduling \non spatial archi\u00adtectures. Technical report, MIT, 2002. [4] S. Amellal and B. Kaminska. Functional synthesis \nof digital systems with tass. Computer-Aided Design of Integrated Circuits and Systems, IEEE Transactions \non, 13(5):537 552, 1994. [5] C. Ancourt and F. Irigoin. Scanning polyhedra with do loops. In PPOPP 1991. \n[6] O. Azizi, A. Mahesri, B. C. Lee, S. J. Patel, and M. Horowitz. Energy\u00adperformance tradeoffs in processor \narchitecture and circuit design: a marginal cost analysis. In ISCA 2010. [7] S. S. Battacharyya, E. A. \nLee, and P. K. Murthy. Software Synthesis from Data.ow Graphs. Kluwer Academic Publishers, 1996. [8] \nS. Borkar and A. A. Chien. The future of microprocessors. Commun. ACM, 54(5):67 77, 2011. [9] D. Burger, \nS. W. Keckler, K. S. McKinley, M. Dahlin, L. K. John, C. Lin, C. R. Moore, J. Burrill, R. G. McDonald, \nW. Yoder, and the TRIPS Team. Scaling to the end of silicon with EDGE architectures. IEEE Computer, 37(7):44 \n55, 2004. [10] N. Clark, M. Kudlur, H. Park, S. Mahlke, and K. Flautner. Application\u00adspeci.c processing \non a general-purpose core via transparent instruc\u00adtion set customization. In MICRO 2004. [11] J. Cong, \nK. Gururaj, G. Han, and W. Jiang. Synthesis algorithm for application-speci.c homogeneous processor networks. \nIEEE Trans. Very Large Scale Integr. Syst., 17(9), Sept. 2009. [12] K. Coons, X. Chen, S. Kushwaha, K. \nS. McKinley, and D. Burger. A Spatial Path Scheduling Algorithm for EDGE Architectures. In ASPLOS 2006. \n[13] L. De Carli, Y. Pan, A. Kumar, C. Estan, and K. Sankaralingam. Plug: Flexible lookup modules for \nrapid deployment of new protocols in high-speed routers. In SIGCOMM 2009. [14] L. de Moura and N. Bj\u00f8rner. \nZ3: An ef.cient SMT solver. In TACAS, 2008. [15] A. Deb, J. M. Codina, and A. Gonzales. Softhv: A hw/sw \nco-designed processor with horizontal and vertical fusion. In International Confer\u00adence on Computing \nFrontiers 2011. [16] A. E. Eichenberger and E. S. Davidson. Ef.cient formulation for optimal modulo schedulers. \nIn PLDI 1997. [17] J. R. Ellis. Bulldog: a compiler for vliw architectures. PhD thesis, 1985. [18] D. \nW. Engels, J. Feldman, D. R. Karger, and M. Ruhl. Parallel processor scheduling with delay constraints. \nIn SODA 2001. [19] H. Esmaeilzadeh, E. Blem, R. S. Amant, K. Sankaralingam, and D. Burger. Dark Silicon \nand the End of Multicore Scaling. In ISCA 2011. [20] H. Esmaeilzadeh, A. Sampson, L. Ceze, and D. Burger. \nNeural accel\u00aderation for general-purpose approximate programs. In MICRO 2012. [21] K. Fan, H. h. Park, \nM. Kudlur, and S. o. Mahlke. Modulo scheduling for highly customized datapaths to increase hardware reusability. \nIn CGO 2008. [22] P. Feautrier. Some ef.cient solutions to the af.ne scheduling problem. International \nJournal of Parallel Programming, 21:313 347, 1992. [23] M. Gebhart, B. A. Maher, K. E. Coons, J. Diamond, \nP. Gratz, M. Marino, N. Ranganathan, B. Robatmili, A. Smith, J. Burrill, S. W. Keckler, D. Burger, and \nK. S. McKinley. An evaluation of the trips computer system. In ASPLOS 2009. [24] G. J. Gordon, S. A. \nHong, and M. Dud\u00b4ik. First-order mixed integer linear programming. In UAI 2009. [25] V. Govindaraju, \nC.-H. Ho, T. Nowatzki, J. Chhugani, N. Satish, K. Sankaralingam, and C. Kim. Dyser: Unifying functionality \nand parallelism specialization for energy ef.cient computing. IEEE Mi\u00adcro, 33(5), 2012. [26] V. Govindaraju, \nC.-H. Ho, and K. Sankaralingam. Dynamically spe\u00adcialized datapaths for energy ef.cient computing. In \nHPCA 2011. [27] S. Gupta, S. Feng, A. Ansari, S. Mahlke, and D. August. Bundled execution of recurring \ntraces for energy-ef.cient general purpose pro\u00adcessing. In MICRO 2011. [28] N. Hardavellas, M. Ferdman, \nB. Falsa., and A. Ailamaki. Toward dark silicon in servers. IEEE Micro, 31(4):6 15, 2011. [29] J. N. \nHooker. Logic, optimization and constraint programming. IN-FORMS Journal on Computing, 14:295 321, 2002. \n[30] J. N. Hooker and M. A. Osorio. Mixed logical-linear programming. Discrete Appl. Math., 96-97(1), \nOct. 1999. [31] Z. Huang, S. Malik, N. Moreano, and G. Araujo. The design of dy\u00adnamically recon.gurable \ndatapath coprocessors. ACM Trans. Embed. Comput. Syst., 3(2):361 384, May 2004. [32] R. Joshi, G. Nelson, \nand K. Randall. Denali: a goal-directed superop\u00adtimizer. In PLDI 2002. [33] K. Kailas and A. Agrawala. \nCars: A new code generation framework for clustered ilp processors. In HPCA 2001. [34] M. Kudlur and \nS. Mahlke. Orchestrating the execution of stream programs on multicore platforms. In PLDI 2008. [35] \nA. Kumar, L. De Carli, S. J. Kim, M. de Kruijf, K. Sankaralingam, C. Estan, and S. Jha. Design and implementation \nof the plug architec\u00adture for programmable and ef.cient network lookups. In PACT 2010. [36] W. Lee, \nR. Barua, M. Frank, D. Srikrishna, J. Babb, V. Sarkar, and S. Amarasinghe. Space-time scheduling of instruction-level \nparal\u00adlelism on a raw machine. In ASPLOS 1998. [37] M. Mercaldi, S. Swanson, A. Petersen, A. Putnam, \nA. Schwerin, M. Oskin, and S. J. Eggers. Instruction scheduling for a tiled data.ow architecture. In \nASPLOS 2006. [38] M. Mercaldi, S. Swanson, A. Petersen, A. Putnam, A. Schwerin, M. Oskin, and S. J. \nEggers. Modeling instruction placement on a spatial architecture. In SPAA 2006. [39] M. Mishra, T. J. \nCallahan, T. Chelcea, G. Venkataramani, M. Budiu, and S. C. Goldstein. Tartan: Evaluating spatial computation \nfor whole program execution. In ASPLOS 2006. [40] R. Nagarajan, S. K. Kushwaha, D. Burger, K. S. McKinley, \nC. Lin, and S. W. Keckler. Static placement, dynamic issue (spdi) scheduling for edge architectures. \nIn PACT 2004. \u00a8a new approach to scheduling for clustered register .le microarchitec\u00adtures. In MICRO \n31. [41] E. Ozer, S. Banerjia, and T. M. Conte. Uni.ed assign and schedule: [42] J. Palsberg and M. \nNaik. Ilp-based resource-aware compilation, 2004. [43] H. Park, K. Fan, S. A. Mahlke, T. Oh, H. Kim, \nand H.-s. Kim. Edge\u00adcentric modulo scheduling for coarse-grained recon.gurable architec\u00adtures. In PACT \n2008. [44] W. Pugh. The omega test: a fast and practical integer programming algorithm for dependence \nanalysis. In Supercomputing 1991. [45] N. Satish, K. Ravindran, and K. Keutzer. A decomposition-based \nconstraint optimization approach for statically scheduling task graphs with communication delays to multiprocessors. \nIn DATE 2007. [46] S. Swanson, K. Michelson, A. Schwerin, and M. Oskin. Wavescalar. In MICRO 2003. [47] \nM. Thuresson, M. Sjalander, M. Bjork, L. Svensson, P. Larsson-Edefors, and P. Stenstrom. Flexcore: Utilizing \nexposed datapath con\u00adtrol for ef.cient computing. In IC-SAMOS 2007. [48] G. Venkatesh, J. Sampson, N. \nGoulding, S. Garcia, V. Bryksin, J. Lugo-Martinez, S. Swanson, and M. B. Taylor. Conservation cores: \nreducing the energy of mature computations. In ASPLOS 2010. [49] H. M. Wagner. An integer linear-programming \nmodel for machine scheduling. Naval Research Logistics Quarterly, 6(2):131 140, 1959. [50] E. Waingold, \nM. Taylor, D. Srikrishna, V. Sarkar, W. Lee, V. Lee, J. Kim, M. Frank, P. Finch, R. Barua, J. Babb, S. \nAmarasinghe, and A. Agarwal. Baring It All to Software: RAW Machines. Computer, 30(9):86 93, 1997. [51] \nM. Watkins, M. Cianchetti, and D. Albonesi. Shared recon.gurable architectures for cmps. In FPGA 2008. \n[52] L. A. Wolsey and G. L. Nemhauser. Integer and Combinatorial Optimization.   \n\t\t\t", "proc_id": "2491956", "abstract": "<p>Specialized execution using spatial architectures provides energy efficient computation, but requires effective algorithms for spatially scheduling the computation. Generally, this has been solved with architecture-specific heuristics, an approach which suffers from poor compiler/architect productivity, lack of insight on optimality, and inhibits migration of techniques between architectures.</p> <p>Our goal is to develop a scheduling framework usable for all spatial architectures. To this end, we expresses spatial scheduling as a constraint satisfaction problem using Integer Linear Programming (ILP). We observe that architecture primitives and scheduler responsibilities can be related through five abstractions: placement of computation, routing of data, managing event timing, managing resource utilization, and forming the optimization objectives. We encode these responsibilities as 20 general ILP constraints, which are used to create schedulers for the disparate TRIPS, DySER, and PLUG architectures. Our results show that a general declarative approach using ILP is implementable, practical, and typically matches or outperforms specialized schedulers.</p>", "authors": [{"name": "Tony Nowatzki", "author_profile_id": "81501680950", "affiliation": "University of Wisconsin-Madison, Madison, WI, USA", "person_id": "P4149103", "email_address": "tjn@cs.wisc.edu", "orcid_id": ""}, {"name": "Michael Sartin-Tarm", "author_profile_id": "81758818057", "affiliation": "University of Wisconsin-Madison, Madison, UNK, USA", "person_id": "P4149104", "email_address": "msartintarm@wisc.edu", "orcid_id": ""}, {"name": "Lorenzo De Carli", "author_profile_id": "81440608205", "affiliation": "University of Wisconsin-Madison, Madison, WI, USA", "person_id": "P4149105", "email_address": "lorenzo@cs.wisc.edu", "orcid_id": ""}, {"name": "Karthikeyan Sankaralingam", "author_profile_id": "81100272510", "affiliation": "University of Wisconsin-Madison, Madison, WI, USA", "person_id": "P4149106", "email_address": "karu@cs.wisc.edu", "orcid_id": ""}, {"name": "Cristian Estan", "author_profile_id": "81100534336", "affiliation": "Broadcom, San Francisco, CA, USA", "person_id": "P4149107", "email_address": "cristian@estan.org", "orcid_id": ""}, {"name": "Behnam Robatmili", "author_profile_id": "81346489946", "affiliation": "Qualcomm Research Silicon Valley, Santa Clara, CA, USA", "person_id": "P4149108", "email_address": "behnamr@qti.qualcomm.com)", "orcid_id": ""}], "doi_number": "10.1145/2491956.2462163", "year": "2013", "article_id": "2462163", "conference": "PLDI", "title": "A general constraint-centric scheduling framework for spatial architectures", "url": "http://dl.acm.org/citation.cfm?id=2462163"}