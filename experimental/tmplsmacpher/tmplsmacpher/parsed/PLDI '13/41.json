{"article_publication_date": "06-16-2013", "fulltext": "\n Static Analysis for Probabilistic Programs: Inferring Whole Program Properties from Finitely Many Paths. \n Sriram Sankaranarayanan Aleksandar Chakarov Sumit Gulwani University of Colorado, Boulder University \nof Colorado, Boulder Microsoft Research, Redmond. srirams@colorado.edu aleksandar.chakarov@colorado.edu \nsumitg@microsoft.com Abstract We propose an approach for the static analysis of probabilistic pro\u00adgrams \nthat sense, manipulate, and control based on uncertain data. Examples include programs used in risk analysis, \nmedical deci\u00adsion making and cyber-physical systems. Correctness properties of such programs take the \nform of queries that seek the probabilities of assertions over program variables. We present a static \nanalysis approach that provides guaranteed interval bounds on the values (assertion probabilities) of \nsuch queries. First, we observe that for probabilistic programs, it is possible to conclude facts about \nthe be\u00adhavior of the entire program by choosing a .nite, adequate set of its paths. We provide strategies \nfor choosing such a set of paths and verifying its adequacy. The queries are evaluated over each path \nby a combination of symbolic execution and probabilistic volume\u00adbound computations. Each path yields \ninterval bounds that can be summed up with a coverage bound to yield an interval that en\u00adcloses the probability \nof assertion for the program as a whole. We demonstrate promising results on a suite of benchmarks from \nmany different sources including robotic manipulators and medical deci\u00adsion making programs. Categories \nand Subject Descriptors D.2.4 [Software Engineer\u00ading] Software/Program Veri.cation. Keywords: Probabilistic \nProgramming, Program Veri.cation, Vol\u00adume Bounding, Symbolic Execution, Monte-Carlo Sampling. 1. Introduction \nThe goal of this paper is to study static analysis techniques for proving properties of probabilistic \nprograms that manipulate un\u00adcertain quantities de.ned by probability distributions. Uncertainty is a \ncommon aspect of many software systems, especially systems that manipulate error-prone data to make medical \ndecisions (eg., should the doctor recommend drugs or dialysis to a patient based on the calculated eGFR \nscore?); systems that predict long term risks of catastrophic events such as .oods and earthquakes (eg., \nshould a nuclear power plant be built at a certain location?); and control systems that operate in the \npresence of sensor errors and external environmental disturbances (eg., robotic manipulators and air \ntraf\u00ad.c control systems). It is essential to learn how the presence of uncertainty in the inputs can \naffect the program s behavior. Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. PLDI 13, June 16 19, 2013, Seattle, WA, USA. Copyright c &#38;#169; \n2013 ACM 978-1-4503-2014-6/13/06. . . $10.00 In general, the problem of reasoning about uncertain systems \nis quite challenging due to (a) real-valued variables that are used to model physical quantities such \nas space, time and temperature; (b) imprecision in the input, modeled by different distributions such \nas uniform, Poisson and Gaussian; and (c) control .ow that is mediated by conditions over the program \nvariables and random variables. These aspects make the resulting programs in.nite state, requiring techniques \nthat go beyond the exhaustive exploration of concrete sets of states. In this paper, we study in.nite \nstate programs that manipulate real-valued variables and uncertain quantities belonging to a wide variety \nof distributions. Our modeling approach is simple: we use standard imperative programs augmented with \nprobabilistic con\u00adstructs to model uncertainties. The overall goal of our approach is to answer questions \nabout the probability of assertions at the exit of the program. Monte-Carlo simulation [34] can be used \nto answer this ques\u00adtion upto some desired level of precision. However, these tech\u00adniques do not provide \nformal guarantees on the accuracy of the answers. In this work, we provide a static analysis approach \nthat can place guaranteed interval bounds on the possible values of the assertion probabilities. As a \nside effect, our approach can perform a strati.ed sampling to provide more accurate answers than stan\u00addard \nrejection sampling. Our static analysis approach has two main components: (A) choosing an adequate set \nof paths and (B) path probability bound computation. Adequate Set of Paths: Unlike static analysis in \nthe non\u00adprobabilistic case, it suf.ces to consider a carefully chosen .nite set of adequate program paths \nfor estimating probability bounds. Let us assume that a set of syntactic program paths S is provided \nwith a lower bound c on their combined path probabilities. We can proceed to estimate the probability \nof any assertion . over the .nitely many paths in the set S and know that the contribution from the unexplored \npaths is at most 1-c. Therefore, given some target coverage probability c (eg., c = 0.95), we wish to \n.nd a set S of syntactic program paths (if such a set exists) such that an execution of the program yields \na path in the set S with probability at least c. We achieve this in two steps: (a) we use a statistical \nprocedure to .nd a set S that achieves the cov\u00aderage bound with high con.dence, and (b) we then compute \na for\u00admal lower bound on the actual coverage of S. Using a two-step ap\u00adproach guarantees that the symbolic \nexecution is always performed over feasible program paths. Secondly, executing probabilistic pro\u00adgrams \nis a natural way of sampling the paths of the program ac\u00adcording to their path probabilities. Path Probability \nComputation: We use symbolic execution along the chosen paths to compute bounds on the path probabil\u00adity: \ni.e, the probability that an execution of the program follows the chosen path. In turn, this reduces \nto .nding the probability of satisfaction for a system of constraints, given probability distribu\u00adtions \non the individual variables. This is a hard problem both in theory and in practice [2]. Therefore, rather \nthan compute the pre\u00ad cise answer, we seek to bound the probability of satisfaction. We present a scheme \nthat can estimate probability bounds for linear as\u00adsertions over integers and reals. Furthermore, given \nmore time our technique can generate bounds that are tighter. Our overall framework weaves the two ideas \ntogether to yield upper and lower bounds for the probability of the assertion for the program as a whole. \nThe paper makes the following contributions: 1. We present an approach to infer bounds on assertion probability \nfor the whole program by considering a suitably chosen .nite set of program paths. 2. We present branch-and-bound \ntechniques over polyhedra to derive tight bounds on path and assertion probabilities. 3. We present \nan experimental evaluation of our approach over a set of small but compelling benchmarks. We showcase \nsome of the successes of our approach and identify limitations to motivate future work.  However, we \nhave by no means solved every aspect of the complex problem of probabilistic program analysis. 1. Our \napproach is restricted to programs with linear assignments and conditionals (see Figure 3 in Section \n3). As such, the high level ideas presented here directly extend to programs with non\u00adlinearities that \narise from multiplications, exponentiations and bitwise operations. The current limitation lies in the \nvolume computation technique (Section 4.2) that handles convex poly\u00adhedra over reals and integers. 2. \nWe support the generation of random variables with known distributions and known correlations speci.ed, \nfor instance, by a given covariance matrix. However, our framework does not handle non-deterministic \nuncertainties or distributions with unknown (arbitrary) correlations. 3. Our focus is on estimating \nprobability bounds for safety prop\u00aderties. We do not consider liveness properties (termination) or expectation \nqueries in this paper.  2. Approach At A Glance In this section, we .rst motivate the main contributions \nof this paper using an example from medical decision making. The probabilis\u00adtic programming language \nand its semantics are described in the subsequent section. Estimating Kidney Disease Risk in Adults Chronic \nkidney dis\u00adease in adults is characterized by the gradual loss of kidney func\u00adtion. A common measure \nof chronic kidney disease is a quantity called the Estimated Glomerular Filtration Rate (eGFR), which \nis computed based on the age, gender, ethnicity and the measured serum Creatinine levels of the patient \n1. There are many formu\u00adlae that are used for computing eGFR, which is typically a log\u00adlinear function \nof its inputs. Figure 1 shows the function that com\u00ad putes the value of eGFR according to the widely \nused CKD-EPI formula 2 . Kidney disease risk in adults is typically diagnosed if log(eGF R) < 4.5, or \nequivalently eGF R < 90. However, the values of age and measured serum Creatinine levels can be erro\u00adneous. \nAge is typically rounded up or down causing a \u00b11variation, while the lab values of serum Creatinine levels \nare assumed to have a 10% variation. While gender and ethnicity are often mistake-free, it is sometimes \npossible to record a wrong gender or ethnicity due to 1Cf. http://www.kidney.org/kidneydisease/aboutckd.cfm \n 2Cf. http://nephron.com/epi_equation. 1 real estimateLogEGFR( real logScr, int age, 2 bool isFemale, \nbool isAA){ 3 var k,alpha: real; 4 var f: real; 5 f= 4.94; 6 if (isFemale) { 7 k = -0.357; 8 alpha = \n-0.329; 9 } else { 10 k = -0.105; 11 alpha = -0.411; 12 } 13 14 if ( logScr < k ) { 15 f = alpha * (logScr \n-k); 16 } else { 17 f = -1.209 * (logScr -k); 18 } 19 f = f -0.007 * age; 20 21 if (isFemale) f = f + \n0.017; 22 if (isAA) f = f + 0.148; 23 return f; 24 } Figure 1. A program that computes the logarithm \nof the eGFR given the logarithm of the measured serum Creatinine value (logScr), the patient s age, whether \nthe patient is female (isFemale) and whether the patient is African American (isAA). We assume that the \ninputs to the function are noisy and wish to know how the noise affects the output value f. 1 void compareWithNoise(real \nlogScr, real age, 2 bool isFemale, bool isAA) { 3 f1 = estimateLogEGFR(logScr, age, isFemale,isAA); 4 \nlogScr = logScr + uniformRandom(-0.1, 0.1); 5 age = age + uniformRandomInt(-1,1); 6 if (flip(0.01)) 7 \nisFemale = not( isFemale ); 8 if (flip(0.01)) 9 isAA = not( isAA ); 10 f2 = estimateLogEGFR(logScr, \nage, isFemale,isAA); 11 estimateProbability (f1 -f2 >= 0.1); 12 estimateProbability (f2 -f1 >= 0.1); \n13 } Figure 2. A probabilistic program that simulates noise in patient data, comparing the error between \noriginal and noisy outputs. transcription mistakes in the (electronic) medical record. Figure 2 shows \na model that adds the appropriate amount of noise to the in\u00adputs of the eGFR calculation. Given these \ninput uncertainties, one naturally worries about the problem of patients being mis-classi.ed purely due \nto the noise in the recorded patient data. Therefore, we ask the question: what is the probability that \nthe presence of noise causes 10% or larger variation in the EGFR values (= 0.1absolute variation in the \nlogarithmic scale)? Such a query is expressed at the end of the function compareWithNoise in Figure 2. \nModeling Input Distributions: The distributions of serum Cre\u00adatinine values are highly correlated with \nage, gender and race. A detailed population survey is provided by Jones et al. [19]. For il\u00ad lustration, \nwe assume a 50% chance of the person being female and 10% chance of being African American. Based on \nthese outcomes, different distributions are chosen for the patients based on their gender. We model input \nserum Creatinine levels as drawn from a normal distribution around the mean with a .xed standard devi\u00adation \nof 0.1 around a mean that varies based on gender: 1.0 for women and 1.2 for men. A full model based on \npopulation data can be constructed and used in our static analysis. Table 3 summa\u00adrizes the two basic \nprimitives that we expect from any distribution supported by our analysis. Estimating Probability of \nAssertions: For the example at hand, our approach proceeds in three steps: 1. We .rst generate a .nite, \nadequate set of paths S such that the probability that a random execution yields a path in S is at least \nc for some .xed c. Fixing c = 0.95, we use the procedure in Algorithm 2 to yield 49 distinct paths. \n2. Next, we analyze each of these paths and estimate intervals for the path probability and the probability \nfor each of the assertions. For the example at hand, this requires less than 5 seconds of computational \ntime for all paths and assertions. 3. We add up the path probability intervals for individual paths \nin the set S to infer a bound of [0.98117, 0.98119] on the total probability of paths in S. This means \nthat the paths in S are guaranteed to be encountered with probability at least 0.98117. Therefore, the \nprobability of drawing a path not in S is at most 0.01883. We will make use of this fact in the next \nstep. 4. We add up path probability intervals for the assertion f1-f2 >= 0.1 to obtain a bound [0.12630, \n0.126306]. However, since we did not cover all paths in the program, this bound is not valid for the \nwhole program. To address this, we add 0.01883, an upper bound on the probability of encountering a path \noutside S. This yields a guaranteed bound of [0.126306, 0.145136] for the probability that a given execution \nsatis.es the assertion. Likewise, the probability of the assertion f2-f1 >= 0.1 is estimated at [0.08503, \n0.10386]. 5. We simulated the program for 1, 000, 000 iterations in Matlab and estimated the probability \nof the assertions, taking nearly as much time as our static analysis. The bounds obtained by our analysis \nare con.rmed.  Thus, our analysis provides bounds on the probability of obtaining a 10% or larger deviation \nin the computed risk score due to errors in the input data. The rest of the paper is organized as follows: \nSection 3 presents the syntax and semantics of probabilistic programs, the process of bounding probabilities \nis explained in Section 4, Section 5 dis\u00ad cusses our implementation and some experimental results on \nan in\u00adteresting set of benchmarks. Related work is discussed in Section 6. We conclude by discussing \nfuture directions. Supplementary mate\u00adrials, including our prototype implementation and the benchmarks \nused in this paper are available on-line from our project page 3 . 3. Probabilistic Programs We now consider \nimperative programs with constructs that can generate random values according to a .xed set of distributions. \nWe then describe queries over probabilistic programs that seek (conditional) probabilities of assertions. \nFig. 3 presents the key parts of the syntax for a probabilis\u00adtic programming language for specifying \nprograms. Let X = {x1, . . . , xk}be a set of program variables partitioned into subsets Q and I of real \nand integer typed variables, respectively. The lan\u00adguage consists of assignments, conditional statements \nand loops. The expressions in the language are restricted in two ways: (1) The language is strongly typed: \ninteger values cannot be converted to 3Cf. http://systems.cs.colorado.edu/research/ cyberphysical/probabilistic-program-analysis/ \n program stmt initSpec assign condStmt while intAssign realAssign intExpr realExpr intRandom realRandom \nboolExpr relop intVariable realVariable queries . declarations init {initSpec *}stmt * queries . assign \n| condStmt | while . intVariable := intConst |realVariable := realConst | intVariable ~intRandom | realVariable \n~realRandom . intAssign |realAssign . if boolExpr stmt+ else stmt+ . while boolExpr stmt+ . intVariable \n:= intExpr . realVariable := realExpr . intConst |intExpr \u00b1intExpr | intConst * intExpr |intRandom \n . realConst |realExpr \u00b1realExpr | realConst * realExpr |realRandom . uniformInt (intConst, intConst) \n| Bernoulli (intConst, realConst) | \u00b7\u00b7\u00b7 . uniformReal (realConst, realConst) | Gaussian (realConst, \nrealConst) | \u00b7\u00b7\u00b7 . boolExpr .boolExpr | intExpr relop intExpr |realExpr relop realExpr | true | false \n . < | > | = | = | = . I . Q . estimateProbability boolExpr( given boolExpr)?  Figure 3. Partial \nsyntax speci.cation for imperative language for modeling programs. reals, or vice-versa. (2) The expressions \ninvolving integer and real variables are af.ne. Inbuilt Random Value Generators: The programs in our \nlan\u00adguage can call a set of inbuilt random number generators to produce values according to known distributions, \nas summarized in Table 2.  Some distributions can be automatically implemented based on other available \nones. For instance, a coin .ip with success probability p . [0, 1] can be simulated by a uniform random \nvariable r ~ uniformReal(0, 1) and the conditional statement involving the condition r = p. Initializer: \nThe program has an initialization section that is assumed to initialize every variable xi . X. The initialization \nfor a variable xi can assign it to a concrete value ci, or to a randomly distributed value by calling \nan in-built random number generator. Queries: Queries in our language involve probabilities of Boolean \nexpressions over program variables, possibly conditioned on other Boolean expressions. Table 1 summarizes \nthe intended meaning of each of the two types of queries. For simplicity of presentation, we restrict \nqueries to conjunctions of linear inequalities. However, queries involving disjunctions .1 ..2 can be \nexpressed by using a special indicator variable x, which can be set to 1iff either disjunct is true. \nThe transformed query evaluates the probability of x = 1. 3.1 Semantics of Probabilistic Programs The \noperational semantics of the execution of assignments, if-then\u00adelse, and while statements are quite standard. \nWe focus on de.ning the semantics of the initialization, the random number generators, and the queries. \nLet P be a probabilistic program written using the syntax in Fig. 3. We associate a set of control locations, \nLoc = {l0, . . . , lk},  Query Remark estimateProbability(.) The execution terminates AND variables \nat exit satisfy .. estimateProbability(. given .) The execution terminates AND variables satisfy . under \nthe condition that they satisfy .. Table 1. Queries on the .nal state of the program and their informal \nmeaning. Name uniformReal(a,b) Support [a, b] . R Density Function 1 b-a l [a,b], a < b. uniformInt(a,b) \nBinomial(n,p) [a, b] . Z [0, n] . Z { 1 b-a l [a,b] b > a 1 a = b Cn x)px(1 -p)n-x , n = 1, p . [0, 1]. \n. Gaussian(m,s) R v1 2ps e -1 2 (x-m) s 2 , s > 0 Table 2. Speci.cation of the inbuilt random value \ngenerators. Note that the function l X(x)is the indicator function for the set X. such that each statement \nin P is associated with a unique control location. The set of labels de.ne a control .ow graph with nodes \ncorresponding to statement labels and edges corresponding to con\u00additional tests and assignments between \nprogram variables. Let l0 be the location after the initializer and lF be the end of the pro\u00adgram, where \nqueries may be posed. A state of the program is a tuple (lj, xxj)given by its current control location \nlj . Loc and R|Q|\u00d7 Z|I| valuations to the program variables xxj . . The initializer assigns each variable \nto a .xed constant value or a random value sampled according to a known probability distribu\u00adtion. All \nrandom variables are assumed to be mutually independent. Since our main goal is to de.ne and evaluate \nqueries, we de.ne the semantics operationally. Let . = {p1, . . . , pN, . . .}be a set of syntactic program \npaths that start from location l0 and lead to the .nal location lF. Each path pj is a .nite sequence \npj : l0 . l1 . \u00b7 \u00b7 \u00b7 . ll . lF, where each (li, li+1)is an edge in the program s CFG. Note that locations \nmay repeat in pj. Symbolic Execution: First, we de.ne symbolic execution of paths in the presence of \nrandom variables. De.nition 3.1 (Symbolic State). The symbolic state of a program P is a tuple s : (l, \nR, ., T )wherein 1. l denotes the current location. 2. R = {r1, . . . , rk}represents a set of random \nvariables. This includes random variables for the initial condition, and the RHS of assignments involving \ncalls to the random value gen\u00aderators (RVG). Fresh variables will be added to the set R, as RVGs are \nencountered along a path. 3. . is an assertion involving the variables in R representing the path condition. \nThe syntax of the program ensures that . does not involve any program variables and is a conjunction \nof linear inequalities over R. 4. T is an af.ne transformation that maps each program variable xi . \nX to an af.ne expression involving variables in R.  Initial Symbolic State: The initial symbolic state \ns0 is given by (l0, R0, true , T0)wherein R0 is an initial set of random variables corresponding to each \ninitialization of a program variable xj by a call to an inbuilt random value generator. The transformation \nT0 associates each program variable xj with a constant c or a random variable rj . R depending on how \nxj is initialized. Symbolic Step: We may now de.ne the single-step execution of the program along a path \np : l0 . lF by means of symbolic states Figure 4. A simple probabilistic program with a probability esti\u00admation \nquery. We consider the assignments to variables x,c as a simultaneous assignment. 1 state x: real; 2 \nstate c: int; 3 init { x := uniformReal(-1,3); 4 c := 0; } 5 - - l0: while (x <= 4) { 6 - - l1: x := \nx + uniformReal(-1,3); 7 c := c + uniformInt(0,2); 8 } 9 - - lF : estimateProbability(c <= 4); s0 . \ns1 . . . . sF . To do so, we specify the update of a symbolic state across an edge (l, m)in the control-.ow \ngraph. First, we handle any calls to random value generators on the edge (l, m). Each such call results \nin the addition of a fresh random variable to the set R with appropriate type, set of support, and probability \ndensity function. Table 2 lists the formal speci.cation of the random variable generated by various inbuilt \nrandom value generators. Next, depending on the type of the edge being traversed, we de.ne the subsequent \nsymbolic state: 1. The edge is labeled by a condition .[X, R], wherein calls to random value generators \nhave been substituted by fresh random variables introduced in R. In this case, we update the path condition \n. to . ' : . . .[x1 . T (x1), . . . , xn . T (xn)], wherein each occurrence of the program variable xi \nin . is substituted by T (xi). 2. The edge is labeled by an assignment xi := e[X, R]involving variables \nxj . X and calls to random value generators replaced by fresh random variables. In this case, we update \nthe transfor\u00admation T to yield a new transformation T ' de.ned as  ' { T (xj) xj = xi T (xj)= e[x1 . \nT (x1), . . . , xn . T (xn)] otherwise As a result, we have de.ned the transformation of the symbolic \nstate s : (l, R, ., T )to a new state s ' : (m, R ' , . ' , T ' )involving the addition of fresh random \nvariables, updates to the path condi\u00adtion . or the transformation T . Example 3.1. Fig. 4 shows a simple \nprogram P that updates a real variable x and an integer c. Each step updates the value of x and c by \ndrawing a uniform random variable of appropriate type from .xed ranges. Consider the path that iterates \nthrough the while loop twice: p : l0 . l1 . l0 . l1 . l0 . lF . The initial state s0 : (l0, R0, .0, T0)is \ngiven by R0 : {r1 : uniformReal[-1, 3]}, .0 : true , T0 : (x . r1, c . 0) We may verify that the state \nat the end of the path is given by /{r1, r2, r3, }r1 = 4 . r1 + r2 = 4 (x : r1 + r2 + r3, )) lF,, , z1, \nz2 .r1 + r2 + r3 > 4 c : z1 + z2 The real-valued variables r1, r2, r3 are distributed as uniformReal(-1, \n3), while z1, z2 are distributed as uniformInt(0, 2). Let p : l0 lF be a .nite path from initial to \nexit node and s(p) : (lF , Rp, .p, Tp) be the .nal symbolic state obtained by propagating the initial \nsymbolic state s0 along the edges in p. A program path p is feasible iff the path condition .p is satis.able. \nLemma 3.1. For any feasible state s : (l, R, ., T )encountered during symbolic execution, the following \nproperties hold: 1. . is a linear arithmetic assertion that can be decomposed into . : .Q ..I where .Q \ninvolves real-valued random variables from R while .I involves integer-valued random variables. 2. T \n(xj) is an af.ne expression for each xj involving integer random variables/constants if xj is integer \ntyped and involving real-valued random variables/constants if xj is real typed.   3.2 Semantics of \nQueries We may use the notion of a symbolic state propagated across program paths to de.ne the semantics \nof queries. We .rst start with unconditional queries that estimate the probability of an assertion .[X]over \nthe program variables. We then extend our de.nition to de.ne the semantics of conditional probability \nqueries. To answer the queries, we start with the set of all feasible program paths . = {pj : l0 lF |pj \nis feasible}, where |.| may be .nite or countably in.nite. Probability of an Assertion: Consider a query \nthat seeks the probability of an assertion . over the program variables. We denote the outcome of the \nquery as P(.). P(.)= LPpj(.), pj.. wherein Ppj(.)denotes the probability of the event that the exe\u00adcution \nproceeds along the path pj, reaching the location lF and . is true in the resulting state at location \nlF . We now formally de.ne Ppj(.) for pj . .. First, let sj : (lF, Rj, .j, Tj)denote the symbolic state \nobtained by executing along the path pj. The transformation Tj maps the program vari\u00adables at lF to af.ne \nexpressions over Rj, the random variables en\u00adcountered along pj. Let . ' : .[X . Tj(X)] denote the transfor\u00admation \nof the program variables in . using Tj. We partition Rj into integer valued variables Zj and real valued \nvariables Qj. Next, we split .j into an integer part .j I conjoined with a real part .j Q. Similarly, \nwe split the condition . ' into .I and .Q. The value Ppj(.)can be de.ned in two parts: 1. For the real \npart, we de.ne the integral over the sets of support for the variables in Qj = {y1, . . . , yk} Q Q l \n(.. .\u00b7 \u00b7 \u00b7 pk(yk)dy1 \u00b7 \u00b7 \u00b7 dyk , pr :j )p1(y1)p2(y2) Q wherein each yj has a density function pi(yi) \nand Q is the joint region of support for the variables y1, . . . , yk taken as the Cartesian product \nof their individual intervals of support. The notation l (.)for an assertion . stands for the indicator \nfunction that maps to 1wherever . is satis.ed and 0elsewhere. 2. For the integer part, we de.ne a summation \nover the sets of support for variables in Zj = {z1, . . . , zl} I I pz : L\u00b7 \u00b7 \u00b7 Ll (.j ..)i1(z1)i2(z2)\u00b7 \n\u00b7 \u00b7 il(zl), z1.I1 zl.Il wherein Ij is the set of support for zj and ij(zj)is the mass function for the \ndistribution de.ning zj. The overall value Ppj(.) is the product pr \u00d7 pz. Note that the restrictions \nto the structure of the conditional statements and queries guarantee that .j and . ' are de.ned by conjunctions \nof linear inequalities. Therefore, the sets de.ned by them are a union of polyhedra over the reals and \nZ-polyhedra over the integers. They are measurable under the Lebesgue and discrete measures, so that \nintegrals and summations over them are well de.ned. Example 3.2. Consider again the symbolic execution \nfrom Ex. 3.1. The set of random variables is RF : {r1, r2, r3, z1, z2} with the real-valued variables \nr1, r2, r3 which are all of the type uniformReal(-1, 3). The integer variables z1, z2 are of the type \nuniformInt(0, 2). We wish to .nd the probability Pp(c = 4) cor\u00adresponding to the query estimateProbability(c \n= 4). First, we transform c = 4 according to the transformation c . z1 + z2 to obtain z1 + z2 = 4. The \noverall probability reduces to .nding the probability that the following assertion holds .Q : r1 + r2 \n= 4 . r1 + r2 + r3 > 4 . .I : z1 + z2 = 4 given the distributions of r1, r2, r3, z1, z2. The constraint \nr1 = 4is seen to be redundant given r1 . [-1, 3] and therefore is dropped. We split the computation into \nan integral over the real part (1)3 l (r1 + r2 = 4.r1 + r2 + r3 > 4) dr1dr2dr3 . [-1,3]3 4 and an integer \npart '2 '2 l (z1 + z2 = 4) (1)2 . z1=0 z2=0 3 The second half of this paper describes how to restrict \nthe probability computation to a suitably chosen .nite set of paths, while bounding the in.uence of the \nunexplored paths and for each path, place tight bounds on the summation and the integral above. We also \nuse Monte-Carlo simulations to estimate the actual values to some degree of con.dence. Conditional Assertions: \nWe now consider queries of the form estimateProbability(.1 given .2)that seeks to estimate the prob\u00adability \nof assertion .1 under the condition that the state at the exit satis.es .2. However, unlike the previous \ncase, it is possible that the conditional probability may be ill de.ned. This is especially the case \nwhen no execution reaches the end of the program or all executions reaching the exit do not satisfy .2. \nIt is well-known that P(.1..2) p.. Pp(.1..2) P(.1|.2)= = .. P.2 p.. Pp(.2) As a result, we can use the \nde.nitions established for computing the probabilities of assertions unconditionally, and derive conditional \nprobabilities. In general, however, it is incorrect to split the condi\u00adtional probabilities as a whole \nas a summation of the conditional probabilities measured along paths. 4. Computing Probabilities In this \nsection, we present techniques to estimate the probabilities of assertions by sampling program paths. \nThe probability of the assertion (overall probability) is simply the summation of the prob\u00adabilities \ncomputed over individual paths. We consider three types of estimation: (a) computing guaranteed lower \nand upper bounds on the individual path probabilities and the overall probability of the assertion and \n(b) approximation (Monte-Carlo simulation) of path probabilities. The overall path probability estimation \nproceeds in two steps: (a) choose a .nite set of paths in the program using a heuristic strategy, so \nthat with high con.dence, the sum of the path proba\u00adbilities of the chosen paths exceeds a coverage bound \nc; and (b) for each path chosen, estimate probability ef.ciently from the path condition. Finally, we \nshow how both parts can be tied together to yield guaranteed whole program bounds and approximations. \nLet us .x a program P and let . be an assertion whose prob\u00adability we wish to estimate. We assume a path \ncoverage target Data: Program P , assertion . and coverage c . (0, 1). Result: Probability estimate \np and bounds [p, p]. /* 1. Heuristically elect a set of paths. */ C := PathSelect(P, c) ; q := 0 ; /* \nlower bound on path probability */ (p, p) := (0, 0) ; /* path + assertion bounds */ p := 0 ; /* Monte-Carlo \nEstimate */ /* 2. Iterate through paths in C */ for (p . C)do /* 2.1 Compute path condition and transformation \n */ (R, ., T ) := SymbolicExecute(P, p); /* 2.2 Transform condition . */ . ' := .[X . T (X)]; /* 2.3 \nLower bound on path probability */ q := q + PolyhedronLowerBound(R, .) ; /* 2.4 Bounds on path and assertion \nprobability */ p := p + PolyhedronLowerBound(R, . . . ' ) ; p := p + PolyhedronUpperBound(R, . . . ' \n) ; /* 2.5 Monte-Carlo estimation */ p := p + MonteCarloSample(R, . .. ' ) ; end /* 3. Adjust upper \nbound estimate to account for uncovered paths. */ p := p + (1 -q) ; Algorithm 1: Algorithm for estimating \nthe probability of an assertion. 0 < c < 1 wherein c is assumed to be very close to 1 (say c = 0.99). \nLet . denote the set of all terminating, feasible pro\u00adgram paths that lead from the initial location \nl0 to the .nal location lF. Algorithm 1 shows the overall algorithm for providing bounds and estimates \nof probabilities for an assertion. The idea is to com\u00adpute a .nite set of paths C according to the coverage \ncriterion and compute the probabilities for the assertion . to hold at the end of each path. Next, an \nactual lower bound on the total probability of all paths in C is also estimated and used to yield an \nupper bound on the overall assertion probability. 4.1 Heuristic Path Selection Strategy The .rst step \nis to select .nitely many feasible, terminating pro\u00adgram paths C = {p1, . . . , pk}. The path probability \nof a path pj is de.ned as Ppj(true ). This denotes the probability that a ran\u00addomly chosen initial condition \nresults in taking the path pj in the .rst place. We write P(pj)to denote the path probability. A subset \nC . .satis.es a coverage goal c iff ' p.C P(p)= c. Satisfying any given coverage goal c is not guaranteed \nsince it is not known a priori if a .nite subset C satis.es the coverage goal. For instance, the program \nmay fail to terminate with probability = 1-c. However, most programs of interest do pnon-terminating \nterminate almost surely with probability 1. Lemma 4.1. Let P be an almost surely terminating program. \nFor any coverage goal c . (0, 1), there is a .nite set of paths C such that ' p.C P(p)= c. Therefore, \nassuming that we are able to .nd enough paths C whose cumulative path probability is known to be at least \nc, we may proceed to estimate the probability of . along paths p . C. We use the notation PC(.)to denote \nthe sum of path probabilities for . along all paths belonging to the set C: PC(.)= ' p.C Pp(.). Theorem \n4.1. For any assertion . and set of paths C that satisfy the coverage criterion c, PC(.)= P(.)= PC(.) \n+ (1 -c). Data: Program P , assertion ., run length K > 0. Result: Set of paths C such that a run of \nK continuously drawn program paths belong to C . count := 0 ; C = { } ; while count < K do p := simulatePath(P) \n; if p . C then count := 0 ; C := C . {p}; else count := count + 1 ; end end Algorithm 2: Algorithm \nfor collecting a set of paths C through random simulation until a run of K > 0continuous paths already \nbelonging to C is obtained. Proof. Since we have C . ., the lower bound is immediate. For the upper bound, \nwe note that P(.) = ' Pp(.) = ' Pp(.) + ' Pp(.) p.. p.C p .C = ' Pp(.) + ' Pp(true ) p.C p .C = ' p.C \nPp(.) + (1 -c)  Our approach uses a statistical approach based on simulation to select an initial set \nC of paths such that the sum of path probabilities of individual paths in C exceeds the coverage bound \nc with high con.dence. Our overall strategy proceeds in two steps: 1. Fix integer parameter K > 0. The \ncriterion for choosing K is described below. 2. The simple procedure shown in Algorithm 2 repeatedly \nruns the program and records the syntactic path taken by each run. If the current path was previously \nunseen, then it is added to the set C. If K consecutive paths belong to C, the algorithm terminates. \n The strategy outlined above obtains a set of program paths C that is likely, but not guaranteed to \nsatisfy the coverage criterion c desired by the user. As such, this strategy is unsuitable for static \nanalysis. We employ it simply as a convenient heuristic to choose an initial set of paths. Our algorithm \nthen proceeds to use proba\u00adbility bounding to .nd guaranteed bounds on the actual coverage. These bounds \nare then used to actually estimate coverage. Selecting the Run Length: We now outline our heuristic strategy \nfor selecting a suitable run length K. Suppose C be a current set of paths that is claimed to satisfy \na coverage bound c. We wish to test this claim quickly. This can be viewed as a statistical hypothesis \ntesting problem of choosing between two hypotheses: H0 : P(C)= c vs. H1 : P(C)< c . In statistical testing \nterms, H0 is called the null hypothesis and H1, the alternative. Our goal is to witness K > 0 successive \nsamples that belong to C, where K is set suf.ciently large to convince us of H0 as opposed to H1. This \ncan be attempted using the sequential probability ratio test (with a slight modi.cation of H0 and H1 \nto introduce an indifference region, see Younes and Simmons [37] and references therein), or using a \nBayesian factor test following Jeffreys (see Jha et al. [17], and references therein). Either approach \ngives rise to a formula that .xes K as a function of c. For illustration, Jeffreys test with uniform \nprior yields a simple formula I-log B l K = . log c The factor B in the numerator is called the Bayes \nfactor and can be set to 100 to yield a answer with high con.dence. For instance, setting c = 0.95, we \nrequire K = 90 to satisfy the Bayes factor test at a con.dence level given by B = 100. However, this \ndoes not imply that we can be absolutely certain (rather than just highly con.dent!) that the set of \npaths satisfy the coverage guarantee. Therefore, we derive guaranteed lower bounds on the coverage of \nthe set of paths C collected. This lower bound forms the basis of our bound estimations. In our approach, \nthe purpose of hypothesis testing is two-fold: (a) It guarantees that all chosen paths in the set C are \nfeasible. As a result, our approach does not waste time exploring infeasible paths. (b) Sampling the \nprogram guarantees that paths with high probability are explored .rst before lower probability (tail) \npaths. This can potentially result in fewer paths explored by our analysis. Path Slicing: The use of \ndynamic path slicing techniques can further enhance the ef.ciency of the procedure in Algorithm 2. The \npaths in the set C are sliced with respect to the variables involved in the query assertion . being evaluated. \nThe use of path slicing ensures that fewer path fragments in C are generated and each path fragment yields \na simpler system of constraints over a smaller set of random variables. Our approach uses the path slicing \napproach originally proposed by Jhala et al. for predicate abstraction [18].  4.2 Computing Path Probabilities \nWe now describe techniques for computing the probability of an assertion . at the exit of a given path \np. As described previously, we perform a symbolic execution along p to obtain a symbolic state at the \nand of the program (lF, R, ., T ). We then consider the assertion . ..[X . T (X)], involving random variables \nin R. As a result, the computation of path probabilities reduces to the problem of estimating the probability \nthat a sample point from R drawn according to the distribution of the individual variables also satis.es \n.. We present algorithms for (a) computing lower and upper bounds; and (b) estimating the probability. \nFirst, we observe that given the structure of the program, the path condition is a conjunction of linear \ninequalities and is there\u00adfore a convex polyhedron over R. How do we then compute the probability of \ndrawing a sample point inside a given polyhedron? First, we note that computing the exact probability \nis closely related to the problem of computing the volume of a n-dimensional convex polyhedron. This \nis known to be .P -hard and therefore computa\u00adtionally hard to solve precisely once the number of dimensions \nis large [2]. We proceed by using bounding boxes that bounds the re\u00ad gion of interest from inside as \nwell as outside. Let R = Q .Z, where Q = {r1, . . . , rk}are the real-valued random variables and Z = \n{z1, . . . , zl} are the integer valued random variables. Let . : .Q . .Z be a conjunction of linear \ninequalities wherein .Q involves the real-valued variables and .Z involves the integer valued variables. \nEach random variable rj (or zk) is distributed according to a known probability distribution function \npj(rj)(or pk(zk))over some set of support. Our goal is to compute the probability that a randomly drawn \nsample : ( rk, zl) the assertion .. r r1, . . . , z1, . . . , satis.es We split this problem into two \nparts one for the integer part and one for the real part. Random Variable Primitives: We ignore the details \nof the dis\u00adtributions, assuming instead that some primitive functions can be computed: (a) given a range \n[a, b], we can compute its probability Pri([a, b]) Jb pi(ri)dri; and (b) we can obtain samples f = ri,j \nfor a j = 1, . . . , N that are distributed according to pi. The same as\u00adsumptions are held for the integer-valued \nrandom variables as well. Table 3 summarizes the primitives. sample () generate a (pseudo) random sample \nstandard Monte-Carlo methods [34]. probability (a,b) estimate probability mass of interval [a, b] using \ncumulative distribution function (CDF). Table 3. Primitives needed for each distribution type to support \nprobability and expectation estimation. Input: Polyhedron .[r1, . . . , rn], each ri ~ Di. Output: Interval \nbounds [p, p]and strati.ed MC sample estimate p queue := {.}; (p, p, p ) := (0, 0, 0); while |queue|> \n0do . := deque(queue ); if stopping criterion then H := boxOverApproximation(.); B := boxUnderApproximation(.); \n(p )(p + boxProbability(H) ) := ; p p + boxProbability(B) p := p + p * strati.edSample(., H); else \n(d, l) = selectDimension(R); /* Perform Branch and Bound */ (.1, .2) := (. . rd = l), (. . rd < l); \nqueue := enqueue(queue , {.1, .2}); end end Algorithm 3: Basic algorithm to compute bounds on the proba\u00adbility \nof a polyhedron represented by a linear assertion .. Computing Probability of Hypercubes: A hypercube \nover Rk can be written as the Cartesian product of intervals: H : [a1, b1]\u00d7 [a2, b2]\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 [ak, bk]. \n To compute the probability of H, we use the available primitives di\u00adrectly. PQ(H)= Pr1([a1, b1])Pr2([a2, \nb2])\u00b7 \u00b7 \u00b7 Prk([ak, bk]). The same calculation holds over the integers as well with minor mod\u00adi.cations \nto ensure that aj, bj values are rounded to the nearest integer above/below, respectively. For a disjoint \nunion of N = 0 'N hypercubes, we have PQ(H1 .\u00b7 \u00b7 \u00b7 . HN)= k=1 PQ(Hk). 4.3 Computing Bounds on Polyhedral \nProbabilities We now address the issue of computing bounds on the probabil\u00adity of a given polyhedral \nset speci.ed by assertion . over a set of real-valued variables Q = {r1, . . . , rk} (or integer valued \nvari\u00adables Z = {z1, . . . , zk}), where each ri is drawn from a known distribution. The basis of our \napproach is to over approximate [.] by a union of hypercubes H1 . H2 . \u00b7 \u00b7 \u00b7 . Hp and under approximate \nby a union H1 .\u00b7 \u00b7 \u00b7 . Hs. The number of hypercubes p, s can be varied to control the quality of the \napproximation. The overall idea is to bound the probability of . by computing the probability of the \nover approximation and the under approximation. Lemma 4.2. Let H . [.] . H. It follows that PQ(H) = PQ(.)= \nPQ(H). We now consider how to obtain over approximations and under approximations of polyhedral sets. \n Figure 5. A worst-case scenario for over-and under approxima\u00adtion by a single hypercube (left) and \na less pessimistic over-and under approximation using many hypercubes. Figure 6. Underapproximation \nby interior ray shooting. Over approximation: Linear programming (LP) solvers can be used to directly \nover approximate a given polyhedron by a single hypercube. To obtain upper (lower) bounds for rj we solve \nthe LP: max(min) rj s.t. .. The resulting hypercube given by the product of intervals obtained for each \nrj over approximates the region de.ned by .. However, the hypercube may be a gross over approximation \nof the original set, yielding quite pessimistic upper bounds. Fig. 5 shows a polyhedron whose over approximation \nby a single hypercube is quite pessimistic. However, if multiple hypercubes were used, then the accuracy \nimproves. As explained in Algorithm 3, this is achieved by repeated splitting along a chosen dimension \nand .nding bounding boxes using LP solvers. Under approximation: The goal is to .nd an under approxima\u00adtion \nof a polyhedron . by a hypercube. As depicted in Fig. 5, there is no best under approximation for a given \npolyhedron. This is un\u00adlike the case for an over approximation, where LP solvers can yield a single best \nbounding box. Our approach is to .nd a sample point r.in the interior of the polyhedron, treating it \nas a hypercube of volume zero where the upper and lower bounds along each dimension are speci.ed by [ \nrj, r j]. Next, we use a simple ray-shooting approach (Cf. Fig. 6) to expand the current box along each \ndimension while satisfying the constraints. This is carried out by iterating over the dimensions in a \n.xed order. Upon termination, this process is guaranteed to yield an under approximation. The size of \nthe hypercube depends on the sample point chosen. In practice, we choose multiple sample points and choose \nthe hypercube with the largest probability mass. Z-polyhedra: As such, the approach presented so far \nextends to Z-polyhedra with minor modi.cations. One modi.cation is to ensure that the bounds of each \ninterval are rounded up/down to account for the integer variables. Additionally, the use of an ILP solver \nto derive the over approximation can yield tighter bounds. On the other hand, since ILP is a harder problem \nthan LP, using LP solvers to over approximate the polyhedron yields safe upper bounds. The computation \nof an inner approximation can also be performed using ray shooting. Monte-Carlo Estimation We have discussed \ntechniques for bounding the probability using boxes. We can extend this to .nd unbiased estimators of \nthe probability by sampling. This is per\u00adformed by drawing numerous samples s.1, . . . , f sN according \nto the underlying distributions for each variable in R for a large number N and count the number N. that \nsatis.es .. We simply compute the ratio p = NN . . In some cases, it may take a prohibitively large number \nN of samples to approximate p to some given con.dence. To circumvent Table 4. Benchmarks used in our \nevaluation with descriptions. The benchmarks are available on-line at our project page, or upon request. \n#var: number of input variables to the program. + indicates random value generation inside loops. EGFR-EPI \nARTRIAL CORONARY ID 11 15 16 #var Description log egfr calc. using the ckd-epi formula. Framingham artrial \n.brillation risk calculation. Framingham coronary risk calculation. INVPEND PACK VOL ALIGN-1,2 CART 7+ \n10+ 8+ 6+ 7+ Inverted pendulum controller with noise. Packing objects with varying weights. Filling a \ntank with .uid at uncertain rates. Pointing to a target with error feedback. Steering a cart against \ndisturbances. this, it is possible to fold the computation of p with the branch-and\u00adbound decomposition \nin Algorithm 3. Here, we over approximate the polyhedron . as the union of K disjoint hypercubes [.] \n. H1 .\u00b7 \u00b7 \u00b7 . HK. Next, we draw Nj samples from inside each Hj and estimate the number of samples N.,j \ninside Hj that satisfy .. This yields an estimate p.j for Hj. The overall probability is given by p.= \n'K pjP(Hj). Note that P(Hj)is computed exactly and j=1 ef.ciently. This scheme integrates the ideas \nof strati.ed sampling and importance sampling, both of which are well-known variance reduction techniques \nfor Monte-Carlo sampling [34]. 5. Implementation and Experiments We now describe the implementation and \nevaluation of the ideas presented thus far. Implementation: Our prototype implementation accepts pro\u00adgrams \nwhose syntax is along the lines of Figure 3 in Section 3. Our front-end simulates the program and collects \nthe unique paths observed until no new paths are seen for K consecutive iterations ( Algorithm 2). The \nvalue of K was chosen for c = 0.95 us\u00ading a Bayes factor test with B = 100. This yields K = 90 for achieving \na coverage of at least 0.95 with a 99% con.dence (un\u00adder a uniform prior). Symbolic execution is performed \nalong the collected paths, yielding the path conditions and transforming the probabilistic queries into \nassertions involving the random variables produced along the path. Finally, we implement the technique \nfor bounds estimation described in Algorithms 1 and 3. Rather than use an expensive linear programming \ninstance at each step of our branch and bound algorithm, we use a cheaper interval constraint propagation \n(ICP) to .nd bounding boxes ef.ciently [15]. Other optimizations used in our implementation include the \nuse of on\u00adthe-.y Cartesian product decomposition of polyhedra and repeated simpli.cation with redundancy \nelimination. Benchmark Programs: We evaluate our program over a set of benchmarks described in Table \n4. A detailed description of the benchmarks used along with our latest implementation are available on-line \nfrom our project page 4 . The .rst class of benchmarks consists of medical decision mak\u00ading formulae \nincluding the eGFR calculation described in Section 2 (EGFR-EPI). We also include a common heart risk \ncalculator and a hypertension risk calculator that uses the results of the Framingham heart study ( ARTRIAL, \nCORONARY)5. The results of this calcula\u00adtion are often taken into account if a patient is a candidate \nfor drugs such as Statins to prevent long term heart attack risk. 4Cf. http://systems.cs.colorado.edu/research/ \ncyberphysical/probabilistic-program-analysis/ 5Cf. http://www.framinghamheartstudy.org/  ID Ns Nu Ts, \nTvc pmc [lb, ub] EGFR EPI 563 45 0.1, 1.1 0.97803 [0.97803,0.97803] ARTIRAL 19547 2520 15.6, 1095 0.94094 \n[0.82809,1.0] CORONARY 7181 1239 3.9, 998.5 0.91992 [0.87239,1.0] INVPEND 90 1 18.5, 0.1 1 [1,1] PACK \n8043 1010 4.6, 24.1 0.95052 [0.95051,0.95052] ALIGN-1 4464 529 3.9, 25.7 0.90834 [0.90769,0.90846] ALIGN-2 \n17892 1606 8.9 , 75.1 0.93110 [0.9304,0.9384] VOL 121 9 3.6 , 1766 0.835 [0,1] CART 5799 774 4.2 , 1612 \n0.94898 [0.0176,1] Table 5. Experimental evaluation of initial adequate path explo\u00adration strategy. \nOur coverage goal was c = 0.95 at a 95% con.\u00addence level. Ns, Nu: number of simulated paths and unique \npaths, respectively, Ts: simulation time in seconds, Tvc : total volume computation time (branch-and-bound \ndepth is set to 15), pmc: monte-carlo probability estimate, [lb, ub]: coverage bounds. The second class \nof benchmarks consist of programs that carry out control tasks over physical artifacts. This includes \nmodels of linear feedback control systems with noise such as the inverted pendulum controller (INVPEND), \na model of a robot that packs objects of uncertain weights in a carton with the goal that the overall \nweight of carton has to be within some tolerance ( PACK), a model of an actuator that .lls a tank with \n.uid where the rate at which the .uid .ows can vary with the goal that the .uid waste has to be minimized \n(VOL)and models of a manipulator that attempts to point a laser at a desired stationary target, wherein \nalignments can fail due to actuator uncertainties. Each failed attempt results in a feedback that reveals \nthe amount and direction of error that can be used to design the displacement for the next attempt. We \npropose two versions that differ in terms of the error distributions and the feedback correction mechanisms \n( ALIGN-1, ALIGN-2). Experimental Evaluation: Table 5 shows the performance of the path selection and \ncoverage estimation strategies. We note that for most of our benchmarks, the selection of adequate paths \nrequired less than 104 simulations and resulted in a smaller set of unique paths that are potentially \nadequate. The time for symbolic execution is a small fraction of the total time taken (Ts). The probability \nbounds computation (Tvc ) time dominates the overall analysis time. The probability bounds computation \nperforms well on many of the examples, solving large problems with as many as 50 integer and real variables. \nIn many cases, the probability bounds obtained are quite tight, indicating that the volume computation \nconverged to fairly precise bounds rapidly. There are two notable exceptions that include the VOL and \nCART examples. The path conditions in these examples produce highly skewed polyhedra that are the worst-cases \nfor a branch and bound approach. As a result, the bounds produced are quite pessimistic. We are investigating \nthe use of zonotope and ellipsoidal based approximations that can handle such instances effectively. \nNext, we observe that while 0.95 is the target coverage, the coverage goal is not con.rmed by the probability \nbounds in two instances ( ALIGN-1,2). However, the actual reported coverages in these examples are fairly \nclose to the target. Note that a failure to achieve coverage goal is not a failure of the technique. \nOur ability to deduce rigorous bounds on the path probability helps ensure that the overall result can \nbe made sound regardless of what coverage we aim for. Furthermore, we may remedy the situation by iteratively \nsearching for more paths until the goals are satis.ed. Table 6 shows the results on many probabilistic \nqueries over the benchmarks. Once again, we report on the time taken for the volume computation and the \nbounds on the queries. We note that whenever path coverage returns a tight bound on the path proba\u00adbilities, \nthe estimation of assertion probabilities also yields similar tight bounds. The bounds provide non-trivial \ninformation about the Table 6. Query processing results on the benchmark examples. Table 5 explains the \nabbreviations used. Bounds in the table include the over-estimation from uncovered paths inferred from \nthe lower bound on c reported in Table 5. Query Tvc pmc [lb, ub] ARTRIAL (2520 paths, c = 0.82809) score \n>= 10 err >= 5 err <= 5 252 48 1076 0.13455 0.0003637 0.94057 [0.12379,0.31685] [0.000257,0.17754] [0.82783,1] \nCORONARY (1239 paths, c = 0.87239) err >= 5.0 22 0.000154 [0,0.12778] err >= 7.0 17 0 [0,0.12778] err \n>= 10.0 17 0 [0,0.12778] err <= -5.0 16 0.0001 [0,0.12778] err <= -7.0 17 0 [0,0.12778] err <= -10.0 \n17 0 [0,0.12778] ALIGN-1 ( 529 paths, c = .90834 ) attempts >= 10 17.0 0.00726 [.00722,0.09889] attempts \n>= 6 24.7 0.11796 [0.11773,0.20997] attempts >= 2 25.6 0.630489 [0.62991,0.72234] err >= 12 11.1 0.001580 \n[0.001570,.093245] err <= -12 10.5 0.001841 [0.001834,0.0935130] err >= 5 21.7 0.084092 [0.084035,.175794] \nerr <= -5 21.8 0.084455 [0.084396,.176163] PACK (1010 paths, c = 0.95051) count = 5 24.3 0.95051 [0.95051 \n, 1.0] count = 6 24.3 0.40364 [0.40364 , 0.45312] count = 7 22.7 0.14192 [0.14192 , 0.19140] count = \n10 21.0 0.000467 [0.000467 , 0.04995] totalWeight = 6 43.7 0.27269 [0.25223 , 0.3420] totalWeight = 5 \n34.9 0.67754 [0.61734 , 0.78481] totalWeight = 4 24.0 0.95051 [0.95051 , 1.0] INVPEND (1 path, c = 1) \npAng <= 1 4.1 0.05138 [0,0.13653] pAng >= -1 0.1 1 [1,1] pAng <= 0.1 0.1 0 [0,0] pAng >= -0.1 0.1 1 [1,1] \n behavior of these programs and are quite hard to estimate by hand even if the programs in question are \nsmall. 6. Discussion and Related Work Reasoning about in.nite-state probabilistic programs is considered \nto be a hard problem. The execution of small and seemingly simple programs that use standard uniform \nrandom number generators can yield complex probability distributions over the program variables. Arithmetic \noperations often give rise to highly correlated program variables whose joint distributions cannot be \nfactored as a product of marginals. Conditional branches and loops can lead to discon\u00adtinuous distributions \nthat cannot be described by simple probability density functions, in general. Therefore, analysis algorithms \nfor probabilistic programs must provide solutions to three basic questions: (a) Representing the pos\u00adsible \nintermediate distributions of variables, (b) Propagating the chosen representation according to the program \nsemantics, and (c) Reasoning about the probabilities of assertions using the given in\u00adtermediate distribution \nrepresentation. Any viable solution for in.\u00adnite state probabilistic programs must restrict the set of \nprograms analyzed through some syntactic or semantic criteria, or deal with information loss due to the \nabstraction of intermediate probability distributions. We now compare our work against other approaches \nfor probabilistic program analysis using these criteria. Probabilistic Abstract Interpretation Monniaux \ns work con\u00adstructs probabilistic analyses by annotating abstract domains such as intervals, octagons \nand polyhedra with upper bounds on the probability measure associated with the abstract objects [28]. \nHow\u00ad ever, the abstraction used by Monniaux associates a measure bound for the entire object, without \ntracking how the overall measure is distributed amongst the individual states present in the concretiza\u00adtion. \nThis restriction makes the resulting analysis quite conserva\u00adtive. The bounds associated with abstract \nobjects become coarser during the course of analysis due to repeated meet and join oper\u00adations. The domain \nof Mardziel et al [25] addresses this limitation for the case of integer-valued program variables. Their \napproach tracks upper and lower bounds for the object as a whole, as well as bounds on the measure associated \nwith each lattice point. This allows the measure associated with an abstract object to be updated due \nto meet, join and projection operations. Our domain can be seen as a disjunction of symbolic states \n(see Def. 3.1), and furthermore our analysis does not require meet or join operations over distributions. \nSimilarly, no upfront explicit probabilistic bounds are attached to symbolic states during our analysis, \nunlike the work of Monniaux or Mardziel et al [25, 28]. On the other hand, the probabilistic bounds on \nsymbolic states are computed after the fact (using probability bounds computation) when the entire path \nhas been analyzed. The sources of loss in our approach include the probability bounds computation and \nthe unexplored program paths. The approaches mentioned thus far, including ours, do not han\u00addle non-linear \noperations. Furthermore, the high complexity of do\u00admain operations on convex polyhedra and approximate \nvolume bound computations can limit the size of programs that can be an\u00adalyzed. The work of Bouissou \net al tackles these problems through the domain of probabilistic af.ne forms [3] by combining tech\u00adniques \nfrom the AI community such as p-Boxes and Dempster-Shafer structures [9, 35] with af.ne forms [7]. Their \napproach rep\u00ad resents variables as af.ne expressions involving two types of ran\u00addom variables (noise \nsymbols): noise symbols that are correlated with the other symbols in an arbitrary, unspeci.ed manner, \nand independent noise symbols. The concept of arbitrarily correlated noise symbols is a key contribution \nthat supports transfer functions for nonlinear operations. However, at the time of writing, Bouissou \net al do not provide meet, join and widening operations. Therefore, their technique is currently restricted \nto straight line probabilistic programs without conditional tests. Our approach uses symbolic states \n(see Def. 3.1) that are a special case of constrained proba\u00ad bilistic af.ne forms with linear inequality \nconstraints over the ran\u00addom variables. However, our approach does not currently handle random variables \nwith arbitrary, unspeci.ed correlations. Extend\u00ading our approach to treat correlated noise symbols will \nform an important part of our future work in this space. Whereas the techniques described so far perform \na forward propagation of the distribution information, it is possible to use backward abstract interpretation \nstarting from an assertion whose probability is to be established, and exploring its preconditions backwards \nalong the program paths. McIver and Morgan proposed a backward abstract interpretation for probabilistic \nprograms with discrete Bernoulli random variables and demonic non-determinism. Their approach uses expectations \nthat are real-valued functions of the program state [26] rather than assertions over the program variable. \nExpectations can be seen as abstractions of distributions. A notable aspect of their work lies in the \nuse of quantitative loop in\u00advariants that are invariant expectations of loops in the program. The automatic \ninference of such invariants was addressed by the recent work of Katoen et al [20]. It is notable that \nvery few approaches, in\u00ad cluding ours, have sought to provide a complete treatment of loops. It is common \nto assume that the loops terminate in all cases after a .xed number of iterations. The combination of \nquantitative loop invariants and the idea of using pre-/post-condition annotations are notable in the \nwork of McIver and Morgan (ibid). A combined ap\u00adproach will form an interesting topic for future work. \n DiPierro et al proposed an entirely different approach to proba\u00adbilistic abstract interpretation that \nviews concrete and abstract do\u00admains as Hilbert spaces, with the abstraction operator as a non\u00adinvertible \nlinear operator [32]. Rather than de.ning the inverse in terms of a set of concrete states, as is traditionally \ndone in ab\u00adstract interpretation, the authors propose to use a Moore-Penrose pseudo-inverse, which corresponds \nto a least squares approxima\u00adtion in .nite dimensional vector spaces. This approach exposes an interesting \nalternative to the traditional abstract interpretation ap\u00adproach to program analysis. However, a key \nproblem lies in relating the results of the analysis proposed by DiPierro et al that provide a close \napproximation to the probability of a query assertion with the classical approach that provides a guaranteed \ninterval bound enclosing the actual probability, such as the approach presented in this paper. The recent \nwork of Cousot and Monerau provides a general framework that encompasses a variety of probabilistic abstract \nin\u00adterpretation schemes [6]. Various techniques mentioned thus far, including ours, can be seen as instances \nof this general frame\u00adwork [3, 6, 25, 28]. The program analysis techniques mentioned thus far focus on \nproviding bounds on the probabilities of assertions for the program as a whole. In many cases, the same \nquestions may be asked of a single path or a .nite set of paths in the program. Recently, Gelden\u00adhuys \net al proposed the integration of symbolic execution with ex\u00adact model counting techniques to estimate \nthe probabilities of vi\u00adolating assertions along individual paths of the program [13]. The tool LattE \nwas used to count lattice points, and thus estimate poly\u00adhedral volumes for the constraints obtained \nalong each path [8]. Like Geldenhuys et al, the approach here also performs a sym\u00adbolic execution along \na chosen set of program. However, our work goes further to infer whole program bounds. Our work computes \nprobability bounds using a branch-and-bound technique, whereas Geldenhuys et al use the solver LattE \noff the shelf for computing precise probabilities. Finally, the approach of Geldenhuys et al is restricted \nto discrete, uniform random variables that take on a .nite set of values. An extension by Filieri et \nal., that is contemporaneous to our work, performs probabilistic symbolic execution with a user\u00adspeci.ed \ncutoff on the path length [11]. Their approach uses the concept of con.dence to estimate the probability \nof the paths that are cutoff prematurely. This is remarkably similar to the con\u00adcept coverage bounds \nused in our work. On one hand, the work of Filieri et al. goes beyond ours to consider the effect of \nnon\u00addeterministic schedulers on multi-threaded programs. On the other, their work continues to use LattE \nas a lattice counting tool to es\u00adtimate probabilities. Therefore, it is restricted to uniform distribu\u00adtions \nor discretized distributions over .nite data domains. Integrat\u00ading the contributions of Filieri et al. \nwith our volume bounding approach can help us move towards a technique that can support integers along \nwith reals and a wide variety of distributions without requiring discretization. Visser et al presented \na set of optimizations that can be used to reduce and simplify the constraints occurring in a program \nin a canonical form, so that the answer from one volume computation can be cached and reused [36]. Currently, \nwe observe that differ\u00ad ent program paths explored in our framework necessarily yield dif\u00adferent constraints. \nHowever, the idea of caching and reusing can be useful for intermediate polyhedra that are obtained during \nthe branch and bound volume computation process. We plan to explore this idea as part of our future work. \nA large volume of work has focused purely on verifying prop\u00aderties of .nite state discrete and continuous \ntime Markov chains and Markov decision processes [23]. The tool PRISM integrates many of these techniques, \nand has been used successfully in many scienti.c and engineering applications as showcased on-line [24]. \nA key aspect of PRISM is the integration of symbolic techniques for representing discrete distributions \nover a large but .nite set of states using extensions to decision diagrams called MTBDDs [25]. Recently, \nin.nite state systems such as probabilistic timed and hy\u00adbrid automata have been considered (Cf. [16, \n22], for instance) in the context of the model-checking approach embodied by PRISM. However, these extensions \nremain inside the decidable sub-classes. Our approach, on the other hand, considers Turing complete, \nin.\u00adnite state programs, incorporating a rich set of probability distribu\u00adtions. Therefore, we do not \ncompute probabilities precisely, settling for intervals that can be useful in many cases. Whereas work \non PRISM uses a very rich temporal speci.cation language, our work is currently restricted to (conditional) \nprobabilities of assertions. Sources of Probabilistic Programs: Probabilistic programs arise in many \nforms of computation involving erroneous or noisy inputs. These include risk analysis, medical decision \nmaking, data anal\u00adysis [1], randomized algorithms [30], differential privacy mecha\u00adnisms [10], simulations \nof fundamental physical processes (Monte-Carlo simulations) [12]. Certain loop optimizations such as \nloop perforation depend on the use of randomization to trade off perfor\u00admance against the precision of \nthe computed results [27]. Program smoothing is an approach to program synthesis proposed by Chaud\u00adhuri \net al [4]. It creates a probabilistic program that is a smoothed version of the original program obtained \nby adding noise to the inputs of the program. Many programming language formalisms have been considered \nfor expressing probabilistic programs including IBAL, a declara\u00adtive language for specifying probabilistic \nmodels [31] probabilistic scheme [33] and Church, an extension of probabilistic scheme [14]. These languages \nsupport a variety of functions such as simula\u00adtions, computing expectations, parameter estimations and \ncomput\u00ading marginal distributions based on Monte-Carlo simulations. Probabilistic Program Semantics: \nThe semantics of imperative probabilistic programs was .rst studied in detail by Kozen. [21]. Signi.cantly, \nKozen provides two types of semantics that are math\u00adematically equivalent but represent different views \nof the proba\u00adbilistic program. The .rst semantics is operational, wherein the program s execution is \nseen as essentially deterministic but gov\u00aderned by a countable set of random numbers generated up front. \nThe second semantics considered by Kozen uses measure theoretic concepts to describe the program as a \ntransformer of probability measure starting from the initial measure which is transformed by the execution \nof the program. This semantics has been the basis for further work, notably by Monniaux [29] and recently \nby Cousot and Monerau [6] on de.ning the semantics of programs that combine probabilistic and non-deterministic \nbehaviors. Our work here uses a simple operational de.nition for the meaning of probabilistic pro\u00adgrams. \nAs a result, we lose the generality that can be achieved by a fundamental measure theoretic treatment \nespoused by the sepa\u00adrate works of Kozen, Monniaux and Cousot. On the other hand, the simplicity of our \nsemantics ensures that we can make inferences on individual program paths and combine them to reason \nabout the program as a whole. Doing so also avoids a loss in precision for many of our benchmarks. Abstract \nMonte-Carlo The abstract Monte-Carlo (AMC) ap\u00adproach of Monniaux reasons about programs that combine \nprob\u00adabilistic and non-deterministic choices [28]. Therein, calls to ran\u00ad dom variables are dealt with \nusing numerical sampling while the non-deterministic choices are explored using abstract interpreta\u00adtion. \nThe AMC approach cannot, in general, be used to guarantee rigorous bounds on path probabilities, unlike \nour approach which can derive such bounds. However, our approach does not incorpo\u00adrate non-determinism. \n Statistical Model Checking Statistical Model Checking (SMC) is yet another recent approach to verify \nprobabilistic properties of systems with high con.dence by using techniques from the .eld of statistical \nhypothesis testing [5, 37]. Using .nitely many samples from the program, their approach can provide high \ncon.dence answers to questions about the probabilities of assertions. Whereas the SMC approach seeks \nto estimate whether a given lower or upper bound holds on the actual probability with high con.dence, \nwe attempt to estimate guaranteed bounds. Volume Computation Our technique integrates the path con\u00adstraints \nwith the problem of .nding the probability of satisfaction of constraints. This is known to be a .P -complete \nproblem and hence computationally hard (almost as hard as PSPACE-complete problems) [2]. The tool LattE \nMachiato integrates many state of the art techniques for counting lattice points in integer polyhedra \nand volume computation for convex polyhedra over the reals [8]. As mentioned earlier, the probabilistic \nsymbolic execution approach of Geldenhuys et al [13] and the probabilistic abstract interpreta\u00ad tion \nproposed by Mardziel et al [25] use the tool LattE to count the number of lattice points exactly. While \nLattE computes exact counts/volumes, our approach focuses on .nding interval bounds for the probabilities. \nWe observed two limitations of LattE that make it less ideal for probabilistic program analysis tasks: \n(a) the existing implementation does not handle non-uniform distributions (over the reals or integers), \nand (b) exact volume determination for real polyhedra is quite expensive, and often runs out of time/mem\u00adory \non the constraints that are obtained from our benchmarks. We attempted to complete at least one of the \nbenchmark examples in our approach using LattE, but were unsuccessful due to timeouts or more often out-of-memory \nerrors. Our approach, which focuses on .nding interval bounds rather than exact computation, can han\u00addle \nprograms over reals as well as integers, and a wide variety of distributions in addition to the uniform \ndistribution. 7. Conclusion and Future Work We have developed an analysis framework for probabilistic \npro\u00adgrams based on considering .nitely many paths and estimating the probability of the remainder. Our \ninitial experimental results are promising. However, we have also identi.ed some challenges for motivating \nfurther work. Our future work will focus on more ef.\u00adcient polyhedral probability estimators using ellipsoidal \nand zono\u00adtope approximations, and the problem of synthesizing sketches, complementing recent approaches \nto this problem [4]. Acknowledgments We thank Prof. Michael Hicks and the anonymous reviewers for their \ndetailed comments. Sankaranarayanan and Chakarov ac\u00adknowledge support from the US National Science Foundation \n(NSF) under award numbers CNS-0953941, CNS-1016994 and CPS-1035845. All opinions expressed are those \nof the authors and not necessarily of the NSF. References [1] C. C. Aggarwal and P. S. Yu. A survey of \nuncertain data algorithms and applications. IEEE Transactions on Knowledge and Data Engineering (TKDE), \n21(5), May 2009. [2] S. Arora and B. Barak. Computational Complexity: A Modern Ap\u00adproach. Cambridge University \nPress, 2009. [3] O. Bouissou, E. Goubault, J. Goubault-Larrecq, and S. Putot. A generalization of p-boxes \nto af.ne arithmetic. Computing, 2012. [4] S. Chaudhuri and A. Solar-Lezama. Smoothing a program soundly \nand robustly. In CAV, volume 6806 of LNCS, pages 277 292. Springer, 2011. [5] E. Clarke, A. Donze, and \nA. Legay. Statistical model checking of analog mixed-signal circuits with an application to a third order \nd - s modulator. In Hardware and Software: Veri.cation and Testing, volume 5394/2009 of LNCS, pages 149 \n163, 2009. [6] P. Cousot and M. Monerau. Probabilistic abstract interpretation. In ESOP, volume 7211 \nof LNCS, pages 169 193. Springer, 2012. [7] L. H. de Figueiredo and J. Stol.. Self-validated numerical \nmethods and applications. In Brazilian Mathematics Colloquium monograph. IMPA, Rio de Janeiro, Brazil, \n1997. Cf. http://www.ic.unicamp. br/~stolfi/EXPORT/papers/by-tag/fig-sto-97-iaaa.ps. gz. [8] J. De Loera, \nB. Dutra, M. Koeppe, S. Moreinis, G. Pinto, and J. Wu. Software for Exact Integration of Polynomials \nover Polyhedra. ArXiv e-prints, July 2011. [9] A. Dempster. A generalization of bayesian inference. Journal \nof the Royal Statistical Society, 30:205 247, 1968. [10] C. Dwork. Differential privacy: A survey of \nresults. In TAMC, volume 4978 of LNCS, pages 1 19. Springer, 2008. [11] A. Filieri, C. S. P.as .areanu, \nand W. Visser. Reliability analysis in symbolic path.nder. In Intl. Conference on Software Engg. (ICSE), \n2013. (To Appear, May 2013). [12] D. Frenkel and B. Smit. Understanding Molecular Simulation: From Algorithms \nto Applications. Academic Press, 2002. [13] J. Geldenhuys, M. B. Dwyer, and W. Visser. Probabilistic \nsymbolic execution. In ISSTA, pages 166 176. ACM, 2012. [14] N. D. Goodman, V. K. Mansinghka, D. M. Roy, \nK. Bonawitz, and J. B. Tenenbaum. Church: a language for generative models. In Uncertainty in Arti.cial \nIntelligence, pages 220 229, 2008. [15] L. Granvilliers and F. Benhamou. Algorithm 852: Realpaver: an \ninterval solver using constraint satisfaction techniques. ACM Trans. On Mathematical Software, 32(1):138 \n156, 2006. [16] H. Hermanns, B. Wachter, and L. Zhang. Probabilistic CEGAR. In CAV, volume 5123 of LNCS, \npages 162 175. Springer, 2008. [17] S. K. Jha, E. M. Clarke, C. J. Langmead, A. Legay, A. Platzer, and \nP. Zuliani. A bayesian approach to model checking biological systems. In CMSB, volume 5688 of Lecture \nNotes in Computer Science, pages 218 234. Springer, 2009. [18] R. Jhala and R. Majumdar. Path slicing. \nIn PLDI 05, pages 38 47. ACM, 2005. [19] C. Jones, G. McQuillan, and et al. Serum creatinine levels in \nthe US population: Third national health and nutrition examination survey. Am. J. Kidney Disease, 32(6):992 \n999, 1998. [20] J.-P. Katoen, A. McIver, L. Meinicke, and C. Morgan. Linear-invariant generation for \nprobabilistic programs. In Static Analysis Symposium (SAS), volume 6337 of LNCS, page 390406. Springer, \n2010. [21] D. Kozen. Semantics of probabilistic programs. J. Computer and System Sciences, 22:328 350, \n1981. [22] M. Kwiatkowska, G. Norman, and D. Parker. A framework for veri.\u00adcation of software with time \nand probabilities. In FORMATS, volume 6246 of LNCS, pages 25 45. Springer, 2010. [23] M. Kwiatkowska, \nG. Norman, and D. Parker. PRISM 4.0: Veri.cation of probabilistic real-time systems. In CAV, volume 6806 \nof LNCS, pages 585 591. Springer, 2011. [24] Kwiatkowska et al. The PRISM model checker. http://www. \nprismmodelchecker.org. [25] P. Mardziel, S. Magill, M. Hicks, and M. Srivatsa. Dynamic enforce\u00adment of \nknowledge-based security policies. In Computer Security Foundations Symposium (CSF), pages 114 128, JUN \n2011. [26] A. McIver and C. Morgan. Abstraction, Re.nement and Proof for Probabilistic Systems. Monographs \nin Computer Science. Springer, 2004. [27] S. Misailovic, D. M. Roy, and M. C. Rinard. Probabilistically \naccurate program transformations. In Static Analysis Symposium, volume 6887 of LNCS, pages 316 333. Springer, \n2011. [28] D. Monniaux. An abstract monte-carlo method for the analysis of probabilistic programs. In \nPOPL, pages 93 101. ACM, 2001. [29] D. Monniaux. Abstract interpretation of programs as markov decision \nprocesses. Sci. Comput. Program., 58(1-2):179 205, 2005. [30] R. Motwani and P. Raghavan. Randomized \nAlgorithms. Cambridge University Press, 1995. [31] A. Pfeffer. IBAL: a probabilistic rational programming \nlanguage. In In Proc. 17th IJCAI, pages 733 740. Morgan Kaufmann Publishers, 2001. [32] A. D. Pierro, \nC. Hankin, and H. Wiklicky. Probabilistic .-calculus and quantitative program analysis. J. Logic and \nComputation, 15(2): 159 179, 2005. [33] A. Radul. Report on the probabilistic language scheme. In DLS, \npages 2 10. ACM, 2007. [34] R. Y. Rubinstein and D. P. Kroese. Simulation and the Monte Carlo Method. \nWiley Series in Probability and Mathematical Statistics, 2008. [35] G. Shafer. A Mathematical Theory \nof Evidence. Princeton University Press, 1976. [36] W. Visser, J. Geldenhuys, and M. B. Dwyer. Green: \nreducing, reusing and recycling constraints in program analysis. In SIGSOFT FSE, page 58. ACM, 2012. \n[37] H. L. S. Younes and R. G. Simmons. Statistical probabilitistic model checking with a focus on time-bounded \nproperties. Information &#38; Computation, 204(9):1368 1409, 2006.   \n\t\t\t", "proc_id": "2491956", "abstract": "<p>We propose an approach for the static analysis of probabilistic programs that sense, manipulate, and control based on uncertain data. Examples include programs used in risk analysis, medical decision making and cyber-physical systems. Correctness properties of such programs take the form of queries that seek the probabilities of assertions over program variables. We present a static analysis approach that provides guaranteed interval bounds on the values (assertion probabilities) of such queries. First, we observe that for probabilistic programs, it is possible to conclude facts about the behavior of the entire program by choosing a finite, adequate set of its paths. We provide strategies for choosing such a set of paths and verifying its adequacy. The queries are evaluated over each path by a combination of symbolic execution and probabilistic volume-bound computations. Each path yields interval bounds that can be summed up with a \"coverage\" bound to yield an interval that encloses the probability of assertion for the program as a whole. We demonstrate promising results on a suite of benchmarks from many different sources including robotic manipulators and medical decision making programs.</p>", "authors": [{"name": "Sriram Sankaranarayanan", "author_profile_id": "81100300829", "affiliation": "University of Colorado Boulder, Boulder, CO, USA", "person_id": "P4149087", "email_address": "srirams@colorado.edu", "orcid_id": ""}, {"name": "Aleksandar Chakarov", "author_profile_id": "81485646372", "affiliation": "University of Colorado Boulder, Boulder, CO, USA", "person_id": "P4149088", "email_address": "aleksandar.chakarov@colorado.edu", "orcid_id": ""}, {"name": "Sumit Gulwani", "author_profile_id": "81100315615", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P4149089", "email_address": "sumitg@microsoft.com", "orcid_id": ""}], "doi_number": "10.1145/2491956.2462179", "year": "2013", "article_id": "2462179", "conference": "PLDI", "title": "Static analysis for probabilistic programs: inferring whole program properties from finitely many paths", "url": "http://dl.acm.org/citation.cfm?id=2462179"}