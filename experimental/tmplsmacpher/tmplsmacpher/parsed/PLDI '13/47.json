{"article_publication_date": "06-16-2013", "fulltext": "\n Halide: A Language and Compiler for Optimizing Parallelism, Locality, and Recomputation in Image Processing \nPipelines Jonathan Ragan-Kelley Connelly Barnes Andrew Adams MIT CSAIL Adobe MIT CSAIL Sylvain Paris \nFr\u00e9do Durand Saman Amarasinghe Adobe MIT CSAIL MIT CSAIL Abstract Image processing pipelines combine \nthe challenges of stencil com\u00adputations and stream programs. They are composed of large graphs of different \nstencil stages, as well as complex reductions, and stages with global or data-dependent access patterns. \nBecause of their complex structure, the performance difference between a naive im\u00adplementation of a pipeline \nand an optimized one is often an order of magnitude. Ef.cient implementations require optimization of \nboth parallelism and locality, but due to the nature of stencils, there is a fundamental tension between \nparallelism, locality, and introducing redundant recomputation of shared values. We present a systematic \nmodel of the tradeoff space fundamen\u00adtal to stencil pipelines, a schedule representation which describes \nconcrete points in this space for each stage in an image processing pipeline, and an optimizing compiler \nfor the Halide image process\u00ading language that synthesizes high performance implementations from a Halide \nalgorithm and a schedule. Combining this compiler with stochastic search over the space of schedules \nenables terse, composable programs to achieve state-of-the-art performance on a wide range of real image \nprocessing pipelines, and across different hardware architectures, including multicores with SIMD, and \nhetero\u00adgeneous CPU+GPU execution. From simple Halide programs writ\u00adten in a few hours, we demonstrate \nperformance up to 5\u00d7 faster than hand-tuned C, intrinsics, and CUDA implementations optimized by experts \nover weeks or months, for image processing applications beyond the reach of past automatic compilers. \nCategories and Subject Descriptors D.3.4 [Programming Lan\u00adguages]: Processors compilers, optimization, \ncode generation Keywords domain speci.c language; compiler; image processing; locality; parallelism; \nredundant computation; optimization; GPU; vectorization 1. Introduction Image processing pipelines are \neverywhere, and are essential to capturing, analyzing, mining, and rendering the rivers of visual information \ngathered by our countless cameras and imaging-based sensors. Applications from raw processing, to object \ndetection and recognition, to Microsoft s Kinect, to Instagram and Photoshop, to Permission to make digital \nor hard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. PLDI 13, June 16 19, 2013, Seattle, WA, USA. \nCopyright c @ 2013 ACM 978-1-4503-2014-6/13/06. . . $15.00 medical imaging and neural scanning all demand \nextremely high performance to cope with the rapidly rising resolution and frame rate of image sensors \nand the increasing complexity of algorithms. At the same time, the shrinking cameras and mobile devices \non which they run require extremely high ef.ciency to last more than a few minutes on battery power. \nWhile power-hungry radios and video CODECs can implement slow-changing standards in custom hardware, \nimage processing pipelines are rapidly evolving and diverse, requiring high performance software implementations. \nImage processing pipelines combine the challenges of stencil computation and stream programs. They are \ncomposed of large graphs of many different operations, most of which are stencil computations. These \npipelines are simultaneously wide and deep: each stage exhibits data parallelism across the many pixels \nwhich it must process, and whole pipelines consist of long sequences of different operations, which individually \nhave low arithmetic intensity (the ratio of computation performed to data read from prior stages and \nwritten to later stages). For example, an implementation of one recent algorithm, local Laplacian .lters \n[3, 22], is a graph of 99 different stages (Fig. 1), including many different stencils and a large data-dependent \nresampling. As a result of this structure, the performance difference between a naive implementation \nof a given pipeline and a highly optimized one is frequently an order of magnitude or more. With current \ntools, often the only way to approach peak performance is by hand-writing parallel, vectorized, tiled, \nand globally fused code in low-level C, CUDA, intrinsics, and assembly. Simple pipelines become hun\u00addreds \nof lines of intricately interleaved code; complex pipelines, like Adobe s Camera Raw engine, become hundreds \nof thousands. Tun\u00ading them requires immense effort by expert programmers, and the end result is not portable \nto different architectures, nor composable with other algorithms, without sacri.cing much of this painstakingly \nearned performance. Libraries of optimized subroutines do not solve the problem, either, since many critical \noptimizations involve fusion for producer-consumer locality across stages. We address this challenge \nby raising the level of abstraction, and decoupling the algorithm de.nition from its execution strategy, \nto improve portability and composability, while automating the search for optimized mappings of the resulting \npipelines to parallel ma\u00adchines and complex memory hierarchies. Effective abstraction and automatic optimization \nenable radically simpler programs to achieve higher performance than hand-tuned expert implementations, \nwhile running across a wide range of architectures. 1.1 Image Processing Pipelines Stencils have been \nwell studied in scienti.c applications in the form of iterated stencil computations, where one or a few \nsmall stencils are applied to the same grid over many iterations [10, 16, 19]. In contrast, we are interested \nin other applications, in image processing Figure 1. Imaging pipelines employ large numbers of interconnected, \nheterogeneous stages. Here we show the structure of the local Laplacian .lter [3, 22], which is used \nfor a variety of tasks in photographic post-production. Each box represents intermediate data, and each \narrow represents one or more functions that de.ne that data. The pipeline includes horizontal and vertical \nstencils, resampling, data-dependent gathers, and simple pointwise functions.  and computer graphics, \nwhere stencils are common, but often in a very different form: stencil pipelines. Stencil pipelines are \ngraphs of different stencil computations. Iteration of the same stencil occurs, but it is the exception, \nnot the rule; most stages apply their stencil only once before passing data to the next stage, which \nperforms different data parallel computation over a different stencil. Graph-structured programs have \nbeen studied in the context of streaming languages [4, 11, 29]. Static communication analy\u00adsis allows \nstream compilers to simultaneously optimize for data parallelism and producer-consumer locality by interleaving \ncompu\u00adtation and communication between kernels. However, most stream compilation research has focussed \non 1D streams, where sliding win\u00addow communication allows 1D stencil patterns. Image processing pipelines \ncan be thought of as programs on 2D and 3D streams and stencils. The model of computation required by \nimage processing is also more general than stencils, alone. While most stages are point or stencil operations \nover the results of prior stages, some stages gather from arbitrary data-dependent addresses, while others \nscatter to arbitrary addresses to compute operations like histograms. Pipelines of simple map operations \ncan be optimized by tradi\u00adtional loop fusion: merging multiple successive operations on each point into \na single compound operation improves arithmetic intensity by maximizing producer-consumer locality, keeping \nintermediate data values in fast local memory (caches or registers) as it .ows through the pipeline. \nBut traditional loop fusion does not apply to stencil operations, where neighboring points in a consumer \nstage depend on overlapping regions of a producer stage. Instead, sten\u00adcils require a complex tradeoff \nbetween producer-consumer locality, synchronization, and redundant computation. Because this tradeoff \nis made by interleaving the order of allocation, execution, and com\u00admunication of each stage, we call \nit the pipeline s schedule. These tradeoffs exist in scheduling individual iterated stencil computations \nin scienti.c applications, and the complexity of the choice space is re.ected by the many different tiling \nand scheduling strategies introduced in past work [10, 16, 19]. In image processing pipelines, this tradeoff \nmust be made for each producer-consumer relationship between stages in the graph often dozens or hundreds \nand the ideal schedule depends on the global interaction among every stage, often requiring the composition \nof many different strategies.  1.2 Contributions Halide is an open-source domain-speci.c language for \nthe complex image processing pipelines found in modern computational pho\u00adtography and vision applications \n[26]. In this paper, we present the optimizing compiler for this language. We introduce: a systematic \nmodel of the tradeoffs between locality, parallelism, and redundant recomputation in stencil pipelines; \n a scheduling representation that spans this space of choices;  a DSL compiler based on this representation \nthat combines Halide programs and schedule descriptions to synthesize points anywhere in this space, \nusing a design where the choices for how to execute a program are separated not just from the de.nition \nof what to compute, but are pulled all the way outside the black box of the compiler;  a loop synthesizer \nfor data parallel pipelines based on simple interval analysis, which is simpler and less expressive than \npolyhedral model, but more general in the class of expressions it can analyze;  a code generator that \nproduces high quality vector code for image processing pipelines, using machinery much simpler than the \npolyhedral model;  and an autotuner that can infer high performance schedules up to 5\u00d7 faster than hand-optimized \nprograms written by experts for complex image processing pipelines using stochastic search.  Our scheduling \nrepresentation composably models a range of tradeoffs between locality, parallelism, and avoiding redundant \nwork. It can naturally express most prior stencil optimizations, as well as hierarchical combinations \nof them. Unlike prior stencil code generation systems, it does not describe just a single stencil scheduling \nstrategy, but separately treats every producer-consumer edge in a graph of stencil and other image processing \ncomputations. Our split representation, which separates schedules from the underlying algorithm, combined \nwith the inside-out design of our compiler, allows our compiler to automatically search for the best \nschedule. The space of possible schedules is enormous, with hundreds of inter-dependent dimensions. It \nis too high dimensional for the polyhedral optimization or exhaustive parameter search employed by existing \nstencil compilers and autotuners. However, we show that it is possible to discover high quality schedules \nusing stochastic search. Given a schedule, our compiler automatically synthesizes high quality parallel \nvector code for x86 and ARM CPUs with SSE/AVX and NEON, and graphs of CUDA kernels interwoven with host \nmanagement code for hybrid GPU execution. It automatically infers all internal allocations and a complete \nloop nest using simple but general interval analysis [18]. Directly mapping data parallel dimensions \nto SIMD execution, including careful treatment of strided access patterns, enables high quality vector \ncode generation, without requiring any general-purpose loop auto-vectorization. The end result is a \nsystem which enables terse, composable programs to achieve state-of-the-art performance on a wide range \nof real image processing pipelines, and across different hardware architectures, including multicores \nwith SIMD, and heterogeneous CPU+GPU execution. From simple Halide programs written in a few hours, we \ndemonstrate performance up to 5\u00d7 faster than hand\u00adtuned C, intrinsics, and CUDA implementations written \nby experts over weeks or months, for image processing applications beyond the reach of past automatic \ncompilers. 2. The Halide DSL We use the Halide DSL to describe image processing pipelines in a simple \nfunctional style [26]. A simple C++ implementation of local Laplacian .lters (Fig. 1) is described by \ndozens of loop nests and hundreds of lines of code. This is not practical to globally optimize with traditional \nloop optimization systems. The Halide version distills this into 62 lines describing just the essential \ndata.ow and computation in the 99 stage pipeline, and all choices for how the program should be synthesized \nare described separately (Sec. 3). In Halide, values that would be mutable arrays in an impera\u00adtive language \nare instead functions from coordinates to values. It represents images as pure functions de.ned over \nan in.nite inte\u00adger domain, where the value of a function at a point represents the color of the corresponding \npixel. Pipelines are speci.ed as chains of functions. Functions may either be simple expressions in their \narguments, or reductions over a bounded domain. The expressions that de.ne functions are side-effect \nfree, and are much like those in any simple functional language, including: Arithmetic and logical operations; \n Loads from external images;  If-then-else expressions;  References to named values (which may be \nfunction arguments, or expressions de.ned by a functional let construct);  Calls to other functions, \nincluding external C ABI functions.  For example, a separable 3 \u00d7 3 unnormalized box .lter is expressed \nas a chain of two functions in x, y: UniformImage in(UInt(8), 2) Var x, y Func blurx(x,y) = in(x-1,y) \n+ in(x,y) + in(x+1,y) Func out(x,y) = blurx(x,y-1) + blurx(x,y) + blurx(x,y+1) This representation is \nsimpler than most functional languages. It does not include higher-order functions, dynamic recursion, \nor additional data structures like lists. Functions simply map from integer coordinates to a scalar result. \nConstrained versions of more advanced features such as higher-order functions are added as syntactic \nsugar, but do not change the underlying representation. This representation is suf.cient to describe \na wide range of image processing algorithms, and these constraints enable .exible analysis and transformation \nof algorithms during compilation. Critically, this representation is naturally data parallel within the \ndomain of each function. Also, since functions are de.ned over an in.nite domain, boundary conditions \ncan be handled safely and ef.ciently by computing arbitrary guard bands of extra values as needed. Guard \nbands are a common pattern in image processing code, both for performance concerns like alignment, and \nfor safety. Wherever speci.c boundary conditions matter to the meaning of an algorithm, the function \nmay de.ne its own. Reduction functions. In order to express operations like his\u00adtograms and general convolutions, \nHalide also needs a way to ex\u00adpress iterative or recursive computations, like summation, histogram, and \nscan. Reductions are de.ned in two parts: An initial value function, which speci.es a value at each \npoint in the output domain.  A recursive reduction function, which rede.nes the value at points given \nby an output coordinate expression in terms of prior values of the function.  Unlike a pure function, \nthe meaning of a reduction depends on the order in which the reduction function is applied. The programmer \nspeci.es the order by de.ning a reduction domain, bounded by minimum and maximum expressions for each \ndimension. The value at each point in the output domain is de.ned by the .nal value of the reduction \nfunction at that point, after recursing in lexicographic order across the reduction domain. This pattern \ncan describe a range of algorithms outside the scope of traditional stencil computation, but essential \nto image processig pipelines, in a way that bounds side effects. For example, histogram equalization \ncombines multiple reductions and a data-dependent gather. A scattering reduction computes a histogram, \na recursive scan integrates it into a CDF, and a point-wise operation remaps the input using the CDF: \nUniformImage in(UInt(8), 2) RDom r(0..in.width(), 0..in.height()), ri(0..255) Var x, y, i Func histogram(i) \n= 0; histogram(in(r.x, r.y))++ Func cdf(i) = 0; cdf(ri) = cdf(ri-1) + histogram(ri) Func out(x, y) = \ncdf(in(x, y)) The iteration bounds for the reduction and scan are expressed by the programmer using \nexplicit reduction domains (RDoms). 3. Scheduling Image Processing Pipelines Halide s representation \nof image processing algorithms avoids imposing constraints on the order of execution and placement of \ndata. Values need to be computed before they can be used, to respect the fundamental dependencies in \nthe algorithm, but many choices remain unspeci.ed: When and where should the value at each coordinate \nin each function be computed?  Where should they be stored?  How long are values cached and communicated \nacross multiple consumers, and when are they independently recomputed by each?  These choices can not \nchange the meaning or results of the algo\u00adrithm, but they are essential to the performance of the resulting \nimplementation. We call a speci.c set of choices for when and where values are computed the pipeline \ns schedule. In the presence of stencil access patterns, these choices are bound by a fundamental tension \nbetween producer-consumer locality, parallelism, and redundant recomputation of shared values. To understand \nthis tradeoff space, it is useful to look at an example. 3.1 Motivation: Scheduling a Two-Stage Pipeline \nConsider the simple two-stage blur algorithm, which computes a 3 \u00d73 box .lter as two 3 \u00d7 1 passes. The \n.rst stage, blurx, computes a horizontal blur of the input by averaging over a 3 \u00d7 1 window: blurx(x,y) \n= in(x-1,y) + in(x,y) + in(x+1,y) The second stage, out, computes the .nal isotropic blur by averaging \na 1 \u00d7 3 window of the output from the .rst stage: out(x,y) = blurx(x,y-1) + blurx(x,y) + blurx(x,y+1) \nA natural way to think about scheduling the pipeline is from the perspective of the output stage: how \nshould it compute its input? There are three obvious choices for this pipeline.  Figure 2. A natural \nway to visualize the space of scheduling choices is by granularity of storage (x-axis), and granularity \nof computation (y-axis). Breadth-.rst execution does coarse-grain computation into coarse-grain storage. \nTotal fusion performs .ne-grain computation into .ne-grain storage (small temporary buffers). Sliding \nwindow strategies allocate enough space for the entire intermediate stage, but compute it in in .ne-grain \nchunks as late as possible. These extremes each have their pitfalls. Breadth-.rst execution has poor \nlocality, total fusion often does redundant work, and using sliding windows to avoid redundant recomputation \nconstrains parallelism by introducing dependencies across loop iterations. The best strategies tend to \nbe mixed, and lie somewhere in the middle of the space. First, it could compute and store every required \npoint in blurx before evaluating any points in out. Applied to a 6 megapixel (3k \u00d7 2k) image, this is \nequivalent to the loop nest: alloc blurx[2048][3072] for each y in 0..2048: for each x in 0..3072: blurx[y][x] \n= in[y][x-1] + in[y][x] + in[y][x+1] alloc out[2046][3072] for each y in 1..2047: for each x in 0..3072: \n out[y][x]=blurx[y-1][x] + blurx[y][x] + blurx[y+1][x] This is the most common strategy in hand-written \npipelines, and what results from composing library routines together: each stage executes breadth-.rst \nacross its input before passing its entire output to the next stage. There is abundant parallelism, since \nall the required points in each stage can be computed and stored independently, but there is little producer-consumer \nlocality, since all the values of blurx must be computed and stored before the .rst one is used by out. \nAt the other extreme, the out stage could compute each point in blurx immediately before the point which \nuses it. This opens up a further choice: should points in blurx which are used by multiple points in \nout be stored and reused, or recomputed independently by each consumer? Interleaving the two stages, \nwithout storing the intermediate results across uses, is equivalent to the loop nest: alloc out[2046][3072] \nfor each y in 1..2047: for each x in 0..3072: alloc blurx[-1..1] for each i in -1..1: blurx[i]= in[y-1+i][x-1]+in[y-1+i][x]+in[y-1+i][x+1] \n out[y][x] = blurx[0] + blurx[1] + blurx[2] Each pixel can be computed independently, providing the \nsame abundant data parallelism from the breadth-.rst strategy. The dis\u00adtance from producer to consumer \nis small, maximizing locality. But because shared values in blurx are not reused across iterations, this \nstrategy performs redundant work. This can be seen as the result of Figure 3. Different points in the \nchoice space in Figure 2 each make different trade-offs between locality, redundant recomputation, and \nparallelism. Here we quantify these effects for our two-stage blur pipeline. The span measures the degree \nof parallelism available, by counting how many threads or simd lanes could be kept busy doing useful \nwork. The Max. reuse distance measures locality, by counting the maximum number of operations that can \noccur between computing a value and reading it back. Work ampli.cation measures redundant work, by comparing \nthe number of arithmetic operations done to the breadth-.rst case. Each of the .rst three strategies \nrepresent an extreme point of the choice space, and is weak in one regard. The fastest schedules are \nmixed strategies, such as the tiled ones in the last two rows. Strategy Span (iterations) Max. reuse \ndist. (ops) Work ampl. Breadth-.rst = 3072 \u00d7 2046 3072 \u00d7 2048 \u00d7 3 1.0\u00d7 Full fusion = 3072 \u00d7 2046 3 \u00d7 \n3 2.0\u00d7 Sliding window 3072 3072 \u00d7 (3 + 3) 1.0\u00d7 Tiled = 3072 \u00d7 2046 34 \u00d7 32 \u00d7 3 1.0625\u00d7 Sliding in tiles \n= 3072 \u00d7 2048/8 3072 \u00d7 (3 + 3) 1.25\u00d7  applying classical loop fusion through a stencil dependence pattern: \nthe body of the .rst loop is moved into the second loop, but its work is ampli.ed by the size of the \nstencil. The two stages can also be interleaved while storing the values of blurx across uses: alloc \nout[2046][3072] alloc blurx[3][3072] for each y in -1..2047: for each x in 0..3072: blurx[(y+1)%3][x]=in[y+1][x-1]+in[y+1][x]+in[y+1][x+1] \nif y < 1: continue out[y][x] = blurx[(y-1)%3][x] + blurx[ y % 3 ][x] + blurx[(y+1)%3][x]  This interleaves \nthe computation over a sliding window, with out trailing blurx by the stencil radius (one scanline). \nIt wastes no work, computing each point in blurx exactly once, and the maximum distance between a value \nbeing produced in blurx and consumed in out is proportional to the stencil height (three scanlines), \nnot the entire image. But to achieve this, it has introduced a dependence between the loop iterations: \na given iteration of out depends on the last three outer loop iterations of blurx. This only works if \nthese loops are evaluated sequentially. Interleaving the stages while producing each value only once \nrequires tightly synchronizing the order of computation, sacri.cing parallelism. Each of these strategies \nhas a major pitfall: lost locality, redun\u00addant work, or limited parallelism (Fig. 3). In practice, the \nright choice for a given pipeline is almost always somewhere in between these extremes. For our two-stage \nexample, a better balance can be struck by interleaving the computation of blurx and out at the level \nof tiles: alloc out[2046][3072] for each ty in 0..2048/32: for each tx in 0..3072/32: alloc blurx[-1..33][32] \nfor y in -1..33: for x in 0..32: blurx[y][x] = in[ty*32+y][tx*32+x-1] + in[ty*32+y][tx*32+x] + in[ty*32+y][tx*32+x+1] \nfor y in 0..32: for x in 0..32: out[ty*32+y][tx*32+x] = blurx[y-1][x] + blurx[y ][x] + blurx[y+1][x] \n  This trades off a small amount of redundant computation on tile boundaries for much greater producer-consumer \nlocality, while still Figure 4. Even simple pipelines exhibit a rich space of scheduling choices, each \nexpressing its own trade-off between parallelism, locality, and redundant recompute. The choice made \nfor each stage is two-fold. The simpler choice is the domain order, which can express thread parallelism, \nvectorization, and traversal order (row-major vs column-major). Dimensions can be split into inner and \nouter components, which recursively expands the choice space, and can express tiling strategies. The \nmore complex question each stage must answer is when its inputs should be computed. Choices include computing \nall dependencies ahead of time (breadth-.rst), computing values as late as possible and then discarding \nthem (total fusion), and computing values as late as possible, but reusing previous computations (sliding \nwindow). The two categories of choice interact. For example, the fastest schedules often split the domain \ninto tiles processed in parallel, and then compute either breadth-.rst or with sliding windows within \neach tile.  leaving parallelism unconstrained both within and across tiles. (In the iterated stencil \ncomputation literature, the redundant regions are often called ghost zones, and this strategy is sometimes \ncalled overlapped tiling [17, 31].) On a modern x86, this strategy is 10\u00d7 faster than the breadth-.rst \nstrategy using the same amount of multithreaded and vector parallelism. This is because the lack of producer-consumer \nlocality leaves the breadth-.rst version limited by bandwidth. This difference grows as the pipeline \ngets longer, increasing the ratio of intermediate data to inputs and outputs, and it will only grow further \nas the computational resources scale exponentially faster than external memory bandwidth under Moore \ns Law. The very fastest strategy we found on this architecture interleaves the computation of the two \nstages using a sliding window over scanlines, while splitting the image into strips of independent scanlines \nwhich are treated separately:  alloc out[2046][3072] for each ty in 0..2048/8: alloc blurx[-1..1][3072] \nfor y in -2..8: for x in 0..3072:  blurx[(y+1)%3][x] = in[ty*8+y+1][tx*32+x-1] + in[ty*8+y+1][tx*32+x] \n+ in[ty*8+y+1][tx*32+x+1] if y < 0: continue for x in 0..3072: out[ty*8+y][x] = blurx[(y-1)%3][x] + \nblurx[ y % 3][x] + blurx[(y+1)%3][x]  Relative to the original sliding window strategy, this sacri.ces \ntwo scanlines of redundant work on the overlapping tops and bottoms of independently-processed strips \nof blurx to instead reclaim .ne-grained parallelism within each scanline and coarse\u00adgrained parallelism \nacross scanline strips. The end result is 10% faster still than the tiled strategy on one benchmark machine, \nbut 10% slower on another. The best choice between these and many other strategies varies across different \ntarget architectures. There are also global consequences to the decision made for each stage in a larger \npipeline, so the ideal choice depends on the composition of stages, not just each individual stages in \nisolation. Real image processing pipelines often have tens to hundreds of stages, making the choice space \nenormous.  3.2 A Model for the Scheduling Choice Space We introduce a model for the space of important \nchoices in schedul\u00ading stencil pipelines, based on each stage choosing at what granular\u00adity to compute \neach of its inputs, at what granularity to store each for reuse, and within those grains, in what order \nits domain should be traversed (Fig. 4). The Domain Order Our model .rst de.nes the order in which the \nrequired region of each function s domain should be traversed, which we call the domain order, using \na traditional set of loop transformation concepts: Each dimension can be traversed sequentially or in \nparallel.  Constant-size dimensions can be unrolled or vectorized.  Dimensions can be reordered (e.g. \nfrom column-to row-major).  Finally, dimensions can be split by a factor, creating two new dimensions: \nan outer dimension, over the old range divided by the factor, and an inner dimension, which iterates \nwithin the factor. After splitting, references to the original index become outer \u00d7 factor + inner. \n Splitting recursively opens up further choices, and enables many common patterns like tiling when combined \nwith other transforma\u00adtions. Vectorization and unrolling are modeled by .rst splitting a dimension by \nthe vector width or unrolling factor, and then schedul\u00ading the new inner dimension as vectorized or unrolled. \nBecause Halide s model of functions is data parallel by construction, dimen\u00adsions can be interleaved \nin any order, and any dimension may be scheduled serial, parallel, or vectorized. For reduction functions, \nthe dimensions of the reduction domain may only be reordered or parallelized if the reduction update \nis associative. Free variable dimensions may be scheduled in any order, just as with pure functions. \nOur model considers only axis-aligned bounding regions, not general polytopes a practical simpli.cation \nfor image processing and many other applications. But this also allows the regions to be de.ned and analyzed \nusing simple interval analysis. Since our model of scheduling relies on later compiler inference to determine \nthe bounds of evaluation and storage for each function and loop, it is essential that bounds analysis \nbe capable of analyzing every expression and construct in the Halide language. Interval analysis is simpler \nthan modern tools like polyhedral analysis, but it can effectively analyze through a wider range of expressions, \nwhich is essential for this design. The Call Schedule In addition to the order of evaluation within \nthe domain of each function, the schedule also speci.es the granularity of interleaving the computation \nof a function with the storage and computation of each function on which it depends. We call these choices \nthe call schedule. We specify a unique call schedule for each function in the entire call graph of a \nHalide pipeline. Each function s call schedule is de.ned as the points in the loop nest of its callers \nwhere it is stored and computed (Fig. 4, top). For example, the three extremes from the previous section \ncan be viewed along these axes: The breadth-.rst schedule both stores and computes blurx at the coarsest \ngranularity (which we call the root level outside any other loops).  The fused schedule both stores \nand computes blurx at the .nest granularity, inside the innermost (x) loop of out. At this granularity, \nvalues are produced and consumed in the same iteration, but must be reallocated and recomputed on each \niteration, independently.  The sliding window schedule stores at the root granularity, while computing \nat the .nest granularity. With this interleaving, values of blurx are computed in the same iteration \nas their .rst use, but persist across iterations. To exploit this by reusing shared values in subsequent \niterations, the loops between the storage and computation levels must be strictly ordered, so that a \nsingle unique .rst iteration exists for each point, which can compute it for later consumers.  Together, \nthe call schedule and domain order de.ne an algebra for scheduling stencil pipelines on rectangular grids. \nComposing these choices can de.ne an in.nite range of schedules, including the vast majority of common \npatterns exploited by practitioners in hand-optimized image processing pipelines. The loop transformations \nde.ned by the domain order interact with the inter-stage interleaving granularity chosen by the call \nschedule because the call schedule is de.ned by specifying the loop level at which to store or compute. \nA function call site may be stored or computed at any loop from the innermost dimensions of the directly \ncalling function, to the surrounding dimensions at which it is itself scheduled to be computed, and so \non through its chain of consumers. Splitting dimensions allows the call schedule to be speci.ed at .ner \ngranularity than the intrinsic dimensionality of the calling functions, for example interleaving by blocks \nof scanlines instead of individual scanlines, or tiles of pixels instead of individual pixels. Since \nevery value computed needs a logical location into which its result can be stored, the storage granularity \nmust be equal to, or coarser than, the computation granularity. Schedule Examples Revisiting the optimized \nexamples from Sec. 3.1, the tiled schedule can be modeled as follows: The domain order of out is split(y, \n32) . ty, y; split(x, 32) . tx, x; order(ty, tx, y, x). (This is similar to the tiled domain or\u00adder shown \nrightmost in Fig. 4.) All dimensions may be parallel. x is vectorized for performance.  The call schedule \nof blurx is set to both store and compute for each iteration of tx in out.  The domain of blurx under \ntx is scheduled order(y, x), and x is vectorized for performance.  The parallel tiled sliding window \nschedule is modeled by making the call schedule of blurx store at ty in out, but compute at .ner granularity, \nat out s y dimension. Then: The domain order of out is split(y, 8) . ty, y; order(ty, y, x). ty may \nbe parallel, x is vectorized for performance. y must be sequential, to enable reuse.  The domain of \nblurx under y only has dimension x, which is vectorized for performance.  4. Compiling Scheduled Pipelines \nOur compiler combines the functions describing a Halide pipeline, with a fully-speci.ed schedule for \neach function, to synthesize the machine code for a single procedure which implements the entire pipeline. \nThe generated pipeline is exposed as a C ABI callable function which takes buffer pointers for input \nand output data, as well as scalar parameters. The implementation is multithreaded and vectorized according \nto the schedule, internally manages the alloca\u00adtion of all intermediate storage, and optionally includes \nsynthesized GPU kernels which it also manages automatically. The compiler makes no heuristic decisions \nabout which loop transformations to apply or what will generate fast code. For all such questions we \ndefer to the schedule. At the same time, the generated code is safe by construction. The bounds of all \nloops and allocations are inferred. Bounds inference generates loop bounds that ultimately depend only \non the size of the output image. Bounded loops are our only means of control .ow, so we can guarantee \ntermination. All allocations are large enough to cover the regions used by the program. Given the functions \nde.ning a Halide pipeline and a fully speci.ed schedule as input (Fig. 5, left), our compiler proceeds \nthrough the major steps below. 4.1 Lowering and Loop Synthesis The .rst step of our compiler is a lowering \nprocess that synthesizes a single, complete set of loop nests and allocations, given a Halide pipeline \nand a fully-speci.ed schedule (Fig. 5, middle). Lowering begins from the function de.ning the output \n(in this case, out). Given the function s domain order from the schedule, it generates a loop nest covering \nthe required region of the output, whose body evaluates the function at a single point in that domain \n(Fig. 5, middle-top). The order of loops is given by the schedule, and includes additional loops for \nsplit dimensions. Loops are de.ned by their minimum value and their extent, and all loops implicitly \nstride by 1. This process rounds up the total traversed domain of dimensions which have been split to \nthe nearest multiple of the split factor, since all loops have a single base and extent expression. At \nthis stage, loop bounds are left as simple symbolic expressions of the required region of the output \nfunction, which is resolved later. The bounds cannot have inter-dependent dimensions between the loops \nfor a single function, so they represent a dense iteration over an axis-aligned bounding box. Each loop \nis labeled as being serial, parallel, unrolled, or vectorized, according to the schedule. Lowering then \nproceeds recursively up the pipeline, from callers to callees (here, from out to blurx). Callees (apart \nfrom those scheduled inline) are scheduled to be computed at the granularity of some dimension of some \ncaller function. This corresponds to an existing loop in the code generated so far. This site is located, \nand code evaluating the callee is injected at the beginning of that loop body. This code takes the form \nof a loop nest constructed using the domain order of the callee. The allocation for the callee is similarly \ninjected at some containing loop level speci.ed by the schedule. In Fig. 5, middle, blurx is allocated \nat the level of tiles (out.xo ), while it is computed as required for each scanline within the tile Figure \n5. Our compiler is driven by an autotuner, which stochastically searches the space of valid schedules \nto .nd a high performance implementation of the given Halide program. The core of the compiler lowers \na functional representation of an imaging pipeline to imperative code using a schedule. It does this \nby .rst constructing a loop nest producing the .nal stage of the pipeline (in this case out), and then \nrecursively injecting the storage and computation of earlier stages of the pipeline at the loop levels \nspeci.ed by the schedule. The locations and sizes of regions computed are symbolic at this point. They \nare resolved by the subsequent bound inference pass, which injects interval arithmetic computations in \na preamble at each loop level that set the region produced of each stage to be at least as large as the \nregion consumed by subsequent stages. Next, sliding window optimization and storage folding remove redundant \ncomputation and excess storage where the storage granularity is above the compute granularity. A simple \n.attening transform converts multidimensional coordinates in the in.nite domain of each function into \nsimple one-dimensional indices relative to the base of the corresponding buffer. Vectorization and unrolling \npasses replace loops of constant with k scheduled as vectorized or unrolled with the corresponding k-wide \nvector code or k copies of the loop body. Finally, backend code generation emits machine code for the \nscheduled pipeline via LLVM.  (out.yi ). The allocation and computation for blurx are inserted at the \ncorresponding points in the loop nest. Reductions are lowered to a pair of loop nests: the .rst initializes \nthe domain, and the second applies the reduction rule. Both alloca\u00adtion and loop extents are tracked \nas symbols of the required region of the function used by its callers. Once lowering has recursed to \nthe end of the pipeline, all functions have been synthesized into a single set of loops.  4.2 Bounds \nInference At this stage, for allocation sizes and loop bounds the pipeline relies on symbolic bounds \nvariables for each dimension of each function. The next stage of lowering generates and injects appropriate \nde.ni\u00adtions for these variables. Like function lowering, bounds inference proceeds recursively back from \nthe output. For each function, it symbolically evaluates the bounds of each dimension based on the bounds \nrequired of its caller and the symbolic indices at which the caller invokes it. At each step, the required \nbounds of each dimen\u00adsion are computed by interval analysis of the expressions in the caller which index \nthat dimension, given the previously computed bounds of all downstream functions. After bounds inference \nhas recursed to the top of the pipeline, it walks back down to the output, injecting de.nitions for the \nbounds variables used as stand-ins during lowering. They are de.ned by expressions which compute concrete \nbounds as a preamble at each loop level (e.g., in Fig. 5, right, the minimum bound of blurx.y is computed \nfrom interval analysis of the index expressions at which it is accessed combined with the bounds of the \ncalling function, out). In practice, hoisting dynamic bounds evaluation expressions to the outermost \nloop level possible makes the runtime overhead of more complex bounds expressions negligible. Interval \nanalysis is an unusual choice in a modern loop synthesis and code generation system. The resulting min/max \nbounds for each dimension are less expressive than the polyhedral model. They can only describe iteration \nover axis-aligned boxes, rather than arbitrary polytopes. However, it is trivial to synthesize ef.cient \nloops for any set of intervals, in contrast to the problem of scanning general polyhedra. For many domains, \nincluding image processing, this is an acceptable simpli.cation: most functions are applied over rectilinear \nregions. Most critically, interval analysis can analyze a more general class of expressions: it is straightforward \nto compute intervals through nearly any computation, from basic arithmetic, to conditional ex\u00adpressions, \nto transcendentals, and even loads from memory. As a result, this analysis can be used pervasively to \ninfer the complete bounds of every loop and allocation in any pipeline represented in Halide. It also \ngeneralizes through constructs like symbolic tile sizes, which are beyond the scope of polyhedral analysis. \nFor cases where interval analysis is over-conservative (e.g., when computing the bounds of a .oating \npoint number loaded from memory which the programmer knows will be between 0 and 1), Halide includes \na simple clamp operator, which simultaneously declares and enforces a bound on an expression. 4.3 Sliding \nWindow Optimization and Storage Folding After bounds inference, the compiler traverses the loop nests \nseeking opportunities for sliding window optimizations. If a realization of a function is stored at higher \nloop level than its computation, with an intervening serial loop, then iterations of that loop can reuse \nvalues generated by previous iterations. Using the same interval analysis machinery as in bounds inference, \nwe shrink the interval to be computed at each iteration by excluding the region computed by all previous \niterations. It is this transformation that lets us trade off parallelism (because the intervening loop \nmust be serial) for reuse (because we avoid recomputing values already computed by previous iterations.) \n For example, in Fig. 5, blurx is stored for reuse within each tile of out, but computed as needed, for \neach scanline within the tile. Because scanlines (out.yi ) are traversed sequentially, intermediate values \nof blurx are computed immediately before the .rst scanline of out which needs them, but may be reused \nmy later scanlines within the tile. For each iteration of out.yi , the range of blurx.y is computed to \nexclude the interval covered by all prior iterations computed within the tile. Storage folding is a second \nsimilar optimization employed at this stage of lowering. If a region is allocated outside of a serial \nloop but only used within it, and the subregion used by each loop iteration marches monotonically across \nthe region allocated, we can fold the storage, by rewriting indices used when accessing the region by \nreducing them modulo the maximum extent of the region used in any given iteration. For example, in Fig. \n5, each iteration of out.yi only needs access to the last 3 scanlines of blurx, so the storage of blurx \ncan be reduced to just 3 scanlines, and the value blurx(x,y+3) will reuse the same memory address as \nblurx(x,y), blurx(x,y-3), and so on. This reduces peak memory use and working set size.  4.4 Flattening \nNext, the compiler .attens multi-dimensional loads, stores, and allo\u00adcations into their single-dimensional \nequivalent. This happens in the conventional way: a stride and a minimum offset are computed for each \ndimension, and the buffer index corresponding to a multidimen\u00adsional site is the dot product of the site \ncoordinates and the strides, minus the minimum. (Cf. Fig. 5, right.) By convention, we always set the \nstride of the innermost dimension to 1, to ensure we can perform dense vector loads and stores in that \ndimension. For images, this lays them out in memory in scanline order. While our model of scheduling \nallows extreme .exibility in the order of execution, we do not support more unusual layouts memory, such \nas tiled or sparse storage. (We have found that modern caching memory hierarchies largely obviate the \nneed for tiled storage layouts, in practice.) 4.5 Vectorization and Unrolling After .attening, vectorization \nand unrolling passes replace loops of constant size scheduled as vectorized or unrolled with transformed \nversions of their loop bodies. Unrolling replaces a loop of size n with n sequential statements performing \neach loop iteration in turn. That is, it completely unrolls the loop. Unrolling by lesser amounts is \nexpressed by .rst splitting a dimension into two, and then unrolling the inner dimension. Vectorization \ncompletely replaces a loop of size n with a single statement. For example, in Fig. 5 (lower right), the \nvector loop over blurx.xi is replaced by a single 4-wide vector expression. Any occurrences of the loop \nindex (blurx.xi ) are replaced with a special value ramp(n) representing the vector [0 1...n - 1]. A \ntype coercion pass is then run over this to promote any scalars combined with this special value to n-wide \nbroadcasts of the scalar expression. All of our IR nodes are meaningful for vector types: loads become \ngathers, stores become scatters, arithmetic becomes vector arithmetic, ternary expressions become vector \nselects, and so on. Later, during code generation, loads and stores of a linear expression of k*ramp(n)+o \nwill become dense vector loads and stores if the coef.cient k = 1, or strided loads and stores with stride \nk otherwise. In contrast to many languages, Halide has no divergent control .ow, so this transformation \nis always well-de.ned and straight-forward to apply. In our representation, we never split a vector into \na bundle of scalars. It is always a single expression containing ramps and broadcast nodes. We have found \nthat this yields extremely ef.cient code without any sort of generalized loop auto-vectorization.  4.6 \nBack-end Code Generation Finally, we perform low-level optimizations and emit machine code for the resulting \npipeline. Our primary backends use LLVM for low\u00adlevel code generation. We .rst run a standard constant-folding \nand dead-code elimination pass on our IR, which also performs symbolic simpli.cation of common patterns \nproduced by bounds inference. At this point, the representation is ready to be lowered to LLVM IR. There \nis mostly a one-to-one mapping between our representation and LLVM s, but two speci.c patterns warrant \nmention. First, parallel for loops are lowered to LLVM code that .rst builds a closure containing state \nreferred to in the body of a for loop. The loop body is lowered to a separate function that accepts the \nclosure as an argument and performs one iteration of the loop. We .nally generate code that enqueues \nthe iterations of the loop onto a task queue, which a thread pool consumes at runtime. Second, many vector \npatterns are dif.cult to express or generate poor code if passed directly to LLVM. We use peephole optimization \nto reroute these to architecture-speci.c intrinsics. For example, we perform our own analysis pass to \ndetermine alignment of vector loads and stores, and we catch common patterns such as interleaving stores, \nstrided loads, vector averages, clamped arithmetic, .xed\u00adpoint arithmetic, widening or narrowing arithmetic, \netc. By mapping speci.c expression IR patterns to speci.c SIMD opcodes on each architecture, we provide \na means for the programmer to make use of all relevant SIMD operations on ARM (using NEON) and x86 (using \nSSE and AVX). GPU Code Generation The data parallel grids de.ning a Halide pipeline are a natural .t \nfor GPU programming models. Our com\u00adpiler uses the same scheduling primitives, along with a few simple \nconventions, to model GPU execution choices. GPU kernel launches are modeled as dimensions (loops) scheduled \nto be parallel and annotated with the GPU block and thread dimensions to which they correspond. The limitations \nof GPU execution place a few constraints on how these dimensions can be scheduled. In particular, a sequence \nof block and thread loops must be contiguous, with no other intervening loops between the block and thread \nlevels, since a kernel launch corresponds to a single multidimensional, tiled, parallel loop nest. Sets \nof kernel loops may not be nested within each other on current GPUs which do not directly implement nested \ndata parallelism. Additionally, the extent of the thread loops must .t within the corresponding limits \nof the target device. Other than that, all the standard looping constructs may still be scheduled outside \nor within the block and grid dimensions. This corresponds to loops which internally launch GPU kernels, \nand loops within each thread of a GPU kernel, respectively. Given a schedule annotated with GPU block \nand thread dimen\u00adsions, our compiler proceeds exactly as before, synthesizing a single set of loop nests \nfor the entire pipeline. No stage before the backend is aware of GPU execution; block and thread dimensions \nare treated like any other loops. The GPU backend extends the x86 backend, including its full feature \nset. Outside the loops over block and thread dimensions, the compiler generates the same optimized SSE \ncode as it would in the pure CPU target. At the start of each GPU block loop nest, we carve off the sub-nest \nmuch like a parallel for loop in the CPU backend, only it is spawned on the GPU. We .rst build a clo\u00adsure \nover all state which .ows into the GPU loops. We then generate a GPU kernel from the body of those loops. \nAnd .nally, we generate the host API calls to launch that kernel at the corresponding point in the host \ncode, passing the closure as an argument. We also generate dynamic code before and after launches to \ntrack which buffers need to be copied to or from the device. Every allocated buffer which is used on \nthe GPU has a corresponding device memory allocation, and their contents are lazily copied only when \nneeded. The end result is not individual GPU kernels, but large graphs of hybrid CPU/GPU execution, \ndescribed by the same scheduling model which drives the CPU backends. A small change in the schedule \ncan transform a graph of dozens of GPU kernels and vectorized CPU loop nests, tied together by complex \nmemory management and synchronization, into an entirely different graph of kernels and loops which produce \nthe same result, expressively modeling an enormous space of possible fusion and other choices in mapping \na given pipeline to a heterogeneous machine. 5. Autotuning Pipeline Schedules We apply stochastic search \nto automatically .nd good schedules for Halide pipelines. The automatic optimizer takes a .xed algorithm \nand attempts to optimize the running time by searching for the most ef.cient schedule. The schedule search \nspace is enormous far too large to search exhaustively. For example in the local Laplacian .lters pipeline, \nwe estimate a lower bound of 10720 schedules. This is derived by labeling functions with three tilings \nper function and all possible store and compute granularities. The actual dimensionality of the space \nis likely much higher. The optimal schedule dependends on machine architecture, image dimensions, and \ncode generation in complex ways, and exhibits global dependencies between choices due to loop fusion \nand caching behavior. In this section, we describe our autotuner. Because the search problem has many \nlocal minima, we use a genetic algorithm to seek out a plausible approximate solution, inspired by the \nsearch procedure in PetaBricks [2]. We .rst describe the schedule search space. We show how domain-speci.c \nknowledge can be used to select reasonable starting points. Then we explain the general operations of \nthe genetic algorithm. Finally, we show how further knowledge can be incorporated as search priors for \nmore effective mutation rules. Schedule Search Space Our full model of scheduling is per call but to \nsimplify autotuning we schedule each function identically across all call sites. The domain transformations \ninclude splitting and reordering dimensions, and marking them parallel, vectorized, or unrolled, or mapping \na pair of dimensions to a GPU grid launch. Variable and block size arguments are randomized and chosen \nfrom small powers of two. Because schedules have complex global dependencies, not all schedules are valid: \nfor example, a schedule could be computed or stored inside a dimension that does not exist in the caller \ns loop order. Genetic operations such as mutate and crossover may invalidate correct parent schedules. \nIn general therefore we reject any partially completed schedules that are invalid, and continue sampling \nuntil we obtain valid schedules. We also verify the program output against a correct reference schedule, \nover several input images. This is just a sanity check: all valid schedules should generate correct code. \nFinally to prevent explosion of generated code due to complex pipelines, we limit the number of domain \nscheduling operations for each function. Search Starting Point One valid starting schedule is to label \nall functions as computed and stored breadth .rst (at the outermost, root granularity). The tuner converges \nfrom this starting point, albeit slowly. We can often do better by seeding the initial population with \nreasonable schedules. For each function we .nd its rectangular footprint relative to the caller (via \nbounds inference) and inline functions with footprint one. Remaining functions are stochastically scheduled \nas either (1) fully parallelized and tiled or (2) simply parallelized over y. We de.ne fully parallelized \nand tiled as tiled over x and y, vectorized within the tile s inner x coordinate, and parallelized over \nthe y outer tile dimension. These choices are selected by a weighted coin that has .xed weight from zero \nto one depending on the individual. This allows us to often discover good starting points for functions \nthat vectorize well, or fall back to naive parallelism when that is not the case. The dimensions x and \ny are chosen from adjacent dimensions at random, except when there are optional bounds annotations provided \nby the Halide programmer (such as the number of color channels): dimensions with small bound are not \ntiled. Genetic Algorithm Search We use a .xed population size (128 individuals per generation, for all \nexamples in this paper) and construct each new generation with population frequencies of elitism, crossover, \nmutated individuals, and random individuals. Elitism copies the top individuals from the previous generation. \nCrossover selects parents by tournament selection, followed by two-point crossover, with crossover points \nselected at random between functions. Random individuals are generated either by the reasonable schedules \ndescribed previously, or with equal probability, by scheduling each function independently with random \nschedule choices. This is directly derived from the PetaBricks autotuner [2]. Schedule Mutation Rules \nWe incorporate further prior knowledge about schedules into our mutation rules. Mutation selects a function \nat random and modi.es its schedule with one of eight operations selected at random. Six are fairly generic: \nrandomize constants, replace a function s schedule with one randomly generated, copy from a randomly \nselected function s schedule, and for the function s list of domain transformations, either add, remove, \nor replace with a randomly selected transformation. Our remaining two mutations incorporate speci.c knowledge \nabout imaging. These are chosen with higher probability. First, a pointwise mutation of compute or storage \ngranularity is generally ineffective at fusing loops. Thus we have a loop fusion rule which schedules \nthe chosen function as fully parallelized and tiled, fol\u00adlowed by scheduling callees as vectorized by \nx, and computed and stored under the tile s inner x dimension. The callee scheduling is repeated recursively \nuntil a coin .ip fails. Finally, we incorporate prior knowledge by applying a template: we replace a \nfunction s schedule with one of three common schedule patterns sampled from a text .le. These are: (1) \ncompute and store under x, and vectorize by x, (2) fully parallelized and tiled, and (3) parallelized \nover y and vectorized over x. If generating a CUDA schedule we inject a fourth pattern which simply tiles \non the GPU. The x and y dimensions are determined as in the starting point. 6. Results To evaluate our \nrepresentation and compiler, we applied them to a range of image processing applications, and compared \nthe best autotuned result found by our compiler to the best previously published expert implementation \nwe could .nd. We selected this set of examples to cover a diversity of algorithms and communication patterns. \nIt includes pipelines ranging from two to 99 stages, and including many different stencils, data-dependent \naccess patterns, and histogram reductions. We describe each application below. Fig. 6 summarizes their \nproperties, and Fig. 7 summarizes their performance. All evaluation was performed on a quad core Xeon \nW3520 x86 CPU, an NVIDIA Tesla C2070 GPU. Blur is the simple example used in Sec. 3.1, which convolves \nthe image with two 3 \u00d7 1 box kernels in two steps, a horizontal 3 \u00d7 1 kernel then a vertical 1 \u00d7 3 kernel. \nThis is a simple example of two consecutive stencils. Our reference comparison is a hand-optimized, manually \nfused and multithreaded loop nest de.ned entirely in SSE intrinsics [26]. This version is 12\u00d7 faster \nthan a simple pair of loops in C compiled by GCC 4.7. The version found by our autotuner is 10% faster, \nstill, while being generated from a two line Halide algorithm rather than 35 lines of intrinsics. # \nfunctions # stencils graph structure Blur 2 2 simple Bilateral grid 7 3 moderate Camera pipeline 32 22 \ncomplex Local Laplacian .lters 99 85 very complex Multi-scale interpolation 49 47 complex Figure 6. \nProperties of the example applications. In some cases, the number of functions exceeds the number of \nprogram lines in Fig. 7, because Halide functions are meta-programmed using higher-order functions. Camera \npipeline transforms the raw data recorded by the camera sensor into a usable image. Its demosaicking, \nalone, is a complex combination of 21 interleaved and inter-dependent stencils. The reference comparison \nis a single carefully tiled and fused loop nest from the Frankencamera, expressed in 306 lines of C++ \n[1]. All producer-consumer communication is staged through scratch buffers, tiles are distributed over \nparallel threads using OpenMP, and the tight loops are autovectorized by GCC. The Halide algorithm is \n145 lines describing 32 functions and 22 different stencils, literally translated from the pseudocode \nin the comments explaining the original source. It compiles to an implementation 3.4\u00d7 faster than the \nhand-tuned original. The autotuned schedule fuses long chains of stages through complex, interleaved \nstencils on overlapping tiles, fully fuses other stages, vectorizes every stage, and distributes blocks \nof scanlines across threads. Multi-scale interpolation uses an image pyramid to interpolate pixel data \nfor seamless compositing. This requires dealing with data at many different resolutions. The resulting \npyramids are chains of stages which locally resample over small stencils, but through which dependence \npropagates globally across the entire image. The reference implementation is a carefully-structured set \nof loop nests which were hand-tuned by an Adobe engineer to generate a fully vectorized implementation \nin GCC. The Halide algorithm is substantially simpler, but compiles to an implementation 1.7\u00d7 faster \nthan the original parallel vector code. The same Halide algorithm also automatically compiles to a graph \nof CUDA kernels and x86 code which is 5.9\u00d7 faster. Bilateral grid is an ef.cient implementation of the \nbilateral .lter, which smoothes an image while preserving its main edges [5, 21]. It .rst scatters the \nimage data into a 3D grid, effectively building a windowed histogram in each column of the grid, then \nblurs the grid along each of is axes with three 5-point stencils. Finally, the output image is constructed \nby trilinear interpolation within the grid at locations determined by the input image. The CPU reference \ncode is a tuned but clean implementation from the original authors in 122 lines of C++. It is partially \nautovec\u00adtorized by GCC, but is nontrivial to multithread (a naive OpenMP parallelization of major stages \nresults in a slowdown on our bench\u00admark CPU), so the reference is single-threaded. The Halide algo\u00adrithm \nis 34 lines, and compiles to an implementation 4.2\u00d7 faster than the original. The speedup comes from \na combination of par\u00adallelism, tile-level fusion of some stages, and careful reordering of dimensions \nto control parallel grain size in the grid. We also compared our implementation to a hand-tuned GPU implementation \nfrom the original authors, written in 370 lines of CUDA code. The same Halide algorithm less than 1/10th \nthe code found a different schedule which was 2.3\u00d7 faster than the hand-written CUDA. The Halide compiler \ngenerates similar CUDA code to the reference, but the autotuner found an unintuitive point in the schedule \nspace which sacri.ced some parallelism in the grid construction step to reduce synchronization overhead \nin the scattering reduction. It also uses a tiled fusion strategy which x86 Halide Expert Speedup Lines \nLines Factor tuned tuned Halide expert shorter (ms) (ms) Blur 11 13 1.2\u00d7 2 35 18\u00d7 Bilateral grid 36 158 \n4.4\u00d7 34 122 4\u00d7 Camera pipe 14 49 3.4\u00d7 123 306 2\u00d7 Interpolate 32 54 1.7\u00d7 21 152 7\u00d7 Local Laplacian 113 \n189 1.7\u00d7 52 262 5\u00d7 CUDA Halide Expert Speedup Lines Lines Factor tuned tuned Halide expert shorter \n(ms) (ms) Bilateral grid 8.1 18 2.3\u00d7 34 370 11\u00d7 Interpolate 9.1 54* 5.9\u00d7 21 152* 7\u00d7 Local Laplacian 21 \n189* 9\u00d7 52 262* 5\u00d7 Figure 7. Comparison of autotuned Halide program running times to hand-optimized \nprograms created by domain experts in C, in\u00adtrinsics, and CUDA. Halide programs are both faster and require \nfewer code lines. (*No GPU reference available, compared to CPU reference.) passes intermediate results \nthrough GPU scratchpad memory to improve locality through the blur steps at the expense of redundant \ncomputation. These tradeoffs were counter-intuitive to the original author, and also much harder to express \nin CUDA, but are easily described by our schedule representation and found by our autotuner. Local Laplacian \n.lters uses a multi-scale approach to tone map images and enhance local contrast in an edge-respecting \nfashion [3, 22]. It is used in the clarity, tone mapping, and other .lters in Adobe Photoshop and Lightroom. \nThe algorithm builds and manipulates several image pyramids with complex dependencies between them. The \n.lter output is produced by a data-dependent resampling from several pyramids. With the parameters we \nused, the pipeline contains 99 different stages, operating at many scales, and with different computational \npatterns. The reference implementation is 262 lines of C++, developed at Adobe, and carefully parallelized \nwith OpenMP, and of.oading most intensive kernels to tuned assembly routines from Intel Per\u00adformance \nPrimitives [14, 20]. It has very similar performance to a version deployed in their products, which took \nseveral months to develop, including 2-3 weeks dedicated to optimization. It is 10\u00d7 faster than an algorithmically \nidentical reference version written by the authors in pure C++, without IPP or OpenMP. The Halide version \nwas written in one day, in 52 lines of code. It compiles to an implementation which is 1.7\u00d7 faster than \nthe highly optimized ex\u00adpert implementation (roughly 20\u00d7 faster than the clean C++ without IPP and OpenMP). \nThe resulting schedule is enormously complex, mixing different fusion, tiling, vectorization, and multithreading \nstrategies throughout the expansive 99 stage graph. In C, it would correspond to hundreds of loops over \nmore than 10,000 lines of code. The same program compiles with a different automatically generated schedule \nto a hybrid CPU/GPU program with 58 unique GPU kernels, each representing a differently tiled and partially \nfused subset of the overall graph, and with the lookup table and several smaller levels of the pyramids \nscheduled as vector code on the CPU. The generated program has more distinct CUDA kernels than the Halide \nalgorithm describing it has lines of code. It runs 7.5\u00d7 faster than the hand-tuned Adobe implementation, \nand is more than 4\u00d7 faster than the best parallel vector implementation on the CPU. This Figure 8. Cross-testing \nof autotuned schedules across resolutions. Each program is autotuned on a source image size. The resulting \nschedule is tested on a target image size giving a cross-tested time.\" This is compared to the result \nof running the autotuner directly on the target resolution. We report the ratio of the cross-tested time \nto the autotuned-on-target time as the slowdown.\" Note that schedules generalize better from low resolutions \nto high resolutions. In theory the slow-down should always be at least one, but due to the stochastic \nnature of the search some schedules were slower when autotuned on the target. Source size (MP) Target \nsize (MP) Cross\u00adtested time (ms) Autotuned on target (ms) Slowdown (vs target autotuned) Blur Bilateral \ngrid Interpolate 0.3 0.3 0.3 30 2 2 13 35 31 11 36 32 1.2\u00d7 0.97\u00d7 0.97\u00d7 Blur Bilateral grid Interpolate \n30 2 2 0.3 0.3 0.3 1.1 9.6 9.7 0.07 6.7 5.2 16\u00d7 1.4\u00d7 1.9\u00d7 is by far the fastest implementation of local \nLaplacian .lters we know of. 6.1 Autotuning Performance These examples took between 2 hours and 2 days \nto tune (from 10s to 100s of generations). In all cases, the tuner converged to within 15% of the .nal \nperformance after less than one day tuning on a single machine. Improvements to the compiling and tuning \ninfrastructure (for example, distributing tests across a cluster) could reduce these times signi.cantly. \nWe generally found the tuned schedules to be insensitive to moderate changes in resolution or architecture, \nbut extreme changes can cause the best schedule to change dramatically. Table 8 shows experiments in \ncross-testing schedules tuned at different resolutions. We observe that schedules generalize better from \nlow resolutions to high resolutions. We also mapped the best GPU schedule for local Laplacian .lter to \nthe CPU, and found that this is 7\u00d7 slower than the best CPU schedule.  6.2 Discussion Across a range \nof image processing applications and target architec\u00adtures, our scheduling representation is able to \nmodel, our compiler is able to generate, and our autotuner is able to discover implemen\u00adtation strategies \nwhich deliver state-of-the-art performance. This performance comes from careful navigation of the extremely \nhigh dimensional space of tradeoffs between locality, parallelism, and re\u00addundant recomputation in image \nprocessing pipelines. Making these tradeoffs by hand is challenging enough, as shown by the much greater \ncomplexity of hand-written implementations, but .nding the ideal points is daunting when each change \na programmer might want to test can require completely rewriting a complex loop nest hundreds of lines \nlong. The performance advantage of the Halide implementations is a direct result of simply testing many \nmore points in the space than a human programmer ever could manually describe at the level of explicit \nloops. Nonetheless, based on our experience, brute force autotuning still poses signi.cant challenges \nof robustness and usability in a real system. The tuning process is an entire extra stage added to a \nprogrammer s normal development process. Straightforward implementations are sensitive to noise in testing \nenvironment and many other factors. In Halide, while simple pipelines (like blur) can tune effectively \nwith only trivial mutation rules, we found heuristic mutation rules are essential to converge in a reasonable \namount of time when tuning more complex imaging pipelines. However these rules may be speci.c to the \nalgorithm structure. For example, different template mutation rules may be necessary for 3D voxel data \nor unconventional color layouts. We also found that it is necessary to maintain diversity to avoid becoming \ntrapped in local minima, which we did by using a large population (128 individuals per generation). Even \nso, the tuner can occasionally become trapped, requiring restarting with a new random initialization, \nand taking the best of several runs. In general, we feel that high dimensional stochastic autotuning \nhas not yet developed the robust methodology or infrastructure for real-world use found elsewhere in \nthe compiler community. 7. Prior Work Image processing pipelines include similar structure to several \nproblems well studied in compilers. Split Compilers Sequoia s mappings and SPIRAL s loop syn\u00adthesis algebra \necho our separation of the model of scheduling from the description of the algorithm, and its lifting \noutside our com\u00adpiler [9, 25]. Stream Programs Compiler optimization of stream programs was studied extensively \nin the StreamIt and Brook projects [4, 11, 29]. In this framework, sliding window communication implements \nstencils on 1D streams [12, 24, 30]. Stream compilers generally did not consider the introduction of \nredundant work as a major optimization choice, and nearly all work in stream compilation has focussed \non these 1D streams. Image processing pipelines, in contrast, are effectively stream programs over 2D-4D \nstreams. Stencil Optimization Iterated stencil computations are important to many scienti.c applications, \nand have been studied for decades. Frigo and Strumpen proposed a cache oblivious traversal for ef.\u00adcient \nstencil computation [10]. This view of locality optimization by interleaving stencil applications in \nspace and time inspired our model of scheduling. The Pochoir compiler automatically trans\u00adforms stencil \ncodes from serial form into a parallel cache oblivious form using similar algorithms [28]. Overlapping \ntiling is a strategy which divides a stencil compu\u00adtation into tiles, and trades off redundant computation \nalong tile boundaries to improve locality and parallelism [16], modeled in our schedule representation \nas interleaving both storage and computa\u00adtion inside the tile loops. Other tiling strategies represent \ndifferent points in the tradeoff space modeled by our representation [19]. Past compilers have automatically \nsynthesized parallel loop nests with overlapped tiling on CPUs and GPUs using the polyhedral model [13, \n16]. These compilers focussed on synthesizing high quality code given a single, user-de.ned set of overlapped \ntiling parameters. Autotuning has also been applied to iterated stencil com\u00adputations, but past tuning \nwork has focussed on exhaustive search of small parameter spaces for one or a few strategies [15]. Image \nProcessing Languages Critically, many optimizations for iterated stencil computations are based on the \nassumption that the time dimension of iteration is large relative to the spatial dimension of the grid. \nIn image processing pipelines, most individual stencils are applied only once, while images are millions \nof pixels in size. Image processing pipelines also include more types of computa\u00adtion than stencils alone, \nand scheduling them requires choices not only of different parameters, but of entirely different strategies, \nfor each of many heterogeneous stages, which is infeasible with either exhaustive search or polyhedral \noptimization. Most prior image pro\u00adcessing languages and systems have focused on ef.cient expression \nof individual kernels, as well as simple fusion in the absence of stencils [6, 8, 23, 27]. Recently, \nCornwall et al. demonstrated fast GPU code generation for image processing code using polyhedral optimization \n[7]. Earlier work on the Halide language included a weaker model of schedules, and required programmers \nto explicitly specify schedules by hand [26]. This is the .rst automatic optimizer, and therefore the \n.rst fully automatic compiler, for Halide programs. We show how they can be automatically inferred, starting \nfrom just the algorithm de.ning the pipeline stages, and using relatively little domain-speci.c knowledge \nbeyond the ability to enumerate points in the space of schedules. Our state-of-the-art performance shows \nthe effectiveness of our scheduling model for representing the underlying choice space. The scheduling \nmodel presented here is also richer than in past work. In particular, it separates computation frequency \nfrom storage frequency in the call schedule, enabling sliding window schedules and similar strategies \nwhich trade off parallelism for redundant work while maintaining locality. In all, this gives a dramatic \nnew result: automatic optimization of stencil computations, including full consideration of the parallelism, \nlocality, and redundancy tradeoffs. Past automatic stencil optimiza\u00adtions have targeted individual points \nin the space, but have not auto\u00admatically chosen among different strategies spanning these multi\u00addimensional \ntradeoffs, and none have automatically optimized large heterogeneous pipelines, only individual or small \nmulti-stencils. Acknowledgments Eric Chan provided feedback and inspiration throughout the design of \nHalide, and helped compare our local Laplacian .lters imple\u00admentation to his in Camera Raw. Jason Ansel \nhelped inform the design of our autotuner. This work was supported by DOE Award DE-SC0005288, NSF grant \n0964004, grants from Intel and Quanta, and gifts from Cognex and Adobe. References [1] A. Adams, E. Talvala, \nS. H. Park, D. E. Jacobs, B. Ajdin, N. Gelfand, J. Dolson, D. Vaquero, J. Baek, M. Tico, H. P. A. Lensch, \nW. Matusik, K. Pulli, M. Horowitz, and M. Levoy. The Frankencamera: An experimental platform for computational \nphotography. ACM Trans. Graph., 29(4), 2010. [2] J. Ansel, C. Chan, Y. L. Wong, M. Olszewski, Q. Zhao, \nA. Edelman, and S. Amarasinghe. PetaBricks: A language and compiler for algorithmic choice. In ACM Programming \nLanguage Design and Implementation, 2009. [3] M. Aubry, S. Paris, S. W. Hasinoff, J. Kautz, and F. Durand. \nFast and robust pyramid-based image processing. Technical Report MIT-CSAIL\u00adTR-2011-049, Massachusetts \nInstitute of Technology, 2011. [4] I. Buck, T. Foley, D. Horn, J. Sugerman, K. Fatahalian, M. Houston, \nand P. Hanrahan. Brook for GPUs: Stream computing on graphics hardware. In SIGGRAPH, 2004. [5] J. Chen, \nS. Paris, and F. Durand. Real-time edge-aware image processing with the bilateral grid. ACM Trans. Graph., \n26(3), 2007. [6] CoreImage. Apple CoreImage programming guide, 2006. [7] J. L. T. Cornwall, L. Howes, \nP. H. J. Kelly, P. Parsonage, and B. Nico\u00adletti. High-performance SIMT code generation in an active visual \neffects library. In Conf. on Computing Frontiers, 2009. [8] C. Elliott. Functional image synthesis. In \nProceedings of Bridges, 2001. [9] K. Fatahalian, D. R. Horn, T. J. Knight, L. Leem, M. Houston, J. Y. \nPark, M. Erez, M. Ren, A. Aiken, W. J. Dally, and P. Hanrahan. Sequoia: programming the memory hierarchy. \nIn ACM/IEEE conference on Supercomputing, 2006. [10] M. Frigo and V. Strumpen. Cache oblivious stencil \ncomputations. In ICS, 2005. [11] M. I. Gordon, W. Thies, M. Karczmarek, J. Lin, A. S. Meli, C. Leger, \nA. A. Lamb, J. Wong, H. Hoffman, D. Z. Maze, and S. Amarasinghe. A stream compiler for communication-exposed \narchitectures. In Inter\u00adnational Conf. on Architectural Support for Programming Languages and Operating \nSystems, 2002. [12] P. L. Guernic, A. Benveniste, P. Bournai, and T. Gautier. Signal A data .ow-oriented \nlanguage for signal processing. IEEE Transactions on Acoustics, Speech and Signal Processing, 34(2):362 \n374, 1986. [13] J. Holewinski, L. Pouchet, and P. Sadayappan. High-performance code generation for stencil \ncomputations on gpu architectures. In ICS, 2012. [14] IPP. Intel Integrated Performance Primitives. http://software.intel.com/en-us/articles/intel-ipp/. \n[15] S. Kamil, C. Chan, L. Oliker, J. Shalf, , and S. Williams. An auto-tuning framework for parallel \nmulticore stencil computations. In IPDPS, 2010. [16] S. Krishnamoorthy, M. Baskaran, U. Bondhugula, J. \nRamanujam, A. Rountev, and P. Sadayappan. Effective automatic parallelization of stencil computations. \nIn PLDI, 2007. [17] J. Meng and K. Skadron. A performance study for iterative stencil loops on gpus \nwith ghost zone optimizations. In IJPP, 2011. [18] R. Moore. Interval Analysis. 1966. [19] A. Nguyen, \nN. Satish, J. Chhugani, C. Kim, and P. Dubey. 3.5-d blocking optimization for stencil computations on \nmodern cpus and gpus. In Supercomputing, 2010. [20] OpenMP. OpenMP. http://openmp.org/. [21] S. Paris, \nP. Kornprobst, J. Tumblin, and F. Durand. Bilateral .lter\u00ading: Theory and applications. Foundations and \nTrends in Computer Graphics and Vision, 2009. [22] S. Paris, S. W. Hasinoff, and J. Kautz. Local Laplacian \n.lters: Edge\u00adaware image processing with a Laplacian pyramid. ACM Trans. Graph., 30(4), 2011. [23] PixelBender. \nAdobe PixelBender reference, 2010. [24] H. Printz. Automatic Mapping of Large Signal Processing Systems \nto a Parallel Machine. Ph.D. Thesis, Carnegie Mellon University, 1991. [25] M. Puschel, J. M. F. Moura, \nJ. R. Johnson, D. Padua, M. M. Veloso, B. W. Singer, J. Xiong, F. Franchetti, A. Gacic, Y. Voronenko, \nK. Chen, R. W. Johnson, and N. Rizzolo. SPIRAL: Code generation for DSP transforms. In Proceedings of \nthe IEEE, volume 93, 2005. [26] J. Ragan-Kelley, A. Adams, S. Paris, M. Levoy, S. Amarasinghe, and F. \nDurand. Decoupling algorithms from schedules for easy optimization of image processing pipelines. ACM \nTrans. Graph., 31(4), 2012. [27] M. A. Shantzis. A model for ef.cient and .exible image computing. In \nACM SIGGRAPH, 1994. [28] Y. Tang, R. Chowdhury, B. Kuszmaul, C.-K. Luk, and C. Leiserson. The Pochoir \nstencil compiler. In SPAA, 2011. [29] W. Thies, M. Karczmarek, and S. Amarasinghe. StreamIt: A language \nfor streaming applications. In International Conference on Compiler Construction, 2002. [30] P.-S. Tseng. \nA Parallelizing Compiler for Disributed Memory Parallel Computers. PhD thesis, Carnegie Mellon University, \n1989. [31] X. Zhou, J.-P. Giacalone, M. J. Garzar\u00e1n, R. H. Kuhn, Y. Ni, and D. Padua. Hierarchical overlapped \ntiling.    \n\t\t\t", "proc_id": "2491956", "abstract": "<p>Image processing pipelines combine the challenges of stencil computations and stream programs. They are composed of large graphs of different stencil stages, as well as complex reductions, and stages with global or data-dependent access patterns. Because of their complex structure, the performance difference between a naive implementation of a pipeline and an optimized one is often an order of magnitude. Efficient implementations require optimization of both parallelism and locality, but due to the nature of stencils, there is a fundamental tension between parallelism, locality, and introducing redundant recomputation of shared values.</p> <p>We present a systematic model of the tradeoff space fundamental to stencil pipelines, a schedule representation which describes concrete points in this space for each stage in an image processing pipeline, and an optimizing compiler for the Halide image processing language that synthesizes high performance implementations from a Halide algorithm and a schedule. Combining this compiler with stochastic search over the space of schedules enables terse, composable programs to achieve state-of-the-art performance on a wide range of real image processing pipelines, and across different hardware architectures, including multicores with SIMD, and heterogeneous CPU+GPU execution. From simple Halide programs written in a few hours, we demonstrate performance up to 5x faster than hand-tuned C, intrinsics, and CUDA implementations optimized by experts over weeks or months, for image processing applications beyond the reach of past automatic compilers.</p>", "authors": [{"name": "Jonathan Ragan-Kelley", "author_profile_id": "81335496364", "affiliation": "Massachusetts Institute of Technology, Cambridge, MA, USA", "person_id": "P4149112", "email_address": "jrk@csail.mit.edu", "orcid_id": ""}, {"name": "Connelly Barnes", "author_profile_id": "81385595142", "affiliation": "Adobe, Cambridge, MA, USA", "person_id": "P4149113", "email_address": "cbarnes@adobe.com", "orcid_id": ""}, {"name": "Andrew Adams", "author_profile_id": "81406596762", "affiliation": "Massachusetts Institute of Technology, Cambridge, MA, USA", "person_id": "P4149114", "email_address": "abadams@csail.mit.edu", "orcid_id": ""}, {"name": "Sylvain Paris", "author_profile_id": "81100495713", "affiliation": "Adobe, Cambridge, MA, USA", "person_id": "P4149115", "email_address": "sparis@adobe.com", "orcid_id": ""}, {"name": "Fr&#233;do Durand", "author_profile_id": "81100055904", "affiliation": "Massachusetts Institute of Technology, Cambridge, MA, USA", "person_id": "P4149116", "email_address": "fredo@csail.mit.edu", "orcid_id": ""}, {"name": "Saman Amarasinghe", "author_profile_id": "81100533031", "affiliation": "Massachusetts Institute of Technology, Cambridge, MA, USA", "person_id": "P4149117", "email_address": "saman@csail.mit.edu", "orcid_id": ""}], "doi_number": "10.1145/2491956.2462176", "year": "2013", "article_id": "2462176", "conference": "PLDI", "title": "Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines", "url": "http://dl.acm.org/citation.cfm?id=2462176"}