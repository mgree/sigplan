{"article_publication_date": "06-16-2013", "fulltext": "\n General Data Structure Expansion for Multi-threading Hongtao Yu Hou-Jen Ko Zhiyuan Li Department of \nComputer Science Purdue University West Lafayette, IN 47907 {htyu, ko16, zhiyuanli}@purdue.edu Abstract \nAmong techniques for parallelizing sequential codes, privatization is a common and signi.cant transformation \nperformed by both compilers and runtime parallelizing systems. Without privatization, repetitive updates \nto the same data structures often introduce spuri\u00adous data dependencies that hide the inherent parallelism. \nUnfortu\u00adnately, it remains a signi.cant challenge to compilers to automati\u00adcally privatize dynamic and \nrecursive data structures which appear frequently in real applications written in languages such as C/C++. \nThis is because such languages lack a naming mechanism to de.ne the address range of a pointer-based \ndata structure, in contrast to arrays with explicitly declared bounds. In this paper we present a novel \nsolution to this dif.cult problem by expanding general data structures such that memory accesses issued \nfrom different threads to contentious data structures are directed to different data .eld\u00ad s. Based on \ncompile-time type checking and a data dependence graph, this aggressive extension to the traditional \nscalar and ar\u00adray expansion isolates the address ranges among different threads, without struggling with \nprivatization based on thread-private stack\u00ads, such that the targeted loop can be effectively parallelized. \nWith this method fully implemented in GCC, experiments are conduct\u00aded on a set of programs from well-known \nbenchmark suites such as Mibench, MediaBench II and SPECint. Results show that the new approach can lead \nto a high speedup when executing the trans\u00adformed code on multiple cores. Categories and Subject Descriptors \nD.3.4 [Programming Lan\u00adguages]: Processors Compilers, run-time environments; F.3.2 [Logics And Meanings \nOf Programs]: Semantics of Programming Languages Program Analysis General Terms Algorithms, Languages, \nPerformance Keywords Parallelization, privatization, multithreading 1. Introduction Data privatization \nis an important technique that has been used by both compilers and runtime parallelizing systems to enable \nloop parallelization. This technique duplicates data structures for each thread, in order to remove spurious \ndata dependencies due to the reuse of such data structures for storing intermediate results. Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI 13, June 16 \n19, 2013, Seattle, WA, USA. Copyright c &#38;#169; 2013 ACM 978-1-4503-2014-6/13/06. . . $10.00 Mature \ndata privatization techniques, such as scalar expansion [4, 5] and array privatization [15, 16, 18, 22, \n23, 36], are widely adopted by parallelizing compilers. Unfortunately, existing compile-time privatization \ntechniques primarily target statically allocated variables. They lack the capa\u00adbility of privatizing \ndynamic and recursive data structures which are extensively used in non-numerical programs. This is because \ndynamic data structures are allocated at run time through alloca\u00adtion routines (e.g. malloc, calloc and \nrealloc). A program often makes many of such calls under various program contexts to create a dynamic \ndata structure (such as linked lists, trees and graphs) that are nonconsecutive in the memory. Therefore, \nthe pro\u00adgram does not have a single address range to precisely contain such data structures, which makes \nit dif.cult to apply traditional privatization methods. This dif.culty is certainly compounded by the \nambiguity of pointer-induced aliases. Several proposals have emerged recently to offer language con\u00adstructs \nto aid the privatization of dynamic data structures. For ex\u00adample, in Paralax [37], the allocation site \nof a heap-allocated object expressed with a base pointer variable is annotated as a constructor. If one \nwishes to privatize that object, one can call its constructor. This method, however, is not effective \nfor programs that have com\u00adplicated pointer aliases and scattered allocation sites. Alternatively, Feng \net al. [12] attempted to solve the problem by devising new language support to guide run time copying \nof data structures to thread-local spaces for private use. Conceptually, such a method can be universal \nand it is not restricted by insuf.cient static anal\u00adysis. Their system, SpiceC, monitors every memory \naccess at run time in order to forward them to private copies when necessary. In spite of their efforts \nto make runtime monitoring less costly, the overhead seems still too high in practice, as we shall show \nin sub\u00adsequent sections. In this paper, we propose a novel compiler technique to expand dynamic data \nstructures such that dependences due to contentious data updates from different threads can be removed. \nThis method in essence keeps N copies of the same data structures in a program by expanding the original \ndata types in the thread-shared address space, each copy for one thread. The original copy serves as \nboth the shared copy (for all N threads) and the private copy for the .rst thread. Given a data dependence \ngraph, the method also partitions all memory accesses in a program into two classes, namely, the shared \naccesses and the thread-private accesses. A shared memory access should access the shared copy of the \ndata structure while a private memory access should only access its own thread-private copy. Memory accesses \nare redirected to their designated copies by adjusting the code at compile time. As the result, thread-private \nmemory accesses from different threads have no cross-thread data dependencies and can be performed in \nparallel. This new approach to removing data contention for arbitrary data structures does not need to \nknow the shape of the data structure, where it is allocated or where it is referenced. In this sense, \nit is a universal framework. 3.1 Creating Private Data Copies Our experiments show that the average running \noverhead is below 5% of the original execution time, making the method practical. The rest of this paper \nis organized as follows. We discuss the technical background of this work in Section 2. We then present \nour new technique, i.e., the compile-time general data structure expansion, in Section 3. Experimental \nresults are given in Section 4. We discuss related work in Section 5 and conclude in Section 6. 2. A \nDependence-based Approach A variable is said to be privatizable if it is written before each read in \nevery loop iteration. By allocating private copies for privatizable variables in each thread participating \nin the loop execution, mem\u00adory accesses to these variables in iterations executed by different threads \ndo not interfere with each other, thus exposing parallelis\u00adm in loops that otherwise appear to be sequential. \nFor Fortran-like numerical programs, privatization can often be performed well by parallelizing compilers, \nwhich are sometimes guided by the pri\u00advate clause in OpenMP [6]. At run time, each thread will have a \nprivate copy of each privatized variable stored in the local stack maintained for that thread. Unfortunately, \nthe privatization process becomes more compli\u00adcated for non-numerical programs which manipulate dynamic \ndata structures. Take the benchmark Dijkstra in Mibench suite [17] for example. This program has a loop \nthat .nds the shortest path be\u00adtween a distinct pair of nodes in each iteration. An internal priority \nqueue, which is essentially a linked list, is manipulated such that new nodes are created and old nodes \nremoved from iteration to it\u00aderation, with no guarantee of contiguous allocation in the memory. Furthermore \nthe graph may be annotated during the search. Con\u00adceptually, it is possible to run 10 iterations in parallel \nto .nd 10 shortest paths between 10 pairs of nodes, but the priority queue must .rst be privatized. As \ndiscussed in the introduction, no simple way exists to privatize these internal data structures, unfortunately. \nOur new approach is to perform general data structure expan\u00adsion in the shared address space based on \na loop-level data depen\u00addence graph. Such a graph can be obtained either from the program\u00admer, the compiler, \nor tools that perform data dependence pro.ling [21, 38 40] with programmer veri.cation. It can also be \nobtained at runtime when candidate loops are running speculatively [27, 28]. The rest of the paper assumes \nthe availability of such a loop-level dependence graph at compile time. 3. Data Structure Expansion We \n.rst use a sample program in Figure 1(a) to illustrate the idea of general data structure expansion. \nFor this example we aim to parallelize the while loop, which cannot be handled by traditional array privatization. \nTo create thread private copies, we extend the memory allocation in line 1 by enlarging the allocation \nsize by a factor of N, where N is the number of threads to run the program, which may be determined as \nlate as runtime. After that, we direct memory accesses to appropriate data copies to satisfy all .ow \ndependencies according to the data dependence graph for the loop. Note that in Figure 1(b) the memory \naccesses in shaded lines are directed to thread-private copies. To simplify the illustration, we do not \nshow data expansion for every scalar variable that must be privatized or expanded in this program. In \nthe rest of this section, we will .rst present our method to create the thread-private copies and then \ndiscuss the redirection of memory accesses. After that, we discuss methods to lower the redirection overhead. \n Thread-private data copies can be logically formed by expanding types for each data structure. Table \n1 shows the expanding rules. Column Declaration lists several general cases of C variable in\u00adstantiation, \nincluding a heap allocation. We show the rules for the int primitive type only, but the rules concerning \nother primitive types, such as char, float, double and all pointer types, have obvious counterparts. \nColumn Expansion , shows the correspond\u00ading instantiations after type expansion. The type expansion simply \nenlarges the size of a data structure by a factor equal to the thread count (denoted by N) at run time. \nNote that the expansion for a stack variable can be implemented by using a variable-length array. However, \nstatically expanding global variables of a variable length is impossible because the global data section \nmust have a .xed size in the .nal binary .le. To deal with global variables we .rst substi\u00adtute each \nof them with a dynamically allocated heap object and then convert each access to the global variable \nto referencing the heap object. After that, the converted global variables can be treated as normal heap \nobjects. Table 1: Type Expansion rules. Rule Declaration Expansion Local scalar int a int a[N] Local \nrecord struct S a struct S a[N] Local array int a[n] int a[N][n] Global scalar int a int *pa = malloc(sizeof(int) \n* N) Global record struct S a S *pa = malloc(sizeof(S) * N) Global array int a[n] int *pa=malloc(sizeof(int)*n*N) \nHeap object malloc(size) malloc(size*N)  Note that when expanding an aggregate data structure, such \nas an array variable or a structure variable, we lay out multiple copies of the variable adjacently in \ntheir entireties. We call this layout mode the bonded mode, in which a data structure is replicated entirely \nwith copies for different threads being adjacent in memory, as shown in Figure 2(a). Alternatively, there \nis the interleaved mode, in which each primitive member of a structure is replicated such that its copies \nare allocated consecutively in memory, but different primitive members of the same structure are interleaved \nin memory, as shown in Figure 2(b). We prefer the bonded mode for a number of reasons. For one thing, \nthe interleaved mode fails to work in some cases in which a data structure is recast between different-sized \ntypes. For example, the zptr array in 256.bzip2 of the SPEC2000 benchmark suite is frequently recast \nbetween the types of 2-byte short integer and 4-type integer. Without knowing the exact size of the array \nelements, it is unclear to the compiler how to place the duplicates of each array element in the interleaved \nmode. Another bene.t of the bonded mode is the potentially lower cache misses in coarse-grain parallel \nthreads because data items used by a single thread are placed as near as possible, rendering a better \ndata locality than in the interleaved mode. 3.2 Partitioning Memory Accesses After creating private data \ncopies for threads, we revise memory ac\u00adcesses to make sure that they refer to the correct data. We partition \nmemory accesses in a program into two mutually exclusive classes, namely the shared accesses and the \nprivate accesses. As mentioned previously, shared accesses should be directed to the .rst thread s data \ncopy while private accesses should reference the correspond\u00ading thread-private copies. The partition \nis based on the given loop\u00adlevel data dependence graph as de.ned below. De.nition 1. (Loop-level Data \ndependence graph) A loop-level data dependence graph G of a loop is represented by < V, E > 1 i n t \n* z p t r = m a l l o c ( s i z e o f ( i n t ) * m ) ; 1 i n t * z p t r = m a l l o c ( s i z e o f \n( i n t ) * m * N ) ; 2 3 w h i l e ( 1 ) . . . { 2 3 w h i l e ( 1 ) . . . { 4 5 i n i t i a l i z e \n. . . ( z p t r ) ; 4 5 i n i t i a l i z e . . . ( z p t r ) ; 6 f o r ( k = 0 ; k < m ; k + + ) 6 f \no r ( k = 0 ; k < m ; k + + ) 7 8 . . . b + = z p t r [ k ] ; 7 8 . . . b + = z p t r [ k + t i d * m \n] ; 9 } 9 } (a) A sample program. (b) The sample program after expanding. Figure 1: Data structure expansion. \nThe sample program is extracted from SPEC CPU2000/Bzip2. In every iteration of the while loop, the memory \nchunk that the variable zptr points to is initialized in line 4 and then referenced throughout the loop \nbody. The corresponding code after data structure expansion is shown in (b). (a) Bonded mode (b) Interleaved \nmode  Figure 2: Expansion of struct {int f1, f2;} A, B; under four threads. where the vertex set V \nconsists of all memory accesses potentially executed in the loop and E is a set of directed edges each \nof which denotes a possible data dependence from the source memory access to the sink memory access. \nA data dependence between its source memory access a and its sink memory access b may occur in different \nways: A loop-independent .ow dependence occurs when b reads from an address to which a writes prior \nto b in the same iteration.  A loop-carried .ow dependence occurs when b reads from an address to which \na writes in a previous iteration, and b is not covered by previous writes to the same address in the \nsame loop iteration.  A loop-independent or loop-carried anti-dependence occurs when b writes to an \naddress from which a reads prior to b in the same iteration (loop-independent) or a previous iteration \n(loop-carried).  A loop-independent or loop-carried output dependence occurs when b writes to an address \nto which a also writes prior to b in the same iteration (loop-independent) or in a previous iteration \n(loop-carried).  Note that we have extended the de.nition of loop-carried .ow dependence to account \nfor privatizable variables. In addition to a loop-level data dependence graph we introduce an additional \nproperty for each memory load or store, respectively. De.nition 2. (Upwards-exposed load) A memory load \nis said to be upwards-exposed in a loop if and only if the value which it reads comes from outside the \nloop. De.nition 3. (Downwards-exposed store) A memory store is said to be downwards-exposed in a loop \nif and only if the value which it writes is used after the loop. It is straightforward to de.ne private \nmemory accesses at the instruction instance level. A load instance is said to be thread\u00adprivate if it \nreads a value de.ned by previous instructions of the the same thread. Similarly, a store instance is \nsaid to be thread-private if the value it writes will be used only in the same thread. However, extending \nsuch de.nitions to the program statement level is not as simple. A static write memory access may induce \nmultiple store instances at run time, some of which may be thread-private while others may not. For instance, \nsuppose the following code snippet is embedded in a loop body. Further assume that the write to *p in \nline L3 is thread-private if and only if condition c holds. Even though the load *p in line L4 is always \nthread-private, we cannot decide at compile time the iterations in which the store *p in line L3 must \nwrite to the private copy instead of the shared copy. L1: if (c) p=&#38;a; L2: else p = &#38;b; L 3 : \n* p = 0 ; L 4 : i f ( c ) a [ i ] = * p ;  The main problem encountered here is that privatizing only \none of the two *p will violate the potential loop-independent depen\u00addence between them. To avoid the \nsemantic violation, we regard a loop-independent dependence as an equivalence relationship be\u00adtween its \nsource and sink memory accesses. Noting that an equiv\u00adalence relationship has the transitive property, \nwe place all equiva\u00adlent memory accesses into a single access class. De.nition 4. (Access class) We consider \na loop-independent de\u00adpendence between its source memory access a and its sink memory access b as an \nequivalence relation a ~ b, which is re.exive, sym\u00admetric and transitive. The access class of a memory \naccess a is the equivalence class of a de.ned as the set [a] = {b . MEMOPS |a ~ b} of all memory accesses \nthat are related to a by ~. MEMO-PS represents all the memory accesses in the loop. Based on the de.nition \nof access classes, we de.ne thread\u00adprivate memory accesses. De.nition 5. (Thread-private access class) \nSuppose a loop is to have its iterations executed by multiple threads. An access class is said to be \nprivate to a thread if and only if all the following conditions are satis.ed: 1. No memory access in \nthe class is an upwards-exposed load or a downwards-exposed store; 2. No memory access in the class \nis involved in any loop-carried .ow dependence; and 3. At least one memory access in the class is involved \nin a loop\u00adcarried anti-or output dependence.  A memory access is said to be private if and only if \nits access class is thread-private. Non-private memory accesses are called shared memory accesses. One \ncould relax the conditions in Def\u00adinition 5 to create more privatization opportunities. Note that even \nif a memory access of a class is involved in a loop-carried .ow de\u00adpendence, privatization is still possible \nfor the class as long as the dependence does not occur across threads. We do not explore these relaxations \nin this paper.  3.3 Redirecting Memory Accesses The memory access redirection can be accomplished by \napplying rules listed in Table 2 to each memory access which has been identi.ed as either a shared access \nor a private access. The variable tid, which represents the index of the current thread, is used to index \nthread-private copies. Thread-private copies for a whole data structure are organized consecutively in \nmemory, from the .rst thread to the last thread, as shown in Figure 2(a). The .rst thread copy (when \nthe value of tid is zero) also serves as the shared copy. Note that in some cases we need to know the \noriginal size of the data structure expanded (denoted by the symbol span) in order to index into thread \nprivate copies. This is because, in C/C++, it is dif.cult to infer the size of a data structure based \non the base pointer alone. Table 2: Redirection rules. Rule Original Shared Private Local scalar a a[0] \na[tid] Local .eld a..eld a[0]..eld a[tid]..eld Local array a[i] a[0][i] a[tid][i] Global scalar a pa[0] \npa[tid] Global .eld a..eld pa[0]..eld pa[tid]..eld Global array a[i] pa[i] pa[i+tid*span/sizeof(*pa)] \nPointer deref *p *p *(p+tid*span/sizeof(*p)) We use an example shown in Figure 3(a) to explain the neces\u00adsity \nof the symbol span. The example is extracted from SPEC CPU2006/456.hmmer. In this example, suppose the \nvariable dec\u00adlarations are local to a function and the program segment is em\u00adbedded in a loop (in that \nfunction) targeted for parallelization. The variables m1, m2 and k are of integer types. In line 6, whether \npoint\u00ader mx points to the m1-sized memory chunk allocated in line 3 or the m2-sized memory chunk allocated \nin line 5 can only be deter\u00admined at run time. After the thread copy creation, the intermediate representation \nwill be in the form shown in the right-hand side. Be\u00adcause we use the bonded mode, without knowing at \ncompile time the exact size of the data structure that mx points to, we have no idea where to redirect \nmx[k] in line 6. The symbol span has the meaning of a shadow variable allocated for each pointer, in \norder to record the original size of the data structure pointed to by the pointer. We also insert additional \nstatements in the right place to compute the symbol span. The creation and computation of the symbol \nspan is prior to the data structure expansion. 3.3.1 Creating Span The symbol span can be implemented \nby attaching a .eld to each pointer variable. We promote a pointer variable to a structure variable whose \n.rst .eld is the pointer variable itself and the second .eld is the span, which is an integer. For the \nexample in Figure 3(a), the promoted version is shown in Figure 4(a). The transformed program has the \nsame semantics as the original program. Note that we do not create thread private copies at this stage. \nAfter the promotion, we insert additional statements to compute the symbol span. This will be addressed \nin Section 3.3.2. We proceed by promoting pointer declarations .rst. If the orig\u00adinal type of a variable \ndeclaration is a pointer type, we promote its pointee type .rst and then promote the pointer type to \na structure type. If the original type is an array type, we promote the element 1 i n t * m x ; i n t \n* m x ; 2 i f ( . . . ) i f ( . . . ) 3 m x = m a l l o c ( m 1 ) ; m x = m a l l o c ( m 1 * N ) ; 4 \ne l s e e l s e 5 m x = m a l l o c ( m 2 ) ; m x = m a l l o c ( m 2 * N ) ; 6 m x [ k ] = 0 ; m x [ \nk + t i d * ? ] = 0 ; (a) Original program (b) After expansion Figure 3: Thread space creation struct \n{ struct { i n t * p o i n t e r , i n t * p o i n t e r , i n t s p a n , i n t s p a n , }mx; }mx; \nif (...) if (...) { m x . p o i n t e r = m a l l o c ( m 1 ) ; m x . p o i n t e r = m a l l o c ( m \n1 ) ; mx.span=m1; } else else { m x . p o i n t e r = m a l l o c ( m 2 ) ; m x . p o i n t e r = m a \nl l o c ( m 2 ) ; mx.span=m2; } m x . p o i n t e r [ k ] = 0 ; m x . p o i n t e r [ k + t i d * m x \n. s p a n / s i z e o f ( i n t ) ] = 0 ; (a) Creating span (b) Computing span Figure 4: Computing span \nbefore thread space creation type correspondingly. Similarly, if the original type is a structure type \nwe promote each of its .eld types. The whole promotion pro\u00adcess proceeds recursively because there may \nexist various combi\u00adnations of aggregate types and pointer types. Figure 5 lists the promotion rules. \nWe apply promotion to all variable instantiations in a program, including global variable dec\u00adlarations, \nlocal variable declarations, formal parameter declarations and heap variable allocations. Figure 6 lists \nthe promotion function used in Figure 5. Note that the promotion function uses int type as the example \nfor all primitive types. After promoting all declara\u00adtions, we adjust each reference site to ensure that \nafter promotion each reference refers to the same data as in the original program. As shown in those \nRef and Deref rules, this is done by replacing each original reference with a new reference to the pointer \n.eld if its type is promoted. We do not list rules for all kinds of refer\u00adences here. Instead we use \nstore assignments to illustrate the idea. Load sites and call-site argument passing are processed similarly. \nAssignments to structure variables are treated as a series of scalar assignments. 3.3.2 Computing Span \nFor each pointer variable to which a span .eld is attached, we in\u00adsert an assignment statement to compute \nthe span .eld immedi\u00adately after each assignment to that pointer. Table 3 lists the rules for computing \na span .eld. The third column shows the statement being inserted after the original assignment shown \nin the second column. Note that for the rule Address taken 2, we fetch the size of the whole structure \nrather than the size of its corresponding .eld. For the rule Pointer arithmetic 2, an integer variable \ni is used to keep the difference between two pointers. In the subsequent pro\u00adgram code, the integer variable \ncan be used to recover any of the two pointers, e.g., by performing p - i or q + i. We promote the type \nof integer variables that are used to keep pointer differences to a structure similar to the one for \npointer promotion, whose .rst .eld is the integer variable itself and the second .eld represents the \nspan. The span .eld for such integer variables are used in the t\u00adwo rules for pointer arithmetic. For \nthe sample program in Figure 3(a), Figure 4(b) lists the corresponding program after creating and inserting \nstatements for computing the span .eld. [Decl Pointer] [Decl Array] [Decl Struct] G f v : T * G f v \n: T [n] G f v : struct(f1 : T1, ..., fm : Tm) G' f v : promote(T *) G' f v : promote(T )[n] G' f v : \nstruct(f1 : promote(T1), ..., fm : promote(Tm)) [Decl Heap] [Decl Function] G f v = malloc(sizeof (T \n)) G f fm : Tm f unc(f1 : T1, ..., fm-1 : Tm-1) G' f v = malloc(sizeof (promote(T ))) G' f fm : promote(Tm) \nf unc(f1 : promote(T1), ..., fm-1 : promote(Tm-1)) [Ref Pointer] [Ref Array] [Ref Field] [Deref Pointer] \nG f v : T * G f v : T * [n] G f v : struct(f1 : T1, ..., fi : T *, ...fm : Tm) G f v : T * * G f v = \nval G f v[i] = val G f v.fi = val G f *v = val G' f v.pointer = val G' f v[i].pointer = val G' f v.fi.pointer \n= val G' f (*(v.pointer)).pointer = val Figure 5: Pointer promotion rules. promote(t): case t of [int] \n: return int [int*] : return struct(int* pointer, int span) [T *] : return struct(promote(T )* pointer, \nint span) Figure 6: Auxiliary function in promotion rules. Table 3: Span computing rules. Rule Original \nInsert after Address taken 1 p = &#38;a p.span = sizeof(a) Address taken 2 p = &#38;s.a p.span = sizeof(s) \nMemory allocation p = malloc(n) p.span = n Pointer assignment p = q p.span = q.span Pointer arithmetic \n1 p = q \u00b1 1 p.span = q.span Pointer arithmetic 2 i = p -q i.span = p.span -q.span Pointer arithmetic \n3 p = p \u00b1 i p.span = p.span \u00b1 i.span  3.4 Reducing Time and Memory Overhead Obviously, the memory access \nredirection will incur overhead. For example, in Table 2, the private memory operation *(p + tid*span/sizeof(*p)) \ncosts more than in the original program because of the complicated memory accessing expression. Opera\u00adtions \nto compute the span .eld (c.f. Table 3) also take extra time. Fortunately, several compiler optimizations \ncan be applied to low\u00ader the overhead. First of all, the pointer assignment p=p+1 does not cause the \nspan .eld of pointer p to be changed. Hence, it is unnecessary to place a p.span=p.span after p=p+1. \nBy applying dead store elimination, p.span=p.span can be removed. Second\u00adly, by constant propagation \nand copy propagation, p and q may be found to always point to the same-sized data structure. In this \ncase, p.span=q.span can be eliminated. Furthermore, if the object that a pointer points to is not involved \nin privatization, we do not pro\u00admote the pointer at all. To avoid memory overhead due to unnecessary \nexpansion, we perform alias analysis in the compiler to .nd out whether a data structure gets referenced \nby private memory accesses (as de.ned in De.nition 5) in the targeted loop, no matter whether the data \nstructure is local, global or heap-allocated. If not, the data structure will not be expanded. 4. Evaluation \n4.1 Experimentation Setup We have implemented the techniques presented above in the GCC compiler, version \n4.7. Our experiment was conducted on a multi\u00adcore machine with two 2.00GHz Quad-Core AMD Opteron 8350 \nprocessors and 32GB RAM, with Linux 3.4.10 installed. In order to assess how these techniques enable \nloop parallelization to speed up sequential programs, our experiment was performed on a set of programs \nfrom well-known benchmark suites such as Mibench [17], MediaBench II [14] and SPEC CPU [1]. Table 4 shows \nthe characteristics of these benchmarks. Note that H263-encoder has two promising loops which take most \nof the computation. We se\u00adlect these benchmarks because of their inherent parallelism which justi.es \nthe use of privatization or data structure expansion, al\u00adthough our techniques are not limited to particular \ndata structures. The benchmarks not selected lack suf.cient parallelism to justify the synchronization \noverhead, regardless of the privatization tech\u00adniques used. To our best knowledge, among previous automatic \nmulti-threading methods, only the dynamic privatization technique can handle the DOALL and DOACROSS loops \nin these bench\u00admarks. The dynamic privatization technique, however, incurs a much higher runtime overhead \nthan general data structure expan\u00adsion, as indicated in our results below. Previous work has exploited \npipeline parallelism for MediaBench [12, 31], but the speedup is not as good as ours. The work.ow of \nour tool is shown in Figure 7. Our experiment consists of two stages, namely, estimating the overhead \nof our tech\u00adniques and evaluating the impact of our techniques on paralleliza\u00adtion. In our experiment, \nwe obtained the data dependence graph for each loop candidate .rst by data dependence pro.ling [38, 39]. \nWe then inspect the program to verify the general validity of the graph. The reason for using pro.ler-generated \ngraphs is because, for all benchmark programs in our experiment, current compile\u00adtime data dependence \nanalysis algorithms are still too conservative and they report false positives that prevent loop parallelization. \nFig\u00adure 8 shows the breakdown of dynamic memory accesses of each loop candidate. These accesses are measured \nat runtime by collect\u00ading only load/store instructions. In this chart, the bars marked as Free of loop-carried \ndep refer to the memory accesses that are free of any loop-carried data dependence. The Expandable bars \nrefer to the thread-private memory accesses according to De.nition 5. Consequently, With loop-carried \ndep refers to all the remain\u00ading memory accesses involved in at least one loop-carried depen\u00addence. Although \nwe cannot infer any real speedup from this chart, it is quite obvious that, for some of these benchmarks, \nif we do not Table 4: From left to right: benchmark name, benchmark suite, code size counted by lines, \nfunction containing the parallelized loop, loop nesting level (1 meaning the outermost loop), type of \nparallelism, loop execution time as percentage of the whole program. Benchmark Suite #LOC Function Level \nParallelism %Time dijkstra MiBench 375 main 1 DOACROSS 99.9% md5 MiBench 420 main 1 DOALL 99.8% mpeg2-encoder \nMediaBench II 7605 motion estimation 3 DOALL 70.6% mpeg2-decoder MediaBench II 9832 picture data 2 DOALL \n97.8% h263-encoder MediaBench II 8105 NextTwoPB 2 DOALL 43.2% MotionEstimatePicture 2 DOALL 37.1% 256.bzip2 \nSPEC CPU2000 4649 compressStream 2 DOACROSS 99.8% 456.hmmer SPEC CPU2006 35992 main loop serial 2 DOACROSS \n99.9% 470.lbm SPEC CPU2006 1155 LBM performStreamCollide 2 DOALL 99.1% have an effective way to remove \nthe data dependences due to con\u00adtentious data structures, we will have to resort to synchronization across \nthreads in order to execute the target loop in parallel, with high overhead. Figure 7: Our parallelization \nframework. Figure 8: Breakdown of memory accesses of promising loops.  4.2 Evaluating the Overhead \nTable 5 lists the number of dynamic data structures privatized for each benchmark. Note that none of \nthese loops can be parallelized by production compilers, such as Intel ICC [3]. We evaluate the overhead \ndue to our techniques at two levels, namely, with and without optimizations. Figure 9a shows the slowdown \ncaused by our data structure expansion without any optimizations mentioned in Section 3.4, with native \nrunning time normalized to 1. The numbers are collected when the measured programs are executed sequentially \non a single core. For most of the benchmarks, the overhead is high before optimizations for the kind \nof challenging loops we target. The last two bars show the harmonic average overhead for all benchmarks, \nwhich brings an 1.8x slowdown. On the contrary, Figure 9b shows the slowdown caused by data structure \nexpansion with optimizations mentioned in Section 3.4. We can see from the chart that the overhead is \nkept below 5% of the original program execution time on average, which makes our techniques quite practical \nas the result. To further assess the time ef.ciency of our techniques, we compare with an alternative \nmethod, i.e., an implementation of Table 5: The number of dynamic data structures privatized for each \nbenchmark. Benchmark #Privatized dijkstra 2 md5 1 mpeg2-encoder 7 mpeg2-decoder 3 h263-encoder 6 256.bzip2 \n4 456.hmmer 8 470.lbm 2  (b) With optimizations.  Figure 9: Overhead of data structure expansion with/without \nopti\u00admizations. the runtime privatization. The memory overhead is also measured when a benchmark is executed \nin parallel. We show the results below. 4.2.1 Runtime Privatization Runtime privatization aims at privatizing \ndata structures at run time, by transferring data between the shared space and thread\u00adprivate spaces. \nThis scheme for handling various kinds of data structures has been adopted by several recent parallel \nprogramming models [12] as well as thread-level speculation (TLS) systems [19, 33]. While there is no \navailable runtime privatization techniques ready for a given data dependence graph, we borrow and adapt \nthe idea from SpiceC [12]. In SpiceC, all memory accesses are monitored by a runtime ac\u00adcess control \nsystem which assures that the data being accessed by one thread are copied from the shared space to the \ncorresponding thread-private space properly. Besides the copying-in operations, the access control system \nis also responsible for committing thread\u00adlocal changes to the shared space. For the purpose of evaluating \nits performance for privatization, we identify private memory access\u00ades in the way described in Section \n3.2 and insert a function call before each private access. These function calls invoke the func\u00adtions \nimplemented in a user-level runtime library and they are in charge of dynamically locating thread-local \nstorage. According to the presentation in [12], the access control of global or stack vari\u00adables can \nbe performed statically by allocating thread-local copies at compile time and adjusting private memory \naccesses according\u00adly. The access control for heap-allocated objects, however, must be performed at runtime \nby invoking those functions in the runtime library for each private pointer dereference because of the \ncom\u00adpiler s inability to resolve alias relations. We also implement their Heap pre.x technique for fast \nlocating thread-local storage. Simply speaking, a heap pre.x is the meta-data of a data structure which \ncontains the location information of the data structure in each pri\u00advate space, and it is located adjacent \nto the data structure in the shared space. Given a pointer pointing to the beginning of a data structure, \na fast way to redirect it to the corresponding thread-local copies is by accessing the heap pre.x. However, \nas the authors of SpiceC [12] suggest, there is no known method yet to decide whether this fast scheme \nis safe. Hence, we extend the scheme in a safe way such that it adapts to the situation in which a given \npointer may point to any part of a data structure rather than just the start of that data structure. \nFigure 10 compares our static data structure expansion with dy\u00adnamic privatization as we implemented. \nFor most of the bench\u00admarks, the result shows that runtime privatization incurs much higher time overhead \nthan ours. However, there also exist a few pro\u00adgrams that issue just a small number of private memory \naccesses, for which the monitoring overhead is low for runtime privatization. On the other hand, such \nmemory accesses are involved in ambigu\u00adous alias relations involving many program statements, causing \nour technique to promote types in a large program scope.   4.3 Evaluating the Effect Our parallelized \nloops invoke the Gomp library [2]. For DOAL-L loops, we use static chunk scheduling. For DOACROSS loop\u00ads, \nwe use dynamic scheduling with the chunk size of one. Be\u00adsides data structure expansion, we also place \nnecessary inter-thread synchronization for the safety of DOACROSS parallelization. Fig\u00adures 11a and 11b \nplot the parallel speedups (Y-axis) of the target loops and the whole program, respectively, over the \noriginal pro\u00adgram. The X-axis marks the number of cores used. The slowdown in the single-core case re.ects \nthe overhead due to privatization, synchronization and the calls to the Gomp library. Speedups are good \nwith md5, mpeg2-encoder and h263-encoder. On the oth\u00ader hand, for dijkstra, mpeg2-decoder, 256.bzip2, \n456.hmmer and 470.lbm, the speedups have plateaued as the number of cores exceeds 4. To explain the \ncause for the observed plateau, we list the in\u00adstruction counts breakdown of an 8-core run for each benchmark \nin Figure 12. For 256.bzip2 and 456.hmmer, of which the paral\u00adlelized loops are DOACROSS, inter-thread \nsynchronization takes the majority of the running time. This means that our synchroniza\u00adtion placement \nalgorithm still has room for improvement. Recent literature has reported several techniques that are \neffective for en\u00adhancing the performance of some of these benchmark programs, but we have not incorporated \nthem in our experiments. Loop paral\u00adlelism can be enhanced by exploiting I/O parallelism in 256.bzip2 \n[13] and by recognizing commutative code regions in 456.hmmer [26]. As for 470.lbm and mpeg2-decoder, \nthere is room for load balance improvement, since Figure 12 shows that a large portion of time was spent \non calling do wait and cpu relax in Gomp. Memory access ef.ciency is also an area for improvement. The \nperformance pro.ling tool Perf [8] reports that the parallel perfor\u00admance of 470.lbm suffers from the \nmemory bandwidth constrain\u00adt when the number of cores exceeds 4. Programs dijkstra and mpeg2-decoder, \non the other hand, suffer from increased cache misses as the number of cores increases. Figure 13 shows \nthe speedup attained by applying runtime pri\u00advatization instead of our expansion technique. For most \nof the benchmarks, there is nearly no speedup due to the large runtime overhead. We have also measured \nthe performance of the paral\u00adlelized code without privatization for each benchmark. Unfortu\u00adnately, this \nwould require excessive synchronization due to the spurious loop-carried dependences, causing a slowdown \ninstead of speedup for the loops. Figure 13: Loop speedup by runtime privatization. Obviously, data \nexpansion method and runtime privatization will both increase the memory use. Figure 14 shows the memory \nuse after applying these two methods respectively, measured as a multiple of the usage by the original \nsequential program. The run\u00adtime privatization, as we implement it, never privatizes any mem\u00adory location \nthat is not recognized as thread-private. Hence, one can regard their memory usage to be necessary for \nachieving the loop speedup that has been shown. In comparison, the .gure also shows that our data expansion \nmethod adds quite little to that mem\u00adory overhead except for the program h263.encoder executed on eight \ncores, in which case our method has increased the memory use by 50%. Note that we have not yet implemented \nthe memory reduction technique in Section 3.4. Nonetheless, the virtual mem\u00adory management in Linux system \nthat we used in the experiment is quite ef.cient in physical memory use a virtual block does not  (a) \nLoop speedup. (b) Total speedup.  get a physical allocation until the block is actually referenced \nin the program. 5. Related Work Parallelizing compilers have traditionally used scalar expansion [4, \n5, 25] to eliminate the loop-carried anti-and output data depen\u00addences associated with scalar variables. \nSome research studies have also found that applying this technique to arrays can help expose parallelism \nin loops which would otherwise be sequential [10]. Ar\u00adray expansion or privatization techniques [11, \n15, 16, 18, 22, 23, 36], which are based on array data.ow analysis and works well for Fortran-like programs, \nare now essential facilities in parallelizing compilers. Going beyond static privatization method, researches \nhave also resorted to dynamically constructing individual data structures for simultaneous multiple threads. \nThe PD Test [28] inspects variable privatizability and performs parallelization at runtime immediately \nbefore a candidate loop is executed in parallel, in an inspector/ex\u00adecutor way. The LRPD Test [29] and \nR-LRPD Test [7] improves the PD Test for speculatively parallelization. Furthermore, to acceler\u00adate dynamic \nprivatization, Hybrid Analysis [30] utilizes compiler\u00adgenerated predicates to aid runtime memory reference \nanalysis. These techniques are still limited to handling Fortran-like array\u00adbased programs. To extend \nthe generality for handling dynamic data structures, the recent software-based thread level speculation \n(TLS) system CorD [32 34] moves shared data structures copies that may be modi.ed by speculative threads \ninto thread-local memory space for private use. Another dynamic privatization technique, called Priva\u00adteer \n[19], also supports speculative execution, but it is based on processes. With processes, privatization \noverhead is reduced be\u00adcause different simultaneous processes maintain separate address spaces. The process-based \nPrivateer, however, does not parallelize DOACROSS and pipelined loops. Behavior-oriented paralleliza\u00adtion \n(BOP) [9, 20] also performs speculative privatization based on multi-processes. In BOP, OS-assisted techniques \nfor copying data between spaces are heavily used and page-level access tracking is employed to detect \nmisspeculations. Software Transactional Memory (STM) systems for thread lev\u00adel speculation also provide \nfacilities to privatize data structures at run time during a transaction. For example, STMlite [24] manipu\u00adlates \ntransactional store operations on a thread-local write set. Ev\u00adery time when a transactional load is \nabout to be executed, the sys\u00adtem .rst checks the transaction s write signature to see if this trans\u00adaction \nhas previously written to the same address as the load. If so, the load is redirected to read data from \nthe thread-local write set; otherwise the load reads directly from the memory location that saves the \nvalue computed by other threads previously. Hash ta\u00adbles are employed for con.ict checking, which incurs \nconsiderable computational overhead. Several parallel programming models provide privatization di\u00adrectives \nto be inserted in sequential codes for parallelization. For example, OpenMP allows user-speci.ed privatization \nfor scalar or array variables. Paralax [37] makes an improvement to OpenMP by providing KILL(var) annotations \nto indicate that a data structure is privatizable. The KILL annotation applies to the scalar, array or \nstructure that var points to. Unfortunately, it does not apply recur\u00adsively to deeper data structures \nthat can be referenced by chasing pointers starting from var. SpiceC [12] supports data privatization \ninvolving dynamic data structures implicitly after parallel regions are identi.ed by the programmer. \nWe have analyzed the run-time cost for the copy-in and copy-out operations used in SpiceC in the previous \nsection. Tournavitis et al. [35] propose a pro.le-driven parallelizing compiler which successfully parallelizes \nmany numeric benchmark programs. Unfortunately, their privatization technique is based on named memory \nlocations such as scalar and array variables. Hence, it can not handle dynamic data structures. Thies \nel al. [31] presents a pro.le-based approach to exploiting coarse-grained pipeline par\u00adallelism in C \nprograms. They privatize necessary data by simply forking the original program into multiple processes. \nThe memo\u00adry spaces of the processes are isolated from one another, yet the processes share exactly the \nsame data layout such that no pointers or instructions need to be adjusted either at compile time or \nrun time. A standard inter-process communication mechanism (such as pipes) is used to send shared data \nfrom one process to anoth\u00ader. At the end of a parallel loop s execution, all of the process\u00ades commit \ntheir modi.ed data into a single process that contin\u00adues after the loop. While such a multi-processes \napproach works well for streaming applications which utilize simple data structures (mainly consecutive \nlinear buffers), it has dif.culties in privatiz\u00ading highly dynamic discrete data structures. For example, \na process enlarging/shrinking a data structure (by using malloc, realloc or free) will cause other processes \nto see different data layouts, making the synchronization and commit dif.cult to manage. 6. Conclusion \nVariable privatization is a crucial step in parallelization of many nonnumerical programs. We have proposed \na compile-time tech\u00adnique to expand arbitrary data structures, thus overcoming the dif\u00ad.culties faced \nby other techniques when trying to privatize dynam\u00adic and irregular data structures. Experiments with \neight benchmark programs show the harmonic mean of 1.93 for the total program speedup with four cores \nand 2.24 with eight cores. Future work to improve our work can be conducted in several areas. Most im\u00adportantly, \nthe state of the art compilers are not yet powerful enough to obtain the data dependence graph required \nfor applying our tech\u00adnique to the benchmarks used in our experiments. As such, we have used off-line \ndata dependence pro.ling to obtain such a graph, with manually performed veri.cation afterwards. In addition \nto pursuing advanced compiler analysis for building such a graph, we will ex\u00adplore effective ways for \nthe programmer to interact with the compil\u00ader in the veri.cation of the data dependence graph. We have \nused fat pointers to facilitate type promotion, which will face interoperabil\u00adity issues when the program \ncalls library routines with no source code for compiler analysis. Binary analysis on such routines can \nbe a useful tool in such situations. Lastly, the bonded mode and the interleaved mode for data structure \nallocation have their respective strengths and weaknesses, as discussed in the paper, which natu\u00adrally \nraises the prospect of devising an adaptive scheme to switch between these two modes. Acknowledgments \nThe authors thank Xinmin Tian for his advice concerning the use of Intel s ICC compiler. This work is \nsponsored in part by the National Science Foundation through grants CNS-0915414, CCF-0811587, and CCF-0702245, \nand by an Intel research grant and an Intel gift. References [1] http://http://www.spec.org/cpu/. [2] \nhttp://gcc.gnu.org/projects/gomp/. [3] http://software.intel.com/en-us/intel-compilers/. [4] M. G. Burke, \nR. Cytron, J. Ferrante, and W. C. Hsieh. Automatic generation of nested, fork-join parallelism. The Journal \nof Supercom\u00adputing, pages 71 88, 1989. [5] R. Cytron and J. Ferrante. What s in a name? -or-the value \nof renaming for parallelism detection and storage allocation. In ICPP 87, pages 19 27, 1987.  [6] L. \nDagum and R. Menon. OpenMP: An industry-standard API for shared-memory programming. IEEE Comput. Sci. \nEng., 5(1):46 55, Jan. 1998. [7] F. Dang, H. Yu, and L. Rauchwerger. The R-LRPD test: Specula\u00adtive parallelization \nof partially parallel loops. In Proceedings of the 16th International Symposium on Parallel and Distributed \nProcess\u00ading, IPDPS 02, pages 20 , 2002. [8] A. de Melo. The new linuxperftools. In Slides from Linux \nKongress, 2010. [9] C. Ding, X. Shen, K. Kelsey, C. Tice, R. Huang, and C. Zhang. Software behavior oriented \nparallelization. In Proceedings of the 2007 ACM SIGPLAN Conference on Programming Language Design and \nImplementation, PLDI 07, pages 223 234, 2007. [10] R. Eigenmann, J. Hoe.inger, Z. Li, and D. A. Padua. \nExperience in the automatic parallelization of four perfect-benchmark programs. In Proceedings of the \n4th International Workshop on Languages and Compilers for Parallel Computing, LCPC 92, pages 65 83, 1992. \n[11] P. Feautrier. Array expansion. In Proceedings of the 2nd International Conference on Supercomputing, \nICS 88, pages 429 441, 1988. [12] M. Feng, R. Gupta, and Y. Hu. SpiceC: scalable parallelism via implicit \ncopying and explicit commit. In Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel \nProgramming, PPoPP 11, pages 69 80, 2011. [13] M. Feng, R. Gupta, and I. Neamtiu. Effective parallelization \nof loops in the presence of I/O operations. In Proceedings of the 33rd ACM SIGPLAN Conference on Programming \nLanguage Design and Implementation, PLDI 12, pages 487 498, 2012. [14] J. E. Fritts, F. W. Steiling, \nJ. A. Tucek, and W. Wolf. Mediabench II video: Expediting the next generation of video systems research. \nMicroprocess. Microsyst., 33(4):301 318, June 2009. [15] J. Gu, Z. Li, and G. Lee. Experience with ef.cient \narray data .ow anal\u00adysis for array privatization. In Proceedings of the 6th ACM SIGPLAN Symposium on \nPrinciples and Practice of Parallel Programming, P-POPP 97, pages 157 167, 1997. [16] M. Gupta. On privatization \nof variables for data-parallel execution. In Proceedings of the 11th International Symposium on Parallel \nProcess\u00ading, IPPS 97, pages 533 541, 1997. [17] M. R. Guthaus, J. S. Ringenberg, D. Ernst, T. M. Austin, \nT. Mudge, and R. B. Brown. Mibench: A free, commercially representative em\u00adbedded benchmark suite. In \nProceedings of the Workload Characteri\u00adzation, 2001. WWC-4. 2001 IEEE International Workshop, WWC 01, \npages 3 14, 2001. [18] M. H. Hall, S. P. Amarasinghe, B. R. Murphy, S.-W. Liao, and M. S. Lam. Detecting \ncoarse-grain parallelism using an interprocedural par\u00adallelizing compiler. In Proceedings of the 1995 \nACM/IEEE Confer\u00adence on Supercomputing (CDROM), Supercomputing 95, 1995. [19] N. P. Johnson, H. Kim, \nP. Prabhu, A. Zaks, and D. I. August. Specula\u00adtive separation for privatization and reductions. In Proceedings \nof the 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI 12, pages \n359 370, 2012. [20] K. Kelsey, T. Bai, C. Ding, and C. Zhang. Fast track: A software system for speculative \nprogram optimization. In Proceedings of the 7th Annual IEEE/ACM International Symposium on Code Generation \nand Optimization, CGO 09, pages 157 168, 2009. [21] M. Kim, H. Kim, and C.-K. Luk. SD3: A scalable approach \nto dy\u00adnamic data-dependence pro.ling. In Proceedings of the 43rd Annual IEEE/ACM International Symposium \non Microarchitecture, MICRO 43, pages 535 546, 2010. [22] Z. Li. Array privatization for parallel execution \nof loops. In Proceed\u00adings of the 6th International Conference on Supercomputing, ICS 92, pages 313 322, \n1992. [23] D. E. Maydan, S. P. Amarasinghe, and M. S. Lam. Array-data .ow analysis and its use in array \nprivatization. In Proceedings of the 20th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, \nPOPL 93, pages 2 15, 1993. [24] M. Mehrara, J. Hao, P.-C. Hsu, and S. Mahlke. Parallelizing sequen\u00adtial \napplications on commodity hardware using a low-cost software transactional memory. In Proceedings of \nthe 2009 ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI 09, pages 166 \n176, 2009. [25] D. A. Padua and M. J. Wolfe. Advanced compiler optimizations for supercomputers. Commun. \nACM, 29(12):1184 1201, Dec. 1986. [26] P. Prabhu, S. Ghosh, Y. Zhang, N. P. Johnson, and D. I. August. \nCom\u00admutative set: a language extension for implicit parallel programming. In Proceedings of the 32nd \nACM SIGPLAN Conference on Program\u00adming Language Design and Implementation, PLDI 11, pages 1 11, 2011. \n[27] L. Rauchwerger, N. M. Amato, and D. A. Padua. A scalable method for run-time loop parallelization. \nInt. J. Parallel Program., 23(6):537 576, Dec. 1995. [28] L. Rauchwerger and D. Padua. The privatizing \nDOALL test: a run\u00adtime technique for DOALL loop identi.cation and array privatization. In Proceedings \nof the 8th International Conference on Supercomput\u00ading, ICS 94, pages 33 43, 1994. [29] L. Rauchwerger \nand D. Padua. The LRPD test: speculative run-time parallelization of loops with privatization and reduction \nparalleliza\u00adtion. In Proceedings of the 1995 ACM SIGPLAN Conference on Pro\u00adgramming Language Design and \nImplementation, PLDI 95, pages 218 232, 1995. [30] S. Rus, L. Rauchwerger, and J. Hoe.inger. Hybrid analysis: \nstatic &#38; dynamic memory reference analysis. In Proceedings of the 16th International Conference on \nSupercomputing, ICS 02, pages 274 284, 2002. [31] W. Thies, V. Chandrasekhar, and S. Amarasinghe. A practical \nap\u00adproach to exploiting coarse-grained pipeline parallelism in C program\u00ad s. In Proceedings of the 40th \nAnnual IEEE/ACM International Sym\u00adposium on Microarchitecture, MICRO 40, pages 356 369, 2007. [32] C. \nTian, M. Feng, and R. Gupta. Speculative parallelization using state separation and multiple value prediction. \nIn Proceedings of the 2010 International Symposium on Memory Management, ISMM 10, pages 63 72, 2010. \n[33] C. Tian, M. Feng, and R. Gupta. Supporting speculative parallelization in the presence of dynamic \ndata structures. In Proceedings of the 2010 ACM SIGPLAN Conference on Programming Language Design and \nImplementation, PLDI 10, pages 62 73, 2010. [34] C. Tian, M. Feng, V. Nagarajan, and R. Gupta. Copy or \ndiscard execution model for speculative parallelization on multicores. In Proceedings of the 41st Annual \nIEEE/ACM International Symposium on Microarchitecture, MICRO 41, pages 330 341, 2008. [35] G. Tournavitis, \nZ. Wang, B. Franke, and M. F. O Boyle. Towards a holistic approach to auto-parallelization: integrating \npro.le-driven parallelism detection and machine-learning based mapping. In Pro\u00adceedings of the 2009 ACM \nSIGPLAN Conference on Programming Language Design and Implementation, PLDI 09, pages 177 187, 2009. [36] \nP. Tu and D. A. Padua. Automatic array privatization. In Proceedings of the 6th International Workshop \non Languages and Compilers for Parallel Computing, LCPC 94, pages 500 521, 1994. [37] H. Vandierendonck, \nS. Rul, and K. De Bosschere. The Paralax infras\u00adtructure: automatic parallelization with a helping hand. \nIn Proceed\u00adings of the 19th International Conference on Parallel Architectures and Compilation Techniques, \nPACT 10, pages 389 400, 2010. [38] H. Yu and Z. Li. Fast loop-level data dependence pro.ling. In Pro\u00adceedings \nof the 26th ACM International Conference on Supercomput\u00ading, ICS 12, pages 37 46, 2012. [39] H. Yu and \nZ. Li. Multi-slicing: a compiler-supported parallel approach to data dependence pro.ling. In Proceedings \nof the 2012 International Symposium on Software Testing and Analysis, ISSTA 12, pages 23 33, 2012. [40] \nX. Zhang, A. Navabi, and S. Jagannathan. Alchemist: A transparent dependence distance pro.ling infrastructure. \nIn Proceedings of the 7th Annual IEEE/ACM International Symposium on Code Generation and Optimization, \nCGO 09, pages 47 58, 2009.  \n\t\t\t", "proc_id": "2491956", "abstract": "<p>Among techniques for parallelizing sequential codes, privatization is a common and significant transformation performed by both compilers and runtime parallelizing systems. Without privatization, repetitive updates to the same data structures often introduce spurious data dependencies that hide the inherent parallelism. Unfortunately, it remains a significant challenge to compilers to automatically privatize dynamic and recursive data structures which appear frequently in real applications written in languages such as C/C++. This is because such languages lack a naming mechanism to define the address range of a pointer-based data structure, in contrast to arrays with explicitly declared bounds. In this paper we present a novel solution to this difficult problem by expanding general data structures such that memory accesses issued from different threads to contentious data structures are directed to different data fields. Based on compile-time type checking and a data dependence graph, this aggressive extension to the traditional scalar and array expansion isolates the address ranges among different threads, without struggling with privatization based on thread-private stacks, such that the targeted loop can be effectively parallelized. With this method fully implemented in GCC, experiments are conducted on a set of programs from well-known benchmark suites such as Mibench, MediaBench II and SPECint. Results show that the new approach can lead to a high speedup when executing the transformed code on multiple cores.</p>", "authors": [{"name": "Hongtao Yu", "author_profile_id": "81503672127", "affiliation": "Purdue University, West Lafayette, IN, USA", "person_id": "P4149010", "email_address": "htyu@purdue.edu", "orcid_id": ""}, {"name": "Hou-Jen Ko", "author_profile_id": "81759022057", "affiliation": "Purdue University, West Lafayette, IN, USA", "person_id": "P4149011", "email_address": "ko16@purdue.edu", "orcid_id": ""}, {"name": "Zhiyuan Li", "author_profile_id": "81409597718", "affiliation": "Purdue University, West Lafayette, IN, USA", "person_id": "P4149012", "email_address": "zhiyuanli@purdue.edu", "orcid_id": ""}], "doi_number": "10.1145/2491956.2462182", "year": "2013", "article_id": "2462182", "conference": "PLDI", "title": "General data structure expansion for multi-threading", "url": "http://dl.acm.org/citation.cfm?id=2462182"}