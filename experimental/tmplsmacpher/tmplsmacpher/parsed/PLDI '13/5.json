{"article_publication_date": "06-16-2013", "fulltext": "\n Scalable Variable and Data Type Detection in a Binary Rewriter Khaled ElWazeer Kapil Anand Aparna Kotha \nMatthew Smithson Rajeev Barua Electrical and Computer Engineering Department, University of Maryland \nCollege Park, MD, 20742, USA {wazeer,kapil,akotha,msmithso,barua}@umd.edu Abstract We present scalable \nstatic analyses to recover variables, data types, and function prototypes from stripped x86 executables \n(without symbol or debug information) and obtain a functional intermedi\u00adate representation (IR) for analysis \nand rewriting purposes. Our techniques on average run 352X faster than current techniques and still have \nthe same precision. This enables analyzing executables as large as millions of instructions in minutes \nwhich is not possible us\u00ading existing techniques. Our techniques can recover variables allo\u00adcated to \nthe .oating point stack unlike current techniques. We have integrated our techniques to obtain a compiler \nlevel IR that works correctly if recompiled and produces the same output as the input executable. We \ndemonstrate scalability, precision and correctness of our proposed techniques by evaluating them on the \ncomplete SPEC2006 benchmarks suite. Categories and Subject Descriptors D.2.7: Software Engineer\u00ading [Distribution, \nMaintenance, and Enhancement]: Restructuring, reverse engineering, and reengineering Keywords reverse \nengineering; binary rewriting; variable recov\u00adery; type recovery; 1. Introduction Reverse engineering \nbinary executable code is commonplace to\u00adday, especially for untrusted code and malware. Agencies as \ndi\u00adverse as anti-virus companies, security consultants, code forensics consultants, law-enforcement agencies \nand national security agen\u00adcies routinely try to understand binary code. Existing tools such as the IDAPro \ndisassembler and the Hex-Rays decompiler [1] help, with the latter producing (non-executable) C-like \npseudocode text. However, existing reverse engineering tools do not exhibit sev\u00aderal desired characteristics. \nFirst, previous tools do not aim to re\u00adcover a fully-functional high-level code (similar to source code) \nfrom executables. These tools neglect variables allocated on the .oating point stack and generate intermediate \nrepresentation (IR) containing incomplete interprocedural interfaces. The recovered IR is suitable for \nhuman understanding but does not capture the com\u00adplete functionality of the input executable. Second, \nthey are either imprecise [1] or recover precise information at the cost of scala\u00adbility. For example, \nDIVINE [5], the most precise variable iden- Permission to make digital or hard copies of all or part \nof this work for personal or classroom use is granted without fee provided that copies are not made or \ndistributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. PLDI 13, June 16 19, 2013, Seattle, WA, USA. Copyright c &#38;#169; \n2013 ACM 978-1-4503-2014-6/13/06. . . $15.00 ti.cation tool proposed in the literature, spends two hours \nwhile analyzing programs of the order of 55,000 assembly instructions. Recovering a functional IR in \na scalable and accurate manner would be invaluable to security professionals. It would enable them to \nwrite compiler passes to extract properties of interest. The recov\u00adered IR can be updated with insertion, \ndeletion, or modi.cation. Running the updated rewritten program enables dynamic source\u00adlevel debugging \ntechniques such as judiciously placed print state\u00adments, and many more. Recovering functional IR is also \nvaluable for legacy binaries for which the source code has been lost. It enables users to .x bugs in \nsuch binaries, modify the functionality, optimize such binaries, or even port them to new hardware systems. \nIn this work, we present static analyses that can recover source level variable and type information \nfrom x86 binaries as large as millions of instructions in a few minutes. The produced informa\u00adtion is \nas accurate as the current state of the art x86 binary anal\u00adysis systems. The recovered information is \nrepresented in a high level compiler IR that is completely functional and produces a cor\u00adrect rewritten \nexecutable when recompiled. Our static techniques combine functionality, precision and scalability; features \nthat col\u00adlectively do not exist in today s binary analysis tools. Our methods also improve the scope \nof variable analysis and type recovery in two ways. First, unlike current binary analysis techniques, \nour recovery mechanisms are able to recognize vari\u00adables allocated on the .oating point stack. Recognizing \nsuch vari\u00adables is a hard problem in the presence of unknown indirect and external calls. Recognizing \n.oating-point stack variables is imper\u00adative for obtaining a functional IR from executables. Another \nway our methods improve the scope of analysis is by accurately identifying all register allocated variables \nused as arguments and returns from functions. Most x86 binary analyses will only identify memory allocated \narguments [4], but do not identify register allocated ones. This is acceptable when recovering pseudocode, \nbut unacceptable when recovering functional code. Some methods perform either brute force techniques \n[22] that are imprecise, or use dynamic analysis to detect arguments and returns [7] which is precise \nbut produces incomplete information. This work presents a step towards a system that rewrites ex\u00adecutables \ninto a functional high-level program representation and incorporates as much source level information \nas possible in a scal\u00adable manner. We envision the need for such a system in various security and binary \nanalysis applications. This work has the fol\u00adlowing contributions: * It produces a correct and running \nIR that can be recompiled to obtain a rewritten executable that works exactly the same way as the input \nexecutable.  Figure 1. SecondWrite Flow * It presents algorithms for solving problems missed while \nana\u00adlyzing executables like resolving .oating point stack accesses and accurately identifying interprocedural \ninterfaces. * It presents a highly scalable mechanism for identifying vari\u00adables and types which is \norders of magnitude faster than cur\u00adrent analysis techniques. Our techniques do not rely on symbol or \ndebug information to be present in binaries. * It utilizes a compiler s intermediate representation \n(LLVM) in its internals which opens the domain of running existing source\u00adlevel analysis and optimization \npasses built up over decades by hundreds of developers. * It is evaluated and shown to recover accurate \nand precise in\u00adformation from C, C++, and Fortran binaries obtained from the SPEC2006 benchmarks suite; \ncompiled using two different compilers in a reasonable amount of time.   2. Analysis and Rewriting \nFramework Figure 1 presents an overview of SecondWrite [6], [3]; our exe\u00ad cutables analysis and rewriting \nframework. SecondWrite translates the input x86 binary code to the intermediate format of the LLVM compiler \n[2]. The disassembler along with the binary reader trans\u00ad lates every x86 instruction to an equivalent \nLLVM instruction. A key challenge in binary frameworks is discovering which por\u00adtions of the code section \nin an input executable are de.nitely code. Smithson et. al. [20] proposed speculative disassembly, coupled \nwith binary characterization, to ef.ciently address this problem. SecondWrite speculatively disassembles \nthe unknown portions of the code segments as if they were code. However, it also retains the unchanged \ncode segments in the IR to guarantee the correctness of data references in case the disassembled region \nwas actually data. SecondWrite employs binary characterization to limit such un\u00adknown portions of code. \nIt leverages the restriction that an indi\u00adrect control transfer instruction (CTI) requires an absolute \naddress operand, and that these address operands must appear within the code and/or data segments. The \nbinary segments are scanned for values that lie within the range of the code segment. The resulting values \nare guaranteed to contain, at a minimum, all of the indirect CTI targets. More information on how we \nform functions and func\u00adtion boundaries using binary characterization is found in [11]. Memory stack \nanalysis is done for every procedure to detect its corresponding memory arguments as explained in [3]. \nThe tech\u00ad niques presented in [3] along with [4] are used to split the physical stack into individual \nabstract stack frames. Global and stack regions appear as arrays of bytes in the IR.  3. Decoding the \n.oating point variables In this section, we describe our technique to decode all the .oating point stack \noperations and represent them in higher level code using .oating point variables, function arguments \nand function returns, instead of the low level stack layout used in the assembly. We begin by introducing \nthe x86 .oating point stack. The .oat\u00ading point hardware stack has a maximum height of 8 which means \nthere are only 8 physical .oating point registers that can be used at any time. The names of those registers, \nas used by the hardware instructions, are dynamic and are relative to the current top of the .oating \npoint stack. If we assume the .xed physical register names are: PST0 -PST7, then the assembly instructions \nwill refer to another set of names ST0 -ST7,where ST0 always refers to the register at the top of the \nstack. For example, if the height of the stack is one, then ST0 refers to PST0. If the stack is full, \nthen ST0 refers to PST7. In general, STx is mapped to PSTy where y = TOP(I) - 1 - x where TOP(I) is the \nstack height at instruc\u00adtion I and 0 = y< TOP(I). Whenever a function returns a .oat\u00ading point value \nin a register, it pushes the value on the .oating point stack. Whenever a function takes .oating point \nvalues as arguments in registers, the caller pushes the values on the .oating point stack. It is assumed \nthat TOP(I) cannot be negative at any instruction I. Decoding the .oating point stack means mapping every \nassem\u00adbly operand among ST0 -ST7 into a corresponding IR register among PST0 -PST7. To do so in the IR, \nwe declare the registers PST0 -PST7 as local variables inside each procedure. It turns out from the previous \nequations that we only need to identify for every instruction I, what is the corresponding TOP(I) in \norder to decode the .oating point operands successfully. This task is not trivial be\u00adcause of the existence \nof indirect and external calls. If there is no indirect or unknown external call in the program, the \nproblem is trivial because we can traverse the control .ow of the program, tracking the .oating point \nstack height at every point, and set the value of TOP(I) at every instruction I depending on the .oating \npoint operations observed. This analysis will not work in the presence of indirect and external calls \nbecause when we hit such a call, we will not know what function is being called and how the height of \nthe stack will be affected by this call. We use a symbolic analysis scheme to solve this problem by maintaining \na symbolic value Xi for every indirect and external call i representing the difference of the .oating \npoint stack height before and after the call. Sometimes we refer to that difference as StackDiff in the \npaper. After doing the symbolic analysis, each TOP(I) will become a symbolic expression in terms of the \nXis. We build symbolic linear equations to solve for Xis. Once the Xisare calculated, TOP(I) will be \nknown for every instruction. It is statically indeterminable to be able to decode the .oating point operations \ncorrectly in all cases in the presence of indirect and external calls. In this work, we show that if \nwe lay out some assumptions, we can actually guarantee a correct and functional representation of the \n.oating point stack operations in all cases that adhere to those assumptions. Our assumptions are: 1. \nAt control-.ow join points, the .oating point stack height must be the same for every predecessor basic \nblock. 2. At indirect and external calls, the .oating point stack height must be zero before the call. \n 3. Every indirect or external call can return at most a single .oat\u00ading point value on the .oating point \nstack.  The above assumptions are correct in compiled code in every case in every compiler we are aware \nof. They are also true in most hand written assembly code, but may not be always true in theory. The \njusti.cations for the assumptions are as follows. (1) If the stack height is not balanced at join points, \nany subsequent .oating point stack access will be indeterminable as it might access different values \ndepending on the path taken at run time. (2) For indirect and external calls, the behavior of their targets \nis usually unknown to the compiler, and hence the compiler must assume they might use all the .oating \npoint stack registers, hence it has to clean the stack before such calls. We can state this assumption \nby saying we assume .oating point registers are scratch registers. Theoretically, a compiler might know \nin some cases the behavior of the functions being called and may not clean the .oating point stack, but \npractically we are not aware of such a compiler. (3) The last assumption above is coming from the fact \nthat we are not aware of any calling convention that allows the return of more than one .oating point \nstack register from indirect calls and externals.  We translate the above assumptions into the symbolic \nanalysis propagation rules present in .gure 2, explained as follows. For in\u00adternal function calls, we \nuse helper variables Y (F ) to represent the symbolic expression representing StackDiff of every function \nF . The executable is traversed in a depth .rst search manner starting from the entry point function \nfor the binary, and from functions that are never called directly in the code. Then we analyze the remaining \nstrongly connected components in the call graph. The assumptions (1) through (3) above represent the \nsymbolic equations in lines (1) through (3) in .gure 2. The actual values of Xis can only be zero or \none because before the call, the stack height is zero according to assumption (2), and the call can return \nat most one value according to assumption (3). The height of the stack cannot go negative and hence the \nactual value of the Xis cannot be negative. The symbolic equations represented by equations (1) through \n (3) in .gure 2 along with the symbolic unknowns Xisare trans\u00adformed into a linear system of equations. \nTo solve those equations, we employ our custom linear solver that categorize the equations into disjoint \ngroups based on the variables used in every equation, and then solve every group only if the number of \nequations is equal to the number of unknowns. We keep propagating calculated values to other groups until \nno more calculated values are present. Most of the Xis are usually solved using equation (3) in .gure \n2.  The remaining unknowns are assumed to take a value of Xi = 1 conservatively. This will be always \ncorrect because from our second assumption above, the stack height is zero before every indirect and \nexternal call. In this case, if we declare by mistake that a particular call modi.es the stack height \nby adding one element; this element will never be accessed. In this case, even if there are subsequent \n.oating point stack operations, they have to push values on the stack before reading them. The .oating \npoint register arguments and returns are declared in the IR as follows: a) Whenever a function has TOP(I) \n> 0 at its entry point instruction I, the function is declared in the IR to take as many .oating point \nvalues as the value of TOP(I). They will be passed as arguments and copied to the correct local variables \naccording to the mapping we described earlier. b) Whenever Xi or Y (F ) are greater than zero at a call \nsite, this call site will be returning one or more .oats in the IR and they will be copied to the corresponding \nlocal variables in the callers according to the TOP(I) value at the call site.  4. Function Prototypes \nRecovery Detecting the complete and accurate set of function arguments and returns is essential in producing \na high quality code that can run correctly if recompiled. If some arguments are missing, the code will \nnot work correctly in all cases. If more unnecessary arguments are identi.ed, the code will run correctly, \nbut will be less understandable by users. We show how to accurately identify the register arguments and \nreturns. Existing techniques show how to identify the exact set of memory arguments. SecondWrite already \nuses a variant of the algo- Unknown Symbolic Values : Xi,where Xi = StackDiff of indirect/external callsite \ni Helper Variables : Y (F ) = StackDiff of function F ,where F is an internal function TOP(I) = top \nof the stack after executing instruction I I. = the previous instruction to I.Atabasic block (BB) entry, \nit is the .rst instruction of BB. Initial Conditions : Root functions not called directly anywhere as \nwell as the entry point function have entry TOP(I) = 0 where I is a NOP instruction inserted at the entry \npoint of each of those functions.  Data .ow rules : At every basic block (BB)entry: TOP(I) = TOP(In),where \nIn are the terminators of the prede\u00adcessors of BB  (1) For every instruction I: I = push ... . TOP(I) \n= TOP(I.)+ 1 I = pop ... . if (TOP(I.)= Xi) Xi =1  (3) TOP(I) = TOP(I.) - 1 I = call F . if (F is \nan external or indirect) TOP(I.) = zero   (2) TOP(I) = Xi else TOP(A) = TOP(I.) where A is the .rst \nNOP instruc\u00adtion in F Analyze F to get Y (F )= func(X1, ..., Xn) TOP(I) = TOP(I.)+ Y(F) I = return from \nF . Y (F )= TOP(I.)-TOP(A), A is the .rst NOP instruction in F .Z = return from F . TOP(Z) = TOP(I.) \nFigure 2. Data .ow rules used to decode the .oating point stack rithm used by Balakrishnan et. al. [4] \nto identify memory arguments [3]. Surprisingly, we did not .nd any related work that recognizes correctly \nand accurately register arguments and returns. Not recog\u00adnizing register arguments and returns is acceptable \nif the goal is to help human understanding of binaries (as for existing methods), but unacceptable if \nthe goal is to generate correct rewritten code (as for our method.) Typical x86 codes have less register \narguments than memory arguments, but they still have large numbers of register arguments especially for \noptimized executables. A brute force algorithm for identifying register arguments and returns is to de.ne \nthe set of registers read without being initialized inside a procedure as arguments, and the registers \nmodi.ed inside a procedure and then later used at some of the call sites as returns. This technique will \nresult in many spurious arguments since all registers which are saved and then restored back in a function \n(such as callee saves) will be declared as arguments and returns for this function, which is not true. \nFurther, this algorithm might miss some arguments if not carefully implemented. For example, a procedure \nnot accessing any register at all might be declared as taking no register arguments, which may not be \ntrue since it might be calling a function which is taking a register argument.  We propose below an \nalgorithm which identi.es accurately all register arguments and returns. Our algorithm is conservative \nsince it will not miss any arguments. It is also accurate since it prunes out unnecessary extra arguments \nin many cases. The main challenge in being accurate and yet conservative is that the stack locations \nused to save registers need to be tracked to make sure they are only used for this purpose, thus allowing \nthose registers to be pruned from the arguments or returns. The stores of the register values at the \nbeginning of the function should dominate the loads used to restore them back. There should not be any \nwrite to those stack locations in between. If those stack locations are read in the middle of a function, \nthe corresponding registers must be declared as arguments. Our register arguments and returns detection \nalgorithm is com\u00adposed of .ve steps. 1) We assume all registers are arguments to every function and there \nare no register returns. 2) We declare all registers written to inside a function or any of its callees \nas poten\u00adtial return registers. 3) We run our algorithm for detecting saved locations by detecting the \nset of stores to the memory stack which are never loaded back except before the return from the function. \nWe call those store instructions DeadStores since they will be even\u00adtually removed from the code. For \neach of the detected dead stores, we determine the corresponding saved register and remove it from the \npotential returns set. 4) We run our algorithm to propagate the register arguments correctly and prune \nunused ones. 5) We prune the unused return registers out. Next, we describe each of those steps in details. \nStep 1 is trivial. We proceed from step two. The second step in our algorithm is to detect the initial \nset of potential return registers. The simple idea is that any register which is being written to inside \na function is a potential return register from this function. For example, if a function foo is calling \nfunction bar,and bar is modifying eax,then foo and bar will be declared as potentially returning eax \ndespite the fact that there is no write to eax inside of foo. We do a post-order depth-.rst search traversal \nof the call graph (which visits child nodes before their parents) and propagate the set of potential \nreturn registers upwards in the call graph by looking for the written-to registers. Whenever we .nd a \ncall to a function, we add its potential returns to the caller function potential returns. We handle \nrecursion using a work list mechanism such that whenever we detect a call to a function which has not \nbeen analyzed yet, we add the caller function back to the work list. After detecting the potential returns, \nwe add them to the IR in every return statement inside every function. If more than one register is returned, \nwe return a structure containing all combined potential return registers. The third step in our algorithm \nis to detect the callee saves reg\u00adisters and exclude them from the list of potential returns. Since callee-saves \nvalues are saved to the memory stack, we need a mem\u00adory analysis technique to track the memory stack \nlocations where they are saved. Tracking memory in executables is not a trivial task. Our saved registers \ndetection does not need a sophisticated mem\u00adory tracking algorithm because it only needs to track stack \nmemory. Neither heap nor global memory need to be tracked. We modify the Value Set Analysis (VSA) algorithm \nproposed by Balakrishnan et. al. [4] by removing global and heap memory tracking, keeping only stack \nmemory tracking. We also remove the context sensitivity from the algorithm since it is not needed in \nthis application. The resulting algorithm is less powerful for general memory tracking but is suf.cient \nfor this purpose. As a quick summary of the VSA algorithm, it derives a con\u00adservative estimate of the \nset of addresses and integer values every memory location and register can contain at any program point. \nEvery set of values is represented as a strided interval with a lower Algorithm 1: The callee-saves detection \nalgorithm 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 \n35 36 37 38 39 Input: A copy of the LLVM IR for a binary Input: PotArgs : maps functions to their potential \nregister arguments Input: PotRets : maps functions to their potential return registers Output: DeadStores \n: maps functions to the dead register stores Output: PotRets : The input map after pruning saved registers \nforeach reg . PotArgs do Create a dummy register dummy ; DummyRegs(reg)= dummy end ADDRS = f foreach \nFunction F do foreach Instruction I in F do if I = store reg, Ptr AND reg . PotArgs then if ValueSet(Ptr) \n= {address} (Singleton) then ADDRS = ADDRS .{(reg,address,I)} end end end foreach (reg,address, I) . \nADDRS do allocate a dummy pointer DummyPtr((reg, address)) at the beginning of F store DummyRegs(reg)to \nDummyPtr((reg, address)) end foreach Instruction I in F do if I is UnsafeInstruction(address) where (reg,address,X) \n. ADDRS then insert a volatile load from DummyPtr((reg, address)) end if I = store value, Ptr AND ValueSet(Ptr) \n.{address}AND (reg,address,X) . ADDRS then insert a store value to DummyPtr((reg, address)) end if I \n= load Ptr AND ValueSet(Ptr) .{address} AND (reg,address,X) . ADDRS then insert I = load DummyPtr((reg, \naddress)) for every use of I insert a cloned use of I end end Run LLVM Memory to Register Promotion on \nAll DummyPtr Run LLVM Dead Code Elimination on F foreach (reg,address, I) . ADDRS do if DummyPtr(reg, \naddress) is deleted AND DummyRegs(reg) has no uses OR only used in return instructions then DeadStores(F) \n= DeadStores(F) .{I} end if DummyRegs(reg) has no uses OR DummyRegs(reg) is used in all return instructions \nof F then PotRets(F) = PotRets(F) -{reg} end end   end and upper bounds; and a stride. In our modi.ed \nimplementation of VSA, we only keep track of the lower and upper bounds. Before we run the saved registers \ndetection algorithm, we con\u00advert the registers inside of each function into the SSA form. This is straight \nforward; indeed in our implementation LLVM already does that. Our algorithm works on a temporary copy \nof the IR. Algorithm 1 detects the dead stores used to save registers and prunes those saved registers \nfrom the potential return register set. Lines 6 through 12 in the algorithm collect the addresses on \nthe stack that are used to store register values. For each of those ad\u00addresses, a simple memory liveness \nanalysis is being conducted us\u00ading standard memory-to-register promotion and dead code elimi\u00adnation compiler \npasses (both these passes are already available in LLVM). Lines 13 through 16 create a dummy memory location \nin the IR for each pair of address and register identi.ed. We initially store a dummy value we create \nto each one of those memory loca\u00adtions. Lines 17 through 28 examine the uses of every address using VSA. \nAt every possible read of an address, we insert a load from the dummy memory location we create. At every \npossible write to that address, we insert a store to that dummy memory location of the stored value. \nAfter that, we run the memory-to-register pro\u00admotion compiler pass again on those memory locations. Finally, \nlines 31 through 38 determine the .nal set of dead stores. If the dummy memory location is promoted successfully \nto registers, and the only use of the dummy value is at the return then it is saved and can safely be \nremoved from the potential return. The correspond\u00ading initial register stores are declared to be dead \nin this case. If the same previous conditions occur and also there are other uses of the dummy value, \nthen the register is removed from the potential re\u00adturns, but the initial store is not dead and is considered \na real use of the register; i.e. the register becomes an argument.  The UnsafeInstruction(address) functions \nappearing in line 18 in the algorithm is responsible of deciding whether the instruction may have side \neffects which can potentially access that address. External calls where any stack address appears in \nthe value sets of one of the arguments are considered unsafe as they may do arithmetic on those addresses \nand potentially read from or write to our address. An example of this behavior is memcpy, strcpy and \nother string manipulating functions from the standard C library. Some external functions are pre-identi.ed \nsafe and known not to do arithmetic on pointer arguments. For example, we parse format strings of printf, \nscanf and similar functions and in some cases we can prove those functions are safe. After detecting \nthe dead stores used to save registers and prun\u00ading the callee-saves from the potential returns, we proceed \nto step four which identi.es the actual register arguments. We traverse the call graph of the executable \nin post-order depth-.rst search traver\u00adsal, which ensures child nodes are visited before their parents. \nFor each potential register argument inside a function, we declare it as an argument if and only if we \nsee a real use of this register in the function. If a register is used in a store instruction among the \ndead stores identi.ed by algorithm 1, the store is not considered a real use. Uses in calls are only \nconsidered real if the callee takes the register as an actual identi.ed argument. A work list mechanism \nis maintained to handle the dependencies between functions. PHI nodes that link multiple SSA versions \nof the same register are not considered uses and are tracked. Returns are not considered real uses because \nif the return is the only use of a register, there is no need to pass it as an argument. Propagating \nthe actual return reg\u00adisters (step 5 in our algorithm) is done in a similar way to the one above except \nthat it works on functions in the forward call graph order and looks for uses of return values at call \nsites. The correctness of our register arguments and returns algorithm is guaranteed for internal functions. \nThe reason is that we start our algorithm initially by having all registers as arguments, and then remove \nthose which are not really used. For returns, we start the algorithm by adding all registers that are \nwritten to inside of a function or one of its callees, we then remove the ones which are unused at call \nsites. The correctness in the presence of indirect calls, external calls and call backs is described \nbelow. Our algorithm runs the same way on indirect calls and is correct. At every indirect call, SecondWrite \ninserts a call translator function that checks the value of the function pointer and calls the corre\u00adsponding \nIR function accordingly. In this case, this call translator is treated the same way as any normal function \nin this algorithm under the assumption that the call translator will call all possible target functions. \nRegarding external calls, they are treated correctly by our algo\u00adrithm in all compiler generated code \nwhere the external function has a standard compiler calling convention; ex; cdecl, fastcall, this\u00adcall, \nstdcall and others. Some external functions like standard C and C++ libraries are known to SecondWrite; \nhence our algorithm will know from the prototypes what registers are needed passing. For the unknown \nprototypes, we pass all registers that form the union of all the possible known calling conventions, \nand return all possible returns from the same union. This is not ef.cient, but will produce correct code \nunder the above assumption. We insert assembly in\u00adstructions before the external call to make sure we \npass the correct register values from the IR to the corresponding physical register in hardware, and \ncopy the physical returns into the correct IR registers after the call. Only if the external call has \na non-standard compiler calling convention is when we might not be able to handle it cor\u00adrectly. We never \nexperienced any such external call in all our tested programs.  5. Variable and Type Recovery In this \nsection, we present our techniques to recover source-level variable information from executables, and \nthen present them with meaningful data types in the IR. Our techniques focus only on memory allocated \nvariables. Register-allocated variables can be handled after detecting register arguments and returns \nusing any compiler liveness analysis that detects a variable for every live range of a register in the \nexecutable. Variable and type recovery from executables is a hard problem because symbol tables are absent. \nEvery memory-allocated vari\u00adable access in the source code is represented by a memory store or load in \nthe executable. Those memory accesses are either di\u00adrect accesses to locations represented by constant \naddresses, or in\u00addirect memory accesses to locations represented by some register value. Direct memory \naccesses can be used to infer variable infor\u00admation by examining the constant memory address being accessed, \nbut indirect memory accesses are unknown accesses and need more advanced memory analysis to reveal the \nunderlying memory loca\u00adtions. That is why pointer analysis is important while recovering variables and \ndata types from executables since it reveals what are the possible memory locations an indirect memory \nreference can possibly access. Researchers in this .eld know this and the best known variable identi.cation \ntechnique from executables (DIVINE [5]) uses an advanced memory analysis technique called value set analysis \n[4], which is a generalized form of alias analysis. DIVINE presents accurate variable identi.cation that \ndetects 88% of the memory\u00adallocated variables in executables. The problem with DIVINE is that it is not \nscalable and requires a very long time to analyze even small programs. Our aim is to present techniques \nwith the same accuracy as DIVINE, but run orders of magnitudes faster. Our key insight that enables scalability \nis that ef.cient variable detection and type recovery do not require a sound pointer analysis. Unsound \npointer analysis usually means incomplete points-to sets. As an example, if variable x points to y and \nz, an unsound pointer analysis might report x points to y only. Usually unsound pointer analysis is unacceptable, \nbut variable detection from executables is a best-effort analysis and no method claims to detect 100% \nof the variables. If we are going to miss some variables anyways because of the nature of the problem \nwe are solving, then we can sacri.ce the soundness of the analysis at the expense of losing some variable \ninformation as losing variable z in the given example above, but with the gains of having a practical \nanalysis that scales well for large executables. The correctness of the recovered IR, while missing some \nvari\u00adables due to the unsound pointer analysis, comes from the fact that the relative ordering between \nvariables in the memory layout is  Variables: UpdateALocs (PtSet(x), S) S)  . z . ALocs(PtSet(x)) \n: y =load x (load loca- PtSet(y) . =PtSet(z) tion x of size S to y) Variables: UpdateScalar (PtSet(x), \nS) y = x PtSet(y) = PtSet(x) if z is a constant then PtSet(y) = PtSet(x) >> z Variables: y = x + z ,PtSet(x)is \nif z is a constant then not empty UpdateStructure (PtSet(x), z) else if z has SCEV bounds and stride \nthen UpdateArray (PtSet(y), stride, bounds) Table 1. Points-to sets propagation and variable detection \nrules maintained in the recovered IR. For example, if we detect two in\u00adteger local variables at offsets \n0 and 20 on a stack frame of size 24 bytes, we will lay out those variables in a structure which has \nthe following three members: a) An integer in the range [0-3]. b) A generic array of bytes in the range \n[4-19]. c) An integer in the range [20-23]. Preserving the layout of the variables in such a structure \nmaintains the correctness of any indirect memory access to this re\u00adgion. The arrays inserted .ll the \nunknown gaps between variables and maintain the memory layout. This representation helps under\u00adstanding \nwhat variables are detected along with their types, and at the same time maintains the functionality \nof the rewritten program. We introduce the concept of a best-effort pointer analysis; where the identi.ed \npoints-to set of each pointer may not be complete, but we terminate the analysis in a certain amount \nof time nevertheless to prevent it from taking too long even before it converges. This analysis is not \ncorrect given the usual criteria for correctness, but suf.ces in the way we use it to identify as many \ndiscrete variables as possible. Our best-effort pointer analysis is a .ow and context insensitive data \n.ow analysis that has the following properties: * It limits the cardinality of the points-to sets to \na .xed number. * It does not track interprocedural information via indirect calls. * The number of \nanalysis iterations is set to a .xed number. Having the above relaxations makes our analysis much faster \nat an extremely small loss in precision. The intuition behind this is as follows: a) A .ow and context \nsensitive pointer analysis is not needed since the variables usually have the same size and type in all \n.ows and contexts of a program. Some exceptions to this might happen which is not common in the programs. \nb) Limiting the car\u00addinality of points-to sets does not affect the precision that much since only few \nvariables will have large points-to sets. c) Propa\u00adgating interprocedural information through indirect \ncalls will only affect functions which are only called indirectly. Those functions are still analyzed, \nbut their arguments will have unknown points-to sets. Given that there are relatively few such functions \nin executa\u00adbles, skipping their arguments propagation is not a big loss. d) Lim\u00aditing the total number \nof iterations will only affect longer chains of pointers. For example, the .rst iteration will always \nreveal some pointers. The second will reveal two-level (double) pointers. Subse\u00adquent iterations reveal \nmore pointer levels. Usually most variables  do not have more than four level pointers, which means \nsubsequent iterations will only reveal very little information.  5.1 Best Effort Static Variable Recovery \nWe show in this section how a simple best-effort pointer analysis can be used for identifying variables. \nThis pointer analysis should be suitable to run on executables where no variables yet identi.ed. We could \nhave modi.ed current memory analysis schemes on ex- A = call foo (arg1, ..., argn) foo has the known \nprototype: retT ype f oo (type1, ..., typen) . x . [1,n] setType(argx, typex) setType(A, retT ype) A \n= B op C op .{+, -, *,/, %, >>, <<}op has type: opT ype A, B, C has empty points-to sets setType({A, \nB, C }, opT ype) A =load B store A, B unifyType(A, ALocs(PtSet(B))) op1 = f (op2, ..., opn) op1 = typecast \nop2 to type unifyType({op1, ..., opn}) Table 2. Typing rules ecutables like [4] to .t our needs, but \nwe show a simpler analysis with similar precision and much better scalability. Before we begin the analysis, \nwe identify all base memory re\u00adgions in the executable. An executable has three base memory re\u00adgions. \n1) The global memory region where global variables are located. 2) The stack memory region where local \nvariables inside functions are located. Stack regions are allocated at the beginning of a function and \ndeallocated at the end of the function. Second-Write already represents those as large arrays in every \nfunction. 3) The heap memory region where dynamically allocated variables are usually located. Those \nare identi.ed by detecting calls to functions like malloc and new in the executable. Every detected memory-allocated \nvariable is represented by an abstraction called ALoc which stands for Abstract Location. The name is \nsimilar to the name used by DIVINE [5]. An ALoc contains an offset inside a base memory region and a \nsize representing the variable size. Variables allocated to registers are represented by IR symbols which \nrepresent the SSA form of those registers. Our pointer analysis conservatively assumes that every detected \nvariable can be a pointer. We assign points-to sets to every IR symbol and detected ALoc. When the analysis \nis done, the actual pointers are identi.ed by tracking if the corresponding points-to sets are not empty. \nWe implement the points-to sets using the ef.cient LLVM sparse bit vector data structure. For every base \nmemory region, we assign it a series of unique bits where the number of bits equals the size of the region \nin bytes. If the size of the base memory re\u00adgion is not known (usually in heap allocated arrays), we \nassume an arbitrary size. This allows us to detect variables with offsets up to that size. Whenever an \naccess is detected beyond that arbitrary size, we do not track it. This is an important part of our best-effort \nanalysis that allows us to recover a subset of the variables on un\u00adsized base memory regions instead \nof totally giving up on them as the case in DIVINE [5]. Whenever a symbol or an ALoc points to some variable \nin a certain memory region, the bit corresponding to the starting address of the variable will be set \nto one. The number of bits set to one equals the number of variables pointed to by a symbol or an ALoc. \nTable 1 shows our detailed propagation rules for the best-effort pointer analysis as well as for detecting \nthe variables. We introduce the following de.nitions to ease the understanding. 1) PtSet(x): takes an \nALoc or an IR symbol x and retrieves its points-to set bit-vector . 2) ALocs(x): takes a bit-vector x \nand retrieves the set of ALocs starting at the addresses that correspond to the set-bits in the bit vector \nx. 3) UpdateALocs(x,y): takes a bit-vector x and asize y and creates ALocs starting at the addresses \ncorresponding to the set-bits in the bit-vector x with the given size y. If existing ALocs overlap the \nnew ALocs, the new and old ALocs will be split into smaller ALocs to avoid the overlap. 4) UpdateStructure(x,y): \ntakes a bit-vector x and a number y.Itde.nes a setof structures starting at the addresses corresponding \nto the set-bits in the bit\u00advector x. Each structure has its last member at offset y.Ifa structure already \nstarts at one of the starting addresses, its last member offset will be updated with the maximum of the \nexisting offset and the new one (y). 5) UpdateArray(x,y,z): takes a bit-vector x, a number y representing \na stride, and another number z representing the upper bound of the array. It de.nes arrays starting at \nthe addresses corresponding to the set-bits in the bit-vector x. Each array has a maximum size z. The \narrays will be declared to have an element size y. Existing arrays will be merged with the new declared \nones and the element size will be set to one if overlapping arrays have con.icting element sizes.  Here \nwe describe brie.y the propagation rules in table 1. For a store instruction, the points-to sets of the \nALocs pointed to by the pointer operand will be unioned with the points-to set of the value stored. This \nis called a weak update in the domain of pointer analysis. A load will set the loaded value points-to \nset to whatever is pointed to by the pointer operand. Stores and loads will create ALocs as they are \nresolved using the UpdateALocs function de\u00adscribed earlier. For pointer arithmetic, the points-to sets \nwill be shifted right according to the positive constant added. If the con\u00adstant is negative, the shift \nwill become to the left. Adding a con\u00adstant to a pointer is a hint about the existence of a structure \nwhere the pointer address is the start address, and the constant represents one .eld offset inside the \nstructure. We use this hint and declare a structure identi.ed by the starting address and the last member \noffset. The structure s last member offset might be updated in sub\u00adsequent pointer arithmetic operations \nthat start from the same base. The structure s last member offset will eventually be the maximum observed \nconstant that was added to the pointer in the program. Adding a non-constant value is an indication that \nan array exists. An array will be declared in this case. We use the Scalar EVolution (SCEV) analysis \nby LLVM to deduce the bounds and the stride of the arithmetic and use this information to describe the \narray. If such information is not present, we do not declare an array. The more pointer analysis rounds \ndone, the more ALocs, struc\u00adtures and arrays are identi.ed in all base memory regions. More pointer analysis \nrounds help identifying multi-level pointers since the .rst round will always reveal single level pointers. \nThe second round will propagate the points-to sets for those ALocs and identify their points-to sets \nleading to the identi.cation of two level point\u00aders. More rounds will reveal more levels. After all iterations \nare done, collected information about arrays gets resolved. For every base memory region, we .ll in the \ngaps be\u00adtween ALocs using arrays. The bounds and stride information are available from our earlier propagation. \nIf no bounds are available, previously de.ned ALocs are used as bounds. If no stride informa\u00adtion is \navailable, a stride of one is used which means the array is an array of bytes. Overlapping arrays are \ncombined into one bigger array as described earlier. At the end of this process, a structure hierarchy \nis created based on the structure information calculated for every base memory re\u00adgion. Using the starting \nand ending offsets previously calculated for every structure, we construct nested hierarchy structures. \nWe de.ne inner and outer structures such that any outer structure must have its starting address less \nthan any starting address of any nested in\u00adner structure, and its ending address larger than any ending \naddress of any nested inner structure. A straight forward algorithm is em\u00adployed to produce this hierarchy. \n 5.2 Data Type Recovery Data type recovery aims at representing every symbol in the IR with a meaningful \ntype. It declares a map between every symbol in the IR and the corresponding detected data type. It uses \nthis map to rewrite the complete IR such that the instructions use the detected types instead of the \ngeneric types that are used by SecondWrite. Without integrating type recovery with some pointer analysis, \ndetected types will be less accurate because of two reasons: 1) Instructions like memory loads and stores \nwill usually be untyped since there is no memory tracking possible. 2) Multi-level pointer types will \nnot be detected because there is no way to track them without having some sort of pointer analysis. To \nachieve the goal of typing memory accesses and IR symbols; and detecting multi-level pointer types, we \nintegrate our best-effort pointer analysis and variable recovery techniques described above with our \ntype recovery system. Any other pointer analysis like [4] can be theoretically used, but will be orders \nof magnitude slower which makes it less practical in large executables. That is the disadvantage of TIE \n[14] which is the state of the art binary type recovery technique. Integrating our variable identi.cation \nsystem with type recovery makes the type recovery simpler because it will need only recover scalar types \nlike integers, .oats and doubles. Structures and arrays are detected as part of the variable identi.cation. \nA pointer is de\u00adtected if the points-to set of the corresponding ALoc or IR symbol is not empty. In this \ncase, we get the ALocs pointed to by that pointer and type them according to our rules. We keep doing \nthis for longer pointer chains as needed. Table 5.1 shows the most important typing rules we have. There \nare two main type sources. a) Known external function calls like standard C/C++ library calls. For those, \nwe set the types of actual arguments passed to be the same as the known argument types from the prototypes \nand we do the same thing for the return value. b) Arithmetic operations with non-pointers: in this case \nthe type is deduced from the semantics of the operation itself whether it is an integer or a .oating \npoint operation . We use the function setType to update the type of the symbol or the ALoc in the type \nmap we declare. For pointer types, we type the ALocs represented by the points-to sets of the corresponding \nvariables. For the other operations in the table, we propagate the types using the function unifyType. \nThis function attempts to set the data type of all the given symbols and ALocs to be the same. At least \none of the symbols or the ALocs given to that function should be typed. Whenever this function .nds con.icting \ntypes, it gives up and does not update any types. It is used for copy operations like type casts and \nphi nodes. It is also used to propagate types through memory as shown in the rules for stores and loads. \nInterprocedural information is propagated by unifying the formal and actual arguments types at a call \ninstruction. The return value data type at the call site is uni.ed with all the data types of all return \nvalues appearing in the return statements inside the called function body. IR Correctness. We are able \nto produce a correct and functional IR even if we do not detect some variables and data types. To be \nable to do that, we rewrite the IR using the following restrictions: 1. We use generic types for the \nsymbols we could not detect types for. The generic types will be wide enough to handle the largest possible \nvariable size that can be allocated to a physical register in the hardware. Type casts are used as needed \nto convert the generic type to actual types used in different operations. 2. We never assign a type \nto an IR symbol that con.icts with its use. For example, if we see a 4 byte load, we will never type \nthe pointer as a pointer to short (2 bytes) even if our analysis detects it this way. Otherwise, the \nload will be wrong. 3. All variables identi.ed for a certain memory allocation will be surrounded by \na structure data type. The order of the variables inside that structure is the same as the order they \nappear in the original executable. The memory regions with no variables declared will be declared as \narrays of bytes and will be placed at   #Inst mcf C 3,357 36 0.15 lbm C 7,740 30 0.11 astar C++ 12,677 \n111 0.39 libquantum C 13,800 73 0.41 bwaves F 19,002 22 0.87 bzip2 C 21,408 51 1.14 sjeng C 32,238 121 \n2.86 milc C 34,183 172 2.38 sphinx C 41,669 210 6.68 leslie3d F 43,432 32 2.78 hmmer C 85,981 242 5.29 \nnamd C++ 103,365 193 11.71 soplex C++ 116,743 1523 20.09 zeusmp F 118,429 68 5.44 omnetpp C++ 148,453 \n3980 59.58 h264 C 170,684 462 19.78 gobmk C 196,230 4188 35.34 cactus C 218,896 962 25.57 povray C++ \n288,957 3678 72.49 perlbench C 313,036 2183 67.89 gromacs C/F 396,450 674 38.14 calculix C/F 506,725 \n771 54.79 dealII C++ 766,555 15619 815.05 gcc C 934,292 6426 354.68 tonto F 1,303,359 2878 342.99  \n  Figure 3. Benchmarks Table the correct offsets inside those structures. This guarantees that every \nunresolved pointer arithmetic will still point to the correct variable in the rewritten executable. \n 6. Results In this section, we present the results showing the effectiveness of our schemes to identify \nvariables and data types. We .rst show results on the overall variable and data type detection process \nand then we show speci.c in-depth results for .oating point vari\u00adables and function prototypes. We evaluate \nour techniques on the SPEC2006 benchmark suite which represents C, C++ and Fortran executables using \ndifferent optimization levels and compiled us\u00ading two different compilers (GCC 4.3 for Linux, and Visual \nStu\u00addio 2010 for Windows). We use a machine with an Intel Core i7 3.33GHz processor with 24 GB of RAM. \nAll the recovered code in all the experiments was recompiled using LLVM 3.0, linked using GCC (Linux) \nand MinGW (Win\u00addows), and then tested on the ref and test inputs provided by the SPEC2006 test suite. \nAll rewritten executables worked successfully and produced the correct answer as provided in the test \nsuite. In the following sections, we show our detailed analysis results. 6.1 Variable and data types \ndetection In this section, we show the accuracy, scalability and quality of the recovered variables and \ntypes and compare them to the state of the art. We compile C benchmarks from SPEC2006 with all debug\u00adging \ninformation present and only use them for comparison. We currently do not support reading complete debugging \ninformation for C++ and Fortran, yet we collected results on those benchmarks without comparing with \nsource code. The .rst experiment shows the quality of the recovered variables using the same metrics \nDIVINE [5] used for comparison purposes. DIVINE [5] compares recovered variables in the binary to corre\u00ad \nsponding variables in the source code of those binaries to deter\u00admine how well it did. It de.nes four \nvariable categories as a result: 1) a matched variable is a recovered variable whose exact size and position \nmatches the variable from the source code. 2) An over re\u00ad.ned variable is when the source code variable \nis divided into more recovered variables; for example, an integer identi.ed as four char\u00adacters. 3) Under \nre.ned variables which are recovered as part of a larger source code variable ; for example, an un-identi.ed \nstructure member. 4) An unknown variable is a variable which is not one of those mentioned categories. \nAs shown from .gure 4, an average of 86% of the variables are matched to the debugging information. We \nrun this experiment on programs ranging from 2,149 instructions (mcf) to 934,292 instructions (gcc). \nDIVINE [5] reports an average of 88% matched variables on programs ranging between 252 to 5,371 instructions. \nThis shows that our schemes has comparable precision to DIVINE [5] but on much bigger benchmarks. The \nlargest benchmark they report variables results on is deltablue with 5,371 instructions. The scalability \nof the variables and type detection is shown in .gure 7. Our analysis scales linearly with program size \nfor larger binaries. The detailed benchmarks sizes and analysis time are shown in table 3. The analysis \ntakes around 6 minutes to analyze tonto which is a Fortran benchmark whose size is 1.3 million instructions. \nThe average analysis speed is 1.7 seconds per 10000 instructions compared to 10 minutes per 10000 instructions \nin DIVINE. Thus our method is 352X faster than DIVINE on average. As mentioned before, the underlying \nreason for our much-faster analysis is using an underlying best-effort pointer analysis that is not guaranteed \nto have complete points-to sets. We consider that while recovering the IR to maintain correctness as \nwe discussed earlier in section 5. dealII is the only program (out of 25) that did not scale well. dealII \nhas very large number of procedures as shown in table 3. The interprocedural data .ow propagation took \nmost of the time in dealII. Still, it is .nishing in around 13 minutes given that it has 766,555 instructions. \nIn order to evaluate our type analysis techniques, we calculate the same metrics that TIE [14] uses. \nTIE de.nes a type range for every variable recovered from the executable. An ordering between basic types \nis speci.ed by a type lattice shown in their paper. The .rst metric they de.ne is the distance which \nis the difference between the lattice heights of the upper and lower bounding types for each type range. \nThe smaller the distance, the more accurate the identi.ed types are. The maximum distance is 4. They \nalso de.ne their detected type range to be conservative if the actual source code type falls inside the \ndetected range. In order to compare with TIE [14], we de.ne a range of types for every variable we detect \nwhere the lower bound is the single detected type by our analysis and the upper bound is the generic \nreg32_t type they de.ne in their lattice. Based on that range, we calculate our distances and conservativeness \nrates. In addition to the distance and conservativeness, we de.ne our own metric that measures the precision \nof multi-level pointers detection. TIE metrics do not show how multi-level pointers are precisely typed \nsince all pointer types have the same height on their lattice [14]. Our precision metric is de.ned as \nthe ratio between the correctly recovered pointer levels to the source level pointer levels. For example, \nif a variable has a double pointer to integer type (int**) in the source code and we identi.ed it as \na single pointer to an integer (int*), then we identi.ed one level only out of the three levels in source, \nwhich are pointer to pointer to integer. Our precision in this example will be 33%. Figure 5 shows the \nconservativeness as well as the precision of our detected types. The conservativeness rate is 96% on \naverage which is slightly higher than 90% that TIE reports. Our precision metric shows that we detect \n73% of the pointer levels on average. The average distance detected for our type recovery system is 1.7 \nwhich is slightly better than the distance of 2 that TIE [14] reports. Some of the larger binaries have \nlower type precision than other smaller ones. This is expected since larger programs tend to have more \nhigher level pointers than smaller ones and those are usually  Figure 4. Accuracy of variable detection \nhard to detect since they rely on the effectiveness of the underlying pointer analysis. The conservativeness \nand distance measures used by TIE do not capture this fact as it is clear from .gure 5. It is worth \nmentioning that our variable and type recovery are integrated together in our system. The scalability \nshown in .gure 7 as well as the detailed analysis time results shown in table 3 are capturing both the \nvariable recovery and the type analysis.  6.2 Decoding the .oating point stack In this section, we show \nthe effectiveness of our techniques in iden\u00adtifying .oating point stack variables. We show the percentage \nof the symbolic values that were not solved using our linear solver and required the conservative assumption \nof Xi =1. As mentioned in section 3, the main challenge while decoding the .oating point stack is to \nidentify whether an indirect or an external call is mod\u00adifying the .oating point stack height. According \nto our assump\u00adtions, whenever we are not sure about an indirect or an external call site, we decide conservatively \nthat it is modifying the .oating point stack by pushing a single value. We show how often we took that \nconservative decision in different binaries. All register allocated .oating point stack variables were \nrecov\u00adered correctly and all the rewritten benchmarks ran correctly and produced correct answers. The \nconservative decision taken does not affect correctness as we explained in section 3. It only adds extra \nreturn values to some indirect and external calls and this might re\u00ad.ect adding more return values to \ninternal functions as well. The next results section quanti.es this effect. On average, we took the conservative \ndecision 28% of the time for non-optimized executables and 25% of the time for optimized ones. This means \nwe are able to identify the exact .oating point arguments and returns for more than 72% of the indirect \nand exter\u00adnal calls on average. We are not aware of any work that identi.es such information. Optimized \nbinaries often have less variables than non-optimized binaries which translates to less .oating point \nstack usage and less number of times when the conservative decision is taken. The conservative decision \nis usually taken more often in C++ binaries because they have more indirect calls with more straight \nline code and smaller functions than C and Fortran binaries, which translates into smaller number of \nequations. 6.3 Register Arguments and Returns In this section we show the accuracy of the detected register \nargu\u00adments and returns. We run our algorithm only for the C and C++ benchmarks shown in table 3 and present \nthe average number of added register arguments and returns (false positives). We never had any false \nnegatives in any of the binaries we tested. We could not compare Fortran binaries since currently, we \ndo not support reading Fortran prototypes from debugging information. Figure 5. Accuracy of type detection \nAs shown from the .gure 6, the average number of false positive arguments is 0.2 per function. The average \nnumber of false positive returns is 0.44 registers per function. These results include the con\u00adservative \n.oating point returns we declare in our analysis, which explains why the average number of returns is \nhigher. C++ exe\u00adcutables tend to have more indirect calls than C executables which explains why they \nhave more false positives. In contrast to the work in [7], our method has three advantages: (i) it is \nguaranteed to discover all arguments; (ii) it has been demon\u00adstrated on a much larger programs; and (iii) \nit is orders of magni\u00adtude faster. First, their method cannot guarantee full coverage of arguments and \nreturns because of being a dynamic analysis. Any unused argument or return during an execution trace \ncan be missed. Missing arguments or returns is acceptable for human understand\u00ading of binaries, but unacceptable \nfor rewriting binaries. Second, although our method produces slightly more false positives then their \nmethod (0.2 vs. 0.15 false positive arguments per function), it has been evaluated on far more functions \n(48,854 functions for our method, vs. just 13 functions for theirs.) Third, our analysis is much faster: \nfor example, it takes only 30 seconds to analyze a pro\u00adgram like soplex which has 116,743 instructions \ncontaining 1,523 procedures and produces prototypes for all of them. In their case, they need the same \n30 seconds to only extract MD5_Final which is a single function of 67 instructions. This shows that our \nanalysis is two to three orders of magnitude faster than their method, at the expense of a small loss \nin precision.  7. Related Work Throughout the paper, we compared our work with the most recent work \ndone in the areas of variable and type recovery [5, 14] and function prototypes identi.cation [7]. In \nthis section, we discuss other work that is relevant to our techniques. Binary rewriting has been considered \nby a number of re\u00adsearchers. There are two main categories when talking about binary rewriters, dynamic \nbinary rewriters and static binary rewriters. Dy\u00adnamic binary rewriters rewrite the binary during its \nexecution. Ex\u00adamples are PIN [16], BIRD [18] and others. None of the dynamic binary rewriters found produce \nhigh-level compiler IR. Examples of existing static binary rewriters include ATOM [13], PLTO [19] and \nUQBT [8]. None of those binary rewriters employ a compiler level intermediate format, like LLVM IR or \nsimilar; rather they de.ne their own low-level custom intermediate format. They do not detect high level \nfeatures such as .oating point stack variables, register arguments to functions and data types. Boomerang \n[12] is an open source decompiler. It has very lim\u00ad ited capabilities and cannot handle large binaries. \nRegister argu\u00adments has to be speci.ed manually. It does not detect any .oat\u00ading point stack operations. \nZhang et al. present a technique to re\u00ad  Figure 6. Accuracy of register arguments and returns cover \nfunction arguments and returns from executable [22]. Their technique is similar the brute force technique \ndescribed in section 4 which leads to imprecise results. Another technique recovering function prototypes \nis presented in [9]. It de.nes a language that can be used to specify machine independent calling conventions. \nIt depends on ABI standards to recover the calling conventions. REWARDS [15] presents a dynamic type \nrecovery technique; TIE [14] shows better precision than REWARDS. We already com\u00ad pared to TIE [14] in \nour results. A technique to automatically re\u00ad construct data types from binaries is presented in [10]. \nIt is used in a tool that aims to produce C code from binaries; however no ac\u00adtual C code generation \nis demonstrated. One main disadvantage in their work is they do not track memory. As we have shown, track\u00ading \nmemory is very important in identifying accurate types. The analysis they produce is intraprocedural \nwhich limits its accuracy. Their algorithm is used by Torshina et. al. [21] in another attempt to reverse \nengineer data types in a tool named TyDec for program decompilation. An early work on type construction \nfrom binaries is by Mycroft [17]. It tries to construct C code from binaries with correct type information. \nHowever, it does not actually show results producing C code. The algorithm does not track memory locations \nand it is not clear if it can produce valid IR or C output code. We are not aware of any work done to \nrecover .oating point stack variables except Hex-Rays [1]. Hex-Rays produces inline as\u00ad sembly in case \nit cannot resolve the variables which is not accept\u00adable for our goal. As far as we know, their work \nis not published.  8. Conclusion This paper shows how an executable can be represented by a com\u00adpiler \nIR with source code level variables, data types and function prototypes. The analysis we present in this \npaper is scalable to large executables which makes it more practical than current techniques. The obtained \nhigh level IR is guaranteed to work correctly for com\u00adpiled executables. The schemes are shown to work \non executables containing up to million instructions.  References [1] Idapro, Hexrays. http://www.hex-rays.com/idapro/. \n [2] The LLVM Compiler Infrastructure. URL http://www.llvm. org. [3] K. Anand, M. Smithson, K. ElWazeer, \nA. Kotha, J. Gruen, N. Giles, and R. Barua. A compiler-level intermediate representation based binary \nanalysis and rewriting system. In EuroSys, 2013. [4] G. Balakrishnan and T. Reps. Analyzing memory accesses \nin x86 executables. In CC, 2004. [5] G. Balakrishnan and T. Reps. DIVINE: Discovering variables in executables. \nIn VMCAI, 2007. Figure 7. Scalability of variable and type detection [6] R. Barua and M. Smithson. Binary \nrewriting without relocation infor\u00admation, May 24 2010. US Patent App. 12/785,923. [7] J. Caballero, \nN. M. Johnson, S. McCamant, and D. Song. Binary code extraction and interface identi.cation for security \napplications. In NDSS, 2010. [8] C. Cifuentes and M. V. Emmerik. UQBT: Adaptable Binary Transla\u00adtion \nat Low Cost. Computer, 33(3):60 66, Mar. 2000. [9] C. Cifuentes and D. Simon. Procedure abstraction recovery \nfrom binary code. In Software Maintenance and Reengineering, 2000. [10] E. Dolgova et al. Automatic reconstruction \nof data types in the decompilation problem. Programming and Computer Software, 35: 105 119, 2009. [11] \nK. Elwazeer, K. Anand, M. Smithson, A. Kotha, and R. Barua. Recovering function boundaries from executables. \nTechnical report, 2013. URL http://www.ece.umd.edu/ barua/ function-boundaries.pdf. [12] M. Emmerik \nand T. Waddington. Using a decompiler for real-world source recovery. In Working Conference on Reverse \nEngineering, 2004. [13] A. Eustace and A. Srivastava. ATOM: a .exible interface for building high performance \nprogram analysis tools. In Proceedings of the USENIX 1995 Technical Conference Proceedings, TCON 95, \n1995. [14] J. Lee, T. Avgerinos, and D. Brumley. TIE: Principled reverse engi\u00adneering of types in binary \nprograms. In NDSS, 2011. [15] Z. Lin, X. Zhang, and D. Xu. Automatic reverse engineering of data structures \nfrom binary execution. In NDSS, 2010. [16] C.-K. Luk, R. Cohn, R. Muth, H. Patil, A. Klauser, G. Lowney, \nS. Wal\u00adlace, V. J. Reddi, and K. Hazelwood. Pin: building customized pro\u00adgram analysis tools with dynamic \ninstrumentation. SIGPLAN Not., 40:190 200, June 2005. [17] A. Mycroft. Type-based decompilation. In Proceedings \nof the 8th European Symposium on Programming, 1999. [18] S. Nanda, W. Li, L.-C. Lam, and T.-c. Chiueh. \nBIRD: Binary Interpre\u00adtation using Runtime Disassembly. In CGO, 2006. [19] B. Schwarz et al. PLTO: A \nLink-Time Optimizer for the Intel IA-32 Architecture. In In Proc. 2001 Workshop on Binary Translation, \n2001. [20] M. Smithson, K. Anand, A. Kotha, K. Elwazeer, N. Giles, and R. Barua. Binary rewriting without \nrelocation information. Techni\u00adcal report, 2010. URL http://www.ece.umd.edu/ barua/ without-relocation-technical-report10.pdf. \n [21] K. Troshina, Y. Derevenets, and A. Chernov. Reconstruction of com\u00adposite types for decompilation. \nIn Source Code Analysis and Manipu\u00adlation (SCAM), 2010. [22] J. Zhang, R. Zhao, and J. Pang. Parameter \nand return-value analysis of binary executables. In COMPSAC, 2007.  \n\t\t\t", "proc_id": "2491956", "abstract": "<p>We present scalable static analyses to recover variables, data types, and function prototypes from stripped x86 executables (without symbol or debug information) and obtain a functional intermediate representation (IR) for analysis and rewriting purposes. Our techniques on average run 352X faster than current techniques and still have the same precision. This enables analyzing executables as large as millions of instructions in minutes which is not possible using existing techniques. Our techniques can recover variables allocated to the floating point stack unlike current techniques. We have integrated our techniques to obtain a compiler level IR that works correctly if recompiled and produces the same output as the input executable. We demonstrate scalability, precision and correctness of our proposed techniques by evaluating them on the complete SPEC2006 benchmarks suite.</p>", "authors": [{"name": "Khaled ElWazeer", "author_profile_id": "81556718656", "affiliation": "University of Maryland College Park, College Park, MD, USA", "person_id": "P4148934", "email_address": "wazeer@umd.edu", "orcid_id": ""}, {"name": "Kapil Anand", "author_profile_id": "81444603258", "affiliation": "University of Maryland College Park, College Park, MD, USA", "person_id": "P4148935", "email_address": "kapil@umd.edu", "orcid_id": ""}, {"name": "Aparna Kotha", "author_profile_id": "81479663532", "affiliation": "University of Maryland College Park, College Park, MD, USA", "person_id": "P4148936", "email_address": "akotha@umd.edu", "orcid_id": ""}, {"name": "Matthew Smithson", "author_profile_id": "81479644816", "affiliation": "University of Maryland College Park, College Park, MD, USA", "person_id": "P4148937", "email_address": "msmithso@umd.edu", "orcid_id": ""}, {"name": "Rajeev Barua", "author_profile_id": "81100583756", "affiliation": "University of Maryland College Park, College Park, MD, USA", "person_id": "P4148938", "email_address": "barua@umd.edu", "orcid_id": ""}], "doi_number": "10.1145/2491956.2462165", "year": "2013", "article_id": "2462165", "conference": "PLDI", "title": "Scalable variable and data type detection in a binary rewriter", "url": "http://dl.acm.org/citation.cfm?id=2462165"}