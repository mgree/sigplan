{"article_publication_date": "06-16-2013", "fulltext": "\n * Limitations of Partial Compaction: Towards Practical Bounds Nachshon Cohen Erez Petrank Technion \nTechnion nachshonc@gmail.com erez@cs.technion.ac.il Abstract Compaction of a managed heap is considered \na costly operation, and is avoided as much as possible in commercial runtimes. In\u00adstead, partial compaction \nis often used to defragment parts of the heap and avoid space blow up. Previous study of compaction limi\u00adtation \nprovided some initial asymptotic bounds but no implications for practical systems. In this work, we extend \nthe theory to obtain better bounds and make them strong enough to become meaningful for modern systems. \nCategories and Subject Descriptors D.3.3 [Language Constructs and Features]: Dynamic Storage Management; \nD.3.4 [Proces\u00adsors]: Memory management (garbage collection); D.4.2 [Stor\u00adage Management]: Allocation/deallocation \nstrategies; D.1.5 [Pro\u00adgramming Technique]: Object Oriented Programming General Terms Algorithms, Theory, \nLanguages. Keywords Memory management, compaction, fragmentation, theory, lower bounds. 1. Introduction \nThe study of the theoretical foundations for memory management is mostly lacking. Little is known about \nthe limitations of various memory management functionalities, and in particular on the space consumption \nof various memory management methods. Previous work consists of Robson s classical results on fragmentation \nwhen no compaction is employed [14, 15], a result on the hardness of achieving cache consciousness [11], \nand work on the effectiveness of conservative garbage collection and lazy reference counting [5, 6]. \nA recent new work by Bendersky et al. [4] attempted to bound the overhead mitigation that can be achieved \nby partial compaction. Memory managers typically suffer from fragmentation. Alloca\u00adtion and de-allocation \nof objects in the heap create holes between objects that may be too small for future allocation and thus \ncreate a waste of available heap space. Compaction can eliminate this prob\u00adlem, but compaction algorithms \nare notoriously costly and are thus not frequently used [1, 9, 10]. Instead, memory managers today ei\u00adther \nuse compaction seldom, or employ partial compaction, where * This work was supported by the Israeli Science \nFoundation grant No. Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. PLDI 13, June 16 19, 2013, Seattle, Washington, USA. Copyright &#38;#169; 2013 ACM 978-1-4503-2014-6/13/06...$15.00. \nonly a (small) fraction of the heap objects are compacted to make space for further allocation [2, 3, \n7, 8, 12]. The work in [4] studies limitations of partial compaction. But in spite of [4] being novel \nand opening a new direction for bounding the effectiveness of partial compaction, their results are only \nsignif\u00adicant for huge heaps and objects. When used with realistic setting of system parameters today, \ntheir lower bounds become meaning\u00adless. For example, suppose a program uses a live heap space of 256MB \nand allocates objects of size at most 1MB. For such a pro\u00adgram, even if the memory manager is limited \nand can only compact 1% of the allocated objects, the results in [4] would only imply that the heap must \nbe of size at least 256MB, which is obvious and not very useful. In this work we extend the lower bounds \non partial compaction to make them meaningful for practical systems. To this end, we propose a new bad \nprogram, that makes memory managers fail in preserving low space overheads. We then improve the mathemat\u00adical \nanalysis of the interaction between the bad program and the memory manager to obtain much better bounds. \nFor example, us\u00ading the parameters mentioned in the previous paragraph, our lower bound implies that \na heap of size 896MB must be used, i.e., a space overhead of 3.5x. In general, the more objects we let \nthe memory manager move, the lower the space overhead that it may suffer. To put a bound on the amount \nof compaction work that is undertaken by the memory manager, we use the model set in [4] and bound the \nfraction of allocated space that may be compacted by a constant fraction 1/c. This means that at any \npoint in the execution, if a space of s words has been allocated so far, then the amount of total compaction \nthat is allowed up to this point in the execution is s/c words, where c is the compaction bound. The \nresults we obtain involve some non-trivial mathematical ar\u00adguments and the obtained lower bound is presented \nin a complex formula that is not easy to digest. The general lower bound is stated in Theorem 1 (on Page \n3). But in order to grasp the improvement over the previously known results, we have depicted in Figure \n1 (on Page 3) the space overhead factor for the parameters mentioned above (which we consider realistic). \nNamely, for a program that uses live space of M = 256MB and whose largest allocated object is of size \nat most n = 1MB, we drew for different values of c the re\u00adquired heap size as a factor of the 256MB live \nspace. Note that if we were willing to execute a full compaction after each de-allocation, then the overhead \nfactor would have been 1. We could have used a heap size of 256MB and serve all allocation and de-allocation \nrequests. But with limited (partial) compaction, our results show that this is not possible. In Figure \n1 we drew our lower bound as well as the lower bound obtained from [4] for these parameters. In fact, \nthroughout the range of c = 10, . . . , 100, the lower bound from [4] gives nothing but the trivial lower \nbound overhead factor of 1, meaning that 256MB are required to serve the program. In contrast, our new \ntechniques show that the space overhead must be at least 2x, i.e., 512MB when 10% of the allocated space \ncan be compacted. And when the compaction is limited to 1% of the allo\u00adcated space, then an overhead \nof 3.5x is required for guaranteeing memory management services for all programs.  In general, the result \ndescribed in this work is theoretical, and does not provide a new system or algorithm. Instead, it describes \na limitation that memory managers can never achieve. As such, it does serve a practical need, by letting \npractitioners know what they cannot aspire to, and should not spend efforts in trying to achieve. The \nlower bounds we provide are for a worst-case scenario and they do not rule out achieving a better behavior \non a suite of bench\u00admarks. But providing a better guaranteed bound on fragmentation (as required for \ncritical systems such as real-time systems) is not possible. Note that this bound holds for manual memory \nmanagers as well as automatic one, even when applying sophisticated meth\u00adods like copying collection, \nmark-compact, Lang-Dupont, MC2 , etc. [9]. Finally, we also make a slight improvement over the state-of\u00adthe-art \nrelated upper bound. The upper bound is shown by present\u00ading a memory manager that keeps fragmentation \nlow against all possible programs. The new upper bound slightly improves over the result of [4], and \nit does so by providing a better memory man\u00adager and a better analysis for its worst space overhead. \nThis slight improvement is depicted in Figure 3 (on Page 4), where we depict the new upper bound and \ncompare it to the previous upper bound of [4]. The upper bound theorem is stated rigorously as Theorem \n2 (on Page 3). Note that the proposed memory manager is not meant to be a practical ef.cient memory manager \nthat can be used in real systems, but it demonstrates the ability to deal with worst-case frag\u00admentation. \nTo achieve the bounds presented in this paper, we stand on the shoulders of prior work, and in particular \nour techniques build on and extend the techniques proposed in [15] and [4]. Organization. In Section \n2 we provide some preliminaries, ex\u00adplain the execution of a memory manager, and state our results. In \nSection 3 we provide an overview over the lower bound and its proof. In Section 4 we provide the actual \nproof of the the lower bound, and we conclude in Section 5. A full version of this work is available \n[13], and contains the complete proof of the lower bound and the upper bound. 2. Problem Description \nand Statement of Results 2.1 Framework We think of an interaction between a program and a memory manager \nthat serves its allocation and de-allocation requests as a series of sub-interactions of the form: 1. \nDe-Allocation: The program declares objects as free 2. Compaction: The memory manager moves objects \nin the heap. 3. Allocation: The program requests to allocate objects by specify\u00ading their sizes to the \nmemory manager and receiving in response the addresses in which the objects got allocated.  In this \nwork, we question the ability of a memory manager to han\u00addle programs within a given heap size. But if \na program allocates continuously and never de-allocates any memory, then the heap size required is trivially \nunbounded. So to let the question make sense, we assume a bound on the space that the program may use \nsimul\u00adtaneously. This bound is denoted by M. A second important parameter of the execution of a program \n(from a memory management point of view) is the variance of sizes for the objects that it allocates. \nIf all objects are of .xed size, say 1, a heap space of M is always suf.cient. Although holes can be \ncreated by de-allocating objects, these holes can always be .lled by newly allocated objects. If we denote \nthe least size of an object by 1, the parameter n will denote the maximum size of an object. It can be \nthought of as the ratio between the largest and smallest allowable objects. We denote by P(M, n) the \n(in.nite) set of all programs that never allocate more than M words simultaneously, and allocate objects \nof size at most n. We denote by P2(M, n) the set of programs whose allocated objects sizes are always \na power of two. As explained in the introduction, if the heap is compacted after every de-allocation, \nfragmentation never occurs. However, frequent compaction is costly and so memory managers either perform \na full compaction infrequently, or just move a small fraction of the objects occasionally. In this paper \nwe adopt the de.nition of [4], and consider memory managers that limit their compaction efforts with \na predetermined fraction of the allocated space. For a constant c > 1, a memory manager is c-partial \nmemory manager if it com\u00adpacts at most 1 of the total space allocated by the program. We c denote the \nset of c-partial memory manager by A (c). Given a program P and a memory manager A, the execution of \nP where A serve as its memory manager is well de.ned. The total heap size that A uses in this case is \ndenoted by HS(A, P) In order to present a lower bound on the space overhead required by any memory manager, \nit is enough to present one bad program whose allocation and de-allocation demands would make all mem\u00adory \nmanagers need a large heap space. For an upper bound, we need to provide a memory manager that would \nmaintain a limited heap space for all possible programs. Our model is phrased above as one that lets \nthe program know the address of each allocated object. This knowledge helps the pro\u00adgram create the fragmented \nmemory. We remark that it is enough to let the program know the allocator s algorithm and when GC is \ninvoked (namely, when de-allocation actually happens) in order to obtain this information and create \nthe large fragmentation.  2.2 Previous work For programs that allocate only objects with size that is \na power of 2, and M|n, and for all memory managers that do not use compaction, Robson [14, 15] proved \nlower and upper bounds that match. For his lower bound, he presented a bad program Po . P2(M, n) that \nmakes any memory manager (that does not use compaction) need a large heap. Speci.cally, 1 ( ) 1 min HS(A, \nPo) =M\u00b7 log(n) + 1-n+ 1 A.A (8) 2 For an upper bound, Robson presented an allocator Ao that satis.es \nthe allocation requests of any program in P2(M, n) using a heap size of ( ) 1 max HS(Ao, P) =M\u00b7 log(n) \n+ 1 -n+ 1. 2 P.P2(M,n) For programs that may allocate objects of arbitrary size (and not only powers \nof 2), one may round each allocation to the closest higher power of two. This rounding may (at the most) \ndouble the size of each object, which means that if the program is allowed to allocate 2M words simultaneously, \nthen we obtain a doubled upper bound of 2(M(1/2\u00b7 log(n) + 1) -n+ 1). When some compaction is allowed \n(but not an unlimited com\u00adpaction effort), much less is known. For the upper bound, Ben\u00addersky and Petrank \n[4] have shown a simple compacting collector Ac .A (c), that uses a heap space of at most max HS(Ac, \nP) =(c+ 1) \u00b7 M P.P(M,n) words, when run with any program in P(M, n). 1Here, and throughout the paper, \nall logarithms are of base 2.  They have also shown a bad program PW that makes all memory managers \nwith a c-partial compaction bound use a large heap. In particular: minA.A (c) HS(A, PW(c)) = () 1 logn \n5n M\u00b7 min c, - for c =4log n 10 log c+1 M log n 1M\u00b7 - n for c > 4log n 6 loglog n+2 2  2.3 This work \nOur main contribution is a new lower bound on the ability of a memory manager to keep the heap de-fragmented. \nWhile the lower bound of [4] is important for modeling the problem, providing some tools for solving \nit, and an asymptotical lower bound, their bound is meaningful only for huge objects and heaps. In particular, \nit provides a lower bound that is higher than the obvious M only for M > n =16T B. In this work we extend \nthe theory enough to obtain meaningful results for practical values of M and n. Theorem 1. For any c-partial \nmemory manager A, and for any M > n > 1there exists a program PF .P2(M, n) such that for any . =log( \n3 c) :. .N 4 min HS(A, PF) =M\u00b7 h (1) A.A (c) where h is set to: ()() .+2 2. 1 . i 3 2. log(n)-2.-1 2n \n- . + 1- 2 . + - - 2 c i=1 2i-1 4 c .+1 M h= 2. log(n)-2.-1 1+ 2-.( 3 - ) 4 c .+1 We remark that the \ntheorem makes use of an integral parameter .. The theorem holds for any . = log( 3 c) : . . N, but obviously \n4 there is one . that makes h the largest and optimizes the bound. Determining this . mathematically \nis possible (if we do not require integral values) but the formula for that is complicated. In practice, \nthere are very few (integral) . values that are relevant for any given setting of the parameters, and \nso it can be easily computed in practice. Since h is given in a complicated formula, the implications \nof HS(A, PF) =M\u00b7 h are not very intuitive. Therefore, we chose some realistic parameters to check how \nthis bound behaves in practice. We chose M, the size of the allocated live space to be 256MB, and n, \nthe size of the largest allocatable object to be 1MB. With these parameters .xed and with the parameter \n. set to the value that maximizes the bound, we drew a graph of h as a function of the compaction quota \nbound c. This graph appears in Figure 1. The x-axis has c varying between 10 to 100. Setting c = 10 means \nthat we have enough budget to move 10% of the allocated space, whereas setting c = 100 means that we \nhave enough budget to move 1% of the allocated space. For these c s, the y-axis represents the obtained \nlower bound as a multiplier of M. For example, when compaction of 2% of all allocated space is allowed \n(c = 50), any memory manager will need to use a heap size of at least 3.15 \u00b7 M. Even with 10% of the \nallocated space being compacted, a heap size of 2 \u00b7 M = 512MB is unavoidable. For these practical parameters, \nprevious results in [4, 14] do not provide any bound, except for the obvious one, that the heap must \nbe at least of size M. We also depicted the lower bound as a function of a varying maximum object size \nn. We .xed the compaction budget to c = 100, and the total size of live objects to M = 256n. The rational \nfor the last parameter setting is that it is uncommon for a single object to create a signi.cant part \nof the heap (larger than half a percent). Setting M to a larger value does not change the bound. We let \nthe size of the largest object n vary between 1KB and 1GB, and for these n values, the y-axis represents \nthe obtained lower bound as a multiplier of M. The graph is depicted in Figure 2. Figure 1. Lower bound \non the waste factor h for realistic parame\u00adters (M = 256MB and n = 1MB)as a function of c Figure 2. Lower \nbound on the waste factor h as a function of n (c=100, M=256n) We could also depict the lower bound as \na function of M, where n and c remain .xed. However, in a practical setting, the size of the largest \nobject is much smaller than the total live space (i.e., n/M is small). Hence the lower bound as a function \nof M is very close to a constant function and it does not provide an additional interesting information. \nWe also consider the upper bound on the size of heap required. In [4] an upper bound of the form (c+ \n1)M was presented. How\u00adever, this upper bound may become non-interesting when Rob\u00adson s upper bound is \nstronger, meaning that the same heap size may be obtained without moving objects at all. This happens \nwhen c > log n + 1. As partial compactors often use a large c to limit the fraction 1/c of moved objects, \nsuch a scenario seems plausible. We provide some improvement to Robson s algorithm when little compaction \nis allowed and obtain better upper bound as follows. 1 Theorem 2. For any c > 2 log n, there exists a \nc-partial memory manager A .A (c), which satis.es allocation requests of any pro\u00adgram P .P with heap \nsize at most () log n 1 max HS(AC, P) =2M\u00b7 . max ai, + 2nlog n P.P(M,n) i=0 4-2/c Where a0 = 1, and the \nvalues of ai, i = 1, . . . , log(n), satisfy the following recursive formula: ( ) i-1 1 ai = 1- . max \n, 2j-i\u00b7 aj j=0 c As the formulas in this theorem are also not easy to grasp, we also drew a graph comparing \npreviously known bounds with the new result. It can be seen in Figure 3 that for c s between 20 and 100 \nwe get improvement with the largest improvement being 15% at c = 20. We consider this result minor and \nthe lower bound the major result in this paper. The proof of the upper bound appears in the full paper \n[13]. 3. Overview and Intuitions In this section we review the proof of the lower bound. The main tool \nin this proof is the presentation of a bad program that cause large fragmentation. Our bad program will \nalways allocate objects Figure 3. Upper bound on the waste factor for realistic parameters (M = 256MB \nand n = 1MB)as a function of c of size which is an exponent of 2, and to simplify the discussion (in \nthis overview), we assume that the memory manager is restricted to using only aligned allocation. This \nmeans that an object of size 2i is placed in an address that is dividable by 2i . The bad program will \nwork in steps, where in each step of the execution, the bad program will allocate only objects of size \n2i, for some i that it will determine. Consider such a step and consider a memory region that starts \nat an address dividable by 2i and spans 2i words. Denote such a memory region a chunk. If a chunk is \nfully populated by objects, there is no fragmentation in this chunk. On the other extreme, if a chunk \nis empty, a new object of size 2i can be placed in this hole , creating a fully populated chunk. Fragmentation \nactually occurs (w.r.t. a chunk) when a chunk is sparsely populated. In this case, the utilization of \nthis chunk is low, yet it can not be used for placing a new object. In the case that no compaction is \nallowed, a bad program should attempt to leave a small object in each such chunk, by de-allocating as \nmany objects as possible, leaving one object in each chunk. The bad program always tries to deallocate \nas much space as possible, while keeping chunks occupied by objects that hinder their reuse by the allocator. \nRecall that the program is restricted in allocating at most M words at any time. Therefore, the larger \nthe de-allocated space, the larger the space that can be allocated in the next step, and the larger the \nheap that the allocator must use. This is the main design goal of the bad program: never allow chunk \nreuse, and allocate as much as possible at each step (by de-allocating as much as possible in the previous \nstep). In case compaction is allowed, avoiding reuse is more dif.cult. In particular, a sparsely populated \nchunk can be evacuated and reused by the memory manager. If the populated space on a chunk is of size \n2i/c or smaller, then the memory manager can move the allocated objects away, losing a compaction budget \nof at most 2i/c, but then allocating an object of size 2i on the cleared chunk and gaining a compaction \nbudget of 2i/c. So reuse of sparsely allocated chunks becomes bene.cial for the memory manager. In order \nto create fragmentation in the presence of compaction, the bad program attempts to maintain dense-enough \nchunks. If the allocated space for a chunk is, say, 2 \u00b7 2i/c, then either the memory manager does not \nreuse this chunk, or it does reuse the chunk, but then it must pay at least 2 \u00b7 2i/c of its compaction \nbudget. Allocation of a new object re-charge compaction budget by 2i/c, thus the memory manager remains \nwith a minus of 2i/c words. This allows bounding the overall reuse by bounding the overall compaction \nbudget. Finally, let us discuss how we deal with objects that are not aligned, and therefore, reside \non the border of two chunks. If objects are aligned, then an object is allocated exactly on one full \nsingle chunk. In order to allocate an object, the chunk it is put on must be entirely free of objects. \nWhen an object s allocation is not aligned, it may reside on two chunks. We start by looking at smaller \nchunks, whose size is a quarter of the allocated object. This means that a non-aligned allocated object \nmust entirely .ll three chunks (and partially sit on two more chunks). These three chunks must be completely \nfree of allocated objects before the allocation can take place. If one of these three chunks is about \nto be reused, then the memory manager must move away every object that resides (even partially) on the \nreused chunk, and lose some compaction budget. Note that we have to make sure that if two adjacent chunks \nare reused, then we do not double count budget loss due to the move of a non-aligned object that resides \non both. 3.1 Improvements over prior work [4] In this subsection we review the main improvements of \nthis work over [4]. These improvements enabled the achievement of a better lower bound, which is meaningful \nwith realistic parameter settings. We mention the three major improvements in this overview. The .rst \nimprovement follows from noting that in the .rst steps, the size of allocated objects are large compared \nto the chunk sizes (which are small in the .rst steps). Therefor, in these .rst steps, it is useful to \nrun a program that is very similar to Robson s bad program [14]. Robson s program is designed for memory \nmanagers that do not compact objects, but when objects are large compared to the chunks, it is not bene.cial \nto the memory manager to do any compaction, and a reduction theorem is developed to show that this program \n(or actually a similar program) creates fragmentation even where compaction is allowed. Furthermore, \nthese .rst steps nicely integrate with the general algorithm that runs in the rest of the steps. The \nsecond improvement consists of a small twist in the algo\u00adrithm that creates a more regimented behavior \nduring the execution, which then allows analyzing the execution and obtaining the im\u00adproved bound. Recall \nthat the bad program attempts to de-allocate as much space as possible in each step so that it can allocate \nas many objects as possible in the next step. It turns out that this be\u00adhavior can create scenarios that \nare hard to analyze. This happens when we allocate a lot of objects in one step, and then very few in \nthe following steps. The second improvement that helps us obtain higher fragmentation is in bounding \nthe amount of memory that is allocated per step. This, perhaps, does not use all the space that can be \nused in one step, but it guarantees a suf.cient amount of alloca\u00adtion in all steps. Allocating a .xed \namount of memory at each step allows a stronger analysis of the program behavior and results in a better \nbound. Indeed the stronger analysis of the more regimented execution is another improvement of this new \nwork. Finally, the third improvement that we mention concerns non\u00adaligned object allocation. When an \nobject is not aligned, it consists of two parts that lie on two different chunks. The reuse of one of \nthese chunks for allocation requires the moving of this object, but such a move may also allow use of \nthe other neighboring chunk. The analysis of these scenarios is not simple. We gain more control over \nthe analysis by virtually assigning the non-aligned object to one of its underlying chunks. Of-course, \nin order to reuse a chunk, this object must still be moved, but the virtual assignment of an object to \none of its underlying chunk allows making easier algo\u00adrithm decisions and also computing tighter bounds \non the amount of reuse that the memory manager can achieve. 4. Lower bound: creating fragmentation In \nthis section we prove a lower bound on the ability of a memory manager to keep the heap defragmented \nwhen its compaction re\u00adsources are bounded. In particular, we introduce a program PF that forces any \nc-partial memory manager to use a large heap size to satisfy all of PF s allocation requests. Let us \nstart by explaining the ideas behind the construction of PF. The program PF works in two stages. The \nsecond stage is probably the major contributor to the fragmentation achieved, but the .rst stage is also \nnecessary to obtain our results. The .rst stage is an adaptation of Robson s malicious program [14] that \nattempts to fragment the memory as much as possible, when working against Figure 4. association of objects \nand half objects with chunks a memory manager that cannot move objects. We will discuss in Section 4.2 \nhow this algorithm behaves against a memory manager that can move objects and show that it buys some \nfragmentation in this case as well. After running for 2. steps, a second stage starts, which behaves \ndifferently. The second stage works in steps i= 2., . . . , log(n) -2and in each step it only requests \nallocations of objects of size 2i+2 words. At each such step of the execution, we consider a partition \nof the heap space into aligned chunks of size 2i words. This means, for example, that each allocated \nobject either consumes four full consecutive chunks if its allocation is aligned, or it consumes at least \nthree full consecutive chunks. Our goal is to show that the memory manager must use many chunks. If at \nany point in the execution x+ 1 chunks of size 2 j are used, even if only one word of each chunk is used, \nthen the heap must contain at least x chunks. (The last chunk may not be entirely contained in the heap.) \nThis means that the memory manager must be using a heap size of at least x\u00b7 2j words. Since we do not \nassume aligned allocation, objects may spread over more than one chunk. Nevertheless, each chunk that \nhas a word allocated on it (at any point of the execution) must be part of the heap. Given an execution \nof the program PF with some given memory manager, we associate with each chunk a set of objects that \nwere allocated on it at some point in the execution. This enables tighter analysis. An object is associated \nwith one of the chunks it resides on. This means that at least one word of the object resides on the \nassociated chunk at the time the object is allocated. We then aim to show that x+ 1 chunks have objects \nassociated with them, and obtain the bound as above. The association of objects with chunks is chosen \ncarefully to establish the bound. Note that chunk sizes dynamically change as we move from step to step. \nOn a step change, each pair of adjacent chunks become a single joint chunk. So, association of chunks \nwith objects changes between steps. The association is also updated during the execution, as objects \nget allocated and de-allocated. The program actively maintains the set of objects associated with each \nchunk, and also uses this set in the second stage to determine which objects to de-allocate. Association \nof an object with a chunk is only removed when PF de-allocates the object. It is not removed when an \nobject is compacted. The fact that we do not move association during compaction and attempt to claim \nthat an additional chunk must be used for the move follows from a analysis strategy: when an object is \nmoved, usually it is possible to put it in used chunks, that are not fully occupied. Since the analysis \ncan gain nothing from checking the object s new location, the bad program will just de-allocate any object \nthat s being moved immediately, again, not attempting to consider the location to which it was moved \ninto, but instead, de-allocating it and using its space for future allocation. Note that the chunk that \nit did occupy will remain part of the heap forever, so associating it with the old chunk makes sense. \nWe denote by OD(t) the set of objects that PF associates with the chunk D at time t, and we sometimes \nomit the t, when its value is clear from the context. In fact, when an object lies on the border of two \nchunks, we sometimes choose to associate it with both chunks. In this case, we associate exactly half \nof it with each of the chunks (ignoring the actual way the object is split between the chunks). This \neven split in association is used to preserve the property that objects sizes are a power of two. This \nre.nement of association implies that a chunk may be associated with half an object, and a single object \nmay be associated with two chunks. From the memory manager point of view, a chunk that contains a small \nnumber of allocated words is a good candidate for com\u00adpaction and reuse. Compaction allows reuse of a \nchunk s space for more allocations. As the program PF controls which objects get de\u00adallocated, PF will \nattempt to make sure that each chunk has enough associated objects to make it non-bene.cial for the memory \nman\u00adager to clear a chunk. The density of a chunk is the total size of objects that are associated with \nit, divided by the chunk size. We de.ne a density parameter 2-. so that PF attempts to keep the den\u00adsity \nof a chunk at least 2-.. This means that the program PF will never choose to de-allocate an object if \nthe de-allocation makes a chunk too sparse, in the sense that its density goes below 2-.. This density \nwill be chosen to be larger than 1/c to make the compaction of objects from such a chunk not bene.cial. \nLoosely speaking, according to the compaction budget rules, the memory manager gains an extra compaction \nbudget of 1|o| c when it allocates o. However, if it needs to move 2-. \u00b7 |o|words to make space for this \nallocation, then its overall compaction budget 1 decreases. (Recall that 2-. > ). An example of density \nthreshold c and association set is depicted in Figure 4. Let the density threshold 2-. be 1/4, which \nconsists of 2 words per chunk of size 8. Half of O2 is associated with Chunk C7, the other half is associated \nwith Chunk C8, and the object O3 is associated with Chunk C9 only. These objects suf.ce to make the density \nof each chunk at least 1/4. The program can free the object O1 since a density of 1/4 is preserved even \nwithout it. The whole issue of maintaining a high enough density is not relevant for the .rst steps in \nthe computation. In these steps chunks are small enough so that when even one word is allocated on the \nchunk, a density of 2-. is achieved. Therefore, we can simply adopt Robson s bad program [14] with a \ntechnical variation so that it can deal with the memory manager s compaction activity. Robson s program \nworks well to blow the heap up for memory managers that do not compact the heap. In our scenario, where \nthe chunks are small and so the density is always high, compaction is not very useful, and so Robson \ns original program can work for us too. However, while compaction is not bene.cial to the memory manager, \ncompaction may still occur and our bad program must deal with it. We build a program PF that will be \nsimilar to Robson s program in the sense that it will keep a similar heap shape, it will make very similar \ndecisions on which objects to de-allocate and it will allocate the same amount of space in each step. \nWe will then show a reduction saying that if there exists a memory manager M that can maintain low fragmentation \nwhile serving PF, then it is possible to create another memory manager M ' , that does not move objects, \nand maintains low fragmentation against Robson s program. Since no memory manager can keep low fragmentation \nagainst Robson s program, we get the lower bound we need. In order to make PF work similarly to Robson \ns program, we need to handle compaction. When objects are moved, several dif\u00adferences are created that \nmight in.uence the execution. First, space gets occupied where an object is moved, so new objects can \nno longer be allocated there. Second, vacancy is created in the old space from which an object was moved, \nso objects might be al\u00adlocated there. And .nally, the different shape of allocated objects may change \nthe de-allocation decisions of the bad program. To han\u00addle the .rst difference, PF simply de-allocates \neach moved object immediately after it gets moved. This makes sure that new objects can be allocated \nas before. To handle the other two problems we in\u00adtroduce ghost objects, which are not really in the \nheap, but are used by PF to remember where objects existed so that Robson s program behavior can still \nbe imitated. The program PF maintains a list of ghost objects. These are ob\u00adjects that have been relocated \nby the memory manager along with their original location. For all of its de-allocation considerations, \nPF treats ghost objects as if they still reside at their original location. In fact, each ghost object \ncontinue to exist until the de-allocation procedure (of Robson s) determines that it should be de-allocated. \nOf-course, these objects have been de-allocated when they became ghosts, so no actual de-allocation is \nrequired by the memory man\u00adager, but at that point, they are removed from the list of ghost ob\u00adjects \nand are not considered further by the de-allocation procedure. Note that memory space on which ghost \nobjects reside may receive allocations of new objects by the memory manager, who is not aware of the \nghost objects. This is .ne. The de-allocation procedure can view both objects as residing on the same \nlocation while making its decisions. This seeming collision is later resolved in the reduction theorem, \nby noting a property of the de-allocation procedure. The de-allocation procedure only cares about location \nof objects moduli 2i. Therefore, one can think of the ghost objects as existing in a separate (far away) \nspace at the same address moduli 2i. This additional space will not be counted as part of the heap size, \nbut it will allow the program PF and a real memory manager to work consistently together. Details follow. \nDe.nition 4.1. [A ghost object] We call an object that was com\u00adpacted by the memory manager during the \nexecution and imme\u00addiately de-allocated by the program PF a ghost object. In the .rst stage of the algorithm, \nsuch objects are considered by PF as still re\u00adsiding as ghosts in the original location where they were \nallocated. They do not impact the behavior of the memory manager, which can allocate objects on a space \nconsumed by ghosts. When ghost objects are de-allocated by the program, they disappear and are no longer \nconsidered by PF in subsequent steps. At each step i of the .rst stage, i = 0, 1, . . . , ., we start \nby considering a partition D(i) of the heap into all aligned chunks of size 2i. (Aligned here means that \nthey start on an address that is divisible by 2i.) The main decision that is taken at each step is which \nobjects should be de-allocated (by the malicious program). To this end, Robson picks an offset fi and \nexamines the word at offset fi (from the beginning of the chunk) for all chunks. De\u00adallocation then is \nexecuted for all objects that do not intersect the fi word of a chunk. Note that all objects that will \nbe allocated thereafter are all of size at least 2i, and therefore, two adjacent chunks that have their \nfi offset word occupied, will never be able to hold a new object between them. De.nition 4.2. [an f-occupying \nobject with respect to step i] An object is f -occupying with respect to step i if it occupies a word \nat address k \u00b7 2i + f for some k .N. As a new step kicks in, the chunks sizes get doubled to 2i+1 , where \neach chunk contains two adjacent chunks of the previous step i. We would like to pick a new offset fi+1 \nfor the larger chunks of size 2i+1. The new offset will be either the old offset on the left 2i-sized \nsub-chunk or the old offset on the right 2i\u00adsized sub-chunk. Robson chooses the new offset to be the \none that maximizes the wasted space. In a way, Robson attempts to keep the smallest objects that will \nstill occupy words at the fi+1 offset. So if one of these two offsets allows capturing more space with \nsmaller objects, this becomes the new offset fi+1. To formalize this, Robson chooses fi+1 to be either \nfi or fi + 2i, according to which maximizes . 2i+1 - |o|. o is fi+1-occupying It is not necessary to \nunderstand the details of Robson s analy\u00adsis, as we adopt it without repeating it for the .rst stage \nof our pro\u00adgram. However, to be able to link Robson s program to PF s .rst stage, we also work with ghost \nobjects. In the summation above, Robson naturally considers all objects in the heap that have been allocated \nbut not de-allocated yet, i.e., the set of live objects. In our modi.ed algorithm, we also consider ghost \nobjects. Namely, we sum over all live objects and also over all objects that were com\u00adpacted from their \noriginal location and thereafter de-allocated by the program. The ghost objects are considered to reside \nat the loca\u00adtion they were allocated (and are not considered at the location to which they were compacted \ninto, as they were already deleted from the heap.) After running Robson s program, PF runs extra . -1 \nnull steps in which it does not allocate anything. This is done just for making all objects in the heap \nbe of size at most 2-. of a chunk size. As will be shown in the analysis, this helps ensuring that a \nlot of space can be de-allocated by PF even while maintaining a density of 2-. . With much space de-allocated, \nPF gains ammunition for allocations in the steps of the second stage. Recall that PF is limited and cannot \nallocate more than M words simultaneously, so it must de-allocate enough space before it can allocate \nagain. The bad program PF is presented in Algorithm 1 (on Page 7). It starts by running some steps that \nare similar to Robson s algorithm and proceeds with newly designed algorithm to deal with compaction, \nand maintain some density in each chunk. When the memory manager moves objects using its compaction quota, \nthe program will not try to take advantage of the moved objects in their new location. There are not \nenough of those to justify the trouble. Instead, it will simply immediately delete these objects, and \nuse the reclaimed space for future allocation. Recall that we denote the density that the program attempts \nto maintain in each chunk by 2-. . Other inputs to PF include M, n, which is the size of the largest \nallocatable object; and c, the compaction budget factor. 4.1 Analysis of Program PF Let us now analyze \nthe behavior of the program PF when executing against a c-partial memory manager A. We distinguish the \nbehavior of the program in the two stages. Denote the set of objects that PF allocates during the .rst \nstage by S1 and during the second stage by S2. Also, denote the total size of the objects in S1 by s1 \nand the total size of the objects in S2 by s2. Finally, denote the set of objects that the memory manager \nchooses to compact during the .rst stage by Q1 and their total size by q1. Similarly, the corresponding \nset of compacted objects in the second stage is Q2 whose accumulated size is q2. The execution of PF \nproceeds in in steps i = 0, 1, . . . , 2. - 1, 2., . . . , log(n) -2. The steps 0, 1, . . . , 2. -1 de.ne \nthe .rst stage (yet, nothing is done in steps . + 1, . . . , 2. -1). The rest of the steps happen in \nthe second stage. At each .rst stage step, we use a partition of the heap into chunks of size 2i, in \nan aligned manner, i.e., each chunks starts at an address that is divisible by 2i. Denote by D(i) the \nset of all aligned chunks of size 2i . Our analysis is simpli.ed by using a potential function u(t), \nwhich we de.ne next. It will turn out that this function provides a lower bound on the heap usage during \nthe execution, and our goal will be to show that it becomes large by the end of the execution. The function \nu(t) will be written as a sum of chunk functions uD(t), one for each chunk in D(i). The function uD(t) \nwill be zero for all chunks that are not used to allocate objects. On the other hand, uD(t) will always \nbe at most 2i (i.e., the size of the chunk D)for chunks that have been used until time t. During the \nanalysis of the second stage, we will need to give special treatment to some of the chunks. The set of \nspecial chunks  ALGORITHM 1: Program PF Input: M,n,c,. 1-2.\u00b7h Initially: Compute x = .+1 During the \nexecution: If the memory manager compacts an object, ask the memory manager to de-allocate this object \nimmediately (before any other action is taken), but add this object to the set of ghost objects, with \nthe same address it held when it was allocated. 1: // Stage I: 2: f0 := 0 3: Allocate as many objects \nof size 1 as is possible (i.e., M such objects.) 4: for i = 1 to . do 5: Pick fi to be either fi-1 or \nfi-1+ 2i-1, according to which of the two maximizes . 2i - |o| o is live or ghost and o is fi-occupying \n6: Free every live or ghost object that is non fi-occupying l( ) J 7: Allocate M-./2iobjects of size \n2i o is live or ghost |o| 8: end for 9: Associate objects with chunks: consider the chunk partition \nD(2. -1) to chunks of size 22.-1. Each f.-occupying object is associated with the chunk that contains \nits f.-occupying word. 10: // Stage II: 11: for i = 2. to log(n) -2do 12: Consider the chunk partition \nD(i) of chunks of size 2i. Each chunk D is composed of chunks D1, D2 of the previous step, we set the \nassociation: OD = OD1 .OD2 13: For each 2i chunk, Free as many objects from OD as possible such that \n.o.OD |o| = 2i-. . When a half object is freed, associate it with the chunk that contains the other half, \nand re-evaluate that chunk. l \u00b7 2-i-2J 14: Allocate x\u00b7 M objects of size 2i+2 if the total size of allocated \nmemory will not exceed M. Each allocated object o fully cover 3 chunks D1, D2, D3, if it cover four, \npick the .rst three. '' }. Set OD1 := {o ' }, OD2 := 0/, OD3 := {o 15: end for will be denoted by E and \nde.ned later in De.nition 4.12. For the analysis of the .rst stage, one can simply think of E as the \nempty set. Let us now set the terminology and then de.ne the potential function. De.nition 4.3. [The \nchunk function uD(t)] Let A be a c-partial memory manager, and let t be any time during the execution \nof PF against A which happen at step i. Let D be a chunk of size 2i. The function uD(t) is de.ned as \nfollows. 2i D.E (t) uD(t) = min(2i-. \u00b7 .o.OD(t) |o|, 2i) otherwise The above de.nition depends on the \nassociation of objects to the chunk D, as determined by the association function OD(t). This association \nis computed explicitly by the program PF and it dynamically changes during the course of execution. Let \nus now de.ne the potential function u(t). De.nition 4.4. [The potential function u(t)] Let A be a c-partial \nmemory manager, and let t be any time during the execution of PF against A. Let i be the step in which \nt occurs. The function u(t) is de.ned as follows. n u(t) =uD(t)- . . 4 D.D(i) The function u(t) is used \nto prove the lower bound as follows. It will later be shown that uD(t) is non-zero only if there exists \nan object o which intersected with D at some point during execution. We consider the heap to be the smallest \nconsecutive space that the memory manager may use to satisfy all allocation requests. Now, if x + 1 chunks \nof size 2i are used during the execution, then at least x of them (all but the last chunk) must fully \nreside in the heap. Thus, the heap size must be at least x\u00b7 2i. As u(t) sums over all used chunks, and \nas it accumulate at most 2i for each of those, we get that u(t) is a lower bound on the heap size. A \ncaveat to that is that u(t) may accumulate 2i also for the last chunk that is not fully used in the heap. \nIt is for this reason that u(t) is de.ned as the sum of all uD(t) minus a single n/4 which is the largest \n2i possible. With this additional term, u(t) is guaranteed to be a lower bound on the size of the heap \nused. Next, we analyze the increase of u(t) during the execution. When PF allocates an object, either \nthe memory manager places it on completely new chunks, which (as will be shown) increases in the value \nof u(t), or, it places the new object in a chunk already oc\u00adcupied by other objects, that have been compacted \naway. As com\u00adpaction is bounded, the latter will not happen too much, and fur\u00adthermore, it will be shown \nthat such a combination of compaction and allocation does not decrease u(t). It will also be shown that \na step change, which in.uences u(t) do not decrease it. Finally, new objects may be placed on top of \nobjects that have been de-allocated earlier by PF. But the program PF will manage its de-allocations \nto not allow reuse of a chunk unless some objects are compacted away from it. Thus, we get that the function \ngrows suf.ciently to provide a good lower bound on the heap usage. The guaranteed growth of u(t) and \nthe implied lower bound are shown in two lemmas 4.5 and 4.6. The .rst lemma, Lemma 4.5, asserts the increase \nof u(t) during the .rst stage. It also bounds from above the amount of space allocated during the .rst \nstage. This bound will be used to analyze the .rst stage. Lemma 4.5. Let A be a c-partial memory manager, \nand let t f irst be the time that PF .nishes the execution of its .rst stage when executing with A as \nits memory manager. Then . + 2 n u(tf irst ) =M\u00b7 -2. \u00b7 q1 - 2 4 Also, the total size of allocated memory \nduring the execution of the .rst stage s1 is bounded by . 1 i s1 =M. + 1- . 2 i=1 2i -1 The analysis \nof the second stage is summarized in Lemma 4.6. This lemma again asserts that u(t) increases. However, \nthe increase depends on the total space allocated in the second stage and also on the compaction budget \nin the second stage, which depends on the space allocated (in both stages). To show that the increase \nin the potential function u(t) is high, this lemma also bounds from below the amount of allocated space \ns2 in the second stage. This second bound uses an additional parameter h, which depends on ., c, n, and \nM and is set to the following complicated expression in order to achieve the strongest possible bound. \n()() .+2 2. 1 . i 3 2. log(n)-2.-1 2n - . + 1- 2 . + - - 2 c i=1 2i-1 4 c .+1 M h= 2. log(n)-2.-1 1+ \n2-.( 3 - ) 4 c .+1 Intuitively, his the wasted space factor. Namely, If M is a bound on the live space \nallocated simultaneously by PF, then h\u00b7 M is the lower bound we show on the size of the heap that the \nmemory manager must use to satisfy the allocation requests of PF. The analysis will show that either \nthe memory manager uses more than M\u00b7 h space, and we are done with the proof of Theorem 1, or the program \nallocates a lot of space, as in the second part of the lemma, which will then be used to show that the \nheap space used in both stages is larger than M\u00b7 h, satisfying the assertion of Theorem 1.  Lemma 4.6. \nLet A be a c-partial memory manager, and let t f inish be the time that PF .nishes its execution with \nA as its memory manager. Then, u(tf inish) -u(tf irst ) = 3 s2-2. \u00b7 q2 4 Additionally, either the memory \nmanager uses more than M \u00b7 h space, or the amount of allocation s2 in the second stage satis.es ( log(n) \n-2. -1)() s2 =M 1-2-. \u00b7 h-2n . + 1 We now show how to obtain the lower bound stated in Theo\u00adrem 1 using \nLemma 4.5 and 4.6. Using the fact that the memory manager compacts at most 1 of the total allocation, \nwe know that c (q1 + q2) = 1(s1 + s2). Thus, c HS(A, PF) = u(tf inish) = u(tf irst ) + (u(tf inish -u(tf \nirst )) . + 2 3 n = M\u00b7 + s2 -2. \u00b7 (q1 + q2) - 24 4 . + 23 2. n = M\u00b7 + s2 - (s1 + s2) - 2 4 c 4 ( ) . \n+ 2 2. 3 2. n = M\u00b7 - s1 + - s2 - 2 c 4 c 4 Now, if HS(A, PF) =M\u00b7 h, then we are done. Otherwise, Lemma \n  4.6 gives us the lower bound on s2: ( log(n) -2. -1)() s2 =M 1-2-. \u00b7 h-2n . + 1 In addition, Lemma \n4.5 implies . 1 i s1 =M. . + 1- 2i=1 2i -1 and using simple algebra we get that HS(A, PF) =M\u00b7 h which \ncompletes the proof of Theorem 1. 4.2 Analysis of the .rst stage We now focus our attention on the .rst \nstage and prove Lemma 4.5. We consider an execution of the malicious program PF with any memory manager \nAand look at the .rst . steps. In Step i, the size of the chunks is 2i words, and therefore the size \nof any chunk through\u00adout the .rst stage is not larger than 2. words. If any object is asso\u00adciated with \na chunk, and since any object is of size at least one word, then the fraction (or density) of live space \nassociated with that chunk must be at least 2-.. When the density is guaranteed to be that high, and \ncompaction is limited by the 1 < 2-. fraction, c compaction is very limited and not very bene.cial to \nthe memory manager. Therefore, for these initial steps (of the .rst stage) we chose to use a program \nthat is very similar to a program presented by Robson. Robson used his program for the case where no \ncom\u00adpaction is allowed. We will analyze the slightly modi.ed program to show that it is still useful \nwhen limited compaction is used by the memory manager. For completeness, let us recall Robson s program \nin Algorithm 2. In the algorithm we use the term f-occupying objects that was de.ned in De.nition 4.2. \nAlso, an object is live if it is in the heap, i.e., has not been de-allocated. A simple example to the \nbehavior of Algorithm 2 is depicted in Figure 5. In this example, the object O3 will be freed in Line \n5, since it is not fi occupying. ALGORITHM 2: Robson s bad program PR 1: Initially: f0 := 0 2: Allocate \nM objects of size 1. 3: for i= 1to . do 4: Pick fi . { fi-1, fi-1 + 2i-1}that maximizes . 2i - |o|. o \nis live and fi-occupying 5: Free all non-fi-occupying objects 6: Allocate as many objects of size 2i \nas possible (within the M live space bound.) 7: end for Figure 5. Lower bound on the waste factor h for \nrealistic parame\u00adters (M = 256MB and n = 1MB)as a function of c The original program was designed to \nmaximize fragmentation when no compaction is allowed. In our adaptation, appearing as the .rst stage \nof Program PF, we also handle the case that the memory manager employs compaction. When a compaction \noccurs, PF im\u00admediately de-allocates the moved objects. But we would still like to adopt the original \nanalysis of Robson without redoing the entire analysis for the slightly modi.ed version that was used \nin the .rst stage of PF. To this end, we use a mind experiment in which we let Robson s malicious program \nPR run against an imaginary memory manager A ' that does not move objects. Clearly, Robson s analysis \nholds for the execution (PR, A ' ), as it holds for all memory man\u00adagers that do not move objects. From \nthis analysis we will be able to also deduce a lower bound on the heap size that A uses while satisfying \nPF s allocation and de-allocation sequence. To make a connection between the .rst stage of PF and PR \nwe note that their only difference is that PF must deal with compacted objects. (Such objects are de-allocated \nby PF, but still count for the decisions on future de-allocation of all objects.) Otherwise, it behaves \nexactly like the original PR. In the discussion in this subsection we only care about the execution of \nthe .rst stage of PF. In what follows, when we mention PF, we only look at the .rst stage of PF. The \nimaginary memory manager A ' is constructed only for this proof and has no use otherwise. We therefore \ndo not care much about its ef.ciency or generality. A ' will be looking at the run of PF against A in \norder to make its allocation decisions. Actually, it will make sure that the program PR makes the same \nallocation requests as PF makes when running against A. But A ' will satisfy them with no compaction. \nSince PR will make the same allocation sequence, A ' knows exactly which allocations to expect during \nthe execution against PR. The memory manager A ' will require more heap space than the original memory \nmanager, but it will make sure that the number of fi-occupying objects in each step i is similar throughout \nthe exe\u00adcution. This is done by maintaining a one to one mapping between objects in the execution of \n(PF, A) and objects in the execution of (PR, A ' ), such that mapped objects are always of the same size, \nand are either both fi-occupying or are both not fi-occupying. Since the allocation sequence of PR and \nPF is determined by the space consumed by fi-occupying objects, we get that these sequences remains the \nsame for both programs. Interestingly, the set of f.\u00adoccupying objects at the end of executing the .rst \nstage can be used to bound the value of the potential function u(tf irst ) (from below) at the end of \nPF s .rst stage. This will be the .nal connection and the thing that will provide the desired bound. \nIt still remains to show how we handle the case that an object is compacted, and why this does not break \nthe maintained mapping between objects. This is exactly the reason why ghost objects were de.ned and \nused. Objects that have been moved by the memory manager and de-allocated by the program PF are considered \nby PF as remaining in their original location (where they were allocated) as ghosts. This means that \nthe memory manager can allocate space at this original location and it simply ignores these ghost objects. \nBut the malicious program PF does consider their sizes when it needs to decide on which objects to delete, \nand how many objects to allocate. If ghost objects are f-occupying, then they are counted in the summation \nthere. Let us now specify the imaginary memory manager A ' (which depends on the execution (PF, A))such \nthat the execution (PF, A) is made similar to the execution (PR, A ' ). De.nition 4.7. [Memory manager \nA ' (A, PF)] The memory man\u00adager A ' (A, PF) works as follows. The k-th object that P ' allocates is \nplaced in a location in the memory whose address is equal modulo 2. to where A placed the k-th object \nthat PF allocated. There are in.nitely many such locations and A ' chooses one of them that does not \ncontain any other object previously allocated in an arbitrary manner. Indeed the arbitrary location that \nA ' uses to place the objects may seem too much, as it may use a huge heap for that. But all we care \nabout in the end is the accumulated size of objects that are f.-occupying, and this set will be the same \nfor both executions of (PF, A) and (PR, A ' ). Robson s analysis will guarantee that this set will be \nlarge, and we will deduce the bound we need. We now prove that the mapping between objects in the execu\u00adtion \nof (PF, A) and objects in the execution of (PR, A ' ) indeed exists and satis.es some nice properties. \nClaim 4.8. Consider the execution of PF against a memory man\u00adager A, and the execution of PR against \nA ' (A, PF), and suppose that both .nished their Step i. There is one to one mapping between objects \nin A and objects in A ' with the following property. 1. A live or a (non-deleted) ghost object in the \nexecution (PF, A) is mapped to a live object in the execution (PR, A ' ) and vise verse (a live object \nin (PR, A ' ) is mapped to either live or ghost object in (PF, A)). 2. The sizes of two mapped objects \nare equal. 3. The addresses of two mapped objects are equal modulo 2. .  Moreover, the total number \nof objects allocated during Step i is equal in both execution. Proof: The one to one mapping we chose \nmaps the k-th object that PF allocated to the k-th object that PR allocated. By the de.nition of A ' \ntheir address is equal modulo 2. and we are done with the third property. The other two properties are \nmore involved. The full proof appears in [13]. We now quote an implicit lemma from Robson s analysis, \nthat will help us in bounding the value of the potential function at the end of the .rst stage of PF. \nClaim 4.9 ([14], inequality 1). After the execution of step i, there are at least M i+21i++1 1 = M2i+\u00b722 \ni objects that are fi-occupying. By the end of the .rst stage execution, in PF Line 9, an object o is \nassociated with the chunk that contains its f.-occupying word. We now want to use Robson s guarantee \nfor many f.-occupying objects in order to say that there are many associated objects, and prove the .rst \npart of Lemma 4.5. Claim 4.10. Let A be a memory manager, and let t f irst be the time PF .nished the \nexecution of its .rst stage against A. Then n u(tf irst ) =M(./2+ 1) -2. \u00b7 q1 - 4 Proof sketch: By Claim \n4.9 there are lots of fi-occupying objects and by Claim 4.8 this also holds for the execution of PF. \nThe proof follows from these two and the de.nition of uD(t). Next we bound from above the amount of memory \nallocated by Robson s algorithm. It is used to bound the amount of compaction allowed for a c-partial \nmemory manager. Claim 4.11. Let A be a memory manager, and consider the execu\u00adtion of PF s .rst stage \nagainst A. The total size of memory that PF allocated is at most . 1 i s1 =M . + 1- . 2 2i -1 i=1 Proof: \nOmitted for lack of space. The proof of Lemma 4.5 directly follows from Claim 4.10 and claim 4.11.  \n4.3 Analysis of the second stage In this section we prove Lemma 4.6, which asserts a substantial growth \nin the potential function during the second stage. We let tf irst represent the time where the .rst stage \ncompletes, and tf inish represent the time the second stage (and the entire algorithm) com\u00adpletes. Let \nus recall the statement of the lemma. Lemma 4.6. Let A be a c-partial memory manager, and let t f inish \nbe the time that PF .nishes its execution with A as its memory manager. Then, 3 s2 -2. u(tf inish) -u(tf \nirst ) = \u00b7 q2. 4 Additionally, either the memory manager uses more than M \u00b7 h space, or the allocated \nspace s2 in the second stage satis.es ( log(n) -2. -1)() s2 =M 1-2-. \u00b7 h-2n . + 1 To show that this lemma \nholds, we look at changes that might occur during the execution. This includes allocation of new objects \n(initiated by PF), compaction of objects (by the memory manager), de-allocation of objects (by PF), and \na change of steps that changes the chunk sizes, and therefore the summation over the chunks and each \nchunks uD(t) function. We will show that we get substantial growth during allocations, and also that \nall other events do not decrease the potential function. Recall that the potential function is de.ned \nas n u(t) = .D.D(i) uD(t) - and we need to show that the value of the 4 potential function grows by \nat least 3 s2-2. \u00b7 q2 during the second 4 stage of the execution. We start by looking at allocations \nand show that whenever PF allocates an object, either the potential function gets larger, or some compaction \noccur (and the potential function does not decrease). We then use the fact that compaction is limited \nto get the potential function growth we need. Suppose an object o is allocated during Step i of the execution \nof PF with A. The size of o is 4 \u00b7 2i (by the de.nition of PF)and so when the memory manager places o \nin the heap, it consumes at least three full consecutive chunks (of size 2i)  and maybe some additional \nspace from the chunk preceding and the chunk that comes after these three consecutive chunks. Denote \nby D1, D2 and D3 the three chunks that are fully covered by o and are selected at Step 14 of Algorithm \n1. All three must be empty when o is placed. We claim that this transition from empty chunks to full \nchunks makes the potential function grow. We will show that the value of uD1(t) + uD2(t) + uD3(t) grows, \nwhich implies that the value of u(t) grows. Note that uD(t) for any other chunk D (other than D1, D2 \nor D3) is not affected by this allocation, because the set of associated objects as well as membership \nin E (which we de.ne in the next paragraph) only changes for D1, D2, and D3. During allocation step of \nPF (Line 14), when an object o is allocated, PF associates D1 with the .rst half of object o and D3 with \nthe second half of Object o. But since an object is associated with at most two chunks (each half can \nbe associated with a chunk), it follows that D2 is left with no object associated with it, in spite of \nit being completely covered by the allocated object o. The goal of the set E is to deal with these middle \nchunks. This set will contain all such middle chunks and make uD2(t) of De.nition 4.3 be set to 2i. We \nnote that this anomaly , of a chunk being covered by an object but with no associated object, is temporary \nand it disappears at the next step change since the middle chunk is joined with either its left or right \nchunk, and they become a single chunk with which o (or o s half) can be associated. Let us now de.ne \nE . De.nition 4.12. [The set E (t)] Let A be a memory manager, and consider any time t during the execution \nof PF s second stage with Aas its memory manager. Let i be the step where t happens. The set of middle \nchunks E (t) .D(i) is the set of chunks that both their left adjacent chunk and their right adjacent \nchunk were fully covered by an object o allocated at step i (thus, the chunk itself is also covered by \no), but half of o was not associated with it. A chunk remains in E (t) until either an new step kicks \nin, or an object is associated with this chunk. The later may happen if o was compacted during step i, \nand another object is allocated there. Let us now claim a simple property about chunks in E . We d like \nto say if a chunk is in E , then the two chunks adjacent to it are associated with a big , or recently \nallocated object. Claim 4.13. Let t be a time during the execution of PF against a memory manager A, \nand let i be the step in which t occurs. Let D1 and D2 be two consecutive chunks such that D1 .E (t). \nThen there exists an object o that is allocated during Step i, such that at time t half of o is associated \nwith D2. The same holds for D1 when D2 .E (t). Proof sketch: When a chunk joins E both chunks to its \nleft and right are associated with half objects allocated in Step i. The chunks D1, D2 and D3 were empty \nbefore the allocation. For each of these chunks, we distinguish between the case where an object was \nassociated with it before the allocation, or that no object was so associated. In the latter case, the \nvalue of the function uD(t) for that chunk grows since an object gets associated with it. In the .rst \ncase, it must be that the memory manager compacted away all objects that were allocated on the chunk \nand made it fully available for allocation. Note that the de.nition of PF rules out a third possibility \nthat these chunks were emptied due to de\u00adallocation of the objects that previously resided on them. Object \nde-allocation is initiated by the program PF only (and not by the memory manager). By the de.nition of \nPF, it only de-allocates an object when there are enough other objects left on the chunk to make the \nremaining space size at least 2i-. . When the second case occurs, i.e., that the memory manager compacts \naway objects from a chunk before placing o, we are not able to show that the value of the potential function \ngrows. However, we get that some compaction occurred, and we use that to bound the number of such events. \nTo this end, we associate some compaction value with the newly allocated object. Recall that the objects \nthat were compacted away from a chunk and then de\u00adallocated immediately by PF are still considered associated \nwith the chunk until a new object is placed on the chunk. Therefore, to determine how much compaction \noccurred to free space for the allocation, we can just check the objects that were associated with these \nchunks right before the allocation. The formal de.nition follows. De.nition 4.14. [Compaction space associated \nwith an object] Let o be an object allocated during the execution of PF s second stage against a memory \nmanager A. Let t + 1be the allocation time (and t be the time just before the allocation), and let D1, \nD2, and D3 be the chunks picked by PF at Step 14 of PF, after allocating o. Then we de.ne the compacted \nspace associated with the object o to be q(o) = . |o ' |+ . |o ' |+ . |o ' | o ' .OD1(t) o ' .OD2(t) \no ' .OD3(t) Next, we establish some properties of the set of objects associ\u00adated with every chunk. Claim \n4.15. Consider a time t during the execution of PF s second stage against a memory manager A, and let \ni be the step where t happens. Then the following three properties hold: 1. The sets {OD : D.D(i)}are \ndisjoint. 2. Every live object o is either associated with a single chunk or its two halves are associated \nwith two chunks. 3. If a live object o is associated with a chunk D, then o intersects  D. Proof sketch: \nThe proof follows from the de.nition of the program PF and its way of associating objects with chunks. \nThe full proof is omitted for lack of space. We now show that the potential function u(t) indeed increases \nsubstantially. We will show that no event causes a decrease in it, while allocations cause suf.cient \nincrease. We will compute the potential function u(t) increase for each allocation, and then sum over \nall allocations to obtain the total increase in u(t) during PF s second stage. Claim 4.16. Let A be a \nmemory manager, let u(t) be the potential function as de.ned in De.nition 4.4, and q(o) be the compaction \nassociated with an object as de.ned in De.nition 4.14. Then during execution of PF against A, the following \nproperties of u(t) holds 1. No event in the execution causes u(t) to decrease. 2. During an allocation \nof an object o, u(t) increase by at least  3 |o|-2. \u00b7 q(o). 4 Proof. The potential function only changes \nwhen the set of chunks changes during step transition or when the set of associated objects of a chunk \nchanges. There are three types of events that may cause such a change. Note that compaction of an object \nby A is not an event that in.uences the potential function, since the object association remains unchanged. \nA step transition causes PF to consider a new partition of the heap. Consider the time t when we consider \na new partition of the heap (when a new step i kicks in). Let D be a chunk of Step i (of size 2i). D \nis composed of two chunks D1 and D2 of Step i-1 (and of size 2i-1). It suf.ces to show that uD(t) =uD1(t) \n+ uD2(t). If either D1, D2 .E (t -1), then by Claim 4.13 there exists an object o that was allocated \nin Step i-1, and half of o is associated with either D1 or D2. Since half of o is associate with D, and \nthe size of half of o is 2i, it holds that .o ' .OD |o ' |= 2i. Thus (recall de.nition 4.3 of uD(t)) \n uD(t) = min(2. \u00b7 2i , 2i) = 2i =uD1(t -1) + uD2(t -1). Otherwise, both chunks are not in E and we know \nthat the objects that are associated with them are disjoint. Therefore, according to Step 12 in PF, |o|= \n|o|+ |o|, o.OD o.OD1 o.OD2  . . . and the statement follows by de.nition of uD(t). PF de-allocates \nan object. By de.nition, PF does not free an object if this decrease .o.OD |o| below 2i-., so uD(t) \ndoes not decrease. PF allocates an object o. Let t be the time PF allocates an object o, and let D1, \nD2, D3 be the three chunks that PF picked. Only the sets OD1, OD2, OD3 are changed during allocation. \nAfter the allocation, size of each half an object is 2 \u00b7 2i, so uD1(t) = uD3(t) = min(2. \u00b7 2\u00b7 2i , 2i) \n= 2i, and D2 .E (t). Thus 3 uD1(t) + uD2(t) + uD3(t) = 3\u00b7 2i = |o| 4 Before the allocation, if either \nchunks was contained in E , then by Claim 4.13 there exists an object o allocated at step i, and half \nof o is associated with D1, D2, or D3. In this case 2. \u00b7 q(o) = 2. \u00b7 2i+1 =3\u00b7 2i. The last inequality \nfollows since . =1. Otherwise, uDi(t -1) = 2. \u00b7 .o ' | for i = 1, 2, 3. In both case we ' .ODi(t-1) |o \nhave 2. \u00b7 q(o) = uD1(t -1) + uD2(t -1) + uD3(t -1). Subtracting the values before and after the allocation \nof o provides the bound in this case and we are done We now turn to bound the amount of memory that PF \nallocates during the execution of its second stage. By its de.nition, In Step l J 14, PF attempts to \nallocate a x\u00b7 M\u00b7 2-i-2objects of size 2i+2 if the total size of allocated memory does not exceed M. In \norder to show that it allocates a lot, we need to show that the bound of not exceeding M simultaneously \nallocated words does not limit this allocation too much. To this end, we need to bound the space occupied \nby live objects in the heap from above, which implies a lower bound on the amount of memory available \nfor allocation. The next proposition asserts that any chunk D in the heap is either empty, or it contains \na single large object, or the total space of its associated objects OD is bounded exactly by 2i/2., which \nis the density that PF attempts to preserve in each chunk. This lemma will later be used to bound the \ntotal size of live objects in the heap. Proposition 4.17. Consider the execution of PF s second stage \nagainst a memory manager A. Let t be a time when PF allocates an object, and let i be the step in which \nt happens. Then for every chunk D .D(i) either |OD(t)| = 1, or .o.OD(t) |o| = 2i-. . Proof sketch: The \nproof of this proposition follows from the behav\u00adior of PF as de.ned in Line 13. Next, we bound the space \nPF allocates during its second stage. Claim 4.18. Consider the execution of PF against a memory man\u00adager \nA. Then either A uses more than M \u00b7 h heap space, or the num\u00adber of words that that PF allocates during \nits second stage satis.es 1-h\u00b7 2-. s2 =(logn-2. -1) -2n. . + 1 Proof sketch: Recall that x is set at \nthe beginning of Algorithm 1 to: 1-h\u00b72-. .+1 . We show that in each step PF s allocation of M\u00b7 x words \ndoes not exceed the M bound, and then show that the accumulated size of allocation is large enough. Recall \nthat in Claim 4.16 we bounded the increase in u(t) when an object o is allocated by 3|o|-2. \u00b7 q(o). Summing \n3|o|over all 4 4 objects allocated during stage two of PF gives 3 s2, which we also 4 computed. We now \nbound the total sum of q(o) over all objects allocated during stage two of PF. The precise value of q(o) \nwas given in De.nition 4.14. Proposition 4.19. Consider the execution of PF against a memory manager \nA. S2 is the set of objects allocated at PF second stage, and q2 is the total size of objects that were \ncompacted during the second stage. Then q2 =.o.S2 q(o). Proof. Omitted for lack of space. Now we are \nready to bound to total increase in the unavailable space at Pt s second stage Claim 4.20. Consider the \nexecution of PF against a memory man\u00adager A, and let t f inish be the time when PF .nished it execution, \nand tf irst be the time when PF .nished execution of .rst stage. Then 3 u(tf inish) =u(tf irst ) + s2 \n-2. \u00b7 q2 4 Proof: Omitted for lack of space. The proof of Lemma 4.6 directly follows from Claim 4.18 \nand Claim 4.20. 5. Conclusion This work contributes to building a solid theoretical foundation for memory \nmanagement. In particular, it extends previous work by providing new lower and upper bounds on the effectiveness \nof partial compaction. Our lower bound is the .rst bound in the literature that carries implications \nfor practical systems existing today, by showing that some desirable (realistic) compaction goals cannot \nbe achieved. References [1] D. Abuaiadh, Y. Ossia, E. Petrank, and U. Silbershtein. An ef.cient parallel \nheap compaction algorithm. In OOPSLA 2004. [2] D. F. Bacon, P. Cheng, and V. Rajan. A real-time garbage \ncollector with low overhead and consistent utilization. In POPL 2003. [3] O. Ben-Yitzhak, I. Goft, E. \nKolodner, K. Kuiper, and V. Leikehman. An algorithm for parallel incremental compaction. In ISMM 2002. \n[4] A. Bendersky and E. Petrank. Space overhead bounds for dynamic memory management with partial compaction. \nPOPL 2011. [5] H.-J. Boehm. Bounding space usage of conservative garbage collec\u00adtors. In POPL 2002. [6] \nH.-J. Boehm. The space cost of lazy reference counting. POPL 2004. [7] C. Click, G. Tene, and M. Wolf. \nThe Pauseless GC algorithm. VEE 2005. [8] D. Detlefs, C. Flood, S. Heller, and T. Printezis. Garbage-.rst \ngarbage collection. In ISMM 2004. [9] R. Jones, A. Hosking, and E. Moss. The Garbage Collection Hand\u00adbook: \nThe Art of Automatic Memory Management. Chapman &#38; Hall, Aug. 2011. [10] H. Kermany and E. Petrank. \nThe Compressor: Concurrent, incremen\u00adtal and parallel compaction. In PLDI 2006. [11] E. Petrank and D. \nRawitz. The hardness of cache conscious data placement. In POPL 2002. [12] F. Pizlo, E. Petrank, and \nB. Steensgaard. A study of concurrent real\u00adtime garbage collectors. In PLDI 2008. [13] N. Cohen and E. \nPetrank. Limitations of Par\u00adtial Compaction: Towards Practical Bounds. http://www.cs.technion.ac.il/%7eerez/%50apers/compaction-full.pdf. \n[14] J. Robson. Bounds for some functions concerning dynamic storage allocation. Journal of the ACM, \n21(3):491 499, 1974. [15] J. Robson. An estimate of the store size necessary for dynamic storage allocation. \nJournal of the ACM, 18(3):416 423, 1971.  \n\t\t\t", "proc_id": "2491956", "abstract": "<p>Compaction of a managed heap is considered a costly operation, and is avoided as much as possible in commercial runtimes. Instead, partial compaction is often used to defragment parts of the heap and avoid space blow up. Previous study of compaction limitation provided some initial asymptotic bounds but no implications for practical systems. In this work, we extend the theory to obtain better bounds and make them strong enough to become meaningful for modern systems.</p>", "authors": [{"name": "Nachshon Cohen", "author_profile_id": "81488651514", "affiliation": "Technion, Haifa, Israel", "person_id": "P4149036", "email_address": "nachshonc@gmail.com", "orcid_id": ""}, {"name": "Erez Petrank", "author_profile_id": "81100377919", "affiliation": "Technion, Haifa, Israel", "person_id": "P4149037", "email_address": "erez@cs.technion.ac.il", "orcid_id": ""}], "doi_number": "10.1145/2491956.2491973", "year": "2013", "article_id": "2491973", "conference": "PLDI", "title": "Limitations of partial compaction: towards practical bounds", "url": "http://dl.acm.org/citation.cfm?id=2491973"}