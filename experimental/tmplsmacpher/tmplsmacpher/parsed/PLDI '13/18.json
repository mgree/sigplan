{"article_publication_date": "06-16-2013", "fulltext": "\n Compiler Testing via a Theory of Sound Optimisations in the C11/C++11 Memory Model Robin Morisset Pankaj \nPawan Francesco Zappa Nardelli ENS &#38; INRIA IIT Kanpur &#38; INRIA INRIA Abstract Compilers sometimes \ngenerate correct sequential code but break the concurrency memory model of the programming language: \nthese subtle compiler bugs are observable only when the miscom\u00adpiled functions interact with concurrent \ncontexts, making them par\u00adticularly hard to detect. In this work we design a strategy to reduce the hard \nproblem of hunting concurrency compiler bugs to differen\u00adtial testing of sequential code and build a \ntool that puts this strategy to work. Our .rst contribution is a theory of sound optimisations in the \nC11/C++11 memory model, covering most of the optimisations we have observed in real compilers and validating \nthe claim that common compiler optimisations are sound in the C11/C++11 mem\u00adory model. Our second contribution \nis to show how, building on this theory, concurrency compiler bugs can be identi.ed by comparing the \nmemory trace of compiled code against a reference memory trace for the source code. Our tool identi.ed \nseveral mistaken write introductions and other unexpected behaviours in the latest release of the gcc \ncompiler. Categories and Subject Descriptors D3.4 [Programming Lan\u00adguages]: Processors compilers; D2.4 \n[Software Engineering]: Software/Program Veri.cation; F.3.1 [Specifying and Verifying and Reasoning about \nPrograms] Keywords C11/C++11 memory model; compiler testing 1. Random testing for concurrency compiler \nbugs The C and C++ languages were originally designed without con\u00adcurrency support: threads were available \nvia external libraries, yielding unexpected behaviours and misunderstandings between programmers and \ncompiler writers.1 The recent revision of the C and C++ standards [6] does provide a precise semantics \nfor threads (formalised in [4]): well-synchronised programs must exhibit only sequentially consistent \nbehaviours, racy programs can have any behaviour, and an escape mechanism with a complex semantics, called \nlow-level atomics, enables programmers to write high\u00adperformance but portable concurrent code. The resulting \nmodel 1 as an example, this recent discussion illustrates the mismatch and tensions between what Linux \nkernel developers expect from compilers and what gcc does: http://gcc.gnu.org/ml/gcc/2012-02/msg00005.html. \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI \n13, June 16 19, 2013, Seattle, Washington, USA. Copyright &#38;#169; 2013 ACM 978-1-4503-2014-6/13/06...$15.00. \n#include <stdio.h> #include <pthread.h> int g1 = 1; int g2 = 0; void *th_1(void *p) { for (int l = 0; \n(l != 4); l++) { if (g1) return NULL; for (g2 = 0; (g2 >= 26); ++g2) ; } } void *th_2(void *p) { g2 = \n42; printf(\"%d\\n\",g2); } int main() { pthread_t th1, th2; pthread_create(&#38;th1, NULL, th_1, NULL); \npthread_create(&#38;th2, NULL, th_2, NULL); (void)pthread_join (th1, NULL); (void)pthread_join (th2, \nNULL); } Figure 1: foo.c, a concurrent program miscompiled by gcc 4.7.0. is intricate and the interactions \nwith compiler optimisations are not entirely understood. Today s C and C++ compilers, whose op\u00adtimisers \nwere initially developed in absence of any well-de.ned memory model, are being extended to support the \nnew concur\u00adrency standard. This is a hard task. Past research showed that all production-quality C compilers \nused to generate incorrect code for accessing volatile variables [10]: the memory model de.ned by the \nvolatile modi.er is trivial compared to the new C11/C++11 model. Correctly supporting the new memory \nmodel in today s compilers will be an error-prone enterprise, requiring signi.cant development effort. \nIt is thus vital to investigate techniques to build assurance in widely used implementations of C and \nC++. Consider for instance the C program in Figure 1. Can we guess its output? This program spawns two \nthreads executing the func\u00adtions th_1 and th_2 and waits for them to terminate. The two threads share \ntwo global memory locations, g1 and g2, but a care\u00adful reading of the code reveals that the inner loop \nof th_1 is never executed and g2 is only accessed by the second thread, while g1 is only accessed by \nthe .rst thread. According to the C11/C++11 standards this program should always print 42 on the standard \nout\u00adput: the two threads do not access the same variable in con.icting ways, so the program is data-race \nfree, and its semantics is de.ned as the interleavings of the actions of the two threads. If we compile \nthe above code with the version 4.7.0 of gcc on an x86 64 machine running Linux, and we enable some optimisations \nwith the -O2 .ag (as in g++ -std=c++11 -lpthread -O2 -S foo.c) then, sometimes, the compiled code prints \n0 to the standard output. This unexpected outcome is caused by a subtle bug in gcc s imple\u00admentation \nof the loop invariant code motion (LIM) optimisation. If we inspect the generated assembly we discover \nthat gcc saves and restores the content of g2, causing g2 to be overwritten with 0: th_1: movl g1(%rip), \n%edx # load g1 (1) into edx movl g2(%rip), %eax # load g2 (0) into eax testl %edx, %edx # if g1 != 0 \njne .L2 # jump to .L2 movl $0, g2(%rip) ret .L2: movl %eax, g2(%rip) # store eax (0) into g2 xorl %eax, \n%eax # return 0 (NULL) ret This optimisation is sound in a sequential world because the extra store always \nrewrites the initial value of g2 and the .nal state is unchanged. However, as we have seen, this optimisation \nis unsound in the concurrent context provided by th_2, and the C11/C++11 standards forbid it. How to \nsearch for concurrency compiler bugs? We have a com\u00adpiler bug whenever the code generated by a compiler \nexhibits a behaviour not allowed by the semantics of the source program. Differential random testing \nhas proved successful at hunting com\u00adpiler bugs. The idea is simple: a test harness generates random, \nwell-de.ned, source programs, compiles them using several com\u00adpilers, runs the executables, and compares \nthe outputs. The state of the art is represented by the Csmith tool by Yang, Chen, Eide and Regehr [24], \nwhich over the last four years has discovered several hundred bugs in widely used compilers, including \ngcc and clang. However this work cannot .nd concurrency compiler bugs like the one we described above: \ndespite being miscompiled, the code of th_1 still has correct behaviour in a sequential setting. A naive \napproach to extend differential random testing to con\u00adcurrency bugs would be to generate concurrent random \nprograms, compile them with different compilers, record all the possible out\u00adcomes of each program, and \ncompare these sets. This works well in some settings, such as the generation and comparison of litmus \ntests to test hardware memory models; see for instance the work by Alglave et al. [3]. However this approach \nis unlikely to scale to the complexity of hunting C11/C++11 compiler bugs. Concurrent pro\u00adgrams are inherently \nnon-deterministic and optimisers can compile away non-determism. In an extreme case, two executables \nmight have disjoint sets of observable behaviours, and yet both be correct with respect to a source C11 \nor C++11 program. To establish cor\u00adrectness, comparing the .nal checksum of the different binaries is \nnot enough: we must ensure that all the behaviours of a compiled executable are allowed by the semantics \nof the source program. The Csmith experience suggests that large program sizes (~80KB) are needed to \nmaximise the chance of hitting corner cases of the optimisers; at the same time they must exhibit subtle \ninteraction patterns (often unexpected, as in the example above) while being well-de.ned (in particular \ndata-race free). Capturing the set of all the behaviours of such large concurrent programs is tricky \nas it can depend on rare interactions between threads; computing all the be\u00adhaviours allowed by the C11/C++11 \nsemantics is even harder. Despite this, we show that differential random testing can be used successfully \nfor hunting concurrency compiler bugs. First, C and C++ compilers must support separate compilation and \nthe con\u00adcurrency model allows any function to be spawned as an indepen\u00addent thread. As a consequence \ncompilers must always assume that the sequential code they are optimising can be run in an arbitrary \nconcurrent context, subject only to the constraint that the whole program is well-de.ned (race-free on \nnon-atomic accesses, etc.), and can only apply optimisations that are sound with respect to the concurrency \nmodel. Second, it is possible to characterise which optimisations are correct in a concurrent setting \nby observing how they eliminate, reorder, or introduce, memory accesses in the traces of the sequential \ncode with respect to a reference trace. Combined, these two remarks imply that testing the correctness \nof compilation of concurrent code can be reduced to validating the traces generated by running optimised \nsequential code against a reference (unopti\u00admised) trace for the same code. We illustrate this idea with \nprogram foo.c from Figure 1. Traces only report accesses to global (potentially shared) memory locations: \noptimisations affecting only the thread-local state cannot induce concurrency compiler bugs. On the left \nbelow, the reference trace for th_1 initialises g1 and g2 and loads the value 1 from g1: Init g1 1 Init \ng1 1 Init g2 0 Init g2 0 Load g1 1 Load g1 1 Load g2 0 Store g2 0 On the right above, the trace of the \ngcc -O2 generated code per\u00adforms an extra load and store to g2 and, since arbitrary store intro\u00adduction \nis provably incorrect in the C11/C++11 concurrency model we can detect that a miscompilation happened. \nFigure 2 shows an\u00adother example, of a randomly generated C function together with its reference trace \nand an optimised trace. In this case it is possible to match the reference trace (on the left) against \nthe optimised trace (on the right) by a series of sound eliminations and reordering of actions. Contributions \nThis approach to compiler testing crucially relies on a theory of sound optimisations over executions \nof C11/C++11 programs. Building on the work by Sev.c.\u00b4ik [17, 19] for an idealised DRF model, our .rst \ncontribution is the study and correctness proof of several criteria for sound optimisations in the C11/C++11 \nmodel, covering all the optimisations we observed in testing real compilers (with the exception of irrelevant \nread introductions and merging of accesses), presented in Section 3. The second contri\u00adbution of this \nwork is a strategy to perform differential testing of compilers against a memory model, reducing the \nhard problem of hunting concurrency compiler bugs to matching memory traces re\u00adalised by sequential code. \nThe cmmtest tool, which we designed and developed, puts the testing strategy to work building on our \nthe\u00adory of C11/C++11 optimisations. These are presented in Section 4. Our third contribution, in Section \n5, is a report on some concur\u00ad rency compiler bugs and other unexpected behaviours caught by preliminary \ntesting of gcc. The gcc developers promptly .xed all the reported bugs and are integrating our soundness \ncriteria with the gcc optimiser. We begin in Section 2 by introducing our theory of semantic optimisations \nand recalling the C11/C++11 memory model and conclude with a discussion of related work. Complete proofs \nand an evaluation release of cmmtest are on the web [1]. 2. Program Transformations and the C11/C++11 \nMemory Model Compiler optimisations are usually described as program transfor\u00admations over an intermediate \nrepresentation of the source code; a typical compiler performs literally hundreds of optimisation passes. \nAlthough it is possible to prove the correctness of individual trans\u00adformations, this presentation does \nnot lend itself to a thorough char\u00adacterisation of what program transformations are valid. const unsigned \nint g3 = 0UL; long long g4 = 0x1; int g6 = 6L; unsigned int g5 = 1UL; void f(){ int *l8 = &#38;g6; \nint l36 = 0x5E9D070FL; unsigned int l107 = 0xAA37C3ACL; g4 &#38;= g3; g5++; int *l102 = &#38;l36; for \n(g6 = 4; g6 < (-3); g6 += 1); l102 = &#38;g6; *l102 = ((*l8) &#38;&#38; (l107 << 7)*(*l102)); } This \nrandomly generated function generates the following traces if compiled with gcc -O0 and gcc -O2. All \nthe events in the optimised trace can be matched with events in the reference trace by performing valid \neliminations and reorder\u00adings of events. Eliminated events are struck-off while the reorder\u00adings are \nrepresented by the arrows. Figure 2: successful matching of reference and optimised traces In a source \nprogram each thread consists of a sequence of instructions. During the execution of a program, any given \nstatic instruction may be iterated multiple times (for example due to looping) and display many behaviours. \nFor example reads may read from different writes and conditional branches may be taken or not. We refer \nto each such instance of the dynamic execution of an instruction as an instruction instance. More precisely, \neach instruction instance performs zero, one, or more shared memory accesses, which we call actions. \nWe account for all the differing ways a given program can execute by identifying a source program with \nthe set of sets of all the actions (annotated with additional information as described below) it can \nperform when it executes in an arbitrary context. We call the set of the actions of a particular execution \nan opsem and the set of all the opsems of a program an opsemset. For instance, the snippet of code below: \nx = 1; y = 1; if (x == y){x = 42;} has, among others, the two opsems below: The opsem on the left corresponds \nto an execution where the reads read the last values written by the thread itself; the opsem on the right \naccounts for an arbitrary context that concurrently updated the value of y to 2. Nodes represent actions \nand black arrows show Figure 3: effect of loop invariant code motion (LIM) on an opsem for (i=0; i<2; \ni++) { t = y; x = t; z = z + y + i; x = y; . for (i=0; i<2; i++) { z = z + t + i; } }  the sequenced-before \nrelation, which orders actions (by the same thread) according to their program order. The sequenced-before \nrelation is not total because the order of evaluation of the arguments of functions, or of the operands \nof most operators, is underspeci.ed in C and C++. The memory accesses are non-atomic, as denoted by the \nNA label. We can then characterise the effect of arbitrary optimisations of source code directly on opsemsets. \nOn a given opsem, the effect of any transformation of the source code is to eliminate, reorder, or introduce \nactions. If the optimiser performs constant propagation, the previous code is rewritten as: x = 1; y \n= 1; if (1 == 1){x = 42;} and its unique opsem is depicted here on the right. This opsem can be obtained \nfrom the one above on the left by eliminating the two read actions. A more complex example is shown in \nFigure 3, where the loop on the left is optimised by LIM. The .gure shows opsems for the initial state \nz=0, y=3 assuming that the code is not run in parallel with an interfering context. Here the effect of \nthe LIM optimisation is not only to remove some actions (shaded) but also to reorder the write to x. \n An opsem captures a possible execution of the program, so by applying a transformation to an opsem we \nare actually optimising one particular execution. Lifting pointwise this de.nition of seman\u00adtic transformations \nto opsemsets enables optimising all the execu\u00adtion paths of a program, one at a time, thus abstracting \nfrom actual source program transformation. In the rest of this section we give a high-level overview \nof the C11/C++11 memory model as formalised by Batty et al. [4], de.n\u00ad ing opsems, opsemsets and executions, \nwhile in the next section we formalise program transformations and prove their correctness. 2.1 Background: \noverview of the C11/C++11 memory model Let l range over locations (which are partitioned into non-atomic \nand atomic), v over values and tid . {1..n} over thread identi.ers. We consider the following actions: \nmem ord, \u00b5 ::= NA | SC | ACQ | REL | R/A | RLX f ::= R\u00b5 l v | W\u00b5 l v | Lock l | Unlock l | Fence\u00b5 | RMW\u00b5 \nl v1 v2 actions ::= aid, tid:f The possible actions are loads from and stores to memory, lock and unlock \nof a mutex, fences, and read-modify-writes of memory lo\u00adcations. Each action is identi.ed by an action \nidenti.er aid (ranged over by r, w, . . .) and speci.es its thread identi.er tid, the location l it \naffects, the value read or written v (when applicable), and the memory-order \u00b5 (when applicable).2 In \nthe drawings we omit the action and thread identi.ers. The thread-local semantics identi.es a program \nwith a set of opsems (ranged over by O): triples (A, sb, asw) where A . P(actions) and sb, asw . A \u00d7 \nA are the sequenced-before and additional-synchronised-with relations. Sequenced-before (de\u00adnoted sb) \nwas introduced above; it is transitive and irre.exive and only relates actions by the same thread; additional-synchronised\u00adwith \n(denoted asw) contains additional edges from thread creation and thread join, and in particular orders \ninitial writes to memory locations before all other actions in the execution. The thread-local semantics \nassumes that all threads run in an arbitrary concurrent context which can update the shared memory at \nwill. This is modelled by reads taking unconstrained values. We say that a set of opsems S is receptive \nif, for every opsem O, for every read action r, t:R\u00b5 l v in the opsem O, for all values v' there is an \nopsem O' in S which only differs from O because the read r returns v' rather than v, and for the actions \nthat are sequenced-after r. Intuitively a set of opsems is receptive if it de.nes a behaviour for each \npossible value returned by each read. We call the set of all the opsems of a program an opsemset, ranged \nover by P . The thread local semantics ensures that opsem\u00adsets are receptive. Opsems and opsemsets are \nsubject to several well-formedness conditions, e.g. atomic accesses must access only atomic locations, \nwhich we omit here and can be found in [4]. We additionally require opsemsets to be sb-pre.x closed, \nassuming that a program can halt at any time. Formally, we say that an opsem O' is an sb-pre.x of an \nopsem O if there is an injection of the ac\u00adtions of O' into the actions of O that behaves as the identity \non actions, preserves sb and asw, and, for each action x . O', when\u00adever x . O and y <sb x, it holds \nthat y . O'. Executions The denotation of each thread in an opsem is ag\u00adnostic to the behaviour of the \nother threads of the program: the thread-local semantics takes into account only the structure of ev\u00adery \nthread s statements, not the semantics of memory operations. In particular, the values of reads are chosen \narbitrarily, without regard for writes that have taken place. The memory model .lters inconsis\u00adtent opsems \nby constructing additional relations and checking the resulting candidate executions against the axioms \nof the model. For this an execution witness (denoted by W ) for an opsem speci.es an interrelationship \nbetween memory actions of different threads via three relations: reads-from (rf) relates a write to all \nthe reads that read from it; the sequential consistent order (sc) is a total order over all SC actions; \nand modi.cation order (mo) or coherence is the union of a per-location total order over writes to each \natomic loca\u00adtion. From these, the model infers the relations synchronises-with (denoted sw), which de.nes \nsynchronisation and is described in detail below, and happens-before (denoted hb), showing the prece\u00addence \nof actions in the execution. Key constraints on executions depend on the happens-before relation, in \nparticular a non-atomic read must not read any write related to it in hb other than its im\u00admediate predecessor. \nThis property is called consistent non-atomic read values, and for writes w1 and w2 and a read r accessing \nthe same non-atomic location, the following shapes are forbidden: 2 we omit consume atomics: their semantics \nis intricate (e.g. happens-before is not transitive in the full model) and at the time of writing no \nmajor compiler pro.ts from the their weaker semantics, treating consume as acquire. By general theorems \n[5], our results remain valid in the full model. For atomic accesses the situation is more complex, and \natomic reads can read from hb-unrelated writes. Happens-before is a partial relation de.ned as the transitive \nclosure of sb, asw and sw: hb = (sb . asw . sw)+ . We refer to a pair of an opsem and witness (O, W ) \nas a can\u00addidate execution. A pair (O, W ) that satis.es a list of consistency predicates on these relations \n(including consistent non-atomic read values) is called a pre-execution. The model .nally checks if none \nof the pre-executions contain an unde.ned behaviour. Unde.ned behaviours arise from unsequenced races \n(two con.icting accesses performed by the same thread not related by sb), indeterminate reads (an access \nthat does not read a written value), or data races (two con.icting accesses not related by hb), where \ntwo accesses are con.icting if they are to the same address and at least one is a non\u00adatomic write. Programs \nthat exhibit an unde.ned behaviour (e.g. a data-race) in one of their pre-executions are unde.ned; programs \nthat do not exhibit any unde.ned behaviour are called well-de.ned, and their semantics is given by the \nset of their pre-executions. Synchronisation Synchronisation between threads is captured by the sw relation. \nThe language provides two mechanisms for es\u00adtablishing synchronisation between threads and enabling race-free \nconcurrent programming: mutex locks and low-level atomics. The semantics of mutexes is intuitive: the \nsc relation, part of the wit\u00adness, imposes a total order over all lock and unlock accesses, and a synchronised-with \n(sw) edge is added between every unlock ac\u00adtion, and every lock of the same mutex that follows it in \nsc-order. Low-level atomics are speci.c to C/C++ and designed as an escape hatch to implement high-performance \nracy algorithms. Atomic op\u00aderations do not race with each other, by de.nition, and their seman\u00adtics is \nspeci.ed by a memory-order attribute. Sequentially consis\u00adtent atomics have the strongest semantics: \nall SC reads and writes are part of the total order sc (acyclic with hb). An SC read can only read from \nthe closest sc-preceding write to the same loca\u00adtion. A subtlety of the model is that, although sc and \nhb must be compatible, sc is not included in hb. Sequentially consistent atom\u00adics, as well as release-acquire \natomics, generate synchronisation edges, which are included in hb. This is best explained for a classic \nmessage-passing idiom. Imagine that one thread writes some (per\u00adhaps multi-word) data x and then an atomic \n.ag a, while another waits to see that .ag write before reading the data: x=0; atomic a=0 x = 1; while \n(0 == a.load(acq)) {}; a.store(1, rel); int r = x; The synchronisation between the release and acquire \nensures that the sender s write of x will be seen by the receiver. Below we depict a typical opsem for \nthis program; we represent the asw relation with the double horizontal line: the init actions are asw-before \nall other events. The witness has an rf arrow between the write\u00adrelease and read-acquire on a to justify \nthat the read returns 1. A read between a write-release and a read-acquire generates an sw edge. Since \nhb includes sb and sw, the write of 1 to x is the last write in the hb order before the read of the \nsecond thread, which is then forced to return 1. Relaxed atomics instead do not generate synchronisation \nedges sw; they are only forbidden to read from the future, i.e. from writes later in hb. Observable behaviour \nThe C11/C++11 memory model does not explicitly de.ne the observable behaviour of an execution. We ex\u00adtend \nthe model with a special atomic location called world, and model the observable side-effects of the program \n(e.g., writes on stdout) by relaxed writes to that location. The relaxed attribute guarantees that these \naccesses are totally ordered with each other, as captured by the mo relation. As a result, the observable \nbehaviour of a pre-execution is the restriction of the mo relation to the dis\u00adtinguished world location. \nIf none of its pre-executions exhibit an unde.ned behaviour, then the observable behaviour of a program \nis the set of all observable behaviours of its executions. 3. Sound Optimisations in the C Memory Model \nC and C++ are shared-memory-concurrency languages with ex\u00adplicit thread creation and implicit sharing: \nany location might be read or written by any thread for which it is reachable from vari\u00adables in scope. \nIt is the programmer s responsibility to ensure that such accesses are race-free. This implies that compilers \ncan per\u00adform optimisations that are not sound for racy programs and com\u00admon thread-local optimisations \ncan still be done without the com\u00adpiler needing to determine which accesses might be shared. . Sev.c\u00b4ik \nshowed that a large class of elimination and reordering transformations are correct (that is, do not \nintroduce any new be\u00adhaviour when the optimised code is put in an arbitrary data-race free context) in \nan idealised DRF model [17, 19]. In this section we adapt and extend his results to optimise non-atomic \naccesses in the C11/C++11 memory model. As we have discussed, we classify program transformations as \neliminations, reorderings, and intro\u00adductions over opsemsets. 3.1 Eliminations of actions The semantic \nelimination transformation is general enough to cover optimisations that eliminate memory accesses based \non data-.ow analysis, such as common subexpression elimination, induction variable elimination, and global \nvalue numbering, including the cases when these are combined with loop unrolling. We de.ne semantic elimination \nand discuss informally its soundness criterion; we then state the soundness theorems and brie.y describe \nthe proof structure. De.nition 3.1. An action is a release if it is an unlock action, an atomic write \nwith memory-order REL or SC, a fence or read\u00admodify-write with memory-order REL, R/A or SC. Semantically, \nrelease actions can be seen as potential sources of sw edges. The intuition is that they release permissions \nto access shared memory to other threads. De.nition 3.2. An action is an acquire if it is a lock action, \nor an atomic read with memory-order ACQ or SC, or a fence or read\u00admodify-write with memory order ACQ, \nR/A or SC. Acquire actions can be seen as potential targets of sw edges; the in\u00adtuition is that they \nacquire permissions to access shared memory from other threads. To simplify the presentation we omit \ndynamic thread creation. This is easily taken into account by stating that spawning a thread has release \nsemantics, while the .rst accesses in sb-order of the spawned function have acquire semantics. Reciprocally, \nthe last actions of a thread have release semantics and a thread-join has acquire semantics. A key concept \nis that of a same-thread release-acquire pair: De.nition 3.3. A same-thread release-acquire pair (shortened \nst\u00adrelease-acquire pair) is a pair of actions (r, a) such that r is a release, a is an acquire, and r \n<sb a. Note that these may be to different locations and never syn\u00adchronise together. To understand the \nrole they play in optimisation soundness, consider the code on the left, running in the concurrent context \non the right: x=0; atomic a1,a2=0 x = 1; while(0==a1.load(acq)) {};a1.store(1,rel); printf(\"%i\",x); \nwhile(0==a2.load(acq)) {}; a2.store(1,rel); x = 2; All executions have similar opsems and witnesses, \ndepicted below (we omitted rf arrows from initialisation writes). No consistent ex\u00adecution has a race \nand the only observable behaviour is printing 1. Eliminating the .rst store to x (which might appear \nredun\u00addant as x is later overwritten by the same thread) would preserve DRF but would introduce a new \nbehaviour where 0 is printed. However, if either the release or the acquire were not in between the two \nstores, then this context would be racy (respectively be\u00adtween the load performed by the print and the \n.rst store, or be\u00adtween the load and the second  store) and it would be correct to optimise away the \n.rst write. More generally, the proof of the The\u00adorem 3.1 below clearly shows that the presence of an \nintervening same-thread release-acquire is a necessary condition to allow a dis\u00adcriminating context to \ninteract with a thread without introducing data races. De.nition 3.4. A read action a, t:RNA l v is eliminable \nin an opsem O of the opsemset P if one of the following applies: Read after Read (RaR): there exists \nanother action r, t:RNA l v such that r <sb a, and there does not exist a memory access to location l \nor a st-release-acquire pair sb-between r and a; Read after Write (RaW): there exists an action w, t:WNA \nl v such that w <sb a, and there does not exist a memory access to location l or a st-release-acquire \npair sb-between w and a; Irrelevant Read (IR): for all values v ' there exists an opsem O ' . P and a \nbijection f between actions in O and actions in O ' , such that f(a) = a ' , t:RNA l v ', for all actions \nu . O different from a, f(u) = u, and f preserves sb and asw. A write action a, t:WNA l v is eliminable \nin an opsem O of the opsemset P if one of the following applies: Write after Read (WaR): there exists \nan action r, t:Rl v such that r <sb a, and there does not exist a memory access to location l or a st-release-acquire \npair sb-between r and a; Overwritten Write (OW): there exists another action w, t:WNA l v ' such that \na <sb w, and there does not exist a memory access to location l or a st-release-acquire pair sb-between \na and w; Write after Write (WaW): there exists another action w, t:WNA l v such that w <sb a, and there \ndoes not exist a memory access to location l or a st-release-acquire pair sb-between w and a. Note that \nthe OW rule is the only rule where the value can differ between the action eliminated and the action \nthat justi.es the elim\u00adination. The IR rule can be rephrased as a read in an execution is irrelevant \nif the program admits other executions (one for each value) that only differ for the value returned by \nthe irrelevant read . De.nition 3.5. An opsem O ' is an elimination of an opsem O if there exists a injection \nf : O ' . O that preserves actions, sb, and asw, and such that the set O\\f(O ' ) contains exactly one \neliminable action. The function f is called an unelimination. To simplify the proof of Theorem 3.1 the \nde.nition above al\u00adlows only one elimination at a time (this avoids a critical pair be\u00adtween the rules \nOW and WaW whenever we have two writes of the same value to the same location), but, as the theorem shows, \nthis de.nition can be iterated to eliminate several actions from one opsem while retaining soundness. \nThe de.nition of eliminations lifts pointwise to opsemsets:  De.nition 3.6. An opsemset P ' is an elimination \nof an opsemset P if for all opsem O ' . P ' there exists an opsem O . P such that O ' is an elimination \nof O. In the previous section we did not describe all the intricacies of the C11/C++11 model but our \ntheory takes all of them into account. For example, a release fence followed by an atomic write behaves \nas if the write had the REL attribute, except that the sw edge starts from the fence action. Another \nexample is given by release sequences: if an atomic write with attribute release is followed im\u00admediately \nin mo-order by one or more relaxed writes in the same thread (to the same location), and an atomic load \nwith attribute acquire reads-from one of these relaxed stores, an sw edge is created between the write \nrelease and the load acquire, analogously to the case where the acquire reads directly from the .rst \nwrite. These subtleties must be taken into account in the elimination correctness proof but do not invalidate \nthe interven\u00ading same-thread release-acquire pair criterion. This follows from a property of the C11/C++11 \ndesign that makes every sw edge relate a release action to an acquire action. For instance, in the program \nbelow it is safe to remove the .rst write to x as OW, because all discriminating contexts will be necessarily \nracy: a1.store(1,rel); x = 1; a1.store(2,rlx); while(0==a2.load(acq)) {}; x = 2;  We establish that \nour semantic transformations have the fol\u00adlowing properties: any execution of the transformed opsemset \nhas the same observable behaviour of some execution of the original opsemset, and the transformation \npreserves data-race freedom. As C11 and C++11 do not provide any guarantee for racy programs we cannot \nprove any result about out-of-thin-air value introduction. Theorem 3.1. Let the opsemset P ' be an elimination \nof the opsem\u00adset P . If P is well-de.ned, then so is P ', and any execution of P ' has the same observable \nbehaviour of some execution of P . We sketch the structure of the proof; details can be found on\u00adline \n[1]. Let the opsemset P ' be an elimination of the opsemset P , and let (O ' , W ' ) be an execution \nof P ' (that is, a pair of an opsem and a witness). Since P ' is an elimination of P , there is at least \none opsem O . P such that O ' is an elimination of O, and an unelimi\u00adnation function f that injects the \nevents of the optimised opsem into the events of the unoptimised opsem. We build the mo and sc rela\u00adtions \nof the witness of O by lifting the mo ' and sc ' relations of the witness W ': this is always possible \nbecause our program transfor\u00admations do not alter any atomic access. Analogously it is possible to build \nthe rf relation on atomic accesses by lifting the rf ' one. To complete the construction of the witness \nwe lift the rf ' relation on non-atomic events as well, and complete the rf relation following the case \nanalysis below on the eliminated events in O: RaR: if i is a read action eliminated with the RaR rule \nbecause of a preceding read action r, and w <rf r, then add w <rf i; RaW: if i is a read action eliminated \nwith the RaW rule because of a preceding write action w, then add w <rf i;  IR: if i is an irrelevant \nread, and there is a write event to the same location that happens before it, then let w be a write event \nto the same location maximal with respect to hb and add w <rf i;  OW: rf is unchanged;  WaW: if i is \na write event eliminated by the WaW rule because of the preceding write event w, then for all actions \nr such that w <rfl r and i <hb r, replace w <rf r by i <rf r;  WaR: if i is a read event eliminated \nby the WaR rule, then every read of the same value at the same location, that happens-after i and that \neither read from a write w <hb i or does not read from any write, now reads from i.  This completes \nthe construction of the witness W and in turn of the candidate execution (O, W ) of P . We must now prove \nthat (O, W ) is consistent, in particular that it satis.es consistent non\u00adatomic read values, for which \nthe construction has been tailored. This proceeds by a long case disjunction that relies on the following \nconstructions: the absence of a release-acquire pair between two accesses a and b in the same thread \nguarantees the absence of an access c in another thread with a <hb c <hb b.  in some cases the candidate \nexecution (O, W ) turns out to have con.icting accesses a and b that are not ordered by hb. We use the \nfact that opsemsets are receptive and closed under sb-pre.x to build another candidate pre-execution \nof P where a and b are still hb-unordered, but for which we can prove it is a pre\u00adexecution (not necessarily \nwith the same observable behaviour). From this we deduce that P is not data-race free and ignore these \ncases.  By construction the pre-execution (O, W ) has the same observable behaviour as (O ' , W ' ); \nwe conclude by showing that (O ' , W ' ) can not have unde.ned behaviours that (O, W ) does not have. \n 3.2 Reorderings of actions Most compiler optimisations do not limit themselves to eliminating memory \naccesses, but also reorder some of them. In this category fall all the code motion optimisations. Of \ncourse not all accesses are reorderable without introducing new behaviours. In this section we state \nand prove the correctness of the class of reorderings we have observed being performed by gcc and clang, \nignoring more com\u00adplex reordering schemes. In particular we omit reorderings across synchronisation actions: \nneither gcc or clang perform them and a more complex statement taking into account partial reorderings \non pre.x-closures of an opsemset would be needed. De.nition 3.7. Two actions a and b are reorderable \nif they access different memory locations and neither is a synchronisation action (that is a lock, unlock, \natomic access, rmw, or fence action). De.nition 3.8. An opsem O ' is a reordering of an opsem O if: (i) \nthe set of actions in O and O ' are equal; (ii) O and O ' de.ne the same asw; (iii) for all actions a, \nb . O, if a <sb b then either a <sbl b or b <sbl a and a is reorderable with b. Like for eliminations, \nthe de.nition of reorderings lifts point\u00adwise to opsemsets: De.nition 3.9. An opsemset P ' is a reordering \nof an opsemset P if for all opsems O ' . P ' there exists an opsem O . P such that O ' is a reordering \nof O. The soundness theorem for reorderings can be stated analo\u00adgously to the one for eliminations and, \nalthough the details are dif\u00adferent, the proofs follow the same structure. In particular the proof shows \nthat, given a witness for an execution of a reordered opsem, it is possible to build the happens-before \nrelation for the witness for the corresponding source opsem by lifting the synchronise-with re\u00adlation, \nwhich is unchanged by the reorderings and relates the same release/acquire events in the two witnesses. \n Theorem 3.2. Let the opsemset P ' be a reordering of the opsemset P . If P is well-de.ned, then so \nis P ', and any execution of P ' has the same observable behaviour of some execution of P . As the sb \nrelation is partial, reordering instructions in the source code can occasionally introduce sb arrows \nbetween reordered ac\u00adtions. For instance, the optimised trace of Figure 3 not only re\u00adorders the WNA \nx 3 with the RNA z 0 and WNA z 3 actions but also introduces an sb arrow between the .rst two events \nRNA y 3 and RNA z 0. Unsurprisingly, adding sb arrows to an opsem reduces the non-determinism of the \ncandidate executions and does not intro\u00adduce new behaviours on well-de.ned programs. De.nition 3.10. \nWe say that an opsem O ' is a linearisation of an opsem O if they contain the same actions and asw relation, \nand whenever x <sb y . O ' it holds that x <sb y . O. This de.nition lifts pointwise to opsemsets. Theorem \n3.3. Let the opsemset P ' be a linearisation of the opsem\u00adset P . If P is well-de.ned then so is P ', \nand any execution of P ' has the same observable behaviour of some execution of P .  3.3 Introductions \nof actions Even if it seems counterintuitive, compilers tend to introduce loads when optimising code \n(introducing writes is incorrect in DRF mod\u00adels most of the time [7], and always dubious see the .nal \nexam\u00ad ple in Section 5). Usually the introduced loads are irrelevant, that is their value is never used. \nThis program transformation sounds harmless but it can introduce data races and does not lend itself \nto a theorem analogous to those for eliminations and reorderings. Worse than that, if combined with DRF-friendly \noptimisations it can introduce unexpected behaviours [19]. We conjecture that a soundness property can \nbe stated relating the source semantics to the actual hardware behaviour, along the lines of: if an opsem \nO ' is obtained from an opsem O by introducing irrelevant reads, and it is then compiled naively following \nthe standard compilation schemes for a given architecture [15, 23], then all the behaviours observable \non the architecture are allowed by the original opsem. In some cases the introduced loads are not irrelevant, \nbut are RaR\u00adeliminable or RaW-eliminable. We proved that RaR-eliminable and RaW-eliminable introductions \npreserve DRF and do not introduce new behaviours under the hypothesis that there is no release action \nin sb order between the introduced read and the action that justi.es it. Details are available online \n[1]. 4. Compiler Testing Building on the theory of the previous section, we designed and implemented \na bug-hunting tool called cmmtest. The tool performs random testing of C and C++ compilers, implementing \na variant of Eide and Regehr s access summary testing [10]. A test case is any well-de.ned, sequential \nC program; for each test case, cmmtest: 1. compiles the program using the compiler and compiler optimi\u00adsations \nthat are being tested; 2. runs the compiled program in an instrumented execution envi\u00adronment that logs \nall memory accesses to global variables and synchronisations; 3. compares the recorded trace with a \nreference trace for the same program, checking if the recorded trace can be obtained from the reference \ntrace by valid eliminations, reorderings and intro\u00adductions.  Test-case generation We rely on a small \nextension of Csmith [24] to generate random programs that cover a large subset of C while avoiding unde.ned \nand unspeci.ed behaviours. We added mutex variables (as de.ned in pthread.h) and system calls to pthread \nmutex lock and pthread mutex unlock. Enforcing balancing of calls to lock and unlock along all possible \nexecution paths of the generated test-cases is dif.cult, so mutex variables are declared with the attribute \nPTHREAD MUTEX RECURSIVE. We also added atomic variables and atomic accesses to atomic variables, labelled \nwith a memory order attribute. For atomic variables we support both the C and the C++ syntax, the former \nnot yet being widely supported by today s compilers. Due to limitations of our tracing infrastructure, \nfor now we instruct Csmith to not generate programs with unions or consts. Compilation Compilation is \nstraightforward provided that the compiler does not attempt to perform whole-program optimisa\u00adtions. \nWith whole-program optimisation, the compiler can deduce that some functions will never run in a concurrent \ncontext and per\u00adform more aggressive optimisations, such as eliminating all mem\u00adory accesses after the \nlast I/O or synchronisation. It is very hard to determine precise conditions under which these aggressive \nwhole\u00adprogram optimisations remain sound; for instance .c\u00b4ik s criteria Sev.are overly conservative and \nresult in false positives when testing Intel s icc -O3. Neither gcc nor clang perform whole-program optimisations \nat the typical optimisation levels -O2 or -O3. Test case execution and tracing The goal of this phase \nis to record an execution trace for the compiled object .le, that is the linearly-ordered sets of actions \nit performs. For the x86 64 ar\u00adchitecture we implemented a tracing framework by binary instru\u00admentation \nusing the Pin tool [13]. Our Pin application intercepts all memory reads and writes performed by the \nprogram and for each records the address accessed, the value read or written, and the size. In addition \nit also traces calls to pthread mutex lock and pthread mutex unlock, recording the address of the mutex. \nWith the exception of access size information, entries in the ex\u00adecution trace correspond to the actions \nof Section 2. Access size information is needed to analyse optimisations that merge adjacent accesses, \ndiscussed below. The raw trace depends on the actual addresses where the global variables have been allocated, \nwhich is impractical for our purposes. The trace is thus processed with additional informations obtained \nfrom the ELF object .le (directly via objdump or by parsing the debugging informations). The addresses \nof the accesses are mapped to variable names and similarly for values that refer to addresses of variables; \nin doing so we also recover array and struct access information. For example, if the global variable \nint g[10] is allocated at 0x1000, the raw action Store 0x1008 0x1004 4 is mapped to the action Store \ng[2] &#38;g[1] 4. After this analysis execution traces are independent of the actual allocation addresses \nof the global variables. Finally, information about the initialisation of the global variables is added \nto the trace with the Init tag. Irrelevant reads The Pin application also computes some data\u00ad.ow information \nabout the execution, recording all the store actions which depend on each read. This is done by tracking \nthe .ow of each read value across registers and thread-local memory. A dependency is reported if either \nthe value read is used to compute the value written or used to determine the control that leads to a \nlater write or synchronisation. With this information we reconstruct an approximation of the set of irrelevant \nreads of the execution: all reads whose returned value is never used to perform a write (or synchronisation) \nare labelled as irrelevant. In addition, we use a second algorithm to identify irrelevant reads following \ntheir characterisation on the opsems: the program is replayed and its trace recorded again but binary \ninstrumentation injects a different value for the read action being tested. The read is irrelevant if, \ndespite the different value read, the rest of the trace is unchanged. This approach is slower but accurately \nidenti.es irrele\u00advant reads inserted by the optimiser (for instance when reorganising chains of conditions \non global variables).  Reference trace In the absence of a reference interpreter for C11, we reuse our \ntracing infrastructure to generate a trace of the pro\u00adgram compiled at -O0 and use this as the reference \ntrace. By doing so our tool might miss potential front-end concurrency bugs but there is no obvious alternative. \nTrace matching Trace matching is performed in two phases. Ini\u00adtially, eliminable actions in the reference \ntrace are identi.ed and labelled as such. The labelling algorithm linearly scans the trace, recording \nthe last action performed at any location and whether release/acquire actions have been encountered since; \nusing this in\u00adformation each action is analysed and labelled following the de.ni\u00adtion of eliminable actions \nof Section 3.1. Irrelevant reads reported by the tracing infrastructure are labelled as such. To get \nan intuition for the occurrences of eliminable actions in program traces, out of 200 functions generated \nby Csmith with -expr complexity 3, the average trace has 384 actions (max 15511) of which 280 (max 14096) \nare eliminable, distributed as follows: IR RaW RaR OW WaR WaW 8 (94) 94 (1965) 95 (10340) 75 (1232) 5 \n(305) 1 (310) Additional complexity in labelling eliminable actions is due to the fact that a compiler \nperforms many passes over the program and some actions may become eliminable only once other actions \nhave been eliminated. Consider for instance this sequence of optimisa\u00adtions from the example in Figure \n2: Store g6 4 Load Load Load g6 g6 g6 4 4 4 RaW- -. Store Store g6 g6 4 1 OW--. Store g6 1 Store g6 1 \n In the original trace, the .rst store cannot be labelled as OW due to the intervening loads. However, \nif the optimiser .rst removes these loads (which is correct since they are RaW), it can subsequently \nremove the .rst store (which becomes an OW). Our analysis thus keeps track of the critical pairs between \neliminations and can la\u00adbel actions eliminable under the assumption that other actions are themselves \neliminated. Irrelevant and eliminable reads are also la\u00adbelled in the optimised trace, to account for \npotential introductions. Before describing the matching algorithm, we must account for one extra optimisation \nfamily we omitted in the previous section: merging of adjacent accesses. Consider the following loop \nupdat\u00ading the global array char g[10]: for (int l=0; l<10; l++) g[l] = 1; The reference trace performs \nten writes of one byte to the array g. The object code generated by gcc -O3 performs only two mem\u00adory \naccesses: Store g 101010101010101 8 and Store g[8] 101 2. This optimisation is sound under hypothesis \nsimilar to those of eliminations: there must not be intervined accesses or re\u00adlease/acquire pairs between \nthe merged accesses; additionally the size of the merged access must be a power of two and the merged \nstore must be aligned. Since the C11/C++11 memory model, as for\u00admalised by Batty et al., is agnostic \nto the size of accesses, we could not formally prove the soundness of this optimisation. The matching \nalgorithm takes two annotated traces and scans them linearly, comparing one annotated action of the reference \ntrace and one of the optimised trace at a time. It performs a depth\u00ad.rst search exploring at each step \nthe following options: if the reference and optimised actions are equal, then consider them as matched \nand move to the next actions in the traces; if the reference action is eliminable, delete it from the \nrefer\u00adence trace and match the next reference action; similarly if the optimised action is eliminable; \n if the reference action merges with later reference actions to match the the optimised action, then \nconsider the merged ac\u00adtions as matched and move to the next actions in the traces; and  if the optimised \naction can be matched by reordering actions in the reference trace, reorder the reference trace and match \nagain.  The algorithm succeeds if it reaches a state where all the actions in the two input traces are \neither deleted or matched. Some of these options are very expensive to explore (for in\u00adstance when combinations \nof reordering, merging and eliminations must be considered), and we guide the depth-.rst search with \na crit\u00adical heuristic to decide the order in which the tree must be explored. The heuristic is dependent \non the compiler being tested. The cur\u00adrent implementation can match in a few minutes gcc traces of up \nto a few hundreds of actions on commodity hardware; these traces are well beyond manual analysis. Outcome \nDuring tracing, cmmtest records one execution of a C or C++ program; using the terminology of the previous \nsection, it should observe a witness for an opsem of the program. Since cmmtest traces sequential deterministic \ncode in an empty concur\u00adrent environment, all reads of the opsem always return the last (in sb order) \nvalue written by the same thread to the same location. The witness is thus trivial: reads-from derives \nfrom sb while mo and sc are both included in sb. Note that the tool traces an execution of the generated \nassembler and as such it records a linearisation of the opsem s sb. Theorem 3.3 guarantees that this \ncannot introduce false positives due to extra sb arrows added between non-atomic accesses. Overestimating \nthe sb relation between atomic accesses might induce false positives because cmmtest cannot reconstruct \nthe original sb relation and considers all atomic accesses as un\u00adreorderable. To prevent this our version \nof Csmith never generates programs with atomic accesses not related by sb. The cmmtest tool compares \ntwo opsems and returns true if the optimised opsem can be obtained from the reference opsem by a se\u00adquence \nof sound eliminations/reorderings/introductions, and false otherwise. If cmmtest returns true then we \ndeduce that this opsem (that is, this execution path of the program) has been compiled cor\u00adrectly, even \nif we cannot conclude that the whole program has been compiled correctly (which would require exploring \nall the opsems of the opsemset, or, equivalently, all the execution paths of the pro\u00adgram). More interestingly, \nwhenever cmmtest returns false, then either the code has been miscompiled, or an exotic optimisation \nhas been applied (since our theory of sound optimisations is not complete). In this case we perform test-case \nreduction using CRe\u00adduce [16], and manually inspect the assembler. Reduced test-cases have short traces \n(typically less than 20 events) and it is immediate to build discriminating contexts and produce bug-reports. \nCurrently the tool reports no false positives for gcc on any of the many thou\u00adsands of test-cases we \nhave tried without structs and atomics; we are getting closer to a similar result for arbitrary programs \n(with the restriction of a single atomic access per expression). Stability against the HW memory model \nCompiled programs are executed on shared-memory multiprocessors which may expose behaviours that arise \nfrom hardware optimisations. Reference map\u00adpings of atomics instructions to x86 and ARM/POWER architec\u00adtures \nhave been proved correct [4, 5]: these mappings insert all the necessary assembly synchronisation instructions \nto prevent hard\u00adware optimisations from breaking the C11/C++11 semantics. On x86 64, cmmtest additionally \ntraces memory fences and locked instructions, and under the hypothesis that the reference trace is ob\u00adtained \nby applying a correct mapping (e.g., by putting fence instruc\u00adtions after all SCatomic writes), then \ncmmtest additionally ensures that the optimiser does not make hardware behaviours observable (for instance \nby moving a write after a fence).  5. Impact on compiler development Concurrency compiler bugs While \ndeveloping cmmtest and tun\u00ading the matching heuristic, we tested the latest svn version of the 4.7 and \n4.8 branches of the gcc compiler. During these months we reported several concurrency bugs (including \nbugs no. 52558, 54149, 54900, and 54906 in the gcc bugzilla), which have all been promptly .xed by the \ngcc developers. In one case the bug report highlights an obscure corner case of the gcc optimiser, as \nshown by a discussion on the gcc-patches mailing list;3 even though the reported test-case has been .xed, \nthe bug has been left open until a general solution is found. In all cases the bugs were wrongly in\u00adtroduced \nwrites, speculated by the LIM or IFCVT (if-conversion) phases, similar to the example in Figure 1. These \nbugs do not only break the C11/C++11 memory model, but also the Posix DRF\u00adguarantee which is assumed \nby most concurrent software written in C and C++. The corresponding patches are activated via the ---param \nallow-store-data-races=0 .ag, which will even\u00adtually become default standard for -std=c11 or -std=c++11 \n.ags. All these are silent wrong-code bugs for which the compiler issues no warning. Unexpected behaviours \nEach compiler has its own set of inter\u00adnal invariants. If we tune the matching algorithm of cmmtest to \ncheck for compiler invariants rather than for the most permissive sound optimisations, it is possible \nto catch unexpected compiler behaviours. For instance, in the current phase of development, gcc forbids \nall reorderings of a memory access with an atomic one. We baked this invariant into cmmtest and in less \nthan two hours of testing on an 8-core machine we found the following testcase: atomic_uint a; int32_t \ng1, g2; int main (int, char *[]) { a.load () &#38; a.load (); g2 = g1 != 0; } whose traces for the function \nmain compiled with gcc 4.8.0 20120627 (experimental) at optimisation levels -O0 and -O2 (or -O3) are: \n As we can observe, the optimiser moved the load of g1 before the two atomic SC loads. Even though we \nconjecture that this reordering is not observable by a non-racy context, this unexpected compiler behaviour \nwas .xed nevertheless (rr190941). Interestingly, cmmtest found one unexpected compiler be\u00adhaviour whose \nlegitimacy is arguable. Consider the program below: atomic_int a; uint16_t g; void func_1 () { for (; \na.load () <= 0; a.store (a.load () + 1)) for (; g; g--); }  Traces for the func_1 function, compiled \nwith gcc 4.8.0 20120911 (experimental) at optimisation levels -O0 and -O2 (or -O3), are: 3 http://gcc.gnu.org/ml/gcc-patches/2012-10/msg01411.html \n The optimiser here replaced the inner loop with a single write: for (; g; g--); -. g = 0; thus substituting \na read access with a write access in the execution path where g already contains 0. The introduced store \nis idempo\u00adtent, as it rewrites the value that is already stored in memory. Since in the unoptimised trace \nthere is already a load at the same location, this extra write cannot be observed by a non-racy context. \nStrictly speaking, this is not a compiler bug. However, whether this should be allowed or not is subject \nto debate. Although we believe no ar\u00adchitecture can detect this introduced write, in a world with hard\u00adware \nor software race detection it might .re a false positive. Also, possibly a bigger issue, the introduced \nwrite can introduce cache contention where there should not be any, potentially resulting in an unexpected \nperformance loss. 6. Related work Randomised techniques for testing compilers have been popular since \nthe 60 s and have been applied to a variety of languages, ranging from Cobol [22] and Fortran [9] to \nC [12, 14, 21] and C++ [25]. A survey (up to 1997) can be found in Boujarwah and Saleh [8], while today \nthe state of art is represented by Yang et al. s work on Csmith [24]. None of these works addresses concurrency \ncompiler bugs and the techniques presented are unable to detect any of our bug reports. The notable exception \nis Eide and Regehr s work [10] on hunt\u00ad ing miscompilation of the volatile quali.er in production-quality \nC compilers. Eide and Regehr generate random well-de.ned de\u00adterministic, sequential programs with volatile \nmemory accesses: miscompilations are detected by counting how many accesses to volatile variables are \nperformed during the execution of an unop\u00adtimised and an optimised binary of the program. The semantics \nof the volatile attribute requires that accesses to volatile variables are never optimised away, so comparing \nthe number of runtime accesses is enough to detect bugs in the optimiser. We were in\u00adspired by Eide and \nRegehr s approach to reduce hunting concur\u00adrency compilation bugs to analysis to differential testing \nof sequen\u00adtial code, but the complexity of the C11/C+11 memory model re\u00adquires us to build a theory of \nsound optimisations and makes the analysis phase far more complicated. As we discussed in Section 3, \nsoundness of optimisations in an idealised DRF model was studied by .c\u00b4ik [17, 19]. We reuse Sev..c\u00b4ik \ns classi.cation of optimisations as eliminations, reorderings Sev.and introductions, but moving from \nan idealised DRF model to the full C11/C++11 memory model brings new challenges: we cannot identify \na program with the set of linear orders of ac\u00adtions it can realise because in C and C++ the sequenced-before \norder is not total; although reasoning about opsems seems un\u00adintuitive, partial orders turn out be be \neasier to work with than the explicit manipulation of trace indices that .c\u00b4ik performs; Sev.  the \nsemantics of low-level atomic accesses and fences must be taken into account when computing synchronisations; \nin particular the weaker consistency and coherency conditions of the release/acquire/relaxed attributes \nmade the soundness proof much more complex.  There are other minor differences, for instance .c\u00b4ik assumes \nthat Sev.every variable has a default value, while C/C++ forbids accessing an uninitialised variable. \nInitially Sev.c.\u00b4ik gave a more restrictive condition for soundness of eliminations [17], namely eliminations \nare forbidden if there is an intervening release or acquire operation rather than a release/acquire pair. \nThis simpler condition appears to be too strong as we have observed compilers eliminate accesses across \na release or acquire access. All our analysis was driven by what optimisations we could actually observe: \nthis led to identify\u00ading WaW eliminations and RaR/RaW introductions, and motivated us to omit roach-motel \nreorderings.  More generally, reasoning about the C11/C++11 memory model is in its infancy. We follow \nthe formalisation of the C11/C++11 memory model given in [4]; the original presentation of the model \ndesign [7] mentions (Sec. 2.1) that the soundness of reorderings under some conditions follows from previous \nwork [2], with no mention of eliminations or introductions. Although the C11/C++11 model is based on \nthe DRF design, we do believe that its complex\u00adity deserves careful proofs taking into account its whole \ndesign. There is a long tradition of research on compiler optimisations that preserve sequential consistency \ndating back to Shasha and Snir [20], mostly focusing on whole program analyses. While these works show \nthat a restricted class of optimisations can maintain the illusion of sequential consistency for all \nprograms, we show that common compiler transformations maintain the illusion of sequential consistency \nfor correctly synchronised programs. 7. Conclusion This paper validates the belief that common compiler \noptimisations are sound in C11/C++11 memory model, contrary to the situation for the Java Memory Model \n[18]. With the exception of optimisa\u00ad tions that change the size of memory accesses (which cannot be \nexpressed in the current formalisation of the memory model) and irrelevant read introduction (which does \nnot preserve DRF), we proved correct all the classes of optimisations performed by widely used optimising \ncompilers (under the hypothesis that a program can halt at any time). We presented a general strategy \nto perform differential testing of real compilers against memory models, and designed and implemented \na bug-hunting tool, cmmtest, building on our theory of optimisations in the C11/C++11 memory model. Subtle \nconcurrency bugs and unexpected behaviours of the latest gcc optimiser have been discovered using cmmtest. \nNone of these could have been found using the existing compiler testing methods. Future work Bug-hunting \nvia random testing is a slow process: each reported bug must be .xed before continuing testing, other\u00adwise \nwith high probability the tool keeps on rediscovering the same bug. Our priority now is to complete the \ntracing infrastructure to support all the features of the C language, most notably bit-.elds, and to \nput cmmtest to work conducting extensive testing of gcc and clang. Since our testing strategy considers \nthe compiler as a black box, it is easy to extend the tool to test other compilers. A preliminary trial \nrun with clang suggests that only minor changes to the matching heuristic are required; contrary to gcc, \nclang sys\u00adtematically reorders accesses around relaxed atomic accesses and performs simultaneous merge \nand reorder of memory actions. Al\u00adthough this article focuses on the x86 64 architecture and x86 64 compiler \nbackends, only the tracer module (and IR data-.ow anal\u00adysis) of the tool are dependent on the binary \narchitecture. Building on Fox s ARM emulator [11], we are implementing a tracing infras\u00ad tructure for \nARMv7 binaries that will allow testing the ARM back\u00adends of compilers. By instrumenting an executable \nsemantics for the LLVM IR (e.g., [26]), it would even be possible to run our anal\u00ad ysis internally within \nthe LLVM optimisers, comparing the traces before and after each optimisation pass. Last but not least, \nthe gcc developers expressed a keen interest in adopting our cmmtest tool as part of their testing infrastructure. \nAcknowledgments We are grateful to Jaroslav .c\u00b4 Sev.ik, Kayvan Memarian, and Peter Sewell for enlightening \ndiscussions, to John Regehr and Xuejun Yang for help with Csmith, to Aldy Hernandez, Andrew MacLeod and \nTorvald Riegel for promptly addressing our bug-reports. This work was partially supported by IRILL and \nANR grant WMC (ANR-11-JS02-011). References [1] The cmmtest tool. http://www.di.ens.fr/~zappa/projects/ \ncmmtest/. [2] S. V. Adve and M. D. Hill. Weak ordering -a new de.nition. In ISCA, 1990. [3] J. Alglave, \nL. Maranget, S. Sarkar, and P. Sewell. Litmus: Running tests against hardware. In TACAS, 2011. [4] M. \nBatty, S. Owens, S. Sarkar, P. Sewell, and T. Weber. Mathematizing C++ concurrency. In POPL, 2011. [5] \nM. Batty, K. Memarian, S. Owens, S. Sarkar, and P. Sewell. Clarifying and compiling C/C++ concurrency: \nfrom C++11 to POWER. In POPL, 2012. [6] P. Becker. Standard for Programming Language C++ -ISO/IEC 14882, \n2011. [7] H.-J. Boehm and S. V. Adve. Foundations of the C++ concurrency memory model. In PLDI, 2008. \n[8] A. S. Boujarwah and K. Saleh. Compiler test case generation methods: a survey and assessment. Information \n&#38; Software Technology, 39(9): 617 625, 1997. [9] C. J. Burgess and M. Saidi. The automatic generation \nof test cases for optimizing fortran compilers. Information &#38; Software Technology, 38 (2):111 119, \n1996. [10] E. Eide and J. Regehr. Volatiles are miscompiled and what to do about it. EMSOFT, 2008. [11] \nA. C. J. Fox and M. O. Myreen. A trustworthy monadic formalization of the ARMv7 instruction set architecture. \nIn ITP, 2010. [12] C. Lindig. Random testing of C calling conventions. In AADEBUG, 2005. [13] C.-K. Luk, \nR. Cohn, R. Muth, H. Patil, A. Klauser, G. Lowney, S. Wal\u00adlace, V. J. Reddi, and K. Hazelwood. Pin: building \ncustomized pro\u00adgram analysis tools with dynamic instrumentation. In PLDI, 2005. [14] W. M. McKeeman. \nDifferential testing for software. Digital Technical Journal, 10(1):100 107, 1998. [15] P. E. McKenney \nand R. Silvera, 2011. http://www.rdrop.com/ users/paulmck/scalability/!paper/N2745r.2011.03.04a. html. \n[16] J. Regehr, Y. Chen, P. Cuoq, E. Eide, C. Ellison, and X. Yang. Test\u00adcase reduction for C compiler \nbugs. In PLDI, 2012. [17] J. Sev ..c\u00b4ik. Program Transformations in Weak Memory Models. PhD thesis, University \nof Edinburgh, 2008. [18] J. Sev ..c\u00b4ik. The Sun Hotspot JVM does not conform with the Java memory model. \nTechnical Report EDI-INF-RR-1252, School of In\u00adformatics, University of Edinburgh, 2008. [19] J. Sev.c.\u00b4ik. \nSafe optimisations for shared-memory concurrent programs. In PLDI, 2011. [20] D. Shasha and M. Snir. \nEf.cient and correct execution of parallel programs that share memory. ACM Transactions on Programming \nLanguages and Systems, 10(2), 1988. [21] F. Sheridan. Practical testing of a C99 compiler using output \ncompar\u00adison. Software: Practice and Experience, 37(14):1475 1488, 2007. [22] R. L. Solder. A general \ntest data generator for COBOL. In AFIPS Joint Computer Conferences, 1962. [23] A. Terekhov. Brief tentative \nexample x86 implementation for C/C++ memory model, 2008. http://www.decadent.org.uk/ pipermail/~cpp-threads/2008-December/001933.html. \n[24] X. Yang, Y. Chen, E. Eide, and J. Regehr. Finding and understanding bugs in C compilers. In PLDI, \n2011. [25] C. Zhao, Y. Xue, Q. Tao, L. Guo, and Z. Wang. Automated test program generation for an industrial \noptimizing compiler. In AST, 2009. [26] J. Zhao, S. Nagarakatte, M. M. K. Martin, and S. Zdancewic. For\u00admalizing \nthe LLVM intermediate representation for veri.ed program transformations. In POPL, 2012.  \n\t\t\t", "proc_id": "2491956", "abstract": "<p>Compilers sometimes generate correct sequential code but break the concurrency memory model of the programming language: these subtle compiler bugs are observable only when the miscompiled functions interact with concurrent contexts, making them particularly hard to detect. In this work we design a strategy to reduce the hard problem of hunting concurrency compiler bugs to differential testing of sequential code and build a tool that puts this strategy to work. Our first contribution is a theory of sound optimisations in the C11/C++11 memory model, covering most of the optimisations we have observed in real compilers and validating the claim that common compiler optimisations are sound in the C11/C++11 memory model. Our second contribution is to show how, building on this theory, concurrency compiler bugs can be identified by comparing the memory trace of compiled code against a reference memory trace for the source code. Our tool identified several mistaken write introductions and other unexpected behaviours in the latest release of the gcc compiler.</p>", "authors": [{"name": "Robin Morisset", "author_profile_id": "81479661074", "affiliation": "ENS &#38; INRIA, Paris, France", "person_id": "P4148990", "email_address": "robin.morisset@ens.fr", "orcid_id": ""}, {"name": "Pankaj Pawan", "author_profile_id": "81758976357", "affiliation": "IIT Kanpur &#38; INRIA, Paris, France", "person_id": "P4148991", "email_address": "pankajpawan5@gmail.com", "orcid_id": ""}, {"name": "Francesco Zappa Nardelli", "author_profile_id": "81453660213", "affiliation": "INRIA, Paris, France", "person_id": "P4148992", "email_address": "francesco.zappa_nardelli@inria.fr", "orcid_id": ""}], "doi_number": "10.1145/2491956.2491967", "year": "2013", "article_id": "2491967", "conference": "PLDI", "title": "Compiler testing via a theory of sound optimisations in the C11/C++11 memory model", "url": "http://dl.acm.org/citation.cfm?id=2491967"}