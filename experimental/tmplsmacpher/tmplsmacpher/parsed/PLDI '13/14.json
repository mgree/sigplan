{"article_publication_date": "06-16-2013", "fulltext": "\n CLAP: Recording Local Executions to Reproduce Concurrency Failures Jeff Huang Charles Zhang Julian \nDolby Hong Kong University of Science and Hong Kong University of Science and IBM Thomas J. Watson Research \nCenter Technology Technology dolby@us.ibm.com smhuang@cse.ust.hk charlesz@cse.ust.hk Abstract We present \nCLAP, a new technique to reproduce concurrency bugs. CLAP has two key steps. First, it logs thread local \nexecution paths at runtime. Second, of.ine, it computes memory dependencies that accord with the logged \nexecution and are able to reproduce the observed bug. The second step works by combining constraints \nfrom the thread paths and constraints based on a memory model, and computing an execution with a constraint \nsolver. CLAP has four major advantages. First, logging purely local ex\u00adecution of each thread is substantially \ncheaper than logging mem\u00adory interactions, which enables CLAP to be ef.cient compared to previous approaches. \nSecond, our logging does not require any syn\u00adchronization and hence with no added memory barriers or \nfences; this minimizes perturbation and missed bugs due to extra synchro\u00adnizations foreclosing certain \nracy behaviors. Third, since it uses no synchronization, we extend CLAP to work on a range of relaxed \nmemory models, such as TSO and PSO, in addition to sequential consistency. Fourth, CLAP can compute a \nmuch simpler execution than the original one, that reveals the bug with minimal thread con\u00adtext switches. \nTo mitigate the scalability issues, we also present an approach to parallelize constraint solving, which \ntheoretically scales our technique to programs with arbitrary execution length. Experimental results \non a variety of multithreaded benchmarks and real world concurrent applications validate these advantages \nby showing that our technique is effective in reproducing concurrency bugs even under relaxed memory \nmodels; furthermore, it is signif\u00adicantly more ef.cient than a state-of-the-art technique that records \nshared memory dependencies, reducing execution time overhead by 45% and log size by 88% on average. Categories \nand Subject Descriptors D.2.5 [Software Engineer\u00ading]: Testing and Debugging Diagnostics; Tracing; Symbolic \nex\u00adecution; Debugging aids General Terms Algorithms, Design, Performance, Theory Keywords Concurrency, \nBug Reproduction, Local Execution, Constraint Solving Permission to make digital or hard copies of all \nor part of this work for personal or classroom use is granted without fee provided that copies are not \nmade or distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. PLDI 13, June 16 19, 2013, Seattle, WA, USA. Copyright &#38;#169; \n2013 ACM 978-1-4503-2014-6/13/06. . . $15.00  Figure 1. CLAP technical overview 1. Introduction When \ndiagnosing the root cause of a concurrency bug, to be able to reproduce the bug is crucial but notoriously \ndif.cult due to the non\u00addeterministic memory races. Researchers have proposed a wide spectrum of techniques \nto address the bug reproduction problem. At one end, the deterministic record-replay techniques [12, \n14, 21, 22, 26, 32] faithfully capture shared memory dependencies online. At the other end, execution \nsynthesis techniques [36, 40] completely rely on of.ine analysis to search for shared memory dependencies \nwithout any runtime monitoring. Standing in between are several hybrid techniques [1, 23, 24, 29, 41] \nthat explore the right balance between online recording and of.ine search. Considering the production-like \nenvironments where we should minimize the diagnostic perturbation to the program execution, there are \nseveral critical drawbacks of the aforementioned bug re\u00adproduction techniques. First, the techniques \nthat introduce locks [14, 21, 22, 41] to track race orders often make the program run much slower (LEAP[14]>6x \nand Chimera[21]>2.4x for programs with heavy shared memory dependencies). What is worse is that they \ncan also exert the so-called Heisenberg effect by eliminating the concurrency bugs while trying to capture \nthem. The locks they insert act as memory barriers and can prevent the instruction re\u00adordering common \nto most modern commodity multiprocessors. Second, recording values traces [23, 24, 41] to match the shared \nloads and stores usually incurs a considerable program slowdown and a large disk space to store the logs \n(Lee et al. [23, 24] and Zhou et al. [41] reported average trace sizes from 2MB/s to 200MB/s). Third, \nthe complete of.ine analysis has limited bug reproduction capabilities due to the explosion of both the \nnumber of program paths and choices of thread schedules. In this work, we propose a new technique, CLAP, \nthat repro\u00adduces concurrency bugs without recording any shared memory de\u00adpendency nor any value or order \ninformation, and without intro\u00adducing any extra synchronization. Our key insight is to reduce this problem \ninto two well-known problems: monitoring thread local execution paths and solving symbolic constraints. \nSince these two problems have been studied for decades, many highly optimized solutions can be directly \nleveraged. For example, for path collec\u00adtion, ef.cient solutions are widely available on both software \nand hardware levels, such as the classical Ball-Larus path pro.ling al\u00adgorithm [4, 20] (around 30% overhead) \nand the hardware monitor\u00ading techniques based on branch predictors [17] and path descrip\u00adtors [31] (as \nlow as 0.6% overhead); for constraint solving, the SMT solvers such as Yices [8] and Z3 [7] are becoming \nincreas\u00adingly powerful with the advances of theorem provers and decision procedures.  As illustrated \nin Figure 1, CLAP has two key phases: 1. Monitoring an instrumented execution of the program. Un\u00adlike \nmost dynamic techniques that collect a global trace, this phase records only the local control-.ow choices \nof each thread. In threads that exhibit bugs, these local traces lead to the occurrence of the bug. \n2. Assembling a global execution that exhibits the bug. This phase in turns has several key steps:  \nFind all the possible shared data access points (called SAP -a read, write, or synchronization) on the \nthread local paths that may cause non-determinism, via a static escape analysis.  Compute the path conditions \nfor each thread with symbolic execution. Given the program input, the path conditions are all symbolic \nformulae with the unknown values read by the SAPs.  Encode all the other necessary execution constraints \n i.e., the bug manifestation, the synchronization order, the memory or\u00adder, and the read-write constraints \n into a set of formulae in terms of the symbolic value variables and the order variables.  Use a SMT \nsolver to solve the constraints, which computes a schedule represented by an ordering of all the SAPs, \nand this schedule is then used by an application-level thread scheduler to deterministically reproduce \nthe bug.   With thread local path monitoring and constraint solving, CLAP achieves several important \nadvances over previous approaches: 1. CLAP obviates logging of shared memory dependencies and program \nstates, and completely avoids adding extra synchroniza\u00adtions. This not only substantially reduces the \nlogging overhead compared to the shared memory recorders [14, 21, 41], but also minimizes the perturbation \nthat extra synchronizations foreclose certain racy behaviors. 2. CLAP not only works for sequential \nconsistent executions, but also for a range of relaxed memory models such as TSO and PSO [2]. We show \nthat the memory order constraints between SAPs can be correctly modeled to respect the memory model relaxation. \nThis is of tremendous importance, because it makes CLAP applica\u00adble for real production setting on commodity \nmultiprocessors that allows the reordering of instructions. 3. CLAP can produce simpler bug-reproducing \nschedules than the original one. We are able to encode preemption-bounding con\u00adstraints over the order \nof shared data accesses to always produce a schedule with the minimal number of thread context switches. \nWith this property, it becomes much easier to understand how the bug oc\u00adcurs due to the prolonged sequential \nreasoning [15, 16]. Moreover, through preemption-bounding, the complexity of constraint solv\u00ading is dramatically \nreduced from exponential to polynomial with respect to the execution length. 4. The constraint solving \nin CLAP is much easier to scale. The solver does not need to directly solve the complex path constraints \n(such as non-linear arithmetic or string constraints), but only to .nd a solution for the order variables \nthat satis.es the path con\u00adstraints. Thus, the solving task can be divided into two parts gen\u00aderating \ncandidate schedules (that respect the memory order) and validating them (using the other constraints). \nSince the .rst part which is searching possible executions does not have complex con\u00adstraints, and the \nsecond which does have complex constraints is fo\u00ad  cused on a single execution, our approach is easier \nthan traditional model checking. Moreover, observing that generating and validat\u00ading multiple candidate \nschedules can be done in parallel, we have also developed a parallel constraint solving algorithm, which \ntheo\u00adretically scales CLAP to programs with arbitrary execution length when there are suf.cient computation \ncores. We have implemented CLAP for C/C++ programs and evalu\u00adated it on a range of multithreaded benchmarks \nas well as several real world applications with intensive shared memory dependen\u00adcies. Our experimental \nresults show that CLAP is highly effective in reproducing concurrency bugs. CLAP was able to compute \ncor\u00adrect schedules that deterministically reproduce all of the evaluated bugs, and incurred only 9.3%-269% \nruntime overhead based on an extension of the Ball-Larus algorithm [4, 20] for collecting the thread \npaths. The constraint solving took 0.5s to 2280s for the se\u00adquential solver, while on an eight-core machine \nwith our parallel solving algorithm, it typically took much less time (0.2s-63s). The computed schedules \nby CLAP typically contain less than three pre\u00ademptive thread context switches, which is much easier to \nreason about for diagnosing the bug. Moreover, compared to a state of art record-replay technique LEAP \n[14], CLAP achieved signi.cantly smaller overhead in both runtime (with 10%-93.9% reduction) and space \n(with 72%-97.7% reduction). We highlight our contributions as follows: We present the design and implementation \nof CLAP, a new concurrency bug reproduction technique that computes shared memory dependencies through \nthread local path collection and constraint solving.  We present a sound modeling of the execution constraints \nwith respect to both the sequential consistent and TSO/PSO mod\u00adels. Any schedule that satis.es the constraints \nis guaranteed to reproduce the bug.  We formulate the thread context switches into the constraints, \nwhich enables CLAP to produce the bug-reproducing schedule with minimal thread context switches and also \nbound the search space of the solver to be polynomial to the execution length.  We present a parallel \nconstraint solving algorithm that scales CLAP to programs with arbitrary execution length in theory. \n We evaluate CLAP on a set of real world concurrent applica\u00adtions. The result demonstrates the ef.ciency \nand the effective\u00adness of our technique.  2. Overview We .rst provide an example to illustrate the key \nchallenges in reproducing concurrency bugs. We then show how CLAP works for the example and outline the \ncore constraint modeling. 2.1 Example Figure 2 shows an example with two threads accessing two shared \nvariables (x and y). Consider the two assertions asserte and assert@ at line 9 and line 18, respectively. \nOn the sequential con\u00adsistent (SC) model, asserte will be violated if the two threads ex\u00adecute following \nthe annotated interleaving 1-10-2-11-3-12-13-4 -5-14-9. However, this assertion violation is dif.cult \nto reproduce because the program contains more than 10000 different inter\u00adleavings [25, 27] and a slightly \ndifferent one may make the bug disappear. Worse, assert@ will never be violated under the SC model, but \ncan be violated under the PSO model that allows the reordering of writes to different memory addresses. \nFor example, suppose line 4 and line 5 are reordered, assert@ will be violated following the schedule \n1-10-2-11-3-12-5-13-14-4-18. For the state of the art bug reproduction solutions [14, 21, 32, 41], having \na small runtime perturbation is challenging, even for   Figure 2. Example: concurrency errors on sequential \nconsistent (left) and partial store order (right) memory models. Figure 3. CLAP constraint modeling \nof the example program in Figure 2. this simple example of less than 20 lines. For instance, to replay \n the order of the corresponding access to v in the to-be-computed either of the assertions, there are \nat least 12 race pairs that need schedule. For the value of each Write, given the program input, to be \ntracked. A more subtle but critical point is that the PSO bug it could be either a concrete value or \na value computed from the might never be captured if one is not careful enough in adding locks symbolic \nvalues of the Reads. To aid the presentation, we also use in tracking the race orders. The memory fencing \neffect of locks can a symbolic variable W i v to denote the value of Write to v at line i prevent the \nreordering in Figure 2 (right) to happen in test runs. in the example. And, if the runtime monitoring \nis disabled in production runs, the Figure 3(a) shows the path constraints. For instance, the con\u00ad bug \nwill surface to bite. Both of these two assertion violations can be reproduced by the 3straints for the \nviolation of are written as @ Rasserty y x x R12 > 0 . R18 > R18, meaning that, for this assertion to \nbe vi\u00ad > 0 . value-based approaches [23, 24, 36, 40], however, at the price of expensive logging of the \nvalue trace. Because both of the two buggy 3olated, both the value returned by the read of at line 3, \nRyy x that by the read of x at line 12, R12, should be larger than 0, and, of , and executions contain \n12 reads and writes, there are 12 corresponding value cells (one per each read/write) needed to be recorded \nand stored into the value logs at runtime. Yet, the search space of the shared memory dependencies based \non the value trace is still enormous, and the search problem has been shown to be still NP\u00ad xy course, \nthe value returned by the read of x at line 18, R18 be larger than that of the read of y at line 18, \nR18 . , should Figure 3(b) shows the read-write constraints. The idea is to match each Read with a corresponding \nWrite, following the rule that a read always returns the value by the most recent write (on complete \nin theory [11]. We next show how CLAP reproduces these two bugs without recording any race order or program \nstate and without using any extra synchronization. 1the same data). For example, consider the read of \nat line 1 (Rxxx x 13 1write access at line 13 (). If W Rx xx it may return either the initial value 0, \nor the value written by the before W 13, and we shall have the order constraint O1 < O13 ), returns 0, \nit should be executed . xx xx 1Ox Otherwise, if R1 returns the value by W 13 be executed after W 13 \nO13 . Therefore, as shown in the .rst row, the read-write (which is b+1), it should  2.2 CLAP In CLAP, \nwe formulate the problem of reproducing concurrency , and we shall have the order constraint > bugs as \na constraint solving problem, the goal of which is to com\u00ad pute a schedule (i.e., an ordering) of the \nshared data accesses in the execution such that the bug can be reproduced. For simplicity, 1constraint \nfor Rx x1. Ox > O13). x 1 1 13 1is written as () (. .R0 O< OR=x x x x =W 13 Figure 3(c) shows the memory \norder constraints, determined by we shall refer to such access to shared data as shared access point \n(SAP), the read-SAP as Read, and the write-SAP as Write. At run\u00adthe memory model. The memory order constraint \nfor SC is the same as the program order among all the per-thread SAPs. For time, CLAP logs only the execution \npath of each individual thread. Then, of.ine, CLAP performs symbolic execution along the thread 1 2 \n3instance, we have O< O< Oy x x meaning that the statement at line 1 should be executed before line in \nthe SC order constraints, paths to collect and encode all the necessary execution constraints over the \norder of the SAPs. During the symbolic execution, since the value returned by each Read is unknown, we \n.rst mark it by 2, and line 2 before line 3. For PSO, the memory order constraint is more relaxed compared \nto that of SC. Because reads and writes on different memory addresses are allowed to be re-ordered, the \na fresh symbolic value and later match it with a Write using the constraints. Figure 3 shows our constraint \nmodeling of the example program strict program order constraint is only applied to the SAPs on the 1 \n2same shared variable. For instance, we only have O< Ox x y x not O2 < O3, as line 2 and line 3 are accessing \ndifferent data. , but for both SC and PSO. There are two type of unknown variables: R the value of read \naccess to v (here v is x or y) at line i, and O i v i v  Figure 4. Two possible solutions returned \nby the solver for the PSO case. The .rst solution (top) is identical to the original sched\u00adule. The second \n(bottom) has the minimal thread context switches. Taking all these constraints, CLAP invokes a SMT solver \nto solve them. Figure 4 shows two possible solutions returned by the solver for the PSO case (the result \nfor SC is similar but simpler so we omit it). In the .rst solution (top), the computed schedule is identical \nto the original one. Following this schedule, at line 18, the values of x and y are 2 and 1, respectively, \nand the violation of assert@ can be reproduced. Better, in the second solution (bottom), the computed \nschedule has only four context switches, much fewer than that of the original schedule, but is still \nsuf.cient to reproduce the assertion violation. The power of our approach is that we can easily add additional \nconstraints to always produce a solution like the second one, which has the minimal number of thread \ncontext switches. We will present more details on this property in Section 4.2. The above example illustrates \nhow CLAP works in a nutshell. We next answer the following two important questions: How to correctly \nmodel all the execution constraints? For ex\u00adample, for presentation easiness, we do not have any synchro\u00adnization \nin the example program. How to model them? (\u00a73)  How dif.cult it is to solve the constraints? How to \nscale the constraint solving task in our approach? (\u00a74)  3. CLAP Execution Constraint Modeling From \na high level view, we encode all the necessary execution con\u00adstraints into a formula F containing two \ntype of unknown variables: V -the symbolic value variables denoting the value returned by Reads, and \nO -the order variables denoting the order of SAPs in the schedule. F is constructed by a conjunction \nof .ve sub-formulae: F = Fpath . Fbug . Fso . Frw . Fmo where Fpath denotes the path constraints, Fbug \nthe bug predicate, Fso the inter-thread order constraints determined by synchroniza\u00adtions, Frw read-write \nconstraints over Reads and Writes, and Fmo the memory order constraints determined by the memory model. \nThese constraints are complete because the intra-thread data and control dependencies are captured by \nFpath and Fbug, and the inter-thread dependencies are captured by Frw , Fso and Fmo. We next present \neach of the constraints in detail. 3.1 Intra-Thread Constraints The intra-thread constraints serve two \npurposes: they force every thread in the computed execution to follow the same control .ow as the corresponding \nthreads in the original execution, and they force Figure 5. An illustration of the synchronization constraints \nthe same bug to happen, i.e., the same assertion to fail. Forcing the same control .ow eliminates the \nsearch over all possible branches that common to the static techniques, thus simplifying the problem. \nPath Constraints (Fpath) The path constraints consist of the con\u00adjunction of the path conditions for \nall threads. The thread path con\u00additions are collected by a symbolic execution of the program fol\u00adlowing \nthe recorded thread path pro.les. On each branch instruc\u00adtion, a new constraint that speci.es the condition \nof the branch taken by the thread, is generated and added to the path constraint. Bug Manifestation Constraint \n(Fbug) In our modeling, the bug is not limited to a crash or a segfault, but general to all properties \nover the program state. Fbug is modeled as a predicate over the .nal program state. For example, a null \npointer dereference x.f() can be de.ned as Vx = N U LL, and a buffer over.ow error can be de.ned as Vlen \n> Vsize . In practice, the predicate could be extracted from the core dump when the program crashed, \nor from the program assertion when the assertion is violated at runtime, or from any other properties \nchecked at runtime.  3.2 Inter-Thread Constraints Inter-thread constraints are of two kinds: the synchronization \ncon\u00adstraints that govern the control .ow between multiple threads, and memory constraints that govern \nthe data .ow between threads. We cover the synchronization constraints and then the memory con\u00adstraints. \nFor the memory order constraints Fmo, we also prove that our approach is applicable to both SC and a \nrange of relaxed mem\u00adory models such as TSO and PSO. Synchronization Order Constraints (Fso) We model \nthe synchronization order constraints according to the locking and the partial order semantics. The locking \nsemantics de\u00adtermines that two sequences of Reads/Writes protected by the same lock should not be interleaved, \nwhile the partial order semantics de\u00adtermines that one SAP should always happen before the other. For \nexample, consider the program in Figure 5. The read of T1 cannot be mapped to the .rst write of T2, because \nit cannot be executed between the two writes due to the lock. The .rst read of T3 cannot be mapped to \nany of the two writes of T4, because it must happen before them due to the fork operation; similarly, \nthe second read of T3 cannot be mapped to the .rst write of T4, because the second write of T4 always \nexecutes between them due to the join operation. Fso is encoded as the conjunction of the locking constraints \nand the partial order constraints. We extract the synchronization operations for each thread and group \nthem by the lock, signal, or thread they operate on. We use the memory address in the symbolic execution \nto identify locks and signals. For thread objects, because each thread creates its children threads in \na deterministic order (i.e., following the program order), we create a consistent identi.cation for all \nthreads based on the parent-children order relationship. For example, suppose a thread ti forks its jth \nchild thread, this child thread is identi.ed as ti:j , and when the thread ti:j forks its kth child thread, \nthe new thread is then identi.ed as ti:j:k. Readers can .nd more details in our previous work [13]. \n Locking constraints The locking semantics is concerned with the lock and unlock operations only. For \neach lock object, we .rst extract the set of lock/unlock pairs that operate on it, following the program \norder locking semantics, i.e., an unlock operation is paired with the most recent lock operation on the \nsame lock by the same thread. For each lock/unlock pair, we then enumerate all the other pairs in the \nset and add their corresponding order constraints with the chosen pair. Speci.cally, let S denote the \nset of lock/unlock pairs on a certain lock and consider a pair al/au. The constraint is written as follows: \n Oau < Oa . l .a/a .S l u (Oal > Oa .Oa > Oau . Oa < Oa ) u lul .a/a .S .a/a .S lu lu The constraint \nabove states that the lock operation al acquires either the initial lock, or the lock released by another \nunlock operation f au. In the .rst case, the order of the unlock operation au should be smaller than \nthat of all the other unlock operations. In the second f case, the order of al should be larger than \nthat of au, and for any ffffff other lock/unlock pair al /au, either the order of al is larger than fff \nthat of au, or the order of au is smaller than that of al. The total size of the locking constraints \nfor each lock object is 2|S|2 + 2|S|. Partial order constraints The partial order semantics is related \nto the thread fork/join and wait/signal operations. For fork and join, their order constraint is simple \nto model, because they can only be mapped to one unique operation. For fork, it is the corresponding \nstart operation of the newly forked thread, and for join, it is the exit operation of the joined thread. \nTherefore, we can simply add the constraints that the order of a fork operation is smaller than that \nof its corresponding start operation. Similarly, the order of a join operation is larger than that of \nits corresponding exit operation. For wait and signal operations, the constraint is slightly more complex. \nBecause a wait operation could have multiple candidate signal operations that it could be mapped to, \nwe have to enumerate all the candidates. Also, because a signal operation can only signal at most one \nwait operation, we need to constrain the number of operations a signal can be mapped to. To model this \nconstraint, we introduce a set of binary variables for each signal operation. Each binary variable denotes \nwhether the signal operation is mapped to a wait operation or not. We then constrain the sum of these \nbinary variables to be less than or equal to one. Consider a wait operation awt , and let S G denote \nthe set of signal operations that operate on the same signal variable as that of awt and by a thread \ndifferent from that of awt . We model the constraint as follows: asg asg (Oasg < Oawt . b= 1)bx = 1 \nawt .asg.SG x.W T In the constraint above, W T denotes the set of wait operations the signal operation \nasg can be mapped to, and bx asg the binary variable that indicates whether asg is mapped to a wait operation, \nx, or not. The total size of the constraints is 2|SG||W T | + |SG|. Memory Model Constraints A crucial \nfactor of the constraint modeling is the memory model under which the buggy execution occurred, as it \ndetermines what values each Read could return. The memory model is a parameter to our system in the sense \nthat we take a declarative speci.cation of the memory model, and combine it with the concrete program \nactions from the trace to produce a set of constraints that determine which reads can see values from \nwhich writes in the program. Con\u00adceptually, this approach is taken from previous work like MemSAT [3]; \nindeed, we could employ speci.cations in that style directly, since our thread path constraints and SAPs \ncorrespond to the ele\u00adments of that model. However, in this work, we are focusing on memory models for \nbus-based shared memory machines in which there is a global order among memory operations as they appear \nto the main memory. This allows us to simplify and use O as a total order; however, this is not an inherent \nlimit and we could employ other kinds of models directly. Speci.cally, in CLAP, we currently implement \nSequential Consistency (SC), Total Store Order (TSO), and Partial Store Order (PSO). We next discuss \nhow Reads and Writes are constrained with respect to the memory order O, and then how O is constrained \nto model SC, TSO and PSO. Read-Write Constraints (Frw ) For a Read, it may be mapped to a Write by the \nsame or a different thread, depending on the order relation between the Writes. Consider a Read r on \na shared variable s, and let W denote the set of Writes on s. We use Or to denote the order of r, Vr \nthe value returned by r, and Owi the order of the Write wi in W . Frw is written as: (Vr = wi . Owi \n< OrOwj < Owi . Owj > Or) .wi.W .wj= wi The constraint above states that, if a Read is mapped to a Write, \nfor this Write, its order is smaller than that of the Read, and there is no other Write that is between \nthem. In our constraint construction, we .rst group all the Reads/Writes by the accessed memory address, \nand then encode the read-write constraint for each Read. Let Nr and Nw denote the number of Reads and \nWrites on a certain shared address, the size of the read-write constraints is 4Nr w, which is N2 polynomial \nto the size of SAPs. Note that this constraint is identical to the corresponding constraints in [3], \nwith the change of O to be a single global order. Memory order constraints (Fmo) For the sequential consistent \nmemory model, the memory order for SAPs is the same as the program order. Consider two consecutive SAPs \na and b by a thread t in the program order, their order constraint is written as Oa < Ob. We encode this \nconstraint for each pair of successive accesses by each thread, following the recorded path pro.le. The \nsize of the constraints is linear to the total number of SAPs in the execution. For TSO, it does not \nrequire the program order and allows re\u00adordering Read and Write on different addresses. The hard con\u00adstraint \nis that the original program order among all the Writes and among all the Reads is preserved. Hence, \nfor all Writes and for all Reads by the same thread, we model the same order relation as the program \norder. In addition, for Reads, we need to make sure each Read returns the value written by the most recent \nWrite on the same data. Therefore, for each Read, we .rst .nd the two Writes that 1) access the same \naddress as that by the Read, and 2) are immediately before and after the Read, respectively, in the program \norder. We then model the order between the Read and the two Writes to be the same as that in the program \norder. Compared to TSO, PSO further relaxes the order relation between Writes and between Reads on different \naddresses. Our constraint model for PSO is hence similar to TSO, except that the order relation between \nWrites and between Reads on different addresses are removed. Soundness With respect to different models, \nalthough the execu\u00adtion order of the SAPs by each thread may or may not be the same as the program order, \nwe prove that our approach of computing a schedule among the SAPs is sound to SC/TSO/PSO memory mod\u00adels. \nMore formally, we have the following theorem: Theorem 1. For SC/TSO/PSO, there always exists a schedule \nof the SAPs in which each load return the value by the most recent store to the same address, and following \nwhich the same program state can be achieved as that of the original buggy execution.  Proof For all \nthese models, once a store is visible to a second pro\u00adcessor, it is instantly visible to all processors. \nIt is impossible for two processors to observe different orders for any pair of stores. Hence, there \nalways exists a total ordering of all the stores under these models. For loads, their order in the schedule \ncan be deter\u00admined by placing them after the stores whose value they accessed in the original execution \nand before the subsequent store, such that a load always return the value by the most recent store. 4. \nConstraint Complexity and Scalable Solving Our constraint solving task is much easier than conventional \nones involving complex constraints such as strings and non-linear math\u00adematics. In CLAP, the solver only \nneeds to compute a solution for the order variables that essentially maps each Read to a certain Write \nin a discrete .nite domain (i.e., the set of Writes on the same data as that of the Read), subject to \nthe order constraints. To fur\u00adther scale CLAP, we have also developed two core techniques to improve \nthe performance of constraint solving: preemption bound\u00ading and parallel solving. We next conduct a brief \nanalysis of the constraint complexity, followed by the detailed discussion of the two techniques. 4.1 \nConstraint Complexity Let Nbr denote the number of conditional branching instructions in the execution. \nLet Nsync denote the number of synchronizations. And let Nsap, Nr, and Nw denote the number of SAPs, \nread-SAPs, and write-SAPs, respectively. Among the synchronizations, let Nl the number of lock/unlock \npairs, and Nwt /Nsg the number of wait/signal operations. Recall Section 3 that our constraint formulae \nconsist of the path constraints Fpath, the bug manifestation constraint Fbug, the syn\u00adchronization constraints \nFso, the read-write constraints Frw , and the memory order constraints Fmo. The size of Fpath is equal \nto Nbr because each branching instruction generates a new conjunc\u00adtion clause over the symbolic value \nvariables. Recall Section 3.2, the size of Fso is 2Nl 2+2Nl+2NsgNwt +Nsg; assuming all SAPs are accessing \na single shared variable, the worst case size of Frw is 4Nr w, and the worst case size of Fmo is equal \nto Nsap+Nsync N2 (for all the three memory models). Because Nsap =Nr+Nw+Nsync and normally Nsap > Nsync \n, the size of Frw is far larger than Fmo and Fso. Therefore, the total size of the constraints can be \napproximated as Nbr+N3 sap. In sum, the worst case complexity of our constraints is linear to the number \nof conditional branches and cubic to the number of shared data accesses in the execution. 4.2 Thread \nContext Switch Constraint Researchers have observed that most real world concurrency bugs can be manifested \nby a small number of thread context switches [28]. In CLAP, this observation can be directly encoded \nas addi\u00adtional constraints to bound the search space of the solver. Recall that each SAP is assigned \nwith an order variable, repre\u00adsenting its position in the computed schedule. Our basic idea is to use \nthe order difference between consecutive SAPs within the same thread to determine whether a context switch \noccurs between them. If the execution of two consecutive SAPs is not interleaved by other threads, their \norder difference will be equal to one; otherwise, the difference will be larger. To obviate modeling \nthe non-preemptive context switches (as they always occur) and to create a uniform constraint for different \nmemory models, we group a sequence of SAPs into segments, and use the number of interleaved segments \nto approximate the size of real context switches. This is a good ap\u00adproximation in practice because the \ncontext switch number is often small. We .rst extract all the synchronization operations that cause non-preemptive \ncontext switches, including wait, join, yield, and exit. For brevity, we call them must-interleave operations. \nWe use these operations to divide the SAPs into segments for each thread. Each segment contains only \none must-interleave operation which leads or ends the segment. Note that the must-interleave opera\u00adtions \nare not allowed to be reordered (because they are synchro\u00adnizations). In the .nal schedule, the leading \n(ending) operation in each segment will always have the smallest (largest) order among all the SAPs in \nthe same segment. For each segment, we then use the difference between the orders of the ending and the \nleading op\u00aderations to determine whether a context switch occurs or not in the segment. Let St denote \nthe set of segments by thread t, al and ae the leading and ending SAP in a segment s, and Ncs the speci.ed \ncontext switch bound. The constraint is written as: 1 if Oae - Oal > |s| - 1 St.T Ss.St= Ncs 0 otherwise. \nThe above formula states that the total number of interleaved segments for all threads is bounded by \nNcs. Minimal thread context switches The method above not only bounds the search space, but can also \nbe used to produce the schedules with minimal number of context switches. Speci.cally, we can start from \nthe constraint with zero thread context switch, and increment the context switch number when the solver \nfails to return a solution. We repeat this process until a solution is found. In this way, we can always \nproduce a schedule with the fewest thread context switches among all the bug-reproducing schedules. \n 4.3 Parallel Constraint Solving Algorithm Our core idea to parallelize the constraint solving is to \ntreat the memory order constraints Fmo separately from the other con\u00adstraints. We .rst generate candidate \nschedules that satisfy Fmo, and then employ the solver to validate each candidate schedule, i.e., checking \nif it satis.es all the other constraints. This method has two salient features. First, we can generate \ndifferent schedules and validate them in parallel. Each single schedule generation and val\u00adidation is \nindependent and fast (requiring only a linear scan of the SAPs and the constraints). The whole constraint \nsolving task can then be divided into many independent subtasks that each works on a candidate schedule. \nSecond, we can generate the schedules with the increasing number of context switches, allowing us to \nbound the search space similar to that of the bounded dynamic schedule exploration approaches [27, 28]. \nA key challenge in this approach is how to avoid generating duplicated schedules. To address this problem, \nwe represent the context switches in a schedule by a set of context switching points (CSP). A CSP is \na location in the schedule where a context switch occurs. It can be uniquely identi.ed in an ordered \nway by a triple (t1,k,t2), denoting that a thread t1 is interleaved by another thread t2 immediately \nbefore the kth SAP of t1. Different CSPs are then combined together into a CSP set, and the set is used \nto guide the schedule generation procedure. Preemption-bounded schedule generation Given a context switch \nnumber c (= 0,1,2,. . . ), we .rst generate all the CSP sets of size c. For each CSP set (including the \nempty set), we fork a separate process that generates the corresponding schedules. Each generated schedule \nis then validated by the solver to determine its correctness. Our algorithms for generating schedules \nfor SC and for TSO/PSO are mostly the same with only slight difference. The complexity of our algorithm \nis linear to the total number of SAPs. The size of the t t schedules is bounded by N c (N + c)!, where \nc is the number of context switches and N is the total number of SAPs.  SC We associate each thread \nwith a stack that contains the SAPs of the thread in the program order. Starting from the main thread, \neach schedule-generation process attempts to pop up one SAP from the thread s stack, subject to a condition: \nthe input set S contains no CSP (t1,k,t2) that matches the current thread (t) and the current SAP (i). \nIf the condition is not met (t = t1 and i = k), it means that a context switch should happen at this \npoint, so we jump to the stack of the interleaving thread (t2). This process is repeated until all SAPs \nare popped up. Note that each process usually generates more than one sched\u00adule. This is because, when \nthe stack of the current thread becomes empty, we have multiple remaining threads to jump to and each \nchoice will generate a different schedule. In that case, we remove the current thread and fork multiple \nchildren processes to continue the schedule generation. Each child process jumps to the stack of a different \nthread in the remaining thread set and repeat the process. TSO/PSO As Reads/Writes are allowed to be \nreordered in TSO and PSO models, we can no longer use a stack to represent the memory order constraints. \nInstead, we use a tree structure (called SAP-tree) to represent the order relation between SAPs for each \nthread. Each node in the SAP-tree represents a SAP and the parent\u00adchild relation between nodes represents \na store-load data depen\u00addence or a memory-model determined order relation. In TSO, the parent-child relation \nis applied on all Writes in the program order, while in PSO it is applied on the Writes for each memory \naddress. Similar to that of SC, the schedule generation process for TSO/PSO repeatedly removes an ancestor \nnode (which has no parent) from a SAP-tree, until the SAP-trees for all threads be\u00adcome empty. Whenever \nthere are multiple ancestor nodes in the tree (because an ancestor node with multiple children nodes \nmay be removed in the previous step), we fork the same number of sub\u00adprocesses each of which continues \nthe schedule generation starting with one of the ancestor nodes. 5. Implementation We have implemented \nCLAP on top of LLVM and KLEE-2.9 [5] with the STP solver [10]. To adapt KLEE to CLAP, we made four key \nmodi.cations. First, KLEE works only with sequential pro\u00adgrams, thus we extended it to support multiple \nthreads. Speci.cally, we modi.ed KLEE to spawn a new instance for each new thread and added the necessary \nextension to uniquely identify threads (see Section 3). Second, we changed KLEE to only follow the recorded \npath pro.le for each thread, and to only collect the path constraints without solving them. At the end \nof the run, we unify the con\u00adstraints collected from each thread as the path constraints. Third, we adapted \nKLEE to return a new symbolic value for the load in\u00adstructions that access shared memory addresses. Fourth, \nwe modi\u00ad.ed the constraint solving utility, Kleaver, to incorporate the path constraints with the other \nexecution constraints. We wrote our own constraint generation engine based on the SAPs collected from \nthe thread local paths during the symbolic analysis phase. Thread Local Path Collection Path monitoring \nis a pluggable component in our approach. Ideally, we can use hardware moni\u00adtoring techniques such as \nthe work of Vaswani et al. [31] which has negligible (around 0.6%) overhead. Our current implementa\u00adtion \nis an extension of the classical Ball-Larus algorithm [4, 20] based on a LLVM function pass. It works \nfor multithreaded C/C++ programs that use PThreads and incurs 9.3%-269% runtime over\u00adhead in our experiment. \nFrom a high level view, we break the whole path into a sequence of segments, each of which is a Ball-Larus \n(BL) pro.le. A new segment starts when a new function is called, or an intra-procedural path is re-entered. \nWe analyze the control .ow graph of each function and insert instrumentations at the fol\u00adlowing points: \nthe entrance/exit of each function, the beginning of basic blocks that have a back edge, and the branching \npoints an\u00adalyzed by the Ball-Larus algorithm. Each function is uniquely la\u00adbeled (represented by a number), \nand each BL path inside the same function is also uniquely labeled (computed as the sum of the en\u00adcoded \nedge weights at runtime). During the execution, we collect the whole path pro.le for each thread by recording \nthe sequence of the labels for the function calls and for the BL paths. To distinguish between the BL \npaths that have the same label but are in differ\u00adent functions, we also log the exit of each function \nto demarcate the sequence of BL paths. The recorded labels are then decoded to produce the whole path \npro.le, and to guide our symbolic analysis. Shared Memory Access Identi.cation Identifying shared data \naccesses is orthogonal to our approach but important for reduc\u00ading the size of the constraints. Naively, \nwe would mark all loads and stores as shared data accesses. This would produce a huge amount of unnecessary \nconstraints, since the constraints with only the thread local data accesses are essentially redundant. \nA better way is to detect shared data accesses at runtime, where the accessed address is available for \nevery memory operation. To overcome the virtual address recycling issue, we also need to record malloc/free \noperations to identify truly shared memory locations. However, this method does not .t our design as \nit inevitably causes additional pro\u00adgram slowdown. In CLAP, we perform a static thread sharing anal\u00adysis \nbased on the Locksmith [30] race detector to identify shared variable accesses, including also the treatment \nof shared pointers and heap locations. Though being conservative, it is very effective to determine thread-local \nlocations and, more importantly, does not introduce any runtime cost. Deterministic Bug Reproduction \nOur application level thread scheduler is implemented based on Tinertia [16]. We add in\u00adstrumentations \nbefore each SAP in the program and employ the dynamic thread interpolation to intercept PThread library \ncalls. Through the inserted instrumentations, we are able to enforce the thread execution order of the \nSAPs to strictly follow that of the computed bug-reproducing schedule. Whenever a thread is going to \nexecute a SAP, we .rst check the schedule to decide whether it is the correct turn for the thread to \ncontinue execution. If not, we put the thread in a postponed thread queue and make it wait until all \nthe SAPs before it have been executed. Challenges and Treatments External Function Calls A known challenge \nin symbolic execu\u00adtion is the existence of external function calls that make the path constraints incomplete. \nAlthough KLEE tried its best to simulate the external environments (i.e., .le systems and system libraries), \nit still suffers from this problem when facing unresolved external calls. However, this problem is not \nfundamental to CLAP because, for bug reproduction, we can record the runtime input/return values of all \nthe external calls, and use the value pairs to construct the con\u00adstraints of the external interfaces. \nA negative side of this method is that it would over-approximate the behavior of the external func\u00adtions, \nwhich reduces the scheduling space explored by CLAP. In practice, to avoid limiting the capability of \nCLAP too much, we choose to .rst resolve the external functions as much as possible. In addition, we \ntry to avoid .agging as symbolic the variables that have value .ows to unresolved external calls. In \nour experiments, the external functions are seldom related to symbolic variables, and we did not face \nmuch dif.culty in this problem. Symbolic Address Resolution Another known issue in symbolic execution \nis the resolution of symbolic memory addresses. As KLEE does not perform any pointer tracking, when facing \nreads or writes to symbolic addresses, it would exhaustively search all allocated objects and forks execution \nfor each possible base object. This usually takes a long time and also produces quite a number of unnecessary \nmemory states. In CLAP, since the symbolic analy\u00adsis phase follows the recorded thread execution path, \nwe only need to explore one memory state for each thread. To avoid the state explosion, we choose to \ndelay the object resolution for symbolic addresses to the constraint solving phase. Speci.cally, during \nsym\u00adbolic execution, we keep track of the base object for each mem\u00adory operation. For each base object, \nwe maintain an ordered list of writes to symbolic addresses performed so far. Each write is re\u00admembered \nas a pair consisting of the location that was updated, and the expression that was written to that location. \nFor any subsequent read, the loaded value is resolved from the ordered list with a set of constraints. \nAs an example, suppose the ordered list of writes to an array is (a[0] = 0, a[1] = 0, . . . , a[n] = \n0, a[i1] = x, a[i2] = y). For a read b = a[j], we create the constraint (j = i2.b = y).(j= i2 . j = i1 \n. b = x) . (j= i2 . j i1 . b = 0), and add it to  = the path constraints. Input Non-determinism Currently, \nCLAP assumes that the pro\u00adgram input is deterministic. CLAP does not record the program in\u00adput as we \nmainly address the problem of scheduling non-determinism in this work (which is more signi.cant and dif.cult). \nIf the program input is non-deterministic, CLAP might not be able to reproduce the bug. Nevertheless, \nsimilar to the treatment of external func\u00adtions, this problem can be addressed by recording and enforcing \nthe same input value during the bug reproduction execution. 6. Experiments We have evaluated CLAP on \na variety of real world multithreaded C/C++ applications with known or seeded bugs collected from [16, \n39], including pbzip2-0.9.4, a parallel implementation of bzip; aget-0.4.1, a parallel FTP/HTTP downloading \nutility; ctrace, a multithreaded tracing library; pfscan, a parallel .le scanner; swarm, a parallel sort \nimplementation; bbuf, a shared bounded buffer im\u00adplementation; and the apache-2.2.9 web server. To assess \nthe limit of CLAP, we also examined with simp race, a simple racey pro\u00adgram [16], and racey, a benchmark \nfor deterministic replay systems [38]. To evaluate CLAP for reproducing bugs on relaxed memory models, \nwe also examined with the implementations of three clas\u00adsical mutual exclusion algorithms -dekker, bakery \nand peterson. Setup CLAP works in three phases: 1) online path collection; 2) of.ine constraint generation \nand solving; 3) bug replay execu\u00adtion. In phase 1, the thread paths are dumped to disk when the bug occurs. \nDue to the rare erroneous thread interleaving, most concurrency bugs are dif.cult to manifest. To trigger \nthe bug in our experiment, we typically inserted timing delays at key places in the program and ran it \nmany times until the bug occurred, and we added the corresponding assertion to denote the bug manifes\u00adtation. \nIn phase 2, the constructed constraints were .rst saved to a .le which is then provided to the solver \nto compute a schedule. In phase 3, CLAP enforced the replay to follow the computed sched\u00adule to reproduce \nthe bug. All our experiments were conducted on an eight-core 3GHz machine with 16GB memory and Linux \n2.6.22. 6.1 CLAP Bug Reproduction Effectiveness Table 1 summarizes our experimental results. Overall, \nCLAP is highly effective in reproducing concurrency bugs. For all the eleven evaluated bugs and injected \nviolations including three re\u00adlaxed memory model related failures, CLAP is able to reproduce all of them \nby producing a schedule with a small number of thread context switches. Most of the computed schedules \ncontain less than three preemptive context switches, except for the racey benchmark, in which at least \n276 context switches are needed to reproduce the injected violation. The size of the constraints range \nfrom 341 to more than 400M clauses with 26 to 2M unknown variables, and the total time for constructing \nand solving these constraints ranges from 2s to around 50 mins. We next discuss the results for several \ninteresting applications in detail. pbzip2-0.9.4 contains a known order violation bug frequently studied \nin concurrency defect analysis techniques [16, 18, 19, 35, 39]. The main thread communicates with a set \nof consumer threads through a FIFO queue with a mutex protecting the data accesses. The bug occurs intermittently \nwhen the main thread nulli.es the mutex before some consumer threads are still using it, causing program \ncrashes. The buggy execution contains four threads com\u00adpressing a 80K .le. There are 18 variables identi.ed \nas shared (including all the global variables and the variables associated with the FIFO queue) by the \nstatic escape analysis [30] and are marked as symbolic. Upon crash, it executed a total number of 4K \ninstruc\u00adtions with 65 SAPs and 473 conditional branches (excluding calls to external libraries). It took \nCLAP 4s to construct the symbolic constraints, containing 102 unknown variables and 5K clauses. The STP \nsolver took around 5s to compute a bug-reproducing schedule with two preemptive thread context switches. \napache-2.2.9 bug #45605 is a multi-variable atomicity violation between a set of listener threads and \nworker threads on accessing a shared queue data structure, causing an assertion violation that .nally \ncrashes the server. We started four clients to simultaneously send a number of requests until the assertion \nwas violated. The col\u00adlected path pro.les contain 28 threads executing a total number of 6.8M instructions \nwith 962K branches. We identi.ed a total num\u00adber of 22 variables as shared symbolic variables in the \nbuggy run. To bound the search space, we constrained the size of preemptive thread context switches to \nbe less than three. It took CLAP around 13 minutes to collect and encode the execution constraints, which \ncontain 81K unknown variables and 10M clauses. CLAP solved the constraints in 344s and produced a bug-reproducing \nschedule with three preemptive thread context switches. racey is a specially designed benchmark [38] \nwith numerous in\u00adtentional races that make it very likely to produce a different output if a different \nrace occurs. We assessed the bug reproduction capa\u00adbility of CLAP by applying it to reproduce the same \noutput in racey. There are three shared variables: a 64-union array upon which the core computation between \nthreads is operated, and two volatile variables for coordinating the start of the threads. To avoid out \nof memory error, we set the loop iterations (MAX LOOP) to 500000. The test execution contained 3 threads, \n7.1M branches, 93M in\u00adstructions, and 1.36M SAPs. It took CLAP around 15 minutes to collect the constraints \ncontaining 513M clauses and 2M unknown variables. CLAP took 38 minutes to solve the constraints and suc\u00adcessfully \ncomputed a schedule with 276 context switches. Since racey is a benchmark speci.cally designed to have \nmany races, it is an outlier in our data. It does not follow the observation that most bugs require few \nthread switches to reproduce, and hence it causes worst-case behavior in our system. Note that real programs \nhave much better results. Relaxed memory model bugs Dekker s and Peterson s algorithms, and Lamport s \nBakery algo\u00adrithm all work well for the SC model, but not for TSO and PSO models. We evaluated with them \nto demonstrate the capability of CLAP for reproducing concurrency bugs on relaxed memory mod\u00adels. To \ntrigger the bugs, we simulated the memory model effects by actively controlling the value returned by \nshared data loads in a similar style to [9]. For TSO, we simulated a FIFO store buffer for each thread, \nand for PSO, we simulated multiple FIFO store buffers, with one per shared variable. Bakery We forked \nfour worker threads, each of which increments a shared integer variable by one in the critical section. \nDue to the Table 1. Overall results -Columns 3-6 report the number of threads (#Threads), the number \nof shared variables (#SV), the number of executed instructions (#Inst) and branches (#Br) in the original \nbuggy execution. Columns 7 reports the number of shared data accesses (#SAPs) in the schedule. Columns \n8-9 report the size of the constraints (#Constraints) and the number of unknown variables in it (#Variables). \nColumns 10-11 report the symbolic analysis time (Time-symbolic) for collecting the constraints, and the \nconstraint solving time (Time-solve) using the STP solver on a single core. For all the evaluated programs, \nCLAP was able to compute a bug-reproducing schedule, and most of the computed schedules contain less \nthan 3 context switches. The total time for collecting and solving the constraints ranges from 2s to \n50mins.  Program LOC #Threads #SV #Inst #Br #SAPs #Constraints #Variables Time (secs) symbolic solve \n#cs success? sim race 75 5 2 103 18 23 341 26 0.8 0.5 0 Y pbzip2 1.8K 4 18 4203 473 65 5324 102 4 5 2 \nY aget 1.2K 4 30 39401 4017 1951 1366923 2485 125 89 1 Y bbuf 381 5 11 2643 189 64 2784 75 3 5 1 Y swarm \n2.2K 3 13 840193 73728 1265 1226098 1776 27 64 2 Y pfscan 925 3 13 2261926 287713 2864 7255156 3101 332 \n160 3 Y apache 643K 28 22 6806939 961779 81237 10153562 15534 770 344 3 Y racey 200 3 3 93035842 7133586 \n1361588 513086300 2010082 857 2280 276 Y bakery 73 5 3 1181 182 218 16355 331 1 8 1 Y dekker 48 3 3 223 \n28 39 699 52 1 2 1 Y peterson 44 3 3 215 28 35 696 48 1 2 1 Y bug, the mutual exclusion for executing \nthe critical section does not hold, and threads accessing the shared variable can race with each other, \nwhich may produce a wrong .nal result of the shared integer. The recorded path pro.les of the buggy execution \ncontain 5 threads with a total of 1181 instructions and 182 branches. It took CLAP less than 1s to collect \nthe constraints that contain 331 unknown variables and 16K clauses for both TSO and PSO. CLAP solved \nthe constraints in 8s and produced a bug-reproducing schedule with one preemptive thread context switch. \nDekker/Perterson We forked two threads each of which loops twice in the critical section for incrementing \na shared integer. Due to the bug, the .nal result of the shared integer could be wrong. The recorded \npath pro.les of dekker contain 3 threads with a total of 223 instructions and 28 branches. It took CLAP \nless than 1s to collect the constraints that contain 52 unknown variables and 699 clauses for both TSO \nand PSO. CLAP solved the constraints in 2s and produced a bug-reproducing schedule with one preemptive \nthread context switch. The result for peterson is similar, as Peterson s algorithm is a slightly simpli.ed \nversion over Dekker s algorithm.  6.2 CLAP Runtime Performance We compared the runtime performance of \nCLAP with an imple\u00admentation of LEAP [14], which is one of the state of art record\u00adreplay techniques \nthat track shared memory dependencies. Because none of the concurrency bug reproduction tools [1, 21, \n29, 32, 40, 41] is available, we choose LEAP for the reason that it incurs min\u00adimum implementation bias \nand it puts the quanti.cations of vari\u00adous runtime characteristics of CLAP into perspective. We ran each \nbenchmark under three different settings natively (without instru\u00admentation and logging), with LEAP, \nand with CLAP, and we mea\u00adsured the corresponding execution time and log size. Table 2 reports the results. \nAll data were average over .ve runs. As expected, since LEAP requires synchronizations to record the \nshared variable access orders, its overhead is large when there are intensive shared memory dependencies \nin the execution. Because most of these benchmarks have frequent shared data accesses (es\u00adpecially racey, \nin which the majority of memory operations are on shared data), both the runtime overhead for recording \nthe shared data accesses and the space needed for storing the log are signi.\u00adcant for LEAP. The runtime \noverhead of LEAP ranges from 21.4% in pbzip2 to as large as 4289% in racey, and the corresponding space \ncost ranges from 19.5K-68.2M. Compared to LEAP, CLAP incurred much less runtime overhead and space cost, \nbecause it only records the thread local paths and does not use any synchro- Program #worst Schedules \n#gen(#cs) #good Time par seq sim race > 106 128(0) 3 0.3s 0.5s pbzip2 > 1015 140(2) 8 0.3s 5s aget > \n1040 13725(1) 16 16s 89s bbuf > 1020 324(1) 36 0.6s 5s swarm > 1030 30855(2) 27 19s 64s pfscan > 1050 \n118714(3) 12 63s 160s apache > 10100 5634627(3) 15 195s 344s racey > 1010000 2528316(2) 0 2h 2280s bakery \n> 1025 1071(1) 22 1.5s 8s dekker > 106 31(1) 12 0.2s 2s peterson > 106 28(1) 11 0.2s 2s Table 3. The \nperformance of parallel constraint solving nization. The runtime overhead of CLAP ranges from 9.3%-269%, \nwhich achieves 10% to 93.9% reduction compared to LEAP. For the space cost (1.1K-3.81M), the improvement \nby CLAP is also signi.cant, with 72% to 97.7% reduction compared to LEAP.  6.3 CLAP Parallel Constraint \nSolving Performance We have evaluated the performance of our parallel constraint solv\u00ading algorithm on \nan eight-core machine. We repeated the schedule generation process with a larger context switch number \n(recall Sec\u00adtion 4.3) until we found at least one correct schedule that satis.es the constraints. Each \ntask of generating and validating one sched\u00adule is handled by a separate thread. Because normally there \nexist a set of correct schedules and multiple threads may work on them simultaneously, we typically have \nfound multiple correct schedules before the whole process is terminated. Table 3 reports the results. \nColumns 2-4 report the worst num\u00adber of possible schedules (computed according to the theoretical results \nin [25, 27]), the number of generated schedules before we stopped (#cs the largest number of context \nswitches among these schedules), and the number of correct schedules among the gen\u00aderated ones. Column \n5 reports the total amount of time it took for .nding these correct schedules. As the table shows, although \nthe worst number of different schedules is exponential, CLAP suc\u00adcessfully generated correct schedules \nfor most of the benchmarks within 200s. For comparison, Column 6 shows the constraint solving time of \nthe sequential version (as also reported in Table 1). For most benchmarks, using our parallel algorithm \nis much faster than the  Program Native LEAP (Overhead%) Time CLAP (Overhead%) Reduction% LEAP Space \nCLAP Reduction% sim race 2ms 4ms (-) 4ms (-) - 448B 126B .72% bbuf 2ms 6ms (-) 4ms (-) .33% 12.2K 1.1K \n.91% swarm 68ms 0.770s (1032%) 0.101s (48.5%) .87% 9.20M 215.6K .97.7% pbzip2 0.140s 0.170ms (21.4%) \n0.153ms (9.3%) .10% 19.5K 1.8K .91% aget 0.231s 0.490s (112%) 0.270s (17%) .45% 683.8K 24.3K .96.4% pfscan \n0.135s 1.537s (1172%) 0.260s (92.6%) .83.1% 1.61M 330.5K .79.5% apache 0.185s 0.248s (34%) 0.220 (19%) \n.11.3% 15.4M 2.30M .85% racey 0.262s 11.5s (4289%) 0.705 (269%) .93.9% 68.2M 3.81M .94.4% Table 2. Runtime \nand space overhead comparison between CLAP and LEAP sequential solution. For example, for the constraint \nof apache, it took the sequential solver 344s to return a correct schedule, while with the parallel algorithm, \nCLAP generated 27 correct schedules in 195 seconds. The only exception is racey, which we use as a stress \ntest for worst case behavior. Using the parallel algorithm, we did not .nd a correct schedule within \ntwo hours. The reason is that a correct schedule for racey should contain at least 276 thread context \nswitches, but in two hours we did not even .nish enumerating the schedules with only two context switches, \ndue to the large number of shared data accesses in the execution.  6.4 Discussion Long running traces \nOur evaluation results demonstrate that CLAP has good scalability with fairly substantial traces in real\u00adworld \nexecutions. For very long runs, reproducing the failure is more challenging that the solver may not be \nable to .nd a solution within a reasonable time budget. In such cases, we need to break up the execution \nso that each execution segment has tractable size of constraints. Checkpointing is a common technique \nused in such contexts. We plan to integrate CLAP with checkpointing in future. Recording synchronizations \nPrevious work [32, 42, 43] has shown that recording synchronization operations is lightweight for many \napplications. Recording the synchronization order can also reduce the size of generated constraints, \nand it is easy for CLAP to do so. We do not record synchronizations in our current version of CLAP, because \nit would need extra synchronization operations, which could limit our ability to capture non-sequential \nbugs. Also, it could still be costly for a certain range of programs containing intensive high-level \nraces on synchronizations. 7. Related Work Lee et al. [23, 24] pioneered the use of of.ine symbolic analysis \nfor deterministic replay on SC and TSO models at the hardware level. While being much inspired by their \nwork, their solution does not meet our goal of designing a lightweight and software-only so\u00adlution. Without \nthe hardware support, their reliance on collecting load values for the SMT solvers to search for shared \nmemory de\u00adpendencies will create a signi.cant space overhead and a consid\u00aderable program slowdown. In \ncontrast, we do not collect any val\u00adues but only the paths taken by the threads, which has much lower \nrecording overhead. Taking a leap from their symbolic analysis, we perform symbolic execution along the \nprogram paths to replace value matching with path constraint satisfaction . This not only allows us to \nexploring more and possibly simpler schedules (i.e., with minimal number of thread context switches), \nbut more im\u00adportantly, it enables us to encode the preemption bound into the constraint model, converting \nthe NP-complete problem [11] to a polynomial search. ODR [1] presents a high-level framework that is \nsimilar with CLAP: using constraint solving to .gure out a schedule that sat\u00adis.es the recorded information. \nHowever, ODR did not provide a concrete constraint solving algorithm and implementation that con\u00adsiders \nreal-world memory consistency and synchronization. Its con\u00adstraint solving approximates many issues that \nCLAP provides con\u00adcrete and accurate solution to. Wang et al. [33, 34] develop a veri.cation framework \nfor pre\u00addictive concurrency trace analysis. Our modeling of the read-write constraints is similar to \ntheir symbolic model, but has several spe\u00adcializations treatments tailored to bug reproduction. For instance, \nwe do not use any synchronization nor log any program state on\u00adline, and we do not require a sequential \nconsistent execution which is needed in their work to obtain a global trace. ESD [40] performs symbolic \nexecution to synthesize program failures without any runtime logging. A key difference between CLAP and \nESD is that we explore only a single path (i.e., the orig\u00adinal path) that is guaranteed to exhibit the \nsame failure, while ESD essentially has to search all program paths and thread interleavings to .nd the \nbug, which faces particular challenges in addressing pro\u00adgrams with loops and recursive calls. To improve \nscalability, ESD relies on heuristics to synthesize races and deadlocks (which is un\u00adsound and could \nmiss real bugs), while CLAP is built upon a sound modeling of the execution constraints. Weeratunge et \nal. [36] present a technique that reproduces con\u00adcurrency bugs by analyzing the failure core dump. Powered \nby ex\u00adecution indexing [37], their technique actively searches for a fail\u00adure inducing schedule by comparing \nthe core dump differences be\u00adtween the failing run and the passing run. Compared to [36], CLAP does not \nrequire the core dump information and hence is able to reproduce a wider range of concurrency errors. \nCHESS [28] detects and reproduces concurrency bugs through dynamic exploration of thread interleaving. \nTo mitigate the sched\u00adule explosion, CHESS employs a context-bounded algorithm [27] to explore schedules \nup to a certain small number of context switches. Our mechanism for encoding the number of thread in\u00adterleavings \nshares the same spirit as CHESS. Differently, our tech\u00adnique is parallelizable because of the static \nnature, while CHESS is dif.cult to parallelize due to the dynamic exploration strategy. PRES [29] proposes \nprobabilistic replay that intelligently ex\u00adplores the thread interleavings through a feedback-based replayer. \nBy continuously rectifying the schedule in the previous failing re\u00adplay, the technique successfully trades \nmultiple replay attempts for ef.cient online recording. The idea of doing lightweight recording at runtime \nand mak\u00ading up for it with a solver has also been successfully applied by Cheung et al. [6] for replaying \nlong-running single-threaded pro\u00adgrams. The technique achieves low recording overhead by logging only \nthe branch choices at runtime and using symbolic analysis to reconstruct the inputs and memory states. \n8. Conclusion Reproducing concurrency bugs is notoriously challenging due to non-determinism. We have \npresented a new technique, CLAP, that achieves signi.cant advances over previous approaches. CLAP does \nnot log any runtime shared memory dependency or program state, works for sequential consistent as well \nas a range of relaxed memory models, and produces the bug-reproducing schedule in parallel with minimal \nthread context switches. With all these prop\u00aderties, we believe CLAP is promising for production settings. \n Acknowledgements We thank the anonymous reviewers for their insightful feedback which has substantially \nimproved the content and presentation of this paper. We are also grateful to Thomas Ball for his invaluable \ncomments on CLAP. The work of the .rst two authors is supported by Hong Kong RGC GRF grants 622208 and \n622909. References [1] Gautam Altekar and Ion Stoica. ODR: output deterministic replay for multicore \ndebugging. In SOSP, 2009. [2] The SPARC Architecture Manual V9. SPARC International, Inc. 1994. [3] Emina \nTorlakand, Mandana Vaziri, and Julian Dolby. MemSAT: check\u00ading axiomatic speci.cations of memory models. \nIn PLDI, 2010. [4] Thomas Ball and James Larus. Ef.cient path pro.ling. In MICRO, 1996. [5] Cristian \nCadar, Daniel Dunbar, and Dawson Engler. Klee: unassisted and automatic generation of high-coverage tests \nfor complex systems programs. In OSDI, 2008. [6] Alvin Cheung, Armando Solar-Lezama, and Samuel Madden. \nPartial replay of long-running applications. In ESEC/FSE, 2011. [7] Leonardo De Moura and Nikolaj Bj\u00f8rner. \nZ3: an ef.cient SMT solver. In TACAS, 2008. [8] Bruno Dutertre and Leonardo De Moura. The Yices SMT solver. \nTechnical report, 2006. [9] Cormac Flanagan and Stephen Freund. Adversarial memory for detect\u00ading destructive \nraces. In PLDI, 2010. [10] Vijay Ganesh and David Dill. A decision procedure for bit-vectors and arrays. \nIn CAV, 2007. [11] Phillip Gibbons and Ephraim Korach. Testing shared memories. 1997. [12] Derek Hower \nand Mark Hill. Rerun: Exploiting episodes for lightweight memory race recording. In ISCA, 2008. [13] \nJeff Huang, and Charles Zhang. LEAN: Simplifying concurrency bug reproduction via Replay-supported Execution \nReduction. In OOPSLA, 2012. [14] Jeff Huang, Peng Liu, and Charles Zhang. LEAP: Lightweight deter\u00administic \nmulti-processor replay of concurrent Java programs. In FSE, 2010. [15] Jeff Huang and Charles Zhang. \nAn ef.cient static trace simpli.cation technique for debugging concurrent programs. In SAS, 2011. [16] \nNicholas Jalbert and Koushik Sen. A trace simpli.cation technique for effective debugging of concurrent \nprograms. In FSE, 2010. [17] Daniel Jim \u00b4enez. Fast path-based neural branch prediction. In MICRO, 2003. \n[18] Guoliang Jin, Linhai Song, Wei Zhang, Shan Lu, and Ben Liblit. Automated atomicity-violation .xing. \nIn PLDI, 2011. [19] Guoliang Jin, Aditya Thakur, Ben Liblit, and Shan Lu. Instrumenta\u00adtion and sampling \nstrategies for cooperative concurrency bug isolation. In OOPSLA, 2010. [20] James R. Larus. Whole program \npaths. In PLDI, 1999. [21] Dongyoon Lee, Peter M. Chen, Jason Flinn, and Satish Narayanasamy. Chimera: \nhybrid program analysis for determinism. In PLDI, 2012. [22] Michael D. Bond and Milind Kulkarni. Respec: \nef.cient online mul\u00adtiprocessor replay via speculation and external determinism. Technical report, Ohio \nState University, 2012. [23] Dongyoon Lee, Mahmoud Said, Satish Narayanasamy, and Zijiang Yang. Of.ine \nsymbolic analysis to infer total store order. In HPCA, 2011. [24] Dongyoon Lee, Mahmoud Said, Satish \nNarayanasamy, Zijiang Yang, and Cristiano Pereira. Of.ine symbolic analysis for multi-processor execution \nreplay. In MICRO, 2009. [25] Shan Lu, Weihang Jiang, and Yuanyuan Zhou. A study of interleaving coverage \ncriteria. In ESEC-FSE, 2007. [26] Pablo Montesinos, Luis Ceze, and Josep Torrellas. Delorean: Record\u00ading \nand deterministically replaying shared-memory multi-processor ex\u00adecution ef.ciently. In ISCA, 2008. [27] \nMadanlal Musuvathi and Shaz Qadeer. Iterative context bounding for systematic testing of multithreaded \nprograms. In PLDI, 2007. [28] Madanlal Musuvathi, Shaz Qadeer, Thomas Ball, G\u00b4erard Basler, Pira\u00admanayagam \nA. Nainar, and Iulian Neamtiu. Finding and reproducing heisenbugs in concurrent programs. In OSDI, 2008. \n[29] Soyeon Park, Yuanyuan Zhou, Weiwei Xiong, Zuoning Yin, Rini Kaushik, Kyu H. Lee, and Shan Lu. PRES: \nprobabilistic replay with execution sketching on multi-processors. In SOSP, 2009. [30] Polyvios Pratikakis, \nJeffrey Foster, and Michael Hicks. Locksmith: context-sensitive correlation analysis for race detection. \nIn PLDI, 2006. [31] Kapil Vaswani, Matthew J. Thazhuthaveetil, and Y. N. Srikant. A programmable hardware \npath pro.ler. In CGO, 2005. [32] Kaushik Veeraraghavan, Dongyoon Lee, Benjamin Wester, Jessica Ouyang, \nPeter M. Chen, Jason Flinn, and Satish Narayanasamy. Dou\u00adbleplay: parallelizing sequential logging and \nreplay. In ASPLOS, 2011. [33] Chao Wang, Sudipta Kundu, Malay K. Ganai, and Aarti Gupta. Sym\u00adbolic predictive \nanalysis for concurrent programs. In FM, 2009. [34] Chao Wang, Rhishikesh Limaye, Malay Ganai, and Aarti \nGupta. Trace based symbolic analysis for atomicity violations. In TACAS, 2010. [35] Dasarath Weeratunge, \nXiangyu Zhang, and Suresh Jaganathan. Ac\u00adcentuating the positive: Atomicity inference and enforcement \nusing cor\u00adrect executions. In OOPSLA, 2011. [36] Dasarath Weeratunge, Xiangyu Zhang, and Suresh Jagannathan. \nAn\u00adalyzing multicore dumps to facilitate concurrency bug reproduction. In ASPLOS, 2010. [37] Bin Xin, \nWilliam Sumner, and Xiangyu Zhang. Ef.cient program execution indexing. In PLDI, 2008. [38] Min Xu, Rastislav \nBodik, and Mark Hill. A .ight data recorder for full-system multiprocessor deterministic replay. In ISCA, \n2003. [39] Jie Yu and Satish Narayanasamy. A case for an interleaving con\u00adstrained shared-memory multi-processor. \nIn ISCA, 2009. [40] Cristian Zam.r and George Candea. Execution synthesis: a technique for automated \nsoftware debugging. In EuroSys, 2010. [41] Jinguo Zhou, Xiao Xiao, and Charles Zhang. Stride: Search-based \ndeterministic replay in polynomial time via bounded linkage. In ICSE, 2012. [42] Dongyoon Lee, Benjamin \nWester, Kaushik Veeraraghavan, Satish Narayanasamy, Peter M. Chen, and Jason Flinn. Respec: ef.cient \non\u00adline multiprocessor replayvia speculation and external determinism. In ASPLOS, 2010. [43] A. Georges, \nM. Christiaens, M. Ronsse, and K. De Bosschere. JaRec: a portable record/replay environment for multi-threaded \nJava applica\u00adtions. In SPE, 2004.    \n\t\t\t", "proc_id": "2491956", "abstract": "<p>We present CLAP, a new technique to reproduce concurrency bugs. CLAP has two key steps. First, it logs thread local execution paths at runtime. Second, offline, it computes memory dependencies that accord with the logged execution and are able to reproduce the observed bug. The second step works by combining constraints from the thread paths and constraints based on a memory model, and computing an execution with a constraint solver.</p> <p>CLAP has four major advantages. First, logging purely local execution of each thread is substantially cheaper than logging memory interactions, which enables CLAP to be efficient compared to previous approaches. Second, our logging does not require any synchronization and hence with no added memory barriers or fences; this minimizes perturbation and missed bugs due to extra synchronizations foreclosing certain racy behaviors. Third, since it uses no synchronization, we extend CLAP to work on a range of relaxed memory models, such as TSO and PSO, in addition to sequential consistency. Fourth, CLAP can compute a much simpler execution than the original one, that reveals the bug with minimal thread context switches. To mitigate the scalability issues, we also present an approach to parallelize constraint solving, which theoretically scales our technique to programs with arbitrary execution length.</p> <p>Experimental results on a variety of multithreaded benchmarks and real world concurrent applications validate these advantages by showing that our technique is effective in reproducing concurrency bugs even under relaxed memory models; furthermore, it is significantly more efficient than a state-of-the-art technique that records shared memory dependencies, reducing execution time overhead by 45% and log size by 88% on average.</p>", "authors": [{"name": "Jeff Huang", "author_profile_id": "81472647687", "affiliation": "Hong Kong University of Science and Technology, Hong Kong, Hong Kong", "person_id": "P4148974", "email_address": "smhuang@cse.ust.hk", "orcid_id": ""}, {"name": "Charles Zhang", "author_profile_id": "81435599989", "affiliation": "Hong Kong University of Science and Technology, Hong Kong, Hong Kong", "person_id": "P4148975", "email_address": "charlesz@cse.ust.hk", "orcid_id": ""}, {"name": "Julian Dolby", "author_profile_id": "81548007967", "affiliation": "IBM Thomas J. Watson Research Center, New York, USA", "person_id": "P4148976", "email_address": "dolby@us.ibm.com", "orcid_id": ""}], "doi_number": "10.1145/2491956.2462167", "year": "2013", "article_id": "2462167", "conference": "PLDI", "title": "CLAP: recording local executions to reproduce concurrency failures", "url": "http://dl.acm.org/citation.cfm?id=2462167"}