{"article_publication_date": "06-16-2013", "fulltext": "\n Machine-Veri.ed Network Controllers Arjun Guha Mark Reitblatt Nate Foster Cornell University Cornell \nUniversity Cornell University arjun@cs.cornell.edu reitblatt@cs.cornell.edu jnfoster@cs.cornell.edu \nAbstract In many areas of computing, techniques ranging from testing to formal modeling to full-blown \nveri.cation have been successfully used to help programmers build reliable systems. But although net\u00adworks \nare critical infrastructure, they have largely resisted analysis using formal techniques. Software-de.ned \nnetworking (SDN) is a new network architecture that has the potential to provide a foun\u00addation for network \nreasoning, by standardizing the interfaces used to express network programs and giving them a precise \nsemantics. This paper describes the design and implementation of the .rst machine-veri.ed S D N controller. \nStarting from the foundations, we develop a detailed operational model for OpenFlow (the most pop\u00adular \nSD N platform) and formalize it in the Coq proof assistant. We then use this model to develop a veri.ed \ncompiler and run-time sys\u00adtem for a high-level network programming language. We identify bugs in existing \nlanguages and tools built without formal founda\u00adtions, and prove that these bugs are absent from our \nsystem. Finally, we describe our prototype implementation and our experiences us\u00ading it to build practical \napplications. Categories and Subject Descriptors F.3.1 [Specifying and Verify\u00ading and Reasoning about \nPrograms]: Mechanical veri.cation Keywords Software-de.ned networking, OpenFlow, formal veri\u00ad.cation, \nCoq, domain-speci.c languages, NetCore, Frenetic. 1. Introduction Networks are some of the most critical \ninfrastructure in modern so\u00adciety and also some of the most fragile! Networks fail with alarm\u00ading frequency, \noften due to simple miscon.gurations or software bugs [8, 19, 30]. The recent news headlines contain \nnumerous ex\u00adamples of network failures leading to disruptions: a con.guration error during routine maintenance \nat Amazon triggered a sequence of cascading failures that brought down a datacenter and the cus\u00adtomer \nmachines hosted there; a corrupted routing table at GoDaddy disconnected their domain name servers for \na day and caused a widespread outage; and a network connectivity issue at United Air\u00adlines took down \ntheir reservation system, leading to thousands of .ight cancellations and a ground stop at their San \nFrancisco hub. One way to make networks more reliable would be to de\u00advelop tools for checking important \nnetwork invariants automati\u00adcally. These tools would allow administrators to answer questions such as: \ndoes this con.guration provide connectivity to every host Permission to make digital or hard copies of \nall or part of this work for personal or classroom use is granted without fee provided that copies are \nnot made or distributed for pro.t or commercial advantage and that copies bear this notice and the full \ncitation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to \nlists, requires prior speci.c permission and/or a fee. PLDI 13, June 16 19, 2013, Seattle, WA, USA. Copyright \nc &#38;#169; 2013 ACM 978-1-4503-2014-6/13/06. . . $15.00 in the network? or does this con.guration correctly \nenforce the access control policy? or does this con.guration have a forward\u00ading loop? or does this con.guration \nproperly isolate trusted and untrusted traf.c? Unfortunately, until recently, building such tools has \nbeen effectively impossible due to the complexity of today s networks. A typical enterprise or datacenter \nnetwork contains thou\u00adsands of heterogeneous devices, from routers and switches, to web caches and load \nbalancers, to monitoring middleboxes and .re\u00adwalls. Moreover, each device executes a stack of complex \nprotocols and is con.gured through a proprietary and idiosyncratic interface. To reason formally about \nsuch a network, an administrator (or tool) must reason about the proprietary programs running on each \ndis\u00adtributed device, as well as the asynchronous interactions between them. Although formal models of \ntraditional networks exist, they have either been too complex to allow effective reasoning, or too abstract \nto be useful. Overall, the incidental complexity of networks has made reasoning about their behavior \npractically infeasible. Fortunately, recent years have seen growing interest in a new kind of network \narchitecture that could provide a foundation for network reasoning. In a software-de.ned network (SD \nN), a program on a logically-centralized controller machine de.nes the overall policy for the network, \nand a collection of programmable switches implement the policy using ef.cient packet-processing hardware. \nThe controller and switches communicate via an open and standard interface. By carefully installing packet-processing \nrules in the hardware tables provided on switches, the controller can effectively manage the behavior \nof the entire network. Compared to traditional networks, S D Ns have two important simpli.cations that \nmake them amenable to formal reasoning. First, they relocate control from distributed algorithms running \non indi\u00advidual devices to a single program running on the controller. Sec\u00adond, they eliminate the heterogeneous \ndevices used in traditional networks switches, routers, load balancers, .rewalls, etc. and replace them \nwith stock programmable switches that provide a standard set of features. Together, this means that the \nbehavior of the network is determined solely by the sequence of con.guration instructions issued by the \ncontroller. To verify that the network has some property, an administrator (or tool) simply has to reason \nabout the states of the switches as they process instructions. In the networking community, there is \nburgeoning interest in tools for checking network-wide properties automatically. Sys\u00adtems such as FlowChecker \n[1], Header Space Analysis [12], Anteater [17], VeriFlow [13], and others, work by generating a logical \nrepresentation of switch con.gurations and using an auto\u00admatic solver to check properties of those con.gurations. \nThe con\u00ad.gurations are obtained by scraping state off of the switches or inspecting the instructions \nissued by an S D N controller at run-time. These tools represent a good .rst step toward making networks \nmore reliable, but they have two important limitations. First, they are based on ad hoc foundations. \nAlthough S DN platforms such as OpenFlow [21] have precise (if informal) speci.cations, the tools make \nsimplifying assumptions that are routinely violated by real  Figure 1: System architecture. hardware \nswitches. For example, some tools assume that switches will process instructions emitted by the controller \nin sequence, even though actual switches often reorder messages. This means that properties they consider \nveri.ed do not always actually hold. Second, the tools are expensive to run and do not scale well. For \nexample, most tools take several minutes to run, even in small to medium-sized networks (VeriFlow [13] \nis a notable exception). This is too slow to be used in large dynamic networks where con\u00ad.gurations change \non the order of seconds. Overall, although these tools are useful for .nding bugs, they cannot provide \nthe rigorous guarantees that networks, as critical infrastructure, require. Our approach. This paper \npresents a different approach. Rather than building tools to .nd bugs in S D N controllers at run-time, \nwe develop a veri.ed S D N controller in the Coq proof assistant and prove it correct against a formal \nspeci.cation and a detailed op\u00aderational model of S DN. With our controller, programmers specify the \nbehavior of the network using the NetCore programming lan\u00adguage [22], which abstracts away from the details \nof the underlying switch hardware and distributed system, and allows programmers to reason in terms of \nsimple hop-by-hop packet-processing steps. The NetCore compiler and run-time system translates programs \nwritten in this language down to low-level packet-processing rules. Because its behavior is veri.ed in \nCoq, we establish the correctness of our controller once and for all, obviating the need for run-time \nor post hoc veri.cation as in most current tools. Architecturally, our system is organized as a veri.ed \nsoftware stack that translates through the following levels of abstraction: NetCore. The highest level \nof abstraction is the NetCore lan\u00adguage, proposed in prior work by Monsanto et al. [22]. Net-Core is \na declarative language that allows programmers to de\u00adscribe what network behavior they want, without \nspecifying how it should be implemented. It offers a collection of intu\u00aditive constructs for matching, \n.ltering, and transforming pack\u00adets, as well as natural logical operators for combining smaller programs \ninto bigger ones such as union and domain restriction. Although NetCore programs are ultimately executed \nin a dis\u00adtributed system the network they have a simple semantics that models their behavior as functions \nfrom packets to packets. Flow tables. The intermediate level of abstraction is .ow ta\u00adbles, a representation \nthat sits between NetCore programs and switch-level con.gurations. There are two main differences be\u00adtween \nNetCore programs and .ow tables. First, NetCore pro\u00adgrams describe the forwarding behavior of a whole \nnetwork, while .ow tables describe the behavior of a single switch. Sec\u00adond, .ow tables process packets \nusing a linear scan through a list of prioritized rules. Hence, to translate operators such as union \nand negation, the NetCore compiler must generate a se\u00adquence of rules that encodes the same semantics. \nHowever, be\u00adcause .ow table matching uses a lower-level packet representa\u00adtion (as nested frames of Ethernet, \nIP, T C P, etc. packets), .ow tables must satisfy a well-formedness condition to rule out in\u00advalid patterns \nthat are inconsistent with this representation.  Featherweight OpenFlow. The lowest level of abstraction \nis Featherweight OpenFlow, a new foundational model we have designed that captures the essential features \nof S D Ns. Feather\u00adweight OpenFlow models switches, the controller, the network topology, as well as \ntheir internal transitions and interactions in a small-step operational semantics. This semantics is \nnon\u00addeterministic, modeling the asynchrony inherent in networks. To implement a .ow table in a Featherweight \nOpenFlow net\u00adwork, the controller instructs switches to install or uninstall rules as appropriate while \ndealing with two important issues: First, switches process instructions concurrently with packets .owing \nthrough the network, so it must ensure that at all times the rules installed on switches are consistent \nwith the .ow table. Second, switches are allowed to buffer instructions and apply them in any order, \nso it must ensure that the behavior is correct no matter how instructions are reordered through careful \nuse of synchronization primitives.  Figure 1 depicts the architecture of our system, and provides an \nout\u00adline for this paper. Overall, our main contributions are as follows: We present the .rst machine-veri.ed \nSDN controller, which gives network programmers robust static guarantees backed by machine-checked proofs \nagainst a foundational model.  We develop Featherweight OpenFlow, the .rst formal model of OpenFlow. \nIt includes all sources of asynchrony and non\u00addeterminism mentioned in the informal OpenFlow speci.ca\u00adtion, \nas well as a precise model of switch .ow table semantics.  We formalize NetCore, .ow tables, and Featherweight \nOpen-Flow in Coq, and develop machine-checked proofs of correct\u00adness for the translations between them. \n We present our prototype implementation, obtained by extract\u00ading our Coq development to OCaml, and \npresent experimental results comparing the performance of our system against unver\u00adi.ed controllers on \nsimple benchmarks.  Besides their use in our system, we hope that the abstractions and theorems presented \nin this paper will be useful to others. Flow tables are a canonical representation of switch state that \nappear in many other systems. Likewise, Featherweight OpenFlow is a comprehensive model that captures \nthe essential forwarding behavior of SDNs in a minimal core calculus. Our design and Coq formalization \nof .ow tables and Featherweight OpenFlow provide  Figure 2: Example network topology. a starting point \nfor developing extensions that model additional S D N features and a foundation for other veri.ed systems. \n2. Overview To motivate the need for veri.ed S D N controllers, consider the simple network depicted \nin Fig. 2. It consists of a single switch connected to four hosts: three clients and a middlebox that \nmonitors H T T P requests. The ports on the switch are numbered 1 to 4; the clients are connected to \nports 1 to 3 and the middlebox to port 4. In addition, the switch has a dedicated link to the controller \nthat is not considered part of the data network. Now imagine we want to build an S DN controller that \nimple\u00adments the following high-level network policy: block SS H traf.c, log H T T P requests, and allow \nclients to send non-S S H traf.c to each other. It is straightforward to formalize this policy as a packet\u00adprocessing \nfunction that maps input packets to (possibly several) output packets: the function drops SSH packets, \nforwards H TT P packets both to their destination and to the middlebox, and forwards all other packets \nto their destination alone. To implement this function in an S D N, however, we would need to specify \nseveral additional low-level details, since switches can\u00adnot implement general packet-processing functions \ndirectly. First, the controller would need to encode the function as a .ow table a set of prioritized \nforwarding rules. Second, it would need to send the switch a series of control messages to add individual \nentries from the .ow table, incrementally building up the complete table. More concretely, the controller \ncould .rst send a message in\u00adstructing the switch to add a .ow table entry that blocks SSH traf.c: Add \n10 {tpDst = 22} {||} Here 10 is a priority number, {tpDst =22} is a pattern that matches S S H traf.c \n(TC P port 22), and {||} is an empty multiset of ports, which drops packets, as intended. Next, the controller \ncould add an entry to process H T T P requests: Add 9 {dlDst = H1, tpDst = 80} {|1, 4|} Note that this \nrule duplicates H TT P (T CP port 80) packets, sending them to the monitor and to their destination.1 \nFinally, the controller could add an entry to forward other packets to their destination: Add 1 {dlDst \n= H1} {|1|} Note that this rule does not apply to SSH and H TT P traf.c, since those packets are handled \nby the higher-priority rules. After these control messages have been sent, it would be nat\u00adural to expect \nthat the network correctly implements the packet\u00adprocessing function described above. But the situation \nis actually 1 The controller would actually need to create rules for each client. To save space, we have \nonly given the rules for H1 here. Packet pk ::= Eth dlSrc dlDst dlTyp nwPk Network layer nwPk ::= IP \nnwSrc nwDst nwProto tpPk | Unknown payload Transport layer tpPk ::= TCP tpSrc tpDst payload | Unknown \npayload Figure 3: Logical packet structure. more complicated: switches have substantial latitude in how \nthey process messages from the controller, and packets may arrive at any time during processing. Establishing \nthat the network correctly implements this function in particular, that it blocks S S H traf.c and logs \nHTT P traf.c requires additional reasoning. Controller-switch consistency. Switches process packets and \ncontrol messages concurrently. In our example, the switch may receive an H TT P request before the .ow \ntable entry that handles H T T P packets arrives. In this case, the switch will send the packet to the \ncontroller for further processing. Since the controller is a general-purpose machine, it can implement \nthe packet-processing function directly, apply it to the incoming packet, and send the re\u00adsults back \nto the switch. However, this means that SD N controllers typically have two different implementations \nof the function: one residing at the controller and another on the switches. A key prop\u00aderty we verify \nis that these two implementations are consistent. Message reordering. SD N switches may process control \nmes\u00adsages in any order, and many switches do, to maximize perfor\u00admance. But unrestricted reordering can \ncause implementations to violate their intended speci.cations. For example, if the rule to drop S SH \ntraf.c is installed after the .nal, low-priority rule that forwards all traf.c, then S S H traf.c will \ntemporarily be forwarded by the low-priority rule, breaking the intended security policy. To ensure that \nsuch reorderings do not occur, a controller must carefully insert barrier messages, which force the switch \nto process all outstanding messages. A key property we verify is that controllers use barriers correctly \n(several unveri.ed controllers ignore this issue). Natural patterns. Another complication is that the \npatterns pre\u00adsented earlier in this section, such as {tpDst = 22}, are actually invalid. To match S S \nH traf.c, it is not enough to simply state that the destination port must be 22. The pattern must also \nspecify that the Ethernet frame type must be IP, and the transport protocol must be T CP. Without these \nadditional constraints, switches will interpret the pattern as a wildcard that matches all packets. Several \nearlier controller platforms did not properly account for this behavior, and had bugs as a result. We \ndevelop a semantics for patterns and iden\u00adtify a class of natural patterns that are closed under the \nalgebraic operations used by our compiler and .ow table optimizer. Roadmap. The rest of this paper develops \ntechniques for estab\u00adlishing that a given packet-processing function is implemented cor\u00adrectly by an \nOpenFlow network. More speci.cally, we tackle the problem of verifying high-level programming abstractions, \nusing NetCore [22] as a concrete instance of a high-level network lan\u00adguage. The next section presents \nNetCore in detail. The following sections describe general and reusable techniques for establishing the \ncorrectness of SD N controllers, including NetCore. 3. NetCore This section presents the highest layer \nof our veri.ed stack: the NetCore language. A NetCore program speci.es how the switches process packets \nat each hop through the network. More formally, a program denotes a total function from port-packet pairs \nto multisets of port-packet pairs. The syntax and semantics of a core NetCore  Switch I D sw . N Port \nI D pt . N Headers h ::= dlSrc | dlDst M AC address | dlTyp Ethernet frame type | nwSrc | nwDst I P address \n| nwProto I P protocol code | tpSrc | tpDst transport port Predicate pr ::= * wildcard | h = n match \nheader | at sw match switch | not pr predicate negation | pr1 and pr2 predicate conjunction Program pg \n::= pr . {|pt1 \u00b7 \u00b7 \u00b7 ptn|} basic program | pg 1 l pg 2 program union | restrict pg by pr program restriction \n[pr] sw pt pk [*] sw pt pk = true [dlSrc=n] sw pt (Eth dlSrc ) = dlSrc=n [nwSrc=n] sw pt (Eth (IP nwSrc \n)) = nwSrc=n [nwSrc=n] sw pt (Eth (Unknown )) = false \u00b7 \u00b7 \u00b7 [at sw'] sw pt pk = sw=sw' [not pr] sw pt \npk = \u00ac([pr] sw pt pk) [pr1 and pr2] sw pt pk = [pr1] sw pt pk . [pr2] sw pt pk [pg ] sw pt pk = {|(pt1, \npk1) \u00b7 \u00b7 \u00b7 (ptn, pkn)|} [pr . {|pt1 \u00b7 \u00b7 \u00b7 ptn|}] sw pt pk = if [pr] sw pt pk then {|(pt1, pk) \u00b7 \u00b7 \u00b7 (ptn, \npk)|} else {||} [pg 1 l pg 2] sw pt pk = [pg 1] sw pt pk l [pg 2] sw pt pk [restrict pg by pr] sw pt \npk = {|(pt', pk') | (pt', pk') . [pg ] sw pt pk . [pr] sw pt pk|} Figure 4: NetCore syntax and semantics \n(extracts). fragment are shown in Fig. 4. To save space, we have elided several header .elds and operators \nnot used in this paper. We can build a NetCore program that implements the example from the previous \nsection by composing several smaller NetCore program fragments. The .rst fragment forwards traf.c to \nH1: pg 1 . dlDst=H1 . {|1|} This basic program consists of a predicate pr and a multiset of actions {|pt1 \n\u00b7 \u00b7 \u00b7 ptn|}. The predicate denotes a set of port-packet pairs, and the actions denote the ports (if any) \nwhere those packets should be forwarded on the next hop. In this instance, the predicate denotes the \nset of all packets whose Ethernet destination (dlDst) address has the speci.ed value, and the actions \ndenote a transfor\u00admation that forwards matching packets to port 1. Note that we rep\u00adresent packets as \nnested sequences of frames (Ethernet, IP, TC P, etc.) as shown in Fig. 3. NetCore provides predicates \nfor matching on well-known header .elds as well as logical operators such as and and or, unlike hardware \nswitches, which only provide priori\u00adtized sets of rules. The next two fragments are similar to pg 1, \nbut forward traf.c to H2 and H3 instead of H1: pg 2 . dlDst=H2 . {|2|} pg 3 . dlDst=H3 . {|3|} Using \nthe union operator, we can combine these programs into a single program that implements forwarding between \nall clients: pg fwd . pg 1 l pg 2 l pg 3 Semantically, the l operator produces the (multiset) union of \nthe results produced by each sub-program. Using the union operator again, we can extend this program \nto one that also forwards H T TP requests to the middlebox: pg fwd l tpDst=80 . {|4|} Note that this \nprogram duplicates packets sent to port 80, forward\u00ading to their destination and also to the logging \nmachine. Finally, we can add the security policy using the restrict by operator, which restricts a program \nby a predicate: restrict (pg fwd l tpDst=80 . {|4|}) by (not tpDst=22) This program is similar the previous \none, but drops SSH traf.c. The advantages of using a declarative language such as NetCore should be clear: \nit provides abstractions that make it easy to estab\u00adlish network-wide properties through compositional \nreasoning. For example, simply by inspecting the .nal program and using the de\u00adnotational semantics (Fig. \n4), we can easily verify that the network blocks SS H traf.c, forwards H T T P traf.c to the middlebox, \nand pro\u00advides pair-wise connectivity between the clients. In particular, even though a controller, switches, \n.ow tables, forwarding rules, are all involved in implementing this program, we do not have to reason \nabout them! This is in contrast to lower-level controller platforms, which require programmers to explicitly \nconstruct switch-level for\u00adwarding rules, issue messages to install those rules on switches, and reason \nabout the asynchronous interactions between switches and controller. Of course, the complexity of the \nunderlying system is not eliminated, but relocated from the programmer to the language implementers. \nThis is an ef.cient tradeoff: functionality common to many programs can be implemented just once, proved \ncorrect, and reused broadly. 4. Flow Tables The .rst step toward executing a NetCore program in an SDN \nin\u00advolves compiling it to a prioritized set of forwarding rules a .ow table. Flow tables are an intermediate \nrepresentation that play a similar role in NetCore to register transfer language (RTL) in tradi\u00adtional \ncompilers. Flow tables are more primitive than NetCore pro\u00adgrams because they lack the logical structure \ninduced by NetCore operators such as union, intersection, negation, and restriction. Also, the patterns \nused to match packets in .ow tables are more restrictive than NetCore predicates. And unlike NetCore \nprograms, which denote total functions, .ow tables are partial: switches redi\u00adrect unmatched packets \nto the controller. As de.ned in Fig. 5, a .ow table consists of a multiset of rules (n, pat , pts) where \nn is an integer priority, pat is a pattern, and pts is a multiset of ports. A pattern is a record that \nassociates each header .eld to either an integer constant n or the special wildcard value *. When writing \n.ow tables, we often elide headers set to * in patterns as well as priorities when they are clear from \ncontext. Pattern semantics. The semantics of patterns is given by the function pk#pat , as de.ned in \nFig. 5. This turns out to be subtly complicated, due to the representation of packets as sequences of \nnested frames a pattern contains a (possibly wildcarded) .eld for every header .eld, but not all packets \ncontain every header .eld. Some .elds only exist in speci.c frame types (dlTyp) or protocols (nwProto). \nFor example, only I P packets (dlTyp = 0x800) have IP source and destination addresses. Likewise, TC \nP (nwProto = 6) and U D P (nwProto = 17) packets have source and destination ports, but I CM P (nwProto \n= 1) packets do not.  Wildcard w ::= n | * Pattern pat ::= {dlSrc = w, dlDst = w, dlTyp = w, nwSrc = \nw, nwDst = w, nwProto = w, tpSrc = w, tpDst = w} Flow table FT . {|n \u00d7 pat \u00d7 {|pt|}|} [FT ] pt pk -{|pt1 \n\u00b7 \u00b7 \u00b7 ptn|} \u00d7 {|pk1 \u00b7 \u00b7 \u00b7 pkm|} .(n, pat, {|pt1 \u00b7 \u00b7 \u00b7 ptn|}) . FT. pk#pat = true '' ' .(n , pat , pts \n' ) . FT . n > n . pk#pat ' = false (MATCHED) [FT ] pt pk -({|(pt1) \u00b7 \u00b7 \u00b7 (ptn)|}, {||}) .(n, pat, pts) \n. FT pk#pat = false (UNMATCHED) [FT ] pt pk-({||}, {|(pt, pk)|}) pk#pat (Eth dlSrc dlDst dlTyp nwPk)#pat \n= dlSrc pat.dlSrc . dlDst pat.dlDst . dlTyp pat.dlTyp . (pat.dlTyp = 0x800 . nwPk#nw pat) nwPk#nw pat \n(IP nwSrc nwDst nwProto tpPk)#nw pat = nwSrc pat.nwSrc . nwDst pat.nwDst . nwProto pat.nwProto . (pat.nwProto \n= 6 . tpPk#tppat) (Unknown payload)#nw pat = true tpPk#tppat (TCP tpSrc tpDst payload)#tppat = tpSrc \npat.tpSrc . tpDst pat.tpDst Unknown payload#tppat = true n w m n = m=n n * = true Figure 5: Flow table \nsyntax and semantics. To match on a given .eld, a pattern must specify values for all other .elds it \ndepends on. For example, to match on IP addresses, the pattern must also specify that the Ethernet frame \ntype is IP: {dlTyp = 0x800, nwSrc = 10.0.0.1} If the frame type is elided, the value of the dependent \nheader is silently ignored and the pattern is equivalent to a wildcard: {nwSrc = 10.0.0.1} = {} In effect, \npatterns not only match packets, but also determine how they are parsed. This behavior, which was ambiguous \nin early versions of the OpenFlow speci.cation (and later .xed) has lead to real bugs in existing controllers \n(Section 5). Although unintuitive for programmers, this behavior is completely consistent with how packet \nprocessing is implemented in modern switch hardware. Flow table semantics. The semantics of .ow tables \nis given by the relation [\u00b7]. The relation has two cases: one for matched packets and another for unmatched \npackets. Each .ow table entry is a tuple containing a priority n, pattern pat, and a multiset of ports \nP : sw \u00d7 pr . [(pat, bool)] P(sw, dlSrc = n) = [({dlSrc = n}, true)] P(sw, nwSrc = n) = [({dlTyp = 0x800, \nnwSrc = n}, true)] \u00b7 \u00b7 \u00b7 P(sw, at sw) = [(*, true)] P(sw, at sw ' ) = [(*, false)] where sw= sw ' P(sw, \nnot pr) = [(pat1, \u00acb1) \u00b7 \u00b7 \u00b7 (patn, \u00acbn), (*, false)] where [(pat1, b1) \u00b7 \u00b7 \u00b7 (patn, bn)] = P(sw, pr) \nP(sw, pr and pr ' ) = [(pat1 n pat ' , b1 . b ' \u00b7 \u00b7 \u00b7 (pat n pat ' , bm . b ' )] 1 1) mn n where [(pat1, \nb1) \u00b7 \u00b7 \u00b7 (patm, bm)] = P(sw, pr) '' ' where [(pat , b ' \u00b7 \u00b7 \u00b7 (pat , b ' )] = P(sw, pr ) 1 1) n n C \n: sw \u00d7 pg . [(pat, pt)] C(sw, pr . pt) = [(pat1, pt1) \u00b7 \u00b7 \u00b7 (pat , ptn), (*, {||})] n where [(pat1, b1), \n\u00b7 \u00b7 \u00b7 , (patn, bn)] = P(sw, pr) where pti = pt if bi = true where pti = {||} if bi = false C(sw, pg \nl pg ' ) = '' '' [(pat1 n pat1, pt1 l pt1), \u00b7 \u00b7 \u00b7 , (patm n patn, ptm l ptn)] + [(pat1, pt1) \u00b7 \u00b7 \u00b7 (patm, \nptm)] + '' '' [(pat1, pt1) \u00b7 \u00b7 \u00b7 (patn, ptn)] where [(pat1, pt1) \u00b7 \u00b7 \u00b7 (patm, ptm)] = P(sw, pg) '''' \n' where [(pat1, pt1) \u00b7 \u00b7 \u00b7 (patn, ptn)] = P(sw, pg ) Figure 6: NetCore compilation. {|pt1 \u00b7 \u00b7 \u00b7 ptn|}. \nGiven a packet and its input port, the semantics forwards the packet to all ports in the multiset associated \nwith the highest-priority matching rule in the table. Otherwise, if no matching rule exists, it diverts \nthe packet to the controller. In the formal semantics, the .rst component of the result pair represents \nforwarded packets while the second component represents diverted packets. Note that .ow table matching \nis non-deterministic if there are multiple matching entries with the same priority. This has serious \nimplications for a compiler e.g., naively combining .ow tables with overlapping priorities could produce \nincorrect results. In the NetCore compiler, we avoid this issue by always working with unambiguous and \ntotal .ow tables. 5. Veri.ed NetCore Compiler With the syntax and semantics of NetCore and .ow tables \nin place, we now present a veri.ed compiler for NetCore. The compiler takes programs as input and generates \na set of .ow tables as output, one for every switch. The compilation algorithm is based on previous work \n[22], but we have veri.ed its implementation in Coq. While building the compiler, we found two serious \nbugs in the original algorithm related to the handling of (unnatural) patterns in the compiler and .ow \ntable optimizer. The compilation function C, de.ned in Fig. 6, generates a .ow table for a given switch \nsw. It uses the auxiliary function P to compile predicates. The compiler produces a list of pattern-action \npairs, but priority numbers are implicit: the pair at the head has highest priority and each successive \npair has lower priority. Because NetCore programs denote total functions, packets not explicitly matched \nby any predicate are dropped. In contrast, .ow tables divert unmatched packets to the controller. The \ncompiler resolves this discrepancy by adding a catch-all rule that drops unmatched packets. For instance: \nC(sw, dlSrc = H1 . {|5|})= {|(2, {dlSrc = H1}, {|5|}), (1, *, {||})|}  The key operator used by the \ncompiler constructs the cross-product of the .ow tables provided as input. This operator can be used \nto compute intersections and unions of .ow tables. Note that im\u00adplementing union in the obvious way by \nconcatenating .ow tables would be wrong. The cross-product operator performs an element-wise intersection \nof the input .ow tables and then merges their actions. To compile a union, we .rst use cross-product \nto build a .ow table that represents the intersection, and then concatenate the .ow tables for the sub-policies \nat lower priority. For example, the following NetCore program, dlSrc = H1 . {|5|} l dlDst = H2 . {|10|} \ncompiles to a .ow table: Priority Pattern Action 4 3 2 1 {dlSrc = H1, dlDst = H2}{dlSrc = H1}{dlDst = \nH2}* {|5, 10|} {|5|} {|10|} {||} The .rst rule matches all packets that match both sub-programs, while \nthe second and third rules match packets only matched by the left and the right programs respectively. \nThe .nal rule drops all other packets. The compilation of other predicates uses similar manipulations \non .ow tables. We have built a large library of .ow table manipulation opera\u00adtors in Coq, along with \nseveral lemmas that state useful algebraic properties about these operators. With this library, proving \nthe cor\u00adrectness theorem for the NetCore compiler is simple only about 200 lines of code in Coq. Theorem \n1 (NetCore Compiler Soundness). For all NetCore programs pg , switches sw, ports pt, and packets pk we \nhave [C(sw, pg )] pt pk = [pg ] sw pt pk. Intuitively, this theorem states that a .ow table compiled \nfrom a NetCore program for a switch sw, has the same behavior as the NetCore program evaluated on packets \nat sw. Compiler bugs. In the course of our work, we discovered that several unveri.ed compilers from \nhigh-level network programming languages to .ow tables suffer from bugs due to subtle pattern semantics. \nSection 4 described inter-.eld dependencies in patterns. For example, to match packets from IP address \n10.0.0.1, we write {nwSrc = 10.0.0.1, dlTyp = 0x800} and if we omit the dlTyp .eld, the I P address is \nsilently ignored. This unintuitive behavior has led to bugs in PA N E [6] and Net\u00adtle [27] as well as \nan unveri.ed version of NetCore [22]. To il\u00adlustrate, consider the following program: nwSrc = 10.0.0.1 \n. {|5|} In NetCore, this program matches all I P packets from 10.0.0.1 and forwards them out port 5. \nBut the original NetCore compiler produced the following .ow table for this program: Because the .rst \npattern is equivalent to the all-wildcard pattern, this .ow table sends all traf.c out port 5. Both PA \nNE and Nettle have similar bugs. Nettle has a special case to handle patterns with I P addresses that \ndo not specify dlTyp = 0x800, but it does not correctly handle patterns that specify a transport port \nnumber but not the nwProto .eld. PAN E suffers from the same bug. Even worse, these invalid patterns \nlead to further bugs when .ow tables are combined and optimized by the compiler. Natural patterns. The \nveri.ed NetCore compiler does not suffer from the bug above. In our formal development, we require that \nall patterns manipulated by the compiler be what we call natural patterns. A natural pattern has the \nproperty that if the pattern speci.es the value of a .eld, then all of that .eld s dependencies are also \nmet. This rules out patterns such as {nwSrc = 10.0.0.1}, which omits the Ethernet frame type necessary \nto parse the IP address. Natural patterns are easy to de.ne using dependent types in Coq. Moreover, we \ncan calculate the cross-product of two natural patterns by intersecting .elds point-wise. Hence, it is \neasy to prove that natural patterns are closed under intersection. Lemma 1. If pat 1 and pat 2 are natural \npatterns, then pat 1 n pat 2 is also a natural pattern. Another important property is that all patterns \ncan be expressed as some equivalent natural pattern (where patterns are equivalent if they denote the \nsame set of packets). This property tells us that we do not lose expressiveness by restricting to natural \npatterns. Lemma 2. If pat is an arbitrary pattern, then there exists a natural pattern pat ', such that \npat = pat ' . These lemmas are used extensively in the proofs of correctness for our compiler and .ow \ntable optimizer. Flow table optimizer. The basic NetCore compilation algorithm described so far generates \n.ow tables that correctly implement the semantics of the input program. But many .ow tables have redun\u00addant \nentries that could be safely removed. For example, a naive compiler might translate the program (* . \n{|5|}) to the .ow table {|(2, *, {|5|}), (1, *, {||})|}, which is equivalent to {|(2, *, {|5|})|}. Worse, \nbecause the compilation rule for union uses a cross-product operator to combine the .ow tables computed \nfor sub-programs, the output can be exponentially larger than the input. Without an optimizer, such a \nnaive compiler is essentially useless e.g., we built an unoptimized implementation of the algorithm in \nFig. 6 and found that it ran out of memory when compiling a program consist\u00ading of just 9 operators! \nOur compiler is parameterized on a function O : FT . FT , that it invokes at each recursive call. Because \neven simple policies can see a combinatorial explosion during compilation, this inline reduction is necessary. \nWe stipulate that O must produce equivalent .ow tables: [O(FT )] = [FT ]. We have built an optimizer \nthat eliminates low-priority entries whose patterns are fully subsumed by higher-priority rules and proved \nthat it satis.es the above condition in Coq. Although this optimization is quite simple, it is effective \nin practice. In addition, earlier attempts to implement this optimization in NetCore had a bug that incorrectly \nidenti.ed certain rules as overlapping which we did not discover until developing this proof. The PANE \noptimizer also had a bug it assumed that combining identical actions is always idempotent. Both of these \nbugs led to incorrect behavior. 6. Featherweight OpenFlow The next step towards executing NetCore programs \nis a controller that con.gures the switches in the network. To prove that such a controller is correct, \nwe need a model of the network. Unfortu\u00adnately, the OpenFlow 1.0 speci.cation, consisting of 42 pages \nof informal prose and C de.nitions, is not amenable to rigorous proof. This section presents Featherweight \nOpenFlow, a detailed oper\u00adational model that captures the essential features of OpenFlow net\u00adworks, and \nyet still .ts on a single page. The model elides a number of details such as error codes, counters, packet \nmodi.cation, and advanced con.guration options such as the ability to enable and disable ports. But it \ndoes include all of the features related to how packets are forwarded and how .ow tables are modi.ed. \nMany ex\u00adisting S D N bug-.nding and property-checking tools are based on  Switch S ::= S(sw, pts, FT \n, inp, outp, inm , outm ) Ports on switch pts . {pt} Controller C ::= C(s, fin , fout ) Input/output \nbuffers inp, outp . {|(pt, pk)|} Link L ::= L((swsrc, ptsrc), pks, (swdst , ptdst )) Messages from controller \ninm . {|SM |} Link to Controller M ::= M(sw, SMS, CMS) Messages to controller outm . {|CM |} Switch \nComponentsDevices Controller state s Message queue from controller SMS . [SM 1 \u00b7 \u00b7 \u00b7 SM n]Controller \ninput relation fin . sw \u00d7 CM \u00d7 s -s Message queue to controller CMS . [CM 1 \u00b7 \u00b7 \u00b7 CM n]Controller output \nrelation fout . s -sw \u00d7 SM \u00d7 s Controller LinkController Components From controller SM ::= FlowMod d \n| PktOut pt pk | BarrierRequest n To controller CM ::= PktIn pt pk | BarrierReply n Table update d ::= \nAdd n pat act | Del pat Abstract OpenFlow Protocol [FT](pt, pk) -({|pt ' 1 \u00b7 \u00b7 \u00b7 pt ' |}, {|pk1 ' \u00b7 \u00b7 \n\u00b7 pk ' |}) n m (FWD) S(sw, pts, FT , {|(pt, pk)|} l inp, outp, inm , outm ) (sw,pt,pk) ------. S(sw, \npts, FT , inp, {|(pt1 ' , pk) \u00b7 \u00b7 \u00b7 (ptn ' 1 \u00b7 \u00b7 \u00b7 m|} l outm ) , pk)|} l outp, inm , {|PktIn pt pk ' \nPktIn pt pk ' (WIRE-SEND) S(sw, pts, FT, inp, {|(pt, pk)|} l outp, inm , outm ) | L((sw, pt), pks, (sw \n' , pt ' )) -. S(sw, pts, FT, inp, outp, inm , outm ) | L((sw, pt), [pk] + pks, (sw ' , pt ' )) (WIRE-RECV) \n L((sw ' , pt ' ), pks + [pk] , (sw, pt)) | S(sw, pts, FT , inp, outp, inm , outm ) -. L((sw ' , pt ' \n), pks, (sw, pt)) | S(sw, pts, FT , {|(pt, pk)|} l inp, outp, inm , outm ) (ADD) S(sw, pts, FT, inp, \noutp, {|FlowMod Add m pat act|} l inm , outm ) -. S(sw, pts, FT l {|(m, pat, act)|}, inp, outp, inm , \noutm ) ''' ''' ' FT rem = {|(n , pat , act ) | (n , pat , act ) . FT and pat = pat |} (DEL) S(sw, pts, \nFT , inp, outp , {|FlowMod Del pat|} l inm , outm ) -. S(sw, pts, FTrem, inp, outp, inm , outm ) pt . \npts (PKTOUT) S(sw, pts, FT , inp, outp, {|PktOut pt pk|} l inm , outm ) -. S(sw, pts, FT, inp, {|(pt, \npk)|} l outp, inm , outm ) fout (s) -(sw, SM , s ' ) (CTRL-SEND) C(s, fin , fout ) | M(sw, SMS, CMS) \n-. C(s ' , fin , fout ) | M(sw, [SM ] + SMS, CMS) fin (sw, s, CM ) -s ' (CTRL-RECV) C(s, fin , fout \n) | M(sw, SMS, CMS + [CM ]) -. C(s ' , fin , fout ) | M(sw, SMS, CMS) SM = BarrierRequest n (SWITCH-RECV-CTRL) \n M(sw, SMS + [SM ] , CMS) | S(sw, pts, FT , inp, outp, inm , outm ) -. M(sw, SMS, CMS) | S(sw, pts, FT \n, inp, outp, {|SM |} l inm , outm ) (SWITCH-RECV-BARRIER) M(sw, SMS + [BarrierRequest n] , CMS) | S(sw, \npts, FT , inp , outp, {||}, outm ) -. M(sw, SMS, CMS) | S(sw, pts, FT , inp, outp, {||}, {|BarrierReply \nn|} l outm ) (SWITCH-SEND-CTRL) S(sw, pts, FT, inp, outp , inm , {|CM |} l outm ) | M(sw, SMS, CMS) \n-. S(sw, pts, FT, inp, outp , inm , outm ) | M(sw, SMS, [CM ] + CMS) Sys1 -. Sys ' 1 (CONGRUENCE) Sys1 \n| Sys2 -. Sys ' 1 | Sys2 Figure 7: Featherweight OpenFlow syntax and semantics. similar (informal) models \n[3, 12, 13]. We believe Featherweight OpenFlow could also serve as a foundation for these tools. 6.1 \nOpenFlow Semantics Initially, every switch has an empty .ow table that diverts all packets to the controller. \nUsing FlowMod messages, the controller can insert new table entries to have the switch process packets \nitself. A non-trivial program may compile to several thousand .ow table entries, but FlowMod messages \nonly add a single entry at a time. In general, many FlowMod messages will be needed to fully con.gure \na switch. However, OpenFlow is designed to give switches a lot of latitude to enable ef.cient processing, \noften at the expense of programmability and understandability: Pattern semantics. As discussed in preceding \nsections, the semantics of .ow tables is non-trivial: patterns have implicit dependencies and .ow tables \ncan have multiple, overlapping entries. (The OpenFlow speci.cation itself notes that scanning the table \nto .nd overlaps is expensive.) Therefore, it is up to the controller to avoid overlaps that introduce \nnon-determinism.  Packet reordering. Switches may reorder packets arbitrarily. For example, switches \noften have both a fast path that uses custom packet-processing hardware and a slow path that pro\u00adcesses \npackets using a slower general-purpose CPU.  No acknowledgments. Switches do not acknowledge when FlowMod \nmessages are processed, except when errors oc\u00adcur. The controller can explicitly request acknowledgements \nby sending a barrier request after a FlowMod. When the switch has processed the FlowMod (and all other \nmessages received before the barrier request), it responds with a barrier reply.  Control message reordering. \nSwitches may process control messages, including FlowMod messages, in any order. This is based on the \narchitecture of switches, where the logical .ow table is implemented by multiple physical tables working \nin parallel each physical table typically only matches headers for one protocol. To process a rule with \na pattern such as {nwSrc = 10.0.0.1, dlTyp = 0x800}, which matches head\u00aders across several protocols, \nseveral physical tables may need to be recon.gured, which takes longer to process than a simple pattern \nsuch as {dlDst = H2}.  Figure 7 de.nes the syntax and semantics of Featherweight Open-Flow, which faithfully \nmodels all of these behaviors. The rest of this section discusses the key elements of the model in detail. \n 6.2 Network Elements Featherweight OpenFlow has four kinds of elements: switches, controllers, links \nbetween switches (carrying data packets), and links between switches and the controller (carrying OpenFlow \nmes\u00adsages). The semantics is speci.ed using a small-step relation, with elements interacting by passing \nmessages and updating their state non-deterministically. Switches. A switch S comprises a unique identi.er \nsw, a set of ports pts, and input and output packet buffers inp and outp. The buffers are multisets of \npackets tagged with ports, (pt, pk). In the input buffer, packets are tagged with the port on which they \nwere received. In the output buffer, packets are tagged with the port on which they will be sent out. \nSince buffers are unordered, switches can process packets in any order. Switches also have a .ow table, \nFT , which determines how the switch processes packets. As detailed in Section 4, the table is a collection \nof .ow table entries, where each entry has a priority, pattern and, a multiset of output ports. Each \nswitch also has a multiset of messages to and from the controller, outm and inm . There are three kinds \nof messages from the controller: PktOut pt pk instructs the switch to emit packet pk on port pt.  FlowMod \nd instructs the switch to add or delete entries from its .ow table. When d is Add n pat act , a new entry \nis created, whereas Del pat deletes all entries that match pat exactly. In our model, we assume that \n.ow tables on switches can be arbitrarily large. This is not the case for hardware switches, where the \nsize of .ow tables is often constrained by the amount of silicon used, and varies from switch-to-switch. \nIt would be straightforward to modify our model to bound the size of the table on each switch.  BarrierRequest \nn forces the switch to process all outstanding messages before replying with a BarrierReply n message. \n Controllers. A controller Cis de.ned by its local state s, an input relation fin , and an output relation \nfout . The local state and these relations are application-speci.c, so Featherweight OpenFlow can be \ninstantiated with any controller whose behavior can be modeled in this way. The fout relation sends a \nmessage to a switch while fin receives a message from a switch. Both relations update the state s. There \nare two kinds of messages a switch can send to the controller: PktIn pt pk indicates that packet pk \nwas received on pt and did not match any entry in the .ow table.  BarrierReply n indicates that sw has \nprocessed all messages up to and including a BarrierRequest n sent earlier.  Data links. A data link \nL is a unidirectional queue of packets between two switch ports. To model bidirectional links we use \nsymmetric unidirectional links. Featherweight OpenFlow does not model packet-loss in links and packet-buffers. \nIt would be easy to extend our model so that packets are lost, for example, with some probability. Without \npacket loss, a packet traces paths from its source to its destinations (or loops forever). With packet \nloss, a packet traces a pre.x of the complete path given by our model under ideal conditions. Control \nlinks. A control link M is a bidirectional link between the switch and the controller that contains a \nqueue of controller messages for the switch and a queue of switch messages headed to the controller. \nMessages between the controller and the switch are sent and delivered in order, but may be processed \nin any order. 7. Veri.ed Run-Time System So far, we have developed a semantics for NetCore (Section 3), \na compiler from NetCore to .ow tables (Section 4), and a low-level semantics for OpenFlow (Section 6). \nTo actually execute NetCore programs, we also need to develop a run-time system that installs rules on \nswitches and prove it correct. 7.1 NetCore Run-Time System There are many ways to build a controller \nthat implements a Net-Core run-time system. A trivial solution is to simply process all packets on the \ncontroller. The controller receives input packets as PktIn messages, evaluates them using the NetCore \nsemantics, and emits the outputs using PktOut messages. Of course, we can do much better by using the \nNetCore com\u00adpiler to actually generate .ow tables and install those rules on switches using FlowMod messages. \nFor example, given the fol\u00adlowing program, dlDst = H1 and not(dlTyp = 0x800) . {|1|}  Location loc ::= \nsw \u00d7 pt Located packet lp ::= loc \u00d7 pk Topology T . loc -loc pg , T I {|lp|} =lp. {|lp|} lps ' = {|(T \n(sw, ptout ), pk) | (ptout , pk) . [pg ] sw pt pk|} (sw,pt,pk) pg , T I {|((sw, pt), pk)|} l {|lp1 \u00b7 \n\u00b7 \u00b7 lp|} ======. lps ' l {|lp1 \u00b7 \u00b7 \u00b7 lp |} n n Figure 8: Network semantics. the compiler might generate \nthe following .ow table, and the controller would emit three FlowMod messages: Add 5 {dlDst = H1, dlTyp \n= 0x800} {||} Add 4 {dlDst = H1} {|1|} Add 3 * {||} However, it would be unsafe to emit just these messages. \nAs dis\u00adcussed in Section 6, switches can reorder messages to maximize throughput. This can lead to transient \nbugs by creating intermedi\u00adate .ow tables that are inconsistent with the intended policy. For example, \nif the Add 3 * {||} message is processed .rst, all pack\u00adets will be dropped. Alternatively, if Add 4 \n{dlDst = H1} {|1|}is processed .rst, traf.c that should be dropped will be incorrectly forwarded. Of \nthe six possible permutations, only one has the prop\u00aderty that all intermediate states either (i) process \npackets according to the program, or (ii) send packets to the controller (which can evaluate them using \nthe program). Therefore, to ensure the switch processes the messages in order, the run-time system must \ninter\u00adsperse BarrierRequest messages between FlowMod messages. Network semantics. The semantics of NetCore \npresented in Sec\u00adtion 3 de.nes how a program processes a single packet at a single switch at a time. \nBut Featherweight OpenFlow models the behav\u00adior of an entire network of inter-connected switches with \nmultiple packets in-.ight. To reconcile the difference between these two, we need a network semantics \nthat models the processing of all packets in the network. In this semantics (Fig. 8), the system state \nis a bag of in-.ight located packets {|lp|}. At each step, the system: 1. Removes a located packet ((sw, \npt), pk), from its state, 2. Processes the packet according to the program to produce a new multiset \nof located packets,  {|lp1 \u00b7 \u00b7 \u00b7 lp|} = [pg ] sw pt pk, n 3. Transfers these packets to input ports, \nusing the topology, T (lp1) \u00b7 \u00b7 \u00b7 T (lpn), and 4. Adds the transferred packets to the system state. \n Note that this approach to constructing a network semantics is not speci.c to NetCore: any hop-by-hop \npacket processing function could be used. Below, we refer to any semantics constructed in this way as \na network semantics.  7.2 Run-Time System Correctness Now we are ready to prove the correctness of the \nNetCore run-time system. However, rather than proving this directly, we instead de\u00advelop a general framework \nfor establishing controller correctness, and obtain the result for NetCore as a special case. Bisimulation \nequivalence. The inputs to our framework are: (i) the high-level, hop-by-hop function the network is \nintended to im\u00adplement, and (ii) the controller implementation, which is required to satisfy natural \nsafety and liveness conditions. Given these pa\u00adrameters, we construct a weak bisimulation between the \nnetwork semantics of the high-level function and an OpenFlow network instantiated with the controller \nimplementation. This construction handles a number of low-level details once and for all, freeing de\u00advelopers \nto focus on essential controller correctness properties. We prove a weak (rather than strong) bisimulation \nbecause Featherweight OpenFlow models the mechanics of packet process\u00ading in much greater detail than \nin the network semantics. For ex\u00adample, consider a NetCore program that forwards a packet pk from one \nswitch to another, say S1 to S2, in a single step. An equiv\u00adalent Featherweight OpenFlow implementation \nwould require at least three steps: (i) process pk at S1, move pk from the input buffer to the output \nbuffer, (ii) move pk from S1 s output buffer to the link to S2, and (iii) move pk from the link to S2 \ns input buffer. If there were other packets on the link (which is likely!), additional steps would be \nneeded. Moreover, pk could take an even more circuitous route if it is redirected to the controller. \nThe weak bisimulation states that the NetCore and Feather-Weight OpenFlow are indistinguishable modulo \ninternal steps. Hence, any reasoning about the trajectory of a packet at the Net-Core level will be preserved \nin FeatherWeight OpenFlow. Observations. To de.ne a weak bisimulation, we need a notion of observation \n(called an action in the p-calculus). We say that the NetCore network semantics observes a packet (sw, \npt, pk) when it removes the packet from its state i.e., just before evaluating it. Likewise, a Featherweight \nOpenFlow program observes a packet (sw, pt, pk) when it removes (pt, pk) from the input buffer on sw \nto process it using the FWD rule. Bisimulation relation. Establishing a weak bisimulation requires exhibiting \na relation OF between the concrete and abstract states with certain properties. We relate packets located \nin links and buffers in Featherweight OpenFlow to packets in the abstract net\u00adwork semantics. We elide \nthe full de.nition of the relation, but de\u00adscribe some of its key characteristics: Packets (pt, pk) \nin input buffers inp on sw are related to pack\u00adets ((sw, pt), pk) in the abstract state.  Packets (pt, \npk) in output buffers outp on sw are related to packets located at the other side of the link connected \nto pt.  Likewise, packets on a data link (or contained in PktOut mes\u00adsages) are related to packets located \nat the other side of the data link (or the link connected to the port in the message).  Intuitively, \npackets in output buffers have already been processed and observed. The network semantics moves packets \nto new loca\u00adtions in one step whereas OpenFlow requires several more steps, but we must not be able to \nobserve these intermediate steps. There\u00adfore, after Featherweight OpenFlow observes a concrete packet \npk (in the FWD rule), subsequent copies of pk must be related to pack\u00adets at the ultimate destination. \nThe structure of the relation is largely straightforward and dic\u00adtated by the nature of Featherweight \nOpenFlow. However, a few parts are application speci.c. In particular, packets at the controller and \npackets sent to the controller in PktIn messages may relate to the state in the network semantics in \napplication-speci.c ways. Abstract semantics. So far, we have focused on NetCore to build intuitions. \nBut our bisimulation can be obtained for any controller that implements a high-level packet-processing \nfunction. We now make this precise with a few additional de.nitions.  De.nition 1 (Abstract Semantics). \nAn abstract semantics is de\u00ad.ned by the following components: 1. An abstract packet-processing function \non located packets: f (lp) = {|lp1 \u00b7 \u00b7 \u00b7 lp |} n 2. An abstraction function, c : s . {|lp|}, that identi.es \nthe packets the controller has received but not yet processed. Note that the type of the NetCore semantics \n(Fig. 8) matches the type of the function above. In addition, because the NetCore con\u00adtroller simply \nholds the multiset of PktIn messages, the abstraction function is trivial. Given such an abstract semantics, \nwe can lift it to a network semantics =lp. as we did for NetCore. We say that an abstract semantics is \ncompatible with a concrete controller implementation, consisting of a type of controller state s, and \ninput and output relations fin and fout , if the two satisfy the following conditions relating their \nbehavior: De.nition 2 (Compatibility). An abstract semantics and controller implementation are compatible \nif: 1. The controller ensures that all times packets are either (i) pro\u00adcessed by switches in accordance \nwith the packet-processing function or (ii) sent to the controller for processing; 2. Whenever the controller \nreceives a packet,  (sw, PktIn pt pk, s) -s ' it applies the packet-processing function f to pk to get \na multi\u00adset of located packets and adds them to its state c(s ' ) = c(s) l f(pk) 3. Whenever the controller \nemits a packet, s -(sw, PktOut pt pk, s ' ) it removes the packet from its state: c(s ' ) = c(s) \\ {|(sw, \npt, pk)|} 4. The controller eventually processes all packets (sw, pt, pk) in its state c(s) according \nto the packet-processing function, and 5. The controller eventually processes all OpenFlow messages. \n The .rst property is essential. If it did not hold, switches could process packets contrary to the \nintended packet-processing rela\u00adtion. Proving it requires reasoning about the messages sent to the switches \nby the controller. In particular, because switches may re\u00adorder messages, barriers must be interspersed \nappropriately. The second and third properties relate the abstraction function c and the controller implementation. \nThe fourth property requires the con\u00adtroller to correctly process every packet it receives. The .fth \nprop\u00aderty is a liveness condition requiring the controller to eventually process every OpenFlow message. \nThis holds in the absence of fail\u00adures on the control link and the controller itself. Given such a semantics, \nwe show that our relation between ab\u00adstract and Featherweight OpenFlow states and its inverse are weak \nsimulations. This implies that the relation is a weak bisimulation, and thus that the two systems are \nweakly bisimilar. Theorem 2 (Weak Bisimulation). For all compatible abstract se\u00admantics and controller \nimplementations, all Featherweight Open-Flow states s and s ', and all abstract states t and t ' : (sw,pt,pk) \nIf s OF t and s ------. s ', then there exists an abstract (sw,pt,pk) '' '' ' '' network state t such \nthat t ======. t and s OF t , and (sw,pt,pk) If s OF t and t ======. t ', then there exists a Featherweight \nOpenFlow state s '', and abstract network states si, s i ' such that (sw,pt,pk) s -. * si ------. si \n' -. * s '' and s '' OF t ' . In this theorem, portions of the OF relation are de.ned in terms of the \ncontroller abstraction function, c supplied as a parameter. In addition, the proofs themselves rely on \ncompatibility (De.nition 2). Finally, we instantiate this theorem for the NetCore controller: Corollary \n1 (NetCore Run-Time Correctness). The network se\u00admantics of NetCore is weakly bisimilar to the concrete \nsemantics of the NetCore controller in Featherweight OpenFlow. 8. Implementation and Evaluation We have \nbuilt a complete working implementation of the system described in this paper, including machine-checked \nproofs of each of the lemmas and theorems. Our implementation is available under an open-source license \nat the following URL: http://frenetic-lang.org Our system consists of 12 KLOC of Coq, which we extract \nto OCaml and link against two unveri.ed components: A library to serialize OpenFlow data types to the \nOpenFlow wire format. This code is a lightly modi.ed version of the Mirage OpenFlow library [16] (1.4K \nLOC).  A module to translate between the full OpenFlow protocol and the fragment used in Featherweight \nOpenFlow (200 LOC).  We have deployed our NetCore controllers on real hardware and used them to build \na number of useful network applications includ\u00ading host discovery, shortest-path routing, spanning tree, \naccess con\u00adtrol, and traf.c monitoring. Using the union operator, it is easy to compose these modules \nwith others to form larger applications. NetCore at home. For the past month, we have used a NetCore \ncontroller to manage the home network of one of the authors. Home networks are small, but they are dynamic \nas devices regularly con\u00adnect and disconnect. Our controller monitors the state of the net\u00adwork and dynamically \nrecon.gures an OpenFlow-enabled wireless router to support the current devices. Because Featherweight \nOpen-Flow does not model dynamic con.gurations, this controller s run\u00adtime system is necessarily unveri.ed. \nHowever, it uses the NetCore compiler as a veri.ed sub-system. In the future, we plan to enrich our OpenFlow \nmodel with additional features, including support for dynamic con.gurations. Controller throughput. Controller \nthroughput is important for the performance of S D Ns. The CBench [26] tool quanti.es con\u00adtroller throughput \nby .ooding the controller with PktIn messages and measuring the time taken to receive PktOut messages \nin re\u00adsponse. This is a somewhat crude metric, but it is still effective, since any controller must respond \nto PktIn messages. We used CBench to compare the throughput of our veri.ed controller with our previous \nunveri.ed NetCore controller, written in Haskell, and with the popular POX and NOX controllers, written \nin Python and C++ respectively. To ensure that the experiment tested throughput and not the application \nrunning on it, we had each controller exe\u00adcute a trivial program that .oods all packets. We ran the experiment \non a dual-core 3.3 GHz Intel i3 with 8GB RAM with Ubuntu 12.04 and obtained the results shown in Fig. \n9 (a). Our unveri.ed NetCore controller is signi.cantly faster than our veri.ed controller. We attribute \nthis to (i) a more mature back\u00adend that uses an optimized library from Nettle [27] to serialize  Veri.ed \nNetCore Unveri.ed NetCore 105 104 Controller Messages/sec Unveri.ed NetCore (Haskell) NOX (Python and \nC++) Veri.ed NetCore (OCaml) POX (Python) 26,022 16,997 9,437 6,150 (a) Control traf.c (bytes)Control \ntraf.c (bytes)  103 20 40 60 80 20 40 60 80 Time (seconds) Time (seconds) MicroFlow 105 104 103 Control \ntraf.c (bytes) 105 104 PacketOut 20 40 60 8020 40 60 80 Time (seconds) Time (seconds) (b) (c) Figure \n9: Experiments: (a) controller throughput results; (b) control traf.c topology; (c) control traf.c results. \nmessages, and (ii) Haskell s superior multicore support, which the controller exploits heavily. However, \ndespite being slower than the original NetCore, the new controller is still fast enough to be useful \nindeed, it is faster than the popular POX controller (al\u00adthough POX is not tuned for performance). We \nplan to optimize our controller to improve its performance in the future. Control traf.c. Another key \nfactor that affects S D N performance is the amount of traf.c that the controller must handle. This metric \nmeasures the effectiveness of the controller at compiling, optimiz\u00ading, and installing forwarding rules \nrather than processing packets itself. To properly assess a controller on these points, we need a more \nsubstantial application than .ood all packets. Using Net-Core, we built an application that computes \nshortest path forward\u00ading rules as well as a spanning tree for broadcast. We ran this pro\u00adgram on the \nsix-switch Waxman topology shown in Fig. 9 (b), with two hosts connected to each switch. In the experiment, \nevery host sent 10 IC M P (ping) broadcast packets along the spanning tree, and received the replies \nfrom other hosts along shortest path routes. We used Mininet [9] to simulate the network and collected \ntraf.c traces using tcpdump. The total amount of network traf.c during the experiment was 372 Kb. We \ncompared our Veri.ed NetCore controller to several others: a (veri.ed) PacketOut controller that never \ninstalls forwarding rules and processes all packets itself; our previous Unveri.ed Net-Core controller, \nwritten in Haskell; and a reactive MicroFlow controller [7] written in Haskell. The results of the experiment \nare shown in Fig. 9 (c). The graphs plot time-series data for every con\u00adtroller, showing the amount of \ncontrol traf.c in each one-second interval. Note that the y axis is on a logarithmic scale. In the plot \nfor our Veri.ed NetCore controller, there is a large spike in control traf.c at the start of the experiment, \nwhere the controller sends messages to install the forwarding rules generated from the program. Additional \ncontrol traf.c appears every 15 sec\u00adonds; these messages implement a simple keep-alive protocol be\u00adtween \nthe controller and switches. The Unveri.ed NetCore con\u00adtroller uses the same compilation and run-time \nsystem algorithms as our veri.ed controller, so its plot is nearly identical. The Mi\u00adcroFlow controller \ninstalls individual .ne-grained rules in response to individual traf.c .ows rather than proactively compiling \ncom\u00adplete .ow tables. Accordingly, its plot shows that there is much more control traf.c than for the \ntwo NetCore controllers. The graph shows how traf.c spikes when multiple hosts respond simultane\u00adously \nto an I CM P broadcast. The fourth plot shows the behavior of the PacketOut controller. Because this \ncontroller does not install any forwarding rules on the switches, all data traf.c .ows to the controller \nand then back into the network. Although these results are preliminary, we believe they demon\u00adstrate \nthat the performance of our veri.ed NetCore controller can be competitive with other controllers. In \nparticular, our veri.ed con\u00adtroller generates the same .ow tables and handles a similar amount of traf.c \nas the earlier unveri.ed NetCore controller, which was written in Haskell. Moreover, our system is not \ntuned for perfor\u00admance. As we optimize and extend our system, we expect that its performance will only \nimprove. 9. Related Work Veri.cation technology has progressed dramatically in the past decades, making \nit feasible to prove useful theorems about real sys\u00adtems including databases [18], compilers [15], and \neven whole op\u00aderating systems [14]. Compilers have been particularly fruitful tar\u00adgets for veri.cation \nefforts [11]. Most prominently, the CompCert compiler translates programs in a large subset of C to PowerPC, \nARM, and x86 executables [15]. The Veri.ed Software Toolchain project provides machine-checked infrastructure \nfor connecting properties obtained by program analysis to guarantees at the ma\u00adchine level [2]. Rocksalt \nveri.es a tool for analyzing machine code against a detailed model of x86 [23]. Another system, Bedrock \nprovides rich Coq libraries for verifying low-level programs [5]. Much earlier, a compiler for a Pascal-like \nlanguage was formal\u00adized and veri.ed as a part of the CLInc stack [31]. Signi.cant portions of many other \ncompilers have been formalized and veri\u00ad.ed, including the LLV M intermediate representation [33], the \nF* typechecker [25], and an extension of CompCert with garbage col\u00adlection [20]. Our work is inspired \nby all of these efforts, but is the .rst to tackle the challenge of building a veri.ed SD N controller. \nOver the past few years, a number of researchers have proposed high-level programming languages for controlling \nnetworks, in\u00adcluding C OOLAI D [4], F M L [10], Frenetic [7], NetCore [22], and PA N E [6]. This work \nuses NetCore [22] as a high-level network programming language. NetCore s original semantics was de.ned \nin terms of handwritten proofs and a complex abstract machine while we use machine-checked proofs and \nFeatherweight Open-Flow. In proving our compiler and run-time system correct, we dis\u00adcovered several \nbugs in the unveri.ed NetCore compiler and run\u00adtime. A portion of the PANE compiler was formalized in \nCoq, but since the proof did not model several subtleties of .ow tables, the compiler still had bugs. \nUnlike our system, PANE does not model or verify any portion of its run-time system. We used some of \nthe PA N E proofs during early development of our system. Lastly, Mi\u00adrage [16], a language for writing \ncloud applications, includes an S D N interface. Our OpenFlow serializers are based on Mirage s.  Formally \nVeri.able Networking (FV N) [28] is a platform for synthesizing protocol implementations from formal \nspeci.cations (though the synthesizer is unveri.ed). Our work attacks the prob\u00adlem of generating and \ndeploying correct network-wide con.gura\u00adtions, rather than building distributed routing protocols. We \nuse for\u00admal methods to build compilers, shifting the need for expertise with formal methods away from \nprogrammers. Xie et al. introduced techniques for statically analyzing the reachability properties of \nnetworks [29]. A number of tools for verifying network con.gurations have been built using these tech\u00adniques, \nincluding Header Space Analysis [12], Anteater [17], and VeriFlow[13]. These tools check whether the \ninstalled network rules have properties speci.ed by the programmer. Our system guarantees that the generated \nnetwork rules preserve the properties of the input program, enabling higher-level veri.cation. NI C E \n[3] uses model-checking and symbolic execution to .nd bugs in OpenFlow controllers written in Python. \nPortions of our Featherweight OpenFlow model are inspired by the bugs discov\u00adered in N I C E. Automatic \nTest Packet Generation [32] analyzes net\u00adwork con.gurations and constructs packets to achieve complete \ncon.guration testing coverage. Retrospective Causal Inference [24] detects minimal input sequences to \ninduce bugs in SDN systems. 10. Conclusions This paper presents a new foundation for network reasoning: \na de\u00adtailed model of OpenFlow, formalized in the Coq proof assistant, and a machine-veri.ed compiler \nand run-time system for the Net-Core programming language. Our main result is a general frame\u00adwork for \nestablishing controller correctness that reduces the proof obligation to a small number of safety and \nliveness properties. In the future, we plan to develop program logics for network pro\u00adgrams, extend Featherweight \nOpenFlow with additional features not included in our current core calculus, and improve the engi\u00adneering \naspects of our system. Acknowledgements. The authors wish to thank Andrew Ferguson and the PLDI reviewers \nfor many helpful comments, and members of the Mirage project, especially Anil Madhavapeddy, for devel\u00adoping \nthe OCaml CStruct and OpenFlow libraries. This work is supported in part by the NSF under grant CNS-1111698, \nthe ONR under award N00014-12-1-0757, and by a Google Research Award. References [1] E. Al-Shaer and \nS. Al-Haj. FlowChecker: Con.guration analysis and veri.cation of federated OpenFlow infrastructures. \nIn SafeCon.g, 2010. [2] A. W. Appel. Veri.ed software toolchain. In ESOP, 2011. [3] M. Canini, D. Venzano, \nP. Pere.s\u00b4ini, D. Kosti\u00b4c, and J. Rexford. A NICE way to test OpenFlow applications. In NSDI, 2012. [4] \nX. Chen, Y. Mao, Z. M. Mao, and J. van der Merwe. Declarative con.guration managaement for complex and \ndynamic networks. In CoNEXT, 2010. [5] A. Chlipala. Mostly-automated veri.cation of low-level programs \nin computational separation logic. In PLDI, 2011. [6] A. D. Ferguson, A. Guha, C. Liang, R. Fonseca, \nand S. Krishnamurthi. Hierarchical policies for software de.ned networks. In HotSDN, 2012. [7] N. Foster, \nR. Harrison, M. J. Freedman, C. Monsanto, J. Rexford, A. Story, and D. Walker. Frenetic: A network programming \nlanguage. In ICFP, 2011. [8] P. Gill, N. Jain, and N. Nagappan. Understanding network failures in data \ncenters: measurement, analysis, and implications. In SIGCOMM, 2011. [9] N. Handigol, B. Heller, V. Jeyakumar, \nB. Lantz, and N. McKeown. Reproducible network experiments using container-based emulation. In CoNEXT, \n2012. [10] T. L. Hinrichs, N. S. Gude, M. Casado, J. C. Mitchell, and S. Shenker. Practical declarative \nnetwork management. In WREN, 2009. [11] T. Hoare. The verifying compiler: A grand challenge for computing \nresearch. JACM, 50(1):63 69, Jan 2003. [12] P. Kazemian, G. Varghese, and N. McKeown. Header space analysis: \nStatic checking for networks. In NSDI, 2012. [13] A. Khurshid, X. Zou, W. Zhou, M. Caesar, and P. B. \nGodfrey. Veri.ow: Verifying network-wide invariants in real time. In NSDI, 2013. [14] G. Klein, K. Elphinstone, \nG. Heiser, J. Andronick, D. Cock, P. Derrin, D. Elkaduwe, K. Engelhardt, R. Kolanski, M. Norrish, T. \nSewell, H. Tuch, and S. Winwood. sel4: Formal veri.cation of an OS kernel. In SOSP, 2009. [15] X. Leroy. \nFormal veri.cation of a realistic compiler. CACM, 52(7): 107 115, Jul 2009. [16] A. Madhavapeddy, R. \nMortier, C. Rotsos, D. Scott, B. Singh, T. Gaza\u00adgnaire, S. Smith, S. Hand, and J. Crowcroft. Unikernels: \nLibrary op\u00aderating systems for the cloud. In ASPLOS, 2013. [17] H. Mai, A. Khurshid, R. Agarwal, M. Caesar, \nP. B. Godfrey, and S. T. King. Debugging the data plane with Anteater. In SIGCOMM, 2011. [18] G. Malecha, \nG. Morrisett, A. Shinnar, and R. Wisnesky. Towards a veri.ed relational database management system. In \nPOPL, 2010. [19] A. Markopoulou, G. Iannaccone, S. Bhattacharyya, C.-N. Chuah, Y. Ganjali, and C. Diot. \nCharacterization of failures in an operational IP backbone network. IEEE/ACM Transactions on Networking, \n16(4): 749 762, Aug 2008. [20] A. McCreight, T. Chevalier, and A. Tolmach. A certi.ed framework for compiling \nand executing garbage-collected languages. In ICFP, 2010. [21] N. McKeown, T. Anderson, H. Balakrishnan, \nG. Parulkar, L. Peterson, J. Rexford, S. Shenker, and J. Turner. Open.ow: Enabling innovation in campus \nnetworks. SIGCOMM CCR, 38(2):69 74, 2008. [22] C. Monsanto, N. Foster, R. Harrison, and D. Walker. A \ncompiler and run-time system for network programming languages. In POPL, 2012. [23] G. Morrisett, G. \nTan, J. Tassarotti, J.-B. Tristan, and E. Gan. RockSalt: Better, faster, stronger SFI for the x86. In \nPLDI, 2012. [24] R. C. Scott, A. Wundsam, K. Zari.s, and S. Shenker. What, Where, and When: Software \nFault Localization for SDN. Technical Report UCB/EECS-2012-178, EECS Department, University of California, \nBerkeley, 2012. [25] P.-Y. Strub, N. Swamy, C. Fournet, and J. Chen. Self-certi.cation: Bootstrapping \ncerti.ed typecheckers in F* with Coq. In POPL, 2012. [26] A. Tootoonchian, S. Gorbunov, Y. Ganjali, M. \nCasado, and R. Sher\u00adwood. On controller performance in software-de.ned networks. In HotICE, 2012. [27] \nA. Voellmy and P. Hudak. Nettle: Functional reactive programming of OpenFlow networks. In PADL, 2011. \n[28] A. Wang, L. Jia, C. Lio, B. T. Loo, O. Sokolsky, and P. Basu. Formally veri.able networking. In \nHotNets, 2009. [29] G. G. Xie, J. Zhan, D. A. Maltz, H. Zhang, A. G. Greenberg, G. Hj \u00b4almt \u00b4ysson, and \nJ. Rexford. On static reachability analysis of IP networks. In INFOCOM, 2005. [30] Z. Yin, M. Caesar, \nand Y. Zhou. Towards understanding bugs in open source router software. In SIGCOMM CCR, 2010. [31] W. \nYoung. Veri.ed compilation in micro-Gypsy. In TAV, 1989. [32] H. Zeng, P. Kazemian, G. Varghese, and \nN. McKeown. Automatic test packet generation. In CoNEXT, 2012. [33] J. Zhao, S. Nagarakatte, M. M. Martin, \nand S. Zdancewic. Formalizing the LLVM intermediate representation for veri.ed program transfor\u00admations. \nIn POPL, 2012.   \n\t\t\t", "proc_id": "2491956", "abstract": "<p>In many areas of computing, techniques ranging from testing to formal modeling to full-blown verification have been successfully used to help programmers build reliable systems. But although networks are critical infrastructure, they have largely resisted analysis using formal techniques. Software-defined networking (SDN) is a new network architecture that has the potential to provide a foundation for network reasoning, by standardizing the interfaces used to express network programs and giving them a precise semantics.</p> <p>This paper describes the design and implementation of the first machine-verified SDN controller. Starting from the foundations, we develop a detailed operational model for OpenFlow (the most popular SDN platform) and formalize it in the Coq proof assistant. We then use this model to develop a verified compiler and run-time system for a high-level network programming language. We identify bugs in existing languages and tools built without formal foundations, and prove that these bugs are absent from our system. Finally, we describe our prototype implementation and our experiences using it to build practical applications.</p>", "authors": [{"name": "Arjun Guha", "author_profile_id": "81331493565", "affiliation": "Cornell University, Ithaca, NY, USA", "person_id": "P4149099", "email_address": "arjun@cs.cornell.edu", "orcid_id": ""}, {"name": "Mark Reitblatt", "author_profile_id": "81490651977", "affiliation": "Cornell University, Ithaca, NY, USA", "person_id": "P4149100", "email_address": "reitblatt@cs.cornell.edu", "orcid_id": ""}, {"name": "Nate Foster", "author_profile_id": "81444600818", "affiliation": "Cornell University, Ithaca, NY, USA", "person_id": "P4149101", "email_address": "jnfoster@cs.cornell.edu", "orcid_id": ""}], "doi_number": "10.1145/2491956.2462178", "year": "2013", "article_id": "2462178", "conference": "PLDI", "title": "Machine-verified network controllers", "url": "http://dl.acm.org/citation.cfm?id=2462178"}