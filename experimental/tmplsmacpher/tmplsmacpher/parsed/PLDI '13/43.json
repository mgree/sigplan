{"article_publication_date": "06-16-2013", "fulltext": "\n Translation Validation for a Veri.ed OS Kernel Thomas Sewell Magnus Myreen Gerwin Klein NICTA &#38; \nUNSW, Sydney, Australia Cambridge University, UK NICTA &#38; UNSW, Sydney, Australia thomas.sewell@nicta.com.au \nmagnus.myreen@cl.cam.ac.uk gerwin.klein@nicta.com.au Abstract We extend the existing formal veri.cation \nof the seL4 operating system microkernel from 9 500 lines of C source code to the binary level. We handle \nall functions that were part of the previous veri.\u00adcation. Like the original veri.cation, we currently \nomit the assem\u00adbly routines and volatile accesses used to control system hardware. More generally, we \npresent an approach for proving re.nement between the formal semantics of a program on the C source level \nand its formal semantics on the binary level, thus checking the validity of compilation, including some \noptimisations, and linking, and extending static properties proved of the source code to the executable. \nWe make use of recent improvements in SMT solvers to almost fully automate this process. We handle binaries \ngenerated by unmodi.ed gcc 4.5.1 at opti\u00admisation level 1, and can handle most of seL4 even at optimisation \nlevel 2. Categories and Subject Descriptors D.4.5 [Operating Systems]: Reliability Veri.cation; D.2.4 \n[Software Engineering]: Soft\u00adware/Program Veri.cation General Terms Veri.cation, Languages Keywords Binary \nVeri.cation, seL4, Microkernel 1. Introduction Our aim is to extend one of the largest formal veri.cations, \nthat of the seL4 microkernel by Klein et al [11], from around 9 500 lines of C source code down to 11 \n736 instructions on the binary level. Recent successes in the formal veri.cation of sizeable programs \nat the implementation level, such as the CompCert C compiler [14], the Verisoft project [1], or the above \nmentioned seL4 microker\u00ad nel [11], suggest that formal veri.cation of small, high-importance software \nsystems is feasible and may become more common in the future. Unfortunately such software systems tend \nto be writ\u00adten in old, low-level, but high-performance languages like C whose standards are dif.cult \nto formalise, are by necessity purposely vi\u00adolated by systems programmers, and are rarely implemented \npre\u00adcisely by their toolchains. This leaves the prospective veri.er with a dif.cult choice. Clean-room \ndesigns of modern low-level systems languages are an attractive research topic, but tend to come with \npragmatic limitations, such as requiring garbage collection for type safety. The alternative is to proceed \nwith the best available seman- Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. PLDI 13, June 16 19, 2013, Seattle, WA, USA. Copyright c&#38;#169; 2013 \nACM 978-1-4503-2014-6/13/06. . . $15.00 tic model of the implementation language and program and hope \nthat any compiler defect or difference between the compiler s and the veri.er s interpretation can be \nfound by testing or other means. In their study on compiler testing, Yang et al reported 325 pre\u00adviously \nunknown defects in 11 different C compilers [34]. Even the formally veri.ed CompCert was found to exhibit \n5 defects, albeit in its unveri.ed front-end only it was the only compiler Yang et al did not manage \nto break in its code generation phase, despite devoting signi.cant resources to the task. We have used \nthis veri\u00ad.ed compiler on the veri.ed C source of the seL4 microkernel, but in this case found the result \nunsatisfactory: There is a remaining chance of a mismatch between CompCert s interpretation of the C \nstandard in the theorem prover Coq and the interpretation of the C standard the seL4 veri.cation uses \n[32] in the theorem prover Is\u00ad abelle/HOL [21], esp. in cases where the standard is purposely vio\u00ad lated \nto implement machine-dependent, low-level operating system (OS) functionality. Reconciling these two \nsemantics is non-trivial. Firstly the logics of Coq and Isabelle/HOL are not directly compat\u00adible. Secondly, \nsince the seL4 semantics is largely shallowly em\u00adbedded, an equivalence proof would have to be performed \nfor each program, similar to our more direct method below. In this paper we present a hybrid approach, \nin which we take precisely the semantics for that speci.c program (seL4) produced by Norrish s C parser \n[32] for Isabelle/HOL as used in the seL4 veri.cation. We then produce a model of the compiled binary \nof this program, extending the approach of Myreen [17, 18] in the HOL4 theorem prover [29], and we check \nthe soundness of this ex\u00ad tracted model against the strongly validated Cambridge semantics for the ARM \narchitecture [8]. Finally we use an SMT-based proof process to establish that the compiled binary matches \nthe expected semantics of the C source. Formally, we prove that the binary is a re.nement of the stated \nC source semantics. This property com\u00adposes with the existing functional correctness proof of seL4. Our \nmethod eliminates one of the Achilles heels of the veri\u00ad.ed compiler approach: the parser and lexer for \nconcrete C syntax. Since the high-level functional veri.cation and the binary veri.\u00adcation connect to \nprecisely the same formal artefact, the output of the now untrusted C parser, the method by which we \narrive at this artefact is irrelevant. Instead, this trust is replaced by trust in the bi\u00adnary veri.cation \ntool and into the import of an executable .le into hexadecimal numbers into the theorem prover HOL4. \nIn fact, it is not even important any more that the compiler con\u00adforms to the C standard, or that the \nC semantics in the theorem prover agrees with the C standard. All that matters is that compiler and prover \nsemantics match and thereby transport high-level prop\u00aderties that have been proved on the C code down \nto the binary level. For seL4, this includes functional correctness, integrity, authority con.nement, \nand non-interference [11, 15, 28]. We provide a full proof of correctness of the compilation by standard \ngcc 4.5.1 of all previously formally veri.ed functions [11] in seL4 at optimisation level 1. We additionally \nproduce proofs for all of seL4 s unveri.ed initialisation functions except for 3 nested loops. At optimisation \nlevel 2, which is the standard high\u00adperformance setting for seL4, we are able to automatically extract \na model for all but four assembly routines (i.e. coverage of 98 %) and produce proofs of correctness \nfor the vast majority of loop\u00adfree functions. Using optimisation level 1 rather than 2 incurs a performance \ncost of 15-20% depending on the system call.  Like the original seL4 veri.cation, we do not handle the \nas\u00adsembly routines and volatile accesses used to directly control the system hardware. The veri.cation \nstrategy is to .rst build a representation of both the C program and the binary within a single restricted \nintermedi\u00adate language. This language consists of simple control .ow mecha\u00adnisms and standard arithmetic \noperations which are common to the C language, CPU instruction sets and the SMT bit-vector theory. It \nis thus highly amenable to analysis by SMT solvers. To prove our re.nement statement, we extract SMT \nformulas with consideration towards program loops as well as proof obligations and assump\u00adtions arising \nfrom the decompilation process and C semantics. While our main target was extending the seL4 veri.cation \nto the binary level, we believe that this approach generalises to other C veri.cations and to other compilers. \nWe have for instance started experimenting with extending the decompilation phase to the out\u00adput of CompCert. \nWhile there are still a number of limitations to overcome that we discuss in Section 4, we believe this \nwork repre\u00ad sents evidence for a general, .exible method of extending formal reasoning about C programs \nto the binary level using standard off\u00adthe-shelf compilers. In the remainder of this paper, we .rst summarise \nthe pieces of previous work we build on in Section 2, before describing the conversions and proof process \nin Section 3. Section 4 evaluates the performance and limitations of our veri.cation method in more de\u00adtail. \nWe discuss related binary veri.cation approaches in Section 5. 2. Background In this section, we describe \nprevious background work our binary veri.cation of seL4 builds on. In particular, these are existing \nproofs on seL4 [11] together with its C semantics [32], and existing work on decompilation [17] and its \nvalidated ARM semantics [8]. 2.1 The seL4 microkernel veri.cation The seL4 kernel is a general-purpose \nOS microkernel measuring about 9 500 source lines of C code1 and 600 lines of assembly code. As usual \nfor microkernels, it provides a minimal set of mech\u00adanisms, including threads, interrupts, virtual memory, \nand inter\u00adprocess communication. Less usual is its capability-based manda\u00adtory access control model and \nexplicitly user-controlled, but kernel\u00adenforced memory allocation mechanism. It can be con.gured as a \nclassical microkernel, as a hypervisor running Linux instances, or as a high-assurance pure separation \nkernel. What makes this kernel special is that its functional correctness has been veri.ed over multiple \nlevels by formal re.nement from an abstract speci.cation down to the C source code level [11], and that \nfurther properties have been established over that speci.cation [28]. It represents a large body of existing \nwork, with over 200 000 lines of Isabelle/HOL proof, that our approach has to interface to if it is to \nextend this veri.cation to the binary level. Figure 1 illustrates the existing veri.cation stack on top \nand our new work interfacing with it on the bottom. The existing work starts with high-level properties \non top, abstract functional speci.cation next, design-level speci.cation further down, and the semantic \nC source code level at the bottom. Our work extends this stack by the semantic description of the binary, \nthereby removing compiler 1 Klein et al report 8 700 lines [11]. We are basing our work on a newer version \nof seL4 that supports further API features. Figure 1. Existing seL4 veri.cation stack. and Isabelle/HOL \nC parser from the trust chain of the proof and replacing it with the ARM semantics and our tool set. \nThe kernel exists for multiple architectures, among them Intel x86 (unveri.ed) and ARMv6/ARMv7 (veri.ed). \nIt is available as a commercial product under the name OKL4:veri.ed and for free download for academic \nuse [22]. Apart from its manageable size, key seL4 features for veri.a\u00adbility were that, as usual for \nmicrokernels, almost all device drivers and therefore most hardware interaction is outside the kernel \nin user-space, and the fact that seL4 is an event-based kernel for a uni-processor setting. This means, \ninterrupts are switched off for most kernel operations. For long-running operations, seL4 contains speci.c \npreemption points where it explicitly checks for interrupts. This allows the veri.cation to proceed on \na sequential language model where the explicit coarse-grained preemption points are nor\u00admal kernel exits. \nThis is important, both for the C semantics used in the original veri.cation as well as for the ARM semantics \nused in the binary decompilation work we build on. As illustrated in Figure 1, the bottom level of the \nexisting ver\u00ad i.cation work is a semantic Isabelle/HOL model of the C source code of seL4, generated \nautomatically by Norrish s C parser [32]. This parser instantiates a generic language framework for impera\u00adtive \nlanguages by Schirmer [27], consisting of a generic operational semantics framework with a Hoare-logic \nand veri.cation condition generator on top, which are proved sound and relatively complete in Isabelle/HOL. \nThis instantiation covers a large subset of stan\u00addard C99 [10] extended with architecture and compiler-speci.c \nas\u00ad sumptions, for instance about endianness and data layout, that sys\u00adtems programmers have to make. \nIts largest limitation is that it does not permit taking the address of local variables. This enables \nthe C model to treat parameters and local variables on the stack sepa\u00adrately from the pointer-addressable \nheap, thereby simplifying veri\u00ad.cation. This limitation could be lifted in future work at the expense \nof having to invest more explicit reasoning about this separation in C source-level veri.cations. The \nmemory model this parser uses in its semantic C embedding is due to Tuch [31, 32]. Its main feature is \nthat it provides abstract C types and comparatively convenient type-based reasoning on top of a precise, \nbyte-wise memory model that formalises the C heap merely as a function from 32-bit words to 8-bit words. \nType information is kept as a so-called heap type description in ghost state for veri.cation convenience, \nand Isabelle/HOL type classes establish abstractions on top. At the rare occasions where kernel code \nbreaks type safety, such as memory allocation, reasoning can fall back to the explicit byte level. This \nmodel enables high-level reasoning in the existing veri.cation, and is also compatible with the binary \nlevel in this work.  2.2 Decompilation from ARM into Higher-Order Logic In this paper, we present how \nwe proved seL4 correct down to the concrete ARM machine code that gcc produces. In order to reason formally \nabout ARM machine code we require a semantics for this machine code, i.e. a formal speci.cation of the \nARM instruction set architecture (ISA). For this purpose, we build on the Cambridge ARM ISA speci.cation \n[8] and proof tools, namely a proof-producing decompiler, which makes reasoning about such complex ISA \nspeci.cations tractable. The Cambridge ARM speci.cation has evolved from a string of ARM related projects \nthat started in 2000 with a hardware veri.ca\u00adtion project where Fox formally speci.ed the ARM ISA version \n3 and proved, using the HOL4 theorem prover [29], that a hardware implementation (the ARM6) implements \nits ISA correctly. Later, the project s focus shifted to software veri.cation: the ARM model was updated \nand extended to cover all modern versions of the ARM ISA, namely versions 4 7 including all operation \nmodes and every instruction. Since the latest model is no longer directly connected via proof to modern \nhardware implementations, Fox and Myreen have extensively validated the latest speci.cation against different \nreal hardware implementations [8]. This latest ARM speci.cation is the most comprehensive formal speci.cation \nof a commercial ISA that is publicly available. Due to its history, the Cambridge ARM speci.cation is \nhighly trustworthy, lengthy and very detailed. Reasoning manually in a theorem prover about such models \nis hopelessly tedious. For this reason, Myreen et al. developed automation that makes machine\u00adcode veri.cation \ntractable even for very complex ISA speci.ca\u00adtions. The main tool is called a decompiler. This tool aids \nprogram veri.cation by extracting a piece of the lengthy model, given a pro\u00adgram to interpret the model \non. More speci.cally, given a snip\u00adpet of machine code, this decompiler extract a function describing \nthe effect of running the code on the ISA speci.cation. Loops in the machine code turn into recursion \nin the extracted functions. The decompiler is automatic and proof producing: for each run, the decompiler \nproves that the extracted function is indeed accu\u00adrate w.r.t. the given code and the ISA speci.cation. \nMyreen [16] has shown that decompilation can be used for veri.cation of size\u00adable case studies, e.g. \ngarbage collectors and Lisp implementations. This paper builds on decompilation, extends it and shows \nthat it can be used in the veri.cation of the seL4 microkernel. 3. Correctness Proof This section describes \nour re.nement approach between C source semantics and binary code. The veri.cation process involves a \ncollection of representations of the input program which are outlined in Figure 2. The two inputs into \nthe process are the C program and binary ELF .le on the top of Figure 2. The three dotted boxes in the \ndiagram represent the three main proof systems used in the binary veri.cation: the interactive LCF-style \nprovers Isabelle/HOL and HOL4, and our SMT-based proof tool that is centred around a common intermediate \nlanguage describing control-.ow graphs. On the left of the diagram, the C program is parsed into the \ntheorem prover Isabelle/HOL using ex\u00adisting tools [32], and then transformed into a form closer to binary \ncode. The .rst such transformation step is within Isabelle/HOL, the second in our external tool. On the \nright of Figure 2, the bi\u00ad nary program is decompiled into HOL4, using the existing vali\u00addated Cambridge \nARM semantics [8]. Since the logics of the two theorem provers are almost identical, these can be translated \ninto Isabelle/HOL in a straightforward way to be exported to the inter\u00admediate graph language. Once the \nC program and binary have both been represented in the common graph language, on the bottom of Figure \n2. Artefacts in the correctness proof the diagram, re.nement between them can be proven with the as\u00adsistance \nof the two SMT solvers Z3 [7] and SONOLAR [23]. The following subsections present each of these steps \nin more detail. The SMT-based proof tool and some of the relevant Isabelle theories are available from \nhttp://www.ssrg.nicta.com.au/ software/TS/graph-refine/. 3.1 Conversion from C Semantics to the Graph \nLanguage This subsection describes the pseudo-compilation process which converts functions in the C model \ninto the intermediate graph language. This also serves as an introduction to the graph language. The \nidea of the graph conversion is to replace the language based control .ow rules with a simpler control \n.ow graph. An ex\u00adample conversion is given in Figure 3. All statements are numbered, and the steps between \nthem become graph edges, giving us a la\u00adbelled, directed graph. The point of doing this is that the context\u00addependent \neffects of break, continue and others are replaced by graph edges which simply specify the number of \nthe next statement. The special label Ret represents return from the function. The graph consists of \nthree types of nodes. Conditional nodes are used to pick between execution paths, and correspond closely \nto decisions made by if and while statements in C. Basic nodes represent normal statements, and update \nthe value of some variable with the result of some calculation (memory is represented as a variable). \nCall nodes are used to represent function calls, which are distinguished from other statements. A number \nof restrictions enforced by the C parser are relevant here: function calls may be embedded in other expressions \nand statements only in very limited ways, statements with multiple effects are forbidden, and switch \nstatements must always be convertible into a chain of if-else statements. The conversion also pseudo-compiles \nall C expressions. An ex\u00adample is on the right hand side of Figure 3. Insofar as C can be seen as a portable \nassembler, the conversion makes this explicit. Point\u00aders become 32-bit words, with address operations \nof various kinds becoming explicit arithmetic. Local variables of structure and array type are replaced \nby collections of local variables representing their .elds. Assignments of structure type are expanded \ninto a sequence of assignments for each .eld. Reads and writes of global variables become reads and writes \nof memory, at symbolic addresses which  Figure 3. Example Conversion of Structure and Statements to \nGraph Language are later instantiated by reading the ELF symbol table. The expres\u00adsions that remain are \nentirely machine compatible: operations such as 32-bit addition and multiplication, left and right shifts, \nsigned and unsigned less-than, and .nally memory access and update of 32-bit and 8-bit values. It is \na theorem of the Tuch memory semantics [31] that memory writes of aggregate types are equivalent to sequences \nof writes of their .elds, assuming the aggregate type contains no padding. Padding creates a number of \nheadaches for us, and so for the purposes of this work we have adjusted the seL4 source to ensure that \nall structures of interest are packed (see Section 4.3). The complication is that C is not merely a portable \nassembler. The C standard mandates a number of restrictions on the way various operations may be used, \nand some of these restrictions go beyond the scope of machine operations. One simple restriction is that \narithmetic on signed operands may not over.ow, and another is that dereferenced pointers must be aligned \nand nonzero. To ensure the standard is followed, the parser inserts a number of Guard statements into \nits output, which allow execution to continue only if some condition is met. The conditions become veri.cation \nobligations in the existing veri.cation work. These guard statements are translated into condition nodes \nwith one of the outbound edges pointing to the special label Err. Given that the guard conditions have \nall been established from the invari\u00adants in previous work, it will become an input assumption to the \ncurrent work that these paths to Err are never taken. These guards were omitted from Figure 3 there \nought to be condition nodes immediately before all memory-using nodes which check pointer validity. The \nkey restriction from the C standard that cannot be realised in machine operations is the strict-aliasing \nrule. This allows the compiler to assume that a given memory address is not in use with two different \nincompatible types. In systems code, program\u00admers occasionally break this assumption (see also Section \n4.4), but most code conforms to it. Since optimisations frequently make use of the rule, we need to make \nthe information it conveys available at the points where it does hold. To do this, we strengthened the \nchecks made in the C parser output against the heap type descrip\u00adtion, a global variable which tracks \nthe expected type of mem\u00adory. The graph conversion then includes the heap type descrip\u00adtion and checks \nin its output. The stronger checks are of the form pvalid htd t p for some heap type description htd, \nC type t and pointer value p. The expected non-aliasing conditions are theorems of the Tuch memory model \n[31], for instance this rule about ints and .oats: pvalid htd int p pvalid htd' .oat p {x | p = x < p \n+ 4} n {x | p ' = x < p' + 4} = {} The semantics of the graph language are straightforward to for\u00admalise \nin Isabelle/HOL or HOL4. The node types are introduced as datatype constructors Basic, Cond and Call. \nA single step start\u00ading from a Basic node updates local variables, and starting from a Cond node decides \nbetween two possible successor labels. The Call nodes create a new stack frame, with a new graph and \nnew local variables, and steps from the Ret and Err labels fold the cur\u00adrent stack frame into the previous \none. The semantics of execution are given by the transitive closure of this single-step relation. Given \nthis formal semantics, we have proven, in Isabelle/HOL, that the converted functions in the graph language \nre.ne the original C se\u00admantics. The details of this formalisation and proof are elided here, and are \nlargely technical.  3.2 Decompiling Compiler Output into Logic The next piece of the puzzle is the right-hand \nside of Figure 2, i.e. how we take binaries the compilers (recent versions of stan\u00addard unmodi.ed gcc) \nproduce, decompile these binaries into func\u00adtions, which we, in turn, translate into the graph language \ndescribed above. By decompilation we mean proof-producing extraction of functional programs from concrete \nbinaries. Here we build on pre\u00advious work on such decompilation [17, 18], but in this text we do not \nassume any prior knowledge of the previous work. Instead, we use a few examples below to demonstrate \nwhat decompilation pro\u00advides and what we had to alter in Myreen s original decompilation approach to \nmake the decompiler s output better compatible with the C semantics to which the left-hand side of the \nFigure 2 con\u00ad nects the decompiler s output. 3.2.1 Simple Decompilation Example To get a .avour of what \ndecompilation provides, consider the following simple C function for taking the average of two integers. \nuint avg (uint i, uint j) { return (i + j) / 2; } When compiled with gcc, this C code is translated \ninto an ARM bi\u00adnary (an ELF .le). Applying relevant objdump tools to the binary, one can produce a text \n.le showing the generated ARM machine code (on the left below) and ARM assembly code (in the centre). \n<avg>: e0810000 add r0, r1, r0 // add r1 to r0 e1a000a0 lsr r0, r0, #1 // shift r0 right e12fff1e bx \nlr // return To decompile the generated machine code, one simply provides the hex codes on the left to \nthe decompiler together with the signa\u00adture of the C function. The decompiler extracts from the machine \ncode a function in logic that describes the state update the machine code performs. The machine code \nabove is converted into the fol\u00adlowing logic function. Note that here, r0 and r1 are 32-bit integers \nand arithmetic + is over 32-bit integers (over.ow wraps around).  avg (r0, r1) = let r0 = r1 + r0 in \nlet r0 = r0 >>> 1 in r0 The decompiler also automatically proves a certi.cate theorem: a theorem relating \nthe extracted function avg with the original ma\u00adchine code. These certi.cate theorems are stated in terms \nof a total\u00adcorrectness Hoare logic with triples {pre }code {post } for machine code. They are de.ned \nand proved in terms of the underlying spec\u00adi.cation of the ARM instruction set architecture [8]. Precise \ndetails on their de.nition be found elsewhere [16]. Informally, the follow\u00ad ing Hoare triple can be read \nto say: if the program counter p points to the start of the machine code and r0, r1 and lr hold initial \nval\u00adues of registers 0, 1 and 14 respectively, then execution will reach a state where the postcondition \nis true, i.e. a state where the value of register 0 is described by avg (r0, r1) and the program counter \nis set to the return address lr. Informally, read * below as and , formally this is a separating conjunction \nfrom separation logic [25] but here used to separate between any machine code resources [16]. { R0 r0 \n* R1 r1 * R14 lr * PC p } p : e0810000 e1a000a0 e12fff1e { R0 (avg (r0, r1)) * R1 * R14 * PC lr } The \nbene.t of decompilation is that the extracted functions provide a convenient abstraction of the machine \ncode from which it is much more tractable to perform further proofs. Further proofs need only deal with \nthe extracted function because the certi.cate theorem states that the behaviour of the extracted functions \nand the original machine code agree according to the speci.cation of the instruction set architecture. \nHow is this form of proof-producing decompilation imple\u00admented in a theorem prover? The original approach \ndoes not use any heuristics and blindly follows the following steps: 1. For each instruction in the machine \ncode, evaluate the speci.ca\u00adtion of the instruction set architecture and prove a machine-code Hoare triple \ndescribing the effect of each instruction. 2. Construct a control-.ow graph (CFG) based on these Hoare \ntriples. Split the code into separate decompilation rounds, e.g. one round for each (nested) loop. 3. \nFor each decompilation round:  (a) Compose the Hoare triples for each path through the code segment, \nmerge these Hoare triples to produce a single Hoare triple describing this code segment. (b) If there \nis a jump to one of the entry points, then apply a special loop rule which introduces a tail-recursive \nfunction, (c) The result is a theorem stated as a Hoare triple. Its postcon\u00addition mentions the effect \nof executing the code segment, expressed as a function applied to the initial values. We read off this \nfunction from the postcondition and return both the (certi.cate) theorem and the extracted function. \n This form of decompilation is described in detail elsewhere [16] and has recently been extended [18] \nto also allow arbitrary use of code pointers, which the original approach struggled to handle.  3.2.2 \nC-Compatible (Stack Aware) Decompilation Decompilation as outlined above can effectively extract functions \nfrom machine code. However, the functions the decompilation pro\u00adduces are not immediately compatible \nwith the C semantics which we target. An extension of the avg example from above illustrates what goes \nwrong. Consider the following C function which takes eight integers as input and calculates their average. \nuint avg8 (uint i1, i2, i3, i4, i5, i6, i7, i8) {return (i1+i2+i3+i4+i5+i6+i7+i8) / 2; } When this is \ncompiled, gcc produces the following ARM assembly. Note that arguments i5-i8 are passed on the stack. \nHence the memory load instructions ldr and ldm. <avg8>: e0811000 add r1, r1, r0 e0811002 add r1, r1, \nr2 e59d2000 ldr r2, [sp] // load e0811003 add r1, r1, r3 e0810002 add r0, r1, r2 e99d000c e0800002 ldmib \nsp, {r2, add r0, r0, r2 r3} // load e0800003 add r0, r0, r3 e59d300c ldr r3, [sp, #12] // load e0800003 \nadd r0, r0, r3 e1a001a0 lsr r0, r0, #3 e12fff1e bx lr The original decompiler knew nothing about how \nC uses the stack and thus treats the stack accesses as ordinary memory accesses. This results in extracted \nfunctions where stack accesses touch memory explicitly, e.g. the last load from above turns into a line: \n. . . let r3 = m(r13 + 12) in . . . This .ts very badly with the C semantics for which this spilling \ninto the stack is completely hidden. The C semantics we use treats all local variables as simple register \nvariables, i.e. according to the C semantics avg8 does not make any memory accesses. To remedy this mismatch, \nwe made the decompiler aware of the stack and the C calling convention for ARM. We made the decom\u00adpiler \ntreat the stack as a separate datastructure. We formalised a new separation-logic inspired stack assertion \nstack sp n stack which is true if the stack pointer (register 13) holds a value sp such that the list \nof elements stack are on the stack and there is space for n elements above the stack pointer, i.e. n \nelements can safely be pushed onto the stack. We can keep this datastructure separate from the rest of \nmemory by proving pre/postconditions where the heap memory is separate from the stack using the separating \nconjunction * from separation logic [16, 25]. stack sp n stack * memory m The altered decompiler now \nhas a new stack simulation phase which attempts to discover where and how the stack pointer can travel \nthrough the code, i.e. it attempts to identify which instruc\u00adtions access the stack and what offsets \nare used, i.e. which stack elements are accessed. This phase comes immediately after CFG exploration \n(i.e. step 2 of the original algorithm). When run on the simple avg8 example, this stack heuristic .nds \nthat all the load instructions are stack accesses. For each of the stack accesses, it derives a new Hoare \ntriple stated in terms of the stack assertion. For example, the last load in the example above is described \nby the following Hoare triple. Here :: is list cons. { R3 r3 * stack sp n (s0::s1::s2::s3::ss) * PC p \n} p : e59d300c { R3 s3 * stack sp n (s0::s1::s2::s3::ss) * PC (p+4) } Given these Hoare triples stated \nin terms of stack, the rest of the decompiler runs just as before. The Hoare triple above turns into \na let-expression of the following form: . . . let r3 = s2 in . . .  With the stack-aware decompiling, \nthe entire code for avg8 turns into the following function which does not mention memory. We have expanded \nthe let-expressions for brevity below. avg8 (r0, r1, r2, r3, s0, s1, s2, s3) = (r1 + r0 + r2 + r3 + s0 \n+ s1 + s2 + s3) >>> 3 The generated certi.cate theorem is stated in terms of the stack assertion. For \navg8, this theorem is: {R0 r0 * R1 r1 * . . . * stack sp n (s0::s1::s2::s3::ss) * PC p} p : e0811000 \ne0811002 ... e12fff1e {let r0 = avg8 (r0, r1, r2, r3, s0, s1, s2, s3) in R0 r0 * R1 * . . . * stack sp \nn (s0::s1::s2::s3::ss) * PC lr} Care is taken to make these Hoare triples adhere exactly to the calling \nconvention so that they can be used in future decompilations of code that call this function. Using this \ncerti.cate theorem, we can decompile a call to avg8 into a let-expression of the following form in the \ndecompilation of a caller. . . . let r0 = avg8 (r0, r1, r2, r3, s0, s1, s2, s3) in . . . Given that we \ndo not allow C code to take the address of a local variable, this stack simulation is relatively straightforward \nto implement in practice, i.e. it is reasonably easy to automatically .nd which locations access the \nstack and which do not (even without use of any debug information). If we were to allow taking the address \nof a local variable, then this approach would fall apart very quickly as the distinction between memory \nand stack locations becomes blurred. Having said that, there is one situation where the C compiler will \nproduce code that passes an address of a stack location around, even if the C code does not seem to do \nso. This happens in cases where a function is to return a struct that does not .t into a single machine \nword, i.e. does not .t into a single register. When a func\u00adtion is to return a struct, it expects to \nget an address into the caller s stack space. This address points to a segment in the caller s part of \nthe stack into which the callee is to write its result. Our approach to dealing with the stack can manage \nthis form of passing around of pointers into the stack. The solution is to ini\u00adtialise the stack heuristic \nappropriately. Instead of starting the stack heuristic off from a state where the caller s stack is an \nopaque un\u00adtouchable part of state, we start it off with a slot for the return struct. If the result is \nn-words long, then we start the stack simulation from a stack which consists of an initial segment ss, \nthe result slots [t1, t2, . . . , tn] and .nally the rest of the stack stack: stack sp n (ss ++ [t1, \nt2, . . . , tn] ++ stack) We also assume (according to the calling convention) that register 0 holds, \non entry to the function, the address of the .rst result slot in the stack, i.e. sp + 4 \u00d7 length ss. \nWith such an initialisation, we can deal with the case of returning a struct directly into the stack. \nThe rest of the decompiler runs exactly as before. The new stack heuristic brings with it a number of \nnew limita\u00adtions, since the decompiler now tries to discover what the compiler did with the stack and \nprove that the stack is kept separate from the heap. Limitations are discussed in Section 4.2.  3.2.3 \nConverting Extracted Functions into Graph Format The pure functions extracted here can be easily converted \ninto the graph representation. An example is shown in Figure 4. We invent a collection of unique variable \nnames for the input values (x and y in the example), the (anonymous) output values (we pick r0 and r1 \nin the example), and each variable .xed in a let expression (a, b and c). If-then-else expressions become \ncondition nodes. Values, such as (3, x), become updates to variables, in this case to a and b as set \nby the let expression they appear in. Let expressions control order of Figure 4. Example conversion \nof let-expression to graph operations but create no nodes. Function calls become function call nodes. \nTail recursion becomes a graph arc back to the beginning of the function. This conversion is currently \ndone automatically, but without proof of correctness. If more assurance is desired, the transforma\u00adtion \ncould in future work be included on the Isabelle/HOL side of the tool and produce a corresponding certi.cate \ntheorem.  3.3 Proof of Re.nement The .nal step in our process is the re.nement proof, which we decompose \nfunction by function. We assume that we can treat other called functions as black boxes fully speci.ed \nby their cor\u00adresponding re.nement theorem, with the calling convention giving us enough information to \nrelate the C and binary behaviour. If the compiler makes use of inter-procedural analysis this may invali\u00addate \nour assumption; we currently do not support this case. This compositional approach reduces our large \ncode base to a series of manageable problems. As recommended in Pnueli et al s original de.nition of \ntransla\u00adtion validation [24], we divide the validation process into a search process and a checker, with \nthe search process discovering a proof script which the checker then validates. This separation is par\u00adtial, \nwith the two processes sharing some code and the proof steps checked being far from minimal. The proof \nscript consists of a problem space description together with a tree of proof rules Restrict, Split and \nLeaf. The rules give structure to the proof, but all the heavy lifting is done by converting proof goals \non the problem space into SMT problems. We will describe these proof components as the proof checker \nsees them. 3.3.1 Inlining and The Problem Space The .rst step in building a proof script is to establish \na problem space, a shared graph namespace into which the binary and C bodies of the function of interest \nare copied. The problem space is free to be modi.ed, in particular by inlining function calls. The search \nprocess attempts to inline suf.ciently that the two function graphs in the problem space can be proven \nequivalent. Inlining is done on the binary side of the problem whenever the called function does not \nmatch any C function, which may be be\u00adcause the compiler invented the function or modi.ed its signature. \nInlining is done on the C side of the problem at any call site which is reachable and which calls a function \nname that does not appear on the binary side. This simple heuristic may fail in the presence of selective \ninlining. It could also potentially inline far too much source code in the case where a function designated \npure or const was dropped because its result was ignored. This heuristic has, however, worked well at \nthe optimisation levels we currently address.  This heuristic considers the C and binary functions completely \nseparately, ignoring the relationship between them for now. This is a deliberate design decision allowing \nus to perform all inlining before any other analyses. The problem space produced after inlining is included \nin the proof script, and the checker trusts that this problem space is deriv\u00adable from the functions \nof interest. Checking that inlining was per\u00adformed correctly did not seem worthwhile.  3.3.2 Conversion \nto SMT The search and checker processes both use SMT solvers exten\u00adsively to make judgements about the \nC and binary execution. These executions will form a sequence of visits to nodes in the problem space \ngraph. The items of interest for a given node n will be the values of variables, should n be visited, \nat the point in execution that n is reached, and also the conditions under which n is reached. The .nal \nobjective of the proof process is to reason about the val\u00adues of returned variables should Ret be reached, \nand the conditions under which Err is reached. These valuations and conditions can be represented in \nthe SMT logic. Figure 5 shows nearly all of the steps of interest. The boxes show the path condition \nand variable values immediately before execution of each node. The function input variables are simply \nnamed, for instance xi, and are unknowns in SMT. The path con\u00addition at the starting node, 1, is simply \ntrue. Basic nodes, such as 1 and 3, update the variable state with values taken from the existing variable \nstate substituted into their expression, for instance, at node 1, x+ 1 is evaluated as xi + 1, and this \nis used to update the value of y. Condition nodes, such as 2, substitute their expression and add it \nto the path condition, such as the path condition at 3. When paths converge, such as at 4, the path condition \nto the node is the union of the conditions on each path, and the variable values are constructed using \nif-then-else expressions if the path via 3 was taken, x has the value from 3, otherwise from 2. Conveniently \nomitted from Figure 5 was the variable state af\u00ad ter calling function f at 4. The return value is named, \nfor instance z after 4 , and becomes another SMT unknown. The SMT con\u00adversion process notes the input \nand output values, and if any other call to f is made, adds SMT assertions that f will return the same \nvalues given the same inputs. If the two calls are on the C and bi\u00adnary side, the argument types may \ndiffer slightly, in which case the assertion takes into account the calling convention. Figure 5 is inaccurate \nin that it is fully expanded. The expres\u00ad sions computed at nodes 1 and 2 would have been given names \nus\u00ading SMTLIB2 s de.nition feature, for instance y after 1 = xi+1, cond at 2 = y after 1 < 3. This reduces \nthe syntactic expan\u00adsion that already is seen at node 4. This process does not handle loops, and the \ngeneralisation will be discussed in Section 3.3.4. The values here are all encoded in the SMTLIB2 QF \nABV logic (quanti.er-free formulae over arrays and bit-vectors). The variables and registers are all \nrepresented by 8-bit and 32-bit vectors. Mem\u00adory is represented as an SMTLIB array, mapping 30-bit to \n32-bit words. We .nd that this representation results in better SMT solver performance than the obvious \n32-bit to 8-bit array, since it makes the most common operations aligned 32-bit reads and writes simple \narray operations, while the rarer 8-bit reads and writes be\u00adcome more complex. Some values cannot be \nsimply encoded, for instance the heap type description and pointer validity assertions described earlier. \nWorkarounds for this are discussed in Section 3.3.6. 3.3.3 Simple Cases and the Leaf Rule With the problem \nspace established, and a process available for converting variable values and path conditions to SMT \nexpressions, the proof checker explores the proof tree. The starting assumptions are that the input variables \nequate as speci.ed by the calling con\u00advention, for example x = r0, y = r1 etc. The main objective is \nto show that the output variables, those taken on the arcs to the Ret label, are equal as speci.ed by \nthe calling convention. It may be assumed in proving this that the path to the Err label is never taken \non the C side. It must also be shown that the path to Err is never taken on the binary side. The Leaf \nrule instructs the proof checker to attempt to prove these .nal goals immediately, by converting the \nvalues of variables at Ret and the path conditions to Err into SMT values and check\u00ading that the negation \nof the required propositions is unsatis.able. 3.3.4 Path Restrictions and the Restrict Rule The conversion \nof variables and path conditions to SMT depends on the node of interest being reachable via some .nite \ncollection of paths. This is problematic for points which are reachable via a loop, which may be reachable \nafter any number of loop iterations. Consider a C function containing a single loop of this form: for \n(i = 0; i < 4; i ++) { ...} The compiler may well fully unroll this loop in the binary, since only 4 \ncopies are needed, making the binary loop-free. The proof script must replicate this logic. The SMT conversion \nprocess cannot describe in general the state at a node in a loop, but it can describe the 1st, 2nd or \nn-th visit to that node, for small values of n. The path condition for the 5th visit to the head of the \nloop described here can be converted to SMT, and the key observation is that this condition is always \nfalse. The Restrict rule names a node and a bound n, and instructs the proof checker to check that the \npath condition to the nth visit to that node is unsatis.able by SMT. The proof checker then intro\u00adduces \na restriction, which asserts that this node is reached less than n times. This promotes the semantic \nlimit on the loop iterations into a syntactic limit used by the SMT conversion process, which can now \nhandle nodes in and beyond the loop. Restrict proof script nodes have a single child, which contin\u00adues \nthe proof with the new restriction in force. In the case described, the subproof may be the Leaf rule, \nwhich, with the restriction avail\u00adable, can reason about Ret and Err and .nish the proof.  3.3.5 Split \nInduction The Split rule is used to handle cases that cannot be .nitely enumerated. The rule names a \nC split point c sp, a binary split point b sp, an equality predicate P and a bound n. Roughly speaking, \nthe checker will prove by induction that for each visit to b sp along the binary execution path, c sp \nis also visited with the variable state related by P . Formally, we de.ne c pci to be the condition that \nc sp is visited at least i times, b pci similarly, and |P |i to be condition that the P holds on the \nvalues of the variables at the i-th visit to c sp and b sp respectively. De.ne Ii to be the property \nthat b pci implies both c pci and |P |i. The checker shows .i > 0. Ii by n-ary induction, that is, by \nproving I1, I2, . . . In directly and also that the induction hypotheses Ii, Ii+1, . . . Ii+n-1 and i \n> 0 imply Ii+n. Having established that the sequence of visits to b sp is matched at c sp, we consider \nthree cases on the length of the sequence. If the sequence is in.nite, then the binary execution and \nC execution are both non-terminating, and this is a valid re.nement. The se\u00adquence may also contain n \nelements, in which case it ends with the elements i, i + 1 . . . i + n - 1. Thirdly, the sequence may \ncontain less than n elements. The proof script considers the lat\u00adter two cases via two subproofs, which \nare children of the Split node in the proof tree. In the loop proof, new hypotheses are in\u00adtroduced: \nb pci+n-1, \u00acb pci+n, Ii, Ii+1, . . . Ii+n-1. In the non\u00ad  Figure 5. Example Conversion to SMT looping \ncase, the new hypothesis is \u00acb pcn. In each case it is ex\u00adpected that the subproof will begin with two \nRestrict rules which use these hypotheses to restrict the number of visits into some .nite set. In the \nlooping case, the set of possible visit counts will be of the form {x | i = x < i + k} rather than {x \n| x < k}. This is an alternative form of the Restrict rule. Some slight generalisations to this induction \nare needed. Firstly, the Split rule may de.ne a sequence offset on either side. A C se\u00adquence offset \nof 2 means that we ignore the .rst two visits to b sp, so b pci is the condition that b sp is visited \nat least i + 2 times, and |P |i is computed on the variable state at the second visit after the i\u00adth \nvisit. This may be needed to handle various optimisations which affect the initial few iterations of \na loop, including a case where the binary sequence is shorter than the C sequence because some iter\u00adations \nhave been unpacked entirely. Secondly, the predicate P may be a function not only of the variable states \nat the respective i-th visits, but also of the value i and the variable states at the .rst visit. If \na C variable is incremented by 1 each iteration, it is simplest to record that it is i - 1 more than \nits .rst valuation. The search process discovers the Split rules essentially by an exhaustive search \nwith some minor optimisations. In practice this seems to be suf.cient, although loop problems are by \nfar the slow\u00adest problems for us to solve. In 33 of our 43 loops, the induction proof succeeds for the \n.rst candidate P for which I0, I1, . . . In-1 hold, whereas in the remaining 10 cases the early check \nwas mostly irrelevant and an average of 15 attempts were required to .nd a suc\u00adcessful condition. The \nvariation in these numbers is large, with the worst offending loop contributing 84 attempts, nearly half \nthe total.  3.3.6 Assertions Assertions are checks introduced by the C parser to ensure the stan\u00addard \nis respected. These checks have all been handled as proof obli\u00adgations in the seL4 veri.cation, and may \nnow be used as assump\u00adtions in this proof. One assertion of the C standard is that no NULL pointer ever \nbe dereferenced. The C parser produces a guard at every statement that uses a pointer which checks that \nthe pointer is non-NULL and appropriately aligned. These guards are converted into inequalities and bit \nchecks for the SMT solver, as are similar guards for arith\u00admetic over.ow, division by zero, etc. Note \nthat, for clarity, we omit these guards in our examples. There should, for instance, be a guard before \nthe x + 1 calculation in Figure 5 to check that x + 1 does not over.ow to negative. The most involved \nguards relate to the strict-aliasing rule in C. The compiler is entitled to assume that no address is \nsimultaneously in use with two different types. We adjusted the C parser to generate strong pointer validity \nassumptions pvalid htd t p for every pointer p that is used with type t when the global heap type description \nis htd. These assertions cannot be translated accurately into any -Machine Operations Inlined gcc -O1 \ngcc -O2 Instructions in Binary 11 736 12 299 Decompiled Functions 260 259 -Placeholders 3 Function Pairings \n260 225 Successes 234 145 Failures 0 18 Aborted 26 62 -Machine Operations 21 13 -Nested Loops 3 2 2 \n47 Time Taken in Proof 59m 4h 23m Table 1. Decompilation and Proof Results SMT theory. Instead, each \ntime we encounter and expression of this form, we introduce new booleans pvalid1, pvalid2, etc to represent \nthem. We then translate the following key theorem: pvalid htd t p pvalid htd ' t ' p distinct types t \nt ' ' ' ' {x | p = x < p + size(t )} n {x | p = x < p + size(t )} = {} The SMT form of this fact is pvalid1 \n.pvalid2 -. p + size(t) - '' ' 1 < p . p + size(t ) - 1 < p. We produce all such theorems, a possibly \nquadratic expansion, though the largest group of pvalid assertions on the same heap type description \nwhich we have seen in successful runs is 20. These assertions appear in path conditions in the C function \ngraph. The proof checker always assumes the negation of the path condition to Err in all its SMT checks, \nthus this information is always available. 4. Evaluation and Discussion 4.1 Results We report on two \nruns of the decompilation and proof, both for gcc builds of seL4 at optimisation level 1 and 2 respectively. \nTable 1 shows the results. Proof timings are taken on a single core of an Intel Core 2 Duo E8400. The \nmajority of the time taken is spent in the SMT solvers. A full decompilation run with proof certi.cates \ntakes an additional 6 8 hours on modern hardware. Our implementation is based on the original decompiler \nimplementation by Myreen et al. [17], which was not optimised for speed. Recent advances [18] may signi.cantly \nimprove this speed. There are 540 functions in seL4, but far less symbols in the binary after inlining. \nOur proof-producing decompiler is able to process the whole binary for gcc -O1 and, at the time of writing, \nall but 3 routines2 (i.e. 98 % of the binary) at -O2. Some functions at -O2 have optimised function signatures, \nand thus cannot be paired via the calling convention with their C counterparts (we address this problem \nat the re.nement stage by inlining these functions everywhere).  We produce proofs for all -O1 functions \nexcept for the 21 ma\u00adchine interface functions left abstract in the seL4 veri.cation, 3 sys\u00adtem initialisation \nfunctions involving nested loops, and 2 functions from the optional fastpath optimisation which inline \nmachine in\u00adterface operations. In the -O2 binary far more functions inline ma\u00adchine interface operations. \nThere are also a number of explicit proof failures, most of which are failures to .nd split points, usually \nbe\u00adcause of loop unrolling, loop invariant code motion and other loop structure optimisations. We use \nthe SMT solvers SONOLAR [23] and Z3 [7]. SONO-LAR solves all of the problems we pose to it whereas Z3 \ntimes out on many of the larger problems. We suspect this is because of our heavy use of the theory of \narrays (to represent memory) which SONOLAR is speci.cally designed to support. However Z3 sup\u00adports all \nof the SMTLIB2 input standard, including retraction of as\u00adsertions, whereas SONOLAR must be restarted \nfor each new prob\u00adlem, thus we get best performance by invoking Z3 with a timeout of 2 seconds and SONOLAR \nif Z3 fails.  4.2 Constraints of the Approach We aim to be general in supporting various .avours of \nC source, compiler and optimisation level. In practice we are limited by the restrictions of Norrish \ns C parser [32] we build on, our stack heuristic and the loop heuristics. The largest single restriction \nimposed by the existing C parser is that it forbids taking the addresses of stack variables. The parser \nalso forbids many uses of function pointers and all uses of variable\u00adlength argument lists [32]. In the \nseL4 veri.cation this restriction simpli.ed reasoning. As discussed in Section 3.2, it also enforces \na static separation of stack accesses and heap accesses here which is needed for the decompilation process \nstack heuristic. The C parser also mandates a level of conformity to the C stan\u00addard which turns out \nto be rare in practice. For instance, duplicat\u00ading a typedef statement, usually because of duplication \nof header .les, is illegal in the C standard and rejected by the parser. Global variables with names \nbeginning with underscores are also rejected. Compilers such as gcc are .exible in this regard, and various \nopen source projects we had hoped to use as additional case studies tend to abuse this .exibility w.r.t. \nthe standard. Both restrictions could be lifted easily in future work, since adherence to the standard \nbe\u00adcomes less important when binary veri.cation is available. The existing C semantics [32] as well as \nour graph language design fail to handle the case where the compiler has a genuine choice in data layout, \nsuch as when writing to a structure containing padding. In principle the parser and graph language could \ninclude some kind of unspeci.ed decision node here, which would then become an existentially quanti.ed \nvalue in the SMT problem. In practice we choose not to handle this situation, because embedded systems \ncode often intentionally avoids padding anyway and the elimination of padding is easy to achieve on the \nsource level. The stack heuristic tries to discover how the binary handles the stack and tries to prove \nthat the stack is kept separate from the heap. The exact use of the stack depends on the compiler, which \nmeans that our heuristics are fragile and at present only target recent versions of gcc. During the project, \nwe switched gcc versions a few times. Each time the stack heuristic required some .ne-tuning. 2 These \nbreak the calling convention, i.e. gcc -O2 has performed aggressive interprocedural optimisations. The \nnew decompiler uses the ARM/C calling convention to make sure that the functions it extracts take input \nand produce out\u00adput that match the input and output signatures of the correspond\u00ading C functions. This \ncorrespondence between signatures is impor\u00adtant because it allows us to modularise the veri.cation problem. \nHowever, when compiling with gcc -O2 we noticed that this corre\u00adspondence breaks down in certain places. \nSometimes gcc optimises function signatures, e.g. removing unused arguments where it can. This required \nspecial care. In the veri.cation, we avoid these bro\u00adken function boundaries by inlining (in the logic) \nthese problem\u00adatic functions, i.e. such function boundaries disappear. The loop heuristic presented in \nSection 3.3 depends on the existence of a split point pair, a point in the loop in the compiled binary \ncode at which execution synchronises with a point in the C loop. There is no logical reason such a point \nneed exist. In the case where a memory write is moved before or after an entire loop, no such point exists, \nand a more general formulation of problem splitting is required. This heuristic has proved suf.cient \nat low optimisation levels, however such generalisations will be needed at higher levels of optimisation. \nThe loop heuristic presented here also does not handle nested loops, which do not occur in the previously \nveri.ed part of seL4. To deal with nested loops, or with the compilation of loops with very complex control \nstructure, it may be necessary to search for multiple split point pairs. This could be attempted in roughly \nthe same manner as searching for a single split point pair, although the computational cost is likely \nto be high. Again, embedded systems code usually does not exhibit such complex control .ow within loops, \neven though simple nested loops may occur. We aim to handle these in future work. 4.3 Workarounds While \nwe aim to support all situations as automatically as possible, we had to make a small number of changes \nto seL4 for the proof to work. As mentioned above, we choose not to handle the situation where the compiler \nis given a genuine choice in data layout. The most common cause of this in seL4 was C enumeration constants, \nwhich the C standard de.nes as being of a type of the compiler s choice, which must accommodate all the \nenumerated values. It happens that gcc on ARM picks the shortest type available, leading to padding in \nmany structures. Furthermore the C parser guessed incorrectly that these types would be 32-bit. In principle \nthe parser could make this .exible, but in practice in Isabelle/HOL it is more dif.cult to have unknown \ntypes than it is to have unknown terms. We worked around this issue by changing the typedef state\u00adments \nused to de.ne the types actually used in seL4 to uint 32t rather than any enum types or uint 8t. This \neliminates all padding. There is a slight memory cost in a small number of structure types, but most \nstructure types remain the same size. We also adjusted a single function which loops over a short array \nstored in a struct in a stack variable, it being dif.cult for the stack heuristic to compute the bounds \non the offset used in the array. Finally, we optimised a function which called a number of generic seL4 \ncapability queries on capabilities of known type within a loop. The switch statement on the capability \ntype appeared in every unrolling of the loop and led to an explosion in the number of possible paths \ncausing the SMT solvers to diverge. Calling the query function speci.c to the capability type solved \nthe problem.  4.4 Issues Found We did not .nd any genuine compiler .aws during this analysis. The seL4 \nteam has reported experience with compiler defects in the past and .xed all known issues by rearranging \ncode, selecting appropriate compiler versions and disabling some compiler .ags, such as -fwhole-program. \n We did, however, .nd a number of small mismatches between C semantics and compiler. None of them were \nserious, but some effort was expended in removing them. These include the treatment of the strict-aliasing \nrule and the handling of reserved sections. The existing proof for seL4 did not speci.cally address the \nstrict-aliasing condition. It is one of the standard violations that sys\u00adtems code must make at some \npoint. The C parser generated guards asserting that pointers dereferenced were aligned and not NULL, \nbut no more. We strengthened these guards to assert that the point\u00aders were to objects given the correct \ntype in the heap type descrip\u00adtion to make use of that information in our SMT re.nement proofs. The veri.cation \nproof can be replayed essentially unchanged, since this fact was nearly always used as a step in proving \nthe align\u00adment conditions where they occurred anyway. Two problems re\u00admain where this strict-aliasing \ncondition is broken. Firstly, as a microkernel, seL4 does not use malloc and free, in\u00adstead implementing \nits own retype mechanism. The heap type de\u00adscription changes during retypes, potentially invalidating \nthe strict\u00adaliasing condition, and there is no way to inform the compiler of this. Fortunately this will \nnever create a problem since objects are never both accessed and retyped in the one system call. Secondly, \nlike standard C libraries, the kernel contains an op\u00adtimised word-at-a-time memset function which is \nused during this retype operation. This word-wise access in memset is not compati\u00adble with the later \ntype of these allocated objects. We had to tweak our C parser guard strengthening to explicitly omit \nthese writes. In principle the compiler might produce unwanted code if it suf.\u00adciently inlined and reordered \nthese functions. The fact that we prove re.nement in these functions is evidence this has not happened. \nReserved sections are regions of memory which should not be adjusted by normal operations. These include \nthe code, constant global objects and certain lookup tables generated by the compiler. With the strict \nstack/heap separation mandated by the C parser, this also includes the stack. The existing veri.cation \ndid not provide suf.cient mechanisms for proving such regions were not adjusted. To include them, we \nonce again strengthened C parser guards for the heap type description, making it clear at all locations \nin the C code that a given set of addresses is not covered by any type and thus unused. This set of addresses \nmodels the reserved regions. We then produce assumptions within the SMT proof that the code and data \nsections created by the compiler live within this set of reserved addresses, as does the stack. While \nwe can prove this condition is maintained, we have to assume it for the initial state. It would be desirable \nto make this assumption into a proof in future work, which would essentially extend the veri.cation to \nthe binary image as loaded in memory, as opposed to the binary image in the ELF .le, i.e. it would further \nremove loading and in-memory relocation from the trusted computing base. 5. Related Work Pnueli et al \n[24] proposed translation validation as a pragmatic al\u00ad ternative to compiler veri.cation. Over a decade \nof competition be\u00adtween these two approaches has yielded a collection of translation validation experiments \n[4, 9, 12, 19, 26, 30, 35, 36] as well as a few veri.ed compilers [5, 13]. This distinction is frequently \nblurred: some phases of the CompCert C compiler use translation valida\u00adtion whereas others are directly \nveri.ed. Our approach is yet another variation within the space of possi\u00adble translation validations. \nLike Tristan et al [30], we do not make use of any hints from the compiler. Like Ryabtsev [26], we tar\u00ad \nget the end-to-end transformation rather than any internal compiler step. Furthermore we do not accept \nany failures or false positives in the process. However, unlike other authors, we are prepared to make \nsmall adjustments to our source code if necessary. What differentiates this approach is that it is strongly \ngrounded in existing semantics at both ends, rather than the compiler s view of its input and output. \nAt the binary level we connect to the extensively validated Cambridge ARM semantics, and at the source \nlevel we connect to the veri.er s view of the C language which has been validated as useful through the \nexisting seL4 proofs. The Verisoft project [1] also uses a veri.ed source to binary con\u00ad version to produce \na veri.ed binary. The project developed a veri\u00ad.ed compiler for a Pascal-like language with C-like syntax \nwhich shared its semantic framework with the veri.cation environment. While the project clearly showed \nthat end-to-end theorems to the binary level are feasible, practical considerations such as perfor\u00admance \nwere not goals of the project. The CompCert [13] veri.ed C compiler could also be used to transport source-level \nproofs to the binary, by building a program logic on the semantics associated with the compiler s speci.ca\u00adtion \n[2]. In our case those semantics are presented differently to those assumed in seL4 s veri.cation, and \nin an incompatible logic. An alternative approach to binary veri.cation is to prove results directly \non the binary semantics with interactive tools. For instance, Bevier s KIT [3] was the .rst operating \nsystem to be completely veri.ed at the assembly level. However, it measured only a few hundred lines \nof assembly in total. More recently, Ni et al [20] ver\u00ad i.ed modern context switching code using the \nXCAP x86 model. Chlipala [6] presents a more sophisticated suite of tools for au\u00ad tomating most of such \nreasoning. Yang and Hawblitzel [33] used binary veri.cation on a mini\u00ad mal type safe language runtime, \nincluding garbage collection, to implement an OS kernel. The veri.cation establishes type safety for \nuser-level programs, but forces all applications into the same language framework. Our veri.cation achieves \nfull functional cor\u00adrectness of the kernel instead, and seL4 uses hardware mechanisms to enforce isolation \nfor arbitrary application programs. 6. Conclusion We have extended the existing formal functional correctness \nproof of the seL4 microkernel from the C source code down to the binary level. This means, the seL4 binary \nconforms to its high level correctness properties stated in Isabelle/HOL. The C source, its semantics, \nand the compiler need no longer be trusted. Our approach validates the output of compiler and linker \nby proving re.nement between the formal semantics of a program on the C source level and its formal semantics \non the binary level. Our re.nement theorem composes with further re.nement stacks on top of the source \ncode such as in seL4, and transports Hoare\u00adlogic properties proved on the source code down to the binary. \nWe achieved this by building on an existing well-established C semantics and strongly validated ARM semantics \nand decompila\u00adtion framework, extending both frameworks and translating them to a common intermediate \nformat particularly amenable to modern high-performance SMT solvers. While our main target was the seL4 \nmicrokernel, we believe that this approach in principle generalises to other C veri.cations, to other \ncompilers, and even to other similar programming lan\u00adguages. We think that the limitations of the C parser \nfront-end and the prover back-end, such as nested loops, can be overcome in fu\u00adture work to result in \na tool chain for fully automatically extending formal C veri.cation down to the binary level. This removes \ncom\u00adpilers from the trusted computing base of high-assurance systems, while still enabling the use of \noff-the-shelf compilation tool chains. Acknowledgements NICTA is funded by the Australian Government \nas represented by the Department of Broadband, Communications and the Digital Economy and the Australian \nResearch Council through the ICT Centre of Excellence program. The second author is funded by the Royal \nSociety, UK. This work was partially supported by EPSRC Research Grant EP/G007411/1.  References [1] \nE. Alkassar, W. Paul, A. Starostin, and A. Tsyban. Pervasive veri.\u00adcation of an OS microkernel: Inline \nassembly, memory consumption, concurrent devices. In P. O Hearn, G. T. Leavens, and S. Rajamani, editors, \nVSTTE 2010, volume 6217 of LNCS, pages 71 85, Edinburgh, UK, Aug 2010. Springer. [2] A. W. Appel. Veri.ed \nsoftware toolchain (invited talk). In G. Barthe, editor, Proc. 20th ESOP, volume 6602 of LNCS, pages \n1 17, Saarbr \u00a8  ucken, Germany, Mar. 2011. Springer. [3] W. R. Bevier. Kit: A study in operating system \nveri.cation. IEEE Trans. Softw. Engin., 15(11):1382 1396, 1989. [4] J. O. Blech, I. Schaefer, and A. \nPoetzsch-Heffter. Translation valida\u00adtion of system abstractions. In Proc. 7th Int. Conf. on Runtime \nveri.\u00adcation, RV 07, pages 139 150, Vancover, Canada, 2007. Springer. [5] A. Chlipala. A certi.ed type-preserving \ncompiler from lambda calcu\u00adlus to assembly language. In J. Ferrante and K. S. McKinley, editors, Proc. \nPLDI 07, pages 54 65. ACM, 2007. [6] A. Chlipala. Mostly-automated veri.cation of low-level programs \nin computational separation logic. In Proc. 32nd PLDI, pages 234 245, San Jose, California, USA, 2011. \nACM. [7] L. M. de Moura and N. Bj\u00f8rner. Z3: An ef.cient SMT solver. In C. R. Ramakrishnan and J. Rehof, \neditors, 14th TACAS, volume 4963 of LNCS, pages 337 340, Budapest, Hungary, Mar. 2008. Springer.  [8] \nA. Fox and M. Myreen. A trustworthy monadic formalization of the ARMv7 instruction set architecture. \nIn M. Kaufmann and L. C. Paulson, editors, 1st Int. Conf. Interactive Theorem Proving, volume 6172 of \nLNCS, pages 243 258, Edinburgh, UK, July 2010. Springer. [9] B. Goldberg, L. D. Zuck, and C. W. Barrett. \nInto the loops: Practical issues in translation validation for optimizing compilers. Proc 3rd Int. Workshop \non Compiler Optimization Meets Compiler Veri.cation (COCV 04). Electr. Notes Theor. Comput. Sci., 132(1):53 \n71, 2005. [10] ISO/IEC. Programming languages C. Technical Report 9899:TC2, ISO/IEC JTC1/SC22/WG14, \nMay 2005. [11] G. Klein, K. Elphinstone, G. Heiser, J. Andronick, D. Cock, P. Derrin, D. Elkaduwe, K. \nEngelhardt, R. Kolanski, M. Norrish, T. Sewell, H. Tuch, and S. Winwood. seL4: Formal veri.cation of \nan OS kernel. In Proc. 22nd SOSP, pages 207 220, Big Sky, MT, USA, 2009. ACM.  [12] S. Kundu, S. Lerner, \nand R. K. Gupta. Translation validation of high\u00adlevel synthesis. Trans. Comp.-Aided Des. Integ. Cir. \nSys., 29(4):566 579, Apr. 2010. [13] X. Leroy. Formal certi.cation of a compiler back-end or: program\u00adming \na compiler with a proof assistant. In J. G. Morrisett and S. L. P. Jones, editors, Proc. 33rd POPL, pages \n42 54. ACM, 2006. [14] X. Leroy. A formally veri.ed compiler back-end. J. Automated Reasoning, 43(4):363 \n446, 2009. [15] T. Murray, D. Matichuk, M. Brassil, P. Gammie, T. Bourke, S. Seefried, C. Lewis, X. \nGao, and G. Klein. seL4: from general pur\u00adpose to a proof of information .ow enforcement. In IEEE Symp. \nSe\u00adcurity &#38; Privacy, Oakland, CA, May 2013.  [16] M. O. Myreen. Formal veri.cation of machine-code \nprograms. PhD thesis, University of Cambridge, 2009. [17] M. O. Myreen, M. J. C. Gordon, and K. Slind. \nMachine-code ver\u00adi.cation for multiple architectures -an application of decompilation into logic. In \nA. Cimatti and R. B. Jones, editors, Formal Methods in Computer-Aided Design (FMCAD), pages 1 8. IEEE, \n2008. [18] M. O. Myreen, M. J. C. Gordon, and K. Slind. Decompilation into logic improved. In G. Cabodi \nand S. Singh, editors, Formal Methods in Computer-Aided Design (FMCAD), pages 78 81. IEEE, 2012. [19] \nG. C. Necula. Translation validation for an optimizing compiler. In Proc. PLDI 00, pages 83 94, Vancouver, \nBC, Canada, 2000. ACM. [20] Z. Ni, D. Yu, and Z. Shao. Using XCAP to certify realistic systems code: \nmachine context management. In Proc. 20th TPHOLs, volume 4732 of LNCS, pages 189 206. Springer, 2007. \n[21] T. Nipkow, L. Paulson, and M. Wenzel. Isabelle/HOL A Proof Assistant for Higher-Order Logic, volume \n2283 of LNCS. Springer, 2002. [22] Open Kernel Labs. seL4 research and evaluation download. http: //ertos.nicta.com.au/software/seL4/, \n2011. [23] J. Peleska, E. Vorobev, and F. Lapschies. Automated test case gen\u00aderation with SMT-solving \nand abstract interpretation. In Proc 3rd Int. Conf. NASA Formal methods, NFM 11, pages 298 312, Pasadena, \nCA, 2011. Springer. [24] A. Pnueli, M. Siegel, and E. Singerman. Translation validation. In B. Steffen, \neditor, Proc. 4th TACAS, volume 1384 of LNCS, pages 151 166. Springer, 1998. [25] J. C. Reynolds. Separation \nlogic: A logic for shared mutable data structures. In Logic in Computer Science (LICS), pages 55 74. \nIEEE Computer Society, 2002. [26] M. Ryabtsev and O. Strichman. Translation validation: From Simulink \nto C. In Proc. 21st Int. Conf. on Computer Aided Veri.cation, CAV 09, pages 696 701, Grenoble, France, \n2009. Springer. [27] N. Schirmer. Veri.cation of Sequential Imperative Programs in Is\u00adabelle/HOL. PhD \nthesis, Technische Universit\u00a8unchen, 2006. at M \u00a8 [28] T. Sewell, S. Winwood, P. Gammie, T. Murray, J. \nAndronick, and G. Klein. seL4 enforces integrity. In M. C. J. D. van Eekelen, H. Geuvers, J. Schmaltz, \nand F. Wiedijk, editors, 2nd Int. Conf. on Interactive Theorem Proving, volume 6898 of LNCS, pages 325 \n340, Nijmegen, The Netherlands, Aug. 2011. Springer. [29] K. Slind and M. Norrish. A brief overview of \nHOL4. In 20th Int. Conf. on Theorem Proving in Higher Order Logics, pages 28 32, Montreal, Canada, Aug. \n2008. [30] J.-B. Tristan, P. Govereau, and G. Morrisett. Evaluating value-graph translation validation \nfor llvm. In Proc. 32nd PLDI, pages 295 305, San Jose, CA, USA, 2011. ACM. [31] H. Tuch. Formal veri.cation \nof C systems code: Structured types, sep\u00adaration logic and theorem proving. J. Automated Reasoning: Special \nIssue on OS Veri.cation, 42(2 4):125 187, Apr. 2009. [32] H. Tuch, G. Klein, and M. Norrish. Types, bytes, \nand separation logic. In M. Hofmann and M. Felleisen, editors, Proc. 34th POPL, pages 97 108, Nice, France, \nJan. 2007. ACM. [33] J. Yang and C. Hawblitzel. Safe to the last instruction: automated veri.cation of \na type-safe operating system. In Proc. PLDI 10, pages 99 110, Toronto, Canada, 2010. ACM. [34] X. Yang, \nY. Chen, E. Eide, and J. Regehr. Finding and understanding bugs in C compilers. In M. W. Hall and D. \nA. Padua, editors, Proc. 32nd PLDI, pages 283 294, San Jose, CA, USA, June 2011. ACM. [35] L. D. Zuck, \nA. Pnueli, Y. Fang, B. Goldberg, and Y. Hu. Translation and run-time validation of optimized code. Runtime \nVeri.cation 2002 (RV 02). Electr. Notes Theor. Comput. Sci., 70(4):179 200, 2002. [36] L. D. Zuck, A. \nPnueli, and B. Goldberg. VOC: A methodology for the translation validation of optimizing compilers. J. \nUCS, 9(3):223 247, 2003.    \n\t\t\t", "proc_id": "2491956", "abstract": "<p>We extend the existing formal verification of the seL4 operating system microkernel from 9500 lines of C source code to the binary level. We handle all functions that were part of the previous verification. Like the original verification, we currently omit the assembly routines and volatile accesses used to control system hardware.</p> <p>More generally, we present an approach for proving refinement between the formal semantics of a program on the C source level and its formal semantics on the binary level, thus checking the validity of compilation, including some optimisations, and linking, and extending static properties proved of the source code to the executable. We make use of recent improvements in SMT solvers to almost fully automate this process.</p> <p>We handle binaries generated by unmodified gcc 4.5.1 at optimisation level 1, and can handle most of seL4 even at optimisation level 2.</p>", "authors": [{"name": "Thomas Arthur Leck Sewell", "author_profile_id": "81384620193", "affiliation": "NICTA and UNSW, Sydney, Australia", "person_id": "P4149096", "email_address": "thomas.sewell@nicta.com.au", "orcid_id": ""}, {"name": "Magnus O. Myreen", "author_profile_id": "81392605670", "affiliation": "Cambridge University, Cambridge, United Kingdom", "person_id": "P4149097", "email_address": "Magnus.Myreen@cl.cam.ac.uk", "orcid_id": ""}, {"name": "Gerwin Klein", "author_profile_id": "81100142186", "affiliation": "NICTA and UNSW, Sydney, Australia", "person_id": "P4149098", "email_address": "gerwin.klein@nicta.com.au", "orcid_id": ""}], "doi_number": "10.1145/2491956.2462183", "year": "2013", "article_id": "2462183", "conference": "PLDI", "title": "Translation validation for a verified OS kernel", "url": "http://dl.acm.org/citation.cfm?id=2462183"}