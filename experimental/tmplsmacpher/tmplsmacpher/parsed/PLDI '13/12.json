{"article_publication_date": "06-16-2013", "fulltext": "\n When Polyhedral Transformations Meet SIMD Code Generation Martin Kong Richard Veras Kevin Stock Ohio \nState University Carnegie Mellon University Ohio State University kongm@cse.ohio-state.edu rveras@cmu.edu \nstockk@cse.ohio-state.edu Franz Franchetti Louis-No \u00a8el Pouchet P. Sadayappan Carnegie Mellon University \nUniversity of California Los Angeles Ohio State University franzf@ece.cmu.edu pouchet@cs.ucla.edu saday@cse.ohio-state.edu \n Abstract Data locality and parallelism are critical optimization objectives for performance on modern \nmulti-core machines. Both coarse-grain par\u00adallelism (e.g., multi-core) and .ne-grain parallelism (e.g., \nvector SIMD) must be effectively exploited, but despite decades of progress at both ends, current compiler \noptimization schemes that attempt to address data locality and both kinds of parallelism often fail at \none of the three objectives. We address this problem by proposing a 3-step framework, which aims for \nintegrated data locality, multi-core parallelism and SIMD exe\u00adcution of programs. We de.ne the concept \nof vectorizable codelets, with properties tailored to achieve effective SIMD code generation for the \ncodelets. We leverage the power of a modern high-level transformation framework to restructure a program \nto expose good ISA-independent vectorizable codelets, exploiting multi-dimensional data reuse. Then, \nwe generate ISA-speci.c customized code for the codelets, using a col\u00adlection of lower-level SIMD-focused \noptimizations. We demonstrate our approach on a collection of numerical kernels that we automatically \ntile, parallelize and vectorize, exhibiting signi.\u00adcant performance improvements over existing compilers. \nCategories and Subject Descriptors D 3.4 [Programming lan\u00adguages]: Processor Compilers; Optimization \nGeneral Terms Algorithms; Performance Keywords Compiler Optimization; Loop Transformations; Af.ne Scheduling; \nProgram synthesis; Autotuning 1. Introduction The increasing width of vector-SIMD instruction sets (e.g., \n128 bits in SSE, to 256 bits in AVX, to 512 bits in LRBni) accentuates the importance of effective SIMD \nvectorization. However, despite the signi.cant advances in compiler algorithms [15, 26, 28, 30, 31, 43] \nover the last decade, the performance of code vectorized by current compilers is often far below a processor \ns peak. A combination of factors must be addressed to achieve very high performance with multi-core vector-SIMD \narchitectures: (1) effective reuse of data from cache the aggregate bandwidth to main memory on multi-core \nprocessors in words/second is far lower than the cumulative peak .op/second; (2) exploitation of SIMD \nparallelism on contiguously located data; and (3) minimization of load, store and shuf.e operations per \nvector arithmetic operation. Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are not made or distributed for \npro.t or commercial advantage and that copies bear this notice and the full citation on the .rst page. \nTo copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c \npermission and/or a fee. PLDI 13, June 16 19, 2013, Seattle, WA, USA. Copyright c @2013 ACM 978-1-4503-2014-6/13/06. \n. . $15.00 While hand tuned library kernels such as GotoBLAS address all the above factors to achieve \nover 95% of machine peak for spe\u00adci.c computations, no vectorizing compiler today comes anywhere close. \nRecent advances in polyhedral compiler optimization [5, 11] have resulted in effective approaches to \ntiling for cache locality, even for imperfectly nested loops. However, while signi.cant per\u00adformance \nimprovement over untiled code has been demonstrated, the absolute achieved performance is still very \nfar from machine peak. A signi.cant challenge arises from the fact that polyhedral compiler transformations \nto produce tiled code generally require auxiliary transformations like loop skewing, causing much more \ncomplex array indexing and loop bound expressions than the orig\u00adinal code. The resulting complex code \nstructure often leads to in\u00adeffective vectorization by even sophisticated vectorizing compilers such \nas Intel ICC. Further, locality enhancing transformations, e.g. loop fusion, can result in dependences \nin the innermost loops, in\u00adhibiting vectorization. In this paper, we present a novel multi-stage approach \nto over\u00adcome the above challenges to effective vectorization of imperfectly nested loops. First, data \nlocality at the cache level is addressed by generating tiled code that operates on data footprints smaller \nthan L1 cache. Code within L1-resident tiles is then analyzed to .nd af.ne transformations that maximize \nstride-0 and stride-1 refer\u00adences in parallel loops as well as minimization of unaligned loads and stores. \nThis results in the decomposition of a L1-resident tile into codelets. Finally, a specialized back-end \ncodelet optimizer ad\u00addresses optimization of register load/store/shuf.e operations, max\u00adimization of \naligned versus unaligned load/stores, as well as regis\u00adter level reuse. Target-speci.c intrinsics code \nis generated for the codelet by the back-end optimizer. This paper makes the following contributions. \n(1) It presents a novel formalization for an af.ne loop transformation algorithm that is driven by the \ncharacteristics of codelets.(2) Unlike most previous approaches that focus on a single loop of a loop-nest \nfor vectorization, it develops an approach that performs integrated analysis over a multi-dimensional \niteration space for optimizing the vector-SIMD code. (3) It represents the .rst demonstration of how \nhigh-level polyhedral transformation technology can be effectively integrated with back-end codelet optimization \ntechnology through a model-driven approach, with signi.cant performance improvement over production and \nresearch compilers. This paper is organized as follows. Sec. 2 gives a high-level overview of our approach, \nand Sec. 3 de.nes the concept of vec\u00adtorizable codelets, the interface between a high-level loop transfor\u00admation \nengine and a low-level ISA-speci.c SIMD code generator. Sec. 4 details our new framework for automatically \nextracting max\u00adimal codelets. Sec. 5 details the framework for ISA-speci.c SIMD code generation. Experimental \nresults are presented in Sec. 6, and related work is discussed in Sec. 7.  2. Overview The key to our \napproach to vectorization is a separation of tasks between a high-level program restructuring stage and \nan ISA\u00adspeci.c SIMD code generation stage. High-level program trans\u00adformation frameworks such as the \npolyhedral/af.ne framework excel at .nding loop transformation sequences to achieve high\u00adlevel, cost-model-based \nobjectives. Examples such as maximizing data locality [7], maximizing parallelism (.ne-grain or coarse-grain \n[11, 16, 29]) and possibly balancing the two [33, 34] illustrate the ability of the polyhedral framework \nto aggressively restructure pro\u00adgrams. On the other hand, SIMD-speci.c concerns such as ISA\u00adspeci.c vector \ninstruction selection, scheduling, and register pro\u00admotion have been implemented using frameworks that \ndepart sig\u00adni.cantly from classical loop transformation engines [15, 28, 30]. In this work we formalize \nthe interface between these two opti\u00admization stages the vectorizable codelet: a tile of code with spe\u00adci.c, \nSIMD-friendly properties. We develop a novel and complete system to generate and optimize vectorizable \ncodelets. High-level overview of algorithm The data locality optimization algorithm in a system like \nPluto [11] is geared towards exposing parallelism and tilability for the outer-most loops .rst, through \naggressive loop fusion. This is critical for achieving good L1 cache data locality through tiling, as \nwell as coarse-grained multi-core parallelization. However, for L1-resident code, those objectives are \noften detrimental: for effective exploitation of vector-SIMD ISA s, we need inner-most parallel loops, \nand the addressing of stride and alignment constraints. Other work has looked at the impact of unroll-and-jam \nfor SIMD vectorization [12], but they do not consider the problem of restructuring programs to maximize \nthe applicability of unroll-and-jam, nor any access stride constraint. In this work, we develop an integrated \nsystem aimed at recon\u00adciling the different considerations in optimizing for coarse-grain parallelism \nand cache data locality versus effective SIMD vector\u00adization. Our end-to-end strategy is as follows. \n1. Transform the program for cache data locality (see Sec. 4.2), using: (1) a model-driven loop transformation \nalgorithm for maximizing data locality and tilability [11]; (2) a parametric tiling method to tile all \ntilable loops found [5]; and (3) an algorithm to separate partial tiles and full tiles [1]. 2. For each \nfull tile, transform it to expose maximal vectorizable codelets (see Sec. 4), using: (1) a new polyhedral \nloop trans\u00adformation algorithm for codelet extraction; (2) unroll-and-jam along permutable loops to increase \ndata reuse potential in the codelet(s); and (3) an algorithm for abstract SIMD vectoriza\u00adtion, addressing \nhardware alignment issues. 3. For each vectorizable codelet, generate high-performance ISA\u00adspeci.c SIMD \ncode (see Sec. 5), using a special codelet com\u00adpiler akin to FFTW s genfft or the SPIRAL system. Further, \nuse instruction statistics or runtime measurements to autotune the codelets if alternative implementations \nexist.  3. Vectorizable Codelets the original program, which satisfy some speci.c properties that enable \neffective SIMD code to be synthesized for it using an ISA\u00adspeci.c back-end code generator. In order to \nillustrate the proper\u00adties, we proceed backwards by .rst describing the process of ab\u00adstract SIMD vectorization \non a code satisfying those properties. 3.1 Abstract SIMD Vectorization We use the term abstract SIMD \nvectorization for the generation of ISA-independent instructions for a vectorizable innermost loop. The \nproperties to be satis.ed by this inner-most loop are summa\u00adrized in the following de.nition: DEFINITION \n1 (Line codelet). A line codelet is an af.ne inner\u00admost loop with constant trip count such that: 1. there \nis no loop-carried dependence, 2. all array references are stride-1 (fastest varying array dimen\u00adsion \nincrements by one w.r.t. the innermost loop) or stride-0 (in\u00adnermost loop absent from array index expressions), \n 3. unaligned load/stores are avoided whenever possible.  To illustrate the process of abstract SIMD \nvectorization, Fig. 1 shows a code example which satis.es the above requirements. It is the transformed \noutput of a sample full tile by our codelet exposure algorithm, to be discussed later. Fig. 3 shows its \nabstract vector variant, after performing a basic unroll-and-jam as shown in Fig. 2. The abstract vector \ncode generation involves peeling the vectorizable loop with scalar prologue code so the .rst store (and \nall subsequent ones) will be aligned modulo V, the vector length. Similarly, an epilogue with scalar \ncode is peeled off. All arithmetic operators are replaced by equivalent vector operations, stride-0 references \nare replicated in a vector by splatting , stride-1 references are replaced by vload/vstore calls, and \nthe vectorized loop has its stride set to the vector length V. The arrays in this example, and the remainder \nof this paper, are assumed to be padded to make each row a multiple of the vector length V. f o r ( i \n= l b i ; i < u b i ; i + = 2 ) f o r ( i = l b i ; i < u b i ; + + i ) / / P r o l o g : p e e l f \no r a l i g n m e n t . f o r ( j = l b j ; j < u b j ; + + j ) { l b j 2 = &#38; ( A [ i -1 ] [ j ] \n) % V R : A [ i -1 ] [ j ] = B [ i -1 ] [ j ] ;  f o r ( j = l b j ; j < l b j + l b j 2 ; + + j ) \n{ S : B [ i ] [ j ] = C [ i ] * A [ i -1 ] [ j ] ;  A [ i -1 ] [ j ] = B [ i -1 ] [ j ] ; } B [ i \n] [ j ] = C [ i ] * A [ i -1 ] [ j ] ; } / / B o d y : c o d e l e t ( a b s t r a c t v e c t o r \ni z a t i o n ) u b j 2 = ( l b j 2 -u b j ) % V f o r ( j = l b j 2 ; j < u b j -u b j 2 ; j + = V \n) { v s t o r e ( A [ i -1 ] [ j ] , v l o a d ( B [ i -1 ] [ j ] ) ) ; Figure 1. After transfo. for \ncodelet exposure v s t o r e ( B [ i ] [ j ] , v m u l ( v s p l a t ( C [ i ] ) , v l o a d ( A [ i \n-1 ] [ j ] ) ) ; v s t o r e ( A [ i ] [ j ] , v l o a d ( B [ i ] [ j ] ) ) ; f o r ( i = l b i ; \ni < u b i ; i + = 2 ) v s t o r e ( B [ i + 1 ] [ j ] , v m u l ( v s p l a t ( C [ i + 1 ] ) , f o \nr ( j = l b j ; j < u b j ; + + j ) { v l o a d ( A [ i ] [ j ] ) ) ; R : A [ i -1 ] [ j ] = B [ i \n-1 ] [ j ] ;  } S : B [ i ] [ j ] = C [ i ] * A [ i -1 ] [ j ] ;  / / E p i l o g : p e e l f o r \nm u l t i p l e o f V . R : A [ i ] [ j ] = B [ i ] [ j ] ;  f o r ( j = u b j -u b j 2 ; j < u b j \n; + + j ) { S : B [ i + 1 ] [ j ] = C [ i + 1 ] * A [ i ] [ j ] ;  A [ i -1 ] [ j ] = B [ i -1 ] [ \nj ] ; B [ i ] [ j ] = C [ i ] * A [ i -1 ] [ j ] ; } } Figure 2. After unroll\u00adand-jam 2 \u00d71 Figure \n3. After abstract SIMD vect. Our objective in this work is to automatically transform full tiles into \ncode fragments satisfying De.nition 1 (i.e., Fig. 1), possibly interleaved with other code fragments \nnot matching those require\u00adments. Each vectorizable inner-loop is a vectorizable codelet. We show in \nSec. 4 how to leverage the restructuring power of the poly\u00adhedral transformation framework to automatically \nreshape the loop nests to satisfy the requirements when possible.  3.2 Vectorizable Codelet Extraction \nEf.cient vector code generation requires to exploit data reuse po\u00adtential. Speci.cally, given a program \nsegment that is L1-resident, we should maximize the potential for register reuse while increas\u00ading the \nnumber of vector computations in the codelet. To do so we use register tiling, or unroll-and-jam, which \namounts to unrolling outer loops and fusing the unrolled statements together. Our framework is well suited \nto perform this optimization: the polyhedral framework can restructure a loop nest to maxi\u00admize the number \nof permutable loops, along which unroll-and-jam can be applied. Arbitrarily complex sequences of fusion/distribu\u00adtion/skewing/shifting \nmay be required to make unroll-and-jam pos\u00adsible, and we show in Sec. 4 how to automatically generate \neffec\u00adtive sequences for a given program to maximizes the applicability of unroll-and-jam. As a result \nof our framework, the innermost vec\u00adtorizable loops (the line codelets) will contain an increased number \nof vector operations and operands for the ISA-speci.c synthesizer to optimize. De.nition 2 lists the \nvarious optimization objectives that drive the high-level transformation engine to ensure the cre\u00adation \nof good (i.e. large) candidate vector codelets.  DEFI NITION 2 (Maximal Codelet Extraction). Given a \nprogram P, maximal codelets are obtained by applying polyhedral loop transformations on P to obtain a \nprogram P ' such that: 1. the number of innermost loop(s) iterations which are parallel is maximized; \n 2. the number of references in P ' which are not stride-0 or stride-1 is minimized; 3. the number of \npermutable dimensions is maximized. 4. the number of unaligned stores is minimized; 5. the number of \nunaligned loads is minimized;  In this work, we apply maximal codelet extraction on each full tile in \nthe program. We then apply unroll-and-jam on all permutable loops, apply abstract SIMD vectorization \non all candidate codelets, before synthesizing ISA-speci.c SIMD code for each of them. 4. Framework for \nCodelet Extraction We now present our high-level framework to automatically trans\u00adform the program to \nextract candidate vectorizable codelets. 4.1 Polyhedral Framework To meet our goal of effectively transforming \na program (region) to expose maximal codelets, we use a powerful and expressive math\u00adematical framework \nfor program representation: the polyhedral model. In the present work, we leverage recent developments \non expressing various optimization constraints (e.g. loop permutabil\u00adity, data reuse, parallelism, etc.) \ninto a single optimization problem that can then be optimally solved using Integer Linear Program\u00adming \n[34, 39]. We .rst review the basis of program representation and optimization in the polyhedral model, \nusing the same notation as in [34]. Background and program representation The polyhedral frame\u00adwork is \na .exible and expressive representation for imperfectly nested loops with statically predictable control \n.ow. Loop nests amenable to this algebraic representation are called static control parts (SCoP) [16, \n19], roughly de.ned as a set of consecutive state\u00adments such that all loop bounds and conditional expressions \nare af.ne functions of the enclosing loop iterators and variables that are constant during the SCoP execution \n(whose values are unknown at compile-time). Numerous scienti.c kernels exhibit those proper\u00adties; they \nare found frequently in image processing .lters (such as medical imaging algorithms) and dense linear \nalgebra operations. Unlike the abstract syntax trees used as internal representation in traditional compilers, \npolyhedral compiler frameworks internally represent imperfectly nested loops and their data dependence \ninfor\u00admation as a collection of parametric polyhedra. Programs in the polyhedral model are represented \nusing four mathematical struc\u00adtures: each statement has an iteration domain, each memory ref\u00aderence is \ndescribed by an af.ne access function, data dependences are represented using dependence polyhedra and \n.nally the pro\u00adgram transformation to be applied is represented using a scheduling function. Iteration \ndomains For all textual statements in the program (e.g. R and S in Fig. 1) the set of its dynamic instances \nis described by a set of af.ne inequalities. When the statement is enclosed by one or more loops, all \niterations of the loops are captured in the iteration domain of the statement. Each executed instance \nof the statement corresponds to a point in the iteration domain; the coordinate of this point is de.ned \nby the value of the surrounding loop iterators when the statement instance is executed. Parametric polyhedra \nare used to capture loop bounds whose values are invariant through loop execution but unknown at compilation \ntime. These polyhedra use parameters in the inequalities de.ning their faces, and are a natural extension \nto standard polyhedra. For instance, in Fig. 1, the iteration domain of R is: DR = {(i, j) .Z2 |lbi =i< \nubi .lbj = j< ubj}. We denote by xxR the vector of the surrounding loop iterators; for R it is (i, j) \nand takes values in DR. Access functions They represent the location of the data accessed by the statement, \nin the form of an af.ne function of the iteration vector. For each point in DR, the access function FR(xxR) \nreturns the A coordinate of the cell of A accessed by this instance of the memory reference. We restrict \nourselves to subscripts of the form of af.ne expressions which may depend on surrounding loop counters \nand global parameters. For instance, the subscript function for the read reference A[i-1][j] of statement \nR is FA(i, j) = (i-1, j). R Data dependences The sets of statement instances between which there is a \nproducer-consumer relationship are modeled as equalities and inequalities in a dependence polyhedron. \nDependences are de\u00ad.ned at the reference granularity of an array element. If two in\u00adstances xxR and xxS \nrefer to the same array cell and one of these references is a write, a data dependence exists between \nthem. To respect program semantics, the producer instance must be executed before the consumer instance. \nGiven two statements R and S, a de\u00adpendence polyhedron, written DR,S, contains all pairs of dependent \ninstances (xxR,xxS). Multiple dependence polyhedra may be required to capture all dependent instances, \nat least one for each pair of array references accessing an array (scalars being a particular case of \nar\u00adray). It is possible to have several dependence polyhedra per pair of textual statements, as they \nmay contain multiple array references. Program transformation Program transformations in the polyhe\u00addral \nmodel are represented by a scheduling function. This function is used to reorder the points in the iteration \ndomain, and the corre\u00adsponding source code can be generated using polyhedral code gen\u00aderation [6]. A \nscheduling function captures, in a single step, what may typically correspond to a sequence of several \ntens of basic loop transformations [19]. It takes the form of a scheduling matrix TR , whose coef.cients \ndrive the program restructuring to be performed. DEFINITION 3 (Af.ne multi-dimensional schedule). Given \na state\u00adment R, an af.ne schedule TR of dimension m is an af.ne form of the d loop iterators (denoted \nxxR)and the p global parameters (de\u00adnoted xn). TR .Zm\u00d7(d+p+1) can be written as: .1,1 . . . . .1,d+p+1 \n. . xxR . TS(xxR) = . . . . . . . . . . . . xn . .m,1 . . . .m,d+ p+1 1 The scheduling function TR maps \neach point in DR to a mul\u00adtidimensional time-stamp (a vector) of dimension m. In the trans\u00adformed code, \nthe instances of R de.ned in DR will be executed in lexicographic order of their associated time-stamp. \nMultidimen\u00adsional timestamps can be seen as logical clocks: the .rst dimension similar to days (most \nsigni.cant), the next one to hours (less signif\u00adicant), etc. The optimization of a full program requires \na collection of af.ne schedules, one for each syntactic program statement. Even seemingly non-linear \ntransformations like loop tiling (also known as blocking) and unrolling can be modeled [19].  4.2 Program \nTransformation for L1-resident Blocking Tiling for locality involves grouping points in an iteration \nspace into smaller blocks (tiles) [43], enabling reuse in multiple direc\u00adtions when the block .ts in \na faster level of the memory hierarchy (registers, L1, or L2 cache). Tiling for coarse-grained parallelism \npartitions the iteration space into tiles that may be executed concur\u00adrently on different processors \nwith a reduced frequencyand volume of inter-processor communication. A tile is atomically executed on \na processor, with communication required only before and after ex\u00adecution. The tiling hyperplane method \n[11] is an effective approach for tiling of imperfectly nested af.ne loops. However, it can only generate \ntiled code for tile sizes that are .xed at compile-time. Al\u00adternatively, parametric tiling [5, 22], used \nin the present work, en\u00adables run-time selection of tile sizes. It has the advantage that tile sizes \ncan be adapted to the problem size and machine used without recompiling the code. The .rst stage of our \noptimization .ow is as follows. Given an input program, we .rst apply a model-driven optimization geared \ntowards maximizing the applicability of tiling [11]. Then parallel parametric tiling is applied, followed \nby full-tile separation. The process of separating full tiles leads to rectangular blocks of code whose \nloop bounds are functions of the tile sizes (which are run\u00adtime parameters). Each full tile is a potential \ncandidate for effec\u00adtive SIMD execution using the codelet extraction and synthesis ap\u00adproach detailed \nbelow. We note that the codelet extraction and syn\u00adthesis techniques are not limited to tilable programs. \nHowever, in our experiments in Sec. 6 we use benchmarks which can be pro\u00adcessed by parallel parametric \ntiling. The downside of parametric tiling is the generation of very com\u00adplex loop bounds for the loops \niterating on the various blocks (inter\u00adtile loops), typically involving complex compositions of ceil(), \nfloor() and round() expressions. Current production compilers of\u00adten fail to successfully analyze the \ndependences in such loops, and therefore are unable to automatically generate the best SIMD code for \nthe full tiles. This leads to signi.cant performance loss in the generated program. However, using the \ninformation extracted by the polyhedral compilation framework, full tiles can be manipu\u00adlated, transformed \nand vectorized using our proposed approach. 4.3 Convex Set of Semantics-Preserving Transformations A \nkey reason for the power and effectiveness of the polyhedral transformation framework is the ability \nto formulate, with a single set of (af.ne) constraints, a set of semantics-preserving (af.ne) program \ntransformations [34, 39]. An optimization problem whose solutions are subject to these constraints will \nnecessarily lead to a semantics-preserving program transformation. To build such a set of constraints, \nthe reasoning is as follows. First, we observe that for all pairs of dependent instances (x R,x S), the \ndependence is strongly satis.ed if T(x R) < T(x S), that is if the producer is scheduled before the consumer. \nAs T is in practice a multi-dimensional function, we can more precisely state that the dependence is \nstrongly satis.ed if T(x R) . T(x S), where . is the lexicographic precedence operator. This can be rewritten \nas T(x S) -T(x R) .x0. Alternatively, given Tp, row p of T, we have .p, Tp(x S) -T(x R) = dp with dp \n. {0, 1}. We note that once a dependence has been strongly satis.ed at a dimension d, then it does not \ncontribute to the semantics constraint and so we have instead .p> d, Tp(x S) -Tp(x R) =-8 (that is, a \nvoid constraint with no impact on the solution space). The constraints for semantics preservation are \nsummarized in Def. 4, and are the starting basis for our new optimization algo\u00adrithm. For each row p \nof the scheduling matrix T and each de\u00adpendence polyhedron DR,S, we associate a Boolean decision vari\u00ad \nable dDp R,S ; these decision variables are used to encode semantics\u00adpreserving properties of a schedule \nso that for each pair of instances in dependence the source instance will be scheduled necessarily before \nthe target instance. We bound the coef.cients of T to be in some arbitrary range, so that we can .nd \nsome arbitrarily large K .Z such that the form (K.xn+ K) is an upper bound of the sched\u00adule latency[34], \ntherefore Tp(x S) -Tp(x R) =-(K.xn+ K) is equiv\u00adalent to Tp(x S) -Tp(x R) =-8. DEFINITION 4 (Semantics-preserving \naf.ne schedules). Given a set of af.ne schedules TR , TS . . . of dimension m, the program se\u00admantics \nis preserved if the three following conditions hold: DR,S DR,S DR,S (i) .DR,S, .p, dp .{0, 1} m (ii) \n.DR,S, . dDp R,S = 1 (1) p=1 (iii) .DR,S, .p.{1, . . . , m}, .(x R,x S).DR,S, (2) p-1 TS p(x S) -TR \np(x R) =- . d.(K.xn+ K) + dp k k=1 As Feautrier proposed, the af.ne form of the Farkas lemma is used \nto build the set of all non-negative functions over a polyhedron [16], a straightforward process as Eq. \n2 represents a non-negative function and DR,S is a polyhedron. As a result, a convex set of constraints \nlinking the various .i, j coef.cients and the d variables is obtained, such that each point in this space \nis a semantics\u00adpreserving schedule [34].  4.4 Codelet-Speci.c Cost Functions We now present the set \nof additional constraints we use to formu\u00adlate our scheduling optimization problem. The complete schedul\u00ading \nalgorithm works in two steps: .rst we .nd a schedule for the program that maximizes parallelism, the \nnumber of stride-0/1 ref\u00aderences, and permutability. Then we apply a second algorithm for minimizing \nthe number of unaligned load/stores on the transformed program. 4.4.1 Additional Properties on the Schedule \nIn the present work, we impose speci.c properties on T: .i, j .N. Enforcing non-negative coef.cients \ngreatly simpli.es the process of .nding good schedules. Large coef.cients for T can lead to complex code \nbeing generated, containing modulo operations [6], and small values for the coef.cients are usually preferred \n[32]. Our ILP formulation attempts to minimize the value of coef.cients, therefore making them as close \nto 0as possible when .i, j =0. In addition, we use the so-called 2d+1 schedule form [19], which forces \nthe scheduling function to be an interleaving of linear and constant dimensions. The number of rows of \nTS is set to 2d+ 1 if S is surrounded by d loops in the original sub-program, and every odd row of TS \nis made constant (that is, .i, j = 0 for all j except j= d+ n+ 1). For instance, in this 2d+1 form, the \noriginal schedule of Fig 1 is TR : (0, i, 0, j, 0) and TS : (0, i, 0, j, 1). Those are computed simply \nby building the AST of the sub-program with loops and statements as nodes, and edges between a parent \nand its children in the AST. Edges are labeled by the syntactic order of appearance of the children, \nand to compute the original schedule one simply takes the path from the root to the statement, collecting \nthe node labels and the edge labels along this path. While not technically required for this work, the \n2d+1 form provides a standardized notation for the schedules, ensures that at least one schedule exists \nin the considered search space (the identity schedule, as shown above), and tends to simplify the generated \nloop bounds when implementing full distribution through the scalar dimensions.  4.4.2 Minimizing Dependence \nDistance The .rst objective we encode is designed to .nd solutions with good data locality. It also helps \nwith other objectives. The mini\u00admization .nds a function that bounds the dependence distance [11], for \neach non-constant scheduling level (the even rows). This func\u00adtion is a form of the program parameters \nxn, and requires additional constraints on T: all coef.cients associated with the parameters xn are set \nto 0. This only removes parametric shifts, and is therefore not a limiting factor unless there are dependences \nof parametric dis\u00adtances in the program, a rare case. This bounding function is de.ned as follows, for \neach row k of the schedule: uk.xn+ wk =TS(x S) -TR(x R) (x R,x S).DR,S (3) uk .Np, wk .N Intuitively, \nthe value of uk.xn + wk is an indicator of an upper bound on the distance between dependent iterations. \nTo minimize the distance between dependent iterations, we minimize the value of uk and wk. If, after \nminimization, uk.xn+ wk = 0, then the depen\u00addent iterations are scheduled at exactly the same time for \nthis loop level, i.e., the loop is parallel. 4.4.3 Maximizing Fine-grain Parallelism Maximizing .ne-grain \nparallelism requires that no dependences in the generated code are carried by the innermost loop(s). \nFeautrier addressed the problem of maximizing .ne-grain parallelism through an aggressive strategy of \nstrongly satisfying all dependences as early as possible [16]. We propose a different form: minimization \nof the number of dependences to be solved by the schedule di\u00admension 2d (e.g., the one corresponding \nto the inner-most loop), together with the objective of minimizing the dependence distance at that level. \nWe use the following optimization objective: DR,S min . d2d (4) DR,S Indeed, if this sum is equal to \n0, and if u2d.xn+ w2d (from the minimization of the dependence distance at that level) is also equal \nto 0, then the inner-most loop is parallel as no dependence is carried by this loop. Minimizing both \nobjectives at the same time ensures that we discover inner-most parallel loops whenever possible.  4.4.4 \nMaximizing Stride-0/1 References We propose a framework to embed directly, as constraints on the coef.cients \nof T, the maximization of the number of stride-0/1 references. It is a complex task to model such a constraint \nas a convex optimization problem, so that we can use standard solvers to .nd coef.cients of T. The reader \nis referred to a technical report [27] for detailed examples. Access functions after scheduling In a \nnutshell, we aim for suf\u00ad.cient conditions for an array reference to be stride-0/1, when the innermost \nloop is parallel. These conditions come from properties of the polyhedral code generation process which \nbuilds the ex\u00adpressions connecting the input iterator symbols (e.g., i, j) with the new loop iterators \ncreated (e.g., t2, t4). For instance, TR = (0, i+ j, 0, j, 0) is a valid schedule for R in Fig. 1. Two \nloops are created after code generation, and their schedule is given by the even dimensions of T: t2= \ni+ j, t4= j. In the access A[i-1][j], after loop transformation, i and j are replaced by expressions \nin the form of the new loop iterators t2, t4. This process is done by constructing a system of equalities \nbetween the tX variables and the original iterators, based on the schedule, and performing Gaus\u00adsian \nelimination. Equalities with one original iterator on the left hand side are then obtained. With the \nabove example, we would get A[t2-t4-1][t4], which is not a stride-1 reference along t4, the inner loop. \nOur approach to model suf.cient conditions for the stride of a reference to be 0or 1is to reason about \nthe system of equalities built to create the new references after loop transformations. Speci.cally, \nwe want to ensure that the innermost loop iterator (to be vectorized) appears only in the right-most \nindex function (the Fastest Varying Dimension, FVD) of an array reference, or does not appear at all \nin the reference. Otherwise, the reference is not stride-0/1. As an illustration of a suf.cient condition \nfor stride-1: assuming the innermost non-constant schedule dimension p represents a par\u00adallel loop, then \nfor a reference to be stride-1 it is enough to have (1) the schedule coef.cients for rows less than p \ncorresponding to the iterators present in non-FVD functions are non-zero at least once; (2) for at least \none of the iterators present in the FVD function, the schedule coef.cients for rows less than p corresponding \nto this iterator are all zero. Convex encoding of access stride properties To encode this kind of property, \nwe resort to two complementary structures in the ILP. First, a new set of Boolean variables .i, j is \nintroduced, to help us model when a schedule coef.cient .i, j is 0 or not. There are as many.i, j variables \nas there are .i, j variables, and for each of them we set .i, j = .i, j. Intuitively, we sum these . \nvariables to count how manyschedule coef.cients are non-zero. For .i, j to accurately capture when a \nschedule coef.cient is non-zero, we maximize . .R i, j so that .R is set to 1 as soon as .R is greater \nor equal to 1.1 i, j i, j Second, we use an auxiliary representation of the access func\u00adtions, using \na normalized matrix GF for an access function F. Intu\u00aditively, it is used to represent which iterators \nappear in the non-FVD index functions, and which appear in the FVD with a coef.cient of 1. GF has two \nrows: all iterators appearing in the non-FVD are represented in the .rst row, and in the second row iterators \nappear\u00ading in the FVD with a coef.cient of 1 are represented. Precisely, for the access function matrix \nF of dimension l\u00d7d+ n + 1, GF is constructed as follows. (1) GF has 2 rows and d columns. (2) .i . {1..l-1}, \n.j. {1..d}, if Fi, j 0 then GF = 1, GF = 0 = 1, j 1, j otherwise; (3) .j.{1..d}, if Fl, j = 1 then GF \n= 1, GF = 0 oth\u00ad 2, j 2, j erwise. This leads to the following de.nition [27], wherein besides the . \nvariables we also de.ne new Boolean variables \u00b5F j and .F j (one pair per loop surrounding the reference \nF), and sF and sF to model this 1 2 seemingly non-convex problem in a convex fashion. DEFINITION 5 (Constraint \nfor Stride-0/1). Given a statement R surrounded by d loops; a legal schedule TR where all loop-carried \ndependences have been strongly satis.ed before level 2d;a memory reference FA for an array Aof dimension \nl with stride-0/1 potential; R \u00b5F j , .F j , sF and sF , a collection of Boolean decision variables; \nand 1 2 GF the normalized matrix for F. When given semantics-preserving TR with parallel inner-dimension, \nthe following constraints d-1 d-1 . g1, j..2k, j =g1, j.\u00b5Fj .. g2, j..2k, j =(d-1).g2, j..Fj .j.{1..d} \nk=1 k=1 dd dd j j . g1, j.\u00b5F = . g1, j.s1F .. g2, j..F = . g2, j-s2F j=1 j=1 j=1 j=1 are veri.ed if \nsF = 1and sF = 1(stride-1), or if sF = 1and sF = 0 12 12 (stride-0). Otherwise it is not stride-1/0. \nThe optimization objective is formulated as follows. 1As we perform lexicographic optimization, we can \nplace this maxi\u00admization as the very last optimization objective to not disrupt the actual performance-driven \noptimization objectives.  DEFINITION 6 (Maximization of Stride-0/1 References). The num\u00adber of non-stride-0/1 \nreferences is minimized by encoding, for each memory reference F in the program, the constraints of Def. \n5, and optimizing the following objectives: sF sF F F max . 1 , max . 2 Example To illustrate the above \nde.nitions, Fig. 4 shows how they are applied on a simple program with a single reference. The stride \noptimization objectives yield the optimum value of s1 = 1 and s2 = 1. s1 = 1 implies that \u00b5 2 = 1, which \nin turn makes .2,2 = 1. Thus, in the .nal schedule .2,2 =1. s2 = 1forces .1 = 0, which propagates and \nsets .2,1 = 0. In the .nal schedule, .2,1 = 0 and .2,2 =1. This corresponds to making the loop j the \nouter loop. for (i = 0; i < M; ++i) for (j = 0; j < N; ++j) A[j][i] = 0; G= [0 1 1 0] 0\u00b7 \u00b5 1 + 1\u00b7 \u00b5 2 \n=(0+ 1) \u00b7 s1 1\u00b7 .1 + 0\u00b7 .2 =1-s2 0\u00b7 .2,1 =0\u00b7 \u00b5 1 1\u00b7 .2,2 =1\u00b7 \u00b5 2 1\u00b7 .2,1 =1\u00b7 .1 0\u00b7 .2,2 =0\u00b7 .2 Figure \n4. Example of stride-0/1 constraints  4.4.5 Maximizing Permutability Our approach to maximize permutability \nis to add constraints to capture level-wide permutability [11], and maximizing the number of such dimensions. \nWe .rst recall the de.nition of permutable dimensions. DEFINITION 7 (Permutability condition). Given \ntwo statements R, S. Given the conditions for semantics-preservation as stated by Def. 4. Their schedule \ndimensions are permutable up to dimension k if in addition: .DR,S, .p.{1, . . . , k}, .(x R,x S).DR,S, \nTS(x S) -TR(x R) =dDp R,S (5) pp Enforcing this constraint on all schedule dimensions may lead to an \nempty solution set, if not all dimensions are permutable. So, we need to relax this criterion in order \nto capture when it is true, while not removing schedules from the solution set. We start by introducing \nBoolean decision variables, .Dp R,S , i.e., one . variable DR,S DR,S per d variable, and set dp =.p . \nThese variables, when set to 1, will make Eq. (2) equal to Eq. (5), and will not impact the solution \nset when set to 0. This is achieved by replacing Eq. (2) by: p-1 DR,S DR,S DR,S TS(x S) -TR(x R) =- (d-.).(K.xn+ \nK) + d . When a dependence has been previously satis.ed (d1 for p p k k p k=1 DR,S p = some p), instead \nof nullifying the legality constraint at subsequent levels, we will attempt to maximize the number of \ncases where Eq. (5) holds, with the following optimization objective for a given dimension p: max . .Dp \nR,S DR,S We note that as d and . variables are connected by an inequality, maximizing . variables implicitly \nforces the strong satisfaction of dependences as early as possible.  4.5 Putting It All Together The \nabove constraints are all embedded into a single optimiza\u00adtion problem, containing numerous Boolean decision \nvariables, as well as the schedule coef.cients for all statements. We combine all our optimization objectives \ninto a single optimization problem that is solved by Integer Linear Programming, by .nding the lex\u00adicographically \nsmallest point in the space. The order of the opti\u00admization objectives determines which objective will \nbe maximal\u00adly/minimally solved .rst, with succeeding objectives being opti\u00admally solved given the max/min \nsolution of the previous objectives. Our combined problem P is: . . DR,S .k max . ., .k = 2d min uk.xmax \n..i, j P: min . d2d , min u2d.xn+ w2d, max . 1 ,sF max . 2 ,sF . DR,S F F . . DR,S n+ wk, k DR,S . i, \nj There may still be multiple solutions to this optimization prob\u00adlem, each satisfying the criteria for \nextraction of maximal codelets as per Def. 2. While technically enumerating all optimal solutions should \nbe performed for best performance, in practice we limit our\u00adselves to the lexicographically smallest \nsolution of the above prob\u00adlem. The resulting schedule is then applied to the program, poly\u00adhedral code \ngeneration is performed, and candidate codelets are detected on the generated code (all parallel innermost \nloops with only stride-0/1 references are candidate codelets). Those candidate codelets are the input \nto the second stage of our optimization frame\u00adwork, which performs retiming+skewing of statements to \nminimize the number of unaligned load/stores. In addition, permutable loops are detected through dependence \nanalysis on the generated code, and are marked as candidate loops for unroll-and-jam.  4.6 Minimizing \nUnaligned Stores and Loads The .nal stage of our method to extract valid candidate codelets for ISA-speci.c \nSIMD synthesis is to minimize the number of un\u00adaligned stores and loads in each candidate codelet found \nby the pre\u00advious loop transformation step. Despite the fact that hardware align\u00adment constraints are \nusually dealt with at a lower compilation level [15], this task should more naturally belong to the high-level \ntrans\u00adformation engine, where multidimensional loop transformations as well as precise array data.ow \nanalysis can be performed. To achieve this objective, we perform a combination of state\u00adment retiming \nat the codelet level, and additional loop skewing for the unroll-and-jammed loops. Intuitively, we shift \nstatements in the codelet such that (when possible) all array elements which are referenced by the vectorizable \nloop become aligned if the .rst el\u00adement being referenced by the loop is aligned. Previous work by Henretty \net al. [23] develops a complete framework for statement retiming so as to minimize stream alignment con.ict \non innermost vectorizable loops. Our candidate codelets .t their de.nition of vec\u00adtorizable loops, and \nto .nd the relevant retiming factor for our op\u00adtimization purpose, we use a slightly adapted version \nof that algo\u00adrithm. The main difference is that we add an additional scalar quan\u00adtity to the shifts found \nby their algorithm so that stores to different arrays need the same iteration peeling quantity to ensure \nalignment with the technique shown in Sec. 3.1. This is not strictly necessary, but simpli.es the code \ngeneration and can reduce the number of peeled iterations in the scalar prologue/epilogue codes. Finally, \nwe need to take care of alignment properties in case of unroll-and-jam of the loops. For this purpose, \nwe resort to skewing the unroll-and-jammable loops by a positive factor, so that the array index expression \nalong the FVD is a multiple of the vector length. Such transformation is always legal, as a positive \nskew cannot change the dependence direction. As an illustration, for a Jacobi\u00ad2D example the access function \nafter parametric tiling and full tile extraction, but before retiming is of the form A[-2t+i][-2t+j]. \nAssuming V = 4, we apply an additional skew by 2 to unroll\u00adand-jam along t, to get A[-4t+i][-4t+j]. In \nthis manner, when unrolling by any factor along t and jamming in j, references will still be vector-aligned \nin j.  In summary, to minimize store/load alignment even in the pres\u00adence of unroll-and-jam, for each \ncandidate codelet we compute an ad-hoc polyhedral transformation made only of shifts and skews, and apply \nthis transformation using a polyhedral code generator. The resulting codelet is then transformed to abstract \nvector code using the technique depicted in Sec. 3.1, creating valid inputs to the next optimization \nstage: ISA-speci.c SIMD synthesis of vectoriz\u00adable codelets, as detailed in Sec. 5. 5. ISA-speci.c SIMD \nGeneration In this section we discuss the back-end compilation step that trans\u00adlates pre-vectorized code \ninto machine-speci.c high-performance code. The .nal high-performance code is structurally similar to \nthe popular auto-tuning package FFTW [18], the auto-tuning BLAS package ATLAS [42], and general-size \nauto-tuning libraries gen\u00aderated by SPIRAL [35, 41]: the main work is performed in small, highly optimized \ncode blocks called codelets, which are automat\u00adically generated. A higher-level approach (the polyhedral \nframe\u00adwork to parallelization) breaks down the bigger computation into a skeleton that performs all the \nbook-keeping and calls the codelets appropriately. The .rst important question is how to restructure \nthe computation to enable the utilization of ef.cient codelets, which we discuss in Sec. 4. The remaining \nproblem to solve is how to compile codelet speci.cations into highly ef.cient code, a problem similar \nto the role of genfft [17] in FFTW. 5.1 Overview Our line codelet generator takes as input a speci.cation \nof the tile code in a LISP-like intermediate representation (a one-to-one trans\u00adlation is systematically \npossible from our abstract SIMD form to this LISP-like form) that captures the necessary amount of informa\u00adtion \nneeded to generate highly ef.cient SIMD code. The representa\u00adtion is designed to convey everything that \nis known by construction (alignments, loop trip count properties, aliasing, etc.) without over\u00adspecifying \nthe problem. Our codelet generator then performs vari\u00adous platform independent transformations such as \ncommon subex\u00adpression elimination (CSE), array scalarization and strength reduc\u00adtion. Furthermore more \nadvances optimizations such as converting unaligned loads into aligned loads or ISA-speci.c strength \nreduc\u00adtion and pattern transformation are performed. Loops are unrolled in a reuse-conscious way that \ndepends on the problem type. The output of the codelet generator is highly optimized sequential C code \naugmented by ISA-speci.c intrinsics. The code contains well\u00adshaped simple loops, large basic blocks in \nsingle static assignment (SSA) form, easily analyzable array index expressions, and explicit SIMD vector \ninstructions. This code is .nally compiled with the target vendor compiler. 5.2 Line Codelet Speci.cation \nThe input to our codelet generator is a program in a LISP-like code representation. The representation \nenables SIMD architecture\u00adindependent meta programs that construct architecture speci.c in\u00adstances when \ninstantiated. Only the information needed for per\u00adforming the operation is captured in the abstraction, \ndetails regard\u00ading the hardware and the exact calling convention for the SIMD in\u00adtrinsics are abstracted \naway using vector primitives that have a type and a length. During the code generation and optimization \nprocess the codelet generator will translate the polymorphic vector opera\u00adtions to their exact implementation \naccording to the targeted plat\u00adform and compiler. The input language expresses the following operations \nin an ISA-independent way: 1) loads and stores that are guaranteed to be naturally aligned (e.g, in 128-bit \nSSE a 128-bit vector load for which the start address is 16-byte aligned), 2) vector splats and compile-time \nconstants, 3) unaligned loads and stores and their displacement loop-carried or constant displacement \nmodulo vec\u00adtor length, 4) in-register transposes across 2, 4,. . . , V registers (in\u00adcluding pack/unpack \nand interleave/deinterleave), and 5) in-register partial and full reductions (e.g., horizontal adds or \na dot-product in\u00adstruction). We show an illustrative example that is generated by the pre\u00advectorization \nstage below. The SIMD tile meta-program is repre\u00adsented as a tree data structure, using standard C-like \ncommands (decl for variable declarations, loop for for loops, assign for assignments, etc.). Multi-dimensional \narray access is directly sup\u00adported (sv_nth2D_addr), and the meta-program aspect can be seen in expressions \nlike isa.vload and isa.vloadu (aligned and un\u00adaligned load instruction in the current SIMD ISA passed \ninto the meta-program through isa). SSA code is supported by constructing fresh variables. The input \nlanguage including meta-programming capabilities is extensible and new primitives needed by new types \nof kernels or new vector ISAs can be added easily. Our example show an editorially simpli.ed input to \nthe codelet generator specifying a 2D Jacobi SIMD vector line codelet. The full example contains more \nannotations attached to the variables. J a c o b i 2 D _ t i l e : = i s a -> l e t ( r a n g e 0 : \n= v a r . f r e s h _ t ( TInt) , . . . , v r e g 0 : = v a r . f r e s h _ t ( i s a . v t y p e ) \n, . . . , func(TInt, \" j a c o b i 2 d _ t i l e \" , [ l o o p _ b o u n d _ a l i g n e d _ 1 , l \no o p _ b o u n d _ a l i g n e d _ 2 , c 4 , c 6 , s h i f t _ 0 , s h i f t _ 1 , l o c a l _ c 8 , \nB , A ] , decl( [ r a n g e 0 , k 0 , c 8 , v r e g 0 , v r e g 1 , v r e g 2 , v r e g 3 , . . . ] \n, chain( . . . assign( v r e g 1 , i s a . v l o a d ( s v _ n t h 2 D _ a d d r ( A , add(mul( V \n( -2 ) , c 4 ) , c 6 ) , sub(add(mul( V ( -2 ) , c 4 ) , c 8 ) , 1 ) , n , 1 ) ) ) , assign( v r e \ng 2 , i s a . v l o a d u ( s v _ n t h 2 D _ a d d r ( A , add(mul( V ( -2 ) , c 4 ) , c 6 ) , sub(sub(add(mul( \nV ( -2 ) , c 4 ) , c 8 ) , 1 ) , 1 ) , n , 1 ) ) ) , . . . assign( v r e g 0 , mul ( 0 . 2 0 0 0 0 \n0 , add(add(add(add( v r e g 1 , v r e g 2 ) , v r e g 3 ) , v r e g 4 ) , v r e g 5 ) ) ) , . . . \n assign( d e r e f ( t c a s t ( T P t r ( d a t a t y p e ) , s v _ n t h 2 D _ a d d r ( A , add( \nadd(mul( V ( -2 ) , c 4 ) , c 6 ) , sub ( 0 , 1 ) ) , add(add(mul( V ( -2 ) , c 4 ) , add( c 8 , sub( \ns h i f t _ 0 , s h i f t _ 1 ) ) ) , sub( 0 , 1 ) ) , n , 1 ) ) ) , v r e g 6 ) ) ) ) ) ;  5.3 Code \nGeneration and Optimization Our codelet generator is a full-.edged highly aggressive basic block compiler \nwith very limited support for loops. It instantiates the generic tile speci.cation for a particular SIMD \nISA and then op\u00adtimizes the resulting code, and outputs a C function with compiler and architecture speci.c \nSIMD intrinsics. Loop unrolling We aggressively unroll loops to produce large basic blocks. Modern superscalar \nout-of-order processors have huge instruction caches, reorder windows, store-to-load forward\u00ading, multiple \noutstanding loads, and many other architectural fea\u00adtures that are fully exercised in steady state only \nby basic blocks with hundreds of instructions. We convert the massive basic blocks into single static \nassignment (SSA) form to enable better register allocation through the compiler. In addition, unrolling \na time loop by a factor of two allows transparently moving between an input vector and a temporary space, \navoiding extra copy operations. Loop pipelining and blocking The best loop unrolling strategy depends \non the type of code that is underlying the tile codelet. Our codelet generator at present supports three \npatterns: First, stencil and .lter-type data .ow graphs require software pipelining and register rotation. \nThis is either supported in hardware (as on Itanium) or must be done in software through unrolling and \nmulti-versioning of variables. Proper loop pipelining achieves the optimal steady state load/store to \ncomputation ratio (e.g., one load, one store and 3 .ops per loop iteration for 1D Jacobi). Jacobi or \nwavelet .lters are examples of this pattern.  Second, dense linear algebra-type data .ow graphs have \nhigh arithmetic intensity. This enables register-level double buffering: on half of the register .le \na register tile computation is performed while on the other half of the register .le in parallel the \nresults from the previous register tile are stored and the data for the new register tile is loaded. \nThe L1 tile of BLAS3 DGEMM is an example of this pattern [42]. Third, O(NlogN)-type data .ow graphs require \ndepth .rst source code scheduling [17]. This utilizes the available registers most ef.ciently and the \nslow-growing reuse help tolerating a lim\u00adited amount of register spilling. FFTW s codelets, and SPIRAL\u00adgenerated \n.xed-size FFT and DCT kernels as well as, sorting net\u00adworks are examples of this pattern. Unaligned vector \narray scalarization Unaligned load operations following aligned store operations are common in stencil-like \ncodes and introduce two performance penalties. Firstly, unaligned loads are substantially more expensive \nthan aligned loads, even on the latest Intel processors (SandyBridge and Nehalem). Secondly, the read-after-write \ndependency introduced by an aligned store fol\u00adlowed by an unaligned load that touch an overlapping memory \narea is costly as it introduces hard barriers for the compiler to reorder in\u00adstructions to hide memory \nlatencies. They also can introduce prob\u00adlems in write buffers and store-to-load forwarding hardware and \nreduce the number of registers that can be used. Our codelet generator performs an important optimization \nto eliminate unaligned load operations. It .rst unrolls the outer (time) loop of a stencil to collect \nthe store and load operations from neigh\u00adboring iterations of the outer loop in the same basic block. \nNext, it replaces unaligned loads with aligned loads and in-register per\u00admutations (vector shifts). Then, \nit replaces the ensuing matching aligned store/load pairs by automatic (register) vector variables, in \neffect scalarizing the SIMD vector array. Finally, it performs ISA\u00adspeci.c strength reduction and common \nsubexpression elimination on the introduced permutation instructions to minimize their cost. Often, more \nthan one instruction sequence can be used to imple\u00adment the vector shift and carefully choosing the sequences \nallows to reuse partial results across multiple iterations of the innermost loop. The result is a large \nbasic block free of unaligned memory accesses and thus free of unaligned read-after-write dependencies \nthat would stall the pipeline. The basic block further can take better advantage of large register .les \nand fully bene.ts from super-scalar out-of-order cores. Alignment versioning On many SIMD ISAs the instructions \nrequired to shift data within SIMD registers to resolve miss\u00adalignment take the shift value as immediate \nand thus require it to be a compile-time constant. On other ISAs different instruction se\u00adquences are \nrequired for different shift values. The miss-alignment may be introduced through a loop variable or \nmay vary across codelet invocations. Finally, there may be multiple arrays that are independently misaligned. \nWhen necessary our codelet generator performs code special\u00adization with respect to unknown array miss-alignment. \nThe largest scope within the codelet for which the miss-alignment is constant (usually either the whole \ncodelet or the outermost loop body) be\u00adcomes a switch statement that contains one case for each com\u00adbination \nof miss-alignments. Each case contains specialized code that inlines the miss-alignment values and the \nresulting speci.c vector shift instruction sequences. The potentially large code size blowup is usually \nno problem given the big instruction cache size and can be compensated through slightly less aggressive \nunrolling. Generic optimizations Finally, the codelet generator applies stan\u00addard basic block optimizations \nused by program generators such as FFTW s genfft, SPIRAL and ATLAS. This includes copy propa\u00adgation, \nconstant folding, common subexpression elimination, array scalarization, and source code scheduling. \nThe codelet generator also performs simpli.cation of array indexing expression and ex\u00adpresses them using \npatterns supported by the ISA, and can generate array-based and pointer-based code. 5.3.1 Quality of \nthe Generated Codelets Below we show the code generated for an 1D Jacobi tile for Intel SSSE3. The instruction \nset operates on 4-way 32-bit .oat vectors and supports the palignr instruction. For readability we extract \nthe kernel as inline function. In the actual generated code the func\u00adtion would be inlined. The example \ncode is a partial tile codelet that does not have a time loop inside and thus unaligned store-to\u00adload \nforwarding through unaligned vector array scalarization is not applied. Nevertheless, all unaligned loads \nhave been replaced by aligned loads and the palignr instruction. The codelet generator implements a software \npipeline that for every loop iteration of the unrolled loop loads one SIMD vector, computes one SIMD \nvec\u00adtor result with two vector adds, one vector multiply and two vector alignment operations, and stores \nthe result. _ _ m 1 2 8 _ _ f o r c e i n l i n e k e r n e l ( _ _ m 1 2 8 i n 0 , _ _ m 1 2 8 i n 1 \n, _ _ m 1 2 8 i n 2 ) { r e t u r n _ m m _ m u l _ p s ( _ m m _ s e t 1 _ p s ( 0 . 3 3 3 3 3 3 3 3 \n3 ) , _ m m _ a d d _ p s ( _ m m _ c a s t s i 1 2 8 _ p s ( _ m m _ a l i g n r _ e p i 8 ( _ m m _ \nc a s t p s _ s i 1 2 8 ( i n 0 ) , _ m m _ c a s t p s _ s i 1 2 8 ( i n 1 ) , 4 ) ) , _ m m _ a d d \n_ p s ( i n 1 , _ m m _ c a s t s i 1 2 8 _ p s ( _ m m _ a l i g n r _ e p i 8 ( _ m m _ c a s t p s \n_ s i 1 2 8 ( i n 1 ) , _ m m _ c a s t p s _ s i 1 2 8 ( i n 2 ) , 1 2 ) ) ) ) ) ; } v o i d j a c \no b i 1 d _ t i l e ( _ _ m 1 2 8 * i n , _ _ m 1 2 8 * o u t , i n t n ) { _ _ m 1 2 8 t 0 , t1 , t \n2 , t 3 , t4 , t 5 , t 6 , t 7 , . . . , t 1 7 , t 1 8 , t 1 9 ; t 0 = i n [ 0 ] ; t 1 = i n [ 1 ] ; \n f o r ( i = 1 ; i < n -1 ; i + = 2 0 ) { t 2 = i n [ i + 1 ] ; o u t [ i + 0 ] = k e r n e l ( t 0 \n, t 1 , t 2 ) ; t 3 = i n [ i + 2 ] ; o u t [ i + 1 ] = k e r n e l ( t 1 , t 2 , t 3 ) ; t 4 = i n \n[ i + 3 ] ; o u t [ i + 2 ] = k e r n e l ( t 2 , t 3 , t 4 ) ; . . . t 1 8 = i n [ i + 1 7 ] ; o u \nt [ i + 1 6 ] = k e r n e l ( t 1 6 , t 1 7 , t 1 8 ) ; t 1 9 = i n [ i + 1 8 ] ; o u t [ i + 1 7 ] \n= k e r n e l ( t 1 7 , t 1 8 , t 1 9 ) ; t 0 = i n [ i + 1 9 ] ; o u t [ i + 1 8 ] = k e r n e l ( \nt 1 8 , t 1 9 , t 0 ) ; t 1 = i n [ i + 2 0 ] ; o u t [ i + 1 9 ] = k e r n e l ( t 1 9 , t0 , t 1 ) \n; } } Below we show the assembly generated for this function by the Intel C++ compiler 11.1 in 64-bit \nmode (EM64T), targeting a SandyBridge processor supporting the VEX encoding and 3\u00adoperand instructions. \nWe see the compact addressing encoding and that on this processor the code looks almost perfect. j a \nc o b i 1 d _ t i l e P R O C ; p a r a m e t e r 1 ( i n ) : r c x ; p a r a m e t e r 2 ( o u t ) : \nr d x ; p a r a m e t e r 3 ( n ) : r 8 d v m o v a p s x m m 1 , X M M W O R D P T R [ r c x ] v m \no v a p s x m m 3 , X M M W O R D P T R [ 1 6 + r c x ] m o v r 8 d , 1 m o v e a x , 1 6 v m o v \na p s x m m 0 , X M M W O R D P T R [ _ 2 i l 0 f l o a t p a c k e t . 2 8 7 ] . B 2 . 2 : : v m o v \na p s x m m 4 , X M M W O R D P T R [ 1 6 + r a x + r c x ] v p a l i g n r x m m 1 , x m m 1 , x m \nm 3 , 4 a d d r 8 , 2 0 v p a l i g n r x m m 5 , x m m 3 , x m m 4 , 1 2 v a d d p s x m m 2 , x \nm m 3 , x m m 5 v a d d p s x m m 5 , x m m 1 , x m m 2 v m u l p s x m m 1 , x m m 0 , x m m 5 v \nm o v a p s X M M W O R D P T R [ r a x + r d x ] , x m m 1 . . . ; a b o u t 3 0 0 l i n e s o f a \ns s e m b l y r e p e a t i n g t h e l a s t 5 l i n e s . . . ; c y c l i n g t h r o u g h t h e \n1 6 X M M r e g i s t r e r v m o v a p s x m m 3 , X M M W O R D P T R [ 3 2 0 + r a x + r c x ] v \np a l i g n r x m m 2 , x m m 1 , x m m 3 , 1 2 v a d d p s x m m 2 , x m m 1 , x m m 2 v a d d p s \nx m m 5 , x m m 4 , x m m 2 v m u l p s x m m 2 , x m m 0 , x m m 5 v m o v a p s X M M W O R D P T \nR [ 3 0 4 + r a x + r d x ] , x m m 2 a d d r a x , 3 2 0 c m p r 8 , 4 0 1 j l . B 2 . 2 r e t \n_ 2 i l 0 f l o a t p a c k e t . 2 8 7 D D 0 3 e a a a a a b H , 0 3 e a a a a a b H , 0 3 e a a a a \na b H , 0 3 e a a a a a b H  SSE 2-way double SSE 4-way single AVX 4-way double AVX 8-way single __ \nm256 *A, _ _ m 1 2 8 * A , A c = A [ 1 ] , _ _ m 2 5 6 d * A , A c = _ m m 2 5 6 _ p e r m u t e 2 f \n1 2 8 _ p s ( A l = _ m m _ c a s t s i 1 2 8 _ p s ( _ m m _ a l i g n r _ e p i 8 ( A c = _ m m 2 5 \n6 _ p e r m u t e 2 f 1 2 8 _ p d ( A [ 0 ] , A [ 1 ] , 0 x 0 1 ) ; _ _ m 1 2 8 d * A , A c = A [ 1 \n] , _ m m _ c a s t p s _ s i 1 2 8 ( A [ 0 ] ) , A [ 0 ] , A [ 1 ] , 0 x 0 1 ) ; A l = _ m m 2 5 6 _ \ns h u f f l e _ p s ( A l = _ m m _ s h u f _ p d ( A [ 0 ] , t 2 , 0 x 2 1 ) , _ m m _ c a s t p s _ \ns i 1 2 8 ( A c ) , 1 2 ) ) ; A l = _ m m 2 5 6 _ s h u f f l e _ p d ( _ m m 2 5 6 _ s h u f f l e _ \np s ( A c , A [ 0 ] , A r = _ m m _ s h u f _ p d ( t 2 , A [ 2 ] , 0 x 1 2 ) ; A r = _ m m _ c a s t \ns i 1 2 8 _ p s ( _ m m _ a l i g n r _ e p i 8 ( A [ 0 ] , A c , 0 x 0 5 ) ; 0 x 1 B ) , A c , 0 x C \n6 ) ; _ m m _ c a s t p s _ s i 1 2 8 ( A c ) , A r = _ m m 2 5 6 _ s h u f f l e _ p d ( A r = _ m m \n2 5 6 _ s h u f f l e _ p s ( Ac , _ m m _ c a s t p s _ s i 1 2 8 ( A [ 2 ] ) , 4 ) ) ; Ac , A [ 1 ] \n, 0 x 0 5 ) ; _ m m 2 5 6 _ s h u f f l e _ p s ( A [ 1 ] , A c , 0 x 1 B ) , 0 x 6 E ) ; Figure 5. \nExtraction of left-shifted (denoted by Al), centered (Ac)and right-shifted (Ar)SIMD vectors from three \naligned vectors A[0], A[1], and A[2] through permutation instructions. We display the data .ow and instruction \nsequence for shifting on SSE 2-way double and 4-way single, and AVX 4-way double and 8-way single, as \nsupported on Intel s SandyBridge processors. One concern is the large size SSE instructions and decoder \nlimitations, however, post-decoder micro-op caches can alleviate this problem on high performance processors. \nIn Sec. 6 we will analyze the tile performance for a collection of kernels, machines and vector lengths. \n  5.4 Cost Model Our codelet generator provides a simple cost model for the gener\u00adated codelets to aid \ncodelet (pre-)selection without the need for pro\u00ad.ling the code. The line codelets are in steady state \nrun cache resi\u00addent (in hot L1 cache). Thus a weighted instruction count is a good .rst order metric \nto predict the relative performance of codelets. The metric captures the fact that generated codelets \nrun close to the machine peak and this places a cost on every instruction, even if executed in parallel. \nThe metric in particular allows to measure the vectorization overhead (number of arithmetic operations \nversus number of all operations). By counting source-level SIMD oper\u00adations like _mm_add_ps, _mm_sub_ps, \nand _mm_unpacklo_ps in\u00adstead of assembly instructions like movaps, lea and mov the metric is more resilient \nto effects on aggressive superscalar out-of-order processors like the Intel Core i7. We pick the coef.cients \nfor var\u00adious operations to provide a relative ordering that favors cheaper operations over more expensive \noperations and models the relative cost of operations in the target micro-architecture. While the met\u00adric \ncannot predict actual execution times it is suf.cient to prune the search space and .nd good candidates \nand aids auto-tuning in the high-level framework. 6. Experimental Results 6.1 Experimental Protocol We \nevaluate our framework on a collection of benchmarks where the core computational part is a SCoP. Speci.cally, \nwe experiment with 5 stencil computations that arise in typical image processing and physical simulation \napplications, and 3 linear algebra bench\u00admarks. Problem sizes are selected to be larger than the last \nlevel cache (LLC) size of 8 MB. Each benchmark is evaluated using single-precision (float) and double-precision \n(double). Experi\u00adments are performed on an Intel SandyBridge i7-2600K (3.4GHz, 4 cores, 32KB L1, 256KB \nL2, 8MB L3), using either the SSE or AVX vector instruction set. The testbed runs a native 64-bit Linux \ndistri\u00adbution. We use Intel ICC 13.0 with -fast -parallel -openmp, except for the sequential baseline \nwhere we use -fast. For AVX experiments, the .ag -xAVX is used. For each benchmark, we apply the following \n.ow: (1) loop transformations for parallel parametric tiling with full-tile sep\u00adaration, and coarse-grain \nshared-memory parallel execution; (2) for each full-tile, perform loop transformation to expose maximal \ncodelets, and perform abstract SIMD vectorization; (3) for each codelet found, perform ISA-speci.c SIMD \nsynthesis. This frame\u00adwork is implemented using PolyOpt/C, a polyhedral compiler based on the LLNL ROSE \ninfrastructure [2]. The SIMD codelet generator is implemented in the SPIRAL system s back-end infrastructure. \nA web page and codelet generation web service with make.le tie-in can be found at [3]. On average, our \nend-to-end framework (not considering auto-tuning time) takes about 25 seconds to generate a program \nversion for benchmarks like Jacobi-2d, laplacian-2d, or correlation; and about 3 minutes for benchmarks \nlike Jacobi-3D, laplacian-3D, or doitgen. 6.2 Codelet Extraction Statistics Table 1 reports statistics \non the ILP scheduling problem developed in this paper. We report for some representative benchmarks the \nnumber of dependence polyhedra for array references, the total number of schedule coef.cient variables \n.i, j, the total number of additional variables inserted to model our problem, the total number of constraints \nin the system to be solved (this includes the semantics preserving constraints from dependence polyhedra), \nand the time to optimally solve the ILP using PIPLib. More details can be found in a technical report \n[27]. Benchmark # deps # refs # .i, j #others # cst. Time (s) jacobi-2d 20 8 140 486 3559 3.0711 laplacian-2d \n20 10 140 502 3591 3.1982 poisson-2d 32 12 140 674 5277 5.8132 correlation 5 9 70 177 640 0.0428 covariance \n3 4 70 176 593 0.0415 doitgen 3 4 117 269 911 0.1383 Table 1. ILP statistics  6.3 Tile Codelet Performance \nTable 2 summarizes the operation count in steady state for the Ja\u00adcobi kernels. This establishes the \nperformance of the tile codelets in a stand-alone fashion. Here each codelet is made L1-resident, and \nis the result of unroll-and-jamming multiple loops in the codelet, leading to multi-loop vectorization \nin practice. We list the theoret\u00adical maximum .oating-point operations per cycle and the actually measured \n.oating-point operations per cycle for SSE and AVX on the processor described above, and also for a 2.66 \nGHZ Intel Ne\u00adhalem processor. The SandyBridge processor supports SSE 4.2 and Table 2. Cost analysis of \nthe tile codelet speci.cations and performance (.op/cycle) on two different machines (Intel SandyBridge \nCore i7 2600K and Intel Nehalem X5680). We provide operation counts in steady state for adds/subtracts, \nmultiply, vector shifts, loads and stores.  Problem SandyBridge 3.4 GHz Nehalem 3.3 GHz SSE 2-way SSE \n4-way AVX 4-way AVX 8-way SSE 2-way SSE 4-way Name + \u00d7 mem shft peak meas peak meas peak meas peak meas \npeak meas peak meas Jacobi 1D 3pt 2 1 2 1-5 3 2.5 6 5.5 4 2.7 8 8.2 3 1.4 6 2.9 Jacobi 2D 5pt 4 1 2 1-5 \n2.5 2.5 5 4.8 5 4.3 10 7.5 2.5 2.4 5 2.4 Jacobi 3D 7pt 6 1 2 1-5 2.3 1.2 4.6 4.5 4.6 3.8 9.3 8.9 2.3 \n1.1 4.6 4 AVX and provides the three-operand VEX encoding for SSE in\u00adstructions, which results in very \ncompact code. A three-point 1D Jacobi stencil in steady state requires one load, one store, two additions, \none multiplication and two shift opera\u00adtions. The SandyBridge processor can transfer 48 bytes/cycle be\u00adtween \nthe L1 cache and the register .le, execute one vector (SSE or AVX) addition and vector multiplication \neach cycle and can per\u00adform one shift/permute operation per cycle. The decoder can de\u00adcode 4 to 5 x86 \ninstructions per cycle and caches decoded micro\u00adops. The hard limiting factor is the two additions needed \nper stencil result and thus the maximum performance is 6 .ops/cycle for 4-way single-precision SSE (one \naddition every cycle and a multiplication every other cycle). We achieve 5.5 .ops/cycle steady-state \nL1 per\u00adformance. In the 2D case the maximum is 5 .op/cycle, which we achieve. This is a testament to \nthe power of the latest generation of Intel processors and shows that our tile codelets are very ef.cient. \nThe table summarizes further results across kernels and ISAs.  6.4 Full Program Performance One of the \nkey aspects of our framework is the ability to perform extensive auto-tuning on a collection of parameters \nwith signi.cant impact on performance. The two categories of parameters we em\u00adpirically tune are (1) \nthe tile sizes, so as to partition the computation in L1-resident blocks; and (2) the unroll-and-jam \nfactor, that affects the number of operations in a codelet. While technically the param\u00adeter space is \nextremely large, it can be greatly pruned based on a number of observations. (1) We test tile sizes whose \nfootprint is L1-cache resident. (2) We set the tile size of the vector dimension to be some multiple \nof the unrolling factor of this loop by the vec\u00adtor length, so as to minimize the peeling required for \nalignment. (3) We keep only tile sizes where the percentage of total computation performed by the full \ntiles is above a given threshold, so as to avoid tile shapes where a signi.cant fraction of the computation \nis done by partial tiles. For 2D benchmarks we use a threshold of 80%, and a threshold of 60% for 3D \nbenchmarks. (4) We unroll-and-jam all permutable dimensions, using unrolling factors of 1, 2, 4 and 8. \nThe auto-tuning is broken into two phases. In the .rst phase, approximately 100 tile sizes are tested \nand the 10 with the most iteration points in full tiles are selected. This process takes 2-3 minutes. \nThe second phase tests 6-8 codelet optimizations for each of the 10 tile sizes, taking about 10-15 minutes. \nTime breakdown over full/partial tiles and tile codelets To gain insights into the effectiveness and \npotential limitations of our framework, we .rst present a breakdown of time expended within tile codelets, \nwithin full tiles, and in partial tiles. Table 3 shows the performance comparison using 3 metrics: ATC, \nthe time spent in all tile codelets; AFT, the time spent in all full tiles; FP, and the total time of \nthe full program. We make the following observations: Peeling is the main reason for loss of performance \nwhen moving from ATC to AFT, and slowdowns can vary between 1.1\u00d7 and 2\u00d7. The fraction of performance \nloss depends on the vector length and unrolling factors used in the FVD. Thus, these two parameters must \nbe tightly coupled with the FVD tile size while remaining tile sizes must be relatively small for minimizing \nthe peeling overhead. Also, partial tiles have a considerable impact on the FP performance. In general \npartial tiles can cause between 1.3\u00d7 to 3\u00d7 slowdown over AFT, and they are particularly detrimental for \nhigher dimensional iteration spaces (e.g. Jacobi-3D and doitgen). In general, bigger tile sizes improve \nperformance but only up to a certain point, as they also gradually push more iterations from full tiles \ninto partial tiles, even reducing the number of tiles that can be executed concurrently. Finally, the \naccumulated effect of peeling and partial tiles execution can yield between 1.4\u00d7 and 6\u00d7 slowdown from \nATC to FP. Benchmark SIMD ISA ATC (sec) AFT (sec) FP (sec) jacobi-2d SSE 0.040 0.046 0.068 jacobi-3d \nSSE 0.458 0.514 1.113 jacobi-2d AVX 0.036 0.054 0.077 jacobi-3d AVX 0.188 0.354 1.062 laplacian-2d SSE \n0.046 0.056 0.071 laplacian-3d SSE 0.482 0.538 1.131 laplacian-2d AVX 0.031 0.042 0.061 laplacian-3d \nAVX 0.197 0.373 0.993 poisson-2d SSE 0.051 0.064 0.090 poisson-2d AVX 0.029 0.049 0.075 correlation SSE \n0.840 0.915 1.179 correlation AVX 0.752 0.917 1.139 covariance SSE 0.846 0.922 1.159 covariance AVX 0.733 \n0.883 1.121 doitgen SSE 0.191 0.353 0.865 doitgen AVX 0.158 0.308 0.834 Table 3. Time breakdown for kernels: \nAll Tile Codelets (ATC), All Full Tiles (AFT=ATC+Peeling) and Full Program (FP=AFT+Partial Tiles) Overall \nperformance Finally, we report in Table 4 the full\u00adapplication performance for the benchmarks. For each \nbenchmark \u00d7 vector ISA \u00d7 data type, we compare our performance in GF/s to that attained by ICC on the \ninput program (both sequentially and with automatic parallelization .ags) and PTile, the performance \nof PTile [5], a state-of-the-art Parallel Parametric Tiling software (tile sizes have been auto-tuned). \nThe performance achieved using only the front-end of our framework (transformation for codelet extrac\u00adtion) \nis reported in the Prevect columns, i.e., we restructure the program to expose codelets but simply emit \nstandard C code for the codelet body and use the C compiler s vectorizer. SIMD reports the performance \nachieved by using our ISA-speci.c SIMD synthesizer on each codelet after codelet extraction. We observe \nvery signi.cant performance improvements over ICC and PTile, our two reference compilers. Up to 50\u00d7 improve\u00adment \nis achieved using AVX for the covariance benchmark, for in\u00adstance. We systematically outperform ICC (both \nwith and without auto-parallelization enabled) with our complete framework using AVX (the SIMD/AVX column) \nsince ICC fails to effectively vec\u00adtorize most of the benchmarks. However, we note that due to the polyhedral \nmodel s limitation of not allowing explicit pointer man\u00adagement, the stencil codes have an explicit copy-back \nloop nest instead of a pointer .ip. We note that the stencil benchmarks are  single double Benchmark \nPB Size ICC PTile Prevect SIMD ICC PTile Prevect SIMD seq par SSE AVX SSE AVX seq par SSE AVX SSE AVX \njacobi-2d 20 \u00d720002 2.96 3.71 6.24 17.15 13.22 25.39 20.96 1.79 1.86 6.25 13.35 11.80 17.86 15.72 laplacian-2d \n20 \u00d720002 4.30 4.44 6.29 18.7 17.85 24.86 26.57 2.15 2.23 6.30 13.19 13.13 16.01 18.53 poisson-2d 20 \n\u00d720002 4.23 6.68 17.47 19.56 14.75 31.77 42.23 3.08 3.36 16.86 15.19 11.31 20.67 29.77 jacobi-3d 20 \u00d72563 \n4.21 4.70 3.82 5.38 5.04 3.84 5.53 2.22 2.06 4.01 2.70 2.87 2.67 3.41 laplacian-3d 20 \u00d72563 4.80 5.44 \n4.39 6.13 5.67 6.99 6.26 2.55 2.38 4.58 3.34 3.17 4.15 3.98 correlation 20003 0.94 0.81 26.21 18.78 19.89 \n33.35 52.43 0.64 0.63 14.38 9.56 9.66 21.28 26.29 covariance 20003 0.97 0.99 26.53 18.70 20.08 34.85 \n55.92 0.65 2.16 13.52 9.66 9.70 21.88 27.23 doitgen 2564 8.63 8.56 14.72 31.35 30.76 41.63 52.66 4.85 \n4.68 9.14 16.85 16.80 26.45 25.96 Table 4. Performance data in GFLOP/s. severely memory-bandwidth bounded. \nAlthough ICC is able to au\u00adtomatically parallelize and vectorize the benchmarks, it is unable to perform \ntime-tiling on the benchmarks and therefore the mem\u00adory bandwidth limits the speedup achieved from parallelism. \nWe also outperform, usually very signi.cantly, the PTile research com\u00adpiler in all but two cases (Jacobi-3D \nand Laplacian-3D, DP). These anomalies are due to our criteria of good tile sizes during the auto-tuning \nphase: tile sizes are selected based on the percentage of points that will be executed in full tiles \nvs. partial tiles. There\u00adfore, a good con.guration for our line codelet is not necessarily the overall \nbest tile con.guration. As explained before, increasing tile sizes shift iterations from full to partial \ntiles (which are slower) and reduce the number of concurrent tiles down to possibly a single one. We \nalso remark that the Prevect performance does not necessar\u00adily follow the vector length (and hence the \narithmetic throughput), nor does it necessarily outperform ICC or PTile. One of the main reason is the \ncomplexity, in terms of number of basic blocks, of the codelets which are generated. By exploiting large \ntile sizes and unroll-and-jam, codelets with hundreds of blocks are generated and ICC simply cannot process \nthe loop nest without splitting it, there\u00adfore negating the potential bene.t of our approach. In general, \nwe have observed that despite being in a form that satis.es the vec\u00adtorizability criteria, the C code \ngenerated by our high-level frame\u00adwork does not lead to effectively exploited SIMD parallelism by the \ncompiler, emphasizing the impact from an explicit coupling with a SIMD synthesizer. In some cases, ICC \ndegrades performance with automatic parallelization, possibly due to inaccurate pro.tability models. \nPTile can also be slower than ICC parallel, due to com\u00adplicated loop bound expressions as seen for single \nprecision Jacobi and Laplacian 3D. The 3D stencils require a careful balance be\u00adtween maximization of \nthe size of full tiles for high performance of the codelets, and keeping the size small enough to ensure \nthat a high percentage of operations are in full tiles. Other tile shapes such as diamond tiling [4] \nmay be needed for better performance. 7. Related Work Automatic SIMD vectorization has been the subject \nof intense re\u00adsearch in the past decades, i.e. [15, 26, 28, 30, 43]. These work are usually focusing \non the back-end part, that is the actual SIMD code generation from a parallel loop [15, 28, 30], or on \nthe high\u00adlevel loop transformation angle only [12, 26, 38, 40]. To the best of our knowledge, our work \nis the .rst to address simultaneously both problems by setting a well-de.ned interface between a pow\u00aderful \npolyhedral high-level transformation engine and a specialized SIMD code generator. Vasilache et al. also \nintegrated SIMD and contiguity constraints in a polyhedral framework, in the R-Stream compiler [40], \nwith similar objectives as ours. However, to the best of our knowledge, they are not considering the \ncoupling of this framework with a powerful back-end SIMD code generator as we do. Other previous work \nconsidered inner-and outer-loop vector\u00adization [31], our proposed work makes also a step forward by doing \n(tiled) loop nest vectorization, as codelets embed in their body up to all iterations of the surrounding \nloops. Program generation (also called generative programming) has gained considerable interest in recent \nyears [8, 9, 13, 21, 36]. The basic goal is to reduce the development, maintenance, and analysis of software. \nAmong the key tools for achieving these goals, domain-speci.c languages provide a compact representation \nthat raises the level of abstraction for speci.c problems and hence enables the manipulation of programs \n[10, 20, 24, 37]. Our codelet generator is an example of such a program generation system. Automating \nthe optimization of performance libraries is the goal in recent research efforts on automatic performance \ntuning, program generation, and adaptive library frameworks that can offer high performance with greatly \nreduced development time. Exam\u00adples include ATLAS [42], Bebop/Sparsity [14, 25], and FFTW [18] for FFTs. \nSPIRAL [35] automatically generates highly ef.cient .xed-size and general-size libraries for signal processing \nalgo\u00adrithms across a wide range of platforms. SPIRAL and FFTW provide automatic SIMD vector codelet generation \nwhile ATLAS utilizes contributed hand-written SIMD vector kernels. While our transformation+synthesis \napproach bears some resemblance with those work, we address a much more general problem which re\u00adquires \nto combine highly complex program transformations that can be modeled effectively only by means of the \npolyhedral frame\u00adwork with ISA-speci.c code generation. 8. Conclusion Automatic short-vector SIMD vectorization \nis ubiquitous in mod\u00adern production and research compilers. Nevertheless, the task of automatically generating \neffective programs addressing the data locality, coarse-grain and SIMD parallelism challenges remains \nonly partly solved in the vast majority of cases. We have made a statement about a viable scheme to achieve \nthis goal, for a class of programs that arises frequently in compute\u00adintensive programs. We have isolated \nand formalized program opti\u00admizations that can be effectively performed by a high-level loop transformation \nengine, from those optimizations that can be ef\u00adfectively implemented by SIMD code generation. We have \nused the power and expressiveness of the polyhedral compilation frame\u00adwork to formalize a series of scheduling \nconstraints so as to form maximal vectorizable codelets, targeting parallelization, data reuse, alignment, \nand the stride of memory references in a single com\u00adbined problem. As a result, we have unleashed the \npower of a cus\u00adtom ISA-speci.c SIMD code synthesizer, which translates those codelets into very effective \n(up to near-peak) SIMD execution. We have demonstrated the power of our approach on a collection of benchmarks, \nproviding very signi.cant performance improvement over an auto-parallelizing production compiler as well \nas a state-of\u00adthe-art research compiler.  Acknowledgments The authors acknowledge support by DOE through \nawards DE-SC0005033 and DE-SC0008844, NSF through awards 0926688, 1116802 and 0926127, by the U.S. Army \nthrough contract W911NF-10-1-0004, and by Intel ECG. References [1] PoCC, the polyhedral compiler collection. \nhttp://pocc.sourceforge.net. [2] PolyOpt/C. http://hpcrl.cse.ohio-state.edu/wiki/index.php/polyopt/c. \n[3] www.spiral.net/software/stencilgen.html. [4] V. Bandishti, I. Pananilath, , and U. Bondhugula. Tiling \nstencil com\u00adputations to maximize parallelism. In ACM/IEEE Conf. on Supercom\u00adputing (SC 12), 2012. [5] \nM. Baskaran, A. Hartono, S. Tavarageri, T. Henretty, J. Ramanujam, and P. Sadayappan. Parameterized tiling \nrevisited. In CGO, April 2010. [6] C. Bastoul. Code generation in the polyhedral model is easier than \nyou think. In IEEE Intl. Conf. on Parallel Architectures and Compilation Techniques (PACT 04), pages \n7 16, Juan-les-Pins, France, Sept. 2004. [7] C. Bastoul and P. Feautrier. More legal transformations \nfor locality. In Euro-Par 10 Intl. Euro-Par conference, LNCS 3149, pages 272 283, Pisa, august 2004. \n[8] D. Batory, C. Johnson, B. MacDonald, and D. von Heeder. Achieving extensibility through product-lines \nand domain-speci.c languages: A case study. ACM Transactions on Software Engineering and Method\u00adology \n(TOSEM), 11(2):191 214, 2002. [9] D. Batory, R. Lopez-Herrejon, and J.-P. Martin. Generating product\u00adlines \nof product-families. In Proc. Automated Software Engineering Conference (ASE), 2002. [10] J. Bentley. \nProgramming pearls: little languages. Communications of the ACM, 29(8):711 721, 1986. [11] U. Bondhugula, \nA. Hartono, J. Ramanujam, and P. Sadayappan. A practical automatic polyhedral program optimization system. \nIn PLDI, June 2008. [12] C. Chen, J. Chame, and M. Hall. Chill: A framework for compos\u00ading high-level \nloop transformations. Technical Report 08-897, USC Computer Science Technical Report, 2008. [13] K. Czarnecki \nand U. Eisenecker. Generative Programming: Methods, Tools, and Applications. Addison-Wesley, 2000. [14] \nJ. Demmel, J. Dongarra, V. Eijkhout, E. Fuentes, A. Petitet, R. Vuduc, C. Whaley, and K. Yelick. Self \nadapting linear algebra algorithms and software. Proc. of the IEEE, 93(2):293 312, 2005. [15] A. Eichenberger, \nP. Wu, and K. O Brien. Vectorization for simd architectures with alignment constraints. In PLDI, 2004. \n[16] P. Feautrier. Some ef.cient solutions to the af.ne scheduling problem, part II: multidimensional \ntime. Intl. J. of Parallel Programming, 21(6):389 420, Dec. 1992. [17] M. Frigo. A fast Fourier transform \ncompiler. In PLDI, pages 169 180, 1999. [18] M. Frigo and S. G. Johnson. The design and implementation \nof FFTW3. Proc. of the IEEE, 93(2):216 231, 2005. [19] S. Girbal, N. Vasilache, C. Bastoul, A. Cohen, \nD. Parello, M. Sigler, and O. Temam. Semi-automatic composition of loop transformations. International \nJournal of Parallel Programming, 34(3):261 317, June 2006. [20] K. J. Gough. Little language processing, \nan alternative to courses on compiler construction. SIGCSE Bulletin, 13(3):31 34, 1981. [21] GPCE. ACM \nconference on generative programming and component engineering. [22] A. Hartono, M. Baskaran, C. Bastoul, \nA. Cohen, S. Krishnamoorthy, B. Norris, J. Ramanujam, and P. Sadayappan. Parametric multi-level tiling \nof imperfectly nested loops. In ICS, 2009. [23] T. Henretty, K. Stock, L.-N. Pouchet, F. Franchetti, \nJ. Ramanujam, and P. Sadayappan. Data layout transformation for stencil computa\u00adtions on short simd architectures. \nIn ETAPS International Conference on Compiler Construction (CC 11), pages 225 245, Saarbrcken, Ger\u00admany, \nMar. 2011. Springer Verlag. [24] P. Hudak. Domain speci.c languages. Available from author on request, \n1997. [25] E.-J. Im, K. Yelick, and R. Vuduc. Sparsity: Optimization framework for sparse matrix kernels. \nInt l J. High Performance Computing Appli\u00adcations, 18(1), 2004. [26] K. Kennedy and J. Allen. Optimizing \ncompilers for modern architec\u00adtures: A dependence-based approach. Morgan Kaufmann, 2002. [27] M. Kong, \nL.-N. Pouchet, and P. Sadayappan. Abstract vector SIMD code generation using the polyhedral model. Technical \nReport Tech\u00adnical Report 4/13-TR08, Ohio State University, Apr. 2013. [28] S. Larsen and S. P. Amarasinghe. \nExploiting superword level paral\u00adlelism with multimedia instruction sets. In PLDI, 2000. [29] A. W. Lim \nand M. S. Lam. Maximizing parallelism and minimizing synchronization with af.ne transforms. In POPL, \npages 201 214, 1997. [30] D. Nuzman, I. Rosen, and A. Zaks. Auto-vectorization of interleaved data for \nsimd. In PLDI, 2006. [31] D. Nuzman and A. Zaks. Outer-loop vectorization: revisited for short simd architectures. \nIn PACT, 2008. [32] L.-N. Pouchet, C. Bastoul, A. Cohen, and J. Cavazos. Iterative opti\u00admization in the \npolyhedral model: Part II, multidimensional time. In PLDI, pages 90 100. ACM Press, 2008. [33] L.-N. \nPouchet, U. Bondhugula, C. Bastoul, A. Cohen, J. Ramanujam, and P. Sadayappan. Combined iterative and \nmodel-driven optimization in an automatic parallelization framework. In ACM Supercomputing Conf. (SC \n10), New Orleans, Lousiana, Nov. 2010. [34] L.-N. Pouchet, U. Bondhugula, C. Bastoul, A. Cohen, J. Ramanujam, \nP. Sadayappan, and N. Vasilache. Loop transformations: Convexity, pruning and optimization. In POPL, \npages 549 562, Austin, TX, Jan. 2011. [35] M. P uschel, \u00a8J. M. F. Moura, J. Johnson, D. Padua, M. Veloso, \nB. Singer, J. Xiong, F. Franchetti, A. Gacic, Y. Voronenko, K. Chen, R. W. Johnson, and N. Rizzolo. SPIRAL: \nCode generation for DSP transforms. Proc. of the IEEE, 93(2):232 275, 2005. [36] D. R. Smith. Mechanizing \nthe development of software. In M. Broy, editor, Calculational System Design, Proc. of the International \nSum\u00admer School Marktoberdorf. NATO ASI Series, IOS Press, 1999. Kestrel Institute Technical Report KES.U.99.1. \n[37] W. Taha. Domain-speci.c languages. In Proc. Intl Conf. Computer Engineering and Systems (ICCES), \n2008. [38] K. Trifunovic, D. Nuzman, A. Cohen, A. Zaks, and I. Rosen. Polyhedral-model guided loop-nest \nauto-vectorization. In PACT, Sept. 2009. [39] N. Vasilache. Scalable Program Optimization Techniques \nin the Poly\u00adhedra Model. PhD thesis, University of Paris-Sud 11, 2007. [40] N. Vasilache, B. Meister, \nM. Baskaran, and R. Lethin. Joint scheduling and layout optimization to enable multi-level vectorization. \nIn Proc. of IMPACT 12, Jan. 2012. [41] Y. Voronenko and M. P\u00a8uschel. Algebraic signal processing theory: \nCooley-tukey type algorithms for real dfts. IEEE Transactions on Signal Processing, 57(1), 2009. [42] \nR. C. Whaley and J. Dongarra. Automatically Tuned Linear Algebra Software (ATLAS). In Proc. Supercomputing, \n1998. math-atlas. sourceforge.net. [43] M. J. Wolfe. High Performance Compilers For Parallel Computing. \nAddison-Wesley, 1996.    \n\t\t\t", "proc_id": "2491956", "abstract": "<p>Data locality and parallelism are critical optimization objectives for performance on modern multi-core machines. Both coarse-grain parallelism (e.g., multi-core) and fine-grain parallelism (e.g., vector SIMD) must be effectively exploited, but despite decades of progress at both ends, current compiler optimization schemes that attempt to address data locality and both kinds of parallelism often fail at one of the three objectives.</p> <p>We address this problem by proposing a 3-step framework, which aims for integrated data locality, multi-core parallelism and SIMD execution of programs. We define the concept of vectorizable codelets, with properties tailored to achieve effective SIMD code generation for the codelets. We leverage the power of a modern high-level transformation framework to restructure a program to expose good ISA-independent vectorizable codelets, exploiting multi-dimensional data reuse. Then, we generate ISA-specific customized code for the codelets, using a collection of lower-level SIMD-focused optimizations.</p> <p>We demonstrate our approach on a collection of numerical kernels that we automatically tile, parallelize and vectorize, exhibiting significant performance improvements over existing compilers.</p>", "authors": [{"name": "Martin Kong", "author_profile_id": "81758704357", "affiliation": "Ohio State University, Columbus, OH, USA", "person_id": "P4148965", "email_address": "kongm@cse.ohio-state.edu", "orcid_id": ""}, {"name": "Richard Veras", "author_profile_id": "81547998256", "affiliation": "Carnegie Mellon University, Pittsburgh, PA, USA", "person_id": "P4148966", "email_address": "rveras@cmu.edu", "orcid_id": ""}, {"name": "Kevin Stock", "author_profile_id": "81486645921", "affiliation": "Ohio State University, Columbus, OH, USA", "person_id": "P4148967", "email_address": "stockk@cse.ohio-state.edu", "orcid_id": ""}, {"name": "Franz Franchetti", "author_profile_id": "81100496091", "affiliation": "Carnegie Mellon University, Pittsburgh, PA, USA", "person_id": "P4148968", "email_address": "franzf@ece.cmu.edu", "orcid_id": ""}, {"name": "Louis-No&#235;l Pouchet", "author_profile_id": "81330496337", "affiliation": "University of California Los Angeles, Los Angeles, CA, USA", "person_id": "P4148969", "email_address": "pouchet@cs.ucla.edu", "orcid_id": ""}, {"name": "P. Sadayappan", "author_profile_id": "81100364545", "affiliation": "Ohio State University, Columbus, OH, USA", "person_id": "P4148970", "email_address": "saday@cse.ohio-state.edu", "orcid_id": ""}], "doi_number": "10.1145/2491956.2462187", "year": "2013", "article_id": "2462187", "conference": "PLDI", "title": "When polyhedral transformations meet SIMD code generation", "url": "http://dl.acm.org/citation.cfm?id=2462187"}