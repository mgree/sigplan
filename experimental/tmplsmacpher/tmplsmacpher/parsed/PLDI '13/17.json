{"article_publication_date": "06-16-2013", "fulltext": "\n Formal Veri.cation of SSA-Based Optimizations for LLVM Jianzhou Zhao Santosh Nagarakatte Milo M. K. \nMartin Steve Zdancewic Computer and Information Science Department, University of Pennsylvania Rutgers \nUniversity jianzhou@cis.upenn.edu santosh.nagarakatte@cs.rutgers.edu milom@cis.upenn.edu stevez@cis.upenn.edu \nAbstract Modern compilers, such as LLVM and GCC, use a static single assignment (SSA) intermediate representation \n(IR) to simplify and enable many advanced optimizations. However, formally verifying the correctness \nof SSA-based optimizations is challenging because SSA properties depend on a function s entire control-.ow \ngraph. This paper addresses this challenge by developing a proof tech\u00adnique for proving SSA-based program \ninvariants and compiler op\u00adtimizations. We use this technique in the Coq proof assistant to cre\u00adate mechanized \ncorrectness proofs of several micro transforma\u00adtions that form the building blocks for larger SSA optimizations. \nTo demonstrate the utility of this approach, we formally verify a vari\u00adant of LLVM s mem2reg transformation \nin Vellvm, a Coq-based formal semantics of the LLVM IR. The extracted implementation generates code with \nperformance comparable to that of LLVM s unveri.ed implementation. Categories and Subject Descriptors \nD.2.4 [Software Engineer\u00ading]: Software/Program Veri.cation -Correctness Proofs; F.3.1 [Logics and Meanings \nof Programs]: Specifying and Verifying and Reasoning about Programs -Mechanical veri.cation; F.3.2 [Log\u00adics \nand Meanings of Programs]: Semantics of Programming Lan\u00adguages -Operational semantics General Terms Languages, \nVeri.cation, Reliability Keywords LLVM, Coq, single static assignment 1. Introduction Compiler bugs can \nmanifest as crashes during compilation, or, much worse, result in the silent generation of incorrect \nprograms. Such mis-compilations can introduce subtle errors that are dif.cult to diagnose and generally \npuzzling to software developers. A re\u00adcent study by Yang et al. [20] used random test-case generation \nto expose serious bugs in mainstream compilers including GCC, LLVM, and commercial compilers. Whereas \nfew bugs were found in the front end of the compiler, various optimization phases of the compiler that \naim to make generated programs faster were a promi\u00adnent source of bugs. Projects like CompCert [9, 16 \n18] are tackling the problem of compiler bugs by mechanically verifying the correctness of com\u00adpilers. \nIndeed, although the aforementioned study uncovered many bugs in other compilers, the only bugs found \nin CompCert were in those parts of the compiler not (yet) formally veri.ed. Yang et al. write [20]: The \napparent unbreakability of CompCert supports Permission to make digital or hard copies of all or part \nof this work for personal or classroom use is granted without fee provided that copies are not made or \ndistributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. PLDI 13, June 16 19, 2013, Seattle, WA, USA. Copyright c . 2013 \nACM 978-1-4503-2014-6/13/06. . . $15.00 a strong argument that developing compiler optimizations within \na proof framework, where safety checks are explicit and machine\u00adchecked, has tangible bene.ts for compiler \nusers. Despite CompCert s groundbreaking compiler-veri.cation ef\u00adforts, there still remain many challenges \nin applying its technology to industrial-strength compilers. In particular, the original Comp-Cert development, \nand the bulk of the subsequent work with the notable exception of CompCertSSA [4] did not use a static \nsingle assignment (SSA) [7] intermediate representation (IR) [9]. In SSA intermediate representations, \neach variable of a func\u00adtion is assigned statically only once, and each variable de.nition must dominate \nall of its uses in the control-.ow graph. These SSA properties simplify or enable many compiler optimizations \n[14], in\u00adcluding: constant folding, sparse conditional constant propagation, aggressive dead code elimination, \nglobal value numbering, global code motion, partial redundancy elimination, and inductive vari\u00adable analysis. \nConsequently, open-source and commercial compil\u00aders such as GCC, LLVM, and Intel CC all use SSA-based \ninterme\u00addiate representations when performing such optimizations. Despite their importance, there are \nfew mechanized formaliza\u00adtions of the correctness properties of SSA transformations. This paper tackles \nthis problem by developing proof techniques suitable for mechanically verifying the correctness of SSA-based \noptimiza\u00adtions. We do so in the context of our Vellvm framework [21, 22], which formalizes the operational \nsemantics of programs expressed in LLVM s SSA-based IR [10] and provides Coq [6] infrastructure to facilitate \nmechanized proofs of properties about transformations on the LLVM IR. The key idea is to generalize the \nSSA scoping predicate, which is an invariant of the operational semantics, to more com\u00adplex safety predicates. \nThe main proof obligation then becomes a preservation argument, but one that takes the dominance relation \nof the SSA form into account. Instances of this idea are found in the literature (see, for example, Menon, \net al. [13]), and related proof techniques have been recently used in CompCertSSA [4], which uses translation \nvalidation to check SSA optimizations (see Sec\u00adtion 8). This work is the .rst to verify SSA algorithms \nin the context of an industrial-strength compiler like the LLVM. To demonstrate the utility of these \ntechniques, we use them to establish the correctness of a variant of mem2reg, a crucial LLVM pass that \nis responsible for promoting un-aliased local vari\u00adables and stack-based temporary values into registers. \nWe build our vmem2reg pass by pipelining several simpler transformations, such as load-after-store elimination, \nwhich are common building blocks of many SSA-based optimizations. To improve compilation time, we also \ninvestigate how to fuse such multiple such passes into one. To summarize our contributions, this paper: \n introduces general-purpose proof techniques for verifying SSA optimizations,  shows that the proposed \nSSA proof techniques are viable for use in mechanized proofs, and veri.es several simple transfor\u00admations, \n   describes and proves correct vmem2reg, an easier-to-verify variant of LLVM s mem2reg optimization, \nwhich is a key SSA-based transformation used by LLVM, and  demonstrates that the extracted implementation \nof the fully ver\u00adi.ed vmem2reg is effective: The vmem2reg fully-veri.ed pass yields an average speedup \nover a LLVM -O0 of 77% ver\u00adsus a speedup of 81% speedup obtained by LLVM s implemen\u00ad  Lock-step ~ ~ \n~ S1 S2 S1 S2 S1 S2 t t t t or  ~ ~ ~ S'1 S'2 S'1 S'2 S'1 (with |S'1| < |S1|) ~ ~  S1 S2 S1 S2 t \n t or ~ ~ S'1 S'2 S'2 (with |S'2| < |S2|) .. tation of mem2reg. Figure 1. Backward simulation diagrams \nthat imply program re.nement. In each diagram, the program states of original and compiled programs To \nstreamline the explanation (Section 3), we describe the proof are on the left and right respectively. \nA line denotes a relation ~ between techniques in the context of Vminus, a simpli.ed subset the full \n program states. Solid lines or arrows denote hypotheses; dashed lines or Vellvm framework [22], but \none that still captures the essence of arrows denote conclusions. SSA. We mechanically veri.ed all the \nclaims of the paper both for Vminus and full Vellvm in Coq.1 We use the backward simulation diagrams \nin Figure 1 to prove that a program transformation satis.es the re.nement property. The 2. Background \n CompCert project uses similar diagrams for forward simulation [9]. 2.1 Program re.nement At a high-level, \nwe .rst need to .nd a relation ~ between pro-In this paper, we use program re.nement to reason about \nthe cor-gram states and their transformed counterparts. The relation must rectness of compilation. Following \nthe CompCert project [9], we hold initially, imply equivalent returned values .nally, and imply de.ne \nprogram re.nement in terms of programs external behav-that stuck states are related. Then, depending \non the transforma\u00adiors (which include program traces of input-output events, whether tion, we prove that \na speci.c diagram holds: lock-step simulation a program terminates, and the returned value if a program \ntermi-is for variable substitution, right option simulation is for instruc\u00adnates): a transformed program \nre.nes the original if the behaviors tion removal, and left option simulation is for instruction inser\u00adof \nthe original program include all the behaviors of the transformed tion. Because the existence of a diagram \nimplies that the source program. We de.ne the operational semantics using traces of a la-and target programs \nshare traces, we can prove the equivalence of beled transition system. program traces by decomposing \nprogram transitions into matched j diagrams. To ensure co-termination, the option simulations are Events \ne :: = v = .d( vj ) parameterized by a measure of program states |S| that must de-Finite traces t :: \n= o | e, t crease to prevent in.nite stuttering problems. Finite or in.nite traces T :: = o | e, T (coinductive) \n t2.2 SSA We denote one small-step of evaluation as con.g . S -. S ' : in program environment con.g \n, program state S transitions to Static Single Assignment (SSA) form [7] is an intermediate repre\u00adthe \nstate S ' , recording events e of the transition in the trace t. sentation distinguished by its treatment \nof temporary variables An event e describes the inputs vj and output v of an external each such variable \nmay be de.ned only once, statically, and each t * use of the variable must be dominated by its de.nition \nwith respect function call named .d. con.g . S -. S ' denotes the re.exive, to the control-.ow graph \nof the containing function.2 Intuitively, transitive closure of the small-step evaluation with a .nite \ntrace t. T the variable de.nition dominates a use if all possible execution con.g . S -. 8 denotes a \ndiverging evaluation starting from S paths to the use go through the de.nition .rst. with a .nite or \nin.nite trace T . Program re.nement is given by the To maintain these invariants in the presence of branches \nand following de.nition. loops, SSA form uses f-instructions, which act like control-.ow DEFIN ITIO \nN 1 (Program re.nement). dependent move operations. Such f-instructions appear only at the j start of \na basic block and, crucially, they are handled specially in the 1. init (prog , .d, vj ,S) means S is \nthe initial program state of dominance relation to cut apparently cyclic data dependencies. the program \nprog with the main entry .d and inputs vj . The left part of Figure 2 shows an example program in SSA \n2. .nal (S, v) means S is the .nal state with the return value v . form, written using the stripped-down \nnotation of Vminus (de.ned 3. (prog , .d, vj j ,t, v) means .SS ' . init (prog , .d, vj j ,S), more formally \nin Section 3). The temporary r3 at the beginning of t * con.g . S -. S ' and .nal (S ' , v ). the block \nlabeled l2 is de.ned by a f-instruction: if control enters the block l2 by jumping from basic block l1, \nr3 will get the value 4. (prog , .d, vj j ,T ) means .S. init (prog , .d, vj j ,S) and T 0; if control \nenters from block l2 (via the back edge of the branch con.g . S -. 8. at the end of the block), then \nr3 will get the value of r5. .. (prog , .d, vj j ,t) means .SS ' . init (prog , .d, vj j ,S), 5. The \nSSA form is good for implementing optimizations because t * con.g . S -. S ' and S ' is stuck. it identi.es \nvariable names with the program points at which they j ) means . t, \u00ac .. (prog , .d, vj j ,t) are de.ned. \nMaintaining the SSA invariants thus makes de.nition 6. de.ned (prog , .d, vj .. .. and use information \nof each variable more explicit. Also, because 7. prog2 re.nes program prog1, written prog1 . prog2, if \n j ) each variable is de.ned only once, there is less mutable state to j j be considered (for purposes \nof aliasing, etc.) in SSA form, which (a) de.ned (prog1, .d, vj (b) (prog2, .d, vj ,t, v) . (prog1, \n.d, vj ,t, v )  j j makes certain code transformations easier to implement. (c) (prog2, .d, vj ,T ) \n. (prog1, .d, vj ,T ) Program transformations like the one in Figure 2 are correct if .. j .. . (prog1, \n.d, vj j (d) (prog2, .d, vj ,t) ,t) the transformed program re.nes the original program (in the sense \nNote that re.nement requires only that a transformed program preserves the semantics of a well-de.ned \noriginal program, but does not constrain the transformation of unde.ned programs. 1 Annotated Coq source \navailable at http://cis.upenn.edu/ stevez/vellvm/release.tgz. described above) and the result is well-formed \nSSA. Proving that 2 In the literature, there are different variants of SSA forms [1]. We use LLVM version: \nmemory locations are not in SSA form; LLVM does not maintain any connection between a variable name in \nthe IR and its original name in the source; the live ranges of variables can overlap. Original Transformed \nl1 : \u00b7\u00b7\u00b7 l1 : \u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7 r4 := r1 * r2 br r0 l2 l3 br r0 l2 l3 l2 : r3 = phi int[0, l1][r5, l2] l2 : r3 \n= phi int[0, l1][r5, l2] r4 := r1 * r2 r5 := r3 + r4 r5 := r3 + r4 r6 := r5 = 100 r6 := r5 = 100 br r6 \nl2 l3 br r6 l2 l3 l3 : r7 = phi int[0, l1][r5, l2] l3 : r7 = phi int[0, l1][r5, l2] r8 := r1 * r2 r9 \n:= r8 + r7 r9 := r4 + r7 Figure 2. An SSA-based optimization. In the original program with en\u00adtry l1, \nr1 * r2 is a partial common expression for the de.nitions of r4 and r8, because there is no domination \nrelation between r4 and r8. Therefore, eliminating the common expression directly is not correct: we \ncannot sim\u00adply replace r8 := r1 * r2 by r8 := r4 since r4 is not available at the de.nition of r8 if \nl2 does not execute before l3 runs. To transform this pro\u00adgram, we might .rst move the instruction r4 \n:= r1 * r2 from the block l2 to the block l1, because the de.nitions of r1 and r2 must dominate the end \nof l1, and l1 dominates l2. Then we can safely replace all the uses of r8 by r4, because the de.nition \nof r4 in l1 dominates l3 and therefore dominates all the uses of r8. Finally, r8 is removed, because \nthere are no uses of r8. such code transformations are correct is nontrivial because they involve non-local \nreasoning about the program. 3. Proof Techniques for SSA This section describes the proof techniques \nwe have developed for formalizing properties of SSA-style intermediate representations. To most clearly \narticulate the approach, we present the results us\u00ading a language called Vminus, which is a minimalist \nSSA language containing only those features salient to the proof technique. Vmi\u00adnus is a subset of the \nLLVM IR formalized in Vellvm [22]. The key idea of the technique is to generalize the invariant used \nfor Vminus s preservation lemma for proving safety to other predicates that are also shown to be invariants \nof the operational semantics. Crucially, these predicates all share the same form, which only constrains \nvariable de.nitions that strictly dominate the current program counter. The remainder of this section \n.rst presents the syntax and op\u00aderational semantics of Vminus, then it gives the static semantics and \nproves safety (which in this context simply amounts to show\u00ading that all variables are well-scoped). \nWith these properties es\u00adtablished, we then show how to generalize the safety invariant to a form that \nis useful for proving program transformations correct and demonstrate its applicability to a number of \nstandard optimiza\u00adtions. Because Vminus is such a stripped-down language, the rel\u00adevant lemmas are relatively \nstraightforward to establish; Section 4 shows how to scale the proof technique to the full Vellvm model \nof LLVM to verify the mem2reg pass. 3.1 The simple SSA language Vminus Syntax Figure 3 gives the syntax \nof Vminus. Every Vminus expression is of type integer. Operations in Vminus compute with values val, \nwhich are either temporaries r or constants cnst that must be integer values. We use R to range over \nsets of identi.ers. All code in Vminus resides in a top-level function, whose body is composed of blocks \nb. Here, b denotes a list of blocks; we also use similar notation for other lists. As is standard, a \nbasic block consists of a labeled entry point l, a series of f nodes, a list of commands cs, and a terminator \ninstruction tmn. In the following, we also use the label l of a block to denote the block itself. Types \ntyp :: = int Constants cnst :: = Int Values val :: = r | cnst Binops bop :: = + | * | &#38;&#38; | = \n| = | = |\u00b7 \u00b7 \u00b7 Right-hand-sides rhs :: = val1 bop val2 Commands c :: = r := rhs Terminators tmn :: = \nbr val l1 l2 | ret typ val j Phi Nodes f :: = r = phi typ [valj , lj ] Instructions insn :: = f | c | \ntmn Non-fs . :: = c | tmn Blocks b :: = l f c tmn Functions f :: = fun {b} Values v :: = Int Locals d \n:: = r . v Frames s :: = (pc, d) Prog Counters pc :: = l.i | l. t [val]d = .v. l3 =(v?l1 : l2) f [l3]= \n.(l3 f3 c3 tmn3). [f3]d l = .d ' . E BR f . (l, [], br val l1 l2, d) -. (l3, c3,tmn3, d') [val1]d = \n.v1. [val2]d = .v2. c = r := val1 bop val2 eval (bop , v1, v2)= v3 E BO P f . (l, (c, c),tmn, d) -. \n(l, c,tmn, d{v3/r}) Figure 3. Syntax and Operational Semantics of Vminus The set of blocks making up \nthe top-level function constitutes a control-.ow graph with a well-de.ned entry point that cannot be \nreached from other blocks. We write f [l]= .b. if there is a block b with label l in function f . Here, \nthe .. (pronounced some ) indicates that the function is partial (might return none instead). As usual \nin SSA, the f nodes join together values from a list of predecessor blocks of the control-.ow graph each \nf node takes a list of (value, label) pairs that indicates the value chosen when control transfers from \na predecessor block with the associated la\u00adbel. The commands c include the usual suite of binary arithmetic \nor comparison operations. We denote the right-hand-sides of com\u00admands by rhs. Block terminators (br and \nret) branch to another block or return a value from the function. We also use metavari\u00adable insn to range \nover f-nodes, commands and terminators, and non-phinodes . to represent commands and terminators. Dynamic \nSemantics The operational semantics rules in Figure 3 are parameterized by the top-level function f , \nand relate evaluation frames s before and after an evaluation step. An evaluation frame keeps track of \nthe integer values v bound to local temporaries r in d and current program counter. We also use s.pc \nand s.d to denote the program counter and locals of s respectively. Because Vminus has no function calls, \nthe rules ignore program traces. This simpli.cation does not affect the essence of the proof techniques. \nSection 5 shows the full Vellvm semantics with traces. Instruction positions are denoted by program counters \npc: l.i indicates the i-th command in the block l; l. t indicates the termi\u00adnator of the block l. We \nwrite f [pc]= .insn . if some insn is at the program counter pc of function f . We also use l.(i +1) \nto de\u00adnote the next program counter of l.i. When l.i is the last command of block l, l.(i +1) = l. t. \nTo simplify presentation of the opera\u00adtional semantics, we use l, c,tmn to unpack the instructions at \na program counter in function f . Here, l is the current block, c and tmn are the instructions of l that \nare not executed yet. It is easy to see how the block &#38; offset speci.cation is equivalent to the \ncontinuation commands representation so we omit the details of the correspondence here. .r.(. uses r \n. r . sdomf (pc)) f ' l . (f, l .. fj j . f . ci @ l.i i . f . tmn @(l. t)) NO NPH I WF B f .. @ pc \n. f . l fj j ci i tmn jj jjj uniq (lj ) lj = preds (f, l) .rj .(valj uses rj . rj . sdomf (lj . t)) len \n( [valj , lj ] ) > 0 f . valj : typ PH I f, l .. r = phi typ [valj , lj ] j f . val1 : int f . val2 \n: int f . val : int f [l1]= .b1. f [l2]= .b2. WF BO P WF BR f . r := val1 bop val2 f . br val l1 l2 \nf .. . @ pc f . . uniq (defs (f )) uniq (labels (f )) f = fun {bj j } f . bj j wf entryf WF NON PH I \nWF F f . . @ pc . f Figure 4. Static Semantics of Vminus (excerpt) Most of the Vminus commands have straight-forward \ninterpre\u00adtation. The arithmetic and logic instructions are all unsurprising the [val]d function computes \na value from the local state d and val, looking up the meanings of variables in the local state as needed; \neval implements arithmetic and logic operations. We use [rhs]d to denote evaluating the right-hand-side \nrhs in the state d. There is one wrinkle in specifying the operational semantics when compared to a standard \nenvironment-passing call-by-value language. All of the f instructions for a block must be executed atomically \nand with respect to the old local value mapping due to the possibility of self loops and dependencies \namong the f nodes. For example the well-formed code fragment bellow has a circular dependency between \nr1 and r2 l0 : \u00b7\u00b7\u00b7 l1 : r1 = phi int[r2, l1][0, l0] r2 = phi int[r1, l1][1, l0] r3 := r1 = r2 br r3 l2 \nl1 l2 : \u00b7\u00b7\u00b7 If control enters this block from l0, r1 will map to 0 and r2 to 1, which causes the conditional \nbranch to fail, jumping back to the label l1. The new values of r1 and r2 should be 1 and 0, and not \n1 and 1 as might be computed if they were handled sequentially. This atomic update of the local state, \nsimilar to parallel assignment, is handled by the [f3]l d function as shown in rule E BR. Static semantics \nVminus requires that a program satisfy certain invariants to be considered well formed: every variable \nin the top\u00adlevel function must dominate all its uses, and be assigned exactly, once statically. At a \nminimum, any reasonable Vminus transfor\u00admation must preserve these invariants; together they imply that \nthe program is in SSA form [7]. Figure 4 shows the judgments to check the SSA invariants with respect \nto the control-.ow graph and program points of the function f . To explain the judgments we need the \nfollowing de.nitions. DEFIN ITIO N 2 (Domination). 1. val uses r \" val = r. 2. insn uses r \" .val. val \nuses r . val is an operand of insn . 3. A variable r is de.ned at a program counter pc of function f \n, written f de.nes r @ pc if and only if f [pc]= .insn . and r is the left-hand side of insn . We write \ndefs (f ) to denote the set of all variables de.ned in f . 4. In function f , block l1 dominates block \nl2, written f . l1 : l2, if every path from the entry block of f to l2 must go through l1; l1 strictly \ndominates l2, written f . l1 . l2, if every path from the entry block of f to l2 must go through l1 and \nl1 .l2.  = 5. In function f , pc1 strictly dominates pc2, written f . pc1 . pc2, if pc1 and pc2 are \nat distinct blocks l1 and l2 respectively and f . l1 . l2; if pc1 and pc2 are in the same block, then \npc1 appears earlier than pc2. 6. sdomf (pc) is the set of variables strictly dominating pc: sdomf (pc)= \n{r | f de.nes r @ pc ' and f . pc ' . pc} Rule WF F of Figure 4 ensures that variables defs (f ) de.ned \nin the top-level function are unique, which enforces the single\u00adassignment part of the SSA property. \nAdditionally, all block la\u00adbels labels (f ) in the function must be unique for a well-formed control-.ow \ngraph, and the entry block should have no predeces\u00adsors (wf entryf ). The rule WF B checks that all instructions \nin reachable blocks (written f . l) satisfy the SSA domination invariant. Because un\u00adreachable blocks \nhave no effects at runtime, the rule does not check them. Rule NONPHI ensures that a . at pc is strictly \ndominated by the de.nitions of all variables used by .. The rule PHI ensures that the number of incoming \nvalues is not zero, that all incoming labels are unique, and that the current block s predecessors is \nthe same as the set of incoming labels. If an incoming value valj from a pre\u00addecessor block lj uses a \nvariable rj at pcj , then pcj must strictly dominate the terminator of lj . Importantly, this rule allows \ncyclic uses of SSA variables of the kind shown in the example above. Dominance analysis Dominance analysis \nplays an important role in the type system, so we must .rst prove the following lem\u00admas about the domination \nrelations. They are needed to establish the SSA-based program properties in the following sections. De\u00adtails \nabout these lemmas can be found our prior work [21]. LEM M A 1 (Domination is transitive). LEM M A 2 \n(Strict domination is acyclic). By Lemma 1, sdomf (pc) has the following properties: LEM M A 3 (sdom \nstep). 1. If l.i and l.(i +1) are valid program counters of f , then sdomf (l.(i+1)) = sdomf (l.i).{r} \nwhere f de.nes r @ l.i. 2. If l. t and l ' .0 are valid program counters of f , and l ' is a suc\u00adcessor \nof l, then sdomf (l ' .0) - defs (f) . sdomf (l. t) where f are from the block l ' and defs (f) denotes \nall vari\u00adables de.ned by f.  3.2 Safety of Vminus  There are two ways that a Vminus program might \nget stuck. First, it might try to jump to an unde.ned label, but this property is ruled out statically \nby WF BR. Second, it might try to access a variable whose value is not de.ned in d. We can prove that \nthis second case never happens by establishing the following safety theorem: THE ORE M 4 (Safety). If \n. f and f . (entry .0, \u00d8) -.* s, then s is not stuck. The proof takes the standard form using preservation \nand progress lemmas with the invariant for frames shown bellow: pc . f .r.(r . sdomf (pc) ..v .d[r]= \n.v.) WF FR f . (pc, d) This rule is similar to the predicate used in prior work for verifying the type \nsafety of an SSA-based language [13]. The invariant WF FR shows that a frame (pc, d) is well-formed if \nevery de.nition that strictly dominates pc is de.ned in d. The initial program state satis.es this invariant \ntrivially: LEM M A 5 (Initial State). If . f then f . (entry .0, \u00d8). The preservation and progress lemmas \nare straightforward but note that they crucially rely on the interplay between the invariant on d projected \nonto sdomf (pc) (Lemma 3), and the PHI and NONPHI rules of the static semantics. LEM M A 6 (Preservation). \nIf . f , f . s and f . s -. s ' , then f . s ' . LEM M A 7 (Progress). If . f , f . s, then s is not \nstuck. 3.3 Generalizing Safety to other SSA Invariants The main feature of the preservation proof, Lemma \n6, is that the constraint on sdomf (pc) is an invariant of the operational seman\u00adtics. But and this is \na key observation we can parameterize rule WF FR by a predicate P , which is an arbitrary proposition \nabout functions and frames: s.pc . f P f (s|f ) GWF FR f, P . s Here, s|f is (s.pc, (s.d)|(sdomf (s.pc))) \nand we write (d|R )[r]= .v. iff r . R and d[r]= .v. and observe that dom (d|R )= R. These restrictions \nsay that we don t need to consider all variables: Intuitively, because SSA invariants are based on dominance \nprop\u00aderties, when reasoning about a program state we need only consider the variable de.nitions that \nstrictly dominate the program counter in a given state. For proving Theorem 4, we instantiated P to be: \nPsafety \" .f. .s. .r.r . dom (s.d) ..v.(s.d)[r]= .v. For safety, it is enough to show that each variable \nin the domina\u00adtion set is well de.ned at its use. To prove program transformations correct, we instantiate \nP with a different predicate, Psem , that re\u00adlates the syntactic de.nition of a variable with the semantic \nvalue: .f. .s. .r.f [r]= .rhs.. (s.d)[r] .\u00b7 . (s.d)[r]= [rhs](s.d) = Just as we proved preservation for \nPsafety , we can also prove preservation for Psem (using Lemma 2): THE ORE M 8. If . f and f, Psem . \ns and f . s -. s ' , then f, Psem . s ' . As we show next, Theorem 8 can be used to justify the cor\u00adrectness \nof many SSA-based transformations. Instantiating P with other predicates can also be useful Section 5 \nshows how. 3.4 The correctness of SSA-based transformations Consider again the example code transformation \nfrom Figure 2. It, and many other SSA-based optimizations, can be de.ned by using a combination of simpler \ntransformations: deleting an unused de.\u00adnition, substituting a constant expression for a variable, substituting \none variable by another, or moving variable de.nitions. Each such transformation is subject to the SSA \nconstraints for example, we can t move a de.nition later than one of its uses and each trans\u00adformation \npreserves the SSA invariants. By pipelining these basic transformations, we can de.ne more sophisticated \nSSA-based pro\u00adgram transformations whose correctness is established by the com\u00adposition of the proofs \nfor the basic transformations. In general, an SSA-based transformation from f to f ' is correct if it \npreserves both well-formedness and program behavior. 1. Preserving well-formedness: if . f , then . f \n' . 2. Program re.nement: if . f , then f . f ' (see Section 2.1).  Each of the basic transformations \nmentioned above can be proved correct by using Theorem 8. For the sake of space, here we present only \nthe correctness of variable substitution (though we proved correct all the mentioned transformations \nin our Coq de\u00advelopment). Section 4 shows how to extend the transformations to implement memory-aware \noptimizations in the full Vellvm. Variable substitution Consider the step of the program transfor\u00admation \nfrom Figure 2 in which the use of r8 on the last line is replaced by r4 (this is valid only after hoisting \nthe de.nition of r4 so that it is in scope). This transformation is correct because both r4 and r8 denote \nthe same value, and the de.nition of r4 (after hoist\u00ading) strictly dominates the de.nition of r8. In \nFigure 2, it is enough to do redundant variable elimination this optimization lets us re\u00adplace one variable \nby another when their de.nitions are syntacti\u00adcally equal; other optimizations, such as global value \nnumbering, allow a coarser, more semantic, equality to be used. Proving them correct follows the same \nbasic pattern as the proof shown below. DEFIN I T I ON 3 (Redundant Variable). In a function f , a variable \nr2 is redundant with variable r1 if: 1. f de.nes r1 @ pc1, f de.nes r2 @ pc2 and f . pc1 . pc2 2. f \n[pc1]= .c1., f [pc1]= .c2. and c1 and c2 have syntactically equal right-hand-sides.  We would like \nto prove that eliminating a redundant variable is correct, and therefore must relate a program f with \nf {r1/r2}. Since substitution does not change the control-.ow graph, it pre\u00adserves the domination relations. \nLEM M A 9. 1. f . l1 : l2 .. f {r2/r1} . l1 : l2 2. f . pc1 . pc2 .. f {r2/r1} . pc1 . pc2  Applying \nLemma 1 and Lemma 9, we have: LEM M A 10. Suppose that in f , r1 is redundant with r2. If . f , then \n. f {r2/r1}. Let two program states simulate each other if they have the same local state d and program \ncounter. We assume that the original program and its transformation have the same initial state. LEM \nM A 11. If . f , r2 is redundant with r1 in f , and (pc, d) is a reachable state, then 1. If val is an \noperand of a non-phinode at program counter pc, then .v. [val]d = .v.. [val{r1/r2}]d = .v.. 2. If pc \nis li . t, and li is a previous block of a block with f-nodes  jj j li li fj , then .d ' . [fj ]d = \n.d ' .. [fj {r1/r2} ]d = .d ' .. Proof (sketch): The proof makes crucial use of Theorem 8. For example, \nto show part 1 for a source instruction r := rhs (with transformed instruction r := rhs{r1/r2}) located \nat program counter pc, we reason like this: if r2 is an operand used by rhs, then r2 . sdomf (pc) and \nby Theorem 8, property Psem , implies that d[r2]= [rhs2]d for some rhs2 de.ning r2. Since r1 is used \nas an operand in rhs{r1/r2}, similar reasoning shows that d[r1]= ADCE, GVN, PRE, SCCP, ...  Figure \n5. The tool chain of the LLVM compiler [rhs1]d , but since r2 is redundant with r1, we have rhs2 = rhs1, \nand the result follows immediately. . Using Lemma 11, we can easily show the lock-step simulation lemma, \nwhich completes the correctness proof: LEM M A 12. If . f , r2 is redundant with r1 in f , f {r1/r2} \n. s1 -. s2, then f . s1 -. s2. 4. LLVM and mem2reg From Vminus to Vellvm Vminus provides a convenient \nminimal setting in which to study SSA-based optimizations, but it omits many features necessary in a \nreal intermediate representation. To demonstrate that our proof techniques can be used for practical \ncompiler optimizations, we next show how to apply them to the LLVM IR used in Vellvm [22]. LLVM [10] \n(Low-Level Virtual Machine) is a robust, industrial\u00adstrength, and open-source compilation framework that \ncompetes with GCC in terms of compilation speed and performance of the generated code. LLVM uses a platform-independent \nSSA-based intermediate representation [10], and provides a large suite of optimization passes, including \naggressive dead code elimination, global value numbering, partial redundancy elimination, and sparse \nconditional constant propagation, among others. Each transforma\u00adtion pass consumes and produces code \nin this SSA form, and they typically have the .avor of the code transformations described above in Section \n3. Figure 5 depicts LLVM s tool chain. The Vellvm infrastructure provides a Coq implementation of the \nfull LLVM 3.0 intermediate language and de.nes (several) operational semantics along with some useful \nmetatheory about the memory model. Figure 6 shows the additional Vellvm features needed to explain the \nfollowing proofs; more details about the operational semantics can be found in our earlier Vellvm work \n[22]. A program prog includes a list of products prod that are either global constants or function de.nitions. \nThe address of a global constant with type typ is a constant typ * gid of type typ*. Vellvm also has \nmemory operations that include stack allocation, loads, and stores. At runtime, a pointer in is represented \nby a block name blk and an offset ofs within the block. All globals are allocated before the start of \na program and stored in a mapping g. A program state S is composed of a memory state M and a list of \nstack states s. t We denote small-step evaluation by con.g . S -. S ' where a program con.guration con.g \nincludes a program prog and globals. Judgment . prog denotes a well-formed program it checks that all \nde.nitions of globals and functions are unique, each function is well-formed, etc. 4.1 The mem2reg Optimization \nPass A critical piece of LLVM s compilation strategy is the mem2reg pass, which takes code that is trivially \nin SSA form and converts it into a minimal, pruned SSA program [15]. This strategy simpli\u00ad.es LLVM s \nmany front ends by moving work in to mem2reg. An SSA form is minimal if each f is placed only at the \ndominance frontier of the de.nitions of the f node s incoming variables [7]. A minimal SSA form is pruned \nif it contains only live f nodes [15]. This pass enables many subsequent optimizations (and, in particu\u00adlar, \nbackend optimizations such as register allocation) to work ef\u00adfectively. Programs prog :: = prod Products \nprod :: = gid = global typ const | f Functions f :: = de.ne typ .d (arg ) {b} Types typ :: = \u00b7\u00b7\u00b7 | typ* \nConstants cnst :: = \u00b7\u00b7\u00b7 | typ * gid Commands c :: = \u00b7\u00b7\u00b7 | r := alloca typ | r := load ( typ * ) r1 | \nstore typ val1 r2 | option r = call typ .d param Values v :: = \u00b7\u00b7\u00b7 | blk .ofs Globals g :: = r .. v Con.gurations \ncon.g :: = prog ,g Allocas a :: = \u00d8 | blk , a Frames s :: = (f, pc, d, a) Call stacks s :: = \u00d8 | s, s \nProgram states S :: = M , s Figure 6. The syntax and program states of Vellvm (excerpt) Figure 7 demonstrates \nthe importance of the mem2reg pass for LLVM s generated code performance. In our experiments, run\u00adning \nonly the mem2reg pass yields a 81% speedup (on average) compared to LLVM without any optimizations; doing \nthe full suite of -O1 level optimizations (which includes mem2reg) yields a speedup of 102%, which means \nthat mem2reg alone captures all but %12 of the bene.t of the -O1 level optimizations. Comparison with \n-O3 optimizations yields similar results. These observations make mem2reg an obvious target for our veri.cation \nefforts. The trivial SSA form is generated directly by compiler front ends, and it uses the alloca instruction \nto allocate stack space for every source-program local variable and temporary needed. In this form, an \nLLVM SSA variable is used either only locally to access those stack slots, in which case the variable \nis never live across two basic blocks, or it is a reference to the stack slot, whose lifetime corresponds \nto the source-level variable s scope. These constraints mean that no f instructions are needed it is \nextremely straightforward for a front end to generate code in this form. As an example, consider this \nC program: int i = 0; while (i<=100) i++; return i;  The trivial SSA form that might be produced by \nthe frontend of a compiler is shown in the left-most column of Figure 8. The r0 := alloca int instruction \non the .rst line allocates space for the source variable i, and r0 is a reference from which local load \nand store instructions access i s contents. The mem2reg pass converts promotable uses of stack-allocated \nvariables to SSA temporaries. An alloca ed variable like r0 is considered to be promotable, written promotable \n(f, r0), if it is created in the entry block of function f and it doesn t escape i.e., its value is never \nwritten to memory or passed as an argument to a function call. The mem2reg pass identi.es promotable \nstack allocations and then replaces them by temporary variables in SSA form. It does this by placing \nf nodes, substituting each variable de.ned by a load with the previous value stored into the stack slot, \nand then eliminating the memory operations (which are now dead). The right-most column of Figure 8 shows \nthe resulting pruned SSA program for this example. Proving that mem2reg is correct is nontrivial because \nit makes signi.cant, non-local changes to the use of memory locations and temporary variables. Furthermore, \nthe speci.c mem2reg algo\u00adrithm used by LLVM is not directly amenable to the proof tech\u00adniques developed \nin Section 3 it was not designed with veri.ca\u00adtion in mind, so it produces intermediate stages that break \nthe SSA invariants or do not preserve semantics. This section therefore de\u00adscribes an alternate algorithm \nthat is more suitable to formalization.  Figure 7. Normalized execution time improvement of the LLVM \ns mem2reg, LLVM s O1, and LLVM s O3 optimizations over the LLVM baseline with optimizations disabled. \nFor comparison, GCC-O3 s speedup over the same baseline is also shown. trivial SSA Maximal f nodes placement \nAfter LAS/LAA/SAS After DSE/DAE After f nodes elimination l1 : r0 := alloca int store int 0 r0 br l2 \nl2 : r1 := load ( int * ) r0 r2 := r1 = 100 br r2 l3 l4 l3 : r3 := load ( int * ) r0 r4 := r3 +1 store \nint r4 r0 br l2 l4 : r5 := load ( int * ) r0 ret int r5 l1 : r0 := alloca int store int 0 r0 r7 := load \n( int * ) r0 br l2 l2 : r6 = phi [r7, l1][r9, l3] store int r6 r0 r1 := load ( int * ) r0 r2 := r1 = \n100 r8 := load ( int * ) r0 br r2 l3 l4 l3 : r10 = phi [r8, l2] store int r10 r0 r3 := load ( int * ) \nr0 r4 := r3 +1 store int r4 r0 r9 := load ( int * ) r0 br l2 l4 : r11 = phi [r8, l2] store int r11 r0 \nr5 := load ( int * ) r0 ret int r5 l1 : r0 := alloca int store int 0 r0 br l2 l2 : r6 = phi [0, l1][r9, \nl3] store int r6 r0 r2 := r6 = 100 br r2 l3 l4 l3 : r10 = phi [r6, l2] r4 := r10 +1 store int r4 r0 br \nl2 l4 : r11 = phi [r6, l2] store int r11 r0 ret int r11 l1 : l1 : br l2 br l2 l2 : r6 = phi [0, l1][r4, \nl3] l2 : r6 = phi [0, l1][r4, l3] r2 := r6 = 100 r2 := r6 = 100 br r2 l3 l4 br r2 l3 l4 l3 : r10 = phi \n[r6, l2] l3 : r4 := r10 +1 r4 := r6 +1 br l2 br l2 l4 : r11 = phi [r6, l2] l4 : ret int r11 ret int r6 \n Figure 8. The SSA construction by the vmem2reg pass 4.2 The vmem2reg Algorithm The vmem2reg algorithm \nis structured to lead to a clean for\u00admalism and yet still produce programs with effectiveness similar \nto the LLVM mem2reg pass. To demostrate the main ideas of vmem2reg, this section describes an algorithm \nthat uses straight\u00adforward micro-pass pipelining. Section 6 presents a smarter way to fuse the micro \npasses, thereby reducing compilation time. Prov\u00ading pipeline fusion correct is (by design) independent \nof the proofs for the vmem2reg algorithm shown in the section. At a high level, vmem2reg (whose code \nis shown in Fig\u00adure 10) traverses all functions of the program, applying the trans\u00adformation vmem2reg \nfn to each. Figure 9 depicts the main loop, which is an extension of Aycock and Horspool s SSA construc\u00adtion \nalgorithm [3]. vmem2reg fn .rst iteratively promotes each promotable alloca by adding f nodes at the \nbeginning of every block. After processing all promotable allocas, vmem2reg fn removes redundant f nodes, \nand eventually will produce a program almost in pruned SSA form,3 in a manner similar to previous algo\u00adrithms \n[15]. 3 Technically, fully pruned SSA requires a more aggressive dead-f\u00adelimination pass that we omit \nfor the sake of simplicity. Section 7 shows that this omission is negligible. The transformation that \nvmem2reg fn applies to each func\u00adtion is a composition of a series of micro transformations (LAS, LAA, \nSAS, DSE, and DAE, shown in Figure 9). Each of these transformations preserves the well-formedness and \nsemantics of its input program; moreover, these transformations are relatively small and local, and can \ntherefore be reasoned about more easily. At each iteration of alloca promotion, vmem2reg fn .nds a promotable \nallocation r. Then f-nodes placement (code shown in Figure 10) adds f nodes for r at the beginning of \nev\u00adery block. To preserve both well-formedness and the original pro\u00adgram s semantics, f-nodes placement \nalso adds additional loads and stores around each inserted f node. At the end of ev\u00adery block that has \nsuccessors, f-nodes placement introduces a load from r, and stores the result in a fresh temporary; at \nthe begin\u00adning of every block that has a predecessor, f-nodes placement .rst inserts a fresh f node whose \nincoming value from a predeces\u00adsor l is the value of the corresponding load added at the end of l, then \ninserts a store to r with the value of the new f node. The second column in Figure 8 shows the result \nof running the f-node placement pass starting from the example program in its trivial SSA form. It is \nnot dif.cult to check that this code is in SSA form. Moreover, the output program also preserves the \nmeaning of the original program. For example, at the end of block l1, the pro\u00ad   Figure 9. Basic structure \nof vmem2reg fn let vmem2reg prog = map (function f . vmem2reg_fn f | prod . prod) prog let f-nodes_placement \nfr = let de.ne typ .d (arg ) {b} = f in let (ldnms, phinms) = gen_fresh_names b in de.ne typ .d (arg \n) {(map (function l f c tmn . let r := alloca typ . f in ' let (f , c1)= match predecessors_of f l \nwith | [] . (f, c) j | lj j . let rj = map (find ldnms) lj j in ' let r = find phinms l in j ' ' (r \n= phi typ [rj , lj ] ::f,store typ r r::c) end in ' let c = match successors_of f l with | [] . c1 ' \n|_ . let r = find ldnms l in ' c1 ++ [r := load ( typ * ) r] end in ' ' l f c tmn) b)} let rec eliminate_stld \nfr = match find_stld_pair fr with | LAS (pc2, val2, r1) . eliminate_stld (f {val2/r1} - r1) r | LAA r1 \n. eliminate_stld (f {0/r1} - r1) r | SAS (pc1, pc2) . eliminate_stld (f - pc1) r | NONE . f end Figure \n10. The algorithm of vmem2reg gram loads the value stored at r0 into r7. After jumping to block l2, the \nvalue of r7 is stored into the location r0, which should con\u00adtain the same values as r7. Therefore, the \nadditional store does not change the status of memory. Although the output program con\u00adtains more temporaries \nthan the original program, these temporaries are used only to connect inserted loads and stores, and \nso they do not interfere with the original temporaries. To remove the additional loads and stores introduced \nby the f-node placement pass and eventually promote allocas to regis\u00adters, vmem2reg fn next applies a \nseries of micro program trans\u00adformations until no more optimizations can be applied. First, vmem2reg \nfn iteratively does the following transforma\u00adtions (implemented by eliminate stld shown in Figure 10): \n1. LAS (r1, pc2, val2) Load After Store : r1 is loaded from r after a store of val2 to r at program counter \npc2, and there are no other stores of r in any path (on the control-.ow graph) from pc2 to r1. In this \ncase, all uses of r2 can be replaced by val2, and the load can be removed. 2. LAA r1 Load After Alloca \n: As above, but the load is from an uninitialized memory location at r. r1 can be replaced by LLVM s \ndefault memory value, and the load can be removed.  3. SAS (pc1, pc2): The store at program counter \npc2 is a store after the store at program counter pc1. If both of them access r, and there is no load \nof r in any path (on the control-.ow graph) from pc1 to pc2, then the store at pc1 can be removed.  \nAt each iteration step of eliminate stld, the algorithm uses the function find stld pair to identify \neach of the above cases. Because the f-node placement pass only adds a store and a load as the .rst and \nthe last commands at each block respec\u00adtively, find stld pair only needs to search for the above cases \nwithin blocks. This simpli.es both the implementation and proofs. Moreover, eliminate stld must terminate \nbecause each of its transformations removes one command. The third column in Fig\u00adure 8 shows the code \nafter eliminate stld. Next, the algorithm uses DSE (Dead Store Elimination) and DAE (Dead Alloca Elimination) \nto remove the remaining unnec\u00adessary stores and allocas. The fourth column in Figure 8 shows the code \nafter DSE and DAE. Finally, vmem2reg fn eliminates unnecessary and dead f nodes. A f-node is unnecessary \n[3] if f is of the form r = j phi typ [valj , lj ] where all the valj s are either equal to r or to \nval. In this case, uses of r can be replaced by val, and the f node can be removed. Aycock and Horspool \n[3] proved that when there is no such f node in a reducible program, the program is of the minimal SSA \nform. The right-most column in Figure 8 shows the .nal output of the algorithm. 5. Correctness of vmem2reg \nWe prove the correctness of vmem2reg using the techniques de\u00adveloped in Section 3. At a high level, the \ncorrectness of vmem2reg is the composition of the correctness of each micro transformation of vmem2reg \nshown in Figure 10. Given a well-formed input pro\u00adgram, each shaded box must produce a well-formed program \nthat preserves the semantics of the input program. Moreover, the mi\u00adcro transformations except DAE and \nf-nodes elimination must pre\u00adserve the promotable predicate, because the correctness of sub\u00adsequent transformations \nrelies on fact that promotable allocations aren t aliased. Formally, let prog {f ' /f } be the substitution \nof f by f ' in prog , and let Uf l be a micro transformation of f applied by vmem2reg. U l must satisfy: \n1. Preserving promotable: when U l is not DAE or f-nodes elimination, if promotable (f, r), then promotable \n(Uf l,r). 2. Preserving well-formedness: if promotable (f, r) when U l is f-nodes placement, and . prog \n, then . prog {Uf l/f }.  3. Program re.nement: if promotable (f, r) when U l is not f\u00adnodes elimination, \nand . prog , then prog . prog {Uf l/f }.   5.1 Preserving promotability At the beginning of each iteration \nfor promoting allocas, the algorithm indeed .nds promotable allocations. LEM M A 13. If prog . f , and \nvmem2reg fn .nds a promotable allocation r in f , then promotable (f, r). We next show that f-nodes placement \npreserves promotable: LEM M A 14. If promotable (f, r), then promotable (f nodes placement f r, r). Each \nof the other micro transformations is composed of one or two more basic transformations: variable substitution, \ndenoted by f {val/r}, and instruction removal, denoted by .lter check f where .lter removes an instruction \ninsn from f if check insn =  ... ... false. For example, f {val2/r1} - r1 (LAS) is a substitution \nfol\u00adlowed by a removal in which check insn = false iff insn de\u00ad.nes r1; DSE of a promotable alloca r \nis a removal in which check insn = false iff insn is a store to r. We .rst establish that substitution \nand removal preserve promotable. LEM M A 15. Suppose promotable (f, r), 1. If \u00ac(val1 uses r), then promotable \n(f {val1/r1},r). 2. If check insn = false . insn does not de.ne r, then promotable (.lter check f, r). \n We can show that the other micro transformations preserve promotable by checking the preconditions \nof Lemma 15. LEM M A 16. Suppose promotable (f, r), r is still promotable after LAS, LAA, SAS or DSE. \nThe substituted value of LAS is written to memory by a store in f , which cannot use r because r is promotable \nin f . The substituted value of LAA is a constant that cannot use r trivially. Moreover, LAS, LAA, SAS \nand DSE remove only loads or stores. 5.2 Preserving well-formedness It is suf.cient to check the following \nconditions to show that a function-level transformation preserves well-formedness: LEM M A 17. Suppose \n1. Uf l and f have the same signature. 2. if prog . f , then prog {Uf l/f } . Uf l.  If . prog , then \n. prog {Uf l/f }. It is easy to see that all transformations vmem2reg applies satisfy the .rst condition. \nWe .rst prove that f-nodes placement preserves the second condition: LEM M A 18. If promotable (f, r), \nprog . f and let f ' be f nodes placement f r, then prog {f ' /f } . f ' . Similarly, to reason about \nother transformations, we .rst estab\u00adlish that substitution and removal preserve well-formedness. LEM \nM A 19. Suppose prog . f , 1. If f . val1 . r2, f ' = f {val1/r2}, then prog {f ' /f } . f ' . 2. If \ncheck insn = false . f does not use insn , and let f ' be .lter check f , then prog {f ' /f } . f ' . \n Here, f . val1 . r2 if f . r1 . r2 when val1 uses r1. Note that the .rst part of Lemma 19 is an extension \nof Lemma 10 that only allows substitution on commands. In vmem2reg, LAS and f-nodes elimination may transform \nf-nodes. LAS, LAA and f-nodes elimination remove instructions after substitution. The following auxiliary \nlemma shows that the substi\u00adtuted de.nition is removable after substitution (by Lemma 2): LEM M A 20. \nIf f . val1 . r2, then f {val1/r2} does not use r2. By Lemma 19, Lemma 20 and the proofs [3], we have: \n LEM M A 21. LAS, LAA, SAS, DSE, DAE and f-nodes elimination preserve well-formedness. 5.3 Program re.nement \nThe proofs of program re.nement use the simulation diagrams in Section 2 and different instantiations \nof the GWF FR rule we developed in Section 3, where instead of just a function f and frame s, we now \nhave a con.guration con.g that also includes the program memory. Promotability As we discussed above, \nthe micro transformations (except f-nodes elimination) rely on the promotable property. We start by establishing \nthe invariants related to promotability, namely that promotable allocations aren t aliased. This proof \nis itself an application of GWF FR. The promotable property ensures that a promotable alloca of a function \ndoes not escape the function can access the data stored at the allocation, but cannot pass the address \nof the al\u00adlocation to other contexts. Therefore, in the program, the pro\u00admotable alloca and all other \npointers (in memory, local tem\u00adporaries and temporaries on the stack) must not alias. Formally, given \na promotable allocation r with type typ* in f , we de.ne Pnoalias (f, r, typ): .con.g . .S. . s1 ++ s \n:: s2 = S.s.f = s.f . [r]s.d = .blk. =. .v.load (S.M , typ, blk)= .v . '' '' ..blk ..typ .\u00acload (S.M \n, typ , blk )= .blk. ..r ' .r . \u00ac[r ' = = =]s.d .blk. ..s ' . s1..r ' .\u00ac[r ' ]s' .d = .blk.  The last \nclause ensures that the alloca and the variables in the callees reachable from f do no alias. In CompCert, \nthe translation from C#minor to Cminor uses properties (in non-SSA form) similar to Pnoalias (f, r, typ) \nto allocate local variables on stack. LEM M A 22 (A promotable alloca is not aliased). At any reach\u00adable \nprogram state S, con.g ,Pnoalias (f, r, typ) . S holds. The invariant holds initially. At all reachable \nstates, the invariant holds because a promotable allocation cannot be copied to other temporaries, stored \nto memory, passed into a function, or returned. Therefore, in a well-de.ned program no external code \ncan get its location by accessing other temporaries and memory locations. Im\u00adportantly, the memory model \nensures that from a consistent initial memory state, all memory blocks in temporaries and memory are \nallocated it is impossible to forge a fresh pointer from an integer. f-node placement Figure 11 pictorially \nshows an example (which is the code fragment from Figure 8) of the simulation relation ~ for proving \nthat the f-node placement preserves semantics. It follows left option simulation, because f-node placement \nonly inserts instructions. We use the number of unexecuted instructions in the current block as the measure \nfunction. The dashed lines indicate where the two program counters must be synchronized. Although the \npass de.nes new variables and stores (shaded in Figure 11), the variables are only passed to the new \nf nodes, or stored into the promotable allocation; additional stores only update the promotable allocation \nwith the same value. Therefore, by Lemma 22, ~ requires that two programs have the same memory states \nand the original temporaries match. LEM M A 23. If f ' = f nodes placement f r, and promotable (f, r), \nand . prog , then prog . prog {f ' /f }. Promotable Allocation  DSE DAE Memory simulation Frame simulation \n Figure 12. The simulation relation for DSE and DAE The interesting case is to show that ~ implies a \ncorrespondence between stuck states. Lemma 22 ensures that the promotable al\u00adlocation cannot be dereferenced \nby operations on other pointers. Therefore, the inserted memory accesses are always safe. LAS/LAA We \npresent the proofs for the correctness of LAS. The proofs for the correctness of LAA is similar. In the \ncode after f\u00adnode placement of Figure 8, r7 := load ( int * ) r0 is an LAS of store int 0 r0. We observe \nthat at any program counter pc between the store and load, the value stored at r0 must be 0 because alive \n(pc1, pc2) holds the store de.ned at pc1 is not overwritten by other writes until pc. To formalize the \nobservation, consider a promotable r with type typ* in f . Suppose find stld pair fr = LAS (pc2, val2, \nr1). Consider the invariant Plas (f, r, typ, pc2,val2): .con.g . .S. .s . S.s. (f = s.f . [val2]s.d = \n.v2.. [r]s.d = .blk.. alive (pc2, s.pc)) =. load (S.M , typ, blk)= .v2. Using Lemma 22, we have that: \nLEM M A 24. If promotable (f, r), then alive (pc2,r1) and at any reachable state S, con.g ,Plas (f, r, \ntyp, pc2,val2) . S holds. Let two programs relate to each other if they have the same program states. \nLemma 24 establishes that the substitution in LAS is correct. The following lemma shows that removal \nof unused instructions preserves semantics in general. LEM M A 25. If check insn = false . f does not \nuse insn , and . prog , then prog . prog {.lter check f/f }. Lemma 20 shows that the precondition of \nLemma 25 holds after the substitution in LAS. Finally, we have that: LEM M A 26. LAS preserves semantics. \nSAS/DSE/DAE Here we discuss only the simulation relations used by the proofs. SAS removes a store to \na promotable allo\u00adcation overwritten by a following memory write. We consider a memory simulation that \nis the identity when the program counter is outside the SAS pair, but ignores the promotable alloca when \nthe program counter is between the pair. Due to Lemma 22 and the fact that there is no load between a \nSAS pair, no temporaries or other memory locations can observe the value stored at the promotable alloca \nbetween the pair. Figure 12 pictorially shows the simulation relations between the program states before \nand after DSE or DAE. Shaded mem\u00adory blocks contain uninitialized values. The program states on the top \nare before DSE, where r2 is a temporary that holds the pro\u00admotable stack allocation and is not used by \nany loads. After DSE, the memory values for the promotable allocation may not match the original program \ns corresponding block. However, values in tem\u00adporaries and all other memory locations must be unchanged \n(by Lemma 22). Note that unmatched memory states only occur after the promotable allocation; before the \nallocation, the two memory states should be the same. The bottom part of Figure 12 illustrates the relations \nbetween programs before and after DAE. After DAE, the correspondence between memory blocks of the two \nprograms is not bijective, due to the removal of the promotable alloca. However, there must exist a mapping \n~ from the output program s memory blocks to the original program s memory blocks. The simulation requires \nthat all values stored in memory and temporaries (except the promotable allocation) are equal modulo \nthe mapping ~. j f-nodes elimination Consider r = phi typ [valj , lj ] (an AH f-node) where all the \nvalj s are either equal to r or some val ' . Lemma 21 showed that f . val ' . r. Intuitively, at any \npc that both val ' and r strictly dominate, the values of val ' and r must be the same. Consider the \ninvariant Pah(f, r, val ' ): .con.g . .S. .s . S.s. f = s.f . [r]s.d = .v1.. [val ' ]s.d = .v2. =. v1 \n= v2 LEM M A 27. con.g ,Pah(f, r, val ' ) . S holds for any reachable program state S. Lemma 27 establishes \nthat the substitution in f-nodes elim\u00adination is correct by using the identity relation. Lemma 20 and \nLemma 25 show that removing dead f-nodes is correct. 5.4 The correctness of vmem2reg Our main result, \nfully veri.ed in Coq, is the composition of the correctness proofs for all the micro program transformations: \nTHE ORE M 28 (vmem2reg is correct). If f ' = vmem2reg f and . prog , then . prog {f ' /f } and prog . \nprog {f ' /f }. 6. Pipeline Fusion In vmem2reg (show in Figure 9), there are two loops of micro pass-pipelinings: \neliminate stld and the redundant f-node elimination. This section gives a smarter pipelining vmem2reg-O \nthat fuses the micro passes, reducing compilation time. For the sake of space, we present the fused version \nof eliminate stld; the fused f-node elimination follows similarly (see our Coq devel\u00adopment). The design \nand correctness proofs described here should be applicable to other optimizations besides vmem2reg. At \na high level, Figure 13 gives eliminate stld O, the fused eliminate stld, which .rst takes a pass to \n.nd all initial eliminations, fuses them, and then takes another pass that elimi\u00adnates load s and store \ns in terms of fused eliminations. We use actions ac to denote micro eliminations: Actions ac :: = r . \nval | pc . # Here, r . val denotes LAS (r, pc, val) or LAA r with the default memory value val and pc \n. # denotes SAS (pc, pc ' ). We use AC to denote a list of actions, and AC (f ) to denote a single pass \nof f that, for each r . val, replaces all uses of r by val, then removes the de.nition of r and, for \neach pc . #, removes the store at pc. To .nd all initial actions, eliminate stld O traverses the blocks \nof a function, uses find stld pairs block to .nd actions for each block, and then concatenates them. \nAt each block, find stld pairs block traverses each command (by find stld pairs cmd), and uses stld state \nto keep track of the search state: ST INIT is the initial state; ST AL typ records the element type of \nthe promotable allocation; ST ST pc val records the value stored by the latest store at pc to the promotable \nallocation. let find_stld_pair_cmd r acc c = let (st, AC ) = acc in match c with | r0 := alloca typ \n. if r = r0 then (ST_AL typ, AC ) else acc | pc0:store typ val1 r2 . let st = ST_ST pc0 val1 in  if \nr = r2 then match st with | ST_ST pc _ . (st , (pc .. #, AC )) |_ . (st , AC ) end else acc | r0 := \nload ( typ * ) r1 . if r = r1 then match st with | ST_ST _ val . (st, (r0 .. val, AC )) | ST_AL typ . \n(st, (r0 .. undef typ, AC )) |_ . acc end else acc |_ . acc end let find_stld_pairs_block rb = let \n(_ _ c _) = b in fold_left (find_stld_pair_cmd r) c (ST_INIT, \u00d8) let rec fuse_actions AC = match AC with \n| \u00d8. \u00d8 ' | r .. val, AC . '' ' let AC = fuse_actions AC in '' let val ' = find_parent AC val in r .. \nval ' ,AC '' {val ' /r} ' ' | pc .. #, AC . pc .. #,fuse_actions AC end let eliminate_stld_O rf = let \nfheader {b} = f in let AC = flat_map (rev (snd (find_stld_pairs_block r))) b in (fuse_actions AC )(f \n) Figure 13. eliminate stld of vmem2reg-O eliminate stld O must fuse the initial actions before transforming \nf . Let values be vertices, and elements in actions be edges. A list of actions found from a well-formed \nfunction forms a forest because SSA ensures acyclicity of def/use chains. fuse actions fuses the forest \nto be a forest with depth one: each non-root node in a tree of the original forest maps to the root of \nthe tree. In fuse actions, find parent AC val returns the parent of val in the forest formed by AC if \nval has a parent; otherwise returns val; AC {val/r} substitutes r in the codomain of AC by val. Suppose \nthat eliminate stld O .nds a list of actions: r4 . r3,r5 . r4,r2 . r1,r3 . r2,r6 . r3, []. fuse actions \nreturns r4 . r1,r5 . r1,r2 . r1,r3 . r1,r6 . r1, []. The interesting part for the correctness of vmem2reg-O \nis showing that the fused pass produces the same output as the pipelined transformations: LEM M A 29 \n(eliminate stld O is correct). If prog . f , then eliminate stld rf = eliminate stld O rf . By Lemma \n29 and Theorem 30, we have that THE ORE M 30 (vmem2reg-O is correct). If f ' = vmem2reg-O f and . prog \n, then . prog {f ' /f } and prog . prog {f ' /f }. Figure 14. Execution speedup over LLVM -O0 for both \nthe ex\u00adtracted vmem2reg and LLVM s original mem2reg pass. 7. Discussion and Evaluation Coq Development \nOur Coq development for SSA optimizations consists of approximately 838 lines of algorithm implementations \nand 50k lines of (not particularly automated) correctness proof scripts and supporting infrastructure. \nWe expect that much of these proofs can be reused for other SSA-based optimizations that is the reason \nwhy we chose the pipeline of micro transformations structure. The development relies on about a dozen \naxioms, almost all of which de.ne either the initial state of the machine (i.e., where in memory functions \nand globals are stored) or the behavior of external function calls. One axiom asserts that memory alignment \nis a power of two, which is not necessary for LLVM programs in general, but is true of almost all real-world \nplatforms. Extracted vmem2reg and experimental methodology We used the Coq extraction mechanism to obtain \na certi.ed implementation of the vmem2reg optimization directly from the Coq sources. mem2reg is the \n.rst optimization pass applied by LLVM4 , so we tested the ef.cacy of the extracted implementation on \nLLVM IR bitcode generated directly from C source code using the clang compiler. At this stage, the LLVM \nbitcode is unoptimized and in trivial SSA form. To prevent the impact of this optimization pass from \nbeing masked by subsequent optimizations, we apply either LLVM s mem2reg or the extracted vmem2reg to \nthe unoptimized LLVM bitcode and then immediately invoke the back-end code generator. We evaluate the \nperformance of the resultant code on a 2.66 GHz Intel Core 2 processor running benchmarks selected from \nthe SPEC CPU benchmark suite that consist of over 336k lines of C source code in total. Figure 14 reports \nthe execution time speedups (larger is better) over a LLVM s-O0 compilation baseline for various benchmarks. \nThe left bar of each group shows the speedup of the extracted vmem2reg, which provides an average speedup \nof 77% over the baseline. The right bar of each group is the bene.t provided by LLVM s mem2reg, which \nprovides 81% on average; vmem2reg captures much of the bene.t of the LLVM s mem2reg. Comparing vmem2reg \nand mem2reg The vmem2reg pass differs from LLVM s mem2reg in a few ways. First, mem2reg promotes allocas \nused by LLVM s intrinsics, while vmem2reg conservatively considers such allocas to potentially escape, \nand so does not promote them. We determined that such intrinsics (used by LLVM to annotate the liveness \nof variable de.nitions) lead to almost all the difference in performance in the equake benchmark. Second, \nalthough vmem2reg deletes most unused f-nodes, it does not aggressively remove them and, therefore, does \nnot generate fully pruned SSA as mem2reg does. However, our results show that this does not impose a \nsigni.cant difference in performance. Compilation time The focus of this work is compiler correctness, \nnot compilation time. Although the code generated by vmem2reg is comparable to that of mem2reg, their \ncompilation times differ, for two reasons. First, the extracted OCaml programs use purely functional \ndata structures that impose O(log n) overhead compared to the ef.cient imperative hash tables available \nin C++. Second, the 4 All results reported are for LLVM version 3.0. pessimistic f-node placement algorithm \nintroduces unnecessary f nodes. In our future work, we plan to use the minimal f-node place\u00adment algorithm \nin vmem2reg, whose correctness is (by design) independent of the proofs presented in this paper. Moreover, \nunlike all or nothing translations, vmem2reg maintains the SSA in\u00advariants at each pass and can thus \nbe applied incrementally to boost performance. In practice, should compilation time be an issue, it is \nalways possible to use mem2reg for development and vmem2reg to ensure correctness for a .nal release. \n8. Related Work and Conclusions Verifying the correctness of compiler transformations is an active research \narea with a sizable amount of literature. We focus here on the work relevant to SSA-based optimizations. \nCompCertSSA CompCertSSA [4] improves the CompCert com\u00adpiler with a veri.ed SSA-based middle-end and a \nGVN optimiza\u00adtion pass. CompCertSSA veri.ed a translation validator for an SSA construction algorithm \nthat takes imperative variables to vari\u00adables in a pruned SSA form. In contrast, our work fully veri.es \nthe SSA construction pass vmem2reg for LLVM directly. A bug in the CompCertSSA compiler will cause the \nvalidator to abort the compilation, whereas verifying the compiler rules out such a pos\u00adsibility. More \npragmatically, translation validation is harder to ap\u00adply in the context of LLVM, because the compiler \ninfrastructure was not created with veri.cation in mind. For example, the Comp-CertSSA translations maintain \na close mapping between source and target variable names so that simulation can be checked by simple \nerasure; this is not feasible in the LLVM framework. The Comp-CertSSA project reports performance measurements \nof only small benchmarks totaling about 6k lines, whereas we have tested our pass on 336k lines, including \nlarger programs. Unsurprisingly, the CompCertSSA and Vellvm proofs share some similarities. For example, \nCompCertSSA s GVN proof uses an invariant similar to the one in our Theorem 8 and Lemma 12. However, \nthe LLVM s strategy of promoting allocas means that our proofs need a combination of both SSA and aliasing \nproperties to prove correctness. Moreover, our proof technique of pipelining micro transformations is \nnovel, and should be broadly applicable. Other related work In less closely related work, Mansky et al. \ndesigned an Isabelle/HOL framework that uses control-.ow graph rewrites to transform programs and uses \ntemporal logic and model\u00adchecking to specify and prove the correctness of program trans\u00adformations [11]. \nThey veri.ed an SSA construction algorithm in the framework. Other researchers have formalized speci.c \nSSA\u00adbased optimizations by using SSA forms with different styles of semantics: an informal semantics \nthat describes the intuitive idea of the SSA form [7]; an operational semantics based on a matrix representation \nof f nodes [19]; a data-.ow semantics based term graphs using the Isabelle/HOL proof assistant [5]. Matsuno \net al. de.ned a type system equivalent to the SSA form and proved that dead code elimination and common \nsubexpression elimination pre\u00adserve types [12]. There are also conversions between the programs in SSA \nform and functional programs [2, 8]. Conclusion We have presented a proof technique for formally verifying \nSSA-based compiler optimizations. Using the Coq proof assistant, we fully mechanized the proof technique \nand the correct\u00adness of several micro optimizations. For the full LLVM IR seman\u00adtics in Vellvm, we have \nformalized and implemented an extractable SSA optimization pass vmem2reg that is an easier-to-prove vari\u00adant \nof LLVM s mem2reg pass but that delivers most of its bene.ts. Acknowledgments We would like to thank \nDelphine Demange, Vivien Durey, and Dmitri Garbuzov for their comments and sug\u00adgestions about this work. \n References [1] Static Single Assignment Book, 2012. Working draft available at http://ssabook.gforge.inria.fr/latest/book.pdf. \n[2] A. W. Appel. SSA is functional programming. SIGPLAN Not., 33(4): 17 20, April 1998. ISSN 0362-1340. \n[3] J. Aycock and N. Horspool. Simple generation of static single assign\u00adment form. In CC, 2000. [4] \nG. Barthe, D. Demange, and D. Pichardie. A formally veri.ed SSA\u00adbased middle-end -Static Single Assignment \nmeets CompCert. In ESOP, 2012. [5] J. O. Blech, S. Glesner, J. Leitner, and S. M \u00a8ulling. Optimizing \ncode generation from SSA form: A comparison between two formal cor\u00adrectness proofs in Isabelle/HOL. Electron. \nNotes Theor. Comput. Sci., 141(2):33 51, 2005. [6] The Coq Proof Assistant Reference Manual (Version \n8.3pl1). The Coq Development Team, 2011. [7] R. Cytron, J. Ferrante, B. K. Rosen, M. N. Wegman, and F. \nK. Zadeck. Ef.ciently computing static single assignment form and the control dependence graph. TOPLAS, \n13:451 490, 1991. [8] R. A. Kelsey. A correspondence between continuation passing style and static single \nassignment form. In IR, number 3, 1995. [9] X. Leroy. A formally veri.ed compiler back-end. Journal of \nAuto\u00admated Reasoning, 43(4):363 446, December 2009. ISSN 0168-7433. [10] The LLVM Reference Manual (Version \n3.0). The LLVM Develop\u00adment Team, 2011. http://llvm.org/releases/3.0/docs/ LangRef.html. [11] W. Mansky \nand E. L. Gunter. A framework for formal veri.cation of compiler optimizations. In ITP, 2010. [12] Y. \nMatsuno and A. Ohori. A type system equivalent to static single assignment. In PPDP, 2006. [13] V. S. \nMenon, N. Glew, B. R. Murphy, A. McCreight, T. Shpeisman, A. Adl-Tabatabai, and L. Petersen. A veri.able \nSSA program repre\u00adsentation for aggressive compiler optimization. In POPL, 2006. [14] S. S. Muchnick. \nAdvanced compiler design and implementation. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, \n1997. ISBN 1-55860-320-4. [15] V. C. Sreedhar and G. R. Gao. A linear time algorithm for placing f-nodes. \nIn POPL, 1995. [16] J.-B. Tristan and X. Leroy. Formal veri.cation of translation valida\u00adtors: a case \nstudy on instruction scheduling optimizations. In POPL, 2008. [17] J.-B. Tristan and X. Leroy. Veri.ed \nvalidation of lazy code motion. In PLDI, 2009. [18] J. B. Tristan and X. Leroy. A simple, veri.ed validator \nfor software pipelining. In POPL, 2010. \u00b4 Static Single Assignment. Rapport de dea (Master s thesis), \nENS Cachan and INRIA Rocquencourt, Sept. 2004. [19] B. Yakobowski. Etude s \u00b4emantique d un langage interm \n\u00b4ediaire de type [20] X. Yang, Y. Chen, E. Eide, and J. Regehr. Finding and understanding bugs in C compilers. \nIn PLDI, 2011. [21] J. Zhao and S. Zdancewic. Mechanized veri.cation of computing dominators for formalizing \ncompilers. In CPP, 2012. [22] J. Zhao, S. Nagarakatte, M. M. K. Martin, and S. Zdancewic. For\u00admalizing \nthe LLVM intermediate representation for veri.ed program transformations. In POPL, 2012. This research \nwas funded in part by the U.S. Government. The views and conclusions contained in this document are those \nof the authors and should not be interpreted as representing the of.cial policies, ei\u00adther expressed \nor implied, of the U.S. Government. This research was funded in part by DARPA contract HR0011-10-9-0008 \nand ONR award N000141110596. This material is based upon work supported by the Na\u00adtional Science Foundation \nunder Grant No. CNS-1116682, CCF-1065166, and CCF-0810947. Any opinions, .ndings, and conclusions or \nrecommen\u00addations expressed in this material are those of the author(s) and do not necessarily re.ect \nthe views of the National Science Foundation.  \n\t\t\t", "proc_id": "2491956", "abstract": "<p>Modern compilers, such as LLVM and GCC, use a <i>static single assignment</i>(SSA) intermediate representation (IR) to simplify and enable many advanced optimizations. However, formally verifying the correctness of SSA-based optimizations is challenging because SSA properties depend on a function's entire control-flow graph.</p> <p>This paper addresses this challenge by developing a proof technique for proving SSA-based program invariants and compiler optimizations. We use this technique in the Coq proof assistant to create mechanized correctness proofs of several \"micro\" transformations that form the building blocks for larger SSA optimizations. To demonstrate the utility of this approach, we formally verify a variant of LLVM's mem2reg transformation in Vellvm, a Coq-based formal semantics of the LLVM IR. The extracted implementation generates code with performance comparable to that of LLVM's unverified implementation.</p>", "authors": [{"name": "Jianzhou Zhao", "author_profile_id": "81435599390", "affiliation": "University of Pennsylvania, Philadelphia, Pennsylvania, USA", "person_id": "P4148986", "email_address": "jianzhou@cis.upenn.edu", "orcid_id": ""}, {"name": "Santosh Nagarakatte", "author_profile_id": "81435608524", "affiliation": "Rutgers University, New Brunswick, New Jersey, USA", "person_id": "P4148987", "email_address": "santosh.nagarakatte@cs.rutgers.edu", "orcid_id": ""}, {"name": "Milo M.K. Martin", "author_profile_id": "81100426086", "affiliation": "University of Pennsylvania, Philadelphia, Pennsylvania, USA", "person_id": "P4148988", "email_address": "milom@cis.upenn.edu", "orcid_id": ""}, {"name": "Steve Zdancewic", "author_profile_id": "81384616728", "affiliation": "University of Pennsylvania, Philadelphia, Pennsylvania, USA", "person_id": "P4148989", "email_address": "stevez@cis.upenn.edu", "orcid_id": ""}], "doi_number": "10.1145/2491956.2462164", "year": "2013", "article_id": "2462164", "conference": "PLDI", "title": "Formal verification of SSA-based optimizations for LLVM", "url": "http://dl.acm.org/citation.cfm?id=2462164"}