{"article_publication_date": "06-16-2013", "fulltext": "\n Steal Tree: Low-Overhead Tracing of Work Stealing Schedulers Jonathan Lif.ander Sriram Krishnamoorthy \nLaxmikant V. Kale Dept. of Computer Science Comp. Sci. &#38; Math. Division Dept. of Computer Science \nUniversity of Illinois Urbana-Champaign Paci.c Northwest National Lab University of Illinois Urbana-Champaign \njli.2@illinois.edu sriram@pnnl.gov kale@illinois.edu Abstract Work stealing is a popular approach to \nscheduling task-parallel pro\u00adgrams. The .exibility inherent in work stealing when dealing with load imbalance \nresults in seemingly irregular computation struc\u00adtures, complicating the study of its runtime behavior. \nIn this paper, we present an approach to ef.ciently trace async-.nish parallel pro\u00adgrams scheduled using \nwork stealing. We identify key properties that allow us to trace the execution of tasks with low time \nand space overheads. We also study the usefulness of the proposed schemes in supporting algorithms for \ndata-race detection and retentive steal\u00ading presented in the literature. We demonstrate that the perturbation \ndue to tracing is within the variation in the execution time with 99% con.dence and the traces are concise, \namounting to a few tens of kilobytes per thread in most cases. We also demonstrate that the traces enable \nsigni.cant reductions in the cost of detecting data races and result in low, stable space overheads in \nsupporting re\u00adtentive stealing for async-.nish programs. Categories and Subject Descriptors D.2.5 [Software]: \nSoftware Engineering Testing and Debugging; D.3.3 [Software]: Pro\u00adgramming Languages Language Constructs \nand Features Keywords work-stealing schedulers; tracing; async-.nish paral\u00adlelism 1. Introduction The \nincrease in number of processor cores anticipated in both com\u00admodity and high-end systems has motivated \nthe study of dynamic task parallelism. The structured parallel programming idioms sim\u00adplify performance-portable \nprogramming and tackle problems re\u00adlated to frequent synchronization on large-scale systems. Work stealing \nis a well-studied dynamic load balancing strategy with several useful characteristics composability, \nunderstandable space and time bounds, provably ef.cient scheduling, etc. [5]. Sev\u00aderal programming models \nsupport dynamic task parallelism us\u00ading work stealing, including OpenMP 3.0 [3], Java Concurrency Utilities \n[11], Intel Thread Building Blocks [14], Cilk [5, 6], and X10 [15]. While several properties have been \nproven about work stealing schedulers, their dynamic behavior remains hard to ana\u00adlyze. In particular, \nthe .exibility exhibited by work stealing in re\u00adsponding to load imbalances leads to less structured \nmapping of work to threads, complicating subsequent analysis. Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. PLDI 13, June 16 19, 2013, Seattle, WA, USA. \nCopyright c &#38;#169; 2013 ACM 978-1-4503-2014-6/13/06. . . $10.00 In this paper, we focus on studying \nwork stealing schedulers that operate on programs using async and finish statements the fundamental concurrency \nconstructs in modern parallel languages such as X10 [15]. In particular, we derive algorithms to trace \nwork stealing schedulers operating on async-.nish programs. Tracing captures the order of events of interest \nand is an effec\u00adtive approach to studying runtime behavior, enabling both online characterization and \nof.ine analysis. While useful, the size of a trace imposes a limit on what can be feasibly analyzed, \nand per\u00adturbation of the application s execution can make it impractical at scale. Tracing individual \ntasks in an async-.nish program is a pro\u00adhibitive challenge due to the .ne granularity and sheer number \nof individual tasks. Such programs often expose far more concurrency than the number of computing threads \nto maximize scheduling .ex\u00adibility. In this paper, we derive algorithms to ef.ciently trace the exe\u00adcution \nof async-.nish programs. Rather than trace individual tasks, we exploit the structure of work stealing \nschedulers to coarsen the events traced. In particular, we construct a steal tree: a tree of steal operations \nthat partitions the program execution into groups of tasks. We identify the key properties of two scheduling \npolicies help-.rst and work-.rst [9] that enables the steal tree to be com\u00adpactly represented. In addition \nto presenting algorithms to trace and replay async\u00ad.nish programs scheduled using work stealing, we demonstrate \nthe usefulness of the proposed algorithms in two distinct contexts optimizing data race detection for \nstructured parallel programs [13] and supporting retentive stealing without requiring explicit enumer\u00adation \nof tasks [12]. The following are the primary contributions of this paper: identi.cation of key properties \nof work-.rst and help-.rst schedulers operating on async-.nish programs to compactly represent the stealing \nrelationships;  algorithms that exploit these properties to trace and replay async-.nish programs by \nef.ciently constructing the steal tree;  demonstration of low space overheads and within-variance per\u00adturbation \nof execution in tracing work stealing schedulers;  reduction in the cost of data race detection using \nan algorithm that maintains and traverses the dynamic program structure tree [13]; and  retentive stealing \nalgorithms for recursive parallel programs, while prior work required explicitly enumerated task collec\u00adtions \n[12].  2. Background and Related Work 2.1 Async-Finish Parallelism An async statement identi.es the \nassociated statement as a task, the basis unit of concurrent execution. A task identi.ed by the async \nstatement can be executed in parallel with the enclosing task, referred to as the parent task. A finish \nstatement identi.es the bounds of concurrency. All computation enclosed by a finish statement, including \nnested concurrent tasks, are required to com\u00adplete before any statement subsequent to the .nish can be \nexecuted.  This async-.nish parallelism model, supported in X10, enables both fully strict and terminally \nstrict computations. In fully strict computations, exempli.ed by Cilk, a finish statement implicitly \nencloses all execution in a task, requiring all tasks transitively nested within a task to complete before \nit returns. The async-.nish model extends the fully strict model by supporting escaping asyncs, allowing \na task to return before its nested concurrent tasks need to complete.  2.2 Work Stealing Schedulers \nWe shall assume that the computation begins with a single task and terminates when the task and its descendents \ncomplete execution. Concurrent tasks can be executed in parallel by distinct threads or processes. Each \nthread maintains a local deque of tasks and alternates between two phases until termination. In the working \nphase, each thread executes tasks from its local deque. When no more local work is available, a thread \nenters the stealing phase to steal from a victim s deque. A stealing thread (a thief) attempts to steal \na task until work is found or termination is detected. The thief pushes the stolen task onto its local \ndeque and enters a new working phase. Each thread pushes and pops tasks from the local end of its deque, \nwhile the thief steals from the other end (steal end) of the victim s deque. We consider two scheduling \npolicies (outlined in Figure 1) for async-.nish programs identi.ed by Guo et al. [9]. In the work\u00ad.rst \npolicy, a thread, upon encountering an async or a finish statement, pushes the currently executing task \nonto the deque and begins to execute the nested task identi.ed. A thief can steal the partially-executed \ntask (a continuation) once it is pushed onto the deque. In the absence of steal operations, this policy \nmirrors the sequential execution order and has been shown to exhibit ef.cient space and time bounds. \nIn the help-.rst policy, the working thread continues to execute the current task, pushing any encountered \nconcurrent tasks onto the deque. Encountering a finish statement, the current task s continuation is \npushed onto the deque to complete processing of the tasks nested within the .nish scope. Finish scopes \nconstrain the help-.rst scheduler, requiring the tasks in the .nish scope to be processed before tasks \nsequentially following the .nish scope can be spawned. This scheduling policy was shown to speedup work \npropagation by Guo et al. [9].  2.3 Tracing and Performance Analysis Several papers have noted for work-.rst \nschedulers that steals can be recorded for the purposes of maintaining series-parallel relation\u00adships \nusing the SP-hybrid algorithm for data-race detection [4, 10] and for optimizing transactional memory \ncon.ict detection [2]. However, these do not provide a general tracing algorithm and re\u00adquire a global \nlock to store threads of work. We provide a gen\u00aderal tracing algorithm that does not require global synchronization \nor locking. We are not aware of any prior work on tracing the more complex help-.rst scheduling policy. \nThe limitations of the prior work in tracing work stealing schedulers is evidenced by the fact that some \nof the most recent work on data-race detection for async\u00ad.nish programs [13] does not exploit the steal \nrelationship a ben\u00ade.cial approach that we demonstrate later in this paper. Work stealing has typically \nbeen studied empirically (e.g., [1, 12]). Tallent and Mellor-Crummey [16] presented blame shifting to \nrelate lack of parallel slack in work stealing to code segments. @async(Task t, Cont this): deque.push(t); \n@.nish(Task t, Cont this): deque.push(this); process(t); @taskCompletion: t = deque.pop(); if(t) process(t); \n//else this phase ends @steal(Cont c, int victim): c=attemptSteal(victim); (b) Help-.rst scheduler @async(Task \nt, Cont this): deque.push(this); process(t); @.nish(Task t, Cont this): deque.push(this); process(t); \n@taskCompletion: //same as help .rst minus //some .nish scope mgmt @steal(int victim): //same as help-.rst \n(a) Work-.rst scheduler Figure 1. Basic actions in the two scheduling policies They assume global information \nabout the number of active and idle workers. 3. Notation We de.ne a continuation step (or simply step) \nto be the dynamic sequence of instructions with no interleaving async, finish, at, or when statements. \nEach continuation step will be executed by exactly one thread and cannot be migrated between threads \nduring execution. A continuation is the portion of execution, in terms of continua\u00adtion steps, reachable \nfrom a given continuation step. In other words, a continuation represents the remainder of the execution \nthat begins with the given continuation step. A task is a continuation marked by an async or finish statement, \nrepresenting all steps executed in the task. In shared-memory environments, all continuations of a task \noccupy the same storage and transform the state from one step to the next. The term continuation is used \nin places where a task is used, except when we intend to speci.cally distinguish a task from a continuation. \nA partial continuation is a continuation that repre\u00adsents a proper subset of the computation represented \nby a task. A task is said to spawn another task when the task s execution encounters an async statement. \nWithout changing program seman\u00adtics, we treat a finish statement synonymously with a finish async. Thus \na finish statement leads to the spawning of a task as well. All child tasks spawned from a given task \nare referred to as siblings and are ordered from left to right. We also refer to a task s left and right \nsiblings, if they exist. Each task is associated with a level. The initial task in a working phase is \nat level 0. A level of a spawned task is one greater than that of the spawning task. 4. Problem Statement \nAn example async-.nish program is shown in Figure 2a. In the .gure, s1, s2, etc. are continuation steps. \nTasks spawned using async statements need to be processed before the execution can proceed past the immediately \nenclosing finish statement. async statements that are not ordered by a finish statement (e.g., the async \nstatements enclosing s5 and s9) can be executed in any order. The objective is to compactly track the \nexecution of each continuation step. The execution in each worker is grouped into phases with each phase \nexecuting continuation steps in a well-de.ned order, starting from a single continuation. In each working \nphase, the computation begins with a single continuation step and involves the execution of all steps \nreached from the initial step minus the continuations that were stolen. The tracing overheads (space \nand time) can be  fn() {s1; async { s5; async x; s6; }s2; async { s7; async { s9; async x; s10; async \nx; s11; .. } s8;.. } s3; async { s12; finish {s13;..}..} s4; } (a) async-finish program (b) Help-.rst \n(c) Work-.rst Figure 2. An example async-.nish parallel program and a snapshot of its execution. Legend: \nT represents the root task; . represents a sequential task; a circle represents a step; a diamond represents \nan async statement; a hexagon represents a finish statement; .. represents a continuation; an arrow represents \na spawn relationship; a rectangular box represents a task; and the shaded region represents the region \nthat is stolen. The deque depicted at the bottom has a steal end that is accessed by thieves and a local \nend that is accessed by the worker thread. signi.cantly reduced if the steps executed in each working \nphase can be compactly represented. Note the difference in the stealing structure between the work\u00ad.rst \nand help-.rst schedulers in Figure 2. While continuations stolen in the work-.rst schedule seem to follow \nthe same structure across all the levels shown, the help-.rst schedule can produce more complicated steal \nrelationships. This distinction is the result of the difference in the scheduling actions of the two \npolicies especially when an async statement encountered, as shown in Figure 1. The challenge is to identify \nthe key properties of help\u00ad.rst and work-.rst scheduling policies to compactly identify the the leafs \nof the tree of steps rooted at the initial step in each working phase. 5. Tracing Help-First Schedulers \nUnder help-.rst scheduling, spawned tasks are pushed onto the deque, while the current task is executed \nuntil the end or a .nish scope is reached. Children of task spawned by async statements are in the same \n.nish scope as the parent. Encountering a finish statement, the current task s continuation is enqueued \nonto the deque and the spawned task in the new .nish scope is immediately executed. When all the tasks \nnested within the finish statement have been processed, the .nish scope is exited and the execution of \nthe parent task is continued, possibly spawning additional tasks. We refer the reader to Guo et al. [9] \nfor the detailed algorithm. Figure 2b shows a snapshot of the help-.rst scheduling of the program in \nFigure 2a. The steps in the outermost task, represented by fn() s1, s2, s3, and s4 are processed before \nany other steps. OB S ERVATI O N 5.1. Two tasks are in the same immediately enclos\u00ading .nish scope if \nthe closest .nish scope that encloses each of them also encloses their common ancestor. LEMM A 5.2. A \ntask at a level is processed only after all its younger siblings in the same immediately enclosing .nish \nscope are pro\u00adcessed. Proof. A work-.rst scheduler enqueues the spawned tasks from the start of the task \ns lexical scope. The spawned tasks are en\u00adqueued onto the deque with the newer child tasks enqueued closer \nto the local-end than the older ones. The child tasks are enqueued until the executing task completes \nor a finish statement is en\u00adcountered. Tasks under the encountered finish statement are im\u00admediately \nprocessed and the execution continues with enqueuing tasks spawned using the async statement. This property, \ncombined with the fact that tasks popped from the local end are immediately processed, requires all younger \nsiblings of a task to be processed before it can be processed. D LEM M A 5.3. A task is stolen at a level \nonly after all its older siblings in the same immediately enclosing .nish scope are stolen. Proof. Recall \nthat all async statements in a task are in the same immediately enclosing .nish scope, and finish statements \nintro\u00adduce new .nish scopes. 1. Consider the case when two async statements in a task have no intervening \nfinish statement. In this case, the corresponding tasks are pushed onto the deque with the older sibling \n.rst. Thus the older sibling is stolen before the younger sibling. 2. Now consider the case where siblings \nin the same immediate .nish scope are separated by one of more intervening finish statements. In particular, \nconsider the execution of the follow\u00ading sequence of statements in a task: async n; ... .nish p; ... \nasync m. Tasks n and m are in the same immediately enclos\u00ading .nish scope, while task p is not. Task \nn is .rst enqueued onto the deque. When the finish statement is processed, the current task s continuation \nc is then pushed onto the deque with the worker immediately processing statement p. At this point, if \na steal occurs, n and c are at the steal end of the deque fol\u00adlowed by tasks spawned from p. Hence, all \nolder siblings (n and c) in the same immediately enclosing .nish scope will be stolen before tasks generated \nfrom p. The execution proceeds past the finish statement only after all the tasks nested by p are complete. \nOnce they are executed, the continuation of the current task c is dequeued (unless it was stolen) and \ntask m is enqueued on top of n in the deque. The execution now devolves onto case 1 and the older sibling \nis stolen before the younger.  D  LEMM A 5.4. At most one partial continuation is stolen at any level \nand it belongs to the last executed task at that level. Proof. A task s partial continuation is enqueued \nonly when it encounters a finish statement. Consider such a task. When the task is being processed, its \nparent s continuation is in the deque, or it has completed processing. Based on lemma 5.2, all the parent \ns younger siblings have been processed. Hence, the queue has the parent s older siblings followed by \na possible partial continuation of the parent followed by this task s older siblings, then this task. \nBy lemma 5.3, this task s partial continuation will not be stolen until all the parent s older siblings \nare stolen. By lemma 5.2 all younger siblings of this task, if any, have been processed at this point. \nThus the deque only has the partial continuation s children, all of which are at levels higher than this \ntask. After this task s partial continuation is stolen, all subsequent tasks will be at higher levels, \nmaking this task the last one at this level. D LEMM A 5.5. All tasks and continuations stolen at a level \nl are immediate children of the last task processed at level l - 1. Proof. We .rst prove by contradiction \nthat the all tasks stolen at a given level are children of the same parent task. Let two stolen tasks \nat a given level be children of distinct parent tasks. Let ta be the lowest common ancestor of of these \ntasks, and t1 and t2 be the immediate children of ta that are ancestors of the two tasks of interest. \nt1 and t2 are thus sibling tasks. Without loss of generality, let t1 be the older sibling. By lemma 5.2, \nt2 is processed before t1. By lemma 5.3, t1 is stolen before any descendent of t2 can be stolen. Thus \nno descendent of t1 can be enqueued if a descendent of t2 is stolen, resulting in a contradiction: either \nthe steals will be at different levels because descendents of t1 cannot be enqueued, or they will be \nchildren of the same parent task. We now prove that the parent task q of all tasks stolen at level l \nis the last task processed at level l - 1. Let t be any task at level l - 1 that is not q. By lemma 5.2, \ntask t must be an older sibling of the parent task q. From lemma 5.3, any task t must be stolen before \nq. By the above proof, any task with a higher level y must be a child of the same parent task. We now \nshow by contradiction that y must be a child of q, the last task processed at level l - 1. If y is a \nchild of some task t, then t is currently being processed. By lemma 5.2, q is processed before t. q has \nnot been processed, hence we have a contradiction. D LEMM A 5.6. The parent q of the tasks stolen at \nlevel l + 1 is a sibling of the tasks stolen at level l. Proof. We prove this by contradiction. By lemma \n5.5, q is last task processed on level l. By lemma 5.2, all younger tasks on level l have been processed. \nIf q is not a sibling of the tasks stolen at level l, q could not have been processed. Thus the stolen \ntasks at level l + 1 could not have been created, resulting in a contradiction. D LEMM A 5.7. The parent \nof the tasks stolen at level l + 1 is either the immediate younger sibling of the last stolen task at \nlevel l, or the immediate younger sibling of the last stolen task at level l in the same immediately \nenclosing .nish scope. Proof. From lemmas 5.4, 5.5, and 5.6, the last task in the same immediately enclosing \n.nish scope that is executed is the closest younger sibling of stolen task that is in the same enclosing \n.nish scope. Thus the last task executed at that level is either the imme\u00addiate right sibling of the \nlast stolen task, say t1, or the closest right sibling in the immediate enclosing .nish scope, say t2. \nWhen both are identical, the outcome is clear. When they are distinct, the im\u00admediate younger sibling \nof the last stolen task is in a distinct .nish scope. If the finish statement is stolen, no further tasks \ncan be processed that are not descendents of this statement, making t1 the last executed task. If the \nfinish statement is not stolen, no de\u00adscendent tasks of this statement can be stolen, making t2 the last \nexecuted task. D TH E O R E M 5.8. The tasks executed and steal operations encoun\u00adtered in each working \nphase can be fully described by (a) the level of the root in the total ordering of the steal operations \non the vic\u00adtim s working phase, and (b) the number of tasks and step of the continuation stolen at each \nlevel. Proof. The tasks stolen at a level can be determined from the number of tasks stolen at that level \nand the identi.cation of these tasks parent (by lemma 5.7) (transitively until level 0 which has just \none task). The position of the partial continuation stolen at a level can be determined from the fact \nit is the last processed task at a given level (lemma 5.4) and from the number of tasks stolen at that \nlevel in the same .nish scope as the parent. Together with the step information tracking, this uniquely \nidenti.es all stolen continuations. D Illustration. A snapshot of execution under the help-.rst work \nstealing policy is shown in Figure 2b. Steps s1, s2, s3, s4, and s11 have been executed. Because s13 \nis encountered in a .nish scope, s12 spawns the task starting with step s13 and continues recursive execution \nbefore continuing the execution past the .nish scope. Meanwhile, the deque consists of tasks s5, s7, \nand the continuation past the .nish scope, represented by f, and were stolen. Note that the help-.rst \nscheduler steals from left-to-right when stealing full tasks, and right-to-left (similar to a work-.rst \nscheduler) when stealing a partial continuation. The subtrees executed in each working phase form a steal \ntree. The root of the tree is the subtree that includes the main continua\u00adtion that began the program. \nEach child is a subtree stolen from the victim s subtree. Each node in the steal tree contains information \nabout the continuations stolen from it, bounding the actual steps executed in that subtree. Each edge \nin the steal tree contains infor\u00admation about the position of the steal from the parent subtree. 6. Tracing \nWork-First Schedulers Under work-.rst scheduling, spawning a task involves pushing the currently executing \nstep s successor onto the deque, with the execution continuing with the .rst step in the spawned task. \nThe continuation that starts the working phase is at level 0, re\u00adferred to as the root continuation. \nTasks spawned by continuations at level l are at level l+1. The work-.rst scheduling policy results in \nexactly one continuation at levels 0, . . . , l - 1, where l is the num\u00adber of continuations in the deque. \nWe observe the tasks spawned during a working phase and prove that there is at most one steal per level, \nand a steal at all levels 0, . . . , l - 1 before a task can be stolen at level l. This allows us to \nrepresent all the steals for a given working phase as a contiguous vector of integers that identify the \ncontinuation step stolen at each level, starting at level 0. OB S E RVAT I ON 6.1. The deque, with l \ntasks in it, consists of one continuation at levels 0 through l - 1 with 0 at the steal-end and the continuation \nat level i spawned by the step that precedes the continuation at level i - 1. The execution of the work-.rst \nscheduler mirrors the sequential execution. The task executing at level l is pushed onto the deque before \nspawning a task at level l + 1. Thus the deque corresponds to one path from the initial step to the currently \nexecuting task in terms of the spawner-spawnee relationship.  LEMM A 6.2. When a continuation is stolen \nat level l (a) at least one task has been stolen at each level 0 through l - 1, (b) no additional tasks \nare created at level l. Proof. The .rst part follows from observation 6.1 and the structure of the deque \nbecause stealing starts from the steal-end tasks at levels 0 through l - 1 must stolen before level l. \nWe prove the second part by induction. Consider the base case when the root of the subtree is the only \ncontinuation at level 0 in the deque. After the root of the subtree is stolen, no continuation exists \nat level 0 to create another task at level 1. Let the lemma be true for all levels 0, . . . , l. When \na continuation at level l is stolen, no further tasks can be created at level l. Now consider the lemma \nfor level l+ 1. After a steal at level l, no further tasks can be created at level l + 1. Once the current \ncontinuation at level l + 1 is stolen, no subsequent tasks are created at level l + 1. D TH EO R E M \n6.3. The tasks executed and steal operations encoun\u00adtered in each working phase can be fully described \nby (a) the level of the root in the total ordering of the steal operations on the vic\u00adtim s working phase, \nand (b) the step of the continuation stolen at each level. Proof. By lemma 6.2, steal operations on a \nvictim are totally ordered, implying that each task stolen from a victim during distinct working phase \nis at a unique level. Because no additional tasks can be created at that level (again by lemma 6.2), \nthe step of the continuation stolen at that level is suf.cient to uniquely identify it. D These observations \nallow the steal points to be maintained in a contiguous array. The size of the array corresponds to the \nnumber of steals in this working phase, and the value at position l in the array corresponds to the index \nof the continuation stolen at level l. The stolen continuations can be identi.ed to absolute or relative \nindices. Absolute indices counting the number of steps executed in this phase at each level does not \neffectively support retentive stealing, as explained later. We employ relative indices, with the value \nat index l corresponding to the number of steps executed in the last task executed at this level. Note \nthat the last task executed at level l is a child of a predecessor step of the continuation stolen at \nlevel l - 1. Given there is only one task at level 0, we store the number of steps by this worker for \nthis initial step. Illustration. A snapshot of execution under the work stealing scheduler is shown in \nFigure 2c. The thread began execution from step s1 and has completed execution of s1, s2, s5, s6, s7, \ns9, and s10. It is currently executing the task spawned by s10 with the the deque consisting of steps \ns3, s8, and s11 bottom to top. These steps have been created but not yet processed. During some point \nin the execution, these tasks have been stolen by thieves. The steal order is s3, s8, followed by s11. \nNote that the execution in the work-.rst scheduling policy is left-to-right, while the steals are right-to-left. \n7. Tracing and Replay Algorithms We now present the algorithms for tracing and replay based on the properties \nidenti.ed for help-.rst and work-.rst schedulers. in Sections 5 and 6. These properties simplify the \nstate to be managed to trace and replay both schedulers. The algorithms rely on some state in addition \nto the basic scheduler actions. This shared state is shown as C++-style pseudo-code in Figure 3. Every \ncontinuation and task has an associated ContinuationHdr that stores the level and step. When a new working \nphase is started or an async or finish is encountered, the bookkeeping keeps track of the current level \nfor each continuation. Each working phase that is executed by a worker maintains the state shown in Working-PhaseInfo \nas part of the trace. A worker s trace is an ordered list of working phases it has executed so far, retained \nas one WorkerState-Hdr object per worker. The root task of the entire computation is not stolen and its \nvictim is set to -1. Each continuation s step is tracked as an integer and is updated on each async or \nfinish statement. 7.1 Tracing The tracing algorithms augment the steal operation to track the steal relationship \nand construct the steal tree. From the earlier discussion, we know that, unlike work-.rst schedulers, \nhelp-.rst schedulers allow multiple tasks to be stolen at each level. Thus for tracing a help-.rst scheduler, \nwe store the number of tasks stolen at each level: struct HelpFirstInfo : WorkingPhaseInfo {vector<int> \nnTasksStolen; //num. tasks stolen at each level, init to 0 }; When a continuation is stolen under help-.rst \nscheduling, the thief marks the steal in the victim s HelpFirstInfo. The HelpFirstInfo for the current \nworking phase on the victim can only be accessed by a single thief, and hence requires no synchronization. \nmyrank is the thief thread s rank. Note that just the number of stolen tasks at each level and the partial \ncontinuation s step is suf.cient to reconstruct all information about the tasks executed in a given working \nphase. @steal(Cont c, int victim, int myrank): // c is the continuation stolen by the thief if c.step \n== 0: // this is a full task wsh[victim].wpi.back().nTasksStolen[c.level] += 1; else: //this is a partial \ncontinuation wsh[victim].wpi.back().stepStolen[c.level] = c.step; wsh[victim].wpi.back().thieves.push \nback(myrank); WorkingPhaseInfo phase; phase.victim = victim; wsh[myrank].wpi.push back(phase); When a \ncontinuation is stolen under work-.rst scheduling, the following marks the steal in the victim s WorkingPhaseInfo. \nFor the work-.rst policy, the actions required are less complex because at most one task can be stolen \nper level. @steal(Cont c, int victim, int myrank): wsh[victim].wpi.back().stepStolen[c.level] = c.step; \nwsh[victim].wpi.back().thieves.push back(myrank); WorkingPhaseInfo phase; phase.victim = victim; wsh[myrank].wpi.push \nback(phase); Note that none of these actions require additional synchroniza\u00adtions, and all the tracing \noverhead incurred is on the steal path.  7.2 Replay The collected traces include timing information, \nwhich allows the traces to be replayed. During replay, each thread executes the working phases assigned \nto it in order. Whenever a stolen task is spawned, rather than adding it to the deque, the corresponding \nthief is informed of its creation. Each thread executes its set of subtrees at the same time point as \nin the original run, after ensuring that the given subtree s root task has been spawned by the corresponding \nvictim. The creation of the initial task in working phase indicates that all dependences before a task \nin that working phase have been satis.ed. During replay, each task tracks whether its children could \nhave been stolen in the trace, i.e., the task is at the frontier, using the following additional .eld \nin the ContinuationHdr:  struct ContinuationHdr {int level; //this task s async level int step; //this \ncontinuation s step }; struct Task : ContinuationHdr { ... }; struct Cont : ContinuationHdr { ... }; \nstruct WorkingPhaseInfo { int victim; // victim stolen from vector<int> stepStolen; //step stolen at \neach level, init to -1 vector<int> thieves; // list of thieves }; struct WorkerStateHdr { //state for \neach working phase vector<WorkingPhaseInfo> wpi; }; WorkerStateHdr wsh[NWORKERS]; //one per worker //initializing \ncomputation s .rst task @init(Task initialTask): initialTask.victim=-1;//victim set to -1 //start of \nworking phase with continuation c @startWorkingPhase(Cont c): c.level = 0; // level of starting frame \n//spawning task t when executing task this @async(Task t, Cont this): t.level=this.level+1; t.step=0; \nthis.step+=1; //spawning task t in new .nish scope when executing task this @.nish(Task t, Cont this): \nt.level = this.level + 1; this.step += 1; Figure 3. Common data structures and level management for \nall algorithms struct ReplayContinuationHeader : ContinuationHdr {bool atFrontier; // could any of its \nchildren have been stolen, initially false }; When a worker encounters a task that was stolen, it marks \nit as stolen and noti.es (in shared-memory) or sends (in distributed memory) the task to the thief. When \na thread starts the execution of a working phase, it waits for the initial task to be created by the \nvictim. The worker with the initial task for the entire computation begins execution, identifying the \ninitial task as being at the frontier. markStolen(Task t): // enqueue t for sending or directly send \nto the next thief in current working phase info s thieves // drop t from execution on this worker @startWorkingPhase(WorkingPhaseInfo \nwpi): // get initial task from wpi.victim, if wpi.victim>=0 @init(Task initialTask, int myrank): if(wsh[myrank].wpi[0].victim \n== -1): initialTask.atFrontier = true; The above actions are used to replay traces from both help-.rst \nand work-.rst schedulers. When help-.rst traces are replayed, the number of child tasks spawned by each \ntask in the same .nish scope is tracked by aug\u00admenting the following to the HelpFirstInfo structure. \nstruct HelpFirstReplayInfo : HelpFirstInfo {vector<int> childCount; //num children for current executing \ntask }; // at the beginning of execution of a task @taskEntry(Task this, int myrank): wsh[myrank].childCount[this.level \n+ 1] = 0; A task spawned by an async statement is marked as being at the frontier if it is the immediate \nyounger sibling of the last child task stolen from this task. When the executing task is at the frontier \nand the child count is less than the number of tasks stolen at the next level, the spawned task is marked \nas stolen. A task spawned by a finish statement can mark the executing task as stolen as well. If the \npartial continuation of the finish statement is not stolen, none of its descendents is stolen either. \n@async(Task t, Continuation this, int myrank, WorkingPhaseInfo current wpi): t.level = this.level + 1; \nt.step=0; this.step+=1; if this.atFrontier: if wsh[myrank].childCount[t.level] < current wpi. stealCount[t.level]: \nmarkStolen(t); else if wsh[myrank].childCount[t.level] == current wpi. stealCount[t.level]: t.atFrontier \n= true; wsh[myrank].childCount[t.level] += 1; @.nish(Continuation t, Continuation this, int myrank, WorkingPhaseInfo \ncurrent wpi): // t does not contribute to calculation of childCount t.level = this.level+1; if this.atFrontier: \nif this.step == wsh[myrank].stepStolen[this.level]: markStolen(this); // continuation of this after spawning \nthe .nish t.atFrontier = true; // only child of stolen parent is -- also at frontier if wsh[myrank].childCount[t.level] \n< current wpi. stealCount[t.level]: assert(wsh[myrank].childCount[t.level] == current wpi.stealCount[t.level]-1); \nmarkStolen(t); When replaying work-.rst traces, the primary action is deter\u00admining whether a task is \nat a frontier. When an async or finish statement is encountered, the following actions are executed: \n@async(Task t, Cont this, int myrank): t.level = this.level+1; t.step=0; this.step += 1; if this.atFrontier: \nif this.step == wsh[myrank].stepStolen[this.level]: markStolen(this); t.atFrontier = true; @.nish(Task \nt, Cont this, int myrank): //same action as for async(t, this, myrank)  7.3 Space Utilization The space \noverhead can be quickly computed from the data struc\u00adtures employed in the algorithms. In the following \nformul\u00e6, bh and bw describe the total number of bytes required to trace help-.rst and work-.rst schedulers, \nrespectively: n bh = v(1 + si) + si(m + k) (Total bytes for help-.rst) i=0 n  bw = v(1 + si) + sim \n(Total bytes for work-.rst) i=0 where n is the total number of working phases, v is the number of bytes \nrequired for a thread identi.er, si is the number of steals in a working phase, m is the number of bytes \nrequired for a step identi.er, and k is number of bytes required to store the maximum number of tasks \nat a given level.  For Figures 5 and 7 that graph the storage required, we use integers to store the \nthread and step identi.ers, and assume that the maximum number of tasks spawned at a given level does \nnot exceed the size of an integer: k = m = v = sizeof(int) = 4 bytes. 8. Applications 8.1 Data-race Detection \nfor Async-Finish Programs Raman et al. [13] perform data race detection by building a dy\u00adnamic program \nstructure tree (DPST) at runtime that captures the relationships between async and finish instances. \nThe async and finish nodes in the tree are internal and the leaves are step nodes that represent a step \n(same as the continuation step in this pa\u00adper) in the program execution where data accesses may occur. \nThe DPST is built dynamically at runtime by inserting nodes into the tree in parallel. Raman et al. present \na DPST implementation that exploits the tree structure to insert nodes into the DPST in parallel without \nsynchronization in O(1) time. Here, we summarize the key cost factors in that implementation and refer \nreaders to Raman et al. [13] for the full description. To detect races, if an application performs a \ndata access during a step, a reference to the step is stored in shadow memory so other steps that read/write \nthis same address can reference it. If two steps access the same memory address, the structure of the \ntree is used to determine if these steps can possibly execute in parallel for any possible schedule. \nCon.icting access to the same memory location by concurrent steps is detected as a data race. Two steps \ncan execute concurrently, identi.ed as satisfying the dynamically-may-happen-in-parallel relationship, \nif their lowest common ancestor (LCA) in the DPST tree is an async node. Each memory location is associated \nwith two steps that read the location and one that last wrote that location. The two read operations \nform the extremal bounds of a DPST sub-tree that contains all concur\u00adrent reads to the memory location \nsince the last synchronization point. Rather than comparing with every step performing a read, a step \nperforming a write-operation might be .agged as causing a data race if it can execute concurrent with \nthe three steps whose reads and writes are being tracked. Data-race detection for read op\u00aderations is \nsimilar but also involves operations to track the extremal bounds of the sub-tree of concurrent reads. \nIt can be seen that computing the lowest common ancestor (LCA) of two steps in computing the dynamically-may-happen-in\u00adparallel \nrelation is a performance critical operation, invoked sev\u00aderal times for each read and write operation. \nIn particular, .nding the LCA is among the most expensive parts of the data-race detec\u00adtion. The DPST \ncreation takes constant time and requires no syn\u00adchronization and updating the memory address locations \natomically only happens when a memory address is written or read and the step is higher in the tree than \nthe previous steps that read. On the other hand, computing the LCA between two steps involves walking \nthe DPST from the two steps to their parent, with the cost proportional to the number of edges walked. \nBecause the DPST contains the entire async-.nish tree, it may be very deep if the application is .ne-grained, \nleading to very expensive LCA calculations. We observe that the steal tree can be used to partition the \nDPST based on the steal relationships. In particular, it can be seen that the LCA of two steps in distinct \nworking phases is the LCA of the initial steps of the two working phases. Exploiting this additional \nstructural information allows us to bypass potentially large portions of the tree when traversing upward \nto locate the LCA. We relate a step/async/.nish in the DPST with the steal tree as we create the DPST. \nFor each node (step, async, or finish) in the DPST a pointer is added to store the working phase it belongs \nto. If a step accesses data, a reference to that step may be stored in the shadow memory. We compute \nand track the absolute level of the initial task executed in each working phase, referred to as the depth \nof the subtree executed in that working phase. The algorithm below shows how we traverse the tree when \ntwo steps accessing the same memory address are executed in different working phases. If the depths of \nthe subtrees are different, we traverse up the deeper one. If they are equal we traverse up both until \nthe depths are different or the ancestors of both steps are in the same working phase subtrees. Once \nwe are in the same subtree, we invoke the LCA method in [13] on the initial tasks of the two subtrees. \nsubtreeLCA(Cont s1, Cont s2): while(s1 and s2 are in di.erent subtrees): while(s1 subtree depth != s2 \nsubtree depth): if (s1 subtree depth < s2 subtree depth): s1 = initial task in s1 s working phase else: \ns2 = initial task in s2 s working phase if((s1 subtree depth == s2 subtree depth) &#38;&#38; (s1 and \ns2 in di.erent subtrees)): s1 = initial task in s1 s working phase s2 = initial task in s2 s working \nphase return LCA(s1,s2) //Raman et al. s algorithm  8.2 Retentive Stealing for Async-Finish Programs \nIterative applications that have identical or slowly evolving charac\u00adteristics are said to exhibit the \nprinciple of persistence [18]. Such applications can be incrementally rebalanced based on performance \npro.les gathered from prior iterations. Lif.ander et al. [12] ex\u00adploited the property of persistence \nto improve the scalability of work stealing in iterative applications. In their approach to load balancing, \neach worker begins an iteration with the a collection of tasks executed by it in the previous iteration. \nThis was shown to sig\u00adni.cantly improve parallel ef.ciency as the iterations progressed, in some cases \nfrom about 10% to about 90%. Async-.nish programs that exclusively exploit recursive paral\u00adlelism have \nsimilar performance challenges. First, the slow propa\u00adgation of work from the single initial task available \nat the beginning of the computation incurs signi.cant ramp-up time. While help-.rst scheduling ameliorates \nsome of the performance challenges, the challenges remains. Second, expanding from a single initial task \nfor every iteration discards the application information on persis\u00adtence. While retentive stealing presented \nby Lif.ander et al. [12] can address these issues, their approach was presented in the context of explicit \nenumeration of tasks to enable retentive stealing. This not only increases the storage overhead, but \nis also infeasible when intervening finish statements are involved. The key insight in enabling retentive \nstealing is to allow the execution in each worker to begin with the working phases in the previous iteration, \nwhile also allowing work stealing to improve load balance. We observe that retentive stealing can be \napplied to recursive parallel programs by building on the replay algorithms presented earlier. In particular, \nwe explain the extensions to the replay algorithms to allow stealing of tasks from a working phase being \nreplayed. During normal execution, a worker can execute a stolen task to completion barring synchronization \nconstraints imposed by the finish statement. However, a working phase being replayed com\u00adpletes execution \nwhen all the tasks in that phase are complete. In particular, the tasks at the frontier of a working \nphase need to be dis\u00adtinguished from other tasks. When stealing from a working phase, we therefore check \nwhether the stolen task is at the frontier. In ad\u00addition to the replay actions, a steal operation correctly \nidenti.es the steps represented by the stolen continuation, which can be extracted from the WorkingPhaseInfo \nstructure. A task can be at the frontier, Figure 4. The ratio of mean execution time with tracing versus \nwithout tracing with a sample size of 15 on the POWER 7 architecture using the shared-memory Cilk runtime. \nThe error bars represent the error in the difference of means at 99% con.dence, using a Student s t-test. \n  Figure 5. The storage overhead in KB/thread with our tracing scheme using the shared-memory Cilk \nruntime on the POWER 7 architecture. The error bars represent the standard deviation of storage size \nwith a sample size of 15. Figure 6. The ratio of mean execution time with tracing versus without tracing \nwith a sample size of 15 on Cray XK6 Titan in distributed\u00admemory. The error bars represent the error \nin the difference of means at 99% con.dence, using a Student s t-test. Figure 7. The storage overhead \nin KB/core with our tracing scheme for distributed-memory on Cray XK6 Titan. The error bars represent \nthe standard deviation of storage size with a sample size of 15. Shared-memory Benchmark Con.guration \nAllQueens nq = 14, sequential cutoff 8 Heat nt = 5, nx = 4096, ny = 4096 Fib n = 43 FFT n = 67108864 \nStrassen n = 4096 NBody iterations = 15, nbodies = 8192 Cholesky n = 2048, z = 20000 LU n = 1024 Matmul \nn = 3000 Distributed-memory AQ nq = 19, sequential cutoff 10 SCF 128 beryllium atoms, chunk size 40 TCE \nC[i, j, k, l]+ = A[i, j, a, b] * B[a, b, k, l] O-blocks 20 14 20 26, V-blocks 120 140 180 100 PG 13K \nsequences Table 1. Benchmark con.gurations. i.e., its descendents stolen in the current working phase, \nonly if its parent is at the frontier. Thus a thief stealing a step not at the frontier can execute all \ntasks reachable from that step. When a stolen task is at the frontier, the WorkingPhaseInfo structure \nassociated with vic\u00adtim s current working phase is also copied. This allows the thief to ensure that \nit does not execute steps reachable from the stolen con\u00adtinuation past the work represented by the victim \ns working phase. The calculations that use the task s level to determine the frontier during replay are \nadapted to take into account the level of the stolen task in the victim s working phase. In this approach, \nsteals of tasks not at frontier just steal the step information and incur the same cost as a steal during \nnormal ex\u00adecution. Steals at the frontier, also need to copy the frontier in\u00adformation from the victim \ns working phase, and thus incur greater data movement overhead on distributed-memory systems. Stealing \nfrom working phases to enable retentive stealing can also lead to working phase fragmentation , potentially \nincreasing the number of working phases and hence the storage overhead as the iterations progress. In \nsection 9, we show that these overheads do not ad\u00adversely impact performance and the storage required \nstays well be\u00adlow the amount required to explicitly enumerate all tasks. 9. Experimental Evaluation The \nshared-memory experiments were performed on a POWER 7 system composed of 128 GB of memory and a quad-chip \nmodule with eight cores per chip running at 3.8 GHz and supporting four\u00adway simultaneous multi-threading. \nThe distributed-memory exper\u00adiments were performed on the OLCF Cray XK6 Titan , an 18688\u00adnode system \nwith one AMD sixteen-core 2.2GHz Bulldozer pro\u00adcessor and 32GB DDR3 memory per node. The implementations \nwere compiled with GCC 4.4.6 on the POWER 7 and PGI 12.5.0 on Titan. The distributed-memory benchmarks \nused MPI (MPICH2 5.5.0) for communication. We implemented the tracing algorithms for shared-memory systems \nin Cilk 5.4.6, and distributed-memory versions using the implemen\u00adtation in [12]. We tried to reproduce \nthe con.gurations used by Lif.ander et al. [12] in their distributed-memory experiments. 9.1 Tracing \non Shared-Memory Systems We evaluated seven of the example benchmarks in the Cilk suite and have added \ntwo more NBody and AllQueens; the con.gura\u00adtions are shown in Table 1. AllQueens is a variant of the \nNQueens Cilk benchmark that searches for all valid board con.gurations rather than just one. The NBody \nbenchmark is from the Computer Language Benchmarks Game suite [8]; we have parallelized the benchmark \nby spawning a task per nbody calculation and using synchronization between iterations for the n-body \nupdates. For these nine benchmarks, we graph the ratio of execution time with our tracing versus the \nexecution time without tracing in Fig\u00adure 4. Each bar is the ratio of the mean of 15 runs with and with\u00adout \ntracing for each benchmark and the error bars are the standard error in the difference of means at 99% \ncon.dence, using a Stu\u00addent s t-test [7]. This .gure shows that our tracing overhead is low and within \nthe run-to-run variability on the machine. We performed these same comparisons on another shared-memory \narchitecture (an AMD x86-64 system) and observed the same trend: low over\u00adhead but high variability between \nruns. Figure 5 shows the storage overhead in KB/thread that was incurred with tracing as we strong-scale \nthe nine Cilk benchmarks. The error bars represent the standard deviation from a sample of 15 runs. For \nall the runs the standard deviation is low, demonstrating that different random stealing schedules do \nnot signi.cantly impact storage overhead. To make the trends more visible, we graph six of the benchmarks \nthat have less overhead on the left and three on the right that have more overhead with different y-axis. \nFor the .rst few scaling points all the benchmarks increase in storage per thread, but this increase \nscales sub-linearly (note that the threads are doubled each time, except for the 96-thread point) with \nthread count. This graph demonstrates that our storage requirements are small, grow slowly with thread \ncount, and have low variation even with differing schedules. The total storage overhead continues to \nincrease with thread count, re.ecting the fact that increasing thread counts increases the number of \nsteals. Despite this increase, we observe that the total trace size, even on 120 threads, is small enough \nto be analyzed on a commodity desktop system. The traces were replayed to determine the utilization across \ntime. Some of the results are shown in Figure 8. These plots, quickly computed from the traces, show \nthe variation in executing identical iterations of Heat on 120 threads and the signi.cant under\u00adutilization \nwhen running LU even at moderate thread counts.  9.2 Tracing on Distributed-Memory Systems For distributed-memory, \nwe evaluate four benchmarks with two different scheduling strategies to measure the execution time and \nstorage overhead that our tracing incurs (the con.gurations are shown in Table 1). The AllQueens (AQ) \nbenchmark is a distributed\u00admemory variant of the AllQueens benchmark. When the depth of recursion exceeds \na threshold the benchmark executes an opti\u00admized sequential kernel. SCF and TCE are computational chem\u00adistry \nbenchmarks designed by Lif.ander et al. [12]. PG is a work stealing implementation of the pair-wise sequence \nalignment ap\u00adplication designed by Wu et al. [17]. We refer the readers to the corresponding papers for \ndetails. We execute the four benchmarks under both work-.rst and help\u00ad.rst scheduling policies. Figure \n6 shows the ratio of execution time with tracing versus the execution time without tracing. For all the \ncon.gurations, the overhead is low and mostly within the error bars, which represent the standard error \nin the difference of means at 99% con.dence, using a Student s t-test. Some of the variation is due to \nobtaining a different job partition on the machine between runs (most likely the reason a couple points \nexecute faster with tracing). Figure 7 shows the storage overhead in KB/core that we incur with our tracing \nschemes. Note that for all the con.gurations except for AQ-WF, the overhead is less than 75 KB/core and \nis constant or decreases with core count. At 32,000 cores the total storage for the traces, assuming \n75 KB/core, is 2.3 GB, which would  Figure 8. Percent utilization (y-axis) over time in seconds (x-axis) \nusing the steal tree traces, colored by a random color for each thread. Figure 9. The percent reduction \nattained by exploiting our traces to reduce the tree traversals of the DPST (dynamic program structure \ntree) to detect races in a shared-memory application [13]. (a) Scaling (b) Help-First (c) Work-First \n (a) Scaling (b) Help-First (c) Work-First   allow performance analysis to be performed easily on a \nshared\u00admemory machine or a medium-scale work station. The AQ-WF con.guration shows a somewhat different \ntrend because the grain size for AQ is very .ne and the WF scheme reduces parallel slack compared to \nthe HF scheme.  9.3 Optimizing Data-Race Detection For several benchmarks in Cilk, we show in Figure \n9 the percent reduction of tree traversals achieved by exploiting the subtree in\u00adformation to bypass \ntraversing regions of the tree. For the Heat benchmark we observe over a 70% reduction in tree traversals; \nfor Matmul around 50 60% reduction; for NBody over 30% reduction; for LU around a 40 50% reduction; and \nfor AllQueens around a 20 30% reduction. The general trend is that increasing the thread count partitions \nthe tree more, causing a further reduction in the number of tree traversals required. Depending on the \nstructure, fur\u00adther segmentation beyond some point may not be bene.cial if the LCA is high up in the \ntree. Heat has this behavior where the best partitions are few and high up in the tree, with further \nsegmentation causing an increase in tree traversals.  9.4 Retentive Stealing We evaluated retentive \nstealing for the two iterative distributed\u00admemory benchmarks: Self-Consistent Field (SCF) and Tensor \nCon\u00adtraction Engine (TCE) (retentive stealing is not applicable for the others). In Figures 10a and 11a \nwe show the scaling behavior of both benchmarks after several warm-up iterations and then running them \n10 iterations to convergence with retentive stealing. The help\u00ad.rst and work-.rst schemes scale almost \nperfectly and the scaling results are comparable to the result in [12]. We graph the full task enumeration \nscheme used in that paper as TCE-Enum and SCF-Enum. We are able to reproduce this result without incurring \nthe overhead of enumerating every task and storing them. For SCF, fully enumerating the tasks requires \n20.7 KB/core on 2K cores; for TCE, full enumerating the tasks requires 8.3 KB/core on 2K cores. For both \nbenchmarks, the steal tree uses signi.cantly less storage per core compared to enumerating all the tasks. \nFor instance, on 2K cores of SCF with help-.rst scheduling we only require 0.34 KB/core to store the \ntraces; this decreases to 0.26 KB/core on 32K cores. Also, our queue size is bounded by d (where d is \nthe depth of the computational tree) for the work-.rst scheme or bd (where b is the branching factor) \nfor the help-.rst scheme. With explicit enumeration the queue size is bounded by the number of tasks, \nwhich may require signi.cation memory overhead. In Figures 10b, 10c, 11b and 11c we graph the amount \nof storage per core required over time for retentive stealing because steals in subsequent iterations \ncause more partitioning of the work. We observe that the convergence rate is application-and scale\u00addependent. \nFor the SCF benchmark, the convergence rate increases with scale under strong scaling as the benchmark \napproaches the limits of its concurrency. We thus anticipate the storage overhead to remain constant \nor increase very slowly for such an iterative application at large scales. TCE appears to be very well-behaved, \nwith subsequent iterations causing almost no increase in storage overhead. 10. Conclusions The widespread \nuse of work stealing necessitates the development of mechanisms to study the behavior of individual executions. \nThe algorithms presented in this paper to ef.ciently construct steal trees enable low overhead tracing \nand replay of async-.nish programs scheduled using work-.rst or help-.rst work stealing schedulers. We \ndemonstrated the broader applicability of this work, beyond replay-based performance analysis, by demonstrating \nits usefulness in optimizing data race detection and extending the class of pro\u00adgrams that can employ \nretentive stealing. As future work, we are considering the use of these tracing algorithms to construct \nskele\u00adtons for dynamic task-parallel programs that help study the stealing and execution structure of \ncomplex applications without requiring an understanding of the detailed domain-speci.c operations. Acknowledgments \nThis work was supported in part by the U.S. Department of En\u00adergy s (DOE) Of.ce of Science, Of.ce of \nAdvanced Scienti.c Computing Research, under award number 59193, and by Paci.c Northwest National Laboratory \nunder the Extreme Scale Comput\u00ading Initiative. This research used resources of the Oak Ridge Lead\u00adership \nComputing Facility at the Oak Ridge National Laboratory, which is supported by the Of.ce of Science under \nContract No. DE-AC05-00OR22725. We thank Vivek Sarkar and Raghavan Ra\u00adman for discussions on their data-race \ndetection algorithm [13]. References [1] U. A. Acar, G. E. Blelloch, and R. D. Blumofe. The data locality \nof work stealing. Theory of Computing Systems, 35(3):321 347, 2002. ISSN 1432-4350. [2] K. Agrawal, J. \nT. Fineman, and J. Sukha. Nested parallelism in trans\u00adactional memory. In Proceedings of the 13th ACM \nSIGPLAN Sympo\u00adsium on Principles and practice of parallel programming, PPoPP 08, pages 163 174, 2008. \n[3] E. Ayguad\u00b4e, N. Copty, A. Duran, J. Hoe.inger, Y. Lin, F. Massaioli, X. Teruel, P. Unnikrishnan, \nand G. Zhang. The design of OpenMP tasks. Parallel and Distributed Systems, IEEE Transactions on, 20(3): \n404 418, 2009. [4] M. A. Bender, J. T. Fineman, S. Gilbert, and C. E. Leiserson. On\u00adthe-.y maintenance \nof series-parallel relationships in fork-join mul\u00adtithreaded programs. In Proceedings of the sixteenth \nannual ACM symposium on Parallelism in algorithms and architectures, SPAA 04, pages 133 144, 2004. [5] \nR. D. Blumofe. Executing multithreaded programs ef.ciently. PhD thesis, Massachusetts Institute of Technology, \n1995. [6] R. D. Blumofe, C. F. Joerg, B. C. Kuszmaul, C. E. Leiserson, K. H. Randall, and Y. Zhou. Cilk: \nan ef.cient multithreaded runtime system. In Proceedings of the .fth ACM SIGPLAN symposium on Principles \nand practice of parallel programming, PPOPP 95, pages 207 216, 1995. [7] J. F. Box. Guinness, Gosset, \nFisher, and Small Samples. Sta\u00adtistical Science, 2(1):45 52, Feb. 1987. ISSN 0883-4237. doi: 10.1214/ss/1177013437. \n[8] B. Fulgham. Computer language benchmarks game, August 2012. URL http://shootout.alioth.debian.org/. \n[9] Y. Guo, R. Barik, R. Raman, and V. Sarkar. Work-.rst and help-.rst scheduling policies for async-.nish \ntask parallelism. In IEEE Interna\u00adtional Symposium on Parallel &#38; Distributed Processing, IPDPS 09, \npages 1 12. IEEE, 2009. [10] T. Karunaratna. Nondeterminator-3: a provably good data-race de\u00adtector that \nruns in parallel. PhD thesis, Massachusetts Institute of Technology, 2005. [11] D. Lea et al. Java speci.cation \nrequest 166: Concurrency utilities, 2004. [12] J. Lif.ander, S. Krishnamoorthy, and L. V. Kale. Work \nstealing and persistence-based load balancers for iterative overdecomposed applications. In Proceedings \nof the 21st international symposium on High-Performance Parallel and Distributed Computing, HPDC 12, \npages 137 148, 2012. [13] R. Raman, J. Zhao, V. Sarkar, M. Vechev, and E. Yahav. Scalable and precise \ndynamic datarace detection for structured parallelism. In Proceedings of the 33rd ACM SIGPLAN conference \non Programming Language Design and Implementation, PLDI 12, pages 531 542, 2012.  [14] J. Reinders. \nIntel Threading Building Blocks: Out.tting C++ for Multi-Core Processor Parallelism. O Reilly Media, \n2007. [15] V. A. Saraswat, V. Sarkar, and C. von Praun. X10: concurrent pro\u00adgramming for modern architectures. \nIn Proceedings of the 12th ACM SIGPLAN symposium on Principles and practice of parallel program\u00adming, \nPPoPP 07, pages 271 271, 2007. [16] N. R. Tallent and J. M. Mellor-Crummey. Identifying performance bottlenecks \nin work-stealing computations. IEEE Computer, 42(12): 44 50, 2009. [17] C. Wu, A. Kalyanaraman, and W. \nR. Cannon. PGraph: ef.cient parallel construction of large-scale protein sequence homology graphs. IEEE \nTransactions on Parallel and Distributed Systems, 23(10):1923 1933, 2012. [18] G. Zheng. Achieving high \nperformance on extremely large parallel machines: performance prediction and load balancing. PhD thesis, \nDepartment of Computer Science, University of Illinois at Urbana-Champaign, 2005.    \n\t\t\t", "proc_id": "2491956", "abstract": "<p>Work stealing is a popular approach to scheduling task-parallel programs. The flexibility inherent in work stealing when dealing with load imbalance results in seemingly irregular computation structures, complicating the study of its runtime behavior. In this paper, we present an approach to efficiently trace async-finish parallel programs scheduled using work stealing. We identify key properties that allow us to trace the execution of tasks with low time and space overheads. We also study the usefulness of the proposed schemes in supporting algorithms for data-race detection and retentive stealing presented in the literature. We demonstrate that the perturbation due to tracing is within the variation in the execution time with 99% confidence and the traces are concise, amounting to a few tens of kilobytes per thread in most cases. We also demonstrate that the traces enable significant reductions in the cost of detecting data races and result in low, stable space overheads in supporting retentive stealing for async-finish programs.</p>", "authors": [{"name": "Jonathan Lifflander", "author_profile_id": "81502795727", "affiliation": "Univeristy of Illinois Urbana-Champaign, Urbana, Illinois, USA", "person_id": "P4149109", "email_address": "jliffl2@illinois.edu", "orcid_id": ""}, {"name": "Sriram Krishnamoorthy", "author_profile_id": "81456623501", "affiliation": "Pacific Northwest National Lab, Richland, Washington, USA", "person_id": "P4149110", "email_address": "sriram@pnnl.gov", "orcid_id": ""}, {"name": "Laxmikant V. Kale", "author_profile_id": "81100348138", "affiliation": "Univeristy of Illinois Urbana-Champaign, Urbana, Illinois, USA", "person_id": "P4149111", "email_address": "kale@illinois.edu", "orcid_id": ""}], "doi_number": "10.1145/2491956.2462193", "year": "2013", "article_id": "2462193", "conference": "PLDI", "title": "Steal Tree: low-overhead tracing of work stealing schedulers", "url": "http://dl.acm.org/citation.cfm?id=2462193"}