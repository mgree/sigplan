{"article_publication_date": "06-16-2013", "fulltext": "\n Taming Compiler Fuzzers Yang Chen Alex Groce Chaoqiang Zhang Weng-Keen Wong Xiaoli Fern Eric Eide John \nRegehr University of Utah Oregon State University Salt Lake City, UT Corvallis, OR chenyang@cs.utah.edu \nagroce@gmail.com zhangch@onid.orst.edu wong@eecs.oregonstate.edu xfern@eecs.oregonstate.edu eeide@cs.utah.edu \nregehr@cs.utah.edu Abstract Aggressive random testing tools ( fuzzers ) are impressively ef\u00adfective \nat .nding compiler bugs. For example, a single test-case generator has resulted in more than 1,700 bugs \nreported for a sin\u00adgle JavaScript engine. However, fuzzers can be frustrating to use: they indiscriminately \nand repeatedly .nd bugs that may not be se\u00advere enough to .x right away. Currently, users .lter out undesirable \ntest cases using ad hoc methods such as disallowing problematic features in tests and grepping test results. \nThis paper formulates and addresses the fuzzer taming problem: given a potentially large number of random \ntest cases that trigger failures, order them such that diverse, interesting test cases are highly ranked. \nOur evalua\u00adtion shows our ability to solve the fuzzer taming problem for 3,799 test cases triggering \n46 bugs in a C compiler and 2,603 test cases triggering 28 bugs in a JavaScript engine. Categories and \nSubject Descriptors D.2.5 [Software Engineer\u00ading]: Testing and Debugging testing tools; D.3.4 [Programming \nLanguages]: Processors compilers; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval \nselection process Keywords compiler testing; compiler defect; automated testing; fuzz testing; random \ntesting; bug reporting; test-case reduction 1. Introduction Modern optimizing compilers and programming \nlanguage runtimes are complex artifacts, and their developers can be under signi.cant pressure to add \nfeatures and improve performance. When dif.cult algorithms and data structures are tuned for speed, internal \nmodu\u00adlarity suffers and invariants become extremely complex. These and other factors make it hard to \navoid bugs. At the same time, compilers and runtimes end up as part of the trusted computing base for \nmany systems. A code-generation error in a compiler for a critical embed\u00added system, or an exploitable \nvulnerability in a widely deployed scripting language runtime, is a serious matter. Random testing, or \nfuzzing, has emerged as an important tool for .nding bugs in compilers and runtimes. For example, a sin\u00adgle \nfuzzing tool, jsfunfuzz [31], is responsible for identifying Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. PLDI 13, June 16 19, 2013, Seattle, WA, USA. \nCopyright c &#38;#169; 2013 ACM 978-1-4503-2014-6/13/06. . . $15.00 more than 1,700 previously unknown \nbugs in SpiderMonkey, the JavaScript engine used in Firefox [32]. LangFuzz [14], a newer randomized testing \ntool, has led to the discovery of more than 500 previously unknown bugs in the same JavaScript engine. \nGoogle s proprietary ClusterFuzz effort hammers away at it [Chrome, includ\u00ading its V8 JavaScript engine] \nto the tune of around .fty-million test cases a day, with apparent success [2]: ClusterFuzz has detected \n95 unique vulnerabilities since we brought it fully online at the end of last year [a four-month period]. \nFor C compilers, Csmith [44] has identi.ed more than 450 previously unknown bugs. While fuzzers are powerful \nbug-.nding tools, their use suffers from several drawbacks. The .rst problem is that failures due to \nrandom test cases can be dif.cult to debug. This has been largely solved by Delta Debugging [45], an \nautomated greedy search for small failure-inducing test cases. In fact, there is some evidence that debugging \nbased on short random test cases is easier than debugging based on human-created test cases [3]. A second \nproblem is the sheer volume of output: an overnight run of a fuzzer may result in hundreds or thousands \nof failure-inducing test cases. Moreover, some bugs tend to be triggered much more often than others, \ncreating needle-in\u00ada-haystack problems. Figure 1 shows that some of the bugs studied in this paper were \ntriggered thousands of times more frequently than others. Compiler engineers are an expensive and limited \nresource, and it can be hard for them to .nd time to sift through a large collection of highly redundant \nbug-triggering test cases. A third problem is that fuzzers are indiscriminate: they tend to keep .nding \nmore and more test cases that trigger noncritical bugs that may also already be known. Although it would \nbe desirable to .x these bugs, the realities of software development where resources are limited and \ndeadlines may be in.exible often cause low-priority bugs to linger un.xed for months or years. For example, \nin November 2012 we found 2,403 open bugs in GCC s bug database, considering priorities P1, P2, and P3, \nand considering only bugs of normal or higher severity. The median-aged bug in this list was well over \ntwo years old. If a fuzzer manages to trigger any appreciable fraction of these old, known bugs, its \nraw output will be very hard to use. A typical work.ow for using a random tester is to (1) start running \nthe random tester against the latest version of the compiler; (2) go to bed; and (3) in the morning, \nsift through the new failure\u00adinducing test cases, creating a bug report for each that is novel and important. \nStep 3 can be time-consuming and unrewarding. We know of several industrial compiler developers who stopped \nusing Csmith not because it stopped .nding bugs, but because step 3 became uneconomical. This paper represents \nour attempt to provide fuzzer users with a better value proposition. Thus far, little research has addressed \nthe problem of making fuzzer output more useful to developers. In a blog entry, Ruderman, Figure 1. A \nfuzzer tends to hit some bugs thousands of times more frequently than others   Figure 2. Work.ow for \na fuzzer tamer the original author of the jsfunfuzz tool, reports using a variety of heuristics to avoid \nlooking at test cases that trigger known bugs, such as turning off features in the test-case generator \nand using tools like grep to .lter out test cases triggering bugs that have predictable symptoms [33]. \nDuring testing of mission .le systems at NASA [12], Groce et al. used hand-tuned rules to avoid repeatedly \n.nding the same bugs during random testing e.g., ignore any test case with a reset within .ve operations \nof a rename. We claim that much more sophisticated automation is feasible and useful. In this paper we \ndescribe and evaluate a system that does this for two fuzzers: one for JavaScript engines, the other \nfor C compilers. We characterize the fuzzer taming problem: Given a potentially large collection of test \ncases, each of which triggers a bug, rank them in such a way that test cases triggering distinct bugs \nare early in the list. Sub-problem: If there are test cases that trigger bugs previ\u00ad ously .agged as \nundesirable, place them late in the list. Ideally, for a collection of test cases that triggers N distinct \nbugs (none of which have been .agged as undesirable), each of the .rst N test cases in the list would \ntrigger a different bug. In practice, perfection is unattainable because the problem is hard and also \nbecause there is some subjectivity in what constitutes distinct bugs. Thus, our goal is simply to improve \nas much as possible upon the default situation where test cases are presented to users in effectively \nrandom order. Figure 2 shows the work.ow for a fuzzer tamer. The oracle in the .gure detects buggy executions, \nfor example by watching for crashes and by running the compiler s output against a reference compiler \ns output. Our rank-ordering approach was suggested by the prevalence of ranking approaches for presenting \nalarms produced by static analyses to users [18, 19]. Our contributions are as follows. First, we frame \nthe fuzzer taming problem, which has not yet been addressed by the research community, as far as we are \naware. Second, we make and exploit the observation that automatic triaging of test cases is strongly \nsynergistic with automated test-case reduction. Third, based on the insight that bugs are highly diverse, \nwe exploit diverse sources of information about bug-triggering test cases, including features of the \ntest case itself, features from execution of the compiler on the test case, and features from the compiler \ns output. Fourth, we show that diverse test cases can be ranked highly by .rst placing test cases in \na metric space and then using the furthest point .rst (FPF) technique from machine learning [10]. The \nmore obvious approach to fuzzer taming is to use a clustering algorithm [27] to place tests into homogeneous \ngroups, and then choose a representative test from each cluster. We show that FPF is both faster and \nmore effective than clustering for all of our case studies. Finally, we show that our techniques can \neffectively solve the fuzzer taming problem for 2,603 test cases triggering 28 bugs in a JavaScript engine \nand 3,799 test cases triggering 46 bugs in a C compiler. Using our methods over this test suite, a developer \nwho inspects the JavaScript engine s test cases in ranked order will more quickly .nd cases that trigger \nthe 28 bugs found during the fuzzing run. In comparison to a developer who examines cases in a random \norder, the developer who inspects in ranked order will be 4.6\u00d7 faster. For wrong-code bugs and crash \nbugs in the C compiler, the improvements are 2.6\u00d7 and 32\u00d7, respectively. Even more importantly, users \ncan .nd many more distinct bugs than would be found with a random ordering by examining only a few tens \nof test cases. Taming a fuzzer differs from previous efforts in duplicate bug de\u00adtection [37, 38, 42] \nbecause user-supplied metadata is not available: we must rely solely on information from failure-inducing \ntest cases. Compared to previous work on dealing with software containing multiple bugs [15, 22, 28], \nour work differs in the methods used (ranking bugs as opposed to clustering), in the kinds of inputs \nto the machine learning algorithms (diverse, as opposed to just predicates or coverage information), \nand in its overall goal of taming a fuzzer. 2. Approach This section describes our approach to taming \ncompiler fuzzers and gives an overview of the tools implementing it. 2.1 De.nitions A fault or bug in \na compiler is a .aw in its implementation. When the execution of a compiler is in.uenced by a fault e.g., \nby wrong or missing code the result may be an error that leads to a failure detected by a test oracle. \nIn this paper, we are primarily concerned with two kinds of failures: (1) compilation or interpretation \nthat fails to follow the semantics of the input source code, and (2) com\u00adpiler crashes. The goal of a \ncompiler fuzzer is to discover source programs test cases that lead to these failures. The goal of a \nfuzzer tamer is to rank failure-inducing test cases such that any pre.x of the ranked list triggers as \nmany different faults as possible. Faults are not directly observable, but a fuzzer tamer can estimate \nwhich test cases are related by a common fault by making an as\u00adsumption: the more similar two test cases, \nor two executions of the compiler on those test cases, the more likely they are to stem from the same \nfault [23]. A distance function maps any pair of test cases to a real number that serves as a measure \nof similarity. This is useful because our goal is to present fuzzer users with a collection of highly \ndissimilar test cases. Because there are many ways in which two test cases can be similar to each other \ne.g., they can be textually similar, cause similar failure output, or lead to similar executions of the \ncompiler our work is based on several distance functions.  2.2 Ranking Test Cases Our approach to solving \nthe fuzzer taming problem is based on the following idea. Hypothesis 1: If we (1) de.ne a distance function \nbetween test cases that appropriately captures their static and dynamic characteristics and then (2) \nsort the list of test cases in furthest point .rst (FPF) order, then the resulting list will constitute \na usefully approximate solution to the fuzzer taming problem. If this hypothesis holds, the fuzzer taming \nproblem is reduced to de.ning an appropriate distance function. The FPF ordering is one where each point \nin the list is the one that maximizes the distance to the nearest of all previously listed elements; \nit can be computed using a greedy algorithm [10]. We use FPF to ensure that diverse test cases appear \nearly in the list. Conversely, collections of highly similar test cases will be found towards the end \nof the list. Our approach to ignoring known bugs is based on the premise that fuzzer users will have \nlabeled some test cases as exemplifying these bugs; this corresponds to the feedback edge in Figure 2. \nHypothesis 2: We can lower the rank of test cases corre\u00adsponding to bugs that are known to be uninteresting \nby seed\u00ading the FPF computation with the set of test cases that are labeled as uninteresting. Thus, the \nmost highly ranked test case will be the one maximiz\u00ading its minimum distance from any labeled test case. \n 2.3 Distance Functions for Test Cases The fundamental problem in de.ning a distance function that will \nproduce good fuzzer taming results is that we do not know what the trigger for a generic compiler bug \nlooks like. For example, one C compiler bug might be triggered by a struct with a certain sequence of \nbit.elds; another bug might be triggered by a large number of local variables, which causes the register \nallocator to spill. Our solution to this fundamental ambiguity has been to de.ne a variety of distance \nfunctions, each of which we believe will usefully capture some kinds of bug triggers. This section describes \nthese distance functions. Levenshtein Distance Also known as edit distance, the Leven\u00adshtein distance \n[20] between two strings is the smallest number of character additions, deletions, and replacements that \nsuf.ces to turn one string into the other. For every pair of test cases we compute the Levenshtein distance \nbetween the following, all of which can be treated as plain text strings: the test cases themselves; \n the output of the compiler as it crashes (if any); and  the output of Valgrind [25] on a failing execution \n(if any).  Computing Levenshtein distance requires time proportional to the product of the string lengths, \nbut the constant factor is small (a few tens of instructions), so it is reasonably ef.cient in practice. \nEuclidean Distance Many aspects of failure-inducing test cases, and of executions of compilers on these \ntest cases, lend themselves to summarization in the form of feature vectors. For example, consider this \nreduced JavaScript test case, which triggers a bug in SpiderMonkey 1.6: __proto__=__parent__ new Error(this) \nLexing this code gives eight tokens, and a feature vector based on these tokens contains eight nonzero \nelements. The overall vector contains one element for every token that occurs in at least one test case, \nbut which does not occur in every test case, out of a batch of test cases that is being processed by \nthe fuzzer tamer. The elements in the vector are based on the number of appearances of each token in \nthe test case. We construct lexical feature vectors for both C and JavaScript. Given two n-element vectors \nv1 and v2, the Euclidean distance between them is: . (v1[i] - v2[i])2 i=1..n For C code, our intuition \nwas that lexical analysis in some sense produced shallower results than it did for JavaScript. To compensate, \nwe wrote a Clang-based detector for 45 additional features that we guessed might be associated with compiler \nbugs. These features include: common types, statement classes, and operator kinds;  features speci.c \nto aggregate data types such as structs with bit.elds and packed structs; obvious divide-by-zero operations; \nand  some kinds of in.nite loops that can be detected statically.  In addition to constructing vectors \nfrom test cases, we also constructed feature vectors from compiler executions. For example, the function \ncoverage of a compiler is a list of the functions that it executes while compiling a test case. The overall \nfeature vector for function coverage contains an element for every function executed while compiling \nat least one test case, but that is not executed while compiling all test cases. As with token-based \nvectors, the vector elements are based on how many times each function executed. We created vectors of: \n functions covered;  lines covered;  tokens in the compiler s output as it crashes (if any); and  \ntokens in output from Valgrind (if any).  In the latter two cases, we use the same tokenization as with \ntest cases (treating output from the execution as a text document), except that in the case of Valgrind \nwe abstract some non-null memory addresses to a generic ADDRESS token. The overall hypothesis is that \nmost bugs will exhibit some kind of dynamic signature that will reveal itself in one or more kinds of \nfeature vector. Normalization Information retrieval tasks can often bene.t from normalization, which \nserves to decrease the importance of terms that occur very commonly, and hence convey little information. \nBefore computing distances over feature vectors, we normalized the value of each vector element using \ntf-idf [34]; this is a common practice in text clustering and classi.cation. Given a count of a feature \n(token) in a test case or its execution (the document ), the tf-idf is the product of the term-frequency \n(tf) and the inverse\u00addocument-frequency (idf) for the token. Term-frequency is the ratio of the count \nof the token in the document to the total number of tokens in the document. (For coverage we use number \nof times the entity is executed.) Inverse-document-frequency is the logarithm of the ratio of the total \nnumber of documents and the total number of documents containing the token: this results in a uniformly \nzero value for tokens appearing in all documents, which are therefore not included in the vector. We \nnormalize Levenshtein distances by the length of the larger of the two strings, which helps handle varying \nsizes for test cases or outputs.  3. A Foundation for Experiments To evaluate our work, we need a large \ncollection of reduced versions of randomly generated test cases that trigger compiler bugs. More\u00adover, \nwe require access to ground truth: the actual bug triggered by each test case. This section describes \nour approach to meeting these prerequisites. 3.1 Compilers Tested We chose to test GCC 4.3.0 and SpiderMonkey \n1.6, both running on Linux on x86-64. SpiderMonkey, best known as the JavaScript en\u00adgine embedded in \nFirefox, is a descendant of the original JavaScript implementation; it contains an interpreter and several \nJIT compil\u00aders. Our selection of these particular versions was based on several considerations. First, \nthe version that we fuzzed had to be buggy enough that we could generate useful statistics. Second, it \nwas im\u00adportant that most of the bugs revealed by our fuzzer had been .xed by developers. This would not \nbe the case for very recent compiler versions. Also, it turned out not to be the case for GCC 4.0.0, \nwhich we initially started using and had to abandon, since maintenance of its release branch the 4.0.x \nseries terminated in 2007 with too many un.xed bugs. 3.2 Test Cases for C We used the default con.guration \nof Csmith [44] version 2.1.0, which over a period of a few days generated 2,501 test cases that crash \nGCC and 1,298 that trigger wrong-code bugs. The default con.guration of Csmith uses swarm testing [13], \nwhich varies test features to improve fault detection and code coverage. Each program emitted by Csmith \nwas compiled at -O0, -O1, -O2, -Os, and -O3. To detect crash bugs, we inspected the return code of the \nmain compiler process; any nonzero value was considered to indicate a crash. To detect wrong-code bugs, \nwe employed differential testing: we waited for the compiler s output to produce a result different from \nthe result of executing the output of a reference compiler. Since no perfect reference compiler exists, \nwe approximated one by running GCC 4.6.0 and Clang 3.1 at their lowest optimization levels and ensuring \nthat both compilers produced executables that, when run, had the same output. (We observed no mismatches \nduring our tests.) Csmith s outputs tend to be large, often exceeding 100 KB. We reduced each failure-inducing \ntest case using C-Reduce [29], a tool that uses a generalized version of Delta debugging to heuristically \nreduce C programs. After reduction, some previously different tests became textually equivalent; this \nhappens because C-Reduce tries quite hard to reduce identi.ers, constants, data types, and other constructs \nto canonical values. For crash bugs, reduction produced 1,797 duplicates, leaving only 704 different \ntest cases. Reduction was less effective at canonicalizing wrong-code test cases, with only 23 duplicate \ntests removed, leaving 1,275 tests to examine. In both cases, the typical test case was reduced in size \nby two to three orders of magnitude, to an average size of 128 bytes for crash bugs and 243 bytes for \nwrong-code bugs. 3.3 Test Cases for JavaScript We started with the last public release of jsfunfuzz \n[31], a tool that, over its lifetime, has led to the discovery of more than 1,700 faults in SpiderMonkey. \nWe modi.ed jsfunfuzz to support swarm testing and then ran it for several days, accumulating 2,603 failing \ntest cases. Differential testing of JavaScript compilers is problematic due to their diverging implementations \nof many of the most bug\u00adprone features of JavaScript. However, jsfunfuzz comes with a set of built-in \ntest oracles, including semantic checks (e.g., ensuring that compiling then decompiling code is an identity \nfunction) and watchdog timers to ensure that in.nite loops can only result from faults. For an ahead-of-time \ncompiler like GCC, it is natural to divide bugs into those that manifest at compile time (crashes) and \nthose that manifest at run time (wrong-code bugs). This distinction makes less sense for a just-in-time \ncompiler such as SpiderMonkey; we did not attempt to make it, but rather lumped all bugs into a single \ncategory. Test cases produced by jsfunfuzz were also large, over 100 KB on average. We reduced test cases \nusing a custom reducer similar in spirit to C-Reduce, tuned for JavaScript. Reduction resulted in 854 \nduplicate test cases that we removed, leaving 1,749 test cases for input to the fuzzer taming tools. \nThe typical failure-inducing test case for SpiderMonkey was reduced in size by more than three orders \nof magnitude, to an average size of 68 bytes. 3.4 Establishing Ground Truth Perhaps the most onerous \npart of our work involved determining ground truth: the actual bug triggered by each test case. Doing \nthis the hard way examining the execution of the compiler for each of thousands of failure-inducing test \ncases is obviously infeasible. Instead, our goal was to create, for each of the 74 total bugs that our \nfuzzing efforts revealed, a patched compiler .xing only that bug. At that point, ground-truth determination \ncan be automated: for each failure-inducing test case, run it through every patched version of the compiler \nand see which one changes its behavior. We only partially accomplished our goal. For a collection of \narbitrary bugs in a large application that is being actively developed, it turns out to be very hard \nto .nd a patch .xing each bug, and only that bug. For each bug, we started by performing an automated \nforward search to .nd the patch that .xed the bug. In some cases this patch (1) was small; (2) clearly \n.xed the bug triggered by the test case, as opposed to masking it by suppressing execution of the buggy \ncode; and (3) could be back-ported to the version of the compiler that we tested. In other cases, some \nor all of these conditions failed to hold. For example, some compiler patches were extraordinarily complex, \nchanging tens of thousands of lines of code. Moreover, these patches were written for compiler versions \nthat had evolved considerably since the GCC 4.3.0 and SpiderMonkey 1.6 versions that are the basis for \nour experiments. Although we spent signi.cant effort trying to create a minimal patch .xing each compiler \nbug triggered by our fuzzing effort, this was not always feasible. Our backup strategy for assessing \nground truth was .rst to approximately classify each test case based on the revision of the compiler \nthat .xed the bug that it triggered, and second to manually inspect each test case in order to determine \na .nal classi.cation for which bug it triggered, based on our understanding of the set of compiler bugs. \n 3.5 Bug Slippage When the original and reduced versions of a test case trigger different bugs, we say \nthat bug slippage has occurred. Slippage is not hard to avoid for bugs that have an unambiguous symptom \n(e.g., assertion violation at line 512 ) but it can be dif.cult to avoid for silent bugs such as those \nthat cause a compiler to emit incorrect code. Although slippage is normally dif.cult to recognize or \nquantify, these tasks are easy when ground truth is available, as it is here. Of our 2,501 unreduced \ntest cases that caused GCC 4.3.0 to crash, almost all triggered the same (single) bug that was triggered \nby the test case s reduced version. Thirteen of the unreduced test cases triggered two different bugs, \nand in all of these cases the reduced version triggered one of the two. Finally, we saw a single instance \nof actual slippage where the original test case triggered one bug in GCC leading to a segmentation fault \nand the reduced version triggered a different bug, also leading to a segmentation fault. For the 1,298 \ntest cases triggering wrong-code bugs in GCC, slippage during reduction occurred .fteen times. For JavaScript, \nbug slippage was a more serious problem: 23% of reduced JavaScript test cases triggered a different bug \nthan the original test case. This problem was not mitigated (as we had  originally hoped) by re-reducing \ntest cases using the slower debug 4.3 Selecting a Distance Function version of SpiderMonkey. In short, \nbug slippage was a problem for SpiderMonkey 1.6 but not for GCC 4.3.0. Although the dynamics of test-case \nreduction are complex, we have a hypothesis about why this might have been the case. Test-case reduction \nis a heuristic search that explores one particular path through the space of all possible programs. This \npath stays in the subset of programs that trigger a bug and also follows a gradient leading towards smaller \ntest cases. Sometimes, the tra\u00adjectory will pass through the space of programs triggering some completely \ndifferent bug, causing the reduction to be hijacked by the second bug. We would expect this to happen \nmore often for a compiler that is buggier. Our observation is that GCC 4.3.0 is basi\u00adcally a solid and \nmature implementation whereas SpiderMonkey 1.6 is not it contains many bugs in fairly basic language \nfeatures. 4. Results and Discussion For 1,979 reduced test cases triggering 46 bugs in GCC 4.3.0 and \n1,749 reduced test cases triggering 28 bugs in SpiderMonkey 1.6, our goal is to rank them for presentation \nto developers such that diverse faults are triggered by test cases early in the list. 4.1 Evaluating \nEffectiveness using Bug Discovery Curves Figures 3 8 present the primary results of our work using bug \ndiscovery curves. A discovery curve shows how quickly a ranking of items allows a human examining the \nitems one by one to view at least one representative of each different category of items [26, 40]. Thus, \na curve that climbs rapidly is better than a curve that climbs more slowly. Here, the items are test \ncases and categories are the underlying compiler faults. The top of each graph represents the point at \nwhich all faults have been presented. As shown by the y-axes of the .gures, there are 28 SpiderMonkey \nbugs, 11 GCC crash bugs, and 35 GCC wrong-code bugs in our study. Each of Figures 3 8 includes a baseline: \nthe expected bug discovery curve without any fuzzer taming. We computed it by looking at test cases in \nrandom order, averaged over 10,000 orders. We also show the theoretical best curve where for N faults \neach of the .rst N tests reveals a new fault. In each graph, we show in solid black the .rst method to \n.nd all bugs (which, in all of our examples, is also the method with the best area under the full curve). \nFor GCC crash bugs and SpiderMonkey, this method also has the best climb for the .rst 50 tests, and for \nGCC wrong-code bugs, it is almost the best for the .rst 50 tests (and, in fact, discovers one more bug \nthan the curve with the best area). For this best curve, we also show points sized by the log of the \nfrequency of the fault; our methods do not always .nd the most commonly triggered faults .rst. Finally, \neach graph additionally shows the best result that we could obtain by ranking test cases using clustering \ninstead of FPF, using X-means to generate clusterings by various features, sorting all clusterings by \nisolation and compactness, and using the centermost test for each cluster. (See Section 4.6 for details.) \n 4.2 Are These Results Any Good? Our efforts to tame fuzzers would have clearly failed had we been unable \nto signi.cantly improve on the baseline. On the other hand, there is plenty of room for improvement: \nour bug discovery curves do not track the theoretical best lines in Figures 3 and 7 for very long. For \nGCC crash bugs, however, our results are almost perfect. Perhaps the best way to interpret our results \nis in terms of the value proposition they create for compiler developers. For example, if a SpiderMonkey \nteam member examines 15 randomly chosen reduced test cases, he or she can expect them to trigger .ve \ndifferent bugs. In contrast, if the developer examines the .rst 15 of our ranked tests, he or she will \nsee 11 distinct bugs: a noticeable improvement. In Section 2 we described a number of ways to compute \ndistances be\u00ad tween test cases. Since we did not know which of these would work, we tried all of them \nindividually and together, with Figures 3 8 showing our best results. Since we did not consider enough \ncase studies to be able to reach a strong conclusion such as fuzzer tam\u00ading should always use Levenshtein \ndistance on test case text and compiler output, this section analyzes the detailed results from our different \ndistance functions, in hopes of reaching some tentative conclusions about which functions are and are \nnot useful. SpiderMonkey and GCC Crash Bugs For these faults, the best distance function to use as the \nbasis for FPF, based on our case studies, is the normalized Levenshtein distance between test cases plus \nnormalized Levenshtein distance between failure outputs. Our tentative recommendation for bugs that (1) \nreduce very well and (2) have compiler-failure outputs is: use normalized Levenshtein distance over test-case \ntext plus compiler-output text, and do not bother with Valgrind output or coverage information. Given \nthat using Levenshtein distance on the test-case text plus compiler output worked so well for both of \nthese bug sets, where all faults had meaningful failure symptoms, we might expect using output or test-case \ntext alone to also perform acceptably. In fact, Levenshtein distance based on test-case text alone (not \nnormalized) performed moderately well for SpiderMonkey, but otherwise the results for these distance \nfunctions were uniformly mediocre at best. For GCC, using compiler output plus C features (Section 2.3) \nperformed nearly as well as the best distance function, suggesting that the essential requirement is \ncompiler output combined with a good representation of the test case, which may not be satis.ed by a \nsimple vectorization: vectorizing test case plus output performed badly for both GCC and SpiderMonkey. \nCoverage-based methods worked fairly well for SpiderMonkey, appearing in six of the top ten functions \nand only two of the worst ten. Interestingly, these best coverage methods for SpiderMonkey all included \nboth line and function coverage. Both coverage-based functions were uniformly mediocre for GCC crashes \n(coverage did not appear in any of the best ten or worst ten methods). For GCC, Valgrind was of little \nvalue, as most failures did not produce any Valgrind output. Memory-safety errors were more common in \nSpiderMonkey, so most test cases did produce Valgrind output; however, for the most part, adding the \ninformation to a distance function still made the function perform worse in the long run. Valgrind output \nalone performed extremely poorly in the long run for both GCC crashes and SpiderMonkey bugs. GCC Crash \nBugs For these bugs, every distance function in\u00adcreased the area under the curve for examining less than \n50 tests by a factor or four or better, compared to the baseline. Clearly there is a signi.cant amount \nof redundancy in the information provided by different functions. All but .ve of the 63 distance functions \nwe used were able to discover all bugs within at most 90 tests: a dramatic im\u00adprovement over the baseline \ns 491 tests. Only Valgrind output alone performed worse than the baseline. The other four poorly perform\u00ading \nmethods all involved using vectorization of the test case, with no additional information beyond Valgrind \noutput and/or test-case output. GCC crash bugs were, however, our easiest target: there are only 11 crash \noutputs and 11 faults. Even so, the problem is not trivial, as the faults and outputs do not correspond \nperfectly two faults have two different outputs, and there are two outputs that are each produced by \ntwo different faults. Failure output alone provides a great deal of information about a majority of the \nfaults, and test-case distance completes the story. SpiderMonkey Bugs The story was less simple for SpiderMonkey \nbugs, where many methods performed poorly and seven methods per\u00ad  Figure 3. SpiderMonkey 1.6 bug discovery \ncurves, .rst 50 tests Figure 4. SpiderMonkey 1.6 bug discovery curves, all tests Figure 5. GCC 4.3.0 \ncrash bug discovery curves, .rst 50 tests Figure 6. GCC 4.3.0 crash bug discovery curves, all tests \nFigure 7. GCC 4.3.0 wrong-code bug discovery curves, .rst 50 Figure 8. GCC 4.3.0 wrong-code bug discovery \ncurves, all tests tests  Figure 9. All bug discovery curves for GCC 4.3.0 wrong-code bugs, sorted by \nincreasing area under the curve if examining only the 50 top-ranked test cases formed worse than the \nbaseline. In this case, compiler output alone did not provide as much direct guidance: there were 300 \ndifferent failure outputs, and only four of 28 faults had a unique identifying output. As a result, while \ncompiler output alone performed very well over the .rst 50 tests (where it was one of the best .ve functions), \nit proved one of the worst functions for .nding all faults, detecting no new faults between the 50th \nand 100th tests ranked. Test-case text by itself performed well for SpiderMonkey with Levenshtein distance, \nor when combined with line coverage, but performed badly as a vectorization without line coverage, appearing \nin six of the worst ten functions. As with GCC crashes, Valgrind output alone performed very badly, requiring \na user to examine 1,506 tests to discover all bugs. Levenshtein-based approaches (whether over test case, \ncompiler output, Valgrind output, or a combination thereof) performed very well over the .rst 50 tests \nexamined. GCC Wrong-Code Bugs Wrong-code bugs in GCC were the trickiest bugs that we faced: their execution \ndoes not provide failure output and, in the expected case where the bug is in a middle end optimizer, \nthe distance between execution of the fault and actual emission of code (and thus exposure of failure) \ncan be quite long. For these bugs, the best method to use for fuzzer taming was less clear. Figures 9 \nand 10 show the performance of all methods that we tried, including a table of results sorted by area \nunder the curve up to 50 tests (Figure 9) and number of test cases to discover all faults (Figure 10). \nIt is clear that code coverage (line or function) is much more valuable here than with crash bugs, though \nLevenshtein distance based on test case alone performs well in the long run (but badly initially). Line \ncoverage is useful for early climb, but eventually function coverage is most useful for discovering all \nbugs. Perhaps most importantly, given the dif.culty of handling GCC wrong-code bugs, all of our methods \nperform better than the baseline in terms of ability to .nd all bugs, and provide a clear advantage over \nthe .rst 50 test cases. We do not wish to overgeneralize from a few case studies, but these results provide \nhope that for dif.cult bugs, if good reduction is possible, the exact choice of distance function used \nin FPF may not be critical. We were disappointed to see that Figures 9 and 10 show no evidence that our \ndomain-speci.c feature detector for C programs is useful for wrong-code bugs; in the tables it appears \nas C-Feature.  4.4 Avoiding Known Faults In Section 2.2 we hypothesized that FPF could be used to avoid \nreports about a set of known bugs; this is accomplished by lowering the rankings of test cases that appear \nto be caused by those bugs. Figure 11 shows, for our SpiderMonkey test cases, an averaged bug discovery \ncurve for the case where half of the bugs were assumed to be already known, and .ve test cases (or fewer, \nif .ve were not available) triggering each of those bugs were used to seed FPF. This experiment models \nthe situation where, in the days or weeks preceding the current fuzzing run, the user has .agged these \ntest cases and does not want to see more test cases triggering the same bugs. The curve is the average \nof 100 discovery curves, each corresponding to a different randomly chosen set of known bugs. The topmost \nbug discovery curve in Figure 11 represents an idealized best case where all test cases corresponding \nto known bugs are removed from the set of test cases to be ranked. The second curve from the top is our \nresult. The third curve from the top is also the average of 100 discovery curves; each corresponds to \nthe case where the .ve (or fewer) test cases for each known bug are discarded instead of being used to \nseed the FPF algorithm, and then the FPF algorithm proceeds normally. This serves as a baseline: our \nresult would have to be considered poor if it could not improve on this. Finally, the bottom curve is \nthe basic baseline where the labeled test cases are again discarded, but then the remaining test cases \nare examined in random order.  As can be seen, our current performance for SpiderMonkey is reasonably \ngood. Analogous results for GCC bugs (not included for reasons of space) were similar, but not quite \nas good for wrong\u00adcode bugs. We speculate that classi.cation, rather than clustering or ranking, might \nbe a better machine-learning approach for this problem if better results are required.  4.5 The Importance \nof Test-Case Reduction Randomly generated test cases are more effective at .nding bugs when they are \nlarge [1]. There are several reasons for this. First, large tests are more likely to bump into implementation \nlimits and software aging effects in the system under test. Second, large tests amortize start-up costs. \nThird, undesirable feature interactions in the system under test are more likely to occur when a test \ncase triggers more behaviors. The obvious drawback of large random test cases is that they contain much \ncontent that is probably unrelated to the bug. They con\u00adsequently induce long executions that are dif.cult \nto debug. Several random testers that have been used in practice, including McKee\u00adman s C compiler fuzzer \n[24] and QuickCheck [6], have included built-in support for greedily reducing the size of failure-inducing \ninputs. Zeller and Hildebrandt [45] generalized and formalized this kind of test-case reduction as Delta \nDebugging. While previous work has assumed that the consumer for reduced test cases is a human, our observation \nis that machine-learning-based methods can greatly bene.t from reduced test cases. First, machine\u00adlearning \nalgorithms can be overwhelmed by noisy inputs: reduced test cases have a vastly improved signal to noise \nratio. Second, a suitably designed test-case reducer has a canonicalizing effect. Figure 12 shows the \ndiscovery curves for the FPF ordering on unreduced JavaScript test cases, accompanied by a table sorted \nby area under the discovery curve for all 2,603 unreduced tests (.rst column after distance function \nname) and also the number of tests required to discover all faults (second column). Note that this is \na different baseline than in previous graphs, as there are no duplicates among the unreduced test cases. \nWhile all methods improve (slightly) on the baseline for .nding all faults, it is dif.cult to consider \nthese approaches acceptable: many methods produce an overall discovery curve that is worse than the baseline. \nWithout test-case reduction, it is essentially impossible to ef.ciently .nd the more obscure SpiderMonkey \nbugs. Based on these poor results, and on the fact that ranking unre\u00adduced test cases (which have much \nlonger feature vectors) is expen\u00adsive, our view is that attempting to tame a fuzzer without the aid of \na solid test-case reducer is inadvisable. The most informative sources of information about root causes \nare rendered useless by overwhelm\u00ading noise. Although we did not create results for unreduced GCC test \ncases that are analogous to those shown in Figure 12 (the line coverage vectors were gigantic and caused \nproblems by .lling up disks), we have no reason to believe the results would have been any better than \nthey were for JavaScript.  4.6 Clustering as an Alternative to Furthest Point First The problem of ranking \ntest cases is not, essentially, a clustering problem. On the other hand, if our goal were simply to .nd \na single test case triggering each fault, an obvious approach would be to cluster the test cases and \nthen select a single test from each cluster, as in previous approaches to the problem [9, 28]. The FPF \nalgorithm we use is itself based on the idea of approximating optimal clusters [10]; we simply ignore \nthe clustering aspect and use only the ranking information. Our initial approach to taming compiler fuzzers \nwas to start with the feature vectors described in Section 2.3 and then, instead of ranking test cases \nusing FPF, use X-means [27] to cluster test cases. A set of clusters does not itself provide a user with \na set of representative test cases, however, nor a ranking (since not all clusters are considered equally \nlikely to represent true categories). Our approach therefore followed clustering by selecting the member \nof each cluster closest to its center as that cluster s representative test. We ranked each test by the \nquality of its cluster, as measured by compactness (whether the distance between tests in the cluster \nis small) and isolation (whether the distance to tests outside the cluster is large) [40]. This approach \nappeared to be promising as it improved considerably on the baseline bug discovery curves (Figures 3 \n8). We next investigated the possibility of independently clustering different feature vectors, then \nmerging the representatives from these clusterings [36], and ranking highest those representatives appearing \nin clusterings based on multiple feature sets. This produced better results than our single-vector method, \nand it was also more ef.cient, as it did not require the use of large vectors combining multiple features. \nThis approach is essentially a completely unsupervised variation (with the addition of some recent advances \nin clustering) of earlier approaches to clustering test cases that trigger the same bug [9]. Our approach \nis unsupervised because we exploit test\u00ad case reduction as a way to select relevant features, rather \nthan relying on the previous approaches assumption that features useful in predicting failure or success \nwould also distinguish failures from each other. However, in comparison to FPF for all three of our case \nstudies, clustering was (1) signi.cantly more complex to use, (2) more com\u00adputationally expensive, and \n(3) most importantly, less effective. The additional complexity of clustering should be clear from our \ndescrip\u00adtion of the algorithm, which omits details such as how we compute normalized isolation and compactness, \nthe algorithm for merging multiple views, and (especially) the wide range of parameters that can be supplied \nto the underlying X-means algorithm. Table 1 compares runtimes, with the time for FPF including the full \nend-to-end effort of producing a ranking and the clustering col\u00adumn only showing the time for computing \nclusters using X-means, with settings that are a compromise between speed and effectiveness. (Increased \ncomputation time to produce more accurate clusters in our experience had diminishing returns after this \npoint, which allowed up to 40 splits and a maximum of 300 clusters.) Comput\u00ading isolation and compactness \nof clusters and merging clusters to produce a ranking based on multiple feature vectors adds additional \nsigni.cant overhead to the X-means time shown, if multiple clus\u00adterings are combined, but we have not \nmeasured this time because our implementation is highly unoptimized Python (while X-means  Time (s) \nProgram / Feature FPF Clustering Figures SpiderMonkey / Valgrind 8.27 23.68 SpiderMonkey / output 8.38 \n46.71 SpiderMonkey / test 8.12 94.26 SpiderMonkey / funccov 9.56 227.78 3, 4 SpiderMonkey / linecov 48.29 \n1,594.04 3, 4 SpiderMonkey / Lev. test+output 998.21 N/A 3, 4 GCC crash bugs / output 0.08 0.71 GCC crash \nbugs / Valgrind 0.09 0.75 5, 6 GCC crash bugs / C-Feature 0.10 1.95 5, 6 GCC crash bugs / test 0.14 15.12 \n5, 6 GCC crash bugs / funccov 1.37 162.22 GCC crash bugs / linecov 18.70 2,021.08 GCC crash bugs / Lev. \ntest+output 75.07 N/A 5, 6 GCC wrong-code bugs / C-Feature 0.49 4.26 7, 8 GCC wrong-code bugs / test \n0.72 67.72 GCC wrong-code bugs / funccov 4.12 1,046.07 7, 8 GCC wrong-code bugs / linecov 60.60 7,127.42 \nGCC wrong-code bugs / Lev. test 667.21 N/A Table 1. Runtimes for FPF versus clustering is a widely \nused tool written in C). Because the isolation and com\u00adpactness computations require many pairwise distance \nresults, an ef.cient implementation should be approximately equal in time to running FPF. The .nal column \nof the table lists the .gures in this paper that show a curve based on the indicated results. If a curve \nrelies on multiple clusterings, its generation time is (at least) the sum of the clustering times for \neach component. Note that because X-means expects inputs in vector form, we were unable to apply our \ndirect Levenshtein-distance approach with clustering, but we include some runtimes for FPF Levenshtein \nto provide a comparison. That clustering is more expensive and complex than FPF is not surprising; clustering \nhas to perform the additional work of computing clusters, rather than simply ranking items by distance. \nThat FPF produces considerably better discovery curves, as shown in Figures 3 8, is surprising. The comparative \nineffectiveness of clustering is twofold: the discovery curves do not climb as quickly as with FPF, and \n(perhaps even more critically) clustering does not ever .nd all the faults in many cases. In general, \nfor almost all feature sets, clustering over those same features was worse than applying FPF to those \nfeatures. The bad performance of clustering was particularly clear for GCC wrong-code bugs: Figure 13 \nshows all discovery curves for GCC wrong-code, with clustering results shown in gray. Clustering at its \nbest missed 15 or more bugs, and in many cases performed much worse than the baseline, generating a small \nnumber of clusters that were not represented by distinct faults. In fact, the few clustering results \nthat manage to discover 20 faults also did so more slowly than the baseline curve. While GCC wrong\u00adcode \nclustering was particularly bad, clustering also always missed at least three bugs for SpiderMonkey. \nOur hypothesis as to why FPF performs so much better than clustering is that the nature of fuzzing results, \nwith a long tail of outliers, is a mismatch for clustering algorithm assumptions. FPF is not forced to \nuse any assumptions about the size of clusters, and so is not confused by the many single-instance clusters. \nA minor point supporting our hypothesis is that the rank ordering of clustering effectiveness matched \nthat of the size of the tail for each set of faults: GCC crash results were good but not optimal, SpiderMonkey \nresults were poor, and GCC wrong-code results were extremely bad. 5. Related Work A great deal of research \nrelated to fuzzer taming exists, and some related areas such as fault localization are too large for \nus to do more than summarize the high points. Relating Test Cases to Faults Previous work focusing on \nthe core problem of taming sets of redundant test cases differs from ours in a few key ways. The differences \nrelate to our choice of primary algorithm, our reliance on unsupervised methods, and our focus on randomly \ngenerated test cases. First, the primary method used was typically clustering, as in the work of Francis \net al. [9] and Podgurski et al. [28], which at .rst appears to re.ect the core problem of grouping test \ncases into equivalence classes by underlying fault. However, in practice the user of a fuzzer does not \nusually care about the tests in a cluster, but only about .nding at least one example from each set with \nno particular desire that it is a perfectly representative example. The core problem we address is therefore \nbetter considered as one of multiple output identi.cation [8] or rare category detection [8, 40], given \nthat many faults will be found by a single test case out of thousands. This insight guides our decision \nto provide the .rst evaluation in terms of discovery curves (the most direct measure of fuzzer taming \ncapability we know of) for this problem. Our results suggest that this difference in focus is also algorithmically \nuseful, as clustering was less effective than our (novel, to our knowledge) choice of FPF. One caveat \nis that, as in the work of Jones et al. on debugging in parallel [15], clusters may not be directly useful \nto users, but might assist fault localization algorithms. Jones et al. provide an evaluation in terms \nof a model of debugging effort, which combines clustering effectiveness with fault-localization effectiveness. \nThis provides an interesting contrast to our discovery curves: it relies on more assumptions about users \nwork.ow and debugging process and provides less direct information about the effectiveness of taming \nitself. In our experience, suf.ciently reduced test cases make localization easy enough for many compiler \nbugs that discovery is the more important problem. Unfortunately, it is hard to compare results: cost-model \nresults are only reported for SPACE, a program with only around 6,200 LOC, and their tests included not \nonly random tests from a simple generator but 3,585 human-generated tests. In the event that clusters \nare needed, FPF results for any k can be transformed into k clusters with certain optimality bounds for \nthe chosen distance function [10]. Second, our approach is completely unsupervised. There is no expectation \nthat users will examine clusters, add rules, or intervene in the process. We therefore use test-case \nreduction for feature selection, rather than basing it on classifying test cases as successful or failing \n[9, 28]. Because fuzzing results follow a power law, many faults will be represented by far too few tests \nfor a good classi.er to include their key features; this is a classic and extreme case of class imbalance \nin machine learning. While bug slippage is a problem, reduction remains highly effective for feature \nselection, in that the features selected are correct for the reduced test cases, essentially by the de.nition \nof test-case reduction.  Finally, our expected use case and experimental results are based on a large \nset of failures produced by large-scale random testing for complex programming languages implemented \nin large, com\u00adplex, modern compilers. Most previous results in failure clustering used human-reported \nfailures or human-created regression tests (e.g., GCC regression tests [9, 28]), which are essentially \ndiffer\u00ad ent in character from the failures produced by large-scale fuzzing, and/or concerned much smaller \nprograms with much simpler in\u00adput domains [15, 23], i.e., examples from the Siemens suite. Li\u00ad blit et \nal. [22] in contrast directly addressed scalability by using 32,000 random inputs (though not from a \npre-existing industrial\u00adstrength fuzzer for a complex language) and larger programs (up to 56 KLOC), \nand noted that they saw highly varying rates of fail\u00adure for different bugs. Their work addresses a somewhat \ndifferent problem than ours that of isolating bugs via sampled predicate values, rather than straightforward \nranking of test cases for user examination and did not include any systems as large as GCC or SpiderMonkey. \nDistance Functions for Executions and Programs Liu and Han s work [23], like ours, focuses less on a \nparticular clustering method and proposes that the core problem in taming large test suites is that of \nembedding test cases in a metric space that has good correlation with underlying fault causes. They propose \nto compute distance by .rst applying fault localization methods to executions, then using distance over \nlocalization results rather than over the traces themselves. We propose that the reduction of random \ntest cases essentially localizes the test cases themselves, allowing us to directly compute proximity \nover test cases while exhibiting the good correlation with underlying cause that Liu and Han seek to \nachieve by applying a fault-localization technique. Reduction has advantages over localization in that \nreduction methods are more commonly employed and do not require storing or even capturing or sampling \ninformation about coverage, predicates, or other metrics for passing test cases. Liu and Han show that \ndistance based on localization algorithms better captures fault cause than distance over raw traces, \nbut they do not provide discovery curves or a detailed clustering evaluation. They provide correlation \nresults only over the Siemens suite s small subjects and test case sets. More generally, the problems \nof distance functions over ex\u00adecutions and test cases [5, 11, 23, 30, 39] and programs them\u00ad selves [4, \n35, 41] have typically been seen as essentially different problems. While this is true for many investigations \ngeneralized program understanding and fault localization on the one hand, and plagiarism detection, merging \nof program edits, code clone, or mal\u00adware detection on the other the difference collapses when we consider \nthat every program compiled is an input to some other program. A program is therefore both a program \nand a test input, which induces an execution of another program. Distance between (compiled) programs, \ntherefore, is a distance between executions. We are the .rst, to our knowledge, to essentially erase \nthe distinction between a metric space for programs and a metric space for execu\u00adtions, mixing the two \nconcepts as needed. Moreover, we believe that our work addresses some of the concerns noted in fault-localization \nefforts based on execution distances (e.g., poor results compared to other methods [16]), in that distance \nfunctions should perform much better on executions of reduced programs, due to the power of feature selection, \nand distances over programs (highly structured and potentially very informative inputs) can complement \nexecution\u00adbased distance functions. Fault Localization Our work shares a common ultimate goal with fault \nlocalization work in general [5, 7, 11, 16, 17, 21, 22, 30] and speci.cally for compilers [43]: reducing \nthe cost of manual debugging. We differ substantially in that we focus our methods and evaluation on \nthe narrow problem of helping the users of fuzzers deal with the overwhelming amount of data that a modern \nfuzzer can produce when applied to a compiler. As suggested by Liu and Han [23], Jones et al. [15], and \nothers, localization may support fuzzer taming and fuzzer taming may support localization. As part of \nour future work, we propose to make use of vectors based on localization information to determine if, \neven after reduction, localization can improve bug discovery. A central question is whether the payoff \nfrom keeping summaries of successful executions (a requirement for many fault localizations) provides \nsuf.cient improvement to pay for its overhead in reduced fuzzing throughput. 6. Conclusion Random testing, \nor fuzzing, has emerged as an important way to test compilers and language runtimes. Despite their advantages, \nhowever, fuzzers create a unique set of challenges when compared to other testing methods. First, they \nindiscriminately and repeatedly .nd test cases triggering bugs that have already been found and that \nmay not be economical to .x in the short term. Second, fuzzers tend to trigger some bugs far more often \nthan others, creating needle-in-the\u00adhaystack problems for engineers who are triaging failure-inducing \noutputs generated by fuzzers. Our contribution is to tame a fuzzer by adding a tool to the back end of \nthe random-testing work.ow; it uses techniques from machine learning to rank test cases in such a way \nthat interesting tests are likely to be highly ranked. By analogy to the way people use ranked outputs \nfrom static analyses, we expect fuzzer users to inspect a small fraction of highly ranked outputs, trusting \nthat lower\u00adranked test cases are not as interesting. If our rankings are good, fuzzer users will get \nmost of the bene.t of inspecting every failure\u00adinducing test case discovered by the fuzzer for a fraction \nof the effort. For example, a user inspecting test cases for SpiderMonkey 1.6 in our ranked order will \nsee all 28 bugs found during our fuzzing run 4.6\u00d7 faster than will a user inspecting test cases in random \norder. A user inspecting test cases that cause GCC 4.3.0 to emit incorrect object code will see all 35 \nbugs 2.6\u00d7 faster than one inspecting tests in random order. The improvement for test cases that cause \nGCC 4.3.0 to crash is even higher: 32\u00d7, with all 11 bugs exposed by only 15 test cases. Acknowledgments \nWe thank Michael Hicks, Robby Findler, and the anonymous PLDI 13 reviewers for their comments on drafts \nof this paper; Suresh Venkatasubramanian for nudging us towards the furthest point .rst technique; James \nA. Jones for providing useful early feedback; and Google for a research award supporting Yang Chen. A \nportion of this work was funded by NSF grants CCF-1217824 and CCF-1054786. References [1] James H. Andrews, \nAlex Groce, Melissa Weston, and Ru-Gang Xu. Random test run length and effectiveness. In Proc. ASE, pages \n19 28, September 2008. [2] Abhishek Arya and Cris Neckar. Fuzzing for security, April 2012. http://blog.chromium.org/2012/04/ \n fuzzing-for-security.html.  [3] Mariano Ceccato, Alessandro Marchetto, Leonardo Mariani, Cu D. Nguyen, \nand Paolo Tonella. An empirical study about the effectiveness of debugging when random test cases are \nused. In Proc. ICSE, pages 452 462, June 2012.  [4] Silvio Cesare and Yang Xiang. Malware variant detection \nusing similarity search over sets of control .ow graphs. In Proc. TRUSTCOM, pages 181 189, November 2011. \n [5] Sagar Chaki, Alex Groce, and Ofer Strichman. Explaining abstract counterexamples. In Proc. FSE, \npages 73 82, 2004. [6] Koen Claessen and John Hughes. QuickCheck: a lightweight tool for random testing \nof Haskell programs. In Proc. ICFP, pages 268 279, 2000. [7] Holger Cleve and Andreas Zeller. Locating \ncauses of program failures. In Proc. ICSE, pages 342 351, May 2005. [8] Shai Fine and Yishay Mansour. \nActive sampling for multiple output identi.cation. Machine Learning, 69(2 3):213 228, 2007. [9] Patrick \nFrancis, David Leon, Melinda Minch, and Andy Podgurski. Tree-based methods for classifying software failures. \nIn Proc. ISSRE, pages 451 462, November 2004. [10] Teo.lo F. Gonzalez. Clustering to minimize the maximum \nintercluster distance. Theoretical Computer Science, 38:293 306, 1985. [11] Alex Groce. Error explanation \nwith distance metrics. In Proc. TACAS, pages 108 122, March 2004. [12] Alex Groce, Gerard Holzmann, and \nRajeev Joshi. Randomized differential testing as a prelude to formal veri.cation. In Proc. ICSE, pages \n621 631, May 2007. [13] Alex Groce, Chaoqiang Zhang, Eric Eide, Yang Chen, and John Regehr. Swarm testing. \nIn Proc. ISSTA, pages 78 88, July 2012. [14] Christian Holler, Kim Herzig, and Andreas Zeller. Fuzzing \nwith code fragments. In Proc. USENIX Security, pages 445 458, August 2012. [15] James A. Jones, James \nF. Bowring, and Mary Jean Harrold. Debugging in parallel. In Proc. ISSTA, pages 16 26, July 2007. [16] \nJames A. Jones and Mary Jean Harrold. Empirical evaluation of the Tarantula automatic fault-localization \ntechnique. In Proc. ASE, pages 273 282, November 2005. [17] James A. Jones, Mary Jean Harrold, and John \nStasko. Visualization of test information to assist fault localization. In Proc. ICSE, pages 467 477, \nMay 2002. [18] Yungbum Jung, Jaehwang Kim, Jaeho Shin, and Kwangkeun Yi. Taming false alarms from a domain-unaware \nC analyzer by a Bayesian statistical post analysis. In Proc. SAS, pages 203 217, September 2005. [19] \nTed Kremenek and Dawson Engler. Z-ranking: using statistical analysis to counter the impact of static \nanalysis approximations. In Proc. SAS, pages 295 315, June 2003. [20] Vladimir I. Levenshtein. Binary \ncodes capable of correcting deletions, insertions, and reversals. Soviet Physics Doklady, 10:707 710, \n1966. [21] Ben Liblit, Alex Aiken, Alice X. Zheng, and Michael I. Jordan. Bug isolation via remote program \nsampling. In Proc. PLDI, pages 141 154, June 2003. [22] Ben Liblit, Mayur Naik, Alice X. Zheng, Alex \nAiken, and Michael I. Jordan. Scalable statistical bug isolation. In Proc. PLDI, pages 15 26, June 2005. \n[23] Chao Liu and Jiawei Han. Failure proximity: a fault localization-based approach. In Proc. FSE, pages \n46 56, November 2006. [24] William M. McKeeman. Differential testing for software. Digital Technical \nJournal, 10(1):100 107, December 1998. [25] Nicholas Nethercote and Julian Seward. Valgrind: a framework \nfor heavyweight dynamic binary instrumentation. In Proc. PLDI, pages 89 100, June 2007. [26] Dan Pelleg \nand Andrew Moore. Active learning for anomaly and rare-category detection. In Advances in Neural Information \nProcessing Systems 18, December 2004. [27] Dan Pelleg and Andrew W. Moore. X-means: Extending K-means \nwith ef.cient estimation of the number of clusters. In Proc. ICML, pages 727 734, June/July 2000. [28] \nAndy Podgurski, David Leon, Patrick Francis, Wes Masri, Melinda Minch, Jiayang Sun, and Bin Wang. Automated \nsupport for classifying software failure reports. In Proc. ICSE, pages 465 475, May 2003. [29] John Regehr, \nYang Chen, Pascal Cuoq, Eric Eide, Chucky Ellison, and Xuejun Yang. Test-case reduction for C compiler \nbugs. In Proc. PLDI, pages 335 346, June 2012. [30] Manos Renieris and Steven Reiss. Fault localization \nwith nearest neighbor queries. In Proc. ASE, pages 30 39, October 2003. [31] Jesse Ruderman. Introducing \njsfunfuzz. http://www.squarefree. com/2007/08/02/introducing-jsfunfuzz/. [32] Jesse Ruderman. Mozilla \nbug 349611. https://bugzilla.mozilla.org/show_bug.cgi?id=349611 (A meta-bug containing all bugs found \nusing jsfunfuzz.). [33] Jesse Ruderman. How my DOM fuzzer ignores known bugs, 2010. http://www.squarefree.com/2010/11/21/ \nhow-my-dom-fuzzer-ignores-known-bugs/. [34] G. Salton, A. Wong, and C. S. Yang. A vector space model \nfor automatic indexing. CACM, 18(11):613 620, November 1975. [35] Saul Schleimer, Daniel S. Wilkerson, \nand Alex Aiken. Winnowing: local algorithms for document .ngerprinting. In Proc. SIGMOD, pages 76 85, \nJune 2003. [36] Alexander Strehl and Joydeep Ghosh. Cluster ensembles a knowledge reuse framework for \ncombining multiple partitions. The Journal of Machine Learning Research, 3:583 617, 2003. [37] Chengnian \nSun, David Lo, Siau-Cheng Khoo, and Jing Jiang. Towards more accurate retrieval of duplicate bug reports. \nIn Proc. ASE, pages 253 262, November 2011. [38] Chengnian Sun, David Lo, Xiaoyin Wang, Jing Jiang, and \nSiau-Cheng Khoo. A discriminative model approach for accurate duplicate bug report retrieval. In Proc. \nICSE, pages 45 54, May 2010. [39] Vipindeep Vangala, Jacek Czerwonka, and Phani Talluri. Test case comparison \nand clustering using program pro.les and static execution. In Proc. ESEC/FSE, pages 293 294, August 2009. \n[40] Pavan Vatturi and Weng-Keen Wong. Category detection using hierarchical mean shift. In Proc. KDD, \npages 847 856, June/July 2009. [41] Andrew Walenstein, Mohammad El-Ramly, James R. Cordy, William S. \nEvans, Kiarash Mahdavi, Markus Pizka, Ganesan Ramalingam, and J\u00fcrgen Wolff von Gudenberg. Similarity \nin programs. In Duplication, Redundancy, and Similarity in Software, Dagstuhl Seminar Proceedings, July \n2006. [42] Xiaoyin Wang, Lu Zhang, Tao Xie, John Anvik, and Jiasu Sun. An approach to detecting duplicate \nbug reports using natural language and execution information. In Proc. ICSE, pages 461 470, May 2008. \n[43] David B. Whalley. Automatic isolation of compiler errors. TOPLAS, 16(5):1648 1659, September 1994. \n[44] Xuejun Yang, Yang Chen, Eric Eide, and John Regehr. Finding and understanding bugs in C compilers. \nIn Proc. PLDI, pages 283 294, June 2011. [45] Andreas Zeller and Ralf Hildebrandt. Simplifying and isolating \nfailure-inducing input. IEEE TSE, 28(2):183 200, February 2002.   \n\t\t\t", "proc_id": "2491956", "abstract": "<p>Aggressive random testing tools (\"fuzzers\") are impressively effective at finding compiler bugs. For example, a single test-case generator has resulted in more than 1,700 bugs reported for a single JavaScript engine. However, fuzzers can be frustrating to use: they indiscriminately and repeatedly find bugs that may not be severe enough to fix right away. Currently, users filter out undesirable test cases using ad hoc methods such as disallowing problematic features in tests and grepping test results. This paper formulates and addresses the fuzzer taming problem: given a potentially large number of random test cases that trigger failures, order them such that diverse, interesting test cases are highly ranked. Our evaluation shows our ability to solve the fuzzer taming problem for 3,799 test cases triggering 46 bugs in a C compiler and 2,603 test cases triggering 28 bugs in a JavaScript engine.</p>", "authors": [{"name": "Yang Chen", "author_profile_id": "81444601555", "affiliation": "University of Utah, Salt Lake City, UT, USA", "person_id": "P4148993", "email_address": "chenyang@cs.utah.edu", "orcid_id": ""}, {"name": "Alex Groce", "author_profile_id": "81100190717", "affiliation": "Oregon State University, Corvallis, OR, USA", "person_id": "P4148994", "email_address": "agroce@gmail.com", "orcid_id": ""}, {"name": "Chaoqiang Zhang", "author_profile_id": "81503687293", "affiliation": "Oregon State University, Corvallis, OR, USA", "person_id": "P4148995", "email_address": "zhangch@onid.orst.edu", "orcid_id": ""}, {"name": "Weng-Keen Wong", "author_profile_id": "81351594350", "affiliation": "Oregon State University, Corvallis, OR, USA", "person_id": "P4148996", "email_address": "wong@eecs.oregonstate.edu", "orcid_id": ""}, {"name": "Xiaoli Fern", "author_profile_id": "81100360432", "affiliation": "Oregon State University, Corvallis, OR, USA", "person_id": "P4148997", "email_address": "xfern@eecs.oregonstate.edu", "orcid_id": ""}, {"name": "Eric Eide", "author_profile_id": "81341490043", "affiliation": "University of Utah, Salt Lake City, UT, USA", "person_id": "P4148998", "email_address": "eeide@cs.utah.edu", "orcid_id": ""}, {"name": "John Regehr", "author_profile_id": "81100459621", "affiliation": "University of Utah, Salt Lake City, UT, USA", "person_id": "P4148999", "email_address": "regehr@cs.utah.edu", "orcid_id": ""}], "doi_number": "10.1145/2491956.2462173", "year": "2013", "article_id": "2462173", "conference": "PLDI", "title": "Taming compiler fuzzers", "url": "http://dl.acm.org/citation.cfm?id=2462173"}