{"article_publication_date": "06-16-2013", "fulltext": "\n Monadic Abstract Interpreters Ilya Sergey Dominique Devriese Matthew Might IMDEA Software Institute, \nSpain iMinds DistriNet, KU Leuven, Belgium University of Utah, USA ilya.sergey@imdea.org dominique.devriese@cs.kuleuven.be \nmight@cs.utah.edu Jan Midtgaard David Darais Dave Clarke Frank Piessens Aarhus University, Denmark Harvard \nUniversity, USA iMinds DistriNet, KU Leuven, Belgium jmi@cs.au.dk darais@seas.harvard.edu {.rstname.lastname}@cs.kuleuven.be \n Abstract Recent developments in the systematic construction of abstract interpreters hinted at the possibility \nof a broad uni.cation of concepts in static analysis. We deliver that uni.cation by show\u00ading context-sensitivity, \npolyvariance, .ow-sensitivity, reachability\u00adpruning, heap-cloning and cardinality-bounding to be independent \nof any particular semantics. Monads become the unifying agent be\u00adtween these concepts and between semantics. \nFor instance, by plug\u00adging the same context-insensitivity monad into a monadically\u00adparameterized semantics \nfor Java or for the lambda calculus, it yields the expected context-insensitive analysis. To achieve \nthis uni.cation, we develop a systematic method for transforming a concrete semantics into a monadically-parameterized \nabstract machine. Changing the monad changes the behavior of the machine. By changing the monad, we recover \na spectrum of machines from the original concrete semantics to a monovariant, .ow-and context-insensitive \nstatic analysis with a singly-threaded heap and weak updates. The monadic parameterization also suggests \nan abstraction over the ubiquitous monotone .xed-point computation found in static analysis. This abstraction \nmakes it straightforward to instrument an analysis with high-level strategies for improving precision \nand performance, such as abstract garbage collection and widening. While the paper itself runs the development \nfor continuation\u00adpassing style, our generic implementation replays it for direct-style lambda-calculus \nand Featherweight Java to support generality. Categories and Subject Descriptors F.3.2 [Logics and Meanings \nof Programs]: Semantics of Programming Languages Program analysis, Operational semantics General Terms \nLanguages, Theory Keywords abstract machines, abstract interpretation, monads, op\u00aderational semantics, \ncollecting semantics, abstract garbage collec\u00adtion, interpreters Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. PLDI 13, June 16 19, 2013, Seattle, Washington, \nUSA. Copyright &#38;#169; 2013 ACM 978-1-4503-2014-6/13/06...$15.00. 1. Introduction Recent work on systematizing \nthe construction of abstract inter\u00adpreters [15, 23, 24] hints at the possibility of broad theoretical \nuni\u00ad.cation within static analysis. Van Horn and Might [23] sketch a method for abstracting abstract \nmachines into static analyzers by bounding the store of the abstract machine: once the store is bounded, \nthe abstraction and then the analysis follows. But, bounding the store is an act of design of human interven\u00adtion. \nHow a designer bounds the store immediately determines clas\u00adsical properties of the analysis such as \nits context-sensitivity and its polyvariance. While not directly expressed in terms of a bound on the \nstore, other classical properties are also related to the abstrac\u00adtion and handling of the store, including \nheap-cloning, reachability\u00adpruning and cardinality-bounding. But, other properties, such as .ow-sensitivity, \npath-sensitvity and some kinds of widening, have little to do with the store, and more to with the (re-)interpretation \nof the abstract semantics during analysis. Fortunately, there is a construct that encompasses all of \nthese concerns: the monad. Monads were originally adapted to program\u00adming languages to provide a durable \nabstraction for mutation in a purely functional language. As such, expressing a semantics monadically \nallows the monad to fully veil the details of interact\u00ading with a store. Yet monads have always provided \nmore than just a means for hiding effects in a purely functional manner: they also allow a near-complete \nreinterpretation of computations expressed monadically, e.g., the instant and powerful non-determinism \nof the list monad. This semantic re.ection in precisely the right dimen\u00adsions allows monads to encapsulate \na variety of concepts in static analysis. The payoff of this realization is immediate: we can monadically \nrefactor semantics for languages as diverse as the lambda calculus and Java, yet de.ne notions like context-sensitivity \nfor both at the same time, with the same monad. Our presentation details every step of the monadic refactoring \nfor continuation-passing style lambda calculus, and then develops the monadic parameters that induce \nstatic analyzers. The imple\u00admentation of the approach in the accompanying code repository replays the \nsame monadic refactoring for a direct-style lambda cal\u00adculus and for Featherweight Java. The monads remain \nthe same. We have chosen Haskell in lieu of formal mathematics for the presentation for two reasons: \n(1) Haskell is directly executable, and (2) Haskell has concise, convenient and readable syntax for ex\u00adpressing \nmonads the central actor in our work. Our fundamental results are no more restricted to Haskell than \nmonads are restricted to Haskell.  1.1 Overview Van Horn and Might s systematic abstraction relies on \nbreaking down recursive structure in the concrete state-space and threading that structure through a \nstore. This refactoring of the state-space imposes a corresponding store-passing-style transformation \non the semantics. Were we writing an interpreter in Haskell rather than a for\u00admal semantics, we would \nrecognize store-passing style as an anti\u00adpattern: in Haskell, monads are the right generalization for \ncaptur\u00ading artifacts like a store in a purely functional manner. When we apply a monadic transformation \nin lieu of a store-passing transfor\u00admation to a concrete semantics, the resulting monad erupts as the \ncatalyst for unifying what appeared as ad hoc choices in the design of classical static analyses. Even \nthe nondeterminism that arises during abstraction of an operational semantics can be captured, ex\u00adplained \nand throttled entirely monadically. By delegating interaction with the store into a monad, the monad \ndetermines not only the polyvariance and context-sensitivity of the analysis, but also the degree to \nwhich the analysis prunes the abstract heap based on reachability (abstract garbage collection) and bounds \nthe cardinality of abstract addresses for shape analysis. In fact, it suggests a refactoring of the traditional \n.xed-point iter\u00adation such that path-sensitivity and .ow-sensitivity also fall out as natural parameters. \nOur monadic abstraction of the semantics orthogonalizes the classic dimensions of static analysis, independent \nof the speci.c semantics in use. We illustrate a systematic method for transforming a concrete semantics \ninto a monadically-parameterized machine, comprising both concrete and abstract interpretation, such \nthat the monad de\u00adtermines the classical properties of an analysis. As in recent work on abstracting \nabstract machines [23], our semantic transformation implicitly utilizes the techniques of Danvy et al. \n[1, 2, 7] in order to calculate an abstract1 machine equivalent to the concrete semantics. It diverges \nwith this line of work by applying a monadic-normal\u00adform transformation [6] (instead of a store-passing-style \ntransfor\u00admation) to the rules for the machine.  1.2 Contributions The central theoretical contribution \nof the paper is identifying and employing monads as a mechanism to abstract over the fundamental characteristics \nof the analysis.  The central practical contribution is an executable proof-of\u00adconcept implementation \nof the described decomposition for a series of calculi.2  We decouple the interpretation of the semantics \nfrom a mono\u00adtonic .xed-point computation, which makes it possible to de.ne analysis widening strategies \nindependently of the semantics and of the analysis other parameters.  We illustrate degrees of freedom \nwhen constructing the analysis using our framework and show that components implementing non-deterministic \ntransitions, polyvariance and abstract count\u00ading are semantics-independent and can be reused for different \ncalculi (e.g., Java and the lambda calculus) and analysis fami\u00adlies.  1Abstract in the sense that it \nmodels the salient intensional behavior of a program. 2The implementation is available for the lambda-calculus \n(both in the form of CPS [15]and CESK-machine [23]) and Featherweight Java [19]: http://github.com/ilyasergey/monadic-cfa \n v . Var is a set of identi.ers lam . Lam ::= (. (v1 . . . vn) call) f,\u00e6 . AExp = Var + Lam call . Call \n::= (f \u00e61 . . . \u00e6n) + Exit . . S = Call \u00d7 Env . . Env = Var . D d . D = Clo clo . Clo = Lam \u00d7 Env Figure \n1: A grammar for CPS and a concrete state-space. 2. Setting the Scene: Analyzing Continuation-Passing \nStyle We start our discovery of the monadic refactoring process with a minimalist higher-order language: \ncontinuation-passing style .\u00adcalculus (CPS). We will apply the systematic abstraction process as described \nby Van Horn and Might [23] in full to CPS. After\u00adward, we ll modify the process by transforming the semantics \ninto monadic normal form after the store-passing transformation. Thus, all interaction with the store \nwill pass through a monad. Because interaction with the store is central to describing facets of mod\u00adern \n.ow analysis, such as context-sensitivity and heap-cloning, we will be able to describe these facets \nindependently of a particular semantics. In CPS (Figure 1), the lambda calculus is partitioned into two \nworlds: call sites and atomic expressions. Atomic expressions are lambda terms and variable references. \nCall sites encode the appli\u00adcation of a function to arguments. A classical abstract machine for CPS [9] \nneeds only two components in its state-space S see Fig\u00adure 1 a control component (Call)and an environment \n(Env). The domain D contains denotable values. CPS is so simple that there is only one kind of denotable \nvalue closures. The injector I : Call . Smaps a program into this state-space: I(call)= (call,[]), In \nCPS, there is only one rule to describe the transition relation, (.). S\u00d7 S. We don t write index ranges \nexplicitly for series of meta-expressions, assuming i = 1..n is obvious form the context. ([[(f \u00e61 . \n. . \u00e6n)] ,.). (call,. '' ), where ([[(. (v1 . . . vn) call)] ,. ' )= A(f,.) di = A(\u00e6i,.) . '' = . ' [vi \n. di], where the evaluator A : AExp\u00d7 Env . Clo evaluates an atomic expression: A(v,.)= .(v) A(lam,.)= \n(lam,.). 2.1 Attempting structural abstraction A structural abstraction carries abstraction component-wise \nacross the state-space and then lifts naturally over internal domains. How\u00adever, a structural abstraction \nof the state-space for CPS yields: S = Env Env = D Call \u00d7 (Var . ( D = Clo) = Env. P((Clo Lam \u00d7 ( (A \nstructural abstraction preserves mutual recursion between clo\u00adsures and environments, and with it, the \nunboundedness of the ab\u00adstract state-space. Since our goal with this abstraction was a .nite abstract \nstate-space (hence a trivially computable abstract seman\u00adtics), structural abstraction alone is insuf.cient. \n  2.2 Cutting recursion with a store To arrive at a .nite state-space, the systematic method detailed \nin Abstracting Abstract Machines [23] calls for transforming the abstract machine into store-passing \nstyle, which cuts the recursive knot between environments and closures by introducing addresses. With \nthe introduction of a store, s . Store = Addr . D, the store-passing transform produces the following \nstate-space: . . S = Call \u00d7 Env \u00d7 Store . . Env = Var . Addr d . D = Clo clo . Clo = Lam \u00d7 Env a . Addr \nis an in.nite set of addresses. The modi.cation of the transition relation is straightforward: . '' ' \n([ (f \u00e61 . . . \u00e6n)] ,., s). (call,. ,s ), where ([[(. (v1 . . . vn) call)] ,. ' )= A(f,., s) di = A(\u00e6i,., \ns) ai = alloc(vi,.) '' ' ' . = . [vi . ai] s = s[ai . di]. The evaluator A : AExp \u00d7 Env \u00d7 Store . D is \nmodi.ed to take the store as an additional argument: A(v,., s)= s(.(v)) A(lam,., s)= (lam,.). And, the \n(presently opaque) address-allocator alloc : Var \u00d7 S . Addr yields a fresh address for each variable. \n 2.3 A second attempt at abstraction With the recursion sliced from the state-space by the store-passing \ntransformation, a structural abstraction succeeds in producing a .nite abstract state-space: . . S = \nCall \u00d7 Env(\u00d7 T Store . . (= Addr Env Var . a s . T= aD) Store Addr . P( d . D = Clo ( dClo Lam \u00d7 ( \n clo . (= Env Addr is a .nite set of abstract addresses, a . a and it induces a straightforward abstract \ntransition relation: . '' ' ([ (f \u00e61 . . . \u00e6n ., . (call, , ), if )] , s) . s ([[(. (v1 . . . vn) call)] \n, ). A(f,., s ) . ' ., a di . A(\u00e6i, s) ai = alloc(vi,. ) '' ' ' . = . [vi . a i] s = s . [ ai . {d \ni}]. Branching to every possible abstract closure introduces a subtle nondeterminism. Naturally, the \nabstract argument evaluator wraps closures as singletons, but looks up variables as in the concrete version: \n A(v, s) .(v)) ., = .)} . ., = s( A(lam, s) {(lam, The join on the abstract store allows each address \nin the .nite set of abstract addresses to soundly represent multiple concrete addresses: s . ' = a. a). \n' ( s . s( s a). And, it should now be clear that the abstract allocator a alloc : S . aVar\u00d7 Addr determines \nthe polyvariance of the analysis (i.e., distinguishing between bindings of the same variable in different \nevaluation contexts [22, 19]) because a alloc determines how many abstract variants are associated with \neach variable. For instance, allocating each time a new address corresponds to associating just one value \nwith a variable at each moment of the program execution. 2.3.1 Example: Monovariant analysis (0CFA) For \nexample, a classical monovariant analysis (0CFA) comes from de.ning the set of abstract addresses to \nbe the set of variables (a= Var), and then using variables as their own abstract Addr0CFA addresses: \na alloc0CFA(v,. )= v.  2.4 Introducing context with time-stamps Van Horn and Might [23] realize that \nthe abstract state-space as it stands lacks suf.cient information to instantiate classical strategies \nfor polyvariance (like k-CFA). To .x this, they introduce time\u00adstamps (to the concrete and abstract semantics) \nas a new component of the state. Time-stamps are used to remember execution context, and are directly \nresponsible for the context-sensitivity of the analysis: . . S Call \u00d7 (Store \u00d7 T = Env \u00d7 TTime t . T \nTime is a .nite set of abstract times. Each transition augments the time through an opaque function, \n(Clo \u00d7 Time and by giving the allocator a tick : (S . Talloc : Var \u00d7 TAddr access to this context instead \nof the whole state: Time . a . '' ' ' ([ (f \u00e61 . . . \u00e6n ., t) . s ), if )] , s, . (call, , ,t ([[(. (v1 \n. . . vn) call)] , ' ). A (f, s) . .,  clo ' di . A(\u00e6i, s) t = clo,. )., tick(d ( ' '' ' a i = alloca(vi,t \n) . = . [vi . a i] s ' = s . [ ai . {d i}]. 2.4.1 Example: k-CFA-style context-sensitivity The introduction \nof this component makes it possible to model k-CFA[22] by de.ning times to be sequences of up to kcall \nsites and addresses to be pairs of variables and call sites: :k T= Call Timek-CFA a\u00d7 T Addrk-CFA = Var \nTimek-CFA, and using corresponding de.nitions clo,(call,. . . ,t )) = .call : tickk-CFA(dt.k ( a allock-CFA(v,(. \n. . ,t ' )) = (v,t ' ) where .\u00b7.k limits its argument to at most length k. 3. Abstracting through a Monad \nIn .ow analysis, interaction with the store determines essential properties of the analysis. Thus, by \nabstracting away interaction with the store through a monad, we introduce an abstraction layer for these \nessential properties. In pure functional programming, the introduction of store\u00adpassing style to mimic \nside effects is an anti-pattern. The estab\u00adlished remedy for this anti-pattern is the use of monadic \nform in conjunction with a state-transformer monad. We can apply this remedy to our semantics as well. \n To make the presentation unambiguous (and executable), we .rst transliterate the abstract semantics \n(for k-CFA at the moment) into Haskell, starting with the syntax for CPS: type Var = String data Lambda \n= [Var ]. CExp deriving (Eq,Ord) data AExp = Ref Var | Lam Lambda deriving (Eq,Ord) data CExp = Call \nAExp [AExp] | Exit deriving (Eq,Ord) The set Call from the de.nition of the state-space corresponds \nto the type CExp in the implementation. The type for the state-space is a 4-tuple: type S = (CExp,Env,Store,Time) \ntype k . v = Map k v type Env = Var . Addr type Store = Addr . P Val data Val = Clo (Lambda,Env) deriving \n(Eq,Ord) type Addr = (Var,Time) type Time = [CExp] where we utilize Haskell s unicode support to keep \nthe implementa\u00adtion and the math as close as possible (e.g., writing P Val instead of Set Val). The transition \nrelation becomes a function next on states into lists of states: next :: S . [S] next .@(Call f aes,., \ns,t)= [(call,. '' ,s ' ,t ' )| proc@(Clo (vs . call,. ' )) . Set .toList (arg (f,., s)), let t ' = tick \n(proc,.) as = [alloc (v,t ' ,proc,.)| v . vs ] ds = [arg (\u00e6,., s)| \u00e6 . aes ] . '' . ' s ' = / [v =. a \n| v . vs | a . as ] = s . [a =. d | a . as | d . ds]] next . = [.] The function arg is the transliteration \nof the argument evaluator, A. The function tick is the transliteration of the ( tick function. And, \nthe function alloc is the transliteration of the a alloc function. Finally, the utility in.x operator \n(/ ):: Ord k . (k . v) . [(k,v)] . (k . v)is used to update a map with a list of key\u00advalue pairs, and \n(=.)is just a synonym for pair constructor. 3.1 Capturing nondeterminism in the monad The function next \nuses the list comprehension notation to more closely mimic the formalism. The list comprehension notation \nis syntactic sugar for the list comprehension monad. We can tiptoe into monadic normal form by expanding \nthe list comprehension [(call,. '' ,s ' ,t ' )| ... ]into its monadic form: mnext :: S . [S] mnext .@(Call \nf aes,.,s,t)= do proc@(Clo (vs . call,. ' )) . Set .toList (arg (f,., s)) let t ' = tick (proc,.) as \n= [alloc (v,t ' ,proc,.)| v . vs ] ds = [arg (\u00e6,., s)| \u00e6 . aes ] . '' . ' s ' = / [v =. a | v . vs | \na . as ] = s . [a =. d | a . as | d . ds ] return (call,. '' ,s ' ,t ' ) mnext . = return . Since function \nevaluation is the only source of nondeterminism, we can clean up these semantics by creating a new function \nfun as a special version of arg for evaluating functions: fun :: (AExp,Env,Store). [Val] fun = Set .toList \n. arg Thus, we can rewrite mnext: mnext :: S . [S] mnext .@(Call f aes,., s,t)= do proc@(Clo (vs . call,. \n' )) . fun (f,., s) \u00b7 \u00b7 \u00b7 --the rest is unchanged To reformulate this function fully into monadic normal \nform, we must refactor and recurry fun, arg, tick, alloc to return singleton lists as well (hence fun \nand arg now have the same type): fun :: (Env,Store). AExp . [Val] arg :: (Env,Store). AExp . [Val] tick \n:: Val . State . [Time ] alloc :: (Time,Val,State). Var . [Addr ] which allows us to rewrite mnext in \nmonadic normal form, re\u00adplacing list comprehensions with a standard function mapM :: Monad m . (a . m \nb) . [a ] . m [b]from a monadic toolset. mnext :: S . [S] mnext .@(Call f aes,., s,t)= do proc@(Clo (vs \n. call,. ' )) . fun (., s)f t ' . tick proc . let as = mapM (alloc (t ' ,proc,.)) vs ds = mapM (arg \n(., s)) aes \u00b7 \u00b7 \u00b7 --the rest is unchanged  3.2 Pulling the store into the monad In the previous section, \nwe refactored the abstract semantics so that nondeterminism is explicitly threaded through the list monad. \nIn this section, we subtly reformulate the semantics in terms of a CPSInterface type class that hides \ninteraction with the store, abstracting over .ve functions that form a semantic interface of the CPS \ncalculus: class Monad m . CPSInterface m where fun :: Env . AExp . m Val arg :: Env . AExp . m Val (.) \n:: Addr . Val . m () alloc :: Time . Var . m Addr tick :: Val . PS . m Time where the type PSencodes \na partial state (with no store): PS = (CExp,Env,Time) Under this interface, we can separate the speci.cation \nof the ab\u00adstract semantics from the speci.c details of how it interacts with the store: mnext :: (CPSInterface \nm). PS . m PS mnext ps@(Call f aes,., t)= do proc@(Clo (vs . call,. ' )) . fun . f t ' . tick proc ps \nas . mapM (alloc t ' )vs ds . mapM (arg .)aes let . '' = . ' / [v =. a | v . vs | a . as ] sequence [a \n. d | a . as | d . ds] return (call,. '' ,t ' ) mnext . = return .  3.3 Pulling time into the monad \nLike the store, time-stamps were another ad hoc addition to the original semantics to engineer them into \na form favorable for static analysis. We can lift time-stamps out of partial states and into monads as \nwell, so that PS = (CExp,Env). The analysis monad can now assume internal access to the current time, \nwhich simpli.es the interface: class Monad m . CPSInterface m where fun :: Env . AExp . m Val arg :: \nEnv . AExp . m Val (.) :: Addr . Val . m () alloc :: Var . m Addr tick :: Val . PS . m () Because the \nallocator alloc can assume access to time (and the store) inside the monad, it no longer needs to take \nit as a parameter, leaving the variable to be bound as its only remaining parameter. The simpli.cation \nin the analysis monad is re.ected in a simpli\u00ad.cation of the de.nition for transition function mnext \nas well, as time is no longer a component of the abstract states datatype PS: mnext :: (CPSInterface \nm). PS . m PS mnext ps@(Call f aes,.)= do proc@(Clo (vs . call,. ' )) . fun . f tick proc ps as . mapM \nalloc vs \u00b7 \u00b7 \u00b7 --the rest is unchanged  3.4 Abstracting over addresses At this point, we have a monadically \nabstracted abstracted3 abstract machine. Plugging in a monad controls nondeterminism, context and access \nto the store. However, we still have one component left to be abstracted. Until now, we have assumed \nthat abstract addresses are repre\u00adsented by a .xed datatype. In practice, the nature of the addresses \ndetermines the polyvariance and context-sensitivity of the analysis, since they usually divide bindings \naccording to different contexts (also known as contours [22]). For example, Shivers 1CFA allo\u00adcates distinct \ncontexts for each call site. Lakhotia et al. [12] intro\u00adduce l-contexts to build a static analysis for \nobfuscated x86 bina\u00adries, employing .nite sequences of unique enclosed function calls. Finally, one can \ntake a bounded set of naturals {n . N| n = N} for some N as contexts, which will give a good precision \nfor suf.\u00adciently big N. Thus, we must abstract over the structure of addresses in or\u00adder to cover at \nleast all the options from above. Unfortunately, addresses form a part of the state-space as a codomain \nof Env; thus, we need to parameterize our semantic domains by the type of addresses a: type PS a = (CExp,Env \na) type Env a = Var . a data Val a = Clo (Lambda,Env a) deriving (Eq,Ord) type Store a = a . P (Val a) \nWith this shift, we are no longer attached to the k-CFA-like repre\u00adsentation of addresses, as both Addr \nand Time are gone. Of course, the signature of the CPSInterface monad and mnext change accordingly, although \nthe body of mnext remains unchanged. The .nal de.nition of the CPSInterface class and the function mnext \nare represented in Figure 2 and are not going to change in the remainder of our story. What is going \nto change is the 3This is not a typo. class Monad m . CPSInterface m a where fun :: Env a . AExp . m \n(Val a) arg :: Env a . AExp . m (Val a) (.) :: a . Val a . m () alloc :: Var . m a tick :: Val a . PS \na . m () mnext :: CPSInterface m a . PS a . m (PS a) mnext ps@(Call f aes,.)= do proc@(Clo (vs . call \n' ,. ' )) . fun . f tick proc ps as . mapM alloc vs ds . mapM (arg .)aes let . '' = / [v =. a | v . vs \n| a . as] . ' sequence [a . d | a . as | d . ds] return (call ' ,. '' ) mnext . = return . Figure 2: \nSemantic interface and a small-step semantics of CPS in a monadic form. implementation of the semantic \ninterface CPSInterface as well as the choice of the underlying monad for the analysis logic. 4. Recovering \na Concrete Interpreter With non-determinism, interaction with the store and time pulled into an analysis \nmonad, and addresses abstracted, our semantics has become highly parameterized. As a result, we can easily \nrecover a concrete interpreter as a sanity check for adequacy. With the standard IO monad as an underlying \nanalysis monad, we can use the real heap as the store and implement mutable references as a datatype \nIOAddr containing essentially a pointer: data IOAddr = IOAddr {lookup :: IORef (Val IOAddr)} We will \nalso need two simple administrative functions in order to write to and read from IOAddrs: readIOAddr \n:: IOAddr . IO (Val IOAddr) readIOAddr = readIORef . lookup writeIOAddr :: IOAddr . Val IOAddr . IO () \nwriteIOAddr = writeIORef . lookup The implementation of the CPSInterface type class directly mimics the \nconcrete semantics de.ned in Section 2.2: instance CPSInterface IO IOAddr where fun . (Lam l)= return \n$Clo (l,.) fun . (Ref v) = readIOAddr (.!v) arg . (Lam l)= return $Clo (l,.) arg . (Ref v) = readIOAddr \n(.!v) addr . v = writeIOAddr addr v alloc v = liftM IOAddr $newIORef . tick = return () When a new address \nis allocated, we pass the function newIORef . as the initial value of the new reference, since the actual \nvalue to be bound at this address is not yet de.ned at the moment of alloca\u00adtion. The function tick is \na no-op: in the real world, time advances without our help. With this interpretation of the semantic \ninterface, the concrete interpreter is simply a recursively de.ned driver loop that iter\u00adates the semantic \ntransition function mnext until an Exit state is reached.  interpret :: CExp . IO (PS IOAddr ) interpret \ne = go (e,Map .empty ) where go :: (PS IOAddr ). IO (PS IOAddr ) go s = do s ' . mnext s case s ' of \nx@(Exit, ). return x y . go y 5. Recovering a Collecting Semantics In this section, we recall key notions \nfrom lattice theory and ab\u00adstract interpretation, crucial for constructing an interpreter for con\u00adcrete \nor abstract small-step collecting semantics of a program. We proceed by translating the theory into programs \nin Haskell and demonstrate an implementation of a simple collecting semantics. 5.1 Basics of lattice \ntheory and abstract interpretation A complete lattice (C;.,.,.,.,.) is a partial order (C;.) such that \nthere exists a least upper bound (or join) .Sand a greatest lower bound (or meet) .S of all subsets S \n. C. In particular .C = . and .C = .. A point x is a .xed point of a function f if f(x) = x. Given two \npartial orders, (C, .) and (A, =), a function f of type C . A is monotone if .x, y : x . y =. f(x)= f(y). \nBy the Knaster-Tarski .xed-point theorem a monotone functional f over a com\u00adplete lattice has a least \n.xed point lfp. f = .{x | f(x) . x}. Algorithmically the least .xed point of a monotone function f over \na complete lattice of .nite height can be computed by Kleene iter\u00adation: . . f(.). f2(.). f3(.). . . \n. since i lfp. f = f(.). (1) i=0 A Galois connection is a pair of functions a, . connecting two partial \norders (C, .) and (A, =), such that .a, c : a(c)= a .. . .-- c . .(a). We typeset Galois connections \nas: (C, .) --.(A, =). a. .-- Given a Galois connection (C, .) --.(A, =) and a monotone a function Fc \n: C . C, an abstract function Fa can be constructed as a . Fc . . =.Fa. Therefore, by the .xed-point \ntransfer theorem [5], we have a(lfp Fc)= lfp Fa when Fa is monotone. The reachable states collecting \nsemantics for a set of pro\u00adgram states S, a transition function ( ) . S \u00d7 S, and a set of initial states \nS0 . S is de.ned on a complete lattice (P(S),.,\u00d8,S,.,n) as a least .xed point of a monotone func\u00adtional \nF, where ' F(X )= S0 . {s | s . X . s s '}. (2) F is uncomputable in general, which makes it a usual \nstarting point for construction of an abstract, monotone transition function F, F such that a . F . . \n.F and computing an approximate .xed = Fpoint as lfp FF. In the remainder of the section we will systematically \nabstract over computation of a small-step collecting semantics for CPS. 5.2 Implementing lattices and \n.xed point computations We de.ne a type class for a lattice following its algebraic de.nition: class \nLattice a where . :: a . :: a (.) :: a . a . Bool (.):: a . a . a (.):: a . a . a We provide the following \ninstances of Lattice for the standard container types that we use heavily for systematic abstraction \nof abstract machines [15]: unit, pairs, powersets and maps: instance Lattice () instance (Lattice a,Lattice \nb). Lattice (a,b) instance (Ord s ,Eq s). Lattice (P s) instance (Ord k ,Lattice v). Lattice (k . v) \nThe monadic interpreter from Figure 2, which we presented at the end of Section 3, is reminiscent of \nthe de.nition (2) of the collecting semantics. However, the fact it uses a monad and is not an endo-function, \nprevents us from using it directly as a relation ( ) in a functional F. What we can do is to explicitly \nmake a separation of concerns between mnext as an implementation of an abstract transition function and \na computation of a least .xed point. We de.ne the type class Collecting to let the implementor of the \nanalysis de.ne the logic of initiating the semantics (i.e., providing an initial set S0 from (2)) and \nmaking a step : class Collecting m a fp | fp . a,fp . m where applyStep :: (a . m a ). fp . fp inject \n:: a . fp The functional dependencies fp . a and fp . m state that the choice of a domain fp determines \nin which monad m the step function should be interpreted [11], as well as what the transition s domain \nand co-domain a should be.4 The computation of the collecting semantics as a least .xed point can be \nthen de.ned directly from the Kleene iteration (1): kleeneIt :: (Lattice a). (a . a). a kleeneIt f = \nloop . where loop c = let c ' = f c in if c ' . c then c else loop c ' Finally, the collecting semantics \n(2) translates gracefully to the following function, mapping a transition function step and an ini\u00adtial \nstate c to a result of the analysis. exploreFP :: (Lattice fp,Collecting m a fp). (a . m a ). a . fp \nexploreFP step c = kleeneIt F where F s = inject c . applyStep step s It is now straightforward to implement \na function that, given an expression, runs the analysis: runAnalysis :: (CPSInterface m a,Lattice fp, \nCollecting m (PS a)fp). CExp . fp runAnalysis e = exploreFP mnext (e,Map .empty ) The signature of runAnalysis \noutlines precisely the three de\u00adgrees of freedom that can be changed in order to obtain different collecting \nsemantics: 1. A monad, accounting for nondeterminism and passing analysis\u00adspeci.c state components (i.e., \ntime and store), 2. An implementation of the semantic interface of a language (e.g., the one in Figure \n2), and 3. The analysis lattice and an implementation of a .xed point com\u00adputation that extracts the \nresult of a single step from the analysis monad and augments the lattice argument appropriately.   \n5.3 StorePassing a simple implementation of a collecting semantics for CPS At this point we are ready \nto reconstruct a simple abstract inter\u00adpreter computing a collecting, store-passing semantics of CPS \nby 4Alternatively, we could use associated types to express the same sort of dependencies [3].  gradually \ninstantiating three main aspects of the analysis, outlined at the end of the previous section. 5.3.1 \nA two-level analysis monad We start from constructing a monad for a simple collecting, store\u00adpassing \nsemantics by employing standard monad transformers StateT and the list monad: type StorePassing s g = \nStateT g (StateT s [ ]) The analysis parameters of type s and g carry state components. They carry the \nstore and the analysis guts respectively, where the latter can contain for example a time value. In a \ndesugared form, the type StorePassing is equivalent to the functional type g . s . [((a,g),s)] for some \na, g and s, so the stack representation of a monad should be read inside-out . That is, a value of type \nStorePassing produces a set (represented by a list) of results of type a, coupled with components of \ntype g and s. 5.3.2 A simple implementation of a CPS semantic interface To turn the StorePassing monad \ninto an interpretation of the semantics, we have to implement the CPSInterface type class. So far, we \nchoose to implement addresses as Haskell Integer s for simplicity. The store is represented as a map \nfrom integer addresses to sets of values. The implementation of the semantic interface is provided below. \ninstance CPSInterface (StorePassing (Store Integer )Integer )Integer where fun . (Lam l)= return $Clo \n(l, .) fun . (Ref v) = lift $getsNDSet $.s . s ! (.!v) arg . (Lam l)= return $Clo (l, .) arg . (Ref v) \n= lift $getsNDSet $.s . s ! (.!v) a . d = lift $modify $ Map.insert a (singleton d ) alloc v = gets id \ntick proc ps = modify $.t . t + 1 The function getsNDSet is a crux of handling non-determinism in a monadic \nimplementation of the analysis as a stateful monad. It allows one to examine the state, get multiple \nresults and non\u00addeterministically choose one. Its signature needs explanation: getsNDSet :: (MonadPlus \nm ,MonadState s m ). (s . Set a). m a However, the type class constraints before . provide a clue. One \ncan see that m is required to be a state monad, i.e., carry an implicit state component of type s, which \ncan be accessed and modi.ed. The argument type s . Set a accounts for the non-deterministic result of \ntype a that might come out of examining the state s. Finally, MonadPlus m is a constraint that ensures \nthat m has a notion of non-deterministic choice, so the obtained results can be combined in a wrapped \nresult m a. What comes as a nice surprise is that StorePassing is an instance of both MonadPlus and MonadState \n. lift is a standard Haskell function for explicit management of the monadic stack, which is used explicitly \nto disambiguate the tar\u00adgets of accesses to the monad stack [13, 21]. In our implementation of CPSInterface \nthe outermost state is reserved to carry guts (i.e., the time component), so we need to employ lift to \naccess the store s, located on a second level of the monad stack. Finally, gets and modify are standard \nhigher-order functions that allow one to examine and modify internal state of the monad. gets :: MonadState \ns m . (s . a). m a modify :: MonadState s m . (s . s). m () For instance, the implementation of tick \nmodi.es the time com\u00adponent on the .rst level of the monadic stack (therefore, no explicit lifting is \nrequired).5  5.3.3 Computing a collecting semantics of CPS The last missing ingredient we need to compute \nthe collecting semantics of CPS is the de.nition of a .xed point computation that uses the StorePassing \nmonad. We can reach a .x-point of type P ((PS a,g),s) step by step with the following de.nitions of applyStep \nand inject. instance (Ord s ,Ord a ,Ord g ,HasInitial g,Lattice s). Collecting (StorePassing s g ) (PS \na) (P ((PS a,g),s)) where inject p = singleton $ ((p,initial),.) applyStep step fp = joinWith runStep \nfp where runStep ((., t),s)= Set .fromList $runStateT (runStateT (step .)t)s The implementation of inject \ninstruments a provided state with initial guts , de.ned by the value initial of the class HasInitial \nfor g and the lattice minimum . for s, and wraps it into a singleton set. The class HasInitial is de.ned \nas class HasInitial g where initial :: g and its implementation for Integer s is trivial (e.g., initial \n= 0). The most interesting element of the implementation is the utility function joinWith, de.ned as \nfollows: joinWith :: (Lattice a). (b . a). Set b . a joinWith f = Set .foldr ((.). f). That is, given \na set of values of type b and a function f of type b . a for a lattice a, joinWith traverses the structure, \napplying f to each of its leaf elements and combines the results using the lattice join (.). In the de.nition \nof applyStep, joinWith takes a function that simply passes the state . and the components t and s to \nthe pro\u00advided function step, runs a monad and collects the result into a set. This function is applied \nto all states in fp and the results are joined. All ingredients to run the analysis are now in place, \nand all we need to do is to use the function runAnalysis from Section 5.2 to compute the result: exp \n:: CExp = ... runAnalysis exp :: P ((PS Integer ,Integer ),Store Integer ) 6. Monadic Parameters for \nAbstract Abstract Machines In Section 5 we have demonstrated how to restore a simple collect\u00ading semantics \nfrom the monadically-parametrized semantic inter\u00adface, presenting a StorePassing monad and an analysis \nwith do\u00admain P ((PS Integer ,Integer ),Store Integer ). More complex analyses differ from this simple \nanalysis in a number of aspects. In this section, we will discuss how our StorePassing monad and CPSInterface \nand Collecting instances can be abstracted further to accomodate this. Speci.cally, we show how to control \npolyvari\u00adance, store representation, abstract counting, abstract garbage col\u00adlection and store cloning. \n5In the present implementation, we allow the time component to grow in.nitely for simplicity, so in principle \nsome implementation of the analysis may not terminate, which can be restricted by modifying the function \ntick .  6.1 Controlling polyvariance So far, we have used Haskell s Integer s as abstract addresses \nin the collecting semantics. To allow experimentation with different addresses, we introduce the following \nclass that provides a uniform view on addresses which may (optionally) incorporate context: class (Ord \na,Eq a). Addressable a c | c . a where t0 :: c valloc :: Var . c . a advance :: Val a . PS a . c . c \nThe type class Addressable has two type parameters: a for ac\u00adtual addresses and c for the type of the \ncontext. Contexts unam\u00adbiguously de.ne the nature of addresses. The type class contains three functions. \nt0 generates an initial context, used to instantiate the HasInitial class from Section 5.3.3. The function \nvalloc, given a variable name and a context, allocates a new address. Finally, the function advance has \nthe opportunity to internalize components of the partial state within the monad, based on the function \ncalled, the current state processed, and a given context. A meaningful instance of the Addressable type \nclass uses a combination of time-stamps, represented by lists of calls of length bounded by some k, and \naddresses that simply pair the time-stamps with variable names. This de.nes an analysis polyvariance: \ndata KTime = KCalls [CExp] deriving (Eq,Ord) data KAddr = KBind Var KTime deriving (Eq,Ord) Here is the \ninstance of Addressable for this pair, relying on the auxiliary class KCFA for generic controlling of \npolyvariance: class KCFA a where getK :: a . Int instance Addressable KAddr KTime where t0 = KCalls \n[] valloc v t = KBind v t advance proc (call,.)t@(KCalls calls) = KCalls $(getK t)(call : calls) The \nCPSInterface instance for StorePassing requires a small change in order to accomodate polyvariance: instance \n(Addressable a t). CPSInterface (StorePassing (Store a)t)a where alloc v = gets (valloc v) tick proc \nps = modify $.t . advance proc ps t \u00b7 \u00b7 \u00b7 --the rest is unchanged Accroding to the A Posteriori soundness \ntheorem of Might and Manolios [17], any allocation policy for a non-deterministic abstract interpreter \n(which our analysis is a particular case of) leads to a sound abstraction of a concrete store-based collecting \nsemantics that uses uniques addresses for each allocation (e.g., integers, as in the example in Section \n5.3.2). Thus, abstracting over addresses yields a family of sound abstract interpreters and requires \nno change in the semantics interface (Figure 2). 6.2 Abstracting over the store component The store \ncomponent is essential for ef.cient implementation of the analysis but can also itself be a source of \nvaluable measurements (e.g., computing the .ows-to information), so our next step is to make an analysis \nstore-generic. We do so by de.ning a StoreLike class enabling creation of initial store, binding, update, \nand lookup as well as providing a mechanism to clean the store up: class (Eq a,Lattice s,Lattice d). \nStoreLike a s d | s . a,s . d where s0 :: s bind :: s . a . d . s replace :: s . a . d . s fetch :: \ns . a . d .lterStore :: s . (a . Bool). s The StoreLike class binds together three components: ad\u00addresses \n(a), the store implementation itself (s) and the store co\u00addomain (d). To save space, we refer the reader \nto our public imple\u00admentation for implementations of StoreLike instances for Store a. Again, we need \nsmall changes in the CPSInterface instance for StorePassing in order to abstract over stores: instance \n(Addressable a t,StoreLike a s (P (Val a))) . CPSInterface (StorePassing s t)a where fun . (Ref v)= lift \n$getsNDSet $.ip fetch (.!v) arg . (Ref v)= lift $getsNDSet $.ip fetch (.!v) a . d = lift $modify $.s \n. bind s a (singleton d) \u00b7 \u00b7 \u00b7 --the rest is unchanged  6.3 Controlling abstract counting Abstract counting \nis a technique to track how many times an ab\u00adstract resource has been allocated [18]. It provides a simple \nbut powerful way of bounding cardinalities over abstractions. Speci.\u00adcally, it bounds them in a way that \nit enables must-alias analysis in imperative languages or environment analysis in higher-order lan\u00adguages. \nCounting enables additional shape analyses or predicate abstractions to be layered on top of an existing \nanalysis [16]. This, in turn, enables an analysis client to perform environmental analy\u00adsis to perform \nadvanced optimization, such as super \u00df-inlining [14]. To introduce an abstract counter for CPS, we need \nto make a small addition to the state-space: . . = Env \u00d7 TCount \u00d7 T S Call \u00d7 (Store \u00d7 TTime \u00b5 . Ta Count \n= Addr . NF n . NF= {0,1,8} . The abstract transition relation changes correspondingly to take possible \nchanges of \u00b5 into account: . '' ' ' ' ([ (f \u00e61 )] , s, t) . s \u00b5 t . . . \u00e6n ., \u00b5, (call, , , , ), where \n([[(. (v1 . . . vn) call)] , ' ). A (f, s) . ., clo . '' d i . ., ai] A(\u00e6i, s) = . ' [vi . t ' = tick(d.) \n= s. [ (clo, s ' ai . {d i}] a i = alloc(vi,t a' ) \u00b5 ' = \u00b5 ..[ ai . 1] The operator ..is the point-wise \nlifted natural abstraction of ad\u00addition over NF. In fact, the structure of the set NFcan vary depending \non the requirements for the analysis results. The only requirement is that NFshould be a lattice. For \ninstance, in a degenerate case, one can turn abstract counting offby setting NF= {8}. Since abstract \ncounts are modi.ed in tandem with the abstract store, counting and dependent enhancements like strong \nupdate can be hidden entirely within and tuned from the monad. We need no refactoring of the state-space \nor the semantics needed to introduce abstract counting, thanks to the introduction of the class StoreLike \nin Section 6.2. All we need to do is supply a different instance of store, namely, a counting one. First, \nwe de.ne a datatype for NF, make it a lattice and de.ne abstract addition:  data AbsNat = AZero | AOne \n| AMany deriving (Ord ,Eq,Show ) (.) :: AbsNat . AbsNat . AbsNat AZero . n = n n . AZero = n n . m = \nAMany instance Lattice AbsNat where \u00b7 \u00b7 \u00b7 Second, we de.ne a type class for abstract counter, a counting \nstore and make the latter an instance of the former: class StoreLike a s d . ACounter a s d where count \n:: s . a . AbsNat type CountingStore a d = a . (d,AbsNat) instance (Ord a ,Lattice d). ACounter a (CountingStore \na d)d where count s a = snd $s !a Because the counter is parameterized over addresses, it too is in\u00addependent \nof speci.c semantics, and in fact can be used with any other semantics. For a full implementation of \nStoreLike a (CountingStore a d)d, we invite the reader to puzzle through it or reference our pub\u00adlic \nimplementation. Once the instance is provided, a CountingStore can be directly plugged into the StorePassing \n, so the abstract counting does not require any changes in the analysis logic, implic\u00ad itly adding the \ncomponent TS. Count to  6.4 Controlling abstract garbage collection Abstract garbage collection [18] \nis a store-sensitive analysis tech\u00adnique that prunes unreachable structure (exactly as ordinary garbage \ncollection does). The net effect is an often dramatic increase in pre\u00adcision as well as a corresponding \ndrop in analysis time. The tech\u00adnique is de.ned in terms of touchability of a value by a binding, the \nadjacency of bindings and reachability of bindings from some entity. In essence, abstract garbage collection \nstands for .nding the set of reachable bindings for a particular state and restricting the domain of \nthe store s to solely these bindings. The abstract bindings T (\u00e6,. ), touched by some abstract clo\u00adsure \npair (\u00e6,. )are de.ned as follows: T (\u00e6,. ) = {. (v): v . free(\u00e6)}  T {(\u00e61,. 1),. . . , (\u00e6n,. n)} = \nT (\u00e61,. 1). . . . . T (\u00e6n,. n) We extend the notion of touching to call sites and abstract states: T \n((f \u00e61 . . . \u00e6n),. ) = T (f, . ). T (\u00e61,. ). . . . T (\u00e6n,. ) T (call, s, = T (call, . ) ., t) and de.ne \nthe abstract adjacency relation for bindings: a s a ' .. a ' . T ( s( a)) Next, the abstract reachable-bindings \nfunction R : F(a) S . P Addr for a given abstract state . computes the set of reachable bindings as all \nbindings we can reach from . with chains of s links: ' * R ( .)= {a : a . T ( .)and a s a '}. . We de.ne \nthe abstract Garbage Collection function, S . G : F S that removes unreachable bindings from the domain \nof s : F t) (call, s| t ), G(call, ., s, = ., R( .), where the vertical bar | operator denotes map restriction, \ni.e., f|X is the function f de.ned at most over elements in the set X. Finally, using the function G, \nwe can de.ne the alternate, GC abstract transition rule, G , so the abstract transition becomes: ' G( \n.) . (STEP-GC) . G . ' From the implementor s point of view, an abstract garbage col\u00adlector modi.es the \ninternal part of the state, i.e., an abstract store, by removing unreachable addresses. Therefore, it \nis natural to de\u00ad.ne abstract garbage collection abstractly as an operation in the analysis monad: class \nMonad m . GarbageCollector m a where gc :: a . m () gc = return () --default implementation The function \ngc takes a partial state as a parameter and returns a monad operation. We also supply a default implementation \nof the function gc as a no-op. The structure of the class StoreLike makes it easy to implement a garbage \ncollector thanks to the function .lterStore (Section 6.2). Weaving the GC into the semantics requires \nonly little change in the .xed point computation for StorePassing : applyStep step = joinWith (.((., \nt),s). ... runStateT (do {. ' . step .;gc . ' ;return . ' })t...)  6.5 Controlling store-cloning By \ndefault, the abstracted abstract machine approach to static anal\u00adysis yields a heap-(/store-)cloning \nanalysis: every state contains a store. Ordinarily, store-cloning should be reserved for situations in \nwhich the extra precision bene.t the target applications. However, for an analysis implemented this way \nit can take time exponential in the size of the input program when computing the reachable states of \nthe abstracted machine [19]. The standard tech\u00adnique to reduce the complexity is to employ widening in \nthe form of Shivers single-threaded store [22]. To use a single-threaded store, we have to reconsider \nthe abstract evaluation function itself. Instead of seeing it as a function that returns the set of reachable \nstates, it is a function that returns a pair, consisting of a set of partial states and a single globally \napproximating store. Although this change requires a signi.cant reworking in the def\u00adinition of the semantics, \nit is quite easy to implement in our frame\u00adwork, since the store is not a component of the program states, \nbut rather an element supplied by an analysis monad. One can also notice that the new semantics can be \ncaptured by establishing the following Galois connection: .-- (P(Sdt Store ),.) --.St )\u00d7 T(3) \u00d7 T. (P(dStore \n,.), a where St = Call \u00d7 Time. Env \u00d7 TBounding the space of d( addresses (Section 6.1) implies .niteness \nof both lattices involved in the Galois connection (3), which means that both a and . are computable. \nIt is straightforward to express them in Haskell: alpha :: (Lattice s,Ord a ,Ord g ). P ((PS a,g),s). \n(P (PS a,g),s) alpha = joinWith (.((p,g), s). (singleton (p,g), s)) gamma :: (Ord a ,Ord g ,Ord s ). \n(P (PS a,g),s). P ((PS a,g),s) gamma (states , s)= Set .map (.(p,g). ((p,g), s)) states In words, alpha \ncombines together all per-state store compo\u00adnents (given the stores form a lattice, hence Lattice s). \nConversely, gamma spreads the store s among all provided states.  the instance of Collecting, taking \nFc = applyStep ' step:6 instance (Ord g ,Ord a ,Ord s ,Lattice s,HasInitial g). Collecting (StorePassing \ns g ) (PS a) (P (PS a,g),s)where inject a = (singleton (a,initial),.) applyStep step = alpha . (applyStep \n' step). gamma 7. Pulling It All Together A high-level overview of the described framework is given in \nFig\u00adure 3. The concepts in the upper half are de.ned on the meta-level, i.e., can be implemented in a \nway so they might be reused by dif\u00adferent languages and analyses. The lower half describes language\u00addependent \nconcepts, which rely, in particular, on the .xed syntax, semantics and the structure of values. Solid \narrows denote type de\u00adpendencies between instances of the components, and dashed ar\u00adrows are for the \noptional logical dependencies, which are, however, not strictly enforced by types. Curiously, the diagram \ncon.rms our point that store logic and abstract counting strategy are orthogonal to the analysis implementation \n(Sections 6.2 and 6.3). As opposite to the abstract concepts, it is much harder to pro\u00advide a straightforward \nhierarchy of actual concept implementations with strict level distinctions. For instance, one can encode \nthe store and monad logic directly into the semantic interface de.ni\u00adtion, moving it, therefore to the \nbottom part of the diagram. Alter\u00adnatively, one can implement the store and the monad independently from \nthe encoding of the semantic interface, just as we did in Sec\u00adtion 3, getting as close as possible to \nthe conceptual decomposition. This approach pays off when composing the resulting analysis, as we demonstrate \nin the following section. 8. Further Examples: k-CFA Family In this section, we expand our original examples \nfrom Section 3 to a family of k-CFA-based abstract interpreters, following the recipe described in Sections \n5 and 6. We omit a few tedious implemen\u00adtation details for the sake of brevity; the full development \nand test programs can be found in the accompanying code repository: http://github.com/ilyasergey/monadic-cfa \n 6We use applyStep ' to refer to the de.nition of applyStep for a domain with per-state store. It is \nslightly different from the actual implementation in Haskell, as one is required to wrap the per-state \nstore -version of StorePassing into a separate datatype in order to make it possible for a type checker \nto distinguish between two versions of applyStep . We elaborate more on this in Section 8. 8.1 A simple \nabstract interpreter for k-CFA First, we .x the k-degree of the analysis by instantiating the class KCFA \nfrom Section 6.1 for k = 1: instance KCFA KTime where getK = const 1 Second, we de.ne the analysis function \nanalyseKCFA by em\u00adploying the runAnalysis function from Section 5.2: analyseKCFA :: CExp . P ((PS KAddr \n,KTime ),Store KAddr ) analyseKCFA = runAnalysis The re.ned type of analyseKCFA is of particular interest. \nAfter .attening the tuples, one can see that it re.ects the abstract domain of the analysis: abstract \nstates coupled with timestamps and abstract stores. 8.2 An abstract interpreter with a shared store \nAs a next step, we apply the widening strategy via a shared store, as described in Section 6.5. We directly \nuse the de.nitions of alpha and gamma and rede.ne the instance of Collecting by providing a new implementation \nof the applyStep function (Section 5.2). In order to overcome Haskell s conventions for type resolution \nin the case of the function applyStep, we might need to de.ne a wrapper record type for the analysis \nresult. newtype Wrap a g s = Wrap {unWrap :: P ((PS a,g),s)} deriving Lattice The de.nition of applyStep \nfor store-sharing widening looks as follows: applyStep step = alpha . unWrap . applyStep step . Wrap \n. gamma Note, the implementation is not recursive, but the inner call to applyStep operates with a different \ndomain. The de.nition of the analysis function has the same implementation, thanks to the type class-based \npolymorphism [25], however, its return type is concep\u00adtually different, as it accounts for a set of states, \ncoupled with a single store: analyseShared :: CExp . (P (PS KAddr ,KTime ),Store KAddr ) analyseShared \n= runAnalysis  8.3 An abstract interpreter with a counting store As the analysis is parametrized with \nthe store explicitly, its instru\u00admentation with a counting machinery is trivial: we just replace the \nsecond component of the result of the single-store-passing analysis with a specialized counting store \n(Section 6.3). type KCFACountingStore = CountingStore KAddr (P (Val KAddr)) analyseWithCount :: CExp \n. (P (PS KAddr ,KTime ),KCFACountingStore) analyseWithCount = runAnalysis 9. Related Work and Conclusion \nWe have illustrated a systematic method for transforming a con\u00adcrete semantics into a monadically-parameterized \nmachine, such that the monad determines the classical properties of an abstract analysis. Our work is \nsituated .rmly in the abstract interpretation tradition established by Cousot and Cousot [4, 5].  Following \na series of complex power-domain constructions, Hu\u00addak and Young [10] devised simpler set-based collecting \ninterpre\u00adtations for both .rst-order and higher-order functional languages. They furthermore outline \nhow to modify their approach to express relational properties. This requires generalizing their collecting \nin\u00adterpretation to sets of value-environment pairs, remniscent of our sets-of-states starting point. \nLike Hudak and Young, our framework can thereby describe relational properties. The way we factor the \nconcrete semantics is very much in the spirit of Nielson s two-level meta-language [20]. Nielson proposes \nan abstract interpretation framework based on the idea of decom\u00adposing a denotational language de.nition \ninto two parts: a core semantics, containing semantic rules and their types, but leaving some function \nsymbols and domain names uninterpreted, and an interpretation that .lls out the missing de.nition of \nfunction sym\u00adbol and domain names. thereby allowing alternate non-standard in\u00adterpretations in addition \nto the standard semantics de.ning the meaning of programs. Such a decomposition is formulated in terms \nof a two-level met\u00adalanguage, where some types are considered to be dynamically \u00adinterpreted,7 and dynamic \nfunctions symbols are represented by combinators, closed over variables of dynamically-interpreted types. \nThis makes it possible to de.ne the meaning of dynamic types and combinators in different ways to express \na lazy stan\u00addard semantics, detection of signs, strictness, and liveness on top of the same semantics \ninterface. Also, Nielson further illustrates that both forward and backward analyses, independent-attribute \nand relational methods can be formulated in terms of the same core semantics given different interpretations. \nIn contrast, our work focuses on abstractions of the operational small-step collecting se\u00admantics, since \nit delivers data for most of the interesting analyses in the setting of a higher-order language. In a \nsense, the functions from the class CPSInterface, such as fun, arg and others, play an analogous role \nto Nielson s dynamic function symbols, and type parameters such as a, s or g are the dynamic types. Our \nframework gains from giving function symbols monadic types: unlike Nielson s framework, where all information \nabout properties of interest is captured by the treatment of values of spe\u00adci.c types, our implementation \nalso allows us to track temporal properties of execution and tweak interpretation properties depend\u00ading \non the context of execution. Moreover, the sharing of stores as a widening strategy, is trivial to implement \nin our decomposition. The well-formedness of an interpretation in our framework is en\u00adsured by the type \nsystem of the host language (Haskell, in our case). The core semantics we present using monads is of \na more de\u00adnotational .avour, as it is expressed by the CPSInterface type class. Recent work by Filinski, \nhowever, demonstrates that a com\u00adplementary, operational representation, is possible using re.ection \nand rei.cation [8]. This correspondence could be used to translate the monad-based de.nitions of the \nsemantics functions into spe\u00adcialized operational rules, which were usually hand-crafted [23]. Our publicly \navailable implementation indicates the robustness of the approach, allowing re-use of multiple semantic \naspects be\u00adtween different analyses and semantic formalisms. Acknowledgements We wish to thank Olivier \nDanvy for discus\u00adsions on Nielson s work as well as for his hospitality during Sergey and Might s visit \nto Aarhus University in December 2011, where the idea of the work was discussed for the .rst time. We \nare grateful to the PLDI 2013 reviewers for their excellent feedback. The work of Ilya Sergey has been \npartially supported by EU Marie Curie CO-FUND Action 291803 Amarout-II Europe . This research is par\u00adtially \nfunded by the Research Foundation -Flanders (FWO), and by 7I.e., those, which can be given a speci.c \ninterpretation as a domain or a lattice. the Research Fund KU Leuven. Dominique Devriese holds a Ph.D. \nfellowship of the Research Foundation -Flanders (FWO). Different parts of Might s effort on this work \nwere partially supported by the DARPA programs APAC and CRASH. References [1] M. S. Ager, O. Danvy, and \nJ. Midtgaard. A functional correspondence between monadic evaluators and abstract machines for languages \nwith computational effects. Theor. Comput. Sci., 342(1):149 172, 2005. [2] M. Biernacka and O. Danvy. \nA syntactic correspondence between context-sensitive calculi and abstract machines. Theor. Comput. Sci., \n375(1-3):76 108, 2007. [3] M. M. T. Chakravarty, G. Keller, S. L. P. Jones, and S. Marlow. Associated \ntypes with class. In POPL, 2005. [4] P. Cousot and R. Cousot. Abstract interpretation: a uni.ed lattice \nmodel for static analysis of programs by construction or approximation of .xpoints. In POPL, 1977. [5] \nP. Cousot and R. Cousot. Systematic design of program analysis frameworks. In POPL, 1979. [6] O. Danvy. \nA new one-pass transformation into monadic normal form. In CC, volume 2622 of LNCS, 2003. [7] O. Danvy. \nDefunctionalized interpreters for programming languages. In ICFP, 2008. [8] A. Filinski. Monads in action. \nIn POPL, 2010. [9] C. Flanagan, A. Sabry, B. F. Duba, and M. Felleisen. The essence of compiling with \ncontinuations. In PLDI, 1993. [10] P. Hudak and J. Young. Collecting interpretations of expressions. \nACM Trans. Prog. Lang. Syst., 13(2):269 290, 1991. [11] M. P. Jones. Type Classes with Functional Dependencies. \nIn ESOP, volume 1782 of LNCS, 2000. [12] A. Lakhotia, D. R. Boccardo, A. Singh, and A. Manacero, Jr. \nContext\u00adsensitive analysis of obfuscated x86 executables. In PEPM, 2010. [13] S. Liang, P. Hudak, and \nM. Jones. Monad transformers and modular interpreters. In POPL, 1995. [14] M. Might. Environment analysis \nof higher-order languages. PhD thesis, Georgia Institute of Technology, 2007. [15] M. Might. Abstract \ninterpreters for free. In SAS, volume 6337 of LNCS, 2010. [16] M. Might. Shape analysis in the absence \nof pointers and structure. In VMCAI, volume 5944 of LNCS, 2010. [17] M. Might and P. Manolios. A posteriori \nsoundness for non\u00addeterministic abstract interpretations. In VMCAI, volume 5944 of LNCS, 2009. [18] M. \nMight and O. Shivers. Improving .ow analyses via GCFA: abstract garbage collection and counting. In ICFP, \n2006. [19] M. Might, Y. Smaragdakis, and D. Van Horn. Resolving and exploiting the k-CFA paradox: illuminating \nfunctional vs. object\u00adoriented program analysis. In PLDI, 2010. [20] F. Nielson. Two-level semantics \nand abstract interpretation. Theor. Comput. Sci., 69(2):117 242, 1989. [21] T. Schrijvers and B. C. Oliveira. \nMonads, zippers and views: virtualizing the monad stack. In ICFP, 2011. [22] O. G. Shivers. Control-.ow \nanalysis of higher-order languages or taming lambda. PhD thesis, Carnegie Mellon University, 1991. [23] \nD. Van Horn and M. Might. Abstracting abstract machines. In ICFP, 2010. [24] D. Van Horn and M. Might. \nAbstracting abstract machines: a systematic approach to higher-order program analysis. Commun. ACM, 54(9):101 \n109, 2011. [25] P. Wadler and S. Blott. How to make ad-hoc polymorphism less ad-hoc. In POPL, 1989. \n     \n\t\t\t", "proc_id": "2491956", "abstract": "<p>Recent developments in the systematic construction of abstract interpreters hinted at the possibility of a broad unification of concepts in static analysis. We deliver that unification by showing context-sensitivity, polyvariance, flow-sensitivity, reachability-pruning, heap-cloning and cardinality-bounding to be independent of any particular semantics. Monads become the unifying agent between these concepts and between semantics. For instance, by plugging the same \"context-insensitivity monad\" into a monadically-parameterized semantics for Java or for the lambda calculus, it yields the expected context-insensitive analysis.</p> <p>To achieve this unification, we develop a systematic method for transforming a concrete semantics into a monadically-parameterized abstract machine. Changing the monad changes the behavior of the machine. By changing the monad, we recover a spectrum of machines---from the original concrete semantics to a monovariant, flow- and context-insensitive static analysis with a singly-threaded heap and weak updates.</p> <p>The monadic parameterization also suggests an abstraction over the ubiquitous monotone fixed-point computation found in static analysis. This abstraction makes it straightforward to instrument an analysis with high-level strategies for improving precision and performance, such as abstract garbage collection and widening.</p> <p>While the paper itself runs the development for continuation-passing style, our generic implementation replays it for direct-style lambda-calculus and Featherweight Java to support generality.</p>", "authors": [{"name": "Ilya Sergey", "author_profile_id": "81436603558", "affiliation": "IMDEA Software Institute, Madrid, Spain", "person_id": "P4149071", "email_address": "ilya.sergey@imdea.org", "orcid_id": ""}, {"name": "Dominique Devriese", "author_profile_id": "81467668211", "affiliation": "KU Leuven, Leuven, Belgium", "person_id": "P4149072", "email_address": "dominique.devriese@cs.kuleuven.be", "orcid_id": ""}, {"name": "Matthew Might", "author_profile_id": "81309498719", "affiliation": "University of Utah, Salt Lake City, Utah, USA", "person_id": "P4149073", "email_address": "might@cs.utah.edu", "orcid_id": ""}, {"name": "Jan Midtgaard", "author_profile_id": "81100381173", "affiliation": "Aarhus University, Aarhus, Denmark", "person_id": "P4149074", "email_address": "jmi@cs.au.dk", "orcid_id": ""}, {"name": "David Darais", "author_profile_id": "81488671612", "affiliation": "Harvard University, Cambridge, Massachusetts, USA", "person_id": "P4149075", "email_address": "darais@seas.harvard.edu", "orcid_id": ""}, {"name": "Dave Clarke", "author_profile_id": "81100391212", "affiliation": "KU Leuven, Leuven , Belgium", "person_id": "P4149076", "email_address": "dave.clarke@cs.kuleuven.be", "orcid_id": ""}, {"name": "Frank Piessens", "author_profile_id": "81548011356", "affiliation": "KU Leuven, Leuven , Belgium", "person_id": "P4149077", "email_address": "frank.piessens@cs.kuleuven.be", "orcid_id": ""}], "doi_number": "10.1145/2491956.2491979", "year": "2013", "article_id": "2491979", "conference": "PLDI", "title": "Monadic abstract interpreters", "url": "http://dl.acm.org/citation.cfm?id=2491979"}