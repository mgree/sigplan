{"article_publication_date": "06-16-2013", "fulltext": "\n Fast Condensation of the Program Dependence Graph Nick P. Johnson Taewook Oh Princeton University, \nPrinceton, NJ {npjohnso, twoh, august}@princeton.edu Abstract Aggressive compiler optimizations are formulated \naround the Pro\u00adgram Dependence Graph (PDG). Many techniques, including loop .ssion and parallelization \nare concerned primarily with dependence cycles in the PDG. The Directed Acyclic Graph of Strongly Con\u00adnected \nComponents (DAGSCC) represents these cycles directly. The na\u00a8ive method to construct the DAGSCC .rst \ncomputes the full PDG. This approach limits adoption of aggressive optimizations because the number of \nanalysis queries grows quadratically with program size, making DAGSCC construction expensive. Consequently, \ncom\u00adpilers optimize small scopes with weaker but faster analyses. We observe that many PDG edges do not \naffect the DAGSCC and that ignoring them cannot affect clients of the DAGSCC. Exploiting this insight, \nwe present an algorithm to omit those analysis queries to compute the DAGSCC ef.ciently. Across 366 hot \nloops from 20 SPEC2006 benchmarks, this method computes the DAGSCC in half of the time using half as \nmany queries. Categories and Subject Descriptors D.3.4 [Programming Lan\u00adguages]: Processors Compilers \nKeywords Demand-driven analysis; Dependence analysis; Pro\u00adgram Dependence Graph; Strongly connected components. \n1. Introduction Users expect compilers to be fast; one study [34] indicates that pro\u00adgrammer productivity \ndrops when compilation takes more than a few seconds. If the bene.t of aggressive optimizations does \nnot outweigh the cost of long compile times, users will avoid them. Making matters worse, analysis precision \ndrastically affects opti\u00admization quality [3, 8, 28, 29, 36], and precise analyses tend to be more expensive \nthan their less-precise counterparts [7, 15] and scale poorly [16, 26]. Despite the performance potential \nof state-of\u00adthe-art transformations, common compilers optimize small, intra\u00adprocedural scopes, instead \nfavoring short compilation times. To extend the bene.ts of precise analysis to the wider commu\u00adnity, \nwe must .rst address compiler scalability. We envision a future where common compilers feature aggressive \noptimizations such as automatic parallelization [4, 24, 28 30, 35, 36] by default. The crit\u00adical path \nto this end is the precise analysis of large program scopes. Many compiler techniques are formulated \naround the Program Dependence Graph (PDG) [6]. Many of those techniques (clients Permission to make digital \nor hard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. PLDI 13, June 16 19, 2013, Seattle, WA, USA. \nCopyright c &#38;#169; 2013 ACM 978-1-4503-2014-6/13/06. . . $15.00 Ayal Zaks David I. August Intel Corporation, \nHaifa, Israel ayal.zaks@intel.com of the PDG) focus primarily on dependence cycles, identi.ed as the \nStrongly Connected Components (SCCs) of the PDG (Figure 1(b)). The Directed Acyclic Graph of the SCCs \n(DAGSCC) or conden\u00adsation of the PDG is a representation that makes dependence cycles explicit (Figure \n1(c)). The DAGSCC contains enough information to support a broad class of compiler techniques. For instance, \nauto\u00admatic parallelization [4, 24, 28 30, 35, 36] asks: can two operations execute concurrently without \nraces, or is synchronization needed? Determine whether their respective components are ordered in the \nDAGSCC. Program slicing [17, 37, 39] asks: Which earlier opera\u00adtions may affect an operation? All operations \nin the same compo\u00adnent and operations in components ordered before it in the DAGSCC. Loop .ssion [2, \n20] asks: Can a loop be split? Ensure that no oper\u00adations in the second half belong to the same component \nas, or any component ordered before components from the .rst half. The DAGSCC holds less information \nthan the PDG and should be cheaper to compute. Yet, standard practice wastefully builds the full PDG \nbefore condensing it to a DAGSCC. The number of potential PDG edges grows quadratically with the scope \nsize (in vertices). Each potential edge adds a quantum of analysis effort (a query) to determine whether \nthat edge exists. The running times of these queries sum to make DAGSCC construction prohibitively expensive, \nespecially since precise analyses are costly [7, 15, 16, 26]. Compiler authors should not sacri.ce analysis \nprecision for cost since imprecision limits optimization [3, 8, 28, 29, 36]. Instead, they should use \nthe most precise analyses and reduce compilation time by exploiting the reduced information of the DAGSCC. \nThis paper presents a technique that computes the DAGSCC more ef.ciently than by .nding SCCs of the full \nPDG. Using partial de\u00adpendence information, the algorithm identi.es dependence edges which cannot affect \nthe clients of the DAGSCC. Next, the algorithm uses a Demand-Driven Analysis framework [13, 32, 40] to \nelide those analysis queries and thus expend effort only on important analysis queries rather than the \nwhole program. This improvement is orthogonal to speeding up each query; it reduces DAGSCC con\u00adstruction \ntime yet maintains high analysis quality since no analysis algorithms change. With these savings, compiler \nauthors may pur\u00adsue more aggressive and costlier analyses while providing the same quality of service \nto compiler end-users. This work contributes: a fast client-agnostic DAGSCC algorithm (Section 3.2); \n an extension of that algorithm for PS-DSWP [28] (Section 3.3);  a proof of correctness (Section 4); \nand,  evaluation of the performance bene.t (Section 5).  Averaged across 366 hot loops from 20 SPEC2006 \nbench\u00admarks, the proposed method computes the DAGSCC with 49.0% fewer queries, reducing construction \ntime by 53.1% (Section 5.2). These savings dramatically affect compiler scalability. The pro\u00adposed method \nanalyzes half of the hot loops (weighted by cover\u00adage) in 456.6s on average, saving 111.5s over the baseline \n(Sec\u00adtion 5.3). These savings broaden the class of transformations that .t the user s time constraints \nwithout reducing analysis precision.  Figure 1: (a) Example PDG; (b) Strongly Connected Components; \n(c) Condensation of the example; (d) Additional edges which redundantly order components do not change \nthe condensation, thus have no marginal value; (e) Additional edges within a component do not change \nthe condensation, thus have no marginal value; (f) Additional edges which violate ordering are valuable \nsince they may change condensation. 2. Insight To construct the DAGSCC of a loop, a na\u00a8ive compiler considers \nthe presence or absence of a dependence edge between every pair of vertices (operations), and condenses \nthat into a DAGSCC. However, not all dependences in the PDG contribute equally to the structure of the \nDAGSCC. Once a PDG is partially computed, some edges have no marginal value since they do not affect \nthe structure of the DAGSCC and thus cannot affect the answer to optimization questions. By eliminating \nthese redundant dependence edges, a compiler computes the DAGSCC with fewer dependence queries in less \ntime. Compiler authors may spend these savings on costlier analyses in pursuit of aggressive optimization. \nAn ideal algorithm would perform queries only for those edges found in a transitive reduction of the \nPDG (to join components), as well as queries to ensure the absence of back edges (to separate components). \nThis, however, leads to a problem: the compiler does not know the PDG a priori, and so it cannot distinguish \nredundant edges from important ones. Instead, this paper proposes an approx\u00adimation of that ideal. Figure \n1(d) demonstrates one class of redundant dependences: edges that order two vertices whose components \nare already or\u00addered. This is a large class of dependences, which grows quadrati\u00adcally in the number \nof components and quadratically in component size. Across SPEC2006, empirical study indicates that two-thirds \nof all loops have 5 968 SCCs and two-thirds of all components have 8.4 1118.0 vertices. Another class \nof redundant dependences are demonstrated by Figure 1(e): edges within a component other than a minimum \ncycle that spans the component. This class grows quadratically in component size and linearly in the \nnumber of components. The only dependences which contribute to .nding the conden\u00adsation graph are the \nclass which join separate components, demon\u00adstrated in Figure 1(f). These grow quadratically in the size \nof com\u00adponents and quadratically in the number of components. Although this is a large class, any one \ndependence between a pair of compo\u00adnents will constrain the entire component. Conversely, the absence \nof these dependences also has value, since only after analysis re\u00adturns negative results can the algorithm \ncon.dently report that the separate components are separate. By periodically interrupting PDG construction \nto recompute strongly connected components, the proposed algorithm eliminates queries for dependences \nwhich are de.nitely in classes (d) and (e) while focusing on those dependence queries which seem to be \nwithin class (f). This approach is informed by the following heuristic: if the compiler can build large \ncomponents quickly, it can safely exclude more edges. Further, this technique performs more computation \nto actively search for opportunities to elide queries. This strategy will not be faster in the worst \ncase since the overhead of recomputing components may overwhelm the bene.ts for loops with a very low \naverage component size. However, the common case is more amenable to this strategy; experiments show \nthat the proposed method is faster for all but 14 of 366 loops. 3. Ef.cient DAGSCC Construction Algorithm \nIn a Program Dependence Graph (PDG), each instruction in the loop is represented as a vertex. Edges are \ndrawn to represent control and data dependences. Figure 2(a) shows an example of a PDG. Control dependences \nrepresent cases where one instruction may prevent another instruction from executing; for instance, an \nif\u00adstatement controls its then-and else-clauses. Data dependences represent the .ow of data between instructions. \nWe distinguish register data dependences from memory data dependences. Register and control dependences \nare computed quickly in practice. Memory dependences represent data .ow through a mem\u00adory location, or \nadditional constraints such as anti-and output\u00addependences. Exactly determining memory dependences is \nunde\u00adcidable. To mitigate this, standard practice allows analyses to fail: given a pair of operations \nwhich access memory, report no-or must-depend, or fall-back to may-depend when an answer cannot be decided. \nClients interpret may-depend conservatively. We use Query(v1.inst, v2.inst, type) to denote a demand-driven \nanalysis query that determines whether there is a memory depen\u00addence from the instruction associated \nwith vertex v1 to the in\u00adstruction associated with vertex v2; type is either Loop-Carried or Intra-Iteration. \nAny analysis no matter its internal structure or operation can be packaged to provide this query interface. \nIn the algorithms below, TarjanSCC refers to Tarjan s Algo\u00adrithm for Strongly Connected Components [33]. \nTarjan s algorithm reports SCC structure as well as a topological sort of those compo\u00adnents and runs \nin time linear in the number of vertices and edges. 3.1 Baseline Algorithm The baseline algorithm (Algorithm \n1) builds a full PDG, including all register, control and memory dependences. To .nd memory dependences, \nit queries every pair of vertices (corresponding to instructions in the IR) which access memory to determine \nif there is a loop-carried or intra-iteration memory dependence. It then computes the strongly connected \ncomponents of that PDG. let E := computeRegisterDeps(V ) . computeControlDeps(V ); foreach vertex vsrc \n. V which accesses memory do foreach vertex vdst . V which accesses memory do if Query(vsrc .inst, vdst.inst, \nLoop-Carried) then let E := E . {(vsrc, vdst, Loop-Carried)}; end if Query(vsrc .inst, vdst.inst, Intra-Iteration) \nthen let E := E . {(vsrc, vdst, Intra-Iteration)}; end end end return TarjanSCC(V, E);  3.2 Client-Agnostic \nAlgorithm We .rst present a client-agnostic version of the proposed algorithm, listed in Algorithm 2. \nThis algorithm is written without destructive updates to simplify the proofs in Section 4, though a real \nimplemen\u00adtation may save space by overwriting old values. We label points X and Y to clarify those proofs. \nSimilar to the baseline algorithm, the client-agnostic method starts by computing register and control \ndependences. This yields a PDG at Point X which is only partially computed since it lacks memory dependences. \nNext, it performs queries only be\u00adtween the vertices of select components in withTheGrain and againstTheGrain. \nThese components are selected so that they will quickly cause components to merge into larger components. \nThis leads to a savings in the number of memory dependence queries since dependences between vertices \nin a common com\u00adponent cannot further constrain the DAGSCC. For simplicity of implementation, we chose \nto recompute the SCCs by invoking TarjanSCC multiple times rather than using an incremental component \nmaintenance algorithm. This reduces the amount of code to write and allows us to use simpler data structures \ninternally. The cost of computing SCCs does not have a signi.cant impact on overall performance. Algorithm \n2: Client-Agnostic computeDagScc(V) let E := computeRegisterDeps(V ) . computeControlDeps(V ) ; let TopSort0 \n:= TarjanSCC(V, E) ; (Point X) let E0 := E . withTheGrain(E, TopSort0) ; for i = 1 to 8 do (Point Y) \nlet E' := againstTheGrain(TopSorti-1) ; if E' = \u00d8 then return TopSorti-1 ; end let E':= Ei-1 . E' ; i \nlet TopSorti := TarjanSCC(V, E') ; i let Ei := E'. withTheGrain(E'i, TopSorti) ; i end The routine withTheGrain \n(Algorithm 3) considers pairs of components cearly and clate where cearly appears before clate in the \ntopological sorting of components. We exploit the feature that Tar\u00adjan s algorithm provides a topological \nsorting of the components with no additional computation. Figure 2(a) shows a topological sort. withTheGrain \nonly performs queries that .ow along topo\u00adlogical order (i.e. from cearly to clate), and only between \ncomponents that are not already immediately ordered.1 Such queries neither 1 This test should exclude \ncomponents that are ordered. We use immediate ordering as a fast approximation to avoid transitive closure. \nConsequently, withTheGrain may conservatively add some edges from class 1(d). cause separate components \nto merge, nor invalidate the topologi\u00adcal sorting of components, as illustrated in Figure 2(b). Algorithm \n3: withTheGrain(E0, TopSort) let E' := \u00d8; let N := size(TopSort); for i = N-1 down to 0 do let clate \n:= TopSort(i); for j = i-1 down to 0 do let cearly := TopSort(j); if \u00achasEdge(cearly, clate, E0) then \nlet E' := E' . .ndOneEdge(cearly, clate); end end end return E ; The routine againstTheGrain (Algorithm \n4) searches for de\u00adpendences between pairs of components. Unlike withTheGrain, againstTheGrain only performs \nqueries which may add edges that violate topological sort order, i.e. those from a vertex in a com\u00adponent \nclate to a vertex in a topologically-earlier cearly. The rationale is that such queries quickly form \nlarger components (Figure 2(c)). Large components have a compounding effect, further reducing the number \nof queries performed later. This routine performs enough queries to test every absence of an edge if \nnone exists, allowing the algorithm to report that two components are separate. It may surprise some \nreaders that againstTheGrain has a break statement in its inner loop. This break stops searching for \nedges from a given component after it .nds one, allowing the algo\u00adrithm to recompute SCCs before resuming \nthe search. This break is not necessary for correctness, however, it bene.ts performance. By recomputing \nSCCs, this formulation of againstTheGrain merges SCCs early, actively searching for opportunities to \nskip more queries. Later invocations of againstTheGrain perform fewer queries because there are fewer, \nlarger components. Algorithm 4: againstTheGrain(TopSort) let E := \u00d8; let N := size(TopSort); for i = \nN-1 down to 0 do let clate := TopSort(i); for j = i-1 down to 0 do let cearly := TopSort(j); let E' \n:= .ndOneEdge(clate, cearly); let E := E . E'; if E' = \u00d8 then break; end end end return E; The routine \nfindOneEdge (Algorithm 5) performs queries from a source component to destination component. It stops \nafter it .nds the .rst edge between them since additional edges would order those two components redundantly. \n 3.3 Client-Aware DAGSCC Constructions The DAGSCC guides clients such as DSWP [29] or loop .s\u00adsion [2, \n20]. Some clients want more information than the DAGSCC offers. The proposed algorithm may be extended \nto needs of partic\u00adular clients. Despite these additional requirements, one can imple\u00adment these extensions \nwhile achieving comparable performance improvements over the baseline. Two dimensions characterize client-speci.c \nextensions of the algorithm: additional requirements of dependence information and opportunities to abort \nearly.  Figure 2: A partially computed PDG. (a) Topological sort (grey lines) imposes a total order \non the partially ordered components. (b) withTheGrain (Algorithm 3) performs queries to discover edges \nbetween components with increasing position. Such edges neither cause SCCs to merge nor invalidate the \ntopological sort. Here, a new edge is discovered from component three to six. (c) againstTheGrain (Algorithm \n4) performs queries to discover edges between components with decreasing position. Here, a new edge is \ndiscovered from component .ve to three. (d) When againstTheGrain discovers new edges the topological \nsort is invalidated and components may merge. Algorithm 5: .ndOneEdge(csrc, cdst) foreach vertex vsrc \n. csrc which accesses memory do foreach vertex vdst . cdst which accesses memory do if Query(vsrc .inst, \nvdst.inst, Loop-Carried) then return {(vsrc, vdst, Loop-Carried)}; end end end foreach vertex vsrc . \ncsrc which accesses memory do foreach vertex vdst . cdst which accesses memory do if Query(vsrc .inst, \nvdst.inst, Intra-Iteration) then return {(vsrc, vdst, Intra-Iteration)}; end end end return \u00d8; Parallel \nStage Decoupled Software Pipelining (PS-DSWP) is an illustrative example of such a client. PS-DSWP is \nan auto\u00admatic thread-extraction technique with great performance poten\u00adtial [28, 36]. PS-DSWP partitions \nthe DAGSCC into pipeline stages such that all communication and synchronization .ow forward in pipeline \norder (i.e. forbidding cyclic communication among worker threads). PS-DSWP delivers scalable speedups \nwhen a large paral\u00adlel stage is available; conversely, PS-DSWP does not transform the code when no signi.cant \nparallel stage is present. PS-DSWP requires slightly more dependence information than is present in the \nDAGSCC, thus creating a meaningful evaluation scenario. Beyond the DAGSCC, PS-DSWP classi.es each SCC \nas either DOALL or Sequential according to the absence or presence of loop-carried dependences. Parallel \nstages are assembled from the DOALL SCCs such that no loop-carried dependence exists among the operations \nin the parallel stage. Algorithm 2 does not guarantee that such queries will be performed. To support \nPS-DSWP, the algorithm must perform additional queries to classify each SCC as DOALL or Sequential. These \nadditional queries are still fewer than the full PDG and DAGSCC guides the compiler to search for such \nqueries. Furthermore, DAGSCC construction may abort as soon as no signi.cant parallel stage is possible. \nWe extend Algorithm 2 for the needs of PS-DSWP in Algo\u00adrithm 6. The routine checkReflexiveLC checks for \nloop-carried dependences from any operation in a DOALL SCC to itself, stop\u00adping after it .nds one. checkWithinSccLC \nchecks for loop-carried dependences from any operation located in a DOALL SCC to any other operation \nin the same SCC. The latter contains the former, but experience suggests that prioritizing re.exive queries \ntends to exclude many components from the parallel stage after only a lin\u00adear number of queries, whereas \nquerying in checkWithinSccLC is quadratic. At the end, the algorithm invokes checkWithinSccLC again since \ncomponents have grown, potentially including more loop-carried dependences. These checks are cheaper \nthan full PDG construction since they only query among DOALL SCCs. Algorithm 6: PS-DSWP-Aware computeDagScc(V) \nlet E := computeRegisterDeps(V ) . computeControlDeps(V ); let TopSort := TarjanSCC(V , E); abortIfPsInsubstantial(V,E,TopSort); \nlet E := E . checkRe.exiveLC(V ); abortIfPsInsubstantial(V,E,TopSort); let E := E . checkWithinSccLC(TopSort) \n; abortIfPsInsubstantial(V,E,TopSort); let E := E . withTheGrain(E, TopSort); while true do let E ' \n:= againstTheGrain(TopSort); if E ' = \u00d8 then break; end let E := E . E ' ; let TopSort := TarjanSCC(V \n, E ); abortIfPsInsubstantial(V,E,TopSort); let E := E . withTheGrain(E, TopSort) ; end let E := E . \ncheckBetweenDoallSccs(TopSort) ; abortIfPsInsubstantial(V,E,TopSort); let E := E . checkWithinSccLC(TopSort) \n; return TopSort ;  Loop-carried dependences between DOALL SCCs prevent the simultaneous assignment \nof those components to the parallel stage. The routine checkBetweenDoallSccs performs queries to .nd \nsuch dependences. These checks are cheaper than full PDG construction, since they only consider pairs \nof DOALL SCCs. abortIfPsInsubstantial cancels construction if no substantial parallel stage is present \nwhenever the upper bound on the parallel stage may change. For evaluation, we say a stage is substantial \nif it contains memory accesses or calls. 4. Proof of Correctness We present a proof that our proposed \nmethod (Algorithm 2) pro\u00adduces a DAGSCC that is equivalent to the one produced by the base\u00adline method \n(Algorithm 1), both in terms of partitioning the set of vertices V into the same SCCs, and in terms of \ndrawing the same edges between SCCs. Both algorithms partition the same set of vertices V . Let CB , \nCP represent the components returned by the baseline and proposed algorithms, respectively. Each algorithm \ncomputes its own set of edges EB and EP , respectively, between pairs of ver\u00adtices in V . Two components \nin the DAGSCC are connected with an edge if there exists an edge between members of those components: \nfor any components c1, c2 . CB, we write c1 .B c2 iff there is an edge (v1, v2) . EB such that v1 . c1 \nand v2 . c2. Similarly, for any components c1, c2 . CP , we write c1 .P c2 iff there is an edge (v1, \nv2) . EP such that v1 . c1 and v2 . c2. Let B(v) . CB denote the strongly connected component which contains \nvertex v as reported by the baseline algorithm. Let P (v) . CP denote the strongly connected component \nwhich contains v as reported by the proposed algorithm. We state our equivalence in Theorems 1 and 2. \nTheorem 1 (CB and CP induce the same partition of V ). For every t, u . V , B(t) = B(u) iff P (t) = P \n(u). Proof. Follows immediately from Lemmas 3 and 5. Theorem 2 ((CB, .B ) is isomorphic to (CP , .P )). \nFor every t, u . V , B(t) .B B(u) iff P (t) .P P (u). Proof. We construct a correspondence . = B(v) . \nP (v). Lemmas 3 and 5 show that . is a bijective function. Lemmas 4 and 6 show that t .B u iff .(t) .P \n.(u). We prove both Theorems using the following lemmas. Lemma 1 (Forward Preservation of Edges, Simpli.ed). \nIgnoring the break in Algorithm 4, if (t, u) . EB then P (t) .P P (u). Proof. During an invocation of \nthe proposed method (Algorithm 2), execution will necessarily reach Point Y. Components evolve during \nthe execution of the proposed algo\u00adrithm; to avoid confusion we refer to speci.c versions of the com\u00adponents. \nLet Pi(v) denote the strongly connected component which contains vertex v at Point Y in the i-th iteration \nof the loop. In other words, Pi(v) .nds the component that contains v within the vari\u00adable TopSorti-1. \nNote that P (v) is the value of Pi(v) during the .nal iteration. We consider three cases based on the \nrelative positions of Pi(t) and Pi(u) in the topological sort of components reported by TarjanSCC, observed \nat Point Y. Case 1: During any iteration i, Pi(u) appears before Pi(t) in the topological sort. During \nthat iteration, the invocation of againstTheGrain (Al\u00adgorithm 4) necessarily reaches an iteration during \nwhich clate = Pi(t). It visits every earlier component cearly, invoking findOneEdge on each until an \nedge is discovered. Ignoring the break statement in Algorithm 4, we will reach an iteration in which \ncearly = Pi(u). findOneEdge againstTheGrain: cearly .- clate . . TopSorti-1: \u00b7 \u00b7 \u00b7 Pi(u) \u00b7 \u00b7 \u00b7 Pi(t) \n\u00b7 \u00b7 \u00b7 findOneEdge (Algorithm 5) will perform queries between the elements of Pi(t) and Pi(u) until an \nedge is found. During the execution of the baseline algorithm, the call to Query(t.inst, u.inst, f ) \nreturns true given that (t, u) . EB. Note that Query depends only on its arguments, so it behaves the \nsame during the execution of the proposed algorithm. If findOneEdge reaches the iteration where (vsrc, \nvdst) = (t, u), then Query(t.inst, u.inst, f ) will again return true, thus adding the edge (t, u). The \nonly case it may not reach that itera\u00adtion is when findOneEdge .nds some other edge between those components. \nThus, Pi(t) .P Pi(u). Case 2: During any iteration i, Pi(t) appears at the same posi\u00adtion as Pi(u) in \nthe topological sort. That is, Pi(t) = Pi(u). By re.exivity, Pi(t) .P Pi(u). Case 3: Pi(u) never appears \nbefore or at the same position as Pi(t) in the topological sort during any iteration. P1(t) appears before \nP1(u) in the topological sort of compo\u00adnents during the .rst iteration of the loop. The topological sort \nis not updated between Point X and Point Y in the .rst iteration, so P1(t) appears before P1(u) in the \ntopological ordering before the invocation of withTheGrain (Point X in Algorithm 2). findOneEdge withTheGrain: \ncearly -. clate . . TopSorti-1: \u00b7 \u00b7 \u00b7 P1(v1) \u00b7 \u00b7 \u00b7 P1(v2) \u00b7 \u00b7 \u00b7  The algorithm withTheGrain necessarily \nreaches an iteration during which cearly = P1(t) and clate = P1(u). If there is not already an immediate \nordering relationship P1(t) .P P1(u), withTheGrain passes those components to findOneEdge. Since (t, \nu) . EB, we know that Query(t.inst, u.inst, f ) returned true. Thus, findOneEdge must .nd an edge (either \n(t, u) or an earlier one) between these components: P1(t) .P P1(u). In all cases, we have Pi(t) .P Pi(u) \nfor some i. Observe that the proposed algorithm may add edges to the graph, but never removes edges \nfrom the graph. Adding edges may cause two separate components to merge into one, but never splits a \ncomponent. Thus, for any vertex v and iteration i: Pi-1(v) . Pi(v). Since P (v) is the value of Pj (v) \nin the .nal iteration j, it follows that P (t) .P P (u). Lemma 2. Considering the break in Algorithm \n4, if (t, u) . EB then P (t) .P P (u). Proof. The only difference between the simpli.ed and proposed \nalgorithms occurs in Lemma 1, Case 1: during iteration i, Pi(u) appears before Pi(t) in the topological \nsort. The invocation of againstTheGrain (Algorithm 4) necessar\u00adily reaches an iteration during which \nclate = Pi(t). It visits every earlier component cearly invoking findOneEdge until an edge is dis\u00adcovered. \nSuppose there is an intervening component Pi(x) Pi(u) = such that findOneEdge discovers an edge (w, \nx) from w . Pi(t) to x . Pi(x). This edge causes the loop to break before visiting cearly = Pi(u). findOneEdge \n againstTheGrain: cearly .- clate . . TopSorti-1: \u00b7 \u00b7 \u00b7 Pi(u) \u00b7 \u00b7 \u00b7 Pi(x) \u00b7 \u00b7 \u00b7 Pi(t) \u00b7 \u00b7 \u00b7 . break \nAfter the new edge is found, the algorithm recomputes compo\u00adnents and may change their relative positions. \nEither Pi+1(u) pre\u00adcedes Pi+1(t) in the topological sort TopSorti+1, or they merge, or Pi+1(t) precedes \nPi+1(u). In the latter case, the subsequent in\u00advocation of withTheGrain immediately detects an edge from \na vertex in Pi+1(t) to a vertex in Pi+1(u). Thus we need only con\u00adsider the case in which they maintain \ntheir relative topological or\u00adder. We argue inductively that such an iteration of Algorithm 2 will be \nfollowed by another iteration that falls into Case 1, yet has one fewer intervening component. Assume \nthat Pi+1(u) precedes Pi+1(t). Observe that the component Pi+1(x) cannot appear be\u00adfore Pi+1(t) because \nof the newly discovered edge (w, x). Conse\u00adquently, there is one fewer intervening component that could \ncause an later invocations of againstTheGrain to break. As a new edge (w, x) was found, the loop in Algorithm \n2 will perform at least one more iteration. Thus, in the next iteration againstTheGrain will be one break \ncloser to Lemma 1. After suf.cient iterations, all in\u00adtervening components have been eliminated and Lemma \n1 Case 1 applies. Lemmas 1 and 2 demonstrate that edges in EB will order components in CP . We next \nstrengthen this statement to show that edges between components in CB will order components in CP in \nLemma 4, but .rst we prove the following. Lemma 3 (Wholeness of Components, Forward). For any vertices \nt, u . V , if B(t) = B(u) then P (t) = P (u). Proof. Vertices t and u belong to the same strongly connected \ncomponent of CB, so there is a path from t to u: (t, t1) , (t1, t2) , . . . , (tj-1, tj ) , (tj , u) \n. EB and a path from u to t: (u, u1) , (u1, u2) . . . , (uk-1, uk) , (uk, t) . EB. By Lemma 2 this implies \nthat there is a cycle across the cor\u00adresponding components of CP : P (t) .P P (t1) .P . . . .P P (tj \n) .P P (u) .P P (u1) .P . . . .P P (uk) .P P (t). This, in turn, implies that P (t) = P (t1) = . . . \n= P (tj ) = P (u) = P (u1) = . . . = P (uk). Lemma 4 (Preservation of Structure, Forward). For any vertices \nt, u . V , if B(t) .B B(u) then P (t) .P P (u). Proof. By de.nition of .B , there is an edge (x, y) . \nEB such that x . B(t) and y . B(u). By Lemma 2 we know P (x) .P P (y). Since components are a partition \nof all vertices, x . B(t) implies B(t) = B(x). Similarly, B(u) = B(y). By Lemma 3, P (t) = P (x) and \nP (u) = P (y). Thus, P (t) .P P (u). Lemma 5 (Wholeness of Components, Reverse). For any two vertices \nt, u . V , if P (t) = P (u) then B(t) = B(u). Proof. Vertices t and u belong to the same strongly connected \ncomponent of CP , so there is a path from t to u: (t, t1) , (t1, t2) , . . . , (tj-1, tj ) , (tj , u) \n. EP and a path from u to t: (u, u1) , (u1, u2) . . . , (uk-1, uk) , (uk, t) . EP . Since the baseline \nperforms all queries, EP . EB , and the same a cycle connects the corresponding components of CB : B(t) \n.P B(t1) .P . . . .P B(tj ) .P B(u) .P B(u1) .P . . . .P B(uk) .P B(t). This, in turn, implies that B(t) \n= B(t1) = . . . = B(tj ) = B(u) = B(u1) = . . . = B(uk). Lemma 6 (Preservation of Structure, Reverse). \nFor any vertices t, u . V , if P (t) .P P (u) then B(t) .B B(u). Proof. By de.nition of .P , there is \nan edge e = (x, y) . EP such that x . P (t) and y . P (u). Since components are a partition of vertices, \nP (x) = P (t) and P (y) = P (u). By Lemma 5, it follows that B(x) = B(t) and B(y) = B(u). Since EP . \nEB , e . EB and therefore B(x) .B B(y). By substitution we obtain that B(t) .B B(u) as desired. 5. Empirical \nValidation To evaluate this technique, we implement the baseline (Sec\u00adtion 3.1), client-agnostic (Section \n3.2), and PS-DSWP-aware (Sec\u00adtion 3.3) algorithms in the LLVM infrastructure [21] revision 164307. Each \nalgorithm is augmented with a 30 minute timeout. Each algorithm uses the same data structures to represent \nthe program dependence graph and strongly connected components. The PDG data structure is a sorted adjacency-list \nrepresentation, which performs well since PDGs tend to be sparse graphs. The data structure is capable \nof representing partial knowledge of memory dependences: between any pair of vertices, a memory dependence \nis present, absent, or unknown. Thus, none of the algorithms will ever perform the same query more than \nonce. The cost of manipu\u00adlating the data structure had negligible effect on most experiments. We evaluated \nthese techniques on 20 SPEC 2006 benchmarks [31]. The experiments exclude eight FORTRAN benchmarks because \nthe front-end supports only C and C++. Each benchmark was com\u00adpiled under two optimization regimens. \nThe less-optimized regi\u00admen uses clang -O1. The more-optimized regimen is designed to create larger scopes \nthat are harder to analyze. Speci.cally, we apply internalization,2 devirtualization of indirect calls, \nand -O3. We pro.le each benchmark to identify 366 hot loops. Hot loops are those loops whose running \ntime consumes at least 5% of appli\u00adcation running time, and which perform at least .ve iterations per \ninvocation, on average. The hot loops found among the benchmarks are summarized in Table 1. It is not \nalways possible to correlate hot loops between the less-and more-optimized regimens; optimiza\u00adtion may \nbreak a hot loop into several, or reduce the execution time of a loop below the threshold. Experiments \nrun on an eight core 1.6GHz Xeon E5310. The ma\u00adchine has 8GB RAM and runs 64-bit Linux 2.6.32. All benchmarks \nare compiled to 64-bit, little-endian code. In this section, we use in\u00adstruction to refer to an LLVM \nvirtual instruction. All measurements experienced negligible variance. 5.1 Evaluation Analysis Framework \nThe overall performance bene.t of the proposed algorithms de\u00adpends greatly upon the performance characteristics \nof the underly\u00ading analysis framework. Many algorithms implement dependence analysis [9, 10, 13, 18, \n22, 23, 32, 38, 40], yet these algorithms are not easily compared. Each occupies a distinct niche in \nthe precision-ef.ciency trade-off [14]. We .x a single dependence analysis framework across all ex\u00adperiments. \nThis framework combines separate analyses under a common, demand-driven interface. The interface accepts \nqueries about the intra-/inter-iteration dependence relationship between two memory operations with respect \nto a loop of interest. As each query enters the analysis framework, it passes through each analy\u00adsis \nin turn to .nd the most optimistic answer; thus, the combination features the strengths of each member. \nThe evaluation analysis includes nineteen separate analyses de\u00adveloped internally at Princeton. These \nanalyses are designed to sup\u00adport automatic thread-extraction in general purpose codes such as SPEC INT \n[31]. Thus, our analyses emphasize precision in codes with linked-data structures and are sensitive to \nloops. The suite of analyses includes control-and data-.ow sensitive analyses, calling\u00adcontext sensitive \nanalyses, induction-variable analyses, analyses specializing in external functions from the C and C++ \nstandard li\u00adbraries, a rudimentary shape analysis, and analyses which reason about call sites. These \nanalyses are either purely demand-driven or 2 Internalization asserts that the input program is the whole \nprogram, i.e. that no external libraries reference any of the program s exported symbols. It is similar \nto marking all global symbols with C s static keyword. Benchmark Less-Optimized Regimen More-Optimized \nRegimen Hot Loops Coverage Size Hot Loops Coverage Size Hottest Coldest Largest Smallest Hottest Coldest \nLargest Smallest 400.perlbench 4 25.6% 5.5% 163 (#1) 9 (#4) 3 25.8% 11.1% 266 (#2) 66 (#3) 401.bzip2 \n9 73.5% 8.5% 597 (#7) 7 (#9) 9 71.5% 5.6% 2236 (#9) 7 (#8) 403.gcc 16 79.2% 5.0% 5800 (#1) 7 (#12) 11 \n79.8% 5.4% 11326 (#1) 40 (#2) 429.mcf 7 99.9% 6.3% 81 (#4) 26 (#1) 8 99.7% 8.6% 1352 (#1) 47 (#8) 433.milc \n9 52.5% 7.5% 159 (#1) 12 (#9) 15 32.5% 5.4% 298 (#1) 19 (#7) 435.gromacs 5 99.9% 18.6% 671 (#1) 23 (#5) \n8 99.4% 6.2% 10191 (#1) 72 (#7) 444.namd 16 99.9% 5.2% 1266 (#10) 9 (#2) 21 100.0% 6.1% 1271 (#14) 66 \n(#10) 445.gobmk 20 100.0% 5.0% 3868 (#7) 12 (#11) 20 99.9% 5.3% 3099 (#7) 39 (#13) 447.dealII 20 100.0% \n5.5% 140 (#17) 10 (#10) 16 100.0% 5.6% 788 (#4) 6 (#6) 450.soplex 6 50.7% 6.4% 118 (#5) 15 (#6) 9 69.4% \n5.5% 1034 (#4) 15 (#7) 453.povray 6 99.9% 28.8% 90 (#5) 23 (#6) 7 99.9% 5.6% 258 (#1) 13 (#7) 456.hmmer \n6 100.0% 6.4% 277 (#2) 11 (#4) 6 100.0% 7.2% 240 (#1) 13 (#5) 458.sjeng 7 100.0% 9.5% 779 (#4) 147 (#1) \n9 99.9% 5.4% 3359 (#7) 13 (#8) 462.libquantum 15 74.2% 4.9% 49 (#4) 5 (#6) 12 94.6% 5.7% 97 (#1) 9 (#11) \n464.h264ref 8 100.0% 6.7% 680 (#8) 159 (#3) 8 100.0% 11.1% 1483 (#8) 128 (#3) 470.lbm 2 99.8% 99.1% 475 \n(#2) 23 (#1) 2 99.6% 99.0% 1175 (#1) 475 (#2) 471.omnetpp 2 100.0% 13.2% 23 (#1) 23 (#1) 2 100.0% 19.0% \n37 (#2) 22 (#1) 473.astar 9 65.4% 5.8% 61 (#3) 9 (#9) 12 56.6% 6.7% 238 (#1) 17 (#6) 482.sphinx3 10 95.0% \n6.6% 429 (#2) 12 (#10) 8 94.5% 5.0% 2170 (#2) 12 (#1) 483.xalancbmk 2 98.0% 7.2% 28 (#2) 12 (#1) 1 97.6% \n97.6% 36 (#1) 36 (#1) Table 1: Hot loops from SPEC2006. Coverage is the percent of running time spent \nin the loop. Size is the number of LLVM IR instructions contained in the loop. Largest and smallest also \ncontain the loop id, where #1 is the hottest loop, and #n is the coldest. are largely demand-driven, \ni.e., a signi.cant portion of analysis ef\u00adfort is performed in response to a query, not ahead of time. \nAnalysis services most queries quickly: half of all queries take less than 287.6\u00b5s (460K cycles); two \nthirds take less than 601.3\u00b5s (962K cycles); 90% take less than 1.0ms (2M cycles). Differences in query \nrunning time are due to differences in query complexity: for instance, analyzing a call site is generally \nmore expensive than analyzing a load instruction. Across multiple runs, the running time of any one query \nexhibits negligible variance, suggesting that noise has minimal impact on timing results.  5.2 Performance \nImprovement The most direct impact of the proposed algorithm is a reduction in DAGSCC construction latency. \nFigure 4(a) shows the time required to construct a DAGSCC for both the client-agnostic and PS-DSWP-aware \nalgorithms. Each point represents a loop from the less-or more-optimized regimen, normalized to the running \ntime of the baseline algorithm (smaller is better). The client-agnostic method is faster for all but \n14 of 366 loops. Performance improvements are due primarily to a reduction in the number of dependence \nanalysis queries. Empirical results concur with the claim that the client-agnostic algorithm normalized \nrunning time is linear in the normalized number of queries. The Pearson s Correlation between the normalized \nconstruction time and normalized number of queries is 0.63. Figures 4(b) (d) show factors which contribute \nto the reduc\u00adtion in queries. The fraction of queries performed by the client\u00adagnostic method is related \nto both the average size of SCCs as well as the number of SCCs, yet is only mildly affected by the size \nof the region. This is because the algorithm elides queries for a class of redundant edges that grows \nwith both average SCC size and number of SCCs (illustrated in Figure 1(d) (e)). Empirical re\u00adsults concur \nwith the claim that the client-agnostic method elides a greater fraction of queries in loops with fewer \nor larger compo-Figure 3: Largest sequence of hot loops analyzed before timeout.  nents. The Spearman \ns Rank3 between the average SCC size and normalized number of queries is -0.52. The Spearman s Rank be\u00adtween \nthe number of SCCs and the normalized number of queries is 0.24. One extreme outlier experiences more \nthan 2\u00d7 slowdown: the fourth-hottest loop from 458.sjeng, located in function std eval. In that loop, \nthe proposed methods decrease the number of queries and the time spent on analysis queries. The cost \nof computing SCCs several times is less than the savings from fewer queries. However, the overhead of \nmanipulating the sparse graph data structure is exceptionally high for this loop, canceling the savings. \nFurther en\u00adgineering work could reduce this overhead. 3 Spearman s Rank is a measure of statistical dependence \n[19]. We use Spearman s Rank to support the claim of a monotone relationship, which is strictly weaker \nthan a linear relationship indicated by Pearson s Correlation.  (a) Improvement in Time vs Improvement \nin Queries (b) Improvement in Queries vs Size of Region (c) Improvement in Queries vs Number of SCCs \n(log scale) (d) Improvement in Queries vs Average SCC Size (log scale) Figure 4: Improvement in running \ntime and in number of queries, normalized to the baseline method. 5.3 Effect on Compiler Scalability \nAnalysis of whole benchmarks matters more than analysis of single loops. Compilers consider hot loops \nto plan an optimization strat\u00adegy. We consider the question: what is the largest sequence of loops which \ncan be analyzed? both in terms of the total number of loops, and in terms of the sum of execution coverage. \nFigure 3 considers the largest sequences of loops that can be analyzed before varying limits on analysis \ntime (log scale). To sim\u00adulate a very large application, this experiment allows the compiler to select \nany of the loops from the entire benchmark suite. The client-agnostic method analyzes more loops than \nthe baseline under the same time constraints. The PS-DSWP client-aware extensions cause a slight performance \ndegradation from client-agnostic yet are still more ef.cient than the baseline. Not all loops are equally \nvaluable. Amdahl s law encourages compilers to ration their time budget towards hot loops. Figure 5 explores \nhow many hot loops (weighted by coverage) each method analyzes by a certain time. The compiler considers \neach loop from hottest to least-hot. On average, the client-agnostic method ana\u00adlyzes 50% of the hot \nloops 111.5s before the baseline, and the client-aware method achieves that 106.1s before the baseline. \nThe proposed methods allow an optimizing compiler to analyze the code which most contributes to running \ntime in shorter use cycles. 6. Related Work Harrold et al. [12] present an algorithm to ef.ciently compute \ncon\u00adtrol dependences for a PDG. This technique does not address mem\u00adory dependence analysis. In our setup, \ncontrol analysis is cheap; memory analysis dominates construction time. There is theoretical evidence \nthat precise analysis must be expensive [16, 26]. Scalability of memory analysis is an area of ongoing \nresearch. Approaches can be classi.ed along levels of abstraction: low-level approaches improve the ef.ciency \nof the analysis, without consid\u00adering the client; mid-level approaches assume some properties of the \nclient to improve ef.ciency; high-level approaches modify the client so it more judiciously employs analysis \nor restructure the code so analysis will perform better. Low-Level Many analyses are implemented as Maximum \nFixed Points (MFP) of a set of data-.ow equations. Several works op\u00adtimize how programs are reduced to \ndata-.ow equations. John\u00adson et al. observe that formulating these equations with respect to the control \n.ow graph is wasteful, proposing instead to formulate them along the Data-Flow Graph thereby reducing \nthe number of identity relationships [18]. Similarly, Duesterwald et al. attempt to optimize the set \nof equations by identifying congruent equa\u00adtions through idempotency and common sub-expression elimina\u00adtion \nprior to computing the MFP [5]. Both techniques eliminate 120.7ms 471.omnetpp 10.5ms 10.8ms 306.3ms \n473.astar 139.3ms 195.5ms 3.3s 447.dealII 166.3ms 168.2ms 364.1ms 470.lbm  174.6ms 188.5ms 493.4ms \n450.soplex 223.4ms 228.2ms 778.0ms 401.bzip2 333.4ms 346.7ms 1.4s 429.mcf 665.6ms 678.5ms 799.2ms 462.libquantum \n764.2ms 60.0ms 43.1s 444.namd 1.3s 67.8ms 15.6s 433.milc 9.1s 9.5s 19.4s 456.hmmer 14.0s 14.0s 51.9s \n482.sphinx3 26.8s 30.9s 582.8s 453.povray 323.7s 459.1s 1007.6s 458.sjeng 383.8s 385.4s (30 min) 445.gobmk \n693.9s 773.3s (30 min) 435.gromacs 837.1s 839.0s (30 min) 400.perlbench (30 min) (30 min) (30 min) 403.gcc \n(30 min) (30 min) (30 min) 464.h264ref (30 min) (30 min) More-Optimized Regimen T50% 41.1ms 2.8ms 2.9ms \n3.8s 20.9ms 21.9ms 3.4s 1.5s 1.6s 4.0s 284.5ms 299.2ms 64.8s 29.8s 29.5s 214.5s 94.1s 95.2s 33.9s 5.0s \n5.3s 204.7s 62.1s 62.1s 1.7s 857.9ms 138.7ms 142.3s 47.1s 49.3s 62.8s 33.8s 35.7s 11.9s 3.6s 3.8s 428.3s \n177.4s 175.8s (30 min) (30 min) (30 min) (30 min) 1113.2s 1103.7s 18.1s 2.3s 2.3s (30 min) (30 min) (30 \nmin) (30 min) (30 min) (30 min) (30 min) (30 min) (30 min) (30 min) (30 min) (30 min) Figure 5: The \nclient-agnostic, client-aware and baseline methods analyzing each benchmark. The horizontal axis measures \ntime (seconds) from 1ms to 30 minutes on a log scale. The vertical axis is the fraction of loops analyzed \nbefore a that time, weighted by loop coverage. The compiler analyzes loops from hottest to coldest. T50% \nshows the times when the Baseline, Client-agnostic, and PS-DSWP Client-aware methods reach 50% or (30 \nmin) if they time out .rst. The client-agnostic method reaches 50% on average 111.5s earlier than baseline. \n unnecessary relationships among program points, reducing MFP convergence time. Calling-context sensitivity \nis a challenge for scalability: proce\u00addures exhibit drastically different memory access behavior depend\u00ading \non actual parameters and global state. One response has been the use of procedure summaries, i.e. functions \nwhich compute a points-to set for a procedure as a function of its parameters and globally accessible \nstate [9, 22]. In such solutions, there are still exponentially many calling contexts, yet summaries \nallow large numbers of contexts to be evaluated quickly. A complementary ap\u00adproach employs Datalog solvers \nwhich scale to the huge number of calling contexts using Binary Decision Diagrams [38]. This ap\u00adproach \nis restricted to analyses expressible in Datalog. Mid-Level Demand-driven analyses employ an orthogonal \nap\u00adproach [13, 32, 40]. Under the observation that many clients need only a fraction of all analysis \nqueries, a demand-driven approach shifts effort from preprocessing into each query. This scales better \nas it only expends effort on queries which affect the client. However, these approaches do not inform \ncompiler authors of how to (re-)structure analysis clients as to perform the minimum number of analysis \nqueries. The algorithms proposed in this paper have a synergistic relationship with demand-driven analyses \nsince the proposed algorithms actively attempts to perform only select queries while still supporting \nadvanced clients. The Pruning-Re.nement method [23] considers a spectrum of Datalog dependence analyses \nranging from cheap-yet-imprecise to expensive-yet-precise. It .rst applies cheaper analyses to queries \nand, if successful, returns that answer to the client. Otherwise, it runs the slower, more precise analyses, \nusing a byproduct of cheap analyses to prune extraneous inputs and reduce running times. This method \nhas dramatic bene.ts on memory usage, but its running time improvements are less pronounced. This method \nis only ap\u00adplicable to analyses which can be formulated in Datalog. High-Level The structure of input \ncode, in particular the divi\u00adsion into procedures, has an effect on analysis and optimization. Program \nrestructuring techniques can improve analyzability. Pro\u00adcedure inlining and partial inlining not only \nreduce the overhead of a procedure call, but also improve memory analysis by disam\u00adbiguating the relationship \nof call sites and callees [1, 25]. Region formation chooses scopes independently of procedure boundaries \nto make interprocedural analysis and optimization scale [11]. Ohata et al. [27] propose merging program \nstatements before building a PDG to reduce memory consumption and analysis time. They modify analysis \nto treat groups of statements as one, leading to faster convergence, similar to Duesterwald et al. [5]. \nHowever, this approach is imprecise and falsely reports that some statements belong to a slice because \nthey are merged with a statement truly in the slice. This technique can only merge non-call statements \nwhich are adjacent in the program source code and control equivalent, and requires tight integration \nwith analysis. Our technique calls for no modi.cations to analysis. Client-driven approaches [10] use \ncheap analyses .rst and retroactively apply precise analyses when imprecision limits the client. This \napproach requires analysis to track imprecision due to polluting assignments, and clients to request \ngreater precision in important cases. This approach improves performance only when those important cases \nare less common than polluting assignments. In terms of the DAGSCC, an augmented client must identify \nthose conservative edges whose removal would split a component into several. In contrast, our method \nalways uses the most precise anal\u00adysis available and only queries edges that may merge components. When \nused to improve analysis quality, the high-level ap\u00adproaches share a common failing: they con.ate the \nseparate con\u00adcerns of analysis and transformation and break abstractions that are useful for the development \nof compilers. 7. Conclusion The DAGSCC provides strong insight into dependence structure over a large \nprogram scope and is suf.cient to drive a large class of compiler optimizations. This paper demonstrates \nthat the DAGSCC can be computed much more ef.ciently than the na\u00a8ive method of computing the strongly \nconnected components of the full PDG. The savings from this technique allow a compiler to analyze larger \nscopes while still providing short turnaround times to the com\u00adpiler s user. This makes aggressive optimization \nof large scopes palatable to a wider audience, and contributes to the universal de\u00adployment of aggressive \nwhole-program optimization. Acknowledgments We thank the entire Liberty Research Group for their support \nand feedback during this work. We also thank the anonymous reviewers for their insightful comments. Additionally, \nwe thank CJ Bell, Sid Sen, Sushant Sachdeva, and Chris Monsanto for commenting on early drafts. This \nmaterial is based on work supported by National Science Foundation Grants 0964328 and 1047879. All opinions, \n.ndings, conclusions, and recommendations expressed throughout this work are those of the authors and \ndo not necessarily re.ect the views of the aforementioned funding agencies. References [1] R. Allen and \nS. Johnson. Compiling C for vectorization, paralleliza\u00adtion, and inline expansion. In Proceedings of \nthe ACM SIGPLAN 1988 Conference on Programming Language Design and Implementation (PLDI), pages 241 249, \nJune 1988. [2] U. Banerjee. Loop Transformations for Restructuring Compilers: The Foundations. Kluwer \nAcademic Publishers, Norwell, MA, 1993. [3] T. Chen, J. Lin, W. Hsu, and P. Yew. An empirical study on \nthe gran\u00adularity of pointer analysis in C programs. Languages and Compilers for Parallel Computing (LCPC), \npages 157 171, 2005. [4] R. Cytron. DOACROSS: Beyond vectorization for multiprocessors. In Proceedings \nof the 1986 International Conference on Parallel Pro\u00adcessing (ICPP), pages 836 884, 1986. [5] E. Duesterwald, \nR. Gupta, and M. L. Soffa. Reducing the cost of data .ow analysis by congruence partitioning. In In International \nCon\u00adference on Compiler Construction, pages 357 373. Springer-Verlag, 1994. [6] J. Ferrante, K. J. Ottenstein, \nand J. D. Warren. The program depen\u00addence graph and its use in optimization. ACM Transactions on Pro\u00adgramming \nLanguages and Systems, 9:319 349, July 1987. [7] J. S. Foster, M. F \u00a8ahndrich, and A. Aiken. Polymorphic \nversus monomorphic .ow-insensitive points-to analysis for C. In Proceed\u00adings of the 7th International \nSymposium on Static Analysis (SAS), pages 175 198, London, UK, UK, 2000. Springer-Verlag. [8] R. Ghiya, \nD. Lavery, and D. Sehr. On the importance of points-to anal\u00adysis and other memory disambiguation methods \nfor C programs. In Proceedings of the ACM SIGPLAN 2001 Conference on Programming Language Design and \nImplementation (PLDI), pages 47 58. ACM Press, 2001. [9] B. Guo, M. J. Bridges, S. Triantafyllis, G. \nOttoni, E. Raman, and D. I. August. Practical and accurate low-level pointer analysis. In Proceedings \nof the 3rd International Symposium on Code Generation and Optimization (CGO), March 2005. [10] S. Z. \nGuyer and C. Lin. Client-driven pointer analysis. In In Inter\u00adnational Static Analysis Symposium, pages \n214 236. Springer-Verlag, 2003. [11] R. E. Hank, W. W. Hwu, and B. R. Rau. Region-based compilation: \nAn introduction and motivation. In Proceedings of the 28th Annual Inter\u00adnational Symposium on Microarchitecture, \npages 158 168, December 1995. [12] M. J. Harrold, B. Malloy, and G. Rothermel. Ef.cient construction \nof Program Dependence Graphs. In Proceedings of the 1993 ACM [13] N. Heintze and O. Tardieu. Demand-driven \npointer analysis. In Proceedings of the ACM SIGPLAN 2001 conference on Programming language design and \nimplementation (PLDI), pages 24 34, New York, NY, 2001. [14] M. Hind. Pointer analysis: Haven t we solved \nthis problem yet? In 2001 ACM SIGPLAN-SIGSOFT Workshop on Program Analysis for Software Tools and Engineering \n(PASTE), 2001. [15] M. Hind and A. Pioli. Evaluating the effectiveness of pointer alias analyses. In \nScience of Computer Programming, pages 31 55, 1999. [16] S. Horwitz. Precise .ow-insensitive may-alias \nanalysis is NP-hard. ACM Transactions on Programming Languages and Systems, 19(1), January 1997. [17] \nS. Horwitz and T. Reps. The use of program dependence graphs in software engineering. In In proceedings \nof the Fourtheenth Interna\u00adtional Conference on Software Engineering (CSE), pages 392 411, 1992. [18] \nR. Johnson and K. Pingali. Dependence-based program analysis. In In Proceedings of the SIGPLAN 93 Conference \non Programming Language Design and Implementation (PLDI), pages 78 89, 1993. [19] M. G. Kendall. Rank \nCorrelation Methods. Charles Grif.n and Company, Limited, London, 1948. [20] K. Kennedy and J. R. Allen. \nOptimizing compilers for modern archi\u00adtectures: a dependence-based approach. Morgan Kaufmann Publish\u00aders \nInc., San Francisco, CA, USA, 2002. [21] C. Lattner and V. Adve. LLVM: A compilation framework for lifelong \nprogram analysis &#38; transformation. In Proceedings of the Annual In\u00adternational Symposium on Code \nGeneration and Optimization (CGO), pages 75 86, 2004. [22] C. Lattner, A. Lenharth, and V. Adve. Making \nContext-Sensitive Points-to Analysis with Heap Cloning Practical For The Real World. In Proceedings of \nthe 2007 ACM SIGPLAN Conference on Program\u00adming Language Design and Implementation (PLDI), San Diego, \nCali\u00adfornia, June 2007. [23] P. Liang and M. Naik. Scaling abstraction re.nement via pruning. In Proceedings \nof the 2011 ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI). [24] W. \nLiu, J. Tuck, L. Ceze, W. Ahn, K. Strauss, J. Renau, and J. Torrel\u00adlas. POSH: a TLS compiler that exploits \nprogram structure. In PPoPP 06: Proceedings of the 11th ACM SIGPLAN Symposium on Principles and Practice \nof Parallel Programming, pages 158 167, 2006. [25] R. Muth and S. Debray. Partial inlining. Technical \nreport, Department of Computer Science, University of Arizona, 1997. [26] R. Muth and S. Debray. On the \ncomplexity of .ow-sensitive data.ow analyses. In In Proc. ACM Symp. on Principles of Programming Languages, \npages 67 80. ACM Press, 2000. [27] F. Ohata, A. Nishimatsu, and K. Inoue. Analyzing dependence locality \nfor ef.cient construction of program dependence graph. Information and Software Technology, 42(13):935 \n 946, 2000. [28] E. Raman, G. Ottoni, A. Raman, M. Bridges, and D. I. August. Parallel-stage decoupled \nsoftware pipelining. In Proceedings of the Annual International Symposium on Code Generation and Optimiza\u00adtion \n(CGO), 2008. [29] R. Rangan, N. Vachharajani, M. Vachharajani, and D. I. August. De\u00adcoupled software \npipelining with the synchronization array. In Pro\u00adceedings of the 13th International Conference on Parallel \nArchitec\u00adtures and Compilation Techniques (PACT), pages 177 188, September 2004. [30] L. Rauchwerger \nand D. A. Padua. The LRPD test: Speculative run\u00adtime parallelization of loops with privatization and \nreduction par\u00adallelization. IEEE Transactions on Parallel Distributed Systems, 10:160 180, February 1999. \n[31] Standard Performance Evaluation Corporation. http://www.spec.org. [32] M. Sridharan, D. Gopan, L. \nShan, and R. Bod\u00b4ik. Demand-driven points-to analysis for java. In Proceedings of the 20th annual ACM \nSIGPLAN conference on Object-oriented programming, systems, lan\u00adguages, and applications (OOPSLA), pages \n59 76, New York, NY, 2005. [33] R. E. Tarjan. Depth-.rst search and linear graph algorithms. SIAM Journal \non Computing, 1(2):146 160, 1972. [34] A. J. Thadhani. Factors affecting programmer productivity during \napplication development. IBM Systems Journal, 23(1):19 35, 1984. [35] C. Tian, M. Feng, V. Nagarajan, \nand R. Gupta. Copy or discard ex\u00adecution model for speculative parallelization on multicores. In Pro\u00adceedings \nof the 41st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), pages 330 341, Washington, \nDC, 2008. IEEE Computer Society. [36] H. Vandierendonck, S. Rul, and K. De Bosschere. The Paralax infras\u00adtructure: \nAutomatic parallelization with a helping hand. In Proceed\u00adings of the 19th International Conference on \nParallel Architecture and Compilation Techniques (PACT), pages 389 400, 2010. [37] M. Weiser. Program \nslicing. In Proceedings of the 5th international conference on Software engineering, (ICSE), pages 439 \n449, Piscat\u00adaway, NJ, 1981. [38] J. Whaley and M. S. Lam. Cloning-based context-sensitive pointer alias \nanalysis using binary decision diagrams. In Proceedings of the ACM SIGPLAN 2004 conference on Programming \nlanguage design and implementation, (PLDI), pages 131 144, New York, NY, 2004. [39] The Wisconsin Program-Slicing \nTool, Version 1.1, 2000. http://research.cs.wisc.edu/wpis/slicing tool/. [40] X. Zheng and R. Rugina. \nDemand-driven alias analysis for C. In Proceedings of the 35th Annual ACM SIGPLAN-SIGACT Symposium on \nPrinciples of Programming Languages (POPL), pages 197 208, New York, NY, 2008.  \n\t\t\t", "proc_id": "2491956", "abstract": "<p>Aggressive compiler optimizations are formulated around the Program Dependence Graph (PDG). Many techniques, including loop fission and parallelization are concerned primarily with dependence cycles in the PDG. The Directed Acyclic Graph of Strongly Connected Components (DAGSCC) represents these cycles directly. The naive method to construct the DAGSCC first computes the full PDG. This approach limits adoption of aggressive optimizations because the number of analysis queries grows quadratically with program size, making DAGSCC construction expensive. Consequently, compilers optimize small scopes with weaker but faster analyses.</p> <p>We observe that many PDG edges do not affect the DAGSCC and that ignoring them cannot affect clients of the DAGSCC. Exploiting this insight, we present an algorithm to omit those analysis queries to compute the DAGSCC efficiently. Across 366 hot loops from 20 SPEC2006 benchmarks, this method computes the DAGSCC in half of the time using half as many queries.</p>", "authors": [{"name": "Nick P. Johnson", "author_profile_id": "81470644754", "affiliation": "Princeton University, Princeton, NJ, USA", "person_id": "P4148930", "email_address": "npjohnso@princeton.edu", "orcid_id": ""}, {"name": "Taewook Oh", "author_profile_id": "81381590681", "affiliation": "Princeton University, Princeton, NJ, USA", "person_id": "P4148931", "email_address": "twoh@princeton.edu", "orcid_id": ""}, {"name": "Ayal Zaks", "author_profile_id": "81100166351", "affiliation": "Intel Corporation, Haifa, Israel", "person_id": "P4148932", "email_address": "ayal.zaks@intel.com", "orcid_id": ""}, {"name": "David I. August", "author_profile_id": "81100388492", "affiliation": "Princeton University, Princeton, NJ, USA", "person_id": "P4148933", "email_address": "august@princeton.edu", "orcid_id": ""}], "doi_number": "10.1145/2491956.2491960", "year": "2013", "article_id": "2491960", "conference": "PLDI", "title": "Fast condensation of the program dependence graph", "url": "http://dl.acm.org/citation.cfm?id=2491960"}