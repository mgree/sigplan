{"article_publication_date": "06-16-2013", "fulltext": "\n SMAT: An Input Adaptive Auto-Tuner for Sparse Matrix-Vector Multiplication Jiajia Li1,2, Guangming Tan1, \nMingyu Chen1, Ninghui Sun1 1State Key Laboratory of Computer Architecture, Institute of Computing Technology, \nChinese Academy of Sciences 2University of Chinese Academy of Sciences {lijiajia, tgm, cmy, snh}@ict.ac.cn \nAbstract Sparse Matrix Vector multiplication (SpMV) is an important ker\u00adnel in both traditional high \nperformance computing and emerging data-intensive applications. By far, SpMV libraries are optimized \nby either application-speci.c or architecture-speci.c approaches, making the libraries become too complicated \nto be used extensive\u00adly in real applications. In this work we develop a Sparse Matrix\u00advector multiplication \nAuto-Tuning system (SMAT) to bridge the gap between speci.c optimizations and general-purpose usage. \nS-MAT provides users with a uni.ed programming interface in com\u00adpressed sparse row (CSR) format and automatically \ndetermines the optimal format and implementation for any input sparse matrix at runtime. For this purpose, \nSMAT leverages a learning model, which is generated in an off-line stage by a machine learning method \nwith a training set of more than 2000 matrices from the UF sparse ma\u00adtrix collection, to quickly predict \nthe best combination of the matrix feature parameters. Our experiments show that SMAT achieves im\u00adpressive \nperformance of up to 51GFLOPS in single-precision and 37GFLOPS in double-precision on mainstream x86 \nmulti-core pro\u00adcessors, which are both more than 3 times faster than the Intel MKL library. We also demonstrate \nits adaptability in an algebraic multi\u00adgrid solver from Hypre library with above 20% performance im\u00adprovement \nreported. Categories and Subject Descriptors F.2.1 [Numerical Algorithms and Problems]: Computations \non matrices; C.1.2 [Multiple Data Stream Architectures (Multiprocessors)]: Parallel processors General \nTerms Algorithms, Performance Keywords sparse matrix-vector multiplication, SpMV, auto-tuning, data mining, \nalgebraic multi-grid 1. Introduction Nowadays, high performance computing technologies are driv\u00aden simultaneously \nby both Exascale FLOPs (ExaFlops) and data\u00adintensive applications. Although the grand challenges of these \nap\u00adplications are diverse, an interesting observation is that they share an intersection in terms of \nsparse linear system solvers. For exam\u00adple, the well-known ExaFlops applications, laser fusion in interna- \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI \n13, June 16 19, 2013, Seattle, WA, USA. Copyright c . 2013 ACM 978-1-4503-2014-6/13/06. . . $15.00 tional \nthermonuclear experimental reactor (ITER) [3] and global cloud system resolving climate modeling [20], \nspend most of the execution time on solving large-scale sparse linear systems. Sim\u00adilarly, lots of large-scale \ngraph analysis applications like PageR\u00adank [9] and HITS [26] involve sparse matrix solvers in identifying \nrelationships. All of these algorithms rely on a core Sparse Matrix Vector multiplication (SpMV) kernel \nconsuming a large percentage of their overall execution time. For example, the algebraic multigrid (AMG) \nsolver [13], an iterative algorithm widely used in both laser fusion and climate modeling, reports above \n90% SpMV operation of its overall iterations. Since 1970s, plenty of researches have been dedicated to \nopti\u00admizing SpMV performance for its fundamental importance, which are generally separated into two paths, \none for developing new application-speci.c storage formats [7, 21, 29 32, 36], and the oth\u00ader for tuning \non emerging processor architectures [10, 11, 16, 22, 27, 28, 34, 35]. This separation leads to low performance \nand low productivity in SpMV solvers and applications: Low Performance: sparse solvers are not aware \nof the diversity of input sparse matrices. It is well-known that for a speci.c sparse matrix SpMV per\u00adformance \nis sensitive to storage formats. More than ten format\u00ads have been developed during the last four decades. \nHowever, most of them show good performance only for a few specif\u00adic applications, and are rarely adopted \nin widely used numer\u00adic solvers such as Hypre [14], PETSc [6], Trilinos [18] etc. In fact, these solvers \nmainly support one popular storage format CSR (compressed sparse row, explained in Section 2.1). Thus, \nthey usually perform poorly in some applications whose sparse matrices are not appropriate for the supported \nCSR format. Be\u00adsides, even considering only one application, patterns of sparse matrices may change at \nruntime. Take AMG as an example a\u00adgain, it generates a series of different sparse matrices by the coarsen \nalgorithm on successive grid levels. Figure 1 illustrates an example of the need for dynamic storage \nformats in Hypre AMG solver. At the .rst few levels, SpMVs desire DIA or COO format (see Section 2.1) \nfor the optimal performance. While at the coarser-level grids, other formats like CSR may be more de\u00adsirable \ndue to high zero-.lling ratio in DIA format. Therefore, a numerical solver should be adaptive to different \nsparse matrix formats for achieving better performance. Low Productivity: sparse libraries provide multiple \ninterfaces to users An alternative solution to the above low performance issue is to rewrite these solvers \nfor every storage format. Unfortunate\u00adly, such changes are too time-consuming for legacy software. An \neasier option may be to provide an SpMV library, which can automatically adapt to various sparse matrix \nformats for Figure 1. An example of dynamic sparse matrix structures in AMG solver and their SpMV performance \nusing different formats. The nnz represents the number of nonzero elements in matrices.  In this paper \nwe propose a novel Sparse Matrix-vector mul\u00adtiplication Auto-Tuner (SMAT) to provide both application-and \narchitecture-aware SpMV kernels. A distinct feature of SMAT is the uni.ed programming interface for different \nstorage formats. Based on a comprehensive investigation (in Section 2) that shows CSR performs the best \nfor most sparse matrices, we de.ne the uni\u00ad.ed interface as CSR format. In this way users only need to \nprepare their sparse matrices in CSR format as the input, and SMAT auto\u00admatically determines the optimal \nstorage format and implementa\u00adtion on a given architecture. Our auto-tuning approach takes advan\u00adtage \nof a black-box machine learning model to train representative performance parameters extracted from the \nsparse matrix pattern and architectural con.guration. Once the model is trained on the target architecture, \nSMAT evaluates candidate storage formats and determines the optimal format and implementation by leveraging \neither existing auto-tuning work [11, 17, 21, 29, 31, 35, 36] or avail\u00adable hand-tuned codes. Apart from \nthe conventional auto-tuning techniques consider\u00ading architectural parameters, the novelty of SMAT system \nis co\u00adoperatively auto-tuning between algorithms and architectures. In other words, SMAT searches for \nthe optimal storage format based on sparse matrix structures at the algorithm level while generat\u00ading \nthe optimal implementation based on the processor architec\u00adture. Though auto-tuning techniques for optimizing \nlibraries and applications have been explored for a long time, the originality of our work lies in an \ninput adaptive mechanism that identi.es char\u00adacteristics of both applications and architectures. Besides, \nthe main contributions of this paper are: We enable reusable training by adopting more than 2000 sparse \nmatrices from real applications and systematically sampling the parameter space, including sparse structure \ncharacteristics and architecture con.guration. We propose a uni.ed interface on top of SpMV kernels \nto avoid the tedious work of manually choosing among sparse storage formats. SMAT facilitates users to \nadopt the tuned library for improving performance with little extra programming effort.  We develop \na .exible and extension-free framework, with which users can add not only new formats and novel imple\u00admentations \nin terms of their needs, but also more features and larger datasets.  We implement SMAT and train it \nusing sparse matrix collection from the university of Florida [12] on two x86 multi-core proces\u00adsors, \nIntel Xeon X5680 and AMD Opteron 6168. With sparse ma\u00adtrices selected from various application areas, \nSpMV kernels gener\u00adated by SMAT achieve impressive performance of up to 51GFLOP-S in single-precision \n(SP) and 37GFLOPS in double-precision (D-P). The improvement of SMAT over Intel MKL is more than 3.2 \ntimes on average. We also evaluate the adaptability of SMAT in AMG algorithm of sparse solver Hypre [14], \nand the results show above 20% performance improvement. The rest of the paper is organized as follows. \nSection 2 presents several storage formats of sparse matrices and our motivation. In Section 3 we propose \nan overview of SMAT system. Then the three important contributions are described in the following sec\u00adtions. \nSection 4 shows the parameter extraction process to build the feature database. After that, we illustrate \nthe machine learning and kernel searching process in Section 5, and the runtime procedure in Section \n6. We give the performance results on representative sparse matrices and a real application in Section \n7, as well as analyze the accuracy and overhead of SMAT. Related work is presented in Sec\u00adtion 8, and \nconclusion in Section 9. 2. Background 2.1 Sparse Matrix Storage Format In order to reduce the complexity \nof both space and computation, a compressed data structure is commonly used to only store nonzero elements \nof a sparse matrix. Since sparse structure is closely re\u00adlated to compression effectiveness and SpMV \nbehavior on speci.c processor architectures (i.e.,vector machine, cache, etc.), more than ten compressed \ndata structures (i.e. formats) have been develope\u00add since 1970s. However, most of them were customized \nonly for certain special cases. By far four basic storage formats CSR, DIA, ELL, COO are extensively \nused in mainstream applications, from which most other formats can be derived. For example, when there \nexist many dense sub-blocks in a sparse matrix, the corresponding blocking variants (i.e. BCSR, BDIA, \netc.) may perform better if an appropriate blocking factor is selected. Here, COO is specially not\u00aded \nbecause it usually performs better in large scale graph analysis applications [36]. In this proof-of-concept \nwork, we start from the four basic formats and make it possible to extend for supporting other formats \nin our auto-tuning system. Figure 2 illustrates a sparse matrix example represented in the four basic \nformats respectively. CSR (Compressed Sparse Row) format contains three arrays: data stores nonzero elements \nof the sparse matrix, indices stores their corresponding column indices, and the beginning positions \nof each row are stored in ptr . COO (COOrdinate) format explicitly stores the row indices. The rows , \ncols and data arrays store the row indices, the column indices, and the values of all nonzero elements \nrespectively. DIA (Diago\u00adnal) format stores non-zeros by the order of diagonals. data array stores the \ndense diagonals, and offsets records the offsets of each diagonal to the principal one. The idea of the \nELL (ELLPACK) for\u00admat is to pack all non-zeros towards left, and store the packed dense Figure 2. Data \nstructures and basic SpMV implementations  2.2 Motivation Although it is a common sense to store sparse \nmatrices in a prop\u00ader format, it is non-trivial to .gure out how to automatically select the combination \nof appropriate storage format and its optimal im\u00adplementation on a speci.c architecture. A distinct difference \nis that previous auto-tuning techniques [11, 17, 21, 29, 31, 35, 36] only take architectural impact into \naccount while the storage format is prede.ned by users. Actually, it is really a challenging job because \nsuf.cient performance characteristics must be extracted from di\u00adverse sparse matrices and then used to \nquickly determine the opti\u00admal format and implementation. For extracting characteristics and showing \npotential bene.ts from tuning storage format based on sparse structures, we con\u00adduct comprehensive experiments \non the UF collection [12], which consists of more than 2000 sparse matrices from real application\u00ads covering \nextensive application areas. For simplicity without loss of accuracy, we exclude the matrices with complex \nvalues and too small size, and eventually studied 2386 sparse matrices in total in this work. Table 1 \nlists their application areas spread in more than 20 domains. Table 1 also summarizes the distribution \nof the optimal storage formats for all 2386 sparse matrices. Column 2 - 5 represent the number of sparse \nmatrices that have af.nity with CSR, COO, DIA, and ELL formats, respectively. The last row calculates \nthe propor\u00adtion of the matrices have af.nity with each format to the whole matrix set respectively. Apparently, \nCSR is favored by most sparse matrices. Furthermore, for highlighting performance variance and plotting \nlegible graphs, we select 16 representatives from the 2386 matrices and compare the performance in GFLOPS \namong the four storage formats without meticulous implementations. As shown in Figure 3 the largest performance \ngap is about 6 times. The reason\u00ads for the variance will be explained in Section 4. Obviously it is not \nreasonable to apply only one storage format in sparse numeric solvers. Given a speci.c format, there \nare already well-studied auto\u00adtuning tools or libraries [11, 17, 21, 29, 31, 35, 36], which can be leveraged \nto generate the optimal SpMV implementation on a speci.c platform. If the matrix structure stays the \nsame during the whole lifetime of an application, even a brute-force search algo\u00adrithm might be reasonable \nto generate the best candidate. As noted in Section 1, certain applications like AMG dynamically change \nsparse matrix structures (see Figure 1). Therefore, two key issues will be addressed in this paper: (i). \ndetermine the optimal storage format for any input sparse matrix; (ii). generate the optimal com\u00adbination \nof format and implementation at runtime with low over\u00adhead. Table 1. Application and distribution of \naf.nity to each format Application Domains CSR COO DIA ELL Total graph 187 114 6 27 334 linear programming \n267 52 3 5 327 structural 224 14 35 4 277 combinatorial 122 50 10 84 266 circuit simulation 110 149 0 \n1 260 computational .uid dynamics 110 8 47 3 168 optimization 113 15 8 2 138 2D 3D 64 21 19 17 121 economic \n67 4 0 0 71 chemical process simulation 47 14 2 1 64 power network 45 15 0 1 61 model reduction 29 34 \n6 1 60 theoretical quantum chemistry 21 0 26 0 47 electromagnetics 17 1 12 3 33 semiconductor device \n28 1 3 1 33 thermal 19 3 3 4 29 materials 12 3 11 0 26 least squares 10 2 0 9 21 computer graphics vision \n8 1 1 2 12 statistical mathematical 2 1 3 4 10 counter-example 3 4 1 0 8 acoustics 5 0 2 0 7 robotics \n3 0 0 0 3 Percentage 63% 21% 9% 7% 2386  Figure 3. Performance variance among different storage formats \nfor 16 representative matrices. 3. SMAT Overview We develop SpMV Auto-Tuner (SMAT) to choose the optimal \ns\u00adtorage format and implementation of SpMV, whose framework is shown in Figure 4. We consider both architecture \nparameters (such as TLB size, cache and register size, prefetch, threading policy, etc., details in [35]) \nand application parameters (such as matrix dimen\u00adsion, diagonal situation, nonzero distribution, etc., \nto be described in Section 4) to optimize an SpMV kernel and evaluate its perfor\u00admance on sparse matrices \nfrom different applications. The large parameter space makes it dif.cult to .nd the most suitable sparse \nmatrix format and optimization method on a speci.c architecture. For this reason, we generate a learning \nmodel using data mining to .nd the optimal result. Figure 5. Application programming interface. The \ncharacter x denotes numerical precision (single-or double-precision). In addition to the bene.t of the \nuni.ed programming interface, there are two more advantages of SMAT reusability and porta\u00adbility: First, \non a speci.c architecture, SMAT generates the learn\u00ading model once in off-line stage, and use this model \nrepeatedly for different input matrices; Second, SMAT extends well to differen\u00adt architectures as the \narchitecture features have been considered already. Besides, SMAT performs beyond existing auto-tuners \nby employing the performance of SpMV implementations to quantize architecture features, instead of using \narchitecture features direct\u00adly. This innovation brings two advantages: First, it makes SMAT smoothly \ncompatible to new architectures, which is the most im\u00adportant thing to auto-tuners and the biggest difference \nfrom other ones; Second, it is more accurate to conduct learning using perfor\u00admance values instead of \na band of architecture features, to narrow down the parameter space while considering both application \nand architecture features. Another main advantage of SMAT is extensibility. With the nature of data \nmining methods, SMAT is really easy for users to add new algorithms and/or more meticulous implementation. \nBesides, it is also open to add new matrices and corresponding records into the database to improve the \nprediction accuracy. Third, if there is a need to balance accuracy and training time, it is also convenient \nto add or remove parameters from the learning model with SMAT. In the following sections, three compositions \nof SMAT will be discussed, which are respectively parameter extraction, learning model generation, and \nruntime process. 4. Parameter Extraction SMAT is an application and architecture aware auto-tuner, and \nit can apparently leverage the well-studied architecture-aware auto\u00adtuners [17, 21, 31, 36] to extract \narchitecture parameters. In this section, we present how to extract sparse structure feature parame\u00adters. \nTo build an application-aware auto-tuner, we extract feature parameters from more than 2000 sparse matrices \nof UF collec\u00adtion [12]. Table 2 summaries a complete list of sparse structure parameters used in SMAT \nsystem. Firstly, four parameters are used to represent the basic structure t of a sparse matrix applicable \nto all four formats (marked by ): M (the number of rows), N (the number of columns), NNZ (the number \nof non-zeros), and aver RD (the average number of non\u00adzeros per row). Besides, a couple of advanced parameters \nare added to describe SpMV behavior for non-default formats in SMAT such as DIA, ELL and COO. Note: the \nup(down) arrows indicate that the corresponding SpMV performance gets higher once the parameter becomes \nlarger(smaller), . The advantages of DIA and ELL formats come from regular da\u00adta access behavior to the \nmatrix, especially the X-vector. Since DIA and ELL store matrices in a diagonal and column major layout \nre\u00adspectively, DIA reads continuous elements of X-vector, while ELL has a large possibility to reuse \nX-elements. However, these two for\u00admats also suffer from the following drawbacks: First, when there are \nonly a few elements distributed in a diagonal(row), DIA(ELL) will .ll up these diagonals(rows) with zero-padding \nand has to do useless computation on zero elements, which will dramatically hurt the overall performance. \nSecond, the diagonal(column)-order loop requires Y-elements to be written once per diagonal(column), \nwhich will produce frequent cache evict and memory write back operation for large sparse matrices. In \norder to address these is\u00adsues, four parameters ({Ndiags, max RD, ER DIA, ER ELL} in Table 2) are extracted \nto represent the DIA and ELL kernels be\u00adhavior. Table 2. Feature parameters of a sparse matrix and the \nrelationship with the formats Parameter Meaning Formula DIAv ELLv CSRv COOv Matrix Dimension M the number \nof rows - v v v v N the number of columns - Diagonal Situation Ndiags the number of diagonals - . NTdiags \nratio the ratio of true diagonals to total diagonals NTdiags ratio= number of true diagonals N diags \n. v v v v Nonzero Distribution NNZ the number of nonzeros - v v v v aver RD the number of nonzeros per \nrow aver RD = N N Z M max RD the maximum number of nonzeros per row max RD=max M 1 {number of nonzeros \nper row} . var RD the variation of the number of nonzeros per row var RD = IM 1 |row degree-aver RD|2 \nM . Nonzero Ratio ER DIA the ratio of nonzeros in DIA da\u00adta structure ER DIA= N N Z N diags\u00d7M . ER ELL \nthe ratio of nonzeros in ELL da\u00adta structure ER ELL = N N Z max RD\u00d7M . Power-Law R a factor of power-law \ndistribu\u00adtion P (k) ~ k-R [1, 4] Our methodology is to perform a statistical analysis based on experiments \non UF sparse matrix collection, by plotting the distri\u00adbution of bene.cial matrices to re.ect the parameter \nin.uence and then de.ning their ranks accordingly. In Figure 6(a) X-axis gives the intervals of Ndiags \nand max RD, and the distribution of DIA and ELL matrices among the intervals is presented on Y-axis in \npercentage. When Ndiags or max RD becomes larger, less num\u00adber of matrices gain performance bene.ts from \nDIA or ELL format, which means small Ndiags and max RD indicate good performance for DIA and ELL. Similarly, \nFigure 6(b) shows the bene.cial ma\u00adtrix distribution among nonzero ratio intervals, from which we can \nconclude that large nonzero ratio is good for DIA (less obvious) and ELL formats. However, we also observe \nexceptions from these two .gures above. For ER DIA parameter there are still nearly half of matrices \nbene.ting from DIA format even when the nonzero ratio is no more than 50%. To improve the combinational \naccuracy of the rules, we introduce two more parameters NTdiags ratio and var RD. We .rstly de.ne a \nnew type of diagonal true diagonal , to repre\u00adsent one occupied mostly with non-zeros. A true diagonal \nfea\u00adtures continuous X access pattern, minor part of zero-padding, and writing Y-vector only once, which \nwill show competitive SpMV performance. We use the ratio of true diagonals out of all diag\u00adonals (NTdiags \nratio)as another parameter. The bene.cial DIA matrix distribution is shown in Figure 6(c), which indicates \nDIA also gains higher performance with larger NTdiags ratio just as ER DIA parameter. However, NTdiags \nratio shows more obvious rules than ER DIA so that it captures application features more ac\u00adcurately. \nSimilarly, we add var RD parameter for ELL format. As we know, if a sparse matrix has a signi.cantly \nvariable number of non-zeros per row, its SpMV performance degrades due to a lot of zero-padding operations. \nSo we introduce var RD to represent the variation of the number of non-zeros per row. Figure 6(d) indicates \nthat small var RD value is good for ELL SpMV performance. Fi\u00adnally, we learn from [36] that COO format \ngains good performance on small-world network. Thus, we choose power-law distribution P (k) . k-R as \nits criterion. Figure 6(e) depicts the distribution on R, of which the interval [1, 4] is preferred by \nCOO matrices. So far, we extract 11 feature parameters to abstract sparse matrix structure and draw several \nrules to achieve higher SpMV perfor\u00admance with a certain format. However, it is still far from predicting \nthe optimal format and implementation only based on rules with the thresholds generated from the simple \nobservations in Table 2. First, for each format there are multiple parameters which are not com\u00adpletely \northogonal to each other. That makes it hard to set accurate threshold values for decision making. Second, \nfrom Figure 6, there are quite a few exceptions to the simple rules, which may bring too many wrong predictions. \nLast, a careful balance is needed for determining the threshold values. Since a rule has both advantages \nand disadvantages, although it can predict accurately for its favored format, it might hurt the prediction \naccuracy of other formats. For example, if we conclude a rule of var RD < 0.5 from Figure 6(d), even \nwhen the matrix satis.es this rule SpMV performance with ELL format still may be lower than CSR format. \nIn this case, it is necessary to balance the overall accuracy of four formats rather than only one. Therefore, \nwe introduce a data mining method based on the parameters in Table 2 to build a learning model, with \nwhich SMAT can generate practical and reliable prediction. 5. Machine Learning Model In this section, \nwe discuss details of the data mining method used to the generate learning model and search for optimal \nimplementation from the kernel library. 5.1 Model Generation We de.ne an attribute collection {M, N, \nNdiags, NTdiags ratio, NNZ, max RD, var RD, ER DIA, ER ELL, R, Best Format} for each matrix, where Best \nFormat is the target attribute. Through training with 2055 UF matrices, we get accurate param\u00adeter values \nof all training matrices. For example, matrix t2d q9 (in training set) has a record of parameter values \nas {9801, 9801, 9, 1.0, 87025, 9, 0.35, 0.99, 0.99, inf, DIA}, where inf means the power-law distribution \ncannot calculate the R value from this matrix since it has no attribute of scale-free network. All of \nthese records together constitute the matrix feature database. As the target attribute, Best Format is \nused to classify possi\u00adble candidates (DIA, ELL, CSR, COO) for a particular sparse ma\u00adtrix. Thus, a mapping \nfrom the parameters set to the Best Format needs to be built. This falls into a classi.cation problem \nthat can be solved by data mining tools. The formulation of map\u00adping is described in Equation 1, where \nx.i(i = 1, . . . , n) rep\u00adresents the set of parameters of a sparse matrix in the training . set, and \nT H stands for the set of thresholds for each attribute. Cn(DI A, ELL, C SR, C OO) represents one of \nthe four cate\u00ad . gories. With the help of data mining method, T H is generated along with the decision \ntree. x2, . . . , .T H ) -Cn(DI A, E LL, C S R, C OO)f(x.1,.xn, .(1) SMAT system uses C5.0 [2] data \nmining tool to generate the decision tree from the training matrix set. With its user-friendly design, \nlittle knowledge besides the feature database is needed to generate the model. The tool extracts informative \npatterns from the input data and outputs the learning model. (a) Ndiags and max RD (b) ER DIA and ER \nELL (c) Figure 6. As we know, C5.0 can generate either decision tree or ruleset pattern. Though the two \npatterns represent similar information, we eventually chose ruleset for two reasons. First, ruleset classi.ers \nare most likely more accurate than decision tree. Although this higher accuracy increases the execution \ntime of the data mining method, it only impacts the off-line stage of SMAT and is reusable after one \ntime decision. Second, it is convenient to convert the rules to IF-THEN sentences, so ruleset is more \nstraightforward to integrate with our system. In order to reduce model inaccuracy, we guide C5.0 to generate \na con.dence factor along with each rule. The con.dence factor is the ratio of the number of correctly \nclassi.ed matrices to the number of matrices falling in this rule, the range of which is between 0 and \n1. The larger the con.dence factor is, the more reliable the rule is. We will show the role of this con.dence \nfactor in Section 6. Using a matrix feature database, SMAT generates a learning model in ruleset pattern \nwith a con.dence factor. The usage of the ruleset model will also be discussed in Section 6.  5.2 Kernel \nSearching Previous literature [10, 11, 22, 31, 35] discussed architectural op\u00adtimization strategies (such \nas blocking methods, software prefetch\u00ading, SSE intrinsic usage, multi-thread, etc.) for speci.c architec\u00adtures. \nIn SMAT a collection of strategies are grouped together as a large kernel library in Figure 4. It is \nimportant to narrow down the optimization implementations of the library to .nd potential opti\u00admal kernels \non a given architecture. SMAT can leverage existing auto-tuning tools to facilitate search in off-line \nstage. Kernel searching is conducted with a performance record table and scoreboard algorithm. We run \nall possible implementations (up to 24 in current SMAT system) and record corresponding perfor\u00admance \nin a table. The implementations are arranged in a speci.c order in this table, and each performance number \nin a record can be indexed by all of the optimization strategies it used. Based on the performance table, \na scoreboard is created to .nd the most ef\u00ad.cient strategy according to SpMV behavior on the target architec\u00adture. \nBeginning from the implementation with a single optimization method, if it shows performance gain compared \nwith basic imple\u00admentation, the corresponding optimization method is marked as 1 on the scoreboard, otherwise \nit is marked by -1. When performance gap between two implementations is less than 0.01, we consider this \noptimization strategy showing no effect on this architecture and neglect it by default. In this way, \nan implementation with multiple optimization strategies should compare its performance with those that \nhave just one less optimization strategy. Thus, we eventually obtain the score of each optimization strategy, \nand then the score of each implementation can be calculated by summing the scores of strategies used \nin it. Through these two algorithms, the implemen\u00adtation with the highest score is considered to be the \noptimal one for the corresponding format on the architecture. The optimal kernels will be invoked when \nthe data mining method builds the learning model and the execute-measure module runs. Figure 7. Runtime \nprocedure of SMAT Feature Extraction In this component the parameters listed in Table 2 are calculated \nwithout actually running SpMV in two separated steps: First, we extract diagonal information for DIA \nformat. For reducing the number of times we traverse the whole matrix, we count the diagonals and nonzero \ndistribution together. Thus, we extract the parameter values of DIA, ELL, and CSR formats in this step. \nSecond, we need to evaluate parameter R for COO format, which will spends non-trivial time due to the \nheavy computation of power-law distribution. From the feature extraction process, we learn that the two \nseparated steps are independent, which will be useful to accelerate the runtime process as described \nin the next paragraph. Rule Tailoring and Grouping C5.0 generates tens of rules in the learning model. \nAs many of these rules are suitable for only a small group of matrices and inaccuracy, we re-order them \naccording to corresponding estimated contribution (a concept from C5.0) to the overall training set. \nThat is, rules reducing error rate the most appear .rst, and rules contributing the least appear last. \nThen we tailor rules top-down until the subset of rules achieve predictive accuracy close to whole ruleset \n(1% accuracy gap is acceptable). From our experiments, we choose rules No.1-15 on Intel platform, which \ndecrease the error rate to 9.6%, which is very close to the result 9.0% achieved by the whole ruleset \nof 40 rules. After .nishing the rule tailoring step, we assign the rules to dif\u00adferent format groups. \nAccording to the rule con.dence factor gener\u00adated with learning model, we pick the largest one within \nthe same format group as format con.dence factor. The format con.dence factor re.ects the reliability \nwhen a corresponding format is cho\u00adsen by the model, and we compare it with the threshold to decide the \nreliability of the prediction.  Figure 8. Representative matrices 7. Experiments and Analysis In this \nsection, we evaluate the performance of SMAT by running benchmark matrices and a real application, followed \nby accuracy and overhead analysis. 7.1 Experimental Setup Platform The experiments are conducted on \ntwo x86 multi\u00adcore processors: One is a Intel Xeon X5680 with 12 3.3GHz cores, (a) Intel (b) AMD Figure \n9. SMAT performance in single-and double-precision on the two platforms Figure 9 shows up to 5 times \nperformance variation among the matrices, which proves it is worth adopting SMAT in different ap\u00adplications \nowing to its adaptability to diverse sparse matrices. For matrices that have af.nity to DIA, ELL or COO \nformat (No.1-8 and No.13-16), the corresponding SpMVs achieve higher performance than those in CSR format \n(No.9-12). The performance gap indi\u00adcates that it is meaningful to implement a high performance SpMV \nlibrary being aware of sparse structures (applications). Moreover, we give performance comparison between \nSMAT and Intel MKL multi-threaded library in Figure 10 in single-and double-precision on Intel platform. \nMKL performance shown in this .gure is the maximum performance number of DIA, CSR, and COO SpMV functions \nin this library. Compared with MK-L, SMAT obtains the maximum speedup of 6.1 times in single\u00adprecision \nand 4.7 times in double-precision. Although this .gure only shows 16 representative matrices, we do collect \nexperimental data for all 331 matrices and the average speedup over MKL is 3.2 times in single-precision \nand 3.8 times in double-precision. SMAT shows big advantages in performance due to the following reasons. \nFirst, we take advantage of optimized SpMV implementations us\u00ad -------- ---- (a) .oat (b) double Figure \n10. The performance of SMAT V.S. MKL  7.3 Analysis Accuracy Table 3 shows the details of on-line decision \nmaking process and its accuracy. When a sparse matrix is input to SMAT system, the learning model performs \nprediction on-line and out\u00adputs the predicted format (noted by Model Prediction Format ). However, there \nare some exceptions that the learning model cannot predict their formats con.dently. SMAT actually executes \nSpMV kernels for once and measures the performance for part of formats, which is noted as Execution in \nthis table. For example, the ex\u00adecution process runs CSR-and COO-based SpMVs to determine the .nal format \nfor matrix No.9-12. In this table Best Format represents the best result of exhaustive search. Though \nthe learning model cannot predict correctly in some cases due to relatively in\u00adtricate features of CSR \nas the most general format, SMAT still can obtain good results on the 16 matrices. For all 331 matrices, \nthe ac\u00adcuracy is 92% (SP) and 82% (DP) on Intel platform, and 85% (SP) and 82% (DP) on AMD platform respectively. \nPrediction Overhead Table 3 also shows the SMAT Over\u00adhead in the last column, which is represented by \nratio of the over\u00adall execution time to the basic CSR-SpMV execution time. In most cases, the overhead \nis no more than 5 times, which is very competi\u00adtive comparing to existing auto-tuning tools [11, 17, \n17, 21, 29, 31, 35, 36] and practical for real applications. But when the learning model fails to get \na con.dent prediction, SMAT overhead increas\u00ades to about 15 times. This is acceptable when an application \nexe\u00adcutes an SpMV kernel hundreds of times. In fact, compared with the overhead of existing auto-tuners \n OSKI (40 times), clSpMV (1-20 times), the overhead of SMAT appears acceptable. As a straightforward \nway to search for the optimal result, one option is to run SpMV kernels for all formats one by one. De\u00adspite \nplenty of implementation variants of one format, the simplest search across basic implementations of \nthe four formats takes more time than SMAT. The overhead of this search method comes from format conversion \nand SpMV execution. For example, the conver\u00adsion from CSR to ELL consumes 39.6 times of CSR-SpMV for \nthe No.11 matrix. The overhead of simple search reaches up to 45 times even if only four implementations \nare explored, which is much higher than the case (16.2) in Table 3. Remember that SMAT is only a proof \nof concept system, and its adoption of reusable ma\u00adchine learning model makes it feasible to extend for \nmore formats and implementations. 7.4 SMAT-based AMG Considering a problem of solving equation Au = \nf with certain precision criterion, where A is a large sparse matrix, u and f are dense vectors, Hypre \nAMG solves this by building N levels of vir\u00adtual grids with a series of grid operators (A0, ...AN-1) \nand grid transfer operators (P0, ...PN-2) in a setup process. Figure 11 illus\u00adtrates a typical V-cycle \nin AMG. Apparently, P-operators perform SpMV between adjacent grids, and A-operators are used to do re\u00adlaxations \nlike Jacobi and Gauss-Seidel methods with SpMV kernel, too. Without a doubt, SpMV consumes most of the \nV-cycle s exe\u00adcution time. We observe an interesting phenomenon that the two se\u00adries of sparse matrices \ndynamically show different sparse features from the original input matrix A. Thus, SMAT is useful on \ndeter\u00admining the optimal format and implementation for the operators at each level, and improving the \noverall performance. Figure 11. V-cycle involved with SpMV kernels in AMG solver. We perform experiments \non cljp and rugeL coarsen methods in the setup process to generate A-operators with different structures. \nThe input matrices are generated by 7-point and 9-point Laplacian methods, respectively. Instead of using \nCSR format all the time, SMAT chooses DIA format for A-operators at the .rst few levels, and ELL format \nfor most P-operators. To do this, we simply replace the SpMV kernel codes with SMAT interfaces with no \nchanges to the original CSR data structure in Hypre. As shown in Table 4, the solving part of SMAT AMG \nachieves more than 20% performance gain than Hypre AMG. Table 4. SMAT-based AMG execution time (millisecond) \nCoarsen Rows Hypre AMG SMAT AMG Speedup cljp 7pt 50 125K 3034 2487 1.22 rugeL 9pt 500 250K 388 300 1.29 \n 8. Related Work There has been a .urry of work related to optimizing sparse matrix vector multiplication \nperformance. While we could not hope to provide a comprehensive set, we attempt to contrast our work \nwith several key representatives. Storage Format Basically, SMAT discovers optimal storage formats for \nsparse matrices. This idea is re.ected in previous work. By far, more than ten storage formats [7, 21, \n25, 28, 29, 31] have been proposed, most of which are either application-speci.c or architecture-speci.c \nso that their applicable domains are limited. Recently, several hybrid storage formats have been developed. \nBor-Yiing Su et al. [29] proposed a Cocktail format that splits a matrix and represents these sub-matrices \nin different formats according to relative advantages of different sparse matrix formats. Similarly, \non NVIDIA GPU CuSparse [23] stores one sparse matrix with a hy\u00adbrid format HYB, which is a combination \nof two basic formats. These new hybrid storage formats show much better performance than conventional \nformats for certain sparse matrices. The major d\u00adifference from SMAT is that the hybrid formats are determined \nstat\u00adically and not applicable to dynamic sparse structures as shown at different levels of AMG solver. \nIn the other aspect, researchers are developing relatively .exible formats. Richard Vuduc et al. [31] \nim\u00adproved BCSR format to VBR to explore dense blocks with different sizes. Kourtis et al. [21] proposed \nCSX to exploit dense structures not limited to dense blocks, but also 1-dimension bars and dense di\u00adagonals \nby compressing metadata. However, the process of search\u00ading sub-blocks costs too much to be conducted \non-line. In contrast, in terms of storage format optimization, SMAT selects the optimal format from the \nexisting formats, instead of designing a new one. The machine learning model makes an on-line decision \nfeasible. It is possible to add new formats by extracting novel parameters and integrating its implementations \nin kernel library in SMAT. Auto-tuning Approach For developing a domain-speci.c performance-critical \nlibrary, auto-tuning approach is promising to resolve both performance and portability problems. There \nare several successful auto-tuning libraries, such as ATLAS [33], FFTW [15], SPIRAL [24], and OSKI [31], \nwidely used in scienti.c computing area. Speci.cally for SpMV, auto-tuning techniques are actively investigated. \nEun-jin Im et al. [19] created BCSR format to better develop the performance of dense blocks in a sparse \nmatrix. Richard Vuduc et al. [31] built OSKI to tune the block size for a matrix in BCSR or VBR formats. \nWilliams et al. [35] deployed a hierarchical strategy to choose the optimal architectural parame\u00adter \ncombinations. Choi et al. [11] implemented Blocked Compress Sparse Row (BCSR) and Sliced Blocked ELLPACK \n(SBELL) for\u00admats on Nvidia GPUs, and tuned the block sizes of them. X.Yang et al. [36] proposed a mixed \nformat and automatically chose the partition size for each format with the help of a model. A common \nfeature of these auto-tuning designs is to focus on implementa\u00adtion tuning on diverse processing architectures \nonly for a single pre-de.ned storage format. In addition to architectural auto-tuning, SMAT extends to \ncooperatively tune storage formats by extracting key performance parameters from input sparse matrices. \nIn fact, algorithm and architecture co-tuning was advocated in PetaBrick\u00ads compiler [4]. Our work further \nproves the value of auto-tuning techniques. Prediction Model A core component of SMAT is the machine\u00adlearning \nbased prediction model for determining the optimal format and implementation. It is a common strategy \nto apply prediction model in auto-tuning approach. ATLAS [33] performed an empir\u00adical search to determine \noptimal parameter values bounded by ar\u00adchitecture features. But an actual run of generated code is needed \nto measure and record its performance to choose the best imple\u00admentation. This empirical search has been \nproven ef.cient to gen\u00aderate high quality BLAS codes, although the search process takes a lot of time. \nRecently, many arising auto-tuners adopt model-driven method such as [11, 21, 29, 31, 36] without the \nneed of actually running the code. Though the model-driven method decreases the prediction time, the \ngenerated code performance is considered low\u00ader than empirical search in most cases. Kamen Yotov et. \nal. [37] did experiments on ATLAS and got this conclusion on ten platforms. Our SMAT system combines \nlearning model and empirical search invoked rarely that ensures the code performance and reduces pre\u00addiction \ntime at the same time. Although clSpMV [29] also used a prediction model to tune its Cocktail format, \nthere are certain cru\u00adcial differences from SMAT. First and foremost, in on-line decision making stage, \nclSpMV uses the maximum GFLOPS measured in of.ine stage. Unfortunately, as our experiments on UF collection \nshows (see Table 1 and Figure 3) the maximum performance of one format is not representative enough to \nre.ect the SpMV perfor\u00admance of all the matrices suitable in this format. It is more accurate to use \nthe features of each input matrix to determine its own best format rather than using a single maximum \nperformance for each format. Second, we extract more features from real matrices in UF collection, which \ncan feed more training data to data mining tool\u00ads so as to generate more reliable rules for the learning \nmodel. W. Armstrong et al. [5] uses reinforcement learning to choose the best format, but users should \ndecide the factor values which in.uence the accuracy of the learning model. SMAT system is more conve\u00adnient \nto automatically generate the model and still achieve similar prediction accuracy. 9. Conclusion In this \npaper, we propose SMAT, an input adaptive SpMV auto\u00adtuner, which encompasses several statistical and \nmachine learning techniques to enable both application-and architecture-dependent, incremental model \ntraining and black-box performance prediction. In particular, we provide a uni.ed interface to eliminate \ntedious work on determining optimal formats and implementations for di\u00adverse sparse matrices. Due to \ncooperatively auto-tuning with both algorithms and architectures, SMAT achieves impressive perfor\u00admance \nof up to 51 (38) GFLOPS in single-precision and 37 (22) GFLOPS in double-precision on Intel (AMD) multi-core \nproces\u00adsor. The average speedup is above 3 times over Intel MKL sparse library. SMAT is also used to \nimprove the performance of alge\u00adbraic multi-grid algorithm from Hypre sparse linear system solver by \nabout 20%. Our work suggests that algorithm and architecture co-tuning is a promising approach for developing \ndomain-speci.c auto-tuning libraries or tools. Acknowledgments We would like to express our gratitude \nto Yin Li from Intel Chi\u00adna and Dr. Trishul Chilimbi for helping us polishing this paper. This work is \nsupported by National Natural Science Foundation of China (61272134, 61033009, 61003062, 60925009), the \nNational Basic Research Program of China (973 Program) (2011CB302502, 2012CB316502), and the joint lab \nof Institute of Computing Tech\u00adnology and Intel. References [1] Intel Math Kernel Library. URL http://software.intel.com/en-us/intel-mkl. \n [2] Data Mining Tools See5 and C5.0. URL http://www.rulequest.com/see5-info.html. [3] M. D. Adam Hill. \nThe international thermonuclear experimental reactor. Technical report, 2005. [4] J. Ansel, C. Chan, \nY. L. Wong, M. Olszewski, Q. Zhao, A. Edelman, and S. Amarasinghe. Petabricks: a language and compiler \nfor algorithmic choice. In Proceedings of the 2009 ACM SIGPLAN conference on Programming language design \nand implementation, PLDI 09, pages 38 49. ACM, 2009. ISBN 978-1-60558-392-1. [5] W. Armstrong and A. \nRendell. Reinforcement learning for automated performance tuning: Initial evaluation for sparse matrix \nformat selection. In Cluster Computing, 2008 IEEE International Conference on, pages 411 420, 2008. [6] \nS. Balay, J. Brown, , K. Buschelman, V. Eijkhout, W. D. Gropp, D. Kaushik, M. G. Knepley, L. C. McInnes, \nB. F. Smith, and H. Zhang. PETSc users manual. Technical Report ANL-95/11 -Revision 3.3, Argonne National \nLaboratory, 2012. [7] N. Bell and M. Garland. Ef.cient sparse matrix-vector multiplication on CUDA. NVIDIA \nTechnical Report NVR-2008-004, NVIDIA Corporation, Dec. 2008. [8] G. Belter, E. R. Jessup, I. Karlin, \nand J. G. Siek. Automating the generation of composed linear algebra kernels. In Proceedings of the Conference \non High Performance Computing Networking, Storage and Analysis, SC 09, pages 59:1 59:12, New York, NY, \nUSA, 2009. ACM. ISBN 978-1-60558-744-8. [9] S. Brin and L. Page. The anatomy of a large-scale hypertextual \nweb search engine. In Proceedings of the seventh international conference on World Wide Web 7, WWW7, \npages 107 117. Elsevier Science Publishers B. V., 1998. [10] A. Buluc, S. Williams, L. Oliker, and J. \nDemmel. Reduced-bandwidth multithreaded algorithms for sparse matrix-vector multiplication. In Proceedings \nof the 2011 IEEE International Parallel &#38; Distributed Processing Symposium, IPDPS 11, pages 721 733. \nIEEE Computer Society, 2011. [11] J. W. Choi, A. Singh, and R. W. Vuduc. Model-driven autotuning of sparse \nmatrix-vector multiply on gpus. In Proceedings of the 15th ACM SIGPLAN Symposium on Principles and Practice \nof Parallel Programming, PPoPP 10, pages 115 126. ACM, 2010. [12] T. A. Davis. The university of .orida \nsparse matrix collection. NA DIGEST, 92, 1994. [13] R. Falgout. An introduction to algebraic multigrid \ncomputing. Computing in Science Engineering, 8(6):24 33, nov.-dec. 2006. ISSN 1521-9615. [14] R. D. Falgout \nand U. M. Yang. hypre: a library of high performance preconditioners. In Preconditioners, Lecture Notes \nin Computer Science, pages 632 641, 2002. [15] M. Frigo and S. G. Johnson. The design and implementation \nof FFTW3. Proceedings of the IEEE, 93(2):216 231, 2005. Special issue on Program Generation, Optimization, \nand Platform Adaptation . [16] D. Grewe and A. Lokhmotov. Automatically generating and tuning gpu code \nfor sparse matrix-vector multiplication from a high-level representation. In Proceedings of the Fourth \nWorkshop on General Purpose Processing on Graphics Processing Units, GPGPU-4, pages 12:1 12:8. ACM, 2011. \n[17] J. Harris. poski: An extensible autotuning framework to perform optimized spmvs on multicore architectures. \n2009. [18] M. A. Heroux, R. A. Bartlett, V. E. Howle, R. J. Hoekstra, J. J. Hu, T. G. Kolda, R. B. Lehoucq, \nK. R. Long, R. P. Pawlowski, E. T. Phipps, A. G. Salinger, H. K. Thornquist, R. S. Tuminaro, J. M. Willenbring, \nA. Williams, and K. S. Stanley. An overview of the trilinos project. ACM Trans. Math. Softw., 31(3):397 \n423, Sept. 2005. ISSN 0098-3500. [19] E. jin Im, K. Yelick, and R. Vuduc. Sparsity: Optimization framework \nfor sparse matrix kernels. International Journal of High Performance Computing Applications, 18:2004, \n2004. [20] M. F. Khairoutdinov and D. A. Randall. A cloud resolving model as a cloud parameterization \nin the ncar community climate system model: Preliminary results. Geophys. Res. Lett., 28(18):3617C3620, \n2001. [21] K. Kourtis, V. Karakasis, G. Goumas, and N. Koziris. Csx: an extended compression format for \nspmv on shared memory systems. In Proceedings of the 16th ACM symposium on Principles and practice of \nparallel programming, PPoPP 11, pages 247 256. ACM, 2011. [22] K. Nagar and J. Bakos. A sparse matrix \npersonality for the convey hc-1. In Field-Programmable Custom Computing Machines (FCCM), 2011 IEEE 19th \nAnnual International Symposium on, pages 1 8, may 2011. [23] CUDA CUSPARSE Library. NVIDIA, 2010. [24] \nM. P\u00a8uschel, J. M. F. Moura, J. Johnson, D. Padua, M. Veloso, B. Singer, J. Xiong, F. Franchetti, A. \nGacic, Y. Voronenko, K. Chen, R. W. Johnson, and N. Rizzolo. SPIRAL: Code generation for DSP transforms. \nProceedings of the IEEE, special issue on Program Generation, Optimization, and Adaptation , 93(2):232 \n275, 2005. [25] Y. Saad. Sparskit : a basic tool kit for sparse matrix computations. Technical Report, \n1994. [26] N. Spirin and J. Han. Survey on web spam detection: principles and algorithms. SIGKDD Explor. \nNewsl., 13(2):50 64, May 2012. ISSN 1931-0145. [27] A. Srinivasa and M. Sosonkina. Nonuniform memory \naf.nity strategy in multithreaded sparse matrix computations. In Proceedings of the 2012 Symposium on \nHigh Performance Computing, HPC 12, pages 9:1 9:8, 2012. [28] J. P. Stevenson, A. Firoozshahian, A. Solomatnikov, \nM. Horowitz, and D. Cheriton. Sparse matrix-vector multiply on the hicamp architecture. In Proceedings \nof the 26th ACM international conference on Supercomputing, ICS 12, pages 195 204, New York, NY, USA, \n2012. ACM. ISBN 978-1-4503-1316-2. [29] B.-Y. Su and K. Keutzer. clspmv: A cross-platform opencl spmv \nframework on gpus. In Proceedings of the 26th ACM international conference on Supercomputing, ICS 12, \npages 353 364. ACM, 2012. [30] X. Sun, Y. Zhang, T. Wang, G. Long, X. Zhang, and Y. Li. Crsd: application \nspeci.c auto-tuning of spmv for diagonal sparse matrices. In Proceedings of the 17th international conference \non Parallel processing -Volume Part II, Euro-Par 11, pages 316 327, 2011. [31] R. Vuduc, J. W. Demmel, \nand K. A. Yelick. OSKI: A library of automatically tuned sparse matrix kernels. In Proc. SciDAC, J. Physics: \nConf. Ser., volume 16, pages 521 530, 2005. [32] R. W. Vuduc and H.-J. Moon. Fast sparse matrix-vector \nmultiplication by exploiting variable block structure. In Proceedings of the First international conference \non High Performance Computing and Communications, HPCC 05, pages 807 816. Springer-Verlag, 2005. [33] \nR. C. Whaley and J. J. Dongarra. Automatically tuned linear algebra software. In Proceedings of the 1998 \nACM/IEEE conference on Supercomputing (CDROM), Supercomputing 98, pages 1 27, Washington, DC, USA, 1998. \nIEEE Computer Society. ISBN 0-89791-984-X. [34] S. Williams, L. Oliker, R. Vuduc, J. Shalf, K. Yelick, \nand J. Demmel. Optimization of sparse matrix-vector multiplication on emerging multicore platforms. In \nProceedings of the 2007 ACM/IEEE conference on Supercomputing, SC 07, pages 38:1 38:12. ACM, 2007. [35] \nS. Williams, L. Oliker, R. Vuduc, J. Shalf, K. Yelick, and J. Demmel. Optimization of sparse matrix-vector \nmultiplication on emerging multicore platforms. Parallel Comput., 35(3):178 194, Mar. 2009. [36] X. Yang, \nS. Parthasarathy, and P. Sadayappan. Fast sparse matrix-vector multiplication on gpus: implications for \ngraph mining. Proc. VLDB Endow., 4(4):231 242, Jan. 2011. [37] K. Yotov, X. Li, G. Ren, M. Garzaran, \nD. Padua, K. Pingali, and P. Stodghill. Is search really necessary to generate high-performance blas?, \n2005.   \n\t\t\t", "proc_id": "2491956", "abstract": "<p>Sparse Matrix Vector multiplication (SpMV) is an important kernel in both traditional high performance computing and emerging data-intensive applications. By far, SpMV libraries are optimized by either application-specific or architecture-specific approaches, making the libraries become too complicated to be used extensively in real applications. In this work we develop a Sparse Matrix-vector multiplication Auto-Tuning system (SMAT) to bridge the gap between specific optimizations and general-purpose usage. SMAT provides users with a unified programming interface in compressed sparse row (CSR) format and automatically determines the optimal format and implementation for any input sparse matrix at runtime. For this purpose, SMAT leverages a learning model, which is generated in an off-line stage by a machine learning method with a training set of more than 2000 matrices from the UF sparse matrix collection, to quickly predict the best combination of the matrix feature parameters. Our experiments show that SMAT achieves impressive performance of up to 51GFLOPS in single-precision and 37GFLOPS in double-precision on mainstream x86 multi-core processors, which are both more than 3 times faster than the Intel MKL library. We also demonstrate its adaptability in an algebraic multigrid solver from Hypre library with above 20% performance improvement reported.</p>", "authors": [{"name": "Jiajia Li", "author_profile_id": "81479658666", "affiliation": "Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China", "person_id": "P4148961", "email_address": "lijiajia@ict.ac.cn", "orcid_id": ""}, {"name": "Guangming Tan", "author_profile_id": "81312482056", "affiliation": "Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China", "person_id": "P4148962", "email_address": "tgm@ict.ac.cn", "orcid_id": ""}, {"name": "Mingyu Chen", "author_profile_id": "81435603122", "affiliation": "Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China", "person_id": "P4148963", "email_address": "cmy@ict.ac.cn", "orcid_id": ""}, {"name": "Ninghui Sun", "author_profile_id": "81100446659", "affiliation": "Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China", "person_id": "P4148964", "email_address": "snh@ict.ac.cn", "orcid_id": ""}], "doi_number": "10.1145/2491956.2462181", "year": "2013", "article_id": "2462181", "conference": "PLDI", "title": "SMAT: an input adaptive auto-tuner for sparse matrix-vector multiplication", "url": "http://dl.acm.org/citation.cfm?id=2462181"}