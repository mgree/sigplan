{"article_publication_date": "06-16-2013", "fulltext": "\n Modular Veri.cation of Linearizability with Non-Fixed Linearization Points Hongjin Liang Xinyu Feng \nUniversity of Science and Technology of China lhj1018@mail.ustc.edu.cn xyfeng@ustc.edu.cn Abstract Locating \nlinearization points (LPs) is an intuitive approach for proving linearizability, but it is dif.cult to \napply the idea in Hoare\u00adstyle logic for formal program veri.cation, especially for verify\u00ading algorithms \nwhose LPs cannot be statically located in the code. In this paper, we propose a program logic with a \nlightweight in\u00adstrumentation mechanism which can verify algorithms with non\u00ad.xed LPs, including the most \nchallenging ones that use the help\u00ading mechanism to achieve lock-freedom (as in HSY elimination\u00adbased \nstack), or have LPs depending on unpredictable future exe\u00adcutions (as in the lazy set algorithm), or \ninvolve both features. We also develop a thread-local simulation as the meta-theory of our logic, and \nshow it implies contextual re.nement, which is equiv\u00adalent to linearizability. Using our logic we have \nsuccessfully ver\u00adi.ed various classic algorithms, some of which are used in the java.util.concurrent \npackage. Categories and Subject Descriptors D.2.4 [Software Engineer\u00ading]: Software/Program Veri.cation \n Correctness proofs, Formal methods; F.3.1 [Logics and Meanings of Programs]: Specifying and Verifying \nand Reasoning about Programs General Terms Theory, Veri.cation Keywords Concurrency; Rely-Guarantee Reasoning; \nLineariz\u00adability; Re.nement; Simulation 1. Introduction Linearizability is a standard correctness criterion \nfor concurrent ob\u00adject implementations [16]. It requires the .ne-grained implementa\u00adtion of an object \noperation to have the same effect with an instanta\u00adneous atomic operation. To prove linearizability, \nthe most intuitive approach is to .nd a linearization point (LP) in the code of the im\u00adplementation, \nand show that it is the single point where the effect of the operation takes place. However, it is dif.cult \nto apply this idea when the LPs are not .xed in the code of object methods. For a large class of lock\u00adfree \nalgorithms with helping mechanism (e.g., HSY elimination\u00adbased stack [14]), the LP of one method might \nbe in the code of some other method. In these algorithms, each thread maintains a descriptor recording \nall the information required to ful.ll its Permission to make digital or hard copies of all or part of \nthis work for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. PLDI 13, June 16 19, 2013, Seattle, WA, USA. Copyright c &#38;#169; \n2013 ACM 978-1-4503-2014-6/13/06. . . $15.00 intended operation. When a thread A detects con.icts with \nanother thread B, A may access B s descriptor and help B .nish its intended operation .rst before .nishing \nits own. In this case, B s operation takes effect at a step from A. Thus its LP should not be in its \nown code, but in the code of thread A. Besides, in optimistic algorithms and lazy algorithms (e.g., Heller \net al. s lazy set [13]), the LPs might depend on unpredictable future interleavings. In those algorithms, \na thread may access the shared states as if no interference would occur, and validate the accesses later. \nIf the validation succeeds, it .nishes the operation; otherwise it rolls back and retries. Its LP is \nusually at a prior state access, but only if the later validation succeeds. Reasoning about algorithms \nwith non-.xed LPs has been a long-standing problem. Most existing work either supports only simple objects \nwith static LPs in the implementation code (e.g.,[2, 5, 19, 30]), or lacks formal soundness arguments \n(e.g., [32]). In this paper, we propose a program logic for veri.cation of linearizability with non-.xed \nLPs. For a concrete implementation of an object method, we treat the corresponding abstract atomic operation \nand the abstract state as auxiliary states, and insert auxiliary commands at the LP to execute the abstract \noperation simultaneously with the concrete step. We verify the instrumented implementation in an existing \nconcurrent program logic (we will use LRG [8] in this paper), but extend it with new logic rules for \nthe auxiliary commands. We also give a new relational interpretation to the logic assertions, and show \nthat at the LP, the step of the original concrete implementation has the same effect as the abstract \noperation. We handle non-.xed LPs in the following way: To support the helping mechanism, we collect \na pending thread pool as auxiliary state, which is a set of threads and their abstract operations that \nmight be helped. We allow the thread that is currently being veri.ed to use auxiliary commands to help \nexecute the abstract operations in the pending thread pool.  For future-dependent LPs, we introduce \na try-commit mecha\u00adnism to reason with uncertainty. The try clause guesses whether the corresponding \nabstract operation should be executed and keeps all possibilities, while commit chooses a speci.c pos\u00adsible \ncase when we know which guess is correct later.  Although our program logic looks intuitive, it is challenging \nto prove that the logic is sound w.r.t. linearizability. Recent work has shown the equivalence between \nlinearizability and contextual re\u00ad.nement [5, 9, 10]. The latter is often veri.ed by proving simula\u00adtions \nbetween the concrete implementation and the atomic opera\u00adtion [5]. The simulation establishes some correspondence \nbetween the executions of the two sides, showing there exists one step in the concrete execution that \nful.lls the abstract operation. Given the equivalence between linearizability and re.nement, we would \nex\u00adpect the simulations to justify the soundness of the LP method and to serve as the meta-theory of \nour logic. However, existing thread\u00adlocal simulations do not support non-.xed LPs (except the recent \nwork [31], which we will discuss in Sec. 7). We will explain the challenges in detail in Sec. 2.  Our \nwork is inspired by the earlier work on linearizability veri.cation, in particular the use of auxiliary \ncode and states by Vafeiadis [32] and our previous work on thread-local simulation RGSim [19], but makes \nthe following new contributions: We propose the .rst program logic that has a formal soundness proof \nfor linearizability with non-.xed LPs. Our logic is built upon the unary program logic LRG [8], but we \ngive a relational interpretation of assertions and rely/guarantee conditions. We also introduce new logic \nrules for auxiliary commands used speci.cally for linearizability proofs.  We give a light instrumentation \nmechanism to relate con\u00adcrete implementations with abstract operations. The system\u00adatic use of auxiliary \nstates and commands makes it possible to execute the abstract operations synchronously with the con\u00adcrete \ncode. The try-commit clauses allow us to reason about future-dependent uncertainty without resorting \nto prophecy variables [1, 32], whose existing semantics (e.g., [1]) is un\u00adsuitable for Hoare-style veri.cation. \n We design a novel thread-local simulation as the meta-theory for our logic. It generalizes RGSim [19] \nand other composi\u00adtional reasoning of re.nement (e.g., [5, 30]) with the support for non-.xed LPs.  \nInstead of ensuring linearizability directly, the program logic and the simulation both establish contextual \nre.nement, which we prove is equivalent to linearizability. A program logic for contextual re.nement \nis interesting in its own right, since con\u00adtextual re.nement is also a widely accepted (and probably \nmore natural) correctness criterion for library code.  We successfully apply our logic to verify 12 \nwell-known algo\u00adrithms. Some of them are used in the java.util.concurrent package, such as MS non-blocking \nqueue [23] and Harris-Michael lock-free list [11, 22].  In the rest of this paper, we .rst analyze the \nchallenges in the logic design and explain our approach informally in Sec. 2. Then we give the basic \ntechnical setting in Sec. 3, including a formal operational de.nition of linearizability. We present \nour program logic in Sec. 4, and the new simulation relation as the meta-theory in Sec. 5. In Sec. 6 \nwe summarize all the algorithms we have veri.ed and sketch the proofs of three representative algorithms. \nWe discuss related work and conclude in Sec. 7. 2. Challenges and Our Approach Below we start from a \nsimple program logic for linearizability with .xed LPs, and extend it to support algorithms with non-.xed \nLPs. We also discuss the problems with the underlying meta-theory, which establishes the soundness of \nthe logic w.r.t. linearizability. 2.1 Basic Logic for Fixed LPs We .rst show a simple and intuitive logic \nwhich follows the LP approach. As a working example, Fig. 1(a) shows the implementa\u00adtion of push in Treiber \nstack [29] (let s .rst ignore the blue code at line 7 ). The stack object is implemented as a linked \nlist pointed to by S,and push(v) repeatedly tries to update S to point to the new node using compare-and-swap \n(cas) until it succeeds. To verify linearizability, we .rst locate the LP in the code. The LP of push(v) \nis at the cas statement when it succeeds (line 7). That is, the successful cas can correspond to the \nabstract atomic PUSH(v) operation: Stk := v::Stk; and all the other concrete steps cannot. Here we simply \nrepresent the abstract stack Stk as 1 readPair(int i, j) { 1 push(int v) { 2 local a, b, v, w; 2 local \nx, t, b; 3 while(true) { 3 x new 4 <a := m[i].d; v := m[i].v;> := node(v); 4do { 5 <b := m[j].d; w := \nm[j].v; 5 t := S; 5 trylinself;> 6 if(v = m[i].v) { 7 <b := cas(&#38;S,t,x); 6 commit(cid > (end, (a, \nb))); 7 if(b) linself;> 6 x.next := t; 7 return (a, b); } 8 } while(!b); 8}} 9} 9 write(int i, d) { (a) \nTreiber Stack 10 <m[i].d := d; m[i].v++;> } (c) Pair Snapshot 1 push(int v) { 2 local p, him, q; 3 p \n:= new thrdDescriptor(cid, PUSH, v); 4 while(true) { 5 if (tryPush(v)) return; 6 loc[cid] := p; 7 him \n:= rand(); q := loc[him]; 8 if (q != null &#38;&#38; q.id = him &#38;&#38; q.op = POP) 9 if (cas(&#38;loc[cid], \np, null)) { 10 <b := cas(&#38;loc[him], q, p); 10 if(b) {lin(cid); lin(him);}> 11 if (b) return; } 12 \n... 13 } } (b) HSY Elimination-Based Stack Figure 1. LPs and Instrumented Auxiliary Commands a sequence \nof values with :: for concatenation. Then push(v) can be linearized at the successful cas since it is \nthe single point where the operation takes effect. We can encode the above reasoning in an existing (unary) \ncon\u00adcurrent program logic, such as Rely-Guarantee reasoning [17] and CSL [24]. Inspired by Vafeiadis \n[32], we embed the abstract oper\u00adation . and the abstract state . as auxiliary states on the concrete \nside, so the program state now becomes (s, (., .)),where s is the original concrete state. Then we instrument \nthe concrete implemen\u00adtation with an auxiliary command linself (shorthand for linearize self ) at the \nLP to update the auxiliary state. Intuitively, linself will execute the abstract operation . over the \nabstract state .,asde\u00adscribed in the following operational semantics rule: (., .) ' (end,.') (linself, \n(s, (., .))) -. (skip, (s, (end,.'))) Here . encodes the transition of . at the abstract level, and end \nis a termination marker. We insert linself into the same atomic block with the concrete statement at \nthe LP, such as line 7 in Fig. 1(a), so that the concrete and abstract sides are executed simultaneously. \nHere the atomic block (C) means C is executed atomically. Then we reason about the instrumented code \nusing a traditional concur\u00adrent logic extended with a new inference rule for linself. The idea is intuitive, \nbut it cannot handle more advanced algo\u00adrithms with non-.xed LPs, including the algorithms with the help\u00ading \nmechanism and those whose locations of LPs depend on the future interleavings. Below we analyze the two \nchallenges in detail and explain our solutions using two representative algorithms, the HSY stack and \nthe pair snapshot.  2.2 Support Helping Mechanism with Pending Thread Pool HSY elimination-based stack \n[14] is a typical example using the helping mechanism. Figure 1(b) shows part of its push method implementation. \nThe basic idea behind the algorithm is to let a push and a pop cancel out each other. At the beginning \nof the method in Fig. 1(b), the thread allocates its thread descriptor (line 3), which contains the thread \nid, the name of the operation to be performed, and the argument. The current thread cid .rst tries to \nperform Treiber stack s push (line 5). It returns if succeeds. Otherwise, it writes its descriptor in \nthe global loc array (line 6) to allow other threads to eliminate its push. The elimination array loc[1..n] \nhas one slot for each thread, which holds the pointer to a thread descriptor. The thread randomly reads \na slot him in loc (line 7). If the descriptor q says him is doing pop, cid tries to eliminate itself \nwith him by two cas instructions. The .rst clears cid s entry in loc so that no other thread could eliminate \nwith cid (line 9). The second attempts to mark the entry of him in loc as eliminated with cid (line 10). \nIf successful, it should be the LPs of both the push of cid and the pop of him, with the push happening \nimmediately before the pop.  The helping mechanism allows the current thread to linearize the operations \nof other threads, which cannot be expressed in the basic logic. It also breaks modularity and makes thread-local \nver\u00adi.cation dif.cult. For the thread cid, its concrete step could cor\u00adrespond to the steps of both cid \nand him at the abstract level. For him, a step from its environment could ful.ll its abstract operation. \nWe must ensure in the thread-local veri.cation that the two threads cid and him always take consistent \nviews on whether and how the abstract operation of him is done. For example, if we let a concrete step \nin cid ful.ll the abstract pop of him, we must know him is indeed doing pop and its pop has not been \ndone before. Otherwise, we will not be able to compose cid and him in parallel. We extend the basic logic \nto express the helping mechanism. First we introduce a new auxiliary command lin(t) to linearize a speci.c \nthread t. For instance, in Fig. 1(b) we insert line 10 at the LP to execute both the push of cid and \nthe pop of him at the abstract level. We also extend the auxiliary state to record both abstract operations \nof cid and him. More generally, we embed a pending thread pool U, which maps threads to their abstract \noperations. It speci.es a set of threads whose operations might be helped by others. Then under the new \nstate (s, (U, .)),the semantics of lin(t) just executes the thread t s abstract operation in U, similarly \nto the semantics of linself discussed before. The shared pending thread pool U allows us to recover the \nthread modularity when verifying the helping mechanism. A con\u00adcrete step of cid could ful.ll the operation \nof him in U as well as its own abstract operation; and conversely, the thread him running in parallel \ncould check U to know if its operation has been .nished by others (such as cid) or not. We gain consistent \nabstract infor\u00admation of other threads in the thread-local veri.cation. Note that the need of U itself \ndoes not break modularity because the required information of other threads abstract operations can be \ninferred from the concrete state. In the HSY stack example, we know him is doing pop by looking at its \nthread descriptor in the elimination array. In this case U can be viewed as an abstract representation \nof the elimination array.  2.3 Try-Commit Commands for Future-Dependent LPs Another challenge is to \nreason about optimistic algorithms whose LPs depend on the future interleavings. We give a toy example, \npair snapshot [27], in Fig. 1(c). The object is an array m, each slot of which contains two .elds: d \nfor thedataand v for the version number. The write(i,d) method (lines 9) updates the data stored at address \ni and increments the version number instantaneously. The readPair(i,j) method in\u00adtends to perform an \natomic read of two slots i and j in the presence of concurrent writes. It reads the data at slots i and \nj separately at lines 4 and 5, and validate the .rst read at line 6.If i s version num\u00adber has not been \nincreased, the thread knows that when it read j s data at line 5, i s data had not been updated. This \nmeans the two reads were at a consistent state, thus the thread can return. We can see that the LP of \nreadPair should be at line 5 when the thread (a) Simple Simulation (b) Pending Thread Pool (c) Speculation \nFigure 2. Simulation Diagrams reads j s data, but only if the validation at line 6 succeeds. That is, \nwhether we should linearize the operation at line 5 depends on the future unpredictable behavior of line \n6. As discussed a lot in previous work (e.g., [1, 32]), the future\u00addependent LPs cannot be handled by \nintroducing history variables, which are auxiliary variables storing values or events in the past ex\u00adecutions. \nWe have to refer to events coming from the unpredictable future. Thus people propose prophecy variables \n[1, 32] as the dual of history variables to store future behaviors. But as far as we know, there is no \nsemantics of prophecy variables suitable for Hoare-style local and compositional reasoning. Instead of \nresorting to prophecy variables, we follow the specu\u00adlation idea [31]. For the concrete step at a potential \nLP (e.g., line 5 of readPair), we execute the abstract operation speculatively and keep both the result \nand the original abstract con.guration. Later based on the result of the validation (e.g., line 6 in \nreadPair), we keep the appropriate branch and discard the other. For the logic, we introduce two new \nauxiliary commands: trylinself is to do speculation, and commit(p) will commit to the appropriate branch \nsatisfying the assertion p.InFig.1(c),weinsert lines 5 and 6 ,where cid > (end, (a, b)) means that the \ncur\u00adrent thread cid should have done its abstract operation and would return (a, b). We also extend the \nauxiliary state to record the mul\u00adtiple possibilities of abstract operations and abstract states after \nspeculation. Furthermore, we can combine the speculation idea with the pending thread pool. We allow \nthe abstract operations in the pend\u00ading thread pool as well as the current thread to speculate. Then \nwe could handle some trickier algorithms such as RDCSS [12], in which the location of LP for thread t \nmay be in the code of some other thread and also depend on the future behaviors of that thread. Please \nsee Sec. 6 for one such example.  2.4 Simulation as Meta-Theory The LP proof method can be understood \nas building simulations be\u00adtween the concrete implementations and the abstract atomic opera\u00adtions, such \nas the simple weak simulation in Fig. 2(a). The lower\u00adlevel and higher-level arrows are the steps of \nthe implementation and of the abstract operation respectively, and the dashed lines de\u00adnote the simulation \nrelation. We use dark nodes and white nodes at the abstract level to distinguish whether the operation \nhas been .nished or not. The only step at the concrete side corresponding to the single abstract step \nshould be the LP of the implementation (labeled LP in the diagram). Since our program logic is based \non the LP method, we can expect simulations to justify its soundness. In particular, we want a thread-local \nsimulation which can handle both the helping mechanism and future-dependent LPs and can en\u00adsure linearizability. \nTo support helping in the simulation, we should allow the LP step at the concrete level to correspond \nto an abstract step made by a thread other than the one being veri.ed. This requires informa\u00adtion from \nother threads at the abstract side, thus makes it dif.cult to build a thread-local simulation. To address \nthe problem, we intro\u00adduce the pending thread pool at the abstract level of the simulation, just as in \nthe development of our logic in Sec. 2.2. The new simula\u00ad  (MName) f . String (Expr) E ::= x | n | E \n+ E | ... (BExp) B ::= true | false | E = E | !B | ... (Instr) c ::= x := E | x := [E] | [E]:= E | print(E) \n| x := cons(E , ...,E ) | dispose(E) | ... (Stmt) C ::= skip | c | x := f(E) | return E | noret |(C)| \nC; C | if (B) C else C | while (B){C} (Prog) W ::= skip | let . in C I... IC (ODecl) . ::= {f1 (x1,C1),...,fn \n(xn,Cn)} Figure 3. Syntax of the Programming Language tion is shown in Fig. 2(b). We can see that a concrete \nstep of thread t could help linearize the operation of t ' in the pending thread pool as well as its \nown operation. Thus the new simulation intuitively supports the helping mechanism. As forward simulations, \nneither of the simulations in Fig. 2(a) and (b) supports future-dependent LPs. For each step along the \ncon\u00adcrete execution in those simulations, we need to decide immedi\u00adately whether the step is at the LP, \nand cannot postpone the decision to the future. As discussed a lot in previous work (e.g., [1, 3, 6, \n21]), we have to introduce backward simulations or hybrid simulations to support future-dependent LPs. \nHere we exploit the speculation idea and develop a forward-backward simulation [21]. As shown in Fig. \n2(c), we keep both speculations after the potential LP, where the higher black nodes result from executing \nthe abstract operation and the lower white nodes record the original abstract con.gura\u00adtion. Then at \nthe validation step we commit to the correct branch. Finally, to ensure linearizability, the thread-local \nsimulation has to be compositional. As a counterexample, we can construct a simple simulation (like the \none in Fig. 2(a)) between the following implementation C and the abstract atomic increment operation \n., but C is not linearizable w.r.t. .. C : local t; t := x; x :=t+1; . : x++ The reason is that the simple \nsimulation is not compositional w.r.t. parallel compositions. To address this problem, we proposed a \ncompositional simulation RGSim [19] in previous work. The idea is to parameterize the simple simulation \nwith the interference with the environment, in the form of rely/guarantee conditions (R and G) [17]. \nRGSim says, the concrete executions are simulated by the abstract executions under interference from \nthe environment R,and all the related state transitions of the thread being veri.ed should satisfy G. \nFor parallel composition, we check that the guarantee G of each thread is permitted in the rely R of \nthe other. Then the simulation becomes compositional and can ensure linearizability. We combine the above \nideas and develop a new compositional simulation with the support of non-.xed LPs as the meta-theory \nof our logic. We will discuss our simulation formally in Sec. 5. 3. Basic Technical Settings and Linearizability \nIn this section, we formalize linearizability of an object implemen\u00adtation w.r.t. its speci.cation, and \nshow that linearizability is equiv\u00adalent to contextual re.nement. 3.1 Language and Semantics As showninFig.3,aprogram \nW contains several client threads in parallel, each of which could call the methods declared in the object \n.. A method is de.ned as a pair (x, C ),where x is the formal argument and C is the method body. For \nsimplicity, we assume there is only one object in W and each method takes one argument only, but it is \neasy to extend our work with multiple objects and arguments. (ThrdID) t . Nat (Mem) s . (PVar . Nat) \n. Int (CallStk) . ::= (sl,x,C ) |. (ThrdPool) K ::= {t1 .1,..., tn .n} (PState) S ::= (sc,so, K) (LState) \ns ::= (sc,so,.) (Evt) e ::= (t,f,n) | (t, ok,n) | (t, obj, abort) | (t, out,n) | (t, clt, abort) (ETrace) \nH ::= e | e::H Figure 4. States and Event Traces We use a runtime command noret to abort methods that \ntermi\u00adnate but do not execute return E. It is automatically appended to the method code and is not supposed \nto be used by programmers. Other commands are mostly standard. Clients can use print(E) to produce observable \nexternal events. We do not allow the object s methods to produce external events. To simplify the semantics, \nwe also assume there are no nested method calls. Figure 4 gives the model of program states. Here we \npartition a global state S into the client memory sc, the object so and a thread pool K. A client can \nonly access the client memory sc, unless it calls object methods. The thread pool maps each thread id \nt to its local call stack frame. A call stack . could be either empty (.)when the thread is not executing \na method, or a triple (sl,x,C ),where sl maps the method s formal argument and local variables (if any) \nto their values, x is the caller s variable to receive the return value, and C is the caller s remaining \ncode to be executed after the method returns. To give a thread-local semantics, we also de.ne the thread \nlocal view s of the state. Figure 5 gives selected rules of the operational semantics. We show three \nkinds of transitions: for the top-level program -. transitions, -.t,. for the transitions of thread t \nwith the methods declaration .,and -_t for the steps inside method calls of thread t. To describe the \noperational semantics for threads, we use an execution context E: (ExecContext) E ::= [ ] | E; C The \nhole [] shows the place where the execution of code occurs. E[ C ] represents the code resulting from \nplacing C into the hole. We label transitions with events e de.ned in Fig. 4. An event could be a method \ninvocation (t,f,n) or return (t, ok,n),afault (t, obj, abort) produced by the object method code, an \noutput (t, out,n) generated by print(E),orafault (t, clt, abort) from the client code. The .rst two events \nare called object events, and the last two are observable external events. The third one (t, obj, abort) \nbelongs to both classes. An event trace H is then de.ned as a .nite sequence of events.  3.2 Object \nSpeci.cation and Linearizability Next we formalize the object speci.cation G, which maps method names \nto their abstract operations ., as shown in Fig. 6. . trans\u00adforms an argument value and an initial abstract \nobject to a return value with a resulting abstract object in a single step. It speci.es the intended \nsequential behaviors of the method. The abstract ob\u00adject representation . is de.ned as a mapping from \nprogram vari\u00adables to abstract values. We leave the abstract values unspeci.ed here, which can be instantiated \nby programmers. Then we give an abstract version of programs W, where clients interact with the abstract \nobject speci.cation G instead of its im\u00adplementation .. The semantics is almost the same as the concrete \nlanguage shown in Sec. 3.1, except that the abstract atomic opera\u00adtion . is executed when the method \nis called, which now operates over the abstract object . instead of over the concrete one so.The  e \n(Ci, (sc,so, K(i))) -.i,. (Ci ' , (sc ' ,so ' ,. ' )) e ' (let . in C1 I...Ci ...ICn, (sc,so, K)) .i \n...ICn, (s ' , K{i . ' })) -. (let . in C1 I...C ,s ' c o (a) Program Transitions .(f)=(y, C) [E]sc \n= n x . dom(sc) . =({y n},x, E[ skip ]) f . dom(.) or [E]sc unde.ned or x . dom(sc) (t,f,n)(t,clt,abort) \n(E[ x := f(E)], (sc,so, .)) ----. (C; noret, (sc,so,.)) (E[ x := f(E)], (sc,so, .)) -------. abort t,. \nt,. . =(sl,x,C) [E]solsl = n = sc{x n} sc s ' [E]= n c (t,out,n)(t,ok,n) (E[ return E ], (sc,so,.)) \n-----.t,. (C, (sc ' ,so, .)) (E[ print(E)], (sc,so, .)) -----.t,. (E[ skip ], (sc,so, .)) (C, so 1 sl) \n--t (C ' ,s ' 1 s ' ) dom(sl)= dom(s ' ) ol l (t,obj,abort) ' (noret,s) -------. abort (C, (sc,so, (sl,x,Cc))) \n-.t,. (C , (sc,s ' , (s ' ,x,Cc))) t,. o l (b) Thread Transitions -* [Ej ]s unde.ned (1 = j = i) or \nx . dom(s) (C, s) --* (skip,s ' ) (C, s) -abort t t (E[ x := cons(E1,...,Ei)],s) --t abort (E[ (C) ],s) \n--t (E[ skip ],s ' ) (E[ (C) ],s) --t abort (c) Thread Transitions Inside Method Calls Figure 5. Selected \nRules of Concrete Operational Semantics (AbsObj) . . PVar AbsVal (MSpec) . . Int . AbsObj Int \u00d7 AbsObj \n(OSpec) G ::= {f1 .1,...,fn .n} (AbsProg) W ::= skip | with G do C I...IC Figure 6. Object Speci.cation \nand Abstract Program abstract operation generates a pair of invocation and return events atomically. \nDue to space limit, we give the semantics in TR [18]. Linearizability. Linearizability [16] is de.ned \nusing the notion of histories, which are special event traces H consisting of only object events (i.e., \ninvocations, returns and object faults). Belowweuse H(i) for the i-th event of H,and |H| for the length \nof H. H|t represents the sub-history consisting of all the events whose thread id is t. The predicates \nis inv(e) and is res(e) mean that the event e is a method invocation and a response (i.e.,a return or \nan object fault) respectively. We say a response e2 matches an invocation e1 iff they have the same thread \nid. A history H is sequential iff the .rst event of H is an invoca\u00adtion, and each invocation, except \npossibly the last, is immediately followed by a matching response. Then H is well-formed iff, for all \nt, H|t is sequential. H is complete iff it is well-formed and every invocation has a matching response. \nAn invocation is pending if no matching response follows it. We handle pending invocations in an incomplete \nhistory H following the standard linearizability de.ni\u00adtion [16]: we append zero or more response events \nto H, and drop the remaining pending invocations. Then we get a set of complete histories, which is denoted \nby completions(H). We give formal de.nitions of the above concepts in TR [18]. De.nition 1 (Linearizable \nHistories). H :lin H ' iff 1. .t.H|t = H ' |t; 2. there exists a bijection p : {1,..., |H|} . {1,..., \n|H ' |} such that .i. H(i)= H ' (p(i)) and  .i, j. i < j . is res(H(i)) . is inv(H(j)) =. p(i) <p(j). \nThat is, H is linearizable w.r.t. H ' if the latter is a permutation of the former, preserving the order \nof events in the same threads and the order of the non-overlapping method calls. We use H [ W, (sc,so)]] \nto represent the set of histories pro\u00adduced by the executions of W with the initial client memory sc, \nthe object so, and empty call stacks for all threads, and use H[ W, (sc,.)]] to generate histories from \nthe abstract program W with the initial client memory sc and the abstract object .. A legal sequential \nhistory H is a history generated by any client using the speci.cation G and an initial abstract object \n.. def G C (., H) = .n, C1,...,Cn,sc.H .H [][(with G do C1 I...ICn), (sc,.)] Then an object is linearizable \niff all its completed concurrent histo\u00adries are linearizable w.r.t. some legal sequential histories. \nDe.nition 2 (Linearizability of Objects). The object s implemen\u00adtation . is linearizable w.r.t. its speci.cation \nG under a re.nement mapping ., denoted by . :. G,iff .n, C1,...,Cn,sc,so,.,H. H .H [] . (.(so)= .) [(let \n. in C1 I...ICn), (sc,so)] ' =..Hc,H ' .Hc . completions(H) . G C (., H ' ) . Hc :lin H Here the mapping \n. relates concrete objects to abstract ones: (RefMap) . . Mem AbsObj The side condition .(so)= . in the \nabove de.nition requires the initial concrete object so to be a well-formed data structure representing \na valid object ..  3.3 Contextual Re.nement and Linearizability Next we de.ne contextual re.nement between \nthe concrete object and its speci.cation, and prove its equivalence to linearizability. This equivalence \nwill serve as the basis of our logic soundness w.r.t. linearizability. Informally, the contextual re.nement \nsays, for any set of client threads, the program W has no more observable behaviors than the corresponding \nabstract program W. Below we use O [ W, (sc,so)]] to represent the set of observable event traces generated \nduring the executions of W with the initial state (sc,so) (and empty stacks). It is de.ned similarly \nas H [ W, (sc,so)]], but now the traces consist of observable events only (output events, client faults \nor object faults). The observable event traces O [ W, (sc,.)]] generated by the abstract program is de.ned \nsimilarly.  (InsStmt) Cc::= skip | c | return E | noret | linself | lin(E) | trylinself | trylin(E) \n| commit(p) |(Cc)| Cc; Cc| if (B) Ccelse Cc| while (B){Cc} (RelState) S ::= (s, .) (SpecSet) . ::= {(U1,.1),..., \n(Un,.n)} (PendThrds) U ::= {t1 .1,..., tn .n} (AbsOp) . ::= (., n) | (end,n) (RelAss) p, q, I ::= true \n| false | E = E | emp | E .. E | x. E | E > (., E) | E > (end,E) | p * q | p . q | p . q | ... (RelAct) \nR, G ::= p P q | [p] | R * R | R . R | ... Figure 7. Instrumented Code and Relational State Model def \n= {(\u00d8, \u00d8)} where . SpecSet def f.g = dom(f) n dom(g)= \u00d8 def .1..2 = U1.U2 . .1..2 , where (U1,.1)..1 \n. (U2,.2)..2 def .1 * .2 = {(U1 1U2,.1 1.2) | (U1,.1)..1 . (U2,.2)..2} def S1 * S2 =(s1 1 s2, .1 * .2) \nwhere S1 =(s1, .1), S2 =(s2, .2),s1.s2 and .1..2 def (s, .1 ..2) if S1 =(s,.1) and S2 =(s,.2)S1 . S2 \n= unde.ned otherwise def [E]s if dom(s)= fv(E) { E} s = unde.ned otherwise (s, .) |= E1 = E2 iff { (E1 \n= E2)} s = true . .= (s, .) |= emp iff s = \u00d8. .= (s, .) |= E1 . E2 iff .l, n, s ' . { (E1,E2)} s1 =(l, \nn) . s = s ' 1{l n}. .= (s, .) |= x. E iff .n, .. { E} s = n . . = {x n} . .= {(\u00d8,.)} (s, .) |= E1 \n> (., E2) iff .s1,s2, t,n. s = s1 1 s2 .{{E1} s1 =t .{{E2} s2 =n . .= {({t (., n)}, \u00d8)} (s, .) |= E1 \n> (end,E2) iff .s1,s2, t,n. s = s1 1 s2 .{{E1} s1 =t .{{E2} s2 =n . .= {({t (end,n)}, \u00d8)} S |= p * q \niff .S1, S2. S=S1 * S2 . S1 |= p . S2 |= q S |= p . q iff .S1, S2. S=S1 . S2 . S1 |= p . S2 |= q SpecExact(p) \niff ..,. ' . (( ,.)|=p) . (( ,. ' )|=p)=. (.=. ' ) Figure 8. Semantics of State Assertions De.nition \n3 (Contextual Re.nement). . .. G iff .n, C1,...,Cn,sc,so,.. .(so)= . =.O [] [(let . in C1 I...ICn), (sc,so)] \n.O [] . [(with G do C1 I...ICn), (sc,.)] Following Filipovi\u00b4c et al. [9], we can prove that linearizability \nis equivalent to contextual re.nement. We give the proofs in TR [18]. Theorem 4 (Equivalence). . :. G \n.. . .. G. 4. A Relational Rely-Guarantee Style Logic To prove object linearizability, we .rst instrument \nthe object imple\u00admentation by introducing auxiliary states and auxiliary commands, which relate the concrete \ncode with the abstract object and oper\u00adations. Our program logic extends LRG [8] with a relational in\u00adterpretation \nof assertions and new rules for auxiliary commands. (S, S ' ) |= p P q iff S |= p . S ' |= q (S, S ' \n) |=[p] iff S |= p . S=S ' (S, S ' ) |= R1 * R2 iff .S1, S2, S ' 1, S2 ' . (S = S1 * S2) . (S ' =S ' \n* S ' ) 1 2 . (S1, S ' ) |= R1 . (S2, S ' ) |= R2 1 2 (S, S ' ) |= R1 . R2 iff .S1, S2, S ' 1, S2 ' . \n(S = S1 . S2) . (S ' =S ' . S ' ) 1 2 . (S1, S ' ) |= R1 . (S2, S ' ) |= R2 1 2 def def Id =[true] True \n= true P true . ' .(n)(.)=(n ' ,. ' )(.,n) -. (. ' ,n ) . . (\u00d8,n) -. (\u00d8,n ' ) ({(U, .)}1 .,n) -. ({(U, \n. ' )}1 . ' ,n ' ) [E, p].[E ' ,q] iff .s, .,n. (s, .) |=(E =n) * p . ' =... ' ,n . (.,n) -. (. ' ,n \n' ) . ((s, . ' ) |=(E ' =n ' ) * q) Ir R iff ([I] . R) . (R . I P I) . Precise(I) Figure 9. Semantics \nof Actions Although our logic is based on LRG [8], this approach is mostly in\u00addependent with the base \nlogic. Similar extensions can also be made over other logics, such as RGSep [32]. Our logic is proposed \nto verify object methods only. Veri.ed object methods are guaranteed to be a contextual re.nement of \ntheir abstract atomic operations, which ensures linearizability of the object. We discuss veri.cation \nof whole programs consisting of both client code and object code at the end of Sec. 4.3. 4.1 Instrumented \nCode and States In Fig. 7 we show the syntax of the instrumented code and its state model. As explained \nin Sec. 2, program states S for the object method executions now consist of two parts, the physical object \nstates s and the auxiliary data .. . is a nonempty set of (U, .) pairs, each pair representing a speculation \nof the situation at the abstract level. Here . is the current abstract object, and U is a pending thread \npool recording the remaining operation to be ful.lled by each thread. It maps a thread id to its remaining \nabstract operation, which is either (., n) (the operation . needs to be executed with argument n)or (end,n) \n(the operation has been .nished with the return value n). We assume . is always domain\u00adexact, de.ned \nas follows: def DomExact(.) = .U, ., U ' ,. ' . (U, .) . . . (U ' ,. ' ) . . ' =. dom(U)=dom(U ) . dom(.)=dom(. \n' ) . It says, all the speculations in . should describe the same set of threads and the same domain \nof abstract objects. Any . containing a single speculation is domain-exact. Also domain-exactness can \nbe preserved under the step of any command in our instrumented language, thus it is reasonable to assume \nit always holds. Below we informally explain the effects over . of the newly introduced commands. We \nleave their formal semantics to Sec. 4.4. The auxiliary command linself executes the un.nished abstract \nop\u00aderation of the current thread in every U in ., and changes the ab\u00adstract object . correspondingly. \nlin(E) executes the abstract opera\u00adtion of the thread with id E. linself or lin(E) is executed when we \nknow for sure that a step is the linearization point. The trylinself command introduces uncertainty. \nSince we do not know if the ab\u00adstract operation of the current thread is ful.lled or not at the cur\u00adrent \npoint, we consider both possibilities. For each (U, .) pair in . that contains un.nished abstract operation \nof the current thread, we add in . a new speculation (U ' ,. ' ) where the abstract operation is done \nand . ' is the resulting abstract object. Since the original (U, .) is also kept, we have both speculations \nin .. Similarly, the  [E1,p].[E2,q] (LINSELF) (LINSELF-END) ht {t > (., E1) * p}linself{t > (end,E2) \n* q} ht {t > (end,E)}linself{t > (end,E)} [E1,p].[E2,q] (TRY) ht {E > (., E1) * p}trylin(E){(E > (., \nE1) * p) . (E > (end,E2) * q)} ' SpecExact(p) p . p (TRY-END) (COMMIT) ht {E > (end,E ' )}trylin(E){E \n> (end,E ' )} ht {p ' . true}commit(p){p ' } ht {p}Cc{q} ht {p}Cc{q} ht {p ' }Cc{q ' } (RET) (FRAME) \n(SPEC-CONJ) ht {t > (end,E)}E[ return E ]{t > (end,E)} ht {p * r}Cc{q * r} ht {p . p ' }Cc{q . q ' } \nht {p} cC{q} (p P q) . G * True [I],G,I ht {p}( cC){q} Ir G p . q . I * true Sta({p, q},R * Id) Ir R \n(ATOM) (ATOM-R) [I],G,I ht {p}( cC){q} R, G, I ht {p}( cC){q} Figure 10. Selected Inference Rules trylin(E) \ncommand introduces speculations about the thread E. When we have enough knowledge p about the situation \nof the ab\u00adstract objects and operations, the commit(p) step keeps only the subset of speculations consistent \nwith p and drops the rest. Here p is a logical assertion about the state S, which is explained below. \n 4.2 Assertions Syntax of assertions is shown in Fig. 7. Following rely-guarantee style reasoning, assertions \nare either single state assertions p and q or binary rely/guarantee conditions R and G. Note here states \nrefer to the relational states S. We use standard separation logic assertions such as true, E1 = E2, \nemp and E1 . E2 to specify the memory s.Asshown in Fig. 8, their semantics is almost standard, but for \nE1 = E2 to hold over s we require the domain of s contains only the free variables in E1 and E2.Hereweuse \n{ E} s to evaluate E with the extra requirement that s contains the exact resource to do the evaluation. \nNew assertions are introduced to specify .. x . E speci.es the abstract object . in ., with no speculations \nof U (abstract operations), while E1 > (., E2) (and E1 > (end,E2)) speci.es the singleton speculation \nof U. Semantics of separating conjunction p * q is similar as in separation logic, except that it is \nnow lifted to assertions over the relational states S. Note that the underlying disjoint union over . \nfor separating conjunction should not be confused with the normal disjoint union operator over sets. \nThe former (denoted as .1 *.2 in Fig. 8) describes the split of pending thread pools and/or abstract \nobjects. For example, the left side . in the following equation speci.es two speculations of threads \nt1 and t2 (we assume the abstract object part is empty and omitted here), and it can be split into two \nsets .1 and .2 on the right side, each of which describes the speculations of a single thread. .1 .2 \n { t1 .1 . ' 2 }.1 t1 t1 , = * t2 t2 { t2 . ' , t2 } .2 2 The most interesting new assertion is p \n. q,where p and q specify two different speculations. It is this assertion that re.ects uncertainty about \nthe abstract level. However, the readers should not confuse . with disjunction. It is more like conjunction \nsince it says . contains both speculations satisfying p and those satisfying q. As an example, the above \nequation could be formulated at the assertion level using * and .: (t1 > .1 * t2 > .2) . (t1 > .1 * t2 \n> . ' ) 2 . t1 > .1 * (t2 > .2 . t2 > . ' ) 2 Rely and guarantee assertions specify transitions over \nS. Here we follow the syntax of LRG [8], with a new assertion R1 . R2 specifying speculative behaviors \nof the environment. The semantics is given in Fig. 9. We will show the use of the assertions in the examples \nof Sec. 6.  4.3 Inference Rules The rules of our logic are shown in Fig. 10. Rules on the top half are \nfor sequential Hoare-style reasoning. They are proposed to verify code C in the atomic block (C ). The \njudgment is parameterized with the id t of the current thread. For the linself command, if the abstract \noperation . of the cur\u00adrent thread has not been done, this command will .nish it. Here [E1,p].[E2,q] \nin the LINSELF rule describes the behavior of ., which transforms abstract objects satisfying p to new \nones satisfy\u00ading q. E1 and E2 are the argument and return value respectively. The de.nition is given \nin Fig. 9. The LINSELF-END rule says linself has no effects if we know the abstract operation has been \n.nished. The LIN rule and LIN-END rule are similar and omitted here. The TRY rule says that if the thread \nE has not .nished the abstract operation ., it can do speculation using trylin(E).The resulting state \ncontains both cases, one says . does not progress at this point and the other says it does. If the current \nthread has already .nished the abstract operation, trylin(E) would have no effects, as shown in the TRY-END \nrule. We omit the TRYSELF rule and TRYSELF-END rule for the current thread, which are similar. The above \ntwo pairs of rules require us to know for sure either the abstract operation has been .nished or not. \nIf we want to support uncertainty in the pre-condition, we could .rst consider different cases and then \napply the SPEC-CONJ rule, which is like the conjunction rule in traditional Hoare logic. The COMMIT rule \nallows us to commit to a speci.c speculation and drop the rest. commit(p) keeps only the speculations \nsatisfy\u00ading p. We require p to describe an exact set of speculations, as de\u00ad.ned by SpecExact(p) in Fig. \n8. For example, the following p1 is speculation-exact, while p2 is not: def p1 = t > (., n) . t > (end,n \n' ) def p2 = t > (., n) . t > (end,n ' ) In all of our examples in Sec. 6, the assertion p in commit(p) \ndescribes a singleton speculation, so SpecExact(p) trivially holds. Before the current thread returns, \nit must know its abstract operation has been done, as required in the RET rule. We also have a standard \nFRAME rule as in separation logic for local reasoning.  Rules in the bottom half show how to do rely-guarantee \nstyle concurrency reasoning, which are very similar to those in LRG [8]. As in LRG, we use a precise \ninvariant I to specify the boundary of the well-formed shared resource. The AT O M rule says we could \nreason sequentially about code in the atomic block. Then we can lift it to the concurrent setting as \nlong as its effects over the shared resource satisfy the guarantee G, which is fenced by the invariant \nI. In this step we assume the environment does not update shared re\u00adsource, thus using Id as the rely \ncondition (see Fig. 9). To allow general environment behaviors, we should apply the AT O M -R rule later, \nwhich requires that R be fenced by I and the pre-and post\u00adconditions be stable with respect to R. Here \nSta({p, q},R) re\u00adquires that p and q be stable with respect to R, a standard require\u00adment in rely-guarantee \nreasoning. More rules are shown in TR [18]. Linking with client program veri.cation. Our relational logic \nis introduced for object veri.cation, but it can also be used to verify client code, since it is just \nan extension over the general-purpose concurrent logic LRG (which includes the rule for parallel com\u00adposition). \nMoreover, as we will see in Sec. 5, our logic ensures contextual re.nement. Therefore, to verify a program \nW , we could replace the object implementation with the abstract operations and verify the corresponding \nabstract program W instead. Since W ab\u00adstracts away concrete object representation and method implemen\u00adtation \ndetails, this approach provides us with separation and in\u00adformation hiding [26] over the object, but \nstill keeps enough in\u00adformation (i.e., the abstract operations) about the method calls in concurrent \nclient veri.cation.  4.4 Semantics and Partial Correctness We .rst show some key operational semantics \nrules for the instru\u00admented code in Fig. 11. A single step execution of the instrumented code by thread \nt is represented as ( C\"-. C ' , S ' When we reach the C, S) t ( C). return E command (the second rule), \nwe require that there be no uncertainty about thread t at the abstract level in .. That is, in every \nspeculation in ., we always know t s operation has been .nished with the same return value E. Meanings \nof the auxiliary commands have been explained before. Here we use the auxiliary de.nition . .t . ' to \nformally de.ne their transitions over .. The semantics of commit(p) requires p to be speculation-exact \n(see Fig. 8). Also it uses (s, .)|p =(s ' , . ' ) to .lter out the wrong speculations. To ensure locality, \nthis .lter allows . to contain some extra resource such as the threads and their abstract operations \nother than those described in p. For example, the following . describes two threads t1 and t2, but we \ncould mention only t1 in commit(p). t1 (.1,n1) (.2,n2)  t1 .: , t2 t2 If p is t1 > (.1,n1),then commit(p) \nwill keep only the left speculation and discard the other. p can also be t1 > (.1,n1) . t1 > (end,n1 \n' ),then commit(p) will keep both speculations. Given the thread-local semantics, we could next de.ne \nthe tran\u00adsition (C,CS) \"-.t ( C R C, S), which describes the behavior of thread t with interference R \nfrom the environment. Semantics preservation by the instrumentation. It is easy to see that the newly \nintroduced auxiliary commands do not change the physical state s, nor do they affect the program control \n.ow. Thus the instrumentation does not change program behaviors, unless the auxiliary commands are inserted \ninto the wrong places and they get stuck, but this can be prevented by our program logic. Soundness w.r.t. \npartial correctness. Following LRG [8], we could give semantics of the logic judgment as R, G, I |= t \n{p}CC{q}, which encodes partial correctness of CCw.r.t. the pre-and post\u00ad (C, s) --t (C ' ,s ' ) C = \nE[ return ] (C, (s, .)) '-. t (C ' , (s ' , .)) .U. (U, )..=. U(t)=(end, [E]s) (E[ return E ], (s, .)) \n'-.t (skip, (s, .)) . .t . ' (E[ linself ], (s, .)) '-. t (E[ skip ], (s, . ' )) [E]s = t ' . .t1 . \n' (E[ trylin(E)], (s, .)) '-.t (E[ skip ], (s, ... ' )) SpecExact(p) (s, .)|p =( , . ' ) (E[ commit(p)], \n(s, .)) '-. t (E[ skip ], (s, . ' )) ( cC, S) '-. t ( cC ' , S ' ) (S, S ' ) |= R ( cC, S) R '-. t ( \ncC ' , S ' ) ( cC, S) R '-. t ( cC, S ' ) Auxiliary De.nitions: U(t)=(., n) .(n)(.)=(n ' ,. ' ) U(t)=(end,n) \n(U, .) --+t (U{t (end,n ' )},. ' ) (U, .) --+t (U, .) (U, .) --+t (U ' ,. ' ) . .t . ' \u00d8.t \u00d8 {(U, .)}1 \n. .t {(U ' ,. ' )}. . ' (s, .)|p =(s ' , . ' ) iff .s '' ,. '' ,.p. (s = s ' 1s '' ) . (. = . ' 1. '' \n) . ((s ' ,.p) |= p) . (. ' |dom(.p) =.p) . (. '' |dom(.p) n .p = \u00d8) def .|D = {(U,.) | dom({(U,.)})=D..U \n' ,. ' . (U 1U ' ,.1. ' )..} def dom(.) =(dom(U), dom(.)) where (U, .) . . Figure 11. Operational Semantics \nin the Relational State Model conditions. We could prove the logic ensures partial correctness by showing \nR, G, I ht {p}CC{q} implies R, G, I |= t {p}CC{q}.The details are shown in TR [18]. In the next section, \nwe give a stronger soundness of the logic, i.e. soundness w.r.t. linearizability. 5. Soundness via Simulation \nOur logic intuitively relates the concrete object code with its ab\u00adstract level speci.cation. In this \nsection we formalize the intuition and prove that the logic indeed ensures object linearizability. The \nproof is constructed in the following steps. We propose a new rely\u00adguarantee-based forward-backward simulation \nbetween the con\u00adcrete code and the abstract operation. We prove the simulation is compositional and implies \ncontextual re.nement between the two sides, and our logic indeed establishes such a simulation. Thus \nthe logic establishes contextual re.nement. Finally we get linearizabil\u00adity following Theorem 4. Below \nwe .rst de.ne a rely-guarantee-based forward-backward simulation. It extends RGSim [19] with the support \nof the helping mechanism and speculations. De.nition 5 (Simulation for Method). (x, C ) :t . iff R;G;p \n.n, s, .. (s, .) |=(t > (., n) * (x = n) * p) =. (C; noret,s) :Rt ;G;p . . Whenever (C, s) :Rt ;G;p ., \nwe have the following: 1. if C ],then = E[ return (a) for any C ' and s ',if (C, s) -_t (C ' ,s ' ), \nthen there exists . ' such that . . . ' , ((s, .), (s ' , . ' )) |=(G * True) and (C ' ,s ' ) :t . ' \n; R;G;p  (b) (C, s) -_ t abort; 2. for any s ' and . ',if ((s, .), (s ' , . ' )) |=(R * Id), then (C, \ns ' ) :Rt ;G;p . ' ;  3. if C = E[ return E ], then there exists n ' such that [E]s = n ' and (s, .) \n|=(t > (end,n ' ) * (x = ) * p).  As in RGSim, (x, C ) :Rt ;G;p . says, the implementation C is simulated \nby the abstract operation . under the interference with the environment, which is speci.ed by R and G. \nThe new simulation holds if the executions of the concrete code C are related to the speculative executions \nof some ..The . could specify abstract operations of other threads that might be helped, as well as the \ncurrent thread t. Initially, the abstract operation of t is ., with the same argument n as the concrete \nside (i.e., x = n). The abstract operations of other threads can be known from the precondition p. For \neach step of the concrete code C, we require it to be safe, and correspond to some steps of ., as shown \nin the .rst condition in De.nition 5. We de.ne the transition . . . ' as follows. ' . . . ' iff .U ,. \n' . (U ' ,. ' ) . . ' =..U, .. (U, .) . . . (U, .) --+* (U ' ,. ' ) , def ' where (U, .) --+ (U ,. ' \n) = .t. (U, .) --+t (U ' ,. ' ) and (U, .) --+t (U ' ,. ' ) has been de.ned in Fig. 11. It says, any \n(U ' ,. ' ) pair in . ' should be reachable from .. Speci.cally, we could execute the abstract operation \nof some thread t ' (which could be the current thread t or some others), or drop some (U, .) pair in \n.. The former is like a step of trylin(t ' ) or lin(t ' ), depending on whether or not we keep the original \nabstract operation of t '. The latter can be viewed as a commit step, in which we discard the wrong speculations. \nWe also require the related steps at the two levels to satisfy the guarantee G * True, G for the shared \npart and True (arbitrary transitions) for the local part. Symmetrically, the second condition in De.nition \n5 says, the simulation should be preserved under the environment interference R * Id, R for the shared \npart and Id (identity transitions) for the local part. Finally, when the method returns (the last condition \nin De.\u00adnition 5), we require the current thread t has .nished its abstract operation, and the return \nvalues match at the two levels. Like RGSim, our new simulation is compositional, thus can ensure contextual \nre.nement between the implementation and the abstract operation, as shown in the following lemma. Lemma \n6 (Simulation Implies Contextual Re.nement). For any ., G and ., if there exist R, G, p and I such that \nthe following hold for all t, 1. for any f such that .(f)=(x, C ),wehave .(f) :t G(f), Rt;Gt ;pt and \nx . dom(I); e 2. Rt = Gt1 , It {Rt,Gt}, pt . I,and Sta(pt,Rt); t1 =t 3. L.J.t pt; then . . G. Here x \n. dom(I) means the formal argument x is always in the local state, and L.J lifts . to a state assertion: \nl.J def = {(s, {(\u00d8,.)}) | .(s)= .}. Lemma 6 allows us to prove contextual re.nement . . G by showing \nthe simulation .(f ) :t G(f) for each method f, Rt;Gt;pt where R, G and p are de.ned over the shared \nstates fenced by the e invariant I, and the interference constraint Rt = t1=t Gt1 holds following Rely-Guarantee \nreasoning [17]. Its proof is similar to the compositionality proofs of RGSim [19], but now we need to \nbe careful with the helping between threads and the speculations. We give the proofs in TR [18]. Objects \nHelping Fut. LP Java Pkg HS Book v Treiber stack [29] v v HSY stack [14] v MS two-lock queue [23] v v \nv MS lock-free queue [23] v DGLM queue [6] v Lock-coupling list v Optimistic list [15] v v v Heller et \nal. lazy list [13] v v v v Harris-Michael lock-free list v Pair snapshot [27] v v CCAS [31] v v RDCSS \n[12] Table 1. Veri.ed Algorithms Using Our Logic Lemma 7 (Logic Ensures Simulation for Method). For \nany t, x, C, ., R, G and p, if there exist I and CCsuch that R, G, I ht {t > (., x) * p} Cc{t > (end, \n) * (x = ) * p} , and Er(CC)=(C; noret),then (x, C ) :t .. R;G;p Hereweuse Er(CC) to erase the instrumented \ncommands in CC.The lemma shows that, verifying CCin our logic establishes simulation between the original \ncode and the abstract operation. It is proved by .rst showing that our logic ensures the standard rely-guarantee\u00adstyle \npartial correctness (see Sec. 4.4). Then we build the simula\u00adtion by projecting the instrumented semantics \n(Fig. 11) to the con\u00adcrete semantics of C (Fig. 5) and the speculative steps . of .. Finally, from Lemmas \n6 and 7, we get the soundness theorem of our logic, which says our logic can verify linearizability. \nTheorem 8 (Logic Soundness). For any ., G and ., if there exist R, G, p and I such that the following \nhold for all t, 1. for any f,if .(f)=(x, C ), there exists CCsuch that Rt,Gt,I ht {t>(G(f),x) * pt} Cc{t>(end, \n) * (x= ) * pt} , Er(CC)=(C; noret),and x . dom(I); e 2. Rt = =t pt . I,and Sta(pt,Rt); t1 Gt1 , 3. \nL.J.t pt; then . . G, and thus . :. G. 6. Examples Our logic gives us an effective approach to verify \nlinearizability. As shown in Table 1, we have veri.ed 12 algorithms, including two stacks, three queues, \nfour lists and three algorithms on atomic memory reads or writes. Table 1 summarizes their features, \ninclud\u00ading the helping mechanism (Helping) and future-dependent LPs (Fut. LP). Some of them are used \nin the java.util.concurrent package (Java Pkg). The last column (HS Book) shows whether it occurs in \nHerlihy and Shavit s classic textbook on concurrent al\u00adgorithms [15]. We have almost covered all the \n.ne-grained stacks, queues and lists in the book. We can see that our logic supports various objects \nranging from simple ones with static LPs to sophis\u00adticated ones with non-.xed LPs. Although many of the \nexamples can be veri.ed using other approaches, we provide the .rst pro\u00adgram logic which is proved sound \nand useful enough to verify all of these algorithms. Their complete proofs are given in TR [18]. In general \nwe verify linearizability in the following steps. First we instrument the code with the auxiliary commands \nsuch as linself, trylin(E) and commit(p) at proper program points. The instrumentation should not be \ndif.cult based on the intuition of the algorithm. Then, we specify the assertions (as in Theorem 8) \n readPair(int i, j) { local a, b, v, w; {I * (cid > (., (i, j)))} 1 while(true) { {I * (cid > (., (i, \nj)) . true)} 2 < a := m[i].d; v := m[i].v; > ' {.v . (I . readCell(i, a, v; v ' )) * (cid > (., (i, j)) \n. true)} 3 < b := m[j].d; w := m[j].v; trylinself; > ' {.v . (I . readCell(i, a, v; v ' ) . readCell(j, \nb, w; )) * afterTry} 4 if (v = m[i].v) { {I * (cid > (end, (a, b)) . true)} 5 commit(cid > (end, (a, \nb))); {I * (cid > (end, (a, b)))} 6 return (a, b); {I * (cid > (end, (a, b)))} 7 }}} Auxiliary de.nitions: \ndef ' readCell(i, d, v; v ' ) =(cell(i, d, v) . (cell(i, ,v ) . v = v ' )) * true def ' ' absRes =(cid \n>(end, (a, b)) . v =v).(cid >(end, ( , b)) . v =v) def afterTry = cid > (., (i, j)) . absRes . true Figure \n12. Proof Outline of readPair in Pair Snapshot and reason about the instrumented code by applying our \ninference rules, just like the usual partial correctness veri.cation in LRG. In our experience, handling \nthe auxiliary commands usually would not introduce much dif.culty over the plain veri.cation with LRG. \nBelow we sketch the proofs of three representative examples: the pair snapshot, MS lock-free queue and \nthe CCAS algorithm. 6.1 Pair Snapshot As discussed in Sec. 2.3, the pair snapshot algorithm has a future\u00addependent \nLP. In Fig. 12, we show the proof of readPair for the current thread cid. We will use . for its abstract \noperation, which atomically reads the cells i and j at the abstract level. First, we insert trylinself \nand commit as highlighted in Fig. 12. The commit command says, when the validation at line 4 succeeds, \nwe must have cid > (end, (a, b)) as a possible speculation. This actually requires a correct instrumentation \nof trylinself.InFig.12, we insert it at line 3. It cannot be moved to other program points since line \n3 is the only place where we could get the abstract return value (a, b) when executing .. Besides, we \ncannot replace it by a linself, because if line 4 fails later, we have to restart to do the original \nabstract operation. After the instrumentation, we can de.ne the precise invariant I, the rely R and the \nguarantee G. The invariant I simply maps every memory cell (d, v) at the concrete level to a cell with \ndata d at the abstract level, as shown below: def I = .i.[1..size].(.d, v. cell(i, d, v)) def where cell(i, \nd, v) =(m[i] .. (d, v)) * (m[i] . d)) Every thread guarantees that when writing a cell, it increases \nthe version number. Here we use [G]I short for (G . Id) * Id . (I P I). def def G =[Write]I Write = .i, \nv. cell(i, ,v) P cell(i, ,v +1) The rely R is the same as the guarantee G. Then we specify the pre-and \npost-conditions, and reason about the instrumented code using our inference rules. The proof follows \nthe intuition of the algorithm. Note that we relax cid > (., (i, j)) in the precondition of the method \nto cid > (., (i, j)) . true to ensure the loop invariant. The latter says, cid may just start (or restart) \nits operation and have not done yet. The readPair method in the pair snapshot algorithm is read\u00adonly \nin the sense that the abstract operation does not update the ab\u00adstract object. This perhaps means that \nit does not matter to linearize the method multiple times. In Sec. 6.3 we will verify an algorithm with \nfuture-dependent LPs, CCAS, which is not read-only . We can still linearize a method with side effects \nmultiple times. 1 enq(v) { 16 deq() { 2 local x, t, s, b; 17 local h, t, s, v, b; 3 x := cons(v, null); \n18 while (true) { 4 while (true) { 19 h := Head; t := Tail; 5 t := Tail; s := t.next; 20 s := h.next; \n6 if (t = Tail) { 21 if (h = Head) 7 if (s = null) { 22 if (h = t) { 8 b:=cas(&#38;(t.next),s,x); 23 \nif (s = null) 9 if (b) { 24 return EMPTY; 10 cas(&#38;Tail, t, x); 25 cas(&#38;Tail, t, s); 11 return; \n} 26 }else { 12 }else cas(&#38;Tail, t, s); 27 v := s.val; 13 } 28 b:=cas(&#38;Head,h,s); 14 } 29 if(b) \nreturn v; } 15 } 30 } } Figure 13. MS Lock-Free Queue Code  6.2 MS Lock-Free Queue The widely-used MS \nlock-free queue [23] also has future-dependent LPs. We show its code in Fig. 13. The queue is implemented \nas a linked list with Head and Tail pointers. Head always points to the .rst node (a sentinel) in the \nlist, and Tail points to either the last or second to last node. The enq method appends a new node at \nthe tail of the list and advances Tail,and deq replaces the sentinel node by its next node and returns \nthe value in the new sentinel. If the list contains only the sentinel node, meaning the queue is empty, \nthen deq returns EMPTY. The algorithm employs the helping mechanism for the enq method to swing the Tail \npointer when it lags behind the end of the list. A thread should .rst try to help the half-.nished enq \nby advancing Tail (lines 12 and 25 in Fig. 13) before doing its own operation. But this helping mechanism \nwould not affect the LP of enq which is statically located at line 8 when the cas succeeds, since the \nnew node already becomes visible in the queue after being appended to the list, and updating Tail will \nnot affect the abstract queue. We simply instrument line 8 as follows to verify enq: < b := cas(&#38;(t.next), \ns, x); if > On the other hand, the original queue algorithm [23] checks Head or Tail (line 6 or 21 in \nFig. 13) to make sure that its value has not been changed since its local copy was read (at line 5 or \n19), and if it fails, the operation will restart. This check can improve ef.ciency of the algorithm, \nbut it makes the LP of the deq method for the empty queue case depend on future executions. That LP should \nbe at line 20, if the method returns EMPTY at the end of the same iteration. The intuition is, when we \nread null from h.next at line 20 (indicating the abstract queue must be empty there), we do not know \nhow the iteration would terminate at that time. If the later check over Head at line 21 fails, the operation \nwould restart and line 20 may not be the LP. We can use our try\u00adcommit instrumentation to handle this \nfuture-dependent LP. We insert trylinself at line 20, as follows: < s := h.next; if > Before the method \nreturns EMPTY, we commit to the .nished ab\u00adstract operation, i.e.,weinsert commit(cid > (end, EMPTY)) \njust before line 24. Also, when we know we have to do another itera\u00adtion, we can commit to the original \nDEQ operation, i.e.,weinsert commit(cid > DEQ) at the end of the loop body. For the case of nonempty \nqueues, the LP of the deq method is statically at line 28 when the cas succeeds. Thus we can instrument \nlinself there, as shown below. < b := cas(&#38;Head, h, s); if > After the instrumentation, we can de.ne \nI, R and G and verify the code using our logic rules. The invariant I relates all the nodes  1 CCAS(o, \nn) { 11 Complete(d) { 2 local r, d; 12 local b; 3 d := cons(cid, o, n); 13 b := flag; 4 r := cas(&#38;a, \no, d); 14 if (b) 5 while(IsDesc(r)) { 15 cas(&#38;a, d, d.n); 6 Complete(r); 16 else 7 r := cas(&#38;a, \no, d); 17 cas(&#38;a, d, d.o); 8} 18 } 9 if(r = o) Complete(d); 19 SetFlag(b){ flag := b;} 10 return \nr; } Figure 14. CCAS Code in the concrete linked list to the abstract queue. R and G specify the related \ntransitions at both levels, which simply include all the actions over the shared states in the algorithm. \nThe proof is similar to the partial correctness proof using LRG, except that we have to specify the abstract \nobjects and operations in assertions and reason about the instrumented code. We show the full proof in \nTR [18].  6.3 Conditional CAS Conditional compare-and-swap (CCAS) [31] is a simpli.ed version of the \nRDCSS algorithm [12]. It involves both the helping mecha\u00adnism and future-dependent LPs. We show its code \nin Fig. 14. The object contains an integer variable a, and a boolean bit flag. The method SetFlag (line \n19) sets the bit directly. The method CCAS takes two arguments: an expected current value o of the variable \na and a new value n. It atomically updates a with the new value if flag is true and a indeed has the \nvalue o; and does nothing otherwise. CCAS always returns the old value of a. The implementation in Fig. \n14 uses a variant of cas: instead of a boolean value indicating whether it succeeds, cas(&#38;a,o,n) \nreturns the old value stored in a.Whenstartinga CCAS, a thread .rst allocates its descriptor (line 3), \nwhich contains the thread id and the arguments for CCAS. It then tries to put its descriptor in a (line \n4). If successful (line 9), it calls the auxiliary Complete function, which restores a to the new value \nn (line 15)ortothe original value o (line 17), depending on whether flag is true. If it .nds a contains \na descriptor (i.e., IsDesc holds), it will try to help complete the operation in the descriptor (line \n6) before doing its own. Since we disallow nested function calls to simplify the language, the auxiliary \nComplete function should be viewed as a macro. The LPs of the algorithm are at lines 4, 7 and 13.If a \ncontains a different value from o at lines 4 and 7, then CCAS fails and they are LPs of the current thread. \nWe can instrument these lines as follows: <r := cas(&#38;a, o, d); if(r!=o If the descriptor d gets \nplaced in a, then the LP should be in the Complete function. Since any thread can call Complete to help \nthe operation, the LP should be at line 13 of the thread which will succeed at line 15 or 17. It is a \nfuture-dependent LP which may be in other threads code. We instrument line 13 using trylin(d.id) to speculatively \nexecute the abstract operation for the thread in d, which may not be the current thread. That is, line \n13 becomes: < b := flag; if (a = d) trylin(d.id); > The condition a=d requires that the abstract operation \nin the de\u00adscriptor has not been .nished. Then at lines 15 and 17, we commit the correct guess. We show \nthe instrumentation at line 15 below (where s is a local variable), and line 17 is instrumented similarly. \n< s := cas(&#38;a, d, d.n); if(s = d) commit(d.id >(end, d.o) * a.d.n); > That is, it should be possible \nthat the thread in d has .nished the operation, and the current abstract a is the new value n. Then we \ncan de.ne I, R and G, and verify the code by applying the inference rules. The invariant I says, the \nshared state includes flag and a at the abstract and the concrete levels; and when a is a descriptor \nd, the descriptor and the abstract operation of the thread d.id are also shared. The rely R and the guarantee \nG should include the action over the shared states at each line. The action at line 4 (or 7)is interesting. \nIf it succeeds, both the descriptor d and the abstract operation will be transferred from the local state \nto the shared part. This puts the abstract operation in the pending thread pool and enables other threads \nto help execute it. The action at line 13 guarantees TrylinSucc . TrylinFail,which demonstrates the use \nof our logic for both helping and speculation. def TrylinSucc =(.t, o, n. (flag . true * notDone(t, o, \nn)) P (flag . true * endSucc(t, o, n))) . Id def where notDone(t, o, n) = t > (CCAS,o,n) * a . o def \nendSucc(t, o, n) = t > (end,o) * a . n TrylinFail is symmetric for the case when flag . false. Here we \nuse R . Id (de.ned in Fig. 8) to describe the action of trylin. It means, after the action we will keep \nthe original state as well as the new state resulting from R as possible speculations. Also, in TrylinSucc \nand TrylinFail, the current thread is allowed to help execute the abstract CCAS of some thread t. The \nsubtle part in the proof is to ensure that, no thread could cheat by imagining another thread s help. \nIn any program point of CCAS, the environment may have done trylin and helped the operation. But whether \nthe environment has helped it or not, the commit at line 15 or 17 cannot fail. This means, we should \nnot confuse the two kinds of nondeterminism caused by speculation and by environment interference. The \nformer allows us to discard wrong guesses, while for the latter, we should consider all possible environments \n(including none). 7. Related Work and Conclusion In addition to the work mentioned in Sec. 1 and 2, there \nis a large body of work on linearizability veri.cation. Here we only discuss the most closely related \nwork that can handle non-.xed LPs. Our logic is similar to Vafeiadis extension of RGSep to prove linearizability \n[32]. He also uses abstract objects and abstract atomic operations as auxiliary variables and code. There \nare two key differences between the logics. First he uses prophecy variables to handle future-dependent \nLPs, but there has been no satisfactory semantics given for prophecy variables so far. We use the simple \ntry-commit mechanism, whose semantics is straightforward. Sec\u00adond the soundness of his logic w.r.t. linearizability \nis not speci.ed and proved. We address this problem by de.ning a new thread\u00adlocal simulation as the meta-theory \nof our logic. As we explained in Sec. 2, de.ning such a simulation to support non-.xed LPs is one of \nthe most challenging issues we have to solve. Although recently Vafeiadis develops an automatic veri.cation \ntool [33] with formal soundness for linearizability, his new work can handle non-.xed LPs for read-only \nmethods only, and cannot verify algorithms like HSY stack, CCAS and RDCSS in our paper. Recently, Turon \net al. [31] propose logical relations to ver\u00adify .ne-grained concurrency, which establish contextual \nre.nement between the library and the speci.cation. Underlying the model a similar simulation is de.ned. \nOur pending thread pool is proposed concurrently with their spec thread pool , while the speculation \nidea in our simulation is borrowed from their work, which traces back to forward-backward simulation \n[21]. What is new here is a new program logic and the way we instrument code to do re\u00adlational reasoning. \nThe set of syntactic rules, including the try\u00adcommit mechanism to handle uncertainty, is much easier \nto use than the semantic logical relations to construct proofs. On the other hand, they support higher-order \nfeatures, recursive types and poly\u00admorphism, while we focus on concurrency reasoning and use only a simple \n.rst-order language.  O Hearn et al. [25] prove linearizability of an optimistic variant of the lazy \nset algorithm by identifying the Hindsight property of the algorithm. Their Hindsight Lemma provides \na non-constructive evidence for linearizability. Although Hindsight can capture the insights of the set \nalgorithm, it remains an open problem whether the Hindsight-like lemmas exist for other concurrent algorithms. \nColvin et al. [3] formally verify the lazy set algorithm using a combination of forward and backward \nsimulations between au\u00adtomata. Their simulations are not thread-local, where they need to know the program \ncounters of all threads. Besides, their simula\u00adtions are speci.cally constructed for the lazy set only, \nwhile ours is more general in that it can be satis.ed by various algorithms. The simulations de.ned by \nDerrick et al. [4] are thread-local and general, but they require the operations with non-.xed LPs to \nbe read-only, thus cannot handle the CCAS example. They also pro\u00adpose a backward simulation to verify \nlinearizability [28]. Although the method is proved to be complete, it does not support thread\u00adlocal \nveri.cation and there is no program logic given. Elmas et al. [7] prove linearizability by incrementally \nrewriting the .ne-grained code to the atomic operation. They do not need to locate LPs. Their rules are \nbased on left/right movers and program re.nements, but not for Hoare-style reasoning as in our work. \nThere are also lots of model-checking based tools (e.g., [20, 34]) for checking linearizability. For \nexample, Vechev et al. [34] check linearizability with user-speci.ed non-.xed LPs. Their method is not \nthread modular. To handle non-.xed LPs, they need users to instrument the code with enough information \nabout the actions of other threads, which usually demands a priori knowledge about the number of threads \nrunning in parallel, as shown in their example. Besides, although their checker can detect un-linearizable \ncode, it will not terminate for linearizable methods in general. Conclusion. We propose a new program \nlogic to verify lin\u00adearizability of algorithms with non-.xed LPs. The logic extends LRG [8] with new \nrules for the auxiliary commands introduced speci.cally for linearizability proof. We also give a relational \nin\u00adterpretation of asssertions and rely/guarantee conditions to relate concrete implementations with \nthe corresponding abstract opera\u00adtions. Underlying the logic is a new thread-local simulation, which \ngives us contextual re.nement. Linearizability is derived based on its equivalence to re.nement. Both \nthe logic and the simulation sup\u00adport reasoning about the helping mechanism and future-dependent LPs. \nAs shown in Table 1, we have applied the logic to verify various classic algorithms. Acknowledgments \nWe would like to thank Matthew Parkinson, Zhong Shao, Jan Hoff\u00admann and anonymous referees for their \nsuggestions and comments on earlier versions of this paper. This work is supported in part by grants \nfrom National Natural Science Foundation of China (NSFC) under Grant No. 61073040 and 61229201, the National \nHi-Tech Research and Development Program of China (Grant No. 2012AA010901), and Program for New Century \nExcellent Talents in Universities (Grant No. NCET-2010-0984). Part of the work is done during Hongjin \nLiang s visit to Yale University in 2012-2013, which is supported by China Scholarship Council. References \n[1] M. Abadi and L. Lamport. The existence of re.nement mappings. Theor. Comput. Sci., 82(2):253 284, \n1991. [2] D. Amit, N. Rinetzky, T. Reps, M. Sagiv, and E. Yahav. Comparison under abstraction for verifying \nlinearizability. In CAV 07. [3] R. Colvin, L. Groves, V. Luchangco, and M. Moir. Formal veri.cation of \na lazy concurrent list-based set algorithm. In CAV 06. [4] J. Derrick, G. Schellhorn, and H. Wehrheim. \nVerifying linearisability with potential linearisation points. In FM 11. [5] J. Derrick, G. Schellhorn, \nand H. Wehrheim. Mechanically veri.ed proof obligations for linearizability. ACM TOPLAS, 33(1):4, 2011. \n[6] S. Doherty, L. Groves, V. Luchangco, and M. Moir. Formal veri.ca\u00adtion of a practical lock-free queue \nalgorithm. In FORTE 04. [7] T. Elmas, S. Qadeer, A. Sezgin, O. Subasi, and S. Tasiran. Simplifying linearizability \nproofs with reduction and abstraction. In TACAS 10. [8] X. Feng. Local rely-guarantee reasoning. In POPL \n09. [9] I. Filipovi \u00b4 Abstraction for c, P. O Hearn, N. Rinetzky, and H. Yang. concurrent objects. Theor. \nComput. Sci., 2010. [10] A. Gotsman and H. Yang. Linearizability with ownership transfer. In CONCUR \n12. [11] T. L. Harris. A pragmatic implementation of non-blocking linked-lists. In DISC 01. [12] T. L. \nHarris, K. Fraser, and I. A. Pratt. A practical multi-word compare-and-swap operation. In DISC 02. [13] \nS. Heller, M. Herlihy, V. Luchangco, M. Moir, W. N. S. III, and N. Shavit. A lazy concurrent list-based \nset algorithm. In OPODIS 05.  [14] D. Hendler, N. Shavit, and L. Yerushalmi. A scalable lock-free stack \nalgorithm. In SPAA 04. [15] M. Herlihy and N. Shavit. The Art of Multiprocessor Programming. Morgan Kaufmann, \nApr. 2008. [16] M. Herlihy and J. Wing. Linearizability: a correctness condition for concurrent objects. \nACM TOPLAS, 12(3):463 492, 1990. [17] C. B. Jones. Tentative steps toward a development method for inter\u00adfering \nprograms. ACM TOPLAS, 5(4):596 619, 1983. [18] H. Liang and X. Feng. Modular veri.cation of linearizability \nwith non-.xed linearization points. Technical report, USTC, March 2013. http://kyhcs.ustcsz.edu.cn/relconcur/lin. \n[19] H. Liang, X. Feng, and M. Fu. A rely-guarantee-based simulation for verifying concurrent program \ntransformations. In POPL 12. [20] Y. Liu, W. Chen, Y. A. Liu, and J. Sun. Model checking linearizability \nvia re.nement. In FM 09. [21] N. A. Lynch and F. W. Vaandrager. Forward and backward simula\u00adtions: I. \nuntimed systems. Inf. Comput., 121(2):214 233, 1995. [22] M. M. Michael. High performance dynamic lock-free \nhash tables and list-based sets. In SPAA 02. [23] M. M. Michael and M. L. Scott. Simple, fast, and practical \nnon\u00adblocking and blocking concurrent queue algorithms. In PODC 96. [24] P. W. O Hearn. Resources, concurrency, \nand local reasoning. Theor. Comput. Sci., 375(1-3):271 307, 2007. [25] P. W. O Hearn, N. Rinetzky, M. \nT. Vechev, E. Yahav, and G. Yorsh. Verifying linearizability with hindsight. In PODC 10,. [26] P. W. \nO Hearn, H. Yang, and J. C. Reynolds. Separation and informa\u00adtion hiding. In POPL 04,. [27] S. Qadeer, \nA. Sezgin, and S. Tasiran. Back and forth: Prophecy variables for static veri.cation of concurrent programs. \nTech Report. [28] G. Schellhorn, H. Wehrheim, and J. Derrick. How to prove algorithms linearisable. In \nCAV 12. [29] R. K. Treiber. System programming: coping with parallelism. Tech\u00adnical Report RJ 5118, IBM \nAlmaden Research Center, 1986. [30] A. Turon and M. Wand. A separation logic for re.ning concurrent objects. \nIn POPL 11. [31] A. Turon, J. Thamsborg, A. Ahmed, L. Birkedal, and D. Dreyer. Logical relations for \n.ne-grained concurrency. In POPL 13. [32] V. Vafeiadis. Modular .ne-grained concurrency veri.cation. \nThesis. [33] V. Vafeiadis. Automatically proving linearizability. In CAV, 2010. [34] M. T. Vechev, E. \nYahav, and G. Yorsh. Experience with model check\u00ading linearizability. In SPIN 09.    \n\t\t\t", "proc_id": "2491956", "abstract": "<p>Locating linearization points (LPs) is an intuitive approach for proving linearizability, but it is difficult to apply the idea in Hoare-style logic for formal program verification, especially for verifying algorithms whose LPs cannot be statically located in the code. In this paper, we propose a program logic with a lightweight instrumentation mechanism which can verify algorithms with non-fixed LPs, including the most challenging ones that use the helping mechanism to achieve lock-freedom (as in HSY elimination-based stack), or have LPs depending on unpredictable future executions (as in the lazy set algorithm), or involve both features. We also develop a thread-local simulation as the meta-theory of our logic, and show it implies contextual refinement, which is equivalent to linearizability. Using our logic we have successfully verified various classic algorithms, some of which are used in the java.util.concurrent package.</p>", "authors": [{"name": "Hongjin Liang", "author_profile_id": "81496643592", "affiliation": "University of Science and Technology of China, Hefei, China", "person_id": "P4149091", "email_address": "lhj1018@mail.ustc.edu.cn", "orcid_id": ""}, {"name": "Xinyu Feng", "author_profile_id": "81539835456", "affiliation": "University of Science and Technology of China, Hefei, China", "person_id": "P4149092", "email_address": "xyfeng@ustc.edu.cn", "orcid_id": ""}], "doi_number": "10.1145/2491956.2462189", "year": "2013", "article_id": "2462189", "conference": "PLDI", "title": "Modular verification of linearizability with non-fixed linearization points", "url": "http://dl.acm.org/citation.cfm?id=2462189"}