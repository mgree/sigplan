{"article_publication_date": "06-16-2013", "fulltext": "\n Using Managed Runtime Systems to Tolerate Holes in Wearable Memories Tiejun Gao Karin Strauss Stephen \nM. Blackburn Australian National University {tiejun.gao,steve.blackburn}@anu.edu.au Abstract New memory \ntechnologies, such as phase-change memory (PCM), promise denser and cheaper main memory, and are expected \nto displace DRAM. However, many of them experience permanent failures far more quickly than DRAM. DRAM \nmechanisms that handle permanent failures rely on very low failure rates and, if directly applied to \nPCM, are extremely inef.cient: Discarding a page when the .rst line fails wastes 98% of the memory. This \npaper proposes low complexity cooperative software and hardware that handle failure rates as high as \n50%. Our approach makes error handling transparent to the application by using the memory abstraction \noffered by managed languages. Once hard\u00adware error correction for a memory line is exhausted, rather \nthan discarding the entire page, the hardware communicates the failed line to a failure-aware OS and \nruntime. The runtime ensures mem\u00adory allocations never use failed lines and moves data when lines fail \nduring program execution. This paper describes minimal exten\u00adsions to an Immix mark-region garbage collector, \nwhich correctly utilizes pages with failed physical lines by skipping over failures. This paper also \nproposes hardware support that clusters failed lines at one end of a memory region to reduce fragmentation \nand im\u00adprove performance under failures. Contrary to accepted hardware wisdom that advocates for wear-leveling, \nwe show that with soft\u00adware support non-uniform failures delay the impact of memory fail\u00adure. Together, \nthese mechanisms incur no performance overhead when there are no failures and at failure levels of 10% \nto 50% suf\u00adfer only an average overhead of 4% and 12%, respectively. These results indicate that hardware \nand software cooperation can greatly extend the life of wearable memories. Categories and Subject Descriptors \nD. Software [D.3 Programming Languages]: D.3.4 Processors, Memory management (garbage collection); B. \nHardware [B.8 Performance and Reliability]: B.8.1 Reliability, Testing, and Fault-Tolerance General Terms \nReliability Keywords Failure tolerance, memory management, phase-change memory 1. Introduction The semiconductor \nindustry continues to exploit Moore s Law by scaling down lithographic feature sizes to improve chip \ndensity. However, due to increased process variation and smaller features, scaling charge-based memories \nsuch as DRAM is becoming in- Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are not made or distributed for \npro.t or commercial advantage and that copies bear this notice and the full citation on the .rst page. \nTo copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c \npermission and/or a fee. PLDI 13 June 16 19, 2013, Seattle, WA, USA. Copyright c &#38;#169; 2013 ACM \n978-1-4503-2014-6/13/06. . . $15.00 Kathryn S. McKinley Doug Burger James Larus Microsoft Research {kstrauss,mckinley,dburger,larus}@microsoft.com \n creasingly dif.cult [12]. Errors increase as smaller features rep\u00adresent a bit with less charge. Process \nvariation makes scaling even more challenging because different memory cells operate with a wider range \nof charges. Memory manufacturers are delivering alternatives to charge\u00adbased memories, such as resistive \nand magneto-resistive memories. Among these, memristors are under advanced development and phase-change \nmemory (PCM) is in production [15]. PCM s storage principle is based on physical atomic arrangements \nof the materials that constitute it, rather than charges it holds, resulting in more stable storage at \nsmaller feature sizes. The result is better scaling for main memory storage, which will keep improving \nmemory density and cost in smaller feature sizes. Moreover, PCM is non\u00advolatile, which presents opportunities \nfor new software designs [7] and lower power operation. However, PCM comes with its own idiosyncrasies. \nStable mem\u00adory cells require more energy to write and fail much quicker com\u00adpared to DRAM, as writes \nchange the material s physical con.gu\u00adration and cause it to wear out. We call memories vulnerable to \nthis issue wearable memories. This paper frames our solution as target\u00ading PCM, but it is applicable \nto any main memory technology that suffers failures. Current DRAM approaches assume failures occur extremely \nrarely. When one line in DRAM fails, software remaps its virtual page to another working physical page \nand discards the entire failed physical page [9]. Even recent work on error correction hardware tailored \nto wearable memories [16, 22, 23, 25] assumes that once the .nite error correction resources are exhausted, \nthe entire page, if not the entire memory, fails. Assuming only pages fail, 4 KB pages and 64 B lines, \nthis solution wastes 98% of working memory. In other words, only 2% of lines need fail and the entire \nwearable memory becomes unusable. This paper proposes a software/hardware cooperative mecha\u00adnism to extend \nthe life of wearable memories. We propose software mechanisms that handle failures at a .ne granularity, \ne.g., a cache line, in which the OS and managed runtime extend the lifetime of wearable memory, with \ngraceful performance degradation as the number of failures (holes) increases. The .rst line of defense \nfor failures is of course the hardware error correction of the memory system. When the number of failed \nbits exceeds the capacity of the error correction hardware, the memory communicates this failure to the \nOS. The OS keeps track of failures at the level of individual lines in a page. The OS reports failed \nlines when the runtime requests memory, and it may also communicate failures that occur during program \nexecution. The failure-aware memory manager never allocates live objects on failed lines and evacuates \nlive objects from lines when they fail. We show how to minimize wasted space and performance degra\u00addation \nfor garbage-collected managed languages, such as C#, Java, JavaScript, and PHP, using the hierarchical, \nlogical line and block heap organization in the Immix mark-region garbage collector [3]. This heap organization \nre.ects hardware cache lines and pages and simpli.es failure tolerance because it manages memory in sizes \nsimilar to the failure granularity. This basic software design delivers a correct, simple failure\u00adaware \napproach for heap data and dynamically generated code with modest performance degradation for low failure \nrates. Even though hardware-unassisted software results in correct behavior, we show that memory fragmentation \ncauses signi.cant overheads when the number of failures rises above 10%. Fragmentation makes it too time \nconsuming to locate free memory for medium and large ob\u00adject allocations. Novel, low complexity failure \nclustering hardware comes to the rescue by redirecting failures within a region (one or more pages) to \na cluster at one end, greatly reducing virtual mem\u00adory fragmentation. This modest hardware support solves \nthe frag\u00admentation problem for software, increasing the number of failures that are performance-transparent. \nWe implement this approach in a Java virtual machine, but it generalizes to any managed language. Since \nPCM is not yet avail\u00adable as a DRAM replacement, we add fault injection between the OS and VM memory \nallocators and execute on current processors with DRAM. Experiments with DaCapo benchmarks [5] show that \nfailure-aware software adds no overhead in the absence of failures, and with 10% and 50% failure ranges \non average adds only 4% and 12%, respectively. We explore the sensitivity of our approach to different \nclustering granularities and software line granularities. These results show that hardware failure clustering \nbrings addi\u00adtional bene.ts because it creates larger contiguous pieces of work\u00ading memory, even entire \nworking pages, with little performance degradation. These results contradict accepted hardware wisdom \non wear leveling [17, 18, 26]. Wear leveling delays any one fail\u00adure, but once memory begins to fail, \nuniformly distributed failures cause fragmentation, whereas concentrated failures fragment mem\u00adory less \nand are more transparent to software. The capability of a managed runtime to relocate objects in\u00adcreases \nthe ability of the system to tolerate uncorrected hardware failures, independent of the underlying hardware \nerror correction mechanism. By allocating around holes in non-homogeneous mem\u00adory pages, our system increases \nthe useful lifetime of PCM mem\u00adory, making it more practical to use as main memory. 2. PCM Background \nand System Design This section explains why DRAM technology is struggling to scale to smaller technologies. \nIt then brie.y describes PCM technolo\u00adgies, approaches to extending PCM lifetimes, and how future sys\u00adtems \nmay incorporate PCM. We use the following failure termi\u00adnology. (1) Static failures occur prior to an \napplication executing. The runtime may never allocate into pre-existing failed memory. (2) Dynamic failures \noccur during an application s execution. 2.1 Scaling Problems for DRAM DRAM technology manufacturers \nare running into dif.culties scal\u00ading DRAM cells to smaller features (thus denser memory chips) because \nDRAM uses a capacitive storage principle. Each cell is a capacitor, which stores electrons. The charge \nheld by the cell is later sensed to determine the state stored in the cell. As cells get smaller, the \ncharge held in each cell drops, which increases the probability that charge escaping from the cell will \ncause a bit .ip. Emerging technologies such as PCM are much more stable than DRAM, so they became interesting \npotential replacement technologies.  2.2 PCM Hardware The basic storage principle of PCM is changing \nthe state of a chalcogenide material [19]. Electrical current heats up cells and then the material cools \ndown into an amorphous or a crystalline state that have different resistances, which encode logical zeroes \nand ones. Due to the physical nature of state changes on writes, cells tend to wear out. The average \nlifetime is currently about 108 writes per cell versus 1015 in DRAM. These changes are isolated and localized \nto individual cells, so failures have no spatial correlation. The peripheral circuitry responsible for \nperforming the write detects failures by reading the value once the write completes. Previous work advocates \nfor hardware and software wear lev\u00adeling [17, 18, 26] that uniformly distributes failures across mem\u00adory \nwith the purpose of wearing it equally and delaying failures. Hardware-only error correction replaces \nbits or entire lines on per\u00admanent failures [16, 22, 23, 25]. These papers assume that once the .nite \nerror correction resources are exhausted, the entire page, if not the entire memory, fails. This was \ninspired by how DRAM sys\u00adtems handle failures today, but it is not adequate for PCM because failures \nare not as rare as in DRAM. This paper shows that, with modest hardware and software support, only the \nfailing line needs to be disabled and removed from the visible memory space. Fur\u00adthermore, software tolerates \nfailures better when they are clustered, rather than uniformly distributed throughout memory. Our software/hardware \ncooperative approach extends system endurance to go beyond what is possible with hardware-only er\u00adror \ncorrection because it successfully uses other lines in the same page. Effectively, the system gradually \nshifts from error correction to error tolerance. An additional bene.t is that error correction re\u00adsources \npreviously used to correct the failed line can be repurposed to correct lines that are still in use when \nthey experience additional bit failures, extending the system lifetime even further. 2.3 System Design \nEven though the bulk of a system s main memory may eventually be composed of PCM, we assume in this paper \nthat the system will have some DRAM protected by regular ECC for operations that must not fail. For example, \nDRAM should store essential OS and runtime data structures. The lower the software s dependency on DRAM \nand perfect PCM, the more failure-tolerant it will be. One may wonder what incentive application writers \nhave to use PCM when DRAM is also available. The right balance between PCM and DRAM sizes will depend \non costs and goals of the system, but if PCM reaches the scale required to service the main mem\u00adory market, \nit will have to be signi.cantly cheaper than DRAM. As such, main memories are likely to use a small DRAM \nmodule and a much larger PCM module, making DRAM scarce. A process storing data in DRAM would have a \nhigher probability of having its pages swapped out into disk, which will likely degrade perfor\u00admance. \nIf the process stores data in PCM, performance improves because this module is large and its contents \nare less likely to be swapped out, if at all. 3. Cooperatively Avoiding Faults This section describes \nour proposed hardware, OS, and runtime support for handling PCM failures. 3.1 Hardware Support We propose \ntwo simple hardware mechanisms. The .rst informs the software of failures, and is the foundation for \ncooperative fail\u00adure tolerance. The other clusters failures to reduce fragmentation. 3.1.1 Hardware: \nFailure Tolerance The hardware cooperates with software by (a) informing software when a write fails, \nand (b) transparently maintaining the data from a failed write while the software adapts to the failure. \nWhen a PCM write fails, the PCM memory module: (1) copies the data and the corresponding physical address \nto a failure buffer,  Figure 1. Hardware Failure Clustering and (2) interrupts the processor. The failure \nbuffer consists of a small quantity of SRAM or DRAM that is part of the PCM mod\u00adule itself, its peripheral \nDIMM circuit, or the memory controller. Every memory read request checks the buffer for the latest value \nwritten to a memory location. This search is performed in paral\u00adlel with the actual access, so it does \nnot affect read latency. If the memory module .nds an entry in the buffer, it returns the corre\u00adsponding \ndata instead of the data read from the PCM array. The OS invalidates entries in this buffer once it .nishes \nhandling them. If other failures happen before a .rst failure is processed, information about these failures \nis stored in the failure buffer in FIFO order. The size of the buffer bounds the number of simultaneous \nfailures that can be tolerated. The physics of PCM results in failures that are not naturally clustered \nin practice and do not happen frequently, so this buffer can be small (no larger than the processors \nload/store queues, which have similar forwarding capabilities). An earlier en\u00adtry with the same address \nis invalidated. When the failure buffer is about to .ll up (enough entries are reserved to drain all \noutstanding writes to PCM), another type of interrupt is sent to the processor, and the PCM memory stops \naccepting any further write requests until the OS handles and clears at least one buffer entry. The hard\u00adware \nand OS thus prevent deadlocks and data loss.  3.1.2 Hardware: Minimizing Fragmentation As failures accumulate, \nthey increase memory fragmentation, mak\u00ading it more dif.cult to allocate data across multiple contiguous \nlines. We propose a hardware mechanism called failure clustering to mitigate this problem. Failure clustering \nlogically pushes failures to one end of a region, consisting of one or more pages. Section 6.4 quanti.es \nfragmentation due to failures and shows that reducing fragmentation greatly reduces software overhead. \nFailure clustering logically remaps failed lines to edges of re\u00adgions, as illustrated in Figure 1. Figure \n1(a) shows a region with no failures. Figure 1(b) shows that, as the region wears out, lines run out \nof error correction resources and fail. Figure 1(c) shows that ideally these lines map to the edges of \nregions, maximizing the amount of contiguous space available for object allocation. To create contiguous \nspace, we propose a simple line redirecting strat\u00adegy. Figure 1(d) shows that every time a line fails, \nthe hardware redirects it to the top or bottom of the region. Once the .rst line fails, the hardware \ninstalls a redirection map in the .rst line (or an\u00adother pre-de.ned line) of the region and a pointer \nto the boundary between lines that work and lines that do not. The redirection map has as many entries \nas lines in a region (128 by default in our experiments) and each entry requires as many bits as the \nlogarithm of this number. Each entry is indexed by the phys\u00adical address offset in the region and produces \nthe actual line offset to which that line was redirected. On each failure, the hardware maps the failed \naddress to the boundary pointer line, advances the boundary, and updates the redirection map, exchanging \nthe offsets of the two lines being swapped. It increments or decrements the boundary, depending on which \nside of the region the failures clus\u00adter. If regions are page-sized, to further maximize the amount of \ncontiguous memory, failures cluster to the top of even regions and to the bottom of odd regions, as shown \nin Figure 1 (e). Figure 1(f) illustrates regions larger than one page, in which case failures of both \npages cluster in only one of them (as long as the number of unavailable lines corresponds to less than \none page). Multiple-page clustering creates logically perfect PCM pages. Assuming a 4 KB page, 64 B lines, \nand a 2-page region size, the redirection map re\u00adquires 889 bits, or two lines, for its metadata: 126 \n7-bit .elds for redirection entries, and one 7-bit .eld for the boundary pointer. It uses the remaining \nbits for error correction. When the .rst failure happens in a page, the hardware sets up a redirection \nmap. The memory module .rst places a fake failure at the location in which it intends to install the \nredirection map in its failure buffer, before inserting the entry corresponding to the .rst failure. \nOnce the OS clears both entries from the list, the memory module installs the redirection map. The hardware \nhandles failures in the redirection map with error correcting codes. On memory operations, the memory \nmodule must identify which lines to redirect. The hardware uses a single bit in the orig\u00adinal error correction \nmetadata to indicate if the line is redirected. The memory module inserts the failure maps at .xed locations \nto ease lookup the top of even regions and bottom of odd regions. The implementation complexity of clustering \nhardware is mod\u00adest. Upon an application reference to a region with failures, the memory module accesses \nthe redirection map using the physical address offset it received from the cache hierarchy as the index. \nIt retrieves a new offset and then accesses the corresponding line. In practice, this functionality requires \nvery simple logic. However, it requires three memory accesses: (1) The .rst access .nds the line is redirected, \nand (2) the second access goes to the redirection table, and (3) the third access reads or writes the \nredirected line. Three accesses could signi.cantly increase latency. A simple solution is to cache recently \nused redirection maps. Note that in the common case of no failure, references require only one memory \naccess irre\u00adspective of whether or not caching is in use.  3.2 Operating System Support When the system \nis new, most, if not all, pages will be perfect (no failures). As PCM pages wear, their cells start to \nfail, and, at some point, hardware error correction is exhausted. Certain lines in a page become unusable, \nalthough other lines are still functional. When a line fails, the operating system receives the interrupt \nnoti.cation of the failure. The memory access may have executed in a user process or the operating system \nitself. This paper focuses on the .rst case. The OS accesses PCM, as discussed below, and must handle \nsituations in which its writes fail, but this paper does not consider storing OS data structures or code \nin PCM. Operating systems can handle the failure in two fundamentally different ways. They can hide line \nfailures from executing pro\u00adcesses by replacing failed pages with perfect pages, changing the process \ns page tables and continuing the process without interrup\u00adtion. This approach masks the failure, preserving \nthe illusion of perfect memory, at the cost of depleting the increasingly scarce re\u00adsource of perfect \npages. The other approach is to notify the process of a line failure, to allow the runtime to reorganize \ndata structures and avoid using memory locations in the failed line. The process continues using the \npage until its defect density renders it unusable. The focus of this paper is the design of failure-aware \nmanaged runtimes that may relocate data transparently to applications. 3.2.1 OS: Data Structures and \nSystem Calls The OS manages DRAM, perfect PCM, and imperfect PCM pages in separate pools. Initially, \nall PCM pages start in the perfect pool. When a page .rst becomes imperfect, the OS moves the page to \nthe imperfect page pool. The OS also tracks the failed lines in these pages. A system with 64 B lines \nand 4 KB pages requires a 64-bit bitmap per page. The OS stores these bitmaps in a table in DRAM with \nan entry corresponding to each physical PCM page in the system. Uncompressed, this table is approximately \n1.6% of the size of the PCM page pool. Run-length encoding or other simple encoding techniques may provide \nhigh compression rates and reduce this overhead, especially when the system is new and the number of \nfailures is low. When the system is shut down, the OS may save the failed line map to persistent storage \nand restore it on system initialization. Alternatively, the OS may rebuild the table by eagerly scanning \nmemory or by lazily rediscovering failures at .rst write. Rediscovery is necessary after abnormal shutdowns \nand incurs a cost proportional to the size of PCM or the number of failures not recorded in persistent \nstorage before the shutdown. Failure-unaware processes may continue allocating perfect memory via normal \nmechanisms such as mmap. A failure-aware process can utilize both perfect and imperfect memory pages. \nPer\u00adfect memory grows scarcer as a system ages. Although imperfect pages require extra management and \noverhead, they are more abun\u00addant. A failure-aware runtime uses a special variation of mmap to acquire \nimperfect pages. This call returns the number of pages re\u00adquested, however not all of the allocated memory \nmay be usable. The runtime uses a map-failures call to get a failure map of the allocated memory region. \nIf the runtime requires more space than the OS returned, it requests additional memory.  3.2.2 OS: Handling \nDynamic Failures When the memory module detects a failure, it raises an interrupt. This failure occurs \nasynchronously with respect to the write opera\u00adtion because a line is only written to PCM when it is \nevicted from the cache subsystem. When the interrupt is raised, the OS inter\u00adrupt handler reads the failure \ninformation from the failure buffer. Before removing any entries from the failure buffer, the OS must \nprevent accesses to the failing addresses because, once it removes them, the memory module no longer \nforwards data to read oper\u00adations on these addresses. The OS .nds the corresponding virtual pages via \nreverse address translation, removes read and write per\u00admissions from them, and updates its own failure \nmap. Reverse ad\u00address translation is relatively expensive, but dynamic failures are very rare and either \nrequire copying a page or a garbage collection. The cost of reverse address translation is small compared \nto both. The OS handler needs to resolve every failure. For failure\u00adunaware processes, the only option \nis to copy the entire affected page to a perfect page. It must do the same for memory regions allocated \nwhen failure-aware processes request perfect memory. For imperfect memory requests, it may use the same \napproach or rely on the runtime to handle the failure. A failure-aware runtime must register a handler \nwith the OS be\u00adfore it uses imperfect memory. When a failure occurs, the OS per\u00adforms an up-call on the \nhandler, passing the addresses and data of all pending failures. The runtime must relocate affected data, \nupdat\u00ading whatever runtime structures are necessary to ensure correctness and transparency before returning \nfrom the handler. As a convenience to the runtime, the OS may optionally recon\u00adstruct the affected page \nusing a DRAM page for the duration of the runtime handler performing its work. Otherwise, the runtime \nuses the partially-failed PCM page and the information passed to the handler to reconstruct the affected \ndata, and must avoid accessing the failed lines while it handles the failure. 3.2.3 OS: Paging and Other \nIssues The OS may occasionally copy data from one imperfect page to another, for example, when an imperfect \npage is swapped out and then brought back into memory. If the failures on the destination page are a \nsubset of the failure locations on the source page, copy\u00ading is straightforward. However, keeping track \nof page compatibil\u00adity is expensive, and prior work shows similar matching processes have limited ef.cacy \nin practice [11]. As we show below, failure clustering helps solve this problem. When moving data from \na previous imperfect page (possibly on disk) to another memory page, the OS has three options: (1) The \nOS may swap data into a perfect page. (2) The OS may swap data into an imperfect page with different \nfailures compared to the previous physical page. The OS must inform the runtime of the new failure map \nby delivering an up-call on the runtime s handler. If the runtime cannot safely move data in the failed \nlines, the OS can try another imperfect page or fall back to a perfect page. (3) Failure clustering enables \na third and simpler approach. Since clustering accumulates failures at one end of the page, the OS simply \nmaps the page it is swapping in onto any available page with the same number or fewer failures.  3.3 \nRuntime and Application Software Support We turn now to failure-aware runtimes. We start with the limitations \ninherent to native (unmanaged) applications. We then overview failure-aware runtime support for managed \napplications. Section 4 describes the basic Immix garbage collection algorithm and how we implement failure-tolerance \nin Immix. 3.3.1 Failure-Aware Applications and Runtimes In principle, any application can be made failure-aware. \nIn practice, forcing developers to manage failures without runtime support is impractical because it \nrequires signi.cant and pervasive changes to ensure live data never occupies failed memory. Static failures \nto memory allocated to the stack for both native and managed languages could be addressed by the compiler \nand runtime stepping around failed lines as they allocate new stack frames. But given the frame allocation/deallocation \nfrequency, the overhead of such a scheme is likely to be unreasonable. Handling dynamic failures to the \nstack is even more challenging. Static failures to memory allocated to the code and data seg\u00adments will \nbe hard to handle because of hard-coded assumptions about offsets to code and data in these segments. \nIn principle, this problem could be addressed via binary rewriting. However, because native programs \nmay hold untyped and undiscoverable references to elements in the code and data segments, handling dynamic \nfail\u00adures to these regions appears to be intractable. The runtime memory manager may handle static failures \nin heap memory. Native runtimes use free list allocators, which could use imperfect memory, and mark \nas unavailable those units of allo\u00adcation that coincide with failed memory. Such an implementation would \nincrease the complexity of managing the free list. However, for large objects and large numbers of failures, \nnative runtimes have a stronger requirement for perfect pages than managed runtimes. For example, managed \nlanguages can split large arrays [21] (see Section 4), whereas native languages require contiguous allocation. \nAdding failure tolerance to the free-list used by native runtimes will incur additional complexity, fragmentation, \nor both. Most of the complexity arises due to (1) mismatches between failure granu\u00adlarity and the free-list \nheap layout, and (2) the need to differentiate between failed and in-use memory both to reduce fragmentation \nand for correctness with conservative collection. Granularity mis\u00admatches arise because native memory \nmanagers use segregated size free-list allocators [2, 6, 8, 14], carving up large .xed-sized regions \n(e.g., one or more pages) into same-sized objects on demand. To add failure tolerance, we have three \nchoices: (a) to use only perfect small page size regions, which incurs fragmentation for medium size \nobjects, and never uses pages with failures, or (b) incur frag\u00admentation in each .xed-size large region, \nor (c) add complexity to allocate and recycle variable-sized regions. Note that these same is\u00adsues arise \nfor managed languages that use free-lists. None of these options are appealing. To summarize, native \napplications could be made tolerant to static failures in the heap with non-trivial changes to a free-list \nallocator, but the OS must still handle all dynamic failures.  3.3.2 Failure-Aware Managed Runtimes \nWe design and implement a solution for failure-aware memory in a managed runtime that dynamically allocates \ncode and data mem\u00adory space. A large and growing set of applications are written in managed languages, \nsuch as C#, Java, JavaScript, Python, and PHP. These languages use safe pointer disciplines and execute \nin virtual machines (VMs) that include dynamic compilers and au\u00adtomatic memory management (garbage collection). \nBecause these languages are memory safe, the garbage collector (GC) allocates and is free to move objects \nin memory (including code objects in some VMs). Moving objects is both safe and transparent to appli\u00adcations. \nWe leverage this functionality to tolerate failures in PCM without putting a burden on developers. A \nfailure-aware garbage collector never allocates objects in failed memory and relocates af\u00adfected data \nwhen new failures happen. In VMs that dynamically allocate code and data, our solution transparently \nhandles both, but we focus on data here. A VM solution reduces the need for perfect memory to the VM \nitself, heap metadata, and large objects. A self-hosting VM may reduce the need even further. The next \nsection provides an overview of how to make garbage collection failure-aware.  3.3.3 Garbage Collection \nTwo high-level modi.cations are necessary for a garbage collector to tolerate failures: (1) exposing \ninformation about failures to the garbage collector (GC), and (2) modifying the allocator to avoid allocating \nlive objects in memory that contains failures. The OS communicates a failure map to the GC. The GC adds \nthe failure map to the heap metadata that records which memory contains live objects and which memory \nis free for allocation. The hardware determines the .nest possible granularity of the failure map. This \npaper uses the cache line size as the .nest failure gran\u00adularity, which is also typically the same granularity \nof hardware write operations to PCM. The failure map may use coarser granu\u00adlarities that trade-off less \nstorage overhead for less available mem\u00adory as failures accrue. We evaluate this trade-off in Section \n6.3. For simplicity, we assume here that the failure map and other heap metadata are stored in perfect \nmemory and do not themselves fail. In a failure-aware VM, (a) the GC must never allocate live objects \nin a failed region of memory, and (b) when a region of memory fails during execution, the collector must \nevacuate all objects in the failed region and allocate them elsewhere. Static failures The GC already \nmaintains the invariant that it only allocates objects into free memory. The GC treats all allocations \nthe same regardless of whether an object allocation is the result the application s call to new(), or \nif the GC itself is allocating an object to evacuate (move) it during a collection. A failure-aware GC \nmaintains the same invariant. It only allocates into free memory. It treats failed memory the same as \nmemory with live objects on it (neither are free). With a static failure map in hand, the collector simply \nallocates into free memory. At a minimum, all memory managers separately manage (i) small or medium objects \nand (ii) large objects. Given suf.cient failures and a large enough object, imperfect memory will not \naccommo\u00addate a large object. As with any allocation, when the failure-aware memory manager .rst attempts \nto allocate an object and there is insuf.cient memory, it triggers a collection to reclaim free mem\u00adory \nand then attempts to allocate again. If the GC still cannot ac\u00adcommodate the object in imperfect memory, \nthe allocator requests perfect memory. We greatly reduce the probability of this situation with hardware \nclustering at the granularity of two or more pages to create logically perfect pages (see Large Objects \nbelow). Dynamic failures If a failure occurs during program execution, the OS may notify the GC and provide \nit with the values the pro\u00adgram intended to write into the affected region. The OS can specify dynamic \nfailures at the granularity of lines or pages. Regardless, the collector must move any affected objects, \nallocating them else\u00adwhere in the heap. This requires that all pointers to the affected object(s) be \nidenti.ed and retargeted to the moved object(s). In a typical generational collector, a failure to memory \nin the nursery will only require a nursery collection, but any other failure will re\u00adquire a full heap \ncollection. If no live objects reside in the affected region, which is unlikely because writes cause \nthe failures, the col\u00adlector marks the memory as failed in the failure map and returns. Pinning support \nis required in some languages, such as C#. If a live object in the failed region is pinned by the application, \nthe VM cannot move it and must notify the OS that it cannot vacate the region. An alternative solution \nis to allocate pinned objects to memory that does not fail, but some systems require in-place pinning \nafter allocation, so this solution will not always work. Since pinning is rare, the probability of a \nline failing that contains a pinned object is low and some expensive action, such as the OS remapping \nthe affected page to another perfect or failure\u00adcompatible physical page would be appropriate. While \nthe collector recovers from a failure, other failures could happen. As explained earlier, the hardware \nand OS handle these failures until the collector is ready to deal with them. Heap Layout To implement \nfailure tolerance, the garbage collec\u00adtor must relate the PCM failure granularity to the data structures \nit uses to manage live objects and free memory. Modern garbage collectors use one of contiguous allocation, \na free-list, or the Im\u00admix mark-region line and block hierarchy [3, 4]. The next section describes how \nto use the Immix heap, and Section 3.3.2 described why modifying a free list is possible, but complex. \nModifying a contiguous heap layout is not even possible, because it simply has no way to skip over or \nmanage small regions of memory without fundamental changes to the algorithm.  Figure 2. Immix Heap Organization \nand Amenability to Failure Tolerance Large Objects Both memory managers and garbage collectors explicitly \nhandle large objects separately [2, 4, 14, 21]. The exact large object size is a function of the granularity \nof the memory manager s block size, i.e., the granularity at which it manages the small and medium size \nobjects. Typical block sizes range from 16 to 64 KB. Immix uses 32 KB by default. Most large objects \nin managed languages are arrays and, for DaCapo Java programs, they consume on average half the heap \n[21]. Without hardware clustering, if many failures are uniformly distributed in memory, large objects \nwill not .t. A purely software solution is to use discontiguous arrays, which divide up arrays into a \nspine that points to smaller .xed-size arraylets and modify the array access code [1, 21]. Discontinuous \narrays were invented to bound pause times for real-time systems. Sartor et al. showed how to make them \nrelatively ef.cient, with average overheads below 13%, even with arraylets as small as 256 bytes. With \nhardware clustering at a two-page granularity or higher, the hardware creates the logically perfect pages \nthat large ob\u00adjects require. For example, with two-page clustering and failures in roughly 50% or less \nof the memory, at least 50% of memory will consist of perfect pages that the allocator may use for large \nobjects. 4. Implementing a Failure-Aware Runtime This section describes the modest changes required to \nmake the Immix mark-region garbage collector failure-aware. The key at\u00adtributes of Immix are well suited \nto the PCM failure mode, in which .xed-size lines fail. (a) Immix already tracks memory at a line granularity \ncategorizing lines as: free, live, or live pinned. (b) Im\u00admix already maintains two invariants: (i) it \nonly allocates into free lines, and (ii) it only ever moves unpinned objects. To make Im\u00admix failure-aware, \nwe simply add a fourth line category to Immix, failed lines, and continue to enforce the same two invariants. \n 4.1 Basic Immix The Immix design was motivated by the tension between three de\u00adsirable performance objectives: \nspace ef.ciency, collection time, and mutator (program) locality [3, 4]. Immix achieves all three per\u00adformance \nobjectives by combining bump-pointer allocation, ef.\u00adcient tracing, and occasional copying of live objects, \nas well as a memory-ef.cient two-level line and block heap organization. A key to this design is a bump-pointer \nallocator that can very ef.ciently skip over unavailable lines. This organization makes Immix an ex\u00adcellent \nmatch for our requirement that the allocator skip over failed PCM lines. Prior work established that \nImmix outperforms prior al\u00adgorithms and heap organizations, and it is now the default collector in Jikes \nRVM [3]. Lines and Blocks Immix manages heap memory at coarse and .ne granularity by dividing memory \ninto blocks and blocks into lines, as depicted in Figure 2. Lines approximate cache lines and blocks \napproximate pages. Objects may span lines within a block, but cannot span blocks. Objects that .t within \none line are regarded as small. Immix has a threshold that designates objects as large and delegates \nthem to a separate page-grained large object space (LOS). This threshold is never larger than a block. \nInitially all blocks are free and Immix allocates objects contiguously into blocks until it .lls the \nheap, which triggers a collection. Collection Immix collection marks live objects by performing a transitive \nclosure over the live object graph starting at the roots (global and stack variables) and then tracing \nreferences. When it encounters an object for the .rst time, it marks the object and its line as live. \nImmix does not copy objects by default; it deals with fragmentation by copying objects on occasion. Some \nmanaged languages, such as C# and JavaScript, require object pinning, in which applications can specify \nthat an object may not be moved. This is typically supported for performant interoperability with native \nC. Immix respects pinned objects and never copies them. After marking, Immix scans line mark tables, \nrecycles partially .lled blocks, and returns completely empty blocks to a global pool of pages for use \nby the whole runtime. Allocation In steady state, Immix allocates objects into available recycled blocks \n.rst, as depicted in Figure 2. The allocator is parallel, with one thread-local allocator for each application \nthread. The allocator initializes a bump pointer to the .rst free line in a block and sets the limit \nto the end of the last free line in this contiguous set of free lines. When the bump pointer .nds that \nan allocation request cannot be satis.ed within the limit, it skips over any unavailable lines to identify \nthe next set of free lines. When it exhausts a recycled block, it requests another from a shared pool \nof recycled blocks. Once all recycled blocks have been exhausted, the allocator requests completely free \nblocks from the global block pool. Once the global pool is exhausted the allocator triggers a collection. \nOther allocators, such as the large object space (LOS) compete for blocks from the global pool. To make \nef.cient use of recycled blocks, and avoid skipping otherwise usable smaller spaces, Immix heuristically \nallocates medium objects (those larger than a line) on a special over.ow block if the object cannot immediately \n.t in the contiguous space available to the bump pointer. Over.ow blocks are sourced from the global \npool of completely free blocks, so have maximal contiguous space. Our failure-aware collector handles \nrequests for completely free blocks by the LOS and over.ow allocator by using perfect memory for such \nrequests (see Section 3.3.3). Sticky Immix The Sticky Immix algorithm adds high-performance generational \nbehavior to the Immix collector. It combines sticky mark bits collection [8] with the Immix algorithm. \nSticky mark bits collectors identify young objects by a bit in their header, rather than by allocating \nthem into a distinct space. By default, it oppor\u00adtunistically copies nursery survivors into any available \nempty or partially .lled blocks. If there is no available memory, it leaves them in place. This collector \nretains all of the attributes of Immix which make it amenable to failure tolerance and to the need to \nskip over failures, whilst gaining the performance of compacting and collecting younger objects .rst. \n Performance Figure 3 shows a sample result that motivates Im\u00admix as a performant baseline for a failure-aware \nruntime. It shows the geometric mean of total execution time of the DaCapo bench\u00admarks con.gured with \nfull-heap mark-sweep (MS), Immix (IX), and the Sticky generational variants (S-MS and S-IX). We extend \nImmix to handle failures because of its high performance and its line-block organization simpli.es the \nalgorithmic design. Figure 3. Performance of DaCapo benchmarks running with vari\u00adous memory management \nalgorithms at multiple heap sizes.  4.2 Failure-Aware Immix This section describes how we modify Sticky \nImmix to be failure\u00adaware. Static Failures When the OS gives Immix a region of memory and a failure bit \nmap, failure-aware Immix marks corresponding lines as failed in its line-mark metadata. Because line \nmarks are bytes and only some of the 256 available states are used, we add an additional state that denotes \na failed line without space overhead. One subtlety is that the Immix line size is not necessarily the \nsame as the PCM line size. For example, the best performing line size for Immix is 256 B without failures, \nwhereas PCM line sizes are typically 64 B. When the Immix line size is greater than the PCM line size, \nwe must mark the entire Immix line failed, even though only part of it failed. Section 6.3 explores this \ntrade-off. The previous section described how Immix uses free blocks for over.ow allocation to accommodate \nmedium sized objects that do not immediately .t in the space available at the bump pointer. With failure-aware \nImmix, the over.ow block is not guaranteed to be perfect and the object may not .t. Thus we change the \nalgorithm to search the remainder of the over.ow block for suitably sized available space. If this search \nfails, the allocator resorts to requesting a free perfect block. Hardware failure clustering greatly \nreduces the likelihood of multiple over.ow allocation failures. Dynamic Failures To address dynamic failures, \nwe use the same mechanism that Immix uses to defragment the heap. Immix oppor\u00adtunistically copies objects \nthat reside on sparsely populated pages to free or partially free blocks during a defragmenting collection. \nFor dynamic failures, we straightforwardly reuse this mechanism to move objects affected by dynamic PCM \nfailures. We mark the affected object(s), the lines that contain it, and its block for evac\u00aduation, and \nthen invoke a copying collection. Although a full heap collection is relatively expensive, dynamic failures \nare rare. The full heap collection time is an estimate of the time needed by the runtime to handle a \nfailure. For the DaCapo benchmarks on Intel Core i7 hardware, it takes 7 msec on average. The worst case \nis 44 msec (hsqldb). The next two are 22 and 12 msec (fop and xalan), with all others under 10 msec. \nTo put this time in context, the average number of garbage collections is 14.7 and the average total \nexecution time for these benchmarks is 1817 msec. We believe that for most applications, this simple \nstrategy is suf.cient for dealing with the rare occurrence of a dynamic failure. 5. Experimental Methodology \nThis section justi.es our use of execution time results and then describes the hardware, benchmarks, \ncollector con.gurations, and methodology for running experiments. At the time of writing, direct evaluation \non systems with PCM main memory is unfortunately not possible, as the only commer\u00adcially available PCM \nparts today are NOR .ash replacements, with very low capacities and incompatible physical interfaces. \nSimu\u00adlation is slow, making whole benchmark execution prohibitively long. Instead, we evaluate our approach \nrunning natively on DRAM, and instrument the managed runtime with a fault injection module between the \nOS memory allocator and the VM memory allocation module. When the latter allocates memory, part of this \nmemory is made defective by the fault injection module. Machine, benchmarks, and collector con.gurations \nAll exper\u00adiments execute on an Intel Core i7 2600 machine with 4 GB of DRAM main memory, running on Ubuntu \n10.04.1 LTS. We use an implementation of Immix in Jikes RVM, a Java virtual machine. We execute the superset \nof all benchmarks in the DaCapo 9.12\u00adbach and DaCapo-2006-10 suites that can run on Jikes RVM. We use \nlusearch-.x, which is the lusearch benchmark patched to .x a bug in the underlying lucene library that \nintroduces pathological allocation behavior [24] by needlessly allocating a large data struc\u00adture in \na hot loop. This bug results in an allocation rate a factor of three higher than any other benchmark. \nAside from reporting it in Figure 4 for completeness, we exclude the buggy version of luse\u00adarch from \nall of our analysis. We compare to the Sticky Immix (S-IX) implementation in MMTk from Jikes RVM 3.1.2 \ndescribed above with a block size of 32 KB and a line size of 256 B. Failures and garbage collection \nexpose application performance to a space-time trade-off that we explore explicitly across a range of \nheap sizes. We use a modest default heap size that is 2\u00d7 the minimum for each benchmark. Failure map \ngeneration and memory accounting We model PCM failures via a failure map. The failure map has one bit \nfor each 64 B PCM line, which indicates whether that line is work\u00ading or has failed. The failure map \ngenerator distributes failures uniformly from a command line argument and produces clustered failure \nmap distributions from uniform distributions. Because the time-space trade-off is central to garbage \ncollected systems, we explicitly control for heap size in all of our experi\u00adments. This introduces two \nimportant and distinct methodological considerations in the context of memory that may fail. First, we \ncompensate the .xed heap sizes according to the failure rate to en\u00adsure that the usable memory is held \nconstant. Section 6.2 discusses heap compensation in detail. Second, and quite separately, appli\u00adcation \nmemory requests now fall into two categories: those from relaxed allocators that are robust to failure \nbecause they can uti\u00adlize fragmented pages, and those from fussy allocators that require perfect PCM \nor DRAM pages (because their allocations are page\u00adgrained). The balance between fussy and relaxed allocation \nvaries greatly as a function of time and among benchmarks. Because our modeling of PCM failure is not \nguaranteed to return a perfect page and a real implementation would have a limited supply of DRAM for \nsuch requests, we use a debit-credit based cost model to account for the cost of using a DRAM page when \nno perfect PCM page is available. Penalizing the use of DRAM is appropriate both because it would be \nscarce in practice and because, without a penalty, DRAM is highly attractive since it is never fragmented, \nwhich may lead to the counter-intuitive situation where higher failure rates offer better performance. \nWhen a fussy allocator requests a perfect page and the allocator has suf.cient memory, but no perfect \npage is available, we model giving it a perfect page (DRAM or PCM) with a one page space Figure 4. Performance \nof DaCapo benchmarks running with a failure-aware Immix implementation at multiple failure levels, normalized \nto the unmodi.ed version of the algorithm.  penalty. Because garbage collection exists as a time-space \ntrade\u00adoff, this space penalty ultimately translates to a time penalty. The relaxed allocator may repay \nthe debt of the fussy allocator. On subsequent allocations, which may follow a collection or not, each \ntime the relaxed allocator is given a perfect page, it only takes the page if there is no outstanding \ndebt. If there is a debt, we reduce the debt by one page and fetch another PCM page for the relaxed allocator. \nNondeterminism and experimental error To eliminate the ef\u00adfects of nondeterminism due to timer-driven \nadaptive recompila\u00adtion, we used the current version of Jikes RVM s replay compila\u00adtion mechanism [10, \n13]. Compiler replay is driven by optimiza\u00adtion pro.les gathered ahead of time for each benchmark and \nthen used for all experiments. Each pro.le identi.es a good optimization plan, stating the optimization \nlevel (if any) for each method, and in\u00adcludes edge counts and a call graph which would normally be gath\u00adered \ndynamically. The replay mechanism .rst runs each benchmark for one iteration without any optimization \n(this forces all classes to be loaded). The system then compiles all methods according to the optimization \nplan, before commencing a second iteration of the benchmark. Our experiments evaluate this second, optimized, \nitera\u00adtion. This methodology eliminates the chaotic nature of timer-based adaptive optimization while \ndelivering performance slightly better than steady-state performance for an adaptively optimized system. \nWe perform 20 invocations of each benchmark, each using the replay methodology, and take the mean of \nthese results, report\u00ading 95% con.dence intervals. The 95% con.dence interval error is generally very \nlow, around 1-2%. We aggregate results across benchmarks using geometric means. Some con.gurations cannot \nexecute some of the benchmarks in very small heaps. When report\u00ading aggregated results, we discard results \nat a given heap size when some of the benchmarks do not complete, which manifests in the graphs as lines \nthat terminate before reaching the y-axis. 6. Evaluation This section starts with an evaluation of PCM-aware \nSticky Immix implementation with failure clustering hardware. It shows how together they dramatically \nmitigate the effect of PCM failures. The remainder of the section explores trade-offs in the design space \nand illustrates the effects of failure-induced memory fragmentation. 6.1 Performance and Memory Overheads \nFigure 4 shows the performance overheads of failure-aware Sticky Immix with two-page failure clustering \n(S-IXPCM 2CL ) at multiple PCM failure rates (0, 10%, 25% and 50%). The .gure normalizes time to the \nunmodi.ed Sticky Immix (S-IX) collector (not shown). All of the Immix collectors use a logical line size \nof 256 B and block size of 32 KB, and receive the same amount of usable mem\u00adory (the equivalent of two \ntimes the minimum heap required by each benchmark to run). Note that S-IXPCM incurs zero overhead 2CL \nwhen there are no PCM failures (the geometric mean of the green bars is 1.0, the same as unmodi.ed Sticky \nImmix). The lack of any measurable overhead corroborates that, as described in Section 4.2, no additional \nmetadata is required for failure-aware Sticky Immix (or we would see slowdowns at this constrained heap \nsize). As the number of failures increases, the performance overheads slowly increase. At failure levels \nof 10%, the overhead is 3.9% on average and never exceeds 7.4% (peach bars). Low overheads are a result \nof the positive effects of failure clustering, which are more thoroughly explained in Section 6.4. The \noverhead grows modestly with failure rate, increasing when the failure rate is 50% to an average of 12.4% \nwith a maximum of 40% for pmd. Some workloads, such as pmd and jython, experience high over\u00adheads. The \nreason is that they allocate many medium sized objects, which makes it more challenging to .nd free memory \nto .t these objects. Benchmarks such as xalan, which predominantly allocate very large objects, are quite \nresilient to failures because they utilize the entirely free pages delivered by two-page failure clustering. \nVir\u00adtual address translation transparently removes any problem of page\u00adlevel fragmentation. As anticipated \nin Section 5, the buggy lusearch benchmark (which we gray-out here and exclude from all further analysis) \ngives a counter-intuitive result, with overhead reducing as failure rate rises. This result is due both \nto lusearch s pathological behavior, and an unobvious interplay between the two-page fail\u00adure clustering \nand the way we use heap space compensation in our analysis (see Section 6.2 and Figure 5). The remainder \nof this section explores the fragmentation problems and the inherent trade-offs due to failures in more \ndetail. 6.2 Memory Reduction vs. Fragmentation Three main factors impact performance as memory degrades. \n(1) As failures accumulate, the functional memory a VM receives when it requests a certain region of \nmemory from the OS is re\u00adduced. This problem is addressed by compensation: the VM simply requests more \nmemory until it gets as much working memory as originally intended. (2) Failures cause heap fragmentation, \nwhich reduces the usability of the available memory. (3) A more subtle effect is false failures, which \noccur when the PCM failure gran\u00adularity is less than the granularity of allocation (similar to false \nsharing in caches). For example, if the Immix line size is greater than the failed cache line size speci.ed \nby the PCM memory or the OS. With an allocation granularity of 256 B lines in Immix, an en\u00adtire line \nbecomes unusable when a single 64 B PCM line fails and false failures overstate the real failure by 192 \nB. This phenomenon is not speci.c to Immix, and will manifest whenever a request is made for contiguous \nmemory greater than the PCM line granularity (regardless of the software algorithm).  Figure 5. Compensation \nfor memory failures is most noticeable at smaller heap sizes. This graph compares the baseline S-IXPCM \nwith no failures (green) to 10% PCM line failure rate and no clustering, with (red) and without (orange) \nmemory compensation, and to S-IXPCM with clustering and compensation (maroon), the best failure-tolerant \ncon.guration. To break down these three effects, Figure 5 summarizes the be\u00adhavior of different con.gurations \nby presenting normalized execu\u00adtion time geometric means (y-axis) over all workloads as a func\u00adtion of \nheap size (x-axis). It compares the baseline (green) with no failures (S-IXPCM ) to PCM-aware Sticky \nImmix with compen\u00adsation (orange) and without (red), both without failure clustering hardware at a 10% \nfailure rate (S-IXPCM 10%). Space compensa\u00adtion provides the same amount of working memory if there were \nno failures. For example, given a heap size h used in the absence of failure, when the failure rate is \nf , we compensate with a heap size of h/(1 - f ). Compensation ensures that the number of bytes of non-faulty \nmemory available to the system is held constant. Consider S-IXPCM under no failures (S-IXPCM , green), \nat 10% failure rates (S-IXPCM 10% NoComp, red), and with working\u00admemory space-compensation at 10% failure \nlevels (S-IXPCM 10%, orange). The gap between S-IXPCM 10% NoComp and S-IXPCM 10% (red and orange) in \nFigure 5 shows the effect of failures re\u00adducing available memory. In small to moderate heap sizes, this \nef\u00adfect is pronounced until the heap grows to 3 times minimum, when the lines converge. The gap between \nthe S-IXPCM 10% (orange) and no failures (green) on the bottom shows the remaining two ef\u00adfects. Fragmentation \nand false failures, when spread uniformly over memory, signi.cantly degrade performance, over 25% in \nsmall to moderate heaps and over 10% even in large heaps. The large effects of fragmentation with or \nwithout heap com\u00adpensation motivate hardware clustering. Comparing no clustering (the top two lines) \nto S-IXPCM 10% (two-page clustering, maroon) 2CL shows that the latter signi.cantly improves performance \nby reduc\u00ading the amount of fragmentation exposed to the software. Reducing fragmentation wins twice. \nFirst, the amount of usable memory is increased, which reduces memory management load, shifting the curve \nleft. Second, application locality is improved, shifting the curve down. The next section examines in \nmore detail the effect of line size on false failures. Heap compensation can have an unexpected interplay \nwith two\u00adpage failure clustering and our means of accounting for requests for perfect memory, as in lusearch \nin Figure 4. When allocation is dominated by page-grain requests, the number of failures on a page is \nunimportant; what matters is whether the page is completely free or not. Consequently, the impact of \nfailure rate on such benchmarks is essentially constant for non-zero failure rates. On the other hand, \nheap compensation makes more memory available as failure rates grow. Thus although in practice the cost \nin terms of usable pages for failure rates of 10%, 25% and 50% remains similar for a benchmark like lusearch, \ncompensation increases, which leads to the counter\u00ad (a) The effect of line size on baseline S-IX.  (b) \nThe effect of line size on S-IXPCM with 10% of lines failed. Figure 6. The presence of failures without \nhardware clustering alters the performance behavior as Immix line size grows: While S-IX uniformly bene.ts \nfrom larger lines, in the presence of failures, S-IXPCM L256 suffers. intuitive improvement in performance \nas failure rates grow that we see in the lusearch result in Figure 4. However, this pathological behavior \nis rare, as Figure 4 shows. In the remainder of the evaluation, we use compensated runs by default. \n6.3 The Effect of Line Size Figure 6 shows the effect of Immix line size on performance, assuming a constant \nPCM line size of 64 bytes and no hardware failure clustering. Figure 6(a) corresponds to the baseline \nS-IX at three Immix line sizes: 64 bytes (S-IX L64), 128 bytes (S-IX L128), and 256 bytes (S-IX L256), \nall running without PCM failure. The interesting trend to note is that larger lines perform better on \nS-IX, and that this advantage increases as the heap becomes smaller. The primary reason is that in a \ntight heap, larger lines mean fewer slow\u00adpath operations and better locality. Smaller lines increase \nmetadata overhead, which is felt most acutely in constrained heaps. The presence of failures changes \nthe Immix line size dynamics signi.cantly. Figure 6(b) shows the same three Immix line sizes (S-IXPCM \nL64, S-IXPCM L128, and S-IXPCM L256) at 10% failure levels, as well as S-IX. False failures make larger \nImmix line sizes less desirable when we constrain the heap without clustering. As the Immix line size \nincreases, it takes only one PCM line failure to make an entire 128-byte or 256-byte line unusable. These \nfalse failures generate more severe loss of usable memory space, which affects performance negatively. \nOnce again, the issue is alleviated, but not eliminated, at larger heap sizes. Figure 7 further illustrates \nthe relationship between perfor\u00admance, Immix line size and failures, this time holding heap size constant \nat 2\u00d7 the minimum and varying failures between 0 and 50% of all lines. To more fully expose fragmentation \nbehavior, these con.gurations do not use hardware failure clustering. In this Figure 7. The effect of \nPCM failures on three different Immix line sizes at a .xed 2\u00d7 heap size. With a 256 B Immix line, failures \ndramatically reduce useful memory causing benchmarks to fail without hardware clustering.  case, the \nlarger the line size, the earlier it starts causing excessive false failures and thus wasting memory, \nso the worse the perfor\u00admance. When the failure rate is zero, larger lines perform better, but as the \nfailure rate increases, the effect of false failures domi\u00adnates. With L256 this happens almost immediately, \nand with L128 the crossover is at about 15% failure rate. Figures 6 and 7 illustrate the two opposing \nforces that in.uence the choice of Immix line size without clustering: lower memory management overhead \nversus higher usable memory waste.  6.4 Failure Clustering The results in Section 6.3 assume that PCM \nfailures occur with a uniform random distribution at the granularity of 64 B lines. When uniformly distributed, \nthese failures cause signi.cant fragmenta\u00adtion, and will lead to as much as 3\u00d7 space overhead due to \nfalse failures with a 256 B Immix line. This section quantitatively ex\u00adplores the effect of clustering \nfailures at granularities greater than 64 B. The number of failed lines remains 10%, 25%, and 50%, but \nwe systematically distribute failures in clusters of 2N failed lines, for sizes 64 B and greater to reveal \nthe effects of fragmentation and how well clustering counterbalances them. Failure Clustering Limit Study \nTo conduct these experiments, we use the S-IXPCM collector, but generate the PCM failure map slightly \ndifferently. Instead of failing individual 64 B lines with probability p, we step through aligned regions \nof size 2N and fail the entire region with probability p. The effect is that the gaps between failures \nare guaranteed to be at least 2N , but the probability of any given line having failed remains p. This \nanalysis motivates hardware support for failure clustering. Figure 8 shows performance when S-IXPCM is \nexposed to 10%, 25%, and 50% failures with failures clustered at powers of two from 64 B to 16 KB. The \nx-axis is logarithmic. Performance is normalized to unmodi.ed S-IX running on regular memory. Performance \ndramatically improves with failure clustering. Clustering mitigates fragmentation by reducing or eliminating \nfalse failures and leaving larger chunks of usable memory for the mem\u00adory manager to use. The greater \nthe number of failures, the more dramatic the effect of failure clustering. The problem is so severe \nthat starting at 25% failures, and at 64-byte failure cluster granu\u00adlarity, the VM cannot execute many \nworkloads to completion even at a 2\u00d7 heap, thus the 25% and 50% curves start at 128 bytes. Yet, using \nclustering at 256 bytes, the performance overhead is reduced to just 20% when 50% of lines fail. Proposed \nFailure Clustering Hardware Figure 9 compares our failure-aware collector with and without clustering \nhardware sup\u00adport at one-and two-page granularity, for 64 B, 128 B, and 256 B Immix lines, and with 0, \n10%, 25%, and 50% of PCM lines failed. Figure 8. S-IXPCM performance with default 256 B lines as fail\u00adures \nare clustered at a variety of power-of-two granularities (x-axis) with 10%, 25% and 50% of lines failed. \nClustering of failures dra\u00admatically mitigates performance penalties of failures. These experiments use \na failure map with uniformly distributed 64\u00adbyte line failures, and then move those failures according \nto our one-and two-page clustering algorithm, alternatively moving all failures to the start or end of \neach clustering region.  (b) Two-page clustering greatly reduces dependency on borrowed pages. Figure \n9. Hardware support for failure clustering mitigates frag\u00admentation, reduces or eliminates false failures, \nand greatly reduces demand for perfect pages. Figure 9(a) shows the effect on average performance, while \nFigure 9(b) shows the impact on average demand for perfect pages. The .rst group of curves (red: S-IXPCM \nL64, S-IXPCM L128, and S-IXPCM L256) does not have hardware support, while the second (blue: S-IXPCM \nL64, S-IXPCM L128, S-IXPCM L256) has one-page 1CL 1CL 1CL clustering and the third (green: S-IXPCM L64, \nS-IXPCM L128, S\u00ad 2CL 2CL IXPCM L256) has two-page clustering. 2CL Figure 9(a) shows that hardware failure \nclustering greatly re\u00adduces the performance overhead due to PCM failures. The .rst group of lines (red, \ntop) consistently perform worse because frag\u00ad  mentation and false failures dominate performance, especially \nwhen lines are large and many lines fail. The 256 B line size fares worst, and the effect of false failures \nis so great that many bench\u00admarks cannot run at 25% failure (hence no data for that point in the graph). \nPerformance is much better and less variable with hard\u00adware support for failure clustering because clustering \neliminates most false failures and greatly reduces fragmentation visible to software. Two-page clustering \n(green) has the further advantage of creat\u00ading completely failure-free pages. Note that for both one-page \nand two-page clustering, 256 B Immix lines result in the best perfor\u00admance. The reason is that clustering \nmitigates the negative effects of fragmentation, leaving only the positive effects of coarser Immix line \ngranularity (lower memory management overheads). Figure 10 shows the individual benchmark results for \none-and two-page failure clustering at various failure rates. Consistent with Figure 9(a), two-page clustering \nreduces overheads considerably unless failure rates are very high. Figure 9(b) shows that the main reason \nis that hardware failure clustering greatly reduces demand for perfect pages. This re.ects the fact that \nclustering has a defrag\u00admenting effect, which greatly increases the probability of a medium sized object \nbeing able to .t on a page suffering failed lines. Perfect pages are used when objects cannot be placed \non a regular page, so the 3\u00d7 reduction in demand for perfect pages by the two-page clus\u00adtering algorithm \nre.ects the strong defragmenting effect of failure clustering. The xalan benchmark makes very heavy use \nof perfect pages and consequently sees a huge advantage in two-page push at 10% failure rate. The counter-intuitive \nlowering of overhead with rising failure rate in xalan with one-page push is due to the effect of heap \ncompensation, as it was with lusearch (Section 6.1). Figure 9(b) also shows an interesting property of \nthe two page clustering. The demand for clean pages is very robust to failure rates even as high as 50%. \nAs long as a two-page region has a failure rate less than 50%, the clustering will yield at least one \nperfect page out of every two. Once the failure rate in a two-page region exceeds 50%, clustering will \nonly yield a free region that is smaller than a page, greatly reducing the stock of failure-free pages. \nFigure 10 shows that two benchmarks, pmd and jython, are very sensitive to this threshold, but the others \ndegrade gradually. 7. Discussion This section discusses implications of our work. 7.1 Managed Language \nIncentives Managed languages offer a layer of memory abstraction, which this paper exploits to attain \nfault tolerance transparently and which native languages lack. Our work thus creates an incentive to \nuse managed languages for future memory technologies. Native appli\u00adcations will need to rely on the operating \nsystem to shield them from dynamic failures, losing an entire PCM page when the .rst line on the page \nhas failed. Modifying a free-list allocator to handle static failures is possible. However, it is more \ncomplex than the so\u00adlution offered by Immix. Without clustering, it would be even more complex. An additional \nconsideration for native runtimes is that be\u00adcause C applications are unsafe they may generate illegal \nreads and writes to failed memory, which the OS will need to handle. Without support for pages with failures \nin a native runtime, native applica\u00adtions will have access to fewer and fewer pages as memory fails, \nmaking managed applications more attractive for PCM memory.  7.2 Wear Leveling Considered Harmful Wear \nleveling strives to uniformly spread writes and wear all mem\u00adory equally. However, the result is that \nfailures end up being evenly spread out through memory, causing memory fragmentation. Our results show \nthat clustering mitigates fragmentation, but only to a certain extent. We argue that evenly wearing memory \nis not the best wear management strategy because of the fragmentation it causes. Concentrating writes \ninto certain regions of memory may reduce total system fragmentation and the effect of failures. We intend \nto investigate smarter wear management strategies in future work. 7.3 Balanced Hardware Clustering Our \nresults show that managed runtimes experience performance improvements with multiple-page regions because \nthis keeps en\u00adtire working pages available longer, making allocating medium and large objects less challenging. \nNative applications need entire working pages, so we expect them to bene.t from this as well. It may \nseem bene.cial to use larger regions. Although initially a higher number of pages are logically intact \n(e.g., three in four-page region), these cases quickly degenerate to the two-page case. When failures \nreach roughly 25%, two of the four pages will be necessary to remap all failures in the region. Moreover, \nlarge regions may create additional complexity such as failure map cache pressure. 7.4 Possible Mitigation \nof Memory Fabrication Issues As DRAM cells get smaller and closer to the atomic scale, pre\u00adcise manufacturing \nof these memory cells gets more challenging and fabrication variation causes cells to have wide ranges \nof sizes and electron-storage capacities. Manufacturing issues may cause certain cells to be born dead \n, i.e., not capable of holding any charge after fabrication. Similar issues may af.ict other memory technologies. \nDRAM manufacturers today have on-chip provisions to replace a small number of failed cells before memory \nparts are shipped, but scaling this solution to a higher number of failures while keeping overheads low \nmay be very challenging. This issue may cause low yields, i.e., only being able to ship the small por\u00adtion \nof fabricated chips that have low number of failures, which increases costs and disrupts the memory industry \nbusiness model. Our solution makes memory chips with arbitrary numbers of fail\u00adures useful, without incurring \nhigh area or cost overhead. Instead of throwing the chips with high failures rates away, manufacturers \ncan bin them into classes. For example, the higher the number of fail\u00adures, the cheaper the memory. This \nprocess is akin what processor manufacturers do today with chip frequency and power dissipation, which \nis also caused by process variation. 8. Conclusions This paper explores an example of Rattner s self-aware \nsys\u00adtems [20], in which hardware and software cooperate to accom\u00adplish a shared system goal, in this \ncase, system endurance. As hardware resources are gradually used up and exhausted, the OS and runtime \ncome to the rescue to tolerate visible failures. This work proposes a cooperative hardware/software system \nwith low hardware and software complexity to mitigate failures in wearable memories. The OS and runtime \ncoordinate to recover data from failed lines and migrate objects affected by failures. We use the memory \nabstraction provided by a managed runtime to handle failures transparently, and exploit the Immix garbage \ncollector s capacity to ef.ciently skip over unusable holes. Even without hardware clustering, the runtime \ncorrectly handles failures, but overhead is 17% with 10% failed memory and 33% with 50% failed memory. \nWhen the hardware assists by clustering failures, software overhead reduces quite signi.cantly to 3.9% \nwith 10% failed memory and 12.4% with 50% failed memory. This paper is the .rst to observe that hardware \nand software cooperation has the potential to achieve substantially better failure tolerance with increasing \nnumbers of hardware failures. 9. Acknowledgments We thank Burton Smith for initial failure map implementation \nideas, and our anonymous reviewers for their valuable feedback. References [1] D. Bacon, P. Cheng, and \nV. T. Rajan. Controlling fragmentation and space consumption in the Metronome, a real-time garbage collector \nfor Java. In Proceedings of the 2003 ACM SIGPLAN Conference on Languages, Compiler, and Tool Support \nfor Embedded Systems, pages 81 92, 2003. [2] E. D. Berger, K. S. McKinley, R. D. Blumofe, and P. R. Wilson. \nHoard: A scalable memory allocator for multithreaded applications. In Proceedings of the 9th International \nConference on Architectural Support for Programming Languages and Operating Systems, pages 117 128, 2000. \n[3] S. M. Blackburn and K. S. McKinley. Immix: A mark-region garbage collector with space ef.ciency, \nfast collection, and mutator perfor\u00admance. In Proceedings of the 2008 ACM SIGPLAN Conference on Programming \nLanguages Design and Implementation, pages 22 32, 2008. [4] S. M. Blackburn, P. Cheng, and K. S. McKinley. \nMyths and realities: The performance impact of garbage collection. In Proceedings of the 2004 ACM SIGMETRICS \nConference on Measurement &#38; Modeling Computer Systems, pages 25 36, 2004. [5] S. M. Blackburn, R. \nGarner, C. Hoffman, A. M. Khan, K. S. McKin\u00adley, R. Bentzur, A. Diwan, D. Feinberg, D. Frampton, S. Z. \nGuyer, M. Hirzel, A. Hosking, M. Jump, H. Lee, J. E. B. Moss, A. Phansalkar, D. Stefanovi \u00b4c, T. VanDrunen, \nD. von Dincklage, and B. Wiedermann. The DaCapo benchmarks: Java benchmarking development and anal\u00adysis. \nIn Proceedings of the 21st Annual ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages, \nand Applica\u00adtions, pages 169 190, 2006. [6] H.-J. Boehm. Conservative GC algorithmic overview. http://www.hpl.hp.com/personal/Hans \nBoehm/gc/gcdescr.html. [7] J. Condit, E. B. Nightingale, C. Frost, E. Ipek, D. Burger, B. C. Lee, and \nD. Coetzee. Better I/O through byte-addressable, persistent memory. In Proceedings of the 22nd ACM Sumposium \non Operating Systems Principles, pages 133 146, 2009. [8] A. Demmers, M. Weiser, B. Hayes, H. Boehm, \nD. Bobrow, and S. Shenker. Combining generational and conservative garbage col\u00ad lection: Framework and \nimplementations. In Proceedings of the 17th ACM SIGPLAN-SIGACT Symposium on Principles of Programming \nLanguages, pages 261 269, 1990. [9] Avoiding server downtime from hardware errors in system memory with \nHP Memory Quarantine. Hewlett-Packard Corporation. [10] X. Huang, S. M. Blackburn, K. S. McKinley, J. \nE. B. Moss, Z. Wang, and P. Cheng. The garbage collection advantage: Improving program locality. In Proceedings \nof the 19th Annual ACM SIGPLAN Confer\u00adence on Object-Oriented Programming Systems, Languages, and Ap\u00adplications, \npages 69 80, 2004. [11] E. Ipek, J. Condit, E. B. Nightingale, D. Burger, and T. Mosci\u00adbroda. Dynamically \nreplicated memory: Building reliable systems from nanoscale resistive memories. In Proceedings of the \n15th Inter\u00adnational Conference on Architectural Support for Programming Lan\u00adguages and Operating Systems, \npages 3 14, 2010. [12] ITRS Working Group. ITRS report. Technical report, International Technology Roadmap \nfor Semiconductors, 2011. [13] Jikes RVM. Compiler Replay, Dec. 2011. http://jikesrvm.org/Exper\u00adimental+Guidelines. \n[14] D. Lea. A memory allocator. http://g.oswego.edu/dl/html/malloc.html. [15] Micron Technology Inc. \nPCM-based MCP. http://www.micron.com/ products/multichip-packages/pcm-based-mcp?source=mb. [16] M. K. \nQureshi. Pay-as-you-go: Low-overhead hard-error correction for phase change memories. In Proceedings \nof the 44th Annual IEEE/ACM International Symposium on Microarchitecture, pages 318 328, 2011. [17] M. \nK. Qureshi, J. Karidis, M. Franceschini, V. Srinivasan, L. Lastras, and B. Abali. Enhancing lifetime \nand security of PCM-based main memory with start-gap wear leveling. In Proceedings of the 42nd Annual \nIEEE/ACM International Symposium on Microarchitecture, pages 14 23, 2009. [18] M. K. Qureshi, V. Srinivasan, \nand J. Rivers. Scalable high perfor\u00admance main memory system using phase-change memory technology. In \nProceedings of the 36th Annual International Symposium on Com\u00adputer Architecture, pages 24 33, 2009. \n[19] S. Raoux, G. Burr, M. Breitwisch, C. Rettner, Y. Chen, R. Shelby, M. Salinga, D. Krebs, S.-H. Chen, \nH. L. Lung, and C. Lam. Phase\u00adchange random access memory: A scalable technology. IBM Journal of Research \nand Development, 52(4.5):465 479, 2008. [20] J. Rattner. Extreme scale computing. Keynote Speech at \nthe 39th International Symposium on Computer Architecture, 2012. [21] J. B. Sartor, S. M. Blackburn, \nD. Frampton, M. Hirzel, and K. S. McKinley. Z-rays: Divide arrays and conquer speed and .exibility. In \nProceedings of the 2010 ACM SIGPLAN Conference on Programming Languages Design and Implementation, pages \n471 482, 2010. [22] S. Schechter, G. Loh, K. Strauss, and D. Burger. Use ECP, not ECC, for hard failures \nin resistive memories. In Proceedings of the 37th Annual International Symposium on Computer Architecture, \npages 141 152, 2010. [23] N. H. Seong, D. H. Woo, V. Srinivasan, J. A. Rivers, and H.-H. S. Lee. SAFER: \nStuck-at-fault error recovery for memories. In Proceedings of the 43rd Annual IEEE/ACM International \nSymposium on Microar\u00adchitecture, pages 115 124, 2010. [24] X. Yang, S. M. Blackburn, D. Frampton, J. \nB. Sartor, and K. S. McKin\u00adley. Why nothing matters: The impact of zeroing. In Proceedings of the 26th \nAnnual ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages, and Applications, pages \n307 324, 2011. [25] D. H. Yoon, N. Muralimanohar, J. Chang, P. Ranganathan, N. P. Jouppi, and M. Erez. \nFREE-p: Protecting non-volatile memory against both hard and soft errors. In Proceedings of the 17th \nInternational Symposium on High Performance Computer Architecture, pages 466 477, 2011. [26] P. Zhou, \nB. Zhao, J. Yang, and Y. Zhang. A durable and energy ef.\u00adcient main memory using phase change memory \ntechnology. In Pro\u00adceedings of the 36th Annual International Symposium on Computer Architecture, pages \n14 23, 2009.    \n\t\t\t", "proc_id": "2491956", "abstract": "<p>New memory technologies, such as phase-change memory (PCM), promise denser and cheaper main memory, and are expected to displace DRAM. However, many of them experience permanent failures far more quickly than DRAM. DRAM mechanisms that handle permanent failures rely on very low failure rates and, if directly applied to PCM, are extremely inefficient: Discarding a page when the first line fails wastes 98% of the memory.</p> <p>This paper proposes low complexity cooperative software and hardware that handle failure rates as high as 50%. Our approach makes error handling transparent to the application by using the memory abstraction offered by managed languages. Once hardware error correction for a memory line is exhausted, rather than discarding the entire page, the hardware communicates the failed line to a failure-aware OS and runtime. The runtime ensures memory allocations never use failed lines and moves data when lines fail during program execution. This paper describes minimal extensions to an Immix mark-region garbage collector, which correctly utilizes pages with failed physical lines by skipping over failures. This paper also proposes hardware support that clusters failed lines at one end of a memory region to reduce fragmentation and improve performance under failures. Contrary to accepted hardware wisdom that advocates for wear-leveling, we show that with software support non-uniform failures delay the impact of memory failure. Together, these mechanisms incur no performance overhead when there are no failures and at failure levels of 10% to 50% suffer only an average overhead of 4% and 12%}, respectively. These results indicate that hardware and software cooperation can greatly extend the life of wearable memories.</p>", "authors": [{"name": "Tiejun Gao", "author_profile_id": "81547992856", "affiliation": "Australian National University, Camberra, Australia", "person_id": "P4149030", "email_address": "tiejun.gao@anu.edu.au", "orcid_id": ""}, {"name": "Karin Strauss", "author_profile_id": "81548047179", "affiliation": "Microsoft Research, Redmond, USA", "person_id": "P4149031", "email_address": "kstrauss@microsoft.com", "orcid_id": ""}, {"name": "Stephen M. Blackburn", "author_profile_id": "81100547435", "affiliation": "Australian National University, Camberra, Australia", "person_id": "P4149032", "email_address": "steve.blackburn@anu.edu.au", "orcid_id": ""}, {"name": "Kathryn S. McKinley", "author_profile_id": "81100402805", "affiliation": "Microsoft Research, Redmond, USA", "person_id": "P4149033", "email_address": "mckinley@microsoft.com", "orcid_id": ""}, {"name": "Doug Burger", "author_profile_id": "81100577220", "affiliation": "Microsoft Research, Redmond, USA", "person_id": "P4149034", "email_address": "dburger@microsoft.com", "orcid_id": ""}, {"name": "James Larus", "author_profile_id": "81100277326", "affiliation": "Microsoft Research, Redmond, USA", "person_id": "P4149035", "email_address": "larus@microsoft.com", "orcid_id": ""}], "doi_number": "10.1145/2491956.2462171", "year": "2013", "article_id": "2462171", "conference": "PLDI", "title": "Using managed runtime systems to tolerate holes in wearable memories", "url": "http://dl.acm.org/citation.cfm?id=2462171"}