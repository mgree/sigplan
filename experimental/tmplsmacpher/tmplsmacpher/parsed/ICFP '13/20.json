{"article_publication_date": "09-25-2013", "fulltext": "\n Higher-Order Functional Reactive Programming without Spacetime Leaks Neelakantan R. Krishnaswami Max \nPlanck Institute for Software Systems (MPI-SWS) neelk@mpi-sws.org Abstract Functional reactive programming \n(FRP) is an elegant approach to declaratively specify reactive systems. However, the powerful abstractions \nof FRP have historically made it dif.cult to predict and control the resource usage of programs written \nin this style. In this paper, we give a new language for higher-order reactive programming. Our language \ngeneralizes and simpli.es prior type systems for reactive programming, by supporting the use of streams \nof streams, .rst-class functions, and higher-order operations. We also support many temporal operations \nbeyond streams, such as terminatable streams, events, and even resumptions with .rst-class schedulers. \nFurthermore, our language supports an ef.cient imple\u00admentation strategy permitting us to eagerly deallocate \nold values and statically rule out spacetime leaks, a notorious source of inef.ciency in reactive programs. \nFurthermore, these memory guarantees are achieved without the use of a complex substructural type discipline. \nWe also show that our implementation strategy of eager deallo\u00adcation is safe, by showing the soundness \nof our type system with a novel step-indexed Kripke logical relation. Categories and Subject Descriptors \nD.3.2 [Data.ow Languages] Keywords Functional reactive programming; Kripke logical re\u00adlations; temporal \nlogic; guarded recursion; data.ow; capabilities; comonads 1. Introduction Interactive programs engage \nin an ongoing dialogue with the envi\u00adronment. An interactive program receives an input event from the \nenvironment, and computes an output event, and then waits for the next input, which may in turn be affected \nby the earlier outputs the program has made. Examples of such systems range from embedded controllers \nand sensor networks, up to graphical user interfaces, web applications, and video games. Programming \ninteractive applications in general purpose pro\u00adgramming languages can be very confusing, since the different \ncomponents of the program do not typically interact via structured control .ow (such as loops or direct \nprocedure calls), but instead operate by registering state-manipulating callbacks with one another, Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. Copyrights for components of this work owned \nby others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, \nor republish, to post on servers or to redistribute to lists, requires prior speci.c permission and/or \na fee. Request permissions from permissions@acm.org. ICFP 13, September 25-27, Boston, MA, USA.. Copyright \nis held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2326-0/13/09. . . \n$15.00. http://dx.doi.org/10.1145/2500365.2500588 which are then implicitly invoked by an event loop. \nReasoning about such programs is dif.cult, since each of these features higher-order functions, aliased \nimperative state, and concurrency is challenging on its own, and their combination takes us to the outer \nlimits of what veri.cation can cope with. These dif.culties, plus the critical nature of many reactive \nsystems, have inspired a great deal of research into languages, libraries and analysis techniques for \nreactive programming. Two of the main strands of work on this problem are the synchronous data.ow languages, \nand functional reactive programming. Synchronous data.ow languages, such Esterel [5], Lustre [7], and \nLucid Synchrone [39], implement a computational model inspired by Kahn networks. Programs are .xed networks \nof stream\u00adprocessing nodes that communicate with each other, each node consuming and producing a statically-known \nnumber of primitive values at each clock tick. These languages deliver strong guarantees on space and \ntime usage, and so see wide use in applications such as hardware synthesis and embedded control software. \nFunctional reactive programming, introduced by Elliott and Hudak [15], also works with time-varying values \n(rather than mutable state) as a primitive abstraction. However, it provides a much richer model than \nsynchronous languages do. Signals are .rst-class values, and can be used freely, including in higher-order \nfunctions and signal-valued signals, which permits writing programs which dynamically grow and alter \nthe data.ow network. FRP has been applied to problem domains such as robotics, games, music, and GUIs, \nillustrating the power of the FRP model. However, this power comes at a steep price. Modeling A-valued \nsignals as streams A. (in the real-valued case, AR) and reactive systems as functions Input. . Output. \nhas several problems. First, this model does not enforce causality (that is, output at time n depends \nonly upon inputs at earlier times), nor does it ensure that feedback (used to de.ne signals recursively) \nis well-founded. Second, since the model of FRP abstracts away from resource usage, it is easy to write \nprograms with signi.cant resource leaks by inadvertently accumulating the entire history of a signal \n that is, space leaks .1 For example, consider the simple program below: bad const : S N . S (S N) 1 \nbad const ns = cons(ns, bad const ns) 2 The bad const function takes a stream of numbers as an argument, \nand then returns that stream constantly. So if it receives the stream (a, b, c, . . .), then it returns \na stream of streams with (a, b, c, . . .) as its .rst element, with (a, b, c, . . .) as its second element, \nand so on 1 In reactive programming, the phrase space leak usually refers only to memory leaks arising \nfrom capturing too much history, since they form the species of memory leak that functional programmers \nare unused to debugging. Relatedly, there are also time leaks , which occur when a signal is sampled \ninfrequently. Under lazy evaluation, infrequent sampling can lead to the head of a stream growing to \nbecome a large computation.  inde.nitely. This is a perfectly natural, even boring, mathematical function \non streams, but many implementations leak memory. Con\u00adsider the following diagram, illustrating how a \nstream data structure evolves over time: . . . t = 0 . . . t = 1 . . . t = 2 . . . t = 3 Here, a stream \nis a computation incrementally producing values. At time 0, the stream yields the value a , and at time \n1, it yields the value b , and so on. Each value in a double-circle represents the stream s current value \nat time t, and the values in white circles represent the stream s future values, and the values in gray \ncircles represent the stream s past values. (So there are n gray circles at time n.) If a pointer to \nthe head of a stream persists, then at time n all of the .rst n + 1 values of the stream will be accessible, \nand so we will need to store all the values the stream has produced all of the grey nodes, plus the \ndouble-circled node. Thus, at time n, we will have to store n elements of history, which means that the \nmemory usage of the bad const function will be O(n) at time n. So the accidental use of a function like \nbad const in a long-running program will eventually consume all of the memory available. To avoid this \nproblem, a number of alternative designs, such as such as event-driven FRP [43] and arrowized FRP [35], \nhave been proposed. The common thread in this work is to give up on treating signals as .rst-class values, \nand instead offer a collection of combinators to construct stream transformer functions (that is, functions \nof type S A . S B) from smaller ones. Streams of streams (and other time-dependent values) are forbidden, \nand each of the exported combinators is designed to ensure that only ef.cient stream transformers can \nbe constructed. Restricting programmers to indirect access to streams is akin to a .rst-order programming \nstyle, since it makes it dif.cult to abstract over stream-manipulating operations. This restriction is \nalso quite similar to those synchronous data.ow languages impose, and Liu et al. [30] exploit this resemblance \nto develop a compilation scheme (reminiscent of the Bohm-Jacopini theorem) to compile arrow programs \ninto ef.cient single-loop code, much as compilers for Esterel and Lucid do. Sculthorpe and Nilsson [40] \nextend the arrowized approach further, by using dependent types to enforce causality. However, dynamic \nmodi.cation of the data.ow graph is very natural for interactive programs. For example, in a graphical \npro\u00adgram, we may wish to associate data.ow networks with windows, which are created and destroyed as \na program executes. To support this, arrowized FRP libraries need to add additional combinators to restore \ndynamic behavior. These combinators make unwanted mem\u00adory leaks possible once more (though less easily \nthan in the original FRP). They also have a somewhat ad-hoc .avor: since .rst-class and higher-order \nstream types are unavailable, somewhat complex and strange types are needed to encode typical usage patterns. \nTo recover a more natural programming style, Krishnaswami and Benton [27] proposed a lambda-calculus \nfor stream programming based on the guarded recursion calculus of Nakano [34], which ensured by typing \nthat all de.nitions are causal and well-founded. Jeffrey [19] and Jeltsch [22] further observed that \nit is possible to use the formulas of linear temporal logic [38] as types for reactive programs. However, \nwhile all of these papers offer a coherent account of causality (not using values from the future, and \nensuring that all recursive de.nitions are guarded), none of them have much to say about the memory behavior \nof their programs. To resolve this problem, Krishnaswami et al. [28] gave a calculus which retains the \ncausality checking and fully higher order style of [19, 22, 27], but which also uses linear types to \ncontrol the memory usage of reactive programs. This calculus passes around linearly\u00adtyped tokens representing \nthe right to allocate stream values, which ensures that no program can ever allocate a data.ow graph \nwhich is larger than the total number of permissions passed in. This solution works, but is too precise \nto be useful in practice. The exact size of the data.ow graph is revealed in a program s type, and so \neven modest refactorings lead to massive changes in the type signature of program components. Contributions \nOur contributions are as follows: 1. We give a new implementation strategy, described using oper\u00adational \nsemantics, for higher-order reactive programming. Our semantics rules out space and time leaks by construction, \nmak\u00ading it impossible to write programs which accidentally retain temporal values over long periods of \ntime. To accomplish this, we describe the semantics of reactive pro\u00adgrams with two operational semantics. \nThe .rst semantics ex\u00adplains how to evaluate program terms in the current clock tick, and the second \nis a tick relation for advancing the clock. Expressions in a reactive language can be divided into two \nclasses, those which must be evaluated now, and those which must be evaluated later, on future clock \nticks. We evaluate current expressions using a call-by-value strategy, and we suspend future expressions \nby placing them in mutable thunks on the heap, in a style reminiscent of call-by-need. Evaluating current \nexpressions in a call-by-value style prevents time leaks, by making streams head-strict [42]. Future \nexpressions can only be evaluated in the future, and the tick relation describes how to advance the clock \ninto the future. This semantics works by going through the heap of thunks, and forcing each expression \nscheduled for execution on the current time step. Furthermore, we achieve our goal of blocking space \nleaks through the simple but drastic measure of deleting all old values, thereby making it operationally \nimpossible to accidentally retain a reference to a value past its lifetime. 2. We give a simple type \ntheory for a higher-order reactive pro\u00ad gramming language. Our type system generalizes earlier work on \nusing temporal logic [19, 22, 27] to type reactive programs. Instead of using formulas of temporal logic \nas types, we take a standard simply\u00adtyped lambda calculus, and introduce a delay modality A of terms \nof type A that can be evaluated on the next tick of the clock , and add to that the new notion of a temporal \nrecursive type ^ \u00b5a. A, which allows de.ning types which are recursive through time. This recursive type \nconstruct allows encoding all of the standard temporal operators as derived constructions within our \ncalculus streams, events, and even resumptions are all de.nable within our system, as are higher-order \nfunctions to construct and manipulate elements of these types. To retain control over space leaks, we \nsimplify the work of Krishnaswami et al. [28] on using linear types to prevent space leaks in reactive \nsystems. We introduce a new kind of type-based capability which grants the right to allocate memory, \nwithout having to enumerate the exact amount of memory used. As a result, we do not need the complexities \nof a substructural type  system, and moreover we no longer re.ect exact memory usage in types, which \nimproves the modularity of reactive programs. 3. We show that our type discipline is sound by means of \na novel step-indexed Kripke logical relation. Deleting old values naturally raises the question of what \nhappens if we accidentally reference a deleted value. Our logical relation lets us prove that no well-typed \nprogram will ever dereference a deallocated location, and that expressions whose types say they do not \nallocate memory, do not actually allocate memory. Furthermore, the logical relation shows that the expression \nrelation is total for well-typed terms, despite the presence of a term-level .xed point. This demonstrates \nthat we can only de.ne well-founded and causal loop structures. Supplementary Material Full proofs of \nthe main results in this paper, and statements and proofs of all the supporting lemmas, can be found \nin the accompanying technical report. 2. Programming Language We begin with a description of our language \ndesign. We give the syntax of types, terms, and values in Figure 1, the syntax of contexts in Figure \n2, and the typing rules in Figures 3, 4, and 5. The operational semantics of expressions is in Figure \n6, and the semantics of advancing the clock is in Figure 7. Overview In a reactive language, an explicit \nnotion of time is exposed to the programmer, and so we will need to extend the operational semantics \nto account for time, and then give a modi.ed type system which properly re.ects the semantics. Our operational \nsemantics consists of two relations. The ex\u00adpression relation (s; e) . (s1; v) describes how to evaluate \nan expression within a single timestep. This relation, given in Figure 6, is a standard big-step, call-by-value \noperational semantics for a functional language with a store. As we will see, the use of a call\u00adby-value \nsemantics rules out time leaks, since they are inherently an artifact of lazy evaluation. However, the \nstores are not unrestricted heaps in the style of ML. Since reactive languages allow programmers to schedule \nwhen different expressions should be executed, there are program expressions which should not be executed \nright away. We put these expressions into a store, with the idea that the code stored in the heap will \nbe evaluated later. In terms of reactive programming idioms, the store represents the data.ow graph of \nthe program, which contains the nodes that will supply values on later ticks of the clock. In terms of \nfunctional programming idioms, the store implements lazy evaluation, in a variant of call-by-need where \nthunks are explicitly scheduled for later execution. To actually advance the clock, we give the tick \nrelation s =. s1, which describes (Figure 7) how the store s is transformed into a store s1 when the \nclock ticks. As expected, our relation evaluates all of the computations scheduled for evaluation on \nthe next tick. The tick semantics also makes space leaks impossible, by construction: brutally but expediently \nit deletes all values that are more than one tick old. It is now trivially the case that it is impossible \nto inadvertently store a history too long but, at what price? Surprisingly, we can show that the price \nis low. We give a type discipline, which not only ensures that all well-typed programs are safe (in that \nthey never try to access a deleted value), but which is also expressive, in that natural reactive programs \n(higher-order or not) are still well-typed. Since the passage of time is a central feature of our operational \nsemantics, we introduce three temporal quali.ers, now, later, and stable, to explain when some expression \nis well-typed, and when a variable may be used. The now quali.er means in the present time step , the \nlater quali.er means on the next time step , and the stable quali.er means at any time step, present \nor future . As a result, our typing judgment takes the form G f e : A q, where q is a quali.er. So the \ninformal reading of the typing judgment is under hypotheses G, the expression e has type A at time q. \nJust as expressions have temporal quali.ers, so too do the hypotheses in the context. The context G (with \na grammar given in Figure 2) consists of a list of hypotheses of the form x : A q, which gives not just \na type A for each variable x, but also gives a quali.er q controlling when we may use the variable x. \nTypes, Terms, and Expression Evaluation The core of our pro\u00adgramming language is essentially the simply-typed \nlambda calculus. The basic types include product types A \u00d7 B with pairs (e, e 1) and projections fst \ne and snd e, disjoint unions A + B with injections 1 11 inl e and inr e and a case statement case(e, \ninl x . e , inr y . e ), and function types A . B with lambda-abstractions .x. e and appli\u00adcations e \ne 1, as well as a variety of primitive types (such as numbers Nand booleans bool). The typing rules for \nthese constructs are all given in Figure 3, and are all standard. Similarly, the reduction rules for \nthese terms are given in the .rst half of Figure 6, and are also standard. To this, we add a number of \nfeatures to deal with time. First, we have the next-step operator A. The intuitive reading of this type \nis that an inhabitant of A is a term that, when evaluated on the next time step, will yield a value of \ntype A the type of A s tomorrow . The introduction form is the delay term de 1 (e), which says that \ne is a term to evaluate on the next time step. Consequently, its typing rule I requires e to be typechecked \nlater. The elimination form is a binding-style elimination form, let d(x) = e in e1. The typing rule \nE asserts that e is of type A, and the variable x is of 1 type A, but with the later quali.er. This ensures \nthat the body e does not use e s value before it is available. Operationally, we do not evaluate the \nbody e of a delayed expression de1 (e) right away. Instead, we extend the store with a pointer l : e \nlater to a thunk e, which will be evaluated on the next time step. These pointers implement a form of \ncall-by-need, ensuring that even if we use a delayed variable multiple times, the delayed expression \nwill itself only be evaluated once. This can be seen in the reduction rule for let d(x) = e in e1. Here, \nif e evaluates to the pointer l, the reduction rule substitutes !l, a dereferencing of 1 the pointer, \nfor x in e . The allocation of a thunk extends the reactive program s data.ow graph, and the growth of \nthe data.ow graph is something we must control. To achieve this, we use the subscripted argument e1 in \nthe delay introduction form de1 (e). This argument must be of the allocation type alloc, and indicates \na permission to allocate heap storage. Since the type alloc represents a pure capability [32], we have \nno introduction or elimination forms for it; a token (denoted by o in our syntax) representing this capability \nmust be passed in to a closed program by the runtime system of the language. Thus, programmers may control \nwhether or not a function allocates by controlling whether or not they pass it an allocation capability. \nPointer expressions l, dereference expressions !l, and resource tokens o are all internal forms of our \nlanguage, and cannot be written by a programmer. (As a result, there are no typing rules for them.) Values \nof A type are time-dependent, in the sense that their representation changes over time: when the clock \nticks, the data.ow graph/store is evaluated, and pointers are updated. This means that they should only \nbe used on particular time steps. Other types, such as natural numbers or booleans, consist of values \nwhose representation does not change over time. These stable values may safely be used at many different \ntimes. Values of other types, such as functions A . B, are either time-dependent or stable, depending \non whether or not they capture any time-dependent values in their environment.  So we also introduce \na modal type DA, which contains only those values of type A which are stable. The introduction form stable(e) \nensures that e has the type A under the stable quali.er, and the elimination form let stable(x) = e in \ne 1 takes a term e of type DA, and binds x to a stable variable. Since values of certain types (base \ntypes, and products and sums of same) are always stable, we also introduce a term promote(e), which takes \na term of an inherently stable type A (as judged by the judgment A stable, de.ned in Figure 5) and returns \na value of type DA. In order to get interesting temporal data structures, we introduce a temporal recursive \ntype, ^A. \u00b5a. This type has the expected introduction into e and elimination out e forms, but their types \nare slightly nonstandard. When constructing a term into e of recursive type ^\u00b5a. A)/a]A. That is, \u00b5a. \nA, we require e to have the type [ (^every occurrence of a in A is substituted with ( ^\u00b5a. A), which \nis the recursive type on the next time step. This lets us use the next-step modality to de.ne data whose \nstructure repeats over time, rather than proceeding only a constant number of steps into the future. \nIn addition to type-level recursion, we also have a term-level recursion operator .x x. e. There are \nno restrictions on the types we may take .xed points at; we only require that if .x x. e is of type A, \nthen we assume x is a later variable. This ensures that we can never construct an unguarded loop. We \nalso include a stream type S A, with an introduction form cons(e, e 1) and an elimination form let cons(x, \nxs) = e in e 1 . The stream type can be encoded using the other constructs of the language (see Section \n3.2), but we include it in order to give a more readable syntax to our examples. However, the cons(e, \ne 1) form does help illustrate why we chose a call-by-value evaluation strategy. Under call-by-value, \nthe head of the stream e is always evaluated to a value. As a result, time leaks are impossible, since \nthe head gets reduced to a value on every tick, and so it doesn t matter how frequently a stream is sampled. \nFinally, the variable rule HY P says that we can only use a variable now, if it is either stable, or \nif it is available now. Then, there are the TSTA B LE and TLATE R rules, which show how to derive terms \nwith the stable and later quali.ers, in terms of the now quali.er. The key to this are the context-clearing \noperations de.ned in Figure 2. The G0 context operation deletes every now and later hypothesis, ensuring \nthat stable terms may only depend on stable variables. The G operation takes a context, and deletes every \nhypothesis marked now, and changes every later hypothesis to now, ensuring that terms which are typechecked \nlater (1) view the later variables as if they were available in the present, and (2) do not access any \nvariables containing possibly out-of-date values. Data.ow Graphs The store s is used to represent the \ndata.ow graph of the program, and contains a collection of pointers, which can be viewed as the nodes \nin a data.ow graph. Each node is either a suspended expression l : e later to be evaluated on the next \ntick of the clock, or points to a presently available value l : v now, or is unde.ned l : null and points \nto nothing. Advancing Time We give the tick relation s =. s1 in Figure 7, which explains how to advance \ntime for a data.ow graph. If the data.ow graph s is empty, then of course the updated store is also empty. \nIn order to evaluate a store s, l : e later, we .rst update the rest of the data.ow graph s to s1, and \nthen evaluate e in the result store s1 to a value v, updating the pointer to l : v now. However, when \nwe see a store s, l : v now containing a value, we evaluate the store, but null out the pointer, setting \nit to l : null. Finally, once nulled out, null pointers stay nulled out. This feature of the operational \nsemantics suf.ces to ensure that FRP-speci.c space leaks are impossible because our data.ow graph never \nstores values for more than a single tick, it follows that values can never accumulate and build up into \na memory leak. As a result, values in data.ow nodes can never persist in memory unless the programmer \nexplicitly writes code to retain them. (Of course, all the memory leaks traditional to functional programming \nare still possible we have simply ensured that reactivity has not added any new sources of leaks.) It \nis worth reiterating that it is the operational semantics, and not the type system, which ensures the \nabsence of space leaks: because we delete everything old, it is simply impossible to remember the past \nunless the programmer explicitly writes the code to do so. The type system merely acts as a set of guard \nrails, ensuring that we do not accidentally follow any invalid pointers. Making data.ow nodes transient \nis a signi.cant departure from existing imperative implementation strategies for FRP libraries, such \nas Scala.React [31] or Racket s FrTime [9, 10]. In these libraries, data.ow nodes are persistent, and \nlast across many time steps, and clever heuristics are used to order updates. Abandoning this strategy \npays many dividends. We have already observed that accidental space leaks are no longer possible, but \nthere are also further practical bene.ts. A real implementation needs to garbage collect null nodes. \nFortunately, the structure of our typing rules ensures that well-typed program terms never contain locations \nthat outlive their tick, and as a result, the usual reachability heuristic of standard garbage collectors \nwill work unchanged. In contrast, persistent data.ow nodes need to manage dependen\u00adcies explicitly, and \nas a result, each data.ow cell knows both which cells it reads, and which cells read it. This bidirectional \nlinkage means that if one cell is reachable, all cells are reachable, defeating garbage collection unless \nspecial (and often expensive) measures such as weak references are used. Complete Programs Finally, we \nneed to say a word about what complete programs in our language are. Since we use an object\u00adcapability \nstyle to control the growth of the data.ow graph, a user program must be a function type, so that it \ncan receive a capability as an argument. Furthermore, since alloc is not a stable type, we will need \nto supply a fresh capability on each tick. For this reason, we take user programs to be closed terms \nof type S alloc . A. If e is such a term, then we will begin evaluation of the program with the call \ne (.x xs. cons(o, d\\(xs))). The term .x xs. cons(o, d\\(xs)) computes into a term which produces a stream \nyielding an allocation token o at each tick, but this term cannot itself be typed under our type system, \nModeling the fact that the runtime system of the language possesses certain capabilities that user programs \ndo not. As a result, our metatheory needs to be done in a semantic style, using a step-indexed Kripke \nlogical relation, rather than showing soundness through the usual syntactic progress and preservation \nlemmas. 3. Examples 3.1 Stream Functions Basic Examples We begin with the constant function on streams. \nIt takes a natural number as an input, and returns a constant stream of that number. const : S alloc \n. N . S N 1 const us n = 2 let cons(u, d(us 1)) = us in 3 let stable(x) = promote(n) in 4 cons(x, du(const \nus 1 x)) 5 In this example, we have a function const which receives a stream of permissions us to allocate, \nand a number n on line 2. On line 3, we take the head and tail of the permission stream, with a pattern-matching-style \nnested delay elimination to bind us 1 to  Types A ::= b | A \u00d7 B | A + B | A . B | A | \u00b5a. A | | | DA \n^S A alloc G f e : A q continued Terms e ::= fst e | snd e | (e, e 1) | inl e | inr e | case(e, inl x \n. e 1 , inr y . e 11) G f e : A later G f e 1 : alloc now | .x. e | e e 1 I 1 G f de 1 (e) : A now | \nde 1 (e) | let d(x) = e in e | stable(e) | let stable(x) = e in e 1 1 G f e : A now G, x : A later f \ne : C now | into e | out e 1 E | cons(e, e 1) | let cons(x, xs) = e in e 1 G f let d(x) = e in e : C \nnow | .x x. e | x | promote(e) | l | !l | o G f e : [ ( ^\u00b5a. A)/a]A now \u00b5I 1 1 G f into e : ^\u00b5a. A now \nValues v ::= (v, v ) | inl v | inr v | .x. e | l | o | stable(v) | into v | cons(v, v 1) G f e : ^\u00b5a. \nA now \u00b5E Stores s ::= \u00b7 | s, l : v now | s, l : e later | s, l : null G f out e : [ ( ^\u00b5a. A)/a]A now \nG f e : A stable Figure 1. Syntax DI G f stable(e) : DA now Quali.ers q ::= now | stable | later G f \ne : DA now G, x : A stable f e 1 : C now 1 DE Contexts G ::= \u00b7 | G, x : A q G f let stable(x) = e in \ne : C now (\u00b7) = \u00b7 G f e : A now A stable PRO M OT E (G, x : A later) = G , x : A now G f promote(e) \n: DA now (G, x : A stable) = G , x : A stable (G, x : A now) = G 1 G f e : A now G f e : (S A) now \n1 SI 0 G f cons(e, e ) : S A now (\u00b7)= \u00b7 0 0 (G, x : A stable)= G , x : A stable G f e : S A now 0 0 \n (G, x : A later)= G 1 G, x : A now, xs : (S A) now f e : C now 0 0 SE (G, x : A now)= G 1 G f let cons(x, \nxs) = e in e : C now Figure 2. Hypotheses, Contexts and Operations on Them 0 G ,x : A later f e : A \nnow FI X G f .x x. e : A now G f e : A q x : Aq q . {now, stable} HY P 1 G f x : A now G f e : A now \nG f e : B now G f (e, e 1) : A \u00d7 B now \u00d7I 0 G f e : A now G f e : A now T S TA B L E TLAT E R G f e \n: A stable G f e : A later G f e : A \u00d7 B now G f e : A \u00d7 B now \u00d7LE \u00d7RE G f fst e : A now G f snd e : \nB now Figure 4. Typing G f e : A now G f e : B now +L I +RI G f inl e : A + B now G f inr e : A + B now \nG f e : A + B now G, x : A now f e 1 : C now G, y : B now f e 11 : C now A stable 1 11 +E G f case(e, \ninl x . e , inr y . e ) : C now A stable B stable A stable B stable G, x : A now f e : B now .I A \u00d7 \nB stable A + B stable G f .x. e : A . B now b stable DA stable G f e : A . B now G f e 1 : A now 1 .E \nG f e e : B now Figure 5. Stability of Types Figure 3. Standard Typing Rules  s =. s1 (s; e) .s1 ; \nv (s; v) . (s; v) 1 1 11 (s; e1) .s ; v1s ; e2.s ; v2 (s; (e1, e2)) .s11; (v1, v2) 1 1 (s; e) .s ; \n(v1, v2)(s; e) .s ; (v1, v2) 1 1 (s; fst e) .s ; v1(s; snd e) .s ; v2 1 1 (s; e) .s ; v(s; e) .s ; \nv 1 1 (s; inl e) .s ; inl v(s; inr e) .s ; inr v 1 1 111 11 (s; e) .s ; inl vs ; [v/x]e .s ; v 1 11 \n11 11 s; case(e, inl x . e , inr y . e ).s ; v 1 1 1111 11 (s; e) .s ; inr vs ; [v/y]e .s ; v 1 11 \n11 11 s; case(e, inl x . e , inr y . e ).s ; v (s; e1) .s1 ; .x. e 11 1 11 11 1 111 s ; e2.s ; v2s \n; [v2/x]e1.s ; v (s; e1 e2) .s111 ; v s; e 1.s1 ; ol . dom(s1) (s; de 1 (e)) .(s1, l : e later); l \n 1 1 111 (s; e) .s ; ls ; [!l/x]e .s ; vl : v now . s s; let d(x) = e in e 1.s11 ; v(s; !l) . (s; v) \n (s; e) .s1 ; v(s; e) .s1 ; into v 1 1 (s; into e) .s ; into v(s; out e) .s ; v (s; e) .s1 ; v 1 (s; \nstable(e)) .s ; stable(v) (s; e) .s1; v 1 (s; promote(e)) .s ; stable(v) 1 1 111 11 (s; e) .s ; stable(v)s \n; [v/x]e .s ; v 111 11 s; let stable(x) = e in e .s ; v 1 1 111 1 (s; e) .s ; vs ; e .s ; v 1 11 1 \ns; cons(e, e ).s ; cons(v, v ) 1 1 111 11 (s; e) .s ; cons(v, l)s ; [v/x, l/xs]e .s ; v 111 11 s; let \ncons(x, xs) = e in e .s ; v 1 (s; [.x x. e/x]e) .s ; v 1 (s; .x x. e) .s ; v Figure 6. Expression Semantics \n 11 11 11 s =. s s ; e.s ; vl . dom(s ) \u00b7 =. \u00b7 s, l : e later =. s11, l : v now 11 11 s =. s l . dom(s \n) s =. s l . dom(s ) s, l : v now =. s1, l : null s, l : null =. s1, l : null Figure 7. Tick Semantics \na later variable. On line 4, we make use of the fact that natural numbers are stable in order to rebind \nn to the stable variable x. This lets us refer to x in both the head and tail of the cons-cell on line \n5. The head of the cons cell is just x itself, and the tail is a delayed stream, which we may allocate \nsince we have a permission u. Below, we give a summation function, which takes a stream of numbers and \nreturns a stream containing the cumulative sum of the stream. To implement this, we introduce an auxiliary \nfunction sum acc which stores the sum in an accumulator variable, and then call it with an initial sum \nof 0. sum acc : S alloc . S N . N . S N 1 sum acc us ns acc = 2 let cons(u, d(us 1)) = us in 3 let cons(n, \nd(ns 1)) = ns in 4 let stable(x) = promote(n + acc) in 5 cons(x, du(sum acc us 1 ns 1 x)) 6 sum : S alloc \n. S N . S N 7 sum us ns = sum acc us ns 0 8 Higher-Order Stream Operations Our language permits the free \nuse of streams of streams, as well as higher-order functions on streams. We illustrate this with a simple \nfunction which takes a stream, and returns the stream of successive tails of that stream. tails : S alloc \n. S A . S (S A) 1 tails us xs = 2 let cons(u, d(us 1)) = us in 3 let cons(x, d (xs 1)) = xs in 4 cons(xs, \ndu(tails us 1 xs 1)) 5 The higher-order map functional is de.nable as follows: map : S alloc . D(A . \nB) . S A . S B 1 map us h xs = 2 let cons(u, d(us 1)) = us in 3 let cons(x, d(xs 1)) = xs in 4 let stable(f) \n= h in 5 cons(f x, du(map us 1 stable(f) xs 1)) 6 Note that the map functional calls its argument function \non every element of the input stream. As a result, we need to use the function at many different time \nsteps, and so we need to give the functional argument the stable type D(A . B) to ensure that we can \nsafely use it both now and later. The fact that all computable streams are de.nable in our language is \nwitnessed by giving the unfold operation, which shows that the universal property of streams is de.nable \nwithin the language. unfold : S alloc . D(X . A \u00d7 X) . X . S A 1 unfold us h x = 2 let cons(u, d(us 1)) \n= us in 3 let stable(f) = h in 4 let (a, d(x 1)) = f x in 5 cons(a, du(unfold us 1 stable(f) x 1)) 6 \n Dynamic Changes of Streams Switching behavior is directly programmable in our language. The swap function \nbelow takes a number n, and two streams xs and ys. It yields the same values as xs on the .rst n time \nsteps, and afterward the same values as ys. swap : S alloc . N . S A . S A . S A 1 swap us n xs ys = \n2 if n = 0 then 3 ys 4 else 5 let cons(u, d(us 1)) = us in 6 let cons(x, d(xs 1)) = xs in 7 let cons(y, \nd(ys 1)) = ys in 8 let stable(m) = promote(n) in 9 cons(x, du(swap us 1 (m - 1) xs 1 ys 1)) 10 What is \ninteresting about this example is that there is nothing surprising about it: it is basically an ordinary \nfunctional program. Unlike many other approaches to reactive programming, we program conditional behavior \nwith ordinary conditional control .ow.  3.2 Type Encodings Streams In order to make the examples more \nreadable, we in\u00adcluded streams as a primitive type in our language, but that is actu\u00adally unnecessary, \nsince they are encodable using recursive types: S A . ^ \u00b5a. A \u00d7 a This encoding looks completely conventional, \nbut because unfolding a recursive type requires guarding the recursive type with a delay modality before \nsubstituting, this type is isomorphic to: S A . A \u00d7 (A \u00d7 (A \u00d7 . . .)) From a logical perspective, the \nstream type S A corresponds to the always operator of temporal logic, which asserts that at every time \nstep, we always have a witness to the proposition A. Events In reactive programs, there are often operations \nwhich take a length of time observable to the user. For example, downloading a .le is an operation which \ncan take many ticks to happen, and the .le value is not available for use until the operation is complete. \nSuch operations, which we call events , are surprisingly dif.cult to model in a stream-based paradigm, \nsince nothing happens until the completion of the process, and once the .nal event happens, no more events \noccur. Stream-based languages have used a number of tricks to encode behaviours like this, but when we \nhave a temporal recursive type, it is straightforward to de.ne a type of events: E A . ^ \u00b5a. A + a Here, \nwe simply replace the product with a sum, which means that an element of type A is either immediately \navailable, or we have to wait until the next tick to try again. (That is, E A . A + (E A). )Events make \nimplementing dynamic switching behavior very natural. switch : S alloc . S A . E (S A) . S A 1 switch \nus xs e = 2 let cons(u, d(us 1)) = us in 3 let cons(x, d(xs 1)) = xs in 4 case(out e, 5 inl ys . ys, \n6 inr t . let d(e 1) = t in 7 1 11 cons(x, du(switch us xs e ))) 8 In this example, the call switch us \nxs e behaves like xs until the event e returns a stream ys, and then it behaves like ys. Events correspond \nto the eventually operator of temporal logic. Since the eventually operator forms a closure operator \non the Kripke structure of times, the event type constructor correspondingly forms a monad, whose monadic \noperations may be de.ned as follows: return : A . E A 1 return x = into (inl x) 2 bind : S alloc . D(A \n. E B) . E A . E B 3 bind us h e = 4 let cons(u, d(us 1)) = us in 5 let stable(f) = h in 6 case(out e, \n7 inl a . f a, 8 inr t . let d(e 1) = t in 9 into (inr (du(bind us 1 stable(f) e 1)))) 10 Here, we perform \nsequencing by taking a function that maps elements of A to B events, and waiting until an A-event yields \nan A to apply the function. Because we do not know when we will need to invoke the function, bind requires \nit to be stable. Events are closely related to promises and futures [4, 16], in that they are proxies \nfor computations which have not yet completed constructing a value. However, unlike most implementations \nof futures, our type E A permits clients to test whether or not the computation has .nished. Because \nwe embed our events into a synchronous programming language, this choice does not introduce nondeterminism \ninto our programming language. Until Just as the always and eventually operators in temporal logic can \nbe merged into the until operator, we can also give a combined operation for processes that produce values \nof type A until they terminate, producing a value of type B. A U B . ^\u00b5a. B + (A \u00d7 a) Note that streams \ncorrespond to producing elements of A until the empty type, S A . A U 0, and events correspond to yielding \nunits until B, E B . 1 U B. Resumptions The fact that we have function spaces and arbitrary recursive \ntypes enables us to go well beyond the expressive power of temporal logic. In this example, we will illustrate \nthis by showing how resumptions [37], a simple interleaving model of concurrency, may be encoded in our \ntype system. \u00b5a. a \u00d7 (A + (I . O \u00d7 a)) RI,O A . ^ Elements of the type RI,O A can be thought of as representations \nof thread values, in an interleaving model of concurrency. The left\u00adhand-side of the pair a \u00d7 (A + (I \n. O \u00d7a)) can be seen as what to do when the thread is not executed on this tick, and the right-hand\u00adside \nsays that either the thread .nishes execution with a value of A, or it takes an input message of type \nI and yields an output message of type O, and continues computing. Given two threads, we can implement \na scheduler which takes two threads and interleaves their execution until one of them .nishes. par : \nS alloc . RI,O A \u00d7 RI,O A . RI,O A 1 par us (p1, p2) = 2 let cons(u, d(us 1)) = us in 3 case((snd (out \np1), 4 inl a . p1, 5 inr f . let d(p1 1 ) = fst (out p1) in 6 let d(p2 1 ) = fst (out p2) in 7 1 111 \nlet p = du(par us p1 p2) in 8 let f1 = .i. let (o, d (p1 11)) = f i in 9 1 1 11 into (o, du(par us p2 \np1 )) in 10 into (p 1 , inr f1)) 11 One line 5, we see that if the .rst process p1 completes on this \ntick, then the parallel composition completes on this tick. If p1 does not complete, then we need to \nreturn (1) how to defer the parallel composition, and (2) the I/O behavior of the parallel composition. \nWe construct the deferred computation for the parallel composition by taking (on lines 6-8) the deferred \ncomputations for each process individually, and then resuming their parallel composition on the next \ntick. On line 9-10, we de.ne the function f1, which produces the same I/O as p1 on this tick, defers \np2 to the next tick, and then on the next tick schedules p2 for execution and defers p1 s next action. \n De.nability of Fixed Points Our calculus has a term-level .xed point operator .x x. e. However, .xed \npoints are de.nable, illus\u00adtrating the high expressiveness that guarded recursive types per\u00admit. We will \nshow the inhabitation of the type D( A . A) . S Alloc . A, in three steps. First, we de.ne the recursive \ntype X ^ \u00b5a. D(S alloc . a . A). We use this type to de.ne the op\u00aderation selfapp, which takes an element \nof type X and applies it to itself, wrapped around a call to a function f : A . A: selfapp : ( A . A) \n. S alloc . X . A 1 selfapp f us v = 2 let cons(u, d(us 1)) = us in 3 let stable(w) = out v in 4 f (du(w \nus 1 (into (stable w)))) 5 Next, we can use this self-application function to implement a .xed\u00adpoint \ncombinator. .xedpoint : D( A . A) . S alloc . A 1 .xedpoint h us = 2 let stable(f) = h in 3 selfapp f \nus (into (stable(selfapp f))) 4 This .xed point operator is essentially a variant of Curry s .xed point \ncombinator [11] Y .f. (.x. f (x x)) (.x. f (x x)), with extra noise to deal with the modal operators \nand iso-recursive types. The most signi.cant difference is that we need to additionally pass in a stream \nof allocation tokens to ensure that we can construct the necessary delay thunks.  3.3 Blocking Space \nLeaks Without Ruling Out Buffering Our operational semantics makes it impossible to implicitly retain \nvalues across multiple time ticks, and our type systems statically rejects programs which try. For example, \nif we try to program a function which takes a stream and returns that stream constantly, then it fails \nto typecheck, as we desire: scary const : S alloc . S N . S (S N) 1 scary const us ns = 2 let cons(u, \nd(us 1)) = us in 3 let stable(xs) = promote(ns) in TYPE ERROR 4 cons(xs, du(scary const us 1 xs)) 5 \nThe reason for this error is that we need to use the argument stream at multiple times, and since streams \nare not a stable type, we cannot promote them into a stable variable, which we need in order to use the \nstream value at multiple times. So we get a compile-time error. We blocked this function de.nition because \nimplementing it would require potentially unbounded buffering, and we do not want to do that implicitly, \nsince that would mean that variable references could create unexpected memory leaks. However, there are \nmany legitimate programs which need to retain data across multiple time steps: for example, we may wish \nto retain data to compute a moving average. Our language does not prohibit these kinds of programs; instead, \nit demands that programmers explicitly write all the buffering code. As an extreme (and somewhat ridiculous) \nexample, we will write the function scary const, which takes a stream argument and repeats it constantly. \nbu.er : S alloc . N . S N . S N 1 bu.er us n xs = 2 let cons(u, d(us 1)) = us in 3 let cons(x, d(xs 1)) \n= xs in 4 let stable(x 1) = promote(x) in 5 cons(n, du(bu.er us 1 x 1 xs 1)) 6 forward : S alloc . S \nN . (S N) 7 forward us xs = 8 let cons(u, d(us 1)) = us in 9 let cons(x, d(xs 1)) = xs in 10 let stable(x \n1) = promote(x) in 11 du(bu.er us 1 x 1 xs 1) 12 scary const : S alloc . S N . S (S N) 13 scary const \nus xs = 14 let cons(u, d(us 1)) = us in 15 let d(xs 1) = forward us xs in 16 cons(xs, du(scary const \nus 1 xs 1)) 17 In this example, we use a function bu.er, which appends a natural number to the head \nof a stream, and then use bu.er to de.ne a function forward, which pushes its argument one tick into \nthe future, and then de.ne scary const, which repeatedly calls forward to keep moving the argument one \ntick into the future. This de.nition makes the memory leak explicit in the source code: our program repeatedly \ncalls forward on the argument stream to scary const, using more memory each time. In general, it is possible \nto de.ne buffering for a recursive type ^\u00b5a. A, if the expression A is constructed from the variable \na, sums of bufferable types A + B, products of bufferable types A \u00d7 B, any stable type DA, or delays \nof bufferable types A. It is not possible to buffer arbitrary members of function types A . B, because \nthe environment of a function closure cannot be examined. 4. Metatheory Overview Kripke logical relations \nhave a long history in giving semantics to higher-order stateful languages [2, 13, 36]. Since our dynamic \ndata.ow graph can be viewed as a store, they are a natural tool for showing the soundness of our type \nsystem. The basic idea behind the technique of logical relations is to give a family of sets of closed \nterms [A] by induction on the structure of the type A, each of which possesses the soundness property \nwe desire. Then, we prove a theorem (the fundamental property) that shows that every well-typed term \ne : A lies within the set [A], and from that we can conclude that the language is sound. In a Kripke \nlogical relation, in addition to the type, the relations are also indexed by some contextual information, \nthe world, which is used to relativize the interpretation of each type. In our setting, this contextual \ninformation will include the store terms are to evaluate under, as well as the permission information \ntelling us whether the term may extend the data.ow graph. Recursive types make de.ning relations by induction \non the syntax of a type dif.cult, since semantically we expect a recursive type to be de.ned in terms \nof its unfolding, and unfolding a recursive type can make it larger. To resolve this issue, we make use \nof the idea of step-indexing, originally introduced by Appel and McAllester [3], in which we extend the \nworld with a natural number n, and interpret a recursive type only at strictly smaller numbers. In this \nsection, we give a high-level overview of our de.nitions, and a brief tour of the soundness proof. We \ngive the full proof in the technical report [25] provided in the supplementary material. Supportedness \nand Location Permutations Before we can de\u00adscribe the structure of our logical relation, there is one \ntechnical issue we need to discuss. The allocation rule in the expression se\u00admantics is non-deterministic \n it chooses a location that is not in the current heap. However, the tick semantics s =. s1, when given \n e s e s free locations of e . dom(s) s supported s supported v s \u00b7 supported s, l : v now supported \ns supported e s s supported s, l : e later supported s, l : null supported Figure 8. De.nition of Supportedness \na nonempty store, removes the most recently-allocated location l from the store, updates the older heap, \nand then updates the removed location. The technical question is: what happens if the update of the older \nheap allocates l itself? Intuitively, this is not a serious problem, since our allocator could always \nhave chosen a different location to have allocated. To formalize this intuition, we introduce the idea \nof supportedness, described in Figure 8. We write e s if all of the free locations in e are in the domain \nof s. We write s supported, when every pointer containing an expression or value is supported by the \nheap cells allocated earlier (that is, to the left in the list). Then, we can prove the following three \nlemmas. Lemma 1 (Permutability). We have that: 1. If p . Perm and (s; e) . (s1; v) then (p(s); p(e)) \n. (p(s1); p(v)). 2. If p . Perm and s =. s1 then p(s) =. p(s1). Lemma 2 (Supportedness). We have that: \n1. If s supported and e s and (s; e) . (s1; v) then v s1 and s1 supported. 2. If s supported and s =. \ns1 then s1 supported.  Lemma 3 (Quasi-determinacy). We have that: 1. If (s; e) . (s1; v 1) and (s; e) \n. (s11; v 11) and s supported and e s, then there is a p . Perm such that p(s1) = s11 and p(s) = s. \n2. If s =. s1 and s =. s11 and s supported, then there is a p . Perm such that p(s1) = s11 and p(s) = \ns.  Here, Perm is the set of .nite permutations on locations, and p(e) and p(s) lift it to expressions \nand stores in the obvious way. Together, these lemmas imply that we can rename locations however we like, \nand that the nondeterminism of the allocator can only affect how locations are named. Kripke Worlds We \nnow describe the structure of the worlds we use in our logical relation, which we lay out in Figure 9. \nA world w is a triple (n, s, a). Here, n is a step index, indicating that an element of this relation \nmust be good for at least n ticks into the future. The term a is a capability. The capability T means \nthat we do not have the permission to extend the data.ow graph, and . means that we do have the permission \nto extend the graph. Finally, the store s must be an element of Heapn, which is the set of supported \nheaps for which the tick relation is de.ned for at least n steps. That is, if s . Heapn, then there are \ns1, . . . , sn such that s =. s1 =. \u00b7 \u00b7 \u00b7 =. sn. As a notational shorthand, we write w.n for the step \nindex of w, w.s for the store component of w, and w.a for the capability of w. We also write p(w) to \nmean the world (w.n, p(w.s), w.a). In most applications, step-indexing has been used purely as a technical \ndevice to force the inductive de.nition of types to be well\u00adfounded. In our setting, in contrast, steps \nhave a concrete operational reading, corresponding directly to the passage of time: a step index of n \ntells us that the tick relation can de.nitely tick at least n times. Each of these components has an \nassociated preorder =. A step index n 1 is below an index n, if n 1 is less than or equal to n. A heap \ns1 is below a heap s, s1 = s, if s1 is an extension of s that is, if s1 has a larger domain than s, \nand agrees with it on the overlap. Finally, a capability a 1 is below a, if either they are the same, \nor a 1 is . and a is T. The intuition behind the order on worlds is that w 1 = w when w 1 is a possible \nfuture state of w. In the future of w, we may have extended the data.ow graph, or we can potentially \nreceive a capability to allocate from our environment. The Logical Relations In Figure 10, we de.ne three \nlogical relations. The set V [A] . w de.nes the value relation, the set of closed values semantically \ninhabiting the type A at the world w, with the parameter . giving the type interpretation of each of \nthe free variables in A. The set E [A] . w gives the expression relation, the set of expressions semantically \ninhabiting the type A (that is, they will evaluate to a value of type A if they are run on the current \ntick). Similarly, we also de.ne L [A] . w, the later expression relation. These are the expressions that \nwill evaluate to a value of type A if they are run on the next time step. Both the expression relation \nE [A] . w and the later relation L [A] . w are de.ned in terms of the value relation. The expression \nrelation consists of closed, supported expressions, which evaluate to values in the value relation. Furthermore, \nthe expression evaluation may extend the heap only if the world contains the capability to allocate. \nIf it does not, then the store will be untouched. The later relation L [A] . w is de.ned by cases. If \nthe world s step index is 0, then we place no constraints on it it is simply the set of closed, supported \nexpressions. If the world s step index is n + 1 and the tick relation sends the current world s store \nto s1, then terms are in L [A] . w if they are in the expression relation of A, at step n and store s1. \nThat is, it consists of those expressions that will be in the expression relation on the next tick. The \nvalue relation is de.ned by induction on the syntax of the type A. Matters .rst become interesting in \nthe function case. First, we require that a lambda term .x. e inhabiting V [A . B] . w be supported with \nrespect to the heap component of the world. As is usual, we quantify over all future worlds w 1 = w, \nbut we also quantify over location permutations p . Perm. Then, we consider all arguments e 1 coming \nfrom the A expres\u00adsion relation at the permuted world p(w 1), and assert that applying the term to the \nrenamed function should also be in the B expression relation at p(w 1). The extra renaming requirement \nsemantically formalizes the idea that we need to ignore the exact choice of names (and indeed is very \nsimilar to the de.nition of the function space in nominal set theory [17]). The later type A is interpreted \nas the set of pointers l, which point to an expression l : e later where e is in the later relation of \nA. As with functions, we quantify over future worlds and renamings. The interpretation of the stability \nmodality V [DA] . w contains values stable(v), where v . V [A] . (w.n, \u00b7, T). That is, these values v \nare not allowed to depend upon the store, or to assume that they have the capability to extend the heap. \nRecursive types ^ \u00b5a. A are interpreted by the interpretation of A, where the environment is extended \nby the interpretation of (^\u00b5a. A). Super.cially, this looks like a circular de.nition, except that the \nnext-step modality is de.ned in terms of the later relation for ^ \u00b5a. A,  There remain a few features \nof the implementation which we Heap0  = {s . Store | s supported} s supported . have not completely \nformalized. First, the operational semantics in this paper forces every thunk when time elapses. Our \nactual Heapn+1 =s . Store .s1. s =. s1 . s1 . Heap n implementation is lazier about this, only forcing \nthunks when they Cap = {T, .} World = {(n, s, a) | n . N. s . Heapn . a . Cap} s1 = s .. .s0. s \u00b7 s0 \n= s1 a 1 = a .. a = a 1 . (a 1 = . . a = T) (n 1, s 1 , a 1) = (n, s, a) .. n 1 = n . s1 = s . a 1 = \na Figure 9. Worlds which lowers the step index, making the de.nition well-founded. Finally, the semantic \ninterpretation of alloc is the token o, if the world has the capability, and is the empty set otherwise. \nSoundness The key property we prove is the following: Theorem 1 (Fundamental Property). The following \nproperties hold: 1. If \u00b7 f e : A later, then e . L [A] \u00b7 w. 2. If \u00b7 f e : A stable, then e . E [A] \u00b7 \n(w.n, \u00b7, T). 3. If \u00b7 f e : A now, then e . E [A] \u00b7 w.  The proof of this theorem follows the usual \npattern for sound\u00adness proofs by logical relations. We extend the de.nition of the expression relation \nto de.ne environments binding expressions to variables, and then prove by induction on the syntax of \nthe typing derivation that all substitutions of well-typed terms by well-formed environments are in the \nlogical relation. As usual, a fair number of auxiliary lemmas must be proved (such as the monotonicity \nof the value relation, and the stability of all relations under renaming of lo\u00adcations), and we refer \ninterested readers to the companion technical report [25] for details. Once we have the fundamental property, \nwe get a soundness property as an almost immediate corollary. Corollary 1. (Soundness) If \u00b7 f e : A , \nthen (\u00b7; e) . (s; v). Furthermore, for all n, v . V [A] . (n, s, .). Note that expression evaluation \nalways terminates. This shows that our type discipline enforces well-founded use of .xed points .x x. \ne. Also, note that s . Heapn for any n, and so we can tick the clock as often as we like. Thus, if we \ncompute a stream value, then each time we tick the clock, the tail pointer will point to a new cons cell, \nwhose head contains the next value of the stream, and whose tail is the pointer to chase on the next \ntick. 5. Implementation While the work described here is largely theoretical, that theory arises from \nan attempt to understand the correctness of a new language, AdjS (the implementation can be downloaded \nfrom the author s website), whose type system closely tracks the type system of this paper. (The major \nextensions are polymorphism and linear types, and a system to infer uses of the promote(e) operation.) \nThe attempt to prove the soundness of the type system was fortuitous: in the course of the formalization \nwe discovered two soundness bugs in our implementation! The .rst soundness bug was that the AdjS compiler \nhas both delay types A and S A as primitive, but only required an allocation token to construct a stream. \nThe second soundness bug was that we had originally treated the allocation type alloc as a promotable \nstable type. Each of these bugs made it possible to build a value of type D( A), and use it to access \na thunk after its lifetime has ended. are read. It should still be the case that no thunk is forced, \nexcept on its scheduled tick, but it is additionally possible for thunks to go unforced. This is not \ndif.cult to model, but it seemed to complicate the de.nition of Kripke extension without suf.cient expository \nadvantages to compensate. More seriously, we have not given a correctness proof of our implementation \nof linear types. While we do have both denotational and type-theoretic models of linear guarded types \n[26], we do not yet have a soundness proof of our implementation strategy for it, in the same way that \nthis paper shows the soundness of our implementation strategy of the functional part of the language. \nThis is because the linear types are used to model GUI widgets, and we need a plausible, yet tractable, \noperational semantics for the GUI widgets. This does not seem impossibly out of reach, though: recently \nLerner et al. [29] have given a formal model of the HTML DOM, and we are currently investigating using \na simpli.ed version of their event model to build the operational semantics we need. In order to make \nthe synchrony hypothesis (namely, that all computations .nish quickly relative to the size of a tick), \nour implementation runs at a .xed clock speed of 60 Hz. This does mean that the runtime wakes even when \nnothing is happening. However, we do not foresee many issues in increasing the clock speed, since Haskell \nimplementations demonstrate that it is possible to sustain very high rates of thunk allocation. In contrast, \nElliott [14] gives an implementation of streams using futures. In our terms, his stream type S A is \u00b5a. \nA \u00d7 E a, where the recursive type is guarded by an event constructor instead of a unit delay. This means \nthat each stream can run at a different rate, permitting the system to quiesce until events become available, \nat the cost of complicating the merging of streams. 6. Related Work Implementing Reactive Programming \nDSLs for reactive pro\u00adgramming languages tend to fall into one of two camps. The purist camp, such as \nYampa [35], typically features fairly simple imple\u00admentation strategies and limited dynamic behaviour, \nwhich fairly closely track the semantic model of stream programming. The prag\u00admatist camp, such the Froc \nlibrary for Ocaml [12], FrTime [10], and Scala.React [31], feature more sophisticated implementations \nbased on data.ow engines and change propagation, and better support for dynamic behavior. Now, it has \nlong been recognized that there are connections be\u00adtween lazy evaluation and the synchronous data.ow \nparadigm [6], but the precise relationship between the two semantics has been unclear to date. Our semantics \nclari.es this connection, by decom\u00adposing streams into a recursive type over the next-step modality of \ntemporal logic. We then show that thunking and lazy evaluation can be used to give realizers for the \nnext-step modality, and that the synchrony assumption (that is, a global notion of time) enables scheduling \nwhen these thunks are forced. In other words, we show that two standard functional programming evaluation \nstrategies call-by-value and call-by-need jointly supply all the computational primitives well-behaved \nreactive programs need. We can also clarify what parts of the sophisticated implemen\u00adtations used by \nthe pragmatist languages are necessary, and which parts are optimizations. These languages typically \nwork by building a graph of data.ow nodes (which might be thought of as a kind of generalized spreadsheet), \nand incrementally recomputing values when node values change. The recomputations are guided by the dependency \nstructure of the data.ow graph, often using quite so\u00ad  V a] . w V A + B .w V [ A \u00d7 B] . w V [A . B] \n. w V A] . w V DA] . w V ^ \u00b5a. A] . w V S A] . w V [ alloc] . w E [A] . (n, s, a) L A . (0, s, a) = \n.(a) w = {inl v | v . V [A] . w} . {inr v | v . V [B] . w} = {(v1, v2) | v1 . V [A] . w . v2 . V [B] \n. w} .x. e w.s . = .x. e 11 1 .p . Perm, w 1 = w, e 1 . E [A] . p(w ). [e /x]p(e) . E [B] . p(w ) = \n{l | w.s = (s0, l : e later, s1) . .p . Perm, w 1 = (w.n, s0, w.a). p(e) . L [A] . p(w 1)} = {stable(v) \n| v . V [A] . (w.n, \u00b7, T)} = {into v | v . V [A] (., V [ (^\u00b5a. A)] . w/a) w} = {cons(v, v 1) | v . V \n[A] . w . v 1 . V [ S A] . w} = {o | w.a = .} . . . e s . . = e , v . V [A] . (n, s11 , a). .s1 = s. \n.s11 = s1 . (s1; e) . (s11; v) . (a = T =. s11 = s1) . = {e . Expr | e s} L [ A] . (n + 1, s, a) = \n{e . Expr | e s . s =. s1 . .w 1 = (n, s1 , a). e . E [A] . w 1} Figure 10. The Logical Relation phisticated \ntechniques: for example, Froc is explicitly based on the implementation strategies in self-adjusting \ncomputation [1]. Our decomposition also clari.es the semantics of imperative implementation techniques \nof FRP, by showing how imperative state supports a purely functional surface language. Again, the decomposition \nof streams via recursive types and the later modality is crucial. Imperative state is essential, since \nit lets us implement the later modality with memoization, and thereby avoid redundant multiple recomputations \nwhen we reference a later variable multiple times. However, persistent state with identity is not necessary \nto implement higher-order FRP, and abandoning it enables signi.cant simpli.cations over the data.ow graph \nstrategy. In a traditional data.ow graph, streams are primitives, and represented by data.ow nodes with \npersistent identity. This greatly complicates correctness proofs: in [27], we gave such a representation \nModeling pure streams with data.ow nodes, and were forced to give a vastly more complex logical relation \nto account for the persistence of data.ow nodes. Logics of Time and Space To our knowledge, Sculthorpe \nand Nilsson [41] .rst suggested using temporal logic to verify FRP programs. There, they gave a standard \n(albeit dependently-typed) arrowized FRP system, and temporal logic was used to describe the behavior \nof these programs. Jeffrey [19] and Jeltsch [22] were the .rst to suggest using LTL directly as a type \nsystem for FRP. Jeltsch [21] proposes a design for a Haskell FRP library with some similarities with \nour approach. As in our approach, streams are viewed as a kind of generator which incrementally produce \nvalues. However, instead of using temporal modalities to control when streams are used, the types of \nstreams are indexed with polymorphic type parameters ( era parameters ) in the style of Haskell s runST \noperation. Though no correctness proof is given, the use of polymorphism suggests that the techniques \nof Jeffrey [20] may be applicable. The Modal \u00b5-calculus Our use of recursive types to model tem\u00adporal \noperations naturally invites comparisons to the modal \u00b5\u00adcalculus [24]. The \u00b5-calculus takes the propositional \ncalculus, adds a next-step modality, and adds constructors for inductive and coin\u00adductive formulas. The \nbiggest difference is that the \u00b5-calculus restricts recursive variables to occur in strictly positive \nposition, but places no guarded\u00adness condition on those variables. In contrast, our notion of recursive \ntype is drawn from Nakano [34], who uses a guardedness condi\u00adtion instead of a positivity condition, \nallowing variables to occur negatively as long as they occur underneath a delay operator. Permitting \nnegative occurrences is extremely powerful: as we have seen, term-level .xed points can be de.ned using \nrecursive types with negative occurrences. The de.nability of .xed points in turn means that guarded \nrecursive types enjoy a form of limit\u00adcolimit coincidence similar to the same property in domain theory. \nIn type-theoretic terms, inductive and coinductive interpretations of a guarded recursive type coincide. \nThis means that it is not possible to distinguish may (coinductive) and must (inductive) properties: \nfor example, our event type constructor says that an event may occur, but does not say it will necessarily \noccur. (Recently, Jeltsch [23] has investigated potential applications of must-operators to reactive \nprogramming.) Our choice to use guarded recursive types was guided by the nature of interactive programs: \nif we begin to download a .le, there is no way to be sure that the download necessarily completes (such \nas the wi. may go down). As a result, placing must-properties in types is perhaps philosophically arguable. \n(On a pragmatic note, expressing .xed points with guarded recursion leads to very natural and idiomatic \ncode.) Our stable type DA is not found in the modal \u00b5-calculus, but may be understood as a constructivization \nof the always modality. When passing from a model-theoretic semantics to a proof theory, it is often \nthe case that a single model-theoretic concept bifurcates into two or more proof-theoretic concepts. \nThe always modality exempli.es this: under a computational interpretation, always A can either be interpreted \nas a single value of type A which is always available (that is, stability DA), or as a different A on \neach tick (that is, streams S A). Very recently, Cave et al. [8] proposed a type-theoretic construc\u00adtivization \nof the modal \u00b5-calculus (without the DA modality). In particular, they use inductive and coinductive \ntypes instead of a Nakano-style recursive type, and use it to express fairness properties (for example, \nof schedulers) with types. While it is too early to make a detailed comparison (larger examples are needed), \nhaving calculi with both kinds of type recursion available seems like a valuable tool for understanding \nthe design space. Stability and Permissions Our stability judgment and promotion rule are inspired by \nthe mobility judgment in the distributed language ML5 [33]. We wanted to identify values we could use \nat multiple times, and they wanted to identity values they could use at multiple locations. Promotion \nis actually de.nable in our calculus, as a kind of eta-expansion. However, operationally these coercions \ntraverse the entire data structure, and so for ef.ciency s sake we added it as a primitive. Allocation \npermissions were introduced by Hofmann [18], to control memory allocation in a linearly-typed language. \nOur observation that in an intuitionistic setting, these tokens correspond to an object capability style \n[32] seems to be new, and potentially has applications beyond reactive programming. However, we do not \nyet have a full Curry-Howard explanation of allocation permissions.  Acknowledgments I would like to \nthank Derek Dreyer, Bob Harper, and the anonymous referees for helpful discussions and suggestions. References \n[1] U. A. Acar. Self-Adjusting Computation. PhD thesis, Carnegie Mellon University, 2005. [2] A. Ahmed, \nD. Dreyer, and A. Rossberg. State-dependent representation independence. In Principles of Programming \nLanguages (POPL), pages 340 353, 2009. [3] A. Appel and D. McAllester. An indexed model of recursive \ntypes for foundational proof-carrying code. ACM Transactions on Programming Languages and Systems (TOPLAS), \n23(5):657 683, 2001. [4] H. Baker and C. Hewitt. The incremental garbage collection of processes. In \nSymposium on Arti.cial Intelligence Programming Languages, August 1977. [5] G. Berry and L. Cosserat. \nThe ESTEREL synchronous programming language and its mathematical semantics. In Seminar on Concurrency, \npages 389 448. Springer, 1985. [6] P. Caspi and M. Pouzet. Synchronous Kahn networks. In Proceedings \nof the .rst ACM SIGPLAN international conference on Functional programming, ICFP 96, pages 226 238, New \nYork, NY, USA, 1996. ACM. ISBN 0-89791-770-7. . [7] P. Caspi, D. Pilaud, N. Halbwachs, and J. Plaice. \nLUSTRE: A declarative language for real-time programming. In Principles of Programming Languages (POPL), \n1987. [8] A. Cave, F. Ferreira, P. Panangaden, and B. Pientka. Fair reactive programming. Technical report, \nMcGill University, 2013. [9] G. H. Cooper. Integrating Data.ow Evaluation into a Practical Higher-Order \nCall-by-Value Language. PhD thesis, Brown University, 2008. [10] G. H. Cooper and S. Krishnamurthi. Embedding \ndynamic data.ow in a call-by-value language. In European Symposium on Programming (ESOP), pages 294 308, \n2006. [11] H. B. Curry. The Paradox of Kleene and Rosser. Transactions of the American Mathematical Society, \n50(3):454 516, Nov. 1941. [12] J. Donham. Froc: a library for functional reactive programming in OCaml. \nhttp://jaked.github.com/froc/, 2010. [13] D. Dreyer, G. Neis, and L. Birkedal. The impact of higher-order \nstate and control effects on local relational reasoning. In International Conference on Functional Programming \n(ICFP), pages 143 156, 2010. [14] C. Elliott. Push-pull functional reactive programming. In Haskell Sym\u00adposium, \n2009. URL http://conal.net/papers/push-pull-frp. [15] C. Elliott and P. Hudak. Functional reactive animation. \nIn International Conference on Functional Programming (ICFP), 1997. [16] D. Friedman and D. Wise. The \nimpact of applicative programming on multiprocessing. In International Conference on Parallel Processing, \npages 263 272, 1976. [17] M. J. Gabbay and A. M. Pitts. A new approach to abstract syntax with variable \nbinding. Formal Aspects of Computing, 13:341 363, 2001. [18] M. Hofmann. Linear types and non-size-increasing \npolynomial time computation. In Logic in Computer Science (LICS), 1999. [19] A. Jeffrey. LTL types FRP: \nlinear-time temporal logic propositions as types, proofs as functional reactive programs. In Programming \nLanguages Meets Program Veri.cation (PLPV), pages 49 60, 2012. [20] A. Jeffrey. Causality for free!: \nparametricity implies causality for functional reactive programs. In Programming Languages Meets Program \nVeri.cation (PLPV), pages 57 68, 2013. [21] W. Jeltsch. Signals, not generators! Trends in Functional \nProgramming, 10:145 160, 2009. [22] W. Jeltsch. Towards a common categorical semantics for linear-time \ntemporal logic and functional reactive programming. Electronic Notes in Theoretical Computer Science, \n286:229 242, Sept. 2012. [23] W. Jeltsch. Temporal logic with until , functional reactive program\u00adming \nwith processes, and concrete process categories. In Programming Languages Meets Program Veri.cation (PLPV), \npages 69 78, 2013. [24] D. Kozen. Results on the propositional \u00b5-calculus. Theoretical Computer Science, \n27(3):333 354, 1983. [25] N. R. Krishnaswami. Proofs for higher-order reactive programming without spacetime \nleaks (supplementary material). Technical report, Max Planck Institute for Software Systems (MPI-SWS), \n2013. [26] N. R. Krishnaswami and N. Benton. A semantic model for graphical user interfaces. In International \nConference on Functional program\u00adming (ICFP), pages 45 57, 2011. [27] N. R. Krishnaswami and N. Benton. \nUltrametric semantics of reactive programs. In Logic in Computer Science (LICS), pages 257 266, 2011. \n[28] N. R. Krishnaswami, N. Benton, and J. Hoffmann. Higher-order functional reactive programming in \nbounded space. In Principles of Programming Languages (POPL), pages 45 58, 2012. [29] B. S. Lerner, M. \nJ. Carroll, D. P. Kimmel, H. Q.-D. La Vallee, and S. Krishnamurthi. Modeling and reasoning about DOM \nevents. In Conference on Web Application Development (WebApps), 2012. [30] H. Liu, E. Cheng, and P. Hudak. \nCausal commutative arrows and their optimization. In International Conference on Functional Programming \n(ICFP), 2009. [31] I. Maier and M. Odersky. Deprecating the Observer Pattern with Scala.React. Technical \nreport, EPFL, 2012. [32] M. Miller. Robust composition: Towards a uni.ed approach to access control and \nconcurrency control. PhD thesis, Johns Hopkins University, 2006. [33] T. Murphy VII., K. Crary, and R. \nHarper. Type-safe distributed programming with ML5. In Conference on Trustworthy Global Computing (TGC), \npages 108 123, 2008. [34] H. Nakano. A modality for recursion. In Logic in Computer Science (LICS), pages \n255 266, 2000. [35] H. Nilsson, A. Courtney, and J. Peterson. Functional reactive program\u00adming, continued. \nIn ACM Haskell Workshop, page 64, 2002. [36] A. Pitts and I. Stark. Operational reasoning for functions \nwith local state. Higher Order Operational Techniques in Semantics, 1998. [37] G. D. Plotkin. A powerdomain \nconstruction. SIAM Journal of Computation, 5(3):452 487, 1976. [38] A. Pnueli. The temporal logic of \nprograms. In Foundations of Computer Science (FOCS), pages 46 57, 1977. [39] M. Pouzet. Lucid Synchrone, \nversion 3. Tutorial and reference manual. Universit \u00b4e Paris-Sud, LRI, 2006. [40] N. Sculthorpe and H. \nNilsson. Safe functional reactive programming through dependent types. In International Conference on \nFunctional Programming (ICFP), 2009. [41] N. Sculthorpe and H. Nilsson. Keeping calm in the face of change. \nHigher Order Symbolic Computation, 23(2):227 271, June 2010. [42] P. Wadler, W. Taha, and D. MacQueen. \nHow to add laziness to a strict language, without even being odd. In Workshop on Standard ML, 1998. [43] \nZ. Wan, W. Taha, and P. Hudak. Event-driven FRP. In Practical Applications of Declarative Languages (PADL), \npages 155 172, 2002.  \n\t\t\t", "proc_id": "2500365", "abstract": "<p>Functional reactive programming (FRP) is an elegant approach to declaratively specify reactive systems. However, the powerful abstractions of FRP have historically made it difficult to predict and control the resource usage of programs written in this style.</p> <p>In this paper, we give a new language for higher-order reactive programming. Our language generalizes and simplifies prior type systems for reactive programming, by supporting the use of streams of streams, first-class functions, and higher-order operations. We also support many temporal operations beyond streams, such as terminatable streams, events, and even resumptions with first-class schedulers. Furthermore, our language supports an efficient implementation strategy permitting us to eagerly deallocate old values and statically rule out spacetime leaks, a notorious source of inefficiency in reactive programs. Furthermore, these memory guarantees are achieved without the use of a complex substructural type discipline.</p> <p>We also show that our implementation strategy of eager deallocation is safe, by showing the soundness of our type system with a novel step-indexed Kripke logical relation.</p>", "authors": [{"name": "Neelakantan R. Krishnaswami", "author_profile_id": "81320491252", "affiliation": "Max Planck Institute for Software Systems (MPI-SWS), Saarbruecken, Germany", "person_id": "P4261248", "email_address": "neelk@mpi-sws.org", "orcid_id": ""}], "doi_number": "10.1145/2500365.2500588", "year": "2013", "article_id": "2500588", "conference": "ICFP", "title": "Higher-order functional reactive programming without spacetime leaks", "url": "http://dl.acm.org/citation.cfm?id=2500588"}