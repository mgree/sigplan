{"article_publication_date": "09-25-2013", "fulltext": "\n Correctness of an STM Haskell Implementation Manfred Schmidt-Schau\u00df David Sabel Goethe-University Frankfurt, \nGermany {schauss,sabel}@ki.cs.uni-frankfurt.de Abstract A concurrent implementation of software transactional \nmemory in Concurrent Haskell using a call-by-need functional language with processes and futures is given. \nThe description of the small\u00adstep operational semantics is precise and explicit, and employs an early \nabort of con.icting transactions. A proof of correctness of the implementation is given for a contextual \nsemantics with may\u00adand should-convergence. This implies that our implementation is a correct evaluator \nfor an abstract speci.cation equipped with a big\u00adstep semantics. Categories and Subject Descriptors D.3.1 \n[Programming Lan\u00adguages]: Formal De.nitions and Theory Semantics; D.3.2 [Pro\u00adgramming Languages]: Language \nClassi.cations Applicative (functional) languages; D.3.3 [Programming Languages]: Lan\u00adguage Constructs \nand Features Concurrent programming struc\u00adtures; F.3.2 [Logics and Meanings of Programs]: Semantics of \nProgramming Languages Operational semantics; F.1.2 [Compu\u00adtation by Abstract Devices]: Modes of Computation \nParallelism and concurrency General Terms Languages, Theory, Veri.cation Keywords Functional Programming, \nConcurrent Programming, Semantics, Software Transactional Memory, Haskell 1. Introduction Due to the \nrecent development in hardware and software, concur\u00adrent and parallel programming is getting more and \nmore important, and thus there is a need for semantical investigations in concurrent programming. In \nthis paper we investigate programming models of concurrent processes (threads) where several processes \nmay access shared memory. Concurrent access and independency of processes lead to the well-known problems \nof con.icting memory use and thus requires protection of critical sections to ensure mutual ex\u00adclusion. \nOver the years several programming primitives like locks, semaphores, monitors, etc. have been introduced \nand used to ensure this atomicity of memory operations in a concurrent setting. How\u00adever, the explicit \nuse of locking mechanisms is error-prone the programmer may omit to set or release a lock resulting \nin deadlocks or race conditions and it is also often inef.cient, since setting too Permission to make \ndigital or hard copies of all or part of this work for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for pro.t or commercial advantage and that copies bear \nthis notice and the full citation on the .rst page. Copyrights for components of this work owned by others \nthan ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post \non servers or to redistribute to lists, requires prior speci.c permission and/or a fee. Request permissions \nfrom permissions@acm.org. ICFP 13, September 25 27, 2013, Boston, MA, USA. Copyright c &#38;#169; 2013 \nACM 978-1-4503-2326-0/13/09. . . $15.00. http://dx.doi.org/10.1145/10.1145/2500365.2500585 many locks \nmay sequentialize program execution and prohibit con\u00adcurrent evaluation. Another obstacle of lock-based \nconcurrency is that composing larger programs from smaller ones is usually im\u00adpossible [6]. A recent \napproach to overcome these dif.culties is software transactional memory (STM) [6, 7, 16, 17] where operations \non the shared memory are viewed as transactions on the memory, i.e. several small operations (like read \nand write of memory lo\u00adcations) are compound to a transaction and then the system (the transaction manager) \nensures that the transaction is performed in an atomic, isolated, and consistent manner. I.e., the programmer \ncan assume that all the transactions are performed sequentially and isolated, while the runtime system \nmay perform them concurrently and interleaved and may even try sequences of actions several times (invisible \nto the programmer). This removes the burden from the programmer to set locks and to keep track of them. \nComposability of transactions is another very helpful feature of STM. We focus on the Haskell-based approach \nin [6, 7], which provides even more advantages like separation of different kinds of side effects (IO \nand STM) by monadic programming and the type system of Haskell. Memory-transactions are indicated in \nthe program by marking a sequence of actions as atomic. Though STM is easy to use for the programmer, \nimplement\u00ading a correct transaction manager is considerably harder. Hurdles are an exact speci.cation \nof the correctness properties, of course to provide an implementation, and to prove its correctness and \nthe validity of the properties. In this paper we will address and solve these problems for a functional, \nconcurrent language which models Concurrent Haskell extended by memory transactions. As a spec\u00adi.cation \ncalculus, we start with SHF (STM-Haskell with futures) that is rather close to STM-Haskell [6, 7] and \nshares some ideas with the concurrent call-by-need process calculus investigated in [12]. In contrast \nto STM-Haskell threads are modelled by futures (which are however implementable in Concurrent Haskell \nusing unsafeInterleaveIO in a safe manner [13]), SHF has no ex\u00adplicit exceptions, it uses call-by-need \nevaluation and bindings in the environment for sharing, and for simplicity it is equipped with a monomorphic \ntype system. We keep the monadic modelling for the separation of IO and STM and the composability, in \nparticular the selection composability by orElse. A big-step operational seman\u00adtics is given for SHF \n, which is obviously correct, since transactions are executed in isolation, and their effects on the \nshared memory are only observable after a successful execution. However, this seman\u00adtics is not implementable, \ndue to undecidable execution conditions in the big-step semantics. Thus the purpose of de.ning SHF stan\u00addard \nreduction is not the implementation but the speci.cation of a correct interpreter for SHF . Secondly, \nwe de.ne a concurrent implementation of SHF by introducing the concurrent calculus CSHF . CSHF is close \nto a real implementation, since its operational semantics is formulated as a detailed, precise and complete \nsmall-step reduction seman\u00adtics where all precautions and retries of the transactions are explic\u00aditly \nrepresented. I.e., the small-step reductions are de.ned with an appropriate granularity to make them \nimplementable. Features of CSHF are registration of threads at transactional variables (TVars) and forced \naborts of transactions in case of con.icts. All applicabil\u00adity conditions for the reductions are decidable. \nCSHF is designed to enable concurrent (i.e. interleaved) execution of threads as much as possible, i.e. \nthere are only very short phases where internal lock\u00ading mechanisms are used to prevent race conditions. \n We also implemented CSHF as a prototypical library in Haskell1 , which provides the same core interface \nas Haskell s STM implementation2 , and is (in contrast to CSHF ) polymorphically typed. Internally it \nperforms the steps as they are de.ned by the small-step semantics of CSHF . Our implementation behaves \nas expected and thus gives evidence of the correct design of CSHF. The main goal of our investigation \nis to show that the con\u00adcurrent implementation ful.lls the speci.cation. Here we use the strong criterion \nof contextual equivalence w.r.t. may-as well as should-convergence in both calculi, where may-convergence \nmeans that a process has the ability to evaluate to a successful process, and should-convergence means \nthat the ability to become success\u00adful is never lost on every reduction path. Observing also should\u00adconvergence \nfor contextual equivalence ensures that it is not per\u00admitted to transform a program P that cannot reach \nan error-state (i.e. a state that is not may-convergent) into a program that can reach an error-state. \nCompared with must-convergence, which re\u00adquires termination on every computation path, should-convergence \nin combination with may-convergence judges busy-wait as non\u00aderroneous behavior, and also contextual equivalence \nis invariant un\u00adder restricting reductions to fair ones (see e.g. [10, 12]). Compared with de.nitions \nof contextual equivalences that include evaluation traces, using may-and should-convergence provides \na coarser def\u00adinition of equivalence. In Main Theorem 4.3 we obtain the important result that the im\u00adplementation \nmapping . is observationally correct [15], i.e. for ev\u00adery context D and process P in SHF it holds: D[P \n] may-converges (or should-converges, resp.), if, and only if .(D)[.(P )] is may\u00adconvergent (should-convergent) \nin CSHF . Observational correct\u00adness thus shows that may-and should-convergence is preserved and re.ected \nby the implementation . (i.e. . is convergence equiva\u00adlent) and the tests used for contextual equivalence \nare translated in a compositional way. A direct consequence is also that . is ade\u00adquate, i.e. the implementation \npreserves all contextual inequalities and thus does not introduce new equalities (which would introduce \nconfusion in the implementation). Note that even the proof of con\u00advergence equivalence of . is a strong \nresult, since it implies that CSHF is a correct evaluator for SHF . Our notion of correctness is rather \nstrong, since it implies the properties of the concurrent program that are aimed at in other pa\u00adpers \nlike atomicity, opacity, asf. A surprising insight is that early abortion of con.icting transactions \nis necessary to make the imple\u00admentation correct, where early means that a committing trans\u00adaction must \nabort other con.icting transactions. This equivalence of SHF and CSHF has the following consequences: \nThere is a guarantee that error-free SHF -programs are mapped to error-free CSHF -ones; error-freeness \nof a concurrent program means that no execution possibility leads to a state which is a dead end insofar \nas success can not be reached from this state. In particular if the SHF \u00adprogram is free of deadlocks, \nthen the translated CSHF-program is also free of deadlocks. The implementation CSHF will also guar\u00adantee \na form of global progress or liveness, if threads are fairly executed: If several transactions in CSHF \nare active, and at least 1 The documented source code and some instructions are available from http://www.ki.cs.uni-frankfurt.de/research/stm \n2 http://hackage.haskell.org/package/stm one transaction could potentially reach its commit-phase if \nrun\u00adning alone, then either the whole process terminates successfully (for other reasons), or at least \none of the transactions will reach its commit-phase also under concurrent execution. Comparing our results \nwith the implementation of STM-Haskell in [7], our work is a justi.cation for the correctness of most \nof the design decisions of their implementation. However, the implemen\u00adtations behave slightly different \nwhich will be discussed in Sec\u00adtion 5. The reason for not using the STM-Haskell implementation in [7] \nis that formalizing this implementation as a calculus is not easily possible, since e.g. it requires \na test for pointer equality, which is cumbersome in a call-by-need calculus with indirections (represented \nby binding chains). Outline. In Section 2 we de.ne the (speci.cation) calculus SHF , by explaining its \nsyntax and operational semantics. The concurrent implementation in form of the calculus CSHF is in\u00adtroduced \nin Section 3. In Section 4 we provide the translation . : SHF . CSHF and prove that . is observationally \ncorrect which shows correctness of the implementation. We discuss related work in Section 5 and .nally \nconclude in Section 6. Due to space reasons not all proofs are included in the paper but can be found \nin the technical report [14]. 2. The SHF-Calculus The syntax of SHF and its processes Proc is in Fig. \n1(a). We assume a countable in.nite set of variables (denoted by x, y, z, . . .) and a countable in.nite \nset of identi.ers (denoted by u, u') to identify threads. In a parallel composition P1 | P2 the processes \nP1 and P2 run concurrently, and the name restriction .x.P restricts the scope of x to process P . A concurrent \nthread (ulx) . e has identi.er u and evaluates the expression e binding the result to the variable x \n(called the future x). A process has usually one distinguished main thread, the main thread, denoted \nby (ulx) .= e. Evaluation of the main thread is enforced, which does not hold for other threads. TVars \nx t e are the transactional (mutable) variables with name x and content e. They can only be accessed \ninside STM-transactions. Bindings x = e model globally shared expressions, where we call x a binding \nvariable. Futures and names of TVars are called component-names. We assume a partitioned set of data \nconstructors, where each family represents a type constructor T . The T -data constructors are ordered \n(denoted with c1, . . . , c|T |). Each type constructor T and each data constructor c has an arity ar(T \n) = 0 (ar(c) = 0, resp.). The functional language has variables, abstractions .x.e, and applications \n(e1 e2), constructor applications (c e1 . . . ear(c)), case-expressions where for every type constructor \nT there is one caseT -construct. For caseT there is exactly one case-alternative (cT,i x1 . . . xar(cT,i) \n. ei) for every constructor cT,i of type con\u00adstructor T where the variables xi become bound with scope \nei. In a pattern cT,i x1 . . . xar(cT,i) the variables xi must be pairwise distinct. We sometimes abbreviate \nthe case-alternatives with alts. Further constructs are seq-expressions (seq e1 e2) for strict eval\u00aduation, \nand letrec-expressions to implement local sharing and re\u00adcursive bindings. In letrec x1 = e1, . . . , \nxn = en in e the vari\u00adables xi must be pairwise distinct and the bindings xi = ei are recursive, i.e. \nthe scope of xi is e1, . . . , en and e. Monadic expressions comprise variants for the IO-and the STM-monad \nof the bind operator >>= for sequential compo\u00adsition of actions, and the return-operator. For the STM-monad \nnewTVar, readTVar, and writeTVar are available to create and access TVars, the primitive retry to abort \nand restart the STM\u00adtransaction, and orElse e1 e2 to compose an STM-transaction from e1, e2: orElse returns \nif e1 is successful and if it catches a retry P, Pi . Proc ::= P1 | P2 | .x.P | (ulx) . e | x = e | x \nt e Functional values: abstractions .x.e and constructor applications  (c e1 ... en) e, ei . Expr ::= \nx | m | .x.e | (e1 e2) | (c e1 . . . ear(c)) Monadic values: all monadic expressions m . MExpr | seq \ne1 e2 | letrec x1 = e1, . . . , xn = en in e Values: functional values and monadic values | caseT e \nof altT,1 . . . altT,|T | cx-values: (c x1 . . . xn) and monadic values where all ar\u00ad where altT,i = \n(cT,i x1 . . . xar(cT,i) . ei) guments are variables m . MExpr ::= returnIO e | e1 >>=IO e2 | future \ne (b) Functional values, monadic values, cx-values, and values | atomically e | returnSTM e | e1 >>=STM \ne2 | retry | orElse e1 e2 | newTVar e | readTVar e | writeTVar e (P1 | P2) | P3 = P1 | (P2 | P3) P1 \n| P2 = P2 | P1 .x1..x2.P = .x2..x1.P P1 = P2 if P1 =a P2 t, ti . Typ ::= IO t | STM t | TVar t | (T \nt1 . . . tn) | t1 . t2 (.x.P1) | P2 = .x.(P1 | P2) if x . FV (P2) (a) Syntax of Processes, Expressions, \nMonadic Expressions and Types (c) Structural Congruence D . PC ::= [\u00b7] | D| P | P | D | .x.D F . FC ::= \nE | (readTVar E) | (writeTVar Ee) E . EC ::= [\u00b7] | (Ee) | (case Eof alts) | (seq Ee) MIO . MCIO ::= [\u00b7] \n| MIO >>=IO e . . MSTM . MCSTM ::= MIO [atomically M STM ] M STM MCSTM ::= [\u00b7] | M STM >>=STM e | orElse \nM STM e LQ . LCQ ::= (ulx) . MQ[F] | (ulx) . MQ[F[xn]]|xn = En[xn-1]|. . .|x2 = E2[x1]|x1 = E1 where \nQ . {IO, STM} and E1 = [\u00b7] (d) Process-, Monadic-, Evaluation-, and Forcing-Contexts future :: (IO a) \n. IO a readTVar :: (TVar a) . STM a ( >>=IO ) :: (IO a1) . (a1 . IO a2) . IO a2 atomically :: (STM a) \n. IO a writeTVar :: (TVar a) . a . STM () ( >>=STM ) :: (STM a1) . (a1 . STM a2) . STM a2 returnSTM :: \na . STM a newTVar :: a . STM(TVar a) orElse :: (STM a) . (STM a) . (STM a) returnIO :: a . IO a retry \n:: (STM a) (e) Polymorphic types of operators G(x) = t, G f e :: IO t G(x) = t, G f e :: t G f P1 :: \nwt, G f P2 :: wt G(x) = TVar t, G f e :: t G f P :: wt G f (ulx) . e :: wt G f x = e :: wt G f P1 | P2 \n:: wt G f x t e :: wt G f .x.P :: wt G(x) = t G(x) = t1, G f e :: t2 G f e1 :: t1 . t2, G f e2 :: t1 \nG f ei :: ti, i = 1, 2 t1 = t3 . t4 or t1 = (T . . .) G f x :: t G f (.x.e) :: t1 . t2 G f (e1 e2) :: \nt2 G f (seq e1 e2) :: t2 .i : G(xi) = ti, .i : G f ei :: ti, G f e :: t G f e :: t1 and t1 = (T . . .), \n.i : G f (ci x1,i . . . xni,i) :: t1, .i : G f ei :: t2 G f (letrec x1 = e1, . . . , xn = en in e) :: \nt G f (caseT e of (c1 x1,1 . . . xn1,1 . e1) . . . (cm x1,m . . . xnm,m . em)) :: t2 .i : G f ei :: ti, \nt1 . . . . . tn . tn+1 . types(c) where c is a constructor, or a monadic operator and types(c) is the \nset G f (c e1 . . . ear(c)) :: tn+1 of monomorphic types of a constructor or a monadic operator c (f) \nTyping rules Figure 1. The calculus SHF , syntax and contexts in e1 then it proceeds with e2. For the \nIO-monad the future\u00adoperator creates threads, and atomically lifts an STM-transaction into the IO-monad \nby executing the transaction. Variable binders are introduced by abstractions, letrec, case\u00adalternatives, \nand .x.P . This induces a notion of free and bound variables and a-renaming and a-equivalence (denoted \nby = a). Let FV (P ) (FV (e), resp) be the free variables of process P (expression e, resp.). We assume \nthe distinct variable convention to hold, that a-renaming is implicitly performed, if necessary. A context \nis a process or an expression with a hole (denoted by [\u00b7]). We write C[e] (C[P ], resp.) for .lling the \nhole of context C by expression e (process P , resp.). For processes we use a structural congruence = \nto equate obviously equal processes, which is the least congruence satisfying the equations of Fig. 1(c). \nSHF is equipped with a monomorphic type system. The syntax of monomorphic types Typ is given in Fig. \n1(a), where (IO t ) means a monadic IO-action with return type t, (STM t) means an STM-transaction action, \n(TVar t ) stands for a TVar-reference with content type t , (T t1 . . . tn) is a type for an n-ary type \nconstructor T , and t1 . t2 is a function type. Although the type system is monomorphic, we overload \nthe constructors and the monadic operators by assuming that they have a polymorphic type according to \nthe usual conventions, however, in the language they are used as monomorphic. The polymorphic types of \nthe monadic operators are shown in Fig. 1(e) where a, ai are type variables. To .x the types during reduction \nand for transformations, we assume that every variable x is explicitly typed and thus has a built\u00adin \ntype G(x). For contexts, we assume that the hole [\u00b7] is typed and carries a type label. The notation \nG f e :: t (G f P :: wt, resp.) means that expression e (process P , resp.) can be typed with type t \n(can be typed, resp.) using G. The typing rules are given in Fig. 1(f). Note that (ulx) . e is well-typed \nif x is of type t and e is of type IO t , and that an STM-or an IO-type for the .rst argument of seq \nis forbidden, to enable that the monad laws hold (see [13]). De.nition 2.1. A variable x is an introduced \nvariable if it is a binding variable or a component-name. A process is well-formed, if all introduced \nvariables and identi.ers are pairwise distinct, and it has at most one main thread. A process P is well-typed \niff P is well-formed and G f P :: wt holds. An expression e is well-typed with type t iff G f e :: t \nholds.  Monadic STM Computations: SHFA (lunitSTM ) (uly) . MSTM [returnSTM e1 >>=STM e2] - --. (uly) \n. MSTM [e2 e1] SHFA (read) (uly) . MSTM [readTVar x] | x t e - --. .z.((uly) . MSTM [returnSTM z] | \nz = e | x t z) where z is fresh (write) (uly) . MSTM [writeTVar x e2] | x t e1 SHFA- --. (uly) . MSTM \n[returnSTM ()] | x t e2 (nvar) (uly) . MSTM [newTVar e] SHFA- --. .x.((uly) . MSTM [returnSTM x] | x \nt e) where x is fresh (ortry) .X.D[(uly) . MSTM [orElse e1 e2]] SHFA,* - ---. .X.D ' [(uly) . MSTM [orElse \nretry e2]] .X.D[(uly) . MSTM [orElse e1 e2]] SHFA- --. .X.D[(uly) . MSTM [e2]] (orret) (uly) . MSTM [orElse \n(returnSTM e1) e2] SHFA- --. (uly) . MSTM [returnSTM e1] SHFA (retryup) (uly) . MSTM [retry >>=STM e1] \n- --. (uly) . MSTM [retry] Monadic IO Computations: SHF (lunitIO ) (uly) . MIO [returnIO e1 >>=IO e2] \n- -. (uly) . MIO [e2 e1] SHF (fork) (uly) . MIO [future e] - -. .z, u ' .((uly) . MIO [return z] | (u \n' lz) . e) where z, u ' are fresh and the created thread is not the main thread SHF (unIO) (uly) . return \ne - -. y = e if the thread is not the main-thread SHFA,* where the context MSTM is maximal, i.e. e1 = \nMSTM [orElse e ' e '' ] .X.D1[(uly) . MIO [atomically e]] - ---. .X.D1 ' [(uly) . MIO [atomically (returnSTM \ne ' )]] (atomic) SHF' D[(uly) . MIO [atomically e]] - -. D ' [(uly) . MIO [returnIO e ]] where D = .X.(D1 \n| D2) and D1 contains all bindings and TVars of D, D2 contains all futures of D, and D' Functional Evaluation: \nSHFa (fevalIO ) P - -. P ' , if P -. P ' for a . {cpIO , absIO , mkbIO , lbetaIO , caseIO , seq} IO \nSHFAa (fevalSTM ) P - --' , if P . P for a . {cpSTM , absSTM , mkbSTM , lbetaSTM , caseSTM , seq . P \n-} STM The reductions with parameter Q . {STM, IO} are de.ned as follows: = .X.(D' 1 | D2) (cpQ) (absQ) \n(mkbQ) (lbetaQ) (caseQ) (caseQ) (seqQ) Closure: LQ[x1] | x1 = x2 | . . . | xn-1 = xn | xn = v . LQ[v] \n| x1 = x2 | . . . | xn-1 = xn | xn = v, if v is an abstraction, a cx-value or a component-name LQ[x1] \n| x1 = x2 | . . . | xm-1 = xm | xm = c e1 . . . en . .y1, . . . yn.(LQ[c y1 . . . yn] | x1 = x2 | . . \n. | xm-1 = xm | xm = c y1 . . . yn | y1 = e1 | . . . | yn = en) if c is a constructor, or returnSTM , \nreturnIO , >>=STM , >>=IO , orElse, atomically, readTVar, writeTVar, newTVar, or future and n = 1, and \nsome ei is not a variable, and where y1, . . . , yn are fresh. LQ[letrec x1 = e1, . . . , xn = en in \ne] . .x1, . . . , xn.(LQ[e] | x1 = e1 | . . . | xn = en), where x1, . . . , xn are fresh LQ[((.x.e1) \ne2)] . .x.(LQ[e1] | x = e2) LQ[caseT (c e1 . . . en) of . . . ((c y1 . . . yn) . e) . . .] . .y1, . . \n. , yn.(LQ[e] | y1 = e1 | . . . | yn = en]), if n > 0 LQ[caseT c of . . . (c . e) . . .] . LQ[e] LQ[(seq \nv e)] . LQ[e], if v is a functional value ' ' SHF' '' SHFA' Pi = D[Pi ], P1 - -. P2 Pi = D[Pi ], P1 - \n--. P2 SHFSHFA P1 - -. P2 P1 - --. P2 Figure 2. The calculus SHF , reductions SHF We de.ne a reduction \nrelation - -. (called standard reduction) for SHF . This reduction relation is on the one hand a small- \nSHF step reduction, since several - -.-steps are performed sequentially before evaluation stops. However, \nit uses as a condition the success SHFA of the intermediate reduction relation - --. in a big-step manner \nfor the atomic execution of an STM-transaction (i.e. the evaluation of e in atomically e and of orElse \ne e '). De.nition 2.2. A well-formed process P is successful, if P has main a main thread of the form \n(ulx) .= return e, i.e. P = main .x1. . . . .xn.((ulx) .= return e | P ' ). De.nition 2.3. The standard \nreduction rules are given in Fig. 2 where the used contexts are de.ned in Fig. 1(d). Standard reduc\u00adtions \nare permitted only for well-formed, non-successful processes. In the following we will denote the transitive \nclosure of a rela- RR,+ R,* tion -. by --. and the re.exive-transitive closure by --.. We SHF,a will \nalso use the notation ----. where a is the name of a reduction SHF rule to make explicit which reduction \nis used by a - -.-step. SHF We de.ne the - -.-redex: For (lunit), (fork), it is the expres\u00adsion in the \ncontext M, for (unIO), it is (uly) . return e, for (mkb), (lbeta), (case), (seq), (cp), (abs) it is the \nexpression (or vari- SHFA able) in the context L. We de.ne the - --.-redex: For (lunitSTM ), (read), \n(write), (nvar), (ortry), (orret) it is the expression in the context MSTM , for (mkbSTM ), (lbetaSTM \n), (caseSTM ), (seqSTM ), (cpSTM ), (absSTM ) it is the expression (or variable) in the context LSTM \n. We explain the standard reduction rules of Fig. 2. The standard SHFSHFA,* (big-step) reduction is - \n-., whereas - ---. de.nes the evalu\u00ad  SHFA,* ation of STM-transactions. The - ---.-reduction sequences \nwill only be performed, if they terminate successfully, see rule (atomic). The rules are divided into \nthree classes: Rules for monadic computations in the STM-monad, rules for monadic computations in the \nIO-monad, and rules for functional evaluation. We start with explaining the most interesting rules the \nrules for the STM-monad: The rule (lunit) implements the semantics of the monadic sequencing operator \n>>= . The rules (read), (write), and (nvar) access and create TVars. The rule (ortry) is a big-step rule: \nif SHFA,* a - ---.-reduction sequence starting with orElse e1 e2 ends in orElse retry e2, then the effects \nare ignored, and orElse e1 e2 is replaced by e2. If the reduction of e1 ends with return e, then rule \n(orret) is used to keep the result as the result of orElse e1 e2. For the IO-monad there is also a rule \n(lunit) for the bind\u00adoperator. The rule (atomic) executes an STM-action in the IO- SHFA,* monad. It is \nalso a big-step rule. If for a single thread the - ---.\u00adreduction successfully produces a return, then \nthe transaction is SHF SHFA,* performed in one step of the - -.-reduction. If the - ---.\u00adreduction ends \nin a retry, in a stuck expressions or does not SHF terminate, then there is no - -.-reduction, and hence \nit is omitted from the operational semantics. The rule (fork) spawns a new concurrent thread and returns \nthe newly created future as the result. The rule (unIO) binds the result of a monadic computation to \na functional binding, i.e. the value of a concurrent future becomes accessible. The rules for functional \nevaluation implement call-by-need re\u00adduction. The rules (cp) and (abs) inline a needed binding x = e \nwhere e must be an abstraction, a cx-value, or a component name. To implement call-by-need evaluation \nthe arguments of constructor applications and monadic expressions are shared by new bindings, similar \nto lazy copying [18]. Since the variable (binding-) chains are transparent, there is no need to copy \nbinding-variables to other places in the expressions. The rule (mkb) moves letrec-bindings into the global \nbindings. The rule (lbeta) is the sharing variant of \u00df\u00adreduction. The (case)-reduction reduces a case-expression, \nwhere perhaps bindings are created to implement sharing. The (seq)-rule evaluates a seq-expression. Since \nthe reduction rules only introduce variables which are SHF fresh and never introduce a main thread, - \n-. preserves well\u00adformedness. Also type preservation holds, since every reduction keeps the type of subexpressions. \nContextual equivalence equates processes P1, P2 if their observable behavior is indistinguishable if \nP1 and P2 are plugged into any process context. For concurrent calculi observing may-convergence, i.e. \nwhether a process can be reduced to a successful process, is not suf.cient and thus we will also observe \nshould-convergence (see [10, 11]). De.nition 2.4. A process P may-converges (written as P .), iff it \nis well-formed and reduces to a successful process, (see Def- SHF,* inition 2.2), i.e. P . iff P is well-formed \nand .P ' : P - --. P ' . P ' successful. If P . does not hold, then P must-diverges written as P .. A \nprocess P should-converges (written as P .), iff it is well-formed and remains may-convergent under reduction, \ni.e. SHF,* P . iff P is well-formed and .P ' : P - --. P ' =. P ' .. If P is not should-convergent then \nwe say P may-diverges written as SHF,* P ., which is also equivalent to .P ' : P - --. P ' .P ' .. Contextual \napproximation =c and contextual equivalence ~c on processes are de.ned as =c := =. n =. and ~c := =c \nn =c where for . . {., .}: P1 =. P2 iff .D . PC : D[P1]. =. D[P2].. SHF The de.nition of - -. implies \nthat non-wellformed processes are always must-divergent. Also, the process construction by D[P ] is always \nwell-typed if P is well-typed, since we assume that variables have a built-in type. Clearly, SHF implements \nmemory transactions in a correct way: Every single transaction is executed atomically and isolated from \nany other transaction. So SHF is a correct speci.cation of STM. However, the semantics has two drawbacks: \nIt is impossible to implement the standard reduction: The rules (atomic) and (ortry) have undecidable \npreconditions, since they SHFA include checking whether a - --.-reduction halts successfully. Since the \ntransactions are executed sequentially, there is only poor concurrency. In the next section we will give \nan implementation using SHF as speci.cation, which overcomes those drawbacks. 3. A Concurrent Implementation \nof STM Evaluation We introduce a concurrent (small-step) evaluation enabling much more concurrency which \nis closer to an abstract machine than the big-step semantics. The executability of every single step \nis decid\u00adable, and every state has a .nite set of potential successors. The undecidable conditions in \nthe rules (atomic) and (ortry) of SHF are now implemented by tentatively executing the transaction, un\u00adder \nconcurrency, thus allowing other threads to execute. Transac\u00adtion execution should guarantee an equivalent \nlinearized execution of transactions. Our ultimate goal is to show that the concurrent execution is correct, \ni.e. to show that it is semantically equivalent to the big-step reduction de.ned for SHF . However, our \napproach uses minimal locking times and thus has a potential danger of con\u00ad.icts and retries. Instead \nof using an optimistic read/write approach which performs a rollback in case of a con.ict [8], we will \nfollow a similar, but slightly more pessimistic read and write: there are no locks at the start of a \ntransaction, initial reads copy contents to the local store, other reads and writes are local; only at \nthe end of a suc\u00adcessful transaction there is a commit phase with global writes such that updates become \nvisible to other transactions where a real, but internal, locking is used for a short time. To have a \ncorrect overall execution, con.icting transactions will be stopped by sending them a retry-noti.cation \n(so-called early con.ict detection), where the knowledge of the potential con.icts is memorized at the \nTVars. We view a transaction as a function V1 . V2 from a set of (read) input TVars V1 to a set of modi.ed \nTVars V2 where V1, V2 may have common elements. The guarantee must be that at the end of transaction \nexecution, the complete transaction could be atomically and instantaneously executed on the TVars V1, \nV2. During this commit phase other running transactions that have read any variable of V2, need to be \naborted (restarted) due to a con.ict. More concretely, the ideas of CSHF are as follows: For every transaction \nthere is a local copy for every TVar it accesses. All reads and writes are performed in the local copies \ninvisible to other concurrently executed threads.  For later con.ict detection every global TVar is \nequipped with a set of registered threads. Before obtaining a local copy the thread identi.er is added \nto this set.  For book-keeping of the accessed TVars every thread has a transaction log, where read, \nwritten, and created TVars are remembered.  If an executing transaction (thread) results in retry, its \nlocal copies of the TVars are removed, its registrations on TVars are removed and the transaction starts \nover again. If retry occurs inside an orElse then the treatment is different (see next item).   To \nimplement the nesting of orElse, the local copies of the TVars are stacks, where the stack depth matches \nthe depth of the orElse-nesting. In case that a transaction stops with retry inside the left argument \nof orElse, all these stacks are popped and the right argument of orElse is executed. However, read TVars \nmust remain in the transaction log, since their values may have an in.uence on the control. For the same \nreason, also the registration at the global TVars must not be changed.  If a transaction executed all \nits actions, then it starts its commit phase. To ensure consistency, .rst all read and all to-be-written \nTVars are locked by the thread identi.er of the committing transaction. This locking is done in one atomic \nstep. After the locks are set, no other thread is able to access the locked TVars. Then there are several \nsteps performed which may be interleaved by other transactions (accessing other TVars):  The committing \nthread removes its registration from all TVars it has accessed. The committing thread reads the registration \nsets to ob\u00adtain the thread identi.ers of all the con.icting transactions which have to be aborted. The \ncorresponding threads are no\u00adti.ed by the committing transaction to abort and restart their transaction. \nThis can be seen as throwing an exception from the committing thread to all other threads which have \nbe\u00adcome inconsistent now. This is indeed implemented in this way in our prototype. In the operational \nsemantics below, simply the current code of the other transactions is replaced by retry and thus noti.ed \nthreads perform as if they got a (transaction) retry: .rst they unregister their thread identi\u00ad.er and \nthen restart. The committing thread updates the global content of the TVars and then removes the locks. \nThe .nal steps of the committing thread are to add newly created TVars as global TVars and then to remove \nall the local copies. Another design decision in the calculus description below is that sharing by bindings \nis carefully maximized, including transpar\u00adent variable-variable-binding chains. One reason is that this \nleads to manageable correctness proofs. Another design principle is to avoid negative conditions: There \nmust not be any conditions that rely on a test whether there are no occurrences of TVars or threads satisfying \ncertain conditions. Now we detail on this idea and introduce the calculus CSHF which has a concurrent \nevaluation of transactions and a modi.ed SHF -syntax: there is an addition of labels, of memory-cells \nat threads and a stack for the orElses. First we describe the syntax changes of the language, and then \nexactly describe the rules, which are close to a concurrent abstract machine for SHF. De.nition 3.1 (Syntax \nof CSHF ). The syntax of CSHF \u00adprocesses is almost the same as for SHF-processes where, how\u00adever, there \nare some extensions and changes. Instead of TVars x t e there are two constructs: 1. Global TVars are \nrepresented by x tg e u g, where the addi\u00adtional third argument u is a locking label and may be empty \n(written as -) or a thread identi.er u that locks the TVar, and g is a list of thread identi.ers for \nthose threads that want to be noti.ed for a retry, when x is updated. 2. A stack of thread-local TVars \nis represented by u tls s, where u is a thread identi.er and s is a stack of sets with local TVars x \ntl e as elements, where x is a name of a TVar and e is an expression.  A thread may have a transaction \nlog, which is only available if a thread is within a transaction. It is written over the thread-arrow \nas T,L;K (uly) .==== e where T is a set of TVars (i.e. the names of the TVars) that are read during the \ntransaction; L is a stack of triples (La, Ln, Lw) where La is a set of the names of all TVars which are \naccessed during the transaction, Ln a set of names of newly generated TVars, Lw a set of locally updated \n(or written) TVars, and the stack re.ects the depth of the orElse-execution; K is a set of TVar-names \nthat is locked by a thread in the commit-phase. Additional (labeled) variants of the operators orElse \nand atomically are required: The operator orElse! indicates that orElse is active, and the operator atomically! \nindicates that a transaction is active, where atomically! has as a second argument an expression that \nis a saved copy of the start expression and that will again be activated after rollback and restart. \nI.e., the sets of monadic expressions and MCSTM -contexts are adapted as: m . MExpr ::= returnIO e | \ne1 >>=IO e2 | future e | returnSTM e | e1 >>=STM e2 | atomically e | atomically! e e ' | retry | orElse \ne1 e2 | orElse! e1 e2 | newTVar e | readTVar e | writeTVar e ::= MIO [atomically! Me] MSTM . MCSTM MSTM \nMM ::= [\u00b7] | e | orElse! MeMSTM . MCSTM MSTM >>=STM MSTM A thread that has a transaction log is called \ntransactional, oth\u00aderwise it is called non-transactional. A CSHF -process that only has non-transactional \nthreads and no u tls s-components is called non-transactional; otherwise, it is called transactional. \nWe say a thread is currently performing a transaction, if the current evaluation focusses on the arguments \nof atomically! (see CSHF the reduction - --. below). De.nition 3.2. A CSHF-process P is well-formed iff \nthe follow\u00ading holds: Variable names of TVars, threads, and binders are in\u00adtroduced at most once; i.e. \nthreads, bindings, and global TVars are unique per variable. For every component x tl e in a stack-entry \nof u tls s, either there is also a global TVar x tg e o g, or the TVar is in the Ln-component of the \nthread-memory (the locally generated TVars). Moreover, for every thread identi.er u, there is at most \none process component u tls s. In every stack entry, names of TVars oc\u00adcur at most once. For every thread \nidenti.er u in u tls s there exists a thread with this identi.er. Though the syntax of CSHF is slightly \ndifferent from SHF , we use the same names for the context classes PC, EC, MCSTM , MC, FC, MCIO , and \nLCQ. If necessary, then we distinguish the context classes using an index C (for concurrent). For the \nPC-contexts we assume that also transactional threads are permitted. De.nition 3.3 (Operational Semantics \nof CSHF ). A well-formed CSHF -process P reduces to another CSHF -process P ' (denoted CSHF '' ' by P \n- --. P ') iff P = D[P1] and P = D[P1] and P1 . P1 by a reduction rule in Fig. 3, 4, and 5. We explain \nthe execution of a transaction and the use of the transaction log, where we also point to the reduction \nrules. Start of a transaction (atomic): When the execution is started a new empty transaction log is \ncreated. Read-operations: (readl) and (readg): A read .rst looks into the local store. If no local TVar \nexists, then the global value is copied into the local store and the own thread identi.er is added to \nthe notify-list of the global TVar. Write-operations (writel) and (writeg): A write command al\u00ad ways \nwrites into the local store, perhaps preceded by a copy from the global into the local store .  Monadic \nIO Computations: CSHF (lunitIO ) (uly) . MIO [returnIO e1 >>=IO e2] - --. (uly) . MIO [e2 e1] CSHF \n(fork) (uly) . MIO [future e] - --. .z, u ' .((uly) . MIO [return z] | (u ' lz) . e) where z, u ' are \nfresh and the created thread is not the main thread SHF (unIO) (uly) . return e - -. y = e if the thread \nis not the main-thread Monadic STM Computations: CSHF\u00d8,[\u00d8] (atomic) (uly) . MIO [atomically e] - --. \n.z.(uly) .= MIO [atomically! z z] | u tls [\u00d8] | z = e T,L CSHFT,L (lunitSTM ) (uly) .== MSTM [returnSTM \ne1 >>=STM e2] - --. (uly) .== MSTM [e2 e1] T,L (readl) (uly) .== MSTM [readTVar x] | u tls ({x tl e1} \n. \u00b7 r) : s CSHFT,L - --. .z.((uly) .== MSTM [returnSTM z] | z = e1 | u tls ({x tl z} . \u00b7 r) : s T,L \n(readg) (uly) .== MSTM [readTVar x] | x tg e1 - g | u tls r : s CSHFT 1,L1 ' - --. .z.((uly) .= = MSTM \n[returnSTM z] | z = e1 | u tls ({x tl z} . \u00b7 r) : s | x tg z - g ) if x . La where L = (La, Ln, Lw) : \nLr, L ' = (La . {x}, Ln, Lw) : Lr, T ' = T . {x} and g ' = ({u} . g) T,L CSHFT,L1 (writel) (uly) .== \nMSTM [writeTVar x e1] | u tls ({x tl e2} . \u00b7 r : s) - --. (uly) .=== MSTM [returnSTM ()] | u tls ({x \ntl e1} . \u00b7 r : s) where L = (La, Ln, Lw) : Lr, L ' = (La, Ln, Lw . {x}) : Lr T,L (writeg) (uly) .== MSTM \n[writeTVar x e1] | x tg e2 - g | u tls (r : s) CSHFT,L1 - --. (uly) .=== MSTM [returnSTM ()] | x tg \ne2 - g | u tls ({x tl e1} . \u00b7 r : s) if x . La, where L = (La, Ln, Lw) : Lr, L ' = (La . {x}, Ln, Lw \n. {x}) : Lr T,L CSHFT,L1 (nvar) (uly) .== MSTM [newTVar e] | u tls (r : s) - --. .x.((uly) .=== MSTM \n[returnSTM x] | u tls ({x tl e)} . \u00b7 r : s) where L = (La, Ln, Lw) and L ' = ({x} . La, {x} . Ln, Lw) \nT,L CSHFT,L (retryup) (uly) .== MSTM [retry >>=STM e1] - --. (uly) .== MSTM [retry] T,L (orElse) (uly) \n.== MSTM [orElse e1 e2] | (u tls ({x1 tl e1,1, . . . , xn tl e1,n} : s) CSHF T,L1 - --. .z1, . . . zn.((uly) \n.=== MSTM [orElse! e1 e2] | u tls (({x1 tl z1, . . . , xn tl zn} : ({x1 tl z1, . . . , xn tl zn}) : s) \n| z1 = e1,1 | . . . | zn = e1,n) where L = (La, Ln, Lw) : Lr, L ' = (La, Ln, Lw) : ((La, Ln, Lw) : Lr) \nT,LCSHFT,L1 (orRetry) (uly) .== MSTM [orElse! retry e2] | u tls (r : s) - --. (uly) .=== MSTM [e2] | \nu tls s where L = Le : L ' T,L CSHFT,L (orReturn)(uly) .== MSTM [orElse! (returnSTM e1) e2] | u tls (r \n: s) - --. (uly) .== MSTM [returnSTM e1] | u tls (r : s) 1 T,L CSHFT,L (retryCGlob) (uly) .== MIO [atomically! \nretry e] | x tg e1 - g - --. (uly) .=== MIO [atomically! retry e] | x tg e1 - g ' if x . T , where T \n' = T \\ {x}, g ' = g \\ {u} \u00d8,L CSHF (retryEnd)(uly) .== MIO [atomically! retry e] | u tls (r : s) - --. \n(uly) . MIO [atomically e] Figure 3. Concurrent Implementation of SHF , transaction reductions OrElse-Evaluation: \nIf evaluation descends into the left expression of orElse, then the stacks of TVar-names and of local \nTVars are extended by duplicating their top element in rule (orElse). For the .nal write in the commit \nphase only the top of the stack is relevant. If evaluation of the left expression is successful, the \nstack remains as it is (by rule (orReturn)). In case of a retry, the top element is popped (by rule (orRetry)) \nand the execution of the second expression then uses the stack before executing the orElse. Note that \nthe information on the read TVars is kept in the set T , since the values may have an in.uence on the \noutcome of the orElse-tree execution. This is necessary, since the big-step semantics of SHF enforces \na left-to-right evaluation of the orElse-tree, where the leftmost successful try will be kept. Note that \nthis is semantically different from making a non-deterministic choice of one of the successful possibilities \nin the orElse-tree. Commit-Phase: At the end of a transaction, there is a lock\u00adprotected sequence of \nupdates: A thread that is in its commit\u00adphase .rst locks all its globally read and to-be-updated TVars \n(rule (writeStart)). Locked variables cannot be accessed by other threads for reading, or modifying the \nnotify-list. Then all own noti.cation-entries are removed (rule (writeClear)). All the threads, which \nare unequal to the running thread, and that are in the notify-list of the updated TVars, will be stopped \nby replacing the current transaction expression by retry (rule (sendRetry)). This mechanism is like raising \n(synchronous) ex\u00adceptions to in.uence other threads. The to-be-updated TVars are written into the global \nstore (rule (writeTV)), then the locks are released (rule (unlockTV)) and fresh TVars are also moved \nto the global store (rule (writeTVn)). Finally, the transaction log is removed (rule (writeEnd)). Rollback \nand Restart: A transaction is rolled back and restarted by the retry-command (if it is not inside an \norElse\u00adcommand). This can occur by a user programmed retry, or if the transaction gets stopped by a con.icting \ntransaction which is committing. The thread removes the noti.cation entries (rule (retryCGlob)) and then \nthe transaction code is replaced by the original expression (rule (retryEnd)). Also, it is easy to extend \nthe monomorphic type system to CSHF and to see that reduction keeps well-typedness.  The commit-phase \nuses thread-local memory K, written over the thread-arrow after a semicolon. The third memory-component \nis the set of locked TVars. (writeStart) start of commit: locking the read and to-be-written TVars T,L \n (uly) .== MIO [atomically! (returnSTM e1) e] | x1 tg e1 ' - g1 | . . . | xn tg en ' - gn CSHFT,L;K \n- --. (uly) .=== MIO [atomically! (returnSTM e1) e] | x1 tg e1 ' u g1 | . . . | xn tg en ' u gn where \nK := {x1, . . . , xn} = T . (La \\ Ln) and L = (La, Ln, Lw) : Lr (writeClear) removing the notify-entries \nof u: T,L;K CSHFT 1,L;K (uly) .=== MIO [atomically! (returnSTM e1) e] | x tg e2 u g - --. (uly) .= = \nMIO [atomically! (returnSTM e1) e] | x tg e2 u g ' if x . T where g ' = g \\ {u} and T ' = T \\ {x} (sendRetry) \nSending other threads (that are in transactions) a retry: \u00d8,L;K T 1,L1 (uly) .=== MIO [atomically! (returnSTM \ne1) e] | x tg e2 u g | (u ' lz) .= = MIO ' [atomically! e3 e4] CSHF\u00d8,L;K T 1,L1 - --. (uly) .=== MIO \n[atomically! (returnSTM e1) e] | x tg e2 u g ' | (u ' lz) .= = MIO ' [atomically! retry e4] \u00b7 if x . \nLw, g = \u00d8, u ' . g, where L = (La, Ln, Lw) : Lr, and g = g ' . {u ' } (writeTV) Overwriting the global \nTVars with the local TVars of u: \u00d8,L;K (uly) .=== MIO [atomically! (returnSTM e1) e] | x tg e2 u [] \n| u tls ({x tl e3} . \u00b7 r : s) CSHF\u00d8,L1;K - --. (uly) .=== MIO [atomically! (returnSTM e1) e] | x tg \ne3 u [] | u tls (r : s) if for all z . Lw \\ Ln the g is empty in the component z tg e2 u g, and if Lw \n\\ Ln = \u00d8 where L = (La, Ln, Lw) : Lr, x . Lw \\ Ln, L ' = (La, Ln, Lw \\ {x}) : Lr. (unlockTV) Unlocking \nthe locked TVars: \u00d8,L;K CSHF\u00d8,L;K1 (uly) .=== MIO [atomically! (returnSTM e1) e] | x tg e2 u [] - --. \n(uly) .=== MIO [atomically! (returnSTM e1) e] | x tg e3 - [] if Lw \\ Ln = \u00d8, and K = \u00d8 where L = (La, \nLn, Lw) : Lr, and K ' = K \\ {x} (writeTVn) Moving the freshly generated TVars of u into global store: \n\u00d8,L;\u00d8 (uly) .== MIO [atomically! (returnSTM e1) e] | u tls ({x tl e3} . \u00b7 r : s) CSHF\u00d8,L;\u00d8 - --. (uly) \n.== MIO [atomically! (returnSTM e1) e] | x tg e3 - [] if x . Ln, where L = (La, Ln, Lw) : Lr and L ' \n= (La, Ln \\ {x}, \u00d8) : Lr (writeEnd) Removing local store and end of commit of transaction: \u00d8,L;\u00d8 CSHF \n(uly) .=== MIO [atomically! (returnSTM e1) e] | u tls (r : s) - --. (uly) . MIO [returnIO e1] if no other \nrules of the commit-phase for u are applicable, i.e., if Ln = Lw = \u00d8, where L = (La, Ln, Lw) : Lr. Figure \n4. Concurrent Implementation of SHF , commit phase of transaction De.nition 3.4. A CSHF-process P is \nsuccessful, iff it is well\u00ad main formed and the main thread is of the form (uly) .= return e. May-and \nshould-convergence in CSHF are de.ned by: P .CSHF CSHF,* ' '' iff P is well-formed and .P : P - ---. \nP . P successful. CSHF,* P .CSHF , iff P is well-formed and .P ' : P - ---. P ' =. P ' .CSHF . Must-and \nmay-divergence of process P are the negations of may-and should-convergence and are denoted by , and \nP .CSHF , resp., where P .CSHF is also equivalent P .CSHF CSHF,* to P - ---. P ' such that P ' .CSHF \n. Contextual approximation =CSHF and equivalence ~CSHF in CSHF are de.ned as =CSHF := and =.CSHF n =.CSHF \n~CSHF := =CSHF n =CSHF where for . . {.CSHF , .CSHF }: P1 =. P2 iff .D . PCC : D[P1]. =. D[P2].. 4. Correctness \nof the Concurrent Implementation In this section we show that CSHF can be used as a correct eval\u00aduator \nfor SHF and its semantics. Hence, we provide a translation from SHF into CSHF: De.nition 4.1. The translation \n. of an SHF-process into a CSHF -process is de.ned homomorphically on the structure of processes: Usually \nit is the identity on the constructs; the only ex\u00adception is .(x t e) := x tg e - [], i.e. initially, \nthe list of threads to be noti.ed is empty and the TVar is not locked. CSHF -processes .(P ) where P \nis an SHF -process are the initial CSHF-processes. We are mainly interested in CSHF -reductions that \nstart with initial CSHF -processes. Since only transactions can introduce lo\u00adcal TVars which are removed \nat the end of a transaction, the fol\u00adlowing lemma holds: Lemma 4.2. Every initial CSHF -process is well-formed \nprovided the corresponding SHF -process is well-formed. Also, every reduc\u00adtion descendant of an initial \nCSHF -process is well-formed. The correctness theorem we want to prove is the following: Main Theorem \n4.3. The translation . : SHF . CSHF is observational correct, i.e. for all process contexts D and SHF\u00adprocesses \nP the equivalences D[P ]. .. .(D)(.(P )).CSHF and D[P ]. .. hold. This also implies .(D)(.(P )).CSHF \nthat . is adequate, i.e. .(P1) ~CSHF .(P2) =. P1 ~c P2. Adequacy is a direct consequence of observational \ncorrect\u00adness (see [15]). Since the translation . is compositional, i.e. .(D)(.(P )) = .(D[P ]) for any \nprocesses context D and pro\u00ad  Functional Evaluation CSHFa (fevalIO ) P - --. P ' , if P -. P ' for \na . {cpIO , absIO , mkbIO , lbetaIO , caseIO , seq} IO CSHFa (fevalSTM ) P - --. P ' , if P -. P ' for \na . {cpSTM , absSTM , mkbSTM , lbetaSTM , caseSTM , seq} STM The reductions with parameter Q . {STM, \nIO} are de.ned as follows (cpQ) LQ[x1] | x1 = x2 | . . . | xn-1 = xn | xn = v . LQ[v] | x1 = x2 | . . \n. | xn-1 = xn | xn = v, if v is an abstraction, a cx-value, or a component name (absQ) LQ[x1] | x1 = \nx2 | . . . | xm-1 = xm | xm = c e1 . . . en . .y1, . . . yn.(LQ[c y1 . . . yn] | x1 = x2 | . . . | xm-1 \n= xm | xm = c y1 . . . yn | y1 = e1 | . . . | yn = en) if c is a constructor, or returnSTM , returnIO \n, >>=STM , >>=IO , orElse, atomically, readTVar, writeTVar, newTVar, or future, and n = 1, and some ei \nis not a variable. (mkbQ) LQ[letrec x1 = e1, . . . , xn = en in e] . .x1, . . . , xn.(LQ[e] | x1 = e1 \n| . . . | xn = en) (lbetaQ) LQ[((.x.e1) e2)] . .x.(LQ[e1] | x = e2) (caseQ) LQ[caseT (c e1 . . . en) \nof . . . ((c y1 . . . yn) . e) . . .] . .y1, . . . , yn.(LQ[e] | y1 = e1 | . . . | yn = en]), if n > \n0 (caseQ) LQ[caseT c of . . . (c . e) . . .] . LQ[e] (seqQ) LQ[(seq v e)] . LQ[e], if v is a functional \nvalue Figure 5. Concurrent implementation of SHF , functional reductions cess P , for the proof of observational \ncorrectness it is suf.cient to show that . preserves and re.ects may-and should convergence: Theorem \n4.4. The translation . : SHF . CSHF is convergence equivalent, i.e. for all SHF -processes P the equivalences \nP . .. and P . .. .(P ).CSHF hold. .(P ).CSHF In the remainder of this section we will prove this theorem \nto complete the proof of Main Theorem 4.3. Thus we have to show that may-and should-convergence are the \nsame for the big-step semantics and the concurrent implementation. In order to be on solid ground, we \n.rst analyze the invariants during transactions and properties of the valid con.gurations. Lemma 4.5. \nThe following properties hold during a CSHF \u00adreduction on a well-formed CSHF -process that is reachable \nfrom a non-transactional CSHF -process. 1. For every component x tl e, either x . Ln of the top entry \nin L, or there is a global TVar x. For every pair of thread identi.er u, and TVar-name x, every stack \nelement of the TVar-stack for u contains at most one entry x tl e. 2. If u is in a notify-list of a \nglobal TVar, then thread u is transac\u00adtional. 3. Every transaction that starts the commit-phase for \nthread u by performing the rule (writeStart) is able to perform all other rules until (writeEnd) is performed, \nwithout retry, nontermina\u00adtion or getting stuck.  For proving convergence equivalence of . (Theorem \n4.4) we have to show four parts: preservation of may-convergence (P . . ), re.ection of may-convergence \n(.(P ).CSHF . .(P ).CSHF P .), preservation of should-convergence (P . . .(P ).CSHF ), and re.ection \nof should-convergence (.(P ).CSHF . P .).  4.1 Preservation of May-Convergence We have to show that \nP . implies .(P ).CSHF , which is not com-SHFA,* pletely straightforward, since the big-step reduction \n- ---. can be tried for free, and only in the case of success, i.e., a returnSTM SHFA,* is obtained by \nan atomic transaction, the changes of the - ---.\u00adreduction sequence are kept. Also, if an orElse-expression \nis re\u00adduced, the big-step reduction is permitted to evaluate the second expression, if it is known that \nthe .rst one would end in a retry. This is different in CSHF , since the execution has to .rst evaluate \nthe left expression of an orElse-expression, and only in case it re\u00adtries , the second expression will \nbe evaluated where the changes of the TVars are not kept, but changes belonging to functional eval\u00aduation \nin the bindings are kept. Analyzing the behavior exhibits that these changes of the process can be proved \nas convergence equiva\u00adlent transformations. As a base case, the following lemma holds: Lemma 4.6. If \nP is successful, then .(P ) is successful. An easy case are non-(atomic)-standard reductions: SHF,a Lemma \n4.7. If P1 - --. P2 where a = (atomic), then CSHF .(P1) - --. .(P2). The more complex cases arise for \nthe transactions in the big\u00adstep reduction. In this case the state of the concurrent implemen\u00adtation \nconsists of stacks, global TVars and stacks of local TVars. We consider several program transformations \nrelated to reduction rules, which are chosen such that it is suf.cient to simulate the modi.cations in \nthe bindings of the retried CSHF -standard reduc\u00adtions and then to rearrange reduction sequences of the \nconcurrent implementation. De.nition 4.8 (CSHF special transformations). The spe\u00adcial transformations \nare de.ned in Fig. 6 where we as\u00adsume that they are closed w.r.t. D-contexts and struc\u00ad a tural congruence, \ni.e. for any transformation -. with a . {(cpBE), (absB), (funrB), (absG), (absAt), (gc)} we ex\u00adtend its \nde.nition as follows: If P = D[P ' ], Q = D[Q ' ], and ' . Q ' aa P -, then also P -. Q. We require the \nnotion of convergence equivalence for pro\u00adgram transformations, which is analogously de.ned to convergence \nequivalence of translations: De.nition 4.9. If P1.CSHF .. .. P2.CSHF and P1.CSHF then we write P1 ~ce \nP2, A program transformation P2.CSHF . (i.e. a binary relation over CSHF -processes) is convergence equivalent \niff P1 . P2 always implies P1 ~ce P2. In the technical report [14] we prove: Lemma 4.10. The rules in \nFig. 6 are convergence equivalent. First we observe the effects of global retries: CSHF,* Lemma 4.11. \nIf there is a CSHF -reduction sequence P1 - ---. P2, where an STM-transaction is started in the .rst \nreduction for thread u, and the last reduction is a global retry, i.e. (retryEnd) CSHFspt for thread \nu, of the transaction, then P1 (- --. . - .) * P2 spt where - . is the union of the CSHF -special-transformations \n (cpBE) x = E[x1] | x1 = x2 | . . . | xm-1 = xm | xm = v -. x = E[v] | x1 = x2 | . . . | xm-1 = xm | \nxm = v, if v is an abstraction, a cx-value or a component-name, and E = [\u00b7]. (absB) x = c e1 . . . en \n-. .x1, . . . xn.x = c x1 . . . xn | x1 = e1 | . . . | xn = en. (funrB) Every functional rule (without \nthe surrounding L-context) in a context x = E[\u00b7], but not (cp) and not (abs). (absG) x tg e o g -. .z.x \ntg z o g | z = e where o . {-, u}(absAt) (xlu) . MIO [(atomically e)] -. (xlu) . MIO [(atomically z)] \n| z = e. (gc) .x1, . . . , xn.P | x1 = e1 | . . . | xn = en -. .x1, . . . xn.P if for all i = 1, . . \n. , n: xi does not occur free in P . (cpxgc) .x.C[x, . . . , x] | x = y -. C[y, . . . , y] where C is \na multi-context, where all holes are in an A-context, and all occurrences of x are indicated in the notation, \ni.e., there are no other occurrences. A is the class of expression contexts where the hole is not within \nan abstraction nor in a letrec-expression, nor in an alternative of a case and not in an argument of \na constructor. Figure 6. Special Transformations for CSHF (cpBE), (absB), (funrB), (absG), and inverse \n(gc)-transformations from De.nition 4.8. Proof. A global retry removes all generated local TVars for \nthis thread. There may remain changes in the global TVars and in the bindings: Transformations (absG) \nmay be necessary for global TVars. Functional transformations in the sharing part are still there, but \nmay be turned into non-standard reductions, if these were triggered only by the thread u. Since (cp)-effects \nin the thread expression are eliminated after a retry in an orElse: These may be (cpBE), (absB), (funrB). \nVarious reductions generate bindings which are no longer used after removal of the .rst expression in \nan orElse, which can be simulated by a reverse (gc). These special reductions replace the transaction \nreductions for thread u, but the others remain, hence the lemma holds. SHF,atomicCSHF,* Lemma 4.12. If \nP1 - ------. P2, then .(P1) - ---. P2 ' , sptx,* sptx and P2 ' - --. .(P2), where --. consists of special \ntransforma\u00adtions in Fig. 6 and their inverses. Proof. The correspondence between the contents of the \nglobal TVar x in CSHF for a single thread u is the local TVar x on the top of the stack, or the global \nTVar if there is no local TVar for x. The rules that change the bindings permanently in the atomic-transaction \nre\u00adduction are: (atomic), (readl), (readg), (orElse), which can be sim\u00adulated by sequences of (absG) \nand reverse (gc). The effects of the functional rules ((cpQ), (absQ), (mkbQ), (lbetaQ), (caseQ)) that \nsurvive a retry are either simulated by (mkbQ), (lbetaQ), (caseQ) in a binding, or by (cpBE), (absB), \nor inverse (gc). An (ortry)\u00adreduction in SHF can be simulated in CSHF by a sequence of reductions starting \nwith (orElse) and ending with (orRetry). How\u00adever, since (ortry) undoes all changes, in CSHF it is necessary \nto undo the changes in bindings by the special transformations. In [14] we show that the claimed rearrangement \nis possible. Theorem 4.13. For all SHF -processes P , we have P . =. .(P ).CSHF . Proof. This follows \nby an induction on the length of the given reduction sequence for P . The base case is covered in Lemma \n4.6. SHF Lemmas 4.7, 4.12, and 4.10 show that if P1 - -. P2, then there CSHF,* is some CSHF -process \nP2 ' such that .(P1) - ---. P2 ' with P2 ' ~ce .(P2). This is suf.cient for the induction step.  4.2 \nRe.ection of May-Convergence In this section we distinguish the different reduction steps within a CSHF \n-reduction sequence .(P1) -* . P2 for the different threads u. A (sendRetry)-reduction belongs to the \nsending thread. A subse\u00adquence of the reduction sequence starting with (atomic) and ending with (writeEnd), \nwhich includes exactly the u-reduction steps in between, and where no other (atomic) or (writeEnd)-reductions \nare contained is called a transaction. A pre.x of a transaction is also called a partial transaction. \nThe subsequence of an u-transaction for thread u starting with (writeStart) and ending with (writeEnd) \nis called the commit-phase of the transaction. If the subsequence for thread u starts with (atomic) and \nends with (retryEnd), without intermediate (atomic) or (retryEnd), then it is an aborted transac\u00adtion. \nThe subsequence from the .rst (retryCGlob) until (retryEnd) is the abort-phase of the aborted transaction. \nA pre.x of an aborted transaction is also called a partial aborted transaction. Theorem 4.14. For all \nSHF -processes P , we have =. P .. .(P ).CSHF CSHF,* Proof. Let .(P1) - ---. P2 where P2 is successful. \nSince the transactions in the reduction sequence may be interleaved with other transactions and reduction \nsteps, we have to rearrange the reduction sequence in order to be able to retranslate it into SHF. Partial \nTransactions that do not contain a (writeStart) within the reduction sequence can be eliminated and thereby \nreplaced by interspersed special transformations using Lemma 4.11, which again can be eliminated from \nthe successful reduction sequence by Lemma 4.10. If the partial transaction contains a (writeStart), \nthen the missing reduction steps can be added within the reduction sequence before a successful process \nis reached, since the commit\u00adphase does not change the successful-property of processes. Aborted Transactions \ncan be omitted since they are replaceable by special transformations, which again can be removed. Grouping \nTransactions: We can now assume that within the reduc- CSHF,* tion sequence .(P1) - ---. P2 where P2 \nis successful, all trans\u00adactions are completed, and that there are no aborted ones. Now we rearrange \nthe reduction sequence: The (writeEnd)-reduction step is assumed to be the point of attraction for every \ntransaction. Moving single reduction steps starts from the rightmost non-grouped trans\u00adaction. For this \nu-transaction, we move the reduction steps that be\u00adlong to it in the direction of its (writeEnd), i.e., \nto the right. The reduction steps that belong to the same transaction and are between (writeStart) and \n(writeEnd) can be moved to the right squeezing out the non-u-reduction steps, which is possible, since \nthe locking prevents other transaction reduction steps of other threads to be in between, and since there \nare no functional reduction steps in be\u00adtween. Now we speak of the u-block of reduction steps, if every \nre\u00adduction step in it belongs to the same transaction and ends with (writeStart). The block is complete, \nif the .rst (leftmost) reduc\u00adtion step is (atomic). The interesting part of the reduction sequence a \nis like -.; S; Bu , where Bu is the u-block; S is a reduction se\u00ad a quence of non-u-reduction steps and \n-. is the reduction step that we want to move to the right, before Bu . For the reduction steps for u \nthat are before (writeStart), we distinguish functional and non-functional reductions. Every standard \nfunctional reduction is triggered (or requested) by one or more threads. This must be taken into account \nwhen reasoning about shifting functional transactional reduction steps within reduction sequences. Another \nissue is which reductions are in S. Since we started with the rightmost transac\u00adtion, the steps in S \nare either non-transactional ones, or if they are transactional ones, then these are for another thread. \nIt is obvious that S does not contain a commit-phase of a transaction that aborts  u. Thus -a . cannot \nbe a u-read of a TVar that is committed in S; it can be a u-write of a TVar, even if writing this TVar \nis committed in S, since there is no registration of threads at written TVars, and a so the rules can \nbe interchanged. Hence, if -. is a non-functional transaction step, then it can be moved to the u-block. \nIf -a . is a functional transaction that is only requested by u, then it will also a be moved to the \nu-block. Now assume that -. is a functional trans\u00adaction that is also requested by another transaction \nof thread u ' . If all such threads are either u or their blocks are to the right of the u-block, then \nwe move the reduction in front of the u-block. Oth\u00aderwise, i.e, if at least one requesting transaction \nis in S, then -a . is now a part of S. Thus the general situation is: the u-block is the current focus \nof shifting and there may be more blocks to the right of this block; S consists of functional reductions \nrequested by threads in S, and of other reduction steps. Also in this general case the following a holds: \nfunctional reductions -. that are only requested by u, can be moved to the u-block. Finally, there will \nbe an (atomic)-reduction, and after the move, we have a complete u-block that corresponds to a complete \ntransaction. If the moving is done exhaustively, then the reduction sequence is almost in a form that \ncan be backtranslated into SHF . Every reduction steps that are not within transactions for a thread \ncan be backtranslated as a SHF -reduction. The last issue in the backtranslation are the local retries \nin orElse-expressions in CSHF , which do not have an effect on the TVars, but may apply functional reductions \nto the bindings, however, in SHF the orElse-retries are without any effect. We use Lemma 4.10 to correctly \nbacktranslate transactions, in particular (orElse)-reductions, including the retries in the transac\u00adtion. \nThe backtranslation is done from left to right. The plan is to show that the orElse-computation tree \nthat is done sequen\u00adtially in CSHF , will also be performed in the same sequence in SHFA,* SHF . A difference \nare that - ---.-reductions are part of the re\u00adductions in CSHF . Therefore, consider the case that evaluation \nof the left expression e1 in a orElse ends in a retry. In CSHF , all the functional reductions in the \nreduction sequence corresponding to e1 can be shifted in the reduction sequence to the right end of the \ncomplete reduction sequence, which can then be ignored leav\u00ading the sequence successful. Using induction \non the depth of the orElse-expressions, we can show that there is a backtranslation of the transaction \nfor thread u as an atomic reduction in SHF . From a bird eyes point of view, a successful CSHF-reduction \nof a process P is .rst rearranged into another a successful CSHF \u00adreduction, and then an interleaved \nrearrangement and backtrans\u00adlation leads to a converging SHF -reduction sequence for P as a SHF -process. \nCorollary 4.15. Let P be an SHF-process. Then P . .. .(P ).CSHF .  4.3 Re.ection of Should-Convergence \nInstead of proving re.ection of should-convergence directly, we equivalently show preservation of may-divergence. \nThe proof is analogous to the preservation of may-convergence in Theo\u00adrem 4.13, where now, however, the \nreduction sequence ends in a must-divergent process. Using Corollary 4.15 as a base case shows that may-divergence \nis preserved: Theorem 4.16. For all SHF -processes P , we have P . =. .(P ).CSHF . 4.4 Preservation \nof Should-Convergence This is the last part of the analysis. A needed lemma is the follow\u00ading (proved \nin [14]), which shows that partial transactions can be eliminated: CSHF,* Lemma 4.17. Let P0 be a nontransactional \nprocess, P0 - ---. Q P1 with P1.CSHF , such that P -. P1 is a suf.x of the reduction Q and P -. P1 is \na partial transaction for a thread u starting with (atomic) but the commit-phase and the retry-phase \nis missing, i.e. there is no reduction (writeStart) nor a (retryCGlob) for thread u. Then P .CSHF . Now \nwe show re.ection of of may-divergence, which is equiv\u00adalent to preservation of should-convergence. Theorem \n4.18. For all SHF -processes P , we have =. P .. .(P ).CSHF Proof. Assume given a non-transactional CSHF \n-process .(P ) CSHF,* with .(P ) - ---. P1 and The reasoning is P1.CSHF . as in Theorem 4.14 with some \ndifferences, since we have to ensure the condition P1.CSHF , which is more complex than the successful \n-condition. Partial transactions: If it includes a (writeStart) or a (retryCGlob), then the transaction \ncan be com- CSHF,* pleted by P1 - ---. P1 ' where P1 ' .CSHF , and so we can assume that these do not \nexist. If the partial transaction does neither include a (writeStart) nor a (retryCGlob), then using \nthe same arguments as in Theorem 4.14, we see that we can assume that the partial transac\u00adtion is grouped \nbefore P1, i.e. the transaction ends with the partial CSHF,* '' '' transaction P1 - ---. P1. Lemma 4.17 \nshows that P 1 .CSHF , which permits to assume that the partial transaction can be omit\u00adted. Aborted \ntransactions: can be omitted since they are replace\u00adable by special transformations, which again can \nbe removed due to Lemma 4.10. Grouping transaction: same arguments as in the proof of Theorem 4.14. Final \nretranslation: similar to the proof of Theorem 4.14.  4.5 Summary We have proved in Theorems 4.13, 4.14, \n4.16, and 4.18 that the translation . mapping SHF -processes into CSHF -processes is convergence equivalent. \nThus we have proved Theorem 4.4 and thus also Main Theorem 4.3 (observational correctness and ade\u00adquacy \nof .), since . is compositional. This shows that CSHF is a correct evaluator for SHF -processes w.r.t. \nthe big-step semantics. Instead of permitting to retry a transaction too often, the strategy from [7] \ncan be applied: activate retried transactions only if some of the read TVars is modi.ed. Our proofs can \nbe used to show that this restriction of reductions is equivalent to the CSHF -semantics. Note that omitting \nthe send retry for abortion would make the implementation incorrect (non-adequate): a thread that runs \ninto a loop if TVar x contains a 1 and returns otherwise, may block this thread inde.nitely, which is \nnot the case in the speci.cation SHF . 5. Related Work General remarks on STM are in [4, 8]. [4] argue \nthat more research is needed to reduce the runtime overhead of STM. We believe that ease of maintenance \nof programs and the increased concurrency provided by STM may become more important in the future.  \nThe paper which strongly in.uenced our work is [7]. SHF borrows from the operational semantics of STM \nHaskell, where differences are that our calculus is extended by futures, mod\u00adels also the call-by-need \nevaluation, but does not include excep\u00adtions and is restricted to monomorphic typing. However, since \nfu\u00adtures can be safely implemented in Concurrent Haskell by using unsafeInterleaveIO [13], this difference \nis rather small. More\u00ad over, our correctness proofs do not rely on having futures in the language and \nthus would also hold, if threads only have a thread identi.er but not a resulting value. Harris et al. \n[7] describe the current implementation of STM Haskell in the Glasgow Haskell Compiler (GHC), however \nno for\u00admal treatment of this implementation is given. The approach taken in the GHC implementation is \nclose to ours with the difference that instead of aborting transactions by the committing transaction \n(i.e. sending retries in CSHF), transactions abort and restart themselves by temporarily checking their \nlocal transaction log against the sta\u00adtus of the global memory, and thus detecting con.icts. This is \ncom\u00adparable to our approach, and we are convinced that the implemen\u00adtations are closely related. However, \nmodelling their approach in a formal semantics would require more effort: For instance, in the GHC implementation \ncomparing the local content (stored in the transaction log) and the global content of a TVar is done \nby pointer equality. Including this equality either has to be de.ned as a side condition of a reduction \nrule or has to be built in as a language construct. One also has to make design decisions like the follow\u00ading: \nShould indirections (i.e. bindings like x = y) be respected, by the pointer equality test? For example, \nif the local content of a TVar is x and the global content is y where the bindings are x = z | z = s \n| y = w | w = z, are x and y equal? Semanti\u00adcally, this is correct (but hard to express in a small-step \nsemantics). Testing such cases in GHC s STM implementation shows that indi\u00adrections are not respected \nin the current implementation. In contrast the semantics of CSHF does not need to compare pointers, since \nit uses the registration mechanism. A difference be\u00adtween our semantics and STM-Haskell is that in CSHF \nreading the content of a TVar and thereafter writing the same content is always a modi.cation which may \nresult in aborting other transac\u00adtions, while in STM-Haskell perhaps the pointer remains the same and \nthus no con.ict modi.cation is detected. Thus our approach may have slightly more restarts than the STM-Haskell \nimplemen\u00adtation. A potential semantical problem in [7] is that exceptions may make local values of TVars \nvisible outside the transaction. A semantical investigation of STM is in [2], where a call-by\u00ad name functional \ncore language with concurrent processes is de\u00ad.ned, and a contextual equivalence is used as equality. \nAlso strong results are obtained by proving correctness of the monad laws and other program equivalences \nw.r.t. their semantics. However, [2] only considers may-convergence in the contextual equivalence, which \nis too weak for reasoning about non-deterministic (and con\u00adcurrent) calculi. Also, seq is missing in \n[2] which is used in Haskell and known to change the semantics, such that validity of the monad laws \nonly holds under further typing restrictions (see [12]). A fur\u00ad ther difference to our work is that [2] \nuse pointer equality. [5] pro\u00ad pose opacity (transactions can only see a committed state of all TVars), \nwhich is satis.ed by CSHF; the complexity bounds from [5] are not applicable to CSHF, since we use visible \nreads. [9] propose to investigate correctness of an implementation, but stick to testing. [1] consider \ncorrectness of implementing STM in a small calculus with call-by-value reduction and a monadic exten\u00adsion \nsimilar to the STM/IO-extension in [7]. The main reasoning tool is looking for traces of effects, and \narguing about commut\u00ading and shifting the effects within traces, where several important properties are \nproved. It is hard to compare the results with ours, however, from an abstract level, their proof method \nappears to ig\u00adnore the should-convergence restriction: There is no argument on forced aborts of transactions. \n[3] investigate liveness properties and show that certain combina\u00adtions of good properties are impossible \nin a model using in.nite traces, random aborts and without fairness conditions. 6. Conclusion We have \npresented a big-step semantics for STM-Haskell as a speci.cation, and a small-step concurrent implementation. \nUsing formal reasoning and the strong notion of contextual equivalence with may-and should-convergence \nwe prove correctness of the implementation. Further research directions are to consider smarter strategies \nfor earlier aborts and retries of con.icting transactions, extending the language, e.g. by exceptions, \nand a polymorphic type system. References [1] A. Bieniusa and P. Thiemann. Proving isolation properties \nfor software transactional memory. In Proc. ESOP 11, LNCS 6602, pp. 38 56, Springer 2011. [2] J. Borgstr \n\u00a8 A compositional om, K. Bhargavan, and A. D. Gordon. theory for STM Haskell. In Proc. Haskell 09, pp. \n69 80. ACM, 2009. [3] V. Bushkov, R. Guerraoui, and M. Kapalka. On the liveness of transactional memory. \nIn Proc. PODC 12, pp. 9 18. ACM, 2012. [4] C. Cascaval, C. Blundell, M. M. Michael, H. W. Cain, P. Wu, \nS. Chiras, and S. Chatterjee. Software transactional memory: why is it only a research toy? Commun. ACM, \n51(11):40 46, 2008. [5] R. Guerraoui and M. Kapalka. On the correctness of transactional memory. In Proc. \nPPoPP 08, pp. 175 184. ACM, 2008. [6] T. Harris, S. Marlow, S. Peyton Jones, and M. Herlihy. Composable \nmemory transactions. In Proc. PPoPP 05, pp. 48 60. ACM, 2005. [7] T. Harris, S. Marlow, S. L. Peyton \nJones, and M. Herlihy. Composable memory transactions. Commun. ACM, 51(8):91 100, 2008. [8] T. Harris, \nJ. R. Larus, and R. Rajwar. Transactional Memory, 2nd edition. Synthesis Lectures on Computer Architecture. \nMorgan &#38; Claypool Publishers, 2010. [9] L. Hu and G. Hutton. Towards a veri.ed implementation of \nsoftware transactional memory. In Proc. TFP 08, vol. 9, pp. 129 144. Intellect, 2009. [10] A. Rensink \nand W. Vogler. Fair testing. Inform. and Comput., 205(2): 125 198, 2007. [11] D. Sabel and M. Schmidt-Schau\u00df. \nA call-by-need lambda-calculus with locally bottom-avoiding choice: Context lemma and correctness of \ntransformations. Math. Structures Comput. Sci., 18(03):501 553, 2008. [12] D. Sabel and M. Schmidt-Schau\u00df. \nA contextual semantics for Con\u00adcurrent Haskell with futures. In Proc. PPDP 11, pp. 101 112. ACM, 2011. \n[13] D. Sabel and M. Schmidt-Schau\u00df. Conservative concurrency in Haskell. In Proc. LICS 12, pp. 561 570. \nIEEE, 2012. [14] M. Schmidt-Schau\u00df and D. Sabel. Correctness of an STM Haskell implementation. Frank \nreport 50, Institut f. Informatik. Goethe-University Frankfurt, 2013. available at http://www.ki. informatik.uni-frankfurt.de/papers/frank/. \n[15] M. Schmidt-Schau\u00df, J. Niehren, J. Schwinghammer, and D. Sabel. Adequacy of compositional translations \nfor observational semantics. In Proc. IFIP TCS 08, IFIP 273, pp. 521 535. Springer, 2008. [16] N. Shavit \nand D. Touitou. Software transactional memory. In Proc. PODC 95, pp. 204 213. ACM, 1995. [17] N. Shavit \nand D. Touitou. Software transactional memory. Distributed Computing, Special Issue, 10:99 116, 1997. \n[18] M. C. J. D. van Eekelen, M. J. Plasmeijer, and J. E. W. Smetsers. Parallel graph rewriting on loosely \ncoupled machine architectures. In Proc. CTRS 90, LNCS 516, pp. 354 369. Springer, 1990.  \n\t\t\t", "proc_id": "2500365", "abstract": "<p>A concurrent implementation of software transactional memory in Concurrent Haskell using a call-by-need functional language with processes and futures is given. The description of the small-step operational semantics is precise and explicit, and employs an early abort of conflicting transactions. A proof of correctness of the implementation is given for a contextual semantics with may- and should-convergence. This implies that our implementation is a correct evaluator for an abstract specification equipped with a big-step semantics.</p>", "authors": [{"name": "Manfred Schmidt-Schau&#223;", "author_profile_id": "81100302294", "affiliation": "Goethe-University, Frankfurt, Germany", "person_id": "P4261237", "email_address": "schauss@ki.cs.uni-frankfurt.de", "orcid_id": ""}, {"name": "David Sabel", "author_profile_id": "81384608645", "affiliation": "Goethe-University, Frankfurt, Germany", "person_id": "P4261238", "email_address": "sabel@ki.cs.uni-frankfurt.de", "orcid_id": ""}], "doi_number": "10.1145/2500365.2500585", "year": "2013", "article_id": "2500585", "conference": "ICFP", "title": "Correctness of an STM Haskell implementation", "url": "http://dl.acm.org/citation.cfm?id=2500585"}