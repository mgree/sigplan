{"article_publication_date": "09-25-2013", "fulltext": "\n Unifying Re.nement and Hoare-Style Reasoning in a Logic for Higher-Order Concurrency Aaron Turon Derek \nDreyer Lars Birkedal MPI-SWS MPI-SWS Aarhus University turon@mpi-sws.org dreyer@mpi-sws.org birkedal@cs.au.dk \n Abstract Modular programming and modular veri.cation go hand in hand, but most existing logics for concurrency \nignore two crucial forms of modularity: higher-order functions, which are essential for building reusable \ncomponents, and granularity abstraction, a key technique for hiding the intricacies of .ne-grained concurrent \ndata structures from the clients of those data structures. In this paper, we present CaReSL, the .rst \nlogic to support the use of granularity abstraction for modular veri.cation of higher-order concurrent \npro\u00adgrams. After motivating the features of CaReSL through a variety of illustrative examples, we demonstrate \nits effectiveness by using it to tackle a signi.cant case study: the .rst formal proof of (partial) correctness \nfor Hendler et al. s .at combining algorithm. Categories and Subject Descriptors D.3.1 [Programming Lan\u00adguages]: \nFormal De.nitions and Theory; F.3.1 [Logics and Mean\u00adings of Programs]: Specifying and Verifying and \nReasoning about Programs Keywords Contextual re.nement; higher-order functions; .ne\u00adgrained concurrency; \nseparation logic; Kripke logical relations. 1. Introduction Over the past decade, a number of Hoare logics \nhave been devel\u00adoped to cope with the complexities of concurrent programming [20, 32, 7, 3, 18, 4, 27]. \nUnsurprisingly, the name of the game in these logics is improving support for modular reasoning along \na vari\u00adety of dimensions. Concurrent Abstract Predicates (CAP) [3], for example, utilizes a deft combination \nof separation logic [26] (for spatially-modular reasoning about resources and ownership), rely\u00adguarantee \n[15] (for thread-modular reasoning in the presence of interference), and abstract predicates [21] (for \nhiding invariants about a module s private data structures from its clients). At the same time, of course, \nthe importance of modularity is not restricted to veri.cation. Programmers use modularity in the design \nof their concurrent programs, precisely to enable reasoning about individual program components in relative \nisolation. And indeed, certain aspects of advanced concurrency logics, such as their aforementioned use \nof abstract predicates, are geared toward building proofs that re.ect the data abstraction inherent in \nwell\u00addesigned programs. Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. Copyrights \nfor components of this work owned by others than the author(s) must be honored. Abstracting with credit \nis permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. Request permissions from permissions@acm.org. ICFP 13, September \n25 27, 2013, Boston, MA, USA. Copyright is held by the owner/author(s). Publication rights licensed to \nACM. ACM 978-1-4503-2326-0/13/09. . . $15.00. http://dx.doi.org/10.1145/2500365.2500600  We contend, \nhowever, that existing concurrency logics are not exploiting the modular design of sophisticated concurrent \nprograms to full effect. In particular, we observe that there are two crucial di\u00admensions of modular \nconcurrent programming that existing logics provide no way to leverage in the construction of proofs: \nnamely, higher-order functions and granularity abstraction. Higher-order concurrency Higher-order functional \nabstraction is of course one of the most basic hammers in the modern program\u00admer s toolkit for writing \nreusable and modular code. Moreover, a number of concurrent programming patterns rely on it: work steal\u00ading \n[2], Concurrent ML-style events [25], concurrent iterators [16], parallel evaluation strategies [29], \nand monadic approaches to con\u00ad current programming [10], just to name a few. Yet, veri.cation of higher-order \nconcurrent programs remains a largely unexplored topic. To our knowledge, only a few existing logics \ncan handle higher-order concurrent programs [27, 17, 14], but these logics are limited in other ways \nin particular, they do not presently support veri.cation of .ne-grained concurrent ADTs. This leads us \ndirectly to the second limitation we observe of the state of the art, concerning granularity abstraction. \nGranularity abstraction via contextual re.nement An easy way to adapt a sequential mutable data structure \nfor concurrent access is to employ coarse-grained synchronization: use a single global lock, and instrument \neach of the operations on the data structure so that they acquire the lock before they begin and release \nit after they complete. On the other hand, more sophisticated implementations of concurrent data structures \nemploy .ne-grained synchronization: they protect different parts of a data structure with different locks, \nor avoid locking altogether, so that threads can access disjoint pieces of the data structure in parallel. \nThere may seem at .rst glance to be a fundamental trade\u00adoff here. Fine-grained synchronization enables \nparallelism, but makes the data structures that use it very tricky to reason about di\u00adrectly, due to \ntheir complex internal coordination between threads. Coarse-grained synchronization sequentializes access \nto the data structure, which is bad for parallelism but perfect for client-side reasoning, since it enables \nclients to reason about concurrent ac\u00adcesses as if each operation takes effect atomically. Fortunately, \nmodular programming comes to the rescue. In par\u00adticular, so long as tricky uses of .ne-grained synchronization \nare con.ned to the hidden state of a carefully crafted ADT, it is possible to prove that the .ne-grained \nimplementation of the ADT is a con\u00adtextual re.nement of some coarse-grained implementation. Contex\u00adtual \nre.nement means that, assuming clients only access the ADT through its abstract interface (so that the \nstate really is hidden), ev\u00adery behavior that clients can observe of the .ne-grained implemen\u00adtation \nis also observable of the coarse-grained one. Thus, clients can pretend, for the purpose of simplifying \ntheir own veri.cation, that they are using the coarse-grained version, yet be sure that their code will \nstill be correct when linked with the more ef.cient .ne-grained version. This is what we call granularity \nabstraction. (Note: gran\u00adularity abstraction is similar to atomicity abstraction [19], but is more general \nin that, as we will see in the iterator example later in this section, it applies even if the target \nof the abstraction is only somewhat coarse-grained.) To illustrate this point more concretely, let us \nconsider a simple motivating example of reasoning about Treiber s stack [28]. (We will in fact use this \nvery example as part of a larger case study later in the paper.) Treiber s stack is a .ne-grained implementation \nof a concurrent stack ADT. Instead of requiring concurrently executing push and pop operations to contend \nfor a global lock on the whole stack (as a coarse-grained implementation would), Treiber s imple\u00admentation \nallows them to race to access the head of the stack using compare-and-set (CAS). (The implementation \nof Treiber s stack is shown in Figure 9, and discussed in detail in \u00a73.3.) Now, the reader may expect \nthat stacks should admit a canoni\u00adcal, principal speci.cation (spec, for short), perhaps something like \nthe following precise spec which tracks the exact contents s of the stack using the abstract predicate \nCon(s): {Con(s)} push(x) {Con(x :: s)} {Con(s)} pop() {ret. (ret = none . s = nil . Con(s)) ' . (.x, \ns.ret = some(x) . s = x :: s' . Con(s'))} The trouble with using this spec in a concurrent setting is \nthat knowledge about the exact contents of the stack is not stable under interference from other threads. \nAs a result, some concurrency logics prohibit this spec altogether. Others permit the spec, but force \nthe Con(s) predicate to be treated as a resource that only one thread can own at a time, thus effectively \npreventing any concurrent access to the stack and defeating the point of using Treiber s stack in the \n.rst place! We want instead to capture the idea that the client threads of a data structure interact \nwith it according to some (application\u00adspeci.c) protocol. Take, for example, the following per-item spec, \nwhich abstracts away from the LIFO nature of the stack and instead imposes an item-level protocol: .x. \n{p(x)} push(x) {true}. {true} pop() {ret. ret = none . (.x. ret = some(x) . p(x))} Given an arbitrary \npredicate p of the client s choice, this per-item spec asserts that the stack contains only elements \nthat satisfy the predicate p. It is pleasantly simple, and suf.cient for the purposes of the case study \nwe present later in the paper. It should be clear, however, that this per-item spec is far from a canonical \nor prin\u00adcipal speci.cation of stacks: the same spec would also be satis.ed, for instance, by queues. \nThis brings us to our key point: different clients have different needs, and so we may want to verify \nthe stack ADT against a range of different Hoare-style specs, but we do not want to have to re\u00adverify \nTreiber s implementation each time. Ideally, we would like to modularly decompose the proof effort into \ntwo parts: 1. Prove that Treiber s implementation is a contextual re.nement of a coarse-grained, lock-based \nimplementation, which serves as a simple reference implementation of the stack ADT. 2. Use Hoare-style \nreasoning to verify that this reference imple\u00admentation satis.es the various specs of interest to clients. \n This decomposition engenders a clean separation of concerns, con\u00ad.ning the dif.culty of reasoning about \nTreiber s particular imple\u00admentation1 to the proof of re.nement, and simplifying the veri.ca\u00adtion of \nthe stack ADT against different client specs. The story we have just told is, in principle, nothing new. \nThe ability to prove granularity abstraction via contextual re.nement has been accepted in the concurrency \nliterature as a useful correct\u00ad 1 Treiber s stack is actually one of the easiest .ne-grained data structures \nto reason about, but our general line of argument applies equally well to more sophisticated implementations, \nsuch as the HSY elimination stack [12]. ness criterion for tricky concurrent data structures, precisely \nbe\u00adcause of the modular decomposition of proof effort that it ought to facilitate [13, 8, 9]. Unfortunately, \ndespite the utility of such a modular decomposition in theory, no existing concurrency logic ac\u00adtually \nsupports it in practice. In particular, very few systems sup\u00adport proofs of contextual re.nement at all, \nand those few that do support re.nement proofs such as the recent work of Liang and Feng [18] do not \nprovide a means of composing re.nement with client-side Hoare-style veri.cation in a uni.ed logic. Granularity \nabstraction for higher-order functions Although supporting granularity abstraction is already a challenge \nfor .rst\u00adorder concurrent programs, it becomes even more interesting for higher-order concurrent programs. \nSuppose, for instance, we wished to add a higher-order iterator, iter, to the concurrent stack ADT. (Such \nconcurrent iterators are already commonplace [16].) Adding iter to Treiber s implementa\u00adtion is trivial: \niter(f) will (without acquiring any lock) just read the current head pointer of the stack and apply f \nto each element of the stack accessible from it. But how do we specify the behavior of this iterator? \nWhat reference implementation should we use in proving granularity abstraction? Unlike for push and pop, \nit does not make sense for the reference implementation of iter(f ) to be maximally coarse-grained (i.e., \nto execute entirely within a crit\u00adical section) because that will not correspond to the reality of the \nimplementation we just described. In particular, the Treiber imple\u00admentation does not freeze modi.cations \nto the stack while f is be\u00ading applied to each element, so it does not contextually re.ne the maximally \ncoarse-grained implementation of iteration. Rather, what the Treiber iterator guarantees is that f will \nbe applied to all the elements that were accessible from some node that was the head of the stack at \nsome point in time. A clean way to specify this behavior is via a reference implementation that (1) acquires \nthe lock, (2) takes a snapshot (i.e., makes a copy) of the stack, (3) releases the lock, and (4) iterates \nf over the snapshot. (For the code of this reference implementation, see Figure 9.) What makes this reference \nimplementation so intriguing is that it is only somewhat coarse-grained, yet as we will show later in \nthe paper, it is nonetheless quite useful as a target for granularity abstraction. This example demonstrates \nthe .exibility of re.nement, a .exi\u00adbility which has not heretofore been tested, since no one has previ\u00adously \napplied granularity abstraction to higher-order ADTs. CaReSL: A logic for higher-order concurrency In \nthis paper, we present CaReSL (pronounced carousel ), the .rst logic to support the use of granularity \nabstraction for modular veri.cation of higher-order concurrent programs. In providing both re.nement \nand Hoare-style reasoning, CaReSL builds on ideas from two distinct lines of research: Kripke logical \nrelations [22, 1, 5]. Logical relations are a funda\u00ad mental technique for proving re.nement in languages \nwith e.g., higher-order functions, polymorphism, and recursive types. Logical relations explain observable \nbehavior in terms of the logical interpretation of each type. For example, two functions of type t . \nt ' are logically related if giving them related in\u00adputs of type t implies that they will produce related \nresults of type t'. Kripke logical relations are de.ned relative to a possi\u00adble world that describes \nthe relationship between the internal, hidden state of a program and that of its spec.  Concurrent separation \nlogics [20, 32, 3]. Separation logic is a reimagining of Hoare logic in which assertions are understood \nrelative to some portion of the heap, with the separating con\u00adjunction P * Q dividing up the heap between \nassertions P and  Q. The motivation for separation is local reasoning: pre-and post-conditions need \nonly mention the relevant portion of the  foreach satis.es Spinlock satis.es Hoare spec (\u00a73.1) Hoare \nspec (\u00a73.2)  CG stack satis.es Per-item Hoare spec (\u00a73.3)  Treiber re.nes CG stack (\u00a73.3) Figure \n1. The structure of the case study and paper heap, and the rest of the heap can be presumed invariant. \nMost concurrent separation logics also add some form of protocols (e.g., rely-guarantee), which facilitate \ncompositional reasoning by constraining the potential interference between threads op\u00aderating concurrently \non some shared portion of the heap. Rather than a combination of these approaches, CaReSL is an attempt \nto unify them. So, for example, the logic does not include re.nement as a primitive notion. Instead, \nre.nement is derived from Hoare-style reasoning: to prove that e re.nes e ', one merely proves a particular \nHoare triple about e, allowing the tools of concurrent separation logic to be exploited in the proof \nof logical relatedness. In the other direction, CaReSL is a modal logic with the possible worlds of Kripke \nlogical relations, which can in turn be used to express the shared-state protocols of concurrent separation \nlogics. The uni.cation yields surprising new techniques, such as the use of modal necessity (D) to hide \nresources that an ADT s client would otherwise have to thread through their reasoning (\u00a73.2).2 The semantics \nof CaReSL is derived directly from the model of Turon et al. [31], who presented the .rst formal proofs \nof re\u00ad .nement for .ne-grained data structures in an ML-like setting (al\u00adthough they did not use it to \nreason about higher-order examples). Compared with Turon et al. s work, which was purely semantic, CaReSL \nprovides a syntactic theory for carrying out re.nement proofs at a much higher level of abstraction. \nThis proof theory, in turn, is inspired by Dreyer et al. s LADR [5], a modal logic for reasoning about \ncontextual equivalence of (sequential) stateful ML programs. CaReSL exempli.es how the key ideas of LADR \nare just as relevant to and arguably even more compelling when adapted to the concurrent setting. To \ndemonstrate the effectiveness of CaReSL, we use it to tackle a signi.cant case study: namely, the .rst \nformal proof of (partial) correctness for Hendler et al. s .at combining algorithm [11]. Flat combining \nprovides a generic way to turn a sequential ADT into a relatively ef.cient concurrent one, by having \ncertain threads perform in one fell swoop the combined actions requested by a whole bunch of other threads. \nThe .at combining algorithm is interesting not only because it itself is a rather sophisticated higher\u00adorder \nfunction, but also because it is modularly assembled from other higher-order components, including a \n.ne-grained stack with concurrent iterator. We therefore take the opportunity to verify .at combining \nin a modular way that mirrors the modular structure of its implementation. Figure 1 illustrates the high-level \nstructure of our proof, which involves an intertwined application of re.nement and Hoare-style veri.cation. \nFor example, we prove that Treiber s stack re.nes the coarse-grained (CG) reference implementation of \nstacks; we then use Hoare-style reasoning to prove that the reference implementa\u00adtion satis.es the per-item \nspec; and .nally we rely on the per-item spec in the proof that the .at combining algorithm re.nes an \neven 2 Previously, such hiding required the subtle anti-frame rule [24]. Flat combiner re.nes CG wrapper \n(\u00a74)  Syntax Val v ::= () | true | false | (v, v) | inji v | rec f(x).e | ..e | \u00a3 Exp e ::= v | if \ne then e else e | e e | e | (e, e) | prji e | inji e | case(e, inj1 x . e, inj2 y . e) | new e | get \ne | e := e | CAS(e, e, e) | newLcl | getLcl(e) | setLcl(e, e) | fork e CType s ::= B | ref t | refLcl \nt | \u00b5a.s Type t ::= s | 1 | a | t \u00d7 t | t + t | \u00b5a.t | .a.t | t . t ECtx K ::= [ ] | if K then e else \ne | K e | v K | \u00b7 \u00b7 \u00b7 Typing contexts . ::= \u00b7 | ., a G ::= \u00b7 | G, x : t O ::= .; G Well-typed expressions \n.; G f e : t O f e : ref s O f eo : s O f en : s O f e : .a.t O f CAS(e, eo, en) : B O f e : t[t ' /a] \nMachine con.gurations TLV L . N .n. Value TPool E . N .n. Expression Heaps h . N .n. (Val . TLV) Con.g \n. ::= h; E none i /. dom(L) Thread-local lookup lLJ(i) 6 some(L(i)) otherwise Pure reduction pure \n' e . e purepure (rec f(x).e) v . e[rec f(x).e/f , v/x] (..e) . e i ' Per-thread reduction h; e . h \n' ; e ipure ' ' h; e . h; e when e . e i h; newLcl . h 1 [\u00a3 . \u00d8]; \u00a3 i h 1 [\u00a3 . L]; setLcl(\u00a3, v) . h 1 \n[\u00a3 . L[i := v]]; () i h; getLcl(\u00a3) . h; v when h(\u00a3) = L, lLJ(i) = v i h; CAS(\u00a3, vo, vn) . h; false when \nh(\u00a3) = vo i h 1 [\u00a3 . vo]; CAS(\u00a3, vo, vn) . h 1 [\u00a3 . vn]; true ' General reduction h; E . h ' ; E i \n' h; e . h ' ; e h; E 1 [i . K[e]] . h ' ; E 1 [i . K[e ' ]] h; E 1 [i . K[fork e]] . h; E 1 [i . K[()]] \n1 [j . e] Figure 2. The calculus: F \u00b5! with fork, CAS, and thread-local refs higher-level abstraction. \nEvery component of the case study in\u00advolves higher-order code and therefore higher-order speci.cations. \n2. The programming model The programming language for our program logic is sketched in Figure 2; the \nfull details are in the appendix [30]. It has a stan\u00ad dard core: the polymorphic lambda calculus with \nsums, products, references, and equi-recursive types. We elide all type annotations in terms, but polymorphism \nis nevertheless introduced and elimi\u00adnated by explicit type abstraction (..e) and application (e ). To \nthis standard core, we add concurrency (through a fork construct), atomic compare-and-set (CAS) and thread-local \nreferences (type refLcl t ). While normal references provide shared state through which threads can communicate, \nthread-local references allow multiple threads to access disjoint values in memory (a different one for \neach thread) using a common location. Formally, each thread-local reference has an associated thread \nID-indexed table L, which is initially empty. Thus for refLcl t , the getLcl operation returns an option \nof type 1 + t , re.ecting whether the thread-local reference has been initialized for the thread that \ninvoked it.3 Thread-local ref\u00aderences are commonly used when a single data structure needs mu\u00adtable, \nthread-local storage for an unbounded number of threads; the .at combining algorithm presented in \u00a74 \nembodies this technique. We de.ne a small-step, call-by-value operational semantics, using evaluation \ncontexts K to specify a left-to-right evaluation order within each thread. The pure . relation gives \nthe reductions that i do not interact with the heap, while . gives general, single-thread reductions \nfor the thread with ID i. These reductions are then lifted to general reduction . of machine con.gurations \n., which consist of a heap h together with an ID-indexed pool E of threads. The type system imposes two \nimportant restrictions. First, re\u00adcursive types \u00b5a.t must be guarded, meaning that all free occur\u00adrences \nof a in t must appear under a non-\u00b5 type constructor. Sec\u00adond, because CAS exposes physical equality \ncomparison of word\u00adsized values at the hardware level, it can only be applied to refer\u00adences of comparable \ntype s, which we take to be base types and locations. If O f eI : t and O f eS : t, we say eI contextually \nre.nes eS , written O |= eI : eS : t , if, for every pair of thread IDs i and j, .C : (O, t ) ' (\u00d8, B). \n.n..EI . \u00d8; [i . C[eI ]] .* hI ; [i . v] 1 EI =. .ES . \u00d8; [j . C[eS ]] .* hS ; [j . v] 1 ES where C : \n(O, t ) . (O ' , t ' ) is the standard typing judgment for contexts (guaranteeing that O f e : t implies \nO ' f C[e] : t '). This de.nition resembles the standard one for sequential languages, but take note \nthat only the termination behavior of the main thread is observed; as with most languages in practice, \nwe assume that once the main thread has .nished, any forked threads are aborted. 3. CaReSL While CaReSL \nultimately provides mixed re.nement and Hoare\u00adstyle reasoning about higher-order concurrent programs, \nit is eas\u00adiest to understand by .rst considering less powerful sublogics , and then gradually incorporating \nmore powerful features: We begin with a core logic that provides separation-logic rea\u00adsoning for sequential \n(but higher-order) programs (\u00a73.1). In this core logic, both Hoare triples and heap assertions live in \na single syntactic class: they are propositions. Thus propositions include e.g., implications between \nHoare triples, which are ubiquitous when reasoning about higher-order code. Propositions also in\u00adclude \n.rst-and second-order quanti.cation, guarded recursion, separating conjunction, and modal necessitation \nall standard concepts whose interplay we explain in \u00a73.1.  We then add concurrency, leading to a concurrent \nseparation logic for higher-order programs (\u00a73.2). The new concept needed to deal with concurrency is \nthat of a protocol governing some shared region of the heap. We express protocols through tran\u00adsition \nsystems whose states provide an abstraction of the heap region s actual state. Threads can gain or lose \nroles in the pro\u00adtocol, which determine the transitions those (and other) threads are permitted to take. \nTraditional rely/guarantee reasoning can be recovered as a particular, restricted way of using protocols. \n Finally, we show how to reason about re.nement, yielding a Concurrent and Re.ned Separation Logic (CaReSL) \nfor higher-order programs (\u00a73.3). As explained in the introduction, CaReSL does not simply add re.nement \nas a primitive notion on top of a concurrent separation logic. Instead, CaReSL treats re.nement as a \nderived notion, expressed as a particular Hoare\u00adstyle speci.cation. In order to do so, however, CaReSL \ndoes require one further extension: the notion of spec resources ,  3 Throughout, we let none 6 inj1 \n() and some(e) 6 inj2 e. .rst introduced by Turon et al. [31], which allow pieces of a program con.guration \n(heap and threads) to be manipulated as logical resources within Hoare-style proofs. While CaReSL is \nthe main contribution of the paper, its sublogics are of independent interest. We explain them by way \nof idiomatic higher-order examples, each of which serves as a component of the case study in \u00a74. 3.1 \nCore logic: Sequential higher-order programs To motivate the basic ingredients of core CaReSL, we begin \nwith the quintessential higher-order stateful combinator, foreach: foreach : .a.(a . 1) . list(a) . 1 \nlist(a) 6 \u00b5\u00df.1 + (a \u00d7 \u00df) foreach 6 .. .f. rec loop(l). case l of none . () | some(x, n) . f(x); loop(n) \n A speci.cation of foreach should explain that foreach f l lifts the imperative behavior of f (which \nworks on elements) to l (a list of elements). But to do so, it needs to quantify over the unknown behavior \nof an unknown function in a way that can be lifted to lists. One possibility is to characterize f by \nmeans of a Hoare triple, while ultimately quantifying over the pre-and post-conditions of that triple. \nSuppose that p(x) and q(x) are predicates on values x, and that r is an arbitrary proposition, such that \n.x. {p(x) * r} f(x) {q(x) * r} The idea is that when f is applied to an element x of the list, it may \nassume both that p(x) holds and that an invariant r holds of a disjoint portion of the heap. If f(x) \ntransforms p(x) to q(x) while maintaining the invariant, then foreach lifts that behavior to an entire \nlist regardless of what the predicates actually are. To capture the assumption that p(x) holds of each \nelement of a list l, we need something like the following recursive predicate:4 Map(l) 6 l = none . (.x, \nn. l = some(x, n) * p(x) * Map(n)) p p Note that, thanks to the use of the separating conjunction *, \nfor each x in l there is a disjoint region of the heap satisfying p(x). These ideas lead to the following \nspec for foreach: .p, q, r. .f. (.x. {p(x) * r} f((x) {q(x) * r}) ( . .l.Map(l) * r foreach f lMap(l) \n* r p q Formalizing this sketch will require a logic with a number of basic features: we need to be \nable to mix Hoare triples with other kinds of connectives (e.g., implication), to quantify over both \nvalues (e.g., f and x) and predicates (e.g., p and q), and to de.ne recursive predicates (e.g., Map). \nWith these goals in mind, we now explore CaReSL s design more systematically. Syntax Figure 3 presents \ncore CaReSL the fragment of the logic appropriate for reasoning about sequential, higher-order code. \nCaReSL is a multi-sorted second-order logic, meaning that its syn\u00adtax is strati.ed into the following \nthree layers. First, there are terms M, N , which come in a variety of sorts S, including the values \nand expressions of the language, as well as thread-local storage (with operations L-J, l and :=). The \njudg\u00adment X f M : S gives the sort of a term (where X is a variable context giving the sorts of term \nand predicate variables). Second, there are propositions P, Q, R, which include the con\u00adnectives of multi-sorted \n.rst-order logic (e.g., .X . S.P ) and second-order logic (e.g., .p . P(S).P ) with their standard mean\u00adings. \nThe judgment X f P : B asserts that the proposition P is well-sorted. Third, there are predicates ., \n., which are just propositions parameterized by a term: a predicate of sort P(S) can be introduced by \n(X . S).P , which binds an unknown term X of sort S in P , 4 Throughout, names of functions and types \nare written in lower case sans\u00adserif, while predicate names are Capitalized Serif. Syntax Terms M ::= \nX | (M, M ) | n | e | M 1 M | lMJ(M) | M[M := M] Sorts S ::= 1 | S \u00d7 S | Nat | Val | Exp | AtomicExp \n| LclStorage Preds . ::= p | (X . S).P | \u00b5p . P(S).. Propositions P ::= True | P . P | P . P | P . P \n| .X . S.P | .X . S.P | rP |.(M) | .p . P(S).P | .p . P(S).P | P * P | M '.I M | A Necessary ( always \n) propositions A ::= pure DP | M . M | M=M | MP M.IS . | {P } M. M {.}M M Contexts X ::= \u00b7 | X , X : \nS | X , p : P(S) P ::= \u00b7 | P , P C ::= X ; P Well-sorted terms X f M : S X , X : S f X : S X f M : LclStorage \nX f N X f lMJ(N) : Val : Nat \u00b7 \u00b7 \u00b7 Well-sorted props. X f P : B Well-sorted preds. X f . : P(S) Figure \n3. Core CaReSL: Syntax and eliminated by .(M) (also written M . .) which substitutes M for the parameter \nof .. Because sorts include unit and products, predicates can express relations of arbitrary arity. The \njudgment X f . : P(S) asserts that . is a well-sorted predicate over terms of sort S. In general, term \nvariables are written X. But to avoid cluttering our rules and proofs with sort annotations, we use variables \nx, y, z for sort Val and i, j, k, f for sort Nat. We abuse notation, writing e.g., v, e or L to stand \nfor a term of sort Val, Exp or LclStorage, respectively. In addition to this standard logical setup, \nCaReSL adopts key connectives from separation logic. In general, these connectives will refer to a variety \nof resources that will be introduced as we go along. In core CaReSL, however, the only resource is the \nheap. The points-to assertion f '.I v holds of any heap containing a location f that points to the value \nv. (Ignore the I subscript for now; we will return to it in \u00a73.3). The separating conjunction P * Q holds \nif the currently-owned resources (here, a portion of the heap) can be split into two disjoint parts satisfying \nP and Q respectively. While the truth of some propositions (e.g., f '.I v) is contin\u00adgent on the presence \nof certain resources, others (e.g., M = N) are necessary: if they hold, they do so regardless of the \ncurrently\u00adowned resources, and therefore will continue to hold in any future state. (Such propositions \nare called pure in separation logic par\u00adlance.) The syntactic subcategory A of necessary propositions \nin\u00adcludes claims about term equality, about the operational semantics pure (M . N), and Hoare triples. \nArbitrary propositions can be made necessary via the D ( always ) modality, where DP holds if P holds \nfor all possible resources. As we will see shortly, necessary propositions play by special rules: they \ncan move freely through Hoare triples and separating conjunctions. Ultimately, CaReSL distinguishes between \ntriples about general expressions e and those about atomic expressions a (which execute in a single step). \nSince this distinction is motivated by concurrency, we postpone its explanation to \u00a73.2. We include the \ndistinction syntactically in core CaReSL, but it can be safely ignored for now. Atomic expressions a \nhave the following grammar: new v | get v | v := v | CAS(v, v, v) | newLcl | getLcl(v) | setLcl(v, v) \nThe propositions for atomic triples5 .P . i.IS a ... and general triples {P } i. e {.} are both parameterized \nby a thread ID i; the expression may access thread-local storage, in which case its behavior is ID-dependent. \n(When the thread ID doesn t matter, we 5 Again, ignore the I S subscript until \u00a73.3. Logical axioms \nand rules X ; A f P C f P C, rP f P L \u00a8 DI MO N O O B X ; P, A f DP C f rP C f P D(P . Q) . DP . DQ A \n* P . A . P A . DA True * P . P DP . P r(P * Q) . rP * rQ C, p : P(S) f P M . (X . S).P . P [M/X ] .2I \nC f .p . P(S). P M . (\u00b5p..) . M . .[\u00b5p../p] Figure 4. Core CaReSL: Selected logical axioms and rules \n write {P } e {.} as short for .i. {P } i. e {.}.) In addition,  since we are working with an expression \nlanguage (as opposed to a command language), postconditions are predicates over the return value of the \nexpression, rather than simply propositions about the .nal state of the heap. But when an expression \nreturns unit, we often abuse notation and write a proposition instead. In order to support recursive \nassertions, the logic includes guarded recursion (\u00b5p . P(S). P ), which entails the following tradeoff. \nOn the one hand, guarded recursion allows occurrences of the recursive predicate p to appear negatively, \nwhich is crucial for modelling recursive types (\u00a73.3) but is usually prohibited for lack of monotonicity. \nOn the other hand, the recursion is guarded in that references to p in P must appear under the 1 modality. \nA proposition 1Q represents the present knowledge that Q holds later, i.e., after at least one step of \ncomputation. Guarded recursion supports a coinductive style of reasoning: to prove P one can as\u00adsume \n1P , but this assumption is only useful after at least one step of computation. As we explain in \u00a75, \nour use of guarded recursion descends from a line of work on step-indexed logical relations, but the \ninteraction with Hoare triples is a novelty of CaReSL. Proof rules The main judgment of CaReSL is written \nC f P , where C = X ; P is a combined context of annotated term/predicate variables X and propositional \nassumptions P. The meaning is the usual one: for any way of instantiating the variables in X , if the \nhypotheses P are true for a given resource (i.e., for the moment, a given heap), then P is true of the \nsame resource. We implicitly assume X f Q : B for every proposition Q in P and likewise that X f P : \nBholds. A few basic logical axioms and proof rules for core CaReSL are shown in Figure 4. Axioms hold \nin an arbitrary well-sorted context. The axioms include all the standard ones for a multi\u00adsorted second-order \nlogic (we show only .2I), as well as several characterizing separating conjunction and the D and 1 modalities. \nWe ll just mention the highlights: Because True is a unit for separating conjunction (and ev\u00adery proposition \nimplies True), propositions are af.ne: we can throw away resources, because (P * Q) . (True * Q) . Q. \n The two conjunctions * and . are identical if at least one of their operands is a necessary proposition. \nConsequently, necessary propositions can be freely copied : A . A * A.  The L \u00a8  OB rule provides a \ncoinductive reasoning principle: to prove P , you may assume P but only under the 1 modality, which guards \nuse of the assumption until at least one step of computation has taken place. On the other hand, MO N \nO says that any proposition can be weakened to one that is guarded. (Both MONO and L \u00a8 OB are inherited \nfrom LADR [5].) We will momentarily see how 1 is eliminated in Hoare-style reasoning. The 1 modality \ndistributes over *, as it does with most other connectives, except implication and recursion.  Atomic \nHoare logic (where I S ::= I | S as explained in \u00a73.3) MTrue i .I S new v Mret. ret '.I S v ' ' Mv '.IS \nv i .I S get v Mret. ret = v ' * v '.IS v ' ' Mv '.IS - i .I S v := v Mret. ret = () * v '.IS v MTrue \ni .I S newLcl Mret. ret '.I S \u00d8 Mv '.IS L i .I S getLcl(v) Mret. ret = lLJ(i) * v '.I S L Mv '.IS L i \n.I S setLcl(v, v ' ) Mret. ret = () * v '.IS L[i := v ' ] ' ' Mv '.IS v i .I S Mret. (v = vo * ret = \ntrue * v '.IS vn) ' CAS(v, vo, vn) . (v = vo * ret = false * v '.I S v ' ) ' ' X ; P f P X ; P f MP i \n.IS a Mx. Q ' X , x; Q ' f Q ACS Q X ; P f MP i .I S a Mx. Q C f MP i .I S a Mx. Q AFR A M E C f MP * \nR i .I S a Mx. Q * R (ADI S J, A EX elided) General Hoare logic pure ' C f e . e ' C f MP i .I a MQ C \nf {P } i . e {.} PR I VAT E PU R E C f {rP } i . a {Q} C f {rP } i . e {.} C f {P } i . e {x. Q} C, x \nf {Q} i . K[x] {R} BI N D C f {P } i . K[e] {R} C f {True} i . v {x. x = v} RE T (C S Q, FR A M E, D \nI S J, EX elided) C f A C f {P * A} i . e {.} C, A f {P } i . e {.} AI N AO U T C f {P } i . e {.} C \nf {P * A} i . e {.} Derivable rules C, f , .x. {rP } i . f x {.} f .x. {P } i . e {.} RE C C f .x. {rP \n} i . (rec f(x).e) x {.} C f {P } i . e {x. .y. (x = inj1 y * rQ1) . (x = inj2 y * rQ2)}.k . {1, 2} : \nC, x, y f {x = injk y * Qk} i . ek {.} C f {P } i . case(e, inj1 y . e1, inj2 y . e2) {.} Figure 5. Core \nCaReSL: Hoare logic The rules for atomic triples (Figure 5) are formulated in the standard style of separation \nlogic. They transcribe the operational semantics of atomic expressions, mentioning only the part of the \nheap relevant to the expression. Atomic Hoare logic supports the usual rules consequence, framing, disjunction, \n.-elimination with one important exception: it does not support a sequencing rule, since a sequence of \natomic expressions is not atomic. All atomic expressions take exactly one step to execute, and in so \ndoing allow us to peel off a layer of the 1 modality. To cut down on clutter, the precondition in an \natomic triple is implicitly understood as being under one 1 modality. The rule PR IVAT E, which lifts \nan atomic triple to a general one, makes this implicit assumption explicit. (In core CaReSL, all resources \nare private; \u00a73.2 adds shared resources.) To handle pure reduction steps (like \u00df-reduction), the PUR \nE rule appeals directly to the operational pure semantics using the necessary proposition e . e '. The \nrest of the rules for general Hoare triples are mostly standard; we show only the nonstandard rules in \nFigure 5. In an expression language, the monadic nature of Hoare logic becomes visible: the BIN D rule \nreplaces the usual rule of sequenc\u00ading, while RET is used to inject a value into a Hoare triple. We also \nhave a rule allowing necessary propositions to move freely from the proof context into preconditions \n(AIN), and vice versa (AOUT). In general, any contingent assumptions like x '.I 3 need to be given explicitly \nin the precondition of a Hoare triple, because the truth of such statements can change over time; the \ntriple says that it is only usable at times when its precondition holds. But in the speci.c case of necessary \npropositions, we can do better: we know that if such a proposition happens to be true now, it will be \ntrue forever, and so it does not need to be given explicitly in the precondition. As we will see in \u00a73.2, \nthese rules will allow us to completely hide pieces of state that are known to always obey a certain \nprotocol. Finally, using L \u00a8 OB and PUR E, together with standard Hoare rules, we can derive specialized \nrules for constructs like recursive functions and pattern matching the two derived rules in Figure 5. \nVerifying foreach Having seen core CaReSL in detail, we can now return to the foreach example. First, \nwe need to rewrite our sketch of the speci.cation more formally. The Map predicate needs to employ guarded \nrecursion: Map. 6 \u00b5m . P(Val). (l . Val). l = none . (.x, n. l = some(x, n) * .(x) * rm(n)) while the \nforeach spec should be annotated with sorts: .p, q . P(Val)..r . P(1). .f. (.x.{p(x) * r} f(x) {q(x) \n* r}) ( ( . .l. Map(l) * rforeach f l Map(l) * r p q To prove foreach correct, we use a kind of Hoare \nproof outline , annotating each program point with a proposition: foreach 6 .. .f. rec loop(l). Prop \ncontext: Variables: f , i, p, q, r, l, loop .x. {p(x) * r} i . f(x) {q(x) * r} ( ( .n. r(Map(n) * r)i \n. loop n Map(n) * r p q {Map(l) * r} p case l of none . {l = none * Map(l) * r} () {Map(l) * r} p q | \nsome(x, n) . {l = some(x, n) * Map(l) * r} p {l = some(x, n) * p(x) * rMap(n) * r} p f(x); {l = some(x, \nn) * q(x) * rMap(n) * r} p loop(n) {l = some(x, n) * q(x) * Map(n) * r} q {Map(l) * r} q Proof outlines \nimplicitly apply the Hoare logic rule corresponding to each language construct: for recursive functions \nand pattern matching we use the derived rules shown earlier; for an atomic expression we use the corresponding \naxiom; when going under a . or .x we use the PU RE rule and prove a triple about the function as applied \nto or x respectively. The frame rule is applied implicitly as needed, while uses of consequence are shown \nby writing a sequence of assertions (each one implying the next). We do not explicitly write the thread \nID in a proof outline, but it is always clear from context (and, for our examples, always the variable \ni). We supplement traditional proof outlines with boxed context as\u00adsertions spelling out an extension \nto both the variable context X and proposition context P. Extending the context with new vari\u00adables introduces \na universal quanti.cation (using e.g., the rule .2I in Figure 4), while adding propositions introduces \nan implication. We use the AIN rule implicitly to bring necessary consequences of the proof context into \nthe Hoare-style outline. The proof for foreach is quite straightforward: the initial case analysis on \nthe input list allows us to expand the de.nition of the Map predicate, which for the nonempty case gives \nus the necessary knowledge to execute f on an element of the list; note the heavy use of the frame rule \nto add invariant propositions. The use of 1 in Map is neatly dispatched by the use of the RE C rule for \nforeach. 3.2 Adding concurrency: Protocols for shared state To reason about concurrency, we need to \nreason about the protocols governing shared (and often hidden) state. Take, for example, the following \nhigher-order spinlock: tryAcq 6 .x. CAS(x, false, true) acq 6 rec loop(x). if tryAcq(x) then () else \nloop(x) rel 6 .x. x := false mkSync : 1 . .a..\u00df .(a . \u00df) . (a . \u00df) mkSync 6 .(). let lock = new false \nin .....f..x. acq(lock); let z = f(x) in rel(lock); z Each invocation of mkSync creates a new wrapper \nthat closes over a fresh, hidden lock. The wrapper can then be used to add simple synchronization to \nan arbitrary function. There are, of course, a variety of ways to use synchronization, but a particularly \ncommon one is to protect access to some shared resource characterized by an invariant p an idea that \nleads to the following speci.cation: ( .p . P(1). {p} mkSync () s. Syncer(s) p where Syncer6 (s . Val). \nD.f, x, q, r. {p * q} f x {z. p * r(z)} p . {q} s fx {z. r(z)} The spec says that when mkSync() is executed, \nthe client can choose some invariant resource p, giving up control over the re\u00adsource in exchange for \na synchronization wrapper s. When s is later applied to a function f, it provides f with exclusive access \nto the resource p (seemingly out of thin air), which f can use however it pleases, so long as the invariant \nis reestablished on completion. Intuitively, the reason that mkSync satis.es its speci.cation is that \nthe lock itself is hidden: all access to it is mediated through the wrapper s, which the client can only \napply to invariant-preserving functions. Hiding enables mkSync to maintain an internal protocol in which, \nwhenever the lock is free, the invariant p holds. To express this protocol, as well as the more sophisticated \nprotocols needed for .ne-grained concurrency (\u00a73.3, \u00a74), CaReSL provides a syntactic account of the semantic \nprotocols of Turon et al. [31]. Protocols A protocol p governs a shared resource abstractly, by means \nof a set of protocol states (S) equipped with a transition relation ( ). Each state s . S has an associated \nproposition .(s) giving its concrete interpretation in terms of e.g., heap resources. The idea is then \nthat any thread is allowed to update the shared resource, so long as at each atomic step those concrete \nupdates correspond to a permitted abstract transition. In general, although all threads must abide by \na given protocol, not all of them play the same role within it. For example, a protocol governing a lock \nmight have two states, Locked and Unlocked, with transitions in both directions, but we usually want \nto allow only the thread that acquired the lock to be allowed to release it (as is the case in the mkSync \nexample). To allow threads to take on or relinquish roles dynamically, protocols employ tokens that individual \nthreads may own. By taking a transition, a thread may earn a token from the protocol; conversely, certain \ntransitions require a thread to pay by giving up a previously-earned token. For the locking protocol, \nwe use a single token Lock: In the Unlocked state, the protocol itself owns the Lock, which we show \nby annotating the state with the token. A thread that transitions from Unlocked to Locked then earns \n(takes ownership of) the Lock. Conversely, to transition back to the Unlocked state, a thread must transfer \nthe Lock back to the protocol a move only possible for the thread that owns the Lock. Formally, a protocol \np = (S, , T , .) is given by a transition system (S and ), a function T giving the set of tokens owned \nby the protocol at each state, and a predicate . on states giving their interpretations. To be able to \ntalk about states and tokens in the logic, we add sorts State and TokSet, for which we will use the metavariables \ns and T respectively; see Figure 6. We leave the grammar of terms for these sorts open-ended, implicitly \nextending Protocols Protocol p ::= (S, , T , .) where S . State, . S \u00d7 S, T . S . TokSet, . token-pure \n'' ' (s; T ) p (s ' ; T ) 6 s (p. ) s . p.T (s) 1 T = p.T (s ' ) 1 T framep(s; T ) 6 (s; AllTokens - \n(p.T (s) 1 T )) (s; T ) rguar ' *' p (s ; T ' ) 6 (s; T ) p (s ; T ' ) (s; T ) rrely ' *' p (s ; T ' \n) 6 framep(s; T ) p framep(s ; T ' ) Syntax Sort S ::= \u00b7 \u00b7 \u00b7 | State | TokSet Prop P ::= \u00b7\u00b7\u00b7 | M M \n| Tid(M) p | M rrely M | M rguar AbProp A ::= \u00b7 \u00b7 \u00b7 p p M | TokPure(P ) Hoare logic (where p[b] 6 .s.b \n= (s, -) . p..(s)) C f {P } i . e {x. Q * p[b]} ( NE W IS L b n C f {P } i . e x. Q * .n. p rely guar \nC f .b ;p b0. Mp[b] * P i .I a Mx. .b ' ;p b. p[b '] * Q UP D IS L ( n n b ' C f b0 p * rP i . ax. .b \n' . p * Q C f {P * Tid(j)} j . e {ret. ret = ()} FO R K C f {P } i . fork e {ret. ret = ()} Logical \naxioms and rules n n p . SP L I T IS L rely rely s1; T1 p * s2; T2 n .s. s; T1 1 T2 p . (s; T1) ;p (s1; \nT1) . (s; T2) ;p (s2; T2) Figure 6. CaReSL: Incorporating concurrency and protocols it as needed for \nparticular transition systems.6 Thus, we can give an interpretation LockInterpp for the lock protocol \nthat is appropriate for an instance of mkSync protecting an invariant p: LockInterp6 (s . State). s = \nLocked . (s = Unlocked * p) p The combination of transition systems and tokens gives rise to token-sensitive \ntransitions. A transition from state/tokens (s; T ) to '' '' state/tokens (s ; T ) is permitted by p, \nwritten (s; T ) p (s ; T ), so long as the law of token conservation holds: the disjoint union of the \nthread s tokens and the protocols tokens T l p.T (s) before the transition must be the same as the disjoint \nunion T ' l p.T (s ' ) af\u00adterwards.7 Token-sensitive transitions constrain both what a thread can do \n(the guarantee moves .guar enabled by its tokens) and what p its environment can do (the rely moves \n.rely enabled by the tokens p owned by other threads). See Figure 6. Island assertions In CaReSL, all \nresources are either privately owned by a thread, or else governed by a shared protocol. When a heap \nassertion like x '.I 3 appears in the pre-or postcondition of a triple, it is understood as asserting \nprivate ownership over the corresponding portion of the heap; no other thread is allowed to access it. \nThus the rules of core CaReSL are immediately sound in a concurrent setting: there is no interference \nto account for. To talk about shared resources, CaReSL includes island asser\u00adtions b n (similar to region \nassertions in RGSep/CAP). As the p name suggests, each island is an independent region of the heap governed \nby its own laws (the protocol p). The number n is the name of the island, which is used to distinguish \nbetween multiple islands with the same protocol; we often leave off the name when 6 To be completely \nformal, we could allow each transition system to come equipped with its own explicit grammar of states \nand tokens. 7 We use notation like p.T to extract named components from a tuple.  it is existentially \nquanti.ed. The term b (of sort State \u00d7 TokSet) as\u00adserts private ownership of a set of tokens,8 and acts \nas a rely-lower bound on the state of the protocol: the current state of the protocol ;rely is some b \n' p b. Thus, in CaReSL every assertion about shared resources is automatically stable under interference. \nWhile a thread s island assertions cannot be invalidated by a change its environment makes, they can \nbe invalidated by a change the thread itself makes. For example, if a thread owns Lock in Locked state \nof the mkSync protocol, the environment cannot change the state at all but the thread itself can move \nto the Un\u00adlocked state. There is, however, a special class of island assertions which are completely \nstable, i.e., that act as necessary proposi\u00adtions: island assertions that do not claim any tokens. To \nunderstand rely why, consider that when no tokens are owned, the p relation degenerates to the underlying \ntransition system of the protocol, which means it contains all possible moves that any thread can make. \nFormally, we have (M; \u00d8) n (M; \u00d8) n p . D p. When island assertions do claim ownership over tokens (a \nform of resource), they can be meaningfully combined by separating conjunction; see the SPL I TIS L rule \nin Figure 6, which takes into account the fact that the assertions give only rely-lower bounds.9 There \nare two Hoare logic rules for working with islands (Fig\u00adure 6). The rule NEWIS L creates a new island \nstarting with an initial state and token set bound b; the resources necessary to satisfy the protocol \ns initial state must be present as private resources, and af\u00adterwards will be shared. (The shorthand \np[b] just gives the interpre\u00adtation at the state in b.) Once an island is established, the only way to \naccess the shared resources it governs is through the UP D ISL rule, which can only be applied to an \natomic expression. Starting from an initial bound b0, the atomic expression a might be (instan\u00adtaneously) \nexecuted in any rely-reachable b; for each such state, the expression is granted exclusive access to \nthe shared resource, but in its single atomic step, it must make a guarantee move in the protocol. This \nrule reveals the important semantic difference be\u00adtween atomic and general triples: through the UP DIS \nL rule, atomic triples gain access to the concrete resources owned by shared is\u00adlands, while general \ntriples only have access to island assertions. Thread creation In a protocol, threads gain and lose roles \n(to\u00adkens) by making deliberate moves within the protocol. But there is also a role that every thread \nplays automatically: the role of be\u00ading a thread with a certain ID. To support protocols that use thread \nIDs (such as the one in \u00a74), CaReSL builds in a proposition Tid(j) that asserts the existence of a thread \nj, and acts as an uncopyable resource: Tid(j) * Tid(j) . False. The resource is introduced by the FOR \nK rule, which also allows the parent thread to transfer an arbitrary P to the newly-forked thread. (The \nparent can keep re\u00adsources for itself via the FR A M E rule.) The trivial postconditions in FO R K may \nseem alarming, but they re.ect the fact that the language has no join mechanism: the only form of communication \nbetween threads is shared state, mediated by a shared protocol. Verifying mkSync The interpretations \np..(s) of protocol states in CaReSL inherit a limitation from their semantic treatment by Turon et al. \n[31]: they must be token-pure, i.e., they cannot assert ownership of island tokens. The necessary proposition \nTokPure(P ) can be used to assert that, for example, an unknown proposition is token-pure and thus safe \nto use in an interpretation, and we need such a restriction on p in mkSync: ( .p . P(1). TokPure(p) . \n{p} i . mkSync () s. Syncer(s) p where Syncer6 (s . Val). D.f, x, q, r. {p * q} f x {z. p * r(z)} p . \n{q} s fx {z. r(z)} 8 An island assertion is only satis.ed if the asserted token set is disjoint from \nthe tokens owned by the protocol at the asserted state. 9 We assume that terms M include disjoint union \non token sets.  The hidden protocol LockProtp for mkSync just puts together the pieces we have already \nseen: LockProtp 6 ({Locked, Unlocked}, , T , LockInterp) p where and T are given by the transition system \nfor locking shown above. The high-level proof outline for mkSync is straightforward: .(). Prop context: \nTokPure(p) Variables: p {p} let lock = new false {p * lock '.I false} {.n. Unlocked; \u00d8 n } LockProtp \n.....f ..x. z {z. r(z)} After allocating the hidden lock, we move it into a fresh island using NE W \nIS L, and then move that island assertion (wrapped with D) into the proof context (using AOUT) before \nreasoning about the returned wrapper function. In this way the protocol is completely hidden from the \nclient, yet available to the closure we return to the client mirroring the fact that lock is hidden from \nthe client but available in the closure. Since the spec Syncerp(s) is a necessary proposition, the client \nmay move the spec into its proof context, and thus freely use the synchronization wrapper s without threading \nany assertion about it through its proof. (Previous logics, like CAP, require at least that the client \nthread through an abstract predicate standing for the lock.) When verifying the closure, we begin with \na precondition q of the client s choice, but then apply AIN to load the hidden island as\u00adsertion into \nthe precondition. The subsequent lines use the locking protocol to acquire the resource p, execute the \nclient s function f, and then return p to the protocol. The tokens in the island asser\u00adtions, which say \nwhat the thread owns at each point, complement those (in p.T ) labelling the corresponding states in \nthe protocol. The triples for acq and rel must ultimately be proved by appeal to the UPD IS L rule. For \nacq, the bound on the protocol is just (Unlocked; \u00d8), which means that the actual state might be either \nLocked or Unlocked. The CAS within acq will return: true if successful; the state must have been Unlocked. \nWe make a guarantee move to (Locked; Lock), gaining the token.  false if it fails; the state must have \nbeen Locked. We do not change the state, and the acq function retries by calling loop.  On the other \nhand, for rel, the bound (Locked; Lock) means that the protocol must be in state Locked: the environment \ncannot move to Unlocked because it does not own Lock. But since rel does own Lock, it can simply update \nthe lock to false, corresponding to a guarantee move to the Unlocked state in the protocol. 3.3 Adding \nre.nement: Reasoning about speci.cation code At this point, we have seen the fragment of CaReSL providing \nHoare-style specs and proofs for higher-order concurrent programs, as exempli.ed (in a simple way) by \nthe mkSync example. This section explains the other major component of the logic: higher\u00adorder granularity \nabstraction, exempli.ed (in a simple way) by the Treiber stack with iterator. Syntax S ::= \u00b7 \u00b7 \u00b7 | EvalCtx \nP ::= \u00b7 \u00b7 \u00b7 | M '.S M | M .S M M ::= \u00b7 \u00b7 \u00b7 | K | M[M] A ::= \u00b7 \u00b7 \u00b7 | P .S P | M rN M Spec rewriting pure \n' C f e . e SPURE C f (P * j .S K[e]) .S (P * j .S K[e ' ]) C f MP j .S a Mx. Q SPRIM C f (P * j .S K[a]) \n.S (.x. Q * j .S K[x]) C f (j .S K[fork e]) .S (j .S K[()] * k .S e) SFORK Hoare logic (AEXECSPEC elided) \nC f {P } i . e {x. Q} C f Q .S R EXECSPEC C f {P } i . e {x. R} C, j rN j ' fP * Tid(j) * j ' .S eSj \n. eI {x. x = ()} FORKS C fP * i ' .S K[fork eS ]i . fork eIi ' .S K[()] Figure 7. CaReSL: Incorporating \nspec resources Spec resources While it is possible to formulate a relational ver\u00adsion of Hoare logic \n(with Hoare quadruples [33]), or to develop special-purpose logics for re.nement [5], our goal with CaReSL \nis to support both standard Hoare-style reasoning and re.nement reasoning in a single uni.ed logic. In \nparticular, we want a treat\u00adment of re.nement that re-uses as much Hoare-style reasoning as possible. \nTo this end, we adapt the idea of speci.cation resources, .rst proposed by Turon et al. [31] as a way \nof proving re.nement when threads engage in cooperation (a point we return to in \u00a74). We will show how \nspec resources make it possible to state and prove re.nements as an entirely derived concept on top of \nthe Hoare logic we have already built up, in particular allowing protocols and triples to serve double-duty \nwhen reasoning about spec code. To prove that eI re.nes eS , one must (intuitively) show that every behavior \nobservable of eI is observable of eS as well. Our strategy is to encode these proof obligations into \ncertain Hoare triples about the execution of eI , but with pre-and postconditions instrumented with pieces \nof the corresponding spec state both heap and code which we treat as resources in CaReSL. These spec \nresources are entirely a .ction of the logic: they do not re.ect anything about the physical state of \neI , but they must be used through logical rules that enforce the operational semantics for eS . There \nare two basic spec resource assertions: the (spec) points\u00adto assertion f '.S v, which claims ownership \nof a fragment of the spec heap containing a location f that points to the value v, and the spec thread \nassertion i .S e, which claims ownership of a spec thread with ID i executing expression e. The separating \nconjunction P * Q works in the usual way with these resources, dividing up the spec heap and thread pool \nbetween the propositions P and Q. We also add a .nal sort, EvalCtx, and terms K and M[N] for expressing \nand combining them, which makes it possible to abstract over the evaluation context for some spec thread. \n(By convention, the variable . is always of sort EvalCtx.) In addition, CaReSL provides a necessary proposition \nP .S Q, which says that the spec resources owned by P can, according to the operational semantics, take \na step to those owned by Q. All other resources must be left invariant. The rule SPURE lifts the pure \nstepping relation from the operational semantics directly. The rule SPRIM, on the other hand, re-uses \natomic triples to support reason\u00ading about atomic spec expressions. (The subscript IS in the laws of \natomic triples allows them to be used in either implementation mode I or spec mode S.) The EXECSPEC Hoare \nrule (and identical AEXECSPEC rule for atomic triples) allows the speci.cation to be Relating expressions \n(eI , eS ) . . 6 .(i rN j), .. {Tid(i) * j .S .[eS ]} i . eI {xI . .xS . .(xI , xS ) * Tid(i) * j .S \n.[xS ]} Relating values 6 (xI , xS ). xI = xS = () [\u00b5a.t 6 \u00b5a.[t] [1 [B] 6 (xI , xS ). (xI = xS = true) \n. (xI = xS = false) [a] 6 a t1 \u00d7 t2 6 (xI , xS ). (prj1 xI , prj1 xS ) . [t1] . (prj2 xI , prj2 xS ) \n. [t2] [ t1 + t2] 6 (xI , xS ). (r.(yI , yS ) . t1 . xI = inj1 yI . xS = inj1 yS ) . (r.(yI , yS ) . \n[ t2] . xI = inj2 yI . xS = inj2 yS ) ' ' 6 (xI , xS ). D.yI , yS .(r(yI , yS ) . [t]) . (xI yI , xS \nyS ) . [t[t . t ] .a.t 6 (xI , xS ). D.a.Type(a) . (xI , xS ) . [t] [ ref t] 6 (xI , xS ). Inv(.(yI , \nyS ) . [t]. xI '.I yI * xS '.S yS ) ( .LI , LS . xI '.I LI * xS '.S LS * [refLcl t] 6 (xI , xS ). Inv \n.(i rN j). (lLI J(i), lLS J(j)) . [1 + t] Invariant protocols Inv(P ) 6 .n. 0; \u00d8 n ({0},\u00d8,[0 .\u00d8],(s).P \n) Type interpretations Type(.) 6 D.(xI , xS ) . .. D(xI , xS ) . . Logical relatedness (i.e., re.nement) \na; x : t f eI . eS : t 6 a, xI , xS ; Type(a), (xI , xS ) . [t] f (eI [xI /x], eS [xS /x]) . [t] Figure \n8. Encoding re.nement in CaReSL executed within any postcondition, i.e., at any point in a proof. Since \nEXECSPEC can be applied repeatedly, a single step of some implementation code e.g., an atomic expression \ncan correspond to an arbitrary number of spec steps. Putting these pieces together, a triple like {j \n.S eS } i . eI {xI . .xS . j .S xS * .(xI , xS )} says that if eI produces some value xI , then eS can \nbe executed to produce some value xS such that .(xI , xS ) holds exactly the kind of observational claim \nwe set out to make. Encoding re.nement To give a full account of re.nement, we also need to ensure that \nupdates to public reference cells by the implementation are matched in lock-step by updates by the spec, \nwhich we will do by imposing a protocol governing public refer\u00adence cells. And to account for thread-local \nreferences, we must also track a correspondence between implementation and speci.cation thread IDs, which \nallows us to state invariants connecting the stor\u00adage on either side. The (necessary) assertion i 1N \nj asserts that the implementation thread i and spec thread j belong to the bijective correspondence, \ni.e., (i 1N j) . (i ' 1N j ' ) . (i = i ' . j = j ' ). Correspondences are introduced using the rule \nFORKS, a variant of FORK that jointly creates fresh implementation and spec threads. And that s all: \nusing these ingredients, we can encode re.ne\u00adment by simply writing down a particular predicate in CaReSL. \nThe encoded predicate expresses a logical relation a variant of the relation given semantically by Turon \net al. [31] following the approach .rst laid out by Plotkin and Abadi [23]. We de.ne the relation in \nstages (see Figure 8). First, we have the proposition (eI , eS ) . ., which is a more general version \nof the triple we suggested above: it includes as\u00adsumptions about thread IDs, and permits compositional \nreasoning about specs by quantifying over an unknown evaluation context .. The expressions do not begin \nwith private ownership of any heap resources because, when proving re.nement, we must assume that any \npre-existing state is shared with the context. As we will see in a moment, this shared state is governed \nby an extremely liberal pro\u00adtocol; all we can assume about the context is that it is well-typed. Second, \nwe have the binary predicate [t ], which is satis.ed by (vI , vS ) when the observations a context can \nmake of vI can stackS : .a. (a . 1) \u00d7 (1 . (1 + a)) \u00d7 ((a . 1) . 1) stackS 6 .. let sync = mkSync(), \nhdS = new (none) let push = .x. hdS := some(x, get hdS ) let pop = .(). case get hdS of none . none | \nsome(x, n) . hdS := n; some(x) let snap = sync (.().get hdS ) let iter = .f. foreach f (snap()) in (sync \npush, sync pop, iter) stackI 6 .. let hdI = new nil where nil 6 new none, let push = rec try(x). cons \ne 6 new some(e) let c = get hdI , n = cons(x, c) in if CAS(hdI , c, n) then () else try(x), let pop = \nrec try(). let c = get hdI in case get c of none . none | some(d, n) . if CAS(hdI , c, n) then some(d) \nelse try() let iter = .f . let rec loop(c) = case get c of none . () | some(d, n) . f(d); loop(n) in \nloop(get hdI ) in (push, pop, iter) Figure 9. Coarse-and .ne-grained stacks, with iterators also be made \nof vS the heart of the logical relation. It is de.ned by induction on the structure of t , since the \nways a value can be observed depend on its type: For ground types, the context can observe the exact \nvalue, so only equal values are in the interpretation. For product types, the context can project both \nsides of the product, so two values are related iff each of their projections are related; similarly \nfor sums. The context can observe functions only by applying them, so one function is related to another \niff, when applied to related values, they produce related results. Recursive and polymorphic types are \ninterpreted via guarded recursion and second-order quanti.cation, respectively; we pun type variables \na, \u00df as predicate variables, which are implicitly assumed to be of sort P(Val \u00d7 Val), and for quanti.cation \nex\u00adplicitly required to satisfy Type(a). These two constraints guar\u00adantee that the relation [t ] is a \nresource-insensitive, binary pred\u00adicate on values. The resource-insensitivity re.ects the fact that re.nement \nbetween values must hold in an arbitrary context of observation/usage including contexts that freely \ncopy the val\u00adues in question which means that we can assume nothing about privately-owned resources. \nThe context can interact with values of reference type by either reading from or writing to them at any \ntime. Thus, references assert the existence of a simple invariant protocol with a single state, whose \ninterpretation says that related locations must continuously point to related values. For thread-local \nreferences, the statement is quali.ed by indexing the storage table by related thread IDs. The uses of \n1 throughout are necessary to ensure that recursive predicate variables a only occur in guarded positions. \nNote that island assertions are implicitly guarded, as are uses of (eI , eS ) . . where eI is not a value, \nbecause any a mentioned therein will not be interpreted until after a step of computation. Finally, eI \nis logically related to eS at some context O and type t if O f eI : eS : t, which is shorthand for a \nuse of the C f P judgment of CaReSL. The proof context closes all type variables a with arbitrary type \ninterpretations and all term variables x with pairs of term variables related at the appropriate types. \nThus, a term with a free variable of type ref t will gain access in its proof context to a shared invariant \nprotocol governing the corresponding location. The protocol in turn forces updates to the reference in \nthe implementation to occur in lock-step with those in the spec. Private references, i.e., those allocated \nwithin the implementation or spec, face no such requirement unless or until they are exposed. Treiber \ns stack, with an iterator We now sketch a simple re.ne\u00adment proof. Figure 9 gives two stack implementations. \nThe .rst, stackS , is a coarse-grained reference implementation (which we think of as a speci.cation). \nIts representation includes a mutable reference hdS to an immutable list, as well as a synchronization \nwrapper sync provided by mkSync. The exported functions to push and pop from the stack simply wrap the \ncorresponding updates to hdS with synchronization. But the iterator is not fully synchronized: it takes \nan atomic snapshot of the stack, then calls its argument f on each element of the snapshot without holding \nany locks. So by the time f is called, the contents of the stack may have changed. The second implementation, \nstackI , is Treiber s stack supple\u00admented with an iterator. Treiber s stack [28] is one of the simplest \nlock-free data structures a kind of Hello World for concurrent program logics. Like stackS , Treiber \ns stack maintains a reference hdI to a list that it uses to represent the stack. Instead of using a lock, \nhowever, it updates hdI directly (and atomically) using CAS, which requires the contents of hdI to have \na comparable type, i.e., to be testable for pointer equality. Thus hdI is of type ref clist(a), where \nclist(a) 6 \u00b5\u00df. ref (1 + (a \u00d7 \u00df)). The push and pop func\u00adtions employ optimistic concurrency: instead \nof acquiring a lock to inform other threads that they are going to make a change, they just take a snapshot \nof the current hdI , compute a new value, and use CAS to install the new value if the identity of the \nhdI has not changed. Intuitively, this strategy works because if the hdI s iden\u00adtity has not changed, \nthen nothing about the stack has changed: all mutation is performed by identity-changing updates to hdI \n. The iterator simply walks over the stack s contents starting from a (pos\u00adsibly stale!) snapshot of \nhdI . To prove that stackI re.nes stackS , we begin as follows: stackI 6 .. Prop context: Type(a), i \nrN j Variables: a, i, j, . {j .S .[stackS ]} let hdI = new nil {j .S .[stackS ] * .x. hdI '.I x * x '.I \nnone} (EX E C SP E C) E . hdS , lock. hdS '.S none * lock '.S false * j .S .[stack ' S ] * .x. hdI '.I \nx * x '.I none We have executed the preambles of both the implementation and spec the code that allocates \ntheir respective hidden state. (Let stack ' S denote the rest of the spec code after the let-binding \nof sync and hdS .) At this point, we will use NEW IS L to move all of this state into a hidden island, \nwith a protocol that we can use when proving re.nement for each exported function. The states s of the \nprotocol are maps Loc .n. Val which simply record the identity and contents of every node added to stackI \n. The transition relation s s ' 6 s . s ' captures the idea that, once a node has appeared in the stack, \nits contents never change. There are no tokens. We interpret a state s as follows: . 6 (s). lock '.S \nfalse * *\u00a3.dom(s) \u00a3 '.I s(\u00a3) * .xI , xS . hdI '.I xI * hdS '.S xS * Link(s, xI , xS ) Because the lock \nis protected by the protocol, we can only execute the exported spec functions with AEX E CSP E C, as \npart of an appli\u00adcation of UPDIS L; by stipulating lock '.S false, the interpretation furthermore requires \nthat we run the spec in big steps , starting and ending in unlocked states. It also says that the implementation \nand spec stack contents are linked, in the following sense: '' ' Link 6 \u00b5p. (s, xI , xS ). .xI , s . \ns .= [xI . xI ] 1 s ' * . ' .yI , zI . x = some(yI , zI ) * I (x ' I = none * xS = none) . . .yS , zS \n. xS = some(yS , zS ) * . ' (yI , yS ) . a * rp(s , zI , zS ) Linking requires that the stack state \ns contain an entire linked list starting from hdI , and that each element of the list be related (at \ntype a) to the corresponding element of the list in hdS . But it does so without mentioning the heap \nat all! This is the key: it means that Link(s, xI , xS ) . DLink(s, xI , xS ), so the Link assertion \ncan be freely copied in the proof for iter when the traversal begins. Since the abstract state s of the \nstack can only grow, all of the nodes mentioned in this snapshot of Link are guaranteed to still be available \nin the region of the heap governed by the protocol, with their original values, as traversal proceeds \neven if, in the meantime, the nodes are no longer reachable from hdI . The full proof outlines are available \nin the appendix [30]. Combining re.nement and Hoare-style reasoning Suppose a client of Treiber s stack \nwishes to use it only in a weak way, via a per-item spec similar to the one suggested in the introduction: \nPerItem 6 (e . Expr). .p, q . P(Val). (.x. TokPure(p(x)) . (p(x) . Dq(x))) . {True} e {obj. obj = (add, \nrem, iter) . DP }, where P 6 .x. {p(x)} add(x) {ret. ret = ()} . {True} rem() {ret. ret = none . (.x. \nret = some(x) * p(ret))} . .f, r. (.x. {q(x) * r} f(x) {r}) . {r} iter(f) {r} This per-item spec associates \na resource p with each element of the stack, which is transferred to and from the data structure when \nadding or removing elements. If, in addition, each per-item resource entails some permanent knowledge \nDq, then that knowl\u00adedge can be safely assumed by a function concurrently iterating over the stack, even \nif the resources originally supporting it have been consumed. As we will see in the case study (\u00a74), \nthis spec for iteration is useful for associating permanent, token-free knowledge about some other protocol \nwith each item that appears in the stack. While the per-item spec could in principle be applied directly \nto Treiber s stack (i.e., we could prove PerItem(stackI )), doing so would repeat much of the veri.cation \nwe just performed. On the other hand, proving PerItem(stackS ) is nearly trivial: we just instantiate \nthe spinlock spec given in \u00a73.2 with the representation invariant Repp 6 .l. hdS '.I l * Mapp(l). To \nverify iter, we need only observe that Mapp(l) . DMapq(l), i.e., the snapshot of the stack provides a \nlist of necessarily true associated facts. (The details are in the appendix [30].) By mixing re.nement \nand Hoare logic, we can thus signi.cantly modularize our veri.cation effort.  3.4 Soundness The model \ntheory of CaReSL is based directly on the semantic model of Turon et al. [31], with minor adjustments \nto accommo\u00ad date the assertions Tid(i) and i 1N j necessary for reasoning about thread-local storage. \n(Since this paper is focused on the comple\u00admentary aim of giving a syntactic proof theory, we do not \ndelve into the model here; it can be found in full in the appendix [30]). Soundness for CaReSL encompasses \ntwo theorems. First, that C f P implies C |= P , where the latter is the semantic entail\u00adment relation \nof the model. Proving this theorem requires validat\u00ading, in particular, the key Hoare logic rules, which \nwe do in the ap\u00adpendix; these proofs resemble the proofs of key lemmas supporting Turon et al. s model. \nSecond, that \u00b7 f (O f eI : eS : t ) implies O |= eI : eS : t , i.e., that the logical relation is sound \nfor contex\u00adtual re.nement. Again, this proof follows the soundness proof of Turon et al., except that \nwe can carry it out at a much higher level of abstraction using CaReSL s proof rules. 4. Case study: \nFlat combining Using CaReSL s combination of Hoare-style and re.nement rea\u00adsoning, we can verify higher-order \nconcurrent algorithms in layers of abstraction and this section shows how to do it. We study the recent \n.at combining construction of Hendler et al. [11], which takes an arbitrary sequential data structure \nand transforms it into a concurrent one in which all operations appear to take effect atomically. One \ncan do so by merely wrapping each operation with synchronization on a global lock indeed, this is .atS \n6 mkSync() : [.a. .\u00df . (a . \u00df) . (a . \u00df)] .atI 6 .[a]. .[\u00df]. .f : [a . \u00df]. (Annotated with types for \nclarity) let lock : [ref B] = new (false) let lclOps : [refLcl op] = newLcl (where op 6 ref (a + \u00df)) \nlet (add, , iter) = stackI [op] let install : [a . op] = . req. case getLcl(lclOps) of some(o) . o := \ninj1 req; o | none . let o = new (inj1 req) in setLcl(lclOps, o); add(o); o let doOp : [op . 1] = .o. \ncase get(o) of inj1 req . o := inj2 f(req) | inj2 res . () in .x : [a]. let o = install(x) let rec loop() \n= case get(o) of inj1 . if not(get lock) and tryAcq(lock) then iter doOp; rel(lock); loop() else loop() \n| inj2 res . res in loop() Figure 10. Flat combining: Spec and implementation what our spec does but \n.at combining takes a cache-friendly approach intended for hard-to-parallelize data structures. The basic \nidea is to allow concurrent threads to advertise, through a lock-free side channel, their intent to perform \nan op\u00aderation on the data structure. One of the threads then becomes the combiner: it locks the data \nstructure and services the requests of all the other threads, one at a time (though new requests may \nbe pub\u00adlished concurrently with the combiner s execution). The algorithm exhibits relatively good cache \nbehavior for two reasons: (1) most of the time, operations do not need to execute any CASes to complete \nand (2) the combining thread enjoys good cache locality when ex\u00adecuting the operations of the sequential \ndata structure. In practice, .at combining yields remarkably strong performance, even when compared against \ncompletely lock-free data structures. The algorithm and its spec The .at combining algorithm was originally \ngiven as informal prose, so our .rst task is to formalize its implementation and .nd an appropriate spec, \nwhile making its higher-order structure explicit. We do so in Figure 10. We model an arbitrary sequential \ndata structure as a closure f of some type a . \u00df, where a represents the input to an operation to be \nperformed, \u00df represents the result, and calling the function performs the operation by imperatively updating \nsome state hidden within the closure. In other words, a . \u00df represents the type of an object. The goal \nof .at combining, then, is to wrap this object with (cache-ef.cient) synchronization. Its spec, .atS \n, simply uses mkSync to provide generic synchronization via global locking. Our .at combining implementation \n.atI uses three data struc\u00adtures to control synchronization.10 First, it uses a global lock to ensure \nthat only one thread at a time acts as the combiner. Sec\u00adond, it uses a Treiber s stack to enable threads \nto enroll in the data structure. To enroll, a thread inserts an operation record of type ref (a + \u00df), \nby which it can both advertise requests (of type a) and then await the results (of type \u00df). The combiner \nperforms these requests by iterating over the stack: for each record containing an inj1 req, it applies \nf to req, obtaining result res and updating the record to inj2 res. Finally, the thread-local reference \nlclOps allows threads to re-use their operation record once they have enrolled. Note that operation records \ncan be added to the stack or reset from \u00df to a at any time even when the combiner holds the lock. 10 \nThis implementation simpli.es Hendler et al. s description in three re\u00adspects: it uses a stack rather \nthan a queue, operation records are never de\u00adenrolled, and the combiner does not coalesce multiple operations \ninto a sin\u00adgle step. The .rst simpli.cation makes little difference, because ultimately we give a modular \nproof using our per-item spec, which could be applied to a queue as well. The other two need only minor \nchanges to the protocol. [Init L 6 Tid(i) i [Req(\u00a3, j, ., xI ) L i [Exec(\u00a3) L \u00a3, j, K, v 6 L(i) = \u00a3 \n* \u00a3 '.I inj1 xI * Tid(i) * .xS . (xI , xS ) . a * j .S .[f ' xS ] S 6 L(i) = \u00a3 * \u00a3 '.I inj1 - * Tid(i) \ni  [Resp(\u00a3, j, ., yI ) L 6 L(i) = \u00a3 * \u00a3 '.I inj2 yI * Tid(i) * .yS . (yI , yS ) . \u00df * j .S i [Ack(\u00a3)] \nL i .[yS ] 6 L(i) = \u00a3 * \u00a3 '.I inj2 - ' where fS 6 .x. acq(lockS ); let r = fS (x) in rel(lockS ); \nr Figure 11. The protocol for the operation record of thread i Unfortunately, if we set out to prove \nthe re.nement \u00b7 f .atI .atS : .a. .\u00df. (a . \u00df) . (a . \u00df) we will run headlong into a problem: it does \nnot hold! To wit: let C 6 let r = newLcl, f = [ ] (.(). getLcl(r)) in fork (setLcl(r, true); f()); setLcl(r, \nfalse); f() When this context is applied to .atS , it always returns some(false), because the .nal f() \nis always executed on the thread whose local r is false. But when applied to .atI , it can also return \nsome(true): the forked thread might act as the combiner, performing both ap\u00adplications of f and thus \nusing its own thread-local value for r. The crux of the problem is that thread-local storage allows functions \nto observe the identity of the thread executing them. We need to rule out this kind of side effect. We \ndo so by unrolling the de.nition of re.nement for functions, and simply removing access to ID-related \nknowledge, leading to a quali.ed re.nement: .a, \u00df, fI , fS . xS ) .pure \u00df) Type(a) . Type(\u00df) . D(.(xI \n, xS ) . a. (fI xI , fS . \u00b7 f .atI fI .atS fS : a . \u00df where (eI , eS ) .pure . 6 .i, j, .. {j .S .[eS \n]} i . eI {xI . .xS . .(xI , xS ) * j .S .[xS ]} This pure notion of related expressions embodies a semantic \nver\u00adsion of purity in a type-and-effect system where the only effect is uses thread-local storage . It \ncoincides exactly with the expres\u00adsion relation in Turon et al. [31], where thread-local storage was \nnot present. It is easy to prove, using CaReSL, that for any well\u00adtyped f : t . t ' that does not use \nthread-local storage (i.e., an effect-free f) we have D.(xI , xS ) . [t ]. (f xI , f xS ) .pure ' [t \n]. The protocol To prove the quali.ed re.nement, we need to for\u00admulate a protocol characterizing the \nalgorithm. We do so by giving a local protocol governing the operation record of each thread, and then \ntying these local protocols together into a single global one. Figure 11 gives the local protocol for \na thread with ID i, where the states have the following informal meanings: . thread i has not yet created \nits operation record Init thread i is creating its operation record Req thread i has requested an operation \nbe performed Exec the combiner is executing thread i s operation Resp thread i s operation has been completed \nAck thread i has acknowledged the completion The cycle in the protocol re.ects that, once thread i has \nenrolled an operation record, it can reuse that record inde.nitely. The labels on transitions signify \nbranching on the values of the listed variables. To fully understand how these constraints are enforced, \nwe must take into account both the tokens and the state interpretations of the protocol. The tokens include \ni and oi (one for each thread i, used only in i s local protocol) and a single global token Lock representing \nownership of the combiner s lock. L The interpretations [-]i of the non-. states are shown in the .gure; \nthe parameter L represents the contents of lclOps. With the exception of . and Ack, all states assert \nownership of Tid(i). On the other hand, when thread i begins executing the algorithm, it will have ownership \nof Tid(i) (recall the de.nition of re.nement, \u00a73.3). Thus, thread i can assume that the protocol is in \nstate . or Ack and only thread i can take a step away from these states. For thread i to take such a \nstep, it must prove that it is thread i by giving up Tid(i), but in return it gains the token i as a \nreceipt that can later be traded back for Tid(i) by moving (back) to Ack. All told, the ownership of \nTid(i) and the corresponding token i account for the .rst two of the guarantees listed above. The third \nguarantee is achieved using a similar strategy: to move to the Exec state, the combiner must prove that \nit holds the lock by temporarily giving up the (global) Lock token, but it receives the (local) token \noi as a receipt. Subsequently, only the combiner can move to Resp, making the opposite trade. After initialization, \neach state also asserts that the location f of the operation record for thread i both corresponds to \nL(i) and points to an appropriate sum injection (depending on the state). A .nal aspect of the local \nprotocol is capturing the cooperation inherent in .at combining: the combiner executes code on behalf \nof other threads. Cooperation is dif.cult for compositional re.nement techniques to handle, because such \ntechniques usually require prov\u00ading that for each piece of implementation code the corresponding piece \nof spec code behaves the same way and thus they typically provide no way to account for the mismatch \nbetween implemen\u00adtation and spec that cooperation entails. While our de.nition of re\u00ad.nement (\u00a73.3) likewise \nimposes a one-to-one relationship between implementation and spec code in its pre-and post-conditions, \nour treatment of spec code as a resource allows ownership to be trans\u00adferred to other threads during \nthe execution of the implementation. Thus, the Req and Resp states of our protocol assert shared ownership \nof i s spec code (running in spec thread j). In moving from Req to Exec the combiner thread must take \nownership of the spec code .[fS ' xS ]; and to subsequently move to Resp, the combiner must actually \nevaluate this code on thread i s behalf until fS ' xS reaches a value yS . Subsequently, when moving \nto the Ack state, thread i regains ownership of both Tid(i) and its spec code, by giving up its receipt \ntoken, i. The global transition system is then a product construction:Note that, while f (the location \nof the operation record) is chosen arbitrarily during initialization, it must remain .xed thereafter. \n E S . N .n OpState, s . LockState, at most one S(i) or s owns Lock State space (S, s) Movement through \nthe local protocol encompasses two roles: that of thread i, and that of the combiner. The protocol guarantees: \n(S ' , s ' )  ? 6 s lock ' ? . .i. S(i) S ' (i) Transitions (S, s) s (1) when thread i begins execution \nof the .at combining algorithm, the current state is either . or Ack, meaning that either there is no \n Owned tokens T (S, s) 6 Tlock(s) . i Ti(S(i)) operation record associated with the thread, or that \nthe associated A global state includes a collection S of local states from the record is ready to be \nre-used; (2) only thread i can move to or from operation record protocol (Figure 11), one for each thread \nID. In Init and Ack; and (3) only the combiner can move to or from Exec. giving the state space, we pun \nthe . state with an unde.ned input to a partial function and thus, we require that only a .nite number \nof threads i have a non-. state in S. Global states also have a single state s drawn from the standard \nlocking protocol (\u00a73.2), re.ecting the state of the global lock. The global Lock token is shared amongst \nall of the combined protocols: in a valid global state, at most one of the local states S(i) or s may \nclaim the Lock token. A transition in the global protocol allows each local protocol to make at most \none transition. Finally, the tokens owned in a global state are just the union of all the tokens claimed \nby the local states. Recall that in the re.nement proof for Treiber s stack (\u00a73.3), the spec lock is \ntreated continuously as a protocol-owned resource with value false, capturing the atomicity of the implementation \ncode: if an implementation step coincides with any spec steps, it must co\u00adincide with an entire critical \nsection s worth, going from unlocked to unlocked spec states in big steps. The combiner implementation, \nby contrast, is executing a function fI that is not necessarily atomic; all we know is that fI re.nes \nfS . We must therefore allow the spec lock to be held for multiple implementation steps, which we do \nby interpreting lock states as follows: [Unlocked 6 lock '.I false * lockS '.S false [Locked] 6 lock \n'.I true In short, the combiner gains private ownership of both the Lock to\u00adken and the spec lock itself \n(i.e., the internal lock used by mkSync; see fS ' in Figure 11) when it acquires the implementation lock. \nFinally, we lift these local interpretations to interpret global states via the following predicate: \nL (S, s). [s] * .L. lclOps '.I i L * *i.dom(S)[S(i)] The proof We close with a high-level view of the \nproof itself; details, as usual, are in the appendix. Unrolling the statement of quali.ed re.nement, \nwe need to prove {Tid(i) * j .S .[.atS fS ]} i . .atI fI {xI . .xS . (xI , xS ) . [a . \u00df] * Tid(i) * \nj .S .[xS ]} under the assumptions xS ) .pure \u00df) i rN j, Type(a), Type(\u00df), D(.(xI , xS ) . a.(fI xI , \nfS The proof begins in much the same way as the re.nement proof for Treiber s stack (\u00a73.3): we execute \nthe let-bound expressions that allocate the hidden state in both the implementation and spec, i.e., ' \nlock '.I false * lclOps '.I \u00d8 * . lockS . j .S .[fS ] * lockS '.S false where lockS is the lock allocated \nby mkSync and fS ' is as in Figure 11. These resources are precisely what we need to apply NEWISL for \nour global protocol, moving them from private to shared ownership. Letting p be the global protocol de.ned \nabove, we can claim D (\u00d8, Unlocked); \u00d8 n p, i.e., permanent knowledge that the global protocol exists. \nWe must then, in the context of this island and our previous assumptions, show that the closure returned \nby .atI re.nes the one returned by .atS , i.e., fS ' , at type a . \u00df. We .rst verify a version .at ' \nI of the .at combining algorithm that is identical to the one in Figure 10, except that it uses the coarse-grained \nstack, allowing us to use the per-item spec of \u00a73.3. We instantiate the per-item spec using the same \npredicate Op for per-item resources and iteration knowledge (p and q, respectively, 11 in the per-item \nspec): Op 6 (\u00a3). .k. [k . Req(\u00a3, -, -, -)] n p. This is a local assertion about the global protocol, \nin that the states of threads other than k can be in any rely-future state of ., i.e., any state whatsoever. \nThe Op predicate just claims that location f is an initialized operation record for some thread. By the \nper-item spec, all locations inserted into the stack must satisfy this property and since we have Op(f) \n. DOp(f), the property can be assumed even when iterating over stale elements of the stack. Operation \nrecords are created via install, whose speci.cation consumes Tid(i) and associated spec resources in \nexchange for the receipt i: ( ' n Tid(i) * j .S .[fS xS ] i . install x o. [i . Req(o, j, ., x)]; i \np 11 We leave off the locking state when it is Unlocked. for any (x, xS ) . a (i.e., for any related \narguments). The combiner actually performs operations via doOp, n {Op(o) * P } doOp o {P } where P 6 \n\u00d8; Lock p * lockS '.S false which assumes that the given location o is a valid operation record, and \nthat the invoking thread owns the Lock token as well as the spec lock. The shape of the spec for doOp \nexactly matches that required for iteration in the per-item spec (\u00a73.3), with P serving as the loop invariant, \nthus allowing us to deduce {P } iter doOp {P }. These auxiliary specs make it straightforward to prove \nre.nement for the closures returned by .at ' I (with coarse-grained stack) and .atS . Finally, suppose \nwe plug the combiner into a client context C that provides a suitable argument f. The above proof (together \nwith soundness, \u00a73.4) allows us to deduce |= C[.at ' I ] : C[.atS ] : t . Likewise, re.nement for Treiber \ns stack allows us to deduce |= C[.atI ] : C[.at ' I ] : t . Since re.nement is transitive, we can compose \nthese together to conclude |= C[.atI ] : C[.atS ] : t. 12 5. Related work In many respects, CaReSL builds \ndirectly on the semantic founda\u00adtion that we and colleagues laid in our prior work [31]. There, we developed \na relational Kripke model of a language very similar to the one considered here, and showed how granularity \nabstraction for several sophisticated (but structurally simple) .ne-grained data structures could be \nestablished by direct appeal to the model. The present work is a natural continuation of that work. First, \nwe pro\u00advide a logic that greatly simpli.es reasoning compared to working with the model; such a proof \ntheory is essential for scaling the ver\u00adi.cation method to large examples. Second, we use our logic not \njust to prove granularity abstraction results in isolation (as we did before), but also to facilitate \nthe modular veri.cation of a higher\u00adorder, structurally complex program. Along the way, we also extend \nour prior model to handle thread-local storage. The logic itself draws inspiration from LADR [5], which \nin turn provided a proof theory for reasoning about a sequential relational Kripke model (ADR [1]). Aside \nfrom incorporating concurrency, CaReSL offers a deeper uni.cation of re.nement and Hoare logic by treating \nre.nement as a mode of use of Hoare logic. Higher-order functions and concurrency While there has been \nstunning progress in logics for concurrency over the last decade, very few of these logics meaningfully \nsupport higher-order pro\u00adgramming, and among those that do, none supports reasoning about .ne-grained \nsynchronization or granularity abstraction. The logic that comes closest is higher-order concurrent ab\u00adstract \npredicates (HOCAP), .rst proposed for reasoning about .rst\u00adorder code [4] (the higher-order refers to \nthe logic of predicates) and very recently applied to higher-order code as well [27]. HO-CAP, like its \npredecessor CAP [3], accounts for concurrency by way of shared region assertions that constrain the possible \nup\u00addates to a shared resource. Our island assertions resemble shared region assertions indeed, we have \nadopted notation suggesting as much but work at a higher level of abstraction (i.e., protocol states), \nseparating knowledge bounding the state of the protocol (treated as a copyable assertion) from the rights \nto change the state (treated as a linear resource: tokens); see [31] for a more detailed comparison. \nBecause CAP lacks granularity abstraction, it is dif.\u00adcult to give a single principal speci.cation for \na data structure. In\u00adstead, one gives specialized specs (like the per-item spec for stacks) re.ecting \nparticular usages envisioned for a client which means new client scenarios necessitate new proofs. HOCAP \nattempts to 12 We are appealing to semantic re.nement here to take advantage of transi\u00adtivity. This reasoning \ncan be internalized in CaReSL by adding a proposition for semantic re.nement and axioms for re.nement \nsoundness and transitiv\u00adity, but there is little to be gained from doing so. address this shortcoming \nby explicitly quantifying over the way a client uses a data structure, but (1) this introduces the potential \nfor a problematic circularity, which clients must explicitly prove does not arise, and (2) it is not \nclear how to scale the approach to handle cooperation between threads (as in the .at combiner). Other \nconcurrency logics that support higher-order functions such as Hobor et al. s extension of concurrent \nseparation logic [14], or the very recent Subjective Concurrent Separation Logic [17] support only reasoning \nabout simple lock-based synchronization, and do not enable the kinds of re.nement proofs we have presented. \nGranularity abstraction Herlihy and Wing s seminal notion of linearizability [13] has long been held \nas the gold standard of cor\u00ad rectness for concurrent data structures, but as Filipovi\u00b4c et al. ar\u00adgue \n[8], what clients of these data structures really want is a contex\u00adtual re.nement property. Filipovi\u00b4c \net al. go on to show that, under certain (strong) assumptions about a programming language, lin\u00adearizability \nimplies contextual re.nement for that language. More recently, Gotsman and Yang generalized both linearizability \nand Filipovi\u00b4c et al. s result (the so-called abstraction theorem) to in\u00adclude potential ownership transfer \nof memory between concurrent data structures and their clients in a .rst-order setting [9]. While in \nprinciple proofs of linearizability can be composed with these results to support granularity abstraction, \nno existing logic has pro\u00advided support for doing so. We found it simpler to work with re.ne\u00adment directly \n(in particular, to encode it directly into a Hoare logic), rather than reasoning about linearizability. \nCaReSL enables clients to layer ownership-transferring protocols on top of a coarse-grained reference \nimplementation, after applying granularity abstraction, as we showed with the per-item spec (\u00a73.3). The \nonly Hoare logic we are aware of that can (formally) prove re.nement results is Liang and Feng s new \nlogic [18] (extending LRG [7]), which is inspired by the use of ghost state in Vafeiadis s thesis [32]. \nThe logic is powerful enough to deal with a range of sophisticated, .ne-grained concurrent algorithms, \nbut it is limited to .rst-order code. In addition, the speci.cations used in re.ne\u00adment are not reference \nimplementations, but are instead essentially atomic Hoare triples. While such speci.cations are appealingly \nab\u00adstract, they present two limitations: (1) they do not model the more general notion of granularity \nabstraction (supporting only atomic\u00adity abstraction, as we explained in footnote 1) and (2) they do not \nsupport the kind of transitive composition of proofs that we used in our case study. As a result, it \nis unclear how to use the logic to build modular proofs layering Hoare logic and re.nement. A radically \ndifferent approach to atomicity abstraction is Lip\u00adton s method of reduction [19], which is based on \nshowing that an atomic step commutes with all concurrent activity, and can there\u00adfore be coalesced into \na larger atomic block with e.g., code that is sequenced after it. Elmas et al. developed a logic for \nproving linearizability via a combination of reduction and abstraction [6], which in some ways resembles \nour interleaved application of re\u00ad.nement and Hoare-style reasoning, but with a rather different way \nof proving re.nement. It is limited, however, to .rst-order code and atomicity re.nement, and like HOCAP \nit is not powerful enough to handle .ne-grained algorithms that employ cooperation. Moreover, it does \nnot allow linearizability proofs to be composed transitively. Acknowledgments We would like to thank \nDavid Swasey for his careful reading of both the paper and its appendix. This work was partially funded \nby the EC FET project ADVENT. References [1] A. Ahmed, D. Dreyer, and A. Rossberg. State-dependent representa\u00adtion \nindependence. In POPL, 2009. [2] R. D. Blumofe, C. F. Joerg, B. C. Kuszmaul, C. E. Leiserson, K. H. Randall, \nand Y. Zhou. Cilk: An ef.cient multithreaded runtime sys\u00adtem. JPDC, 37(1):55 69, Aug. 1996. [3] T. Dinsdale-Young, \nM. Dodds, P. Gardner, M. Parkinson, and V. Vafeiadis. Concurrent abstract predicates. In ECOOP, 2010. \n[4] M. Dodds, S. Jagannathan, and M. Parkinson. Modular reasoning for deterministic parallelism. In POPL, \n2011. [5] D. Dreyer, G. Neis, A. Rossberg, and L. Birkedal. A relational modal logic for higher-order \nstateful ADTs. In POPL, 2010. [6] T. Elmas, S. Qadeer, A. Sezgin, O. Subasi, and S. Tasiran. Simplifying \nlinearizability proofs with reduction and abstraction. In TACAS, 2010. [7] X. Feng. Local rely-guarantee \nreasoning. In POPL, 2009. [8] I. Filipovi \u00b4c, P. O Hearn, N. Rinetzky, and H. Yang. Abstraction for concurrent \nobjects. Theoretical Computer Science, 411, 2010. [9] A. Gotsman and H. Yang. Linearizability with ownership \ntransfer. In CONCUR, 2012.  [10] T. Harris, S. Marlow, S. Peyton-Jones, and M. Herlihy. Composable memory \ntransactions. In PPOPP, 2005. [11] D. Hendler, I. Incze, N. Shavit, and M. Tzafrir. Flat combining and \nthe synchronization-parallelism tradeoff. In SPAA, 2010. [12] D. Hendler, N. Shavit, and L. Yerushalmi. \nA scalable lock-free stack algorithm. In SPAA, 2004. [13] M. P. Herlihy and J. M. Wing. Linearizability: \na correctness condition for concurrent objects. TOPLAS, 12(3):463 492, 1990. [14] A. Hobor, A. W. Appel, \nand F. Z. Nardelli. Oracle semantics for concurrent separation logic. In ESOP, 2008. [15] C. B. Jones. \nTentative steps toward a development method for inter\u00adfering programs. TOPLAS, 5(4):596 619, 1983. [16] \nD. Lea. The java.util.concurrent ConcurrentHashMap. [17] R. Ley-Wild and A. Nanevski. Subjective auxiliary \nstate for coarse\u00adgrained concurrency. In POPL, 2013. [18] H. Liang and X. Feng. Modular veri.cation of \nlinearizability with non-.xed linearization points. In PLDI, 2013. [19] R. J. Lipton. Reduction: a method \nof proving properties of parallel programs. Commun. ACM, 18(12):717 721, 1975. [20] P. W. O Hearn. Resources, \nconcurrency, and local reasoning. Theor. Comput. Sci., 375(1-3):271 307, 2007. [21] M. Parkinson and \nG. Bierman. Separation logic and abstraction. In POPL, 2005. [22] A. M. Pitts and I. Stark. Operational \nreasoning for functions with local state. In HOOTS, 1998. [23] G. Plotkin and M. Abadi. A logic for parametric \npolymorphism. In TLCA, 1993. [24] F. Pottier. Hiding local state in direct style: a higher-order anti-frame \nrule. In LICS, 2008. [25] J. H. Reppy. Higher-order concurrency. PhD thesis, Cornell Univer\u00adsity, 1992. \n[26] J. C. Reynolds. Separation logic: A logic for shared mutable data structures. In LICS, 2002. [27] \nK. Svendsen, L. Birkedal, and M. Parkinson. Modular reasoning about separation of concurrent data structures. \nIn ESOP, 2013. [28] R. Treiber. Systems programming: coping with parallelism. Technical report, Almaden \nResearch Center, 1986. [29] P. W. Trinder, K. Hammond, H.-W. Loidl, and S. L. Peyton Jones. Algorithm \n+ strategy = parallelism. JFP, 8(1):23 60, Jan. 1998. [30] A. Turon, D. Dreyer, and L. Birkedal. Unifying \nre.nement and Hoare\u00adstyle reasoning in a logic for higher-order concurrency: Appendix. http://www.mpi-sws.org/~turon/caresl/appendix.pdf. \n[31] A. Turon, J. Thamsborg, A. Ahmed, L. Birkedal, and D. Dreyer. Logical relations for .ne-grained \nconcurrency. In POPL, 2013. [32] V. Vafeiadis. Modular .ne-grained concurrency veri.cation. PhD thesis, \nUniversity of Cambridge, 2008. [33] H. Yang. Relational separation logic. TCS, 375(1-3):308 334, 2007. \n \n\t\t\t", "proc_id": "2500365", "abstract": "<p>Modular programming and modular verification go hand in hand, but most existing logics for concurrency ignore two crucial forms of modularity: *higher-order functions*, which are essential for building reusable components, and *granularity abstraction*, a key technique for hiding the intricacies of fine-grained concurrent data structures from the clients of those data structures. In this paper, we present CaReSL, the first logic to support the use of granularity abstraction for modular verification of higher-order concurrent programs. After motivating the features of CaReSL through a variety of illustrative examples, we demonstrate its effectiveness by using it to tackle a significant case study: the first formal proof of (partial) correctness for Hendler et al.'s \"flat combining\" algorithm.</p>", "authors": [{"name": "Aaron Turon", "author_profile_id": "81418594363", "affiliation": "MPI-SWS, Saarbruecken, Germany", "person_id": "P4261285", "email_address": "turon@mpi-sws.org", "orcid_id": ""}, {"name": "Derek Dreyer", "author_profile_id": "81548019178", "affiliation": "MPI-SWS, Saarbruecken, Germany", "person_id": "P4261286", "email_address": "dreyer@mpi-sws.org", "orcid_id": ""}, {"name": "Lars Birkedal", "author_profile_id": "81100622053", "affiliation": "Aarhus University, Aarhus, Denmark", "person_id": "P4261287", "email_address": "birkedal@cs.au.dk", "orcid_id": ""}], "doi_number": "10.1145/2500365.2500600", "year": "2013", "article_id": "2500600", "conference": "ICFP", "title": "Unifying refinement and hoare-style reasoning in a logic for higher-order concurrency", "url": "http://dl.acm.org/citation.cfm?id=2500600"}