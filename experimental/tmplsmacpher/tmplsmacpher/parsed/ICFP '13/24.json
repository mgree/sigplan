{"article_publication_date": "09-25-2013", "fulltext": "\n Weak Optimality, and the Meaning of Sharing Thibaut Balabonski INRIA, Gallium team thibaut.balabonski@inria.fr \nAbstract In this paper we investigate laziness and optimal evaluation strate\u00adgies for functional programming \nlanguages. We consider the weak .-calculus as a basis of functional programming languages, and we adapt \nto this setting the concepts of optimal reductions that were de.ned for the full .-calculus. We prove \nthat the usual implemen\u00adtation of call-by-need using sharing is optimal, that is, normalizing any .-term \nwith call-by-need requires exactly the same number of reduction steps as the shortest reduction sequence \nin the weak .\u00adcalculus without sharing. Furthermore, we prove that optimal reduc\u00adtion sequences without \nsharing are not computable. Hence sharing is the only computable means to reach weak optimality. Categories \nand Subject Descriptors I.1.3 [Languages and Sys\u00adtems]: Evaluation strategies General Terms Theory, Languages \nKeywords Strategies, Laziness, Weak reduction, Sharing, Opti\u00admality, Computability. 1. Introduction \nThe computation steps described by a functional program can usu\u00adally be scheduled in many different ways. \nHowever, the schedules do not all have the same ef.ciency. Consider for instance the fol\u00adlowing program: \nlet id x = x and cst1 x = 1 and diag x = (x, x) in diag (cst1 (id 2)) Its reduction space, that is, the \ndirected graph that sums up all the possible evaluations of the program, is pictured in Fig. 1. Each \nevaluation strategy choses a path in this graph. In particular, call-by-value reaches the result in three \nevaluation steps, with one inef.ciency: it evaluates the argument id 2,which is not needed, since its \nvalue is going to be discarded by the func\u00adtion cst1. Call-by-name also reaches the result in three evaluation \nsteps, with another inef.ciency: it performs twice the call to the function cst1 that is duplicated by \nthe function diag. Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. Copyrights \nfor components of this work owned by others than the author(s) must be honored. Abstracting with credit \nis permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. Request permissions from permissions@acm.org. ICFP 13, September \n25 27, 2013, Boston, MA, USA. Copyright is held by the owner/author(s). Publication rights licensed to \nACM. ACM 978-1-4503-2326-0/13/09. . . $15.00. http://dx.doi.org/10.1145/2500365.2500606   Figure 1. \nReduction space There are two ways of evaluating this program in only two steps. The .rst one is an \nhybrid strategy that .rst performs the call to cst1 and then the call to diag. The second one, call\u00adby-need, \ncombines call-by-name with an additional mechanism of memoization or sharing, which enables a re-use \nof already\u00adcomputed results, and which is meant to make up for duplications. This additional mechanism \nadds shortcuts in the original single\u00adstep reduction space. Since its introduction by Wadsworth s in \n1971, call-by-need is considered as a good strategy for avoiding at the same time non\u00adneeded and duplicated \ncomputation steps. The point of this paper is to turn this folkloric fact into a precise theorem. In \nparticular we will prove that call-by-need is an optimal strat\u00adegy with respect to some reasonable restriction \nof the .-calculus, that is, call-by-need always reaches the result of a computation in the least number \nof evaluation steps, or, call-by-need always picks the shortest path in some realistic reduction space. \nThe restriction that we consider comes from the following re\u00admark: the .-calculus is too powerful for \nmost functional program\u00adming languages. Indeed, while different languages use different strategies, most \nof them use only a subset of the reduction space known as weak evaluation, in which no anticipated evaluation \nof a function body is performed before the function is applied to its arguments. With this restriction \nto weak evaluation we lose some strategies, and the shortest path in the full reduction space of the \n.-calculus may not be preserved in the weak reduction space, as shown in the following example. Example \n1.1. Let a be a fresh variable. Consider the .-term t = (.x.x(xa))(.y.(.z.z)y) Its normal form is a, \nand it can be reached in four \u00df-steps by .rst reducing .y.(.z.z)y to .y.y. If we restrict ourselves to \nweak reduction however, we need .ve steps to reach the result. However, we consider weak evaluation as \nmore realistic than the full .-calculus, since this restriction is crucial for the imple\u00admentation, and \nthus in this paper we investigate weak optimality, that is, the evaluation strategies that are optimal \nin the restricted-, weak-reduction space. The main contributions are the following: We adapt L \u00b4evy \ns theory of optimal evaluation to the weak .-calculus (Section 4), and identify two strategies that reach \noptimality (Section 6). One of these weakly optimal strategies is the original call-by-need, which proves \nthe good behavior of Wadsworth s technique. The second weakly optimal strategy does not use sharing at \nall, which proves that the new reduction sequences allowed by sharing are not shorter than the shortest \nsingle-step reduction sequences.  We prove that optimal single-step reduction strategies cannot be computable \n(Section 3). This result completes the previ\u00adous statement by showing that, while sharing does not enable \nstrictly shorter reduction sequences, call-by-need nevertheless offers a computable way of reaching weak \noptimality.  We use a labelling of the weak .-calculus as a simple way to give a faithful account of \nall the causal dependencies between computation events (Section 4).  We show that the weak .-calculus, \nwhile it has the appearance of the .-calculus, has the properties of a .rst-order rewriting system (Section \n5).  Since it is a better .t for a conference format, we focus here on the case of the weak .-calculus. \nHowever, the results of the pa\u00adper hold for the more general case of weak reduction in orthogonal higher-order \nrewriting. The interested reader is referred to the au\u00adthor s doctoral dissertation [Bal12b].  2. The \nweak .-calculus In this section we recall the de.nitions and the basic properties of the weak .-calculus. \nAs in the previous paper [Bal12c], the .\u00adcalculus is expressed in the higher-order rewriting framework \nof Combinatory Reduction Systems (CRS). This may seem arti.cial at the beginning, but it will allow a \nclean handling of the labels and of the connection between weak reduction and .rst-order rewriting. First, \nlet us introduce the CRS in a nutshell. We see here only the basic syntax and mechanisms; a comprehensive \npresentation can be found in [KvOvR93]. De.nition 2.1 (Combinatory Reduction Systems / CRS). The gram\u00admar \nof metaterms in a CRS is: t ::= x | [x]t | f(t1, ..., tn) | Z(t1, ..., tn) where x is a variable, [x] \ndenotes the binding of a variable, f is an n-ary function symbol taken in a signature S, and Z is an \nn-ary metavariable. A term is a metaterm without metavariable, and an n-ary con\u00adtext is a term that contains \nn occurrences of the special nullary symbol D (the hole). A reduction rule is a pair L . R of closed \nmetaterms satisfying the following conditions: the metavariables in L appear as Z(x1, ..., xn) with \nx1, ..., xn distinct bound variables; and  all the metavariables of R also appear in L.  A rule matches \na term by application of a valuation s that maps n-ary metavariables to n-ary contexts avoiding variable \ncapture. Reduction by a rule L . R with valuation s inacontext c is c[Ls] . c[Rs]. Example 2.2. The .-calculus \ncan be seen as a CRS with: a signature S. = {., @} where the symbol . is unary and the application symbol \n@ is binary; a unique reduction rule @(.([x]Z(x)),Z') . Z(Z'). The .-term (.x.x)y is encoded in the \nCRS term @(.([x]x),y). However, we will still use usual .-calculus notation wherever this can lighten \nthe presentation. Example 2.3. First order rewriting systems, also called Term Rewriting Systems (TRS) \nare also a particular case of CRS, where metaterms do not contain any binders. The absence of binders \nhas some consequences. In particular, since any metavariable Z appearing in the left member L of a reduction \nrule L . R is supposed to appear as Z(x1, ..., xn) where the xi s are bound variables, we are restricted \nto nullary metavariables that appear simply as Z. This restricts a lot the re\u00adduction rules that can \nbe de.ned, and explains the simpler behavior of .rst-order compared to higher-order rewriting. In this \npaper we will consider a signature that strictly contains the one of the .-calculus. De.nition 2.4 (Terms). \nFrom now on we consider the signature S = {e, ., @}.F where: e is a unary symbol called dummy symbol, \nand  F = n.N Fn where for all n, Fn is a countable set of n-ary symbols called supercombinators. From \nnow on, by term we mean a CRS term over the signature S. Write . for the set of terms. We use the usual \nnotion of position in a term. Write t|p for the subterm of t rooted at position p, and call two positions \np1 and p2 disjoint if none is a pre.x of the other. We also use the usual notion of free variables. Write \nt{x:=u} for the substitution by u of all the free occurrences of the variable x in t. Write dom(s) for \nthe domain of a n-ary substitution s.A context is a term also containing the nullary symbol D, called \nhole. Replacement c[t] of a hole in a context c by a term t is as usual. The supercombinator symbols \nin the sets Fn are used in Sec\u00adtion 5 for the reduction of the weak .-calculus to a .rst-order rewrit\u00ading \nsystem. Until then they play no role and may be safely ignored. The dummy symbol e has no meaning in \nitself. It is needed for the causal interpretation of labels (Section 4.5), and serves in particular \nas a container for dynamically created labels. As a con\u00adsequence, occurrences of e should not interfere \nwith \u00df-reduction. This leads to the following countable set of rules to simulate \u00df\u00adreduction by allowing \nany number of e s between the application and the .-abstraction. De.nition 2.5 (Beta-reduction). The \nrules for \u00df-reduction are as follows: \u00df0: @(.x.Z(x),Z') .\u00df e(Z(Z')) \u00df1: @(e(.x.Z(x)),Z') .\u00df e(Z(Z')) \n\u00df2: @(e(e(.x.Z(x))),Z') .\u00df e(Z(Z')) ... Write . : t . t' a reduction step . from a term t toaterm t'. \nWrite src(.)= t for the source of ., tgt(.)= t' for its target, and root(.) for the position in src(.) \nof the redex reduced by ..A normal form is a term that is the source of no reduction step. The usual \nnotions of ancestors and descendants [Ter03], which track subterms along reduction in the .-calculus \nare straightforwardly adapted1 , as illustrated in Example 2.6. A residual of a redex r is a 1 The book \n[Ter03] gives several de.nitions of descendants that differ in particular in how they treat variables. \nIn this paper we only need to consider the descendants of applications, hence the simplest de.nition \nis enough. descendant of r which is still a redex. A redex r in the target tgt(.) of a reduction step \n. is created by . if it is not a residual of a redex in the source src(.).A development of a set of redexes \nR in a term t is a reduction sequence ( : t _ t ' where each step reduces a residual of a redex in R. \nA development ( is complete if there is no remaining residual of any redex in R.Aredex r in a term t \nis needed if any reduction sequence from t to a normal form reduces at least one residual of r. The e \ns in the right hand sides of the \u00df-rules are used for the correct labelling of collapsing reductions \n(Section 4.5). The use of the dummy symbol e is inspired by the notion of expansion in term rewriting \nsystems [Ter03, Chap. 8]. Example 2.6. The rule \u00df1 enables a reduction step . : @(e(.x.@(x, x)),y) . \ne(@(y, y)) The two occurrences of y in the target are the descendants of the y in the source, and the \nlatter is the ancestor of the formers. The e in the source has no descendant and the e in the target \nhas no ancestor. In the usual, non-weak .-calculus, \u00df-reduction can be applied in any context. In contrast, \nthe weak .-calculus introduces some restrictions on the contexts in which a reduction rule can be ap\u00adplied. \nIntuitively, reduction will be forbidden at any position (called frozen) that belongs to the effective \nscope of a binder, which we call skeleton. This restriction is formally based on the following de.nitions. \nDe.nition 2.7 (Free expression). A free expression2 of a .-term .x.t is a subterm s of t such that any \nfree variable occurrence in s is also free in .x.t. A free expression is maximal if it is not a subterm \nof another free expression. De.nition 2.8 (Pre.x). A n-ary pre.x of a .-term t is a n\u00adary context c such \nthat there are n.-terms t1, ..., tn satisfying t = c[t1, ..., tn]. We will be mostly interested in the \npre.x that is obtained by removing all the maximal free expressions of a .-abstraction .x.t, which represents \nthe parts of the term that are not independent from the binder, and which we call skeleton of .x.t. Example \n2.9 illus\u00adtrates this notion, and de.nition 2.10 then gives a direct de.nition. Example 2.9. Suppose \nw1 and w2 are two variables, and consider the .-term t = .x.(.y.y)(.z.z(w1w2)x) Its maximal free expressions \nare .y.y and w1w2, while its skeleton is the binary pre.x .x.D(.z.zDx) The skeleton is marked with bold \nlines in the following picture. .x @ .y .z y @ @ x z @ w1 w2 De.nition 2.10 (Skeleton). For any set \nof variables .,call .\u00adskeleton of a .-term t and write ((t)). for the pre.xof t de.ned 2 Originally called \nabstractable expression by Wadsworth. as follows: ((t)). = D . n fv(t)= \u00d8 Otherwise: ((x)). = x x . \n. ((e(t))). = e(((t)).) ((.x.t)). = .x.((t))..{x} ((t1t2)). = ((t1)).((t2)). ((f(t1, ..., tn))). = f(((t1)). \n, ..., ((tn)).) Call skeleton of a .-abstraction .x.t its pre.x .x.((t)){x}. Write ((.)) for the set \nof all skeletons. De.nition 2.11 (Frozen positions). A position p of a .-term t is called frozen if it \nis contained in the skeleton of a .-abstraction of t.Call a frozen redex any \u00df-redex whose main @ symbol \n(the root of the redex) occurs at a frozen position. De.nition 2.12 (Weak \u00df-reduction).Weak \u00df-reduction \nis de.ned by the application of the \u00df-reduction rules to any \u00df-redex that is not frozen. From now on \nwe call redex only a non-frozen redex, that is a redex for weak reduction. A weak normal form is a term \nthat contains no (non-frozen) redex. Example 2.13. Let a be a fresh variable. Consider the .-term t = \n.x.@(.y.y, @(x, @(.z.z, a))) Its frozen positions are E, 1, 12, and 121 (corresponding to .x, x, and \nthe two applications in between). Hence the redex @(.y.y, @(x, @(.z.z, a))) is frozen, while the redex \n@(.z.z, a) is not. The weak normal form of t is .x.@(.y.y, @(x, a)). This notion of weak \u00df-reduction \n.rst appeared in works by Howard [How70] and by C\u00b8 agman and Hindley [cH98], as a mean to describe in \nthe syntax of the .-calculus the reduction behavior of the combinators S and K of Combinatory Logic. \nFor this reason it has been called combinatory weak reduction. In this setting, C\u00b8 agman and Hindley \nproved in particular that the weak .-calculus had the Church-Rosser property. Proposition 2.14 (Church-Rosser, \nC\u00b8 agman and Hindley [cH98]). The weak .-calculus is con.uent.  3. (Un-)Computability In this section \nwe prove the Uncomputability Theorem 3.14, which states that any optimal strategy for the weak .-calculus, \nthat is any reduction strategy that selects a shortest weak reduction sequence for any .-term, is necessarily \nuncomputable. A similar result has already been proved by Barendregt et al. for the usual non-weak .-calculus \n[BBKV76]. However, a key step of their proof does not hold if we restrict ourselves to weak reduction. \nIn Section 3.1 we re\u00adview the structure of the proof of Barendregt et al. and show where it gets broken \nby weak reduction. Then in Section 3.2 we patch their proof thanks to an original use of the .-lifting \ntransformation. In this whole section we ignore the dummy symbol e and the supercombinator symbols in \nF. 3.1 Barendregt et al. s proof of uncomputability The proof by Barendregt et al. proceeds by exhibiting \ntwo recur\u00adsively inseparable sets of .-terms A and B3 and by showing how to separate them using a hypothetical \noptimal reduction strategy. The set A (resp. B)is de.ned as the set of all .-terms admitting 3 A and \nB are recursively inseparable if there is no recursive set C such that A. C and Bn C = \u00d8. some cleverly-chosen \nta (resp. tb) as normal form , and these two sets A and B are known to be recursively inseparable thanks \nto a result by Scott [Sco68]. The two sets A and B are then separated using a total com\u00adputable function \n.(.) on .-terms such that: Each .(t) contains exactly two \u00df-redexes, written rt a and rt b .  From \n.(t), any optimal reduction strategy reduces rt a if t _ ta, and rt b if t _ tb (and nothing is speci.ed \nif t reduces neither to ta nor to tb).  Their .rst step in de.ning the .-term .(t) is to build an enu\u00admerator,that \nis a .-term e in normal form such that, if nt is a G\u00a8odel number for a .-term t,and if 'nt' is a representation \nin the .-calculus of nt,then e'nt'_ t. This enumerator e works by building applicative combinations of \nthe standard S, K,and I combinators. In particular e'nt' reduces to (t)CL, the usual en\u00adcoding of t in \ncombinatory logics S, K, I. Then, after reverting all the combinators into .-terms, which gives a .-term \n((t)CL).,itis folklore that ((t)CL). _ t. However, the latter is not true anymore in the weak .-calculus, \nas the following example shows. Example 3.1. Consider the .-term t = .x.xx Then we have (t)CL = SI I \n((t)CL). =(.xyz.xz(yz))(.x.x)(.x.x) and ((t)CL). _ t in the non-weak .-calculus. However, the weak normal \nform of ((t)CL). is .z.((.x.x)z)((.x.x)z) Hence we do not have ((t)CL). _ t in the weak .-calculus. The \nnext section provides a new function .(.) that is suited for use with the weak .-calculus.  3.2 Uncomputability \nfor the weak .-calculus First, let us state that Scott s inseparability result is still valid for the \nweak .-calculus. Proposition 3.2 (Inseparability, Scott [Sco68]). Let A and B be two non-empty sets of \n.-terms that are closed under weak \u00df\u00adreduction. Then A and B are recursively inseparable. Proof. Similar \nto Barendregt s proof [Bar84, Chap. 6], that does not need non-weak \u00df-reduction. The main mechanism of \nour function .(.) is a decomposition of a .-term into several components, all of which are in weak normal \nform. We rely on the fact that any skeleton is a normal form for weak reduction (all the positions of \na skeleton are indeed frozen), and we use a technique inspired by .-lifting to represent a .-term as \nan assembly of skeletons. The .rst step, corresponding to what Danvy and Schultz call parameter lifting \n[DS00], makes each .-abstraction closed by extracting its maximal free expressions4 . 4 Note that ordinary \n.-lifting extracts only the free variables; switching to free expressions corresponds to fully lazy .-lifting \n[PJ87]. De.nition 3.3 (Free Expressions Lifting (fel)). fel(x)= x fel(t1t2)= fel(t1)fel(t2) fel(.x.t)= \n(.y1...ynx.s[y1, ..., yn])fel(t1)... fel(tn) s = ((t)){x} t = s[t1, ..., tn] y1, ..., yn fresh variables \n Lemma 3.4 (Free expressions lifting). For any .-term t we have fel(t) _ t. Proof. By induction on t. \n Example 3.5. Suppose a, b and c are three free variables and consider the term t = .x1.((.x2.x2a)b)(x1c) \n Then free expression lifting proceeds as follows: fel(t)= (.y1y2x1.y1(x1y2))fel((.x2.x2a)b)fel(c) =(.y1y2x1.y1(x1y2))(fel(.x2.x2a)fel(b))fel(c) \n=(.y1y2x1.y1(x1y2))((.y1 ' x2.x2y1 ' )fel(a)fel(b))fel(c) =(.y1y2x1.y1(x1y2))((.y1 ' x2.x2y1 ' )ab)c \nMoreover, we have fel(t) _ t in three weak reduction steps. The second step, corresponding to what Danvy \nand Schultz call block .oating , moves each function to toplevel. This is done by replacing each skeleton \nby a fresh variable and mapping these new variables to the skeletons they denote. De.nition 3.6 (Skeletons \nFloating (sf)). sf(x)= (x, \u00d8) sf(t1t2)=(u1u2,s1 l s2) .i .{1, 2}, sf(ti)=(ui,si) sf(.x.t)= (z, {z := \n.x.t}) z fresh where l is the union of two substitutions with disjoint domains. Lemma 3.7 (Skeletons \n.oating). Let t be a .-term and p a position. Write sf(t)=(u, s).Then u s = t. Proof. By induction on \nt. Example 3.8. Consider the term fel(t) obtained at the end of example 3.5. Skeletons .oating sf(fel(t)) \nyields the term u = z1(z2ab)c and the substitution s = { z1 := .y1y2x1.y1(x1y2); z2 := .y1 ' x2.x2y1 \n' } such as u s = fel(t). Finally, the result of the two previous steps allows us to decom\u00adpose a .-term \nt as an application s0s1...sn with n = 1 and where each si is a weak normal form. De.nition 3.9 (Decomposition). \nLet t be a .-term. Write sf(fel(t)) = (u, s)  If dom(s)= \u00d8,thende.ne dec(t)=(.z.u)I  Otherwise write \ndom(s)= {z1, ..., zn}, and de.ne dec(t)=(.z1...zn.u)s(z1)...s(zn)   Lemma 3.10 (Decomposition). For \nany .-term t we have dec(t) _ t. Proof. By case on whether skeletons .oating yields an empty sub\u00adstitution \nor not. De.nition 3.11 (Separating function). The separating function .(.) is de.ned as follows: let \nt be a .-term; write dec(t)= s0s1...sn;then .(t)=(.x.xs0x)(.y.ys1...sn(II)) where I = .z.z is the identity. \nExample 3.12. Consider the term t from example 3.5. Its decomposition is dec(t)= (.z1z2.z1(z2ab)c) (.y1y2x1.y1(x1y2)) \n(.y1 ' x2.x2y1 ' ) where each of the three components is a weak normal form, and where we can check that \ndec(t) _ t in .ve weak reduction steps. Finally we de.ne .(t)= (.x.x(.z1z2.z1(z2ab)c)x) (.y.y(.y1y2x1.y1(x1y2))(.y1 \n' x2.x2y1 ' )(II)) opt Lemma 3.13 (Separation). Consider an optimal strategy - . for the weak .-calculus, \nand let s0, s1, ..., sn be .-terms in weak normal form. Write u =(.x.xs0x)(.y.ys1...sn(II)),where I = \n.z.z is the identity. Then the following holds: If s0s1...sn _ .xyz.z opt then u - . (.y.ys1...sn(II))s0(.y.ys1...sn(II)) \nIf s0s1...sn _ a opt then u - . (.x.xs0x)(.y.ys1...snI) Proof. Similar to Barendregt s [Bar84, Lemma \n13.5.5]. Now we are able to prove the Uncomputability Theorem 3.14. Theorem 3.14 (Uncomputability). There \nis no computable optimal reduction strategy for the weak .-calculus. Proof. De.ne A = {t . . | t _ .xyz.z} \nand B = {t . . | t _ a}. Both sets are closed by weak \u00df-reduction, hence by Scott s Inseparability Proposition \n3.2 the sets A and B are recursively inseparable. opt Now suppose there is a recursive optimal strategy \n- . for the weak .-calculus. Then the set opt C = {t . . | .(t) - . t ' at position E} is computable. \nAnd yet, by Separation Lemma 3.13 the set C separates A and B. Contradiction. Finally, we have proved \nthat optimal reduction strategies for the weak .-calculus cannot be computable. In the following sections \nwe will investigate other approaches to optimality, in particular by using sharing.  4. Weak optimality \nIn this section we adapt Vuillemin-L\u00b4evy s theory of optimal\u00adity [L\u00b4e80] to the weak .-calculus, which \nwill give a characteri\u00adzation of optimal reduction in a setting in which some sharing of computation \nis allowed. 4.1 A short introduction to Vuillemin-L\u00b4evy s optimality This approach is intended to give \na lower bound on the number of evaluation steps needed for normalizing a term, by characterizing an ideal \ncase in which there is no unneeded computation step and no duplication at all. Since such an ideal reduction \nsequence cannot be reached in the pure .-calculus it is either non-existing or non-computable this characterization \nof optimality proceeds by identifying families of computation steps that regroup all the duplicates of \na given original redex. Hence, the families describe which redexes should be shared in order to achieve \na computation without duplication. The main idea behind families is the following: consider a reduction \nsequence ( : t1 . t2 . ... . tn and two redexes r1 and r2 in tn.If there is a redex r0 in one of the \nti s that is a common ancestor to r1 and r2,then r1 and r2 are in the same family (they are two duplicate \ncopies of the original r0). Hence two redexes are in the same family if their duplication could have \nbeen avoided by .rst reducing a common ancestor. Example 4.1. In this reduction sequence we follow the \nunderlined expression (.z.z)(.w.w). (.x.xx)(.y.((.z.z)(.w.w)y)) . (.y.((.z.z)(.w.w)y))(.y.((.z.z)(.w.w)y)) \n . (.z.z)(.w.w)(.y.((.z.z)(.w.w)y))  The two occurrences of (.z.z)(.w.w) are residuals of the .rst \noccurrences; they are in the same family. However, this criterion is not enough: two redexes can be in \nthe same family even if their common ancestor is not visible in the sequence (. Example 4.2. Now reduce \nthe two underlined occurrences in the last term of example 4.1. We observe two new redexes (underlined \nbelow) that have no visible ancestor, and therefore no visible common ancestor. (.w.w)(.y.(.w.w)y) Such \na common ancestor exists somewhere though. Consider this other possible evaluation sequence and follow \nthe underlined terms. (.x.xx)(.y.((.z.z)(.w.w)y)) . (.x.xx)(.y.((.w.w)y)) . (.y.((.w.w)y))(.y.((.w.w)y)) \n . (.w.w)(.y.(.w.w)y)  Hence, by permuting the original sequence we have found a com\u00admon ancestor to \nthe two underlined redexes, which are thus in the same family. Reducing them both .nally yields the term \n.y.y Hence, the de.nition of families requires to reason globally on the reduction spaces and the permutations \nof their steps. To circumvent this problem, L\u00b4evy also gave a direct characterization of families, through \na labelled .-calculus that is shown to be stable by permutation of reduction steps. With this additional \ntool, the labels directly tell which redexes should be reduced in a shared way. Unfortunately the de.nition \ndoes not give an implementation. One of the dif.culties is that optimal sharing in the .-calculus requires \nsharing contexts (as the context (.w.w)D in the previous example). The .rst algorithms realizing optimal \nsharing in the non-weak .\u00adcalculus were found ten years later [Lam90]. In this section we characterize \nfamilies and optimality for the weak .-calculus. We use an abstract characterization of Vuillemin\u00adL\u00b4evy \noptimality that has been proposed by Glauert and Khasi\u00addashvili. 4.2 Glauert &#38; Khasidashvili s Deterministic \nFamily Structures Glauert and Khasidashvili [GK96] propose an abstract framework for characterizing optimality \nthat is composed by two layers: 1. a well-behaved notion of residual (so-called Deterministic Residual \nStructure or DRS); and 2. a notion of family built above the notion of residual and enjoy\u00ading additional \ngood properties (so-called Deterministic Family Structure,or DFS).  Deterministic Residual Structures \nare de.ned by only two ax\u00adioms, which are usually satis.ed by the natural notions of residual in .-calculus-based \nsystems. De.nition 4.3 (Deterministic Residual Structure, DRS [GK96]). A Deterministic Residual Structure \n(DRS) is a rewriting system equipped with a residual relation satisfying the following proper\u00adties: \n[FD] All developments are terminating; all complete develop\u00adments of a given set of redexes have the \nsame target; all com\u00adplete developments of a given set of redexes de.ne the same residuals.  [Acyclicity] \nLet r and re be two distinct redexes of a term t such that r erases5 re.Then re does not erase r.  The \nde.nition of Deterministic Family Structures is more com\u00adplex. First, since the families of redexes in \na term t are linked to the history of t that is to [the permutations of] a reduction sequence of which \nt is the target this de.nition considers families of reduction steps with history. De.nition 4.4 (Reduction \nstep with history). A reduction step with history is a pair ((, .) where ( is a reduction sequence, . \nis a reduction step, and tgt(()= src(.). Two reduction steps with history ((1,.1) and ((2,.2) are coinitial \nif src((1)= src((2). Second, we need to include the notions of residual and of per\u00admutation in the de.nition \nof families. For this they de.ne a notion of copy, which has to be understood as a residual relation \nmodulo permutation. De.nition 4.5 (Copy). Let ((1,.1) and ((2,.2) be two coinitial reduction steps with \nhistory. Suppose (1 has no residual after (2 6 . Let (2/(1 be a complete development of the residuals \nof (2 after (1.If .2 is a residual of .1 after (2/(1,then ((2,.2) is said to be a copy of ((1,.1), written \n((1,.1) C ((2,.2). De.nition 4.6 (Deterministic Family Structure [GK96]). Let R be aDRS.Let . be an equivalence \nrelation over coinitial reduction steps with history, whose equivalence classes are called families. \nWrite Fam((, .) for the family (ie the equivalence class) of a re\u00adduction step with history ((, .).Let \nY . be a binary relation over families, called contribution relation. The triple (R, .,Y .) is a Deterministic \nFamily Structure (DFS) if the following axioms are satis.ed: [Initial] For any coinitial distinct reduction \nsteps .1 and .2 we have Fam(\u00d8,.1) = Fam(\u00d8,.2). [Copy] C ...  [FFD] Any reduction sequence that contracts \nredexes of a .nite number of families is .nite.  5 r erases re means that re has no residual after the \nreduction of r. 6 This means that any task of i1 is either completed or erased by i2.  [Creation] For \nany reduction step with history ((, .),if . cre\u00adates a reduction step . ' then Fam((, .) Y . Fam((., \n. ' ).  [Contribution] For any two families f1 and f2, we have f1 Y  . f2 if and only if for any reduction \nstep with history ((2,.2) . f2 there is a reduction step with history ((1,.1) . f1 such that (1.1 is \na pre.xof (2. De.nition 4.7 (Family reduction). In a DFS, a family reduction step is a complete development \nof a set R of redexes of a term t which all belong to a given family f.Itis complete if R contains all \nthe redexes of t that belong to the family f.Itis needed if one of the redexes of R is needed. Family \nreduction gives a new, richer reduction space in which shared evaluation is possible. From now on, we \nare interested in optimality in this bigger reduction space, that is in optimality with sharing. Proposition \n4.8 (Optimality in DFS [GK96]). In any DFS, a family reduction sequence that reduces only complete needed \nfamilies is optimal. Our goal for the rest of the section is to build a DFS over the weak .-calculus, \nin order to characterize weak optimality. That means we need to give a concrete de.nition of the abstract \nconcept of family and to prove that all the axioms are satis.ed. Here is a summary of the required ingredients: \nwe need to exhibit concrete notions of family (.) and of contri\u00adbution (Y .); following L\u00b4evy, we will \ndo this through a labelling of the weak .-calculus presented in Section 4.4;  in Section 4.3 we derive \na classi.cation of redex creations that both guide the de.nition of labels and enables the proof of the \n[Creation] axiom; and  in Section 4.5 we show that the labels give a faithful account of the causal \ndependencies between reduction steps, which will be the core of the proof of the [Contribution] axiom. \n The proof of the .nite development axioms [FD] and [FFD] is postponed to Section 5, where we establish \na connection between the weak labelled .-calculus and a .rst-order rewriting system, which allows us \nto re-use .nite development results from the .rst\u00adorder rewriting literature. 4.3 Redex creation in \nthe weak .-calculus In this subsection we do not use the dummy symbol e yet. In the .-calculus, a redex \nis a connection between a .-abstraction and an argument. Hence a redex is created each time a new connec\u00adtion \nof this kind appears. Example 4.9. The three reduction steps below show the three ways of connecting \na .-abstraction .x.t to an argument u described by L \u00b4evy [L\u00b4e78]. ((.y..x.t)v)u . (.x.t{y := v})u ((.y.y).x.t)u \n. (.x.t)u (.y.yu).x.t . (.x.t)u These examples are valid independently of the restriction to weak reduction. \n   In the weak .-calculus there is a new creation case, which can be observed when a frozen redex gets \nunfrozen. Example 4.10. In the source of the following reduction step, the expression (.x.yx)u is not \na redex because it is frozen by the variable y. (.y.((.x.yx)u))t . (.x.tx)u   For the rest of this \npaper, it is enough to classify the redex creations in two categories: either the left part of an application \nevaluates to a .-abstraction, or an application subterm is affected by a substitution. This classi.cation \nis formalized by Redex creation Theorem 4.12. We begin with a lemma that considers the case of a redex \ncreation at the root of a term. Lemma 4.11 (Root redex creation). Let . : t = c[r] . c[r ' ]= t ' be \na reduction step with r =(.x.u)v and r ' = u{x := v}. Suppose c = D and t ' is a redex rc =(.y.uc)vc \nthat is created by ..Then the following holds: (1) c = Dvc and r ' = .y.uc. Proof. Case analysis on c \n= D: Case c = .z.c ' ,or c = f(t1, ..., c ' , ..., tn). Then there exists '' '' u such that t = .z.u \n' (resp. t = f(t1, ..., u , ..., tn)). Contradiction with the hypothesis that t ' is a redex. Case c \n=(.y.uc)c ' . Then the ancestor t of rc is already a redex. Contradiction with the hypothesis that the \nredex rc is created by ..  Case c = c ' vc.Case analysis on c ' :  Case c ' = D.Then c = Dvc and r \n' = .y.uc. Hence proposition (1) holds. ' '' '' '' '' Case c = t1c or c t2.Then c = t1c vc or c = c \nt2vc, '' ''' and there exists a pair t1,t2 such that t = t1t2vc. Contra\u00addiction with the hypothesis that \nt ' is a redex. Case c ' = f(t1, ..., c '' , ..., tn) is similar. Case c ' = .y.c '' .Then c =(.y.c \n'' )vc, and the ancestor t of rc is already a redex. Contradiction with the hypothesis that the redex \nrc is created by .. Theorem 4.12 (Redex creation). Let . : t = c[r] . c[r ' ]= t ' be a reduction step \nwith r =(.x.u)v and r ' = u{x := v}.Let rc =(.y.uc)vc be a redex in t ' that is created by .. Write t \n' = cc[rc]. Then one of the following holds: (1) c = cc[Dvc] and r ' = .y.uc. (2) The position root(rc) \nis a descendant of a position of the x\u00adskeleton of u. Proof. Case analysis on the relative positions \nroot(r ' ) and root(rc). If root(r ' ) and root(rc) are disjoint, then the redex rc already exists in \nt. Contradiction with the hypothesis that the redex rc is created by ..  If root(rc) is a pre.xof root(r \n' ), then there is a c ' such that  ''' '' rc = c [r ] and t = cc[c [r ]].Since rc is created by ., \nits ' '' ancestor c [r] is not a redex in t = cc[c [r]]. Suppose c [r] is a frozen redex. Then in c ' \n[r] there is a free occurrence of a variable x that is bound in cc. Case analysis on the position of \nthis variable occurrence: If x appears in r,then r is a frozen redex. Contradiction with the hypothesis \nthat the redex r is reduced. If x appears in c ' ,then rc = c ' [r ' ] still contains x and rc is a \nfrozen redex. Contradiction with the hypothesis that rc is aredex. Hence c ' [r] is not a frozen redex, \nand we can apply Root redex ' '' creation Lemma 4.11 to the step c [r] . c [r ] to conclude. If root(r \n' ) is a pre.xof root(rc),then rc is a subterm of r ' = u{x := v}, and there are three cases to consider: \n If rc is in a substituted occurrence of v, then its ancestor is already a redex. Contradiction with \nthe hypothesis that the redex rc is created by ..  If root(rc) is in the x-skeleton of u but is not \nthe position of an occurrence of x, then proposition (2) holds.  If root(rc) is in u but not in the \nx-skeleton of u, then the ancestor of rc is equal to rc and is already a redex. Contra\u00addiction with the \nhypothesis that the redex rc is created by ..  4.4 Weak labelled .-calculus From now on we use the full \nsignature S. The classi.cation of redex creations 4.12 will guide the de.\u00adnition of the labels following \na simple principle: the labels have to record all possible redex creations. More precisely, each redex \nr (and by extension each reduction step .)is given a name that contains all the information relevant \nto the creation of r. Thus we expect at least two things: If a redex r is created by a reduction step \n., then the name of . should in.uence the name of r in some way.  Once a redex is created, its name \nnever changes.  These two properties are formalized in Direct contribution Lemma 4.21 and Residuals \nLemma 4.22. These properties will allow us to use the labels to de.ne L \u00b4evy-style redex families. A \nlabel is either an atomic label (here a position for technical convenience) or a combination [O,a] of \na name O and a label a.A name is a sequence of labels obtained by collecting the individual labels of \nthe meaningful positions of the corresponding redex. Most of these de.nitions are taken from the previous \npaper [Bal12c]; the obtained system is a slight simpli.cation of a labelling presented by Blanc, L\u00b4evy \nand Maranget [BLM07]. De.nition 4.13 (Labels). The set L of labels and the set N of redex names are de.ned \nby the following grammar: Labels L a ::= p | [O,a] p position Names N O ::= a1...an n = 2 The labelled \nsignature SL is de.ned as SL = {fa | f .S,a .L} where arities are preserved. Let (.) be the forgetful \nmorphism that maps terms over SL to terms over S by removing their labels. For any term t over SL and \nany position p . pos(t), write tp(t) for the label of t at position p. In this section, call initial \na term over SL whose labels are all atomic (that is, a position) and different. The contribution relation \nis a straightforward containment re\u00adlation on the labels. The strength of this simple syntactic notion \nis that it is equivalent to a more semantic notion of causal dependency between reduction steps, which \nwe will prove in Section 4.5. De.nition 4.14 (Contribution). The direct contribution relation Y.dc is \nde.ned on names by the following criterion: O Y.dc O1[O,a]O2 for any name O, any possibly empty names \nO1, O2, and any label a.The contribution relation Y.c is the transitive closure of Y.dc. De.nition 4.15 \n(Name of a redex). A labelled redex is a term over SL of the form r =@.(ea1 (...ean (.. x.t)),u) Its \nname is the sequence name(r)= .a1...an . Labelled \u00df-reduction introduces the name of the reduced redex \nat all the places identi.ed by Redex creation Theorem 4.12 as possibly related to a redex creation: the \nlabels of the skeleton are modi.ed, and a new label is created at the root of the reduced redex. De.nition \n4.16 (Uniform relabelling). For any labelled n-ary con\u00adtext c and any label O,the uniform relabelling \n[O,c] of c by O is de.ned by the following equations: [O, D]= D [O,x]= x e[O,a] [O,ea(c)] = (c) .[O,a] \n[O,.a x.c]= x.[O,c] @[O,a] [O, @a(c1,c2)] = ([O,c1], [O,c2]) f[O,a] [O,fa(c1, ..., cn)] = ([O,c1], ..., \n[O,cn]) De.nition 4.17 (Labelled \u00df-reduction). Labelled \u00df-reduction is de.ned by the following rule scheme: \n@.(ea1 (...ean (.. x.s[Z1, ..., Zn])),Z) .\u00df [.a1...an ., eE(s)]{x := Z}[Z1, ..., Zn] where s is a {x}-skeleton. \nExample 4.18. In this example we work on a labelling of the ordinary .-term ((.x.xa)(.y.y))b. Consider \nthe labelled term t = @a(@\u00df(e. (.d x.@.(e.(x),e. (a))),.\u00b5 y.y),e.(b)) The term t contains a labelled \nredex \u00df.d.. . \u00b5 r = @(e(.x.@(e(x),e(a))),.y.y) with name O= \u00df. d. The skeleton of the .-abstraction of \nr is d .. .x.@(e(x), D) This skeleton is the only part whose labels are modi.ed by labelled \u00df-reduction. \nHence a [\u00df.d,E] [\u00df.d,.] [\u00df.d,.] \u00b5 . . t . @(e(@(e(.y.y),e(a))),e(b)) In the target of this reduction \nstep, a labelled redex [\u00df.d,.] [\u00df.d,.] \u00b5 . rc =@(e(.y.y),e(a)) with name Oc =[\u00df. d, .][\u00df. d, .]\u00b5 such \nthat O Y.dc Oc. Finally, we de.ne a notion of parallel labelled reduction that is meant to represent \nshared evaluation. De.nition 4.19 (Parallel reduction). Write .\u00af: t . t ' if there exists a name O and \na complete development ( : t _ t ' of all the labelled \u00df-redexes in t of name O.  4.5 Causality In this \nsection we show that the labels give a faithful account of causality in our system. Causal soundness \nLemma 4.26 and Causal completeness Lemma 4.28 show that the set N of the names that contribute to a given \nredex name O is exactly the set of the names of the reduction steps that are needed to create a redex \nof name O from an initial term. We formalize this fact using the following notion of needed names, which \nextends the notion of needed redexes. De.nition 4.20 (Needed name). A name O1 is said to be needed for \na name O2, written O1 Y.n O2, if every reduction sequence (.2 with src((.2) initial and name(.2)= O2 \nis such that ( contains a reduction step of name O1. This subsection is devoted to the proof that the \nsemantic notion of neededness Y.n is equivalent to the syntactic notion of contri\u00adbution Y.c (Causality \nTheorem 4.29). We begin with three lemmas that are not new, but that are useful to derive stronger causality \nproperties. Lemma 4.21 (Direct contribution). For any reduction step . : t . t ' of name O,if .c is \na reduction step from t ' of name Oc that is created by .,then O Y.dc Oc. Proof. By a case analysis on \nthe creation of .c, guided by Redex creation Theorem 4.12. Lemma 4.22 (Residuals). For any reduction \nstep . : t . t ' ,if .a is a reduction step from t of name O, then any descendant in t ' of root(.a) \nis the root of a redex with same name O. Proof. Similar to the proof given by Blanc, L \u00b4evy, and Maranget \n[BLM07]. The main point is that a non-frozen redex cannot be relabelled. Lemma 4.23 (Finite developments). \nLet t be a labelled .-term and R a set of reduction steps from t. Then the three following hold: All \nthe developments of R are .nite.  All the complete developments of R have the same target.  All the \ncomplete developments of R de.ne the same descen\u00addants and residuals.  Proof. Deduced from .nite developments \nfor orthogonal TRS once the results from Section 5 are established. Causal soundness is the inclusion \nof Y.c in Y.n. It is proved by tracking the origins of the labels and by checking that the labels do \nnot record spurious contributions. Lemma 4.24 (Reversed direct contribution). Let ( be a reduction sequence \nfrom an initial term. If the target of ( contains a label of the form [O,a], then the sequence ( contains \na reduction step of name O. Proof. By induction on the length of (, remarking that, at each step, any \nlabel [O,a] is created or descends from an identical label. Lemma 4.25 (Direct neededness). Let ( be \na reduction sequence from an initial term. Let . be a reduction step from t = tgt(() and name O.If O \n' is a name such that O ' Y.dc O,then ( contains a reduction step of name O ' . Proof. Write O= .1....n.Byde.nition \nthere exists i such that .i =[O ' ,a].Byde.nition of the name of a redex, there exists a position p . \npos(t) such that tp(t)=[O ' ,a] for some label a. Then by Lemma 4.24 the reduction sequence ( contains \na reduction step of name O ' . Lemma 4.26 (Causal soundness). Let O1 and O2 be two names such that O1 \nY.c O2.Then O1 Y.n O2. Proof. By induction on the length of a shortest sequence O1 Y.dc ... Y.dc O2. \n If O1 Y.dc O2, then by Direct neededness Lemma 4.25 any reduction sequence from an initial term to a \nterm containing of redex of name O2 contains a reduction step of name O1,which means that O1 Y.n O2. \n  If O1 Y.dc O3 Y.c O2 and O3 Y.n O2, then similarly O1 Y.n O3 and by transitivity of neededness O1 \nY.n O2.  Causal completeness is the inclusion of Y.n in Y.c. It is proved by checking that reduction \nsteps whose names are not comparable can be permuted. Lemma 4.27 (Permutation). If t0 .O0 t1 .O t2 and \nO0 Y.c O, then one of the following holds: t0 .O t2. There is a t1 ' such that t0 .O t1 ' .O0 t2. Proof. \nSince O0 Y.c O, by contraposition of Direct contribution Lemma 4.21 the redexes of name O in t1 are the \nresiduals of redexes in t0. In particular t0 .O0 t1 .O t2 is a complete development of the set of all \nredexes in t0 of name O0 or O.By Finite developments Lemma 4.23 the term t2 is the target of any complete \ndevelopment of these redexes, and in particular of the one .rst developing all the redexes of name O \nand then developing all the redexes of name O0 if they have not been erased by the .rst step. Lemma 4.28 \n(Causal completeness). Let O0 and O be two names such that O0 Y.n O.Then O0 Y.c O. \u00af Proof. Write R for \nthe set of all parallel labelled reduction se\u00adquences of minimal length from an initial term to a term \ncontaining \u00af redexes of name O.There is a n such that every sequence (\u00af. R [Acyclicity] In the .-calculus, \nif a redex r erases a redex re,then re is a strict subterm of r. The subterm relation being acyclic, \nthere is no possible cross-erasure. The axioms for DFS are then checked as follows: [Initial] Let .1 \nand .2 be two reduction steps from a common initial term t.If .1 and .2 are different, in particular \nthey apply to different positions and have different names. [Copy] Suppose ((1,.1) C ((2,.2), then in \nparticular .2 . .1/((2/(1). By Residuals Lemma 4.22 the reduction steps .2 and .1 have the same name, \nhence ((1,.1) and ((2,.2) are in the same family. [FFD] Orthogonal TRS satisfy the Finite Family Developments \nproperty [vO97]. Hence the weak .-calculus does the same by Bisimulation Proposition 5.10. [Creation] \nDirect application of Direct contribution Lemma 4.21. ... .O Q n \u00af . Write R * for the set of all sequences \nhas the form .O Q 1 [Contribution] This axiom can be reworded as Y.c =Y.n. Conclu\u00ad sion by Causality \nTheorem 4.29. \u00af Y.c O. (\u00af. R for which there is at least one i .{1, ..., n} such that fi O\u00af Suppose \nR * is not empty and let i be the greatest integer such Thus, by Glauert and Khasidashvili s Optimality \nProposition 4.8, that there is a sequence (\u00af. R\u00af* satisfying OifY.c O.Let (\u00afbe any complete needed weak \nfamily reduction sequence is optimal, such a sequence in R\u00af* satisfying OfY.c O. The sequence (\u00afhas \ni which we call weak optimality. Such concrete reduction strategies the form .O .O.O.O +1 ff By de.nition \nof (\u00afwe have OY.c O.Since OY.c O, i+1 i ff by transitivity of Y.c we get Oi Y.c Oi+1. Then by Permutation \n5. Weak reduction and .rst-order rewriting Lemma 4.27 we can build one of the following sequences: In \nthis section we show that the weak .-calculus behaves as an Qi Q Qi Qn . will be detailed in Section \n6. ... ... 1 orthogonal .rst-order rewriting system7. First in Section 5.1 we re\u00ad A sequence .Ostrictly \nshorter .O.O.O -1 +1 than (\u00af, which contradicts the minimality of the length of (\u00af. Q i Q i QQ n ... \n... 1 call the formalization of .-lifting as a higher-order rewriting system given in the previous paper \n[Bal12c], which can be used to prove that the weak .-calculus is strongly bisimilar to an orthogonal \n.rst\u00adwith Ofi A sequence .O .O.O.O Y.c O, +1 of same length as .\u00afbut which contradicts the maximality \nof i. Q i QQ i Q n ... ... 1 order term rewriting system; then in Section 5.2 we show that the transformation \nmoreover preserves concepts and properties such as \u00af Hence the set R * must be empty, and every name \nO0 that appears residuals or neededness. 5.1 Fully lazy .-lifting \u00af in a reduction sequence in R satis.es \nO0 Y.c O. Finally, let O0 be a name such that O0 Y.n O.By de.ni\u00ad tion 4.20, every reduction sequence \n( from an initial term to a term that contains redexes of name O contains a step of name O0.It is We \nformalize fully lazy .-lifting as a higher-order rewriting system that replaces a skeleton by a single \nsupercombinator symbol. The in particular the case of any reduction sequence developing a se\u00adlabelling \nof the transformation presented here is a slight simpli.ca\u00ad \u00af quence in R. Hence O0 Y.c O. tion of the \none of the previous paper [Bal12c], but it has roughly the same properties. De.nition 5.1 (Expansor). \nAn expansor is a bijection . : F. Finally, Causal soundness Lemma 4.26 and Causal complete\u00adness Lemma \n4.28 can be combined to prove that the syntactic con\u00ad tribution relation Y.c and the semantic neededness \nrelation Y.n are equal. ((.)) that preserves arity. For any f, g .F we say that g contains f and we \nwrite g . f if f appears in the expansion .(g).An Theorem 4.29 (Causality). Y.c = Y.n.  4.6 The labelled \nweak .-calculus as a DFS Using the labels of Section 4.4 we get a straightforward de.nition for families. \nDe.nition 4.30 (Families). The family relation over reduction steps with history is de.ned by the following \ncondition: ((1,.1) ((2,.2) if and only if the reduction steps .1 and .2 have the same name. Hence families \ncorrespond to names, and we extend the contribution relation Y.c to families. Notice that we do not use \nthe histories (1 and (2 in the previous de.nition. Indeed, thanks to the causal labelling, the labels \nin the target of a reduction sequence ( already contains all the relevant pieces of information about \n(. Lemma 4.31. The labelled weak .-calculus equipped with its family relation is a Deterministic Family \nStructure. Proof. We .rst check that the labelled weak .-calculus is a DRS: [FD] Direct application of \nthe Finite developments Lemma 4.23. expansor . is well-founded if there is no in.nite sequence f1 . \nf2 . ... . For the rest of the paper, let . be a .xed well-founded expansor. Such a bijection between \nF and ((.)) exists since for any n the set of n-ary contexts is countable. For any .-term t containing \nF-symbols, the expansor . de\u00adscribes a structure that is implicit in t, in some sense hidden in the F-symbols. \nThe notion of extended position in a .-term t gives a partial account of this structure by indicating \nordinary positions of t as well as internal positions of the F-symbols. De.nition 5.2 (Extended positions). \nThe set of the .-extended positions of a .-term, or simply .-positions of t, written pos.(t), is de.ned \nas follows: pos.(x)= {E}pos.(.x.t)= {E}. 1 \u00b7 pos.(t) pos.(t1t2)= {E}. 1 \u00b7 pos.(t1) . 2 \u00b7 pos.(t2) pos.(f(t1, \n..., tn)) = {E}. (0 \u00b7 pos.(.(f))) . ( i \u00b7 pos.(ti)) i 7 A rewriting system is orthogonal when its rewriting \nrules cannot interfere with each other. This set if .nite since . is well-founded. Example 5.3. Consider \nthe .-term t = f(@(z1,z2),z3) and suppose the following expansions (with f and g two binary symbols): \n.(f)= .x.Dg(D,x) .(g)= .y.yDD The following picture shows t and the expansions of the symbols f and g. \n. f .x z3 @ @ . z1 z2  g .y x @  @ y The set of positions of t is pos(t)= {E, 1, 11, 12, 2}.The \nsetof .-positions of t is: pos.(t) = {E, 1, 11, 12, 2, 0, 01, 012, 0122, 0120, 01201, 012011, 0120111} \n The following table gives a correspondence between some .\u00adpositions of t and symbols from t, .(f),or \n.(g). E f 0 .x 012 g 0120 .y With the next two de.nitions we propose an initial labelling that takes \ninto account the expansions of the term. Intuitively, each node is labelled by the position it would \nhave if the term were fully expansed. De.nition 5.4 (Expansed positions). Let c be a n-ary context and \np . pos(c) a position of c.The expansion of p relative to c is the position (p)c,. de.ned by: (E)c,. \n= E (1 \u00b7 p)e(c),. =1 \u00b7 (p)c,. (1 \u00b7 p).x.c,. =1 \u00b7 (p)c,. (i \u00b7 p)c1c2,. i \u00b7 (p)ci,. = i .{1, 2} (i \u00b7 p)f(c1,. \n. . ,cn),. =(qi).(f),. \u00b7 (p)ci,. i .{1, ..., n}qi position of ith hole in .(f) De.nition 5.5 (Extended \ninitial labelling). For any position p, the function ip(.) maps .-terms to labelled .-terms through the \nfollowing rules: ip(x)= x ip(e(t)) = (ip\u00b71(t)) ep .p ip(.x.t)= x.ip\u00b71(t) ip(t1t2)=@p(ip\u00b71(t1),ip\u00b72(t2)) \nip(f(t1, ..., tn)) = (ip\u00b7q1 (t1), ..., ip\u00b7qn (tn)) fp .j. qj =(j)f(t1,...,tn),. Let t be a .-term. The \ninitial labelling of t is the labelled .-term iE(t), also written i(t). Example 5.6. Consider the .-term \nt = f(@(z1,z2),z3) from example 5.3. Then i(t) = fE(@11(z1,z2),z3) With this extended initial labelling, \nwe can now de.ne the target system, that is the .rst-order rewriting system to which the weak .\u00adcalculus \nis bisimilar. De.nition 5.7 (Target redex). A target redex is a term over SL of the form @.(ea1 (...ean \n(f. (t1, ..., tn))),u) Its name is the sequence name(r)= .a1...an . De.nition 5.8 (Source &#38; Target \nreductions). The source reduc\u00adtion is the labelled \u00df-reduction of de.nition 4.17. The target re\u00adduction \nis given by the following rule scheme: @.(ea1 (...ean (f. (Z1, ..., Zn))),Z) .f [.a1...an., [Ok, ... \n[O1 ,eE(ip(s))]] ]{x := Z}[Z1, ..., Zn]  where . =[Ok, ... [O1 ,p]] and .(f)= .x.s. De.nition 5.9 (Lambda-lifting). \nThe .-lifting rewriting system is de.ned by the following rules applied in any context (no restriction \nto weak reduction here). For any label a, variable x, and {x}-skeleton s, the pair .a x.s[Z1, ..., Zn] \n.c (.-1(.x.s ))a(Z1, ..., Zn) is a .\u00adlifting rule of rank 0.  A pair f(Z1, ..., Zn) . f ' (Z1, ..., \nZn) is called a .-lifting rule of rank n +1 if there is a reduction step .(f) . .(f ' ) by a .-lifting \nrule of rank n.  Proposition 5.10 (Bisimulation, Balabonski [Bal12c]). The .\u00adlifting relation _lft \nis a strong bisimulation in the global system. It is also known that the .-lifting reduction relation \nis conver\u00adgent, and compatible with sharing. In the next section we add new results that ensure that \n_lft preserves the properties that are useful to the study of optimality. 5.2 Properties preserved by \nthe reduction to .rst order For any term t, write lift(t) for the normal form of t by .-lifting. Similarly, \nfor any position p of a term t, write lift(p) for the set of its descendants along any complete lifting \nof t, and for any source reduction step . from t, write lift(.) for the residual of . after any complete \nlifting of t. In this section we show that lift(.) preserves descendants, residuals, and neededness. \nLemma 5.11 (Labels and descendants). Let t be a labelled term whose labels are initial and different, \nand t _ t ' be a reduction sequence. Then a position p ' . pos(t ' ) is a descendant of a position p \n. pos(t) if and only if there are names O1, ..., On such that tt1 (p ' )=[On, ... [O1 ,tt(p)]]. Proof. \nBy induction on the length of the sequence, with the follow\u00ading generalized statement: For any labelled \nterm t, reduction sequence ( : t . t ' ,and position p0, write Pa = {p . pos(t) |.k, .Ok... O1 ,tp(t)=[Ok, \n... [O1 ,p0]]} '' ' Pa = {p . pos(t ) |.k ' , .Ok1 ... O1 ,tp(t)=[Ok1 , ... [O1 ,p0]]} for the sets \nof the positions of t and t ' where p0 appears. Then P ' = Pa/( a  With this characterization of descendants, \nwe can use con.u\u00adence results on a labelled system to get results about descendants and residuals. Lemma \n5.12 (Preservation of descendants). Let t be a .-term, p . pos(t) a position, and . : t . t ' a source \nreduction step. Then lift(p/.)= lift(p)/lift(.). Proof. By Bisimulation Proposition 5.10 we have the \nfollowing diagram: t (lft lift (t) lift(.) ' ( ' lift(t ' ) t By de.nition of descendants along a reduction \nsequence we have lft) as well as p/((lftlift(.)) = (p/(lft)/lift(.)= lift(p)/lift(.). Since the sequences \n.( ' lift(p/.)= (p/.)/( ' lft = p/(.( ' lft and (lftlift(.) have same target, by Labels and descendants \nLemma 5.11 we also have p/(.( ' lft)= p/((lftlift(.)). By combin\u00ading these equations we get lift(p/.)= \nlift(p)/lift(.). The preservation of residuals is an immediate corollary of the preservation of descendants. \nLemma 5.13 (Preservation of residuals). Let . : t . t ' , .a : t . u and .r : t ' . u ' be three source \nreduction steps. Then .r is a residual of .a after . if and only if lift(.r) is a residual of lift(.a) \nafter lift(.). Finally, we deduce that .-lifting also preserves the notion of neededness, which will \nallow us in the next section to transfer a result on needed strategies from .rst-order rewriting to the \nweak .-calculus. Theorem 5.14 (Preservation of neededness). Let t be a normaliz\u00ading term. Then a source \nreduction step . from t is needed if and only if the target reduction step lift(.) is needed in the target \nsystem. Proof. Let . be a needed source reduction step from t.Let (f = .f 1 ....n f be a normalizing \ntarget reduction sequence from lift(t). By Bisimulation Proposition 5.10 we have a normalizing source \nreduction sequence ( = .1....n from t. By hypothesis . is needed, hence there exists i such that .i is \na residual of .. By Preservation of residuals Lemma 5.13, the reduction step .i f is a residual of lift(.). \nHence lift(.) is needed. We show similarly that if lift(.) is needed, then . is also needed. The properties \nstated in this section show that the weak .\u00adcalculus, while it has the appearance of the .-calculus, \nhas the properties of a .rst-order rewriting system. This .rst-order essence of weak reduction will become \neven more apparent in the next section. 6. Two weakly optimal strategies We conclude our investigation \nby identifying two strategies that re\u00adalize the weak optimality characterized in Section 4. The .rst \nstrat\u00adegy (Section 6.1) is call-by-need: Wadsworth s well-known tech\u00adnique for ef.cient evaluation of \nthe .-calculus [Wad71], which is also the source of mainstream lazy evaluation mechanisms. The sec\u00adond \nstrategy (Section 6.2) is a bit more intriguing, since it reaches optimality without using any sharing \n(but of course, following Un\u00adcomputability Theorem 3.14 the latter cannot be computable). 6.1 Call-by-need \nAs shown in the previous paper [Bal12c], the labelled weak .\u00adcalculus de.ned in Section 4 describes a \ngraph-based evaluation mechanism for the .-calculus, which is exactly the one proposed by Wadsworth [Wad71]. \nThis is done through the technique of sharing\u00advia-labelling [Mar91, Bal12a], that has .rst been introduced \nby Maranget. The main idea is as follows: if two subterms td have the same label d, then they represent \nthe same node in the corresponding graph (this node is marked with a star in the picture below), and \nhave to be reduced in only one step. . d) @ @ @a(@\u00df(t,td),t . . '. '. @ @ . @a(@\u00df(t,t ),t ) ' t t * \nt t This principle allows us to interpret labelled terms as graphs, pro\u00advided a consistency property \nof the labels is satis.ed: two subterms that have the same label must be equal. Finally, the following \nproposition ensures that parallel reduction preserves the consistency of labels, and thus describes a \ngraph rewriting system. Moreover, this graph rewriting system is known to correspond to Wadsworth s original \ntechnique [Bal12c]. Proposition 6.1 (Sharing-via-labelling, Balabonski [Bal12c]). The following property \nis preserved by parallel reduction: Sharing If tp1 (t)= tp2 (t) then t|p1 = t|p2 . Hence Wadsworth s \ncall-by-need implements complete family reduction. Moreover, call-by-need is known to reduce only needed \nredexes [BKKS87], and thus Glauert and Khasidashvili s Optimal\u00adity Proposition 4.8 applies. Theorem 6.2 \n(Optimality). Wadsworth s call-by-need is a weakly optimal reduction strategy. Besides marking a well-known \nreduction strategy as weakly optimal, this optimality theorem reveals a difference between weak and non-weak \nreduction: while any implementation of non-weak optimality requires a sharing of contexts (see example \n4.2), the sharing of subterms expressible by sharing-via-labelling is enough for weak optimality. 6.2 \nInnermost needed reduction Now we are going to combine the results of the previous sections to prove \nthat the shortest weak reduction sequences without sharing have the same length as the complete needed \nfamily reduction sequences. First we transfer a suf.cient condition for a reduction strategy to be optimal \nfrom the .rst-order rewriting literature to our system, then we check that this condition describes a \nsubset of complete needed family reduction. Proposition 6.3 describes a suf.cient condition for a reduction \nstrategy to be optimal in orthogonal term rewriting systems, that has been established by Khasidashvili. \nUsing our formalization of .-lifting, we transfer this result to the weak .-calculus. Proposition 6.3 \n(Khasidashvili s optimality criterion [Kha93]). Let t be a term in an orthogonal TRS, and ( a normalizing \nreduc\u00adtion sequence of source t in which each step reduces an innermost needed redex. Then no normalizing \nreduction sequence of source t is shorter than (. Theorem 6.4 (Optimality criterion). Let t be a .-term, \nand ( a normalizing weak reduction sequence of source t in which each step reduces an innermost needed \nredex. Then no normalizing weak reduction sequence of source t is shorter than (. Proof. Suppose there \nis a normalizing weak reduction sequence ( ' of source t that is shorter than (.Let (f (resp. ( ' f ) \nbetheimageof ( (resp. ( ' ) in the target system. By Bisimulation Proposition 5.10, (f and (f ' have \nthe same source, are both normalizing, and (f ' is shorter than (f . Moreover, by Preservation of neededness \nTheo\u00adrem 5.14, each reduction step in the sequence (f reduces an in\u00adnermost needed redex. Hence by Khasidashvili \ns Optimality Propo\u00adsition 6.3 the reduction sequence (f is of minimal length, which contradicts the existence \nof the sequences ( ' f and ( ' . This criterion shows once again that weak reduction is closer to .rst-order \nrewriting than to the usual non-weak .-calculus. Indeed, Guerrini proved with a counter-example that \ninnermost needed reduction is not optimal in the non-weak .-calculus [Gue96]. Lemma 6.5. An innermost \nneeded reduction step does not dupli\u00adcate any needed redex. Proof. Deduced from Khasidashvili s results \n[Kha93] using .\u00adlifting. Thus an innermost needed reduction sequence performs only needed reduction steps \non redexes that have not been duplicated, and such a sequence is an instance of complete needed family \nreduction. Theorem 6.6 (Optimality). Innermost needed reduction is a weakly optimal reduction strategy. \nThis optimality theorem gives away another important differ\u00adence between weak and non-weak reduction. \nIndeed, Lamping ex\u00adhibited an example for which non-weak L \u00b4evy-optimality cannot be realized without \nsharing [Lam90]. 7. Conclusion We characterized optimal evaluation in the weak .-calculus and identi.ed \ntwo interesting weakly optimal strategies: the well\u00adknown call-by-need strategy, and innermost needed \nreduction. The latter illustrates the fact that the weak .-calculus has the properties of a .rst-order \nrewriting system, and also shows that weakly opti\u00admal evaluation does not require sharing. However, we \nalso showed that the optimal strategies without sharing cannot be computable. Finally, in the setting \nof weak reduction the meaning of sharing is not to make the shortest path shorter, but rather to make \nit effec\u00adtively reachable. Acknowledgments Thanks to Delia Kesner, Olivier Danvy and Vincent van Oostrom, \nwho offered me the tools to explore this topic. Thanks also to Franc\u00b8ois Pottier, Mike Rainey, and the \nICFP reviewers for their helpful comments and suggestions. References [Bal12a] Thibaut Balabonski. Axiomatic \nsharing-via-labelling. In Ashish Tiwari, editor, RTA, volume 15 of LIPIcs, pages 85 100. Schloss Dagstuhl \n-Leibniz-Zentrum fuer Informatik, 2012. [Bal12b] Thibaut Balabonski. La pleine paresse, une certaine \nopti\u00admalit \u00b4e. Ph.D. thesis, Universit \u00b4e Paris 7 Diderot, 2012. [Bal12c] Thibaut Balabonski. A uni.ed \napproach to fully lazy sharing. In John Field and Michael Hicks, editors, POPL, pages 469 480. ACM, 2012. \n[Bar84] Hendrik Pieter Barendregt. The Lambda Calculus Its Syntax and Semantics, volume 103 of Studies \nin Logic and the Foun\u00ad dations of Mathematics. North-Holland Publishing Company, Amsterdam, revised edition \nedition, 1984. [BBKV76] Hendrik Pieter Barendregt, Jan A. Bergstra, Jan Willem Klop, and Henri Volken. \nSome notes on lambda reduction. Technical Report 22, University of Utrecht, Dpt. of mathematics, 1976. \n[BKKS87] Hendrik Pieter Barendregt, Richard Kennaway, Jan Willem Klop, and M. Ronan Sleep. Needed reduction \nand spine strate\u00adgies for the lambda calculus. Information and Computation, 75(3):191 231, 1987. [BLM07] \nTomasz Blanc, Jean-Jacques L \u00b4evy, and Luc Maranget. Sharing in the weak lambda-calculus revisited. In \nErik Barendsen, Herman Geuvers, Venanzio Capretta, and Milad Niqui, editors, Re.ections on Type Theory, \nLambda Calculus, and the Mind, pages 41 50. ICIS, Faculty of Science, Radbout University Nijmegen, 2007. \nEssays Dedicated to Henk Barendregt on the Occasion of his 60th Birthday. [cH98] Naim C\u00b8 agman and J. \nRoger Hindley. Combinatory weak reduction in lambda calculus. Theoretical Computer Science, 198(1-2):239 \n247, 1998. [DS00] Olivier Danvy and Ulrik Pagh Schultz. Lambda-dropping: transforming recursive equations \ninto programs with block structure. Theoretical Computer Science, 248(1-2):243 287, 2000. [GK96] John \nR. W. Glauert and Zurab Khasidashvili. Relative normal\u00adization in deterministic residual structures. \nIn H \u00b4el `ene Kirchner, editor, CAAP, volume 1059 of Lecture Notes in Computer Sci\u00adence, pages 180 195. \nSpringer, 1996. [Gue96] Stefano Guerrini. Theoretical and Practical Issues of Optimal Implementations \nof Functional Languages. Ph.D. thesis, Uni\u00adversit `a di Pisa, 1996. [How70] William Alvin Howard. Assignment \nof ordinals to terms for primitive recursive functionals of .nite type. In Intuitionism and Proof Theory, \npages 443 458, 1970. [Kha93] Zurab Khasidashvili. Optimal normalization in orthogonal term rewriting \nsystems. In Claude Kirchner, editor, RTA,vol\u00adume 690 of Lecture Notes in Computer Science, pages 243 \n258. Springer, 1993. [KvOvR93] Jan Willem Klop, Vincent van Oostrom, and Femke van Raamsdonk. Combinatory \nreduction systems: Introduction and survey. Theoretical Computer Science, 121(1&#38;2):279 308, 1993. \n[Lam90] John Lamping. An algorithm for optimal lambda calculus reduction. In Frances E. Allen, editor, \nPOPL, pages 16 30. ACM Press, 1990. [L \u00b4e78] Jean-Jacques L \u00b4evy. R\u00b4eductions correctes et optimales \ndans le lambda-calcul. Ph.D. thesis, Universit \u00b4e Paris VII, 1978. [L \u00b4e80] Jean-Jacques L \u00b4evy. Optimal \nreductions in the lambda-calculus. In To H.B. Curry: Essays on Combinatory Logic, Lambda Calculus and \nFormalisms, pages 159 191, 1980. [Mar91] Luc Maranget. Optimal derivations in weak lambda-calculi and \nin orthogonal terms rewriting systems. In David S. Wise, editor, POPL, pages 255 269. ACM Press, 1991. \n[PJ87] Simon L. Peyton-Jones. The Implementation of Functional Programming Languages. Prentice-Hall, \n1987. [Sco68] Dana Scott. A system of functional abstraction, 1968. Lectures delivered at University \nof California, Berkeley, Cal., 1962/63. [Ter03] Terese. Term Rewriting Seminar Terese.Vol. 55 of Cam\u00adbridge \nTracts in Theoretical Computer Science. Cambridge University Press, 2003. [vO97] Vincent van Oostrom. \nFinite family developments. In Hubert Comon, editor, RTA, volume 1232 of Lecture Notes in Com\u00adputer Science, \npages 308 322. Springer, 1997. [Wad71] Christopher P. Wadsworth. Semantics and Pragmatics of the Lambda \nCalculus. Ph.D. thesis, Oxford, 1971.    \n\t\t\t", "proc_id": "2500365", "abstract": "<p>In this paper we investigate laziness and optimal evaluation strategies for functional programming languages. We consider the weak lambda-calculus as a basis of functional programming languages, and we adapt to this setting the concepts of optimal reductions that were defined for the full lambda-calculus. We prove that the usual implementation of call-by-need using sharing is optimal, that is, normalizing any lambda-term with call-by-need requires exactly the same number of reduction steps as the shortest reduction sequence in the weak lambda-calculus without sharing. Furthermore, we prove that optimal reduction sequences without sharing are not computable. Hence sharing is the only computable means to reach weak optimality.</p>", "authors": [{"name": "Thibaut Balabonski", "author_profile_id": "81466647879", "affiliation": "Inria, Le Chesnay, France", "person_id": "P4261253", "email_address": "thibaut.balabonski@inria.fr", "orcid_id": ""}], "doi_number": "10.1145/2500365.2500606", "year": "2013", "article_id": "2500606", "conference": "ICFP", "title": "Weak optimality, and the meaning of sharing", "url": "http://dl.acm.org/citation.cfm?id=2500606"}