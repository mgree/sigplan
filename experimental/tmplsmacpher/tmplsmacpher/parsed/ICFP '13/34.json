{"article_publication_date": "09-25-2013", "fulltext": "\n Hoare-Style Reasoning with (Algebraic) Continuations Germ \u00b4an Andr \u00b4es Delbianco Aleksandar Nanevski \nIMDEA Software Institute IMDEA Software Institute Universidad Polit \u00b4ecnica de Madrid aleks.nanevski@imdea.org \ngerman.delbianco@imdea.org Abstract Continuations are programming abstractions that allow for manip\u00adulating \nthe future of a computation. Amongst their many ap\u00adplications, they enable implementing unstructured \nprogram .ow through higher-order control operators such as callcc. In this pa\u00adper we develop a Hoare-style \nlogic for the veri.cation of programs with higher-order control, in the presence of dynamic state. This \nis done by designing a dependent type theory with .rst class callcc and abort operators, where pre-and \npostconditions of programs are tracked through types. Our operators are algebraic in the sense of Plotkin \nand Power, and Jaskelioff, to reduce the annotation burden and enable veri.cation by symbolic evaluation. \nWe illustrate work\u00ading with the logic by verifying a number of characteristic examples. Categories and \nSubject Descriptors D.3.3 [Language Constructs and Features]: Control structures; F.3.1 [Logics and Meanings \nof Programs]: Specifying and Verifying and Reasoning about Programs Logic of programs, Pre-and post-conditions; \nF.3.3 [Studies of Program Constructs]: [Control Primitives] General Terms Languages, Veri.cation Keywords \nContinuations, Hoare Logic, Dependent Types, callcc 1. Introduction Continuations are powerful abstractions \nthat model the future of a computation [36]. They have a ubiquitous presence in program\u00adming languages: \nthey allow for a family of program transformation techniques in the style of many CPS transformations \n[11], they un\u00adderlie the denotational semantics of programs with jumps [16, 42], they give computational \ncontent to classical proofs [20], they have been used to structure computational effects [17, 21] and \nalso to de\u00adsign compilation techniques. Moreover, certain programming lan\u00adguages provide .rst-class control \noperators which manipulate con\u00adtinuations, e.g. the variants of callcc in Scheme, ML and Haskell, or \nthe related C and F control operators [15, 14]. The ability to manipulate the future makes these operators \nmore powerful than plain goto-like instructions, but it also hinders the formal reasoning about programs. \nAlthough the state of the art concerning formal reasoning about continuations is vast, it has focused \npredominantly (with notable exceptions discussed below) Permission to make digital or hard copies of \nall or part of this work for personal or classroom use is granted without fee provided that copies are \nnot made or distributed for pro.t or commercial advantage and that copies bear this notice and the full \ncitation on the .rst page. Copyrights for components of this work owned by others than the author(s) \nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers \nor to redistribute to lists, requires prior speci.c permission and/or a fee. Request permissions from \npermissions@acm.org. ICFP 13, September 25 27, 2013, Boston, MA, USA. Copyright is held by the owner/author(s). \nPublication rights licensed to ACM. ACM 978-1-4503-2326-0/13/09. . . $15.00. http://dx.doi.org/10.1145/2500365.2500593 \n on the semantic modelling of higher-order control operators and CPS transformations [15, 20, 11, 47], \nand veri.cation of programs directly in the semantic model [41, 13]. In contrast, in this paper we are \ninterested in developing a Hoare-style logic in which one can systematically specify and verify full \nfunctional correctness of programs with higher-order jumps, in the presence of dynamic mutable state \nand the ability to capture continuations and return them as results of computations, potentially encapsulated \ninto closures. The ability to capture and return continuations makes our task more dif.cult when compared \nto the previous work on Hoare logics for .rst-order jumps (i.e. goto s) in high-level languages [9, 25, \n2] and low-level machine code [38, 46, 23]. In particular, the higher order nature of callcc entails \nthe need for a Hoare logic capable of reasoning about (potentially higher-order) functions. It also makes \nit somewhat more dif.cult to design the speci.cation methodology, i.e. decide on just what kind of information \nshould the proof devel\u00adoper provide in the form of annotations when specifying a program involving callcc, \nand how should that information relate the con\u00adtext in which the continuation is captured to the context \nin which it is invoked. The presence of dynamic state signi.cantly complicates mat\u00adters, and differentiates \nour work from recent Hoare-style logics for higher-order jumps [5, 10]. From the semantic point of view, \nsup\u00adporting dynamic state requires building a model in which executing a continuation does not roll back \nthe mutable state to the point at which the continuation is captured. From the speci.cation point of \nview, it requires reconciling callcc with Separation logic [31, 37]. In this paper, we accomplish the \ntask in a novel manner, combining Separation assertion logic with large footprint semantics and large \nfootprint inference rules for veri.cation in the style of symbolic evaluation. More concretely, our contribution \nin this paper is the devel\u00adopment of HTTcc , a framework for Hoare-style reasoning with higher-order \nprograms, control effects and mutable, dynamic state. To the best of our knowledge, this is the .rst \nformal system for rea\u00adsoning about such combination of features. We de.ne a dependent type-theory with \n.rst class continuations which uses monadic types indexed by pre-and postconditions as our speci.cations \nin the style of Separation logic. This has been done before for a language with state, e.g. in Hoare \nType Theory (HTT) [29, 30], but now we ex\u00adtend the approach to a language with continuations. In particular, \nwe rely on dependent records (i.e. iterated S-types) as an essential tool for speci.cation of programs \nwhich capture the continuation in a closure that is later invoked. In order to make speci.cation and \nveri.cation in HTTcc more palatable, we focus on a speci.c choice of control operators which are algebraic \nin the sense of Plotkin and Power [33, 34] and Jaske\u00adlioff [22]; that is, the control operators commute \nwith sequential composition. We argue that the algebraic control operators are less burdensome for veri.cation \nthan the non-algebraic alternatives, be\u00ad  1. {x . v} 2. c . callcc f. ret (abort f (x := !x + 1 ; ret \n(ret ()))); 3. {x . v A { x . v + 1 } c { .} Y x . v + 2 A { x . v + 3 } c { x . v + 3}} 4. x := !x \n+ 1 ; 5. {x . v + 1 A { x . v + 1 } c { .} Y x . v + 3 A { x . v + 3 } c { x . v + 3}} 6. c 7. {x \n. v + 3}  Figure 1. An idealised proof outline for inc3. cause in practice they require less annotations \nto be manually pro\u00advided by the user. In particular, the algebraic callcc typically re\u00adquires manual \ndescription of only the jumping behaviour of the code, whereas, in our experience, the non-algebraic \nvariants require manual description of both the jumping and the normal termination of the callcc block. \nTo test our design in practice, we have implemented HTTcc as a shallow embedding in the Calculus of Inductive \nConstructions (CIC), as realised in Coq [27, 6] and Ssre.ect [19]. We have mechanised its denotational \nsemantics and soundness proofs and used the framework to verify a survey of standard examples which cover \nthe usual continuation idioms. A subset of the examples is presented in Section 6. All of our Coq code \nis available on-line [12]. 2. Overview The language of HTTcc uses monads, as in Haskell, to separate \nthe purely functional and the imperative fragment of the language. In addition to capturing continuations \nvia callcc and jumping to them via abort, the imperative fragment supports recursion (which we consider \na side effect), and heap-mutating commands such as allocation, deallocation, reading from and writing \ninto heap locations. We use ret v for a monadic command returning a value v, and x . e1;e2 for monadic \nbind (i.e. a sequential composition), which .rst executes e1, then binds the return result to x before \nexecuting e2. We abbreviate with e1;e2 when x ./FV (e2). As customary with monadic languages, we assume \nthat a command is evaluated only upon being bound in a sequential composition. Until then, the command \ns execution is suspended. 2.1 Algebraicity As our callcc and abort are non-standard, we brie.y illustrate \nthem by example. Figure 1 shows an informal proof outline in a style of partial correctness Hoare logic \nor Separation logic for a function inc3, which uses a backward jump to increment the value of pointer \nx by 3. Our actual syntax for inc3, and the manual assertions needed for its speci.cation, will be introduced \nin Section 2.3. We .rst discuss the behaviour of the function, and then return to the proof outline. \nIn the .rst command (line 2), inc3 captures the continuation, which, at that point, is . c. x := !x + \n1; c, corresponding to the code in lines 4 - 6, with the variable c abstracted. The con\u00adtinuation is \nencapsulated inside a continuation object f, which may be viewed as a label for jumps. Next, the body \nof callcc is executed, and the suspended command abort f (x := !x + 1; ret (ret ())) is bound to c. 1 \nThe program continues by incre\u00admenting x (line 4), after which c follows (line 6). Execution of c causes \na jump to the continuation encapsulated inside f. However, before the control is passed to the continuation, \nthe second argu\u00adment of abort x := ! x+1; ret (ret ()) is executed. Thus, x is incremented again, and \nret () is passed as the argument c to the continuation encapsulated inside f. Passing the control to \nthe continuation corresponds to a backward jump to line 4. Thus, x is incremented once again, followed \nby execution of c. Since the lat\u00adter variable is now bound to ret (), its execution falls through and \ninc3 returns (), after having incremented x three times. The non-standard aspect of our control operators \nis that abort allows executing arbitrary side-effectful command in the above case x := ! x + 1 as part \nof performing the jump. This is differ\u00adent from the customary callcc and throw [28, 48, 49], as the lat\u00adter \nonly passes a value upon a jump. We refer to this side-effectful command as .nalisation code, as it is \nexecuted immediately be\u00adfore the jump, thus ending the normal control .ow. Obviously, throw can be mimicked \nby abort by using a trivial .nalisation code which immediately returns. Dually, abort f e can be imple\u00admented \nby sequential composition which executes e, followed by throwing the obtained value to f. However, choosing \nabort as a primitive, awards special sta\u00adtus to the .nalisation code, which makes the control operators \nal\u00adgebraic [22, 33, 34]. More concretely, callcc commutes with se\u00adquential composition, a property we \nuse in Section 4 to formulate a methodology for Hoare-style speci.cation and veri.cation by sym\u00adbolic \nevaluation. We illustrate how the commutation arises by the following equations, which can be derived \nfrom Jaskelioff [22]. x . (callcc f. e1);e2 = callcc g.x .[(g.x. e2)/f]e1; e2 (1) abort (g . x. e2) e1 \n= abort g (x . e1; e2) (2) where g . x. e2 . . t. g (x . t;e2) Consider equation (1). The continuation \ncaptured inside f on the left side of the equation includes e2. On the right side, callcc has been commuted \nout of the scope of x, and thus e2 cannot be part of the continuation captured inside g. To induce that \nthe expressions on the two sides of equality behave the same, jumps to g on the right side must be preceded \nby an execution of e2. Because our primitives provide for .nalisation code, we could enforce such a discipline \nby using e2 as .nalisation code for every abort to g. This is achieved by uniformly substituting f in \ne1 with a new continuation object g . x.e2. The new object is engineered so that aborting to it object \nbehaves like aborting to g with the .nalisation code extended by e2, as captured in equation (2). Thus, \nexecution of e2 precedes the jumps to g, just as required.  2.2 Proof outline As customary in Separation \nLogic, heaps are .nite maps from the type ptr of pointers (isomorphic to N) to values. The predicate \nx . v holds only of a singleton heap with a pointer x, storing the value v. We use A , Y , for point-wise \nconjunction and disjunction of heap predicates, and . and . for the always true and always false predicate, \nrespectively. In subsequent examples, we will also use separating conjunction P * Q, which holds of a \nheap h if h can be split into disjoint parts satisfying predicates P and Q, respectively. We will also \nuse the predicate this h, which holds only of heaps equal to h. We write P h or alternatively, h . P, \nwhen the predicate P holds of the heap h. We retain ., ., True and False for the customary propositional \n(i.e., non-separation) connectives. Referring to Figure 1, the line 1 states that the program starts \nwith the initial heap containing a pointer x storing the integer v (though we omit the type annotations). \nHTTcc uses large footprint 1As usual with monads, the binding strips the outer ret.  annotations which \ndescribe the full heap in which the program runs, rather than just a subheap that the program needs (in \ncontrast to the small footprint annotations from Separation logic). Thereby, the proof outline in Figure \n1 describes the behaviour of inc3 when the heap contains exactly the pointer x storing v, but no other \npointers. For now, we restrict ourselves to this simple case in order to focus on algebraicity, but we \nexplain in Section 3 how to generalise the annotations of inc3 to cover larger heaps. Going back to Figure \n1, at line 3, after the continuation is cap\u00adtured in line 2, the current heap is unchanged, but the program \nvari\u00adable c is bound to the command abort f (ret (ret ( ))), which itself has to be speci.ed. The Hoare \ntriple {x . v + 1} c {.} indicates that c should be executed only after x is incremented (precondition \nx . v + 1), and that the execution of c causes a jump (postcondition .). This behaviour precisely corresponds \nto the intended use for c in line 6. However, as c in line 6 executes a backward jump, the assertion \nin line 3 has to describe the state right after the jump, but before the program proceeds with exe\u00adcuting \nline 4 for the second time. This is the role of the second disjunct in line 2. It shows that the program \npoint is reached for the second time with x incremented twice. As described in Sec\u00adtion 2.1, at that \npoint c is bound to ret ( ), and can be speci.ed by {x . v+ 3} c {x . v+ 3} to indicate that this second \ninstance of c will be executed after x is incremented once more (precondition x . v + 3). The second \nexecution of c does not jump, but falls through with the heap unchanged (postcondition x . v + 3). After \nx is incremented in line 4, the assertion in line 5 accounts for the change in the heap: compared to \nline 3, the value of x is incremented in both disjuncts, while the speci.cations for c remain unchanged. \nNow, command c is safe to execute in line 6, because the heap in both disjuncts satis.es the respective \npreconditions for c. The program terminates satisfying x . v + 3 (line 7), which is a disjunction of \nthe postcondition of c from line 5.  2.3 Proof annotations as dependent types The crucial point in the \nproof outline for inc3 is deciding on the speci.cations for c in line 3, which indicate the intended \nuse of c in the rest of the program (i.e., c is executed when x stores v + 1 and v + 3, jumping in the \n.rst case, and falling through in the second). Such speci.cations depend on the structure of the rest \nof the program, and cannot be gleaned solely from the body of callcc in line 2. In this paper, we adopt \nthe approach that such information is provided by the programmer in the form of annotations. This is \nsimilar to the way loop invariants often have to be provided when verifying structured programs. In HTTcc \n, we use type annotations for this purpose. However, because the annotations clearly depend on run-time \nvalues (e.g., the contents of the pointer x in Figure 1), we have to use dependent types. In particular, \nHTTcc features two type constructors which we use to provide annotations for side-effectful programs \nand for con\u00adtinuation objects. Intuitively, the type SK * {P} A{Q} classi.es programs with precondition \nP, postcondition Q and return value of type A. The type Kont * {R}{P} A{Q} classi.es continuation objects. \nR describes what holds when the continuation is captured by callcc, and is refered to as the initial \ncondition. Precondition P describes what must hold of the heap when aborting to the continuation object; \nthus immediately before the .nalisation code is executed. postcondition Qdescribes the heap and the value \nob\u00adtained after the execution of the .nalisation code, but before the actual jump. In both types, the \nassertions R, P and Qmay depend on program values. Additionally, Qmay depend on the dedicated variable \nr:A, naming the return value. We employ the notation [v1 :A1 ,. . . , vn :An ]. SK * {P} A{Q}, often \nomitting the types Ai, to specify that v1,. . . , vn are variables that may scope through P and Q (and \nsimilarly for R, P and Q inc3 (x:ptr): [v]. SK * {x . v} ( ) {x . v + 3} do c . callccj f : [v]. Kont \n* {j. x . v} {x . v + 1} S SK ( ) {r. x . v + 2 A spec r . (x . v + 3, x . v + 3)*}. do (ret [abort f \n(x := !x + 1 ; ret [ret ( )])]) : [v]. SK * {x . v A j. x . v} S SK ( ) {r. x . v A spec r . (x . v + \n1,.)*}; x := !x + 1 ; cmd c. Figure 2. Speci.cation of inc3 via type annotations. in Kont * types). In \nHoare logic terminology, such variables are known as logical; they may appear in assertions, but not \nin the code. In .rst-order Hoare logics, logical variables have global scope and are used to relate the \ninitial and ending states of a computation. In our setting, the scope of logical variables is local to \nthe type in which they are bound. This is required in any Hoare logic for a language with procedures \nand recursion [24] such as HTTcc , where a logical variable used in the speci.cation of a recursive procedure, \nmay have to be instantiated differently to satisfy the preconditions of different recursive calls. Additionally, \nwe employ a dependent record type S SK A, which packages a precondition and a postcondition together \nwith a computation, and thus abstracts existentially over them. In other words, a value of type S SK \nA is a structure of the form [P, Q, e], where e : SK * {P} A{Q}. As we show subsequently, values of this \ntype will be used whenever we nest the monadic types, i.e. build computations that return other computations, \nwhich may potentially capture continuations. Given c : S SK A, we will use spec c and cmd c to project \nthe components when necessary: spec : [P, Q, e]. (P, Q) cmd : [P, Q, e]. e We will abuse the notation \nand write [e]instead of [(P, Q), e], as the Hoare triple type of e and hence its P and Qcomponents can \nusually be inferred, as we describe in Section 4. We will also use the symbol . to denote the usual Hoare \nordering (i.e., pre\u00adstrengthening/post-weakening) on pairs (P, Q). We now present in Figure 2 the fully \nannotated version of inc3, as it is written in HTTcc . Apart from the obvious typing annotations and \nthe explicit use of the aforementioned constructors and projections for S SK ( ), there are additional \nsyntactic elements that did not appear in Figure 1. We use the expression do e : SK * {P} A{Q} (potentially \nwith logical variables) whenever we want to explicitly ascribe the speci.cation (P, Q)to e, rather than \nuse the tightest speci.cation that the system infers for e. Such ascription will entail a proof obligation \nthat (P, Q)is valid for e, as we explain in Section 4. When the type ascription is explicitly bound to \na variable x, we write x : SK * {P} A{Q} do e. We also make explicit that callcc captures the current \nheap, in addition to the current continuation, through the heap variable j that is bound by callcc and \nwhich we declare as an index (e.g., callccj ). The variable jscopes over the whole callcc body, including \nthe type of the continuation object f. However, it is introduced strictly for purposes of speci.cation, \nand we shall use it only in the annotations, but not in the executable code over which it scopes. The \nrole of jis to relate the values of the various logical variables in its scope. In Figure 2, e.g., the \nassertion j . x . v appears in the type of the continuation object f and in the type of the body of callcc, \nthus implying that the (distinct) logical variables named v in the two types, in fact denote the same \nvalue  that stored in x at the entry to callcc. In Figure 1 we used a single global logical variable \nv for this purpose, but, as explained, global logical variables do not scale to Hoare-style reasoning \nabout procedural languages. Figure 2 uses the constructor [ ] twice. Once around c1 = abort f (x := !x \n+ 1 ; ret [ret ( )]) and again around c2 = ret ( ), which is embedded inside c1. As explained in Section \n2.1 at different points of execution, both c1 and c2 are assigned to the variable c, and thus, all three \nmust have the same type. Because the Hoare types of c1 and c2 actually differ in the pre-and postcon\u00additions, \nwe employ [ ]to abstract over the whole speci.cations, coercing c1 and c2 to S SK(). The individual speci.cations \nof c1 and c2 are then re-established using the spec r projection in other program annotations. For example, \n[c1]is the return value of callcc. The explicitly ascribed SK * type states that spec r . (x . v + 1,.)*, \nexposing that c1 performs a jump and should be executed only when x is incremented once. 2 On the other \nhand, [c2] is the value of the .nalisation code of the abort to f in c1. Hence, the variable r in the \npostcondition of f s type stands for [c2], and the formula spec r . (x . v + 3, x . v + 3)* in f s postcondition \nexposes that c2 does not change the state, but should be executed only when x is incremented by 3. The \ntypes of f and the callcc body in Figure 2 explicitly provide the required information about the intended \nuses of the code bound to c at various execution points. Indeed, the postconditions of these two types \nare essentially the two disjuncts from line 3, Figure 1, with the formulas involving spec r replacing \nthe Hoare triples over c. In this sense, the spec projection out of the record type S SK represents Hoare \ntriples when they are nested, i.e. used within the assertions of other Hoare triples. It is important, \nhowever, that the two disjuncts in line 3, Figure 1 are speci.ed in two different places in inc3 from \nFigure 2. In particular, the type of f provides the disjunct describing what happens when f is jumped \nto, whereas the inner type ascription provides the disjunct describing the normal return value of the \ncallcc block. This pattern whereby f speci.es only the jumping behaviour is characteristic of the algebraic \ncallcc operator. On the other hand, as we shall see in Section 6, the annotations describing the non-jumping \nbehaviour in the body of callcc can often be inferred (though in the case of inc3 we had to explicitly \nascribe them because the return value nests a jump). In the non-algebraic case, in our experience, f \nhas to always be manually annotated with the full disjunction of the jumping and non-jumping cases, resulting \nin larger and more cumbersome annotations and proofs. 3. Notation, logical variables, large footprint \nWhile logical variables are very useful in speci.cations, they are somewhat inconvenient to work with \nin the meta theory. The main hindrance is that premises of inference rules may contain Hoare triples \nwith differing contexts of logical variables, which have to coalesce in some way into a logical context \nof the Hoare triple in the conclusion. Typically, simple conjoining of contexts is not what is wanted, \nas some sharing of variables is desired. But then, it becomes problematic how to specify just exactly \nwhich variables should be shared in the conclusion, and which should not. To circumvent the issue, in \nthis section we introduce SK and Kont types which do not use logical variables at all, and show how SK \n* and Kont * types with logical variables from Section 2, become merely notational abbreviations. 2The \nreader can ignore the operation (-)* for now; it will be de.ned in Section 3. As a .rst step, we introduce \nthe type SK A(P, Q)of effectful computations where P is a precondition over heaps, as discussed in the \nprevious section, but Q is a binary postcondition, ranging over the ending result of the program (of \ntype A) and the initial and ending heaps, much as in VDM-style speci.cations [7, 30]. We use CIC-style \nnotation to classify logical propositions by the type Prop, and represent predicates as functions into \nProp. De.nition 1 (A-specs). Given P : preT and Q : postTA an A-speci.cation, or A-spec, is a pair (P, \nQ): specTAwith: preT heap . Prop postTA A . heap . heap . Prop specTA preT \u00d7 postTA The notation [.]. \nSK * {P} A{Q} from Section 2, with unary {P} and {Q} is an abbreviation, as we illustrate next. Example \n2. The type [v]. SK * {x . v} ( ) {x . v + 3} of inc3 from Figure 2, is an abbreviation of the speci.cation: \nSK () (. i. . v. i . x . v, . r i m. . v. i . x . v . m . x . v + 3) The SK type introduces an explicit \nquanti.cation over v in the precondition to guarantee that inc3 is safe to execute in the heap containing \n(only) the pointer x. The universal quanti.cation in the postcondition expresses that upon termination, \nthe value pointed to by x is incremented by 3. To express this property, the precondition x . v has to \nbe repeated as part of an implication in the binary postcondition, which is obviously cumbersome, thus \nmotivating the following notation, generalising to a context .. 3 De.nition 3 (SK *). Given a type A, \nP : preT, Q : postTA the type notation [.]. SK * {P} A{Q} is de.ned as: [.]. SK * {P} A{Q} SK A(P, Q) \n* . where (P, Q)* . is the A-spec de.ned as follows: (P, Q) * (. i. ... i . P, . r i m. ... i . P . m \n. Q) . The second step is to introduce the type Kont A R Q of continu\u00adation objects. R:preTis the initial \ncondition, describing the heap at the point of a jump. Q:postTA is a binary postcondition relating the \ninitial heap with the ending heap and value of the .nalisation code. The Kont type will always be in \nscope of a variable j de\u00adnoting the heap at the point of continuation capture (the index of callcc in \nFigure 2); thus Rand Qmay depend on jas well. We next show how the the type Kont * from Section 2, is \na notation over Kont, but .rst we need to generalise somewhat. As the following example illustrates, \nthe Kont * types actually require two different kinds of logical variables. Example 4. Consider a continuation \nobject which is captured when the heap j = x . v, can be jumped to only when x is incremented by some \np, and whose .nalisation code increments p further by 1. The scenario uses the program variable x and \ntwo logical variables, v and p. However, v and pclearly have different nature; v is implicitly universally \nquanti.ed, while quanti.cation over pis existential when describing the precondition for the jump, and \nuniversal in the description of the .nalisation code. Thus, we put v and p into two different contexts, \nand describe the continua\u00adtion object by the type [v],(p). Kont * {j. x . v}{x . v + p} A{x . v + p+ \n1} We refer to the two kinds of variables and contexts as box and diamond variables and contexts, respectively. \nIn Figure 2 we only 3The notation also explains the operator (-)* (with the empty context .), that was \nused in Figure 2.  used the box contexts, and in general, when the diamond context is empty, we simply \nomit it. However, diamond context is not always empty, as we will illustrate in the ping pong program \nin Section 6.2. De.nition 5 (Kont *). Let . and G be contexts of logical vari\u00adables, and jbe a distinguished \nheap variable, possibly occurring freely in R : preT, P : preT and Q : postTA. Then the no\u00adtation [.],(G). \nKont * {R}{P} A{Q}, is an abbreviation for the following Kont type without logical variable contexts. \n[.],(G). Kont * {R}{P} A{Q} Kont A (. i. ... R . .G. i . P) (. r m i. ... R . .G. i . P . m . Q) All \nthe variables in . are universally quanti.ed in the notation, whereas the variables in Gare existentially \nquanti.ed in the initial condition, and universally in the postcondition. We close the section by revisiting \nthe issue of large footprint annotations. In Section 2, inc3 could execute only in a heap with exactly \nthe pointer x, and no others. We now explain how to specify inc3 to admit execution in the presence of \nadditional pointers. We will use a (box) logical variable of type heap, to describe the sub-heaps that \ndo not contain x. For example, the more general annotation for inc3 is given below. inc3 (x:ptr): [v,h]. \nSK * {x . v * this h}() {x . v + 3 * this h} do c . callccj f : [v,h]. Kont * {j. x . v * this h} {x \n. v + 1 * this h}S SK ( ) {r. x . v + 2 * this h A spec r . (x . v + 3 * this h, x . v + 3 * this h)*}. \ndo (ret [abort f (x := !x + 1 ; ret [ret ()])]) : [v,h]. SK * {x . v * this hA j. x . v * this h} S SK \n() {r. x . v * this h A spec r . (x . v + 1 * this h, .)*}; x := !x + 1 ; cmd c. The annotations introduce \na logical variable h, the assertions this h, and separating conjunction *, to name the part of the heap \ndisjoint from the pointer x. The occurrence of the same this h in both the pre-and postcondition of inc3, \nspeci.es that inc3 keeps this part of the heap invariant. Moreover, h is local to the Hoare triples in \nwhich it appears (unlike in .rst-order Hoare or Separation logic, where logical variables are global). \nThus, a speci.cation of inc3 can be extended to a larger heap merely by instantiating h, rather than \nby means of a dedicated frame rule as in Separation logic. Because of the repeated occurrences of this \nh in the assertions, this style of annotation is a bit more verbose than in Separation logic, where the \ninvariance of residual heaps is implicit. However, in practice, the extra logical variable did not affect \nour proofs. We discuss in Section 7 the reasons for needing large footprints, and some alternative designs. \n4. Inference rules We develop the semantics of HTTcc using the Calculus of Induc\u00adtive Constructions (CIC) \nas the meta logic. The choice provides us directly with a method to prototype HTTcc as a shallow embed\u00adding \nin Coq, thus inheriting a number of useful constructs, such as ret : .v:A. [h]. SK * {this h} A{r. this \nhA r = v} alloc : .v:A. [h]. SK * {this h} ptr {r. r . v * this h} dealloc : .x:ptr. [B,v:B,h]. SK * \n{x . v * this h} () {this h} := : .x:ptr. .v:A. [B,w:B,h]. SK * {x . w * this h} () {x . v * this h} \n! : .x:ptr. [v:A,h]. SK * {x . v * this h} A {r. x . v * this hA r = v} Figure 3. HTTcc typing assignment \nfor primitive commands. dependent .and Stypes, which we have already used in Section 2. For the sake \nof brevity, we omit the treatment of such standard con\u00adstructs (it can be found in [27, 6]), and freely \nassume the standard typing rules and the various syntactic categories of CIC, such as, e.g., variable \ncontexts. We only present our impure monadic exten\u00adsions: the typing rules for the SK and Kont types, \nand related terms. In Section 5, we develop the semantic model for HTTcc , which we mechanised in Coq, \nto show the soundness of the extension. The speci.c rules of HTTcc are of two distinct kinds. The .rst \nkind consists of typing rules, which serve to infer the default pro\u00adgram speci.cations (weakest precondition \nfor memory safety, and strongest postcondition wrt. that precondition). The inference is im\u00adportant in \npractice because it minimises the amount of annotations that the user has to provide manually. The second \nkind consists of structural lemmas that formalise the reasoning about Hoare\u00adordering of speci.cations. \nAs illustrated in Figure 2, such reason\u00ading is needed in several situations: (1) We may explicitly need \nto ascribe a custom speci.cation to a program, and this requires a proof that the default speci.cation \ncan be pre-weakened and post\u00adstrengthened into a desired one, and (2) We may use the relation . , to \nexplicitly declare that a [-]-abstracted command satis.es a predetermined speci.cation. The two kinds \nof rules are discussed in Sections 4.1 and 4.2, respectively. 4.1 Typing rules From here on, we use \ns and variants to range over speci.cations (P, Q), with pre s and post s projecting out the components. \nBIND rule in Figure 4 perhaps best exempli.es the inference na\u00adture of our typing rules. Given programs \ne1 and e2 with speci.ca\u00adtion s1 and s2, respectively, the rule infers the tightest speci.ca\u00adtion for \nthe sequential composition x . e1;e2 as follows. Because the execution of the compositions starts with \ne1, the inferred pre\u00adcondition must require that pre s1 holds of the initial heap i. After e1 terminates \nwith an intermediate value x and heap h satisfying post s1 x i h, it must be that pre (s2 x)h so that \ne2 is safe to run. The inferred postcondition declares the existence of an intermedi\u00adate value x, and \nand a heap obtained after e1 but before e2, as per relational composition post s1 x . post (s2 x)r. BIND, \nas well as the other rules in Figure 4, use SK and Kont types without logical variable contexts, which \nis why we introduced such types in Section 3 in the .rst place. Omitting logical variables facilitates \nspeci.cation inference, as it circumvents the issue of reconciling potentially different contexts of \nlogical variables that may appear in the type for e1 and the type for e2. ABORT rule infers a precondition \nthat has a dual role. The .rst con\u00adjunct R i ensures that the continuation object f is aborted to only \nin heaps i for which the initial condition R is satis.ed. The sec\u00adond conjunct s . (RA this i, Q)ensures \nthat e is an appropriate .nalisation code for f; that is, the speci.cation s of e can be weak\u00adened into \na precondition Rand postcondition Q, as required by the type of f. Additionally, this iallows the proof \nof the weakening to  G . e1 : SK As1 G, x : A . e2 : SK B (s2 x) BIND G . x . e1;e2 : SK B (. i. pre \ns1 i. . x h. post s1 x i h . pre (s2 x)h, . r i m. . x.(post s1 x . post (s2 x)r)i m) G, j : heap, f \n: Kont A(R j) (Q j). e : SK A(s j) CALLCC G . callccj f . e : SK A(. i. pre (s i)i, . r i m. post (s \ni)r i m . (R. Q i r)i m) G . f : Kont A R Q G . e : SK As ABORT G . abortB f e : SK B (. i. R i . s \n. (RA this i, Q), . r i m. False) G . e1 : SK As1 s1 . s2 G . e : bool G . e1 : SK As1 G . e2 : SK As2 \nDO IF G . do e1 : SK As2 G . if e then e1 else e2 : SK A(if e then s1 else s2) G . f : ( . x:A. SK (Bx) \n(s x)) . .x:A. SK (Bx) (s x) FIX G . .x f : . x:A. SK (Bx) (s x) Figure 4. HTTcc typing rules for speci.cation \ninference. exploit the knowledge that the heap in which the .nalisation code executes is exactly i. The \nexact de.nition of . will be given in Sec\u00adtion 4.2. Because abort does not return any values, its return \ntype B is arbitrary and can be supplied by the user. We omit annotating this type in examples as it can \nbe usually inferred from the context. CALLCC rule in Figure 4, infers the speci.cation for callcc f. \ne. The premise of the rule introduces the heap variable j, which, as illustrated in Section 2, provides \na common point for f and e to synchronise on, thereby .xing the values of various logical variables in \nrelation to j. In the conclusion of the rule, j will be instantiated with the initial heap of callcc; \nthat is, with the heap at the point of continuation capture. The speci.cation of f allows aborting to \nf only in heaps satis\u00adfying R j. After f s .nalisation code terminates, the resulting heap and value \nsatisfy Q j. If e has a speci.cation s, then the tightest speci.cation of callcc f. e can be inferred \nas follows. Because the execution of the whole command starts with e, the inferred pre\u00adcondition has \nto be derived out of e s precondition pre (s j). The unknown jis instantiated with the actual initial \nheap, to obtain the tightest precondition . i. pre (s i)i. The postcondition is a disjunc\u00adtion expressing \nthat e may produce two different outcomes: it ei\u00adther aborts to f, or it does not. The left disjunct \npost (s i)r i m de\u00adscribes the non-aborting case; it simply equals the postcondition of e with jinstantiated \nby i. In those cases when e actually aborts, the disjunct will be False, as it embeds the postcondition \nof the ABORT rule. The right disjunct is a relational composition (R . Q i r)i m which describes the \naborting case, as follows. In the aborting case, there exists a heap call it h that is current at the \npoint of abort. Due to the speci.cation of f, R must relate i and h. Additionally, m and r are obtained \nafter the .nalisation code of f is executed at h, and thus Q i r must relate hand m as well. The annotations \nspecifying the continuation object (R and Q) appear in a negative position in the premise of CALLCC, \nand can\u00adnot be inferred automatically. In contrast, the speci.cation s for e is generated by e s typing \nderivation. The property that only the aborting case has to be speci.ed manually, differentiates the \nalge\u00adbraic callcc from the standard, non-algebraic alternatives. A de\u00adtailed comparison is presented \nin Section 7. Other rules The rule DO implements a type ascription, requiring a proof that the speci.cation \ns1 can be weakened into s2, as in the usual Hoare logic rule of consequence. The IF rule uses the type\u00adlevel \nconditional, available in CIC, to compute the speci.cation of a program-level conditional out of the \ntypes of the components. The FIX rule implements the usual typing for recursive procedures, requiring \nthat an SK type be established for the procedure body, un\u00adder a hypothesis that the recursive calls satisfy \nthe same type. The primitive stateful commands have standard Separation logic speci\u00ad.cations (Figure \n3), except that they are extended to the large foot\u00adprint idiom by naming the unused heap with the logical \nvariable h. Additional notation In the rest of the paper, we will use explicit names for the derived \nspeci.cations in Figure 4. For example, we write callcc s R Q s for (. i. pre (s i)i, . r i m. post (s \ni)r i m . (R. Q i r)i m), abort s R Q s for (. i. R i . s . (RA this i, Q), . r i m. False), and similarly \nfor bind s, read s, etc.  4.2 Structural lemmas and symbolic evaluation The Hoare ordering on two speci.cations \ns1 and s2 is de.ned as: s1 . s2 . i. pre s2 i . verify i s1 (. r:A m. post s2 y i m) where for any . \n: A.heap.prop: verify i s . pre s i . . r m. post s r i m . .r m The de.nition of s1 . s2 states that \npre s1 weakens into pre s2 and post s2 strengthens into post s1, as in the Hoare logic rule of consequence. \nIt is split into two stages in order to exploit the hypothetical reasoning of CIC and Coq. In practice, \nthe hypothesis pre s2 i will always move into the hypothesis context of Coq, leaving verify to describe \nthe remaining proof goal. The HTTcc proof obligations arise directly from the side con\u00addition about Hoare \nordering in the rule DO in Figure 4, when the user wants to ascribe a desired speci.cation to a program. \nIn the practical work with HTTcc , the proof obligations are discharged by applying a number of carefully \ncrafted lemmas about verify that implement veri.cation by symbolic evaluation. We illustrate the process \nhere, and discuss the relationship to algebraicity of control operators in Section 4.3. For instance, \nlet e1 and e2 be computations with speci.cations s1 and s2 x, respectively. The inferred speci.cation \nfor x . e1;e2 is bind s s1 s2. An HTTcc proof obligation establishing that in some heap i, the execution \nof the sequential composition produces a heap and a value satisfying ., will have the form: verify i(bind \ns s1 s2). The idea of symbolic evaluation is to discharge such a goal as follows. It suf.ces to show \nthat verifying a composite B-spec (bind s s1 s2)can be reduced to verifying s1 against a . ' , where \n. ' itself involves verifying s2 against .. The process is iterated as long as s2 contains sequential \ncompositions, and can be seen as a sequence of applications of the following lemmas in HTTcc :  STEP \n: verify is1(. y m. verify m (s2 y).). verify i(bind s s1 s2). VALDO : pre s i . (. x i m. post s x i \nm . . x m). verify is . The STEP lemma implements the iterative step, and VALDO lemma applies in the \nend, when there are no outstanding sequential com\u00adpositions to be stepped through. The VALDO lemma may \nbe specialised to streamline the sym\u00adbolic evaluation for speci.c commands. For example, let f : Kont \nA R Q and e : SK A s. The inferred spec for abort f e is abort s R Q s = (. i. R i . s . (R, Q), . r \ni m. False). Taking this spec for s in VALDO, after some simpli.cation, we obtain: VALABORT : R i . verify \nis (.r. .m. Q r i m). verify i(abort s R Q s). In other words, to verify abort to f, we need to show \nthat Rholds of the current heap, and that the supplied .nalisation code satis.es Qafter running in i. \nIn case e1 = callccj f. e, the inferred spec is callcc s R Q s = (. i. pre (s i)i, . r i m. post (s i)r \ni m . (R. Q i r) i m), for some j and f : Kont A(R j) (Q j) and e : SK A (s j). Tak\u00ading this spec for \ns, and after some rearrangement of the disjunction in the postcondition of callcc s, VALDO can be specialised \ninto the following lemma. VALCC : verify i(s i). . (. r:A m. (R. (Q i r))i m . . r m). verify i(callcc \ns R Q s). The .rst hypothesis corresponds to the case when e does not abort. In that case, the goal reduces \nto verifying (s i)against .. The sec\u00adond hypothesis corresponds to the aborting case. In that case e \npro\u00adduces an ending heap m and value x satisfying (R. (Q i))x i m; that is e .rst reaches an aborting \nheap out of i (predicate R), and then executes the .nalisation code (Q i). Then we just need to prove \nthat .holds after the execution of the .nalisation code. Specialised symbolic execution lemmas can be \nproved for all other primitive commands. Furthermore, because verify is an ordi\u00adnary logical de.nition, \nusers can establish such lemmas for their own programs as well, directly in the logic. Example 6. We \ncan implement the usual throw command, as an abort with an immediately-returning .nalisation code. Given \nf : Kont A R Q and a value v:A, we de.ne: throw g v abort f (ret v) The inferred speci.cation throw s \nR Q v = abort s R Q (ret s v) can be proved to satisfy a streamlined version of VALABORT, which exploits \nthe trivial nature of the .nalisation code to simplify one of the hypotheses. VALTHROW : R i . Qv i i \n. verify i(throw s R Q v).  4.3 Algebraicity at the level of speci.cations In this section, we show \nthat the algebraicity property of callcc can be expressed as a lemma over speci.cations, similar to the \nsym\u00adbolic evaluation lemmas from the previous section. This lemma, which we call ALGCC, expresses that \nwe can commute the speci\u00ad.cation forms bind s and callcc s, thus lifting to the level of spec\u00adi.cations \nthe algebraicity equation (1) from Section 2. However, stating the ALGCC lemma requires generalising \nsomewhat the def\u00adinition of callcc s R Q s1. We .rst introduce the generalised def\u00adinition, which we \nrename algcc s R s0 s1, and then describe the intuition behind it. Consider a program of the form x . \n(callccj f. e1);e2, where f : Kont A(R j) (Q j), e1 : SK A (s1 j), and x:A . e2 : SK B (s2x). Then, side-by-side, \nthe two de.nitions look like: callcc s R Q s1 = (. i. pre (s1 i)i . r i m. post (s1 i)r i m . (R. Q i \nr)i m) algcc s R s0 s1 = (. i. pre (s1 i)i. . h.R i h . pre (s0 i)h, . r i m. post (s1 i)r i m .(R. post \n(s0 i)r)i m) The main difference is that where callcc s uses only the postcondi\u00adtion Q(abstracting over \nj)to specify the .nalisation code, algcc s takes a full speci.cation s0 (also abstracting over j), which \nin\u00adcludes a precondition as well. In callcc s, the precondition for the .nalisation code is assumed to \nbe the trivially true heap predicate, and we can prove the equation callcc s R Q s1 = algcc s R(. j. \n(., Q j))s1. Intuitively, callcc s could use the trivial precondition for the .nalisation code, because \nR already describes what holds of the heap at the point of abort, and this is the same heap in which \nthe .nalisation code executes. However, the main property of algebraic commutation is that it changes \nthe .nalisation code by extending it with e2, though it does not change the point of aborting. The new \nde.nition algcc s thus divorces R and pre s0, so that the algebraicity lemma can express that R remains \n.xed, while pre s0 changes. The changes to pre s0 cannot be arbitrary, however, as the .nalisation code \nalways executes in a heap in which abort is called. Thus, the precondition of algcc s includes a conjunct \nthat R implies pre s0 for every initial heap i, and aborting heap h. We can now state the algebraicity \nlemma for callcc, with the omitted proof included in our Coq .les. ALGCC : verify i(bind s (algcc s R \ns0 s1)s2). .. verify i(algcc s R (. j. bind s (s0 j)s2) (. j. bind s (s1 j)s2)) . 5. Denotational semantics \nIn this section we present the semantics of HTTcc as a shallow embedding into CIC. That is, we provide \na semantic interpretation function [-] that maps HTTcc types SK and Kont into types de.ned in CIC, but \nacts as identity on all the other types inherited from CIC, such as nat or heap. Similarly, the interpretation \nof terms maps the monadic constructs such as callcc, abort, etc., into CIC-terms, while acting as an \nidentity on all the other terms inherited from CIC. The interpretation extends homomorphically to variable \ncontexts as well. 5.1 Semantics of types As customary in the case of control operators, our denotational \nsemantics is parameterised wrt. the type X of return results for the continuations. We further require \nthat X is a complete lattice, thus providing us with means to model the .xed point combinator in CIC, \nusing the Knaster-Tarski theorem. De.nition 7. Given a type A, and predicates P : preT and Q : postTA, \nthe types SK A(P, Q)and Kont A P Q are interpreted as follows. [SK A(P, Q)] .i:heap. [P] i . (.r:[A]. \n.m:heap. [Q] r i m .X). X [Kont A P Q] [SK A(P, Q)] . .j:heap. [P] j. X A computation of SK type takes \na heap i satisfying the pre\u00adcondition P, and a continuation requiring the postcondition Qas a precondition. \nIntuitively, the continuation applies to the ending value and heap of the computation, to produce a result \nin X. As the latter type is a complete lattice, the possible results include di\u00advergence, denoted by \nthe bottom element of the lattice. We don t model the faulting behaviour such as type or memory errors \n(e.g., de-referencing a dangling pointer), but instead rely on the proofs of [P] i and [Q] r i m to statically \nensure that a computation exe\u00adcutes only in heaps and continuations for which such errors do not occur. \nIn this sense, well-typed (i.e., well-speci.ed) computations in HTTcc do not fault, as usual for type \nsystems and for fault\u00adavoiding Hoare logics such as Separation logic.  A further useful intuition about \nour model may be gained if one erases the dependencies on P and Qin De.nition 7. This results in the \nfollowing informal equations: [SK A] = heap . ([A] . heap . X). X [Kont A] = [SK A] . heap . X The .rst \nequation shows that the SK A type is essentially the stan\u00addard state-passing continuation monad. The \nsecond equation shows that a continuation object semantically requires two arguments: an explicit .nalisation \ncode (the SK A type), and a heap. Of course, the .nalisation code is explicitly provided by the program \nas an ar\u00adgument of abort. Importantly, the heap argument is implicitly sup\u00adplied by the denotation of \nabort as the heap current at the point of aborting to f. Of course, the parametrisation with .nalisation \ncode is what makes our control operators algebraic, and is directly inspired by Jaskelioff [22]. On the \nother hand, the further heap argument makes the Kont Atype implement non-rollbacking continuations. In \ncon\u00adtrast, the algebraic callcc presented by Jaskelioff for a state and continuations monad does roll-back \nthe state to the one captured together with the current continuation.  5.2 Semantics of computations \nFor the sake of brevity, we present only the denotational semantics of the control operators callcc and \nabort. Moreover, we illustrate only the simpli.ed setting where the dependencies on P and Qare erased \nfrom the types (as in the above informal equations), and the dependencies on the proofs of [P] iand [Q] \nr i m are erased from the terms. The full dependency-respecting denotations for all the monadic commands \nfrom Figures 4 and 3 are implemented in the companion Coq .les. De.nition 8 (callcc f. e). Given f : \n[Kont A] and e : [SK A], the denotation of callcc f. e of type [SK A] is de.ned as: 4 [callcc] : ([KontA] \n. [SK A]). [SK A] [callcc f. e] .i : heap. .k : [A] . heap . X. [.c : [SK A]. .h : heap. c h k/f ][e] \ni k Intuitively, executing callcc f. e corresponds to applying the denotation to the initial (i.e., captured) \nheap i and continuation k. The body e is executed using the same heap and continuation. However, .rst \nthe variable f : [Kont A] is bound to a continuation object that, when supplied the .nalisation code \nc and a heap hthat is current at the point of aborting to f, executes c in h passing the control (i.e., \njumping) to k. De.nition 9 (abortB f e). Given f : [Kont A] and e : [SK A] , the denotation of abortB \nf e of type [SK B] is given by: [abortB ] : [Kont A] . [SK A] . [SK B] [abortB f e] .i : heap. .k : [B] \n. heap . X. [f] [e] i Theorem 10 (Soundness). If G . e : Athen [G] .C I C [e] : [A]. Proof. The proof \nis by induction on the structure of e. The inter\u00adesting cases are when e is one of the monadic commands \n(corre\u00adspondingly, when Ais an SK type), as in all other cases the seman\u00ad 4Omitting the index j to callcc, \nwhich may only appear in the erased dependencies. 1. rember-up-to-last (x : A) (xs : list A): [h]. SK \n* {this h} (list A){r. this hA r = rember x xs [ ]} 2. do (callccj exit : [h]. Kont * {j. this h}  \n{if x . xs then this helse .} (list A) {r. this hA r = rember x xs [ ]}. 3. .x (. f : remberT. . ys : \nlist A. 4. if ys is y:: ys ' then 5. zs . f ys ' ; 6. if x == y then throw exit zs 7. else ret (y::zs) \n 8. else ret [ ]) xs)  where remberT .ys : list A. SK * {this jA .ps. xs = ps++ys} (list A) {r. this \njA r . remberP x ys [ ]} remberP x xs acc if xs is y:: ys ' then if x == ythen \u00d8 else remberP x ys ' \n(acc++[y]) else {ps | ps = acc} Figure 5. rember-up-to-last in HTTcc . tic function is trivial. When \ne is a monadic command, the sound\u00adness proof for the command is intertwined with the de.nition of the \ndenotation of e. For example, in the case of e = callccj f. e the denotation will involve parametrization \non the proofs of [P] i and [Q] r i m, for the appropriate P and Q, that we have simpli\u00ad.ed away in our \ndiscussion. Such proof parameters are used in the denotations to build larger proofs on-the-.y, as necessary \nto make the various sub-terms of the denotation typecheck, until ultimately the whole denotation term \ntypechecks wrt. the speci.cation given in Figure 4. Similar considerations apply for other monadic terms \nas well. One exception is the .xed point construct .x f, whose soundness is proved by an appeal to Knaster-Tarski \ntheorem over the monotone completion of f. The Knaster-Tarski theorem applies because [ST A (P, Q)] is \na complete lattice, being de.ned as a function space into a complete lattice X. We have mechanised all \nthe steps of the proof in our Coq .les. 6. A short veri.cation survey We present the veri.cation of two \nexamples exhibiting non-trivial patterns for programming with continuations. The .rst example, rember-up-to-last, \nillustrates downwards or exit continuations, whereby a jump is used to escape early from a recursive \ncall, whilst discarding suspended computations. The second example, ping pong, illustrates upwards continuations \nor unstructured loop, where the executions of a captured continuation are interleaved with user code, \nthereby mimicking the cooperating behaviour of (a simpli.ed version of) coroutines. In the companion \n.les [12], we verify other examples as well, such as escaping from in.nite loops and using continuations \nto implement error handlers.  1. rember (x : A) (xs acc : list A) : list A 2. if xs is y::ys ' then \n 3. if x == y then rember x ys ' [ ] 4. else rember x ys ' (acc++[y]) 5. else acc.  Figure 6. Purely \nfunctional rember. 6.1 rember-up-to-last Let A be a type supporting decidable equality; i.e., there \nexists a function == :A.A.bool. Given x :A and a list xs :list A, rember-up-to-last x xs, returns the \nending segment of xs after the last occurrence of x; if x does not occur in xs, the whole xs is re\u00adturned \n[18, p. 55]. For example, if xs = [23,16,42,4,42,8,15,16], then rember-up-to-last 4 xs = [42,8,15,16], \nrember-up-to-last 16 xs = [ ] and rember-up-to-last 42 xs = [8,15,16]. However, rember-up-to-last 7xs \n= xs. Figure 5 implements rember-up-to-last in HTTcc , following the ML implementation by Thielecke [47]. \nFor simplicity, we use purely-functional lists instead of imperative, singly-or doubly\u00adlinked lists. \nThe implementation works as follows. In line 2, it cap\u00adtures the continuation with callcc. Then, it recurses \nover the input list xs, searching for x (lines 3 5), rebuilding the input list on the way back (line \n7). If x is found (line 6), it jumps out of the loop, by throwing to the captured continuation (Example \n6). The returned value zs is the list rebuilt so far, while the outstanding iterations of the loop intended \nto further rebuild the list, are cancelled. We also develop a purely-functional version rember (Figure \n6) by induction on xs. rember does not use callcc, but instead relies on tail recursion and the accumulator \nacc to keep track of the currently rebuilt list. Of course, the implementation with jumps is preferable \nfrom the ef.ciency standpoint, but we require the pure rember in order to specify rember-up-to-last. \nWe now analyse the type annotations in Figure 5. Those for rember-up-to-last (line 1) are quite intuitive. \nThe function can run in an arbitrary heap h(this hin the precondition). It leaves the heap unchanged \n(this h in the postcondition), and the return result r is the same as running the tail-recursive rember \nwith the empty initial accumulator. The continuation object exit (line 2) is speci.ed as follows. The \ninitial condition exposes that the captured heap j equals h. A jump to exit may occur only when the precondition \nis satis.ed; in this case, only when x . xs. The postcondition states that the heap is unchanged, and \nthe return value equals running rember, as expected. The most interesting annotations is the loop invariant \nremberT provided as the type of the recursive function f. The precondition states that f is only ever \napplied to the tail ys of xs; hence xs can be partitioned as ps++ys. The partitioning is made explicit \nin the precondition, as it will be required when proving that we are throwing the correct ending segment \nin line 6. In the postcondition, remberT has to state that f produces the correct result. Importantly, \nhowever, it cannot use the helper function rember!rember requires an accumulator argument, but as f itself \nis not tail recursive, it is not clear which value to supply for the accumulator. Unlike in the speci.cations \nof rember-up-to-last and exit, it s incorrect to use [ ], as that does not re.ect the looping behaviour. \nIt s also incorrect to existentially abstract over the accumulator, as that produces too weak a property \nwhich then does not imply the postcondition of rember-up-to-last, where the accumulator is [ ]. The workaround \nis to de.ne a helper function remberP, which is similar to rember, but records when the jumps in f appear, \nas 1. {. h. this h} 2. do (callccj exit . 3. {. h. this hA j= h} 4. {this jA xs = [ ]++xs} 5. .x \n(. f ys. 6. {. ps. this jA xs = ps++ys} 7. if ys is y:: ys ' then 8. {. ps. this jA xs = ps++(y:: \nys ' )} 9. zs . f ys ' ; 10. {. ps. this jA xs = ps++(y:: ys ' )A zs . remberP x ys ' [ ]}  11. if \nx == y then 12. {. ps. this jA xs = ps++(x::ys ' )A zs . remberP x ys ' [ ]} 13. {this jA x . xs A \nzs = rember x xs [ ]} 14. throw exit zs 15. {.} 16. else ret (y::zs) 17. . ps. this j A xs = ps++(y:: \nys ' ) A y::zs . remberP x(y::ys ' ) [ ]} 18. else ret [ ] 19. {. ps. this jA xs = ps A [ ] . remberP \nx [ ] [ ]} 20. )xs) 21. {this jA r . remberP x xs [ ]} 22. {. h. this hA (r . remberP x xs [ ] Y (x \n. xs A r = rember x xs [ ]))} 23. {. h. this hA r = rember x xs [ ]}  Figure 7. Proof outline for rember-up-to-last \nx xs. follows. remberP returns either an empty set of values, to signalise a jump, or a singleton set, \nwith the correct value, in the non\u00adjumping case. More precisely, we have the following lemma: rmb rmbP \n: r . remberP x xs acc . r = rember x xs acc. Then, the loop invariant remberT can assert that the return \nvalue r is always in the set de.ned by remberP. In the case of a jump, this property is evidently false, \nsince the set is empty. But, in such a case, the program behaviour is described by the annotation on \nexit anyway, and the loop invariant need not bother describing it again. Figure 7 presents a proof outline \nfor rember-up-to-last x xs. The trivially true assertion .h. this h in line 1 corresponds to un\u00adfolding \nthe notation for the type SK * with a precondition this h, and a logical variable h. In line 3, the current \nheap h is captured into the variable j, corresponding to substituting hfor jin the rule BNDCC, Section \n4. In the proof outline, we cannot represent the substitution because jappears in the rest of the code, \nso instead we equate jwith hin the assertion. Line 6 starts the veri.cation of the loop; it shows that \nthe precondition of the loop invariant remberT holds; the heap is unchanged wrt. the captured heap, and \nthere ex\u00adists a partition xs = ps++ys. One critical point in the proof is Line 13, where we need to establish \nzs = rember x xs [ ], which is the precondition for throw. This property can be proved out of the partitioning \nxs = ps++(x :: ys ' )and zs . remberP x ys ' [ ] available in Line 12, by using rmb rmbP and additional \ntwo helper lemmas (though we elide the derivation here):  rmb cat : rember x (ps ++ys)acc = rember ys \n(rember x acc ps) rmb in : rember x xs acc = if x . xs then rember x xs [ ] else (acc ++ xs) Another \ncritical point is line 17, where we need to prove y :: zs . remberP x (y :: ys ' ) [ ], required to establish \nthe postcondition of the loop. The property is proved out of zs . remberP x ys ' [ ] available in line \n10, after unfolding the de.nition of remberP once, and using the following lemma about remberP: zs . \nremberP x ys acc . y::zs . remberP x ys (y::acc). Line 19 describes what holds in the else branch of \nthe main con\u00additional in the loop, and line 21 establishes that the loop invari\u00adant holds at the end \nof the loop. Line 21 is a common weakening of lines 15, 17 and 19. Line 22 includes a disjunction, showing \nthat the line can be reached by a normal termination of the loop (line 21), or by a jump to exit. Line \n23 is obtained out of line 22 by applying rmb rmbP, and establishes the speci.ed postcondition of rember-up-to-last. \n   6.2 Ping-Pong cooperation In Section 2 we presented inc3, which used a closure to capture a continuation \nand execute it twice. Here, we generalise the idea to n calls to abort, thus iterating n times the captured \ncontinuation. The result is an interleaving between the captured continuation and the .nalisation code \nin the closure, which creates a cooperation pattern resembling that of coroutines [47] albeit one without \nthe full power of coroutine fork or yield operations, which further require storing continuations into \nthe heap [40, 35]. Unlike inc3, where both the captured continuation and the .nalisation code executed \nthe same code, we will have different computations for the captured continuation, ping, and for the .nalisation \ncode in the closure, pong, and use the pre-and postconditions to show that these are interleaved in the \nevaluation of ping pong. Since our motivation is to verify the cooperation pattern, rather than ping \nand pong per se, we give a trivial implementation for the latter functions: ping incr x and pong incr \ny, where: incr z : [v h ]. SK * {z . v * this h} ( ) {z . v + 1 * this h} do (v . !x; x := v + 1 ) Figure \n8 presents ping pong, whose structure is close to that of inc3. In line 2, callcc captures the continuation \ncorresponding to the rest of the program .c. ping;cmd c and binds it to the con\u00adtinuation object k. The \nbody of callcc returns a function f de.ned recursively on n. If n is non-zero (line 5), f returns a closure \nwhich aborts to the captured continuation with the .nalisation code con\u00adsisting of pong followed by the \nrecursive call to f. In the zero case, f returns the computation pong. The result of this recursive func\u00adtion \nis bound to c in the sequel. When n > 0, c will be bound to a closure with n calls to abort nested in \nthe .nalisation code: [abort k (pong;ret [abort k (\u00b7 \u00b7 \u00b7 abort k(ret [pong]))])] n calls to abort k \nAfter callcc, ping is evaluated to increment x, and then c is evaluated. If n = 0, and thus c is bound \nto [pong], the value at y is incremented an the function terminates. If n > 0, the outermost abort in \nthe closure above is executed, thus running pong as the .nalisation code and passing the rest of the \nnested computations in the closure, bound to c, to the captured continuation. This results in a backward \njump to line 7. The loop continues until the closure is consumed in full and the function terminates. \nAs a result ping and pong are interleaved n + 1 times. 1. ping pong (n : nat): [v w h ]. SK * {x . v \n* y . w * this h}( ) {x . v + n + 1 * y . w + n + 1 * this h} 2. do (c . callccj k : [v w h ](p). Kont \n* {j. x . v * y . w * this h} {if n = 0then x . v + p+ 1 * y . w + p* this h else .} S SK () {r. x . \nv + p+ 1 * y . w + p+ 1 * this h A spec r . (x . v + p+ 2 * y . w + p+ 1 * this h, if p+ 1 = n then x \n. v + n + 1 * y . w + n + 1 * this h else .)*}. 3. .x (. f : pingpongT. . z : nat. 4. if z is Suc z \n' then 5. ret [abort k(pong; (f z ' ))] 6. else ret [pong]) n; 7. ping; 8. cmd c).  where pingpongT \n.z : nat. [v w p h ]. SK * {x . v + p * y . w + p* this h A j. x . v * this h* y . w A z = n A p+ z = \nn} S SK () {r. x . v + p * y . v + p* this h A spec r . (x . v + p+ 1 * y . v + p * this h, if p = n \nthen x . v + n + 1 * y . w + n + 1 * this h else .)*} Figure 8. ping pong cooperation. This behaviour \nis re.ected in the type of ping pong: when the function is executed in a heap containing at least the \npointers x and y storing some natural number, the result after n + 1 executions of ping and pong is a \nheap with the same shape, where the values of x and y are both incremented n + 1 times. The type invariant \npingpongT describes the speci.cation of the loop that de.nes the closure f described above: on each iteration \nof the recursive call, we insert calls to pong deeper into the closure, which will execute after the \ncorresponding ping. Then, we make explicit that on each recursive call, the values stored in the heap \nhave been incremented appropriately. We introduce a (box) logical variable p to account for this fact: \nwhen the recursive call to f occurs, p calls to ping and p calls to pong have occurred. The recursive \ncall produces a closure whose spec we de.ne using .. The closure should be run in a heap after the (p+ \n1)-execution of ping. As for the postcondition, If p n, then the closure s head is a = call to abort, \nline 5, and the postcondition is .. If p = n, i.e. this is the last iteration of f, then the closure \ncorresponds to the one in line 6, entailing that this is the last execution of the loop, if n > 0 or \nthat there was no loop at all otherwise. Hence, the .nal heap of the closure results x . v + n + 1 * \ny . w + n + 1 * this h.  Unlike the case of inc3, the continuation object k here is aborted to more \nthan once. As a result, the precondition in Kont * has to accommodate for the changes in the state in \neach of the dif\u00adferent jumping points. This is solved by using a (diamond) logical variable p, which \nallows us to discriminate the changes of each par\u00adticular jumping point wrt. the captured heap j. The \nprecondition in k states that the calls to abort occur when n > 0. Then, the heap at each jumping point \nshould re.ect the effect of p+ 1 executions of ping and p executions of pong. The postcondition states \nthat the .nalisation code performs p+ 1 execution of pong, and that it returns a closure which, again, \nis meant to run after the next ping. The postcondition in the closure is similar to the one in pingpongT, \nalbeit the current iteration being p + 1 rather than p, as one ping has already executed. In an online \nAppendix [12], we present a detailed proof outline for ping pong. The Coq script can be found in the \nsource .les. 7. Discussion and related work Reasoning with non-algebraic callcc To understand the differ\u00adence \nin speci.cation and reasoning between algebraic and non\u00adalgebraic control operators, we have repeated \nour formal devel\u00adopment for a non-algebraic set of operators, which we also make available online [12]. \nAs a basis, we gave control operators a more familiar type for non-parameterised continuation monads \n[49]: callcc : ((A . SK B). SK A). SK A throw : (A . SK B). A . SK B Continuation objects are ordinary \nside-effectful functions of type A . SK B, which do not make provisions for .nalisation code, and are \nthus not algebraic [22]. The same remark applies to the type given to the C-operator in [48], which is \na different, but closely related control operator [15, 14]. We managed to soundly parameterise the monad \nwith Hoare\u00adstyle assertions using the following typing rules. j, f : . x:A. SK B (Sx j, . r i m. False). \ne : SK A(P j, Q j) .j.(P j, Q j). (. i.j = i. R i, S) . callccj f. e : SK A(R, S) . f : . x:A. SK B (R, \n. ri m. False) . e : A . throw f e : SK B (R, . r i m. False) The intuition for the callcc rule is that \nthe user must provide the ending speci.cation pair (R, S). The (binary) postcondition S for the whole \ncommand is used as a precondition for the continuation object f, after Sis .rst instantiated with the \nvalue x that is passed to f, and the captured heap j. The rule has a side condition requiring that the \nspeci.cation (P j, Q j) inferred for the body e, can be weakened into the desired (R, S), under the knowledge \nthat the captured heap jequals the initial heap i. The requirement that the speci.cation (R, S)has to \nbe provided by hand, practically differentiates the algebraic and non-algebraic operators. In the non-algebraic \ncallcc, the speci.cation is mono\u00adlithic, and the postcondition S is usually a disjunction whose cases \nspecify both the jumping and the non-jumping behaviour of the code. The algebraic callcc separates the \ntwo cases; the jump\u00ading is manually speci.ed in the type of f, but the non-jumping speci.cation can often \nbe inferred by the typing rules from the structure of e, say, if e is straight line code, or by using \nthe in\u00advariants provided with the loops in e, otherwise. Our examples rember-up-to-last and ping pong \nillustrate the point, whereby e s speci.cation directly corresponds to the supplied loop invariants. \nIn the non-algebraic case, specifying these two examples incurs an overhead that the same annotation \nhas to be provided twice; once as the loop invariant, and again as part of the speci.cation of callcc. \nSmall vs. large footprints The need for large footprints and ex\u00adplicit naming of residual heaps arises \nin HTTcc due to the control operators. The HTTcc typing rule for A BORT, requires .rst dis\u00adcharging a \nprecondition that the heap i at the point of the jump is related by the initial condition R to the heap \njat the point of con\u00adtinuation capture. R is an ordinary predicate on heaps, rather than a Hoare triple. \nThus, the usual idea of Separation logic, whereby a Hoare triple leaves the unused parts of a program \nimplicitly un\u00adchanged, does not directly apply, and we have to name the unused parts in iand jin order \nto explicitly state their equality. We have considered an alternative whereby the denotational se\u00admantics \nof control operators may automatically determine the un\u00adused part of the heap, by subtracting out of \nthe current heap the por\u00adtion described by the assertions. However, for this to work, the as\u00adsertions \nwould have to be precise, i.e., uniquely determine the por\u00adtion to be subtracted. But then, the precision \nof each used assertion has to be formally proved. Thus, we opted for slightly increasing the speci.cation \nburden by introducing explicit this h predicates, as a trade-off for not having to prove precision of \nassertions. Despite the slight overhead of large footprints, we have not found them too problematic in \npractice. The naming of the resid\u00adual heaps is intuitive and can be done systematically. Moreover, the \nlogical variables used in the naming are local to the Hoare triple. This makes a big difference from \nordinary .rst-order Hoare or Separation logic, where the global nature of logical variables makes such \na naming scheme and correspondingly, the use of large footprints a complete non-starter. But mostly, \nit is the pres\u00adence of separating conjunction *, which makes HTTcc capable of reasoning about heap disjointness \nwith the same ease inherent in Separation logic, irrespective of whether the annotations describe full \nor partial heaps. Speci.cation-only variables and implicit constructions Our callcc primitive is indexed \nby a speci.cation-only variable j, which binds the heap at the point of continuation capture. jshould \nbe used only in the assertions and proofs, but not in the executable parts of the callcc block. Unfortunately, \nCoq (and consequently HTTcc ) does not currently provide any means for enforcing this syntactic distinction. \nDeclaring variables such as jas speci.cation\u00adonly is natively supported by Coq*, an extension of Coq \nbased on the Implicit Calculus of Constructions [3]. In the future, we plan to explore embedding HTTcc \ninto Coq*, to make use of this feature. Higher-order heaps and semantic models for callcc Dreyer et al. \n[13] and St\u00f8vring and Lassen [41] develop semantics models and methods for equational reasoning in such \nmodels, for pro\u00adgrams with continuations and mutable store. A speci.c focus in both works is on higher-order \nheaps [26, 50, 39]; that is, the ability to store computations (and continuations as a special case) \ninto the heap. HTTcc s model is much simpler in this particular respect. While it allows programs that \nreturn continuations, and closures encapsulating continuations, it does not allow programs that store \nside-effectful computations into the heap. The reason is that we de\u00ad.ned SK and Kont types in terms of \nheap, rather than mutually recursively with heap, as required for stored computations. In the future, \nwe will develop a model for HTTcc with stored compu\u00adtations. We plan to build on the model for HTT by \nSvendsen et al. [43], which includes higher-order heaps, but no control opera\u00adtors. Our ultimate aim \nis to implement proper coroutines, following Reppy [35, cp. 10], and use them to verify concurrency primitives. \nMoreover, our inference of weakest pre-and strongest postcondi\u00adtions by the rules presented in Figure \n4 is also in the spirit of char\u00adacteristic formulae [32, 1, 8].  Hoare logics for higher-order control \nCrolard and Polonowski [10] have recently developed a Hoare logic for control operators, in which speci.cations \nare carried out in types. While in this re\u00adspect, the approach is similar to HTTcc from the high-level \npoint of view, there is a number of differences. For example, Crolard and Polonowski only consider mutable \nstack variables with block scope, but no pointers or aliasing. Procedures are not allowed to contain \nfree variables, and type dependencies contain .rst-order data only, such as natural numbers. In contrast, \nin HTTcc , we al\u00adlow the full expressiveness of CIC, including S-types over speci.\u00adcations, which, as \nwe illustrated, is required for specifying closures that return captured continuations. Berger [5] presents \na .rst-order Hoare logic for callcc in an otherwise purely functional language. One of the main features \nof the logic is the polarity distinction be\u00adtween the types of programs that perform jumps ( jumping-to \n) and the types of labels for jumps ( being-jumped-to ). From the point of view of reasoning, the logic \nallows nesting Hoare triples inside the assertions. This is necessary for specifying closures with cap\u00adtured \ncontinuations, and achieves the same effect as S-types over speci.cations in our dependently-typed setting. \nHoare reasoning through dependent types Related systems that employ Hoare-style speci.cation via types \nare HTT [30] and F* [44]. HTT is a direct precursor of HTTcc , but does not include the control operators. \nIt uses an embedding of (small footprint) Separation logic via monads into Coq to formulate annotations \nand discharge veri.cation conditions. A similar idea of Hoare mon\u00adads in Coq, without control operators, \nhas also been considered by Swierstra [45]. F* speci.es computations using a somewhat different monad \nfrom the above work. Instead of postconditions ranging over the input and output heaps, F* considers \npredicate transformers ranging over sets of input and output heaps. F* does not include a separate form \nof preconditions to specify safety; thus, its Hoare logic is not fault-avoiding as is HTTcc , or other \nsystems based on Separation logic. F* relies on Z3 for automatic discharge of veri.cation conditions. \nIn order to facilitate automation, its as\u00adsertion logic is a .rst-order fragment supported by Z3. F* \ndoes not consider callcc, or other abstractions required by it, such as S-types over speci.cations. CPS \ntranslation CPS translation in the case of dependent types has been studied by Barthe and Uustalu [4] \nwho show the impos\u00adsibility of CPS-translating dependent inductive and S-types. As HTTcc essentially \nrelies on S-types to encode nested Hoare triples as inhabitants of the S SK type it seems impossible \nto present HTTcc as a CPS translation into a callcc-free fragment of HTTcc . 8. Conclusions In this paper, \nwe have presented HTTcc , a higher-order type theory for veri.cation of programs with callcc control \noperators. HTTcc supports mutable state in the style of Separation logic, and, to the best of our knowledge, \nis the .rst Hoare logic or type theory to sup\u00adport the combination of higher-order functions, mutable \nstate and control operators. The support for mutable state comes with a twist, however. While our assertion \nlogic embeds separating conjunction *, we used large footprint speci.cation style, which we found nec\u00adessary \nto relate heaps captured with the continuation to heaps at the point of a jump. We use algebraic control \noperators, initially intro\u00adduced by Jaskelioff [22], which we here adapt to non-rollbackable state. We \nargue that in practice, the algebraic operators require less manual program annotations, than the non-algebraic \nvariants. We have implemented HTTcc as an embedding in Coq, and veri.ed a number of characteristic example \nprograms that use callcc [12]. Acknowledgements We thank the anonymous referees for their helpful feedback \nand comments on the paper. We also thank Mauro Jaskelioff for the fruitful discussions about algebraic \neffects. This research was partially supported by the Spanish MINECO projects TIN2010-20639 Paran10 and \nTIN2012-39391-C04-01 Strongsoft, AMAROUT grant PCOFUND-GA-2008-229599, and Ramon y Cajal grant RYC-2010-0743. \nReferences [1] ACETO, L., AND ING \u00b4 \u00b4 OLFSD OTTIR, A. Characteristic formulae: From automata to logic. \nBulletin of the EATCS 91 (2007). [2] ARBIB, M. A., AND ALAGIC, S. Proof rules for gotos. Acta Inf. 11 \n(1979). [3] BARRAS, B., AND BERNARDO, B. The implicit calculus of construc\u00adtions as a programming language \nwith dependent types. In FoSSaCS (2008). [4] BARTHE, G., AND UUSTALU, T. CPS translating inductive and \ncoinductive types. In PEPM (2002). [5] BERGER, M. Program logics for sequential higher-order control. \nIn FSEN (2009). [6] BERTOT, Y., AND CAST \u00b4 ERAN, P. Interactive Theorem Proving and Program Development. \nCoq Art: The Calculus of Inductive Construc\u00adtions. 2004. [7] BJ\u00d8RNER, D., AND JONES, C. B., Eds. The \nVienna Development Method: The Meta-Language (1978), vol. 61 of LNCS. \u00b4 gram Veri.cation. PhD thesis, \nUniversit\u00b4 [8] CHARGU ERAUD, A. Characteristic Formulae for Mechanized Pro\u00ad e Paris-Diderot, 2010. [9] \nCLINT, M. , AND HOARE, C. A. R. Program proving: Jumps and functions. Acta Inf. 1 (1972). [10] CROLARD, \nT. , AND POLONOWSKI, E. Deriving a Floyd-Hoare logic for non-local jumps from a formul\u00e6-as-types notion \nof control. J. Log. Algebr. Program. 81, 3 (2012). [11] DANVY, O., AND FILINSKI, A. Representing control: \nA study of the CPS transformation. MSCS 2, 4 (1992). [12] DELBIANCO, G. A., AND NANEVSKI, A. Supporting \nmaterial. http://software.imdea.org/ germand/HTTcc, March 2013. [13] DREYER, D., NEIS, G., AND BIRKEDAL, \nL. The impact of higher\u00adorder state and control effects on local relational reasoning. In ICFP (2010). \n[14] FELLEISEN, M., FRIEDMAN, D. P. , DUBA, B., AND MERRILL, J. Beyond Continuations. Tech. Rep. 216, \nIndiana University, 1987. [15] FELLEISEN, M., FRIEDMAN, D. P., KOHLBECKER, E. E. , AND DUBA, B. F. Reasoning \nwith continuations. In LICS (1986). [16] FELLEISEN, M., WAND, M., FRIEDMAN, D., AND DUBA, B. Ab\u00adstract \ncontinuations: a mathematical semantics for handling full jumps. In LISP and functional programming (1988). \n[17] FILINSKI, A. Representing monads. In POPL (1994). [18] FRIEDMAN, D. P., AND FELLEISEN, M. The Seasoned \nSchemer. 1996. [19] GONTHIER, G. , MAHBOUBI, A., AND TASSI, E. A Small Scale Re.ection Extension for \nthe Coq system. Tech. Rep. 6455, INRIA, 2008. [20] GRIFFI N, T. A formulae-as-types notion of control. \nIn POPL (1990). [21] HYLAND, M., LEVY, P. B., PLOTKIN, G. D., AND POWER, J. Combining algebraic effects \nwith continuations. TCS 375, 1-3 (2007). [22] JASKELIOFF, M. Modular monad transformers. In ESOP (2009). \n[23] JENSEN, J. B., BENTON, N., AND KENNEDY, A. High-level separa\u00adtion logic for low-level code. In POPL \n(2013). [24] KLEYMANN, T. Hoare logic and auxiliary variables. Formal Aspects of Computing 11 (1999). \n [25] KOWALTOWSKI, T. Axiomatic approach to side effects and general jumps. Acta Inf. 7 (1977). [26] \nKRISHNASWAMI, N. R. Verifying Higher-Order Imperative Pro\u00adgrams with Higher-Order Separation Logic. PhD \nthesis, Carnegie Mellon University, 2011. [27] THE COQ DEVELOPMENT TEAM. The Coq proof assistant reference \nmanual. TypiCal Project, 2012. Version 8.4. [28] MOGGI, E. Computational lambda-calculus and monads. \nIn LICS (1989). [29] NANEVSKI, A., MORRISETT, J. G., AND BIRKEDAL, L. Hoare type theory, polymorphism \nand separation. JFP 18, 5-6 (2008). [30] NANEVSKI, A., VAFEIADIS, V., AND BERDINE, J. Structuring the \nveri.cation of heap-manipulating programs. In POPL (2010). [31] O H EARN, P. W., REYNOLDS, J. C. , AND \nYANG, H. Local reason\u00ading about programs that alter data structures. In CSL (2001). [32] PARK, D. M. \nR. Concurrency and automata on in.nite sequences. In Theoretical Computer Science (1981). [33] PLOTKIN, \nG. D. , AND POWER, A. J. Computational effects and operations: An overview. ENTCS 73 (2004). [34] PLOTKIN, \nG. D., AND POWER, J. Algebraic operations and generic effects. Applied Categorical Structures 11, 1 (2003). \n[35] REPPY, J. H. Concurrent Programming in ML. Cambridge University Press, Cambridge, England, 1999. \n[36] REYNOLDS, J. C. The discoveries of continuations. LISP and Symbolic Computation 6, 3-4 (1993). [37] \nREYNOLDS, J. C. Separation logic: A logic for shared mutable data structures. In LICS (2002). [38] SAABAS, \nA., AND UUSTALU, T. A compositional natural semantics and Hoare logic for low-level languages. TCS 373, \n3 (2007). [39] SCHWINGHAMMER, J. , BIRKEDAL, L., REUS, B., AND YANG, H. Nested Hoare triples and frame \nrules for higher-order store. LMCS 7, 3 (2011). [40] SPRINGER, G., AND FRIEDMAN, D. P. Scheme and the \nArt of Programming. MIT Press and McGraw-Hill, 1989. [41] ST\u00d8VRING, K., AND LASSEN, S. B. A complete, \nco-inductive syntactic theory of sequential control and state. In POPL (2007). [42] STRACHEY, C., AND \nWADSWORTH, C. P. Continuations: A mathe\u00admatical semantics for handling full jumps. HOSC 13, 1/2 (2000). \n[43] SVENDSEN, K. , BIRKEDAL, L., AND NANEVSKI, A. Partiality, state and dependent types. In TLCA (2011). \n[44] SWAMY, N., WEINBERGER, J., SCHLESINGER, C., CHEN, J., AND LIVSHITS, B. Verifying higher-order programs \nwith the Dijkstra monad. In PLDI (2013). [45] SWIERSTRA, W. A hoare logic for the state monad. TPHOLs \n09. [46] TAN, G., AND APPEL, A. W. A compositional logic for control .ow. In VMCAI (2006). [47] THIELECKE, \nH. Categorical Structure of Continuation Passing Style. PhD thesis, University of Edimburgh, 1997. [48] \nTHIELECKE, H. Control effects as a modality. JFP 19, 1 (2009). [49] WADLER, P. Monads and composable \ncontinuations. LISP and Symbolic Computation 7, 1 (1994). [50] YOSHIDA, N., HONDA, K. , AND BERGER, M. \nLogical reasoning for higher-order functions with local state. LMCS 4, 4 (2008).  \n\t\t\t", "proc_id": "2500365", "abstract": "<p>Continuations are programming abstractions that allow for manipulating the \"future\" of a computation. Amongst their many applications, they enable implementing unstructured program flow through higher-order control operators such as callcc. In this paper we develop a Hoare-style logic for the verification of programs with higher-order control, in the presence of dynamic state. This is done by designing a dependent type theory with first class callcc and abort operators, where pre- and postconditions of programs are tracked through types. Our operators are algebraic in the sense of Plotkin and Power, and Jaskelioff, to reduce the annotation burden and enable verification by symbolic evaluation. We illustrate working with the logic by verifying a number of characteristic examples.</p>", "authors": [{"name": "Germ&#225;n Andr&#233;s Delbianco", "author_profile_id": "81548049183", "affiliation": "IMDEA Software Institute, Pozuelo de Alarc&#243;n, Spain", "person_id": "P4261283", "email_address": "german.delbianco@imdea.org", "orcid_id": ""}, {"name": "Aleksandar Nanevski", "author_profile_id": "81100503327", "affiliation": "IMDEA Software Institute, Pozuelo de Alarc&#243;n, Spain", "person_id": "P4261284", "email_address": "aleks.nanevski@imdea.org", "orcid_id": ""}], "doi_number": "10.1145/2500365.2500593", "year": "2013", "article_id": "2500593", "conference": "ICFP", "title": "Hoare-style reasoning with (algebraic) continuations", "url": "http://dl.acm.org/citation.cfm?id=2500593"}