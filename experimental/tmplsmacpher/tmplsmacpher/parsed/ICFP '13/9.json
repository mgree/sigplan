{"article_publication_date": "09-25-2013", "fulltext": "\n Fun with Semirings A functional pearl on the abuse of linear algebra Stephen Dolan Computer Laboratory, \nUniversity of Cambridge stephen.dolan@cl.cam.ac.uk Abstract Describing a problem using classical linear \nalgebra is a very well\u00adknown problem-solving technique. If your question can be formu\u00adlated as a question \nabout real or complex matrices, then the answer can often be found by standard techniques. It s less \nwell-known that very similar techniques still apply where instead of real or complex numbers we have \na closed semir\u00ading, which is a structure with some analogue of addition and multi\u00adplication that need \nnot support subtraction or division. We de.ne a typeclass in Haskell for describing closed semir\u00adings, \nand implement a few functions for manipulating matrices and polynomials over them. We then show how these \nfunctions can be used to calculate transitive closures, .nd shortest or longest or widest paths in a \ngraph, analyse the data .ow of imperative programs, optimally pack knapsacks, and perform discrete event \nsimulations, all by just providing an appropriate underlying closed semiring. Categories and Subject \nDescriptors D.1.1 [Programming Tech\u00adniques]: Applicative (Functional) Programming; G.2.2 [Discrete Mathematics]: \nGraph Theory graph algorithms Keywords closed semirings; transitive closure; linear systems; shortest \npaths 1. Introduction Linear algebra provides an incredibly powerful problem-solving toolbox. A great \nmany problems in computer graphics and vision, machine learning, signal processing and many other areas \ncan be solved by simply expressing the problem as a system of linear equations and solving using standard \ntechniques. Linear algebra is de.ned abstractly in terms of .elds, of which the real and complex numbers \nare the most familiar examples. Fields are sets equipped with some notion of addition and multi\u00adplication \nas well as negation and reciprocals. Many discrete mathematical structures commonly encountered in computer \nscience do not have sensible notions of negation. Booleans, sets, graphs, regular expressions, imperative \nprograms, datatypes and various other structures can all be given natural no\u00adtions of product (interpreted \nvariously as intersection, sequencing Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting \nwith credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, \nrequires prior speci.c permission and/or a fee. Request permissions from permissions@acm.org. ICFP 13, \nSeptember 25 27, 2013, Boston, MA, USA. Copyright is held by the owner/author(s). Publication rights \nlicensed to ACM. ACM 978-1-4503-2326-0/13/09. . . $15.00. http://dx.doi.org/10.1145/2500365.2500613 \nor conjunction) and sum (union, choice or disjunction), but gener\u00adally lack negation or reciprocals. \nSuch structures, having addition and multiplication (which dis\u00adtribute in the usual way) but not in general \nnegation or reciprocals, are called semirings. Many structures specifying sequential actions can be thought \nof as semirings, with multiplication as sequencing and addition as choice. The distributive law then \nstates, intuitively, a followed by a choice between b and c is the same as a choice between a followed \nby b and a followed by c. Plain semirings are a very weak structure. We can .nd many examples of them \nin the wild, but unlike .elds which provide the toolbox of linear algebra, there isn t much we can do \nwith something knowing only that it is a semiring. However, we can build some useful tools by introducing \nthe closed semiring, which is a semiring equipped with an extra opera\u00adtion called closure. With the intuition \nof multiplication as sequenc\u00ading and addition as choice, closure can be interpreted as iteration. As \nwe see in the following sections, it is possible to use something akin to Gaussian elimination on an \narbitrary closed semiring, giv\u00ading us a means of solving certain linear equations over any struc\u00adture \nwith suitable notions of sequencing, choice and iteration. First, though, we need to de.ne the notion \nof semiring more precisely. 2. Semirings We de.ne a semiring formally as consisting of a set R, two distin\u00adguished \nelements of R named 0 and 1, and two binary operations + and \u00b7, satisfying the following relations for \nany a, b, c . R: a + b = b + a a + (b + c) = (a + b) + c a + 0 = a a \u00b7 (b \u00b7 c) = (a \u00b7 b) \u00b7 c a \u00b7 0 = \n0 \u00b7 a = 0 a \u00b7 1 = 1 \u00b7 a = a a \u00b7 (b + c) = a \u00b7 b + a \u00b7 c (a + b) \u00b7 c = a \u00b7 c + b \u00b7 c We often write a \n\u00b7 b as ab, and a \u00b7 a \u00b7 a as a 3 . Our focus will be on closed semirings [12], which are semir\u00adings with \nan additional operation called closure (denoted *) which satis.es the axiom: * ** a = 1 + a \u00b7 a = 1 + \na \u00b7 a If we have an af.ne map x . ax + b in some closed semiring, * *** then x = a b is a .xpoint, since \na b = (aa + 1)b = a(a b) + b. So, a closed semiring can also be thought of as a semiring where af.ne \nmaps have .xpoints. The de.nition of a semiring translates neatly to Haskell:  infixl 9 @. infixl 8 \n@+ class Semiring r where zero, one :: r closure :: r -> r (@+), (@.) :: r -> r -> r There are many useful \nexamples of closed semirings, the sim\u00adplest of which is the Boolean datatype: instance Semiring Bool \nwhere zero = False one = True closure x = True (@+) = (||) (@.) = (&#38;&#38;) It is straightforward \nto show that the semiring axioms are satis\u00ad.ed by this de.nition. In semirings where summing an in.nite \nseries makes sense, we can de.ne a * as: 2 3 1 + a + a + a + . . . since this series satis.es the axiom \na * = 1 + a \u00b7 a *. In other semirings where subtraction and reciprocals make sense we can de.ne a * as \n(1 - a)-1. Both of these viewpoints will be useful to describe certain semirings. The real numbers form \na semiring with the usual addition and multiplication, where a * = (1 - a)-1. Under this de.nition, 1 \n* is unde.ned, an annoyance which can be remedied by adding an extra element 8 to the semiring, and setting \n1 * = 8. The regular languages form a closed semiring where \u00b7 is con\u00adcatenation, + is union, and * is \nthe Kleene star. Here the in.nite geometric series interpretation of * is the most natural: a * is the \nunion of a n for all n. 3. Matrices and reachability Given a directed graph G of n nodes, we can construct \nits adjacency matrix M, which is an n \u00d7 n matrix of Booleans where Mij is true if there is an edge from \ni to j. We can add such matrices. Using the Boolean semiring s de.\u00adnition of addition (i.e. disjunction), \nthe effect of this is to take the union of two sets of edges. Similarly, we de.ne matrix multiplication \nin the usual way, e where (AB)ij = k Aik \u00b7 Bkj . The product of two Boolean matrices A, B is thus true \nat indices ij if there exists any index k such that Aik and Bkj are both true. In particular, (M2)ij \nis true if there is a path with two edges in G from node i to node j. In general, Mk represents the paths \nof k edges in the graph G. A node j is reachable from a node i if there is a path with any number of \nedges (including 0) from i to j. This reachability relation can therefore be described by the following, \nwhere I is the identity matrix: 2 3 I + M + M + M + . . . This looks like the in.nite series de.nition \nof closure from above. Indeed, suppose we could calculate the closure of M, that is, a matrix M * such \nthat: M * = I + M \u00b7 M * M * would include the paths of length 0 (the I term), and would be transitively \nclosed (the M \u00b7 M * term). So, if we can show that n \u00d7 n matrices of Booleans form a closed semiring, \nthen we can use the closure operation to calculate reachability in a graph, or equivalently the re.exive \ntransitive closure of a graph. Remarkably, for any closed semiring R, the n \u00d7 n matrices of elements \nof R form a closed semiring. This is a surprisingly powerful result: as we see in the following sections, \nthe closure operation can be used to solve several different problems with a suitable choice of the semiring \nR. We de.ne addition and multiplication of n \u00d7 n matrices in the usual way, where: (A + B)ij = Aij + \nBij n (A \u00b7 B)ij = Aik \u00b7 Bkj k=1 The matrix 0 is the n \u00d7 n matrix where every element is the under\u00adlying \nsemiring s 0, and the matrix 1 has the underlying semiring s 1 along the main diagonal (so 1ii = 1) and \n0 elsewhere. In Haskell, we use the type Matrix, which represents a matrix as a list of rows, each a \nlist of elements, with a special case for the representation of scalar matrices (matrices which are zero \neverywhere but the main diagonal, and equal at all points along the diagonal). This special case allows \nus to de.ne matrices zero and one without knowing the size of the matrix. data Matrix a = Scalar a | \nMatrix [[a]] To add a scalar to a matrix, we need to be able to move along the main diagonal of the matrix. \nTo make this easier, we introduce some helper functions for dealing with block matrices. A block matrix \nis a matrix that has been partitioned into several smaller matrices. We de.ne a type for matrices that \nhave been partitioned into four blocks: type BlockMatrix a = (Matrix a, Matrix a, Matrix a, Matrix a) \nIf a, b, c and d represent the n \u00d7 n matrices A, B, C, D, then BlockMatrix (a,b,c,d) represents the 2n \n\u00d7 2n block matrix: A B C D Joining the components of a block matrix into a single matrix is straightforward: \nmjoin :: BlockMatrix a -> Matrix a mjoin (Matrix a, Matrix b, Matrix c, Matrix d) = Matrix ((a hcat b) \n++ (c hcat d)) where hcat = zipWith (++) For any n \u00d7 m matrix where n, m = 2, we can split the matrix \ninto a block matrix by peeling off the .rst row and column: msplit :: Matrix a -> BlockMatrix a msplit \n(Matrix (row:rows)) = (Matrix [[first]], Matrix [top], Matrix left, Matrix rest) where (first:top) = \nrow (left, rest) = unzip (map (\\(x:xs) -> ([x],xs)) rows) Armed with these, we can start implementing \na Semiring in\u00adstance for Matrix. instance Semiring a => Semiring (Matrix a) where zero = Scalar zero \none = Scalar one Scalar a @+ Scalar b = Scalar (a @+ b) Matrix a @+ Matrix b = Matrix (zipWith (zipWith \n(@+)) a b) Scalar s @+ m = m @+ Scalar s Matrix [[a]] @+ Scalar b = Matrix [[a @+ b]]  m @+ s = mjoin \n(first @+ s, top, left, rest @+ s) where (first, top, left, rest) = msplit m Scalar a @. Scalar b = \nScalar (a @. b) Scalar a @. Matrix b = Matrix (map (map (a @.)) b) Matrix a @. Scalar b = Matrix (map \n(map (@. b)) a) Matrix a @. Matrix b = Matrix [[foldl1 (@+) (zipWith (@.) row col) | col <-cols] | row \n<-a] where cols = transpose b De.ning closure for matrices is trickier. Lehmann [12] gave a de.nition \nof M * for an arbitrary matrix M which satis.es the axioms of a closed semiring, and two algorithms for \ncalculating it. The .rst of these generalises the Floyd-Warshall algorithm for all\u00adpairs shortest paths \n[6], while the second is a semiring-.avoured form of Gaussian elimination. Both are speci.ed imperatively, \nvia indexing and mutation of matrices represented as arrays. However, an elegant functional im\u00adplementation \ncan be derived almost directly from a lemma used to prove the correctness of the imperative algorithms. \nGiven a block matrix A B M = C D Lehmann shows that its closure M * will satisfy A * + B'. * C' B'. * \nM * = . * C' . * where B' = A * B, C' = CA * and . = D A * B. The closure + +C C * * of a 1 \u00d7 1 matrix \nis easily calculated since a = a , so this leads directly to an implementation of closure for matrices: \nclosure (Matrix [[x]]) = Matrix [[closure x]] closure m = mjoin (first @+ top @. rest @. left , top @. \nrest , rest @. left , rest ) where (first, top, left, rest) = msplit m first = closure first top = first \n@. top left = left @. first rest = closure (rest @+ left @. top) Multiplying a p \u00d7 q matrix by a q \u00d7 \nr matrix takes O(pqr) operations from the underlying semiring. The closure function, when given a n \u00d7 \nn matrix, does O(n 2) semiring operations via matrix multiplication (by multiplying 1 \u00d7 n and n \u00d7 n matrices, \nor n \u00d7 1 and 1 \u00d7 n), plus O(n 2) semiring operations via matrix addition and msplit, plus one recursive \ncall. The recursive call to closure is passed a (n - 1) \u00d7 (n - 1) matrix, and so the total number of \nsemiring operations done by closure for an n \u00d7 n matrix is O(n 3). Thus, closure has the same complexity \nas calculating transitive closure using the Floyd-Warshall algorithm. However, since it processes the \nentire graph and always pro\u00adduces an answer for all pairs of nodes, it is slower than standard al\u00adgorithms \nfor checking reachability between a single pair of nodes. 4. Graphs and paths We ve already seen that \nthe re.exive transitive closure of a graph can be found using the above closure function, but it seems \nlike a lot of work just to de.ne reachability! However, choosing a richer underlying semiring allows \nus to calculate more interesting properties of the graph, all with the same closure algorithm. The tropical \nsemiring (more sensibly known as the min-plus semiring) has as its underlying set the nonnegative integers \naug\u00admented with an extra element 8, and de.nes its + and \u00b7 opera\u00adtors as min and addition respectively. \nThis semiring describes the length of the shortest path in a graph: ab is interpreted as a path through \na and then b (so we sum the distances), and a + b is a choice between a path through a and a path through \nb (so we pick the shorter one). We express this in Haskell as follows, using the value Unreachable to \nrepresent 8: data ShortestDistance = Distance Int | Unreachable instance Semiring ShortestDistance where \nzero = Unreachable one = Distance 0 closure x = one x @+ Unreachable = x Unreachable @+ x = x Distance \na @+ Distance b = Distance (min a b) x @. Unreachable = Unreachable Unreachable @. x = Unreachable Distance \na @. Distance b = Distance (a + b) For a directed graph with edge lengths, we make a matrix M such that \nMij is the length of the edge from i to j, or Unreachable if there is none. M is represented in Haskell \nwith the type Matrix ShortestDistance, and calling closure cal\u00adculates the length of the shortest path \nbetween any two nodes. To see this, we can appeal again to the in.nite series view of closure: (Mk)ij \nis the length of the shortest path with k edges from node i to node j, and M * is the sum (which in this \nsemiring means minimum ) of Mk for any k. Thus, (M * )ij is the length of the shortest path with any \nnumber of edges from node i to node j. Often we re interested in .nding the actual shortest path, not \njust its length. We can de.ne another semiring that keeps track of this data, where paths are represented \nas lists of edges, each represented as a pair of nodes. There may not be a unique shortest path. If we \nare faced with a choice between two equally short paths, we must either have some means of disambiguating \nthem or be prepared to return multiple results. In the following implementation, we choose the former: \nwe assume nodes are ordered and choose the lexicographically least of multiple equally short paths. data \nShortestPath n = Path Int [(n,n)] | NoPath instance Ord n => Semiring (ShortestPath n) where zero = NoPath \none = Path 0 [] closure x = one x @+ NoPath = x NoPath @+ x = x Path a p @+ Path a p | a < a = Path a \np | a == a &#38;&#38; p < p = Path a p | otherwise = Path a p x @. NoPath = NoPath NoPath @. x = NoPath \nPath a p @. Path a p = Path (a + a ) (p ++ p ) The @. operator given here isn t especially fast since \n++ takes time linear in the length of its left argument, but this can be avoided by using an alternative \ndata structure with constant-time appends such as difference lists. We construct the matrix M, where \nMij is Path d [(i,j)] if there s an edge of length d between nodes i and j or NoPath if there s none. \nCalculating M * in this semiring will calculate not only the length of the shortest path between all \npairs of nodes, but give the actual route.  To calculate longest paths we can use a similar construction. \nWe have to be slightly more careful here, because a graph with cycles contains arbitrarily long paths. \nAs well as nonnegative integer distances, we have two other possible values: LUnreachable, indicating \nthat there is no path between two nodes, and LInfinite, indicating that there s an in.nitely long path \ndue to a cycle of positive length. This forms a semiring as shown: data LongestDistance = LDistance Int \n| LUnreachable | LInfinite instance Semiring LongestDistance where zero = LUnreachable one = LDistance \n0 closure LUnreachable = LDistance 0 closure (LDistance 0) = LDistance 0 closure _ = LInfinite x @+ LUnreachable \n= x LUnreachable @+ x = x LInfinite @+ _ = LInfinite _ @+ LInfinite = LInfinite LDistance x @+ LDistance \ny = LDistance (max x y) x @. LUnreachable = LUnreachable LUnreachable @. x = LUnreachable LInfinite \n@. _ = LInfinite _ @. LInfinite = LInfinite LDistance x @. LDistance y = LDistance (x + y) We can .nd \nthe widest path (also known as the highest-capacity path) by using min instead of addition as semiring \nmultiplication (to pick the narrowest of successive edges in a path). By working with real numbers as \nedge weights, interpreted as the probability of failure of a given edge, we can calculate the most reliable \npath. There is an intuition for closure for an arbitrary graph and an arbitrary semiring. Each edge of \nthe graph is assigned an element of the semiring, which make up the elements of the matrix M. Any path \n(sequence of edges) is assigned the product of the elements on each edge, and M * is the sum of the products \nassigned to ij every path from i to j. The fact that product distributes over sum means we can calculate \nthis, using the above closure algorithm, in polynomial time. We can use this intuition to construct powerful \ngraph analyses, simply by making an appropriate semiring and calculating the closure of a graph s adjacency \nmatrix. For instance, we can make a semiring of subsets of nodes of a graph, where + is intersection \nand \u00b7 is union. We set Mij = {i, j }, and calculate M *. Each path is assigned the set of nodes visited \nalong that path, and taking the sum over all paths calculates the intersection of those sets, or the \nnodes visited along all paths. Thus, M * ij gives the set of nodes that are visited along all paths from \ni to j, or in other words, the graph dominators of j with start node i. 5. Linear equations and regular \nlanguages One of the sharpest tools in the toolbox of linear algebra is the use of matrix techniques \nto solve systems of linear equations. Since we get to specify the semiring, we de.ne what linear means. \nMany problems can then be described as systems of linear equations, even though they re far from linear \nin the classical sense. Suppose we have a system of equations in some semiring on a set of variables \nx1, . . . , xn, where each xi is de.ned by a linear combination of the variables and a constant term. \nThat is, each  x . . . . . . . . y A 0 x 0 A 0 .B. = .y 0 z. .B.+ .0. C 0 0 0 C 1 z Figure 1. A .nite \nstate machine and its matrix representation equation is of the following form, where aij and bi are givens: \nxi = ai1x1 + ai2x2 + \u00b7 \u00b7 \u00b7 + bi We arrange the unknowns xi into a column vector X, the coef\u00ad.cients A \ninto a square matrix A and the constants b into a column vector B. The system of equations now becomes: \nX = AX + B This equation de.nes X as the .xpoint of an af.ne map. As we saw in section 2, it therefore \nhas a solution X = A * B, which can be calculated with our de.nition of closure for matrices. The above \nwas a little cavalier with matrix dimensions. Techni\u00adcally, our machinery for solving such equations \nis only de.ned for n \u00d7 n matrices, not column vectors. However, we can extend the column vectors X and \nB to n \u00d7 n matrices by making a matrix all of whose columns are equal. Solving the equation with such \nmatri\u00adces comes to the same answer as using column vectors directly, so we keep working with column vectors. \nHappily, our Haskell code for manipulating matrices accepts column and row vectors without problems, \nas long as we don t try to calculate the closure of any\u00adthing but a square matrix. If we have some system \nthat maps input to output, where the mapping can be described as a linear map X . AX + B, then the .xpoint \nX = A * B gives a stable state of the system. As we see below, Kleene s proof that all .nite state machines \naccept a regular language and the McNaughton-Yamada algo\u00adrithm [15] for constructing a regular expression \nto describe a state machine can also be described as such linear systems. Given a description of a .nite \nstate machine, we can write down a regular grammar describing the language it accepts. For every x transition \nqA -. qB , we have a grammar production A . xB, and for every accepting state qA we have a production \nA . e. We can group these productions by their left-hand sides to give a system of equations. For instance, \nin the machine of Figure 1 there is a state qB with transitions qB -y-z . qA and qB . qC , so we get \nthe equation: B = yA + zC In the semiring of regular languages (where addition is union, multiplication \nis concatenation, and closure is the Kleene star), these are all linear equations. For an n-state machine, \nwe de.ne the n \u00d7 n matrix M where Mij is the symbol on the transition from the ith state to the jth, \nor 0 (the empty language) if there is no such transition. The vector A is constructed by setting Ai to \n1 (the language containing only e) when the ith state is accepting, and 0 otherwise. The languages are \nrepresented by the vector of unknowns L, where Li is the language accepted starting in the ith state. \nFor our example machine, M and A are shown at the right of Figure 1. Then, the regular grammar is described \nby: L = M \u00b7 L + A  This equation has a solution given by L = M * \u00b7 A. We can use our existing closure \nfunction to solve these equations and build a regular expression that describes the language accepted \nby the .nite state machine. In Haskell, we de.ne a free semiring which simply records the syntax tree \nof semiring expressions. To qualify as a semiring we must have a @+ b == b @+ a, a @. one == a, and so \non. We sidestep this by cheating: we don t de.ne an Eq instance for FreeSemiring, and consider two FreeSemiring \nvalues equal if they are equal according to the semiring laws. However, to make our FreeSemiring values \nmore compact, we do implement certain simpli.cations like 0x = 0. data FreeSemiring gen = Zero | One \n| Gen gen | Closure (FreeSemiring gen) | (FreeSemiring gen) :@+ (FreeSemiring gen) | (FreeSemiring gen) \n:@. (FreeSemiring gen) instance Semiring (FreeSemiring gen) where zero = Zero one = One Zero @+ x = x \nx @+ Zero = x x @+ y = x :@+ y Zero @. x = Zero x @. Zero = Zero One @. x = x x @. One = x x @. y = x \n:@. y closure Zero = One closure x = Closure x If we construct M as a Matrix (FreeSemiring Char), then \ncalculating M * \u00b7 A will give us a vector of FreeSemiring Char, each element of which can be interpreted \nas a regular expression describing the language accepted from a particular state. For the example of \nFigure 1, closure then tells us that the lan\u00adguage accepted with state A as the starting state is x(yx) \n* z. This algorithm produces a regular expression that accurately describes the language accepted by \na given state machine, but it is not in gen\u00aderal the shortest such expression. 6. Data.ow analysis Many \nprogram analyses and optimisations can be computed by data.ow analysis. As an example, we consider the \nclassical live\u00adness analysis, which computes which assignments in an impera\u00adtive program assign values \nwhich will never be read ( dead ), and which ones may be used again ( live ). We construct the program \ns control .ow graph by dividing it into control-free basic blocks and with edges indicating where there \nare jumps between blocks. For a given basic block b, the set of variables live at the start of the block \n(INb) are those used by the basic block itself (USEb) before their .rst de.nition, and those which are \nlive at the end of the block (OUTb) but not assigned a new value during the block (DEFb). The variables \nlive at the end of a basic block are those live at the start of any successor. This gives a system of \nequations: INb = (OUTb n DEFb) . USEb  OUTb = INb1 b1.succ(b) x := 1 A z := 0 while x < y: B x := x \n* 2 C z := z + 1 return x D x := x * 2 z := z + 1 Figure 2. A simple imperative program to calculate \nthe smallest power of two greater than the input y, and its control .ow graph. The variable z does not \naffect the output. An example program is given in Figure 2, where DEF and USE are as follows: DEFA = \n{x, z} USEA = \u00d8 DEFB = \u00d8 USEB = {x, y} DEFC = {x, z} USEC = {x, z} DEFD = \u00d8 USED = {x} If we solve for \nINb and OUTb, we .nd that z is not live upon entry to D (that is, z ./IND). However, z is considered \nlive on entry to C, even though it is never affects the output of the program. We see how to remedy this \nusing faint variables analysis below, but .rst we show how the classical live variables analysis can \nbe calculated using our semiring machinery. We de.ne a semiring of sets of variables, where 0 is the \nempty set, 1 is the set of all variables in the program, + is union, \u00b7 is intersection, and x * = 1 for \nall sets x. Our system of equations can be represented as follows in this semiring: OUTb = INb1 b1.succ(b) \n= DEFb1 \u00b7 OUTb1 + USEb1 b1.succ(b) . ... = . DEFb1 \u00b7 OUTb1 . + . USEb1 . b1.succ(b) b1.succ(b) This is \na system of af.ne equations over the variables OUTb, e with coef.cients DEFb1 and constant terms USEb1 \n. b1.succ(b) As before, we can solve it by building a matrix M containing the coef.cients and a column \nvector A containing the constant terms. The solution vector OU Tb is given by M * \u00b7 A, using the same \nclosure algorithm. Data.ow analyses can be treated more generally by studying the transfer functions \nof each basic block. We consider backwards analyses (like liveness analysis), where the transfer functions \nspec\u00adify INb given OUTb (the discussion applies equally well to for\u00adwards analyses, with suitable relabelling). \nThe equations have the following form: INb = fb(OUTb) OUTb = join INb1 b1.succ(b) or, more compactly, \nOUTb = join f(OUTb1 ) b1.succ(b) For many standard analyses, we can de.ne a semiring where fb is linear, \nand join is summation. This semiring is often the semiring of sets of variables (as above) or expressions, \nor its dual where + is intersection, \u00b7 is union, 0 is the entire set of variables and 1 is the empty \nset.  Such analyses are often referred to as bit-vector problems [11], as the sets can be represented \nef.ciently using bitwise operations. The available expressions analysis can be expressed as a linear \nsystem using this latter semiring, where the set of available expres\u00adsions at the start of a block is \nthe intersection of the sets of available expressions at the end of each predecessor. Other analyses \ndon t have such a simple structure. The faint variables analysis is an extension of live variables analysis \nwhich can detect that certain assignments are useless even though they are considered live by the standard \nanalysis. For instance, in Figure 2, the variable z was considered live at the start of block C, even \nthough all statements involving z can be deleted without affecting the meaning of the program. Live variable \nanalysis will consider x live since it may be used on the next iteration. Faint variable analysis .nds \nthe strongly live variables: those used to compute a value which is not dead. We can write transfer functions \nfor faint variables analysis, which when given OUTb compute INb. For instance, in our ex\u00adample x . INC \nif x . OUTC (since x is used to compute the new value of x), and similarly z . INC if z . OUTC . These \ntransfer functions don t fall into the class of bitvector problems, and so our previous tactic won t \nwork directly. However, they are in the more general class of distributive data.ow prob\u00adlems [10]: they \nhave the property that fb(A.B) = fb(A).fb(B). Happily, such transfer functions form a semiring. We de.ne \na datatype to describe such functions which distribute over set union. For more generality, we consider \nan arbitrary com\u00admutative monoid instead of limiting ourselves to set union. Haskell has a standard de.nition \nof monoids, but they are not generally required to be commutative. We de.ne the typeclass Commutative-Monoid \nfor those monoids which are commutative. It has no meth\u00adods and instances are trivial; it serves only \nas a marker by which the programmer can certify his monoid does commute. class Monoid m => CommutativeMonoid \nm instance Ord a => CommutativeMonoid (Set a) With that done, we can de.ne the semiring of transfer functions: \nnewtype Transfer m = Transfer (m -> m) instance (Eq m, CommutativeMonoid m) => Semiring (Transfer m) \nwhere zero = Transfer (const mempty) one = Transfer id Transfer f @+ Transfer g = Transfer (\\x -> f x \nmappend g x) Transfer f @. Transfer g = Transfer (f . g) Multiplication in this semiring is composition \nand addition is pointwise mappend, which is union in the case of sets. The dis\u00adtributive law is satis.ed \nassuming all of the transfer functions them\u00adselves distribute over mappend. The closure of a transfer \nfunction is a function f * such that f * = 1 + f \u00b7 f *. When applied to an argument, we expect that f \n* * (x) = x + f(f (x)). The closure can be de.ned as a .xpoint iteration, which will give a suitable \nanswer if it converges: closure (Transfer f) = Transfer (\\x -> fixpoint (\\y -> x mappend f y) x) where \nfixpoint f init = if init == next then init else fixpoint f next where next = f init Convergence of this \n.xpoint iteration is not automatically guar\u00adanteed. However, it always converges when the transfer functions \nand the monoid operation are monotonic increasing in an order of .nite height (such as the set of variables \nin a program), so this gives us a valid de.nition of closure for our transfer functions. We can then \ncalculate M *, where M is the matrix of basic block transfer functions. M * gives us their transitive \nclosure , which are the transfer functions of the program as a whole. Calling these functions with a \ntrivial input (say, the empty set of variables in the case of faint variable analysis) allows us to generate \nthe solution to the data.ow equations. 7. Polynomials, power series and knapsacks Given any semiring \nR, we can de.ne the semiring of polynomials in one variable x whose coef.cients are drawn from R, which \nis written R[x]. We represent polynomials as a list of coef.cients, where the ith element of the list \nrepresents the coef.cient of x i . Thus, 3 + 4x 2 is represented as the list [3, 0, 4]. We can start \nde.ning an instance of Semiring for such lists. The zero and unit polynomials are given by (where the \none on the right-hand-side refers to r s one): instance Semiring r => Semiring [r] where zero = [] one \n= [one] Addition is fairly straightforward: we add corresponding co\u00adef.cients. If one list is shorter \nthan the other, it s considered to be padding with zeros to the correct length (since 1 + 2x = 1 + 2x \n+ 0x 2 + 0x 3 + . . . ). [] @+ y = y x @+ [] = x (x:xs) @+ (y:ys) = (x @+ y):(xs @+ ys) The head of the \nlist representation of a polynomial is the con\u00adstant term, and the tail is the sum of all terms divisible \nby x. So, the Haskell value a:p corresponds to the polynomial a + px, where p is itself a polynomial. \nMultiplying two of these gives us: (a + px)(b + qx) = ab + (aq + pb + pqx)x This directly translates \ninto an implementation of polynomial mul\u00adtiplication: [] @. _ = [] _ @. [] = [] (a:p) @. (b:q) = (a @. \nb):(map (a @.) q @+ map (@. b) p @+ (zero:(p @. q))) If we multiply a polynomial with coef.cients ai \n(that is, the e polynomial i aix i) by one with coef.cients bi resulting in the polynomial with coef.cients \nci, then the coef.cients are related by: n cn = aibn-i i=0 This is the discrete convolution of two sequences. \nOur de.nition of @. is in fact a pleasantly index-free de.nition of convolution. In order to give a valid \nde.nition of Semiring, we must de.ne the operation s * such that s * = 1 + s \u00b7 s *. This seems impossible: \nfor instance, there is no polynomial p such that p = 1 + xp, since the degrees of both sides don t match. \nTo form a closed semiring, we need to generalise somewhat and consider not just polynomials, but arbitrary \nformal power series, which are polynomials which may be in.nite, giving us the semir\u00ading R[[x]]. Our \npower series are purely formal, representing sequences of elements from a closed semiring. We have no \nnotion of evaluat\u00ading such a series by specifying x. We think of the formal power series 1 + x + x 2 \n+ x 3 . . . as the sequence 1, 1, 1, . . . , and require no in.nite sums, limits or convergence. As such, \nmultiplication by x simply means shifting the series by one place , and we can write pxq = pqx (but not \npqx = qpx) even when the underlying semiring does not have commutative multiplication.  Since Haskell \ns lists are lazily de.ned and may be in.nite, our existing de.nitions for addition and multiplication \nwork for such power series, as demonstrated by McIlroy in his functional pearl [14]. Given a power series \ns = a + px, we seek its closure s * = b + qx, such that s * = 1 + s \u00b7 s * : b + qx = 1 + (a + px)(b + \nqx) = 1 + ab + aqx + p(b + qx)x The constant terms must satisfy b = 1 + ab, so a solution is given by \nb = a *. The other terms must satisfy q = aq + ps *. This states that q is the .xpoint of an af.ne map, \nso a solution is given ** * by q = a ps and thus s = a * (1 + ps * ). This translates neatly into lazy \nHaskell as follows: closure [] = one closure (a:p) = r where r = [closure a] @. (one:(p @. r)) This allows \nus to solve af.ne equations of the form x = bx + c, where the unknown x and the parameters b and c are \nall power series over an arbitrary semiring. This form of problem arises naturally in some dynamic programming \nproblems. As an example, we consider the unbounded integer knapsack problem. We are given n types of \nitem, with nonnegative integer weights w1, . . . , wn and nonnegative integer values v1, . . . , vn. \nOur knap\u00adsack can only hold a weight W , and we want to .nd out the maxi\u00admal value that we can .t in \nthe knapsack by choosing some number of each item, while remaining below or equal to the weight limit \nW . This problem is NP-hard, but admits an algorithm with complexity polynomial in W (this is referred \nto as a pseudo-polynomial time algorithm since in general W can be exponentially larger than the description \nof the problem). The algorithm calculates a table t, where t(w) is the maximum value possible within \na weight limit w. We set t(0) to 0, and for all other w: t(w) = max (vi + t(w - wi)) 0=wi=w This expresses \nthat the optimal packing of the knapsack to weight w can be found by looking at all of the elements which \ncould be inserted and choosing the one that gives the highest value. The algebraic structure of this \nalgorithm becomes more clear when we rewrite it using the max-plus semiring that we earlier used to calculate \nlongest paths, which we implemented in Haskell as LongestDistance. Confusingly, in this semiring the \nunit ele\u00adment is the number 0, since that is the identity of the semiring s multiplication, which is \naddition of path lengths. The zero element of this semiring is 8, which is the identity of max. We take \nvi and t(w) to be elements of this semiring. Then, in this semiring s notation, t(0) = 1 w t(w) = vi \n\u00b7 t(w - wi) i=0 We can combine the two parameters vi and wi into a single e polynomial V = i vix wi . \nFor example, suppose we .ll our knapsack with four types of coin, of values 1, 5, 7 and 10 and weights \n3, 6, 8 and 6 respectively. The polynomial V is given by: 368 6 V = x + 5x + 7x + 10x Since we are using \nthe max-plus semiring, this is equivalent to: 3 68 V = x + 10x + 7x Represented as a list, the wth element \nof V is the value of the most valuable item with weight w (which is zero if there are no items of weight \nw). Similarly, we represent t(w) as the power series e T = i t(i)x i. The list representation of T has \nas its wth element the maximal value possible within a weight limit w. We can now see that the de.nition \nof t(w) above is in fact the convolution of T and V . Together with the base case, that t(0) is the semiring \ns unit, this gives us a simpler de.nition of t(w): T = 1 + V \u00b7 T The above can equally be written as \nT = V *, and so we get the following elegant solution to the unbounded integer knapsack problem (where \n!! is Haskell s list indexing operator): knapsack values maxweight = closure values !! maxweight Note \nthat our previous intuition of x * being the in.nite sum 1 + x + x 2 + . . . applies nicely here: the \nsolution to the integer knapsack problem is the maximum value attainable by choosing no items, or one \nitem, or two items, and so on for any number of items. Instead of using the LongestDistance semiring, \nwe can de.ne LongestPath in the same way that we de.ned ShortestPath above, with max in place of min. \nUsing this semiring, our above de.nition of knapsack still works and gives the set of elements chosen \nfor the knapsack, rather than just their total value. 8. Linear recurrences and Petri nets The power \nseries semiring has another general application: it can express linear recurrences between variables. \nSince the de.nition of linear can be drawn from an arbitrary semiring, this is quite an expressive notion. \nAs we are discussing functional programming, we are obliged by tradition to calculate the Fibonacci sequence. \nThe nth term of the Fibonacci sequence is given by the sum of the previous two terms. We construct the \nformal power series F whose nth coef.cient is the nth Fibonacci number. Multiplying this sequence by \nx k shifts along by k places, so we can rewrite the recurrence relation as: 2 1 + xF + x F = F This de.nes \nF = 1 + (x + x 2)F , and so F = (x + x 2) *. So, we can calculate the Fibonacci sequence as closure [0,1,1]. \nThere are of course much more interesting things that can be described as linear recurrences and thus \nas formal power series. Cohen et al. [4] showed that a class of Petri nets known as timed event graphs \ncan be described by linear recurrences in the max\u00adplus semiring (the one we previously used for longest \npaths and knapsacks). A timed event graph consists of a set of places and a set of transitions, and a \ncollection of directed edges between places and transitions. Atomic, indistinguishable tokens are consumed \nand produced by transitions and held in places. In a timed event graph, unlike a general Petri net, each \nplace has exactly one input transition and exactly one output transition, as well as a nonnegative integer \ndelay, which represents the processing time at that place. When a token enters a place, it is not eligible \nto leave until the delay has elapsed. When all of the input places of a transition have at least one \ntoken ready to leave, the transition .res . One token is removed from each input place, and one token \nis added to each output place of the transition. For simplicity, we assume that transitions are instant, \nand that a token arrives at all of the output places of a transition as soon as one is ready to leave \neach of the input places. If desired, transition processing times can be simulated by adding extra places. \n t1 C (0) A (1) B (1) D (0) E (3) t2 t3 t4 Figure 3. A timed event graph with four transitions t1, \nt2, t3, t4 and .ve places A, B, C, D, E with delays in parentheses, where all but two of the places are \ninitially empty. In Figure 3, the only transition which is ready to .re at time 0 is t1. When it .res, \nit removes a token from B and adds one to A. This makes transition t2 .re at time 1, which adds a token \nto B causing t1 to .re at time 2, then t2 to .re at time 3 and so on. When t2 .res at times 1, 3, 5, \n. . . , a token is added to place D. The .rst three times this happens, t3 .res, but after that the supply \nof tokens from C is depleted. t4 .res after the tokens have waited in E for a delay of three steps, so \nt4 .res at times 4, 6 and 8. Simulating such a timed event graph requires calculating the times at which \ntokens arrive and leave each place. For each place p, we de.ne the sequences IN(p) and OUT(p). The ith \nelement of IN(p) is the time at which a token arrives into p for the ith time. The ith element of OUT(p) \nis the time at which a token becomes available from p for the ith time, which may be some time before \nit actually leaves p. In the example of Figure 3, we have: IN(A) = 0, 2, 4, 6 . . . OUT(A) = 1, 3, 5, \n7 . . . IN(B) = 1, 3, 5, 7 . . . OUT(B) = 0, 2, 4, 6 . . . IN(C) = OUT(C) = 0, 0, 0 IN(D) = 1, 3, 5, \n7 . . . OUT(D) = 1, 3, 5, 7 . . . IN(E) = 1, 3, 5 OUT(E) = 4, 6, 8 We say that a place p ' is a predecessor \nof p (and write p ' . pred(p)) if the output transition of p ' is the input transition of p. Since transitions \n.re instantly, a place receives a token as soon as all of its predecessors are ready to produce one. \nIN(p)i = max OUT(p ' )i p1.pred(p) Exactly when the ith token becomes available from a place p depends \non the amount of time tokens spend processing at p, which we write as delay(p), and on the number of \ntokens ini\u00adtially in p, which we write as nstart(p). The times at which the .rst nstart(p) tokens become \nready to leave p are given by the sequence START(p), which is nondecreasing and each element of which \nis less than delay(p). In the example we assume the initial tokens of B and C are immediately available, \nso we have START(B) = 0 and START(C) = 0, 0, 0. Thus, the time that the ith token becomes available from \np is given by: START(p)i < nstart(p)OUT(p)i =i IN(p)i-nstart(p) + delay(p) i = nstart(p) By adopting \nthe convention that IN(p)i is -8 when i < 0 and that START(p)i is -8 when i < 0 or i = nstart(p), we \ncan write the above more succinctly as: OUT(p)i = max(START(p)i, IN(p)i-nstart(p) + delay(p)) This gives \na set of recurrences between the sequences: the value of OUT(p) depends on the previous values of IN(p). \nWe now shift notation to make the semiring structure of this problem apparent. We return to the max-plus \nalgebra, previously used for longest distances and knapsacks, where we write max as +, and addition as \n\u00b7. Instead of sequences, let s talk about formal power series, where the ith element of the sequence \nis now the coef.cient of x i . With our semiring goggles on, the above equations now say: IN(p) = OUT(p \n' ) p1.pred(p) nstart(p) OUT(p) = delay(p) \u00b7 x\u00b7 IN(p) + START(p) We can eliminate IN(p) by substituting \nits de.nition into the second equation: nstart(p) ' OUT(p) = delay(p)\u00b7x\u00b7OUT(p )+START(p) p1.pred(p) What \nwe re left with is a system of af.ne equations, where the unknowns, the coef.cients and the constants \nare all formal power series over the max-plus semiring. We can solve these exactly as before. We build \nthe matrix M nstart(p) containing all of the delay(p) \u00b7 xcoef.cients, and the column vector S containing \nall of the START(p) sequences, and then calculate M * \u00b7 S (which, as before, can be done with a single \ncall to closure and a multiplication by S). The components of the resulting vector are power series; \ntheir coef.cients give OUT(p) for each place p. Thus, we can simulate a timed event graph by representing \nit as a linear system and using our previously-de.ned semiring machinery. 9. Discussion It turns out \nthat very many problems can be solved with linear algebra, for a de.nition of linear suited to the problem \nat hand. There are surprisingly many questions that can be answered with a call to closure with the right \nSemiring instance. Even still, this paper barely scratches the surface of this rich theory. Much more \ncan be found in books by Gondran and Minoux [9], Golan [7, 8] and others. The connections between regular \nlanguages, path problems in graphs, and matrix inversion have been known for some time. The relationship \nbetween regular languages and semirings is described in Conway s book [5]. Backhouse and Carr\u00b4e [3] used \nregular alge\u00adbra to solve path problems (noting connections to classical linear algebra), and Tarjan \n[17] gave an algorithm for solving path prob\u00adlems by a form of Gaussian elimination. A version of closed \nsemiring was given by Aho, Hopcroft and Ullman [2], along with transitive closure and shortest path algo\u00adrithms. \nThe form of closed semiring that this paper discusses was given by Lehmann [12], with two algorithms \nfor calculating the clo\u00adsure of a matrix: an algorithm generalising the Floyd-Warshall all\u00adpairs shortest-paths \nalgorithm [6], and another generalising Gaus\u00adsian elimination, demonstrating the equivalence of these \ntwo in their general form. More recently, Abdali and Saunders [1] refor\u00admulate the notion of closure \nof a matrix in terms of eliminants , which formalise the intermediate steps of Gaussian elimination. \nThe use of semirings to describe path problems in graphs is widespread [9, 16]. Often, the structures \nstudied include the extra axiom that a + a = a, giving rise to idempotent semirings or dioids. Such structures \ncan be partially ordered, and it becomes possible to talk about least .xed points of af.ne maps. These \nhave proven strong enough structures to build a variant of classical real analysis [13].  Cohen et al. \n[4], as well as providing the linear description of Petri nets we saw in section 8, go on to develop \nan analogue of classical linear systems theory in a semiring. In this theory, they explore semiring versions \nof many classical concepts, such as stability of a linear system and describing a system s steady-state \nas an eigenvalue of a transfer matrix. Acknowledgments The author is grateful to Alan Mycroft for suffering \nthrough early drafts of this work and offering helpful advice, and to Raphael Proust, Stephen Roantree \nand the anonymous reviewers for their useful comments, and .nally to Trinity College, Cambridge for .nancial \nsupport. References [1] S. Abdali and B. Saunders. Transitive closure and related semiring properties \nvia eliminants. Theoretical Computer Science, 40:257 274, 1985. [2] A. V. Aho, J. E. Hopcroft, and J. \nD. Ullman. The Design and Analysis of Computer Algorithms. Addison-Wesley, 1974. [3] R. Backhouse and \nB. Carr\u00b4e. Regular algebra applied to path-.nding problems. IMA Journal of Applied Mathematics, 2(15):161 \n186, 1975. [4] G. Cohen and P. Moller. Linear system theory for discrete event systems. In 23rd IEEE \nConference on Decision and Control, pages 539 544, 1984. [5] J. Conway. Regular algebra and .nite machines. \nChapman and Hall (London), 1971. [6] R. Floyd. Algorithm 97: shortest path. Communications of the ACM, \n5(6):345, 1962. [7] J. S. Golan. Semirings and their applications. Springer, 1999. [8] J. S. Golan. Semirings \nand af.ne equations over them. Kluwer Academic Publishers, 2003.  [9] M. Gondran and M. Minoux. Graphs, \ndioids and semirings. Springer, 2008. [10] J. Kam and J. Ullman. Global data .ow analysis and iterative \nalgorithms. Journal of the ACM (JACM), 23(1):158 171, 1976. [11] U. Khedker and D. Dhamdhere. A generalized \ntheory of bit vector data .ow analysis. ACM Transactions on Programming Languages and Systems (TOPLAS), \n16(5):1472 1511, 1994. [12] D. Lehmann. Algebraic structures for transitive closure. Theoretical Computer \nScience, 4(1):59 76, 1977. [13] V. P. Maslov. Idempotent analysis. American Mathematical Society, 1992. \n[14] M. D. Mcilroy. Power series, power serious. Journal of Functional Programming, 9(3):325 337, 1999. \n[15] R. McNaughton and H. Yamada. Regular expressions and state graphs for automata. IRE Transactions \non Electronic Computers, 9(1):39 47, 1960. [16] M. Mohri. Semiring frameworks and algorithms for shortest-distance \nproblems. Journal of Automata, Languages and Combinatorics, 7(3): 321 350, 2002. [17] R. Tarjan. Solving \npath problems on directed graphs. Technical report, Stanford University, 1975.  \n\t\t\t", "proc_id": "2500365", "abstract": "<p>Describing a problem using classical linear algebra is a very well-known problem-solving technique. If your question can be formulated as a question about real or complex matrices, then the answer can often be found by standard techniques.</p> <p>It's less well-known that very similar techniques still apply where instead of real or complex numbers we have a closed semiring, which is a structure with some analogue of addition and multiplication that need not support subtraction or division.</p> <p>We define a typeclass in Haskell for describing closed semirings, and implement a few functions for manipulating matrices and polynomials over them. We then show how these functions can be used to calculate transitive closures, find shortest or longest or widest paths in a graph, analyse the data flow of imperative programs, optimally pack knapsacks, and perform discrete event simulations, all by just providing an appropriate underlying closed semiring.</p>", "authors": [{"name": "Stephen Dolan", "author_profile_id": "81553203456", "affiliation": "University of Cambridge, Cambridge, United Kingdom", "person_id": "P4261228", "email_address": "stephen.dolan@cl.cam.ac.uk", "orcid_id": ""}], "doi_number": "10.1145/2500365.2500613", "year": "2013", "article_id": "2500613", "conference": "ICFP", "title": "Fun with semirings: a functional pearl on the abuse of linear algebra", "url": "http://dl.acm.org/citation.cfm?id=2500613"}