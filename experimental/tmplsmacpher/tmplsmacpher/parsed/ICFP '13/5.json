{"article_publication_date": "09-25-2013", "fulltext": "\n Optimising Purely Functional GPU Programs Trevor L. McDonell Manuel M. T. Chakravarty Gabriele Keller \nBen Lippmeier University of New South Wales, Australia {tmcdonell,chak,keller,benl}@cse.unsw.edu.au \nAbstract Purely functional, embedded array programs are a good match for SIMD hardware, such as GPUs. \nHowever, the naive compilation of such programs quickly leads to both code explosion and an excessive \nuse of intermediate data structures. The resulting slow\u00addown is not acceptable on target hardware that \nis usually chosen to achieve high performance. In this paper, we discuss two optimisation techniques, \nsharing recovery and array fusion, that tackle code explosion and elimi\u00adnate super.uous intermediate \nstructures. Both techniques are well known from other contexts, but they present unique challenges for \nan embedded language compiled for execution on a GPU. We present novel methods for implementing sharing \nrecovery and array fusion, and demonstrate their effectiveness on a set of benchmarks. Categories and \nSubject Descriptors D.3.2 [Programming Lan\u00adguages]: Language Classi.cation Applicative (functional) lan\u00adguages; \nConcurrent, distributed, and parallel languages Keywords Arrays; Data parallelism; Embedded language; \nDy\u00adnamic compilation; GPGPU; Haskell; Sharing recovery; Array fu\u00adsion 1. Introduction Recent work on \nstream fusion [12], the vector package [23], and the parallel array library Repa [17, 19, 20] has demonstrated \nthat (1) the performance of purely functional array code in Haskell can be competitive with that of imperative \nprograms and that (2) purely functional array code lends itself to an ef.cient parallel implementation \non control-parallel multicore CPUs. So far, the use of purely functional languages for programming data \nparallel SIMD hardware such as GPUs (Graphical Processing Units) has been less successful. Vertigo [13] \nwas an early Haskell EDSL producing DirectX 9 shader code, though no runtime perfor\u00admance .gures were \nreported. Nikola [22] produces code competi\u00ad tive with CUDA, but without supporting generative functions \nlike replicate where the result size is not statically .xed. Obsidian [10] is additionally restricted \nto only processing arrays of a .xed, implementation dependent size. Additionally, both Nikola and Ob\u00adsidian \ncan only generate single GPU kernels at a time, so that in Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page. Copyrights for components of this work owned by others than the author(s) \nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers \nor to redistribute to lists, requires prior speci.c permission and/or a fee. Request permissions from \npermissions@acm.org. ICFP 13, September 25 27, 2013, Boston, MA, USA. Copyright is held by the owner/author(s). \nPublication rights licensed to ACM. ACM 978-1-4503-2326-0/13/09. . . $15.00. http://dx.doi.org/10.1145/2500365.2500595 \n programs consisting of multiple kernels the intermediate data struc\u00adtures must be shuf.ed back and forth \nacross the CPU-GPU bus. We recently presented Accelerate, an EDSL and skeleton-based code generator targeting \nthe CUDA GPU development environ\u00adment [8]. In the present paper, we present novel methods for op\u00ad timising \nthe code using sharing recovery and array fusion. Sharing recovery for embedded languages recovers the \nsharing of let-bound expressions that would otherwise be lost due to the embedding. Without sharing recovery, \nthe value of a let-bound expression is recomputed for every use of the bound variable. In contrast to \nprior work [14] that decomposes expression trees into graphs and fails to be type preserving, our novel \nalgorithm preserves both the tree structure and typing of a deeply embedded language. This enables our \nruntime compiler to be similarly type preserving and simpli.es the backend by operating on a tree-based \nintermediate language. Array fusion eliminates the intermediate values and additional GPU kernels that \nwould otherwise be needed when successive bulk operators are applied to an array. Existing methods such \nas foldr/build fusion [15] and stream fusion [12] are not applica\u00ad ble to our setting as they produce \ntail-recursive loops, rather than the GPU kernels we need for Accelerate. The NDP2GPU system of [4] does \nproduce fused GPU kernels, but is limited to simple map/map fusion. We present a fusion method partly \ninspired by Repa s delayed arrays [17] that fuses more general producers and consumers, while retaining \nthe combinator based program repre\u00adsentation that is essential for GPU code generation using skeletons. \nWith these techniques, we provide a high-level programming model that supports shape-polymorphic maps, \ngenerators, reduc\u00adtions, permutation and stencil-based operations, while maintaining performance that \noften approaches hand-written CUDA code. In summary, we make the following contributions: We introduce \na novel sharing recovery algorithm for type-safe ASTs, preserving the tree structure (Section 3).  We \nintroduce a novel approach to array fusion for embedded array languages (Section 4).  We present benchmarks \nfor several applications including Black-Scholes options pricing, Canny edge detection, and a .uid .ow \nsimulation, including a comparison with hand\u00adoptimised CPU and GPU code (Section 5).  This paper builds \non our previous work on a skeleton-based CUDA code generator for Accelerate [8]. Although we motivate \nand eval\u00ad uate our novel approaches to sharing recovery and array fusion in this context, our contribution \nis not limited to Accelerate. Speci.\u00adcally, our sharing recovery applies to any embedded language based \non the typed lambda calculus and our array fusion applies to any dynamic compiler targeting bulk-parallel \nSIMD hardware. We discuss related work in detail in Section 6. The source code for Accelerate including \nour benchmark code is available from https://github.com/AccelerateHS/accelerate.  2. Optimising embedded \narray code Accelerate is a domain speci.c language, consisting of a carefully selected set of array operators \nthat we can produce ef.cient GPU code for. Accelerate is embedded in Haskell, meaning that we write Accelerate \nprograms using Haskell syntax, but do not compile arbitrary Haskell programs to GPU machine code. Accelerate \nis strati.ed into array computations, wrapped in a type constructor Acc, and scalar expressions, represented \nby terms of type Exp t, where t is the type of value produced by the expression. This strati.cation is \nnecessary due to the hardware architecture of GPUs and their reliance on SIMD parallelism, as we discussed \nin our previous work [8]. 2.1 Too many kernels For example, to compute a dot product, we use: dotp :: \nVector Float -> Vector Float -> Acc (Scalar Float) dotp xs ys = let xs = use xs ys = use ys in fold (+) \n0 (zipWith (*) xs ys ) The function dotp consumes two one-dimensional arrays (Vector) of .oating-point \nvalues and produces a single (Scalar) .oating\u00adpoint result. From the return type Acc (Scalar Float), \nwe see that dotp is an embedded Accelerate computation, rather than vanilla Haskell code. The functions \nzipWith and fold are de.ned in our library Data.Array.Accelerate, and have massively parallel GPU im\u00adplementations \nwith the following type signatures: zipWith :: (Shape sh, Elt a, Elt b, Elt c) => (Exp a -> Exp b -> \nExp c) -> Acc (Array sh a) -> Acc (Array sh b) -> Acc (Array sh c) fold :: (Shape ix, Elt a) => (Exp \na -> Exp a -> Exp a) -> Exp a -> Acc (Array (ix:.Int) a) -> Acc (Array ix a) The type classes Elt and \nShape indicate that a type is admissible as an array element and array shape, respectively. Array shapes \nare denoted by type-level lists formed from Z and (:.) for example, Z:.Int:.Int is the shape of a two-dimensional \narray (see [8, 17] for details). The type signatures of zipWith and fold clearly show the strati.cation \ninto scalar computations using the Exp type constructor, and array computations wrapped in Acc. The arguments \nto dotp are of plain Haskell type Vector Float. To make these arguments available to the Accelerate computation \nthey must be embedded with the use function: use :: Arrays arrays => arrays -> Acc arrays This function \nis overloaded so that it can accept entire tuples of arrays. Operationally, use copies array data from \nmain memory to GPU memory, in preparation for processing by the GPU. The above Haskell version of the \nGPU-accelerated dot product is certainly more compact than the corresponding CUDA C code. However, when \ncompiled with the skeleton-based approach we described in previous work [8], it is also signi.cantly \nslower. The CUDA C version executes in about half the time (see Table 2). The slow-down is due to Accelerate \ngenerating one GPU ker\u00adnel function for zipWith and another one for fold. In contrast, the CUDA C version \nonly uses a single kernel. The use of two sepa\u00adrate kernels requires an intermediate array to be constructed, \nand in a memory bound benchmark, such as dotp, this doubles the run\u00adtime. To eliminate this intermediate \narray we need to fuse the ad\u00adjacent aggregate array computations. Although there is an existing body \nof work on array fusion, no existing method adequately deals with massively parallel GPU kernels. We \npresent a suitable fusion framework as the .rst major contribution of this paper. 2.2 Too little sharing \nAs a second example, consider the pricing of European-style op\u00adtions using the Black-Scholes formula. \nThe Accelerate program is in Figure 1. Given a vector of triples of underlying stock price, strike price, \nand time to maturity (in years), the Black-Scholes for\u00admula computes the price of a call and a put option. \nThe function callput evaluates the Black-Scholes formula for a single triple, and blackscholes maps it \nover a vector of triples, such that all individual applications of the formula are executed in parallel. \nIf we compare the performance of the GPU code generated by Accelerate with that of an equivalent implementation \nin CUDA C, the Accelerate version is almost twenty times slower. As blackscholes includes only one aggregate \narray computation, the problem can t be a lack of fusion. Instead, as we noted previ\u00adously [8], it is \ndue to the embedding of Accelerate in Haskell. The function callput includes a signi.cant amount of sharing: \nthe helper functions cnd , and hence also horner, are used twice for d1 and d2 and its argument d is \nused multiple times in the body. Our embedded implementation of Accelerate rei.es the abstract syntax \nof the (deeply) embedded language in Haskell. Consequently, each occurrence of a let-bound variable in \nthe source program creates a separate unfolding of the bound expression in the compiled code. This is \na well known problem that has been solved elegantly by the sharing recovery algorithm of Gill [14], which \nmakes use of stable names. Unfortunately, Gill s original approach (1) rei.es the abstract syntax in \ngraph form and (2) it assumes an untyped syntax representation. In contrast, Accelerate is based on a \ntyped tree representation using GADTs and type families in conjunction with type-preserving compilation \nin most phases. In other words, we use Haskell s type checker to statically ensure many core properties \nof our Accelerate compiler. The fact that the compiler for the embedded language is type preserving, \nmeans that many bugs in the Accelerate compiler itself are caught by the Haskell compiler during development. \nThis in turn reduces the number of Accelerate compiler bugs that the end-user might be exposed to. As \nwe require typed trees, where sharing is represented by let bindings rather than untyped graphs, we cannot \ndirectly use Gill s approach to sharing recovery. Instead, we have developed a novel sharing recovery \nalgorithm, which like Gill s, uses stable names, but unlike Gill s, operates on typed abstract syntax. \nOur algorithm produces a typed abstract syntax tree, and we are able to recover exactly those let bindings \nused in the source program. This is the second major contribution of this paper.  2.3 Summary In summary, \na straightforward skeleton-based implementation of an embedded GPU language suffers from two major inef.ciencies: \nlack of sharing and lack of fusion. Both sharing recovery in em\u00adbedded languages, and array fusion in \nfunctional languages have received signi.cant prior attention. However, we have found that none of the \nexisting techniques are adequate for a type-preserving embedded language compiler targeting massively \nparallel SIMD hardware, such as GPUs. 3. Sharing recovery Gill [14] proposed to use stable names [27] \nto recover the sharing of source terms in a deeply embedded language. The stable names  blackscholes \n:: Vector (Float, Float, Float) riskfree, volatility :: Float -> Acc (Vector (Float, Float)) riskfree \n= 0.02 blackscholes = map callput . use volatility = 0.30 where callput x = horner :: Num a => [a] -> \na -> a let (price, strike, years) = unlift x horner coeff x = x * foldr1 madd coeff r = constant riskfree \nwhere v = constant volatility madd a b = a + x*b v_sqrtT = v * sqrt years d1 = (log (price / strike) \n+ cnd :: Floating a => a -> a (r + 0.5 * v * v) * years) / v_sqrtT cnd d = d2 = d1 -v_sqrtT let poly \n= horner coeff cnd d = let c = cnd d in d >* 0 ? (1.0 -c, c) coeff = [0.31938153, -0.356563782, cndD1 \n= cnd d1 1.781477937, -1.821255978, cndD2 = cnd d2 1.330274429] x_expRT = strike * exp (-r * years) rsqrt2pi \n= 0.39894228040143267793994605993438 in k = 1.0 / (1.0 + 0.2316419 * abs d) lift ( price * cndD1 -x_expRT \n* cndD2 in , x_expRT * (1.0 -cndD2) -price * (1.0 -cndD1)) rsqrt2pi * exp (-0.5*d*d) * poly k Figure \n1. Black-Scholes option pricing in Accelerate of two Haskell terms are equal only when the terms are \nrepresented by the same heap structure in memory. Likewise, when the abstract syntax trees (ASTs) of \ntwo terms of an embedded language have the same stable name, we know that they represent the same value. \nIn this case we should combine them into one shared AST node. As the stable name of an expression is \nan intensional property, it can only be determined in Haskell s IO monad. Accelerate s runtime compiler \npreserves source types through\u00adout most of the compilation process [8]. In particular, it converts the \nsource representation based on higher-order abstract syntax (HOAS) to a type-safe internal representation \nbased on de Bruijn indices. Although developed independently, this conversion is like the unembedding \nof Atkey et al. [1]. Unembedding and sharing re\u00ad covery necessarily need to go hand in hand for the following \nrea\u00adsons. Sharing recovery must be performed on the source represen\u00adtation; otherwise, sharing will already \nhave been lost. Nevertheless, we cannot perform sharing recovery on higher-order syntax as we need to \ntraverse below the lambda abstractions used in the higher\u00adorder syntax representation. Hence, both need \nto go hand in hand. As shown by Atkey et al. [1], the conversion of HOAS in Haskell can be implemented \nin a type-preserving manner with the exception of one untyped, but dynamically checked environment look \nup, using Haskell s Typeable. To preserve maximum type safety, we do not want any further operations \nthat are not type pre\u00adserving when adding sharing recovery. Hence, we cannot use Gill s algorithm. The \nvariant of Gill s algorithm used in Syntactic [2] does not apply either: it (1) also generates a graph \nand (2) discards static type information in the process. In contrast, our novel algo\u00adrithm performs simultaneous \nsharing recovery and conversion from HOAS to a typed de Bruijn representation, where the result of the \nconversion is a tree (not a graph) with sharing represented by let bindings. Moreover, it is a type-preserving \nconversion whose only dynamically checked operation is the same environment lookup deemed unavoidable \nby Atkey et al. [1]. In the following, we describe our algorithm using plain typed lambda terms. This \navoids much of the clutter that we would incur by basing the discussion on the full Accelerate language. \nIn addi\u00adtion to its implementation in Accelerate, a working Haskell imple\u00admentation of our algorithm \nfor the plain lambda calculus can be found at https://github.com/mchakravarty/hoas-conv. 3.1 Our approach \nto sharing recovery Before we formalise our sharing recovery algorithm in the follow\u00ading subsections, \nwe shall illustrate the main idea. Consider the fol\u00adlowing source term: let inc = (+) 1 in let nine = \nlet three = inc 2 in (*) three three in (-) (inc nine) nine This term s abstract syntax DAG is the \nleftmost diagram in Fig\u00adure 2. It uses @ nodes to represent applications; as in this grammar: T t | x \nC t :: T t | .x. T x t :: T t | T1 @ T2 .x t1 .T t2 :: T t1.t2 t1.t2 t1 T t2 T . C where C . (constants) \nT1 @ T2 :: The left de.nition does not track types, whereas the right one does. We implement typed ASTs \nin Haskell with GADTs and work with typed representations henceforth. Typed HOAS conversion with sharing \nrecover proceeds in three stages: 1. Prune shared subterms: A depth .rst traversal over the AST an\u00adnotates \neach node with its unique stable name, where we build an occurrence map of how many times we ve already \nvisited each node. If we encounter a previously visited node, it repre\u00adsents a shared subterm, and we \nreplace it by a placeholder con\u00adtaining its stable name. The second diagram in Figure 2 shows the outcome \nof this stage. Each node is labeled by a number that represents its stable name, and the dotted edges \nindicate where we encountered a previously visited, shared node. The placeholders are indicated by underlined \nstable names. 2. Float shared terms: All shared subterms .oat upwards in the tree to just above the \nlowest node that dominates all edges to the original position of that shared subterm see the third diagram \nin Figure 2. Floated subterms are referenced by circled stable names located above the node that they \n.oated to. If a node collects more than one shared subterm, the subterm whose origin is deeper in the \noriginal term goes on top here, 9 on top of 5. Nested sharing leads to subterms .oating up inside other \n.oated subterms here, 8 stays inside the subterm rooted in 5.   Figure 2. Recovering sharing in an \nexample term 3. Binder introduction: Each .oated subterm gets let-bound right above the node it .oated \nto (rightmost diagram in Figure 2). While we use explicit, bound names in the .gure, we introduce de \nBruijn indices at the same time as introducing the lets.  3.2 Prune shared subterms First, we identify \nand prune shared subtrees, producing a pruned tree of the following form (second diagram in Figure 2): \n .T t where e :: .Tt --binder conversion level .t C t :: :: .Tt .Tt --pruned subtree (name) .e..Tt2 :: \n.Tt1.t2 .T t1.t2 1 @ .T t1 2 :: .Tt2 A stable name (here, of type Name) associates a unique name with \neach unique term node, so that two terms with the same stable name are identical, and are represented \nby the same data structure in memory. Here, we denote the stable name of a term as a superscript during \npattern matching e.g., 1. is a constant with stable name ., just as in the second and third diagram \nin Figure 2. An occurrence map, O :: Name . Int, is a .nite map that determines the number of occurrences \nof a Name that we encoun\u00adtered during a traversal. The expression O. yields the number of occurrences \nof the name ., and we have . . O = (O. > 0). To add an occurrence to O, we write . CO. We will see in \nthe next sub\u00adsection that we cannot simplify O to be merely a set of occurring names. We need the actual \noccurrence count to determine where shared subterms should be let-bound. The identi.cation and pruning \nof shared subtrees is formalised by the following function operating on closed terms from T t : prune \n:: Level . (Name . Int) . Tt . ((Name . Int), .T t ) prune e O e . | . . O = (. C O, .) prune e O e . \n| otherwise = enter (. C O) e where enter O c = (O, c) enter O (.x.e) = let ' (O', e ) = prune (e + \n1) O ([e/x]e) in ' (O', .e.e ) enter O (e1 @ e2) = let ' (O1, e1) = prune e O e1 ' (O2, e2) = prune e \nO1 e2 in '' (O2, e1 @ e2) The .rst equation of prune covers the case of a term s repeated occurrence. \nIn that case, we prune sharing by replacing the term e . by a tag . containing its stable name these \nare the dotted lines in the second diagram in Figure 2. To interleave sharing recovery with the conversion \nfrom HOAS to typed de Bruijn indices, prune tracks the nesting Level of lambdas. Moreover, the lambda \ncase of enter replaces the HOAS binder x by the level e at the binding and usage sites. Why don t we \nseparate computing occurrences from tree prun\u00ading? When computing occurrences, we must not traverse shared \nsubtrees multiple times, so we can as well prune at the same time. Moreover, in the .rst line of prune, \nwe cannot simply return e in\u00adstead of . e is of the wrong form as it has type T and not .T ! As far \nas type-preservation is concerned, we do lose information due to replacing variables by levels e. This \nis the inevitable loss described by Atkey et al. [1], which we make up for by a dynamic check in an environment \nlookup, as already discussed.  3.3 Float shared subterms Second, we .oat all shared subtrees out to \nwhere they should be let-bound, represented by (see third diagram in Figure 2) t t .T . . : .T t1 > .T \n. t T where .t .t :: T C t .t :: T t2 .t1.t2 ....T :: T .t1.t2 @ .t1 .t2 T T :: T 1 2 A term in .T comprises \na sequence of .oated-out subterms labelled by their stable name as well as a body term from .T from which \nthe .oated subterms where extracted. Moreover, the levels e that replaced lambda binders in .T get replaced \nby the stable name of their term node. This simpli.es a uniform introduction of de Bruijn indices for \nlet and lambda bound variables. We write . : .T for a possibly empty sequence of items: .1 : .T 1, . \n. . , .n : .T n , where denotes an empty sequence. The .oating function .oat maintains an auxiliary \nstructure of .oating terms and levels, de.ned as follows: i t i i G . . : .T | . : \u00b7 | . : e These are \n.oated subtrees named . of which we have collected i occurrences. The occurrence count indicates where \na shared sub\u00adterm gets let bound: namely at the node where it matches O.. This is why prune needed to \ncollect the number of occurrences in O. When the occurrence count matches O., we call the .oated term \nsaturated. The following function determines saturated .oated terms, which ought to be let bound:  bind \n:: (Name . Int) . G . .t.. : .T t bind O = i bind O (. : e, G) | O. == i = . : e, bind O G i bind O \n(. : , G) = bind O G Note that G does not keep track of the type t of a .oated term .T t ; hence, .oated \nterms from bind come in an existential package. This does not introduce additional loss of type safety \nas we already lost the type of lambda bound variables in . : i e. It merely means that let bound, just \nlike lambda bound, variables require the dynamically checked environment look up we already discussed. \nWhen .oating the .rst occurrence of a shared tree (not pruned by prune), we use . : i .T t . When .oating \nsubsequent occurrences (which were pruned), we use . i : \u00b7. Finally, when .oating a level, to replace \nit by a stable name, we use . i : e. We de.ne a partial ordering on .oated terms: .1 i : x < .2 j : y \niff the direct path from .1 to the root of the AST is shorter than that of .2. We keep sequences of .oated \nterms in descending order so that the deepest subterm comes .rst. We write G1 l G2 to merge two sequences \nof .oated terms. Merging respects the partial order, and it combines .oated trees with the same stable \nname by adding their occurrence counts. To combine the .rst occurrence and a subsequent occurrence of \na shared tree, we preserve the term of the .rst occurrence. We write G \\ . to delete elements of G that \nare tagged with a name that appears in the sequence .. We can now formalise the .oating process as follows: \n.oat :: (Name . Int) . .T t . (G , .T t ) .oat O e . = (. 1 : e, . ) 1 .oat O . = (. : \u00b7, .) .oat O e \n. = let ' (G , e ) = descend e .b : eb = bind O G ' d = .b : eb > e in if O. == 1 then (G \\ .b, d) else \nwhere (G \\ .b l {. : d}, .) descend :: .T t . (G, .T t ) descend c = ( , c) descend (.e.e) = let ' (G \n, e ) = .oat O e in if . . ' i. (. ' i : e) . G then (G \\ {. ' }, .. ' .e ' ) else (G , . .e ' ) descend \n(e1 @ e2) = let (G 1, e1 ' ) = .oat O e1 (G 2, e2 ' ) = .oat O e2 in (G 1 l G 2, e1 ' @ e2 ' ) The .rst \ntwo cases of .oat ensure that the levels of lambda bound variables and the names of pruned shared subterms \nare .oated regardless of how often they occur. In contrast, the third equation .oats a term with name \n. only if it is shared; i.e., O. is not 1. If it is shared, it is also pruned; i.e., replaced by its \nname . just as in the third diagram of Figure 2. Regardless of whether a term gets .oated, all saturated \n.oated terms, .b : eb, must pre.x the result, e ', and be removed from G . When descend ing into a term, \nthe only interesting case is for lambdas. For a lambda at level e, we look for a .oated level of the \nform . ' : e. If that is available, . ' replaces e as a binder and we remove . ' : e from G . However, \nif . ' : e is not in G , the binder introduced by the lambda doesn t get used in e. In this case, we \npick an arbitrary new name; here symbolised by an underscore .  3.4 Binder introduction Thirdly, we \nintroduce typed de Bruijn indices to represent lambda and let binding structure (rightmost diagram in \nFigure 2): Tt C t T t envwhere :: env envenv .t T t :: .(t1, env)t2 envT t1.t2 T :: envT t1.t2 @ envT \nt1 envT t2 :: 1 2 let envT t1 in (t1, env)t2 envT t2 T :: 1 2 env With this type of terms, e :: Tt means \nthat e is a term repre\u00adsenting a computation producing a value of type t under the type environment env. \nType environments are nested pair types, possi\u00adbly terminated by a unit type (). For example, (((), t1), \nt0) is a type environment, where de Bruijn index 0 represents a variable of type t0 and de Bruijn index \n1 represents a variable of type t1. We abbreviate let e1 in \u00b7 \u00b7 \u00b7 let en in eb as let e in eb. Both . \nand let use de Bruijn indices . instead of introducing explicit binders. To replace the names of pruned \nsubtrees and of lambda bound variables by de Bruijn indices, we need to construct a suitable type environment \nas well as an association of environment entries, their de Bruijn indices, and the stable names that \nthey replace. We maintain the type environment with associated de Bruijn indices in the following environment \nlayout structure: 1 env.env where env.() . :: 1 .t 1 env.env envenv.(env , t) ; :: Together with a layout, \nwe use a sequence of names . of the same size as the layout, where corresponding entries represent the \nsame variable. As this association between typed layout and untyped sequence of names is not validated \nby types, the lookup function lyt . i getting the ith index of layout lyt makes use of a dynamic env.env \nenv type check. It s signature is (.) :: N . 1 . .t . Now we can introduces de Bruijn indices to body \nexpressions: env.env . envT t body :: . . . .T t body lyt (..,0, . . . , ..,n ) . | . == ..,i = lyt \n. i body lyt .. c = c body lyt .. (...e) = .(binders lyt + (., ..) e) body lyt .. (e1 @ e2) = (binders \nlyt .. e1) @ (binders lyt .. e2) The .rst equation performs a lookup in the environment layout at the \nsame index where the stable name . occurs in the name environment .. The lookup is the same for lambda \nand let bound variables. It is the only place where we need a dynamic type check and that is already \nneeded for lambda bound variables alone. In the case of a lambda, we add a new binder by extending the \nlayout, denoted lyt+, with a new zeroth de Bruijn index and shifting all others one up. Keeping the name \nenvironment in sync, we add the stable name ., which .T used as a binder. In the same vein, we bind n \n.oated terms . : e with let bind\u00adings in body expression eb, by extending the type environment n times \n(map applies a function to each element of a sequence):  (Before fusion) (After producer/producer fusion) \n (After consumer/producer fusion) Figure 3. Produce/producer and consumer/producer fusion env.env . \nenvTt binders :: . . . .T t binders lyt .. (. : e > eb) = let map (binders lyt ..) e in body lyt +n (., \n..) eb where n = length (. : e) We tie the three stages together to convert from HOAS with sharing recovery \nproducing let bindings and typed de Bruijn indices: T t t . () hoasSharing :: T hoasSharing e = let (O, \ne ' ) = prune 0 e ( , e '' ) = .oat O e ' in binders . e '' 4. Array fusion Fusion in a massively data-parallel, \nembedded language for GPUs, such as Accelerate, requires a few uncommon considerations. Parallelism. \nWhile fusing parallel collective operations, we must be careful not to lose information essential to \nparallel execution. For example, foldr/build fusion [15] is not applicable, because it produces sequential \ntail-recursive loops rather than massively parallel GPU kernels. Similarly, the split/join approach used \nin Data Parallel Haskell (DPH) [16] is not helpful, although fused operations are split into sequential \nand parallel subcomputations, as they assume an explicit parallel scheduler, which in DPH is written \ndirectly in Haskell. Accelerate compiles massively parallel array combinators to CUDA code via template \nskeleton instantiation, so any fusion system must preserve the combinator representation of the intermediate \ncode. Sharing. Existing fusion transforms rely on inlining to move pro\u00adducer and consumer expressions \nnext to each other, which allows producer/consumer pairs to be detected. However, when let-bound variables \nare used multiple times in the body of an expression, un\u00adrestrained inlining can lead to duplication \nof work. Compilers such as GHC, handle this situation by only inlining the de.nitions of let\u00adbound variables \nthat have a single use site, or by relying on some heuristic about the size of the resulting code to \ndecide what to inline [26]. However, in typical Accelerate programs, each array is used at least twice: \nonce to access the shape information and once to access the array data; so, we must handle at least this \ncase separately. Filtering. General array fusion transforms must deal with .lter\u00adlike operations, for \nwhich the size of the result structure depends on the value of the input structure, as well as its size. \nAccelerate does not encode .ltering as a primitive operation, so we do not need to consider it further.1 \nFusion at run-time. As the Accelerate language is embedded in Haskell, compilation of the Accelerate \nprogram happens at Haskell runtime rather than when compiling the Haskell program. For this reason, optimisations \napplied to an Accelerate program contribute to its overall runtime, so we must be mindful of the cost \nof analysis and code transformation. On the .ip-side, runtime optimisations can make use of information \nthat is only available at runtime. Fusion on typed de Brujin terms. We fuse Accelerate programs by rewriting \ntyped de Bruijn terms in a type preserving manner. However, maintaining type information adds complexity \nto the def\u00adinitions and rules, which amounts to a partial proof of correctness checked by the type checker, \nbut is not particularly exciting for the present exposition. Hence, in this section, we elide the steps \nneces\u00adsary to maintain type information during fusion. 4.1 The Main Idea All collective operations in \nAccelerate are array-to-array transfor\u00admations. Reductions, such as fold, which reduce an array to a \nsin\u00adgle element, yield a singleton array rather than a scalar expression. Hence, we can partition array \noperations into two categories: 1. Operations where each element of the result array depends on at most \none element of each input array. Multiple elements of the output array may depend on a single input array \nelement, but all output elements can be computed independently. We refer to these operations as producers. \n 2. Operations where each element of the result array depends on multiple elements of the input array. \nWe call these functions consumers, in spite of the fact that they also produce an array.  Table 1 summarises \nthe collective array operations that we support. In a parallel context, producers are more pleasant to \ndeal with be\u00adcause independent element-wise operations have an obvious map\u00adping to the GPU. Consumers \nare a different story, as we need to know exactly how the computations depend on each other to im\u00adplement \nthem ef.ciently. For example, a parallel fold (with an asso\u00adciative operator) can be implemented ef.ciently \nas a tree reduction, but a parallel scan requires two separate phases [9, 31]. Unfortu\u00ad nately, this \nsort of information is obfuscated by most fusion tech\u00adniques. To support the different properties of \nproducers and con\u00adsumers, our fusion transform is split into two distinct phases: Producer/producer: \nfuse sequences of producers into a single producer. This is implemented as a source-to-source transfor\u00admation \non the AST.  Consumer/producer: fuse producers followed by a consumer into the consumer. This happens \nduring code generation, where we specialise the consumer skeleton with the producer code.  1 filter \nis easily implemented as a combination of the core primitives, and is provided as part of the library. \n Producers map :: (Exp a -> Exp b) -> Acc (Array sh a) -> Acc (Array sh b) map a function over an array \nzipWith :: (Exp a -> Exp b -> Exp c) -> Acc (Array sh a) -> Acc (Array sh b) apply funciton to. . . -> \nAcc (Array sh c) . . . a pair of arrays backpermute :: Exp sh -> (Exp sh -> Exp sh) -> Acc (Array sh \na) backwards permutation -> Acc (Array sh e) replicate :: Slice slix => Exp slix extend array across. \n. . -> Acc (Array (SliceShape slix) e) . . . new dimensions -> Acc (Array (FullShape slix) e) slice :: \nSlice slix remove existing dimensions => Acc (Array (FullShape slix) e) -> Exp slix -> Acc (Array (SliceShape \nslix) e) generate :: Exp sh -> (Exp sh -> Exp a) -> Acc (Array sh a) array from index mapping Consumers \nfold :: (Exp a -> Exp a -> Exp a) -> Exp a -> Acc (Array (sh:.Int) a) tree reduction along. . . -> Acc \n(Array sh a) . . . innermost dimension scan{l,r} :: (Exp a -> Exp a -> Exp a) -> Exp a -> Acc (Vector \na) left-to-right or right-to-left -> Acc (Vector a) . . . vector pre-scan permute :: (Exp a -> Exp a \n-> Exp a) -> Acc (Array sh a) forward permutation -> (Exp sh -> Exp sh ) -> Acc (Array sh a) -> Acc \n(Array sh a) stencil :: Stencil sh a stencil => (stencil -> Exp b) -> Boundary a map a function with \nlocal. . . -> Acc (Array sh a) -> Acc (Array sh b) . . . neighbourhood context Table 1. Summary of Accelerate \ns core collective array operations, omitting Shape and Elt class constraints for brevity. In addition, \nthere are other .avours of folds and scans as well as segmented versions of these. Separating fusion \ninto these two phases reduces the complexity of the task, though there is also a drawback: as all collective \nopera\u00adtions in Accelerate output arrays, we might wish to use the output of a consumer as an input to \na producer as in map g . fold f z. Here, the map operation could be fused into the fold by apply\u00ading \nthe function g to each element produced by the reduction be\u00adfore storing the .nal result in memory. This \nis useful, as Accelerate works on multidimensional arrays, so the result of a fold can be a large array \nrather than just a singleton array. Our approach cur\u00adrently does not fuse producer/consumer pairs, only \nconsumer/pro\u00adducer and producer/producer combinations. Figure 3 illustrates how fusion affects the AST: \nblue boxes p1 to p7 represent producers, where p5 is a producer like zipWith with two input arrays. The \nconsumers are c1 and c2. Firstly, we fuse all producers, with the exception of p1 whose result is used \nby both c1 and p2. Next, we plug the fused producers into consumers where possible. Again, p1 is left \nas is. It would be straightforward to change our implementation such that it would fuse p1 into both \np2 and c1. This would duplicate the work of p1 into both p2 and c1, which, despite reducing memory traf.c, \nis not always advanta\u00adgeous. Our current implementation is conservative and never dupli\u00adcates work; we \nplan to change this in future work as the restricted nature of Accelerate means that we can compute accurate \ncost es\u00adtimates and make an informed decision. In contrast, producer/con\u00adsumer fusion of c1 into p4 would \nrequire fundamental changes.  4.2 Producer/producer fusion for parallel arrays The basic idea behind \nthe representation of producer arrays in Accelerate is well known: simply represent an array by its shape \nand a function mapping indices to their corresponding values. We previously used it successfully to optimise \npurely functional array programs in Repa [17], but it was also used by others [11]. However, there are \nat least two reasons why it is not always bene.cial to represent all array terms uniformly as functions. \nOne is sharing: we must be able to represent some terms as manifest arrays so that a delayed-by-default \nrepresentation can not lead to arbitrary loss of sharing. This is a well known problem in Repa. The other \nconsideration is ef.ciency: since we are targeting an architecture designed for performance, we prefer \nmore speci.c operations. An opaque indexing function is too general, conveying no information about the \npattern in which the underlying array is accessed, and hence no opportunities for optimisation. We shall \nreturn to this point in Section 5, but already include a form of structured traversal over an array (Step) \nin the following de.nition: data DelayedAcc a where Done :: Acc a -> DelayedAcc a Yield :: (Shape sh, \nElt e) => Exp sh -> Fun (sh -> e) -> DelayedAcc (Array sh e) Step :: (Shape sh, Shape sh , Elt e, Elt \ne ) => Exp sh -> Fun (sh -> sh) -> Fun (e -> e ) -> Idx (Array sh e) -> DelayedAcc (Array sh e ) We have \nthree constructors: Done injects a manifest array into the type. Yield de.nes a delayed array in terms \nof its shape and a func\u00adtion which maps indices to elements. The third constructor, Step, encodes a special \ncase of the more general Yield that represents the application of an index and/or value space transformation \nto the argument array. The type Fun (sh -> e) is that of a term rep\u00adresenting a scalar function from \nshape to element type. The type Idx (Array sh e) is that of a de Bruijn index representing an array valued \nvariable. Representing the argument array in this way means that both Step and Yield are non-recursive \nin Acc terms, and so they can always be expressed as scalar functions and em\u00adbedded into consumers in \nthe second phase of fusion. We represent all array functions as constructors of the type DelayedAcc. \nProducer/producer fusion is achieved by tree con\u00adtraction on the AST, merging sequences of producers \ninto a single one. All array producing functions, such as map and backpermute, are expressed in terms \nof smart constructors for the DelayedAcc type. The smart constructors manage the integration with succes\u00adsive \nproducers, as shown in the following de.nition of mapD, the delayed version of the map function:  mapD \n:: (Shape sh, Elt a, Elt b) => Fun (a -> b) -> DelayedAcc (Array sh a) -> DelayedAcc (Array sh b) mapD \nf (Step sh p g v) = Step sh p (f . g) v mapD f (Yield sh g) = Yield sh (f . g) The function composition \noperator (.) is overloaded here to work on scalar function terms. With this de.nition we now have the \nwell known fusion rule that reduces mapD f . mapD g sequences to mapD (f . g). Similarly, the de.nition \nof delayed backpermute means that backpermuteD sh p (backpermuteD q arr) re\u00adduces to backpermute sh (q \n. p) arr: backpermuteD :: (Shape sh, Shape sh , Elt e) => Exp sh -> Fun (sh -> sh) -> DelayedAcc (Array \nsh e) -> DelayedAcc (Array sh e) backpermuteD sh p acc = case acc of Step _ ix f a -> Step sh (ix . p) \nf a Yield _ f -> Yield env sh (f . p) Done env a -> Step sh p identity (toIdx a) Of course, combinations \nof maps with backpermutes also reduce to a single producer. As desired, this approach also works on producers \nwhich take their input from multiple arrays. This is in contrast to foldr/build [15], which can fuse \none of the input arguments, but not both. The de.nition of zipWithD considers all possible combinations \nof constructors (only some of which we list here) and can therefore fuse producers of both arguments: \nzipWithD :: (Shape sh, Elt a, Elt b, Elt c) => Fun (a -> b -> c) -> DelayedAcc (Array sh a) -> DelayedAcc \n(Array sh b) -> DelayedAcc (Array sh c) zipWithD f (Yield sh1 g1) (Yield sh2 g2) = Yield (sh1 intersect \nsh2) (\\sh -> f (g1 sh) (g2 sh)) zipWithD f (Yield sh1 g1) (Step sh2 ix2 g2 a2) = Yield ... In this manner, \nsequences of producers fuse into a single producer term; then, we turn them back into a manifest array \nusing the function compute. It inspects the argument terms of the delayed array to identify special cases, \nsuch as maps or backpermutes, as shown in the following snippet of pseudo-code: compute :: DelayedAcc \na -> Acc a compute (Done a) = a compute (Yield sh f) = Generate sh f compute (Step sh p f v) | sh == \nshape a, isId p, isId f = a | sh == shape a, isId p = Map f a | isId f = Backpermute sh p a | otherwise \n= Transform sh p f a where a = Avar v Since we operate directly on the AST of the program, we can inspect \nfunction arguments and specialise the code accordingly. For example, isId :: Fun (a->b) -> Bool checks \nwhether a function term corresponds to the term .x.x.  4.3 Consumer/Producer Fusion Now that we have \nthe story for producer/producer fusion, we dis\u00adcuss how to deal with consumers. We pass producers encoded \nin the DelayedAcc representation as arguments to consumers, so that the consumers can compute the elements \nthey need on-the-.y. Con\u00adsumers themselves have no DelayedAcc representation, however. Consumers such \nas stencil, access elements of their argument array multiple times. These consumers are implemented carefully \nnot to duplicate work. Indeed, even when the argument of such a consumer is a manifest array, the consumer \nshould ensure that it caches already fetched elements, as GPUs impose a high perfor\u00admance penalty for \nrepeated memory loads. Some consumers can be implemented more ef.ciently when given a producer expressed \nin terms of a function from a multi-dimensional array index to an ele\u00adment value. Other consumers prefer \nfunctions that map the .at lin\u00adear index of the underling array to its value. Our consumer-friendly representation \nof delayed arrays therefore contains both versions: data Embedded sh e = (Shape sh, Elt e) => DelayedArray \n{ extent :: Exp sh , index :: Fun (sh -> e) , linearIndex :: Fun (Int -> e) } embedAcc :: (Shape sh, \nElt e) => Acc (Array sh e) -> Embedded sh e embedAcc (Generate sh f) = DelayedArray sh f (f . (fromIndex \nsh)) embedAcc (Map f (AVar v)) = DelayedArray (shape v) (indexArray v) (linearIndexArray v) embedAcc \n... The function embedAcc intelligently injects each producer into the Embedded type by inspection of \nthe constructor, as shown in the code snippet above. In theory, compute and embedAcc could be combined \nto go directly from the delayed representation which is convenient for producer/producer fusion to the \none for consumer/producer fusion. However, these two steps happen at different phases in the compilation, \nso we want to limit the visibility of each delayed representation to the current phase. Producers are \n.nally embedded into consumers during code generation. During code generation, the code for the embedded \npro\u00adducers is plugged into the consumer template. The codegenAcc function inspects the consumer and generates \ncode for the argu\u00adments of the consumer. It then passes these CUDA code snippets to a function specialised \non generating code for this particular con\u00adsumer (mkFold in the example below), which combines these \nsnip\u00adpets with the actual CUDA consumer code: codegenAcc :: DeviceProperties -> OpenAcc arrs -> Gamma \naenv -> CUTranslSkel arrs codegenAcc dev (OpenAcc (Fold f z a)) aenv = mkFold dev aenv (codeGenFun f) \n(codeGenExp z) (codegenDelayedAcc $ embedAcc a) codegenAcc dev (OpenAcc (Scanl f z a)) aenv = ... As \na result, the code producing each element is integrated directly into the consumer, and no intermediate \narray needs to be created.  4.4 Exploiting all opportunities for fusion As mentioned previously, we \nneed to be careful about fusing shared array computations, to avoid duplicating work. However, scalar \nAccelerate computations that manipulate array shapes, as opposed to the bulk array data, can lead to \nterms that employ sharing, but can never duplicate work. Such terms are common in Accelerate code, and \nit is important to that they do not inhibit fusion. Consider the following example that .rst reverses \na vector with backpermute and then maps a function f over the result. Being a sequence of two producer \noperations, we would hope these are fused into a single operation: reverseMap f a = map f $ backpermute \n(shape a) (\\i->length a-i-1) a Unfortunately, sharing recovery, using the algorithm from Sec\u00adtion 3, \ncauses a problem. The variable a is used three times in the arguments to backpermute; hence, sharing \nrecovery will introduce a let binding at the lowest common meet point for all uses of the array. This \nplaces it between the map and backpermute functions: reverseMap f a = map f $ let v = a in backpermute \n(shape v) (\\i->length v-i-1) v This binding, although trivial, prevents fusion of the two producers, \nand it does so unnecessarily. The argument array is used three times: twice to access shape information, \nbut only once to access the array data in the .nal argument to backpermute. Fortunately, there is a \nsimple work around. Recall that our delayed array constructors Step and Yield carry the shape of the \narrays they represent. Hence, we can eliminate all uses of the array that only access shape information, \nleaving us with a single reference to the array s payload. That single reference enables us to remove \nthe let binding and to re-enable fusion. Similarly, we may .oat let bindings of manifest data out (across \nproducer chains). This helps to expose further opportunities for producer/producer fusion. For example, \nwe allow the binding of xs to .oat above the map so the two producers can be fused: map g $ let xs = \nuse (Array ...) in zipWith f xs xs While .oating let bindings opens up the potential for further optimisations, \nwe are careful to not increase the lifetime of bound variables, as this would increase live memory usage. \n5. Benchmarks Benchmarks were conducted on a single Tesla T10 processor (com\u00adpute capability 1.3, 30 \nmultiprocessors = 240 cores at 1.3GHz, 4GB RAM) backed by two quad-core Xenon E5405 CPUs (64-bit, 2GHz, \n8GB RAM) running GNU/Linux (Ubuntu 12.04 LTS). The reported GPU runtimes are averages of 100 runs. 5.1 \nExecution Overheads Runtime program optimisation, code generation, kernel loading, data transfer, and \nso on can contribute signi.cant overheads to short lived GPU computations. Accelerate mitigates these \noverheads via caching and memoisation. For example, the .rst time a particu\u00adlar expression is executed \nit is compiled to CUDA code, which is reused for subsequent invocations. In the remainder of this section \nwe factor out the cost of runtime code generation, and the associ\u00adated caching, by reporting just the \nruntimes of the GPU kernels for our system as well as the others we compare against. 5.2 Dot product \nDot product uses the code from Section 2.1. Without fusion the in\u00ad termediate array produced by zipWith \nis created in GPU memory before being read back by fold. This is a simple memory-bound benchmark; hence, \nfusion roughly doubles the performance. The Data.Vector baseline is sequential code produced via stream \nfusion [12], running on the host CPU. The Repa version runs in parallel on all eight cores of the host \nCPU, using the fusion method reported in [17]. The NDP2GPU version [4] compiles NESL code [6] to CUDA. \nThe performance of this version suffers because the NDP2GPU compiler uses the legacy NESL compiler for \nthe front\u00adend, which introduces redundant administrative operations that are not strictly needed when \nevaluating a dot product. The Accelerate version is still slightly slower than CUBLAS. The fused code \nembeds a function of type (sh -> a) into the reduction. The extra arithmetic for converting the multidimentional \nsh index to a linear index is signi.cant for this simple benchmark. 5.3 Black-Scholes options pricing \nBlack-Scholes uses the code from Figure 1. The source contains several let-bound variables that are used \nmultiple times, and with\u00adout sharing recovery the corresponding values are recomputed for each occurrence. \nThe Accelerate version is faster than the reference CUDA version because the latter contains a common \nsubexpression that is not eliminated by the CUDA compiler. The CUDA version is part of the of.cial NVIDIA \nCUDA distribution. The common subexpression performs a single multiplication. 5.4 N-Body gravitational \nsimulation The n-body example simulates Newtonian gravitational forces on a set of massive bodies in \n3D space, using the naive O(n 2) algo\u00adrithm. In a data-parallel setting, the natural implementation .rst \ncomputes the forces between every pair of bodies, before reduc\u00ading the components applied to each body \nusing a segmented sum. Without fusion this approach requires O(n 2) space for the interme\u00addiate array \nof forces, which exhausts the memory of our device for more than about 5k bodies. With fusion, the reduction \nconsumes each force value on-the-.y, so that the program only needs O(n) space to store the .nal force \nvalues. Even with fusion the hand-written CUDA version is over 10\u00d7 faster, as it uses on-chip shared \nmemory to reduce the memory bandwidth requirements of the program. The shared memory is essentially a \nsoftware managed cache, and making automatic use of it remains an open research problem [21]. 5.5 Mandelbrot \nfractal The Mandelbrot set is generated by sampling values c in the com\u00adplex plane, and determining whether \nunder iteration of the complex quadratic polynomial zn+1 = z 2 + c that |zn| remains bounded n however \nlarge n gets. As recursion in Accelerate always proceeds through the host language, we de.ne each step \nof the iteration as a collective operation and unfold the loop a .xed number of times. Table 2 shows \nthat without fusion performance is poor because storing each step of the iteration saturates the memory \nbus. The CUDA version is about 70% faster because it includes a custom software thread block scheduler \nto manage the unbalanced work\u00adload inherent to this benchmark.  5.6 Fluid Flow Fluid .ow implements \nJos Stam s stable .uid algorithm [32], a fast approximate algorithm intended for animation and games, \nrather than accurate engineering simulation. The core of the algorithm is a .nite time step simulation \non a grid, implemented as a matrix relaxation involving the discrete Laplace operator (\\2). Relaxation \n Contender Accelerate Accelerate Accelerate Benchmark Input Size (ms) full (ms) no fusion (ms) no sharing \n(ms) Black Scholes 20M 6.70 (CUDA) 6.19 (92%) (not needed) 116 (1731%) Canny 16M 50.6 (OpenCV) 78.4 (155%) \n(not needed) 82.7 (164%) Dot Product 20M 1.88 (CUBLAS) 2.35 (125%) 3.90 (207%) (not needed) Fluid Flow \n2M 5461 (Repa -N7) 107 (1.96%) (not needed) 119 (2.18%) Mandelbrot (limit) 2M 14.0 (CUDA) 24.0 (171%) \n245 (1750%) 245 (1750%) N-Body 32k 54.4 (CUDA) 607 (1116%) (out of memory) (out of memory) Radix sort \n4M 780 (Nikola) 442 (56%) 657 (84%) 657 (84%) SMVM (protein) 4M 0.641 (CUSP) 0.637 (99%) 32.8 (5115%) \n(not needed) Table 2. Benchmark Summary is performed by stencil convolution using the standard four-point \nLaplace kernel. The program is very memory intensive, performing approximately 160 convolutions of the \ngrid per time step. The Repa Non-zeros version running on the host CPUs is described in [20]; it suffers \nName (nnz/row) CUSPAccelerateAccelerate no fusion from a lack of memory bandwidth compared with the GPU \nversion.  5.7 Canny edge detection Edge detection applies the Canny algorithm [7] to square images of \nvarious sizes. The algorithm consists of seven phases, the .rst six are naturally data parallel and performed \non the GPU. The last phase uses a recursive algorithm to connect pixels that make up the output lines. \nIn our implementation this phase is performed sequentially on a host CPU, which accounts for the non-linear \nslowdown visible with smaller images. We also show the runtime for just the .rst six data parallel phases. \nThe data parallel phases are slightly slower than the baseline OpenCV version. This is as our implementation \nof stencil convolu\u00adtion in Accelerate checks whether each access to the source array is out of bounds, \nand must have boundary conditions applied. To address this shortcoming we intend to separate computation \nof the border region which requires boundary checks, from the main in\u00adternal region which does not [19], \nbut we leave this to future work. 5.8 Radix sort Radix sort implements the algorithm described in Blelloch \n[5] to sort an array of signed 32-bit integers. We compare our implemen\u00adtation against a Nikola [22] \nversion.2 The Accelerate version is faster then Nikola because Nikola is limited to single kernel programs \nand must transfer intermediate results back to the host. Hand written CUDA implementations such as in \nthe Thrust [29] library make use of on-chip shared memory and are approximately 10\u00d7 faster. As mentioned \nearlier, automatically making use of GPU shared memory remains an open research problem [21].  5.9 Sparse-matrix \nvector multiplication (SMVM) SMVM multiplies a sparse matrix in compressed row format (CSR) [9] with \na dense vector. Table 3 compares Accelerate to the CUSP library [3], which is a special purpose library \nfor sparse matrix operations. For test data we use a 14 matrix corpus derived from a variety of application \ndomains [33]. Compared to our previous work [8] the fusion transformation converts the program to a single \nsegmented reduction. The corre\u00adsponding reduction in memory bandwidth puts Accelerate on par with CUSP \nfor several test matrices. In a balanced machine SMVM should be limited by memory throughput, and a dense \nmatrix in sparse format should provide an upper bound on performance. 2 We repeat .gures of [22] as Nikola \nno longer compiles with recent GHC versions. The .gures from [22] were obtained using the same GPU as \nours. Dense 4M (2K) 14.48 14.62 3.41 Protein 4.3M (119) 13.55 13.65 0.26 FEM/Spheres 6M (72) 12.63 9.03 \n4.70 FEM/Cantilever 4M (65) 11.98 7.96 4.41 Wind Tunnel 11.6M (53) 11.98 7.33 4.62 FEM/Harbour 2.37M \n(50) 9.42 6.14 0.13 QCD 1.9M (39) 7.79 4.66 0.13 FEM/Ship 3.98 (28) 12.28 6.60 4.47 Economics 1.27M (6) \n4.59 0.90 1.06 Epidemiology 1.27M (4) 6.42 0.59 0.91 FEM/Accelerator 2.62M (22) 5.41 3.08 2.92 Circuit \n959k (6) 3.56 0.82 1.08 Webbase 3.1M (3) 2.11 0.47 0.74 LP 11.3M (2825) 5.22 5.04 2.41 Table 3. Overview \nof sparse matrices tested and results of the benchmark. Measurements are in GFLOPS/s (higher is better). \nHowever, with matrices such as FEM/Spheres which contain only a few non-zeros per row (S 2 \u00d7 warp size \n= 64), Accelerate is slightly slower than CUSP. We conjecture that this is due to the way our skeleton \ncode vector read of each matrix row is coalesced and aligned to the warp boundary to maximise global \nmemory throughput, but is then not able to amortise this extra startup cost over the row length. Matrices \nwith large vectors and few non-zeros per row (e.g., Epidemiology), exhibit low .op/byte ratio and poorly \nsuit the CSR format, with all implementations performing well below peak. 6. Related work Repa [17] is \na Haskell library for parallel array programming on shared-memory SMP machines. Repa uses the delayed/manifest \nrepresentation split on which our DelayedAcc type is based, though the idea of representing arrays as \nfunctions is folklore. With Repa the conversion between array representations is done manu\u00adally and can \ncause shared expressions to be recomputed rather than stored. Such recomputation can improve runtime \nperformance de\u00adpending on the algorithm. In Accelerate the conversion is automatic and conservative, \nso that shared expressions are never recomputed. Vertigo [13], Nikola [22] and Obsidian [10] are EDSLs \nin Haskell and were mentioned in Section 1. Vertigo is a .rst-order language for writing shaders, and \ndoes not provide higher-order combinators such as map and fold. Nikola uses an instance of Gill s approach \n[14] to sharing recovery, is limited to single GPU kernel programs, and performs no fusion.  Obsidian \n[10] is a lower level language where more details of the GPU hardware are exposed to the programmer. \nRecent versions of Obsidian [11] implement Repa-style delayed pull arrays as well as push arrays. Whereas \na pull array represents a general producer, a push array represents a general consumer. Push arrays allow \nthe intermediate program to be written in continuation passing style (CPS), and helps to compile (and \nfuse) append-like operations. Baracuda [18] is another Haskell EDSL that produces CUDA GPU kernels, though \nis intended to be used of.ine, with the kernels being called directly from C++. The paper [18] mentions \na fusion system that appears to be based on pull arrays, though the mecha\u00adnism is not discussed in detail. \nBarracuda steps around the sharing problem by requiring let-bindings to be written using the AST node \nconstructor, rather than using Haskell s native let-expressions. Delite/LMS [28] is a parallelisation \nframework for DSLs in Scala that uses library-based multi-pass staging to specify complex optimisations \nin a modular manner. Delite supports loop fusion for DSLs targeting GPUs using rewrite rules on a graph-based \nIR. NDP2GPU [4] compiles NESL code down to CUDA. As the source language is not embedded there is no need \nfor sharing recovery. NDP2GPU performs map/map fusion but cannot fuse maps into reduction combinators. \nSato and Iwasaki [30] describe a C++ library for GPGPU pro\u00ad gramming that includes a fusion mechanism \nbased on list homo\u00admorphisms [25]. The fusion transformation itself is implemented as a source to source \ntranslation. SkeTo [24] is a C++ library that provides parallel skeletons for CPUs. SkeTo s use of C++ \ntemplates provides a fusion system similar to delayed arrays, which could be equivalently implemented \nusing CUDA templates. The authors of SkeTo note that the lack of type inference in C++ leads them to \nwrite their array code as nested expressions to avoid intermedi\u00adate variable bindings and their required \ntype annotations. Acknowledgements. This work was supported in part by the Aus\u00adtralian Research Council \nunder grant number LP0989507. References [1] R. Atkey, S. Lindley, and J. Yallop. Unembedding domain-speci.c \nlanguages. In Haskell Symposium, 2009. [2] E. Axelsson. A generic abstract syntax model for embedded \nlan\u00adguages. In ICFP: International Conference on Functional Program\u00adming. ACM, 2012. [3] N. Bell and \nM. Garland. Implementing sparse matrix-vector multi\u00adplication on throughput-oriented processors. In Proc. \nof High Perfor\u00admance Computing Networking, Storage and Analysis. ACM, 2009. [4] L. Bergstrom and J. Reppy. \nNested data-parallelism on the GPU. In ICFP: International Conference on Functional Programming. ACM, \n2012. [5] G. E. Blelloch. Pre.x sums and their applications. Technical Report CMU-CS-90-190, Nov. 1990. \n[6] G. E. Blelloch. NESL: A nested data-parallel language. Technical Report CMU-CS-95-170, Carnegie Mellon \nUniversity, 1995. [7] J. F. Canny. A Computational Approach to Edge Detection. Pattern Analysis and Machine \nIntelligence, (6), 1986. [8] M. M. Chakravarty, G. Keller, S. Lee, T. L. McDonell, and V. Grover. Accelerating \nHaskell array codes with multicore GPUs. In DAMP: Declarative Aspects of Multicore Programming. ACM, \n2011. [9] S. Chatterjee, G. E. Blelloch, and M. Zagha. Scan primitives for vector computers. In Proc. \nof Supercomputing. IEEE Computer Society Press, 1990. [10] K. Claessen, M. Sheeran, and J. Svensson. \nObsidian: GPU program\u00adming in Haskell. In IFL: Implementation and Application of Func\u00adtional Languages, \n2008. [11] K. Claessen, M. Sheeran, and B. J. Svensson. Expressive array constructs in an embedded GPU \nkernel programming language. In DAMP: Declarative Aspects and Applications of Multicore Program\u00adming. \nACM, 2012. [12] D. Coutts, R. Leshchinskiy, and D. Stewart. Stream fusion from lists to streams to nothing \nat all. In ICFP: International Conference on Functional Programming. ACM, 2007. [13] C. Elliott. Programming \ngraphics processors functionally. In Haskell Workshop. ACM Press, 2004. [14] A. Gill. Type-Safe Observable \nSharing in Haskell. In Haskell Sympo\u00adsium, 2009. [15] A. Gill, J. Launchbury, and S. L. Peyton Jones. \nA short cut to deforestation. In FPCA: Functional Programming Languages and Computer Architecture. ACM, \n1993. [16] G. Keller and M. M. T. Chakravarty. On the distributed implementa\u00adtion of aggregate data structures \nby program transformation. In Work\u00adshop on High-Level Parallel Programming Models and Supportive En\u00advironments. \nSpringer-Verlag, 1999. [17] G. Keller, M. M. T. Chakravarty, R. Leshchinskiy, S. L. Peyton Jones, and \nB. Lippmeier. Regular, Shape-polymorphic, Parallel Arrays in Haskell. In ICFP: International Conference \non Functional Program\u00adming. ACM, 2010. [18] B. Larsen. Simple optimizations for an applicative array \nlanguage for graphics processors. In DAMP: Declarative Aspects of Multicore Programming. ACM, 2011. [19] \nB. Lippmeier and G. Keller. Ef.cient Parallel Stencil Convolution in Haskell. In Haskell Symposium. ACM, \n2011. [20] B. Lippmeier, M. Chakravarty, G. Keller, and S. Peyton Jones. Guiding parallel array fusion \nwith indexed types. In Haskell Symposium. ACM, 2012. [21] W. Ma and G. Agrawal. An integer programming \nframework for opti\u00admizing shared memory use on GPUs. In PACT: Parallel Architectures and Compilation \nTechniques. ACM, 2010. [22] G. Mainland and G. Morrisett. Nikola: Embedding compiled GPU functions in \nHaskell. In Haskell Symposium. ACM, 2010. [23] G. Mainland, R. Leshchinskiy, and S. Peyton Jones. Exploiting \nvector instructions with generalized stream fusion. In ICFP: International Conference on Functional Programming. \nACM, 2013. [24] K. Matsuzaki and K. Emoto. Implementing fusion-equipped parallel skeletons by expression \ntemplates. In IFL: Implementation and Appli\u00adcation of Functional Languages. Springer-Verlag, 2010. [25] \nE. Meijer, M. M. Fokkinga, and R. Paterson. Functional programming with bananas, lenses, envelopes and \nbarbed wire. In FPCA: Functional Programming and Computer Architecture, 1991. [26] S. Peyton Jones and \nS. Marlow. Secrets of the Glasgow Haskell Compiler inliner. J. Funct. Program., 12(5), July 2002. [27] \nS. Peyton Jones, S. Marlow, and C. Elliott. Stretching the Storage Manager: Weak Pointers and Stable \nNames in Haskell. In IFL: Imple\u00admentation of Functional Languages. Springer Heidelberg, 2000. [28] T. \nRompf, A. K. Sujeeth, N. Amin, K. J. Brown, V. Jovanovic, H. Lee, M. Odersky, and K. Olukotun. Optimizing \ndata structures in high\u00adlevel programs: New directions for extensible compilers based on stag\u00ading. In \nPOPL: Symposium on Principles of Programming Languages. ACM, 2013. [29] N. Satish, M. Harris, and M. \nGarland. Designing ef.cient sorting algorithms for manycore GPUs. In IPDPS: Parallel and Distributed \nProcessing. IEEE Computer Society, 2009. [30] S. Sato and H. Iwasaki. A skeletal parallel framework with \nfusion optimizer for GPGPU programming. In APLAS: Asian Symposium on Programming Languages and Systems. \nSpringer-Verlag, 2009. [31] S. Sengupta, M. Harris, Y. Zhang, and J. D. Owens. Scan primitives for GPU \ncomputing. In SIGGRAPH/EUROGRAPHICS Symposium on Graphics Hardware. Eurographics Association, 2007. [32] \nJ. Stam. Stable .uids. In SIGGRAPH: Computer graphics and Interactive Techniques. ACM Press, 1999. [33] \nS. Williams, L. Oliker, R. Vuduc, J. Shalf, K. Yelick, and J. Demmel. Optimization of sparse matrix-vector \nmultiplication on emerging mul\u00adticore platforms. Parallel Computing, 35(3), 2009.    \n\t\t\t", "proc_id": "2500365", "abstract": "<p>Purely functional, embedded array programs are a good match for SIMD hardware, such as GPUs. However, the naive compilation of such programs quickly leads to both code explosion and an excessive use of intermediate data structures. The resulting slow-down is not acceptable on target hardware that is usually chosen to achieve high performance.</p> <p>In this paper, we discuss two optimisation techniques, sharing recovery and array fusion, that tackle code explosion and eliminate superfluous intermediate structures. Both techniques are well known from other contexts, but they present unique challenges for an embedded language compiled for execution on a GPU. We present novel methods for implementing sharing recovery and array fusion, and demonstrate their effectiveness on a set of benchmarks.</p>", "authors": [{"name": "Trevor L. McDonell", "author_profile_id": "81479661217", "affiliation": "University of New South Wales, Sydney, Australia", "person_id": "P4261215", "email_address": "tmcdonell@cse.unsw.edu.au", "orcid_id": ""}, {"name": "Manuel M.T. Chakravarty", "author_profile_id": "81548019082", "affiliation": "University of New South Wales, Sydney, Australia", "person_id": "P4261216", "email_address": "chak@cse.unsw.edu.au", "orcid_id": ""}, {"name": "Gabriele Keller", "author_profile_id": "81100011375", "affiliation": "University of New South Wales, Sydney, Australia", "person_id": "P4261217", "email_address": "keller@cse.unsw.edu.au", "orcid_id": ""}, {"name": "Ben Lippmeier", "author_profile_id": "81488641994", "affiliation": "University of New South Wales, Sydney, Australia", "person_id": "P4261218", "email_address": "benl@ouroborus.net", "orcid_id": ""}], "doi_number": "10.1145/2500365.2500595", "year": "2013", "article_id": "2500595", "conference": "ICFP", "title": "Optimising purely functional GPU programs", "url": "http://dl.acm.org/citation.cfm?id=2500595"}