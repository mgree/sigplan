{"article_publication_date": "09-25-2013", "fulltext": "\n Automatic SIMD Vectorization for Haskell Leaf Petersen Dominic Orchard Neal Glew Intel Labs Computer \nLaboratory Intel Labs leaf.petersen@intel.com University of Cambridge aglew@acm.org dominic.orchard@cl.cam.ac.uk \n Abstract Expressing algorithms using immutable arrays greatly simpli.es the challenges of automatic \nSIMD vectorization, since several im\u00adportant classes of dependency violations cannot occur. The Haskell \nprogramming language provides libraries for programming with immutable arrays, and compiler support for \noptimizing them to eliminate the overhead of intermediate temporary arrays. We de\u00adscribe an implementation \nof automatic SIMD vectorization in a Haskell compiler which gives substantial vector speedups for a range \nof programs written in a natural programming style. We com\u00adpare performance with that of programs compiled \nby the Glasgow Haskell Compiler. Categories and Subject Descriptors D.3.4 [Programming Lan\u00adguages]: \nProcessors Compilers; D.3.4 [Programming Languages]: Processors Optimization; D.1.1 [Programming Techniques]: \nAp\u00adplicative (Functional) Programming Keywords Vectorization, SIMD, Compiler Optimization, Haskell, Functional \nLanguages 1. Introduction In the past decade, power has increasingly been recognized as the key limiting \nresource in micro-processors, in part due to the limited battery on mobile devices, but more fundamentally \ndue to the non-linear scaling of power usage with frequency. This has forced micro-processor designs \nto explore (or re-explore) different avenues at both the micro-architectural and the architectural levels. \nOne successful architectural trend has been the increasing emphasis on data parallelism in the form of \nsingle-instruction multiple-data (SIMD) instruction sets. The key idea of data parallelism is that signi.cant \nportions of many sequential computations consist of uniform (or almost uni\u00adform) operations over large \ncollections of data. SIMD instruction sets exploit this at the machine instruction level by providing \ndata paths for and operations on .xed-width vectors of data. Program\u00admers (or preferably compilers) are \nthen responsible for .nding data parallel computations and expressing them in terms of .xed-width vectors. \nIn the context of languages with loops (or compiled to loops), this corresponds to choosing loops for \nwhich multiple it\u00aderations can be computed simultaneously. Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page. Copyrights for components of this work owned by others than the author(s) \nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers \nor to redistribute to lists, requires prior speci.c permission and/or a fee. Request permissions from \npermissions@acm.org. ICFP 13, September 25 27, 2013, Boston, USA. Copyright is held by the owner/author(s). \nPublication rights licensed to ACM. ACM 978-1-4503-2326-0/13/09. . . $15.00. http://dx.doi.org/10.1145/2500365.2500605 \n The task of automatically .nding SIMD vectorizable code in the context of a compiler has been the subject \nof extensive study over the course of decades. For common imperative programming languages, such as C \nand Fortran, in which programs are structured as loops over mutable arrays of data, the task of rewriting \nloops in SIMD vector form is straightforward. However, the task of discov\u00adering which loops permit a \nvalid rewriting (via dependency analy\u00adsis) is in general undecidable, and has been the subject of extensive \nresearch (see e.g. [13] and overview in [15]). Even quite sophisti\u00adcated compilers are often forced to \nrely on programmer annotations to assert that a given loop is a valid target of SIMD vectorization (for \nexample, the INDEPENDENT keyword in HPF [7]). A great deal of the dif.culty in .nding vectorizable loops \nsim\u00adply goes away in the context of a functional language. In functional languages, the absence of mutation \neliminates large classes of the more dif.cult kinds of dependence violations that block vectoriza\u00adtion \nin compilers for imperative languages. In this sense, SIMD vectorization would seem to be quite low-hanging \nfruit for func\u00adtional language compiler writers. In this paper, we describe the implementation of an \nautomatic SIMD vectorization optimization pass in the Intel Labs Haskell Research Compiler (HRC). We \nargue that in a functional language context, this optimization is straight\u00adforward to implement and gives \nexcellent results. HRC was originally developed to compile programs written in an experimental strict \nfunctional language, and the vectorization optimization that we present here was developed in that context. \nHowever, the main bulk of our compiler infrastructure was de\u00adsigned to be fairly language agnostic, and \nwe have more recently used it to build an experimental Haskell compiler using the Glas\u00adgow Haskell Compiler \n(GHC) as a front end. While Haskell is quite a complex language on the surface, the GHC front end reduces \nit to a relatively simple intermediate representation based on System F, which is fairly straightforward \nto compile using HRC. By inter\u00adcepting the GHC intermediate representation (IR) after the main high-level \noptimization passes have run, we gain the bene.t of the substantial and sophisticated optimization infrastructure \nbuilt up in GHC, and in particular we bene.t from its ability to optimize away temporary intermediate \narrays. We make the following three contributions: We design a core language with arrays and SIMD vector \nprim\u00aditives (Section 3).  We present a detailed compositional vectorization process over this language \n(Section 4).  We evaluate our performance compared to GHC with the native and LLVM backends, showing \nsigni.cant scalar speedups over GHC and additional vector speedups of up to 6.5\u00d7 over our own scalar \ncompiler, on a selection of array processing bench\u00admarks (Section 5).  We begin with a brief overview \nof HRC and its relationship to GHC.  2. Intel Labs Haskell Research Compiler HRC provides an optimization \nplatform aimed at producing high\u00adperformance executables from functional-style code. The core in\u00adtermediate \nlanguage of the compiler is a static single-assignment style, control-.ow graph based intermediate representation \n(IR). It is strict and has explicit thunks to represent laziness. While the intermediate representation \nis largely language agnostic, it is not paradigm agnostic. The core technologies of the compiler are \ndesigned speci.cally around compiling functional-language pro\u00adgrams. Most importantly for the purposes \nof this paper, the com\u00adpiler relies on maintaining a distinction between mutable and im\u00admutable data, \nand focuses almost all of its optimization effort on the latter. The compiler is structured as a whole-program \ncompiler however, none of the optimizations in the compiler rely on this property in any essential way. \nIn addition to the usual inlining-style optimizations, conti.cation [5] is performed to turn many uses \nof (mutual) recursion into loops. This optimization is crucial for our purposes since we structure our \nSIMD-vectorization optimization as an optimization on loops. Many standard compiler optimizations have \nbeen implemented in the compiler, including loop-invariant code motion and a very general simpli.er in \nthe style of Appel and Jim [2]. The compiler also implements a number of inter\u00adprocedural representation \noptimizations using a .eld-sensitive .ow analysis [11]. The compiler generates output in a modi.ed extension \nof the C language called Pillar [1]. Among other things, the Pillar lan\u00adguage supports tail calls and \nsecond-class continuations. Most im\u00adportantly, it also provides the necessary mechanisms to allow for \naccurate garbage collection of managed memory. While an early version of Pillar was implemented as a \nmodi.cation of a C com\u00adpiler, Pillar is currently implemented as a source to source transla\u00adtion targeting \nstandard C, which is then compiled using either the Intel C Compiler or GCC. Our garbage collector and \na small run\u00adtime are linked in to produce the .nal executable. 2.1 Haskell and GHC The Haskell language \nand the GHC extensions to it provide a large base of useful high-level libraries written in a very powerful \npro\u00adgramming language. These libraries are often written using a high\u00adlevel functional-language style \nof programming, relying on excel\u00adlent compiler technology to achieve good performance. While we wished \nto compile Haskell programs and take advantage of the many libraries, we had no interest in attempting \nthe monumental task of duplicating all of the high-level compiler technology imple\u00admented in GHC. The \napproach we took therefore was to essentially use GHC as a Haskell frontend, thereby taking advantage \nof the existing type-checker and high-level optimizer. GHC provides some support for writing its main \nSystem-F-style internal representation (called Core) out to .les. While this code is not entirely maintained, \nwe were able to easily modify it to support all the current Core features we require. We use this facility \nto build a Haskell compiler pipeline by .rst running GHC to perform type checking, desugaring, and all \nof its Core-level optimizations, and then writing out the Core IR immediately before it would otherwise \nbe translated to the next-lower representation. HRC then reads in the serialized Core, and after several \ninitial transformations and optimizations (including making thunks explicit), performs a closure-conversion \npass and transforms it down to our main CFG based intermediate representation for subsequent optimization. \n 2.2 GHC Modi.cations Using this approach, we are able to provide a fairly close to func\u00adtionally complete \nimplementation of the Haskell language as im\u00adplemented by GHC. However, some of the choices of primitives \nused in GHC do not match up well with the needs of HRC, and so some modi.cation of GHC itself was required. \nA notable example of this for the purposes of this paper is the treatment of arrays in the Data.Vector \nlibraries. The GHC libraries Data.Vector and REPA [6] provide clean high-level interfaces for generating \nimmutable arrays. Program\u00admers can use typical functional-language operations like maps, folds, and zips \nto compute arrays from other arrays. These oper\u00adations are naturally data parallel and amenable to SIMD \nvector\u00adization by HRC. For example, consider a map operation over an immutable array represented with \nthe Data.Vector library. Such ar\u00adrays are represented internally to the libraries using streams, and \nso map .rst turns the source array into a stream, applies the map operation to the stream, and then unstreams \nthat back into the .nal array. Unstreaming is implemented by running a monadic compu\u00adtation that .rst \ncreates a mutable array of the appropriate size, then initializes the elements of the array using array \nupdate, and .nally does an unsafe freeze of the mutable array to an immutable array type [8]. GHC has \noptimizations which usually eliminate all of the intermediate streaming, mapping, and unstreaming operations. \nThe result of this optimization is a piece of code that sequences the array creation, the calling of \na tail-recursive function that initial\u00adizes the array elements, and the unsafe freeze. After translation \ninto our internal representation, HRC can then contify the tail-recursive function into a loop, resulting \nin a natural piece of code that can be effectively optimized using standard loop-based optimizations \nadapted to leverage the bene.ts of immutability. Unfortunately, observe that as described in the previous \npara\u00adgraph we do not actually get the bene.ts of immutability with an unmodi.ed GHC. Instead, we end \nup with code that creates a mutable array and then initializes it with general array update operations \na style of code that we argue is drastically harder to optimize well in general, and to perform SIMD \nvectorization on in particular. As we discuss in Section 4.7, immutable arrays make determining when \na loop can be SIMD vectorized much easier. For our purposes then, what we would like GHC to generate \ninstead of the code described above is code that creates a new uninitialized immutable array and then \nuses initializing writes to initialize the array. The two invariants of initializing writes are that \nreading an array element must always follow the initializing write of that ele\u00adment, and that any element \ncan only be initialized once. This style of write preserves the bene.ts of immutability from the compiler \nstandpoint, since any initializing write to an element is the unique de.nition of that element. Initializing \nwrites are key to representing the initialization of an immutable array using the usual loop code, while \npreserving the ability to leverage immutability. In order to get immutable code written in this style \nfrom GHC and the libraries discussed above, we modi.ed GHC with new primitive immutable array types1 \nand primitive operations on them. The additional operations include creation of a new uninitialized array, \ninitializing writes, and a subscript operation. We also mod\u00adi.ed the Data.Vector library and a small \npart of the REPA library to use these immutable arrays instead of mutable arrays and unsafe freeze (of \ncourse, as with the standard GHC primitives, correctness requires that the library writer use the primitives \ncorrectly). With these modi.cations, the code produced by GHC is suitable for op\u00adtimization by HRC, including \nin many cases SIMD vectorization.  2.3 Vectorization in HRC SIMD vectorization in HRC targets loops \nfor which the compiler can show that executing several iterations in parallel is seman\u00adtics preserving, \nrewriting them to use SIMD vector instructions. Usually these loops serve to initialize immutable arrays, \nor to per\u00ad 1 GHC does already have some such types however, we added new ones to avoid compatibility \nissues  Register kind k ::= s | v k k k Variables x ,y ,z ,... -231 Constants c ::= , . . . , (231 - \n1) Operations op ::= +, -, *, /, . . . Instruction I ::= z s = c vs s | z = (x0, . . . , x 7) | z s = \nx v!i s ss | z = op(x0, . . . , x n) v vv | z = (op)(x0, . . . , x n) | z s = new[x s] s ss | z = x [y \n] | z v = x s[(y v)] | z v = (x v)[y s] | z v = (x v)[(y v)] ss s | x [y ] . z | x s[(y v)] . z v | (x \nv)[y s] . z v | (x v)[(y v)] . z v sss ss Comparisons cmp ::= x < y | x = y s | x = y Labels L ::= L0 \n, L1 , . . . Transfers t ::= goto L(x0 k , . . . , x n k ) | if (cmp) goto L0(x0 k , . . . , x n k ) \nelse goto L1(y0 k , . . . , y k ) m | halt Blocks B ::= L(x k 0 , . . . , x k n): I0 . . . Im t Control \nFlow Graph ::= Entry L in {B0, . . . , Bn} Figure 1. Syntax form reductions over immutable arrays. The \ncompiler generates the SIMD loops and all of the related control logic simply as code in our intermediate \nlanguage. Rather than requiring the compiler to generate only the SIMD instructions provided by the target \nma\u00adchine, we provide a more uniform SIMD instruction set that may be a superset of the machine-supported \ninstructions. Our small run\u00adtime provides a library of impedance-matching macros that imple\u00adments the \nuniform SIMD instructions in terms of SIMD intrisics understood by the underlying C compiler. The C compiler \nis then responsible for low-level optimizations including register alloca\u00adtion and instruction selection. \nIn the next section we de.ne a vector core language with which to discuss the vectorization process. \nThis core language is clean, uniform, and is almost exactly faithful to (a small subset of) our actual \nintermediate representation. 3. Vector Core language We de.ne here a small core language in which to \npresent the vec\u00adtorization optimization. We restrict ourselves to an intra-procedural fragment only, \nsince for our purposes here we are not interested in inter-procedural control .ow. Programs in the language \nare given as control-.ow graphs written in a variant of static single-assignment (SSA) form. To keep \nthings concrete, our language is based on a 32\u00adbit machine with vector registers that are 256-bits wide; \nhandling other register and vector widths is a straightforward extension. 3.1 Objects The primitive objects \nof the vector core language consist of heap objects and register values. Heap objects are variable-length \nim\u00admutable arrays subscripted by 32-bit integers. The elements con\u00adtained in the .elds of heap objects \nare constrained to be 32-bit reg\u00adister values. Register values are small values that may be bound to \nvariables and are implicitly represented at the machine level via registers or spill slots. The register \nvalues consist of 32-bit integers, 32-bit pointers to heap values, and 256-bit SIMD vectors each con\u00adsisting \nof eight 32-bit register values. Adding additional primitive register values such as .oating-pointer \nnumbers, booleans, etc. is straightforward and adds nothing essential to the presentation. 3.2 Variables \nVariables are divided into two classes according to size: scalar variables x s that may only be bound \nto 32-bit register values and vector variables x v that may only be bound to 256-bit vector values. In \npractice of course, a real intermediate language will also have additional variable sizes: 64-bit variables \nfor doubles and 64-bit integers, and a range of vector widths possibly including 128-and 512-bit vectors. \nNote that the variables x s and x v are considered to be distinct variables. Informally, we will sometimes \nuse x v to denote the variable containing a vectorized version of an original program variable x s however, \nthis is intended to be suggestive only and has no semantic signi.cance.  3.3 Instructions The instructions \nof the vector language serve to introduce and oper\u00adate on the primitive objects of the language. Most \nof the instructions bind a new variable of the appropriate register kind, with the excep\u00adtion of the \narray-initialization instructions which write values into uninitialized array elements. The move instruction \nz s = c binds a variable to the 32-bit integer constant c. The vector introduction vs s instruction z \n= (x0, . . . , x 7) binds the vector variable z v to a vec\u00adtor composed of the contents of the eight \nlisted 32-bit variables. We vs s use the derived syntax z = (c0, . . . , c 7) to introduce a vector of \nconstants this may be derived in the obvious way by .rst binding each of the constants to a fresh 32-bit \nvariable. A component of a vector may be selected out using the select instruction z s = x v!i, which \nbinds z s to the ith component of the vector in x v (where i must be in the range 0 . . . 7). s ss The \ninstruction z = op(x0, . . . , x n) corresponds to a family of instructions operating on primitive 32-bit \nintegers, indexed by the operation op (e.g. addition, subtraction, negation etc.). Primitive operations \nmust be fully saturated that is, the number of operands provided must equal the arity of the given operation. \nWe lift the primitive operations on integers to pointwise primitive operations v vv on vectors with the \nz = (op)(x0 , . . . , x n) instruction, which applies op pointwise across the vectors. That is, semantically \nz v = (op)(x0 v , . . . , x n v ) behaves identically to a sequence of instructions performing the scalar \nversion of op to each of the corresponding elements of the argument vectors, and then packaging up the \nresult as the .nal result vector. Array objects in the vector language are immutable sequences of 32-bit \ndata allocated in the heap. New uninitialized array objects are allocated via the z s = new[x s] instruction \nwhich allocates a new array of length x s (where x s must contain a 32-bit integer) and binds z s to \na pointer to the array. The initial contents of the array are unde.ned, and every element must be initialized \nbefore being read. It is an unchecked invariant of the language that every element of the array is initialized \nbefore it is read, and that every element of the array is initialized at most once. It is in this sense \nthat the arrays are immutable the write operation on arrays serves only as an initializing write but \ndoes not provide for mutation. Ordinary ss s scalar element initialization is performed via the x [y \n] . z operation which initializes the element of array x s at offset y s to s ss z s. Scalar reads from \nthe array are performed using the z = x [y ] instruction, which reads a single element from the array \nx s at offset y s and binds z s to it.  In order to support SIMD vectorization, the vector language \nalso provides a general set of vector subscript and initialization operations permitting vectors of elements \nto be read from and written to arrays (or vectors of arrays) in a single instruction. The set of instructions \nwe provide are more general than what is supported by most existing machine architectures. This is done \nintentionally since it is often bene.cial to vectorize a loop even if some instructions must be emulated \nvia translation to scalar operations. Moreover, we believe it is useful to present the language in its \nmost general form it is always possible to restrict the set of generated instructions as desired. We \ndefer discussion of the situations in which these various styles of loads and stores arise to subsequent \nsections on the actual vectorization operation. The simplest vector array loads and stores are the operations \nthat read multiple elements directly from a single array into a vector variable, or initialize multiple \nelements of a single array with elements contained in a vector variable. The instruction z v = x s[(y \nv)] takes a single array x s and a vector of offsets y v and binds z v to a vector containing the elements \nof x s from the given offsets. That is, semantically we may treat the instruction z v = x s[(y v)] as \nequivalent to the following list of instructions: y s 0 = y v!0 z s 0 = x s[y s 0] . . . y s 7 = y v!7 \nz s 7 = x s[y s 7] v s s = (z 7) z 0, . . . , z This instruction is commonly known as a gather instruction. \nThe initializing store x s[(y v)] . z v, commonly known as the scatter instruction, writes the elements \nof the vector z v to the .elds of x s at offsets given by the vector y v . There are several interesting \nspecial cases of scatters and gathers that are worth further discussion. A common idiom that arises in \nSIMD vectorization consists of array loads and stores such that the index vector is constructed by adding \nmultiples of a constant stride to a scalar index. We choose to introduce this idiom as derived syntax \nas follows, where i is a constant valued stride: def vss vs z = x [(y :i)] = z = x [(y v)] v def ss sv \nx [(y :i)] . z = x [(y v)] . z . . y v = (y s , . . . , y s) l where y v = (0, . . . , 7 * i) . b v v \nv y = (+)(yl , y b ) Note that the stride multiplications (e.g. 7 * i) are meta-level oper\u00adations. We \nalso provide derived syntax for the further special case in which the stride is known to be one: def \nvss vss z = x [(y :)] = z = x [(y :1)] v def x s[(y s:)] . z = x s[(y s:1)] . z v For the purposes of \nsimplicity of the vector language, we have left these as derived forms. However, it is worth noting that \nmany archi\u00adtectures provide support for these idioms directly (and in fact may provide support only for \nthese idioms) and hence from a pragmatic standpoint it can be important to provide primitive support \nfor these addressing modes. A second mode of addressing for array loads and stores covers the case where \na vector of arrays is indexed pointwise by a single .xed scalar offset. The z v = (x v)[y s] instruction \nproduces a vector z v such that the ith element of z v is produced by indexing the ith array from the \nvector of arrays x v, using offset y s. Similarly, the (x v)[y s] . z v instruction sets the element \nof the ith array in the vector x v at offset y s to the value in the ith position of z v . The .nal mode \nof addressing for array loads and stores covers the case in which a vector of arrays is indexed pointwise \nusing v v)[(y a vector of offsets. The z = (x v)] instruction produces a vector z v such that the ith \nelement is produced by indexing the ith array from the vector of arrays x v using the ith offset from \nthe vector of offsets y v. Similarly, the (x v)[(y v)] . z v instruction sets the element of the ith \narray in the vector x v at the offset given by the ith element of the vector of offsets y v to the value \nin the ith position of z v .  3.4 Basic Blocks Instructions are grouped into basic blocks consisting \nof a labeled entry, a sequence of instructions, and a transfer which terminates the block and either \ntransfers control to another block or terminates the program. We assume an in.nite supply of distinct \nprogram labels ranged over by metavariable L used to label basic blocks that we distinguish by numbering \ndistinct labels with distinct integers (as L0 , L1, etc.). Basic block headers consist of the label of \nthe block and a list of entry variables which are de.ned on entry to the block. The entry variables are \ngiven their de.ning values by the transfer which transfers control to the block. Block entry variables \nmay have scalar or vector kind. Transfers terminate basic blocks, and serve to transfer control within \na program. The goto L(x0 k , . . . , x n k ) transfer shifts control to the block labeled with L. Well-formed \nprograms must have matching arities on transfers and the block headers which they target, that is, the \nblock labeled with L must have entry variables kk kk z0 , . . . , z n. The transfer of control effected \nby goto L(x0 , . . . , x n) k k also de.nes the entry variables z0 , . . . , z n to be the values of \nk k x0 , . . . , x n. The conditional control transfer kk kk if (cmp) goto L0(x0 , . . . , x n) else \ngoto L1(y0 , . . . , y m) behaves as goto L0(x0 k , . . . , x n k ) if the comparison holds, and as k \nk goto L1(y0 , . . . , y m) if the comparison does not hold. Note that the targets of the two different \narms of the conditional are not required to be distinct. The halt transfer terminates the program. We \ndo not consider inter-procedural control-.ow in this core language since it is not relevant to the vectorization \noptimization however it is straight\u00adforward to add additional transfers to account for this if desired. \n 3.5 Programs Programs in the core language consist of a single control .ow graph. A control .ow graph \nconsists of a designated entry label L and set of associated basic blocks, each with a distinct label, \none of which (the entry block) is labeled with the entry label L. The entry block must expect no entry \nvariables. Program execution proceeds by starting at the designated entry block and executing block instructions \nand transfers until a halt instruction is reached (if ever). Well-formed programs must not contain transfers \nwhose target label is not in the control-.ow graph, and every block in the control-.ow graph must be \nlabeled with a distinct label. As usual with static single-assignment form, variables may only be de.ned \nonce, must be de.ned before their .rst use, and have scope given by the dominator tree of the control-.ow \ngraph rooted at the entry label. The style of static single-assignment form used in this core language \nis similar to that used in compilers such as the MLton compiler [16].  4. Vectorization The SIMD-vectorization \noptimization attempts to rewrite a loop to a new loop that executes multiple iterations of the original \nloop on each iteration using SIMD instructions and registers. While the core idea is quite simple, there \nare a number of issues that must be addressed in order to preserve the semantics of the original program. \nIn the .rst part of this section we introduce the key issues using a series of examples. Before doing \nso, we .rst review some preliminary concepts. 4.1 Loops We have informally described our vectorization \nefforts as focusing on loops. Loops tend to account for a high proportion of executed instructions in \nprograms, and as such are natural targets for intro\u00adducing SIMD-vector code. We de.ne loops using the \nstandard no\u00adtion of a natural loop, merging loops with shared headers in the usual way to ensure that \nthe nesting structure of loops form a tree. We do not assume a reducible control-.ow graph (since in \ngeneral the translation of mutually-recursive functions into loops may in\u00adtroduce irreducible control \n.ow). We focus our vectorization efforts more speci.cally on the innermost loops that make up the leaves \nof the loop forest of the control-.ow graph. For simplicity, we only target bottom-test loops. A separate \noptimization pass inverts top\u00adtest loops to form bottom-test loops, both to enable vectorization and \nloop-invariant code motion. 4.2 Induction variables Our notion of induction variable is standard, but \nwe review it in some detail here since induction variables play a critical role in our algorithm. We \nde.ne the base induction variables of a loop to be the entry variables of the entry block of the loop \nsuch that the de.nition of that variable provided on the loop back edge is produced by adding a constant \nto the variable itself, and where the initial value passed into the loop is a compile-time constant. \nThe step of a base induction variable is the constant added each time around the loop. The full set of \ninduction variables is the least set of variables satisfying: A base induction variable is an induction \nvariable. s ss A variable de.ned by x = +(y , z ), where y s is an induction variable and z s is a constant \n(de.ned by z s = c), is an induction variable. s ss A variable de.ned by x = *(y , z ), where y s is \nan induction variable and z s is a constant, is an induction variable. Our implementation also deals \nwith the symmetric versions of the last two cases, and also considers operations such as subtraction \nand negation. Arbitrary initial values can be allowed for base in\u00adduction variables and loop invariants \ncan be allowed in place of constants at the cost of some additional complexity as discussed below, but \nwe do not currently support this. Induction variables can be characterized as af.ne functions of the \niteration count of the loop. The characteristic function for an induction variable is is an equation \nof the form is = p*#+d where p is a constant (the step of the induction variable), and d is also a constant \n(the initial value of the induction variable). The symbol # stands for the iteration number: the value \nof the induction variable is for iteration j can be computed directly from its characteristic function \nsimply by replacing # with j. The characteristic function for an induction variable is derived as follows: \nThe characteristic function of a base induction variable is p * # + d where p is the step of the base \ninduction variable as de.ned above, and d is the constant initial value of the induction variable. The \ncharacteristic function of an induction variable de.ned by x s = +(y s , z s) is p * # + d + c where \nz s is a constant (de.ned by z s = c), and p * # + d is the characteristic function of y s .  The characteristic \nfunction of an induction variable de.ned by  s ss x = *(y , z ) is c*p*#+ c*d where z s is a constant \n(de.ned by z s = c), and p * # + d is the characteristic function of y s . A subtle but important point \nis that in these computations the com\u00adpiler must ensure that the over.ow semantics of the underlying \nnu\u00admeric type are respected to avoid introducing or eliminating nu\u00admeric over.ow. It is possible to extend \nthe de.nition of charac\u00adteristic functions to allow loop invariants to take the place of the constants \nin the above de.nition at the expense of representing the characteristic function symbolically, thereby \nsomewhat complicat\u00ading code generation and over.ow avoidance as discussed below.  4.3 Vectorization \nby example Consider the following simple program which computes the point\u00adwise sum of two arrays bs and \nc s, each of length ls . L0(): L1(is): (1) a s = new[ls] x s = bs[is] is ss 0 = 0 y = c [is] s ss goto \nL1(i0 s) z = +(x ,y ) a s[is] . z s is 1 = +(is , 1) if (is 1 < ls) goto L1(i1 s) else goto Lexit (is \n1) Ignoring the crucial issue of whether or not it is in fact valid to vectorize this loop, there are \na number of issues that need to be addressed in order to produce a vector version of this program. 4.3.1 \nThe vector loop The core of the vectorization optimization is to produce an inner loop, each iteration \nof which executes multiple iterations of the original loop. For Example 1, this corresponds to generating \ncode to vector-load eight elements of the arrays b and c at appropriate offsets, perform a pointwise \nvector addition of the loaded elements, and to perform a vector write of the eight result elements into \nthe new array a. The comparison which controls the exit test at the bottom of the loop must also be adjusted \nappropriately to account for the fact that multiple iterations are being performed. Finally, on the exit \nedge of the loop, the use of the last value of the induction variable is 1 must also be accounted for. \nTo motivate the general treatment of the vectorization optimiza\u00adtion, we .rst consider the individual \ninstructions of Example 1, be\u00adginning with the load instruction x s = bs[is]. Executing this in\u00adstruction \nduring iteration j of the loop loads a single element from the array bs at an offset given by the value \nof is. In the vector loop, we wish to perform iterations j, j + 1, . . . , j + 7 simulta\u00adneously. The \nvector version of this instruction then must load eight elements from bs at offsets given by the values \nof is at iteration j, j + 1, . . . , j + 7. A natural .rst attempt at vectorization might be to simply \nassume that vector versions of all variables are available, and to then generate the corresponding vector \ninstruction using the vector variables. In this case, if we assume the existence of vari\u00adables bv and \niv containing the values of bs and is for iterations j, . . . , j + 7 then the vector instruction x v \n= (bv)[(iv)] puts into x v the appropriate values of x s for iterations j, . . . , j + 7. While this \napproach is correct (and in the most general case is sometimes necessary) a key insight in SIMD vectorization \nis that it is often possible to do much better by using knowledge of loop invariants and induction variables. \nIn this case, the variable bs is loop invariant, and consequently bv will consist simply of eight copies \nof the same value. Semantically then, we can replace our use of the general vector load instruction with \nthe simpler gather instruction x v = bs[(iv)], which does not require us to produce the vector version \nof bs and which may also have a more ef.cient implementation. More importantly, the variable is here \nis a base induction variable with step 1. It is consequently possible to predict the value of is at each \nof the next eight iterations: that is, the value of iv will always be (is, is+1, . . . , is+7). The vector \nload can then be seen to be a load of eight contiguous elements from bs, for which we have provided the \nderived instruction x v = [(is:)], which  bs performs a contiguous load of eight elements, as desired. \nSince this form of load is generally well supported in vector hardware, generating this specialized form \nin preference to the general form is highly desirable. The second load instruction can then be vectorized \nin an exactly analogous fashion, resulting in two vector variables x v and y v. The s ss addition instruction \nz = +(x , y ) can then be computed using v vv vector addition on the vector variables, as z = (+)(x , \ny ). To produce a vector version of the instruction a s[is] . z s which writes the computed value to \nthe new array we follow a similar argument to that above for the loads to produce the a s[(is:)] . z \nv instruction which performs a contiguous write of the values in z v into the array a s starting at offset \nis . The last instruction of the loop, is 1 = +(is , 1), is the induction variable computation, and as \nsuch requires special treatment. The result of this instruction is used for three purposes: in the computa\u00adtion \nof the test to decide whether to proceed with another iteration, to provide a new value for the induction \nvariable in the next itera\u00adtion, and to provide the value passed out on the exit edge. For the latter \ntwo uses, the required value is easy to see. Each iteration of the vector loop corresponds to eight iterations \nof the scalar loop, and we require on entry to the loop that the induction variable is contain the value \nappropriate for the .rst of the eight iterations. Given the value of is for iteration j then, the back \nedge of the loop requires the value of is 1 for iteration j + 7, which is one plus the value of is on \niteration j + 7. Similarly, the exit edge of the loop requires the value of is 1 at iteration j+7. Since \nthe computation of the value of is for each iteration depends on the value of itself in the previous \niteration, we might imagine ourselves to be stuck. However, the nature of induction variables as af.ne \ntransformations means that we can compute the value of is 1 at iteration j+7 directly: each iteration \nadds 1 to the original value, hence the value of is at iteration j + 7 is given by is + 7, and the value \nof is 1 at iteration j + 7 is is + 7 + 1. Hence we can compute the appropriate value of is 1 for both \nthe back and the exit edge as is + 8. The remaining use of the induction variable is 1 is to compute \nthe loop exit test. The key insight is that it is no longer suf.cient to check that there is at least \none iteration remaining: in order to exe\u00adcute the vector loop again, we must have at least eight remaining \nit\u00aderations. Upon completion of a vector iteration computing scalar it\u00aderations j, . . . , j + 7, the \nquestion that must be answered is whether the scalar loop would always execute iterations j + 8, . . \n. , j + 15. Because of the monotonic nature of the induction variable compu\u00adtation this in turn can be \nreduced to the question of whether or not the value of is 1 at iteration j + 14 is less than ls. Since \nis 1 is an in\u00adduction variable which is incremented by 1 on each iteration, this corresponds to changing \nthe exit test to ask whether is + 15 < ls .  4.3.2 Entry and exit code Using the ideas from previous \nsection, it is straightforward to write a vector version of the core loop. In addition to the issues \ntouched on above, vectorizing this example also requires accounting for the possibility that there may \nbe insuf.cient iterations to allow use of the vector loop, and also for the possibility that there may \nbe extra iterations left over if the exit test succeeds with fewer than eight but more than zero iterations \nleft. To account for these we keep around the original scalar loop, and use it to perform any iterations \nthat cannot be computed with the vector loop. A preliminary test is inserted before the loop choosing \neither to target the scalar loop (if less than eight total iterations will be performed) or the vector \nloop (otherwise). Similarly, after the vector loop exits, a check is done for the presence of extra iterations, \nwhich if required are again performed using the scalar loop. Using these transformations, we arrive at \nthe .nal desired vec\u00adtorized program. check L0 (): L(): a s = new[ls] is 0 = 0 if (is 3 < ls) goto L1(is \n3) if (7 < ls) goto L2(is 0) else goto Lexit (is 3) else goto L1(i0 s) L2(is 2): L1(is): v = bs[(is s \n= bs[is x 2:)] x ] vs ss y = c [(i2 s:)] y = c [is] v vv sss z = (+)(x ,y ) z = +(x ,y ) sv ss a [(i2 \ns:)] . z a [is] . z is 3 = +(is 2, 8) is 1 = +(is , 1) is 4 = +(is 2, 15) if (is 1 < ls) goto L1(is 1) \nexit (is if (i4 s < ls) goto L2(i3 s) else goto L1)check() else goto L (2)  4.4 Automatic vectorization \nThe reasoning of the previous section leads us to the desired result, but seems completely ad hoc and \nunsuitable to implementation in a compiler. Fortunately, it is possible to derive a simple composi\u00adtional \ntransformation which accomplishes the same thing in a gen\u00aderal manner. The essential design principles \nthat guide the design of the op\u00adtimization are those of orthogonality and compositionality. As we will \nsee, while the local transformation of instructions produces specialized code depending on the particulars \nof the instruction, each instruction is nonetheless translated in a compositional fash\u00adion in the sense \nthat the translation does not depend on how the re\u00adsult is used. A consequence of this choice is that \nthe compositional translation may produce numerous extraneous, redundant, or loop\u00adinvariant instructions, \nwhich could be avoided in a less composi\u00adtional approach. The principle that we follow is that the elimina\u00adtion \nof extraneous, redundant, and loop-invariant instructions is an orthogonal issue, which is already addressed \nby dead-code elimina\u00adtion, common sub-expression elimination, and loop-invariant code motion respectively. \nSo long as the vectorization transformation is de.ned appropriately, we can safely rely on these optimizations \nto sweep away the chaff, leaving behind only the essential bits. The core of the vector transformation \nworks by translating each scalar instruction into a sequence of instructions which compute three separate \nvalues: the scalar value, the vector value, and the last value. For a vector loop computing iterations \nj, . . . , j + 7 of a scalar loop, the scalar value of a variable x is the value computed at iteration \nj, the last value is the value computed at iteration j+7, and the vector value is the vector containing \nthe computed values for all of the iterations. While it is clear that there is always a degenerate implementation \nthat simply computes the vector variable and then computes the scalar and last values by projecting out \nthe .rst and last iteration value, we often compute the scalar and last values separately. This is crucial \nfor a number of reasons: in the .rst place, we may be able to compute the vector variable more ef.ciently \nas a direct function of the scalar value; but more importantly, it will frequently be the case that the \nvector variable (or similarly the last value) will be unused. It is therefore important not to introduce \na data-dependence of the scalar (or last) value on the vector value lest we keep an expensive-to-compute \nvector value live unnecessarily.  For clarity in the translation, we de.ne meta-operators for pro\u00adducing \nfresh variables from existing scalar variables. For a variable x s, we take fv x s to be a fresh scalar \nvariable, which by convention will contain the scalar value of x s in the vector loop. Similarly, we \ns take lv x to be a fresh scalar variable, which by convention will contain the last value of x s; and \nwe take vv x v to be a fresh vector variable, which by convention will contain the vector value of x \ns . Induction variables require a small amount of additional mech\u00adanism. For every base induction variable \nis with step s, we say that the af.ne basis of is is the vector (0 * s, 1 * s, . . . , 7 * s) (the multi\u00adplications \nhere are at the meta-level). Note that for a base induction variable is, adding is to each of the elements \nof its af.ne basis gives the values of is for each of the next eight iterations. Other (non-base) induction \nvariables also have af.ne bases, which are computed by code emitted as part of the transformation described \nbelow, starting from the bases of the base induction variables. We use the notation bviv to denote a \nfresh variable which by convention holds the af.ne basis for the scalar induction variable is . It is \noccasionally necessary to introduce a promoted version of a variable: that is, for a variable x s, to \nproduce a vector variable containing the vector (x s , . . . , x s). By convention, we use pvx v to denote \na fresh variable containing the promoted version of x s . In order to simplify the algorithm, it is convenient \nto apply it only to loops for which all variables de.ned in the loop and used outside of the loop (that \nis, live out from the loop) are explicitly passed as parameters on the exit edge. This is in no way necessary, \nbut avoids the need to rename variables throughout the rest of the program to preserve the SSA property. \nThis does not restrict the generality of the algorithm since it is always possible to transform loops \ninto this form. The algorithm applies to single block loops of the form L(is 0, . . . , is ): n I0 . \n. . Im if (is < ) goto L(is n1) ls 01, . . . , is s s else goto Lexit (x0, . . . , x p) where the variables \nis 0, . . . , is are base induction variables, is is an n induction variable with a positive step (the \ncase where the step is zero or negative make no sense), and the variables x0 s , . . . , x p s are the \nlive out variables of the loop. All the instructions I0, . . . , Im must be scalar instructions (and \nthus de.ne scalar variables), and cannot create new arrays. It is straightforward to support exit tests \nof the form is = ls as well. In all cases, the variable ls must be de.ned outside of the loop (and hence \nbe loop invariant). We assume the loop has a preheader,2 that is a block that ends with goto L(iis 0, \n. . . , iis ) and that only this block and the loop n transfer to L. As suggested by the ad hoc development \nfrom the previous section, the vectorization transformation must produce three new pieces of code: an \nentry test that decides whether there are suf.cient iterations to enter the vector loop; the core vector \nloop itself that performs the SIMD iterations; and a cleanup test that checks if there are any iterations \nremaining after the SIMD loop has .nished, which must be processed by the original scalar loop which \nremains in the program unchanged. By convention we take Lvec and Lcheck to be fresh labels for the new \nvector loop and cleanup test respectively. 2 Assuming preheaders does not restrict our algorithm -transforming \na pro\u00adgram into an equivalent one where all loops have preheaders is a standard technique. 4.4.1 Entry \ntests The job of the entry test is to determine if there are suf.cient iterations to execute the vector \nloop, and if not to go straight to the original scalar loop. It also needs to set up some initial values \nfor use in the vector loop. In particular, any variables used in the loop but not de.ned in the loop \nmust have scalar, last value, and vector versions for use by the instructions in the vector loop. First, \nfor each variable y s used in the loop but not de.ned in it (and not an entry variable), we add the following \ninstructions to the end of the instructions of the preheader: fv s s y = y lv s s y = y vv v s y = (y \n,...,y s) Next, we must determine whether the vector loop should be entered at all. The desired property \nof the entry test is that the vector loop should only be entered if there are at least 8 iterations to \nbe performed. The monotonic nature of induction variables means that this question is equivalent to asking \nwhether at least one more iteration remains after performing the .rst 7 iterations: that is, we wish \nto know the result of the exit test of the original loop on the 7th iteration. If the characteristic \nfunction for is is s * # + d, then the value of is on the 7th iteration can be obtained by replacing \n# with 6 and calculating the result statically. The appropriate entry test is then of the form iis < \nls where iis is de.ned as iis = s * 6 + d. The value s * 6 + c is a compiler computed constant, and the \ncompiler can statically determine that this does not over.ow. If we generalize the notion of characteristic \nfunction to include loop\u00adinvariants in addition to constants this test may require generating multiplication \nand addition instructions, and code must also be emitted to fall back to the scalar code if there is \na possibility that the additional arithmetic might over.ow.  4.4.2 Vector loop To generate code for \nthe main vector loop, we perform three oper\u00adations. We .rst generate the last value and vector versions \nof base induction variables, using their steps to produce these directly from the scalar values. For \neach of the original instructions of the loop we then generate instructions to compute the scalar, vector, \nand last value versions of the instruction. Finally, we adjust the exit test. For each entry variable \nis j for 0 = j = n, which is a base induc\u00adtion variable of step sj , we produce the vector, basis, promoted, \nand last value versions of the induction variable as follows. The basis variable and promoted variables \nbvij v and pvij v are de.ned as bviv j = (0 * sj , . . . , 7 * sj )pv j = (fv j , . . . , fv j ) iv \nis is The vector version of is j can then be de.ned directly as follows and the last value lvij s can \nbe de.ned directly using the step. vv = (+)(bv pv iv iv iv j j , j ) lvis j = +(fvis j , 7 * sj ) This \ninitial step de.nes vector and last value variables for each base induction variable of the loop, as \nwell as basis and promoted variables. The translation of each instruction then proceeds com\u00adpositionally, \nwith the translated version of each instruction making use of the vector and last value variables produced \nby the trans\u00adlation of all of the previous instructions. In addition, each induc\u00adtion variable makes \nuse of the previously de.ned basis variables to produce its own vector, last value, basis, and promoted \nvariables directly. The complete de.nition of this translation is given in Fig\u00adure 2. Adjusting the exit \ntest of the loop is again straightforward using the step information. The desired new exit test must \ncheck whether  Instruction Vector translation Side conditions z s = c fv z s = c vv z v = (fv z s , \n. . . , fv z s)lv z s = fv z s z s = +(x s , y s) fv z s = +(fv x s , fv y s) bv z v = bv x v pvz v = \n(fv z s , . . . , fv z s)vv z v = (+)(bv z v , pvz v) lv z s = +(lv x s , fv y s) If z s and x s are \ninduction variables. z s = *(x s , y s) fv z s = *(fv x s , fv y s) bv z v = (*)(bv x v , vv y v) pvz \nv = (fv z s , . . . , fv z s)vv z v = (+)(bv z v , pvz v) lv z s = *(lv x s , fv y s) If z s and x s \nare induction variables. z s = op(x s 0, . . . , x s n) fv z s = op(fv x s 0, . . . , fv x s n) vv z \nv = (op)(vv x v 0, . . . , vv x v n) lv z s = op(lv x s 0, . . . , lv x s n) If z s is not an induction \nvariable. z s = x s[y s] fv z s = fv x s[fv y s] vv z v = fv x s[(fv y s:)] lv z s = fv x s[lv y s] If \nx s is loop invariant, and y s is an induction variable with step 1. z s = x s[y s] fv z s = fv x s[fv \ny s] vv z v = fv x s[(fv y s:i)] lv z s = fv x s[lv y s] If x s is loop invariant, and y s is af.ne with \nstep i z s = x s[y s] fv z s = fv x s[fv y s] vv z v = fv x s[(vv y v)] lv z s = fv x s[lv y s] If x \ns is loop invariant, and y s is not an induction variable. z s = x s[y s] fv z s = fv x s[fv y s] vv \nz v = (vv x v)[fv y s] lv z s = lv x s[fv y s] If x s is not loop invariant, and y s is loop invariant. \nz s = x s[y s] fv z s = fv x s[fv y s] vv z v = (vv x v)[(vv y v)] lv z s = lv x s[lv y s] If both x \ns and y s are not loop invariant. x s[y s] . z s fv x s[(fv y s:)] . vv z v If x s is loop invariant, \nand y s is an induction variable with step 1. x s[y s] . z s fv x s[(fv y s:i)] . vv z v If x s is loop \ninvariant, and y s is an induction variable with step i. x s[y s] . z s fv x s[(vv y v)] . vv z v If \nx s is loop invariant, and y s is not an induction variable. x s[y s] . z s (vv x v)[fv y s] . vv z v \nIf x s is not loop invariant, and y s is loop invariant. x s[y s] . z s x y z v If both x s and y s are \nnot loop invariant. (vv v)[(vv v)] . vv is < ls would succeed for at least eight more iterations. Since \nis is an induction variable, letting s be its step, it increases by s on each iteration. Thus the value \nof is on the next eight iterations will be lvis + 0 * s, . . . , lv + 7 * s. These will all be less than \nls if the is last of them is less than ls (recall that ls is loop invariant). Thus the desired new exit \ncondition is lvis + 7 * s < ls . Putting this all together the vector loop will be as follows, where \ni's is a fresh variable: Lvec (fvis fvis 0, . . . , n): Instructions for base induction variables Transformed \nI0, ..., Im i's is = +(lv , 7 * s) (lv lv if (i's < ls) goto Lvec i01 s , . . . , is n1) else goto Lcheck() \nNote that the back edge targets the vector loop, the exit edge targets the cleanup check, and the last \nvalues of the base induction variables are passed on the back edge. Figure 2. Vector Block Instruction \nTranslation 4.4.3 On the subject of over.ow For the most part, the vectorization transformation here \nis careful to only compute the same values computed in the original program. Generating entry and exit \ntests violate this property. As mentioned in Section 4.4.1, the compiler can check statically that over.ow \ndoes not occur when computing constants for the initial entry test. For the exit test, it is suf.cient \nto check before entry to the vector loop that ls < MI - 7 * s where MI is the largest representable integer \nand s is the step of is. For signed types, this check must be adjusted in the obvious way when loop bounds \nor steps are negative. In all cases, if the checks fail then the original scalar loop is used to perform \nthe calculation using the original code.  4.4.4 Cleanup check The cleanup check is responsible for determining \nif scalar iterations remain to be performed after the vector loop has exited. This is done simply by \nperforming the original loop exit test. Scalar iterations remain to be performed exactly when lvis < \nls, where lvis is the last value for is computed in the vector loop (since this represents the  Entry \ntest Vector loop Cleanup check Optimized result L0(): a s = new[ls] is 0 = 0 fv a s = a s vv a v = (a \ns , . . . , a s)lv a s = a s fvbs = bs vvbv = (bs, . . . , bs)lvbs = bs fv c s = c s vv c v = (c s , \n. . . , c s)lv c s = c s fvls = ls vvlv = (ls, . . . , ls)lvls = ls iis = 6 * 1 + 1 if (iis < ls) goto \nL2(is 0) else goto L1(is 0) L2(fvis): bviv = (0 * 1, . . . , 7 * 1)pviv = (fvis , . . . , fvis)vviv = \n(+)(bviv , pviv) lvis = +(fvis , 7 * 1) fv x s = fvbs[fvis] vv x v = fvbs[(fvis:)] lv x s = fvbs[lvis] \nfv y s = fv c s[fvis] vv y v = fv c s[(fvis:)] lv y s = fv c s[lvis] fv z s = +(fv x s , fv y s) vv z \nv = (+)(vv x v , vv y v) lv z s = +(lv x s , lv y s) fv a s[(fvis:)] . vv z v fvis 1 = +(fvis , 1) bviv \n1 = bviv pviv 1 = (fvis 1, . . . , fvis 1)vviv 1 = (+)(bviv 1, pviv 1 ) lvis 1 = +(lvis , 1) i 's 1 = \n+(lvis 1, 7 * 1) if (i 's 1 < ls) goto L2(lvis 1) else goto Lcheck() Lcheck(): is 2 = +(fvis , 7 * 1) \nis 3 = +(is 2, 1) if (is 3 < ls) goto L1(is 3) else goto Lexit (is 3) L0(): a s = new[ls] is 0 = 0 iis \n= 7 if (iis < ls) goto L2(is 0) else goto L1(is 0) L2(fvis): lvis = +(fvis , 7) vv x v = bs[(fvis:)] \nvv y v = c s[(fvis:)] vv z v = (+)(vv x v , vv y v) a s[(fvis:)] . vv z v lvis 1 = +(lvis , 1) i 's 1 \n= +(lvis 1, 7) if (i 's 1 < ls) goto L2(lvis 1) else goto Lcheck() Lcheck(): is 2 = +(fvis , 7) is 3 \n= +(is 2, 1) if (is 3 < ls) goto L1(is 3) else goto Lexit (is 3) Figure 3. Vectorized version of Example \n1 value of is from the last completed iteration). Since the vector loop dominates the cleanup check, \nall its entry variables and variables it de.nes are in scope, so the cleanup check could be formed as: \nLcheck (): if (lvis ls is is < ) goto L(lv 01, . . . , lv n1) (lv s lv s else goto Lexit x0, . . . , \nxp) Note that the last values of the base induction variables are passed to the scalar loop and the last \nvalues of the live-out variables (x0 s , . . . , x s ) are passed to the exit label. p However, instead \nof using the last values computed in the vec\u00adtor loop, it is better to recompute them in order to avoid \nintroducing data dependencies that may keep instructions live in the loop which could otherwise be eliminated. \nThis turns out to be straightforward: the portion of the translation in Figure 2 that computes last values \ncan simply be repeated. This results in a complete calculation of all of the last values, including lvis. \nWhile most of these instruc\u00adtions will turn out to be unnecessary, a single pass of dead code elimination \nis suf.cient to eliminate them.  4.5 Transformed example The actual process of producing SIMD vector \nversions of loops seems at .rst quite ad hoc and dif.cult to do cleanly, but turns out to be amenable \nto a simple and straightforward translation which systematically produces vector versions of scalar instructions, \nrely\u00ading on orthogonal optimizations to clean up unnecessary generated code. To illustrate the results \nof this process, Figure 3 shows the results of applying the algorithm to Example 1. First note that the \nbase induction variables are just is of step 1, and the only other induction variable is i1 s also of \nstep 1. The variables a s , bs , c s , and ls are used in the loop but not de.ned by it. Block L0 is \nthe preheader for the loop. We .rst transform the preheader to de.ne variables used by but not de.ned \nby the loop and to do the entry test. This results in Figure 3 under the header Entry test . Next we \ngenerate the instructions for the base induction variable, trans\u00adformed instructions of the loop, and \nadjusted exit condition to form the vector loop. The result of this appears under the header Vec\u00adtor \nloop . Finally we form the cleanup check by regenerating the last-value computations and using the original \nloop-exit condition. For simplicity we just show enough instructions to compute the last value of is \n1, the only needed last value. This block appears under the header Cleanup check . This code, as expected, \nis terribly verbose. However, by apply\u00ading copy propagation followed by dead-code elimination, the ex\u00adtraneous \nparts disappear, yielding the code shown under the header Optimized result . By applying common sub-expression \nelimina\u00adtion this code can be further improved by eliminating the calcula\u00ad is tion of is 2 in favor of \nlvis, and is 3 in favor of lv 1. Finally, simplifying the chained arithmetic expressions yields the same \nvectorized loop as derived by the initial ad hoc vectorization shown in Section 4.3.  4.6 Reductions \nThe SIMD vectorization process de.ned so far primarily applies to loops which serve to create new arrays. \nAn additional idiom that is important to handle in vectorization is that of loops which serve (in total \nor in part) to compute a reduction using a binary operator. That is, instead of, or in addition to, initializing \na new array, each itera\u00adtion of the loop accumulates a value into an accumulator parameter as a function \nof the value from the last iteration. For example, the innermost dot product of a matrix multiply routine \ngenerally takes the form of a reduction. Adding support for reductions to the vectorization transforma\u00adtion \nis not dif.cult. In the same manner that we identify certain variables as induction variables and treat \nthem specially, we iden\u00adtify variables that .t the pattern of reductions and add additional cases to \nthe vectorization transformation to handle them differently. Speci.cally, we say that a variable x s \nis a reduction variable if it is a parameter to the loop that is not an induction variable, and which \nis de.ned on the back edge of the loop by the application of an associative, commutative binary operator \napplied to x s and some distinct variable. All uses of x s and its associated back edge variable must \nbe in reductions. For example, the loop:  s L0(): L1(is , x ): (3) s ss x0 = 1 y = c [is] is s ss 0 \n= 0 x1 = +(x ,y ) s goto L1(is 0, x 0) is 1 = +(is , 1) ls (is s if (is 1 < ) goto L1 1, x 1) exit (x \ns else goto L1) computes one more than the sum of the elements of the array c s via a reduction (assuming \nthat ls is the length of c s) and passes out the s ss result on the exit edge as x1. We say that x and \nx1 are reduction variables in this loop, with initial value 1. The general strategy for SIMD vectorization \nof reductions is to pass a vector variable around the loop, each element of which con\u00adtains a partial \nreduction. Implicitly, we re-associate the reduction so that element i of the vector variable contains \nthe partial reduction of all of the values of the iterations which are congruent to i modulo the vector \nwidth (8 here). After exiting the vector loop, performing a horizontal reduction on the vector variable \nitself results in the full reduction of all the iterations (albeit performed in a different order, hence \nthe requirement of associativity and commutativity). More concretely, to vectorize a reduction variable \nx s, we simply promote x s to a vector variable x v. In the preheader of the loop, we lift the de.nition \nof the initial value for the reduction variable to contain the original initial value in element 0, and \nthe unit for the reduction operation in all other elements. In Example 3 this corresponds to de.ning \nthe initial value as x0 v = (1, 0, . . . , 0)(since 0 is unit for addition). The reduction operation \nin the loop is lifted to perform the same vector operation pointwise, hence v vv x1 = (+)(x , y ) for \nthe example. Finally, in the cleanup check block, the lifted reduction variable x1 v is itself reduced \nto produce the .nal scalar result of the portion of the reduction performed by the vector loop (which \nis then passed in as the initial value to the scalar loop if subsequent iterations remain to be computed). \nThis .nal reduction may be performed using scalar operations. It is possible to maintain last value and \nscalar values for reduction variables within the vector loop as well as the vector value, but doing so \nrequires performing a reduction on the vector value on each iteration which is potentially expensive \nenough to make SIMD vectorization a poor idea. Consequently, we simply choose to reject SIMD vectorization \nof loops which would require the calculation of scalar or last values for reduction variables. Unfortunately, \nthe associativity requirement for binary opera\u00adtions used in reductions can be problematic since most \n.oating point operations (e.g. addition and multiplication) are not fully as\u00adsociative. In practice, \nreductions over .oating point operators are important idioms. By default, HRC does not re-associate .oating \npoint operations, and hence refuses to vectorize loops which per\u00adform reductions over .oating point data. \nWe provide a .ag which permits the compiler to re-associate .oating point operations only for SIMD vectorization \n(but not other optimizations) which allows such loops to be vectorized at the expense of occasionally \nproduc\u00ading slightly different results from the sequential version.  4.7 Dependence Analysis At this \npoint, one might be concerned that we have left aside a critical point in our development. While we have \nshown how to vectorize a loop, we have said nothing about when (or why) it might be valid to do so. The \ncrux of any kind of automatic vectorization lies in determining when it is semantics preserving to execute \nmultiple iterations of a scalar loop in parallel. In the most general case this can be a tremendously \nhard problem to solve, and there is a vast literature on techniques for showing that particular loops \ncan validly be vectorized (see for example Bik [3] for an overview). For the particular case of immutable \narrays however, the problem becomes drastically easier. In this section we give a brief overview of why \nthis is the case. We say that an instruction I2 is dependent on an instruction I1 if I2 must causally \nfollow I1. There are a number of different reasons that this ordering might be required, each inducing \na different kind of dependence. Flow dependence (or data-dependence) is a standard data.ow dependence \ninduced by a read of the value of a variable or location which was written at an earlier point in the \nprogram execution. This is sometimes known as a read-after-write dependence. An anti-dependence on the \nother hand is induced by a write after a read: that is, a location or variable is updated with a new \nvalue after an old value has been read, and therefore the write must happen after the read to avoid changing \nthe value seen by the read. Finally, output dependence is dependence induced by a write after a write: \nthat is, a location or variable is updated with a new value which overwrites an old value written by \na previous statement (and hence the writes cannot be re-ordered without affecting the .nal value of the \nwritten to location or variable). The key insight for automatic vectorization of immutable arrays is \nthat neither anti-dependence nor output dependence can occur. This is obvious by construction, since \nthe nature of immutability is such that the only writes are initializing writes. Therefore, there cannot \nbe a write after a read (since this would imply a read of uninitialized data), nor can there be a write \nafter a write (since this would imply mutation). The only remaining dependence then is .ow-dependence. \nFlow-dependences which are carried by variable de.nitions and uses are straightforward to account for, \nand do not generally cause unusual trouble. In general, it is possible for loop-carried .ow-dependences \nto block vectorization, however. For example, consider the following simple program: L0(): L1(is): a \ns = new[ls] a s[is] . is a s[0] . 0 x s = a s[is - 1] goto L1(1) if (is < ls) goto L1(is + 1) exit s \nelse goto L(x ) While this program uses only initializing writes, it nonetheless in\u00adcurs a loop-carried \ndependence in which a read in one iteration de\u00adpends on a write in a previous iteration. Clearly then, \nthe problem of dependence analysis is not completely trivial even for functional programs even after \neliminating the problems of anti-dependences and output-dependences. Note that while this example reads \nand writes a s through the same variable name, there is nothing pre\u00adventing the binding of a s to a different \nvariable name, and hence allowing array accesses (reads or initializing writes) via an alias. In practice \nhowever, all programs that we have considered (and indeed, based on the way that immutable arrays are \nproduced, all programs that we are at all likely to consider) have the very useful property that they \nare allocated and initialized in the same lexical scope. The implication of this is that it is usually \ntrivially easy to conservatively determine that an initializing write in a loop does not alias with any \nof the reads in the loop simply by checking that no aliases for the newly allocated array are introduced \nbefore the loop. As a result, a very naive analysis is quite successful at breaking read after write \ndependences in this style of loops. 5. Benchmarks We show performance results for eight benchmarks, measured \non an Intel Xeon E5-4650 (Sandybridge) processor implementing the AVX SIMD instruction set. For each \nbenchmark, the best time out of .ve runs was recorded. Three of the benchmarks are small  Figure 4. \nSpeedup over HRC Scalar (HRC Scalar=1). synthetic benchmarks. The .rst computes the pointwise sum of \ntwo arrays of 32-bit .oating point numbers using the following code: addV v1 v2 = zipWith (+) v1 v2 The \nsecond computes the horizontal sum of an array of 32-bit .oating point numbers using the following code: \nsumV v = foldl (+) 0 v Both of the kernels use the operations from our modi.ed version of the Data.Vector.Unboxed \nlibrary. The third kernel computes a dot product, implemented precisely as the composition of the two \nprevious kernels above. These code snippets are each run in a benchmark harness which generates test \ndata and repeatedly runs the same kernel a number of times in order to increase the runtime to reasonable \nlengths for benchmarking. The fourth benchmark is an n-body simulation kernel which uses the naive quadratic \nalgorithm to calculate accelerations for n gravitational bodies, represented as quadruples of 32-bit \n.oat\u00ading point numbers (three dimensions plus mass). For our measure\u00adments, one time step is computed \nfor 100,000 bodies. The code is written in an straightforward manner using the REPA array library. The \n.fth benchmark is the matrix-matrix multiply routine in\u00adcluded in the REPA array library. The matrix \nelements are dou\u00adble precision (64-bit) .oating point numbers. The program gener\u00adates random arrays, \nperforms a single multiplication, computes a checksum, and writes the result out to a .le. The portion \nof the program that is timed and reported here is the matrix multiplica\u00adtion and the checksum computation. \nThe measurements were taken using a 3000 by 3000 element array. The sixth benchmark is a two dimensional \n.ve-by-.ve stencil convolution written using the REPA stencil libraries [9]. The con\u00advolution is repeatedly \nperformed 4000 consecutive times on a 1000 by 1000 element array of 32-bit .oating point numbers. The \nseventh benchmark is a one dimensional convolution of an 8192 element .oating-point stencil applied to \nan array of complex numbers represented as pairs of 32-bit .oats. The REPA stencil libraries could not \nbe applied to this problem, but an elegant and simple solution was arrived at which extracts a slice \nof the complex array, zips it with the stencil array, and then reduces with a fold. The .nal benchmark \nis the blur image processing example in\u00adcluded with the REPA distribution. The benchmark was modi.ed \nslightly to match our benchmark harness requirements, with tim\u00ading code inserted around the kernel of \nthe computation to avoid including .le input/output times in the measurements. No changes to the code \nimplementing the algorithm itself were made. The tim\u00adings presented measure the result of performing \ntwenty successive blurrings on a 4592x3056 pixel image. The performance of these eight benchmarks is \ngiven in Figure 4. Each bar shows speedup over our standard scalar compiler without vectorization, since \nthe primary goal is to demonstrate the perfor\u00admance increase obtained from SIMD vectorization. Speedup \nfor a given build is computed by dividing the HRC Scalar runtime by the runtime for that build: thus \nHRC Scalar is always at 1, and higher numbers indicate better performance. We also show the per\u00adformance \nof unmodi.ed GHC 7.6.1 using both the standard back end and the new LLVM backend [14]. This is intended \nprimarily to demonstrate that the scalar code from which we begin is suit\u00adably optimized. Both the unmodi.ed \nand our modi.ed version of GHC were run with optimization .ags O2 , msse2 . Various combinations of \nthe .ags fno-liberate-case , funfolding-use\u00adthreshold , and funfolding-keeness-factor were also used \nfol\u00adlowing the documentation of the REPA libraries, and based on some limited experimentation as to which \narguments gave better performance. HRC was run with .ags requiring it to preserve .oat\u00ading point value \nsemantics and requiring the backend C compiler to do the same. The only exception was that the .ag discussed \nin Section 4 to permit the re-association of .oating point operations during vectorization of reductions \nwas used. For some benchmarks, this caused small perturbations in the results. For the vector-vector \naddition benchmark, our baseline com\u00adpiler gives large speedups over both the standard GHC backend and \nthe LLVM backend. Unfortunately this benchmark is poorly structured for measuring .oating point performance. \nPerformance analysis shows that the run time is almost entirely dominated by memory costs associated \nwith initializing the new arrays allocated on each iteration. The vector sum benchmark on the other hand \nper\u00adforms a reduction over the input vector producing only a scalar re\u00adsult. Consequently memory latency \nissues are less important since the input vectors are in cache, and SIMD vectorization gives 1.8\u00d7 speedup. \nFor the dot product kernel, the GHC frontend successfully eliminates the intermediate array, and further \noptimization by HRC results in a vectorizable loop for a 2\u00d7 speedup. The N-Body simulation benchmark \nbene.ts signi.cantly from SIMD vectorization, since the core of the inner loop does a sub\u00adstantial amount \nof .oating point work on each iteration. While our baseline compiler gives only slightly better performance \nthan the LLVM backend on the benchmark, the SIMD vectorized version gets a 4.4\u00d7 speedup over the scalar \nversion. The matrix multiplication routine gets some limited bene.t from SIMD vectorization (1.3\u00d7). This \nbenchmark uses double precision which reduces the expected speedup, and more work is done outside of \nthe core inner (vectorizable) loop. Memory latency is also a factor in this benchmark.  The one dimensional \nconvolution benchmark gets an excellent SIMD vector speedup of 6.7\u00d7. Good cache locality makes this benchmark \nlargely compute bound. Unfortunately, while GHC and REPA do an excellent job of fusing away the implied \nintermediate arrays, GHC is unable to eliminate all of the allocation in the inner loop and so performs \nextremely poorly on this benchmark. HRC is able to eliminate all of the remaining allocation and so achieves \nsigni.cantly better baseline performance. Both the two dimensional convolution and blur benchmarks suffered \noriginally from a sequential optimization of the REPA libraries in which the inner loop was unrolled \nfour times3 [9]. While this gave substantial speedup on the scalar builds, it limited the vector speedup \nbecause the generated code used non unit-stride vector loads which are not supported natively on the \nmeasurement hardware. To address this, we modi.ed our version of the REPA library to eliminate the unrolling. \nThis penalizes our scalar build substantially, but greatly bene.ts the vector build since contiguous \nvector loads can be emitted. The GHC builds were measured using the original REPA optimizations. The \n2D convolution benchmark gets a 5\u00d7 speedup, and the blur benchmark gets a 3\u00d7 speedup. Of particular importance \nto note with respect to all of the bench\u00admarks but most especially for the last two is that the timings \npre\u00adsented here are not for the vectorized loops alone, but for the entire kernel of the program (essentially \neverything except the input (or input generation) and output). 6. Related and Future Work, Conclusions \nThere is a vast and rich literature on automatic SIMD vectorization in compilers. Bik [3] provides an \nexcellent overview of the essen\u00adtial ideas and techniques in a modern setting. Many standard com\u00adpiler \ntextbooks also provide at least some rudimentary introduction to SIMD vectorization. The NESL programming \nlanguage [4] pio\u00adneered the idea of writing nested data parallel programs in a func\u00adtional language, \nan idea which has more recently been picked up by the Data Parallel Haskell project [12]. A related project \nfocusing on regular arrays [6] (REPA) has had good success in getting array computations to scale on \nmulti-processor machines. Programs writ\u00adten using the REPA libraries are promising targets for SIMD vec\u00adtorization, \neither automatic as in this work, or via explicit language constructs in the library implementation. \nMainland et al. [10] have pursued this latter approach. They implement primitives in GHC that correspond \nto SIMD instructions and modify the stream fusion used in the Data.Vector library to generate these SIMD \nprimitives. In this way, they achieve SIMD vectorization in the library, where we achieve SIMD vectorization \nin the compiler. Two different, pos\u00adsibly complimentary, ways to achieve the same ends. Our initial implementation \nof SIMD vectorization targets only simple loops with no conditional tests. An obvious extension to this \nwork would be to incorporate support for predicated instructions to allow more loops to be SIMD vectorized. \nHowever, hardware sup\u00adport for this is somewhat limited in current processors, and the per\u00adformance effects \nof generating predicated code are harder to pre\u00addict. A more limited effort to target the special case \nof condition\u00adals performing array bounds checks seems like a promising area for investigation. As functional \nlanguages tend to allocate heav\u00adily, supporting allocation of heap objects inside vectorized loops could \nbe important. A vector allocation instruction allocates 8 ob\u00adjects and places pointers to them in a vector \nvariable. Implementing this operation for .xed-size objects is straightforward and should be quick a \nsingle limit check, vector instructions to compute the 3 Thanks to an anonymous reviewer for pointing \nout the source of this unrolling which we had embarrassingly missed. pointers, and a scatter to initialize \nthe object meta-data used by the garbage collector. Implementing vectorized allocation of variable\u00adlength \narrays should also be straightforward. A special-case in\u00adstruction for when the length is loop-invariant \nwould likely have a faster implementation. To summarize, we have utilized the naturally data-parallel \ntyp\u00adical functional-language operations on immutable arrays such as maps, folds, zips, etc., utilized \nthe large body of work on eliminat\u00ading intermediate arrays, and utilized our own internal operations \non immutable arrays to produce nice loops amenable to SIMD vector\u00adization. Implementing that SIMD vectorization \nautomatically is a surprisingly straightforward process, and one that we have shown can produce excellent \nspeedups on functional language workloads. A key part of this simplicity is the use of immutable objects \nand operations on them, and in particular our use of initializing writes. As utilizing SIMD instruction \nsets is important on modern archi\u00adtectures, these techniques are an important and low-hanging fruit for \nfunctional-language implementations. References [1] T. Anderson, N. Glew, P. Guo, B. T. Lewis, W. Liu, \nZ. Liu, L. Petersen, M. Rajagopalan, J. M. Stichnoth, G. Wu, and D. Zhang. Pillar: A parallel implementation \nlanguage. In V. Adve, M. J. Garzar \u00b4an, and P. Petersen, editors, Languages and Compilers for Parallel \nComputing, pages 141 155. Springer-Verlag, Berlin, Heidelberg, 2008. [2] A. Appel and T. Jim. Shrinking \nlambda expressions in linear time. Journal of Functional Programming, 7(5), Sept. 1997. [3] A. J. C. \nBik. The Software Vectorization Handbook. Intel Press, 2004. [4] G. E. Blelloch. Programming parallel \nalgorithms. Communications of the ACM, 39(3):85 97, Mar. 1996. ISSN 0001-0782. [5] M. Fluet and S. Weeks. \nConti.cation using dominators. In Interna\u00adtional Conference on Functional Programming, pages 2 13, Florence, \nItaly, 2001. ACM. [6] G. Keller, M. M. Chakravarty, R. Leshchinskiy, S. Peyton Jones, and B. Lippmeier. \nRegular, shape-polymorphic, parallel arrays in Haskell. In International Conference on Functional Programming, \npages 261 272, Baltimore, Maryland, USA, 2010. ACM. [7] K. Kennedy, C. Koelbel, and H. Zima. The rise \nand fall of High Perfor\u00admance Fortran: an historical object lesson. In History of Programming Languages, \npages 1 22. ACM, 2007. [8] R. Leshchinskiy. Recycle your arrays! In Practical Aspects of Declar\u00adative \nLanguages, pages 209 223. Springer, 2009. [9] B. Lippmeier and G. Keller. Ef.cient parallel stencil convolution \nin haskell. In Haskell Symposium, pages 59 70. ACM, 2011. [10] G. Mainland, R. Leshchinskiy, and S. Peyton \nJones. Exploiting Vector Instructions with Generalized Stream Fusion. In International Con\u00adference on \nFunctional Programming. ACM, 2013. [11] L. Petersen and N. Glew. GC-safe interprocedural unboxing. In \nCom\u00adpiler Construction, pages 165 184, Tallinn, Estonia, 2012. Springer-Verlag. [12] S. Peyton Jones. \nHarnessing the multicores: Nested data parallelism in Haskell. In Asian Symposium on Programming Languages \nand Systems, pages 138 138, Bangalore, India, 2008. Springer-Verlag. [13] W. Pugh. A practical algorithm \nfor exact array dependence analysis. Communications of the ACM, 35(8):102 114, 1992. [14] D. A. Terei \nand M. M. Chakravarty. An LLVM backend for GHC. SIGPLAN Notices, 45(11):109 120, 2010. [15] N. Vasilache, \nC. Bastoul, A. Cohen, and S. Girbal. Violated de\u00adpendence analysis. In International Conference on Supercomputing, \npages 335 344. ACM, 2006. [16] S. Weeks. Whole-program compilation in MLton. In Workshop on ML, pages \n1 1, Portland, Oregon, USA, 2006. ACM.   \n\t\t\t", "proc_id": "2500365", "abstract": "<p>Expressing algorithms using immutable arrays greatly simplifies the challenges of automatic SIMD vectorization, since several important classes of dependency violations cannot occur. The Haskell programming language provides libraries for programming with immutable arrays, and compiler support for optimizing them to eliminate the overhead of intermediate temporary arrays. We describe an implementation of automatic SIMD vectorization in a Haskell compiler which gives substantial vector speedups for a range of programs written in a natural programming style. We compare performance with that of programs compiled by the Glasgow Haskell Compiler.</p>", "authors": [{"name": "Leaf Petersen", "author_profile_id": "81100348011", "affiliation": "Intel Corporation, Santa Clara, CA, USA", "person_id": "P4261209", "email_address": "leaf.petersen@intel.com", "orcid_id": ""}, {"name": "Dominic Orchard", "author_profile_id": "81453649563", "affiliation": "University of Cambridge, Cambridge, United Kingdom", "person_id": "P4261210", "email_address": "dominic.orchard@cl.cam.ac.uk", "orcid_id": ""}, {"name": "Neal Glew", "author_profile_id": "81100385858", "affiliation": "Intel Corporation, Santa Clara, CA, USA", "person_id": "P4261211", "email_address": "aglew@acm.org", "orcid_id": ""}], "doi_number": "10.1145/2500365.2500605", "year": "2013", "article_id": "2500605", "conference": "ICFP", "title": "Automatic SIMD vectorization for Haskell", "url": "http://dl.acm.org/citation.cfm?id=2500605"}