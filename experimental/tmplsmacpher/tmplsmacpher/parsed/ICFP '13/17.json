{"article_publication_date": "09-25-2013", "fulltext": "\n Wellfounded Recursion with Copatterns A Uni.ed Approach to Termination and Productivity Andreas Abel \nBrigitte Pientka Department of Computer Science, School of Computer Science, Ludwig-Maximilians-University \nMunich, Germany McGill University, Montreal, Canada andreas.abel@i..lmu.de bpientka@cs.mcgill.ca Abstract \nIn this paper, we study strong normalization of a core language based on System F. which supports programming \nwith .nite and in.nite structures. Building on our prior work, .nite data such as .nite lists and trees \nare de.ned via constructors and manipulated via pattern matching, while in.nite data such as streams \nand in.\u00adnite trees is de.ned by observations and synthesized via copattern matching. In this work, we \ntake a type-based approach to strong normalization by tracking size information about .nite and in.\u00adnite \ndata in the type. This guarantees compositionality. More im\u00adportantly, the duality of pattern and copatterns \nprovide a unifying semantic concept which allows us for the .rst time to elegantly and uniformly support \nboth well-founded induction and coinduc\u00adtion by mere rewriting. The strong normalization proof is struc\u00adtured \naround Girard s reducibility candidates. As such our system allows for non-determinism and does not rely \non coverage. Since System F. is general enough that it can be the target of compi\u00adlation for the Calculus \nof Constructions, this work is a signi.cant step towards representing observation-centric in.nite data \nin proof assistants such as Coq and Agda. Categories and Subject Descriptors D.3.3 [Programming Lan\u00adguages]: \nLanguage Constructs and Features Data types and struc\u00adtures, Patterns, Recursion; F.3.3 [Logics and Meanings \nof Pro\u00adgrams]: Studies of Program Constructs Program and recursion schemes, Type structure; F.4.1 [Mathematical \nLogic and Formal Languages]: Mathematical Logic Lambda calculus General Terms Languages, Theory Keywords \nRecursion, Coinduction, Pattern matching, Productiv\u00adity, Strong normalization, Type-based termination \n 1. Introduction Integrating in.nite data and coinduction with dependent types is tricky. For example, \nin the Calculus of (Co)Inductive Construc- Permission to make digital or hard copies of all or part of \nthis work for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting \nwith credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, \nrequires prior speci.c permission and/or a fee. Request permissions from permissions@acm.org. ICFP 13, \nSeptember 25 27, 2013, Boston, MA, USA. Copyright is held by the owner/author(s). Publication rights \nlicensed to ACM. ACM 978-1-4503-2326-0/13/09. . . $15.00. http://dx.doi.org/10.1145/2500365.2500591 \ntions, the core theory underlying Coq (INRIA 2012), coinduction is broken, since computation does not \npreserve types (Gim \u00b4enez 1996). In Agda (Norell 2007), a dependently typed proof and pro\u00ad gramming environment \nbased on Martin-L \u00a8of Type Theory, induc\u00adtive and coinductive types cannot be mixed in a compositional \nway (Altenkirch and Danielsson 2010). In previous work (Abel et al. 2013) we have introduced copatterns \nas a novel perspective on de.ning in.nite structures that might serve as a new foundation for coinduction \nin dependently-typed languages, overcoming the problems in the present solutions. In the copattern approach, \n.nite data such as .nite lists and trees are de.ned as usual via constructors and manipulated via pattern \nmatching, while in.nite data such as streams and in.nite trees are de.ned by observations and synthesized \nvia copattern matching. For example, instead of conceiving streams as built by the constructor cons, \nwe consider the observations head and tail about streams as primitive. Programs about streams are de.ned \nin terms of the observations head and tail. Our previous work left the question of termination of recursive \nfunction and the productivity of in.nite objects open. Both issues are crucial since we want to program \ninductive proofs as recursive functions and coinductive proofs as in.nite objects or corecursive functions \nproducing in.nite objects. In this article, we adapt type\u00adbased termination (Hughes et al. 1996; Amadio \nand Coupet-Grimal 1998; Barthe et al. 2004; Blanqui 2004; Abel 2008b; Sacchini 2013) to de.nitions by \ncopatterns. A syntactic termination check would ensure that recursive calls occur only with arguments \nsmaller than the ones of the original call. In type-based termination, inductive types are tagged with \na size expression that denotes the (ordinal) maximal height of the trees inhabiting it, i. e., an upper \nbound on the number of constructors in the longest path of the tree. To prove termination of a recursive \nfunction means to show that it can safely handle arguments of ar\u00adbitrary size. This can be established \nby well-founded induction: to show that a function can handle arguments up to a .xed size a, we may assume \nit already safely processes arguments of any smaller size b < a. This induction principle can be turned \ninto a typing rule for recursive functions, using sized types and size quanti.ca\u00adtion. How can this be \ndualized to coinduction? A stream is produc\u00adtive if we can make arbitrarily deep observations, i. e., \nif we can take its tail arbitrarily many times. To show that a stream de.nition is productive, we also \nproceed by well-founded induction. To show that it can safely handle a observations, we may assume that \nb ob\u00adservations are .ne for any b < a. The number of observations we can safely make is called the depth \nof the stream, or more general, of the coinductive structure. One should not be mislead and think of \nthe depth as size ; streams do not have a size since they are not tree-structures in memory they only \nexist as processes that con\u00adtinuously yield elements on demand. But it is fruitful to transfer the concept \nof depth to (co)recursive functions. The depth of a function is the maximal size of arguments it can \nsafely handle. As we are only interested in streams of in.nite depth in the end, we care only about functions \nof in.nite depth. Yet to establish productivity and termination, we need to induct on depth.  The type-based \ntermination approach is in contrast to common approaches taken in systems such as Coq (INRIA 2012) and \nAgda (Norell 2007) which employ a syntactic guardedness check to en\u00adsure corecursive programs are productive: \nall corecursive calls must occur under a constructor. This ensures that the next unit of infor\u00admation \ncan be computed in a .nite amount of time (Sijtsma 1989). However, this approach has also known limitations: \nit is dif.cult to handle higher-order programs such as g f = cons 0 (f (g f )) where the productivity \nof g depends on the behavior of the func\u00adtion f. It is also not compositional, i. e., we cannot easily \nabstract over a constructor cons in a productive program and replace it with a function f. Both limitations \nare due to the lack of information we have about f in the syntactic guardedness check. Types on the other \nhand already track information about each argument to a def\u00adinition and its output. Type-based termination \npiggy-backs on the typing analysis and avoids a separate formal system to traverse the de.nitions. By \nindexing types with sizes, we are able to carry more precise information about input and output arguments \nand their re\u00adlation which is then veri.ed simultaneously while type checking the de.nitions. The contributions \nof our work are: We present Fcop, an extension of System F. by inductive and coinductive types, sizes \nand bounded size quanti.cation, pattern and copattern matching and lexicographic termination measures. \n. In contrast to previous approaches on type-based termination, we use well-founded induction on ordinals \ninstead of conven\u00adtional induction that distinguishes between zero, successor and limit ordinals. Disposing \nof this case distinction, we operate within constructive foundations of mathematics (Taylor 1996).  \nWell-founded induction leads to a construction of inductive types by in.ationary iteration, which has \nbeen utilized to justify cyclic proofs in the sequent calculus (Sprenger and Dam 2003). We are the .rst \nto utilize in.ationary iteration in a type system.  Well-founded induction alleviates the need for a \nsemi-continuity check for sized types of recursive functions (Hughes et al. 1996; Abel 2008b) which sometimes \ndisguises itself as a monotonic\u00ad ity check (Barthe et al. 2004; Blanqui 2004; Barthe et al. 2008; Sacchini \n2013). Thus, we put type-based termination on leaner and better understandable foundations.  Since we \nconstruct in.nite objects by copattern matching, stan\u00addard rewriting becomes strongly normalizing even \nfor corecur\u00adsive de.nitions, and productivity becomes an instance of termi\u00adnation. Thus, we achieve a \nuni.ed treatment of recursion and corecursion that is central to type-based termination.  Our typing \nrules are formulated as a bidirectional type-checking algorithm that can be implemented as such. See, \ne. g., Mini-Agda (Abel 2012).  We prove soundness of Fcop by an untyped term model based  . on Girard \ns reducibility candidates. The proof exhibits semantic counterparts of pattern and copattern typing and \naccounts for incomplete and overlapping rewrite rules. Due to lack of space, we leave out Fcop s inference \nrules con\u00ad . cerning the kind and type level, the description of program typing, and most details of \nthe soundness proof. The full development can be found in the extended version (Abel and Pientka 2013). \n 2. Copatterns and Termination Let us illustrate how to program with copatterns using a simple example \nof generating a stream of zeros. A streams s over an element type A is given by the two observations \nhead and tail: We can inspect the head of s by applying the projection s .head and obtain an element \nof A. To obtain the tail of s, we use the projection s .tail. We can then de.ne the stream of zeros recursively \nby the following two clauses: zeros .head = 0 zeros .tail = zeros More generally, zeros can be coded \nas repeat 0 with repeat a .head = a repeat a .tail = repeat a The left hand side of each clause is considering \nthe de.niendum, here repeat, in a copattern, here \u00b7 a .head and \u00b7 a .tail, resp. A copattern consists \nof a hole, \u00b7, applied to a sequence of patterns and/or projections. The hole is .lled, e. g., by the \nde.niendum. In this case, we have .rst a variable pattern, a, and then a projection head/tail. The de.nition \nof repeat is complete because the given copat\u00adterns are covering all possible cases (Abel et al. 2013). \nRewriting with the equations for repeat terminates in all situations, since one projection is consumed \nin each rewriting step. For example, pro\u00adjecting the (n + 1)st element (counting from 0) of repeat a, \ni. e., repeat a .tailn+1 .head reduces in one step to repeat a .tailn .head and after n more steps to \nrepeat a .head. 2.1 Example: Fibonacci Let us look at programming with copatterns and type-based ter\u00admination \nfor a more interesting example, the stream of Fibonacci numbers. It can be elegantly implemented in terms \nof zipWith f s t which pointwise applies the binary function f to the elements of streams s and t. zipWith \nf s t .head = f (s .head) (t .head) zipWith f s t .tail = zipWith f (s .tail) (t .tail) .b .head = 0 \n.b .tail .head = 1 .b .tail .tail = zipWith (+) .b (.b .tail) The last equation states in terms of streams \nthat the (n + 2)nd element of the Fibonacci stream is the sum of the nth and the (n + 1)st. It looks \nlike .b is a terminating de.nition since .b .tail .tail only refers to .b and .b .tail, thus, one projection \nis removed in each recursive call. However, termination of .b is also dependent on good properties of \nzipWith. For instance, the following faulty clause for zipWith would make .b .tail .tail .head loop: \nzipWith f s t .head = f (s .tail .head) (t .tail .head) .b .tail .tail .head = zipWith (+) .b (.b .tail) \n.head = (.b .tail .head) + (.b .tail .tail .head) = (.b .tail .head) + (.b .tail .head) + (.b .tail .tail \n.head) = . . . The problem is that the faulty zipWith adds again one tail pro\u00adjection that has been removed \nin going from the original call .b .tail .tail to the recursive call .b .tail, thus, we are left with \nthe same number of projections, leading to an in.nite call cycle. What we learn from this counterexample \nis that in order to reason about termination of stream expressions, we need to trade the naive image \nof streams as in.nite sequences for a notion of streams that can safely be subjected to a many projections, \nwhere a = . can be a natural number or (the smallest) in.nity .. We refer to such streams as sized streams, \nor streams having depth a. Clearly, if a stream of depth a is required, we can safely supply a stream \nof depth \u00df = a, thus, sized streams are subject to contravariant subtyping.  The original zipWith delivers, \nif called with input streams of depth a, an output stream of the same depth. This allows us to reason \nabout the termination of .b as follows. We show that .b is a stream of arbitrary depth a by induction \non a = .. Cases a < 2 are easy. The interesting case is a = n + 2 when we take two tail projections and \nthen another n projections, thus, n + 2 projections in total. Then we may assume (by induction hypothesis) \nthat on the rhs taking up to n + 1 projections of .b is .ne, thus, .b and .b .tail behave well under \nanother n projections they both can be assigned depth n using subtyping. Passing them to zipWith (+) \nreturns in turn a stream of the same depth n, hence the lhs .b .tail .tail can be assigned depth n and, \nconsequently, .b depth n + 2, which was our goal. The faulty zipWith, however, needs streams of depth \nn + 1 to deliver a stream of depth n. Since .b .tail can only safely be assumed to have depth n, not \ndepth n + 1, the termination proof attempt fails, and rightfully so. In this model proof we assumed that \ntaking a projection will decrease the depth by exactly one. In the following, we will loosen this assumption \nand let projections take us to any strictly smaller depth.  2.2 Type-based termination for copatterns \nIn this section, we present the key ideas behind Fcop, our polymor\u00ad . phic core language for type-based \ntermination checking of recur\u00adsive de.nitions involving inductive and coinductive types. We il\u00adlustrate \nhow the integration of size expressions into the type system captures and mechanizes the informal reasoning \nabout termination employed in the previous section. Size quanti.cation for inductive and coinductive \ntypes. Besides quanti.cation over types .A:*. B we have quanti.cation over sizes .i<a. B. To unify these \ntwo forms of quanti.cation we add to the base kind * of types the base kinds <a denoting sets of ordinals \nbelow a and conceive .i<a. B as shorthand for .i:(<a). B. Thus, size expressions fall in the same syntactic \nclass as type expres\u00adsions. We introduce a special ordinal 8, the closure ordinal for all (co)inductive \ntypes we consider. As far as streams are concerned, 8 can be thought of as .. In general, valid size \nexpressions are of the form a ::= i + n | 8 + n where i is a size variable and n a concrete number (we \ndrop +0). The type of streams of depth a over element type A will be denoted by StreamaA, and we consider \nthe following typing rules for the projections: s : StreamaA s : StreamaA (1) s .head : .i<a.. A s .tail \n: .i<a. . StreamiA These rules state that if you want to project a stream of depth a, you will need to \nprovide a witness that you are able to do so, i. e., an ordinal i < a.. In case of tail, this witness \nserves also as the depth of the projected stream. For instance, if s : Streami+2A, then s .tail (i + \n1) .head i : A. Bound normalization a ., de.ned by (i + n). = i + n and (8 + n). = 8 + 1, allows us to \nturn bounds a = 8 into 8 + 1 and project from the .xpoint Stream8A without information loss. For s : \nStream8A we have s .tail 8 : Stream8A since 8 < 8. = 8 + 1, re.ecting that the tail of a fully de.ned \nstream has in.nite depth as well. In practice, we often use the following derived rule which elim\u00adinates \nthe universal quanti.er and directly compares sizes. s : StreamaA . s : StreamaA . b<ab<a s .head b : \nA s .tail b : StreambA More generally, following previous work (Abel et al. 2013), we represent coinductive \ntypes as recursive records . R, with R = {d1 : F1; . . . ; dn : Fn} giving (sized) types to the projections \nd1..n as follows: r : .aR r .dk : .i<a.. Fk(.iR) For instance, with Streami A = .i{head : .X. A; tail \n: .X. X }we obtain the typing of head and tail presented above (1). Consid\u00ad ering R as a .nite map from \nprojections to type constructors, we write Rdk for Fk. Dually, inductive types are recursive variants \n\u00b5S with S = (c1 : F1; . . . ; cn : Fn) and constructor typing t : .i<a.. Fk(\u00b5 iS) . ck t : \u00b5aS For instance, \n.nite lists can be de.ned as follows: ListiA = \u00b5 i(nil : .X. 1; cons : .X. A \u00d7 X). Integrating the quanti.er \nrules, we derive the following inferences for constructors and de\u00adstructors: s : Sc(\u00b5 bS) . r : .aR . \nb<ab<a. cbs : \u00b5aS r .d b : Rd(.bR) Specifying termination measures. The polymorphically typed version \nof zipWith of.cially looks as follows, where we write .i=a as abbreviation for .i<(a + 1): zipWith : \n.i=8. |i| . .A:*. .B:*. .C:*. (A . B . C) . StreamiA . StreamiB . StreamiC zipWith i A B C f s t .head \nj = f (s .head j) (t .head j) zipWith i A B C f s t .tail j = zipWith j A B C f (s .tail j) (t .tail \nj) The .rst equation has type C and the second one type Streamj C. The kind of j is <i due to the typing \nof head and tail, thus, zipWith is well-de.ned (and terminating) by induction on its .rst argument, the \nsize argument. The associated termination measure is located after the size variable(s) and, in general, \na tuple |a, b, c| of size expressions under the lexicographic order.1 In this case, it is just the unary \ntuple |i|, meaning that the termination measure is just the value of size variable i. The measure is \nnot of.cially part of the type; it is rather an annotation that allows us to termination check the clauses \nwithout having to infer a termination order. High-level idea of size-based termination checking. When \nwe check a corecursive de.nition such as the second clause of zipWith we start with traversing the left \nhand side (lhs). We .rst introduce assumption i=8 into the context and now hit the measure anno\u00adtation \n|i| in the type. At this point we introduce the assumption zipWith : .j=8. |j|<|i| . .A:*. .B:*. .C:*. \n(A . B . C) . Streamj A . Streamj B . Streamj C which will be used to check the recursive call on the \nright hand side (rhs). It has a constraint |j| < |i|, a lexicographic comparison of size expression tuples \n(which here just means j < i), that is checked before ap\u00adplying zipWith j to A. Continued checking of \nthe lhs introduces further assumptions A, B, C : *, f : A . B . C, s : StreamiA, t : StreamiB, and j \n< i. Checking the rhs succeeds since the 1 The notation for termination measures is taken from Xi (2002) \n constraint |j| < |i| is satis.ed and s .tail j : Streamj A and t .tail j : Streamj B. In the following, \nwe abbreviate .A:* to just .A and .i=8 to just .i. With all size and type-arguments, the de.nition of \nthe Fibonacci stream becomes: .b : .i. |i| . StreamiN .b i .head j = 0 .b i .tail j .head k = 1 .b i \n.tail j .tail k = zipWith k N N N (+) (.b k) (.b j .tail k) In the last line, the lhs introduces size \nvariables i and j < i and k < j and an assumption .b : .i'. |i'| < |i| . Streami/ Nand expects a rhs \nof type StreamkN. Since k < j < i, both recursive calls are valid, and the expressions .b k and .b j \n.tail k both have type StreamkN. With zipWith k N N N : StreamkN . StreamkN . StreamkN, the rhs is well-typed, \nand .b is terminating.  2.3 Example: Stream processor Ghani et al. (2009) describe programs for continuous \nstream func\u00ad tions Stream A . Stream B in terms of a mixed coinductive\u00adinductive data type SP with two \nconstructors get : (A . SP) . SP and put : (B \u00d7 SP) . SP. We use this example to illustrate how our foundation \nsupports size-based reasoning on such mixed datatypes and lexicographic termination measures for mutually \nre\u00adcursive functions. A stream processor can either get an element v : A from the input stream and enter \na new state, depending on the read value, or it can put an element w : B on the output stream and enter \na new state. To be productive, it can only read .nitely many values from the input stream before writing \na value on the output stream, thus, SP is actually a nesting of a least .xed-point into a greatest one: \nSP = . X. \u00b5Y. (A . Y ) + (B \u00d7 X). We express this nesting by the de.nition of two data types, an inductive \nvariant SP\u00b5 and a coinductive record type SP. . SPi X = \u00b5 i(get : .Y. A . Y ; put : .Y. B \u00d7 X) \u00b5 SPi \n= .i{out : .X. SP8X} . \u00b5 Inside the coinductive type, we use the inductive type SP\u00b5 at size 8 since we \nwant to allow an arbitrary (.nite) number of gets between two puts. We get the following derived rules \nfor typing constructors and destructors: f : A . SPb X . w : B sp : X . \u00b5 b<ab<a getbf : SPa X putb(w, \nsp) : SPa X \u00b5 \u00b5 sp : SPa . . b < a sp .out b : SP8 \u00b5 SPb . In the context of stream processors it is \nconvenient to consider streams as given by a single destructor force which returns head and tail in a \npair, thus, StriA = .i{force : .X. A \u00d7X}. Dedicated projections hd and tl can be de.ned by hd : .i. Stri+1A \n. A hd i s = fst (s .force i) tl : .i. Stri+1A . StriA tl i s = snd (s .force i) with fst and snd the \nobvious .rst and second projections from pairs. Via bound normalization, facilitating Str8 = Str8+1, \nwe obtain instances hd 8 : Str8A . A and tl 8 : Str8A . Str8A. Running a stream processor on an input \nstream produces an output stream as follows (informally coded in a Haskell-like lan\u00adguage): run (get \nf ) (v, vs) = run (f v) vs run (put(w, sp)) vs = (w, run sp vs ) We represent this function via two \nmutually recursive functions, one handling SP\u00b5 and one SP. : run\u00b5 : .i.j. /|i, j+1| . SPj (SPi . ) . \nStr8A . B \u00d7 StriB \u00b5 run\u00b5 i j (getjf) vs = run\u00b5 i j' (f (hd 8 vs)) (tl 8 vs) / run\u00b5 i j (putj(w, sp)) \nvs = (w, run. i sp vs ) run. : .i. |i, 0| . SPi . . Str8A . StriB run. i sp vs .force i' = run\u00b5 i' 8 \n(sp .out i') vs The recursive run\u00b5 handles a sequence of gets terminated by put and emits the head of \na forced stream B \u00d7 StriB. The tail is pro\u00adduced by the corecursive run. which, upon forcing, calls run\u00b5 \nagain. The termination is guaranteed by the lexicographic mea\u00adsures, which decrease in each recursive \ncall: run\u00b5 . run\u00b5 : |i, j + 1| > |i, j' + 1| since j > j' run\u00b5 . run. : |i, j + 1| > |i, 0| run. . run\u00b5 \n: |i, 0| > |i', 8 + 1| since i > i' Note that since we are not doing induction on SPi . , but coinduction \ninto Stri, we could use SP8 . instead of SPi . in the types of run\u00b5 and run. . However, the given types \nare more precise: instead of a stream processor of in.nite depth, they only require a stream processor \nof depth i to produce a stream of depth i.  3. Syntax In this section, we formally de.ne Fcop, our higher-order \npolymor\u00ad . phic lambda-calculus with sized inductive and coinductive types, polarized higher-order subtyping, \nand de.nitions by pattern and copattern matching. As in previous work (Abel 2008b) we choose System F. \nrather than System F as basis since the notion of a type constructor is required (at least, semantically) \nif one wants to talk its .xed-points, i. e., about (co)inductive types. SizeVar 3 i, j SizeExp 3 a, b \n::= i + n | 8 + n (n = 0) SizeExp+ 3 a +, b+ ::= a | n Measure 3 m ::= \u00b7 | a + , m Pol 3 p ::= . | + \n| - | T SizeCxt 3 . ::= \u00b7 | ., i:p(<a) Figure 1. Sizes and measures. 3.1 Sizes Fig. 1 gives a grammar \nfor sizes, measures, and size contexts. A size expression a consists of a base, which is either a size \nvariable i or 8, and an offset, a natural number n. a ::= i + n | 8 + n We omit the offset when 0. Each \nsize variable i comes with a bound i < a, which is recorded in a size context . ::= \u00b7 | ., i:p(<a). A \nsize context is considered as .nite map from size variables i to their polarity p (see below) and their \nkind <a. We write =a for <(a + 1) and size for =8. Extended size expressions a + allow as a third base, \nn, i.e. just a natural number. Measures m are tuples of extended size expressions. There are a number \nof trivial judgements concerning well-formedness and partial ordering of (extended) size expressions \nand measures (see Table 1). These judgements may use the bounds stored in size context . and are all \nde.ned as expected; their inference rules can be found in the extended version.  . f a size a is well-formed \n. f a < b strict size comparison . f a = b size comparison . f a + extended size a + is well-formed . \nf a + < b+ strict comparison . f a + = b+ comparison . fn m measure m is a well-formed n-tuple . f m \n< m ' strict lexicographic measure comparison . f m = m ' lexicographic measure comparison . f .. ' . \n' is consistent for each valuation of . Table 1. Size-related judgements. In constraint-based systems, \nstrong normalization is usually lost in inconsistent contexts.2 While our size contexts . are always \nconsistent, i. e., enjoy a valuation3 . of the declared size variables (by natural numbers even), we \nneed sometimes a stronger property that a size context extension . ' is consistent with a .xed valuation \n. of ., i. e., . ' must be consistent even when we apply . to its declared bounds. For instance, i=8, \nj<i is consistent, but j<i is not a consistent extension of i=8 under valuation .(i) = 0, since there \nis no solution for j. We write . f .. ' if . ' consistently extends . in this sense. This judgement is \ninspired by Blanqui and Riba (2006). SKind 3 . ::= * | o | . . . ' Kind 3 . ::= * | <a | p. . . ' TyCxt \n3 . ::= \u00b7 | ., X:p. Cxt 3 G ::= \u00b7 | G, x : A | G, x : ?A TyVar 3 X, Y, Z, i, j TyAtom 3 K ::= a | X \n| 1 | \u00d7 | . | .. | .. Type 3 F, G, A, B, C ::= K | .X:.. F | F G aS | .a | \u00b5 R Var 3 x, y, z Cons 3 c \nProj 3 d Variant 3 S ::= (c1:F1; . . . ; cn:Fn) n = 0 Record 3 R ::= {d1:F1; . . . ; dn:Fn} n = 0 3 ' \n' MType A, B ::= ... m . A CType 3 ?A, ?B ::= ... c . A Cond 3 c ::= m<m ' Figure 2. Kinds and type \nconstructors.  3.2 Kinds and type constructors The type constructors of F. are assigned kinds . ::= \n* | . . . ' , with base kind * classifying all proper types and function kinds . . . ' the (higher-order) \ntype operators. We add a second base kind . ::= \u00b7 \u00b7 \u00b7 | o that classi.es size expressions, which we locate \nat the type level, since they are computationally irrelevant and can be erased during compilation, just \nas the types are. 2 For instance, in extensional type theory, X : Type, p : X = (X . X) f (.x:X. x x)(.x:X. \nx x) : X. The blame is on the false equality assumption X = X . X which is used for type conversion. \n3 A valuation . of size context . is a map from size variables i to sizes .(i) that ful.lls the constraints \nfor the size variables given by .. Formally, .(i) < [[a] . must hold in case i:p(<a) . ., where [[a] \n. is the value of size expression a in environment .. These simple kinds . form with the type constructor \na simply\u00ad typed type-level lambda calculus. We re.ne these kinds into Fcop\u00ad . kinds p' . ::= * | <a | \n. . . where <a re.nes o into the kind of size expressions b < a. The p polarized function kind . . . \n' , also written p. . . ' , allows us to express that the classi.ed type constructor is co-variant (p \n= +), contravariant (p = -), constant (p = T) or mixed-variant or of unknown variance (p = .). The polarities \np are partially ordered . = +, - = T according to their information content. This and the order on size \nexpressions induce a subkinding relation . f . = . ' on kinds of the same structure, i. e., the same \nunderlying simple kind |.| = |. ' |. Here, when comparing two o-kinds (<a) = (<b), we resort to size \ncomparison a = b. The default variance is . (no information) and we may omit it, writing simply . . . \n' or ., i:(<a), which is further abbreviated by ., i<a. Kinding or type variable contexts . ::= \u00b7 | ., \nX:p., which provide scoping and kinding information for type constructors, generalize size contexts from \nbounds (<a) to arbitrary kinds .. We may use a . where a . is formally required, silently erasing all \nnon-size variables from .. More generally, context restriction . I X of context . to a set of variables \nX deletes the bindings for all Y . X from .. . f . kind . is well-formed in . . f . = . ' . is a subkind \nof . ' . f . ' kinding context . ' is well-formed in . . f .. ' is consistent for each valuation of . \n. ' Table 2. Kind-related judgements. The judgement . f .. ' (see Table 2) states that . ' is consis\u00adtent \nfor each valuation of .. Only the size declarations matter here, so it is a straightforward extension \nof . f .. ' . Figure 2 contains a grammar for the type constructors of Fcop. . Its core is a simply-kinded \nlambda-calculus X | .X:.. F | F G with constants 1, \u00d7, ., .., and .. to form unit, product, function, \nuniversal, and existential types. Size expressions a are considered type constructors so that sizes can \nbe abstracted over and applied. We use the following short-hands: .XF for .X:.. F if . inferable A \u00d7 \nB for (\u00d7) A B product type A . B for (.) A B function type .X:.. A for ..(.X:|.|. A) universal type .X:.. \nA for ..(.X:|.|. A) existential type .i<a. A for .<a (.i:o. A) bounded universal .i<a. A for .<a (.i:o. \nA) bounded existential .i. A for .i:size. A unbounded universal .i. A for .i:size. A unbounded existential. \n We also write ... A for the universal abstraction of all type vari\u00adables of . in type A. The simple \nkind annotation . in .X:.. F allows us to infer a unique simple kind for closed type constructors. The \nsimple kind of an open type constructor depends only on the simple kinds of its free type variables. \nThis property simpli.es the interpretation [ F ]] of type constructors as set-theoretic functions on \nsemantic types we will give later. For the purpose of type checking, we are only interested in \u00df\u00adnormal \ntype constructors. We write F G for the normalizing @. application of F to an argument G of simple kind \n.. We may write @. instead of @|.|, or even just @. Sized inductive \u00b5 aS and coinductive types .aR are \ngiven in terms of variant rows S and record rows R. A variant row S = (c1:F1; . . . ; cn:Fn) is a .nite \nmap from variant labels ci, called constructors, to type constructors Sci = Fi. Dually, a record row \nR maps record labels d, called destructors or projections, to type constructors Rd. Instead of presenting, \nfor instance, streams as  .a X. {head : A; tail : X}, we move the abstraction over X into the record \nrow as .a{head : .X.A; tail : .X.X }, in order to formulate the typing rules more conveniently. Finally, \nwe have constrained types ... m<m ' . A that allow its inhabitants to be used only if the condition m \n< m ' is ful.lled. We use them to restrict recursive calls to situations where the termination measure \nhas decreased. Recursive function de.nitions come with measured types ' A ::= ... m . A. These are not \nproper types but rather blueprints for constrained types. The idea is that kinding context . declares \nsome size variables that are used in measure m (and type A). When we analyze the body of a recursive \nfunction of measure type ' A and the variables of . are in scope (thus, the measure m is well-formed), \nwe make ' '' a copy B = .. ' . m . A ' of A by renaming the variables of . to . '. Then, by measure replacement \n' B<m we create the constrained type .. ' . m ' <m . A ' which is used to type the recursive occurrences \nof the function in its body. . f A type A is well-formed . f F . . F has kind . (inference) . f F t . \nF has kind . (checking) . f G typing context G is well-formed . f A = A ' A is subtype of A ' . f F \n=p F ' . . F is hi.-ord. subtype of F ' (. inferred) . f F =p F ' t . F is hi.-ord. subtype of F ' (. \ngiven) Table 3. Type-related judgements. Table 3 lists judgements for well-kindedness and partial order\u00ading \nof types and type constructors. The judgements for types A only invoke the judgments for type constructors \nF in checking mode at base kind (t *). The judgements for constructors are bidirectional with inference \nmode that computes the kind . and checking mode that starts with a given .. Bidirectional checking is \ncomplete since we are only interested in normal type constructors. The rules for these judgements are \ngiven in an extended version of this article. A thorough discussion of polarized higher-order subtyping, \ni. e., subtyping for type constructors that take variance into account, is available in Abel (2008a) \nand Steffen (1998), we just recapitulate the basic principle here: A constructor F with X1:p1.1, . . \n. , Xn:pn.n f F t . is interpreted as an operator p1pn .X1 . . . .Xn.F : .1 . . . . .n . . with variance \ngiven as noted in its kinding context. This induces the kinding rules, for instance X:-*, Y :+* f X . \nY : * is valid since function space is contravariant in its domain and covariant in its codomain. In \nparticular, the hypothesis rule X:p. f X : . is only valid if p = +, i. e., p = . which just states that \n.X.X : . . . is a well-formed operator, or p = + which additionally states that .X.X is monotone. Using \nthe hypothesis rule on p = - or p = T is invalid since .X.X is neither an antitone nor a constant operator. \nGiven a partial order G = G ', its p-parameterized version G =p G ' can be de.ned as follows: G =+ G \n' = G = G ' G =- G ' = G ' = G G =. G ' = G = G ' and G ' = G G =T G ' = true p-variance of a constructor \nF . p. . . ' means that F G = F G ' . . whenever G =p G ' t .. (The reader is advised to play through \nthe four cases for p in his mind.) Theoretically, =p ' the p-parameterized versions . f F F . . . of \nhigher\u00adorder subtyping could be de.ned from a non-parameterized version . f F = F ' . . . , but to avoid \nthe potential exponential blowup due to duplication of work in case of =., the p-parameterized versions \nare taken as primitive. Exp 3 r, s, t ::= u | v | .DXterm Intro 3 v ::= () | (t1, t2) | c t | Gt introduction \nApp 3 u ::= x | f | r e applicative Fun 3 f, g function name Elim 3 e ::= t | G | .d elimination Pat \n3 p ::= x | () | (p1, p2) | c p | X p pattern Copat 3 q ::= p | X | .d copattern PatSp 3 q ::= Xq pattern \nspine DCl 3 D ::= {q . t} def. clause Def 3 DX::= {D1; . . . ; Dn} def. clauses Figure 3. Terms, (co)patterns, \nand clauses.  3.3 Terms and (co)patterns Figure 3 presents the abstract syntax of Fcop terms t, which \nare cat\u00ad . egorized into introductions v, applicative terms u, and anonymous objects .DX. Introductions \n(), (t1, t2), c t and Gt construct tuples and inductive and existential types. Applicative terms x, f, \nand r e are identi.ers and generalized applications of a term r to an elimi\u00adnation e, which can be a \nterm s for function elimination, a type G for instantiation of a polymorphic function, or a destructor \n.d for projection from a coinductive type. For each introduction form v we have the corresponding form \nof pattern p, and for each elimination form e there is a copattern q. Application copatterns are just \npatterns p to match the argument, type application copatterns Q are either type variables X or the special \nsize pattern 8, which matches anything, and projection copatterns are simply destructors d that match \nthe same destructor in an elimination. A sequence of Xq of copatterns is called a pattern spine q, in \ncorrespondence to an elimination spine Xe. Generalized lambda abstraction .DXintroduces an object whose \nbehavior is given by the clauses DX, each of which consists of a lhs, a (possibly empty) copattern sequence \nXq, and a rhs, a term t. Objects subsume both record and . expressions of traditional functional languages. \nHere are a few simple examples: .{x . t} ordinary .-abstraction .xt .{X . t} type abstraction .X t .{(x, \ny) . x} .rst projection from pair .{X x y . y X x} elimination of existential .{A x y .head 8 . x ; A \nx y .tail 8 . y} cons for Stream8A .{\u00b7 . s; \u00b7 . t} non-deterministic choice s . t The meaning, given \nby the operational semantics, is that when\u00adever .DXis applied to a sequence of eliminations Xe that match \nthe copatterns Xq of a clause with rhs t under a substitution s and a type substitution t, then (.DX) \nXe reduces to tst , the rhs instanti\u00adated by the substitutions computed from pattern matching. Using \nXe / Xq s; t for pattern matching, the basic rule for contraction r . r ' becomes: Xe / qk s; t --- \n.{q . t} Xe Xe ' . tkst Xe '  As usual, r is called a redex and r ' its reduct if r . r '. We allow \noverlapping lhss, a spine Xe may match different pattern spines q, resulting in different contractions \nof the same redex. Also, if no lhs in the clauses DXmatches XDX e, the expression . Xe is stuck. While \na coverage checker as described in previous work (Abel et al. 2013) could exclude overlapping and incomplete \nclauses in well-typed programs, we do not require coverage in this paper and con.ne ourselves to show \nstrong normalization, i. e., the absence of in.nite reduction sequences. Not all stuck terms are pathological; \nsince we are matching the whole pattern spine in one go, partially applied functions such as .{xy . t}s \nare stuck, but can become unstuck if more arguments are supplied. The existence of partially applied \nfunctions will re\u00adquire careful treatment in the normalization proof, because non\u00adcontractibility of \na non-introduction term is not preserved under application (as would be in the case of .-calculus). X \nDecl 3 d ::= f : A = D declaration ' X MDecl 3 ' d ::= f : A = D declaration with measure ' X Block \n3 \u00df ::= mutualm d mutual block Prg 3 P ::= \u00dfX; u program Sig 3 S ::= Xd signature Figure 4. Declarations, \nblocks, and programs.  3.4 Declarations and programs An Fcop . program consists of a sequence \u00dfXof mutual \nblocks and an applicative term u, the entry point (this could be the name of the main function or a call \nto the main function with some initial argu\u00ad ' ments). Each mutual block mutualm Xd is a sequence X'd \nof mutually recursive declarations with a lexicographic termination measure of length m. Each declaration \nf : ' A = DXassigns to a function sym\u00adbol f its measured type ' A and its clauses DX. Measures serve \ntheir purpose during checking of the mutual block and are discarded af\u00adterwards. Erasure of measure r' \nd. yields a (unmeasured) declaration f : A = DX; after checking a mutual block and erasing the mea\u00adsures, \nthe individual declarations of the block become part of the signature S which is used for type-checking \nand evaluation of the remainder of the program. An applied function f Xe reduces if one of its clauses \ndoes: (.DX) Xe . t X (f : A = D) . S f Xe . t The one-step reduction relation t -. t ' is the compatible \nclosure of the contraction relation t . t ', i. e., t -. t ' if t ' is the result of contracting exactly \none redex in (an arbitrary subterm of) t. Strong normalization of reduction will be shown to hold for \nwell-typed programs. .; G f r C Infer type C for term r .; G f t t C Term t checks against type C .; \nG f {q . t} t A Clause q . t checks against type A .; G f XD t A Clauses D check against type A .; G \nf.0 p t A Pattern p checks against type A .; G | A f.0 q C Pattern spine q eliminates A into C Table \n4. Type checking.  3.5 Type checking Table 4 lists the judgements involved in type checking Fcop pro\u00ad \n. grams. Type-checking terms is bidirectional and a straightforward adaption of Abel et al. (2013) to \npolymorphism, bounded quanti.\u00ad cation, and constraints. The rules are given in .gures 5 and 6, and we \nbrie.y explain them. Inference .; G f r C . A function symbol f s type S(f) is looked up in the signature, \nand a variable x s type G(x) in the typing context. If G(x) is a constrained type ... c . A, the variable \nx must be immediately applied to size arguments Xa satisfying both . and the condition c; after all, \na constrained type is, for consistency reasons, not a proper type for an expression. An application r \ns of a function r of inferred type A . B has type B if the argument s checks against type A. Instantiation \nr G of a polymorphic term r of inferred type ..F has type F @. G if G has kind .. In particular, r could \nbe of type .i<a. A, then G must be a size expression < a to succeed. If r is of coinductive type .a R, \nthen r .d has type .j<a..Rd (.j R), see Section 2.3. There are two rules to switch direction. Checking \nr against type C succeeds if r s type is inferred as A and A is a subtype of C. Also, we can add type \nascription (t : A) to the term language; then inference of (t : A) succeeds and yields A if A is a well-formed \ntype and t checks against A. While type ascription is needed to bidirectionally type check redexes or \nstuck terms, it is dispensable if one con.nes to checking normal terms (in the sense that no elim\u00adination \nis applied to a . in the source program). We will consider type ascriptions be removed before execution \nof the program, so they do not pop up in the operational and denotational semantics. Checking .; G f \nt t C . Introductions and .s are checked against a given type. Checking a pair Gt of a type expression \nG and a term t against an existential type .kF succeeds if G has kind . and t is of the correct instance \nF @. G. Checking a constructor term c t against an inductive type \u00b5 aS succeeds if t checks against .j \n< a.. Sc (\u00b5j S). This means that t should be essentially a pair b ' ' t of a size b < a. and t be a correct \nargument to constructor c, i. e., having variant Sc instantiated to \u00b5j S. If a = 8, by bound normalization \nb = 8 is a valid size index, which implies that in a value v in the .xpoint \u00b5 8S all size witnesses can \nuniformly be 8. To check .DXwe check all clauses Dk. Clause checking .; G f {q . t} t A . We .rst check \nthat pattern spine q eliminates indeed type A. As a result, we obtain a kinding context . ' which binds \nthe type variables X contained in q and a typing context G ' which binds the pattern variables x contained \nin q s patterns, and a remaining type C of lhs and rhs. We now need to make sure that . f .. ' such that \nany valuation of . can be extended to a valuation of . '. Complementing the original contexts .; G by \nthe pattern contexts . ' ; G ' we check the rhs t against C. Pattern spine checking .; G | A f.0 q C \n. We eliminate type A which is well-formed in .0. If there are no copatterns in q, thus, the clause has \nan empty lhs, we simply return A which must be the type of the rhs. If we encounter an application pattern \np, the eliminated type must be a function type A . B. We check p against A and obtain pattern contexts \n.1; G1. We continue to check the remaining copatterns, obtaining more pattern contexts .2; G2 and a result \ntype C, which we return together with the concatenated pattern contexts. Concatenation, and thus, pattern \nspine checking fails if the contexts do not have disjoint domains. A common variable would mean a non-linear \nlhs, which we exclude. If we encounter a projection pattern .d, the eliminated type must be a coinductive \ntype .aR. Taking projection .d yields type .j<a.. Rd(.j R), thus, we continue to eliminate this type \nby ap\u00ad  .; G f r C Expression typing (inference mode). In: ., G, r with . f G. Out: C with . f C (x:A) \n. G (x : ... c . A) . G . f Xa t . t = Xa/. . f ct .; G f f S(f ) .; G f x A .; G f x Xa At .; G f r \nA . B .; G f s t A .; G f r .aR .; G f r ..F . f G t . .; G f r s B .; G f r.d .j<a.. Rd (.j R) .; G \nf r G F @. G Switching. . f A .; G f t t A .; G f r A . f A = C .; G f (t : A) A .; G f r t C .; G f \nt t C Expression typing (checking mode). In: .; G, t, C with . f G and . f C. Out: success/failure. \n.; G f t1 t A1 .; G f t2 t A2 .; G f t t .j <a.. Sc (\u00b5j S) .; G f () t 1 .; G f (t1, t2) t A1 \u00d7 A2 .; \nG f c t t \u00b5aS .; G f D t A . f G t . .; G f t t F @. G .; G f DXt A .; G f Gt t ..F .; G f . XD t A \nand .; G f XD t A de.nition typing. In: ., G, A, D or XD with . f G and G f A. Out: success/failure. \n. ' ; G ' | A f. Xq C . f .. ' ., . ' ; G, G ' f t t C .; G f Dk t A for all k .; G f {Xq . t} t A .; \nG f XD t A Figure 5. Type checking rules. .; G f.0 p t A Pattern typing (linear). In: .0, p, A with \n.0 f A. Out: ., G with .0, .; G f p t A. .1; G1 f.0 p1 t A1 .2; G2 f.0 p2 t A2 \u00b7; x:A f.0 x t A \u00b7; \u00b7 \nf.0 () t 1 .1, .2; G1, G2 f.0 (p1, p2) t A1 \u00d7 A2 .; G f.0 p t .j <a.. Sc (\u00b5j S) .; G f.0,X:. p t F @. \nX .; G f.0 c p t \u00b5aS X:., .; G f.0 X p t ..F .; G | A f.0 Xq C Pattern spine typing. In: .0, A, Xq with \n.0 f A. Out: ., G, C with .0, .; G f C and .0, .; G, z:A f z Xq C. .1; G1 f.0 p t A .2; G2 | B f.0 Xq \nC \u00b7; \u00b7 | A f.0 \u00b7 A .1, .2; G1, G2 | A . B f.0 p Xq C .; G | .j<a.. Rd (.j R) f.0 Xq C .; G | F @. X f.0,X:. \nXq C .; G | .aR f.0 .d Xq C X:., .; G | ..F f.0 X Xq C Figure 6. Pattern Typing. plying it to a fresh \nsize variable. The general form of a universal type ..F is eliminated by a type variable pattern X; we \nrecord X:. in the type variable pattern context and continue eliminating F @. X. Pattern typing .; G \nf.0 p t A . This judgement checks pat\u00adtern p against type A which is valid in kinding context .0, and \nre\u00adturns pattern contexts .; G. Pattern x succeeds against any type, re\u00adturning singleton context x:A. \nThe empty tuple () succeeds against the unit type 1, binding no variables. The pair pattern (p1, p2) \nsucceeds against the product type A1 \u00d7 A2 if each component pi checks against its type Ai. The resulting \npattern contexts are concatenated, checking for disjointness. A constructor pattern c p checks against \nan inductive type \u00b5 aS if p checks against .j < . j j a . Sc (\u00b5S). The latter succeeds if p = p ', then \nwe add size vari\u00adable j<a to the pattern context and continue checking p ' against Sc (\u00b5j S). This is \nan instance of checking against the general exis\u00adtential type ..F . In the next section, we will validate \nall the typing rules by exhibiting a semantics of strongly normalizing terms based on Girard s reducibility \ncandidates (Girard et al. 1989).  4. Semantics In this section we show strong normalization of Fcop \nby a term . model. Types are interpreted as reducibility candidates ` a la Girard adapted to our needs. \nOur semantic constructions rely only on the terms and the operational semantics of Fcop, not to the types, \nkinds, . or inference rules. Based on the operational semantics, semantic types and kinds are constructed \nthat interpret the syntactic types, yet syntactic types are never used for semantic constructions.4 We \nconsider this conceptual hygiene important from a philosophic per\u00adspective: we use types just as a vehicle \nto assign properties to our programs; clearly, they have no run-time signi.cance. While in the end we \nmanaged to keep syntactic types out of the semantic con\u00adstructions, it was hard to get the semantic counterpart \n(Lemma 9) of pattern spine typing (Figure 6) right.  One clari.cation: Since Fcop has Church-style polymorphism \n. with explicit type abstraction and application, we can of course not talk about terms and operational \nsemantics without mention\u00ading syntactic types. However, we never refer to the structure of syntactic \ntypes, they remain abstract, and we could remove ev\u00aderything but type variables from our type language \nwithout alter\u00ading the construction of semantic types and semantic typing judge\u00adments . In particular, \nin the construction of the semantic universal type . KF = {r . SN | r G . F(G) for all G . Type, G . \nK}there is no connection between the syntactic type constructor G and the semantic type constructor G \n(of semantic kind K). Type appli\u00adcations serve only to make type-checking decidable, they do not play \nany role in evaluation. Preliminaries. We use partially applied relations to denote sets. For instance, \nwe write (t -. ) or simply t-. for the set {t ' | t -. t ' } of reducts of t. Similarly, <a = {\u00df | \u00df \n< a}. The identity substitution is denoted by sid. Strong normalization. Classically, a term t is strongly \nnormaliz\u00ading if it admits no in.nite reduction sequences t -. t1 -. t2 starting with t. Inductively, \nwe de.ne t . SN if all of its reducts are already in SN: (t -. ) . SN t . SN Naturally, if t . SN then \nall its reducts and subterms are also strongly normalizing. We extend the notion SN to other syntactic \ncategories: An elimination e is strongly normalizing, e . SN, if it either is not a term (but a type \nG or a projection .d), or if it is a strongly normalizing term. A de.nition clause D = {qX. t} is strongly \nnormalizing if t . SN. Simulation. Our typing rules (see Figure 5) state that a de.nition .DX: A or (f \n: A = DX) is well-typed if each of the clauses Dk is of type A, individually. In the absence of a coverage \ncheck, there is no concept of the clauses make sense together . We would like to see this independence \nof clauses re.ected in our semantics. In particular, we would like to have compositionality, i. e., if \neach clause of a de.nition is semantically meaningful (in particular, does not lead to non-termination), \nthen the clauses are meaningful together. For functions, our type-checker works exactly like that: each \nclause is checked individually, using the termination measure; an interaction between clauses need not \nbe taken into account. X One idea is to say that a de.ned function f : A = D reduces non-deterministically \nto one of its clauses Dk, however, this imme\u00addiately destroys strong normalization, because Dk might \nmention f. We need to defer unfolding of f until the pattern of one of its clauses matches. Thus, instead \nwe say that f Xe reduces if (.DX)Xe reduces; f is simulated by its clauses DX. In general, a term r is \nsimulated by terms Xr, written r C Xr , iff each of its contractions under some eliminations is accounted \nfor by one of the terms Xr, formally .Xe, t. r Xe . t =. .k. rk Xe . t. Closing reducibility candidates \nby simulation is one of the new ideas of our proof. Lemma 1 (Simulation). 1. .{D1; . . . ; Dn} C .D1, \n. . . , .Dn. 4 Humbly following the masters (Vouillon and Melli`es 2004). 2. If (f : A = DX) . S then \nf C .DX. 3. If r C r1, . . . , rn then r e C r1 e, . . . , rn e.  4.1 Semantic Types In order to show \nstrong normalization we model types as sets of strongly normalizing terms, more precisely, as reducibility \ncandi\u00addates `a la Girard. We choose reducibility candidates over Tait s sat\u00adurated sets, since they allow \nus to show strong normalization in the absence of standardization and con.uence. As a consequence, we \ncan model de.nitions with incomplete and overlapping patterns. A set of terms A is a reducibility candidate \n(Girard et al. 1989), written A . CR, if the following conditions hold. CR1 A . SN: each term in A is \nstrongly normalizing . CR2 if t . A then (t -. ) . A: A is closed under reduction . CR3 if t . Ne and \n(t -. ) . A then t . A: A contains a neutral already if all its redexes are in A . CR4 if t . Intro and \n(t -. ) . A and t C Xt . A then t . A: A is closed under simulation . Condition CR4, is new; it introduces \nmulti-clause objects .DXand function symbols f into a semantic type (candidate). Lemma 2 (Multi-clause \nobjects). 1. If .D1, . . . , .Dn . A then .DX. A. 2. If (f : A = DX) . S and .DX. A, then f . A.  In \nCR3, Ne is a suitable set of so-called neutral terms. These are good , i. e., inhabit a candidate, as \nsoon as all their reducts are good. For Girard s technique to work, neutral terms need to include redexes \nsuch as (.x.t) s Xe and variables x, and need to be closed under application, i. e., r neutral implies \nr s neutral. In case of pure lambda calculus, any term which is not a lambda-abstraction can be considered \nneutral. In our setting of matching the whole pattern spine Xq against the eliminations Xe, things are \nmore subtle. For instance, the partial application .{x y . x x} d with d = .{x . x x} is stuck (and even \nin normal form). However, it cannot be neutral and inhabit every candidate (following CR3), in particular \nsemantic function types, since it reduces to the diverging term d d if applied to one more argument. \nThus, we can only accept stuck terms as neutral which cannot become unstuck by extra eliminations. This \nleads to the following de.nition: De.nition 3 (Neutral term, terminally stuck). A applicative term u \n. App is terminally stuck if u Xe is not a redex for all eliminations Xe. A term r is neutral, written \nr . Ne, if it is a redex or terminally stuck. As Girard s, our re.ned notion of neutrality includes redexes, \nvariables, and is closed under eliminations. Further, if r . Ne then any reduction in r e is either a \nreduction in r or in e. A reducibility candidate A is never empty since Var . A by virtue of CR3. Closure. \nFor a set A . SN which is closed under reduction let A be the least reducibility candidate . A. Inductively, \nA is de.ned as the closure under neutrals and simulation: t . A t . Ne (t -. ) . A t . A t . A t . Intro \n(t -. ) . A t C Xt . A t . A A . A is a closure operation, i. e., it is monotone (A . B implies A . B), \nextensive (A . A), and idempotent (A . A). Note  that the closure operator never adds introduction \nterms such as (), (t1, t2), c t, or Gt to a term set A. Thus, for introductions v . A we have v . A already. \nCR is closed under arbitrary intersections and forms, under the inclusion . order, a complete lattice \nwith greatest element SN and least element \u00d8. Semantic types. In the following, let A, B . CR be candidates, \nP a proposition, K some index set and F . K . CR a family of reducibility candidates. The following operations, \nexcept the conditional P . A, construct new candidates from existing ones. A . B = {r . SN | .s . A. \nr s . B} . KF = {r . SN | .G . Type, G . K. r G . F(G)} P . A = {r . Exp | r . A if P } 1 = {()} A1 \u00d7 \nA2 = {(t1, t2) | t1 . A1 and t2 . A2} . KF = {Gt | G . Type, .G . K, t . F(G)} Note that the condition \nr . SN in the de.nition of A . B is redundant, since x . A by CR3 and r x . SN implies r . SN. However, \nin the de.nition of . KF it is important since K could be empty, e. g., K = <0. Conditional types are \nnot .rst-class; P . A only forms a candidate if P is true, otherwise, it is just a set of expressions. \nLemma 4 (Semantic typing rules). The following inferences are trivial consequences of the construction \nof semantic types: r . A . B s . A r . . KF G.K r s . B r G . F(G) t1 . A1 t2 . A2 G . K t . F(G) () \n. 1 (t1, t2) . A1 \u00d7 A2 Gt . . KF Besides de.nitions (which we will treat in Section 4.5), rules for \nconstructors and destructors are missing. We will describe semantic (co)inductive types in the next section. \n 4.2 Ordinals and Fixed-Points Previous approaches to type-based termination (Hughes et al. 1996; Amadio \nand Coupet-Grimal 1998; Barthe et al. 2004; Blanqui 2004; Sacchini 2013) have de.ned approximants of \nleast \u00b5 aF and greatest .xed-points .aF of monotone type constructors F . + CR . CR by conventional induction \non ordinal a, distinguishing zero (0), successor (a + 1), and limit ordinals (.). 0 .0 \u00b5 F = \u00d8 F = SN \na+1F .a+1F \u00b5 = F (\u00b5 a F) = F (.a F) . .. .aF \u00b5 F = \u00b5aF F = a<. a<. In this work, we adopt the approach \nof Sprenger and Dam (2003) for approximations in \u00b5-calculus and use well-founded induction instead, which \namounts to construct \u00b5 aF by in.ationary iteration and .aF by de.ationary iteration.  a a\u00df \u00b5 F = F (\u00b5\u00df \nF) . F =F (. F) \u00df<a \u00df<a In this de.nition, F does not have to be monotone to obtain an ascending chain \nof approximants in case of \u00b5 and a descending chain for .. However, if F is monotone, one can derive \nabove equations as special cases for a being zero, successor, or limit ordinal, if such a distinction \non ordinals exists. Intuitionistically, this distinction is not valid (Taylor 1996); by building on well\u00adfounded \ninduction, we remain within constructive foundations. Let a, \u00df, . range over ordinals. We write . \u00df<aF(\u00df) \nfor . <aF and analogously for . . Let S . Cons CR . CR and R . Proj CR . CR where we write the .rst argument, \nthe constructor c, or the destructor d, resp., as index, thus, Sc and Rd resp. We de.ne the ath approximants \n\u00b5 aS, .aR . CR of recursive variant and record type as follows. \u00b5 aS = {c t | c . dom(S) and t . . \u00df<aSc(\u00b5\u00df \nS)} .aR = {r . SN | .d . dom(R). r.d . . \u00df<aRd(.\u00df R)} Since . <aF is monotonic in a for any F, so is \n\u00b5 aS. Dually . <aF and .aR are antitonic in a. We obtain chains: \u00d8 = \u00b5 0S . \u00b5 1S . . . . . \u00b5 . S . \u00b5 \n.+1S . . . . SN = .0R . .1R . . . . . .. R . ..+1R . . . . If \u00b5 aS = \u00b5 . S for some a > . then \u00b5 \u00dfS = \n\u00b5 . S for all \u00df > . and we say that the chain has become stationary at .. Since the set Exp of expressions \nis countable and all elements of these chains are subsets of Exp, the chains must become stationary latest \nat the .rst uncountable ordinal O. We call the ordinal at which all such chains of our language are stationary \nthe closure ordinal and denote it by 8. Since it does not make sense to inspect chains beyond the closure \nordinal, we introduce bound normalization . 8 + 1 if a = 8, a= a otherwise. a. .a. Note that \u00b5 aS = \n\u00b5 S and .aR = R. In the following we will talk about ordinals that are as big as 8+n for .nite n, but \nnot bigger ones, so all ordinals will be in O = {a | a < 8 + .}, a set closed under successor. As size \nindex to a least or greatest .xed point, only the ordinals in Size = {a | a = 8} are interesting. Thus, \nif no bound for an ordinal \u00df is given, we assume \u00df . Size, for instance, we write . \u00df F(\u00df) instead of \n. \u00df.SizeF(\u00df) or . SizeF. The stationary point \u00b5 8S is a pre-.xed point in the sense 8 8+1S that t . Sc(\u00b5 \n8S) implies c t . \u00b5 = \u00b5 8S. Dually, .8R is a post-.xed point as r . .8R = .8+1R implies r.d 8 . Rd(.8R). \nNote that we do not require R or S to be monotone for the implications to hold in these directions. Yet \nwe do if we want \u00b5 8S and .8R to be .xed-points. Lemma 5 (Fixed-points). If Sc, Rd be monotone for all \nc . dom(S) and d . dom(R), then 1. \u00b5 8S = {c bt | c . dom(S), b . Type, t . Sc(\u00b58S)}, and 2. .8R = {r \n| .d . dom(R), b . Type. r.d b . Rd(.8R)}.  Proof. For 1, it is suf.cient to show ., meaning that \u00b5 \n8S is a post-.xed point. Note that by de.nition  \u00b5 8S = {c bt | c . dom(S), b . Type, t . Sc(\u00b5\u00dfS)}, \n\u00df<8 so we conclude by monotonicity of Sc and the closure operator, using \u00b5 \u00dfS . \u00b5 8S. For 2, it is suf.cient \nto show that .8R is a pre-.xed point. So, if r.d b . Rd(.8R) for all d . dom(R) and b . Type, then r \n. .8R. It is suf.cient to show r.d b . Rd(.\u00df R) for all \u00df < 8, and this follows from .8R . .\u00dfR by monotonicity \nof Rd.  4.3 Kinds Higher kinds are interpreted as p-variant set-theoretical function p spaces K1 . K2 \nover the base kinds CR and <a. For . a size valuation mapping size variables to ordinals, kind interpretation \n[[.] . is de.ned in the obvious way.  A semantic kinding context D maps type variables X to seman\u00adtic \nkinds K. Semantic kinding contexts classify type environments . mapping type variables to semantic types \nor type constructors; we have . . D if .(X) . D(X) for all X . dom(D). Since kinds in D can depend on \nsize variables declared earlier in D, semantic kinding contexts are dependent. Given D and a family D \n' (. . D), the dependent concatenation of D and D ' is written SDD ' .  4.4 Type constructors Type constructors \nof higher kind are interpreted as operators on semantic types. For . a type environment mapping type \nvariables to semantic types or type constructors, type interpretation [ F ] . maps the syntactic type \nconstructors to the corresponding semantic ones. Kind and type interpretation model the kind-and type-level \njudgements in the usual way. For lack of space, we cannot provide more detail here, see the extended \nversion of this paper instead.  4.5 Patterns, copatterns, .-abstractions In this section, we explain \npatterns and copatterns by developing semantic notions of pattern and pattern spine typing. These pro\u00advide \nus with semantic conditions when a de.nition .DXinhabits a semantic type A. As a consequence, we can \nprove soundness of syntactic pattern, pattern spine, and expression typing. Semantic typing contexts \nand semantic pattern typing. A se\u00admantic typing context E . CXT(\u00b7) (E for typing environment) is a .nite \nmap from term variables to semantic types, so E . Var CR. We write \u00b7 for the empty semantic typing context, \nx:A for the singleton and E, E ' for the disjoint union. Semantic substitution typ\u00ading s . E is de.ned \nas s(x) . E(x) for all x . dom(E). A parameterized semantic typing context E . CXT(D) is a family E(.) \nof semantic typing contexts indexed by semantic type substitutions . that belong to a semantic kinding \ncontext D. Each instance E(.) is a partial function from variables to semantic types. We overload the \nnotation for non-parameterized semantic typing contexts by setting \u00b7(.) = \u00b7 and (x:A)(.) = x:A(.) and \n'' ' (E, E )(.) = E(.), E (.) with dom(E(.)) n dom(E (.)) = \u00d8. For two differently parameterized semantic \ntyping contexts E1 . CXT(D1) and E2 . CXT(D2) we let their disjoint union E1 * E2 . CXT(D1, D2) be de.ned \nby (E1 * E2)(.1 . D1, .2 . D2) = (E1(.1), E2(.2)). Further, if E . CXT(SDD ' ) and . . D we let the partial \napplication E(., ) . CXT(D ' (.)) be de.ned by E(., )(. ' ) = E(., . ' ). If C(G)(.) is a type parameterized \nby another type G and a type substitution ., we let CX be de.ned by (CX)(.) = C(.(X))(. \\ X). In particular, \n(CX)(G/X, .) = C(G)(.). The notations DX and EX are de.ned analogously. A pattern p is semantically of \ntype A in context E if it acts as a bidirectional (invertible) map from E to A, i. e., ps . A for all \ns . E, and, for any substitution s with ps . A we have s . E. Extending this to type substitutions we \nde.ne semantic pattern typing by A / p D; E :.. .t, s. (.. . D. s . E(.)) .. pts . A. Here, and in the \nfollowing, t denotes a syntactic type substitution. Note that it is unconstrained, it needs not bear \na relationship with the semantic type substitution .. One could have expected that semantic pattern typing \nimplies that p matches any introduction term v . A. But since we are not interested in pattern coverage, \nbut merely strong normalization, we do not require this strong guarantee.5 Lemma 6 (Semantic pattern \ntyping). The following implications, written as rules, hold. A / x \u00b7; (x:A) 1 / () \u00b7; \u00b7 A1 / p1 D1; E1 \nA2 / p2 D2; E2 A1 \u00d7 A2 / (p1, p2) D1, D2; E1 * E2 . \u00df<a. Sc(\u00b5 \u00df S) / p D; E \u00b5aS / c p D; E F(G) / p D(G); \nE(G) for all G . K . KF / X p SX:KD; EX Theorem 7 (Soundness of pattern typing). Let f .0, . and .0, \n. f G. If .; G f.0 p t A and .0 . [[.0] then [[A] / p [[.]]; [[G]]. .0 .0 (.0, ) Proof. By induction \non .; G f.0 p t A using the inferences of Lemma 6. Semantic typing in context. Given a parameterized \nsemantic type C . D ' . CR we de.ne weakening WDC . (D, D ' ) . CR of C by semantic kinding context D \nas (WDC)(. . D, . ' ) = C(. ' ). Given a semantic type family C . (D, D ' ) . CR and a semantic type \nsubstitution . . D, we let the partial application C(., ) . D ' . CR be de.ned by C(., )(. ' ) = C(., \n. ' ). Semantic typing under a context is de.ned by D; E f t . C :.. .. . D, s . E(.), t. tt s . C(.) \nLet P be a proposition depending on the pattern variables and pattern type variables of a copattern spine \nXq. We de.ne the fol\u00adlowing shorthand for the replacement of the pattern variables by expressions obtained \nfrom matching Xq against an elimination list Xe: P [Xe/Xq ] :.. .t, s. Xe / Xq t ; s . P ts Semantic \npattern spines. A pattern spine Xq has to be understood by its purpose, to serve as the lhs of a de.nition. \nSemantically, q eliminates type A into C at contexts D; E if any de.nition .{Xq . t} that can be formed \nwith Xq is in A as long as the rhs t is in C under contexts D; E. We further generalize this to partially \napplied de.nitions .{Xq ' qX. t}Xe where Xe matches Xq ' . We let A | Xq D; E; C :.. .t, Xe . SN..Xq \n' . D; E f t[Xe/Xq ' ] . C =. .{qX' Xq . t}Xe . A. Lemma 8 (Semantic clause typing). The following implication \nholds: A | Xq D; E; C D; E f t . C . . D .{Xq . t} . A Proof. With sid . E(.) we have t = tsid . C(.) \n. SN. The rest follows by de.nition of semantic pattern spine typing with empty Xe and empty Xq ' . Note \nthat we cannot proceed if D is inconsistent. Lemma 9 (Semantic pattern spine typing). The following implica\u00adtions \nhold. A1 / p D1; E1 A2 | Xq D2; E2; C A | \u00b7 \u00b7; \u00b7; A A1 . A2 | p Xq D1, D2; E1 * E2; WD1 C 5 On the contrary, \nwe can live with junk introductions in our semantic types. For instance, it would not endanger normalization \nto throw the empty tuple into each semantic type.  . \u00df<a. Rd (.\u00df R) | Xq D; E; C .aR | .d Xq D; E; C \n.G . K. F(G) | Xq D(G); E(G); C(G) . KF | X Xq SX:KD; EX; CX Theorem 10 (Soundness of pattern spine typing). \nLet f .0, . and .0, . f G. If .; G | A f.0 qXC and .0 . [[.0] then [[A]]| Xq [[.]]; [[G]]; [ C]] .0 .0 \n(.0, ) (.0, ). Proof. By induction on .; G | A f.0 Xq C using Lem. 9. Semantic declaration and signature \nwell-formedness. Having understood de.nitons by clauses .DXwe can now show that any well-typed term inhabits \nits corresponding semantic type. For func\u00adtion symbols f, we simply assume it, by postulating a sematically \nwell-formed signature S. We de.ne |= d and |= S by X |= (f : A = D) :.. f . [ A] |=S :.. .d . S. |= d. \nTheorem 11 (Soundness of expression typing). Assume |= S. Let f . and . f G and . f C and D = [[.]] and \nE(.) = [[G]]. and C(.)= [ C]].. 1. If .; G f r C in S then D; E f r . C. 2. If .; G f t t C in S then \nD; E f t . C. 3. If .; G f DXt C in S then D; E f .DX. C.  What remains to be proven is that well-typed \nprograms yield, after measure erasure, semantically well-formed signatures. This is shown mutual block \nby mutual block using a lexicographic induction on ordinals as given by the termination measure assigned \nto each block. A formal description of program typing and its soundness proof has to be delegated to \nthe long version of this paper due to lack of space.  5. Conclusion Our work provides a uniform type-based \napproach to proving ter\u00admination of (co)inductive de.nitions. It is centered around patterns and copatterns \nwhich allow us to reason about both .nite and in.\u00adnite data by well-founded induction. Proving strong \nnormalization for this language is a signi.cant step towards understanding well\u00adfounded corecursion in \nterms of the depth of observation we can safely make. As a next step, we plan to extend our work to full \ndependently typed systems to allow coinductive de.nitions to be de.ned and reasoned with by observations. \nThis will put coinduction in these systems on a robust foundation. We have already implemented size\u00adbased \ntype checking for patterns and copatterns in MiniAgda (Abel 2012) which gives us con.dence in the approach. \n References A. Abel. Polarized subtyping for sized types. Math. Struct. in Comput. Sci., 18:797 822, \n2008a. Special issue on subtyping, edited by Healfdene Goguen and Adriana Compagnoni. A. Abel. Semi-continuous \nsized types and termination. Logical Meth. in Comput. Sci., 4(2:3):1 33, 2008b. CSL 06 special issue. \nA. Abel. Type-based termination, in.ationary .xed-points, and mixed inductive-coinductive types. Electr. \nProc. in Theor. Comp. Sci., 77:1 11, 2012. Proceedings of FICS 2012. A. Abel and B. Pientka. Wellfounded \nrecursion with copatterns: A uni.ed approach to termination and productivity. URL http://www.tcs. ifi.lmu.de/~abel/icfp13-long.pdf. \nExtended version, 2013. A. Abel, B. Pientka, D. Thibodeau, and A. Setzer. Copatterns: Programming in.nite \nstructures by observations. In Proc. of the 40th ACM Symp. on Principles of Programming Languages, POPL \n2013, pages 27 38. ACM Press, 2013. T. Altenkirch and N. A. Danielsson. Termination checking in the presence \nof nested inductive and coinductive types. Short note supporting a talk given at PAR 2010, Workshop on \nPartiality and Recursion in Interactive Theorem Provers, FLoC 2010, 2010. R. M. Amadio and S. Coupet-Grimal. \nAnalysis of a guard condition in type theory (extended abstract). In Proc. of the 1st Int. Conf. on Foundations \nof Software Science and Computation Structure, FoSSaCS 98, volume 1378 of Lect. Notes in Comput. Sci., \npages 48 62. Springer, 1998. G. Barthe, M. J. Frade, E. Gim\u00b4enez, L. Pinto, and T. Uustalu. Type-based \ntermination of recursive de.nitions. Math. Struct. in Comput. Sci., 14 (1):97 141, 2004. G. Barthe, B. \nGr\u00b4egoire, and C. Riba. Type-based termination with sized products. In Computer Science Logic, 22nd Int. \nWksh., CSL 2008, 17th Annual Conf. of the EACSL, volume 5213 of Lect. Notes in Comput. Sci., pages 493 \n507. Springer, 2008. F. Blanqui. A type-based termination criterion for dependently-typed higher-order \nrewrite systems. In Rewriting Techniques and Applications (RTA 2004), Aachen, Germany, volume 3091 of \nLect. Notes in Comput. Sci., pages 24 39. Springer, 2004. F. Blanqui and C. Riba. Combining typing and \nsize constraints for checking the termination of higher-order conditional rewrite systems. In Proc. of \nthe 13th Int. Conf. on Logic for Programming, Arti.cial Intelligence, and Reasoning, LPAR 2006, volume \n4246 of Lect. Notes in Comput. Sci., pages 105 119. Springer, 2006. N. Ghani, P. Hancock, and D. Pattinson. \nRepresentations of stream pro\u00adcessors using nested .xed points. Logical Meth. in Comput. Sci., 5(3), \n2009. E. Gim\u00b4enez. Un Calcul de Constructions In.nies et son application a la v\u00b4eri.cation de syst`emes \ncommunicants. PhD thesis, Ecole Normale Sup\u00b4erieure de Lyon, 1996. Th`ese d universit\u00b4e. J.-Y. Girard, \nY. Lafont, and P. Taylor. Proofs and Types, volume 7 of Cambridge Tracts in Theoret. Comput. Sci. Cambridge \nUniversity Press, 1989. J. Hughes, L. Pareto, and A. Sabry. Proving the correctness of reactive systems \nusing sized types. In Proc. of the 23rd ACM Symp. on Principles of Programming Languages, POPL 96, pages \n410 423, 1996. INRIA. The Coq Proof Assistant Reference Manual. INRIA, version 8.4 edition, 2012. URL \nhttp://coq.inria.fr/. U. Norell. Towards a Practical Programming Language Based on De\u00adpendent Type Theory. \nPhD thesis, Dept of Comput. Sci. and Engrg., Chalmers, G\u00a8oteborg, Sweden, 2007. J. L. Sacchini. Type-based \nproductivity of stream de.nitions in the calculus of constructions. In Logics in Computer Science (LICS \n2013), June 25\u00ad28, 2013, New Orleans, 2013. B. A. Sijtsma. On the productivity of recursive list de.nitions. \nACM Trans. Prog. Lang. Syst., 11(4):633 649, 1989. C. Sprenger and M. Dam. On the structure of inductive \nreasoning: Circular and tree-shaped proofs in the \u00b5-calculus. In Proc. of the 6th Int. Conf. on Foundations \nof Software Science and Computational Structures, FoS-SaCS 2003, volume 2620 of Lect. Notes in Comput. \nSci., pages 425 440. Springer, 2003. M. Steffen. Polarized Higher-Order Subtyping. PhD thesis, Technische \nFakult\u00a8at, Universit\u00a8at Erlangen, 1998. P. Taylor. Intuitionistic sets and ordinals. J. Symb. Logic, \n61(3):705 744, 1996. J. Vouillon and P.-A. Melli`es. Semantic types: A fresh look at the ideal model \nfor types. In Proc. of the 31st ACM Symp. on Principles of Programming Languages, POPL 2004, pages 52 \n63. ACM Press, 2004. H. Xi. Dependent types for program termination veri.cation. J. Higher-Order and \nSymb. Comput., 15(1):91 131, 2002.   \n\t\t\t", "proc_id": "2500365", "abstract": "<p>In this paper, we study strong normalization of a core language based on System F-omega which supports programming with finite and infinite structures. Building on our prior work, finite data such as finite lists and trees are defined via constructors and manipulated via pattern matching, while infinite data such as streams and infinite trees is defined by observations and synthesized via copattern matching. In this work, we take a type-based approach to strong normalization by tracking size information about finite and infinite data in the type. This guarantees compositionality. More importantly, the duality of pattern and copatterns provide a unifying semantic concept which allows us for the first time to elegantly and uniformly support both well-founded induction and coinduction by mere rewriting. The strong normalization proof is structured around Girard's reducibility candidates. As such our system allows for non-determinism and does not rely on coverage. Since System F-omega is general enough that it can be the target of compilation for the Calculus of Constructions, this work is a significant step towards representing observation-centric infinite data in proof assistants such as Coq and Agda.</p>", "authors": [{"name": "Andreas M. Abel", "author_profile_id": "81100320357", "affiliation": "Ludwig-Maximilians-University, Munich, Germany", "person_id": "P4261241", "email_address": "andreas.abel@ifi.lmu.de", "orcid_id": ""}, {"name": "Brigitte Pientka", "author_profile_id": "81100506891", "affiliation": "McGill University, Montreal, PQ, Canada", "person_id": "P4261242", "email_address": "bpientka@cs.mcgill.ca", "orcid_id": ""}], "doi_number": "10.1145/2500365.2500591", "year": "2013", "article_id": "2500591", "conference": "ICFP", "title": "Wellfounded recursion with copatterns: a unified approach to termination and productivity", "url": "http://dl.acm.org/citation.cfm?id=2500591"}