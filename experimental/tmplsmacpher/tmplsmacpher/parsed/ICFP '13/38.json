{"article_publication_date": "09-25-2013", "fulltext": "\n Calculating Threesomes, with Blame Ronald Garcia University of British Columbia rxg@cs.ubc.ca Abstract \nCoercions and threesomes both enable a language to combine static and dynamic types while avoiding cast-based \nspace leaks. Coercion calculi elegantly specify space-ef.cient cast behavior, even when augmented with \nblame tracking, but implementing their semantics directly is dif.cult. Threesomes, on the other hand, \nhave a straight\u00adforward recursive implementation, but endowing them with blame tracking is challenging. \nIn this paper, we show that you can use that elegant spec to produce that straightforward implementation: \nwe use the coercion calculus to derive threesomes with blame. In particular, we construct novel threesome \ncalculi for blame tracking strategies that detect errors earlier, catch more errors, and re.ect an intuitive \nconception of safe and unsafe casts based on traditional subtyping. Categories and Subject Descriptors \nD.3.1 [Software]: Program\u00adming Languages Formal De.nitions and Theory General Terms Languages, Theory \nKeywords casts, space ef.ciency, threesomes, coercions, labeled types 1. Introduction Plenty of research \nhas gone into designing languages that integrate static and dynamic typing. This paper extends existing \nwork on effectively implementing such languages and exposes connections between two ways of specifying \nthem. A popular approach in this area is to extend a language with a dynamic type and use casts to mediate \nbetween static and dynamic code. Here we focus on .(T ), a family of languages based on the lambda calculus \nextended with a dynamic type * and explicit casts between types (see Fig. 1). The language was introduced \nby Siek and Taha (2006), but since then a variety of semantics have been developed for it. Herman et \nal. (2007) observed that higher-order casts casts between function types cause space leaks in programs \n(Sec. 3). To avoid this hazard, they used coercions (Henglein 1994), a low\u00ad level representation for \nsequences of casts, to accumulate higher\u00adorder casts in a compact form. Siek and Wadler (2010) prevent \nthese same space leaks using threesomes, a special pairing of casts that can represent any sequence of \nstandard casts. Permission to make digital or hard copies of all or part of this work for personal or \nclassroom use is granted without fee provided that copies are not made or distributed for pro.t or commercial \nadvantage and that copies bear this notice and the full citation on the .rst page. Copyrights for components \nof this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy \notherwise, or republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. Request permissions from permissions@acm.org. ICFP 13, September 25 27, 2013, Boston, MA, \nUSA. Copyright c &#38;#169; 2013 ACM 978-1-4503-2326-0/13/09. . . $15.00. http://dx.doi.org/10.1145/2500365.2500603 \nCoercions and threesomes both prevent space leaks, but they have different strengths and weaknesses. \nCoercion calculi describe space-ef.cient casts in a way that is easy to understand, and it is straightforward \nto extend them with support for blame tracking, but implementing them directly is dif.cult (Sec. 3.4). \nThreesomes, on the other hand, have a straightforward recursive implementation, but endowing them with \nblame tracking was a subtle and opaque process (Sec. 3.1.1). Moreover, the Threesome Calculus (Siek and \nWadler 2010) supports only one blame tracking strategy, but other blame tracking strategies have been \ndeveloped on top of coercions with useful properties like catching earlier errors and admitting a traditional \nnotion of subtyping to characterize safe casts (Siek et al. 2009). We desire an effective implementation \napproach for these other strategies. This paper addresses this problem with the following contributions: \n We develop three new threesome calculi that correspond to blame tracking strategies identi.ed by Siek \net al. (2009). These calculi lay groundwork for effectively implementing languages with these other strategies, \nthereby supporting earlier error de\u00adtection and an intuitive conception of safe and unsafe casts based \non traditional subtyping (Secs. 5 6).  To produce these results, we .rst revisit the coercion-and threesome-based \nvariants of the Blame Calculus (Siek and Wadler 2010; Siek et al. 2009; Wadler and Findler 2009). These \ncalculi were developed independently and proven to correspond after the fact. Here, we start from the \ncoercion calculus and transform it into a threesome calculus in small, easily veri.ed steps (Sec. 4). \nThis reconstruction provides our strategy for developing new threesomes, but it also clearly shows that \ncoer\u00adcions and threesomes are two alternative representations of the same concept, one more suited to \nspeci.cation and the other to implementation.  2. Related Work Henglein (1994) introduced the Coercion \nCalculus and used it to develop a core theory of dynamic typing that adds a dynamic type to ... In this \nmodel, all programs are typeable and their semantics are given by translation to an internal language \nwith coercions to mediate between types. Siek and Taha (2006) introduced the concept of gradual typ\u00ading, \nwhereby a program explicitly combines static and dynamic typing using a notion of type consistency. In \ncontrast to Henglein (1994), some programs (deemed type-inconsistent ) are rejected statically. Well-typed \nprograms are given meaning by translation to .(T ), which was directly given operational semantics. Herman \net al. (2007) observed that gradually-typed programs could exhibit space leaks that are not visible in \nthe surface lan\u00adguage, including leaks on tail calls. They give an equivalent se\u00admantics for programs \nusing the coercion calculus and prove that it is space ef.cient.  Matthews and Findler (2009) introduced \nand explored ab\u00adstract semantics for programming systems involving multiple inter\u00adoperating languages; \nin parallel, Tobin-Hochstadt and Felleisen (2006) developed a framework for interlanguage migration, \nwhereby dynamically typed modules may coexist with statically-typed mod\u00adules. These works use blame to \nestablish type soundness theorems that extend Milner s conception of type safety in light of dynamic \ntyping: only dynamically typed modules can go wrong. Wadler and Findler (2009) adapted the results of \nthe above work on multi-language semantics and inter-language migration to the context of languages like \n.(T ) that freely mix dynamic and static types throughout. Their Blame Calculus adds blame labels (Findler \nand Felleisen 2002) to casts and satis.es a soundness theorem that ensures that only the dynamic part \nof a program can be blamed for runtime type errors. Siek et al. (2009) introduced a family of related \nsemantics for .(T ) based on coercions. Four variants of the coercion calculus explore two axes of cast \nbehavior: how aggressively or lazily type inconsistencies are detected, and whether blame tracking strategies \nwill blame casts between types that satisfy the traditional subtyping relation. Finally, Siek and Wadler \n(2010) introduce threesomes as an alternative approach to implementing space-ef.cient casts. This paper \nbuilds on these works by constructing threesome calculi for all of the language semantics captured by \nSiek et al. (2009). In doing so, it answers af.rmatively the open question whether the full design space \ncould be given threesome semantics, it clari.es the relationship between the Threesome Calculus and its \ncorresponding coercion calculus, and it lays groundwork for extending threesomes to more fully-featured \nlanguage designs. 3. Background: Casts, Threesomes, and Coercions In this section we discuss the background \nmaterial that underlies this work. Most everything in this section is prior work, though the presentation \nof Lazy UD in this section veers slightly from prior work in ways that make the development herein more \nconvenient. We have proven that the system here is equivalent to the original, but it s not worth dwelling \non. The .(T ) language (Fig. 1) extends the lambda calculus with a dynamic type *, and a runtime cast \nexpression (T2 . T1)l e, which attempts to coerce the value of expression e from type T1 to type T2. \nIf it succeeds, then computation proceeds with the value at the new type; if it fails, then a cast error \nuses the label l to pinpoint blame. By convention, casts always associate to the right, so (T3 .T2)l2 \n(T2 .T1)l1 e means (T3 .T2)l2 ((T2 .T1)l1 e). Basic casts are straightforward. For instance, we can cast \nnum\u00adbers to and from *: (Int .*)l2 (*.Int)l1 42 -. * 42 but projecting a *-wrapped integer to the wrong \ntype fails: (Bool .*)l2 (*.Int)l1 42 -. * blame l2 The label on the projection l2 from * to Bool is used \nto report failure. Languages like .(T ) also support higher-order casts on function-typed objects. Their \nmeaning is dictated by the structure of the relevant types. As a basic example, consider the expression: \nf = (* . Bool .* . *)l2 .x : *.(*.Int)l1 42 The function inside of the l2 cast can never successfully \nbehave as * . Bool, but we cannot know this without analyzing the body of the function. To detect such \nerrors without analyzing functions, higher-order casts adopt the strategy of delaying resolution of the \ncast, and carry its blame label so that failure can be detected when the function is used (Findler and \nFelleisen 2002). The cast splits when the casted function is applied, and its pieces are resolved using \nthe .rst-order strategy: f ((*.Int)l3 7) -. (Bool .*)l2 ((.x : *.(*.Int)l1 42) (*.*)l2 (*.Int)l3 7) \n-. (Bool .*)l2 ((.x : *.(*.Int)l1 42) (*.Int)l3 7) -. (Bool .*)l2 (*.Int)l1 42 -. blame l2. Space Ef.ciency: \nAs Herman et al. (2007) observe, delayed higher\u00ad order casts can lead to undesirable space consumption, \nboth by accumulating casts around values as well as on the control stack. To demonstrate the .rst, they \npresent the following example. let rec even(n : Int, k : Dyn.Bool) : Bool = if (n = 0) then k((*.Bool)l1 \nTrue) else odd(n -1, (Bool . Bool .* . Bool)l2 k) and odd(n : Int, k : Bool.Bool) : Bool = if (n = 0) \nthen k(False) else even(n -1, (* . Bool .Bool . Bool)l3 k) Whenever even or odd is called at non-zero \nvalue, another wrapper is placed around the function k. Despite these functions being tail recursive, \na trace of this routine shows the accumulating casts. even(3,k) -.* odd(2,(Bool . Bool .* . Bool)l2 k) \n-.* even(1,(* . Bool .Bool . Bool)l3 (Bool . Bool .* . Bool)l2 k) -.* odd(0,(Bool . Bool .* . Bool)l2 \n(* . Bool .Bool . Bool)l3 (Bool . Bool .* . Bool)l2 k) -. . . . Casts can also accumulate on the runtime \nstack and break proper tail recursion. Blame Tracking Strategies: Wadler and Findler (2009) adapted the \nnotion of blame tracking from higher-order contracts to higher\u00adorder casts. Later, Siek et al. (2009) \nobserved that various calculi detect cast failures differently for the same expressions. Their de\u00adsign \nspace for casts varies its cast detection strategies along two dimensions. First, some cast calculi detect \nfailures more aggressively than others. Take for example the following expression: (Bool.Int .*.*)l2 \n(*.*. Int.Int)l1 (.x : Int. x) The top-level type constructors of both casts are consistent, but the \ninner types marked in gray, Int and Bool, con.ict. According to the design space, a lazy blame tracking \nstrategy evaluates this expression without error: an error is reported later, though, if the function \nis ever applied. An eager strategy, however, reports an error immediately. Second, when a failure does \noccur, the part of the source pro\u00adgram that is identi.ed as the culprit depends on the blame strategy \nin place. The Blame Calculus was designed to enable a statically\u00adtyped language to interact with a dynamically \ntyped counterpart. It uses casts to mediate between the two, and blame is allocated according to the \nphilosophy that well-typed programs can t be blamed. Alternatively, the * type has also been used to \nexpress .ne-grained combination of dynamic and static typing within a sin\u00adgle language, as in gradual \ntyping (Siek and Taha 2006). When a failure happens in this model, the concern is with which speci.c \nlocation in the uni.ed source program failed. This correlates with the failure of a particular cast that \nwas injected when the program was compiled.  Syntax x . Variables, k . Constants, B . BaseTypes, l . \nLabels atomic types P ::= B | * types T ::= P | T . T expressions e ::= k | x | .x : T . e | e e | (T \n.T )l e G e : T Well-typed Expressions G e : T1 \u00b7 \u00b7 \u00b7 G (T2 .T1)l e : T2 Figure 1. The .(T ) Calculus \nThis difference in strategy manifests in the cast calculus. Take the following expression: (.y : *.(* \n. Int .*)l3 y) ((*.Bool . Bool)l2 .x : Bool. x) ((*.Int)l1 1) It fails with blame under both strategies, \nbut blame is assigned quite differently. The Blame Calculus blames the context around the l2 cast, i.e., \n((.y. : *.(* . Int .*)l3 y) D) (*.Int)l1 1 because on considering the two types in the l2 cast, * is \nmore dynamic than Bool . Bool. The failure is attributed to the code on the more dynamic side of the \nl2 cast: its surrounding context. In general, when neither type in a cast is de.nitively more static \nthan the other, the direction blamed for a failure depends on the particular expressions involved.1 The \nstrategy associated with gradual typing, on the other hand blames the l3 cast. In this model, the * type \nis viewed as a universal supertype to which any other type can safely be upcast. 2 Thus the l3 cast is \na downcast that tries to extract more type information from a dynamically typed value, but fails. This \nmodel, which we call the D model, never blames upcasts. For comparison, we call the model of the Blame \nCalculus UD , since it blames with respect to both upcasts and downcasts. Siek et al. (2009) present \n4 coercion-based calculi named Lazy UD, Lazy D, Eager UD, and Eager D that exhibit these different points \nin the design space of casts while supporting space\u00adef.ciency. The Lazy UD variant corresponds directly \nto the Blame Calculus of Wadler and Findler (2009), which was speci.ed in terms of casts rather than \ncoercions, and subject to the space leaks identi.ed by (Herman et al. 2007). 3.1 Introduction to Threesomes \nIn this subsection, we give a high-level intuition for how three\u00adsomes work. In Sec. 4, we derive a fully \ndetailed semantics for a threesome calculus, so we defer some details until then. Threesomes represent \nany sequence of casts as exactly two casts, a cast (T2 . T1) from a less precise type T1 to a more precise \ntype T2 followed by a cast (T3 .T2) to a less precise type T3. The middle type T2 captures all the type \ninformation carried by the sequence of casts it represents. This representation detects cast failures \nexactly when traditional casts would. For succinctness, T2 such threesomes are written (T3 .= T1). The \nprecision relation is formalized as a (partial) ordering of types <:n: . <:n T B<:n B T<:n * T1 <:n T3 \nT2 <:n T4 T1 . T2 <:n T3 . T4 1 Sec. 3.1 gives a precise characterization of more static. 2 This model \nis orthogonal to, and compatible with, languages that already support subtyping. See Sec. 4.5.  Precision \namounts to replacing the dynamic part of a type with a non-dynamic type, or introducing a type inconsistency \nto re.ect con.icting information. Wadler and Findler call this relation na\u00a8ive subtyping because it corresponds \nto the classic misguided de.nition of subtyping for functions, covariant in the domain types, with * \nas the top (least precise) type and . as the bottom (most precise) type. This partial order induces a \ngreatest lower bound &#38; on types, which is used to implement threesomes. To ensure that &#38; is well \nde.ned, the type . is used to represent type inconsistencies. The de.nition of &#38; is as follows: * \n&#38; T = T &#38; * = T B &#38; B = B (T1 . T2) &#38; (T3 . T4) = (T1 &#38; T3) . (T2 &#38; T4) T1 &#38; \nT2 = . if T1#T2 This de.nition appeals to shallow inconsistency #, which is when two non-* types have \ndifferent leading type constructors IT l: T1#T2 iff IT1l where IBl = B and IT1 . T2l = * . *. = IT2l \nFormally, the greatest lower bound compares the structure of two types, and when a * in one type meets \na static type in the other, the static type is the used. For example, (Bool . *) &#38; (* . Int) = (Bool \n. Int), and so the cast (Bool . * .* . Int) Bool.Int translates to the (Bool . * .= * . Int) threesome. \nThe . type is the greatest lower bound of any two shallowly inconsistent types. Any sequence of threesomes \ncan be represented as a sin\u00adgle threesome by taking the innermost and outermost types of the sequence \nas the new outer types, and taking the greatest lower bound of all the middle types as the new middle \ntype, and thereby summarizing all the type information from the casts. Since (Bool . Int) &#38; (Bool \n. .) = (Bool . .), for instance, the following pair of threesomes reduces to a single threesome: Bool.Int \nBool.. (Bool . * .= * . Int)(* . Int .= Bool . *)f Bool.. -. (Bool . * .= Bool . *)f . If we consider \nthe even-odd example again, and ignore blame for now, we can compile its casts to threesomes. Then the \nearlier execution trace demonstrates the space savings of threesomes: even(3,k) -.* Bool.Bool odd(2,(Bool \n. Bool .= * . Bool)k) -.* Bool.Bool even(1,(* . Bool .= * . Bool)k) -.* Bool.Bool odd(0,(Bool . Bool \n.= * . Bool)k) -. . . . Rather than accumulating like casts, threesomes reduce to a single equivalent \nthreesome at each step. 3.1.1 Adding Blame to Threesomes is Subtle Threesomes are easy to implement, \nsince &#38; is a simple recursive function, and threesomes without blame are easy to understand. However, \nthreesomes become somewhat mystifying when they are enhanced with blame tracking. To support blame tracking, \nSiek and Wadler (2010) sprinkle blame labels throughout the structure of the middle types, now calling \nthem labeled types, and extend compo\u00adsition to propagate those labels. Developing the annotation strategy \nwas a subtle and time-consuming process (Siek 2011b), involving trial and error.3 One is hard-pressed \nto recognize the blame strat\u00adegy as encoded in the labeling of the types or the structure of the composition \noperation.  Syntax atomic types P ::= B | * types T ::= P | T . T dynables G ::= B | * . * coercions \nc ::= .P | G! | G?l | c . c | c . c | Faill wrappers o ::= G! | u . u | G! . (u . u) failures f ::= Faill \ncanonicals u ::= .P | Faill | G! | u . u | G! . (u . u) | G?l | Faill . G?l | G! . G?l | (u . u) . G?l \n| G! . (u . u) . G?l c : T2 . T1 Well-typed Coercions : P . P .P B?l : B . * (* . *)?l : (* . *) . * \nB! : * . B (* . *)! : * . (* . *) Faill : T2 . T1 c1 : T2 . T1 c2 : T4 . T3 c2 : T3 . T2 c1 : T2 . T1 \nc1 .c2 : T1 .T4 . T2 .T3 c2 . c1 : T3 . T1 c -. c Notions of Reduction (1) .P . c -. c (2) c . .P -. \nc (3) B?. B! -. .B  l l (4) (* . *)?. (* . *)! -. (.* . .*) (5) (c1 . c2) . (c3 . c4) -. (c3 . c1) \n. (c2 . c4)  l l (6) Fail. c -. Failif c = G! or c = (c1 . c2) l l (7) c . Fail-. Fail l l (8) G1?. \nG2! -. Failif G1#G2 l ((T .T ))= c Cast Translation ll ll ((P .P ))= .P ((*.B))= B! ((B .*))= B? l ll \n((* .(T3 . T4)))= (* . *)! . (((T3 .*)). ((* .T4))) l lll (((T1 . T2).*))= (((* .T1)). ((T2 .*))) . (* \n. *)? ll l ((T1 . T2 .T3 . T4))= ((T3 .T1)). ((T2 .T4)) l l ((T1 .T2))= Failif T1#T2 Figure 2. Lazy UD \nCoercion Calculus  3.2 Lazy UD Coercions, In Detail Armed with a general understanding of threesomes, \nwe now turn attention to coercions. Since this is the starting point for our work, we immediately explain \nthem in detail. Fig. 2 presents the Lazy UD Coercion Calculus. Coercions are expressions in a small language \nthat characterizes type casts. Identity coercions .P behave as no\u00adops. Injection coercions G! cast values \nof type G to the * type. Projection coercions G?l do the opposite, casting values from * to G, or blaming \nthe l label if they fail. Failure coercions Faill report failure and indicate which label is to blame. \nArrow coercions c . c represent coercions between two function types. Sequences of coercions are composed \nusing the . operator. Coercion expressions are identi.ed up to associativity of composition. Under the \nUD model, a cast failure reports a blame label, which indicates which cast to consider, as well as a \npolarity, which indi\u00ad 3 e.g. We initially tried to label bottom types with a single label ... . However, \nthat approach fails to capture the correct blame tracking behav\u00adior. (Siek and Wadler 2010) cates whether \nthe term inside the cast or its surrounding context is to blame for the failure. To implement this behavior, \neach blame label attached to a coercion is endowed with a polarity during com\u00adpilation from casts to \ncoercions. Source-language casts are deemed to have positive polarity, but polarity reverses when compiling \nthe domain of any cast between function types. The result is that coer\u00adcions that lie to the left of \nan odd number of coercion arrows have negative polarity; the rest have positive polarity. An inversion \noper\u00adation l on labels reverses their polarity. This operation is involutive, meaning l = l. Coercions \nare typed according to the type of input they admit and the output type they produce. Note that for arrow \ncoercions, typing is contravariant in their domains. From here on, we only consider well-typed coercions. \nTo characterize coercion reduction, we .rst isolate some vari\u00adeties of legal coercions. The canonical \ncoercions u are the normal forms of coercions with respect to coercion reduction. The wrap\u00adpers o represent \ndelayed (sequences of) casts wrapped around sim\u00adple program values. Failure coercions f are used in the \ncoercion\u00adbased lambda calculus to indicate when casting a value has failed. Coercion reduction is the \ncompatible closure of the notions of reduction c -. c; for simplicity we use the same notation for both. \nType casts are translated to coercions using the ((T . T ))l function. For example, the paired casts \n(* . Bool .Bool . Bool)l3 (Bool . Bool .* . Bool)l2 compile and reduce to the well-typed coercion expression \nf (Bool! . Bool?l3 ) . .Bool : (* . Bool) . (* . Bool). 3.2.1 Properties of Coercion Reduction In this \nsubsection we state some of the classic properties of the coercion calculus. Though these have been proven \npreviously, we prove them again to account for the changes from Henglein (1994) (speci.cally the introduction \nof blame, and the removal of non\u00adatomic .T coercions). These properties of the coercion calculus are \nused in the development to follow.4 Proposition 1. Coercion reduction -. preserves typing. Proof. By \ncases on reduction. Proposition 2 (Siek (2011a)). -. is strongly normalizing. Proof. Consider as a size \nmetric the lexicographic order on 1) the size of the coercion (raw number of non-.P nodes) and 2) the \nnumber of function coercions. Then coercion reduction decreases that metric. Proposition 3. c is normal \niff c is a u. Proof. The only-if part is proven by induction on the typing judg\u00adment f c : T2 . T1. The \nonly interesting case is c1 . c2 which can be proven by cases, using the induction hypothesis to deduce \nthat c1 = u1 and c2 = u2. The if part is proven by induction on the structure of u. Proposition 4. Coercion \nreduction -. is con.uent. Proof. Since reduction is normalizing, we need only con.rm that critical pairs \nconverge. Among the reduction rules, two pairs of rules overlap with each other completely: 1. .P . \nc and c . .P ; 2. .P . c and c . Faill;  Also, .ve redex pairs c1 . c2 and c2 . c3 interact because \n. is associative: 4 We have proven all of these except for con.uence for all four coercion calculi discussed \nin this paper. Con.uence does not hold for the eager calculi.  Syntax expressions e ::= k | x | .x : \nT. e | e e | (u)e | blame l simple values s ::= k | .x : T. e values v ::= s | (o)s canonicals w ::= \nv | blame l cast-free contexts F ::= 0 | E [0 e] | E [v 0] evaluation contexts E ::= F | F [(u)0] G e \n: T Well-typed Expressions G e : T1 u : T2 . T1 \u00b7 \u00b7 \u00b7 G (u)e : T2 (c) (T ) (c) C[ \u00b7] : .. . Program Translation \n. . . C(c) [ (T2 .T1)l t] = (((T2 .T1))l)C(c)[ t] e -. e Notions of Reduction (9) (.x : T.e) v -. [v/x] \ne (10) ((u1 . u2)s) v -. (u2)(s ((u1)v)) * (11) (u1)(u2)e -.c (u3)e if (u1 . u2) -. u3 (12) (.B)s -.c \ns (13) (f)s -.c blame l e -. e Standard Reduction e1 -. e2 e1 -.c e2 E [e1] -. E [e2] F [e1] -. F [e2] \nE = 0 E [blame l] -. blame l Figure 3. The .(c) Calculus 1. .P . c2 . c3; 2. c1 . c2 . .P ; 3. c1 . \nc2 . Faill;  4. Faill . (c11 . c12) . (c21 . c22); and 5. (c11 . c12) . (c21 . c22) . (c31 . c32). \n In each case, reduction converges.  3.3 The .(c) Calculus To use the coercion calculus, we translate \n.(T ) programs to .(c), the simply-typed lambda calculus with coercions, presented in Fig. 3 (Siek et \nal. 2009). Its syntax differs from .(T ) only in that cast expressions are replaced with normal coercion \nexpressions (u)e. The translation from .(T ) to .(c) is simple: The C(c)[ t] function translates t by \nreplacing each cast (T1 . T2)l with its corresponding coercion ((T1 .T2))l . The .(c) notions of reduction \nare split into plain reductions e -. e and coercion reductions e -.c e. Rule 12 is specialized to .B, \nbecause there are no simple values s of type *. Rule 11 appeals to coercion calculus reduction to compose \nand normalize two sequential coercions.To ensure that coercions are compressed on the stack, coercion \nreductions -.c can only happen in a context with no immediately pending coercion. Plain reductions -., \non the other hand, can happen in any evaluation context. If a coercion fails, a program results in blame. \nIn general, .(c) is parameterized over the de.nition and semantics of the coercion calculus. By changing \nthe coercion language, we can switch the blame-tracking strategy. Proposition 5. The .(c) calculus is \ntype safe. Proposition 6 ((Siek et al. 2009)). The .(c) calculus is space ef.cient. Proposition 7 ((Siek \net al. 2009)). The .(c) calculus with Lazy UD coercions corresponds to the Blame Calculus. If we consider \nthe even-odd example again, we can compile its casts to coercions and see the space savings in the resulting \ntrace: even(3,k) -.* odd(2,(B! . .)k) -.* even(1,(B! . B?l3 . .)k) -.* odd(0,(B! . .)k) -. . . . This \nsequence of coercions repeats for larger numbers.  3.4 Implementing Coercions is Hard Coercions cleanly \nand elegantly specify space-ef.cient cast seman\u00adtics, which is one reason they were used to characterize \nthe de\u00adsign space of casts in (Siek et al. 2009). However, they are dif\u00ad.cult to implement directly for \ntwo reasons. First, coercions are de.ned up to associativity, which can make some redexes harder to .nd, \nand can interfere with developing a deterministic reduc\u00adtion strategy. Second, some coercion compositions \nremain inert, meaning that they do not reduce, but composing with another co\u00adercion may warrant a reduction. \nThese two properties interact with each other in troublesome ways. Consider the coercion expression ((* \n. *)?l .((* . *)!.(c1 . c2))).(c3 . c4). The subexpres\u00adsion (* . *)! . (c1 . c2) is inert, so it does \nnot reduce. However, the coercions (* . *)?l and (* . *)! could reduce if composed directly, but in order \nto do so, the compositions must be reassoci\u00adated .rst. The result, .* . .*, could then compose with c1 \n. c2. Meanwhile, the two arrow coercions c1 . c2 and c3 . c4 could also reduce, but are also blocked \nby how they are associated. One might try to devise an ad hoc reassociation scheme or represent a sequence \nof coercions as lists, but it would necessarily involve pair\u00adwise comparisons, bidirectional search, \nand splicing into the mid\u00addle of complex coercion expressions. In short, it remains a chal\u00adlenge to design \na redex search and reduction strategy that is com\u00adplete in the face of inert compositions and associativity, \nyet simple enough to reason about and implement correctly. We do exactly this in the next section. 4. \nThreesomes for Lazy UD This section presents a way to transform Lazy UD coercions into Lazy UD Threesomes \nusing small, easily veri.ed steps. The strat\u00adegy we apply and explain here is used later to produce similar \nre\u00adsults for other coercion calculi. 4.1 Composing normal coercions as a function Our strategy begins \nby focusing speci.cally on normal coercions u. Recall that coercions reduction is both con.uent and strongly \nnormalizing. These two properties ensure there exists, in theory, a well-de.ned composition function \nfor normal coercions. De.nition 1 (Normal Composition). Let NORMCT2.T1 be the set of well-typed normal \ncoercions f u : T2 . T1. Then de.ne the normal composition function 8 : NORMCT3.T2 \u00d7 NORMCT2.T1 . NORMCT3.T1 \nas follows: u1 8 u2 = u3 iff (u1 . u2) -.* u3. Using this de.nition, we can restate rule 11 of .(c) as \nfollows: (11) (u1)(u2)e -.c (u1 8 u2)e This rephrasing does not resolve our issues with the coercion \ncal\u00adculus, because 8 is de.ned in terms of nondeterministic coercion reduction, the source of our troubles. \nWe address this next.  Syntax dynables G ::= B | * . * supercoercions \u00a8c ::= Faill | FaillGl | G! | \nG?l | G!?l | .P | \u00a8c . \u00a8c | \u00a8c !. \u00a8c | \u00a8c .? l \u00a8c | \u00a8c !.? l \u00a8c wrappers \u00a8o ::= G! | \u00a8c . \u00a8c | \u00a8c !. \n\u00a8c failures \u00a8 f ::= Faill S c\u00a8: T . T Well-typed Supercoercion \u00b7 \u00b7 \u00b7 S l1Gl2 S Fail: T . * G!?l : * \n. * SS SS c\u00a81 : T2 . * c\u00a82 : * . T3 c\u00a81 : * . T1 c\u00a82 : T4 . * S S l c\u00a81 !. c\u00a82 : * . T2 . T3 c\u00a81 .? c\u00a82 \n: T1 . T4 . * S c\u00a81 : * . * S c\u00a82 : * . * S c\u00a81 !.? l c\u00a82 : * . * N[ \u00a8c] = c Supercoercion Meaning N[ \n.P ] = .P l l N [ Fail] = Fail l1Gl2 ] l1 . G?l2 N[ G!]] = G! N [ G?l] = G?l N [ G!?l] = G! . G?l N \n[ \u00a8c1 . c\u00a82] = N [ \u00a8c1] . N [ \u00a8c2] N [ \u00a8c1 !. c\u00a82] = (* . *)! . (N [ \u00a8c1] . N [ \u00a8c2]]) N [ Fail= Fail \nl N [ \u00a8c1 .? c\u00a82] = (N [ \u00a8c1] . N [ \u00a8c2]]) . (* . *)?l l N [ \u00a8c1 !.? c\u00a82] = (* . *)! . (N [ \u00a8c1] . N \n[ \u00a8c2]]) . (* . *)?l Figure 4. LazyUD Supercoercions  4.2 Supercoercions Our next step toward a simple \nimplementation is to replace the set of normal coercion expressions u . NORMC with a new set of composition-free \ncoercions c\u00a8. SUPERC, one for each form that a normal coercion can take (Fig. 4). We call them supercoercions, \nsince a single supercoercion may stand for the composition of several traditional coercions. We de.ne \nthe meaning of our supercoercions in terms of the normal coercions by giving a bijective mapping. Proposition \n8. N [ \u00b7] is one-to-one and onto. Supercoercions correspond directly to normal coercions accord\u00ading to \nthe (reversible) mapping N [ \u00b7] . The types for supercoercions correspond to normal coercion types, so \nboth N [ \u00b7] and N -1[ \u00b7] preserve typing. Proposition 9. fS c\u00a8: T2 . T1 iff f N [ \u00a8c] : T2 . T1. Proof. \nBy induction on derivations for each direction. Composing supercoercions We de.ne composition of supercoer\u00adcions \nin terms of composition 8 of normal coercions. De.nition 2. The function : SUPERCT3.T2 \u00d7 SUPERCT2.T1 \n. SUPERCT3.T1 is de.ned as follows: c\u00a81 c\u00a82 = N -1[ N [ \u00a8c1] 8 N [ \u00a8c2]]]]. Since 8, N -1[ \u00b7] , and \nN [ \u00b7] are well-de.ned, so is , but it is still de.ned in terms of normal coercion composition. However, \nstarting from this abstract de.nition, we can calculate a new recur\u00adsive de.nition over the structure \nof supercoercions. To produce our equations, we rely on the fact that normal arrow coercions compose \nin a syntax-directed way. Proposition 10. (u1 . u2) 8 (u3 . u4) = (u3 8 u1) . (u2 8 u4) Proof. Straightforward \ncalculation. Now, using Def. 2 and Prop. 10, we calculate a recursive de.ni\u00adtion of . Here is one example \ncase: (\u00a8c1 .?l c\u00a82) (\u00a8c3 !. c\u00a84) l \u00a8 = N -1[ N [ (\u00a8c1 .?c2)]] 8 N [[(\u00a8c3 !. c\u00a84)]]]] = N -1[[((N [ \n\u00a8c1] . N [ \u00a8c2] ) . (* . *)?l) 8 ((* . *)! . (N [ \u00a8c3] . N [ \u00a8c4]]))]] but by reduction, (N [ \u00a8c1] . \nN [ \u00a8c2]]) . (* . *)?l . (* . *)! . (N [ \u00a8c3] . N [ \u00a8c4]]) -. * (N [ \u00a8c1] . N [ \u00a8c2]]) . (N [ \u00a8c3] . \nN [ \u00a8c4]]) -. * (N [ \u00a8c3] 8 N [ \u00a8c1] ) . (N [ \u00a8c2] 8 N [ \u00a8c4]]) (by Prop. 10), so, N -1[[(N [ \u00a8c3] 8 \nN [ \u00a8c1]]) . (N [ \u00a8c2] 8 N [ \u00a8c4]])]] = N -1[[(N [ \u00a8c3] 8 N [ \u00a8c1]])]] . N -1[[(N [ \u00a8c2] 8 N [ \u00a8c4] )]] \n= (\u00a8c3 c\u00a81) . (\u00a8c2 c\u00a84) so we have (\u00a8c1 .?l c\u00a82) (\u00a8c3 !. c\u00a84) = (\u00a8c3 c\u00a81) . (\u00a8c2 c\u00a84). We calculate \nnew equations for all pairs of compatible supercoer\u00adcions to get the full recursive de.nition.5 Supercoercions \ncan be used to normalize arbitrary coercion expressions. To see this, recall that each atomic coercion \nis also a supercoercion, so we can compose them using . Combining this with the translation from normal \narrow coercions, we get a translation from non-normal coercions to supercoercions: \u00a8 C[ c] = c if c is \natomic; \u00a8 \u00a8\u00a8 C[ c1 . c2] = C[ c1] C[ c2] ; \u00a8 \u00a8\u00a8 C[ c1 . c2] = C[ c1] . C[ c2] For example, the problematic \ncoercion from Sec. 3.4 translates \u00a8 directly to its corresponding supercoercion (assuming C[ ci] = \u00a8ci): \n((* . *)?l ((* . *)! (\u00a8c1 . c\u00a82))) (\u00a8c3 . c\u00a84) = ((* . *)?l (\u00a8c1 !. c\u00a82)) (\u00a8c3 . c\u00a84) = (\u00a8c1 . c\u00a82) \n (\u00a8c3 . c\u00a84) = (\u00a8c3 c\u00a81) . (\u00a8c2 c\u00a84) Then, since we can always map supercoercions back to normal co\u00adercions, \nwe immediately see that c1 8 c2 = N [ C \u00a8 [ c1 . c2]]]], that is, N [ C \u00a8 [ \u00b7]]]] implements a recursive \nnormalizer for traditional coer\u00adcions, overcoming our troubles with associativity and inertness. .(\u00a8 \n 4.3 c): A Supercoercion-based calculus Since supercoercions are isomorphic to normal coercions, we \ncan skip the back-and-forth translation steps and instead de.ne a di\u00adrect implementation semantics for \n.(c): simply replace normal co\u00adercions u with supercoercions c\u00a8and the normal composition 8 with . The \nresulting calculus .(c\u00a8), presented in Fig. 5, is a determinis\u00adtic calculus with an easy-to-implement \ncomposition operator. like .(c), this calculus is parameterized on the de.nition of supercoer\u00adcions. \nProposition 11. Consider the compatible extension of N -1[ \u00b7] over .(c) terms. If t is a .(c) term, then \n(c) (c\u00a8) N -1 .f t -. * w iff .f N -1[ t] -. * [ w] . Proof. By a straightforward bisimulation. To complete \nour implementation, we compile .(T ) terms to .(c\u00a8) terms using the C(c\u00a8)[ \u00b7] function. 5 We formalized \nand checked completeness of our de.nition using Twelf. Its equations are correct by construction.  Syntax \nexpressions e ::= k | x | .x : T. e | e e | (c\u00a8)e | blame l simple values s ::= k | .x : T. e values \nv ::= s | (o\u00a8)s canonicals w ::= v | blame l cast-free contexts F ::= 0 | E [0 e] | E [v 0] evaluation \ncontexts E ::= F | F [(c\u00a8)0] S G e : T Well-typed Expressions G S e : T1 S c\u00a8: T2 . T1 . . . G S (c\u00a8)e \n: T2 (c\u00a8) (T ) (c\u00a8) C[ \u00b7] : .. . Program Translation . . . C(c\u00a8)[ (T2 .T1)l t] = (N -1[ ((T2 .T1))l] \n)C(c\u00a8)[ t] e -. e Notions of Reduction (9 ) (.x : T.e) v -. [v/x] e (10 ) ((c\u00a81 . c\u00a82)s) v -. (c\u00a82)(s \n((c\u00a81)v)) (11 ) (c\u00a81)(c\u00a82)e -.c (c\u00a81 c\u00a82)e (12 ) (.B)s -.c s (13 ) (f \u00a8 )s -.c blame l Figure 5. The \n.(c\u00a8) Calculus  4.4 Simpli.cation: from Supercoercions to Threesomes Supercoercions, while effective, \nare not particularly succinct. They consist of 11 forms, including the original coercions. Furthermore, \nsupercoercion composition is de.ned with 60 equations. Intu\u00aditively, this happens because coercion computation \nwas already de.ned for coercions and now the additional supercoercions must repeat the same computations \nfor each possible normal form. For example, consider two composition equations: (\u00a8c1 . c\u00a82) (\u00a8c3 . c\u00a84) \n= (\u00a8c3 c\u00a81) . (\u00a8c2 c\u00a84) (\u00a8c1 .?l c\u00a82) (\u00a8c3 !. c\u00a84) = (\u00a8c3 c\u00a81) . (\u00a8c2 c\u00a84) The derivations for both \nequations involved composing arrow co\u00adercions, so each internalizes that reduction step. This computation \nappears as part of different equations because several different nor\u00admal form coercions contain an arrow \ncoercion component within them. We would like to .nd a representation for normal coercions that removes \nthis computational redundancy, and in fact we do. To address this blow-up, we apply correctness-preserving \ntransforma\u00adtions to re.ne our large but correct system into a streamlined and equivalent counterpart. \nTo our delight, the resulting system is in fact nearly exactly the Threesomes of (Siek and Wadler 2010). \n4.4.1 Primitive arrow coercions are redundant In the following two analogous equations, B?l B! = .B \n(* . *)?l (* . *)! = .* . .* the equation for base types produces a single .B , while the rule for arrows \ncreates a complex arrow coercion. However, this complex coercion behaves as if it were a primitive identity \ncoercion .*.*. Other primitive coercion equations reveal similar correspon\u00addences. For example, consider \nthese two equations: B!?l B! = B! (* . *)!?l (* . *)! = .* !. .* They have corresponding primitive \nleft-hand sides, but the result of composing primitive arrow compositions is a complex arrow com\u00adposition. \nIn this case, one would expect that the .* !. .* coercion behaves as if it were the primitive (* . *)! \ncoercion, and indeed it does. For instance, if we substitute for (* . *)! on the left side of the above \nequation, then the de.nition of gives (* . *)!?l (.* !. .*) = .* !. .*. This correspondence generalizes \nto all primitive arrow coercions. Proposition 12. Let S1[ \u00a8c] be de.ned by S1[[(* . *)!]] = .* !. .* \nS1[ (* . *)?l] = .* .?l .* S1[[(* . *)!?l] = .* !.?l .* S1[ \u00a8c] = \u00a8c otherwise. Then S1[ \u00a8c1 c\u00a82] = \nS1[ \u00a8c1] S1[ \u00a8c2] . This means that the primitive arrow supercoercions are redun\u00addant and can be discarded. \nThis simpli.cation is safe because it preserves failure supercoercions Faill . We produce a new translation \nS = S1 . N -1 from normal coercions to supercoercions. This mapping is not bijective though, e.g. N [ \nS[[(* . *)?l]]]] = N [ .* .?l .*] = (* . *)?l . (.* . .*).  4.4.2 Merge primitive coercions into type-polymorphic \ncounterparts Consider again the base type equations from the last subsection: B?l B! = .B B!?l B! = \nB! They are examples of redundancy in supercoercions: the deriva\u00adtions for both equations involve reducing \nthe coercion B?l . B! (recall that N [ B!?l] = B! . B?l). Surprisingly, we can factor away this redundancy; \ndoing so depends on the internal structure and the types of the base type coercions. The base type coercions \ncan be grouped based on their behavior and syntactic structure. The supercoercions B! and .B both coerce \nfrom a source type B, and perform safe coercions that will not cause blame, so they have no blame label. \nConsider them group 1. On the other hand, B?l and B!?l both coerce from source type *, and perform a \npossibly unsafe coercion, so they each carry a blame label. Consider them group 2. Now, looking back \nat both equations, each is an instance of composing a supercoercion from group 2 with one from group \n1 to get another from group 1. Within these groups, the members can be differentiated based on their \ntypes: for group 1, f B! : * . B but f .B : B . B; for group 2, f B?l : B . * but f B!?l : * . *. In \nfact, these four uni-typed coercions can be replaced with two polymorphic coercions, one for each group, \nso long as the type of the context where they are used is known. We de.ne two coercions based on the \nindices from their group: B, and Bl . Each can be typed according to two typing rules: f B : * . B f \nBl : * . * f B : B . B f Bl : B . * We can then de.ne a new set of supercoercions c\u00a8' by replacing the \nold base type supercoercions with these new ones. We de.ne the meaning of the polymorphic supercoercions \nin terms of the origi\u00adnal supercoercions. Since the typings of the new supercoercions matter, however, \nwe .rst de.ne a translation SD[ \u00b7] that yields type derivations D rather than simply supercoercions. \nSince the source supercoercions are uni-typed, the translation is unambiguous. The de.nition is the compatible \nextension of the following equations: SD[ B!]] = f B : * . B SD[ B!?l] = f Bl : * . * SD[ .B] = f B : \nB . B SD[ B?l] = f Bl : B . *. Proposition 13. SD[ \u00b7] is bijective and preserves typing. This translation \nretains a one-to-one correspondence between typing derivations for supercoercions, but there are fewer \ntar\u00adget supercoercions than source supercoercions. Ultimately we want to exploit that simpli.cation, \nso we de.ne the translation S2[ \u00a8c] = |SD[ \u00a8c] | from supercoercions to polymorphic supercoer\u00adcions, \nwhere |D| extracts a supercoercion from its type derivation.  This gives us the translation that we \nwant and, somewhat surpris\u00adingly, composition on the resulting polymorphic coercions is well\u00adde.ned. \nProposition 14. The equation S2[ \u00a8c1] 2 S2[ \u00a8c2] = S2[ \u00a8c1 c\u00a82] de.nes a function 2 on the polymorphic \nsupercoercions c\u00a8' . Proof. By simultaneous induction on c\u00a81 and c\u00a82. This result means that we can replace \nthe base type superco\u00adercions with polymorphic counterparts and still get a well-de.ned notion of composition \nthat preserves failure Faill . We take this as our notion of supercoercions and drop the primes and subscripts. \nThis polymorphic simpli.cation can also be applied to arrow coercions. By doing so, we can combine c\u00a8. \nc\u00a8and c\u00a8!. c\u00a8as a single coercion c\u00a8. c\u00a8and replace c\u00a8.? l c\u00a8and c\u00a8!.? l c\u00a8with the single coercion c\u00a8.l \nc\u00a8. This transformation is subject to the same correctness criteria as above and similarly decreases \nthe number of supercoercions and the number of composition equations.  4.4.3 Introduce Optional Data \nand Coalesce With a slight change of representation, we can merge the two base type coercions, B, and \nBl , into one. First, we produce a new form of coercion which features optional labels p, where E indicates \nthe absence of a label. Now we can merge both base type coercions into one coercion of the form Bp. Similarly, \nwe can merge arrow casts into the form c\u00a8.p c\u00a8. We can now refer to atomic and arrow coercions uniformly \nas an optionally labeled type constructor Qp, standing for either an atomic coercion Bp or an arrow coercion \nc\u00a8.p c\u00a8. At this point we are close to uncovering our labeled types. In direct analogy with leading type \nconstructors IT l, we de.ne IQpl to yield Qp s leading constructor; similarly we de.ne coercion inconsistency \nQp 1#Q2 p to hold when two coercions have incompatible top-level constructors. Introducing optional labels \nfurther simpli.es our de.nition of composition. For example, the four equations: B B = B, B Bl = Bl \n, Bl B = B, and Bl1 Bl2 = Bl2 become one equation: Bp1 = Bp2 . Notice that the function need not Bp2 \neven inspect p2: it simply copies it over to the result. The same thing happens with arrow coercions. \nUsing the same strategy, we merge the Faill and FaillGl forms by making the G and l pair into an optional \nannotation H. We call it a projector since it represents an optional projection before a failure cast. \nFinally, we de.ne a helper function [\u00b7] that interprets a labeled type Qp into a projector H. This completes \nour simpli.cation process, yielding the labeled types and composition operator of Fig. 6. We de.ne S[ \n\u00b7] to be the transformation from coercions to la\u00ad beled types that results from composing the supercoercion \ntransla\u00adtion N -1[ \u00b7] and transformations Si[ \u00b7] corresponding to each step of simpli.cation.6 The resulting \nlabeled types closely mirror those of Siek and Wadler (2010), but for a few cosmetic differences: 1. \nIn general c\u00a8corresponds directly to labeled types P ; 2. Our .* corresponds to the labeled type *; \n 3. The Bp and c\u00a8.p c\u00a8coercions are the same; 4. FaillH corresponds to .lGp, with H playing the role \nof Gp. In the original Threesomes, only the label p was optional. Our derivation reveals, however, that \nG is only needed if an actual label is present.  6 For brevity, we omit Si[ \u00b7] de.nitions for polymorphic \narrows and optional data. Syntax dynables G ::= B | * . * optional labels p ::= E | l failure projectors \nH ::= E | Gl labeled constructors Qp ::= Bp | c\u00a8.p c\u00a8 lH | Qp labeled types c\u00a8::= .* | Failwrappers o\u00a8::= \nB' | c\u00a8.' c\u00a8failures f \u00a8 ::= Faill c\u00a8 c\u00a8= c\u00a8 Composition .* c\u00a8= c\u00a8 .* = c\u00a8Bp1 Bp2 Bp2 = (\u00a8c1 .p1 c\u00a82) \n (\u00a8c3 .p2 c\u00a84) = (\u00a8c3 c\u00a81) .p2 (\u00a8c2 c\u00a84) , p2 l1 p2 l1 Q 2 Q Q= Fail if Q1#Q2 1 2 l1H l1H c\u00a8 Fail= \nFaill l[Qp] Qp l1Gl2 Qp3 Fail= Fail Fail= Faill1[Qp3 ] if rQl = G l1Gl2 Qp Fail= Faill2[Qp] if rQl \n= G Qp rQpl = G = H Failure ProjectorType Constructor ' Q= E rBpl = B , lrc\u00a81 .p c\u00a82l = (* . *). B= \nBl , l c\u00a81 .c\u00a82= (* . *)l l ((* .*)) Cast Translation l l'l' ((* .*))= .* ((* .B))= B((B .B))= B l l \nl l' if T1#T2 ((B .*))= B((T1 .T2))= Fail l l'l ((* .T1 . T2))= ((T1 .*)).((* .T2)) l lll ((T1 . T2 .*))= \n((* .T1)).((T2 .*)) l l'l ((T1 . T2 .T3 . T4))= ((T3 .T1)).((T2 .T4)) Figure 6. Lazy UD Labeled Types \nCast translation is de.ned by composing the original cast-to\u00adcoercions translation ((T .T ))l with S[ \n\u00b7] . We explicitly calculated this direct de.nition, but it exactly mirrors that in (Siek and Wadler \n2010). Now, if we consider our running example from even-odd, the casts (* . Bool .Bool . Bool)l3 (Bool \n. Bool .* . Bool)l2 compile and compose to the labeled type Booll3 . BoolE.  4.4.4 From .(c\u00a8) to the \nThreesome Calculus .T Just as we replaced normal coercions in .(c) with supercoercions to produce .(c\u00a8), \nso can we now replace supercoercions with labeled types. However, there is a catch. Consider rule (12 \n) of .(c\u00a8). The .B supercoercion corresponds to the labeled type BE, but so does the B! supercoercion. \nTo disambiguate these two possible situations, we can use the type information for the labeled type. \nWe replace the supercoercions c\u00a8in .(c\u00a8) with threesomes (T2 .c\u00a8= T1), where f c\u00a8: T2 . T1, resulting \nin the Threesome Calculus of Siek and Wadler (2010). To compile casts to threesomes, we produce the corresponding \nlabeled type and attach the source and target types: ((T2.T1))l T [ (T2 .T1)l ] = (T2 .= T1). Proposition \n15. Consider the compatible extension of T [ \u00b7] over .(c) terms. If t is a .(c) term, then (c) T .f t \n-. * w iff .f T [ t] -. * T [ w] . Proof. By a straightforward bisimulation.  Syntax c\u00a8 expressions \ne ::= k | x | .x : T. e | e e | (T2 .=T1)e | blame l simple values s ::= k | .x : T. e o\u00a8 values v ::= \ns | (T2 .= T1)s canonicals w ::= v | blame l cast-free contexts F ::= 0 | E [0 e] | E [v 0] , c\u00a8 evaluation \ncontexts E ::= F | F (T2 .= T1)0 S G e : T Well-typed Expressions G S e : T1 S c\u00a8: T2 . T1 . . . c\u00a8 G \nS (T2 .= T1)e : T2 T (T ) T C[ \u00b7] : .. . Program Translation . . . CT [ (T2 .T1)l t] = (T [ ((T2 .T1))l] \n)CT [ t] e -. e Notions of Reduction (9 ) (.x : T.e) v -. [v/x] e (10 ) c\u00a81 . ' c\u00a82 c\u00a82c\u00a81 ((T1 . T2 \n.= T3 . T4)s) v -. (T2 .=T4)(s ((T3 .=T1)v)) c\u00a81c\u00a82c\u00a81 c\u00a82(11 ) (T3 .=T2)(T2 .=T1)e -.c (T3 .= T1)e B \n' (12 ) (B .=B)s -.c s \u00a8 f (13 ) (T2 .=T1)s -.c blame l Figure 7. The Threesome Calculus (.T ) Re-constructing \nthe Threesome Calculus sheds new light on the relationship between Lazy UD coercions and threesomes, \nbut the key bene.t of this process is that it gave us a strategy for devel\u00adoping new threesomes corresponding \nto alternative blame tracking strategies.  4.5 Extensions The development presented here focuses on \nthe particulars of the Blame Calculus, but our results extend to more developed lan\u00adguages. We brie.y \ndiscuss some examples next. Other Type Constructors In addition to function types, Hen\u00ad glein (1994) \nspeci.es coercions for arbitrary type constructors tc(T1, . . . , Tn). Each type constructor induces \nthree new coer\u00adcions: c1 : T11 . T12 . . . cn : Tn1 . Tn2 tc(c1, . . . , cn) : tc(T11, . . . , Tn1) . \ntc(T12, . . . , Tn2) tc(*, . . . , *)! : * . tc(*, . . . , *) tc(*, . . . , *)?l : tc(*, . . . , *) . \n* as well as two new coercion reduction rules: tc(*, . . . , *)?l . tc(*, . . . , *)! -. tc(.*, . . . \n, .*) tc(c11, . . . , c1n) . tc(c21, . . . , c2n) -. tc(c11 . c21, . . . , c1n . c2n) The tc type constructor \nis added to the set of dynables G, which means that rule (8) also accounts for cases like tc?l . B!. \nFinally, casts between tc and * are translated much like function types: ((tc(T1, . . . , Tn).*))l = \n((tc(T1, . . . , Tn).tc(*, . . . , *)))l . ((tc(*, . . . , *).*))l ((tc(T11, . . . , T1n).tc(T21, . . \n. , T2n)))l = tc(((T11 .T21))l , . . . , ((T1n .T2n))l) Props. 4 and 2 are easily extended to establish \ncon.uence and strong normalization for the resulting system, and just like Prop 10 shows for function \ncoercions, all type-constructor coercions com\u00adpose in a syntax-directed way. Proposition 16. tc(u11, \n. . . , u1n) 8 tc(u21, . . . , u2n) = tc(u11 8 u21, . . . , u1n 8 u2n). Using Prop. 16, we can derive \nsupercoercions and simplify them to threesomes. Finally, the corresponding lambda calculus would have \nbeen extended to describe how wrapped values are treated. For instance, to extend the lambda calculus \nwith pairs and pair coercions c \u00d7 c, we also add reduction rules for projecting coerced values. These \ntranslate directly to .(T ) rules: fst (c\u00a81 \u00d7 c\u00a82)v -. (c\u00a81)(fst v) snd (c\u00a81 \u00d7 c\u00a82)v -. (c\u00a82)(snd v) \nSubtyping The languages discussed here use subtyping rela\u00adtions to characterize which casts are safe, \nbut these relations are orthogonal to subtyping as a programming language feature. In fact, a programming \nlanguage with a priori support for subtyping can be easily extended to support dynamic types. To do so, \n* is added as above, and its only subtype is itself: * <: *. Then, is\u00adsues of subtyping and issues of \ncasting are kept orthogonal. For instance, in a language with record types, each record is treated as \nits own type constructor and coercions extend as in the last sec\u00adtion. Siek and Taha (2007) analyze the \ncombination of structural subtyping and dynamic types in more detail. Their Ob(\u00b7)language <: uses (space-inef.cient) \ncasts to combine structural subtyping and dynamic types in exactly this way. Alternative Blame Strategies \nIn this section, we systematically reconstructed an existing artifact. While interesting in its own right, \nthe biggest payoff is that we can use the same process to produce new artifacts. Next, we use this approach \nto produce three new threesome calculi for three alternate blame strategies. 5. Lazy D Threesomes The \nLazy D blame strategy retains the laziness of the Blame Calcu\u00adlus (i.e., Lazy UD), but distributes blame \nin a manner that s consis\u00adtent with the traditional notion of subtyping, treating * as the top of the \nsubtype hierarchy. In this section we develop threesomes that implement the Lazy D strategy. To get a \nLazy D threesome cal\u00adculus, simply replace the Lazy UD labeled types in .T with their Lazy D counterparts. \nThe Lazy D coercion calculus extends the Lazy UD calculus by adding primitive injections and projections \nat every type: G ::= B | T . T Under Lazy D, a cast between any arbitrary function type and * translates \nto a primitive coercion: ((*.T1 . T2))l = (T1 . T2)! ((T1 . T2 .*))l = (T1 . T2)?l Under Lazy UD, such \ncasts translated to complex coercions. In ad\u00addition, D-style blame labels do not need polarity: blame \nis ascribed to the particular cast that failed, so the identity of the blame label suf.ces. The reduction \nrules for Lazy D must address how complex injections interact with complex projections. This is handled \nby the following reduction rule: T1?l . T2! -. ((T1 .T2))l  This rule subsumes the Lazy UD rules 3, \n4, and 8 all of the Lazy UD rules that involve injections composed with projections. For Lazy D, however, \nthis rule must be stated in this general format because there are an unbounded number of concrete instances. \nTaking the above differences into account, we can de.ne a set of Lazy D supercoercions that denote normal \nLazy D coercion expressions. The main difference with the Lazy D calculus is that primitive injections \nG! and projections G?l are indexed by arbitrary types. One side effect of this is that normal arrow coercions \nlike (c1 . c2) . G?l must track the particular type G associated with the leading projection. This difference \nmanifests in the syntax of supercoercions as well as the associated meaning function N : N [ \u00a8c1 .?(T1.T2)l \nc\u00a82] = (N [ \u00a8c1] . N [ \u00a8c2]]) . (T1 . T2)?l N [ \u00a8c1 T1.T2 !. c\u00a82] = (T1 . T2)! . (N [ \u00a8c1] . N [ \u00a8c2]]) \nT1.T2 !.?(T3.T4)l N [ \u00a8c1 c\u00a82] = (T1 . T2)! . (N [ \u00a8c1] . N [ \u00a8c2] ) . (T3 . T4)?l The type indices on \narrow supercoercions correspond to leading projections and trailing injections between arrow types and \n*. Now that any arrow type can be immediately projected to or injected from *, we must keep track of \nwhich type is speci.cally intended. This affects how blame propagates among coercions. Using the same \ncalculational technique as in Sec 4.2, we de.ne Lazy D supercoercion composition. The directly calculated \ncom\u00adposition function for Lazy D is more complex than that for Lazy UD because of the intervening type \nprojections and injections that must be compiled and combined for arrow coercions. For example, (\u00a8c1 \n.?(T1.T2)l c\u00a82) (\u00a8c3 T3.T4 !. c\u00a84) = (\u00a8c3 ((T3 .T1))l c\u00a81) . (\u00a8c2 ((T2 .T4))l c\u00a84) Lazy D composition \nmust appeal to cast translation as part of its de.nition. Given a set of supercoercions and composition, \nwe apply the same simpli.cation techniques to arrive at a set of labeled types. Only a few details differ, \nprimarily with respect to eradicating primitive arrows, and merging the different forms of complex arrow \ncoercions. Here we focus on the key differences. As with Lazy UD (cf. Sec. 4.4.1), primitive arrow coercions \ncan be replaced with complex arrow coercions, but here they become more complex. We can deduce the structure \nof the correspondence by specializing the corresponding reduction rule: (T1 . T2)?l . (T1 . T2)! -. ((T1 \n.T1))l . ((T2 .T2))l Much like in the case of UD, the resulting coercion behaves like a primitive identity \ncoercion .T1.T2 and this observation motivates an analogous simpli.cation. We can de.ne a function I(T \n) that constructs identity coercions for arbitrary types: I(P ) = .P I(T1 . T2) = I(T1) . I(T2) Note \nthat I(T ) = ((T . T ))l for arbitrary label l. Given this, we can de.ne a simpli.cation. Proposition \n17. Let S1[ \u00a8c] be de.ned by S1[[(T1 . T2)!]] = I(T1) T1.T2 !. I(T2) S1[[(T1 . T2)?l] = I(T1) .?(T1.T2)l \nI(T2) S1[[(T1 . T2)!?l] = I(T1) T1.T2 !.?(T1.T2)l I(T2) S1[ \u00a8c] = \u00a8c otherwise. Then S1[ \u00a8c1 c\u00a82] = \nS1[ \u00a8c1] S1[ \u00a8c2] . As with Lazy UD, this change makes some supercoercions larger, but these forms often \narise at runtime as casts interact any\u00adway, so the size bene.t of the original form is short-lived. Further\u00admore, \ncasts retain the same asymptotic space bound at run-time, bounded by the size of the types that appear \nin the program. Thus, the resulting supercoercions are still space-ef.cient. Syntax dynables optional \nlabels projectors injectors labeled constructor labeled types wrappers failures G p H S Q \u00a8c \u00a8o \u00a8 f ::= \n::= ::= ::= ::= ::= ::= ::= B | T . T E | l E | Gl E | T . T Bp | \u00a8c S.H \u00a8c .* | FaillH | Q B ' | \u00a8c \nS. ' \u00a8c Faill \u00a8c \u00a8c = \u00a8c Composition .* \u00a8c = \u00a8c .* = \u00a8c Bp1 Bp2 = Bp2 S' 'H SH (\u00a8c1 . c\u00a82) (\u00a8c3 \n.c\u00a84) = (\u00a8c3 c\u00a81) .(\u00a8c2 c\u00a84) S.(T1.T2)l1 T3.T4.H (\u00a8c1 c\u00a82) (\u00a8c3 c\u00a84) = (\u00a8c3 ((T3 .T1))l1 c\u00a81) S.H \n(\u00a8c2 ((T2 .T4))l1 c\u00a84) Q1 Q2 = Faill[Q2] if Q1#Q2, [Q1] = Gl lH lH c\u00a8 Fail= Fail l' l[Q] Fail Q = \nFailFaill1G1l2 Q = Faill1[Q] if rG1l = rQl Faill1G1l2 Q = Faill2[Q] if rG1l = rQl [Q] = H Projector \nrQl = G Type Constructor rT l = G ' B = E ,rBp l = B l B= Bl SH rc1 .c2l = (* . *) , c1 S. ' c2 = E rBl \n= B , SGl c1 .c2 = Gl rT1 . T2l = (* . *) l ((* .*)) Cast Translation . . . l lT1.T2. ' l ((* .T1 . T2))= \n((T1 .T1))((T2 .T2)) l l' (T1.T2)l l ((T1 . T2 .*))= ((T1 .T1)).((T2 .T2)) l l'l ((T1 . T2 .T3 . T4))= \n((T3 .T1)). ((T2 .T4)) Figure 8. Lazy D Labeled Types Under Lazy D, merging arrows leads to two different \noptional labels: projectors H, which represent a possible projection from *, and injectors S, which represent \na possible injection to *. H is analogous to the optional label for Lazy UD failure coercions, and is \nused similarly in Lazy D. Once again, we merge the two Fail supercoercions as FaillH , and introduce \nLazy D analogues of [\u00b7] and I\u00b7l to unify the de.nition of composition. The projector function I\u00b7l is \nmore complex to account for projector indices on arrow coercions. Fig. 8 presents the corresponding Lazy \nD labeled types. When used to instantiate .T , they satisfy the equivalent of Prop. 15. To demonstrate \nthe difference between D and UD cast compi\u00adlation, consider the cast (Bool . Bool .*)l . It compiles \nto the Lazy D labeled type Bool .?(Bool.Bool)l Bool. Since Lazy UD is just Lazy D with restricted dynables, \nwe can interpret the Lazy UD counterpart of this cast as being equivalent to Bool .?(*.*)l Bool. These \ntwo labeled types propagate blame quite differently. The Lazy D supercoercions are more complex than \ntheir Lazy UD counterparts. This is a side-effect of Lazy D s larger space of coercions. In fact, if \nwe restrict G to only B and * . *, it is easy to check that the equations relate directly to those for \nLazy UD.  6. Eager Threesomes As discussed in Sec. 3, eager coercion calculi differ from lazy variants \nin that they detect and report some cast failures earlier. In particular, an arrow coercion c1 . c2 cannot \nbe in normal form and have any of its leaves be a Faill cast. Such failures percolate upwards. For instance, \nunder Lazy UD, Faill . .* was normal. That is no longer the case. To support eager checking, the grammar \nfor normal coercions must change. wrappers o ::= G! | m . m | G! . (m . m) failures f ::= Faill | Faill \n. (m . m) normal values m ::= .P | o | Faill . (m . m) | G?l | o . G?l | f . G?l normal coercions u ::= \nFaill | m To characterize the normal eager coercions, we differentiate be\u00adtween normal coercions at \nthe toplevel and normal values m, which can safely appear under an arrow. In particular, Faill by itself \ncan\u00adnot appear under an arrow. During reduction, de.nitive failures percolate upwards based on two new \nrules: ll ll (Fail. c) -. Fail(m . Fail) -. Fail To ensure that the inner failures of complex expressions \ntake prece\u00addence, arrow coercions can only be combined if the subcoercions are known to not be failures. \n(m1 . m2) . (m3 . m4) -. (m3 . m1) . (m2 . m4) Otherwise, a coercion like (Faill1 . c1) . (Faill2 . c2) \ncould reduce to either Faill2 or Faill1 , depending on reduction order. Furthermore, we restrict rule \n6 from the Lazy UD coercion calculus to ensure that a failing coercion never consumes an arrow coercion \nto its right: Faill . G! -. Faill Without this restriction, a coercion like Faill1 . (* . Int?l2 ) . \n(* . Bool!) could reduce to either Faill1 or Faill2 . Finally, to support eager blame tracking we introduce \nFaill . (m1 . m2) as an additional form of failure f. Since the coercion calculus can no longer reduce \nFaill . (m1 . m2), it is up to .(c) to detect it. As with the lazy variants, these calculi are strongly \nnormalizing, but in contrast con.uence does not hold. This means that the eager coercion calculi does \nnot independently specify a deterministic strategy for casts. However, the derivation of labeled types, \nin concert with the space-ef.cient evaluation semantics, amounts to imposing an order of evaluation on \neager coercions. Because of the new reduction rules, complex arrow coercions interact with composition \ndifferently. To capture this difference, we de.ne an operation : . that combines normal coercions according \nto whether either is a failure. m :. m = m . m Faill :. u = Faill m := Faill . Faill Armed with this, \nwe can succinctly characterize how eager arrow coercions compose. 6.1 Eager D and Eager UD Threesomes \nApplying our calculational approach to the eager calculi largely follows the same pattern as for the \nlazy calculi. As with the normal eager coercions, non-failure supercoercion values m\u00a8must be dif\u00adferentiated \nfrom the full set of supercoercions c\u00a8. Thus, the simple arrow supercoercions have the form m\u00a8. m\u00a8. To \nrepresent the no\u00adtion of a failure coercion followed by an arrow coercion, Eager D Syntax dynables G \n::= B | T . T optional labels p ::= E | l projectors H ::= E | Gl injectors S ::= E | T . T | l labeled \ntypes \u00a8c ::= .* | FaillH | Bp | \u00a8m S.H \u00a8m values \u00a8m ::= .* | Bp | \u00a8m S.H \u00a8m wrappers \u00a8o ::= B ' | \u00a8m \n' . ' \u00a8m | \u00a8m T .T. ' \u00a8m failures \u00a8 f ::= Faill | \u00a8m1 l. ' \u00a8m2 c\u00a8 c\u00a8= c\u00a8 Composition .* c\u00a8= c\u00a8 .* = \n\u00a8c Bp1 Bp2 = Bp2 , l1 p2 l1 Bp2 B B= Fail 2 if B1 = B2 1 2 T1.T2.l1. H H Bl1 (\u00a8\u00a8\u00a8\u00a8m1 m2) = m1 m2 SGl1 \nl1[Bp2 ] ( \u00a8.m2) Bp2m1 \u00a8= Fail lH lH c\u00a8 Fail= Fail l1.H H l1. c\u00a8 (\u00a8\u00a8\u00a8\u00a8m1 m2) = m1 m2 l l[Bp] Bp Fail= \nFail l1Bl2 Bp l1[Bp] Fail= Fail p l1B1l2 Bp l2[B Fail= Fail2 ] if B1 = B2 2 T1.T2.H l2.H Faill1Bl2 \n (\u00a8\u00a8\u00a8\u00a8m1 m2) = m1 m2 l'H lH Fail ( \u00a8.m2) = m1 \u00a8m1 \u00a8\u00a8.m2 l HlH T1.T2. Fail (\u00a8\u00a8\u00a8.m2m1 m2) = m1 \u00a8 S''H H \nS:S.(T1.T2)l T3.T4.H ( \u00a8. m2) ( \u00a8.m4) = ( \u00a8\u00a8.m2 m4)m1 \u00a8m3 \u00a8m3 m1) (\u00a8\u00a8 (\u00a8\u00a8m3 m\u00a84) =m1 m2) ( \u00a8( \u00a8\u00a8(( \n\u00a8) m4) .H m3 (((T3 .T1))l m1)) S:m2 ((T2 .T4))l \u00a8 S:H Bp = H Projector c\u00a8.c\u00a8 Arrow Constructor ' \n.H B = E m1 S :m2 \u00a8S.H \u00a8\u00a8\u00a8= m1 m2 , l lS:l . ' B= Bl Failc\u00a82 = Fail S :l l . ' \u00a8Fail= Fail m1 .(T1.T2)l2 \nl1 S: Failc\u00a82 = S:l1 I(T1) l1.(T1.T2)l2 I(T2) m\u00a81 .(T1.T2)l2 Fail= l ((* .*)) Cast Translation . . . \nl lT1.T2 :' l ((* .T1 . T2))= ((T1 .T1)). ((T2 .T2)) l l' :(T1.T2)l l ((T1 . T2 .*))= ((T1 .T1)).((T2 \n.T2)) l 'l l : ((T1 . T2 .T3 . T4))= ((T3 .T1)). ((T2 .T4)) Figure 9. Eager D Labeled Types extends the \nLazy D supercoercions with two failure arrow superco\u00adercions, whose interpretations are as follows: l \nF N [ \u00a8m1 . m2] =Faill . N [ \u00a8\u00a8 \u00a8m1 . m2] l1 F .?(T1.T2)l2 N [ \u00a8\u00a8= m1 . m2] .(T1 . T2)?l2 ) m1 m2] Faill1 \n.(N [ \u00a8\u00a8 Composition for Eager D supercoercions introduces a number of new equations that characterize \nfailure arrow supercoercions. These feed into the de.nition of Eager D labeled types. Simplifying Eager \nD supercoercions involves two twists com\u00adpared to Lazy D. First, Faill1(T1.T2)l2 coercions can be replaced \nl1 .?(T1.T2)l2 by failure arrows I(T1) FI(T2). Furthermore, the downcast label S on arrow types adds \nan extra blame label compo\u00adnent for representing failure arrows and several equations account The eager \nthreesome calculi in particular point to some signif-  Syntax icant bene.ts to our approach. Composing \neager labeled types no dynables G ::= B | * . * optional labels p ::= E | l longer corresponds to the \ngreatest lower bound of the two types, failure projectors H ::= E | Bl i.e., .&#38;(T1 . T2) = . does \nnot apply for eager threesomes. This labeled types \u00a8::= .* \u00a8p.p \u00a8suggests that the original approach \nof experimenting with labels on c | FaillH | Bp | m m values \u00a8::= .* | Bp | m m m \u00a8\u00a8 ' p.p threesomes \nwithout blame might have led to troubles. On the other wrappers o\u00a8::= B ' | c\u00a8. ' c\u00a8 \u00a8 l | l. ' hand, \neach of the three new threesome calculi were quicker to de\u00ad failures f ::= Fail\u00a8\u00a8 m1 m2 velop than the \nLazy UD reconstruction once it was completed. Composition\u00a8c \u00a8c = \u00a8c \u00a8 a fresh perspective on the original \nlabeled types and Threesome .* \u00a8c = \u00a8c .* Bp1 Bp2 p1.p2 \u00a8m2) ( \u00a8m3 ' .p3 \u00a8m4)( \u00a8m1 = = = c Bp2 ( \n\u00a8m3 \u00a8m1) p1.p3 ( \u00a8m2 \u00a8m4): Calculus. Labeled types were developed from scratch, starting from the concept \nof threesomes without blame, and adding blame labels Bl 1 Bp 2 Bl ( \u00a8m1 p1.p2 \u00a8m2) = = Faill[Bp 2 ] \nif B1 = B2 ( \u00a8m1 l.p2 \u00a8m2) later. We construct the same concept, but do so from a different starting \npoint, normal coercions, and proceed by stepwise re.ne\u00ad ( \u00a8m1 p1.l \u00a8m2) Bp2 = Faill[Bp2 ] ment, maintaining \ncorrectness at each step. This process reveals \u00a8c FaillH = FaillH that threesomes with blame can be \nviewed as a streamlined im\u00ad \u00a8c ( \u00a8m1 l.p \u00a8m2) Faill Bp Faill ( \u00a8m1 ' .p \u00a8m2) Faill1Bl2 Bp Faill1B1l2 \n Bp 2 Faill1Bl2 ( \u00a8m1 ' .p \u00a8m2) = = = = = = \u00a8m1 l.p \u00a8m2 Faill[Bp] \u00a8m1 l.p \u00a8m2 Faill1[Bp] Faill2[Bp 2 \n] if B1 = B2 \u00a8m1 l2.p \u00a8m2 plementation strategy for coercions, and exposes some of the rich structure \ncontained in the original system. If such languages are to have practical impact, they will need to be \nscaled beyond the basic lambda calculus to support more types (e.g., parametric polymorphism (Ahmed et \nal. 2011)). As another step toward full-.edged implementations, we will endeavor In addition to new \nthreesome-based semantics, the paper offers to extend our threesome calculi with these features, .rst \nextending p B= H Failure Projector p.p c\u00a8c\u00a8 : Arrow Constructor coercion calculi to support them. ' \np1 \u00a8 l p . p2.: : p1.p2 \u00a8\u00a8\u00a8 B = E m1 m2 = m1 m2 8. Acknowledgements , ' l \u00a8 Fail B= Bl c2 = 'l l \u00b4 \np Eric Tanter for motivating this work and enlightening discussions about this topic. We also thank Philip \nWe thank Jeremy Siek and . .:lp1: \u00a8 m1 FailFail= Fail l2 \u00a8 c2 = l1 l1.Wadler and anonymous reviewers \nfor their helpful comments. l2 Fail= .* l2 .* p . . . . References Cast Translation Amal Ahmed, Robert \nBruce Findler, Jeremy Siek, and Philip Wadler. Blame : l l'l . : \u00a8 m1 l ((*.*)) for all. In Proc. Symposium \non Principles of Programming Languages, ((* .T1 . T2))= ((T1 .*))((*.T2)) POPL 11, pages 201 214, New \nYork, NY, USA, 2011. ACM. : l lll . ((T1 . T2 .*))= ((* .T1))((T2 .*)) Robert Bruce Findler and Matthias \nFelleisen. Contracts for higher-order functions. In Int. Conf. on Functional Programming, October 2002. \nFritz Henglein. Dynamic typing: syntax and proof theory. Science of : l l'l . ((T1 . T2 .T3 . T4))= \n((T3 .T1))((T2 .T4)) Figure 10. Eager UD Labeled Types for failure arrows. Fig. 9 presents the Eager \nD labeled types. Com\u00ad pilation of casts for Eager D is the same as for Lazy D except that the arrow constructors \n: . must be used to percolate failures up\u00adwards. l.E Finally, we add c\u00a81 c\u00a82 as an additional failure \ncoercion f so that .T can detect it as a failure. This corresponds to the analogous addition to the coercion \ncalculus. Having computed Eager D labeled types, the Eager UD variant can be easily computed by restricting \nthe projectable types G to base types B and (* . *). This restriction removes the need for type annotations \non arrow coercions, leading to a much simpler form. Fig. 10 presents the Eager UD labeled types. Compilation \nof casts for Eager UD is the same as for Lazy UD except for the use of arrow constructors to propagate \nfailures. Note that Eager UD blame labels are polarized. 7. Discussion The primary contributions of this \npaper are novel threesome seman\u00adtics for coercion calculi, each of which supports desirable proper\u00adties, \nlike catching more errors sooner and a blame strategy based on traditional subtyping. It was previously \nunknown whether the la\u00adbeling protocol for threesomes could be adapted to capture D-style blame tracking \nor the Eager variant of UD. This paper answers that question af.rmatively. Computer Programming, 22(3):197 \n230, June 1994. David Herman, Aaron Tomb, and Cormac Flanagan. Space-ef.cient grad\u00adual typing. In Trends \nin Functional Prog., page XXVIII, April 2007. Jacob Matthews and Robert Bruce Findler. Operational semantics \nfor multi\u00ad language programs. ACM Trans. Program. Lang. Syst., 31(3):12:1 12:44, April 2009. Jeremy Siek. \nStrong normalization for coercion calculi. Unpublished Manuscript, 2011a. Jeremy Siek, 2011b. Private \nCorrespondence. Jeremy Siek and Walid Taha. Gradual typing for functional languages. In Proc. Scheme \nand Functional Programming Workshop, September 2006. Jeremy Siek and Walid Taha. Gradual typing for objects. \nIn Proc. European Conference on Object-Oriented Programming, ECOOP 07, pages 2 27, Berlin, 2007. Springer-Verlag. \nJeremy Siek and Philip Wadler. Threesomes, with and without blame. In Proc. Symposium on Principles of \nProgramming Languages, POPL 10, pages 365 376, New York, NY, USA, 2010. ACM. Jeremy Siek, Ronald Garcia, \nand Walid Taha. Exploring the design space of higher-order casts. In Proc. European Symposium on Programming \nLanguages, ESOP 09, pages 17 31, Berlin, 2009. Springer-Verlag. Sam Tobin-Hochstadt and Matthias Felleisen. \nInterlanguage migration: from scripts to programs. In Companion to Symposium on Object\u00adoriented Programming \nSystems, Languages, and Applications, OOPSLA 06, pages 964 974, New York, 2006. ACM. Philip Wadler and \nRobert Bruce Findler. Well-typed programs can t be blamed. In Proc. European Symposium on Programming \nLanguages, ESOP 09, pages 1 16, Berlin, 2009. Springer-Verlag.   \n\t\t\t", "proc_id": "2500365", "abstract": "<p>Coercions and threesomes both enable a language to combine static and dynamic types while avoiding cast-based space leaks. Coercion calculi elegantly specify space-efficient cast behavior, even when augmented with blame tracking, but implementing their semantics directly is difficult. Threesomes, on the other hand, have a straightforward recursive implementation, but endowing them with blame tracking is challenging. In this paper, we show that you can use that elegant spec to produce that straightforward implementation: we use the coercion calculus to derive threesomes with blame. In particular, we construct novel threesome calculi for blame tracking strategies that detect errors earlier, catch more errors, and reflect an intuitive conception of safe and unsafe casts based on traditional subtyping.</p>", "authors": [{"name": "Ronald Garcia", "author_profile_id": "83058886957", "affiliation": "University of British Columbia, Vancouver, BC, Canada", "person_id": "P4261292", "email_address": "rxg@cs.ubc.ca", "orcid_id": ""}], "doi_number": "10.1145/2500365.2500603", "year": "2013", "article_id": "2500603", "conference": "ICFP", "title": "Calculating threesomes, with blame", "url": "http://dl.acm.org/citation.cfm?id=2500603"}