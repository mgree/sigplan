{"article_publication_date": "09-25-2013", "fulltext": "\n Exploiting Vector Instructions with Generalized Stream Fusion Geoffrey Mainland Roman Leshchinskiy \nSimon Peyton Jones Microsoft Research Ltd rl@cse.unsw.edu.au Microsoft Research Ltd Cambridge, England \nCambridge, England gmainlan@microsoft.com Abstract Stream fusion [6] is a powerful technique for automatically \ntrans\u00adforming high-level sequence-processing functions into ef.cient im\u00adplementations. It has been used \nto great effect in Haskell libraries for manipulating byte arrays, Unicode text, and unboxed vectors. \nHowever, some operations, like vector append, still do not perform well within the standard stream fusion \nframework. Others, like SIMD computation using the SSE and AVX instructions available on modern x86 chips, \ndo not seem to .t in the framework at all. In this paper we introduce generalized stream fusion, which \nsolves these issues. The key insight is to bundle together mul\u00adtiple stream representations, each tuned \nfor a particular class of stream consumer. We also describe a stream representation suited for ef.cient \ncomputation with SSE instructions. Our ideas are im\u00adplemented in modi.ed versions of the GHC compiler \nand vector library. Benchmarks show that high-level Haskell code written using our compiler and libraries \ncan produce code that is faster than both compiler-and hand-vectorized C. Categories and Subject Descriptors \nD.3.3 [Software]: Program\u00adming Languages Keywords Haskell; vectorization; SIMD; stream fusion 1. Introduction \nIt seems unreasonable to ask a compiler to be able to turn numeric algorithms expressed as high-level \nHaskell code into tight machine code. The compiler must cope with boxed numeric types, handle lazy evaluation, \nand eliminate intermediate data structures. However, the Glasgow Haskell Compiler has become suf.ciently \nsmart that Haskell libraries for expressing numerical computations, such as Repa [17, 21], no longer \nhave to sacri.ce speed at the altar of abstraction. The key development that made this sacri.ce unnecessary \nis stream fusion [6]. Algorithms over sequences whether they are lists or vectors (arrays) are expressed \nnaturally in a functional language using operations such as folds, maps, and zips. Although highly modular, \nthese operations produce unnecessary intermediate structures that lead to inef.cient code. Eliminating \nthese interme\u00addiate structures is termed deforestation, or fusion. Equational laws, Permission to make \ndigital or hard copies of all or part of this work for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for pro.t or commercial advantage and that copies bear \nthis notice and the full citation on the .rst page. Copyrights for components of this work owned by others \nthan the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. Request \npermissions from permissions@acm.org. ICFP 13, September 25 27, 2013, Boston, MA, USA. Copyright is \nheld by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2326-0/13/09. . . $15.00. \nhttp://dx.doi.org/10.1145/2500365.2500601 simonpj@microsoft.com such as map f .map g = map (f .g), allow \nsome of these intermedi\u00adate structures to be eliminated; .nding more general rules has been the subject \nof a great deal of research [8, 12, 15, 23, 29, 30, 34]. Stream fusion [6], based on the observation \nthat recursive structures can be transformed into non-recursive co-structures for which fusion is relatively \nstraightforward, was the .rst truly general solution. Instead of working directly with lists or vectors, \nstream fusion works by re-expressing these functions as operations over streams, each represented as \na state and a step function that transforms the state while potentially yielding a single value. Alas, \ndifferent operations need different stream representations, and no single representation works well for \nall operations (\u00a72.3). Furthermore, for many operations it is not obvious what the choice of representation \nshould be. In this paper we solve this problem with a new generalized stream fusion framework where the \nprimary abstraction over which operations on vectors are expressed is a bundle of streams. The streams \nare chosen so that for any given high-level vector operation there is a stream in the bundle whose representation \nleads to an ef.cient implementation. The bundle abstraction has no run-time cost because standard compiler \noptimizations eliminate intermediate bundle structures. In addition, we describe several optimized stream \nrepresentations. Our contributions are as follows: We describe a generalization of stream fusion that \nbundles together multiple, alternate representations of a stream. This allows the stream consumer to \nchoose the representation that is most ef.cient for its use case (Section 3.1).  We show in Section \n3.2 how generalized stream fusion enables the use of bulk memory operations, such as memcpy and memset, \nfor manipulating vectors.  Generalized stream fusion opens up entirely new opportunities for optimization. \nWe exploit this opportunity in Section 3.3 by crafting a stream representation well-suited for SIMD-style \nvector operations (SSE, AVX etc). The result is a massive performance gain (up to a factor of more than \nthree) at a much lower programming cost than using conventional approaches (Section 5).  Using generalized \nstream fusion, we describe in Section 3.5 how to modify Data Parallel Haskell [4] so that DPH programs \ncan automatically take advantage of SIMD operations without requiring any code modi.cation.  Everything \nwe describe is fully implemented in GHC and its libraries (Section 4). Our benchmarks compare to the \nvery best C and C++ compilers and libraries that we could .nd. Remarkably, our benchmarks show that choosing \nthe proper stream representations can result in machine code that beats compiler-vectorized C, and that \nis competitive with hand-tuned assembly. Moreover, using DPH, programs can easily exploit SIMD instructions \nand automatically parallelize to take advantage of multiple cores.  2. Stream Fusion Background We begin \nby providing the background necessary for understanding stream fusion. There is no new material here \nit is all derived from Coutts et al. [6]. However, we describe fusion for functions of vectors of unboxed \nvalues, as implemented in the vector [19] library, rather than fusion for functions over lists. Some \nof the implementation details are elided, but the essential aspects of stream fusion as we describe them \nare faithful to the implementation. 2.1 The key insight The big idea behind stream fusion is to rewrite \nrecursive functions, which are dif.cult for a compiler to automatically optimize, as non\u00adrecursive functions. \nThe abstraction that accomplishes this is the Stream data type: data Stream a where Stream :: (s . Step \ns a) . s . Int . Stream a data Step s a = Yield a s | Skip s | Done A stream is a triple of values: \nan existentially-quanti.ed state, represented by the type variable s in the above de.nition, a size, \nand a step function that, when given a state, produces a Step. A Step may be Done, indicating that there \nare no more values in the Stream, it may Yield a value and a new state, or it may produce a new state \nbut Skip producing a value. The presence of Skip allows us to easily express functions like .lter within \nthe stream fusion framework. To see concretely how this helps us avoid recursive functions, let us write \nmap for vectors using streams map :: (a . b) . Vector a . Vector b map f = unstream . maps f . stream \nThe functions stream and unstream convert a Vector to and from a stream. A Vector is converted to a stream \nwhose state is an integer index and whose step function yields the value at the current index, which \nis incremented at each step. To convert a stream back into a Vector, unstream allocates memory for a \nnew vector and writes each element to the vector as it is yielded by the stream unstream embodies a recursive \nloop. Though imperative, the allocation and writing of the vector are safely embedded in pure Haskell \nusing the ST monad [18]. The real work is done by maps, which is happily non-recursive. maps :: (a . \nb) . Stream a . Stream b maps f (Stream step s) = Stream step' s where step' s = case step s of ' Yield \nx s' . Yield (f x) s Skip s' . Skip s' Done . Done With this de.nition, the equational rule mentioned \nin the Introduc\u00adtion, map f . map g = map (f . g), falls out automatically. To see this, let us .rst \ninline our new de.nition of map in the expression map f . map g. map f . map g = unstream . maps f . \nstream . unstream . maps g . stream Given this form, we can immediately spot where an intermedi\u00adate structure \nis formed by the composition stream . unstream. This composition is morally the identity function [7, \n\u00a73.7-3.8], so we should be able to eliminate it entirely. Rewrite rules [28] en\u00adable programmers to express \nalgebraic identities such as stream . unstream = id in a form that GHC can understand and automati\u00adcally \napply. Stream fusion relies critically on this ability, and the vector library includes exactly this \nrule. With the rule in place, GHC transforms our original composition of maps into map f . map g = unstream \n. maps f . maps g . stream Conceptually, stream fusion pushes all recursive loops into the .nal consumer. \nThe two composed invocations of map become a composition of two non-recursive calls to maps. The inliner \nis now perfectly capable of combining maps f . maps g into a single Stream function. Stream fusion gives \nus the equational rule map f . map g = map (f . g) for free.  2.2 Fusing the vector dot product The \nmotivating example we will use for the rest of the paper is the vector dot product. A high-level implementation \nof this function in Haskell might be written as follows: dotp :: Vector Double . Vector Double . Double \ndotp v w = sum (zipWith (*) v w) It seems that this implementation will suffer from severe inef.ciency \nthe call to zipWith produces an unnecessary inter\u00admediate vector that is immediately consumed by the \nfunction sum. In expressing dotp as a composition of collective operations, we have perhaps gained a \nbit of algorithmic clarity but in turn we have incurred a performance hit. We have already seen how stream \nfusion eliminates intermediate structures in the case of a composition of two calls to map. Previous \nfusion frameworks could handle that example, but were stymied by the presence of a zipWith. However, \nstream fusion has no problem fusing zipWith, which we can see by applying the transformations we saw \nin Section 2.1 to dotp. The .rst step is to re-express each Vector operation as the composition of a \nStream operation and appropriate conversions between Vectors and Streams at the boundaries. The functions \nzipWith and sum are expressed in this form as follows. zipWith :: (a . b . c) . Vector a . Vector b . \nVector c zipWith f v w = unstream (zipWiths f (stream v) (stream w)) sum :: Num a . Vector a . a sum \nv = foldl's 0 (+) (stream v) It is now relatively straightforward to transform dotp to eliminate the \nintermediate structure. dotp :: Vector Double . Vector Double . Double dotp v w = sum (zipWith (*) v \nw) = foldl's 0 (+) (stream (unstream (zipWiths (*) (stream v) (stream w)))) = foldl's 0 (+) (zipWiths \n(*) (stream v) (stream w)) This transformation again consists of inlining a few de.nitions, something \nthat GHC can easily perform, and rewriting the compo\u00adsition stream . unstream to the identity function. \nAfter this trans\u00adformation, the production (by zipWith) and following consumption (by sum) of an intermediate \nVector becomes the composition of non-recursive functions on streams. We can see how iteration is once \nagain pushed into the .nal consumer by looking at the implementations of foldl's and zipWiths. The .nal \nconsumer in dotp is foldl's, which is implemented by an explicit loop that consumes stream values and \ncombines the yielded values with the accumulator z using the function f (the call to seq guarantees that \nthe accumulator is strictly evaluated).  foldl ' s :: (a . b . a) . a . Stream b . a foldl ' s f z (Stream \nstep s) = loop z s where loop z s = z seq case step s of ' Yield x s ' . loop (f z x) s Skip s ' . \nloop z s ' Done . z However, in zipWiths there is no loop the two input streams are consumed until either \nboth streams yield a value, in which case a value is yielded on the output stream, or until one of the \ninput streams is done producing values. The internal state of the stream associated with zipWiths contains \nthe state of the two input streams and a one-item buffer for the value produced by the .rst input stream. \nzipWiths :: (a . b . c) . Stream a . Stream b . Stream c zipWiths f (Stream stepa sa na) (Stream stepb \nsb nb) = Stream step (sa, sb, Nothing) (min na nb) where step (sa,sb,Nothing) = case stepa sa of ' Yield \nx sa ' . Skip (sa , sb,Just x) ' Skip sa ' . Skip (sa , sb,Nothing) Done . Done step (sa,sb,Just x) = \ncase stepb sb of Yield y sb ' . Yield (f x y) (sa, sb ' ,Nothing) Skip sb ' . Skip (sa, sb ' ,Just x) \nDone . Done Given these de.nitions, call-pattern specialization [24] in concert with the standard GHC \ninliner suf.ce to transform dotp into a single loop that does not produce an intermediate structure, \nand the moral result is shown here, where !! is in.x byte array indexing. dotp (Vector n u) (Vector m \nv) = loop 0.0 0 0 where loop z i j | i < n . j < m = loop (z + u !! i * v !! j) (i + 1) (j + 1) | otherwise \n= z If there is any doubt that this results in ef.cient machine code, we give the actual assembly language \ninner loop output by GHC using the LLVM back end [31]. Stream fusion preserves the ability to write compositionally \nwithout sacri.cing performance. .LBB2_3: movsd (%rcx), %xmm0 mulsd (%rdx), %xmm0 addsd %xmm0, %xmm1 addq \n$8, %rcx addq $8, %rdx decq %rax jne .LBB2_3  2.3 Stream fusion inef.ciencies Unfortunately, while \nstream fusion does well for the examples we have shown, it still does not produce ef.cient implementations \nfor many other operations one might want to perform on vectors. We next give examples of two classes \nof operations that are either inef.\u00adcient or impossible to handle in the stream fusion framework. As \nwe describe in Section 3, generalized stream fusion can implement both of these classes of operations \nef.ciently. 2.3.1 Bulk memory operations Appending two vectors is a simple operation for which an obvious, \nef.cient implementation exists a vector large enough to hold the result is allocated, and the two vectors \nbeing appended are copied, one after the other, to the destination using an optimized memcpy. Similarly, \nreplicating a scalar across a vector should be implemented by a call to memset. In other words, we would \nlike to be able to take advantage of highly optimized bulk memory operations like memcpy and memset in \nour vector library. Unfortunately, this is far from straightforward. To see why, let us look at how we \nmight implement vector append using stream fusion. In this framework, vector append is .rst rewritten \nin terms of stream, unstream, and a worker function appends. append :: Vector a . Vector a . Vector a \nappend u v = unstream (appends (stream u) (stream v)) appends :: Stream a . Stream a . Stream a appends \n(Stream stepa sa na) (Stream stepb sb nb) = Stream step (Left sa) (na + nb) where step (Left sa) = case \nstepa sa of Yield x sa ' . Yield x (Left sa ' ) Skip sa ' . Skip (Left sa ' ) Done . Skip (Right sb) \nstep (Right sb) = case stepb sb of Yield x sb ' . Yield x (Right sb ' ) Skip sb ' . Skip (Right sb ' \n) Done . Done The function appends appends two streams by .rst yielding all the values produced by the \n.rst stream and then those values produced by the second stream. When passed to unstream, this results \nin two loops, one executed after the other, that write elements one by one into a newly allocated vector. \nSimilarly, replication compiles to a loop that writes, one by one, a constant to each element of a newly \nallocated vector. While it is theoretically possible for a compiler to transform this into code equivalent \nto memcpy and memset, this requires signi.cant low-level optimization capabilities and is currently beyond \nthe reach of GHC. How could we possibly rewrite appends into two calls to memcpy? We cannot. The dif.culty \nis that a Stream produces a single value at a time. To take advantage of memcpy, it seems we need a stream \nthat produces entire vectors, one at a time. However, this latter representation, though ideal for vector \nappend, would be a dismal failure if we wished to calculate a dot product!  2.3.2 SIMD computation with \nvectors The inadequacy of the single-value-at-a-time nature of streams be\u00adcomes even more apparent when \nattempting to opportunistically utilize the SIMD instructions available on many current architec\u00adtures, \ne.g., SSE on x86 and NEON on ARM. These instructions operate in parallel on data values that contains \ntwo (or four or eight, depending on the hardware architecture) .oating point numbers. To avoid notational \nconfusion, we call these multi-values, or sometimes just multis. To enable sum to use SIMD instructions, \nwe would like a stream representation that yields multi-values (rather than scalars), with perhaps a \nbit of scalar dribble at the end of the stream when the number of scalar values is not divisible by the \nsize of a multi. A stream of scalar values is useless for SIMD computation. However, a stream of multi-values \nisn t quite right either, because of the dribble problem. Perhaps we could get away with a stream  newMVector \n:: Int . ST s (MutableVector s a) sliceMVector :: MutableVector s a . Int . Int . MutableVector s a readMVector \n:: MutableVector s a . Int . ST s a writeMVector :: MutableVector s a . Int . a . ST s () copyMVector \n:: Vector a . MutableVector s a . ST s () freezeMVector :: MutableVector s a . ST s (Vector a) Figure \n1: Operations on mutable vectors. that yielded either a scalar or a multi at each step, but this would \nforce all scalar-only operations to handle an extra case, complicating the implementations of all operations \nand making them less ef.cient. There is a better way! 3. Generalized Stream Fusion We saw in the previous \nSection that different stream operations work best with different stream representations. In this Section, \nwe describe new stream representations that take advantage of bulk memory operations (\u00a73.2) and enable \nSIMD computation with vectors (\u00a73.3). Finally, we show how to use our framework to transparently take \nadvantage of SIMD instructions in Data Parallel Haskell programs (\u00a73.5). 3.1 Bundles of streams The idea \nunderlying generalized stream fusion is straightforward but its effects are wide-ranging: instead of \ntransforming a function over vectors into a function over streams, transform it into a function over \na bundle of streams. A bundle is simply a collection of streams, each semantically identical but with \na different cost model. Individual stream operations can therefore choose to work with the most advantageous \nstream representation in the bundle. We give a simpli.ed version of the Bundle data type here. data Bundle \na = Bundle {sSize :: Size , sElems :: Stream a , sChunks :: Stream (Chunk a) , sMultis :: Multis a } \nThe sElems .eld of the Bundle data type contains the familiar stream of scalar values that we saw in \nSection 2. In Section 3.3 we describe the representation contained in the sMultis .eld of the record, \nwhich enables the ef.cient use of SSE instructions. As we show next, the stream of Chunks contained in \nthe sChunks .eld of the record enables the use of bulk memory operations.  3.2 Taking advantage of bulk \nmemory operations with Chunks We observed in Section 2.3.1 that append could take advantage of memcpy \nif a stream could produce whole vectors rather than single scalar values in one step. The mechanism we \nuse is slightly more general: a Chunk is a computation that destructively initializes a vector slice \nof a particular length: data Chunk a = Chunk Int (.s.MutableVector s a . ST s ()) A MutableVector s a \nis the mutable cousin of a Vector a. The extra type variable s is the state token that allows us to safely \nembed imperative code in pure computations using the ST monad [18]. Some of the operations on mutable \nvectors are given in Figure 1. These include operations such as copyMVector which ultimately uses a variant \nof memcpy. A producer can use these to generate Chunks which initialize large parts of a vector in one \nstep: stream :: Vector a . Bundle a stream v = Bundle {...sChunks = singleton chunk... } where chunk \n= Chunk (length v) (copyMVector v) Here, stream produces only one Chunk which copies the entire vector \nin one ef.cient bulk operation. To convert a Bundle to a vector, we now simply allocate a single mutable \nvector of suf.cient size and initialize it by applying each Chunk to the appropriate slice: unstream \n:: Bundle a . Vector a unstream (Bundle {sChunks = Stream step s n }) = runST $ do mvec . newMVector \nn loop mvec 0 s where loop mvec i s = case step s of Yield (Chunk k .ll) s ' . do .ll (sliceMVector \nmvec i k) ' loop mvec (i + k) s Skip s ' . loop mvec i s ' Done . freezeMVector mvec This representation \nis ideal for both append and replicate. The former appends the sChunks components of the two bundles \nusing the version of appends from Section 2.3.1. When unstream uses the resulting stream of chunks to \nmanifest the vector, it automatically takes advantage of bulk memory operations. Similarly to stream, \nreplicate produces only one Chunk that ultimately invokes memset. Thus, if we append a vector produced \nby stream and a vector produced by replicate, we will perform one memcpy followed by one memset which \nis as ef.cient as possible. Though ideal for appending vectors ef.ciently, streams of chunks are not \na useful representation for operations like zipWith or fold. This is not a problem, however, since the \nstream-of-values represen\u00adtation is still available in the sElems component of the Bundle. In general, \neach consumer will choose the best representation for the particular operation. What happens when we \nappend two streams that resulted from separate maps operations? There is a natural conversion from a \nstream of scalar values to a stream of chunks each scalar value becomes a chunk that writes a single \nvalue to a mutable vector. The result of maps is therefore a Bundle that contains a degenerate stream \nof chunks in the sChunks .eld. There is not much we can do about this mapping a function over each element \nin the two streams precludes us from using memcpy to append them. Fortunately, the degenerate stream \nof chunks produced by maps does not in the end impose any overhead thanks to the optimizer. Furthermore, \nif we really are appending two vectors without .rst altering their contents, generalized stream fusion \ndoes not force us to give up an implementation in terms of ef.cient memory copies in order to gain fusion \nfor other operations, like map. While a consumer chooses one representation, a producer outputs a bundle \ncontaining all representations, though some may be degenerate. The degenerate form a stream of scalar \nvalues results in no worse an implementation that non-generalized stream fusion. In other words, generalized \nstream fusion always produces code that is at least as good as that produced by stream fusion. It might \nseem that maintaining a bundle of streams risks work duplication since the result is computed multiple \ntimes. Crucially, the stream representations are functions which only do work when applied by a consumer \nsuch as unstream and just like unstream, all consumers pick exactly one representation and discard all \nothers so no work is ever duplicated. Standard optimizations are typically able to resolve this at compile \ntime so that no code is duplicated,  class MultiType a where data Multi a --Associated type --The number \nof elements of type a in a Multi a. multiplicity :: Multi a . Int --A Multi a containing the values 0, \n1, ..., --multiplicity -1. multienum :: Multi a --Replicate a scalar across a Multi a. multireplicate \n:: a . Multi a --Map a function over the elements of a Multi a. multimap :: (a . a) . Multi a . Multi \na --Fold a function over the elements of a Multi a. multifold :: (b . a . b) . b . Multi a . b --Zip \ntwo Multi a s with a function. multizipWith :: (a . a . a) . Multi a . Multi a . Multi a Figure 2: The \nMultiType type class and its associated type, Multi. either. The scheme does not rely on lazy evaluation \nand would work .ne in a call-by-value language.  3.3 A stream representation .t for SIMD computation \nModifying the stream fusion framework to accommodate SIMD op\u00aderations requires a more thoughtful choice \nof representation. How\u00adever, proper SIMD support opens up the possibility of dramatically increased performance \nfor a wide range of numerical algorithms. We focus on SIMD computations using 128-bit wide vectors and \nSSE instructions on x86 since that is what our current implementation supports, although the approach \ngeneralizes. Our implementation represents SIMD values using the type family Multi. We have chosen the \nname to avoid confusion with the Vector type which represents arrays of arbitrary extent. In contrast, \na value of type Multi a is a short vector containing a .xed number of elements known as its multiplicity \nof type a. On a given platform, Multi a has a multiplicity that is appropriate for the platform s SIMD \ninstructions. For example, on x86, a Multi Double will have multiplicity 2 since SSE instructions operate \non 128-bit wide vectors, whereas a Multi Float will have multiplicity 4. Multi is implemented as an associated \ntype [3] in the MultiType type class; their de.nitions are shown in Figure 2. MultiType includes various \noperations over Multi values, such as replicating a scalar across a Multi and folding a function over \nthe scalar elements of a Multi. These operations are de.ned in terms of new primitives we added to GHC \nthat compile directly to SSE instructions. Given a value of type Vector Double, how can we operate on \nit ef.ciently using SSE instructions within the generalized stream fusion framework? An obvious .rst \nattempt is to include a stream of Multi Doubles in the stream bundle. However, this representation is \ninsuf.cient for a vector with an odd number of elements since we will have one Double not belonging to \na Multi at the end. Let us instead try this instead: a stream that can contain either a scalar or a Multi. \nWe call this stream a MultisP because the producer chooses what will be yielded at each step. data Either \na b = Left a | Right b type MultisP a = Stream (Either a (Multi a)) Now we can implement summation using \nSIMD operations. Our strategy is to use two accumulating parameters, one for the sum of the Multi values \nyielded by the stream and one for the sum of the scalar values. Note that (+) is overloaded: we use SIMD \n(+) to add summ and y, and scalar (+) to add sum1 and x. msumPs :: (Num a, Num (Multi a)) . MultisP a \n. a msumPs (Stream step s ) = loop 0.0 0.0 s where loop summ sum1 s = case step s of ' Yield (Left x) \ns ' . loop summ (sum1 + x) s Yield (Right y) s ' . loop (summ + y) sum1 s ' Skip s ' . loop summ sum1 \ns ' Done . multifold (+) sum1 summ When the stream is done yielding values, we call the multifold member \nof the MultiType type class to fold the addition operator over the components of the Multi. This implementation \nstrategy works nicely for folds. However, if we try to implement the SIMD equivalent of zipWiths, we \nhit a roadblock. A SIMD version of zipWiths requires that at each step either both of its input streams \nyield a Multi or they both yield a scalar if one were to yield a scalar while the other yielded a Multi, \nwe would have to somehow buffer the components of the Multi. And if one stream yielded only scalars while \nthe other yielded only Multis, we would be hard-pressed to cope. Instead of a stream representation where \nthe producer chooses what is yielded, let us instead choose a representation where the stream consumer \nis in control. data MultisC a where MultisC :: (s . Step s (Multi a)) . (s . Step s a) . s . MultisC \na  The idea is for a MultisC a to be able to yield either a value of type Multi a or a value of type \na the stream consumer chooses which by calling one of the two step functions. Note that the existential \nstate is quanti.ed over both step functions, meaning that the same state can be used to yield either \na single scalar or a Multi. If there is not a full Multi available, the .rst step function will return \nDone. The remaining scalars will then be yielded by the second step function. This representation allows \nus to implement a SIMD version of zipWiths (not shown here) and to slightly improve summation: msumCs \n:: (Num a,Num (Multi a)) . MultisC a . a msumCs (Stream mstep sstep s ) = mloop 0.0 s where mloop summ \ns = case mstep s of Yield x s ' . mloop (summ + x) (Left s ' ) Skip s ' . mloop summ (Left s ' ) Done \n. sloop summ 0.0 s sloop summ sum1 s = case sstep s of ' Yield x s ' . sloop summ (sum1 + x) s ' Skip \ns ' . sloop summ sum1 s Done . multifold (+) sum1 summ Regrettably, a MultisC still isn t quite what \nwe need. Consider appending two vectors of Doubles, each of which contains 41 elements. We cannot assume \nthat the two vectors are laid out consecutively in memory, so even though the stream that results from \nappending them together will contain 82 scalars, this stream is forced to yield a scalar in the middle \nof the stream. One might imagine an implementation that buffers and shifts partial Multi values, but \nthis leads to very inef.cient code. The alternative is for appends to produce a stream in which either \na scalar or a Multi is yielded at each step but that was the original representation we selected and \nthen discarded because it was not suitable for zips!  The .nal compromise is to allow either but not \nboth of these two representations. We cannot allow both hence there is only one new bundle member rather \nthan two because while we can easily convert a MultisC a into a MultisP a, the other direction is not \nef.ciently implementable. The .nal de.nition of the Multis type alias is therefore type Multis a = Either \n(MultisC a) (MultisP a) Each stream function that can operate on Multi values consumes the Multis a in \nthe sMultis .eld of the stream bundle. It must be prepared to accept either a MultisC or a mixed stream \nof scalars and Multi s, as this .nal de.nition of msums shows: msums :: (Num a, Num (Multi a)) . Bundle \na . a msums (Bundle {sMultis = Left s}) = msumCs s msums (Bundle {sMultis = Right t }) = msumPs t However, \nwe always try to produce a MultisC and only fall back to a MultisP as a last resort. Even operations \nthat can work with either representation are often worth specializing for the MultisC form. In the case \nof msums above, this allows us to gobble up as many Multi values as possible and only then switch to \nconsuming scalars, thereby cutting the number of accumulating parameters in half and reducing register \npressure. One could imagine attempting a representation that somehow guarantees longer runs of Multis, \nbut this would add complexity and we doubt it would have any advantage over the MultisC rep\u00adresentation, \nwhich has a distinct phase shift between operations on Multi and operations on scalars. For operations \nlike zip that operate on multiple streams, we would need to guarantee that both streams have the same \nstructure it simply does not do to have one stream in the pair yield a scalar while the other yields \na Multi. The MultiC/MultiP distinction neatly captures this requirement by framing it in terms of who \nhas control over what is yielded next, consumers or producers.  3.4 A SIMD version of dotp With a stream \nrepresentation for SIMD computation in hand, we can write a SIMD-ized version of the dot product from \nSection 2. dotp_simd :: Vector Double . Vector Double . Double dotp_simd v w = msum (mzipWith (*) v w) \nThe only difference with respect to the scalar implementation in Section 2.2 is that we use variants \nof foldl ' and zipWith specialized to take function arguments that operate on values that are members \nof the Num type class. While we could have used versions of these functions that take two function arguments \n(our library contains both varietals), one for scalars and one for Multis, the forms that use overloading \nto allow the function argument to be used at both the type a . a . a and Multi a . Multi a . Multi a \nare a convenient shorthand. mfold ' :: (PackedVector Vector a, Num a,Num (Multi a)) . (.b.Num b . b . \nb . b) . a . Vector a . a mzipWith :: (PackedVector Vector a,Num a,Num (Multi a)) . (.b.Num b . b . \nb . b) . Vector a . Vector a . Vector a msum :: (PackedVector Vector a,Num a,Num (Multi a)) . Vector \na . a msum = mfold ' (+) 0  The particular fold we use here, mfold ', maintains two accu\u00admulators (a \nscalar and a Multi) when given a MultisP a, and one accumulator when given a MultisC a. The initial value \nof the scalar accumulator is the third argument to mfold ', and the initial value of the Multi accumulator \nis formed by replicating this scalar argument across a Multi. The result of the fold is computed by combining \nthe elements of the Multi accumulator and the scalar accumulator using the function multifold, just as \nour implementation of msums. Note that the .rst argument to mfold ' must be associative and commuta\u00adtive. \nThe PackedVector type class constraint on mfold ' , mzipWith, and msum ensures that the type a is an \ninstance of MultiType and that elements contained in the vector can be extracted a Multi a at a time. \nThe stream version of mfold ' , mfold ' s, can generate ef.cient code no matter what representation is \ncontained in a Multis a. On the other hand, the stream version of mzipWith, mzipWiths, requires that \nboth its vector arguments have a MultisC representation. Since there is no good way to zip two streams \nwhen one yields a scalar and the other a Multi, if either bundle argument to mzipWiths does not have \na MultisC representation available, mzipWiths falls back to an implementation that uses only scalar operations. \n 3.5 Automatically parallelizing SIMD computations Using SIMD instructions does not come entirely for \nfree. Consider mapping over a vector represented using multis: mmap :: (PackedVector Vector a) . (a . \na) . (Multi a . Multi a) . Vector a . Vector a  To map ef.ciently over the vector it does not suf.ce \nto pass a function of type (a . a), because that does not work over multis. We must also pass a semantically \nequivalent multi-version of the function. For simple arithmetic, matters are not too bad: foo :: Vector \nFloat . Vector Float foo v = mmap (. x y . x + y * 2) (. x y . x + y * 2) v The two lambdas are at different \ntypes, but Haskell s overloading takes care of that. We could attempt to abstract this pattern like this: \nmmap :: (PackedVector Vector a) . (.a.Num a . a . a) . Vector a . Vector a  But that attempt fails \nif you want operations in class Floating, say, rather than Num. What we want is a way to automatically \nmulti-ize scalar functions (such as (. x y . x + y * 2) above), so that we get a pair of a scalar function \nand a multi function, which in turn can be passed to map. There is another problem: mmap only takes a \nfunction of type (a . a) which is less general (and hence less useful) than usual. The reason for this \nis the limited range of functions that the hardware offers over Multis, which is re.ected in the type \nof multimap in the MultiType class (Figure 2). Happily, both problems are already solved by the vectorization \ntransformation of Data Parallel Haskell [4, 25], itself based on the pioneering NESL [1]. Given a function \nf over scalars, DPH produces ' a function f which operates on vectors of scalars and is equivalent to \nmap f, but uses only primitive collective operations rather than iteration. Of course, a Multi is a special \ncase of a vector and MultiType includes enough functionality to provide the primitive collective operations \nthat DPH needs which means we can use it to produce SIMD code. Better still, DPH allows us to take advantage \nof multiple cores, as well as the SIMD instruction in each core. There are many moving parts here, but \nin principle we can get the convenience of scalar vector code, and automatically exploit both SIMD instructions \nand multi-core.  We start with an advantage: DPH is already built on the stream abstraction provided \nby the vector library. We modi.ed the DPH libraries to use our bundle abstraction instead. Because DPH \nprograms are vectorized by the compiler so that all scalar operations are turned into operations over \nwide vectors, by implementing these wide vector operations using our new SIMD functions like msum, programs \nwritten using DPH automatically and transparently take advantage of SSE instructions no code changes \nare required of the programmer.  3.6 How general is generalized stream fusion? We do not mean to suggest \nthat the representations we have chosen for our Bundle data type are complete in any sense except that \nthey allows us to take advantage of bulk memory operations and SIMD instructions, which was our original \ngoal. Generalized stream fusion is not general because we have .nally hit upon the full set of representations \none could possibly ever need, but because the frameworks we have put forth admit multiple new, specialized \nrepresentations. The key features of generalized stream fusion are 1) the ability to add new specialized \nstream representations, notably without requiring the library writer to rewrite the entire library; 2) \nleveraging the compiler to statically eliminate all intermediate Bundle structures and leave behind the \nsingle representation that is actually necessary to perform the desired computation; and 3) not requiring \nthe end user to know about the details of Bundles, or even that they exist. Generalized stream fusion \nprovides a representation and alge\u00adbraic laws for rewriting operations over this representation whose \nusefulness extends beyond Haskell. Although we have implemented generalized stream fusion as a library, \nit could also be incorporated into a compiler as an intermediate language. This was not necessary in \nour implementation because GHC s generic optimizer is powerful enough to eliminate all intermediate structures \ncreated by gener\u00adalized stream fusion. In other words, GHC is such a good partial evaluator that we were \nable to build generalized stream fusion as a library rather than incorporating it into the compiler itself. \nWriting high-level code without paying an abstraction tax is desirable in any language, and compilers \nother than GHC could also avoid this tax by using the ideas we outline in this paper, although perhaps \nonly by paying a substantial one-time implementation cost. 4. Implementation There are three substantial \ncomponents of our implementation. We .rst modi.ed GHC itself to add support for SSE instructions. This \nrequired changing GHC s register allocator to allow overlapping register classes. Previously, single-and \ndouble-precision registers could only be drawn from disjoint sets of registers even though on many platforms, \nincluding x86 (when using SSE) and x86-64, there is a single register class for both single-and double-precision \n.oating point values. This change was also necessary to allow SSE vectors to be stored in registers. \nWe then added support for primitive SIMD vector types and primitive operations over these types to GHC \ns dialect of Haskell. These primitives are fully unboxed [26]. The STG [22] and C--[27] intermediate \nlanguages, as well as the LLVM code generator [31], were also extended to support compiling the new Haskell \nSIMD primitives. Boxed wrappers for the unboxed primitives and the MultiType type class and its associated \nMulti type complete the high-level support for working directly with basic SIMD data types. Because the \nSIMD support we added to GHC utilizes the LLVM back-end, it should be relatively straightforward to adapt \nour modi.cations for other CPU architectures, although at this time only x86-64 is supported. Second, \nwe implemented generalized stream fusion in a modi.ed version of the vector library [19] for computing \nwith ef.cient unboxed vectors in Haskell. We replaced the existing stream d o u b l e c d d o t p ( d \no u b l e * u , d o u b l e * v , i n t n ) { d o u b l e s = 0 . 0 ; i n t i ; f o r ( i = 0 ; i < n \n; + + i ) s + = u [ i ] * v [ i ] ; r e t u r n s ; } Figure 3: C implementation of vector dot product \nusing only scalar operations. fusion implementation with an implementation that uses the Bundle representation \nand extended the existing API with functions such as mfold ' and mzipWith that enable using SIMD operations \non the contents of vectors. The examples in this paper are somewhat simpli.ed from the actual implementations. \nFor example, the actual implementations are written in monadic form and involve type class constraints \nthat we have elided. Vectors whose scalar elements can be accessed in SIMD-sized groups, i.e., vectors \nwhose scalar elements are laid out consecutively in memory, are actually represented using a PackedVector \ntype class. These details do not affect the essential design choices we have described, and the functions \nused in all examples are simply type-specialized instances of the true implementations. Third, we modi.ed \nthe DPH libraries to take advantage of our new vector library. The DPH libraries are built on top of \nthe stream representation from a previous version of the vector library, so we .rst updated DPH to use \nour bundle representation instead. We next re-implemented the primitive wide-vector operations in DPH \nin terms of our new SIMD operations on bundles. While we only provided SIMD implementation for operations \non double-precision .oating point values, this part of the implementation was quite small, consisting \nof approximately 20 lines of code not counting #ifdefs. Further extending SIMD support in DPH will be \neasy now that it is based on bundles rather than streams. Our register allocation patch and our SIMD \npatches supporting SSE instructions have already been accepted into mainline GHC. Our modi.cations to \nthe vector and DPH libraries are available in a public git repository. We expect these library modi.cations \nto also be adopted by their respective maintainers. Our branch of GHC also supports AVX instructions. \nWe are working with all maintainers to ensure that the work we report on in this paper will be available \nin the GHC 7.8 release along with our support for AVX instructions. In the evaluation that follows, we \nfocus on SSE instructions because the libraries we use for comparison do not yet support AVX. 5. Evaluation \nOur original goal in modifying GHC and the vector library was to make ef.cient use of SSE instructions \nfrom high-level Haskell code. The inability to use SSE operations from Haskell and its impact on performance \nis a de.ciency that was brought to our attention by Lippmeier and Keller [20]. The .rst step we took \nwas to write a small number of simple C functions utilizing SSE intrinsics to serve as benchmarks. This \ngave us a very concrete goal to generate machine code from Haskell that was competitive with these C \nimplementations. For comparison, we also implemented a C version of ddotp using only scalar operations, \nshown in Figure 3. The C implemen\u00adtation we use as a benchmark for double-precision dot product, to which \nwe have added support for SSE by hand, is given in Figure 4. We repeat the de.nition of the Haskell implementation \nhere.  # i n c l u d e < x m m i n t r i n . h > # d e f i n e V E C T O R _S I Z E 2 t y p e d e f \nd o u b l e v 2 s d __a t t r i b u t e __ (( v e c t o r _s i z e ( s i z e o f (d o u b l e ) * V E \nC T O R _S I Z E ) ) ) ; u n i o n d 2 v { v 2 s d v ; d o u b l e d [ V E C T O R _S I Z E ] ; } ; d \no u b l e d d o t p ( d o u b l e * u , d o u b l e * v , i n t n ) { u n i o n d 2 v d 2 s = { 0 . 0 \n, 0 . 0 } ; d o u b l e s ; i n t i ; i n t m = n &#38; ( ~ V E C T O R _S I Z E ) ; f o r ( i = 0 ; \ni < m ; i + = V E C T O R _S I Z E ) d 2 s . v += ( * ( ( v 2 s d * ) ( u + i ) ) ) * ( * ( ( v 2 s d \n* ) ( v + i ) ) ) ; s = d 2 s . d [ 0 ] + d 2 s . d [ 1 ] ; f o r (; i < n ; + + i ) s + = u [ i ] * \nv [ i ]; r e t u r n s ; } Figure 4: C implementation of vector dot product using SSE intrinsics. ddotp \n:: Vector Double . Vector Double . Double ddotp v w = mfold ' (+) 0 (mzipWith (*) v w) Though not exactly \nonerous, the C version with SSE support is already unpleasantly more complex than the scalar version. \nAnd surely the Haskell version, consisting of a single line of code (not including the optional type \nsignature) is a good bit simpler. Also note that the Haskell programmer can think compositionally it \nis natural to think of dot product as pair-wise multiplication followed by summation. The C programmer, \non the other hand, must manually fuse the two loops into a single multiply-add. Furthermore, as well \nas being constructed compositionally, the Haskell implementation can itself be used compositionally. \nThat is, if the input vectors to ddotp are themselves the results of vector computations, generalized \nstream fusion will potentially fuse all operations in the chain into a single loop. In contrast, the \nC programmer must manifest the input to the C implementation of ddotp as concrete vectors in memory there \nis no potential for automatic fusion with other operations. Figure 5 compares the single-threaded performance \nof several implementations of the dot product, including C and Haskell ver\u00adsions that only use scalar \noperations as well as the implementation provided by GotoBLAS2 1.13 [9, 10]. Times were measured on a \n3.40GHz Intel i7-2600K processor, averaged over 100 runs. To make the relative performance of the various \nimplementations clearer, we show the execution time of each implementation relative to the scalar C version \n(from Figure 4), which is normalized to 1.0, in Figure 6. Surprisingly, both the naive scalar C implementation \n(Figure 3) and the version written using SSE intrinsics (Figure 4) perform approximately the same. This \nis because GCC in fact vectorizes the scalar implementation. However, the Haskell implementation is almost \nalways faster than both C versions; it is 5-20% slower for very short vectors (those with fewer than \nabout 16 elements) and 1- Figure 5: Single-threaded performance of double-precision dot product implementations. \nC implementations were compiled us\u00ading GCC 4.8.1 and compiler options -O3 -msse4.2 -ffast-math -ftree-vectorize \n-funroll-loops. Sizes of the L1, L2, and L3 caches are marked. Figure 6: Relative performance of single-threaded \nddot implemen\u00adtations. All times are normalized relative to the hand-written, compiler\u00advectorized, C \nimplementation. 2% slower just when the working set size exceeds the capacity of the L1 cache. Not only \ndoes Haskell outperform C on this benchmark, but it outperforms GCC s vectorizer. Once the working set \nno longer .ts in L3 cache, the Haskell implementation is even neck\u00adand-neck with the implementation of \nddotp from GotoBLAS, a collection of highly-tuned BLAS routines hand-written in assembly language that \nis generally considered to be one of the fastest BLAS implementation available. Unfortunately, at the \ntime of publication we do not yet have a license for Intel s compiler. However, we note that anecdotally \nit performs a much better job vectorizing loops than GCC, especially for the simple loops shown here. \nWe view this as future challenge for Haskell.  5.1 Prefetching and loop unrolling Why is Haskell so \nfast? Because we have exploited the high-level stream-fusion framework to embody two additional optimizations: \nloop unrolling and prefetching. The generalized stream fusion framework allowed us to imple\u00adment the \nequivalent of loop unrolling by adding under 200 lines of code the to vector library. We changed the \nMultisC data type to incorporate a leap, which is a Step that contains multiple values of type Multi \na. We chose Leap to contain four values so loops are unrolled four times since on x86-64 processors this \ntends not to put too much register pressure on the register allocator. Adding multiple Leaps of different \nsizes would also be possible. MultisC consumers may of course chose not to use the Leap stepping function, \nin which case loops will not be unrolled. data Leap a = Leap a a a a data MultisC a where MultisC :: \n(s . Step s (Leap (Multi a))) . (s . Step s (Multi a)) . (s . Step s a) . s . MultisC a  Prefetch \ninstructions on Intel processors allow the program to give the CPU a hint about memory access patterns, \ntelling it to prefetch memory that the program plans to use in the future. In our library, these prefetch \nhints are implemented using prefetch primitives that we added to GHC. When converting a Vector to a MultisC, \nwe know exactly what memory access pattern will be used each element of the vector will be accessed in \nlinear order. The function that performs this conversion, stream, takes advantage of this knowledge by \nexecuting prefetch instructions as it yields each Leap. Only consumers using Leaps will compile to loops \ncontaining prefetch instructions, and stream will only add prefetch instructions for vectors whose size \nis above a .xed threshold (currently 8192 elements), because for shorter vectors the extra instruction \ndispatch overhead is not amortized by the increase in memory throughput. Loop unrolling and prefetching \nproduce an inner loop for our Haskell implementation of ddotp that is shown in Figure 7. Not only can \nthe client of our modi.ed vector library write programs in terms of boxed values and directly compose \nvector operations instead of manually fusing operations without paying an abstraction penalty, but he \nor she can transparently bene.t from low-level prefetch magic baked into the library. Of course the same \nprefetch magic could be expressed manually in the C version. However, the author who wrote the code in \nFigure 4 did not know about prefetch at the time of implementation. We suspect that many C programmers \nare in the same state of ignorance. In Haskell, this knowledge is embedded in a library, and clients \nbene.t from it automatically.  5.2 Speeding up Haskell To see how easy it is to integrate SIMD instruction \ninto existing programs, we rewrote a number of functions from various packages using our modi.ed vector \nlibrary. In all cases, we simply identi.ed suitable operations (typically maps and folds) and replaced \nthem by multi-enabled versions. We did not modify or refactor the algo\u00adrithms. The speedup of the rewritten \nversions is shown in Figure 8. The sum , kahan , dotp , saxpy , and rbf benchmarks are short programs \nthat make heavy use of numerics ( kahan implements Kahan summation [16]). One benchmark, variance , is \nadapted from the statistics [2] Haskell package. These .rst six bench\u00admarks were run using vectors containing \n216 Doubles. The mixture benchmark is adapted from the StatisticalMethods [5] Haskell package and implements \nthe expectation-maximization (EM) al\u00ad .LBB4_12: prefetcht0 1600(%rsi,%rdx) movupd 64(%rsi,%rdx), %xmm3 \nprefetcht0 1600(%rdi,%rdx) movupd 80(%rsi,%rdx), %xmm0 movupd 80(%rdi,%rdx), %xmm2 mulpd %xmm0, %xmm2 \nmovupd 64(%rdi,%rdx), %xmm0 mulpd %xmm3, %xmm0 addpd %xmm1, %xmm0 addpd %xmm2, %xmm0 movupd 96(%rsi,%rdx), \n%xmm3 movupd 96(%rdi,%rdx), %xmm1 movupd 112(%rsi,%rdx), %xmm4 movupd 112(%rdi,%rdx), %xmm2 mulpd %xmm4, \n%xmm2 mulpd %xmm3, %xmm1 addq $64, %rdx leaq 8(%rax), %rcx addq $16, %rax addpd %xmm0, %xmm1 cmpq %r9, \n%rax addpd %xmm2, %xmm1 movq %rcx, %rax jle .LBB4_12 Figure 7: Inner loop of Haskell ddotp function. \n Figure 8: Speedup of benchmarks from using modi.ed vector. gorithm for a mixture of two Gaussians [13, \n\u00a78.5.1]. The .nal benchmark, quickhull , is adapated from the examples included with DPH. The .rst six \nbenchmarks consist almost entirely of numeric operations for which SSE version are available; correspondingly, \nthey show a signi.cant speedup, from more than 1.5\u00d7 to more than 3\u00d7. The .nal two functions contain features \nthat frustrated our efforts, such as use of non-numeric operations or data-dependent control .ow, which \nis dif.cult to vectorize. One benchmark, mixture , has a run time that is exponential in the size of \nits input, so we could only run it on very small data sets. Nevertheless, we were able to gain at least \na minimum of a few percentage points even on these dif.cult-to-vectorize benchmarks just by picking some \nlow\u00adhanging fruit. Notably, we were unable to improve the run time of  Figure 9: Performance of Haskell \nand C Gaussian RBF implementa\u00adtions.. benchmarks that made heavy use of operations like exponentiation \nfor which we do not have vectorized versions. Using Intel s Math Kernel Library (MKL), which provides \nsuch primitives, would enable further speedups. Nonetheless, we expect that many Haskell programs that \nmake some use of numerical operations on sequences will have at least a few opportunities for vectorization. \nRewriting a small amount of code for a 5% performance improvement is worthwhile; rewriting a small amount \nof code for a 3\u00d7 performance improvement is an unbelievably good deal.  5.3 Abstraction without cost \nUsing Haskell for implementing kernels such as dotp provides us with another signi.cant advantage: these \nkernels can fuse with other vector operations! Fusion is not possible with BLAS routines but can dramatically \nincrease performance. Consider the Gaussian radial basis function [13], de.ned as -.lx-yl2 K(x,y) = e \nA C programmer implementing this function could take advan\u00adtage of high-performance BLAS routines in \ntwo possible ways. The .rst is to write x - y to a temporary vector and then call BLAS. Of course this \nrequires an intermediate structure, which is undesirable. The second possibility is to apply the identity \nlx - yl2 = x \u00b7 x - 2x \u00b7 y + y \u00b7 y and call into the BLAS three times without creating a temporary vector. \nThis does not require an inter\u00admediate structure, but it requires making multiple passes over each input \nvector. It is also numerically unstable. The Haskell program\u00admer, on the other hand, could straightforwardly \nwrite the following implementation rbf :: Double . Vector Double . Vector Double . Double rbf . v w = \nexp (-. * msum (mzipWith norm v w)) where norm x y = square (x - y) square x = x * x We show the performance \nof three implementations of the Gaussian RBF in Figure 9. The .rst is the Haskell function rbf whose \nde.nition we have just given. The second is the C version that uses the above identity to avoid temporary \nallocation, and the third allocates an intermediate value. The Haskell version with SSE support is the \nclear winner due to fusion, it touches each element of the two vectors only once. d o u b l e n o r m \n2 ( V e c t o r X d c o n s t &#38; v ) { r e t u r n v . d o t ( v ) ; } d o u b l e e i g e n _r b \nf ( d o u b l e nu , V e c t o r X d c o n s t &#38; u , V e c t o r X d c o n s t &#38; v ) { V e c \nt o r X d t e m p = u -v ; r e t u r n e x p ( -n u * n o r m 2 ( u -v ) ) ; } 5.4 The cost of abstraction \nin C++ C is a straw man when fusion of array operations becomes critical for performance we know of no \nC compiler that can perform this kind of optimisation. An imperative programmer would not attack these \nsorts of problems with C in any case, but likely turn to C++, for which there are a number of libraries \ntailored to matrix com\u00adputation that do perform fusion. These libraries all use expression templates \n[32], a technique pioneered by the Blitz++ [33] library. We have implemented C++ versions of the Gaussian \nRBF using three different libraries: Blitz++ 0.10 [33], Boost uBLAS 1.53 [14], and Eigen 3.1.2 [11]. \nWe give performance numbers for all three, but our discussion focuses on Eigen, which utilizes SSE instructions \nand is generally considered the most performant library in this class. Figure 10 shows the Gaussian RBF \nimplemented using Eigen. Note that we have implemented the square of the L2-norm ourselves, a point we \nreturn to later. In this speci.c case the Eigen library has the squared L2-norm in its API, but in more \ncomplicated examples there might be no such built-in provision. After writing the code in Figure 10, \nwe decided to actually read the Eigen documentation. We reproduce a highly relevant note here [11]: To \nsummarize, the implementation of functions taking non\u00adwritable (const referenced) objects is not a big \nissue and does not lead to problematic situations in terms of compiling and running your program. However, \na naive implementation is likely to introduce unnecessary temporary objects in your code. In order to \navoid evaluating parameters into temporaries, pass them as (const) references to MatrixBase or ArrayBase \n(so templatize your function). Having read this, we realized our mistake and modi.ed our code, producing \nthe version in Figure 11. Avoiding temporary allocation in Eigen and other libraries can require some \ncare because expression templates are not vectors or matrices, but represent computations that, when \nrun, compute a matrix or vector. Programmer-written abstractions must be careful to abstract over expression \ntemplates, and not over matrices or vectors. Figure 12 shows the performance of the Haskell and C++ implementations \nof the Gaussian radial basis function. Our initial version using Eigen (from Figure 10, labeled Eigen \n(bad norm2) ) performs quite poorly but the modi.ed one is the fastest by far. Interestingly, once the \nworking set no longer .ts in L3 cache, the Boost version, which does not use SSE instructions, outperforms \nall SSE-enabled variants except the fast Eigen version. We suspect that Boost and Eigen interact better \nwith the processor s prefetch prediction than the other libraries. Clearly properly -written C++ can \noutperform Haskell. The challenge is in .guring out what proper means. In the domain covered by vectorlib, \na Haskell programmer can write straightfor\u00ad  t e m p l a t e <t y p e n a m e D e r i v e d > t y p \ne n a m e D e r i v e d : : S c a l a r n o r m 2 ( c o n s t M a t r i x B a s e < D e r i v e d > &#38; \nv ) { r e t u r n v . d o t ( v ) ; } d o u b l e e i g e n _r b f ( d o u b l e nu , V e c t o r X d \nc o n s t &#38; u , V e c t o r X d c o n s t &#38; v ) { V e c t o r X d t e m p = u -v ; r e t u r \nn e x p ( -n u * n o r m 2 ( u -v ) ) ; } Figure 12: Performance of Haskell and C++ Gaussian RBF implemen\u00adtations. \nward, declarative code and expect the compiler to handle fusion. The C++ programmer must worry about \nthe performance implications of abstraction. Furthermore, unlike Eigen, our implementation is immature, \nand there are several straightforward changes that will further improve the absolute performance of Haskell \ncode. There is room for improvement in our use of prefetching. More importantly, we currently cannot \nrely on memory allocated by GHC to be properly aligned for SSE move-aligned instructions, so all SSE \nmove instructions are unaligned. Differentiating between unaligned and aligned memory will allow us to \navoid the large performance hit incurred by our current implementation.  5.5 Generalized stream fusion \nin Data Parallel Haskell As we described in Section 3.5, we have modi.ed Data Parallel Haskell (DPH) \nto generate vectorized programs that work over bundles, and hence exploit both SSE and multi-core parallelism. \nFigure 13 shows the performance of several dot product implementa\u00adtions, including DPH implementations \nwith and without our rewrit\u00adten run-time. These measurements were taken on a 32-core (4 \u00d7 8) AMD Opteron \n6128 running at 2GHz. In addition to transparently taking advantage of SSE instructions, our modi.ed \nversion of DPH transparently takes advantage of additional cores. Unfortunately, DPH does not scale particularly \nwell on this benchmark. This is Figure 13: Performance of double-precision dot product implementa\u00adtions. \nThe DPH implementations are multi-threaded, and the vector library and hand-written C implementations \nare single-threaded. Error bars show variance. undoubtedly in part because we not only used a development \nversion of DPH, but we also then performed substantial surgery to adapt it to our version of vector. \nWe do not know why the scalar version outperformed the SSE-enabled version in the 16-core case. 6. Related \nWork Wadler [34] introduced the problem of deforestation, that is, of eliminating intermediate structures \nin programs written as compo\u00adsitions of list transforming functions. A great deal of follow-on work [8, \n12, 15, 23, 29, 30] attempted to improve the ability of compilers to automate deforestation through program \ntransforma\u00adtions. Each of these approaches to fusion has severe limitations. For example, Gill et al. \n[8] cannot fuse left folds, such as that which arises in sum, or zipWith, and Svenningsson [29] cannot \nhandle nested computations such as mapping a function over concatenated lists. Our work is based on the \nstream fusion framework described by Coutts et al. [6], which can fuse all of these use cases and more. \nThe vector library uses stream fusion to fuse operations on vectors rather than lists, but the principles \nare the same. 7. Conclusion Generalized stream fusion is a strict improvement on stream fusion; by re-casting \nstream fusion to operate on bundles of streams, each vector operation or class of operations can utilize \na stream representation tailored to its particular pattern of computation. We describe two new stream \nrepresentations, one supporting bulk memory operations, and one adapted for SIMD computation with short-vector \ninstructions, e.g., SSE on x86. We have added support for low-level SSE instructions to GHC and incorporated \ngeneralized stream fusion into the vector library. Using our modi.ed library, programmers can write compositional, \nhigh-level programs for manipulating vectors without loss of ef.ciency. Benchmarks show that these programs \ncan perform competitively with hand-written C. Although we implemented generalized stream fusion in a \nHaskell library, the bundled stream representation could be used as an intermediate language in another \ncompiler. Vector operations would no longer be .rst class in such a formulation, but it would allow a \nlanguage to take advantage of fusion without requiring implementations of the general purpose optimizations \npresent in GHC that allow it to eliminate the intermediate structures produced by generalized stream \nfusion.  Acknowledgments The authors would like to thank Simon Marlow for his help debug\u00adging GHC s \nruntime system and code generator. We are also grateful for Andrew Fitzgibbon s many insightful comments. \nReferences [1] G. E. Blelloch, J. C. Hardwick, J. Sipelstein, M. Zagha, and S. Chatterjee. Implementation \nof a portable nested data-parallel language. Journal of Parallel and Distributed Computing, 21(1):4 14, \n1994. [2] Bryan O Sullivan. statistics: A library of statistical types, data, and functions, aug 2012. \nURL http://hackage.haskell.org/ package/statistics. [3] M. M. T. Chakravarty, G. Keller, S. Peyton Jones, \nand S. Marlow. As\u00adsociated types with class. In Proceedings of the 32nd ACM SIGPLAN-SIGACT symposium \non Principles of Programming Languages, POPL 05, page 1 13, New York, NY, USA, 2005. [4] M. M. T. Chakravarty, \nR. Leshchinskiy, S. Peyton Jones, G. Keller, and S. Marlow. Data Parallel Haskell: a status report. In \nProceedings of the 2007 workshop on Declarative Aspects of Multicore Programming, DAMP 07, page 10 18, \nNice, France, 2007. [5] Christian H\u00f6ner zu Siederdissen. StatisticalMethods: collection of useful statistical \nmethods., aug 2011. URL http://hackage. haskell.org/package/StatisticalMethods. [6] D. Coutts, R. Leshchinskiy, \nand D. Stewart. Stream fusion: from lists to streams to nothing at all. In Proceedings of the 12th ACM \nSIGPLAN International Conference on Functional Programming, pages 315 326, Freiburg, Germany, 2007. [7] \nDuncun Coutts. Stream Fusion: Practical shortcut fusion for coinduc\u00adtive sequence types. PhD thesis, \nUniversity of Oxford, 2010. [8] A. Gill, J. Launchbury, and S. L. Peyton Jones. A short cut to deforestation. \nIn Proceedings of the conference on Functional Programming Languages and Computer Architecture, FPCA \n93, page 223 232, New York, NY, USA, 1993. [9] K. Goto and R. v. d. Geijn. Anatomy of high-performance \nmatrix multiplication. ACM Trans. Math. Softw., 34(3):1 25, 2008. [10] K. Goto and R. v. d. Geijn. High-performance \nimplementation of the level-3 BLAS. ACM Trans. Math. Softw., 35(1):1 14, 2008. [11] G. Guennebaud, B. \nJacob, and others. Eigen v3, 2010. URL http://eigen.tuxfamily.org. [12] G. W. Hamilton. Extending higher-order \ndeforestation: transforming programs to eliminate even more trees. In K. Hammond and S. Curtis, editors, \nProceedings of the Third Scottish Functional Programming Workshop, page 25 36, Exeter, UK, UK, aug 2001. \n[13] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. 2 edition, 2009. \n[14] Joerg Walter, Mathias Koch, Gunter Winkler, and David Bellot. Boost basic linear algebra -1.53.0, \n2010. URL http://www.boost.org/ doc/libs/1_53_0/libs/numeric/ublas/doc/index.htm. [15] P. Johann. Short \ncut fusion: proved and improved. In Proceedings of the 2nd international conference on Semantics, applications, \nand implementation of program generation, SAIG 01, page 47 71, Berlin, Heidelberg, 2001. [16] W. Kahan. \nPracniques: further remarks on reducing truncation errors. Commun. ACM, 8(1):40 , jan 1965. [17] G. Keller, \nM. M. Chakravarty, R. Leshchinskiy, S. Peyton Jones, and B. Lippmeier. Regular, shape-polymorphic, parallel \narrays in Haskell. In Proceedings of the 15th ACM SIGPLAN International Conference on Functional Programming, \nICFP 10, page 261 272, New York, NY, USA, 2010. [18] J. Launchbury and S. L. Peyton Jones. State in Haskell. \nLisp and Symbolic Computation, 8(4):293 341, 1995. [19] R. Leshchinskiy. vector: Ef.cient arrays, oct \n2012. URL http: //hackage.haskell.org/package/vector. [20] B. Lippmeier and G. Keller. Ef.cient parallel \nstencil convolution in Haskell. In Proceedings of the 4th ACM Symposium on Haskell, Haskell 11, page \n59 70, New York, NY, USA, 2011. [21] B. Lippmeier, M. Chakravarty, G. Keller, and S. Peyton Jones. Guiding \nparallel array fusion with indexed types. In Proceedings of the 2012 Symposium on Haskell, Haskell 12, \npage 25 36, New York, NY, USA, 2012. [22] S. Marlow and S. Peyton Jones. Making a fast curry: Push/Enter \nvs. Eval/Apply for higher-order languages. Journal of Functional Programming, 16(4-5):415 449, 2006. \n[23] S. Marlow and P. Wadler. Deforestation for higher-order functions. In Proceedings of the 1992 Glasgow \nWorkshop on Functional Program\u00adming, page 154 165, London, UK, UK, 1993. [24] S. Peyton Jones. Call-pattern \nspecialisation for Haskell programs. In Proceedings of the 12th ACM SIGPLAN International Conference \non Functional Programming, ICFP 07, page 327 337, New York, NY, USA, 2007. [25] S. Peyton Jones, R. Leshchinskiy, \nG. Keller, and M. Chakravarty. Harnessing the multicores: Nested data parallelism in Haskell. In Programming \nLanguages and Systems, page 138. 2008. [26] S. L. Peyton Jones and J. Launchbury. Unboxed values as .rst \nclass citizens in a non-strict functional language. In Proceedings of the 5th ACM Conference on Functional \nProgramming Languages and Computer Architecture, pages 636 666, 1991. [27] S. L. Peyton Jones, N. Ramsey, \nand F. Reig. C--: a portable assembly language that supports garbage collection. In International Conference \non Principles and Practice of Declarative Programming, sep 1999. [28] S. L. Peyton Jones, T. Hoare, and \nA. Tolmach. Playing by the rules: rewriting as a practical optimisation technique. In Proceedings of \nthe 2001 ACM SIGPLAN Workshop on Haskell, 2001. [29] J. Svenningsson. Shortcut fusion for accumulating \nparameters &#38; zip-like functions. In Proceedings of the seventh ACM SIGPLAN International Conference \non Functional Programming, ICFP 02, page 124 132, New York, NY, USA, 2002. [30] A. Takano and E. Meijer. \nShortcut deforestation in calculational form. In Proceedings of the seventh international conference \non Functional Programming and Computer Architecture, FPCA 95, page 306 313, New York, NY, USA, 1995. \n[31] D. A. Terei and M. M. Chakravarty. An LLVM backend for GHC. In Proceedings of the third ACM Symposium \non Haskell, Haskell 10, page 109 120, New York, NY, USA, 2010. [32] T. Veldhuizen. Expression templates. \nC++ Report, 7(5):26 31, jun 1995. [33] T. L. Veldhuizen. Arrays in Blitz++. In D. Caromel, R. R. Oldehoeft, \nand M. Tholburn, editors, Computing in Object-Oriented Parallel Environments, number 1505 in Lecture \nNotes in Computer Science, pages 223 230. jan 1998. [34] P. Wadler. Deforestation: transforming programs \nto eliminate trees. Theoretical Computer Science, 73(2):231 248, jun 1990.    \n\t\t\t", "proc_id": "2500365", "abstract": "<p>Stream fusion is a powerful technique for automatically transforming high-level sequence-processing functions into efficient implementations. It has been used to great effect in Haskell libraries for manipulating byte arrays, Unicode text, and unboxed vectors. However, some operations, like vector append, still do not perform well within the standard stream fusion framework. Others, like SIMD computation using the SSE and AVX instructions available on modern x86 chips, do not seem to fit in the framework at all. </p> <p>In this paper we introduce generalized stream fusion, which solves these issues. The key insight is to bundle together multiple stream representations, each tuned for a particular class of stream consumer. We also describe a stream representation suited for efficient computation with SSE instructions. Our ideas are implemented in modified versions of the GHC compiler and vector library. Benchmarks show that high-level Haskell code written using our compiler and libraries can produce code that is faster than both compiler- and hand-vectorized C.</p>", "authors": [{"name": "Geoffrey Mainland", "author_profile_id": "81100519306", "affiliation": "Microsoft Research Ltd, Cambridge, England UK", "person_id": "P4261212", "email_address": "gmainlan@microsoft.com", "orcid_id": ""}, {"name": "Roman Leshchinskiy", "author_profile_id": "81330494188", "affiliation": "Self, London, United Kingdom", "person_id": "P4261213", "email_address": "rl@cse.unsw.edu.au", "orcid_id": ""}, {"name": "Simon Peyton Jones", "author_profile_id": "81100271851", "affiliation": "Microsoft Research Ltd, Cambridge, England UK", "person_id": "P4261214", "email_address": "simonpj@microsoft.com", "orcid_id": ""}], "doi_number": "10.1145/2500365.2500601", "year": "2013", "article_id": "2500601", "conference": "ICFP", "title": "Exploiting vector instructions with generalized stream fusio", "url": "http://dl.acm.org/citation.cfm?id=2500601"}