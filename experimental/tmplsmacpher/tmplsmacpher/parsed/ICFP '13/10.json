{"article_publication_date": "09-25-2013", "fulltext": "\n Ef.cient Divide-and-Conquer Parsing of Practical Context-Free Languages Jean-Philippe Bernardy Koen \nClaessen Chalmers University of Technology and University of Gothenburg {bernardy,koen}@chalmers.se \nAbstract We present a divide-and-conquer algorithm for parsing context\u00adfree languages ef.ciently. Our \nalgorithm is an instance of Valiant s (1975), who reduced the problem of parsing to matrix multiplica\u00adtions. \nWe show that, while the conquer step of Valiant s is O(n 3) in the worst case, it improves to O(log3 \nn), under certain condi\u00adtions satis.ed by many useful inputs. These conditions occur for example in program \ntexts written by humans. The improvement happens because the multiplications involve an overwhelming \nma\u00adjority of empty matrices. This result is relevant to modern comput\u00ading: divide-and-conquer algorithms \ncan be parallelized relatively easily. Categories and Subject Descriptors F.4.2 [Grammars and Other Rewriting \nSystems]: Parsing Keywords Divide-and-Conquer, Parsing, Complexity, Context-Free Languages, Iteration \n1. Introduction Recent years have seen the rise of parallel computer architectures for the masses. Multicore \nCPUs and GPUs are legion. One would expect functional programs to be a perfect match for these archi\u00adtectures. \nIndeed, thanks to the absence of side-effects, functional programs are conceptually easy to parallelise. \nHowever, functional programmers have traditionally relied heavily on lists as the data\u00adstructure of choice. \nThis tradition hinders the adaptation of func\u00adtional programs to the age of parallelism. Indeed, the \nvery linear structure of lists imposes a sequential treatment of them. In an eloquent 2009 ICFP invited \ntalk, Guy Steele harangued the func\u00adtional programming crowds to stop using lists and use sequences, \nrepresented as balanced trees. If a computation over them follows the divide-and-conquer skeleton, and \nuses an associative operator to cheaply combine intermediate results at each node, their fractal structure \nallows to take advantage of many processors in parallel; in fact as many as there are leaves in the tree. \nAn additional bene.t of the structure its ability to support in\u00adcremental computation. That is, if one \nremembers the intermediate results of the computation for each node, then after changing a sin- Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. Copyrights for components of this work owned \nby others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, \nor republish, to post on servers or to redistribute to lists, requires prior speci.c permission and/or \na fee. Request permissions from permissions@acm.org. ICFP 13, September 25 27, 2013, Boston, MA, USA. \nCopyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2326-0/13/09. \n. . $15.00. http://dx.doi.org/10.1145/2500365.2500576 gle leaf in the tree, it suf.ces to recompute \nthe results for the nodes which are on the path from the root to the given leaf. If the tree is balanced, \nthis means that one only has to run the association oper\u00adator only a few times to update the result after \na single incremental change. Some problems are naturally solved by divide-and-conquer al\u00adgorithms. This \nis the case for example of vector operations, which treat each element independently of the others. However, \nmany problems require creativity to discover ef.cient divide-and-conquer solutions. This is the case \nof the problem of parsing context free languages. Valiant (1975) discovered a divide-and-conquer algorithm \nfor context-free recognition. However, given Valiant s assumptions, the cost of the conquer step is cubic. \nThis means that the conquer step dominates the cost of the algorithm: what we gain by running sub-problems \nin parallel is dwarfed by the cost of what we must run sequentially. Therefore the divide-and-conquer \nstructure does not yield a signi.cant performance bene.t. In this paper, we show that on most inputs, \none can carefully implement Valiant s algo\u00adrithm to get a polylogarithmic conquer step, yielding good \noverall performance. We make the following contributions: We give a new correctness proof of the Valiant \nalgorithm. We re-construct Valiant algorithm by calculating it from its spec\u00adi.cation, in a style reminiscent \nof Bird and de Moor (1997) (Sec. 2.4).  We characterize a new subclass of context-free languages, cor\u00adresponding \nto strings parsed in a hierarchical manner (Def. 8, Sec. 3.1).  We show that, for languages of the new \nsubclass, the time complexity of the conquer step is O(log3 n) (Sec. 3.2).  We propose a re.nement of \ncontext-free grammars (and the corresponding modi.cations of Valiant s parser) which allows to parse \niterative structures hierarchically instead of linearly (Sec. 4).  We conjecture that any given context-free \nprogramming lan\u00adguage, understood as the set of strings actually written in it (not the language prescribed \nby its grammar) belong to the subclass that we propose if iteration is represented hierarchically as \nwe prescribe.  We have implemented the parsing technique described above, and have integrated it in \nthe BNFC parser generator (Forsberg and Ranta 2012).   2. Context Free Parsing In this section we review \nthe basics of context free (CF) parsing, introduce our notation, and review CYK and Valiant algorithms. \n2.1 Conventions and Notations We assume a CF grammar G, given by a quadruple (S, N, P, S), where S is \na .nite set of terminals, N is a .nite set of non-terminals of which S is the starting symbol, and P \na .nite set of productions. We furthermore assume an input w . S * a sequence of terminal symbols of \nlength |w|. The input symbol at position i is denoted w[i]. A sub-string of w starting at position i \n(included) and ending at position j (excluded), is denoted w[i..j]. Metasyntactic variables standing \nfor arbitrary strings of terminals will have the form w1, w2, . . .. The letters A, B, C, . . ., stand \nfor arbitrary non\u00adterminals, while a, \u00df, . . . stand for arbitrary strings (elements of (S . N) *) and \nt stands for a terminal symbol. Each production rule associates a non-terminal with a string it can generate. \nDe.nition 1. aA\u00df -. a.\u00df iff. (A ::= .) . P * De.nition 2. -. stands for the re.exive transitive closure \nof -.. The input string w belongs to the language L generated by G * iff. S -. w. Any CF grammar G de.ning \na language L can be converted to a grammar G' in Chomsky Normal Form (CNF) de.ning the same language \nL, with |G'| = |G|2 (Chomsky 1959). Hence we will assume without loss of generality a grammar in CNF. \nIn CNF, the rules are restricted to the following forms S ::= E (nullary) A ::= t (unary) A0 ::= A1A2 \n(binary) but it is easy to handle the empty string specially, so we conven\u00adtionally exclude it from \nthe input language and thus exclude the nullary rule. In sum, we assume that P contains only unary and \nbi\u00adnary rules. The reader avid of details is directed to Lange and Lei\u00df (2009) for a pedagogical account \nof reduction to CNF. Given a grammar speci.ed as above, the problem of parsing is reduced to .nding a \nbinary tree such that each leaf corresponds to a symbol of the input and a suitable unary rule; and each \nbranch corresponds to a suitable binary rule. Essentially, parsing is equiva\u00adlent to consider all possible \nbracketings of the input, and verify that they form a valid parse.  2.2 Charts as Matrices, Parsing \nas Closure In this section we show how to specify parsing as an equation on matrices. We start by abstracting \naway from the grammar, via a ring-like structure. We de.ne the operations 0, +, \u00b7 and s as follows. De.nition \n3 (0, +, \u00b7 on P(N)). 0 = \u00d8 x + y = x . y x \u00b7 y = {A | A0 . x, A1 . y, A ::= A0A1 . P } si = {A | A ::= \nw[i] . P } The (\u00b7) operation fully characterizes the binary production rules of the grammar, while s \ncaptures the unary ones. We have the following properties: (0,+) forms a commutative monoid (the usual \nmonoid of sets with union); 0 is absorbing for (\u00b7); and (\u00b7) distributes over (+). However, and crucially, \n(\u00b7) is not associative. x + 0 = x 0 + x = x (x + y) + z = x + (y + z) x \u00b7 (y + z) = x \u00b7 y + x \u00b7 z x \u00b7 \n0 = 0 0 \u00b7 x = 0 We will then use matrices of sets of non-terminals to record which non-terminals can \ngenerate a given substring. The intention is that * A . Cij iff. A -. w[i..j]. See Fig. 1 for an illustration. \nIn parsing terminology, a structure containing intermediate parse results is called a chart. We call \nthe set of charts C. De.nition 4. C = P(N)N\u00d7N We lift the operations 0, +, \u00b7 from sets of non-terminals \nto matrices of sets of nonterminals, in the usual manner. De.nition 5 (0, +, \u00b7 on C). 0ij = 0 (A + B)ij \n= Aij + Bij (A \u00b7 B)ij =Aik \u00b7 Bkj k The properties are lifted accordingly. The operation s is used to \ncompute an upper diagonal matrix corresponding to the input w, as follows. De.nition 6 (Initial matrix). \nThe initial matrix, written I(w), is a square matrix of dimension |w| + 1 such that I(w)i,i+1 = si I(w)i,j \n= 0 if j= i + 1 Let W (1) = I(w). Note that W (1) = si contains all the non\u00ad i,i+1 terminals which can \ngenerate the substring w[i..i + 1]. Let W (2) = W (1)W (1) (2) + I(w). It is easy to see that W = si \n\u00b7 si+1, hence i,i+2 it contains all the non-terminals which can generate the substring = W (2) \u00b7 W (2) \nw[i..i + 2]. Consider now W (3) + I(w). We have (3) (2) (2) (2) (2) W = W \u00b7 W + W \u00b7 W i,i+3 i,i+2 i+2,i+3 \ni,i+1 i+1,i+3 = (si \u00b7 si+1) \u00b7 si+2 + si \u00b7 (si+1 \u00b7 si+2) and (3) (2) (2) W = W \u00b7 W i,i+4 i,i+2 i+2,i+4 \n= (si \u00b7 si+1) \u00b7 (si+2 \u00b7 si+3) Hence W (3) contains all possible parsing of 3 symbols, and all balanced \nparsings of 4 symbols. By iterating n times, one obtains all the parsings of n symbols. (However, as \na hint to our method for ef.cient parsing, it suf.ces to repeat the process log n + 1 times to obtain \nall balanced parsings of n symbols). De.nition 7 (Transitive closure). If it exists, the transitive closure \nof a matrix W , written W +, is the matrix C such that C = C \u00b7 C + W A consequence of the above is C \n. C \u00b7 C + I(w). It is clear by now that, consequently, every possible bracketing of the products I(w) \n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 I(w) is contained in C, and thus all possible parsings of w[i..j] are found in Cij . Conversely, \nbecause C . C\u00b7C+W , if Cij contains a non-terminal then it must generate w[i..j]. Algorithms which parse \nby computing a chart are known as chart parsers. The above procedure speci.es a recognizer: by constructing \nI(w)+ one .nds if w is parsable, but not the corresponding parse.  Figure 1. Example charts. In each \nchart a point at position (x, y) corresponds to a substring starting at x and ending at y. The .rst parameter \nx grows downwards and the second y one rightwards. The input string w is represented by the diagonal \nline. Dots in the upper-right part represent nonterminals. The .rst chart witnesses * * A -. w[i..j] \nand B -. w[k..l]. An instance of the rule Z ::= X Y is exempli.ed on the second chart. Even though we \nfocus on the recognition problem in this paper, it is straightforward to specify parsers by using matrices \nof parse trees instead of non-terminals, and adapting the operations accordingly, as we have done in \nour BNFC implementation. Following the divide-and-conquer strategy, our parser is based on a function \nbin combining two charts I(w1)+ , I(w2)+ and a terminal t into a chart I(w1tw2)+ .  2.3 Cocke Younger \nKasami A straightforward manner to turn the above speci.cation into an algorithm is as follows. Let us \n.rst remark that the product of two upper triangular ma\u00adtrices is upper triangular. Hence the closure \nof an upper triangular matrix must also be upper triangular. Hence, in every chart ever considered, every \nelement at the diagonal and below it equals zero. Expanding index-wise the equation C = C \u00b7 C + I(w) \nyields: n Cij = I(w)ij + Cik \u00b7 Ckj k=0 Because C is upper triangular, Cik = 0 if k = i and Ckj = 0 if \nk > j. Hence the sum can be limited to the interval [i + 1..j] j Cij = I(w)ij + Cik \u00b7 Ckj k=i+1 Observing \nthat the summand equals 0 on the upper diagonal and I(w)ij = 0 otherwise, we distinguish on that condition \nand obtain the two equations: Ci,i+1 = si (1) j Cij = Cik \u00b7 Ckj if j > i + 1 (2) k=i+1 These equations \ngive a method to compute Cij by induction on j - i. The equations can be re-interpreted in term of parses \nand non-terminals as follows. Either we parse a single token wi, and the nonterminals generating it \nare given directly from unary rules, or  we parse a longer string. In this case we split it at any interme\u00addiate \nposition k, and combine the intermediate results (found in Cik and Ckj ) in every possible way according \nto binary rules.  By applying the above rules naively, computation time is exponen\u00adtial in the length \nof the input; however by memoizing each inter\u00admediate result (for example by using lazy dynamic programming \n(Allison 1992)) the complexity is merely cubic. The resulting dy\u00adnamic programming algorithm is known \nas CYK, owing to its in\u00addependent discoverers: Cocke (1969), Kasami (1965) and Younger (1967). In the \nCYK algorithm, any element of the chart is computed only on the basis of elements strictly closer to \nthe diagonal. Hence it can be used to program the combination step of a divide-and\u00adconquer algorithm. \nThe combination of two charts and a terminal C = bin(A, w[i], B), is de.ned as follows. Elements of C \nin the upper left corner are copied from A; elements of the bottom right corner are copied from B; and \nelements from the top right corner are computed using si and the CYK formula (Eq. (2)). Are we done? \nUnfortunately, no. The above operator has to compute a matrix of size n\u00d7m, and computing each element \ntakes time linear in n + m. The complexity of bin is therefore cubic; and dwarfs the time spent on computing \nthe sub-charts A and B. For the divide-and-conquer strategy to be ef.cient, we need the running time \nof the combination operator to be less than the running time of the recursive calls.  2.4 Valiant A \nmore subtle way to turn the transitive closure speci.cation into an algorithm is the following. Our task \nis to .nd a function \u00b7 + which maps a matrix W to its transitive closure C = W +. As above, we do so \nby re.nement of the de.nition of transitive closure, but we adopt a divide and conquer approach rather \nthan a direct one. If W is a 1 by 1 matrix, C = W = 0. Otherwise, let us divide W and C in blocks as \nfollows (for ef.ciency the blocks should be roughly of the same size; but the reasoning holds for any \nsizes): A ' AX X ' W =C = 0 B0 B ' Then the condition that C is the transitive closure of W becomes \n A '' '' A ' A ' X XXAX B '=B '\u00b7B '+ 0 000 B Applying matrix multiplication and sum block-wise: ' '' \nA = A A + A ''' '' X = A X + X B + X ' '' B = B B + B Because A and B are smaller than W (and still upper \ntriangular), we know how to compute A ' and B ' recursively (A ' = A+ , B ' = B+). There remains to .nd \nan algorithm to compute the top-right corner X ' of the matrix. That is (renaming variables for convenience) \nthe problem is reduced to .nding a recursive function V which maps A, B and X to Y = V (A, X, B), such \nthat Y = AY +Y B +X. In terms of parsing, the function V combines the chart A of the .rst part of the \ninput with the chart B of the second part of the input, via a partial chart X concerned only with strings \nstarting in A and ending in B, and produces a full chart Y . Let us divide each matrix in blocks again: \n Y11 Y12 X11 X12 Y =X = Y21 Y22X21 X22 A11 A12 B11 B12 A =B = 0 A220 B22 (Again we assume that splitting \ncan be done; the base cases can be obtained by dropping the .rst rows and/or the second columns in  \n In sum, with the above de.nitions, we have the following ex\u00adpression for V in the recursive case A11 \nA12 X11 X12 B11 B12 Y11 Y12 V,, = . 0 A22 X21 X22 0 B22Y21 Y22 In the base cases, some or all of the \ntop and/or right sub-matrices are empty and the corresponding recursive calls are omitted. In terms of \nparsing, initially the partial chart X contains at the bottom\u00adleft position a single non-zero element \ncorresponding to the symbol at the interface of A and B. Recursive calls progressively .ll this chart, \nquadrant by quadrant. The above algorithm was .rst described by Valiant (1975). A graphical summary is \nshown in Fig. 2. From Valiant s function V , one can construct the bin operator as follows: A V (A, X, \nB)bin(A, t, B) = 0 B . . where X = ..... 0 \u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7 0 . . . . .. . . . . . 0 0 . ..... Figure 2. The recursive \nstep of function V . The charts A and B are already complete. To complete the matrix X, that is, compute \nY = V (A, X, B), one splits the matrices and performs 4 recursive calls. Each recursive call is depicted \ngraphically. In each .gure, to complete the dark-gray square, multiply the light-gray rectangles and \nadd them to the dark-gray square, then do a recursive call on triangular matrix composed of the completed \ndark-gray square and the triangles. the above splits.) The condition on Y then becomes Y11 Y12 A11 A12 \nY11 Y12 = \u00b7 Y21 Y22 0 A22 Y21 Y22 Y11 Y12 B11 B12 X11 X12 + \u00b7 + Y21 Y22 0 B22 X21 X22 By applying matrix \nmultiplication and sum block-wise: Y11 = A11Y11 + A12Y21 + Y11B11 + 0 + X11 Y12 = A11Y12 + A12Y22 + Y11B12 \n+ Y12B22 + X12 Y21 = 0 + A22Y21 + Y21B11 + 0 + X21 Y22 = 0 + A22Y22 + Y21B12 + Y22B22 + X22 By commutativity \nof (+) and 0 being its unit: Y11 = A11Y11 + X11 + A12Y21 + Y11B11 Y12 = A11Y12 + X12 + A12Y22 + Y11B12 \n+ Y12B22 Y21 = A22Y21 + X21 + 0 + Y21B11 Y22 = A22Y22 + X22 + Y21B12 + Y22B22 si 0 \u00b7 \u00b7 \u00b7 0 An advantage \nof Valiant s algorithm over CYK is that it treats whole subcharts at once, via matrix-level multiplication \nand addi\u00adtion, while CYK explicitly refers to each element of C individually. In particular, when using \na sparse-matrix representation, the mul\u00adtiplication of an empty chart with any other chart is instantaneous. \nThe ability to handle this case ef.ciently is key: we observe that in many cases, charts are sparse, \nand composition of charts is ef.cient. When using a straighforward representation of sparse matrices, \nthe implementation of Valiant s algorithm is an elegant functional program, as can be seen in Fig. 3. \n3. Sparse Matrix Assumption and Complexity Analysis 3.1 Model of the Input In practice, matrices representing \ncharts are expected to be sparse for large inputs, that is, a given substring is unlikely to be generated \nby a given non-terminal. Indeed, in most cases, the substring starts in the middle of a construction \nand ends in the middle of some other, usually unrelated other construction. This effect is illustrated \nin Fig. 4. In the remainder of the paper, we assume that inputs conform to this assumption. Before explaining \nwhere it is coming from, we give its formal de.nition. De.nition 8 (Assumption). There exists a constant \na such that, for any input, the distribution of non-zero elements in the chart C corresponding to it \nis bounded as follows. For any square subchart A of C above the diagonal, Because each of the sub-matrices \nis smaller and because of the . . absence of circular dependencies, Y can be computed recursively: #A \n= Y21 = V (A22, X21 , B11) Y11 = V (A11, X11 + A12Y21 , B11)  ... 1 a (j - i)2 (i,j).dom(A) ... where \n#A is the number of non-zero elements in matrix A. Y22 = V (A22, X22 + Y21B12 , B22) Y12 = V (A11, X12 \n+ A12Y22 + Y11B12, B22) We have ignored the base cases so far because they are straight\u00adforward, except \nfor the following point. When computing V (A, X, B) on matrices of dimension 1 \u00d7 1, it is guaranteed \nthat A and B are equal to 0. Indeed, in that case X is just above the diagonal. There\u00adfore A and B are \non it and must then be 0. The result matrix is therefore equal to X. We stress that the assumption involves \nnot a grammar per se, but the language itself (i.e. the set of possible input strings we consider), when \nseen as strings generated by a given grammar in CNF. So, for any given a, every non-trivial grammar will \nadmit strings that break the assumption, but usually the set of strings we consider behaves well in practice. \nThe above formula merits justi.cation. Before using it to eval\u00aduate the complexity of the parsing algorithm, \nwe will build a more  import Prelude (Eq (. .)) class RingLike a where zero :: a (+) :: a . a . a (\u00b7) \n:: a . a . a data M a = Q (M a ) (M a ) (M a ) (M a ) | Z | One a q Z Z Z Z = Z qa b c d = Q a b c d \none x = if x = zero then Z else One x instance (Eq a , RingLike a ) . RingLike (M a ) where zero = Z \nZ + x = x x + Z = x One x + One y = one (x + y) Q a11 a12 a21 a22 + Q b11 b12 b21 b22 = q (a11 + b11) \n(a12 + b12) (a21 + b21) (a22 + b22) Z \u00b7 x = Z x \u00b7 Z = Z One x \u00b7 One y = one (x \u00b7 y) Q a11 a12 a21 a22 \n\u00b7 Q b11 b12 b21 b22 = q (a11 \u00b7 b11 + a12 \u00b7 b21) (a11 \u00b7 b12 + a12 \u00b7 b22) (a21 \u00b7 b11 + a22 \u00b7 b21) (a21 \n\u00b7 b21 + a22 \u00b7 b22) v :: (Eq a , RingLike a ) . M a . M a . M a . M a va Zb = Z v Z (One x ) Z = One x \nv (Q a11 a12 Z a22) (Q x11 x12 x21 x22) (Q b11 b12 Z b22) = q y11 y12 y21 y22 where y21 = v a22 x21 b11 \ny11 = v a11 (x11 + a12 \u00b7 y21 ) b11 y22 = v a22 (x22 + y21 \u00b7 b12) b22 y12 = v a11 (x12 + a12 \u00b7 y22 + y11 \n\u00b7 b12) b22 Figure 3. Data structure for charts as sparse matrices (M), and im\u00adplementation of the function \nV . The tricky parts compared to the mathematical development of Sec. 2.4 is the handling of empty ma\u00adtrices. \nCare must be taken to create empty matrices (Z) whenever they contain only zero elements. This is done \nby using the smart constructors q and one in matrix multiplication. The input matrices a and b are empty \niff. the matrix x has dimension one. For con\u00adcision, this implementation supports only matrices of size \n2n for some n. It can be extended to matrices of arbitrary dimension in a straightforward manner by adding \nconstructors for row and col\u00adumn matrices, to be used as leaves. An implementation supporting arbitrary \nmatrix dimensions, as well as the optimization explained in Sec. 6.2 can be found in the BNFC repository: \nhttps://github.com/BNFC/bnfc/blob/master/source/ runtime/Data/Matrix/Quad.hs precise intuition for it \nby examining its consequences and its pos\u00adsible causes. By symbolically evaluating the sum , we can deduce \nwhat the above assumption implies for each of the following shapes for A: if A is a square of size n \ntouching the diagonal, then #A = ia log nl  if A is a square of size n at distance kn to the diagonal \nwith k = 1, then #A = ia(2 log(k + 1) - log(k + 2) - log(k))l. Hence, remarkably, the upper bound of \nnon-zero elemens in a square chart of dimension n which is at least n elements away from the diagonal \nis independent from n.  A stochastic way to formulate the idea behind our assumption is the following. \nWhen considering k random substrings of size n in a corpus of strings representative of the language, \none .nds ak on average that n2 of them correspond to a single nonterminal. That is, by doubling the size \nof the substring considered, it will be four times less likely to be parsable. This condition agrees \nwith experience. In order for the assumption to be satis.ed, it is also suf.cient to assume that the \nparse trees corresponding to the inputs are balanced. That is, if every node in the chart corresponding \nto a substring of size n is combinable with exactly one other symbol of size n (precluding ambiguity), \nthen the assumption is veri.ed. In fact the assumption can be relaxed as follows. Consider the triangle-shaped \nsubchart T n which touches the diagonal and a non-terminal A at distance k from it. We assume all symbols \nin the triangle but closer to the diagonal combine to form A. If the symbol A can be combined with exactly \none other symbol of size \u00dfk with 0 < \u00df = 1, it will yield exactly one symbol at distance log(n) (1 + \n\u00df)k. Inductively we compute that is of the order of log(1+\u00df) nodes in the triangle, which is compatible \nwith our condition, with a = 1/log2(1 + \u00df). k Experiments The assumption we make is not strictly speaking \nveri.able experimentally, since for any chart there exists an a such that the assumption is veri.ed. \nHowever, one can gain con.dence in the assumption by plotting the probability of a string to be parsable \nagainst its size. One should observe that this probability decreases with the square of the size. In \npractical terms, given a chart corresponding to a large input, if one observes a drastic cut-off in the \ndensity of non-zero elements when departing from a certain distance from the diagonal, then the input \nis compatible with our assumption. In Fig. 4, we show a chart corresponding to a fragment of C code, \nobtained using our algorithm. This chart, along with all other inputs for which we have run this experiment, \nexhibits the expected features. The assumption is also con.rmed, albeit indirectly, by observing that \nthe cost analysis which depends on it holds in practice. Non-suitable inputs Any input which uses nesting \nin linear pro\u00adportion to the size of its input will violate our assumption. For ex\u00adample, the lisp program \ncomposed of n successive applications of  1. Both A and B touch the diagonal. We have p = O(log n) and \nq = O(log n) and thus Mn(A, B) = O(log2 n). 2. Either A or B is at least n away from the diagonal. In \nthis case there either p or q is bounded by a constant and thus Mn(A, B) = O(log n). 3. Both A and B \nare at least n away from the diagonal. In this case there both p and q are bounded by a constant and \nthus Mn(A, B) = O(1).  Before proceeding we recall a standard result: Theorem 1 (Master Theorem, Cormen \net al.). Assume a function Tn constrained by the recurrence Tn = aT bn + f (n) Figure 4. The chart corresponding \nto a fragment of a C program. The input program can be found in appendix. Two remarkable features merit \ncommentary. First, the staircase shapes, which are explained in Sec. 4.4. Second, some small sub-matrices \nnear the diagonal appear to be dense. These regions correspond to argument lists in the C program, and \nthis iteration structure is implemented by linear recursion rather than our special encoding of Sec. \n4. cons does not satisfy our assumption. (cons x (cons x (. . . (cons x nil) . . .))) It appears however \nthat few programs are written in this style, except perhaps for machine-generated ones. Linear constructions \nare often present, but they are then supported by special syntax. (Such an equation will typically come \nfrom a divide-and-conquer algorithm, where a is the number of sub-problems at each recursive step, n/b \nis the size of each sub-problem, and f(n) is the running time of dividing up the problem space into a \nparts, and combining the sub-results together.) c If we let e = logb a and f(n) = O(n logd n), then Tn \n= O(n e) if c < e c Tn = O(n logd+1 n) if c = e Tn = O(n c logd n) if c > e 3.2.2 Cost of the Conquer \nStep We proceed to estimate the running cost Vn of the valiant function V on a matrix of size n. We do \nso assuming that we know the resulting chart Y = V (A, X, B). That is, Vn maps Y to the cost of running \nV (A, X, B). We have the following recurrence: Vn(0) = 0 Y11 Y12 Vn= V(Y21) + V(Y11) + V(Y22) + V(Y12) \nnnnn Y21 Y22 2222 Indeed the above lisp program is invariably written as: + M (A12, Y21) + M(Y21, B12) \nnn 2 2 (list x x . . . x) + M (A12, Y22) + M(Y11, B12) nn 2 Hence we provide special treatment for such \nspecial iteration syn\u00ad 2 Because A and B are upper-triangular matrices, the subcharts A12taxes. We show \nin Sec. 4 how to deal with them, while respecting and B12 touch the diagonal. We distinguish two cases: \neither Y our assumption. touches the diagonal or it is at least at a distance n from it.  3.2 Cost \nEstimation For simplicity we consider only inputs of sizes which are powers of 2. This implies that we \nonly need to consider square matrices in our analysis. We will estimate the cost as the number of elementary \nmultiplications (multiplications on sets of non-terminals) to be performed. We .rst remark that because \ncharts are always divided in the middle, a subchart X considered by the algorithm is always square, and \nat a distance kn to the diagonal, where k is some natural number and n is the dimension of X. When k \n= 0 we say that Fn Y is far away In the latter case we pose Fn = Vn (F for far), and all sub-matrices \nof Y are far from the diagonal. Hence all matrix multiplications involve at most one matrix touching \nthe diagonal, and the total combined cost of multiplications is O(log n). The recurrence specializes \nthen to: Fn(0) = 0 F1(Y ) = 1 Y11 Y12 = F(Y11) + F(Y12) + F(Y21) + F(Y22) nnnn Y21 Y22 2222 X touches \nthe diagonal and when k > 0 we say that X is far from the diagonal. This distinction is crucial, because \nmatrices touching the diagonal have O(log n) elements in them, whereas matrix far away have a constant \nnumber of elements in them. 3.2.1 Cost of Matrix Multiplications We start by estimating the cost of \nthe multiplication of two matrices A and B of size n, Mn(A, B). Assuming that p = #A and q = #B, then \nmatrix multiplication needs to perform O(pq) elementary multiplications. Indeed, each non-zero element \nin A needs to be multiplied at most with every non-zero element in B. We now assume that A and B are \nalso a subcharts, and distinguish the following cases. + O(log n) Because Y has a constant number of \nnon-zero elements, most re\u00adcursive calls will return immediately, and on average only one re\u00adcursive \ncall need to be counted. Hence we use the Master Theorem with a = 1, b = 2 and f(n) = O(log n). We are \ntherefore in the case c = e, and obtain Fn = O(log2 n). Y touches the diagonal In the former case, let \nVn = Cn (C for close), and Y21 touches the diagonal. Hence two of the matrix multiplications involve \npairs of matrices at the diagonal, and the combined cost of multiplications is O(log2 n). Only the recursive \ncall involving Y12 touches the diagonal, and the three others involve matrices far from it. Therefore \nthe recurrence specializes to: 200 C1 = 1 2 Cn = C + 3F+ O(log n) nn 2 2 2 n + O(log2 n) 150= C We use \nthe Master Theorem with a = 1, b = 2 and f(n) = O(log2 n). We are therefore in the case c = e, and obtain \nCn = O(log3 n).  3.2.3 Total Cost We can proceed to compute the total cost of our algorithm on an input \nstring of size n = |w|, again using the Master Theorem. We always divide the input into two parts, so \nb = 2. We assume that the input is provided as a balanced tree representing the matrix I(w), and so the \ncost of the divide step is zero. Therefore f(n) is the cost of the conquer step only. This step involves \na matrix close to the diagonal, so f(n) = Cn = O(log3 n), and in turn c = 0 and d = 3. If we assume \na sequential execution of sub-problems then a = 2. In turn, e = 1 and T (n) = O(n).  If we assume perfect \nparallelisation of sub-problems, or an incremental situation, where one of the sub-solution can be reused, \nthen a = 1. In turn, e = 0 and T (n) = O(log4 n).  Valiant s evaluation for Vn is O(n . ), for some \n. between 2 and 3 (the exact value depends on the matrix multiplication algorithm used). In his case \nc = . and d = 0, yielding T (n) = O(n . ) in all cases: according to Valiant s original analysis, making \nan incremental or parallel version of his algorithm would lead no bene.t, while our analysis reveals \nthat a big payoff is at hand.  3.2.4 Experiments We have conducted two sets of experiments on the running \ntime of the algorithm. All timings were obtained using the C RI TER I O N library (O Sullivan 2013), \non an Intel Core 2 at 2.13GHz. All pro\u00adgrams were compiled with GHC 7.6.1. In the .rst set, we have mea\u00adsured \nthe performance on a practical language on practical inputs, to con.rm that the function is fast enough \nto use as an incremental parser in an interactive setting. To do so, we have run our BNFC implementation \non a C grammar to produce the s and (\u00b7) functions, and tested the running time of the V function on a \nlarge C program, extracted from the Linux kernel scheduler (https://github. com/torvalds/linux/blob/master/kernel/sched/core.c \npreprocessor directives as well as typedefs found in it were expanded by hand.) The input was divided \ninto a left part and a right part of equal sizes, and a middle symbol. The complete charts for the left \nand the right part were computed, then we measured the time of the V function on the charts and the singleton \nchart contain\u00ading the middle symbol. After collecting 100 samples, C RI TER I O N reported a mean runtime \nof 320.1469 \u00b5s, with a standard deviation of 23.06691 \u00b5s. This is well within acceptable limits for interactive \nuse: most people cannot perceive a delay less than a millisecond. In the second set of experiments, we \ntested the V function on generated inputs of various sizes, to con.rm our calculation of the worst case \nrunning time. The grammar is that corresponding to the encoding of t * (the nonterminal t repeated an \narbitrary number of times) using the technique described in Sec. 4 (which ensures that our assumption \nis veri.ed with a close to 1). The inputs were a repetition of that terminal symbol. The results are \nshown in Fig. 5. We observe that the measurements, when drawn on a semi-logarithmic scale, .t a quadratic \ncurve. The observed running therefore appears better than the theoret\u00adical cost estimation, which predicts \na cubic curve. The best expla- Running time [\u00b5s] 100 50 0 Input size Figure 5. Running time of the V \nfunction in function of the size of the input, using semi-logarithmic scale. The grammar is that corresponding \nto the encoding of t * using the technique described in Sec. 4. The next data point (input size 223) \ncould not be obtained due to running out of memory. The curve is the graph of a quadratic function which \n.ts the measurements. nation we have for this discrepancy is that we made a pessimistic assumption in \nour analysis: we have estimated the cost of the ma\u00adtrix product to be at worst O(log2 n) if each argument \nmatrix has O(log n) elements. However, this occurs only if the elements are lined up so that every pair \nneeds to be considered. This case may occur in practice, but we conjecture that it happens so infrequently \nthat it does not contribute signi.cantly to the total running time. This experiment thus suggests that \na more precise analysis of the algorithm can be done, which we leave for future work. 4. Iteration in \nContext-Free Grammars 4.1 The Problem With Iteration While we have worked hard to ensure the ef.cient \nhandling of the non-associative aspect of CF parsing, we have neglected so far that most CF languages \nfeature regular iteration; that is, associative concatenation rules. Without special treatment, such \nassociative rules cause severe inef.ciencies in the algorithm as presented so far. Iteration is technically \nknown as Kleene star, and is written here as a post.x *. In context-free grammars, it can be (and usually \nis) encoded as either as left or right recursion. For example a rule A ::= Y * is typically encoded as \nfollows. A ::= E A ::= AY The problem with this encoding is two-fold. First, inputs consisting mostly \nof a sequence of Y necessarily violate our assumption on inputs: the depth of the parse tree grows linearly \nwith the size of the input. Second, the generated AST will necessarily be linear. Conse\u00adquently, as we \nhave seen in the introduction, this linear shape would preclude ef.cient parallel or incremental processing \nof the AST by computations consuming it.  Figure 6. Matching a list using the oracle-sensitive algorithm. \nWe assume that only one non-terminal Y is involved and thus show only the bit-tags. Considering only \nthe non-terminals which cannot be combined using the rule Y ::= Y 0Y 1, the charts features a sequence \nof Y 1 (of increasing size), followed by a sequence of Y 0 (of decreasing size). One could possibly imagine \nworking around the .rst problem with creative algorithmic devices. However it is clear that the sec\u00adond \nproblem is intrinsic to the encoding of iteration as linear recur\u00adsion. Hence we take the stance that \nspecial support for iteration is necessary in any parallel or incremental parser.  4.2 Towards an Ef.cient \nEncoding Instead of a linear, unary encoding of iterations, one can attempt a binary tree encoding. One \nmight propose the following encoding: A ::= AA A ::= Y However this encoding accepts all possible associations \nof se\u00adquences of Y s, in particular also linear ones. One might attempt to mend the rules by using a \nmore clever encoding, say: Ak+1 ::= AkAk Ignoring that it codes only lists of size 2n for some n, our \nsecond condition on inputs is still be violated. Indeed, in a sequence of Y , any subsequence of length \n2n for some n will be recognized. This means that there will be a lot of overlap between possible parse \ntrees. In the remainder of the section we describe a way to keep the rule A ::= AA, but tweak the parsing \nalgorithm so that for any sequence of Y s only a single association is considered.  4.3 Oracle-Sensitive \nParsing Overview Each nonterminal will come with a bit indicating whether it should be used either as \na left or right-child in the parse tree. The bit will be chosen by an oracle upon production of the nonterminal, \nso that the tree will be balanced. We write Y b for the non-terminal Y annotated with bit b. The main \nrule constructing trees is then written: Y 1 Y ::= Y 0 This restricts which trees are explored. After \nparsing with this rule, we obtain a sequence of Y 1 (unmatched right children) of growing size followed \nby a sequence of Y0 (unmatched left children), as depicted in Fig. 6. These nodes will then be collected \nusing special rules. Assuming that C0 and D0 delimit the list of non-terminals Y *, the collecting rules \nwould be written: C ::= C0 D ::= D0 ::= C Y 1 ::= Y 0D And the .nal list can be produced by the rule \nL ::= C D. The delimiters C0 and D0 are necessary so that only one col\u00adlection of Y 1 and only one collection \nof Y 0 are needed; thereby ensuring a good performance. Without delimiters, every combina\u00adtion of sequences \nof Y 1 and Y 0 would need to be considered. An intermediate situation is where only one delimiter is \npresent, say the opening one. In that case, only one list of Y 1 is considered, but many sequences of \nY 0 would be considered. Oracle-Sensitive Grammar Formalism In general, we extend productions so that \nnon-terminals on a right-hand-side are tagged with a bit. Formally, we extend the syntax of the productions \nas follows, where b1, b2, . . . range over bits: A ::= Bb1 Cb2  A ::= t, for t . S  We allow, as a \nshorthand, to write non-annotated non-terminal in a production right-hand-side. The production then stands \nfor a pair of rules with either annotation. That is A ::= a0Ba1 is a shorthand for the pair of rules \nA ::= a0B0a1 and A ::= a0B1a1. Algorithm An implementation will take a grammar written using a special \nconstruction for iteration and translate it to the above formalism appropriately. The parsing algorithm \nper se remains the same as previously, except for the operator combining non\u00adterminals, which is changed \nas follows. De.nition 9. b b1 b2 b2 x \u00b7 y = {A| B. x, C . y, A ::= Bb1 C . P } where the output bit b \ncomes from the oracle. The transitive closure function modi.ed to use the above ver\u00adsion of the (\u00b7) operator \nis called T in the remainder. Formalization and proof We proceed to prove that the above implementation \nindeed recognizes the intended language. Firstly, we must de.ne the meaning of our extended grammar formalism \nand show that it corresponds to our needs. The main issue is that the algorithm behaves non-deterministically, \nin the sense that the grammar-writer does not have access to the bits generated by the oracle. The rest \nof the section is structured as fol\u00adlows: 1. we de.ne a generation relation restricted to a given source \nbits .;  2. we show that the algorithm decides the relation for a speci.c (but intangible) .;  3. we \nnarrow the acceptable grammars to those which are oblivi\u00adous to . (describe languages independent of \n.);  4. we provide a toolkit which enables to identify and construct oblivious grammars;  5. and .nally \nwe show that our encoding of iteration preserves obliviousness.  . Oracle We de.ne a new generation \nrelation -., indexed by a stream of bits .. This stream of bits wholly models the oracle. The meaning \nof production rules annotated with bits can then be given. We .rst de.ne a 1-step generation relation \nindexed by a single bit. De.nition 10 (bit-indexed generation). b if (A ::= Bb1 Cb2 ) . P , then w0Aba \n-. w0Bb1 Cb2 a  b,. if (A ::= x) . P , then w0Aba -. w0xa Crucially, the rules require the relation \nto act on the .rst non\u00adterminal in a string. This forces the bit-stream . to be used in a deterministic \nway. Otherwise, the relation could use each bit of . in a arbitrary place, essentially bypassing the \ninstructions of the oracle, transmitted via the bitstream .. . De.nition 11 (stream-indexed generation). \nThe relation a -. w is inductively de.ned as follows. . w -. w b.b,. If a -. . and . -. w then a -. w \n. Algorithm The algorithm decides the -. relation, but only for one particular bit-stream . (which the \ngrammar-writer has no con\u00adtrol over). Theorem 2. . 1. if Ab . T (I(w))ij then ...Ab -. wij .  2. if \nAb . T (I(w))ij then ...Ab -. wij  Proof. By induction on the decomposition structure of the matrix \n(done by T ). Obliviousness Ultimately, we do not want the language de.ned using our formalism to depend \non the actual stream . of bits generated by the oracle, since this is out of the control of the grammar \nwriter. That is, if a string is recognized using some ., it should be recognized with every .. We .rst \nremark that the set of strings generated by any given tagged non-terminal will always depend on .. Hence \ninstead we have to consider the strings generated by sets of non-terminals (and in general sets of strings). \nWe thus de.ne the following relations, using G, . and . to range over sets of strings. De.nition 12. \n. . G -. w iff. ....a . G. a -. w . . G -. w iff. ....a . G. a -. w De.nition 13. A set of strings G \nis called oracle-oblivious if the set of strings of terminals generated by it is insensitive to non\u00ad \n.. determinism; that is, for any w0, if G -. w0 then G -. w0. De.nition 14. We note A the set {A0, A1}. \nDe.nition 15 (well-formed grammar). A oracle-sensitive gram\u00admar is well-formed if S is oracle-oblivious. \nWe can then show that obliviousness ful.lls its purpose: the sensitivity to . introduced in the algorithm \nis indeed hidden by obliviousness. Theorem 3. If A is oracle-oblivious then .b A -. wij iff A. T (I(w))ij \n, for some bit b Proof. The left-to-right direction is the contrapositive of Th. 2.2. The right-to-left \ndirection is obtained by composing Th. 2.1 with the de.nition of obliviousness for A . A kit for well-formed \ngrammars Given a grammar de.nition using bit-annotations arbitrarily, it is hard to decide whether it \nis well-formed. Hence we de.ne the following relation, which enables us to reason about obliviousness \ncompositionally. De.nition 16. G =* . . iff for every w0, .. if G -. w0 then . -. w0. .. if . -. w0 then \nG -. w0. The above relation is constructed to transport obliviousness: Lemma 1. If G =* . . and . is \noracle oblivious, then so is G. Proof. Direct consequence of the de.nition. Lemma 2. 1. =* . is re.exive \nand transitive ** * 2. If G =. . then G. =. .. and .G =. .. 3. Assume a non-terminal A and G its set \nof productions. Then  * A =. G. Proof. 1. and 3. are a direct consequences of the de.nitions. The proof \nof 2. is tedious but straightforward, and similar in style to the proof of Lem. 3 and thus omitted. The \nabove lemma means that, if productions are written without bit annotations (they generate all possible \nannotations), then they preserve obliviousness. Hence, a grammar written without annota\u00adtions is necessarily \nwell formed. Because our encoding of iteration also preserves obliviousness, this in turn means that, \nif one uses an\u00adnotations only to encode iteration in the pattern we prescribe, the grammar will be well-formed. \nEncoding iteration As a reminder, we encode L ::= C0Y0 * D0, as Y ::= Y0 Y 1 ::= Y 0 C ::= C0 1 ::= C \nY D ::= D0 0 ::= Y D L ::= C D * Y * Theorem 4. L =. C 0 0 D0 Proof. We construct the relation in the \nfollowing stages. 1. L C 0{Y 1} * {Y 0} * 2. D0 Y * 3. C0 D0 Y * 4. C0 0 D0 Most of the steps are consequences \nof Lem. 2. Only the step between stages 2. and 3. requires special treatment: it depends on the relation \n1 0 * * {Y } * {Y } * =. Y Proving it requires to preservation lemmas for every w0: .Y * . if {Y 1} \n* {Y 0} * -. w0 then -. w0. Y *..  if -. w0 then {Y 1} * {Y 0} * -. w0.  The .rst one is an easy \nconsequence of the ability to chose any . possible . in the -. relation. The second one is the angular \nstone of our method, and is proved in the following lemma. . Lemma 3. Let w . S * and a . Y *. If a -. \nw then there exists . \u00df . {Y 1} * and . . {Y 0} * such that \u00df. -. w.  Proof. By induction on the length \nof a. If a is in the required form, we have the result. If not, then the subsequence Y 0Y 1 can be found \nat least once in a: a = a0Y 0Y 1a1 We can decompose w into two parts w0 and w1 such that . a0 -. w0 0 \n1 . Y Y a1 -. w1 b. But, for any b, we have Y ba1 -. Y 0Y 1a1. Therefore, Y ba1 -. . w1 and in turn a0Y \nba1 -. w. We can then use the induction hypothesis on a0Y ba1 to obtain \u00df and . satisfying the conditions \nof the theorem.  4.4 Performance The above encoding yields good performance in practice, even with a \nstraightforward implementation of the oracle providing the stream of bits .. Indeed, Fig. 6 shows the \nchart generated from a sample C program. It exhibits the drastic cut-off in non-zero node density formalized \nin Def. 8, except for a few linear shapes, as one can observe. These are caused by our implementation \nof the oracle, which is naive. In our implementation, the bit which is generated is a parameter of the \nfunction V , and it is .ipped (deterministically) for some recursive calls. This means that, inside a \ngiven subchart, all instances of associative rules either right\u00adassociate or left-associate, yielding \na linear arrangement of results in the chart. Yet, this strategy for bit generation is the best we have \nfound with respect to observed performance. The reason might be that more even distributions of results \nin the chart worsens the locality of non-zero data, yielding smaller zero subcharts. 5. Related Work \n5.1 Our Own Previous Work Claessen (2004) wrote a paper titled parallel parsing processes , but which \nhas only tenuous connections with the present work. The paper of 2004 presents a parsing technique based \non usual sequen\u00adtial parsers, but where disjunction is represented by processes run\u00adning concurrently. \nAn advantage of that technique is that the parser processes the input string in chunks that can be discarded \nas soon as the parser has analyzed them. Bernardy (2009) has shown how to combine the above idea with \nthe online parsers of Hughes and Swierstra (2003). This makes the resulting parsing algorithm suitable \nfor incremental parsing in an editing environment such as Yi (Bernardy 2008). However the method is brittle, \nbecause grammars need to be expressed in a special-purpose formalism, and error-correction must be bake-in \nthe grammar. In contrast, the method presented here accepts gram\u00admar in Backus-Naur Form (see Sec. 6.6); \nonly iterative structures need to be changed to use the special construction of Sec. 4. One does not \nhave to worry about error recovery since all substrings are parsed. 5.2 Special Support for Iteration \nThe assumption we make on inputs, which is tied to the balancing of the parse trees is partially inspired \nby work by Wagner and Gra\u00adham (1998). They show that linear parse trees cannot be handled ef.ciently, \nsince updating a structure requires time proportional to its depth. Wagner and Graham then deduce that \nef.cient incremen\u00adtal parsing requires a special purpose support for iteration, as we have done in Sec. \n4. 5.3 General CF Parsing Perhaps the most well known method for parsing general CF lan\u00adguages is that \nof Tomita (1986). This method has in common with ours that it achieves linear performance on well-behaved \ninputs, while degrading gracefully to the best possible performance (cu\u00adbic) in the worst case. The main \ndifference between the methods is that Tomita s algo\u00adrithm processes the input sequentially, while we \ncan process it any bottom-up order. This means that the condition for well-behaved inputs is different \nfor either methods. In Tomita s case, the condi\u00adtion is that, at any point during the parsing, the amount \nof ambiguity is small (bound by a constant), implying that the next action of the parser is most of the \ntime determined by the next symbol in the in\u00adput. In our case, it is captured by Def. 8, which essentially \nmeans that the input should be hierarchical. Tomita s condition does not imply ours: linearly arranged \ninputs can be deterministic. Check\u00ading the other implication is left for future work. It is not easy \nto conclude: our condition imposes non-local conditions which may or may not restrict non-determinism \nin a linear processing of the input. The chief advantage of our method is its divide-and-conquer structure, \nwhich means that is can be used in a standard parallel or incremental framework. Tomita inherits essential \nuse of the sequential processing of the input from LR parsing, making his technique not amenable to parallelization. \n 5.4 Parallel Parsing There is a wealth of previous work devoted to ef.cient recogni\u00adtion and parsing \nof context-free languages on abstract parallel ma\u00adchines, so much that a comprehensive survey of the \n.eld does not .t in this paper. The situation can however be summarized as fol\u00adlows: to the best of our \nknowledge, before this work, algorithms proposed for parallel parsing either need an unrealistic number \nof processors, or they target a language class which is too restrictive to be of practical interest. \nToo many processors Sikkel and Nijholt (1997) describe a paral\u00adlel algorithm (in section 6.3) which can \nrecognize a string of length n in O(log n) time, but it requires O(n 6) processors in the worst case. \nA line of work involving Rytter gives a dozen of complexity results for various sub-classes of CF and \nvarious abstract machines. The most closely related results are perhaps the following. Chytil et al. \n(1991) present a simple parallel algorithm recog\u00adnizing unambiguous context-free languages on a CREW \nPRAM in time log2 n with only n 3 processors. The similarity with our work is that the authors restrict \nthe languages they accept to a well\u00adbehaved subset of CF to obtain sensible running time. In our opin\u00adion \nthe present work captures better the actual sets of inputs found in the actual practice of CF parsing. \nToo restrictive grammars Rytter and Giancarlo (1987) analyze an algorithm which can parse a bracket grammar \nin O(log n) time and O(n/ log n) processors. This is fast and does not use too many processors, but is \nrestricted to languages where the grouping of non-terminals is completely explicit in the input: each \nproduction rule starts with an opening bracket and ends with a closing bracket. 6. Discussion 6.1 Destructive \nUpdates We were tempted to solve the problem of iteration by using destruc\u00adtive updates. That is, to \nhave associative rules such as Y ::= Y Y consume their arguments. That is, when a Y non-terminal is added \nto the chart using the above rule, the two Y non-terminals that com\u00adpose it would be removed. We have \nattempted this solution, but  faced a couple of issues, which will not surprise an audience of 6.5 Unexploited \nIncrementality functional programmers. On the theoretical side, reasoning about parsing with destruc\u00adtive \nupdates of the chart has proven too complicated to .t in this paper. The generation relation describing \nwhich strings are recognized by a parser is hard to de.ne, let alone reason about. A major dif.culty \nis to combine destructive updates with a no\u00adtion of non-determinism similar to that described in Sec. \n4. In\u00addeed, the user has no control on which particular consuming rule will .re .rst, since this depends \non the particular of the im\u00adplementation of Valiant s algorithm (the order in which matrix multiplications \nare run, etc.) and the exact positioning of the substrings.  On the practical side, the presence of \nupdates makes for a more complicated implementation. It would also mean to abandon (so far unexploited) \nparallel opportunities in the matrix multiplica\u00adtion and the V function.   6.2 Optimization In many \ngrammars, a fair proportion of non-terminals occur only either on the left, or on the right of binary \nproductions. Assume for example that A only ever occurs on the left. It is wasteful in this case to consider \nA for right-combinations, as does the algorithm we have presented so far. This optimization is available \nto many CF parsing algorithms, but it is especially useful to us, because it acts in synergy with the \ndetection of empty matrices. Indeed, by having separate matrices of left-combinable and right-combinable \nnon-terminals, each matrix will be sparser. This means some combinations can be discarded in blocks, \nthat is, at the level of matrices instead at the level of individual non-terminals. An additional bene.t \nof this optimization is that it pays for the cost of tagging non-terminals with an extra bit, as we suggest \nin Sec. 4. Indeed, 0-tagged non-terminals occur only on the left of binary productions, and 1-tagged \nnon-terminals occur only on the right in our encoding of iteration. Therefore this optimization eliminates \nall the cost of tagging: instead of tagging a non-terminal with a bit, it suf.ce to insert it only in \nthe relevant matrix. 6.3 Implementation An implementation of the parsing method presented here, includ\u00ading \nspecial support for iteration as presented in Sec. 4 and the opti\u00admization presented above, is implemented \nas a new back-end for the BNFC tool, (Forsberg and Ranta 2012) available in version 2.6. The tool takes \na grammar in BNF with annotations for ef.\u00adcient repetition. When running the tool with the option -cnf, \nit produces a Haskell implementation of CNF tables and an instance of the Valiant s algorithm using it. \nAs other BNFC back-ends, our implementation produces full parsers, not mere recognizers. 6.4 Unexploited \nParallelism The parallelisation that we suggest can take advantage of at most a number of processors \nproportional to the length of the input. When parsing using Valiant s algorithm, there is more parallelism \nto take advantage of (for example two of the recursive calls in the V function are independent from each \nother). However, running in parallel all recursive calls to V would require asymptotically more processors \nthan the length of the input. We do believe that this is not a reasonable assumption to make when parsing \na whole input. However, in the case of incremental parsing, where only a tiny fraction of the input will \nbe re-parsed, one might want to take advantage of such extra parallelism opportunities. We have suggested \nthat the incremental version of the parser should run the V function O(log n) times when changing one \nsymbol in the input. In fact, it might be possible to use a better implementation of the chart data structure, \nwhich would support an incremental update with a single run of the V function. Indeed, when changing \na single symbol of the input, only the part of the chart which depends on that symbol (the square whose \nbottom-left corner is the symbol in question) needs to be recomputed. This improved re-use of results \nis left for future work. 6.6 Chomsky Normal-Form Even though we assume that we transform the grammar \nto CNF for ease of presentation, this is not actually the best form to use in an implementation. In fact, \nit is better to convert the grammar to 2NF (where productions have at most 2 symbols) and derive the \noperations (\u00b7) and s using a slightly modi.ed algorithm, using the method described by Lange and Lei\u00df \n(2009), as we have done in our implementation. The conversion from Backus-Naur Form (BNF) to CNF (or \n2NF) involves a division of long productions into binary ones. This is usually done by chaining the binary \nrules linearly. If the productions of the input grammar are long, this impacts negatively the performance \nof our algorithm, which performs best on balanced inputs. Fortunately it is not dif.cult to divide long \nproductions into a balanced tree of binary rules. The CNF grammar is suitable not only for recognition \nof lan\u00adguages, but also for parsing: the parse trees obtained by the con\u00adverted grammar are essentially \na binarization of the trees obtained by the grammar in BNF. The aspect which cannot be preserved by the \nconversion is the presence of cycles of unit rules. However, the elimination of such cycles can only \nbe seen as a bene.t: they intro\u00adduce an unbounded amount of ambiguity in the grammar, and are a symptom \nof a mistake in the grammar speci.cation.  6.7 A New Class of Languages The assumption we make on the \ninput (depending on a constant a), de.nes implicitly a new class of languages. The class lies between \nregular and context-free languages. We call the class a-balanced context-free languages, or BCF(a). The \nuse of the parameter a contrasts with that of the parameter k in classes such as LL(k) or LR(k). While \nLL(k) or LR(k) restricts the form that a CF grammar can take, BCF(a) does not. Instead, it restricts \nthe form that the strings of the language can take as a whole. We have found that for a given grammar, \nprograms are written with a shallow nesting structure, instead of a deep one (with the ex\u00adception of \nregular iteration) and hence we have anecdotal evidence that any given programming languages is a member \nof BCF(a). Together with observation that the parsing problem for BCF(a) is simpler than that of general-context \nfree languages, this makes BCF(a) worthy of study. In fact, because the assumption we make is not one \nwhich is enforced by usual CF grammars, but we still observe it to hold in practice, it must mean that \nthe assumption is self-imposed by the writers of these inputs, namely programmers. This is not too sur\u00adprising, \nas our assumption can be violated only by programs which exhibit an amount of nesting comparable to the \ntotal length of the input. As folklore goes, programmers are adverse to deeply-nested constructions. \nIndeed, understanding a program with n levels of nesting requires to remember n levels of context. The \nlink between the ability for a computer to ef.ciently parse an input in parallel and incrementally and \nfor a human to do so is intriguing, and we hope that the present paper sheds an interesting light on \nit.  6.8 Generalization The body of the paper does not depend on the particulars of CF recognition: \nwe abstract over it via an arbitrary association oper\u00adator. This means that other applications can be \ndevised. A natural extension is to support CF parsing, as we have done in our im\u00adplementation. More exotic \nextensions are also possible. A .rst ex\u00adample would be to support symbol tables, which are necessary \nfor proper parsing of C. In this extension, non-terminals would be as\u00adsociated with two symbol sets, \none that they assume comes from the environment and one which they provide to the environment. The combination \noperator would reconcile these two sets. A sec\u00adond example is probabilistic parsing. Here, a probability \nwould be associated with each non-terminal and production rule, and the as\u00adsociation operator would simply \nmultiply the probabilities. In fact, our method can be seen as a general way to turn a non-associative \noperator into an associative one by computing all possible associations. The ef.ciency is recovered by \nthe ability to .lter out most of the results; either because the original operator discards them, or \nbecause there is (possibly hidden) associativity which can be taken advantage of.  6.9 The Old as New \nIt strikes us that a parsing algorithm published in 1975 .nds an ap\u00adplication in the area of parallelisation \nfor computer architectures of the 2010 decade. Further, Valiant gives no indication that the algo\u00adrithm \ndescribed should .nd any practical parsing application. As it seems, he aims only to tie the complexity \nof context-free recogni\u00adtion to that of matrix multiplication (via the transitive closure oper\u00adation). \nIndeed, in the case of parsing (in contrast to mere recognition), subtraction of matrices is not de.ned. \nHence one cannot use the ef\u00ad.cient Strassen algorithm (Strassen 1969) for multiplication, and in turn \nthe complexity of general context-free parsing using Valiant s method is cubic, and fails to beat the \nsimpler CYK algorithm. Our contribution is to recognize that Valiant s algorithm per\u00adforms well for parsing \npractical inputs, given a special handling of iteration and a sparse matrix representation (even when \nusing the naive matrix multiplication algorithm). If we also account for the ease of making parallel \nand incremental implementations of the al\u00adgorithm thanks to its divide and conquer structure, we must \nclassify Valiant s algorithm as a practical method of parsing. In fact, Valiant s algorithm offers such \na combination of sim\u00adplicity and performance that we believe it deserves a prominent place in textbooks, \non par with LALR algorithms. 7. Conclusions At the start of this work, we set out to .nd an associative \nopera\u00adtor with sub-linear complexity that could be used to implement a divide-and-conquer algorithm for \nparsing. The goal was to obtain a parallelizable parsing algorithm that would double as an incremen\u00adtal \nparsing algorithm. We managed to .nd such an operator, but the desired complexity only holds under certain \nassumptions that luck\u00adily do seem to hold in practice. The conditions hold when the recur\u00adsive nesting \ndepth of a program text only grows, say logarithmically in terms of the total length of the program. \nAn unanticipated result of our work is thus the de.nition of a new class of languages. We were also forced \nto come up with a special way of dealing with iteration (frequently occurring in grammars) so it would \nnot break this practical assumption. Acknowledgments The proof-method used in the presentation of Valiant \ns algorithm was suggested by Patrik Jansson. Engaging discussions about the complexity of Valiant algorithm \nwere conducted with Devdatt Dub\u00adhashi. Peter Ljungl \u00a8 of pointed us to some most relevant related work. \nThomas Ba\u00b0\u00b0oblom, Darius Blasband, Peter Ljungl \u00a8 ath Sj \u00a8of, as well as anonymous reviewers, gave useful \nfeedback on drafts of the paper. This work has been partially funded by the Swedish Foundation for Strategic \nResearch, under grant RAWFP. References L. Allison. Lazy Dynamic-Programming can be eager. Information \nPro\u00adcessing Letters, 43(4):207 212, 1992. J.-P. Bernardy. Yi: an editor in Haskell for Haskell. In Proc. \nof the .rst ACM SIGPLAN symposium on Haskell, pages 61 62. ACM, 2008. J.-P. Bernardy. Lazy functional \nincremental parsing. In Proc. of the 2nd ACM SIGPLAN symposium on Haskell, pages 49 60. ACM, 2009. R. \nBird and O. de Moor. Algebra of programming. Prentice-Hall, Inc., 1997. N. Chomsky. On certain formal \nproperties of grammars. Information and control, 2(2):137 167, 1959. M. Chytil, M. Crochemore, B. Monien, \nand W. Rytter. On the parallel recognition of unambiguous context-free languages. Theor. Comp. Sci., \n81(2):311 316, 1991. K. Claessen. Parallel parsing processes. J. Funct. Program., 14(6):741 757, 2004. \nJ. Cocke. Programming languages and their compilers: Preliminary notes. Courant Institute of Mathematical \nSci.s, New York University, 1969. T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein. Introduction \nto algorithms, second ed. MIT press, 2001. M. Forsberg and A. Ranta. BNFC Quick reference, chapter Appendix \nA, pages 175 192. College Publications, 2012. R. J. M. Hughes and S. D. Swierstra. Polish parsers, step \nby step. In Proc. of ICFP 2003, pages 239 248. ACM, 2003. T. Kasami. An ef.cient recognition and syntax \nanalysis algorithm for context-free languages. Technical report, DTIC Document, 1965. M. Lange and H. \nLei\u00df. To CNF or not to CNF? an ef.cient yet presentable version of the CYK algorithm. Informatica Didactica \n(8)(2008 2010), 2009. B. O Sullivan. The Criterion benchmarking library, 2013. W. Rytter and R. Giancarlo. \nOptimal parallel parsing of bracket languages. Theor. computer science, 53(2):295 306, 1987. K. Sikkel \nand A. Nijholt. Parsing of context-free languages, pages 61 100. Springer-Verlag, 1997. V. Strassen. \nGaussian elimination is not optimal. Numerische Mathematik, 13:354 356, 1969. 10.1007/BF02165411. M. \nTomita. Ef.cient Parsing for Natural Language. Kluwer Adademic Publishers, 1986. L. Valiant. General \ncontext-free recognition in less than cubic time. J. of computer and system sciences, 10(2):308 314, \n1975. T. A. Wagner and S. L. Graham. Ef.cient and .exible incremental parsing. ACM Transactions on Programming \nLanguages and Systems, 20(5): 980 1013, 1998. D. Younger. Recognition and parsing of context-free languages \nin time n. Information and control, 10(2):189 208, 1967.     \n\t\t\t", "proc_id": "2500365", "abstract": "<p>We present a divide-and-conquer algorithm for parsing context-free languages efficiently. Our algorithm is an instance of Valiant's (1975), who reduced the problem of parsing to matrix multiplications. We show that, while the conquer step of Valiant's is <i>O</i>(<i>n</i><sup>3</sup>) in the worst case, it improves to <i>O</i>(log<i>n</i><sup>3</sup>), under certain conditions satisfied by many useful inputs. These conditions occur for example in program texts written by humans. The improvement happens because the multiplications involve an overwhelming majority of empty matrices. This result is relevant to modern computing: divide-and-conquer algorithms can be parallelized relatively easily.</p>", "authors": [{"name": "Jean-Philippe Bernardy", "author_profile_id": "81372591366", "affiliation": "Chalmers University of Technology and University of Gothenburg, Gothenburg, Sweden", "person_id": "P4261229", "email_address": "bernardy@chalmers.se", "orcid_id": ""}, {"name": "Koen Claessen", "author_profile_id": "81100206977", "affiliation": "Chalmers University of Technology and University of Gothenburg, Gothenburg, Sweden", "person_id": "P4261230", "email_address": "koen@chalmers.se", "orcid_id": ""}], "doi_number": "10.1145/2500365.2500576", "year": "2013", "article_id": "2500576", "conference": "ICFP", "title": "Efficient divide-and-conquer parsing of practical context-free languages", "url": "http://dl.acm.org/citation.cfm?id=2500576"}