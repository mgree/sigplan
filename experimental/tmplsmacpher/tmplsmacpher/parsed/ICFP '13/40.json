{"article_publication_date": "09-25-2013", "fulltext": "\n Optimizing Abstract Abstract Machines J. Ian Johnson Nicholas Labich Matthew Might Northeastern University \nNortheastern University University of Utah ianj@ccs.neu.edu labichn@ccs.neu.edu might@cs.utah.edu David \nVan Horn Northeastern University dvanhorn@ccs.neu.edu Abstract The technique of abstracting abstract \nmachines (AAM) provides a systematic approach for deriving computable approximations of evaluators that \nare easily proved sound. This article contributes a complementary step-by-step process for subsequently \ngoing from a naive analyzer derived under the AAM approach, to an ef.cient and correct implementation. \nThe end result of the process is a two to three order-of-magnitude improvement over the systematically \nderived analyzer, making it competitive with hand-optimized im\u00adplementations that compute fundamentally \nless precise results. Categories and Subject Descriptors F.3.2 [Semantics of Pro\u00adgramming Languages]: \nProgram analysis Keywords abstract machines; abstract interpretation 1. Introduction Program analysis \nprovides sound predictive models of program be\u00adhavior, but in order for such models to be effective, \nthey must be ef.ciently computable and correct. Past approaches to designing program analyses have often \nfeatured abstractions that are far re\u00admoved from the original language semantics, requiring ingenuity \nin their construction and effort in their veri.cation. The abstracting abstract machines (AAM) approach \n[30, 32] to deriving program analyses provides an alternative: a systematic way of transform\u00ading a programming \nlanguage semantics in the form of an abstract machine into a family of abstract interpreters. It thus \nreduces the burden of constructing and verifying the soundness of an abstract interpreter. By taking \na machine-oriented view of computation, AAM makes it possible to design, verify, and implement program \nana\u00adlyzers for realistic language features typically considered dif.cult to model. The approach was originally \napplied to features such as higher-order functions, stack inspection, exceptions, laziness, .rst\u00adclass \ncontinuations, and garbage collection. It has since been used to verify actor-[8] and thread-based [20] \nparallelism and behavioral Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are not made or distributed for \npro.t or commercial advantage and that copies bear this notice and the full citation on the .rst page. \nCopyrights for components of this work owned by others than the author(s) must be honored. Abstracting \nwith credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, \nrequires prior speci.c permission and/or a fee. Request permissions from permissions@acm.org. ICFP 13, \nSeptember 25 27, Boston, MA, USA.. Copyright is held by the owner/author(s). Publication rights licensed \nto ACM. ACM 978-1-4503-2326-0/13/09. . . $15.00. http://dx.doi.org/10.1145/2500365.2500604  Figure \n1. Factor improvements over the baseline analyzer for the Vardoulakis and Shivers benchmark in terms \nof the rate of state transitions and total analysis time. (Bigger is better.) Each point is marked with \nthe section that introduces the optimization. contracts [29]; it has been used to model Coq [24], Dalvik \n[23], Er\u00adlang [9], JavaScript [31], and Racket [29]. The primary strength of the approach is that abstract \ninterpreters can be easily derived through a small number of steps from exist\u00ading machine models. Since \nthe relationships between abstract ma\u00adchines and higher-level semantic models such as de.nitional in\u00adterpreters \n[27], structured operational semantics [26], and reduction semantics [12] are well understood [6], it \nis possible to navigate from these high-level semantic models to sound program analyz\u00aders in a systematic \nway. Moreover, since these analyses so closely resemble a language s interpreter (a) implementing an \nanalysis re\u00adquires little more than implementing an interpreter, (b) a single im\u00adplementation can serve \nas both an interpreter and analyzer, and (c) verifying the correctness of the implementation is straightforward. \nUnfortunately, the AAM approach yields analyzers with poor performance relative to hand-optimized analyzers. \nOur work takes aim squarely at this ef.ciency gap, and narrows it in an equally systematic way through \na number of simple steps, many of which are inspired by run-time implementation techniques such as lazi\u00adness \nand compilation to avoid interpretative overhead. Each of these steps is proven correct, so the end result \nis an implementa\u00adtion that is trustworthy and ef.cient. In this article, we develop a systematic approach \nto deriving a practical implementation of an abstract-machine-based analyzer using mostly semantic means \nrather than tricky engineering. Our goal is to empower programming language implementers and re\u00adsearchers \nto explore and convincingly exhibit their ideas with a low barrier to entry. The optimizations we describe \nare widely appli\u00adcable and apparently effective to scale far beyond the size of pro\u00adgrams typically considered \nin the recent literature on .ow analysis for functional languages. 2. At a glance We start with a quick \nreview of the AAM approach to develop an analysis framework and then apply our step-by-step optimiza\u00adtion \ntechniques in the simpli.ed setting of a core functional lan\u00adguage. This allows us to explicate the optimizations \nwith a minimal amount of inessential technical overhead. Following that, we scale this approach up to \nan analyzer for a realistic untyped, higher-order imperative language with a number of interesting features \nand then measure improvements across a suite of benchmarks. At each step during the initial presentation \nand development, we evaluated the implementation on a set of benchmarks. The high\u00adlighted benchmark in \n.gure 1 is from Vardoulakis and Shivers [33] that tests distributivity of multiplication over addition \non Church numerals. For the step-by-step development, this benchmark is par\u00adticularly informative: 1. \nit can be written in most modern programming languages, 2. it was designed to stress an analyzer s ability \nto deal with complicated environment and control structure arising from the use of higher-order functions \nto encode arithmetic, and 3. its improvement is about median in the benchmark suite con\u00adsidered in section \n6, and thus it serves as a good sanity check for each of the optimization techniques considered.  We \nstart, in section 3, by developing an abstract interpreter according to the AAM approach. In the initial \nabstraction, each state carries a store (what is called per-state store variance). The space of stores \nis exponential in size; without further abstraction, the analysis is exponential and thus cannot analyze \nthe example in a reasonable amount of time. In section 4, we perform a further abstraction by widening \nthe store. The resulting analyzer sacri.ces precision for speed and is able to analyze the example in \nabout 1 minute. This step is described by Van Horn and Might [32, \u00a73.5 6] and is necessary to make even \nsmall examples feasible. We therefore take a widened interpreter as the baseline for our evaluation. \nSection 5 gives a series of simple abstractions and implemen\u00adtation techniques that, in total, speed \nup the analysis by nearly a factor of 500, dropping the analysis time to a fraction of a second. Figure \n1 shows the step-wise improvement of the analysis time for this example. The AAM approach, in essence, \ndoes the following: it takes a machine-based view of computation and turns it into a .nitary approximation \nby bounding the size of the store. With a limited address space, the store must map addresses to sets \nof values. Store updates are interpreted as joins, and store dereferences are interpreted by non-deterministic \nchoice of an element from a set. The result of analyzing a program is a .nite directed graph where nodes \nin the graph are (abstract) machine states and edges denote machine transitions between states. The techniques \nwe propose for optimizing analysis fall into the following categories: 1. generate fewer states by avoiding \nthe eager exploration of non\u00addeterministic choices that will later collapse into a single join point. \nWe accomplish this by applying lazy evaluation tech\u00adniques so that non-determinism is evaluated by need. \n 2. generate fewer states by avoiding unnecessary, intermediate states of a computation. We accomplish \nthis by applying com\u00adpilation techniques from functional languages to avoid interpre\u00adtive overhead in \nthe machine transition system.   Figure 2. Example state graphs for Earl et. al. program. Gray states \nfollow variable references, ev states are black, and all others are white. Part (a) shows the baseline \nanalyzer result. It has long cor\u00adridor transitions and diamond subgraphs that fan-out from non\u00addeterminism \nand fan-in from joins. Part (b) shows the result of performing nondeterminism lazily and thus avoids \nmany of the di\u00adamond subgraphs. Part (c) shows the result of abstract compila\u00adtion that removes interpretive \noverhead in the form of intermediate states, thus minimizing the corridor transitions. The end result \nis a more compact abstraction of the program that can be generated faster. 3. generate states faster. \nWe accomplish this by better algorithm design in the .xed-point computation we use to generate state \ngraphs. Figure 2 shows the effect of (1) and (2) for the small motivating ex\u00adample in Earl, et al. [10]. \nBy generating signi.cantly fewer states at a signi.cantly faster rate, we are able to achieve large performance \nimprovements in terms of both time and space. Section 6 describes the evaluation of each optimization \ntech\u00adnique applied to an implementation supporting a more realistic set of features, including mutation, \n.rst-class control, compound data, a full numeric tower and many more forms of primitive data and operations. \nWe evaluate this implementation against a set of bench\u00admark programs drawn from the literature. For all \nbenchmarks, the optimized analyzer outperforms the baseline by at least a factor of two to three orders \nof magnitude. Section 7 relates this work to the literature and section 8 con\u00adcludes. 3. Abstract interpretation \nof ISWIM In this section, we give a brief review of the AAM approach by de.ning a sound analytic framework \nfor a core higher-order func\u00adtional language: Landin s ISWIM [16]. In the subsequent sections, we will \nexplore optimizations for the analyzer in this simpli.ed set\u00ad Expressions e = var\u00a3(x) | lit\u00a3(l) | lam\u00a3(x, \ne) | app\u00a3(e, e) | if\u00a3(e, e, e) Variables x = x | y | . . . Literals l = z | b | o Integers z = 0 | 1 \n| -1 | . . . Booleans b = tt | ff Operations o = zero? | add1 | sub1 | . . . Figure 3. Syntax of ISWIM \nValues v, u = clos (x, e, .) | l | . States . = ev t(e, ., s, .) | co (., v, s) | ap t(v, v, s, .) Continuations \n. = halt | fun (v, a.) | arg (e, ., a.) | ifk (e, e, ., a.) Addresses a . Addr Times t . Time Environments \n. . Var . Addr Stores s . Addr . P(Value) Figure 4. Abstract machine components ting, but scaling these \ntechniques to realistic languages is straight\u00adforward and has been done for the analyzer evaluated in \nsection 6. ISWIM is a family of programming languages parameterized by a set of base values and operations. \nTo make things concrete, we consider a member of the ISWIM family with integers, booleans, and a few \noperations. Figure 3 de.nes the syntax of ISWIM. It includes variables, literals (either integers, booleans, \nor operations), .-expressions for de.ning procedures, procedure applications, and conditionals. Expressions \ncarry a label, f, which is drawn from an unspeci.ed set and denotes the source location of the expression; \nlabels are used to disambiguate distinct, but syntactically identical pieces of syntax. We omit the label \nannotation in contexts where it is irrelevant. The semantics is de.ned in terms of a machine model. The \nmachine components are de.ned in .gure 4; .gure 5 de.nes the transition relation (unmentioned components \nstay the same). The evaluation of a program is de.ned as its set of traces that arise from iterating \nthe machine transition relation. The traces function produces the set of all proofs of reachability for \nany state . from the injection of program e (from which one could extract a string of states). The machine \nis a very slight variation on a standard abstract machine for ISWIM in eval, continue, apply form [6]. \nIt can be systematically derived from a de.nitional interpreter through a continuation-passing style \ntransformation and defunctionalization, or from a structural operational semantics using the refocusing \nconstruction of Danvy and Nielsen [7]. Compared with the standard machine semantics, this de.nition is \ndifferent in the following ways, which make it abstractable as a program analyzer: the store maps addresses \nto sets of values, 1 not single values,  continuations are heap-allocated, not stack-allocated,  1 \nMore generally, we can have stores map to any domain that forms a Galois connection with sets of values, \nenabling . to produce elaborate abstractions of base values (e.g., interval or octagon abstractions). \nWe use sets of values for a simpler exposition. traces(e) = {ev t0 (e, \u00d8, \u00d8, halt) -. .} where . -. .' \nde.ned to be the following let t' = tick(.) ev (var (x), ., s, .) -. co (., v, s) if v . s(.(x)) ev (lit \n(l), ., s, .) -. co (., l, s) ev t(lam (x, e), ., s, .) -. co (., clos (x, e, .), s) 1 tt ev t(app\u00a3(e0, \ne1), ., s, .) -. ev (e0, ., s', arg\u00a3(e1, ., a.)) where a. = allockontt\u00a3(s, .) 1 s' = s U [a. . {.}] t \nev t(if\u00a3(e0, e1, e2), ., s, .) -. ev (e0, ., s', ifkt(e1, e2, ., a.)) where a. = allockontt\u00a3(s, .) s' \n= s U [a. . {.}] t co (arg\u00a3(e, ., a.), v, s) -. ev t(e, ., s, funt\u00a3(v, a.)) t co (funt\u00a3(u, a.), v, s) \n-. ap\u00a3(u, v, ., s) if . . s(a.) 1 t co (ifkt(e0, e1, ., a.), tt, s) -. ev(e0, ., s, .) if . . s(a.) \n1 t co (ifkt(e0, e1, ., a.), ff, s) -. ev(e1, ., s, .) if . . s(a.) 1 tt ap\u00a3(clos (x, e, .), v, s, .) \n-. ev(e, .', s', .) where a = alloc(.) .' = .[x . a] s' = s U [a . {v}] t ap\u00a3(o, v, s, .) -. co (., v', \ns) if v' . .(o, v) Figure 5. Abstract abstract machine for ISWIM there are timestamps (t . Time) and \nsyntax labels (f) threaded through the computation, and  the machine is implicitly parameterized by \nthe functions alloc, allockont, tick, ., and spaces Addr, Time (and initial t0 . Time).  Concrete interpretation \nTo characterize concrete interpretation, set the implicit parameters of the relation given in .gure 5 \nas follows: alloc(.) = a where a /. the s within . allockontt\u00a3(s, .) = a. where a. ./s These functions \nappear to ignore f and t, but they can be used to determinize the choice of fresh addresses. The U on \nstores in the .gure is a point-wise lifting of .: s U s' = .a.s(a) . s'(a). The resulting relation is \nnon-deterministic in its choice of addresses, however it must always choose a fresh address when allocating \na continuation or variable binding. If we consider machine states equivalent up to consistent renaming \nand .x an allocation scheme, this relation de.nes a deterministic machine (the relation is really a function). \nThe interpretation of primitive operations is de.ned by setting . as follows: z + 1 . .(add1, z) z - \n1 . .(sub1, z) tt . .(zero?, 0) ff . .(zero?, z) if z= 0 Abstract interpretation To characterize abstract \ninterpretation, set the implicit parameters just as above, but drop the a . s condi\u00adtion. The . relation \ntakes some care to not make the analysis run forever; a simple instantiation is a .at abstraction where \narithmetic operations return an abstract top element Z, and zero? returns both tt and ff on Z. This family \nof interpreters is also non-deterministic in choices of addresses, but it is free to choose addresses \nthat are al\u00adready in use. Consequently, the machines may be non-deterministic when multiple values reside \nin a store location. It is important to recognize from this de.nition that any allo\u00adcation strategy \nis a sound abstract interpretation [21]. In particular, concrete interpretation is a kind of abstract \ninterpretation. So is an interpretation that allocates a single cell into which all bindings and continuations \nare stored. The former is an abstract interpretation with uncomputable reachability and gives only the \nground truth of a program s behavior; the latter is an abstract interpretation that is easy to compute \nbut gives little information. Useful program anal\u00adyses lay somewhere in between and can be characterized \nby their choice of address representation and allocation strategy. Uniform k-CFA [25], presented next, \nis one such analysis. Uniform k-CFA To characterize uniform k-CFA, set the alloca\u00adtion strategy as follows, \nfor a .xed constant k: Time = Label * t0 = e t alloc(ap (clos (x, e, .), v, s, .)) = xl\u00a3thk \u00a3 allockont \nt(s, .) = \u00a3t \u00a3 tick(ev t(e, ., s, .)) = t tick(co (arg t(e, ., a.), v, s)) = t t tick(ap (u, v, .)) = \nl\u00a3th \u00a3 k lth0 = lehk = t0 l\u00a3thk+1 = \u00a3lthk The L\u00b7Jk notation denotes the truncation of a list of symbols \nto the leftmost k symbols. All that remains is the interpretation of primitives. For abstract interpretation, \nwe set . to the function that returns Z on all inputs a symbolic value we interpret as denoting the set \nof all integers. At this point, we have abstracted the original machine to one which has a .nite state \nspace for any given program, and thus forms the basis of a sound, computable program analyzer for ISWIM. \n4. From machine semantics to baseline analyzer The uniform k-CFA allocation strategy would make traces \nin .g\u00adure 5 a computable abstraction of possible executions, but one that is too inef.cient to run, even \non small examples. Through this sec\u00adtion, we explain a succession of approximations to reach a more appropriate \nbaseline analysis. We ground this path by .rst formu\u00adlating the analysis in terms of a classic .xed-point \ncomputation. 4.1 Static analysis as .xed-point computation Conceptually, the AAM approach calls for \ncomputing an analysis as a graph exploration: (1) start with an initial state, and (2) com\u00adpute the transitive \nclosure of the transition relation from that state. All visited states are potentially reachable in the \nconcrete, and all paths through the graph are possible traces of execution. We can cast this exploration \nprocess in terms of a .xed-point calculation. Given the initial state .0 and the transition relation \n-., we de.ne the global transfer function: F.0 : P(State)\u00d7P(State\u00d7State) . P(State)\u00d7P(State\u00d7State). Internally, \nthis global transfer function computes the successors of all supplied states, and then includes the initial \nstate: F.0 (V, E) = ({.0} . V ' , E ' ) '' ' E = {(. , . ) | . . V and . -. . } '' '' V = {. | (. , . \n) . E } Then, the evaluator for the analysis computes the least .xed-point of the global transfer function: \neval(e) = lfp(F.0 ), where .0 = ev t0 (e, \u00d8, \u00d8, halt). The possible traces of execution tell us the \nmost about a pro\u00adgram, so we take traces (e) to be the (regular) set of paths through the computed graph. \nWe elide the construction of the set of edges in this paper. To conduct this naive exploration on the \nVardoulakis and Shiv\u00aders example would require considerable time. Even though the state space is .nite, \nit is exponential in the size of the program. Even with k = 0, there are exponentially many stores in \nthe AAM framework. In the next subsection, we .x this with store widening to reach polynomial (albeit \nof high degree) complexity. This widening ef\u00adfectively lifts the store out of individual states to create \na single, global shared store for all. 4.2 Store widening A common technique to accelerate convergence \nin .ow analyses is to share a common, global store. Formally, we can cast this optimization as a second \nabstraction or as the application of a widening operator 2 during the .xed-point iteration. In the ISWIM \nlanguage, such a widening makes 0-CFA quartic in the size of the program. Thus, complexity drops from \nintractable exponentiality to a merely daunting polynomial. Since we can cast this optimization as a \nwidening, there is no need to change the transition relation itself. Rather, what changes is the structure \nof the .xed-point iteration. In each pass, the algorithm will collect all newly produced stores and join \nthem together. Then, before each transition, it installs this joined store into current state. To describe \nthis process, AAM de.ned a transformation of the reduction relation so that it operates on a pair of \na set of contexts (C) and a store (s). A context includes all non-store components, e.g., the expression, \nthe environment and the stack. The transformed relation, H -., is (C, s) H' , s ' ), -. (C ' ' where \nC ' = {c | wn(c, s) -. wn(c , sc), c . C} s ' = {sc | wn(c, s) -. wn(c ' , sc), c . C} wn : Context \u00d7 \nStore . State wn(ev (e, ., .), s) = ev (e, ., s, .) wn(co (v, .), s) = co (v, ., s) wn(ap (u, v, .), \ns) = ap (u, v, s, .) To retain soundness, this store grows monotonically as the least upper bound of \nall occurring stores. 4.3 Store-allocate all values The .nal approximation we make to get to our baseline \nis to store-allocate all values that appear, so that any non-machine state that contains a value instead \ncontains an address to a value. The AAM approach stops at the previous optimization. However, the funcontinuation \nstores a value, and this makes the space of con\u00adtinuations quadratic rather than linear in the size of \nthe program, for a monovariant analysis like 0-CFA. Having the space of con\u00adtinuations grow linearly \nwith the size of the program will drop the overall complexity to cubic (as expected). We also need to \nallocate an address for the argument position in an apstate. To achieve this linearity for continuations, \nwe allocate an ad\u00address for the value position when we create the continuation. This address and the \ntail address are both determined by the label of the application point, so the space becomes linear and \nthe overall com\u00adplexity drops to cubic. This is a critical abstraction in languages with n-ary functions, \nsince otherwise the continuation space grows 2 Technically, we would have to copy the value of the global \nstore to all states being stepped to .t the formal de.nition of a widening, but this representation is \norder-isomorphic to that. super-exponentially (O(n n)). We extend the semantics to addition\u00adally allocate \nan address for the function value when creating the fun continuation. The continuation has to contain \nthis address to remember where to retrieve values from in the store. The new evaluation rules follow, \nwhere t ' = tick(.): 1 co t(arg (e, ., a.), v, s) -. ev t(e, ., s ' , fun (a, a.)) where a = alloc(.) \ns ' = s U [a . {v}] Now instead of storing the evaluated function in the continuation frame itself, we \nindirect it through the store for further control on complexity and precision: 1 co t(fun (a, a.), v, \ns) -. ap\u00a3 t(u, a, ., s ' ) if . . s(a.), u . s(a) where a = alloc (.) s ' = s U [a . {v}] Associated \nwith this indirection, we now apply all functions stored in the address. This nondeterminism is necessary \nin order to continue with evaluation. 5. Implementation techniques In this section, we discuss the optimizations \nfor abstract interpreters that yield our ultimate performance gains. We have two broad cate\u00adgories of \nthese optimizations: (1) pragmatic improvement, (2) tran\u00adsition elimination. The pragmatic improvements \nreduce overhead and trade space for time by utilizing: 1. timestamped stores; 2. store deltas; and \n3. imperative, pre-allocated data structures.  The transition-elimination optimizations reduce the overall \nnumber of transitions made by the analyzer by performing: 4. frontier-based semantics; 5. lazy non-determinism; \nand 6. abstract compilation.  All pragmatic improvements are precision preserving (form complete abstractions), \nbut the semantic changes are not in some cases, for reasons we will describe. We did not observe the \npreci\u00adsion differences in our evaluation. We apply the frontier-based semantics combined with times\u00adtamped \nstores as our .rst step. The move to the imperative will be made last in order to show the effectiveness \nof these techniques in the purely functional realm. 5.1 Timestamped frontier The semantics given for \nstore widening in section 4.2, while simple, is wasteful. It also does not model what typical implementations \ndo. It causes all states found so far to step each iteration, even if they are not revisited. This has \nnegative performance and precision consequences (changes to the store can travel back in time in straight-line \ncode). We instead use a frontier-based semantics that corresponds to the classic worklist algorithms \nfor analysis. The difference is that the store is not modi.ed in-place, but updated after all frontier \nstates have been processed. This has implications for the analysis precision and determinism. Speci.cally, \nhigher precision, and it is deterministic even if set iteration is not. The state space changes from \na store and set of contexts to a set of seen abstract states (context plus store), S, a set of contexts \nto step (the frontier), F , and a store to step those contexts with, s: ' '' (S, F, s) H, F ) -. (S \n. S ,s We constantly see more states, so S is always growing. The frontier, which is what remains to \nbe done, changes. Let s start with the result of stepping all the contexts in F paired with the current \nstore (call it I for intermediate): '' '' I = {(c , s ) | wn(c, s) -. wn(c , s ), c . F } The next store \nis the least upper bound of all the stores in I: s ' = {s | ( , s) . I} The next frontier is exactly \nthe states that we found from stepping the last frontier, but have not seen before. They must be states, \nso we pair the contexts with the next store: F ' = {c | (c, ) . I , (c, s ' ) ./S} Finally, we add what \nwe know we had not yet seen to the seen set: ' ' ' S = {(c, s ) | c . F } To inject a program e into \nthis machine, we start off knowing we have seen the .rst state, and that we need to process the .rst \nstate: inject (e) = ({(c0, .)}, {c0}, .) where c0 = ev (e, ., halt) Notice that now S has several copies \nof the abstract store in it. As it is, this semantics is much less ef.cient (but still more pre\u00adcise) \nthan the previously proposed semantics because membership checks have to compare entire stores. Checking \nequality is expen\u00adsive because the stores within each state are large, and nearly every entry must be \nchecked against every other due to high similarities amongst stores. And, there is a better way. Shivers \noriginal work on k-CFA was susceptible to the same problem, and he suggested three com\u00adplementary optimizations: \n(1) make the store global; (2) update the store imperatively; and (3) associate every change in the store \nwith a version number its timestamp. Then, put timestamps in states where previously there were stores. \nGiven two states, the analysis can now compare their stores just by comparing their timestamps a constant-time \noperation. There are two subtle losses of precision in Shivers original timestamp technique that we can \n.x. 1. In our semantics, the store does not change until the entire frontier has been explored. This \navoids cross-branch pollu\u00adtion which would otherwise happen in Shivers semantics, e.g., when one branch \nwrites to address a and another branch reads from address a. 2. The common implementation strategy for \ntimestamps destruc\u00adtively updates each state s timestamp. This loses temporal in\u00adformation about the \ncontexts a state is visited in, and in what order. Our semantics has a drop-in replacement of timestamps \nfor stores in the seen set ( S ), so we do not experience precision loss.  S . Store * S . N\u00d7 Context \nF . Context  ( -.T S . S ' , F ' , s ' , S ' , t ' ) S , F, s, S, t) H( where I = {(c ' , sc) | wn(c, \ns) -. wn(c ' , sc), c . F } s ' = {sc | ( , sc) . I} ' (t + 1, s ' S ' ) if s '= s (t , S ' ) = (t, \nS) otherwise ' F = {c | (c, ) . I , (c, t ' ) ./S } S ' = {(c, t ' ) | c . F ' } inject (e) = ({(c0, \n0)}, {c0}, ., .:e, 0) where c0 = ev (e, ., halt) The observation Shivers made was that the store is increasing \nmonotonically, so all stores throughout execution will be totally ordered (form a chain). This observation \nallows you to replace stores with pointers into this chain. We keep the stores around in S to achieve \na complete abstraction. This corresponds to the temporal information about the execution s effect on \nthe store. Note also that F is only populated with states that have not been seen at the resulting store. \nThis is what produces the more precise abstraction than the baseline widening. The general .xed-point \ncombinator we showed above can be specialized to this semantics, as well. In fact, His a functional -.T \nrelation, so we can get the least .xed-point of it directly. Lemma 1. H -. maintains the invariant that \nall stores in S are totally ordered and s is an upper bound of the stores in S. Lemma 2. Hmaintains the \ninvariant that S is in order with -.T respect to : and s = hd(S). Theorem 1. His a complete abstraction \nof H -.T -.. The proof follows from the order isomorphism that, in one direction, sorts all the stores \nin S to form S, and translates stores in S to their distance from the end of S (their timestamp). In \nthe other direction, timestamps in S are replaced by the stores they point to in S.  5.2 Locally log-based \nstore deltas The above technique requires joining entire (large) stores together. Additionally, there \nis still a comparison of stores, which we estab\u00adlished is expensive. Not every step will modify all addresses \nof the store, so joining entire stores is wasteful in terms of memory and time. We can instead log store \nchanges and replay the change log on the full store after all steps have completed, noting when there \nis an actual change. This uses far fewer join and comparison operations, leading to less overhead, and \nis precision-preserving. We represent change logs as . . Store. = (Addr \u00d7 P(Storeable )) * . Each s U \n[a . vs] becomes a log addition (a, vs):., where . begins empty (E) for each step. Applying the changes \nto the full store is straightforward: replay : (Store. \u00d7 Store ) . (Store \u00d7 Boolean ) replay ([(ai, vsi), \n. . .] , s) = (s ' , d?(vsi, s(ai)) . . . .) where s ' = s U [ai . vsi] U . . . ' ' ? ' d?(vs, vs ) = \nvs = vs U vs We change the semantics slightly to add to the change log rather than produce an entire \nmodi.ed store. The transition relation is identical except for the addition of this change log. We maintain \nthe invariant that lookups will never rely on the change log, so we can use the originally supplied store \nunmodi.ed. A taste of the changes to the reduction relation is as follows: -.s. . (Context \u00d7 Store ) \n\u00d7 (Context \u00d7 Store.) 1 t t' ' (ap\u00a3(clos (x, e, .), a, .), s) -.s. (ev (e, . , .), (a , s(a)):E) where \na ' = alloc(.) . ' = .[x . a ' ] We lift -.s. to accommodate for the asymmetry in the input and output, \nand change the frontier-based semantics in the follow\u00ading way: ' ( -.s. ( S ' , F , s ' , S ' , t ' \n) S , F, s, S, t) HS . where I = {(c ' , .) | (c, s) -.s. (c ' , .)} (s ' , .?) = replay (appendall ({. \n| ( , .) . I}), s) ' (t + 1, sS) if .? (t , S ' ) = (t, S) otherwise ' F = {c | (c, ) . I , (c, t ' \n) ./S } S ' = {(c, t ' ) | c . F ' } appendall (\u00d8) = e appendall ({.} . .) = append (., appendall (.)) \n Here appendall combines change logs across all non-deterministic steps for a state to later be replayed. \nThe order the combination happens in doesn t matter, because join is associative and commu\u00adtative. Lemma \n3. (c, s) -.s. (c ' , .) iff wn(c, s) -.wn(c ' , replay (., s)) By cases on -.s. and -.. Lemma 4 (.? \nmeans change). Let replay (., s) = (s ' , .?). s ' = s iff .?. By induction on .. Theorem 2. H -.T -.s. \nis a complete abstraction of H. Follows from previous lemma and that join is associative and commutative. \n 5.3 Lazy non-determinism Tracing the execution of the analysis reveals an immediate short\u00adcoming: there \nis a high degree of branching and merging in the exploration. Surveying this branching has no bene.t \nfor precision. For example, in a function application, (f x y), where f, x and y each have several values \neach argument evaluation induces n\u00adway branching, only to be ultimately joined back together in their \nrespective application positions. Transition patterns of this shape litter the state-graph:  To avoid \nthe spurious forking and joining, we delay the non\u00addeterminism until and unless it is needed in strict \ncontexts (such as the guard of an if, a called procedure, or a numerical primitive application). Doing \nso collapses these forks and joins into a linear sequence of states:  This shift does not change the \nconcrete semantics of the lan\u00adguage to be lazy. Rather, it abstracts over transitions that the origi\u00adnal \nnon-deterministic semantics steps through. We say the abstrac\u00adtion is lazy because it delays splitting \non the values in an address until they are needed in the semantics. It does not change the exe\u00adcution \norder that leads to the values that are stored in the address. We introduce a new kind of value, addr \n(a), that represents a delayed non-deterministic choice of a value from s(a). The following rules highlight \nthe changes to the semantics: force : Store \u00d7 Value . P(Value) force(s, addr (a)) = s(a) force(s, v) \n= {v} ev (var (x), ., ., s) -.L co (., addr (.(x)), s) t t co (arg (e, ., a.), v, s) -.L ev 1 (e, ., \ns ' , funt(af , a.)) \u00a3 \u00a3 where af = alloc(.) s ' = s U [a . force(s, v)] t co (ifkt(e0, e1, ., a.), v, \ns) -.L ev 1 (e0, ., s, .) if . . s(a.), tt . force(s, v) Since if guards are in strict position, we must \nforce the value to determine which branch to to take. The middle rule uses force only to combine with \nvalues in the store -it does not introduce needless non-determinism. We have two choices for how to implement \nlazy non-determinism. Option 1: Lose precision; simplify implementation This seman\u00adtics introduces a \nsubtle precision difference over the baseline. Con\u00adsider a con.guration where a reference to a variable \nand a binding of a variable will happen in one step, since store widening leads to stepping several states \nin one big step. With laziness, the refer\u00adence will mean the original binding(s) of the variable or the \nnew binding, because the actual store lookup is delayed one step (i.e. laziness is administrative). Option \n2: Regain precision; complicate implementation The ad\u00administrative nature of laziness means that we could \nremove the loss in precision by storing the result of the lookup in a value repre\u00adsenting a delayed nondeterministic \nchoice. This is a more common choice in 0CFA implementations we have seen, but it interferes with the \nnext optimization due to the invariant from store deltas we have that lookups must not depend on the \nchange log. Theorem 3 (Soundness). If . -. . ' and . . . then there exists a ' ''' . such that . -.L \n. and . . . Here . is straightforward the left-hand side store must be contained in the right-hand-side \nstore, and if values occur in the states, the left-hand-side value must be in the forced corresponding \nright-hand-side value. The proof is by cases on . -. . ' .  5.4 Abstract compilation The prior optimization \nsaved time by doing the same amount of rea\u00adsoning as before but in fewer transitions. We can exploit \nthe same idea same reasoning, fewer transitions with abstract compila\u00adtion. Abstract compilation transforms \ncomplex expressions whose abstract evaluation is deterministic into abstract bytecodes. The abstract \ninterpreter then does in one transition what previously took many. Refer back to .gure 2 to see the effect \nof abstract compila\u00adtion. In short, abstract compilation eliminates unnecessary alloca\u00adtion, deallocation \nand branching. The technique is precision pre\u00adserving without store widening. We discuss the precision \ndiffer\u00adences with store widening at the end of the section. The compilation step converts expressions \ninto functions that expect the other components of the ev state. Its de.nition in .gure 6 shows close \nsimilarity to the rules for interpreting ev states. The next step is to change reduction rules that create \nev states to instead call these functions. Figure 7 shows the modi.ed reduction relation. The only change \nfrom the previous semantics is that ev [ ] : Expr . Store . Env \u00d7 Store. \u00d7 Kont \u00d7 Time . State  ' t \n= tick(\u00a3, ., s, t) [var (x)]s = .t(., ., .).co (., addr (.(x))), . [lit (l)]s = .t(., ., .).co (., l), \n. [lam (x, e)]s = .t(., ., .).co (., clos (x, [e], .)), . t [app \u00a3(e0, e1)]s = .t(., ., .).[e0]t1 (., \n. ' , arg ([e1], ., a.)) \u00a3 where a. = allockontt (s, .) \u00a3 . ' = (a., {.}):. [if\u00a3(e0, e1, e2)]s = .t(., \n., .).[e0]t1 (., . ' , ifkt([e1], [e2], ., a.)) where a. = allockontt(s, .) \u00a3 . ' = (a., {.}):. Figure \n6. Abstract compilation traces(e) = {inject([e]t0 (., e, halt)) -. .} where . inject(c, .) = wn(c, replay(., \n.)) wn(c, s) -. wn(c ' , s ' ) .. c [-.]s c ' , . . is such that replay(., s) = s ' t co (arg (k, ., \na.), v) [-.]s kt(s)(., ., funt(af , a.)) \u00a3 \u00a3 where af = alloc(.) . = (af , force(s, v)):e t co (funt(af \n, a.), v) [-.]s ap (u, a, .), (a, force(s, v)):e \u00a3 \u00a3 if u . s(af ), . . s(a.) co (ifkt(k0, k1, ., a.), \ntt) [-.]s k0 t (s)(., e, .) if . . s(a.) co (ifkt(k0, k1, ., a.), ff) [-.]s k1 t (s)(., e, .) if . . \ns(a.) kt1 t ap (clos (x, k, .), a, .) [-.]s (s)(. ' , ., .) \u00a3 where . ' = .[x . a] . = (a, s(a)):e ap \n(o, a, .) [-.]s co (., u), e where v . s(a), u . .(o, v) Figure 7. Abstract abstract machine for compiled \nISWIM state construction is replaced by calling the compiled expression. For notational coherence, we \nwrite .t(args . . .) for .(args . . . , t) and kt(args . . .) for k(args . . . , t). Correctness The \ncorrectness of abstract compilation seems obvi\u00adous, but it has never before been rigorously proved. What \nconsti\u00adtutes correctness in the case of dropped states, anyway? Applying an abstract bytecode s function \ndoes many steps in one go, at the end of which, the two semantics line up again (modulo representa\u00adtion \nof expressions). This constitutes the use of a notion of stutter\u00ading. We provide a formal analysis of \nabstract compilation without store widening with a proof of a stuttering bisimulation [3] between this \nsemantics and lazy non-determinism without widening to show precision preservation. The number of transitions \nthat can occur in succession from an abstract bytecode is roughly bounded by the amount of expression \nnesting in the program. We can use the expression containment or\u00adder to prove stuttering bisimulation \nwith a well-founded equiva\u00adlence bisimulation (WEB) [? ]. WEBs are equivalent to the notion of a stuttering \nbisimulation, but are more amenable to mechaniza\u00adtion since they also only require reasoning over one \nstep of the reduction relation. The trick is in de.ning a well-founded ordering that determines when \nthe two semantics will match up again, what Manolios calls the pair of functions erankt and erankl (but \nwe don t need erankl since the uncompiled semantics doesn t stutter). We de.ne a re.nement, r, from \nnon-compiled to compiled states (built structurally) by committing all the actions of an ev state (de.ned \nsimilarly to [ ], but immediately applies the func\u00adtions), and subsequently changing all expressions \nwith their com\u00adpiled variants. Since WEBs are for single transition systems, a WEB re.nement is over \nthe disjoint union of our two semantics, and the equivalence relation we use is just that a state is \nrelated to its re.ned state (and itself). Call this relation B. Before we prove this setup is indeed \na WEB, we need one lemma that applying an abstract bytecode s function is equal to re.ning the corresponding \nev state: Lemma 5 (Compile/commit). Let c, . ' = [e]t r(s)(., ., r(.)). Let wn(c ' , s ' ) = r(ev t(e, \n., s, .)). wn(c, replay (. ' , s)) = wn(c ' , replay (., s ' )). The proof is by induction on e. Theorem \n4 (Precision preservation). B is a WEB on -.L l -. The proof follows by cases on -.L l -. with the WEB \nwitness being the well-order on expressions (with a . element), and the following erankt , erankl functions: \nerankt (ev t(e, ., s, .)) = e erankt (.) = . otherwise erankl (s, s ' ) = 0 All cases are either simple \nsteps or appeals to the well-order on erankt s range. The other rank function, erankl is unnecessary, \nso we just make it the constant 0 function. The [ -.] cases are trivial. Wide store and abstract compilation \nIt is possible for different stores to occur between the different semantics because abstract compilation \ncan change the order in which the store is changed (across steps). This is the case because some corridor \nexpressions may compile down to change the store before some others, meaning there is no stuttering relationship \nwith the wide lazy semantics. Although there is a difference pre-and post-abstract compilation, the result \nis still deterministic in contrast to Shivers technique. The soundness is in tact since we can add store-widening \nthe correct unwidened semantics with an easy correctness proof. Call [H the result of the widening operator \nfrom the previous section on -.] [ -.].  5.5 Imperative, pre-allocated data structures Thus far, we \nhave made our optimizations in a purely functional manner. For the .nal push for performance, we need \nto dip into the imperative. In this section, we show an alternative representation of the store and seen \nset that are more space-ef.cient and are amenable to destructive updates by adhering to a history for \neach address. The following transfer function has several components that can be destructively updated, \nand intermediate sets can be elided by adding to global sets. In fact, the log of store deltas can be \nremoved as well, by updating the store in-place, and on lookup, using the .rst value timestamped = the \ncurrent timestamp. We start with the purely functional view. 5.5.1 Pure setup for imperative implementation \nThe store maps to a stack of timestamped sets of abstract values. Throughout this section, we will be \ntaking the parameter t to be the current time, or the length of the store chain at the beginning of the \nstep. s . Store = Addr . ValStack V . ValStack = (N\u00d7 P(Storeable )) * To allow imperative store updates, \nwe maintain an invariant that we never look up values tagged at a time in the future: vs if V = (t ' \n, vs):V ' , t ' = t lookup(V , t) = ' ' vs if V = (t ' , vs):(t '' , vs ):V ' , t ' > t To construct \nthis value stack, we have a time-parameterized join operation that also tracks changes to the store. \nIf joining with a time in the future, we just add to it. Otherwise, we re making a change for the future \n(t + 1), but only if there is an actual change. s Ut [a . vs] = s[a . V ], .? where (V, .?) = s(a) Ut \nvs e Ut vs = (t, vs), tt '''' ' (t , vs):V Ut vs = (t , vs U vs ):V , d?(vs, vs ' ) if t > t * V Ut vs \n= (t + 1, vs * ):V , tt if vst = vs where vst = lookup(s(a), t) * vs = vs U vst V Ut vs = V, ff otherwise \n For the purposes of space, we reuse the [ -.] semantics, al\u00adthough the replay of the produced . objects \nshould be in-place, and the lookup function should be using this single-threaded store. Because the store \nhas all the temporal information baked into it, we rephrase the core semantics in terms of a transfer \nfunction. The least .xed-point of this function gives a more compact representa\u00adtion of the reduction \nrelation of the previous section. System = (.State) \u00d7 Store \u00d7 N State . N* ) \u00d7 P(. F : System . System \nF(S , F, s, t ) = ( S ' , F ' , s ' , t ' ) where I = {(c ' , .) | c . F, c [ -.]s* c ' , .} s * = .a.lookup(s(a), \nt) (s ' , .?) = replay (appendall ({. | ( , .) . I}), s) ' t + 1 if .? t = t otherwise ' F = {c | (c, \n) . I , .? . S (c) = t: } t ' : ' S(c) if c . F S ' = .c. S (c) otherwise We prove semantic equivalence \nwith the previous semantics with a lock-step bisimulation with the stack of stores abstraction, which \nfollow from equational reasoning from the following lem\u00admas: Lemma 6. Stores of value stacks completely \nabstract stacks of stores. This depends on some well-formedness conditions about the order of the stacks. \nThe store of value stacks can be translated to a stack of stores by taking successive snapshots of the \nstore at different timestamps from the max timestamp it holds down to 0. Vice versa, we replay the changes \nacross adjacent stores in the stack. We apply a similar construction to the different representation \nof seen states in order to get the .nal result: Theorem 5. F is a complete abstraction of [H -.].  \n5.5.2 Pure to imperative The intermediate data structures of the above transfer function can all be streamlined \ninto globals that are destructively updated. In particular, there are 5 globals: 1. S : the seen set, \nthough made a map for faster membership tests and updates. 2. F : the frontier set, which must be persisent \nor copied for the iteration through the set to be correct. 3. s: the store, which represents all stores \nthat occur in the ma\u00adchine semantics. 4. t: the timestamp, or length of the store chain. 5. .?: whether \nthe store changed when stepping states in F .  The reduction relation would then instead of building \nstore deltas, update the global store. We would also not view it as a general relation, but a function \nthat adds all next states to F if they have not already been seen. At the end of iterating through F \n, S is updated with the new states at the next timestamp. There is no cross-step store poisoning since \nthe lookup is restricted to the current step s time, which points to the same value throughout the step. \n 5.5.3 Pre-allocating the store Internally, the algorithm at this stage uses hash tables to model the \nstore to allow arbitrary address representations. But, such a dynamic structure isn t necessary when \nwe know the structure of the store in advance.In a monovariant allocation strategy, the domain of the \nstore is bounded by the number of expressions in the program. If we label each expression with a unique \nnatural, the analysis can index directly into the store without a hash or a collision. Even for polyvariant \nanalyses, it is possible to compute the maximum number of addresses and similarly pre-allocate either \nthe spine of the store or (if memory is no concern) the entire store. 6. Evaluation We have implemented, \noptimized, and evaluated an analysis frame\u00adwork supporting higher-order functions, state, .rst-class \ncontrol, compound data, and a large number of primitive kinds of data and operations such as .oating \npoint, complex, and exact rational arith\u00admetic. The analysis is evaluated against a suite of Scheme bench\u00admarks \ndrawn from the literature. 3 For each benchmark, we collect analysis times, peak memory usage (as determined \nby Racket s GC statistics), and the rate of states-per-second explored by the analysis for each of the \noptimizations discussed in section 5, cumulatively applied. The analysis is stopped after consuming 30 \nminutes of time or 1 gigabyte of space 4. When presenting relative numbers, we use the timeout limits \nas a lower bound on the actual time required (i.e., one minute versus timeout is at least 30 times faster), \nthus giving a conservative estimate of improvements. For those benchmarks that did complete on the baseline, \nthe optimized analyzer outperformed the baseline by a factor of two to three orders of magnitude. We \nuse the following set of benchmarks: 3 Source code of the implementation and benchmark suite available \nat https://github.com/dvanhorn/oaam 4 All benchmarks are calculated as an average of 5 runs, done in \nparallel (each isolated to a core), on an 12-core, 64-bit Intel Xeon machine running at 2.40GHz with \n12Gb of memory. Speed state Program LOC Time (sec) Space (MB) sec    nucleic 3492 m 66.9 m 238 \n44 9K matrix 747 t 3.4 294 114 68 87K nbody 1435 t 22.9 361 171 67 57K earley 667 1.1K 0.4 409 114 \n252 95K maze 681 t 2.6 332 114 55 118K church 42 44.9 0.1 86 114 714 56K lattice 214 348.5 0.2 231 \n114 382 104K boyer 642 m 13.4 m 130 39 39K mbrotZ 69 373.6 0.1 295 114 540 63K Figure 8. Overview \nperformance comparison between baseline and optimized analyzer (entries of t mean timeout, and m mean \nout of memory). 1. nucleic: a .oating-point intensive application taken from molec\u00adular biology that \nhas been used widely in benchmarking func\u00adtional language implementations [13] and analyses (e.g. [14, \n34]). It is a constraint satisfaction algorithm used to determine the three-dimensional structure of \nnucleic acids. 2. matrix tests whether a matrix is maximal among all matrices of the same dimension \nobtainable by simple reordering of rows and columns and negation of any subset of rows and columns. It \nis written in continuation-passing style (used in [14, 34]). 3. nbody: implementation [35] of the Greengard \nmultipole algo\u00adrithm for computing gravitational forces on point masses dis\u00adtributed uniformly in a cube \n(used in [14, 34]). 4. earley: Earley s parsing algorithm, applied to a 15-symbol in\u00adput according to \na simple ambiguous grammar. A real program, applied to small data whose exponential behavior leads to \na peak heap size of half a gigabyte or more during concrete exe\u00adcution. 5. maze: generates a random \nmaze using Scheme s call/cc op\u00aderation and .nds a path solving the maze (used in [14, 34]). 6. church: \ntests distributivity of multiplication over addition for Church numerals (introduced by [33]). 7. lattice: \nenumerates the order-preserving maps between two .\u00adnite lattices (used in [14, 34]). 8. boyer: a term-rewriting \ntheorem prover (used in [14, 34]). 9. mbrotZ: generates Mandelbrot fractal using complex numbers.  \n10. graphs: counts the number of directed graphs with a distin\u00adguished root and k vertices, each having \nout-degree at most 2. It is written in a continuation-passing style and makes extensive use of higher-order \nprocedures it creates closures almost as often as it performs non-tail procedure calls (used by [14, \n34]).  Figure 8 gives an overview of the benchmark results in terms of absolute time, space, and speed \nbetween the baseline and most optimized analyzer. Figure 9 plots the factors of improvement over the \nbaseline for each optimization step. To determine the impact of each section s technique on preci\u00adsion, \nwe evaluated a singleton variable analysis to .nd opportuni\u00adties to inline constants and closed functions. \nWe found no change in the results across all implementations, including Shivers times\u00adtamp approximation \nfrom an empirical point of view, these tech\u00adniques are precision preserving despite the theoterical loss \nof pre\u00adcision. Our step-wise optimizations strictly produce better analysis times with no observed loss \nof precision. The .nal result is a systematically derived and veri.ed implementation that oper\u00adates within \na small factor performance loss compared to a hand\u00ad  (a) Total analysis time speed-up (baseline / optimized) \n (b) Rate of state transitions speed-up (optimized / baseline) (c) Peak memory usage improvement (baseline \n/ optimized)  Figure 9. Factors of improvement over baseline for each step of optimization (bigger \nis better). optimized, unveri.ed implementation. Moreover, much of the per\u00adformance gains are achieved \nwith purely functional methods, which allow the use of these methods in rewriting tools and others with \nrestricted input languages. Peak memory usage is often consider\u00adably improved by the end of the optimization \nsteps, but the effect of mutation has unexpected consequence on the analyzers memory consumption, likely \ndue to a larger-than-necessary preallocated store. Comparison with other .ow analysis implementations \nThe anal\u00adysis considered here computes results similar to Earl, et al. s 0-CFA implementation [10], which \ntimes out on the Vardoulakis and Shiv\u00aders benchmark because it does not widen the store as described \nfor our baseline evaluator. So even though it offers a fair point of com\u00adparison, a more thorough evaluation \nis probably uninformative as the other benchmarks are likely to timeout as well (and it would require \nsigni.cant effort to extend their implementation with the features needed to analyze our benchmark suite). \nThat implemen\u00adtation is evaluated against much smaller benchmarks: the largest program is 30 lines. Vardoulakis \nand Shivers evaluate their CFA2 analyzer [33] against a variant of 0-CFA de.ned in their framework and \nthe ex\u00adample we draw on is the largest benchmark Vardoulakis and Shivers consider. More work would be \nrequired to scale the analyzer to the set of features required by our benchmarks. The only analyzer we \nwere able to .nd that proved capable of analyzing the full suite of benchmarks considered here was the \nPolymorphic splitting system of Wright and Jagannathan [34]. 5 Unfortunately, these analyses compute \nan inherently different and incomparable form of analysis via a global acceptability judgment. Consequently, \nwe have omitted a complete comparison with these implementations. The AAM approach provides more precision \nin terms of temporal-ordering of program states, which comes at a cost that can be avoided in constraint-based \napproaches. Conse\u00adquently implementation techniques cannot be ported between these two approaches. However, \nour optimized implementation is within an order of magnitude of the performance of Wright and Jaganathan \ns analyzer. Although we would like to improve this to be more competitive, the optimized AAM approach \nstill has many strengths to recommend it in terms of precision, ease of implemen\u00adtation and veri.cation, \nand rapid design. We can get closer to their performance by relying on the representation of addresses \nand the behavior of alloc to pre-allocate most data structures and split the abstract store out into \nparts that are more quickly accessed and up\u00addated. Our semantic optimizations can still be applied to \nan analysis that does abstract garbage collection [22], whereas the polymorphic splitting implementation \nis tied strongly to a single-threaded store. 7. Related work Abstracting Abstract Machines This work \nclearly closely fol\u00adlows Van Horn and Might s original papers on abstracting abstract machines [30, 32], \nwhich in turn is one piece of the large body of research on .ow analysis for higher-order languages (see \nMidt\u00adgaard [19] for a thorough survey). The AAM approach sits at the con.uence of two major lines of \nresearch: (1) the study of abstract machines [17] and their systematic construction [27], and (2) the \ntheory of abstract interpretation [4, 5]. Frameworks for .ow analysis of higher-order programs Be\u00adsides \nthe original AAM work, the analysis most similar to that pre\u00adsented in section 3 is the in.nitary control-.ow \nanalysis of Nielson and Nielson [25] and the uni.ed treatment of .ow analysis by Ja\u00adgannathan and Weeks \n[15]. Both are parameterized in such a way 5 This is not a coincidence; these papers set a high standard \nfor evaluation, which we consciously aimed to approach. that in the limit, the analysis is equivalent \nto an interpreter for the language, just as is the case here. What is different is that both give a constraint-based \nformulation of the abstract semantics rather than a .nite machine model. Abstract compilation Boucher \nand Feeley [1] introduced the idea of abstract compilation, which used closure generation [11] to improve \nthe performance of control .ow analysis. We have adapted the closure generation technique from compositional \nevaluators to abstract machines and applied it to similar effect. Constraint-based program analysis for \nhigher-order languages Constraint-based program analyses (e.g. [18, 25, 28, 34]) typically compute sets \nof abstract values for each program point. These val\u00adues approximate values arising at run-time for each \nprogram point. Value sets are computed as the least solution to a set of (inclusion or equality) constraints. \nThe constraints must be designed and proved as a sound approximation of the semantics. Ef.cient implementa\u00adtions \nof these kinds of analyses often take the form of worklist\u00adbased graph algorithms for constraint solving, \nand are thus quite different from the interpreter implementation. The approach thus requires effort in \nconstraint system design and implementation, and the resulting system require veri.cation effort to prove \nthe con\u00adstraint system is sound and that the implementation is correct. This effort increases substantially \nas the complexity of the an\u00adalyzed language increases. Both the work of maintaining the con\u00adcrete semantics \nand constraint system (and the relations between them) must be scaled simultaneously. However, constraint \nsys\u00adtems, which have been extensively studied in their own right, en\u00adjoy ef.cient implementation techniques \nand can be expressed in declarative logic languages that are heavily optimized [2]. Conse\u00adquently, constraint-based \nanalyses can be computed quickly. For example, Jagannathan and Wright s polymorphic splitting imple\u00admentation \n[34] analyses the Vardoulakis and Shivers benchmark about 5.5 times faster than the fastest implementation \nconsidered here. These analyses compute very different things, so the perfor\u00admance comparison is not \napples-to-apples. The AAM approach, and the state transition graphs it generates, encodes temporal properties \nnot found in classical constraint-based analyses for higher-order programs. Such analyses (ultimately) \ncompute judgments on program terms and contexts, e.g., at ex\u00adpression e, variable x may have value v. \nThe judgments do not relate the order in which expressions and context may be evaluated in a program, \ne.g., it has nothing to say with regard to question like, Do we always evaluate e1 before e2? The state \ntransition graphs can answer these kinds of queries, but evaluation demonstrated this does not come for \nfree. 8. Conclusion Abstract machines are not only a good model for rapid analysis de\u00advelopment, they \ncan be systematically developed into ef.cient al\u00adgorithms that can be proved correct. We view the primary \ncontribu\u00adtion of this work as a systematic path that eases the design, veri.ca\u00adtion, and implementation \nof analyses using the abstracting abstract machine approach to within a factor of performant constraint-based \nanalyses. Acknowledgments We thank Suresh Jagannathan for providing source code to the polymorphic splitting \nanalyzer [34] and Ilya Sergey for the introspective pushdown analyzer [10]. We thank Sam Tobin-Hochstadt \nfor encouragement and feedback he was the .rst to prompt us to look into how to make effective imple\u00admentations \nof the AAM approach. We thank Vincent St-Amour and Mitchell Wand for feedback on early drafts and Greg \nMorrisett and Matthias Felleisen for discussions. We thank our anonymous re\u00adviewers for their detailed \ncomments. This material is based on re\u00adsearch sponsored by DARPA under the programs Automated Pro\u00adgram \nAnalysis for Cybersecurity (FA8750-12-2-0106) and Clean-Slate Resilient Adaptive Hosts (CRASH). The U.S. \nGovernment is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding \nany copyright notation thereon. References [1] Dominique Boucher and Marc Feeley. Abstract compilation: \nA new implementation paradigm for static analysis. In Tibor Gyim \u00b4othy, editor, Compiler Construction: \n6th International Conference, CC 96 Link \u00a8oping, Sweden, April 2426, 1996 Proceedings, pages 192 207, \n1996. [2] Martin Bravenboer and Yannis Smaragdakis. Strictly declarative spec\u00adi.cation of sophisticated \npoints-to analyses. In OOPSLA 09: Pro\u00adceedings of the 24th annual ACM SIGPLAN Conference on Object-Oriented \nProgramming, Systems, Languages, and Applications, 2009. [3] M. C. Browne, E. M. Clarke, and O. Gr \u00a8umberg. \nCharacterizing .nite kripke structures in propositional temporal logic. Theor. Comput. Sci., 59(1-2):115 \n131, 1988. [4] Patrick Cousot and Radhia Cousot. Abstract interpretation: a uni.ed lattice model for \nstatic analysis of programs by construction or ap\u00adproximation of .xpoints. In POPL 77: Proceedings of \nthe 4th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Lan\u00adguages, pages 238 252. ACM, 1977. \n[5] Patrick Cousot and Radhia Cousot. Systematic design of program analysis frameworks. In Proceedings \nof the 6th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages, POPL 79, pages 269 282. \nACM, 1979. [6] Olivier Danvy. An Analytical Approach to Program as Data Ob\u00adjects. DSc thesis, Department \nof Computer Science, Aarhus Univer\u00adsity, 2006. [7] Olivier Danvy and Lasse R. Nielsen. Refocusing in \nreduction seman\u00adtics. Research Report BRICS RS-04-26, Department of Computer Sci\u00adence, Aarhus University, \n2004. [8] E. D Osualdo, J. Kochems, and C.-H. L. Ong. Automatic veri.cation of Erlang-style concurrency. \nTechnical report, University of Oxford DCS Technical Report, 2012. http://mjolnir.cs.ox.ac.uk/ soter/papers/erlang-verif.pdf. \n[9] E. D Osualdo, J. Kochems, and C.-H. L. Ong. Soter: An automatic safety veri.er for Erlang. Technical \nreport, University of Oxford DCS Technical Report, 2012. http://mjolnir.cs.ox.ac.uk/soter/ papers/soter-demo.pdf. \n[10] Christopher Earl, Ilya Sergey, Matthew Might, and David Van Horn. Introspective pushdown analysis \nof higher-order programs. In Pro\u00adceedings of the 17th ACM SIGPLAN International Conference on Functional \nProgramming, ICFP 12, pages 177 188. ACM, 2012. [11] Marc Feeley and Guy Lapalme. Using closures for \ncode generation. Comput. Lang., 12(1):47 66, 1987. [12] Matthias Felleisen, Robert B. Findler, and Matthew \nFlatt. Semantics Engineering with PLT Redex. MIT Press, 2009. [13] Pieter H. Hartel, Marc Feeley, Martin \nAlt, Lennart Augustsson, Pe\u00adter Baumann, Marcel Beemster, Emmanuel Chailloux, Christine H. Flood, Wolfgang \nGrieskamp, John H. G. Van Groningen, Kevin Ham\u00admond, Bogumil Hausman, Melody Y. Ivory, Richard E. Jones, \nJasper Kamperman, Peter Lee, Xavier Leroy, Rafael D. Lins, Sandra Loose\u00admore, Niklas R \u00a8ojemo, Manuel \nSerrano, Jean P. Talpin, Jon Thackray, Stephen Thomas, Pum Walters, Pierre Weis, and Peter Wentworth. \nBenchmarking implementations of functional languages with pseu\u00addoknot , a .oat-intensive benchmark. Journal \nof Functional Program\u00adming, 6(04):621 655, 1996. [14] Suresh Jagannathan, Peter Thiemann, Stephen Weeks, \nand Andrew Wright. Single and loving it: must-alias analysis for higher-order languages. In POPL 98: \nProceedings of the 25th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, pages 329 \n341. ACM, 1998. [15] Suresh Jagannathan and Stephen Weeks. A uni.ed treatment of .ow analysis in higher-order \nlanguages. In POPL 95: Proceedings of the22nd ACM SIGPLAN-SIGACT Symposium on Principles of Program\u00adming \nLanguages, pages 393 407. ACM Press, 1995. [16] P. J. Landin. The next 700 programming languages. Commun. \nACM, 9(3):157 166, 1966. [17] Peter J. Landin. The mechanical evaluation of expressions. The Computer \nJournal, 6(4):308 320, 1964. [18] Philippe Meunier, Robert B. Findler, and Matthias Felleisen. Modular \nset-based analysis from contracts. In POPL 06: Conference record of the 33rd ACM SIGPLAN-SIGACT Symposium \non Principles of Programming Languages, pages 218 231. ACM, 2006. [19] Jan Midtgaard. Control-.ow analysis \nof functional programs. ACM Computing Surveys, 2011. [20] Matthew Might and David Van Horn. A family \nof abstract interpre\u00adtations for static analysis of concurrent Higher-Order programs. In Eran Yahav, \neditor, Static Analysis, volume 6887 of Lecture Notes in Computer Science, pages 180 197. Springer Berlin \nHeidelberg, 2011. [21] Matthew Might and Panagiotis Manolios. A posteriori soundness for non-deterministic \nabstract interpretations. In Proceedings of the 10th International Conference on Veri.cation, Model Checking, \nand Ab\u00adstract Interpretation, VMCAI 09, pages 260 274. Springer-Verlag, 2009. [22] Matthew Might and \nOlin Shivers. Improving .ow analyses via GCFA: Abstract garbage collection and counting. In Proceedings \nof the 11th ACM International Conference on Functional Programming (ICFP 2006), pages 13 25, 2006. [23] \nMatthew Might and David Van Horn. Scalable and precise abstractions of programs for trustworthy software. \nTechnical report, 2012. [24] Greg Morrisett. Harvard university course cs252r: Advanced func\u00adtional language \ncompilation. http://www.eecs.harvard.edu/ ~greg/cs252rfa12/. [25] Flemming Nielson and Hanne R. Nielson. \nIn.nitary control .ow anal\u00adysis: a collecting semantics for closure analysis. In POPL 97: Pro\u00adceedings \nof the 24th ACM SIGPLAN-SIGACT Symposium on Princi\u00adples of Programming Languages, pages 332 345. ACM \nPress, 1997. [26] G. D. Plotkin. A structural approach to operational semantics. Tech\u00adnical Report DAIMI \nFN-19, 1981. [27] John C. Reynolds. De.nitional interpreters for Higher-Order pro\u00adgramming languages. \nHigher-Order and Symbolic Computation, 11(4):363 397, 1998. [28] Paul A. Steckler and Mitchell Wand. \nLightweight closure conversion. ACM Trans. Program. Lang. Syst., 19(1):48 86, 1997. [29] Sam Tobin-Hochstadt \nand David Van Horn. Higher-order symbolic execution via contracts. In Proceedings of the ACM International \nConference on Object Oriented Programming Systems Languages and Applications, OOPSLA 12, pages 537 554. \nACM, 2012. [30] David Van Horn and Matthew Might. Abstracting abstract machines: a systematic approach \nto higher-order program analysis. Communica\u00adtions of the ACM, 54:101 109, 2011. [31] David Van Horn and \nMatthew Might. An analytic framework for JavaScript. CoRR, abs/1109.4467, 2011. [32] David Van Horn and \nMatthew Might. Systematic abstraction of ab\u00adstract machines. Journal of Functional Programming, 22(Special \nIs\u00adsue 4-5):705 746, 2012. [33] Dimitrios Vardoulakis and Olin Shivers. CFA2: a Context-Free ap\u00adproach \nto Control-Flow analysis. Logical Methods in Computer Sci\u00adence, 7(2), 2011. [34] Andrew K. Wright and \nSuresh Jagannathan. Polymorphic splitting: an effective polyvariant .ow analysis. ACM Trans. Program. \nLang. Syst., 20(1):166 207, 1998. [35] Feng Zhao. An O(N) algorithm for Three-Dimensional N-Body sim\u00adulations. \nMaster s thesis, MIT, 1987.   \n\t\t\t", "proc_id": "2500365", "abstract": "<p>The technique of abstracting abstract machines (AAM) provides a systematic approach for deriving computable approximations of evaluators that are easily proved sound. This article contributes a complementary step-by-step process for subsequently going from a naive analyzer derived under the AAM approach, to an efficient and correct implementation. The end result of the process is a two to three order-of-magnitude improvement over the systematically derived analyzer, making it competitive with hand-optimized implementations that compute fundamentally less precise results.</p>", "authors": [{"name": "J. Ian Johnson", "author_profile_id": "83059126957", "affiliation": "Northeastern University, Boston, MA, USA", "person_id": "P4261295", "email_address": "ianj@ccs.neu.edu", "orcid_id": ""}, {"name": "Nicholas Labich", "author_profile_id": "83058747357", "affiliation": "Northeastern University, Boston, MA, USA", "person_id": "P4261296", "email_address": "labichn@ccs.neu.edu", "orcid_id": ""}, {"name": "Matthew Might", "author_profile_id": "81309498719", "affiliation": "University of Utah, Salt Lake City, UT, USA", "person_id": "P4261297", "email_address": "might@cs.utah.edu", "orcid_id": ""}, {"name": "David Van Horn", "author_profile_id": "81337494657", "affiliation": "Northeastern University, Boston, MA, USA", "person_id": "P4261298", "email_address": "dvanhorn@ccs.neu.edu", "orcid_id": ""}], "doi_number": "10.1145/2500365.2500604", "year": "2013", "article_id": "2500604", "conference": "ICFP", "title": "Optimizing abstract abstract machines", "url": "http://dl.acm.org/citation.cfm?id=2500604"}