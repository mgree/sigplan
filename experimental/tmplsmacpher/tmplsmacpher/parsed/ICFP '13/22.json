{"article_publication_date": "09-25-2013", "fulltext": "\n A Short Cut to Parallelization Theorems Akimasa Morihata Tohoku University morihata@riec.tohoku.ac.jp \nAbstract The third list-homomorphism theorem states that if a function is both foldr and foldl, it has \na divide-and-conquer parallel imple\u00admentation as well. In this paper, we develop a theory for obtaining \nsuch parallelization theorems. The key is a new proof of the third list-homomorphism theorem based on \nshortcut deforestation. The proof implies that there exists a divide-and-conquer parallel pro\u00adgram of \nthe form of h (x merge y) = h1 x 8 h2 y, where h is the subject of parallelization, merge is the operation \nof integrating independent substructures, h1 and h2 are computations applied to substructures, possibly \nin parallel, and 8 merges the results cal\u00adculated for substructures, if (i) h can be speci.ed by two \ncertain forms of iterative programs, and (ii) merge can be implemented by a function of a certain polymorphic \ntype. Therefore, when require\u00adment (ii) is ful.lled, hhas a divide-and-conquer implementation if h has \ntwo certain forms of implementations. We show that our ap\u00adproach is applicable to structure-consuming \noperations by catamor\u00adphisms (folds), structure-generating operations by anamorphisms (unfolds), and \ntheir generalizations called hylomorphisms. Categories and Subject Descriptors D.1. [Programming Tech\u00adniques]: \nConcurrent Programming Parallel Programming; D.1.2 [Programming Techniques]: Automatic Programming Program \nTransformation; D.3.3 [Programming Languages]: Language Con\u00adstruct and Features Polymorphism Keywords \nDivide-and-conquer Parallelism; Shortcut Deforesta\u00adtion; Third List-homomorphism Theorem 1. Introduction \nFunctional programs are suitable for parallel evaluation. Since they are side-effect free, we can simultaneously \nevaluate independent subexpressions. However, sometimes there are not enough inde\u00adpendent tasks. For \nexample, consider the list-summation function. sum [] = 0 sum (a : x) = a + sum x There are essentially \nno independent tasks in this case, and there\u00adfore, we cannot gain any advantage from parallel computing. \nWe can improve parallelizablity by transforming it to a different implementation. Permission to make \ndigital or hard copies of all or part of this work for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for pro.t or commercial advantage and that copies bear \nthis notice and the full citation on the .rst page. Copyrights for components of this work owned by others \nthan ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post \non servers or to redistribute to lists, requires prior speci.c permission and/or a fee. Request permissions \nfrom permissions@acm.org. ICFP 13, September 25 27, 2013, Boston, MA, USA. Copyright c &#38;#169; 2013 \nACM 978-1-4503-2326-0/13/09. . . $15.00. http://dx.doi.org/10.1145/2500365.2500580 sum [] = 0 sum [a] \n= a sum (x + y) = sum x + sum y Here, the third equation intends to express a divide-and-conquer implementation: \nthe total sum of a longer list x + y is calculated by summing up the results of shorter sublists x and \ny. By evaluating sum x and sum y in parallel, we can achieve a parallel speedup. It is preferable that \nsuch divide-and-conquer implementations be systematically obtained from standard, sequential implementa\u00adtions. \nThe third list-homomorphism theorem (Gibbons 1996a) is useful for this purpose. Theorem 1 (The third \nlist-homomorphism theorem (Gibbons 1996a)). For any function h :: [A]. B, suppose that there ex\u00adist f< \n:: A . B . B, fr :: B . A . B, and e :: B such that h = foldr f< e = foldl fr e. Then, there exists an \noperator (8):: B . B . B such that h(x + y)= hx 8 hy for any x and y. The theorem states that if a function \ncan be written by both foldr and foldl, it can be evaluated in a divide-and-conquer manner as well. It \nhas been used (Gibbons 1996a,b; Gorlatch 1999; Geser and Gorlatch 1999; Morita et al. 2007; Liu et al. \n2011; Chi and Mu 2011) as a basis of deriving divide-and-conquer parallel programs from sequential programs \nwritten by foldr and/or foldl. One might expect that there are similar parallelization theorems that \nessentially state two implementations of certain forms imply a divide-and-conquer implementation . Suppose \nthat a computation on a data structure can be implemented by two different iteration patterns. This fact \nimplies order insensitivity, namely, we can to some extent change the order of operations applied to \nelements. Order insensitivity is an indication of a potential parallelism. In\u00addeed, several such parallelization \ntheorems are known, including the generalization of the third list-homomorphism to tree opera\u00adtions (Morihata \net al. 2009) and their dual (Mu and Morihata 2011). Yet, it was unclear when such parallelization theorems \nhold. The aim of this paper is to develop a theory for this kind of parallelization. We consider a general \ndivide-and-conquer pattern expressed in the form, h(x merge y)= h1 x 8 h2 y (2) Here, h :: S . B is the \nsubject of parallelization and merge :: S1 . S2 . S is a function of integrating substructures x and \ny. The equation means that we can calculate h for x merge y by calculating h1 :: S1 . B1 and h2 :: S2 \n. B2 for substructures x and y, possibly in parallel, and then merging them by using (8):: B1 . B2 . \nB. We prove Equation (2) holds if: h can be written by using h1 as well as by using h2, where h1 and \nh2 are certain forms of recursive functions, and  merge can be speci.ed by a function of a certain polymorphic \ntype. Therefore, if merge satis.es the latter condition, we have a paral\u00adlelization theorem stating that \nh has a divide-and-conquer imple\u00admentation if it has two implementations of certain forms. Now let us \noutline our development. First, we try to determine when 8 exists for given h, merge, h1, and h2 in Equation \n(2). A necessary and suf.cient condition for this has been developed by Mu and Morihata (2011). However, \nit is un\u00adsatisfactory. To apply it to the case of the third list-homomorphism theorem, it requires m2 \nsuch that h(x + y)= m2 (x, hy), (3) instead of h being foldr . Equation (3) is apparently less intuitive. \nFunctional programmers routinely write their programs by using foldr and/or foldl, and one of the most \ninteresting points of the third list-homomorphism theorem is that such routines lead to par\u00adallel programs. \nWe would thus like to keep this point. Our key insight is to view Equation (3) as a deforestation prob\u00adlem \n(Wadler 1988): we would like to develop an implementation, speci.ed by m2, that avoids generating an \nintermediate list by + . We adopt shortcut deforestation (Gill et al. 1993; Gill 1996) for this purpose. \nWhen h is foldr , shortcut deforestation shows that such an m2 exists because + can be speci.ed by a \nfunction of a certain polymorphic type. A similar discussion holds for the case of foldl. In summary, \nshortcut deforestation enables us to prove the third list-homomorphism theorem from the fact that + can \nbe implemented with certain polymorphic functions. The generalizations of shortcut deforestation (Takano \nand Mei\u00adjer 1995; Johann 2002) enable us to naturally generalize our ap\u00adproach from foldr /foldl to a \nclass of data-consuming operations called catamorphisms or folds (Malcolm 1990; Meijer et al. 1991; Bird \nand de Moor 1997). Moreover, our approach can also deal with their dual, i.e., data-generating operations \ncalled anamorphisms or unfolds. Roughly speaking, we consider divide-and-conquer struc\u00adture generations \nof the form, h(v 8 w)= h1 v merge h2 w, (4) and prove that when merge has implementations using functions \nof certain polymorphic types, 8 in Equation (4) exists if h has two implementations by anamorphisms. \nFurthermore, we can also deal with hylomorphisms, which subsume both catamorphisms and anamorphisms. \nThese generalizations suggest that our approach is a modest and yet important step towards a new vista \nof functional parallel programming. Our major contributions are summarized as follows. We provide a \nnew proof to the third list-homomorphism the\u00adorem by using shortcut deforestation (Section 3). Shortcut \nde\u00adforestation has been well studied for optimizing functional pro\u00adgrams; however, to the best of the \nauthor s knowledge, its con\u00adnection to parallelization has not been noticed.  We show that the proof \ncan be naturally generalized to catamor\u00adphisms (Section 4). It subsumes the study by Morihata et al. \n(2009).  We prove that similar theorems hold for anamorphisms, and moreover, the results for catamorphisms \nand anamorphisms can be further generalized to hylomorphisms (Section 5). The result for anamorphisms \nis a generalization of the study by Mu and Morihata (2011). To the best of the author s knowledge, generalizations \nto hylomorphisms have not been studied so far.   2. Preliminary 2.1 Functions and Right Inverses We \nwill use Haskell notations with some extensions. For de.ning functions, we may use functions rather than \nconstructors in pat\u00adterns, such as sum (x + y) = sum x + sum y. The semantics of such equations are their \nleast .xed point. We may sometimes equate the curried variants with the uncurried de.nitions of binary \noperators. For example, (++) may be regarded as a function of type ([a],[a]) . [a]. We write idA to denote \nthe identity function on type A. Subscripts will be omitted if they are not important or ap\u00adparent from \nthe context. We restrict ourselves to using total, terminating, and well\u00adde.ned functions that deal with \n.nite structures. For simplicity of presentations, we may sometimes use functions that appear to be partial \nor possibly non-terminating, and in such cases, we will assume that bad behaviors are avoided somehow. \nFor example, in the above sum , we do not consider binding empty lists to x nor y, so as to avoid non-termination. \nA function g is said to be a right inverse of a function f if f = f.g.f. That is, for any value v in \nthe range of f, v = f (gv). For example, wrap a = [a]is a right inverse of sum , maximum , etc., and \nid is a right inverse of idempotent functions such as sort and .lter p. Any function has a right inverse \nif its domain is enumerable or the axiom of choice is assumed under the set\u00adtheoretic model of functions. \nWe will assume the existence of right inverses, and use f 1 to denote a right inverse of f. 2.2 Categories \nIn order to formalize datatype-generic theorems, we will follow the preceding studies (Malcolm 1990; \nMeijer et al. 1991; Bird and de Moor 1997) and borrow some of the notions from category theory. A category \nconsists of a collection of objects and a collection of arrows between objects. We mainly consider the \ncategory Set, in which objects are sets, arrows are functions, and compositions of arrows are function \ncompositions. Functors are mappings between categories. They should pre\u00adserve identity arrows and arrow \ncompositions. Formally, in Set, F is a functor if FidA = idFA and F(f . g) = Ff . Fg. An easy way to \nunderstand functors is to regard them as type constructors. For example, FInt de.ned by FInt X = (Int \n,X)for a type X and FInt f (n, x) = (n, f x) for a function f and an integer n is a functor. A bifunctor \nis a binary operator, and it behaves as a functor if one of its operands is .xed. Formally, is a bifunctor \nif idA idB = idA B and (f.f') (g.g ')= (f g).(f. g '). A typical bifunctor is the product \u00d7 de.ned by \nA \u00d7 B = (A, B)for types A and B and (f \u00d7 g)(a, b)= (f a, g b)for functions f and g.  2.3 Algebras and \nCatamorphisms For a functor F, an F-algebra is a pair (A,f::FA . A). An algebra morphism between two \nF-algebras, from (A1,f1)to (A2,f2), is an arrow h:: A1 . A2 such that h. f1 = f2 . Fh. An F-algebra is \nsaid to be initial if there exists a unique algebra morphism to each F-algebra. An initial F-algebra \nis unique up to isomorphisms, and we denote it as (\u00b5F,inF). The unique algebra morphism from the initial \nF-algebra is called a catamorphism (aka fold). ([f])F denotes a catamorphism to an F-algebra (A,f). Catamorphisms \nhave two important properties: ([inF])F = id\u00b5F.  ([f])F . inF = f. F([f])F.  Roughly speaking, the \ninitial F-algebra captures a data struc\u00adture: F determines its shape (its branching structure) and inF \ncor\u00adresponds to constructors. In Set, \u00b5F is the least .xed point of X = FX.  For example, the initial \nLc-algebra (\u00b5Lc,inLc ), where LcX = {(1,())} . {(2,(n, x)) | n . Int . x . X}, inLc (1,()) = [], and \ninLc (2,(n, x)) = n : x, captures cons lists of integers. Catamorphisms for Lc-algebras consists of the \nfollowing variants of foldr . foldr :: (LcA . A). [Int ]. A foldr f[] = f(1,()) foldr f(n : x) = f(2,(n, \nfoldr fx)) According to inLc , [] and n : x are interpreted as (1,()) and (2,(n, x)), respectively. They \nare passed to fafter a recursive call. Similarly, snoc lists, in which elements are added to the last, \ncan be captured by the initial Ls-algebra (\u00b5Ls,inLs ), where LsX = {(1,())} . {(2,(x, n)) | x . X . n \n. Int }, inLc (1,()) = [], and inLc (2,(x, n)) = x+ [n]. Catamorphisms for Ls-algebras are foldl. foldl \n:: (LsA . A). [Int ]. A foldl f[] = f(1,()) foldl f(x + [n]) = f(2,(foldl fx, n)) In order to make their \nconnections to catamorphisms explicit, we will use the above de.nitions rather than the standard de.ni\u00adtions \nof foldr and foldl. While they are de.ned only for lists of in\u00adtegers, we could deal with polymorphic \nstructures as well, by using bifunctors; however, for simplicity of presentations, we will only consider \nstructures that consist of integers.  2.4 Shortcut Deforestation We will use shortcut deforestation \n(Gill et al. 1993) and its gener\u00adalizations. Deforestation (Wadler 1988) is a method of eliminating intermediate \ndata structures passed between two functions, often called a generator and a consumer. We will make use \nof the follow\u00ading generalization of shortcut deforestation, which is a variant of the cata-augment rule \n(Gill 1996; Johann 2002). Theorem 5. For any initial F-algebra (\u00b5F,inF)and functors G and H, we have \nH([f])F . gen inF = gen f. G([f])F if gen :: .a. (Fa . a). Ga . Ha. Suppose that the consumer is a catamorphism, \n([f])F, and the generator is a function of a certain polymorphic type using the given constructors, gen \ninF. Then, Theorem 5 enables us to elim\u00adinate the intermediate structure by using f instead of inF and \npre\u00adprocessing the additional input (the apart in Ga) by ([f])F.  3. Proving the Third List-homomorphism \nTheorem by Shortcut Deforestation First, we show that the third list-homomorphism theorem can be proved \nby using shortcut deforestation with an abstract result (Lemma 6). The proof leads to the generalizations \nstudied in the following sections. 3.1 Necessary and Suf.cient Conditions for Simultaneous Computations \nOur starting point is the following lemma, which is essentially due to Mu and Morihata (2011). Lemma \n6. For any bifunctor and functions h :: (A B). C, h1 :: A . A. , and h2 :: B . B. , there exist m1 :: \n(A. B). C and m2 :: (A B'). C such that h = m1 . (h1 id)= m2 . (id h2) if and only if there exists \nm :: (A. B'). C such that h = m . (h1 h2). In fact, m = h . (h 1 1 h2 1). Proof. We will later prove \nits generalization, Lemma 12. Our objective is to calculate h . Since its input consists of sub\u00adcomponents \nA and B, we would like to apply h1 and h2 to them simultaneously. It follows from Lemma 6 that such a \nparallel eval\u00aduation is possible if hcan be calculated in the following two ways: we apply h1 to the \nApart and then merge the result with Bby using m1; we apply h2 and then merge it by using m2. The theorem \nalso states that the function m, which is used for merging the results of the two components, can be \nderived from the right inverses of h1 and h2. For example, consider modk (n1 + n2) where modk n = mod \nn k. Lemma 6 is applicable because modk (n1 + n2) = modk (modk n1 +n2)= modk (n1 +modk n2). Now we reason \nas follows. modk (n1 + n2) = { Lemma 6; since modk is idempotent, id is its right inverse } modk (id \n(modk n1)+ id (modk n2)) = { unfolding id } modk (modk n1 + modk n2) Therefore, we can calculate modk \nn1 and modk n2 in parallel and get modk (n1+n2). This would be worthwhile if the cost of modk is proportional \nto the input.  3.2 Shortcut Deforestation for Con.rming the Premises Lemma 6 appears to be so generic \nthat it can subsume the third list-homomorphism theorem. However, this intuition is wrong. In the case \nof the third list-homomorphism theorem, we would like to .nd m such that h(x + y)= m (hx, hy). Lemma \n6 requires the following condition. .m1,m2. h(x + y)= m1 (hx, y)= m2 (x, hy) (7) However, this condition \nis not satisfactory. For instance, in the case of sum , we can ful.ll Requirement (7) by m1 (v,y) = v+sum \nyand m2 (x, w)= sum x+w, but it appears to be almost equivalent to developing a parallel implementation. \nIt would more easier for functional programmers to con.rm that h is both foldr and foldl. To resolve \nthis issue, we use shortcut deforestation, in particular Theorem 5. Recall that foldr and foldl are catamorphisms \nfor two representations of lists. Therefore, Requirement (7) follows from Theorem 5 if + can be speci.ed \nby two polymorphic functions. It is actually possible. x + y = gen1 inLs (x, y)= gen2 inLc (x, y) gen1 \n:: .a. (Lsa . a). (a, [Int ]) . a gen1 f(x, []) = x gen1 f(x, y+ [b]) = f(2,(gen1 f(x, y),b)) gen2 :: \n.\u00df. (Lc\u00df . \u00df). ([Int ],\u00df). \u00df gen2 f([],y) = y gen2 f(a : x, y) = f(2,(a, gen2 f(x, y))) Now we prove \nTheorem 1. Proof of Theorem 1. From Lemma 6, it is suf.cient to show that Requirement (7) is met. Since \nh = foldl f1 = foldr f2, The\u00adorem 5 guarantees that it is satis.ed by m1 = gen1 f1 and m2 = gen2 f2. \n  4. Generalizing The Third List-homomorphism there exists m :: A B . C such that Theorem h = m . (([f1])F \n ([f2])G). 1 4.1 Meta-theorems for Parallelizing Catamorphisms In fact, m = h . (([f1])F ([f2]) G ). \n1 Our approach can be naturally generalized to arbitrary catamor\u00adphisms. Let us consider parallelizing \nh (x merge y). We assure that there exists m such that h(x merge y)= m (h1 x, h2 y)by Proof. From Lemma \n6, it is suf.cient to show that there exist m1 and m2 such that proving that (i) hcan be written as two \nkinds of catamorphisms and (ii) merge can be written as two kinds of polymorphic functions. To be precise, \nwe use two merging functions because two different catamorphisms have different types. The idea is embodied \nin the following theorem. Theorem 8. Let (\u00b5F,inF)and (\u00b5G,inG)be two initial algebras and be a bifunctor. \nAssume that merge1 :: (\u00b5F \u00b5G). \u00b5F and merge2 :: (\u00b5F \u00b5G). \u00b5Gare de.ned by using polymorphic functions \nas follows. merge1 = gen1 inF gen1 :: .a. (Fa . a). (a \u00b5G). a merge2 = gen2 inG gen2 :: .\u00df. (G\u00df . \u00df). \n(\u00b5F \u00df). \u00df Then, for any h :: (\u00b5F \u00b5G). C such that h = ([f1])F . merge1 = ([f2])G . merge2, there exists \nan m such that h = m . (([f1])F ([f2])G). 1 1 In fact, m h . (([f1]) F ([f2])G ). = h = m1 . (([f1])F \n id)= m2 . (id ([f2])G). Theorem 5 shows m1 = E1 . gen1 f1 and m2 = E2 . gen2 f2. We will prove the \nformer case here; the proof of the other case is similar. h = { premise, and unfolding merge1 } E1 . \nH1([f1])F . gen1 inF = { Theorem 5 } E1 . gen1 f1 . (([f1])F id) Theorem 9 states that if each mergei \nis implemented by a func\u00adtion of a certain polymorphic type, we obtain a parallelization the\u00adorem that \nparallelize h speci.ed by a catamorphism with an addi\u00adtional function, Ei. It is worth noting that the \nadditional function improves expressiveness. For example, Theorem 9 can cope with mutumorphisms (Fokkinga \n1989) and adjoint folds (Hinze 2013). Keen readers might notice that parallelization theorems derived \nfrom Theorem 9 are applicable to almost all computations. For instance, since h = h . foldl inLs = h \n. foldr inLc for any h:: [Int ]. C, Theorem 9 is applicable to any list operation. How\u00adever, in this \ncase, the theorem does not provide any useful insight for obtaining ef.cient parallel programs: Note \nthat id is the right Proof. We will prove a more general theorem, Theorem 9. Theorem 8 works as a meta-theorem \nthat derives theorems like the third list-homomorphism theorem. Given a structure \u00b5F \u00b5G that contains \nsubstructures, \u00b5F and \u00b5G, suppose that there exist two merging functions, merge1 and merge2, de.ned by \nusing polymorphic functions, gen1 and gen2. Then, if h can be described by two patterns, each of which \nis a composition of a catamorphism and a merging function, h can be calculated by simultaneously applying \n([f1])F and ([f2])G to the substructures. Theorem 8 directly generalizes the third list-homomorphism \ntheorem. In this case, F = Ls, G = Lc, = \u00d7, and both merge1 and merge2 are (++). Two catamorphisms correspond \nto foldl and foldr . An isomorphism between the snoc lists (\u00b5Ls) and the cons lists (\u00b5Lc) is implicitly \nused. We can generalize Theorem 8. Note that shortcut deforestation is applicable if there is a composition \nof a catamorphism and a merging function. This implies that h can include other computa\u00adtions. Theorem \n9. Let (\u00b5F,inF) and (\u00b5G,inG) be two initial al\u00adgebras, be a bifunctor, and H1 and H2 be functors. Further\u00admore, \nsuppose that merge1 :: (\u00b5F \u00b5G). H1\u00b5F and merge2 :: (\u00b5F \u00b5G). H2\u00b5Gare de.ned as follows. merge1 = gen1 \ninF gen1 :: .a. (Fa . a). (a \u00b5G). H1a merge2 = gen2 inG gen2 :: .\u00df. (G\u00df . \u00df). (\u00b5F \u00df). H2\u00df Then, for any \nh :: (\u00b5F \u00b5G). C, E1 :: H1A . C, E2 :: H2B . C, f1 :: FA . A, and f2 :: FB . B such that h = E1 . H1([f1])F \n. merge1 = E2 . H2([f2])G . merge2, inverse of both of foldl inLs and foldr inLc ; hence, the theorem \nyields h (x + y) = h (id (foldl inLs x)+ id (foldr inLc y)), which is trivial because id . foldl inLs \n= id . foldr inLc = id. As we observed above, the parallel implementations obtained from our theorems \nmight be useless and/or terribly slow. This is a limitation of our approach. We will discussions this \nissue in Section 6.2. Another issue is the way of dividing the given structure. In order to use Theorems \n8 and 9, we need to provide a division, speci.ed by \u00b5F \u00b5G, of the input. The division determines ef.ciency \nof the obtained parallel programs. For linear structures such as lists, it is reasonable to divide the \ninput at the middle. For more complex structures, it is nontrivial to provide a good division that supports \nrecursive divisions, yields substructures of similar sizes, and is achieved ef.ciently. However, ways \nof providing good divisions are beyond the scope of this paper.  4.2 Deriving Divide-and-conquer Tree \nOperations Let us demonstrate the use of Theorem 9 by developing a paral\u00adlelization theorem for the binary \ntrees de.ned below. data Tree = Lf Int | Nd Tree Int Tree First, we decide the way of dividing trees. \nA naive strategy is to isolate siblings and thereby deal with them in parallel. Our theorem proves that \na tree operation can be implemented with this strategy if it is both left-to-right, namely iterating \nthe left subtree .rst, and right-to-left, namely iterating the right subtree .rst. Instead, we will employ \na different approach because the above strategy is little effective if the input tree is slender and \ntall, like a list. To exploit more parallelism, we divide up a tree not only at the root but also at \nan arbitrary internal node. Division at an internal node results in three substructures: two subtrees, \nand a one\u00adhole context that retains the upper part. Therefore, we need a data structure for representing \none-hole contexts. We will use Huet s zipper (Huet 1997).  n 1Case (i):Case (ii):  rootleft . (n1, \nt1). n 2 (t2, n2)right ~ . n 3 t1 t2 t3 Figure 1. Correspondence between a zipper structure and a one\u00adhole \ncontext: the dashed line denotes the hole, and thus, the corre\u00adsponding subtree is missing. root data \nZipper = | Zipper (Int ,Tree)left . . | Zipper (Tree,Int )right . right For readability, we will use \npost.x constructors, leftand . . . Figure 1 outlines the correspondence between the zipper structure \nand the one-hole context. Zippers essentially retain the paths from leftright their root to their hole. \n. and . correspond to paths to the left and right children, respectively. The following operator, (<)::Zipper \n. Tree . Tree, plugs a tree into the hole of a zipper. root < t = t (z (n, r)left= n r . . )<t z< Nd \nt (z (l, n)right . )< t = z < Nd l n t Next, we formalize zippers by using initial algebras. We make \nisomorphisms Zipper ~ \u00b5Fand Tree ~ \u00b5G, as follows. FX = {(1,())} . {(2,(x, n, t)) | x . X . n . Int \n. t . Tree}. {(3,(x, t, n)) | x . X . t . Tree . n . Int }inF (1,()) = root . inF (2,(x, n, t)) = x \n(n, t)left . x (t, n)right inF (3,(x, t, n)) = . GX = {(1, n)| n . Int } . {(2,(x1, n, x2)) | x1, \nx2 . X . n . Int } inG (1, n) = Lf n inG (2,(x1, n, x2)) = Nd x1 n x2 Next, we prepare two merging functions. \nIt is natural to choose (<) as merge 2. We also need merge 1 that merges a tree into a zipper. However, \nwhile merging a zipper with a tree results in a tree, a zipper represents not a tree but a one-hole context. \nThus, we use (.):: Zipper . Tree . (Zipper ,Int ), characterized as follows, as the other merging function. \nIt calculates a zipper as well as an integer, which corresponds to the value of the missing leaf. ' ' \nz . t = (z, n) such that z < t = z<Lf n Finally, let us con.rm that they can be implemented by func\u00adtions \nof the required polymorphic type. z . t = gen1 inF (z, t) where gen1 :: .a. (Fa . a). (a, Tree). (a, \nInt ) gen1 f(z, Lf n) =(z, n) gen1 f(z, Nd l n r) = gen1 f(f(2,(z, n, r)), l) z< t = gen2 inG (z, t) \nwhere gen2 :: .\u00df. (G\u00df . \u00df). (Zipper , \u00df). \u00df f(root gen2 . , t) = t gen2 ., t) = gen2 f(z, f (2,(t, n, \nr))) f(z (n, r)left f(z (l, n)right gen2 . , t) = gen2 f(z, f (2,(l, n, t))) Now that all the requirements \nare ful.lled, the following corol\u00adlary follows from Theorem 9.    Figure 2. Top-down calculation \nof the least common ancestor: the black bullets denote null pointers, and the double circles denote candidates \nof the least common ancestors. The dashed lines denote subtrees that have not been explored yet. Corollary \n10. For any h :: (Zipper \u00d7 Tree). C, f1 :: FA . A, f2 :: GB . B, E1 :: (A\u00d7 Int ). C, and E2 :: B . C, \n' ' h(z, t) = let (z, n)= z . t in E1 (([f1])F z, n) = E2 (([f2])G (z< t)) implies that there exists \nm :: (A\u00d7 B). C such that h(z, t)= m (([f1])F z, ([f2])G t). Actually, m = F \u00d7 ([f2]) 1 h. (([f1]) 1 \nG ). While catamorphisms on trees are bottom-up, those on zippers are top-down. Thus, Corollary 10 enables \nus to derive parallel programs from top-down and bottom-up implementations. In order to implement the \nobtained programs, we need an im\u00adplementation of our division strategy. For this purpose, we can use \nthe m-bridge technique (Reif 1993), which enables us to divide a tree into reasonably-sized segments, \neach of which is either a sub\u00adtree or a one-hole context. Its details are beyond the scope of this paper \nand thus omitted. Example: Least Common Ancestor Let us demonstrate the use of Corollary 10 by deriving \na parallel program for .nding the least common ancestor of null pointers. We assume that the address \nof each node is stored as its node value; thus, null pointers are 0-valued leaves. It is easy to implement \nthe task in a bottom-up manner. Let lca be the function, lca (Lf n) = if n = 0then Just n else Nothing \nlca (Nd l n r) = lca aux (lca l)n (lca r) where lca aux (Just -)n (Just -) = Just n lca aux (Just n1)- \nNothing = Just n1 lca aux Nothing - (Just n2) = Just n2 lca aux Nothing - Nothing = Nothing lca results \nin Nothing if the tree contains no null pointer, and Just n otherwise, where n is the address of the \nleast common ancestor. lca aux integrates the results of subtrees: if both subtrees contain null pointers, \nit returns the root; otherwise, it propagates the results of the subtrees. From this de.nition, it is \neasy to see that lca is a catamor\u00adphism for Tree (i.e., \u00b5G): lca = ([f2])G, where f2 (1, n) = if n = \n0 then Just n else Nothing and f2 (2,(v1, n, v2)) = lca aux v1 n v2. To apply Corollary 10, we also need \na top-down implementation of lca by using a catamorphism on Zipper (i.e., \u00b5F). In calculating lca while \ngoing down a path, we face possibly three situations: (i) null pointers have been found on two different \nsubtrees along the path, and therefore, the least common ancestor has been already known; (ii) null pointers \nhave been found in exactly one subtree, and thus, the least common ancestor is either one in the subtree \nor the branching node to the subtree, and (iii) no null pointer has   (t3, n3)right .   been found. \nFigure 2 shows cases (i) and (ii). We use the following datatype for expressing these cases. data Resultlca \n= Fixed Int | Or Int Int | None Fixed n corresponds to the .rst case, where n is the least common ancestor. \nOr n1 n2 retains two possible candidates, and the least common ancestor is n1 if another null pointer \nwill be found, and n2 otherwise. None means that no null pointer is found. The following is a zipper \nversion of lca . lca Zipper (z, n)= E1 (lca Z z, n) lca Z (root . ) = None lca Z (z (n, r)left. ) = lca \nZ (lca Z z)n (lca r) aux lca Z (z (l, n)right = lca Z (lca Z z)n (lca l) where lca Z (Fixed = Fixed \nn . ) aux aux n)- - lca Z (Or n1 n2)- (Just -) = Fixed n1 aux lca Z (Or n1 n2)- Nothing = Or n1 n2 aux \nlca Z None n1 (Just n2) Or n1 n2 aux = lca Z None - Nothing = None aux E1 (Fixed n ' ,-) = Just n ' E1 \n(Or n1 n2, n) = if n = 0then Just n1 else Just n2 E1 (None , n) = if n = 0then Just n else Nothing Although \nthe above is rather complicated, it can be implemented by a zipper catamorphism: lca Z = ([f1])F where \nf1 (1,()) = None and f1 (2,(z, n, t)) = f1 (3,(z, t, n)) = lca Z z n (lca t). aux We have con.rmed that \nCorollary 10 is applicable. Therefore, there is a divide-and-conquer implementation of lca . Moreover, \nthe implementation can be derived from the right inverses of lca and lca Z, which are not dif.cult to \n.nd. lca 1 (Just n) = Nd n (Lf 0) (Lf 0) lca 1 Nothing = Lf 1 1 . (n, Lf 0) left root. (1,Lf 0) left \nlca Z (Fixed n) = . 1 . (n1,Nd n2 (Lf 0) (Lf 0)) left lca Z (Or n1 n2) = root . root lca Z 1 None = . \nNow let us work out a divide-and-conquer implementation of lca (z < t). In the following, we will focus \non the case of lca Z z = Or n1 n2 and lca t = Just n. lca (z < t) = { Corollary 10 } 1 1 lca (lca Z \n(lca Z z)< lca (lca t)) = { unfolding lca Z 1 , lca 1, and (<)}lca (Nd (Nd n (Lf 0) (Lf 0)) n1(Nd n2 \n(Lf 0) (Lf 0))) = { unfolding lca }Just n1 Similar calculations will yield implementations for the other \ncases. We will omit them. Consequently, we obtain the following pro\u00adgram. lca (z < t)= lca DC (lca Z \nz) (lca t) aux where lca DC (Fixed = Just n aux n)- lca DC (Or n1 n2) (Just -) = Just n1 aux lca DC (Or \nn1 n2)Nothing = Just aux n2 lca DC None (Just = Just n aux n) lca DC None Nothing = Nothing aux This \nprogram implemented by using the m-bridge technique runs in O(n/p + p)time for a tree of size n on the \nPRAM of p processors. Hence, if n . p, it .nds the least common ancestor in time O(n/p)regardless of \nthe height of the tree.  4.3 Deriving Divide-and-conquer Set Operations As an another application of \nTheorem 9, let us consider paral\u00adlelizing set manipulations described by the following fold Set . Let \nadd a x = {a} . x. fold Set (.)e \u00d8 = e fold Set (.)e (add a x) = a . fold Set (.)e x To make the function \nwell-de.ned, we need a.(b.v)= b.(a.v) and a.(a.v)= a.v for v in the range of fold Set (.)e. Among the \nalgebras that have these properties, \u00d8 and add form the initial one and fold Set corresponds to the catamorphisms. \nTheorem 9 leads to a parallelization theorem for fold Set , which states that any set operation implemented \nby fold Set is potentially parallelizable. Corollary 11. For any h :: Set A . C, E :: B . C, (.) :: (A, \nB). B, and e :: B h = E. fold Set (.)e implies that there exists m such that h(x . y)= m (fold Set \n(.)e x, fold Set (.)e y), where m (v, w)= h ((fold Set (.)e) 1 v . (fold Set (.)e) 1 w). Proof. From \nTheorem 9 and the commutativity of (.), it is suf.\u00adcient to show that (.)can be speci.ed by a certain \npolymorphic function. x . y = gen add \u00d8 (x, y) where gen :: .\u00df. (A . \u00df . \u00df). \u00df . (Set A, \u00df). \u00df gen (.)e \n(\u00d8, y)= y gen (.)e (add a x, y)= a . gen (.)e (x, y) Remark To be precise, Corollary 11 is not an instance \nof Theo\u00adrem 9: gen takes not a function of the type of P\u00df . \u00df, where P is a functor such that \u00b5P ~ Set \nA, but (.):: (A . \u00df . \u00df) with e :: \u00df. Especially, gen is unaware of the algebraic properties of P, such \nas the associativity and the commutativity, which make P different from Lc. Hence, the fact that gen \nhas that polymorphic type is actually not suf.cient to prove the corollary, though the rest of the proof \nis straightforward and is very similar to that of Theo\u00adrem 9. We use this formalization because it seems \ndif.cult to take the algebraic properties of P into account without spoiling rela\u00adtional parametricity \n(Reynolds 1983; Wadler 1989), which is the basis of shortcut deforestation. Example: Finding Celebrities \nWe apply Corollary 11 to the .nding celebrities problem studied by Bird and Curtis (2006). In the attendees \nof a party, we call a person a celebrity if he/she is known by all the attendees but knows only the other \ncelebrities. A set of celebrities is unique, but may be empty. From the perspective of graph theory, \nit can be viewed as a problem on a directed graph to .nd a clique that satis.es a kind of isolation property. \nAssume that there is a celebrity1. Bird and Curtis (2006) showed that the following linear-time program \n.nds the set of celebrities. celeb = fold Set step \u00d8 where step a \u00d8 = {a}step a (add b y) = (if b know \n a then {a} else \u00d8) . (if a know bthen add b y else \u00d8) 1 To be precise, so as to let celeb be a function, \nwe assume that every subset of the input has a nonempty set of celebrities.  It might be hard to see \nthat this program correctly .nds celebri\u00adties. Nevertheless, Corollary 11 states that it is parallelizable. \nMore\u00adover, since celeb is idempotent, id is its right inverse. Therefore, we can obtain the following \ndivide-and-conquer implementation. celeb (x . y) = { Corollary 11 }fold Set step \u00d8 (id (celeb x). id \n(celeb y)) = { Unfolding id, and a simpli.cation }fold Set step (celeb y) (celeb x) We can improve ef.ciency \nas follows. We call two persons friends if they know each other. Observe that a friend of a celebrity \nis a celebrity, whereas a friend of a non-celebrity is not. In fact, celeb maintains a chain of friendship \nrelations. Now, we check whether a . celeb x and b . celeb y know each other. bknows a: If a (and thereby \nany of celeb x) is not a celebrity, b(as well as celeb y) is not either. Therefore, if there are celebrities, \nthey should contain celeb x. bdoes not know a: Apparently a is not a celebrity, and thus, none of celeb \nx is a celebrity. The same goes for celeb y. Consequently, we obtain the following program. celeb (x \n. y)= m (celeb x, celeb y) where m (\u00d8,y ) = y m ( x, \u00d8) = x m (add a y) x, add b = (if b know a then \nadd a x else \u00d8) . (if a know bthen add by else \u00d8) Suppose that sets are implemented by linked lists \nthat support constant-time concatenations. Then, m is a constant-time function, and hence, we can .nd \nthe celebrities in O(n/p+ log p)time on a PRAM of pprocessors, where n is the number of attendees.  \n 5. Dualization and Generalization to Hylomorphisms So far, we have developed a theory for deriving parallel \nprograms from two catamorphisms, i.e., structure-consuming operations. In parallel programs, data distributions, \nrather than consumptions, of\u00adten form bottlenecks. Therefore, data generations should be paral\u00adlelized: \neach processor receives a seed of the data and generates part of the data independently. We can view \nthe development of such parallel data generations as a parallelization of the dual of catamorphisms, \ncalled anamorphisms (Malcolm 1990; Meijer et al. 1991; Bird and de Moor 1997), which generate structures. \nHere, we provide a theorem for deriving divide-and-conquer implemen\u00adtations for programs speci.ed by \nanamorphisms. Moreover, we show that our theory can be further generalized to compositions of catamorphisms \nand anamorphisms, called hylo\u00admorphisms. In hylomorphisms, structures are generated and imme\u00addiately \nconsumed; thus, they correspond to a kind of on-the-.y structure manipulation. 5.1 Brief Introduction \nto Relations Following Mu and Morihata (2011), we will use relations rather than functions for formalizing \nthe dualization. Since inputs and out\u00adputs are symmetric for relations, properties about relations can \nbe dualized by taking relational converses. We stress that our objective is the development of functions \nrather than relations. Relations are only used for formalizations and proofs. In the following, we brie.y \nintroduce relations (Details can be found in (Bird and de Moor 1997)). A relation Rbetween two sets A \nand B is a subset of B \u00d7 A. We write its type as R :: A . B. A relation R:: A . B is a function if .(b, \na),(b ' , a). R. b = b ' and .a . A. .(b, a) . R. We may write b = R a to mean (b, a) . R if R is a function, \nand b . R a otherwise. The relational composition of R:: A . B and S :: B . C is de.ned by (c, a) . (S \n. R) .. (.b. (c, b) . S . (b, a) . R). Note that it generalizes the function compositions. The relational \nconverse of R :: A . B, denoted by R. :: B . A, is de.ned by (a, b). R. .. (b, a). R. Note that (R. S). \n= S. . R. . In order to formalize structure manipulations using relations, we extend our categorical \nmodel from Set to Rel, where objects are sets, arrows are relations, and arrow compositions are relational \ncompositions. In Rel, we require that each functor F preserves converses, i.e., (FR). = F(R.). Such functors \nare called relators. All of the standard functors are relators, and therefore, we will take functors \nto mean relators. We also require bifunctors to have similar properties. Lemma 6 can be generalized to \nrelations. Lemma 12. For any bifunctor , the relations H :: A1 A2 . B, H1 :: A1 . B1, H2 :: A2 . B2, \nR1 :: B1 . A1 and R2 :: B2 . A2, assume H1 . R1 . H1 = H1 and H2 . R2 . H2 = H2. Then, there exist relations \nM1 :: B1 A2 . B and M2 :: A1 B2 . B such that H = M1 . (H1 id)= M2 . (id H2) if and only if there \nexists a relation M :: (B1 B2). B such that H = M . (H1 H2). Proof. The if part is easy. M1 = M . (id \n H2) and M2 = M . (H1 id)apparently satisfy the requirement. The only if part is a generalization of \nMu and Morihata (2011) from the product to arbitrary bifunctors. The proof is almost the same as theirs, \nand M = H . (R1 R2)satis.es the requirement. H = { premise } M1 . (H1 id) = { H1 . R1 . H1 = H1, and \n is a bifunctor }M1 . (H1 id). (R1 id). (H1 id) = { premise }M2 . (id H2). (R1 id). (H1 id) = { \nH2 . R2 . H2 = H2, and is a bifunctor }M2 . (id H2). (R1 R2). (H1 H2) = { premise }H . (R1 R2). \n(H1 H2)  5.2 Dualization 5.2.1 Anamorphisms The dual of an F-algebra, called an F-coalgebra, is a \npair (A, . :: A . FA). An F-coalgebra morphism from (A1, .1)to (A2, .2) is an arrow h:: A1 . A2 such \nthat Fh. .1 = .2 . h. From any F-coalgebra, a .nal F-coalgebra has a unique coalgebra morphism. A .nal \nF-coalgebra is unique up to isomorphisms and is denoted by (.F,out F). [(.)]F denotes the unique coalgebra \nmorphism from the F-coalgebra (A, .), and is called an anamorphism or an unfold. Anamorphisms have the \nfollowing two properties. [(out F)]F = id.F.  F[(.)]F . . = out F . [(.)]F.  Anamorphisms are generalizations \nof unfoldr . More precisely, in Set, the following unfoldr captures the anamorphisms for (.Lc,out Lc \n). unfoldr :: (A . LcA). A . [Int ] unfoldr . a = case . a of (1,()) . [] (2,(n, a ' )) . n : unfoldr \n. a '  Similarly, the following unfold l captures the anamorphisms for snoc lists. unfold l :: (A . \nLsA)) . A . [Int ] unfold l . a = case . a of (1,()) . [] (2,(a ' ,n)) . unfold l . a ' + [n] They can \nbe de.ned in Rel as well, but we will omit them because we are less interested in relational programs. \nIn Rel, the initial F-algebra and the .nal F-coalgebra have a close connection. For a .nal F-coalgebra \n(.F,out F), there exists unique h such that Fh . . = out F . h for any F-coalgebra (A, .). This is equivalent \nto that there exists unique h. such that .. .Fh. = h. .out F . for any F-algebra (A,..). Thus, (.F,out \nF) is a .nal F-coalgebra if and only if (.F,out F.) is an initial F\u00adalgebra. In other words, inF = out \nF . and \u00b5F = .F. Moreover, this discussion also clari.es ([f]). F = [(f.)]F and ([f.])F = [(f)]. F 2 \n.  5.2.2 Parallelization Theorem for Anamorphisms Since anamorphisms are converses of catamorphisms \nin Rel, one may think that theorems about catamorphisms can be immediately dualized by taking the relational \nconverses. However, there is a technical dif.culty. It is known that nondeterminism and partiality complicate \nrelational parametricity (Reynolds 1983; Wadler 1989); therefore, we cannot simply generalize Theorem \n5 to relations. Nevertheless, the following theorem for parallelizing anamor\u00adphisms holds. Theorem 13. \nSuppose the same situation as Theorem 9 about the initial algebras (\u00b5F,inF)and (\u00b5G,inG), functors H1 \nand H2, bi\u00adfunctor , and functions merge1::\u00b5F \u00b5G . H1\u00b5Fand merge2:: \u00b5F \u00b5G . H2\u00b5G. Then, for any relation \nh ::C . (\u00b5F \u00b5G)such that h = merge1 . . H1[(.1)]F . t1 = merge2 . . H2[(.2)]G . t2, where .1 :: A . \nFA and .2 :: B . GB are functions (however t1 :: C . H1Aand t2 :: C . H2B may be relations), there exists \na relation s :: C . (A B)such that h = ([(.1)]F [(.2)]G). s. Actually, s = F [(.2)] 1 h. ([(.1)] 1 \nG ). Proof. The key is the following lemma. It can be proved from relational parametricity. Because its \nproof contains the techniques of relational calculations, we leave it for the Appendix. Lemma 14. For \nany initial algebra (\u00b5F,inF), functor G, bifunc\u00adtor , functions g :: .a. (Fa . a). (a B). Ga and . :: \nA . FA, there exists a relation R:: GA . (A B)such that (g inF). . G[(.)]F = ([(.)]F id). R. Now, by \ntaking the converse of Lemma 12, we can see that it is suf.cient that there exist s1 and s2 such that \n h = ([(.1)]F id). s1 = (id [(.2)]F). s2. We prove the case of s1. The case of s2 is similar. h = \n{ premise } merge1 . . H1[(.1)]F . t1 = { Lemma 14, and let s1 = R. t1 }([(.)]F id). s1 2 This might \nappear to be strange. Usually \u00b5F and .F are respectively in\u00adterpreted as a set of all .nite structures \nand one of all possibly in.nite struc\u00adtures. However, in the presence of relational converses, one can \ngenerate in.nite structures if and only if one can consume them as well; therefore, \u00b5Fand .Fcoincide. \nThe objective is to generate either \u00b5For \u00b5G. We use h to obtain \u00b5F \u00b5G, which can be transformed into \n\u00b5F and \u00b5G by merge1 and merge2, respectively. hpotentially generates all possible divi\u00adsions, namely, \nthose from which each mergei yields the .nal re\u00ad sult. If h has two implementations, each of which consists \nof an anamorphisms after an additional computation, Theorem 13 guar\u00adantees that the independent substructures \ncan be generated in par\u00adallel. To be precise, there exists a relation s that yields seeds from which \nall possible divisions can be calculated. 5.2.3 Example: downfrom Here, we illustrate the use of Theorem \n13 with a small example, an downfrom function de.ned as follows. downfrom n = if n = 0then []else n : \ndownfrom (n - 1) We can write downfrom by two anamorphisms, unfold l and unfoldr . downfrom = unfold \nl .1 . t1 = unfoldr .2 t1 n = (n, 1) .1 (n, m)= if n < m then (1,()) else (2,((n, m + 1),m)) .2 (n, m)= \nif n = 0then (1,()) else (2,(n, n - 1)) Now let h = (++). . downfrom ; namely, h n generates (x, y) such \nthat x + y = downfrom n. Recall that lists have two appropriate merging functions, both being essentially \n(++). Thus, from Theorem 13, we can .nd s such that h = (unfold l .1 \u00d7 unfoldr .2). s, as follows. Note \nthat (unfold l .1) 1 [n, n - 1,. . . ,m]= (n, m)and (unfoldr .2) 1 [n, n - 1,. . . ,1] = n. ((n1,m),n2). \ns n . { Theorem 13 } .(x, y). h n. (unfold l .1) 1 x = (n1,m). (unfoldr .2) 1 y = n2 . { unfolding h \n} .x, y. x + y = [n, n - 1,. . . ,1] . (unfold l .1) 1 x = (n1,m). (unfoldr .2) 1 y = n2 . { unfolding \n+ } .k. (unfold l .1) 1 [n, n - 1,. . . ,k+ 1] = (n1,m). (unfoldr .2) 1 [k,k - 1,. . . ,1] = n2 . { unfolding \n(unfoldl .1) 1 and (unfoldr .2) 1 } .k. (n, k+ 1) = (n1,m). k = n2 Consequently, we have proven that \ndownfrom n = unfold l .1 (n, k+ 1) + unfoldr .2 k for any n = k = 0. This equation expresses a divide-and-conquer \nimplementation of downfrom, where the pre.x and suf.x are gen\u00aderated independently.  5.2.4 Dif.culties \nof Parallelizing Anamorphisms Theorem 13 is theoretically interesting, and it has a few applica\u00adtions \nsuch as those discussed by Mu and Morihata (2011). How\u00adever, from the author s own experience, Theorem \n13 is more dif.\u00adcult to use than Theorem 9. First of all, Theorem 13 concerns relations. One may want \nto take a functional approach, namely, develop a function s ' such that downfrom = (++).(unfold l .1\u00d7unfoldr \n.2).s ' . Unfortunately, this is not always satisfactory: s ' n = ((n, n),n - 1) satis.es the requirement, \nand it yields [n]and [n - 1,n - 2,. . . ,1] after applying (unfold l .1 \u00d7 unfoldr .2). Since the pre.x \npart is always a singleton, it is like a variant of unfoldr rather than divide\u00adand-conquer. Theorem 13 \nstates, by using relations, that we could generate not particular but arbitrary pre.x/suf.x pairs. Another \nproblem is that it is usually more dif.cult to derive right inverses for anamorphisms. On one hand, right \ninverses of catamorphisms generate structures. This means that we are often able to .nd a non-recursive \nright inverse that yields a structure of a constant size, like wrap. This fact is also helpful for automatic \npar\u00adallelization (Morita et al. 2007). On the other hand, right inverses of anamorphisms consume structures, \nand therefore, they are es\u00adsentially recursive functions. Needless to say, recursive functions are more \ndif.cult to reason about.   5.3 Hylomorphisms Next, we consider hylomorphisms, which are compositions \nof cata\u00admorphisms and anamorphisms. Given an initial algebra (\u00b5F,inF), a hylomorphism [ f, .] F is de.ned \nby [ f, .] F = ([f])F.[(.)]F. The composition makes sense because the initial F-algebra and the .nal \nF-coalgebra coincide in our setting. Hylomorphisms are generalizations of catamorphisms and anamorphisms, \n([f])F = [ f, out F] F and [(.)]F = [ inF,.] F. Moreover, hylomorphisms include functions that do not \nexplic\u00aditly concern structures. Instead, they implicitly construct and consume structures in the form \nof recursion trees. For example, prod n = if n = 1 then 1 else n * prod (n - 1) is a hylomor\u00adphism, because \nprod = foldr (*)1. downfrom. The intermediate list corresponds to a series of arguments of recursive \ncalls. 5.3.1 Parallelization Theorem for Hylomorphisms Combining Theorems 9 and 13 yields a parallelization \ntheorem for hylomorphisms. Theorem 15. Suppose the same situation as Theorems 9 and 13 about the initial \nalgebras (\u00b5F,inF) and (\u00b5G,inG), functors H1 and H2, bifunctor , and functions merge1 :: (\u00b5F \u00b5G). H1\u00b5F \nand merge2 :: (\u00b5F \u00b5G). H2\u00b5G. Assume that for h :: C1 . C2 there exist t1 :: C1 . H1A1, t2 :: C1 . H2B1, \n.1 :: A1 . FA1, .2 :: B1 . GB1, f1 :: FA2 . A2, f2 :: GB2 . B2 , E1 :: H1A2 . C2, and E2 :: H2B2 . C2 \nsuch that we have the fol\u00adlowing conditions. merge1 and merge2 are surjective h = E1 . H1[ f1,.1] F \n. t1 = E2 . H2[ f2,.2] G . t2  merge1 . . H1[(.1)]F . t1 = merge2 . . H2[(.2)]G . t2  E1 . H1([f1])F \n. merge1 = E2 . H2([f2])G . merge2  Then, there exist m :: (A2 B2). C2 and s :: C1 . (A1 B1) such \nthat h = m . ([[f1,.1] F [ f2,.2] G). s. Actually, m = E1 . H1([f1])F . merge1 F ([f2]) 1 . (([f1]) \n1 G )and s = F [(.2)] 1 1 . . H1[(.1)]F . t1. ([(.1)] 1 G ). merge Proof. h = E1 . H1[ f1,.1] F . t1 \n= E2 . H2[ f2,.2] G . t2 . { hylomorphisms and functors } h = E1 . H1([f1])F . H1[(.1)]F . t1 = E2 . \nH2([f2])G . H2[(.2)]G . t2 . { for any surjective function f, f . f. = id } h = E1 . H1([f1])F . merge1 \n. merge . 1 . H1[(.1)]F . t1 = E2 . H2([f2])G . merge2 . merge . 2 . H2[(.2)]G . t2 . { Theorems 9 and \n13 } .m, s. h = m . (([f1])F ([f2])G). ([(.1)]F [(.2)]G). s . { bifunctor and hylomorphisms } .m, \ns. h = m . ([[f1,.1] F [ f2,.2] G). s In Theorem 15, we essentially parallelize the catamorphism part \nand anamorphism part independently, by using Theorems 9 and 13, and then reconstruct hylomorphisms. It \nis worth noting that the derived implementation does not need intermediate data structures between the \ncatamorphisms and anamorphisms.  5.3.2 Example: Calculating Power Series As an example, let us calculate \na0 + a1x + \u00b7 \u00b7 \u00b7 anx n for a given [a1,. . . ,an]and x. power series as x = ps (as,x, 0) where ps ([],-,-) \n=0 ps (a : as,x, k) = a * x k + ps (as, x, k+ 1) Although power series is not a catamorphism, because \nof the additional parameters x and k, it is essentially a hylomorphism. Consider ana = unfoldr . and \ncata = foldr f where . and f are de.ned as follows; then, ps = cata . ana . . ([],-,-) =(1,()) . (a : \nas)x k = (2,((a, x, k),(as,x, k+ 1))) f(1,()) = 0 f(2,((a, x, k),r)) = a * x k + r In order to apply \nTheorem 15, we would like to implement ana and cata in terms of unfold l and foldl. This is not dif.cult. \nana (as,x, k)= unfold l . ' (as,x, k+ length as - 1) . ' ([],-,-) =(1,()) . ' (as + [a]) x k = (2,((as,x, \nk- 1),(a, x, k))) cata = foldl f ' f ' (1,()) = 0 f ' k (2,(r,(a, x, k))) = r + a * x Therefore, from \nTheorem 15, there exist m and s such that ps = m . ([[f ' ,. ' ] Ls \u00d7 [ f,.] Lc ). s. In order to derive \nm and s, we need the right inverses of the catamorphisms and anamorphisms. These are not dif.cult to \nconstruct. Accordingly, s is derived from ana : ((as1,x1,k1),(as2,x2,k2)),. s ([a0,. . . ,an],x, k) . \n{ Theorem 15 and the de.nition of ana } .r1,r2. r1 + r2 = [(a0,x, k),. . . ,(an,x, k+ n)] . ) 1 (as1,x1,k1)= \n(unfold l . ' r1 . (as2,x2,k2)= (unfoldr .) 1 r2 . { unfolding (unfold l . ' ) 1 and (unfoldr .) 1 } \n.k ' . (as1,x1,k1)= ([a0,. . . ,ak.-1],x, k+ k ' - 1) . (as2,x2,k2)= ([ak. ,. . . ,an],x, k + k ' ) and, \nm is derived from cata: m (v1,v2) = { Theorem 15 }) 1 cata ((foldl f ' v1 + (foldr f) 1 v2) = { unfolding \n(foldl f ' ) 1 and (foldr f) 1 }cata ([(v1,-,0)] + [(v2,-,0)]) = { unfolding + and cata }v1 + v2. In \nsummary, we have derived the following divide-and-conquer implementation, where k ' is any integer of \n1 = k ' = n. ps ([a0,. . . ,an],x, k)= [ f ' ,. ' ] Ls ([a1,. . . ,ak.-1],x, k + k ' - 1) + [ f, .] Lc \n([ak. ,. . . ,an],x, k+ k ' ) Theorem 15 is especially useful for dealing with functions, like power \nseries , that are not catamorphisms because of their having tiny additional computations such as distributing \nvalues. Paralleliz\u00ading their anamorphism parts is usually not dif.cult; often they can be reduced to \na few well-known patterns. Their parallelization can thus be achieved by parallelizing catamorphisms. \n   6. Limitations 6.1 Problems Unsuitable for Our Formalism Although we have generalized the third \nlist-homomorphism theo\u00adrem, there are still several cases that do not .t our formalism. For example, \nconsider dividing a list into two sublists, each of which consists of even/odd-numbered elements. A typical \napplica\u00adtion is the fast Fourier transformation. Here, we show a compact program by Misra (1994). We \nassume that lists x and y are of the same length, n, which is a power of two. .t (interleave x y) = let \np = .t x q = .t y .n = exp(-pi/n) in zipWith (+) p(zipWith (*)[.0 ,.1 ]q) n,. . . ,.n-1 + zipWith (-)p(zipWith \n(*)[.0 ,.1 ]q) nn n,. . . ,.n-1 nn interleave ([],[]) = [] interleave (a : x, b : y) = a : b : interleave \n(x, y) The problem is that it seems impossible to implement interleave using a function of the required \npolymorphic type. We require that interleave consumes one of the input and keeps the other as it is, \nbut interleave simultaneously consumes both inputs. We face a similar problem in manipulating sorted \nlists. The merging operations for sorted lists are as follows. merge ([],y) = y merge (x, []) = x merge \n(a : x, b : y) = if a = bthen a : merge x (b : y) else b : merge (a : x)y Since merge simultaneously \nmanipulates both lists, it seems im\u00adpossible for it to have the required polymorphic type. Another problem \nis that our theorems are not very useful for de\u00adriving speci.c subclasses of divide-and-conquer implementations. \nRecall that Theorem 1 derives programs of the form of h(x+ y)= h x 8 h y. For the sake of ef.ciency, \nwe may want h to have more speci.c implementations. For example, we may require (8) to be (++), like \nmap and .lter . However, our theorems do not have enough .exibility to take account of such additional \nrequirements. 6.2 Computational Complexities of Derived Implementations The ef.ciency of the obtained \ndivide-and-conquer programs is a very important consideration because, without it, there would be no \nreason to use parallel programs. However, our theorems state nothing about ef.ciency. Moreover, as mentioned \nin Section 4.1, they can deal with almost every function if we disregard ef.ciency. This fact may cast \ndoubt on the usefulness of our theorems. Actually, it is possible to have a discussion about computational \ncosts not about the time complexity but about the communica\u00adtion complexity. Recall Lemma 6. The lemma \nsuggests that we can evaluate h1 and h2 independently, i.e., on different processors, and communicate \ntheir results. Therefore, the sizes of the ranges of h1 and h2 provide an estimate of how much information \nthe divide-and-conquer implementation needs to communicate. More formally, if either h1 or h2 results \nin an O(f(n))-bit output for any n-bit input, the communication complexity of h is O(f(n)). For example, \nsince sum is both foldr and foldl, there exists m such that sum . (++) = m . (sum \u00d7 sum ), and therefore, \nits commu\u00adnication complexity is O(1) if we regard the sizes of integers as constant. Low communication \ncost implementations are often fast. There\u00adfore, the viewpoint of communication complexity would provide \na strategy for developing ef.cient implementations. For example, in the case of Theorem 9, it is reasonable \nto melt away as much infor\u00admation as possible by using the catamorphism part and to let each Ei be a \ntiny computation, such as a projection from a tuple. However, this strategy is not always useful. In \nparticular, it is useless for parallelizing isomorphisms. For example, consider calculating Fibonacci \nnumbers. .b n = let (a, b)= .b ' n in a where .b ' 0 = (1,0) .b ' n = let (a, b)= .b ' (n - 1) in (a \n+ b, a) .b ' is a hylomorphism, and moreover, Theorem 15 is applicable. ' '' 1 ' ' 1 Hence, .b (n1 + \nn2) = .b (.b (.b n1)+ .b (.b ' n2)). However, because .b ' is injective3 , this is a kind of tautology \nand does not improve ef.ciency at all. In fact, there is an ef.cient divide-and-conquer algorithm for \ncalculating Fibonacci numbers. In the following, (\u00b7)denotes matrix multiplication. a b\u00ab '' .b n = let \n= .b n in a 0 c '' 1 0\u00ab where .b 0 = 0 1 '' 1 1\u00ab .b 1 = 0 1 '' '' .b (n1 + n2) = .b '' n1 \u00b7 .b n2 The \nkey is to retain rather redundant information, in the form of ma\u00adtrices, for ef.ciently merging the results \nof subcomputations. How\u00adever, our theorems are not helpful for introducing such redundant information. \nIn summary, it seems reasonable to conclude that our theo\u00adrems specify necessary, rather than suf.cient, \nconditions for cer\u00adtain kinds of divide-and-conquer parallel evaluations. They are still unique and interesting, \nbecause most of the existing parallelization methods discuss only suf.cient conditions.  7. Discussion \nWe have studied a theory for developing parallelization theorems in the form of two iterative implementations \nlead to a parallel imple\u00admentation . The key is a new proof of the third list-homomorphism theorem based \non shortcut deforestation. The proof can be eas\u00adily generalized to catamorphisms. We found that similar \nresults hold for anamorphisms and moreover hylomorphisms. These re\u00adsults may help to usher in a new paradigm \nfor parallel functional programming. 7.1 Generalizations of the Third List-homomorphism Theorem There \nhave been several studies on generalizing the third list\u00adhomomorphism theorem. Morihata et al. (2009) \ngeneralized the third list-homomorphism theorem to deal with manipulations of algebraic data structures. \nTheir idea is to use zipper structures for reducing operations of al\u00adgebraic data structures to those \nof lists and thereby to exploit par\u00adallelization methods for list operations. Our theorems generalize \ntheirs. We showed that reductions to lists are not essential; more\u00adover, our theorems can deal with other \nstructures besides algebraic data structures. Mu and Morihata (2011) pointed out that the proof of the \nthird list-homomorphism theorem has an interesting structure that leads to the dualization and generalization \nof the theorem. The proof structure they focused on is essentially Lemma 12. Our theory is built upon \ntheir approach. We have shown that this proof structure 3 .b is not injective because .b 0 = .b 1 = 1. \n alone is insuf.cient for obtaining useful theorems like the third list\u00adhomomorphism theorem and demonstrated \nthat shortcut deforesta\u00adtion can be an aid. Gibbons et al. (2001) showed necessary and suf.cient con\u00additions \nfor functions being catamorphisms or anamorphisms. A function h characterized by h (x + y) = h x 8 h \ny is a cata\u00admorphism on yet another list structure, called join lists, in which lists are constructed \nby + . Therefore, their theory provides a nec\u00adessary and suf.cient condition for functions being such \ndivide\u00adand-conquer list operations. However, the necessary and suf.\u00adcient condition is similar to the \npremise of Lemma 6, and thus, not as useful as the third list-homomorphism theorem. It is also worth \nnoting that our theorems are not restricted to deriving cata\u00admorphisms/anamorphisms. For instance, the \ndivide-and-conquer downfrom is not an anamorphism.  7.2 Parallelization of Recursive Functions The third \nlist-homomorphism theorem have been used as a basis of parallelization (Gibbons 1996a,b; Gorlatch 1999; \nGeser and Gorlatch 1999; Morita et al. 2007; Liu et al. 2011; Chi and Mu 2011). The study by Morita et \nal. (2007) is the most related to ours, because like ours, it relies on the derivation of right inverses. \nThey built an automatic parallelization system on a method of automatically .nding right inverses. The \nsystem could be used to automate parallelizations based on Theorem 9; moreover, it would also be useful \nwhen applying Theorem 15 to functions whose anamorphism parts are easy to deal with. However, it cannot \nderive right inverses that contain recursions, and hence, cannot automate parallelizations based on Theorem \n13. There are other methods of deriving parallel implementations from sequential recursive functions, \nsuch as (Hu et al. 1997; Chin et al. 1998; Hu et al. 1998; Keller and Chakravarty 1998; Xu et al. 2004; \nMatsuzaki et al. 2006; Morihata and Matsuzaki 2010). While they only deal with functions that operate \non linear structures (Hu et al. 1997; Chin et al. 1998; Hu et al. 1998; Xu et al. 2004) or trees (Keller \nand Chakravarty 1998; Matsuzaki et al. 2006; Morihata and Matsuzaki 2010), any initial algebra is in \nthe domain of our theory. Moreover, while most of the existing methods only consider functions that consume \nstructures, ours can deal with anamorphisms and hylomorphisms as well. Thus, one can argue that ours \nis one of the most generic theories, whereas each of others has its own strong point, such as suitability \nfor automation and ef.ciency of the resulting parallel programs. 7.3 Connections to Shortcut Deforestation \nOur methods rely on shortcut deforestation (Gill et al. 1993; Gill 1996; Takano and Meijer 1995; Johann \n2002) and its basis, para\u00admetricity (Reynolds 1983; Wadler 1989). We viewed Theorem 5 as follows: the \npolymorphic type guarantees that the structure ain the argument Gais to be used as is, according to the \nparameter passed to gen; therefore, we need to process that part in advance, possi\u00adbly in parallel with \nother computations. Similar observations have been appeared in the literature on deforestation of monadic \npro\u00adgrams (Ghani and Johann 2007) and accumulative programs (Kat\u00adsumata and Nishimura 2008). This is \nnot surprising. For example, for deforesting accumulative programs, we can consume structures constructed \nin accumulative parameters before they actually form a part of the output. This means that, like parallelization, \ndeforesta\u00adtion of accumulative programs require the order of computations to be changed. Further studies \non connections between deforestation and parallelizations may lead to more interesting insights. We used \nparametricity rather than shortcut deforestation to prove the theorem about anamorphisms, namely Theorem \n13. One might think that it is more natural to use the dual of the shortcut de\u00adforestation, often called \nthe unfoldr-destroy rule (Takano and Mei\u00adjer 1995; Svenningsson 2002). We did not take this approach \nfor two reasons. First, as mentioned in Section 5.2.4, it is essential to formalize the theorem by using \nrelations, and therefore, function\u00adbased approaches are not suitable for our purpose. Secondly, in order \nto use the unfoldr-destroy rules, we should implement merge by using a function of yet another polymorphic \ntype. This, how\u00adever, makes the generalization to hylomorphisms (Theorem 15) less elegant. We required \nthat the merging operations be implemented by functions of certain polymorphic types. There have been \na few stud\u00adies (Launchbury and Sheard 1995; Chitil 1999; Yokoyama et al. 2005) on automatically deriving \nsuch polymorphic implementa\u00adtions. They would enable us to automatically obtain parallelization theorems \neven for user-de.ned data structures.   Acknowledgments The author is grateful to Shin-Cheng Mu and \nZhenjiang Hu. My discussion with them were helpful in the preliminary develop\u00adments. The author is also \ngrateful to anonymous reviewers for pointing out a .aw of Corollary 11 and giving comments useful to \nimprove presentations. The author is supported by a JSPS Grant\u00adin-Aid for Young Scientists (B) 24700019. \n References R. S. Bird and S. A. Curtis. Functional pearls: Finding celebrities: A lesson in functional \nprogramming. J. Funct. Program., 16(1):13 20, 2006. R. S. Bird and O. de Moor. Algebra of Programming. \nPrentice Hall, 1997. Y.-Y. Chi and S.-C. Mu. Constructing list homomorphisms from proofs. In Programming \nLanguages and Systems -9th Asian Symposium, APLAS 2011, Proceedings, volume 7078 of Lecture Notes in \nComputer Science, pages 74 88. Springer, 2011. W.-N. Chin, A. Takano, and Z. Hu. Parallelization via \ncontext preservation. In Proceedings of the 1998 International Conference on Computer Languages, ICCL \n98, pages 153 162. IEEE, 1998. O. Chitil. Type inference builds a short cut to deforestation. In Proceedings \nof the 4th ACM SIGPLAN International Conference on Functional Programming, ICFP 99, pages 249 260. ACM, \n1999. M. M. Fokkinga. Tupling and mutumorphisms. Squiggolist, 1(4), 1989. A. Geser and S. Gorlatch. Parallelizing \nfunctional programs by generaliza\u00adtion. J. Funct. Program., 9(6):649 673, 1999. N. Ghani and P. Johann. \nMonadic augment and generalised short cut fusion. J. Funct. Program., 17(6):731 776, 2007. J. Gibbons. \nThe third homomorphism theorem. J. Funct. Program., 6(4): 657 665, 1996. J. Gibbons, Computing downwards \naccumulations on trees quickly. Theor. Comput. Sci., 169(1):67 80, 1996. J. Gibbons, G. Hutton, and T. \nAltenkirch. When is a function a fold or an unfold? Electr. Notes Theor. Comput. Sci., 44(1):146 160, \n2001. A. Gill. Cheap Deforestation for Non-strict Functional Languages. PhD thesis, Department of Computing \nScience, Glasgow University, 1996. A. Gill, J. Launchbury, and S. Peyton Jones. A short cut to deforestation. \nIn FPCA 93 Conference on Functional Programming Languages and Computer Architecture, pages 223 232. ACM, \n1993. S. Gorlatch. Extracting and implementing list homomorphisms in parallel program development. Sci. \nComput. Program., 33(1):1 27, 1999. R. Hinze. Adjoint folds and unfolds an extended study. Sci. Comput. \nProgram., 2013. In Press. Z. Hu, H. Iwasaki, and M. Takechi. Formal derivation of ef.cient parallel programs \nby construction of list homomorphisms. ACM Trans. Program. Lang. Syst., 3(19):444 461, 1997. Z. Hu, M. \nTakeichi, and W.-N. Chin. Parallelization in calculational forms. In POPL 98: Proceedings of the 25th \nACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, pages 316 328. ACM, 1998.  G. P. \nHuet. The zipper. J. Funct. Program., 7(5):549 554, 1997. P. Johann. A generalization of short-cut fusion \nand its correctness proof. Higher-Order and Symbolic Computation, 15(4):273 300, 2002. S. Katsumata and \nS. Nishimura. Algebraic fusion of functions with an accumulating parameter and its improvement. J. Funct. \nProgram., 18 (5-6):781 819, 2008. G. Keller and M. M. T. Chakravarty. Flattening trees. Euro-Par 98 Parallel \nProcessing, 4th International Euro-Par Conference, Proceedings, volume 1470 of Lecture Notes in Computer \nScience, pages 709 719. Springer, 1998. J. Launchbury and T. Sheard. Warm fusion: Deriving build-catas \nfrom recursive de.nitions. In Conference Record of FPCA 95 SIGPLAN-SIGARCH-WG2.8 Conference on Functional \nProgramming Languages and Computer Architecture, pages 314 323. ACM, 1995. Y. Liu, Z. Hu, and K. Matsuzaki. \nTowards systematic parallel programming over MapReduce. In Parallel Processing -17th International Confer\u00adence, \nEuro-Par 2011, Proceedings, Part II, volume 6853 of Lecture Notes in Computer Science, pages 39 50. Springer, \n2011. G. Malcolm. Data structures and program transformation. Sci. Comput. Program., 14(2-3):255 279, \n1990. K. Matsuzaki, Z. Hu, and M. Takeichi. Towards automatic parallelization of tree reductions in dynamic \nprogramming. In SPAA 2006: Proceedings of the 18th Annual ACM Symposium on Parallel Algorithms and Architectures, \npages 39 48. ACM, 2006. E. Meijer, M. M. Fokkinga, and R. Paterson. Functional programming with bananas, \nlenses, envelopes and barbed wire. In Functional Programming Languages and Computer Architecture, 5th \nACM Conference, Proceedings, volume 523 of Lecture Notes in Computer Science, pages 124 144. Springer, \n1991. J. Misra. Powerlist: A structure for parallel recursion. ACM Trans. Program. Lang. Syst., 16(6):1737 \n1767, 1994. A. Morihata and K. Matsuzaki. Automatic parallelization of recursive functions using quanti.er \nelimination. In Functional and Logic Pro\u00adgramming, 10th International Symposium, FLOPS 2010, Proceedings, \nvolume 6009 of Lecture Notes in Computer Science, pages 321 336. Springer, 2010. A. Morihata, K. Matsuzaki, \nZ. Hu, and M. Takeichi. The third homo\u00admorphism theorem on trees: Downward &#38; upward lead to divide-and\u00adconquer. \nIn Proceedings of the 36th ACM SIGPLAN-SIGACT Sym\u00adposium on Principles of Programming Languages, POPL \n2009, pages 177 185. ACM, 2009. K. Morita, A. Morihata, K. Matsuzaki, Z. Hu, and M. Takeichi. Automatic \ninversion generates divide-and-conquer parallel programs. In Proceed\u00adings of the ACM SIGPLAN 2007 Conference \non Programming Language Design and Implementation, pages 146 155. ACM, 2007. S.-C. Mu and A. Morihata. \nGeneralising and dualising the third list\u00adhomomorphism theorem: functional pearl. In Proceeding of the \n16th ACM SIGPLAN International Conference on Functional Programming, ICFP 2011, pages 385 391. ACM, 2011. \nJ. H. Reif, editor. Synthesis of Parallel Algorithms. Morgan Kaufmann Publishers, 1993. J. C. Reynolds. \nTypes, abstraction and parametric polymorphism. Information Processing, 83:513 523, 1983. J. Svenningsson. \nShortcut fusion for accumulating parameters &#38; zip-like functions. In Proceedings of the 7th ACM SIGPLAN \nInternational Conference on Functional programming, ICFP 02, pages 124 132. ACM, 2002. A. Takano and \nE. Meijer. Shortcut deforestation in calculational form. In Conference Record of FPCA 95 SIGPLAN-SIGARCH-WG2.8 \nConference on Functional Programming Languages and Computer Architecture, pages 306 313. ACM, 1995. P. \nWadler. Deforestation: Transforming programs to eliminate trees. In ESOP 88, 2nd European Symposium on \nProgramming, Proceedings, volume 300 of Lecture Notes in Computer Science, pages 344 358. Springer, 1988. \nP. Wadler. Theorems for free! In FPCA 89 Conference on Functional Programming Languages and Computer \nArchitecture, pages 347 359. ACM, 1989. D. N. Xu, S.-C. Khoo, and Z. Hu. PType system: A featherweight \nparallelizability detector. In Programming Languages and Systems: Second Asian Symposium, APLAS 2004, \nProceedings, volume 3302 of Lecture Notes in Computer Science, pages 197 212. Springer, 2004. T. Yokoyama, \nZ. Hu, and M. Takeichi. Calculation rules for warming-up in fusion transformation. In On-line Pre-Review \nProcessings of the Sixth Symposium on Trends in Functional Programming, TFP 2005, pages 399 412, 2005. \n Appendix: Proof of Lemma 14 We prove the following equation. .. . G[(.)]F . ginF = G[(.)]F . ginF . \n([(.)]F id). ([(.)]F id) (16) Since is a bifunctor (i.e., a birelator), Lemma 14 follows from its \nconverse, where R = (G[(.)]. F . g inF . ([(.)]F id)). . For the relations R and S, R . S is de.ned \nby set-theoretic inclusion. Note that R . S implies P . R. Q . P . S . Q. We can prove Equation (16) \nby showing both directions of inclusions. We .rst prove . as follows. G[(.)]. F . g inF . ([(.)]F id). \n([(.)]F . id) = { is a bifunctor }G[(.)]. F . g inF . ([(.)]F . [(.)]F . id) . { Since [(.)]F is a \nfunction, [(.)]F . [(.)]. F . id }G[(.)]. F . g inF . (id id) = { is a bifunctor }G[(.)]. F . g inF \nThe other direction can be proved by using relational para\u00admetricity. G[(.)]. F . g inF . G[(.)]. F . \ng inF . ([(.)]F id). ([(.)]F . id) . { f . R . S .. R . f. . S for any function f }G[(.)]F . G[(.)]. \nF . g inF . g inF . ([(.)]F id). ([(.)]. F id) . { Gis a functor and is a bifunctor }  G([(.)]F \n. [(.)]. F). g inF . g inF . ([(.)]F . [(.)]. F id) . { relational parametricity; note that Fis a functor \n}[(.)]F . [(.)]. F . inF . inF . F[(.)]F . F[(.)]. F . { inF is an isomorphism, and in. F = out F }out \nF . [(.)]F . [(.)]. F . out F . . F[(.)]F . F[(.)]. F . { universal property of anamorphisms }F[(.)]F \n. .. .. . F[(.)]. F . F[(.)]F . F[(.)]. F . { since . is a function, . . .. . id }  True  \n\t\t\t", "proc_id": "2500365", "abstract": "<p>The third list-homomorphism theorem states that if a function is both <i>foldr</i> and <i>foldl</i>, it has a divide-and-conquer parallel implementation as well. In this paper, we develop a theory for obtaining such parallelization theorems. The key is a new proof of the third list-homomorphism theorem based on shortcut deforestation. The proof implies that there exists a divide-and-conquer parallel program of the form of <i>h</i>(<i>x</i> '<i>merge</i>' <i>y</i>) = <i>h</i><sub>1</sub> <i>x</i> odot <i>h</i><sub>2</sub> <i>y</i>, where <i>h</i> is the subject of parallelization, <i>merge</i> is the operation of integrating independent substructures, <i>h</i><sub>1</sub> and <i>h</i><sub>2</sub> are computations applied to substructures, possibly in parallel, and odot merges the results calculated for substructures, if (i) <i>h</i> can be specified by two certain forms of iterative programs, and (ii) <i>merge</i> can be implemented by a function of a certain polymorphic type. Therefore, when requirement (ii) is fulfilled, <i>h</i> has a divide-and-conquer implementation if <i>h</i> has two certain forms of implementations. We show that our approach is applicable to structure-consuming operations by catamorphisms (folds), structure-generating operations by anamorphisms (unfolds), and their generalizations called hylomorphisms.</p>", "authors": [{"name": "Akimasa Morihata", "author_profile_id": "81372591868", "affiliation": "Tohoku University, Sendai-shi, Miyagi, Japan", "person_id": "P4261250", "email_address": "morihata@riec.tohoku.ac.jp", "orcid_id": ""}], "doi_number": "10.1145/2500365.2500580", "year": "2013", "article_id": "2500580", "conference": "ICFP", "title": "A short cut to parallelization theorems", "url": "http://dl.acm.org/citation.cfm?id=2500580"}