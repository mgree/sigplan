{"article_publication_date": "05-09-2003", "fulltext": "\n Static Array Storage Optimization in MATLAB Pramod G. Joisha ECE Department Northwestern University, \nEvanston, IL 60208 pjoisha@ece.northwestern.edu ABSTRACT An adaptation of the classic register allocation \nalgorithm to the problem of array storage optimization in MATLAB is presented. The method involves the \ndecomposition of an interference graph s color classes using inferred type infor\u00admation. A key trait \nis the use of symbolic types, along with control .ow, in performing the decomposition. On a bench\u00admark \nsuite spanning the published test suites of some recent research MATLAB compilers, our implementation \nproduces savings in the average virtual memory size, with respect to code generated by a commercial MATLAB \ncompiler, of be\u00adtween 51% and 139% in 6 out of 11 programs, and savings between 0.7% and 47% in the remaining. \nIn absolute terms, this ranged from 123KB to over 9MB. Substantial improve\u00adments in other categories \nof memory, such as resident sets and dynamic data (stack plus heap), were also observed and are reported. \nSpeedups in execution times of at least over an order of magnitude in 4 programs, of over 100% in 4 of \nthe remaining, and 10% and over in the rest are also reported. Categories and Subject Descriptors D.3.2 \n[Programming Languages]: Language Classi.ca\u00adtions very high-level languages; D.3.4 [Programming La\u00adnguages]: \nProcessors compilers and optimization General Terms Algorithms, Design, Languages, Performance 1. INTRODUCTION \nThe automatic optimization of program storage is a well\u00adknown problem, one that has been extensively \ninvestigated and e.ciently tackled in the past. Because programs in early computers were constrained \nby the sizes of their cores, initial work centered on main memory allocation [19, 14, 15]. As large main \nmemories became cheaply available, interest in this area waned with the focus shifting to register allo\u00adcation \n[6, 8, 4, 3, 23]. This paper revisits the main mem- Permission to make digital or hard copies of all \nor part of this work for personal or classroom use is granted without fee provided that copies are not \nmade or distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. PLDI 03, June 9 11, 2003, San Diego, California, USA. Copyright \n2003 ACM 1-58113-662-5/03/0006 ...$5.00. Prithviraj Banerjee ECE Department Northwestern University, \nEvanston, IL 60208 banerjee@ece.northwestern.edu ory allocation problem in the context of a typeless \narray\u00adbased programming language called MATLAB and shows how the e.cient management of array storage \nis crucial to its successful static compilation. The fact that we have to contend with arrays whose type \ncharacteristics are not ex\u00adpressly speci.ed in a program is an important distinction between our work \nand previous e.orts at name reclamation [11, 22] and storage reduction [24, 9]. The issue of storage \noptimization is particularly relevant to MATLAB because the language is increasingly being used in the \nprototyping of applications that ultimately get deployed on limited mem\u00adory platforms such as DSPs, FPGAs \nand embedded devices, and because it espouses an array-centric programming id\u00adiom, one in which the elemental \nmanipulation of data is eschewed for whole array processing. Closer to our work is Fabri s approach [15] \nwhich viewed main memory allocation as a weighted graph coloring prob\u00adlem. Nodes in the interference \ngraph were weighted by the sizes of the arrays in question and a coloring was sought that minimized the \ntotal allocated storage. What compli\u00adcates the problem in MATLAB is that size information may be absent \nsince the language lacks an explicit declaration of type. In fact, even when all array sizes are known, \nthe problem does not naturally map to graph coloring the way the register allocation problem does. Factors \nsuch as partial interference (see \u00a7 2.1) further ravel the picture. Surprisingly, though the same problems \nexisted in APL, completely di.erent techniques, like drag-along and beat\u00ading [1], delayed evaluation \n[16], chaining [7] and demand driven execution [5], were used to reduce storage overheads. Research compilers \nthat translate MATLAB statically [13] and just-in-time [2] have emerged in recent years. However, storage \noptimization has been an unaddressed area in MAT-LAB. And although systems such as MaJIC [2] that com\u00adpile \ncode during execution are better poised to e.ciently manage storage, we must keep in mind that such run-time \nsystems will themselves compete with the application for platform storage space. Therefore, we believe \nthat there ex\u00adists a niche of applications that could greatly bene.t from the better static management \nof MATLAB s array storage. 1.1 Algorithm Overview The input to our algorithm is a control-.ow graph (CFG) \nthat is in the Static Single-Assignment (SSA) [12] form. The algorithm, referred to as GCTD (Graph Coloring \nwith Type\u00adbased Decomposition) consists of two phases. The .rst in\u00advolves the creation and coloring of \nan interference graph to determine variables that can share common storage. Be\u00adcause the objective isn \nt the ultimate assignment of variables to a machine s register set, we neither seek an n-coloring, where \nn is the number of available machine registers, nor deal with related issues such as register spilling, \nas is the case in [6]. Instead, the goal of the .rst phase is to deter\u00admine, at least conservatively, \nnoninterfering classes of vari\u00adables that can be safely assigned to the same area of memory without changing \nthe meaning of the program. To this end, the aim is to .nd a coloring of the interference graph.1 The \ntreatment of interference in the .rst phase is similar to that in [6] except that a new kind of con.ict \narising from opera\u00adtor semantics needs to be considered and resolved, perhaps through the involvement \nof inferred type information (see \u00a7 2.3). The second phase decomposes the color classes on the basis \nof expression types. Basically, the idea is to partition each color class into groups so that all variables \nin a group are laid out in memory starting from the same location as a certain distinguished set of variables \nin the group. Members of that distinguished set have the predominant storage re\u00adquirement in the group. \nThe cornerstone of the second phase is a partial order that relies on program variable types, and in \nthe symbolic case, on control-.ow information too.  2. INTERFERENCE GRAPH (PHASE 1) Theinterferencegraph \nGf * =(Vf ,Ef ), constructed at the level of a user-de.ned MATLAB function f,has a nodeset Vf that represents \nthe variables de.ned in the CFG and an edge set Ef that represents the interference among the variables. \nTwo variables are considered to interfere if their def-use chains (du-chains) overlap. In particular, \nwe use the Chaitin et al. [6] notion of interference by which two vari\u00adables interfere if there exists \nan execution path in f and a point therein at which: (1) the de.nitions of both variables are visible; \n(2) both variables are not dead; and (3) the vari\u00adables have di.erent values. Because determining these \ncri\u00adteria is in general undecidable, we use an approximation of interference by considering variables \nthat are both available and live at each assignment [6, 3]. A variable v is regarded available at a statement \ns if there is a possible execution path from a de.nition of v to s.A variable w is treated live at s \nif there is a possible execution path from s toause of w along which w is not rede.ned. Liveness and \navailabil\u00adity as de.ned here are conservative because they indicate a potential, rather than a de.nitive, \nuse and de.nition. Our approach to detecting variables that satisfy these two con\u00additions is e.ectively \nthat described in [3]. A basic block is traversed backwards starting with the set of variables that are \nlive and available at its end. A de.nition encountered during the traversal is interfered with members \nof the set. Before moving to the next statement, the set is updated by removing the variable (or variables) \nde.ned at that state\u00adment and adding variables used at the same statement. 2.1 Partial Interference The \nconcept of interference described in [6] was intended for scalars. When directly applied to variables \nthat can also be arrays, which is the case in MATLAB programs, it can be more pessimistic than necessary. \nThis will happen when variables only partially interfere. As an example, the SSA conforming intermediate \nrepresentation (IR) shown below 1Currently, our implementation seeks a coloring that is as close to the \nminimal coloring as possible. This, however, may not yield the .nal optimal solution to the problem. \ncreates a pair of 2 \u00d7 2matrices in a and b, 2 assigns to c the .rst element in a, 3 then computes the \nsum of b and c in d, and .nally displays d. Because the du-chain of a crosses that of b, there clearly \nis an interference between a and b. a . rand(2, 2); b . rand(2, 2); c . subsref(a, 1); d . b+c; disp(d); \n However, by inspecting the index used in the access, we see that this interference is only in the .rst \nelement of a. Although a and b are considered to fully interfere in our current implementation, which \ncauses them to be assigned to disjoint areas in memory, note that their storage areas could have been \noverlapped allowing for the use of a total of .ve double precision memory locations to perform the above \ncomputationinits entirety. 2.2 Handling Copies A copy statement of the form X . Y will by itself not \nintroduce an interference between X and Y. However, if the coloring step ascribes di.erent colors to \nX and Y, the .nal code generation stage will have to emit code to copy Y to X. Because variables in MATLAB \ncan be arrays, it is even more important, in relation to a situation that deals only with scalars, to \navoid copy operations whenever possible. In [6], this was attacked by coalescing X and Y and then rebuilding \nthe interference graph since the action of coalescing can alter the graph; the process was repeated up \nto a .xed number of times until no further coalescences were possible. Our strategy is di.erent: we preprocess \nthe CFG to free it of copies by subjecting it to a copy propagation pass followed by dead-code elimination.4 \nObviously, all copies cannot be eliminated this way; one such case is shown in the IR below. s1 . f(s0,s2) \nt1 . f(t0,t2) . . . t2 . s1 s2 . ...  In the above, copy propagating s1 from t2 . s1 to t1 . f(t0,t2) \nwill change the meaning of the IR. Of course, t2 and s1 in the interference graph will also not be coalescent \nbecause they interfere due to overlapping du-chains. 2.2.1 SSA Inversion Copies Since converting back \nfrom the SSA form will reintroduce copies [12], it is imperative that the name de.ned at a join 2Assignments \nwill be denoted using the . symbol. 3subsref(a, i1, i2, ..., im) returns the array ele\u00adment a(i1, i2, \n..., im). 4The translator has over 20 passes that perform a vari\u00adety of transformations such as global \ncommon-subexpression elimination, constant folding, constant propagation, type determination, code selection \nand code generation. node share storage with the names used at that node when\u00adever possible. This will \nmake the reintroduced copies iden\u00adtity assignments and therefore trivially removable. To achi\u00adeve this \nfor the join node Z . f(X,Y), the nodes in the interference graph corresponding to Z and X (and then \nZ and Y) are examined for any interference. If they don t in\u00adterfere, they are coalesced and the interference \ngraph is ap\u00adpropriately updated. This will force any coloring to assign the same color to Z and X so \nthat the copy Z . X rein\u00adtroduced during the SSA inversion step becomes a trivially removable identity \nassignment. Of course, such coalescings will a.ect the chromatic number of the graph and constrain the \ncoloring .nally obtained. But we have found the folding of copies to be indispensable to the generation \nof e.cient code, because even a few copies, involving large-sized arrays and nested within loops, can \nsigni.cantly impact the gener\u00adated code s performance through excessive paging activity.  2.3 Interference \ndue to Operator Semantics Assignments in our IR have right-hand sides that consist of at most a single \nMATLAB operation like *, or a pseudo operation like the f function. Assignments in MATLAB can be cast \ninto this Single Operator (SO) form through the introduction of temporaries. The code generation pass \nthen directly maps each of the SO form IR assignments to C code. Consider the IR assignment c . a*b.If \na and b are scalars, storage for c and a,or c and b, can be shared, as\u00adsuming that these pairs don t \notherwise interfere. We say that c can be computed in-place in either operand in the C mapping without \nviolating the semantics of the operation. However, * can operate on both scalars and nonscalars in MATLAB. \nFor instance, when a and b are nonscalar matri\u00adces, * performs the elementary matrix multiplication oper\u00adation. \nIn that situation, computing c in-place in either a or b will risk violating the semantics of the operation \nbe\u00adcause elements in a or b could get overwritten before being fully used. However, when either a or \nb is a scalar, and the other operand is an arbitrary array, c is produced by the elementwise multiplication \nof the scalar with the array. In that case, c can be computed in-place in the array operand. Thus, the \nstorage coalescing pass inspects each IR assign\u00adment of the form Y .op(X1,X2,...,Xm), and inserts ad\u00additional \ninterferences between Y and an Xi (1 =i =m)on the basis of what op is, and the type information inferred \nfor each Xi. Hence, for the case c . a*b,an interferenceis not added between c and a, and between c and \nb,if either a or b can be determined to be scalars. Since c canthenbe calculated in-place in the larger \nsized operand, we still have to .gure out which of the two operands can accommodate c. This is done in \nthe second phase of the GCTD pass. 2.3.1 Array Addition An example of a di.erent op is in the IR statement \nc . a+b.In MATLAB, + is the array addition operation; it always computes the elementwise sum of its operands. \nSince c can be computed in-place in either a or b provided the operand is su.ciently sized, no additional \ninterferences have to be inserted between c and either operand in this case. Figure 1 displays exactly \nhow the C code generated by the translator performs this in-place computation. (Not shown are preceding \nshape correctness and resizing checks.) 2.3.2 Right-Hand Side Array Indexing In the IR statement c . \nsubsref(a, 1) seen in \u00a72.1, op is subsref, which corresponds to the right-hand side array indexing operation \nin MATLAB. We call this the R\u00adindexing operation for short. Note that in the case seen in \u00a7 2.1, c can \nbe computed in-place in a.(There are only two possibilities: subsref(a, 1) is either illegal, which will \nhappen if a is the empty array, or it is legal, in which case it is a scalar.) However, if the statement \nwere c . subsref(a, e) where e was unknown, such an in-place computation may not be possible. This is \nbecause MATLAB permits e to itself be an array; this allows an arbitrary permutation of the elements \nof a to be returned. For instance, if e were the MATLAB colon expression 4:-1:1, subsref(a, e) would \ngive the elements of the 2 \u00d72matrix a in reverse. Again, type information could be used to di.erentiate \nthese situations in this case, whether the subscript e is a scalar.  2.3.3 Left-Hand Side Array Indexing \nMATLAB also o.ers a left-hand side array indexing op\u00aderation. In source form, this operation, which we \ncall L\u00adindexing for short, is speci.ed by the construction a(l1, l2, ..., lm) . r This is represented \nin our IR as a . subsasgn(a, r, l1, l2, ..., lm). When cast to the SSA form, it becomes b . subsasgn(a, \nr, l1, l2, ..., lm) The meaning of this operation is as follows: b has the same elements as a, except \nfor elements located by the m sub\u00adscripts l1, l2 andso onuntil lm, which are set to elements from r.The \nsubsasgn operator therefore exhibits seman\u00adtics similar to that of the Update operation described in \n[12] except for two important departures: Each subscript li (1 = i = m) can be an arbitrary array. The \nlocations in b that are set to elements in r are obtained by taking the Cartesian product of the elements \nin the subscripts. If pi is the number of ele\u00adments in subscript li, r is required to have the shape \np1 \u00d7p2 \u00d7\u00b7\u00b7\u00b7\u00d7pm. (There are corner cases that need to be handled. See [21] for details.)  If the maximum \nelemental value in li exceeds the ex\u00adtent of aalong the ith dimension, bhas that maximum value for its \nextent along the ith dimension. Locations created in b due to such expansions are set to 0.  In fact, \nthere is one more case that can arise due to an ad\u00additional shrinkage feature that MATLAB provides for \nthe L-indexing operation. For example, a(:, :, 2) . [] deletes all elements on the second page [21] of \nan x\u00d7y\u00d7z array, shrinking it to an x\u00d7y \u00d7(z -1) array. This func\u00ad tionality isn t currently supported \nby the translator. 2.3.3.1 In-Place L-Indexing. If the requisite amount of storage is available, can \nb be computed in-place in a for all possible li without risking the if ((___STC(_826s_4, 1) == 1 &#38;&#38; \n___STC(_826s_4, 2) == 1)) { /* First operand is a scalar. */ for (__i = 0; __i < (___STC(_833s_4, 1)*___STC(_833s_4, \n2)); __i++) _811s_4[__i] = _811s_4[0]+_804s_4[__i]; } else if ((___STC(_846s_4, 1) == 1 &#38;&#38; ___STC(_846s_4, \n2) == 1)) { /* Second operand is a scalar. */ for (__i = 0; __i < (___STC(_833s_4, 1)*___STC(_833s_4, \n2)); __i++) _811s_4[__i] = _811s_4[__i]+_804s_4[0]; } else  { /* Both operands have identical shapes. \n*/ for (__i = 0; __i < (___STC(_833s_4, 1)*___STC(_833s_4, 2)); __i++) _811s_4[__i] = _811s_4[__i]+_804s_4[__i]; \n} Figure 1: Partial  C Codefor theIR 811s 4 . 811s 4+ 804s 4 from the capr Benchmark    violation \nof semantics? It turns out that in the absence of the shrinkage feature, this can always be done by computing \nthe elements of b from the last to the .rst. This works because there are only two cases that need consideration: \n The array b doesn t expand and has the same shape as a. Then, those elements of a that get carried over \nto b will occupy the same positions in memory.  The array b expands along one or more dimensions of \n a. Then, those elements of a that get carried over to b will occupy the same or higher positions in \nmemory.5  Hence, forming b backwards will ensure that the elements in a get carried over to b before \nthey can get overwritten.   2.4 Coloring Heuristic Asimple O(Vf + Ef ) greedy heuristic that attempts \nto use as few colors as possible has been used in our current implementation. The heuristic visits each \nnode in turn, in the lexical order of the corresponding variable de.nitions, and assigns to it the smallest \namong the colors used so far that is consistent with its neighbors. If no such color exists, a new color \nis created and the node is assigned that color.  3. TYPE-BASED ALLOCATION (PHASE 2) Let Vf (c) be a \ncolor class in the interference graph G * f . Let S(v) be the size of the storage allocated for a variable \nv in Vf (c). Intuitively, if the storage sizes of all variables in Vf (c) were known, they could all \nbe overlaid starting from the same location as the largest sized variable among them. Unfortunately for \nthe compiler writer, MATLAB features both implicit typing (an expression s type may not be ex\u00adplicit \nfrom program syntax) and dynamic typing (an expres\u00adsion s type maydependonthe control-.owpathexercised \nat run time). Consequently, not only may S(v) be stati\u00adcally indeterminable, but it may also vary during \nexecution. Our heuristic approach to getting around these problems is to construct a storage-size partial \norder : that tries to capture a containment relationship while promoting spatial reuse. (The precise \nde.nition of this binary relation is given 5MATLAB organizes its arrays `ala FORTRAN. But this in \u00a7 3.2.) \nThe construction is done using inferred type infor\u00admation. Once : is formed, its maximal6 elements are \nfound, which are then used to decompose Vf (c) into groups. If there are q maximal elements under :, \nVf (c) is partitioned into a collection of q groups, one for each maximal element, such that the storage \nsizes of variables in a group are bounded by the maximal element corresponding to that group. 3.1 The \n Type Inference Engine The translator currently obtains the types of all vari\u00adables de.ned in a program \nusing an inference engine called  (MAthematica system for General-purpose Infer\u00adring and Compile-time \nAnalyses) [17]. Given a Mathematica representation of a MATLAB program,  infers the value range .(w), \nintrinsic type t (w), shape tuple s(w)and rank (i.e., array dimensionality) .(w) of each variable w.If \n cannot explicitly infer the extents or dimensionality of some w, it will return symbolic expressions \nfor s(w)and .(w) respectively. The unique aspect of these shape-tuple and rank expressions is that inferences \nare reused when\u00adever symbolic equivalence can be established [18]. That is,  will return the shape-tuple \nexpression s(v)for the shapetupleof w if it can establish that s(v) will de.nitely be live (in the data-.ow \nsense) at the de.nition of w and that s(v)and s(w) will always be equivalent.     3.2 The Storage-Size \nPartial Order The GCTD pass determines the storage size of a variable u either by statically estimating \nit, or by symbolically calcu\u00adlating it to be |s(u)||t (u)| where |s(u)| denotes the number of elements \nin an array whose shape tuple is s(u),7 and where |t (u)| denotes the storage size of a scalar having \nthe intrinsic type t (u).8 The pass uses the following formulation 6An element x is maximal under a partial \norder : if there exists no y such that x . y. 7An instance of a shape tuple would be (1, 4, 5), which \nde\u00adnotes the shape of an array having the extents 1, 4 and 5 in the .rst, second and third dimensions \nrespectively. 8Intrinsic types in can be any of BOOLEAN, BYTE, INTEGER, REAL, COMPLEX, NONREAL and the \nabstract  observation holds even if the layout was row major. illegal intrinsic type i that signi.es \nintrinsic type errors. to relate the storage sizes of two variables u and v: . . . . . S(u)and S(v)can \nbe . . . .statically estimated, . . .t(u)= t(v)and S(u) = S(v), . . . . . . S(u) : S(v)i. (1) S(u)and \nS(v) cannot be . . . .statically estimated, . . . .u is available at . . . . the de.nition of v, . . \n. . t(u)= t(v)and S(u) = S(v). . Because available at the de.nition is both a re.exive and transitive \nrelationship (see \u00a7 2), it is easy to see from Re\u00adlation 1 that : is indeed a partial order. The motivation \nbehind the formulation is to identify two categories of ar\u00adrays: those whose layouts can be .xed at compile \ntime and those among the dynamically allocated arrays whose stor\u00adages can be grown in a regular way. \nThe two disjoint criteria for : require identical intrinsic types; this was intentional so as to avoid \nboth the use of type castings in the generated C code and the issue of alignment restriction in C. 3.2.1 \nStack Allocation Static estimation of storage size is done in two situations: 1. the inferred shape tuple \ns(u)of a variable u is ex\u00adplicit that is, of the form (p1,p2,...,pk) where each extent pi (1 = i = k) \nis an integer; or 2. the variable u is de.ned at the join node u . f(v,w) and the sizes of both v and \nw are statically estimable.  The estimated size in the .rst case is |s(u)||t(u)|, while in the second \ncase it is max(S(v),S(w)). Arrays whose sizes can be statically estimated get allocated on the stack \nin the C translation. In particular, scalars in MATLAB can be directly mapped to scalar automatics in \nC. Besides en\u00adabling procedures to be reentrant, a stack allocation disci\u00adpline automatically materializes \nand disappears objects as procedure activation records get pushed and popped. Back\u00adend C compilers can \nalso take advantage of the fact that the relative displacements of each of the arrays within a stack \nframe will be compile-time knowns; knowing this could be helpful while gathering data dependency or aliasing \ninfor\u00admation. Furthermore, because all statically estimable sizes of the same intrinsic type within a \ncolor class form a single chain under the partial order :, the corresponding variables, which will form \na single group, could all be allocated within a single array whose size is the maximal element in the \nchain. 3.2.2 Heap Allocation Variables that satisfy neither condition in \u00a7 3.2.1 are deem\u00aded as having \nstatically inestimable sizes. These variables get allocated on the heap. Their storage sizes are expressed \nsymbolically as |s(u)||t(u)|. The GCTD pass binds all vari\u00adables within a group to a common storage area. \nCode gener\u00adated by our implementation also attempts to alleviate heap memory pressure by resizing storage \non the .y to the spe\u00adci.c needs of each variable in a group. This is in contrast to the stack allocation \ncase where the storage size of a group remains .xed at the maximal during a procedure activation. By \nincorporating the available at the de.nition clause, the second criterion of Relation 1 tries to identify \nstretches in the execution path along which arrays grow in one direction. Speci.cally, if S(u) : S(v) \nby the second criterion, then u must both be available at the de.nition of v anddeadafter it. (Otherwise, \nu wouldhave interferedwith v in Phase 1.) Thus, chained elements in : connected by the second cri\u00adterion \nwould potentially correspond to de.nitions and uses that get performed in sequence at run time and where \nthe de.nitions step through nondecreasingly sized arrays. ( Po\u00adtentially because availability as de.ned \nin \u00a7 2 is conserva\u00adtive.) Moreover, the closer uis to v in the control-.ow path, the better would be \nthe spatial reuse characteristics because u would still be in the higher levels of a memory hierarchy \nwhen it gets resized to v. In fact, the closest that u could be to v is if it is used in the same statement \nthat de.nes v. Closeness may thus be fostered by ensuring that the chains included in each group are \nas long as possible. Example 1: Nonresized Arrays with Symbolic Types When presented with the IR shown \nbelow on the left, the intrinsic types of t1, t2 and t3 will be inferred by to be COMPLEX, assuming \nthat nothing is known about t0. t1 . t0-1.345; s . . s-1.345; t2 . 2.788. t1; =. s . . 2.788. s; t3 \n. tan(t2); s . . tan(s); will also return the symbolic expression s(t0)for the shape tuples of t1, t2 \nand t3 [18]. This re.ects the fact that under all executions of the IR, the shapes of t1, t2 and t3, \nwhich are all results of elementwise operations in MAT-LAB, will be identical to that of t0. Assuming \nthat t0, t1 and t2 are dead on exit from the code fragment, we see that no pair of variables interfere, \neither due to overlapping du\u00adchains or due to operator semantics. Therefore all nodes in the fragment \ns interference graph can be ascribed the same color. Furthermore, because s(t0)= s(t1)= s(t2)= s(t3), \nt(t0)= t(t1)= t(t2)= t(t3), and each ti is available at the de.nition of ti+1 (0 = i = 2), we get S(t0) \n: S(t1) : S(t2) : S(t3). Thus, all the variables in the fragment can be bound to a common storage that \nwill be reused dur\u00ading the fragment s execution. In fact, since |s(t0)||t(t0)| = |s(t1)||t(t1)| = |s(t2)||t(t2)| \n= |s(t3)||t(t3)|, the storage sizes for all four variables can be statically determined to be the same. \nThis means that at each of their de.nitions, their associated storage needn t be resized at run time. \nThe IR shown above on the right indicates this by using the . superscript to denote a de.ned array that \nisn t resized. Example 2: Expandable Arrays with Symbolic Types A more interesting example that may \ninvolve array expan\u00adsion is showninthe IR givenbelow. Here, an x\u00d7 y identity matrix is created in a, \nwhich is then used to create a matrix b through the subsasgn operator. a . eye(x, y); b . subsasgn(a, \n1, i1, i2); Assuming that a is dead at the end of the code fragment, we see that the du-chains of a and \nb don t overlap. And because b canbe formedin a provided the latter is large enough (see \u00a7 2.3.3), we \nobserve that a and b don t inter\u00adfere. Hence, a minimal coloring of the interference graph will put both \na and b in the same color class. For the in\u00adtrinsic types of a and b, will return BOOLEAN;it will also \nreturn expressions for the shape tuples of a and b that in the absence of further information on x, y, \ni1 and i2 will likely be symbolic. However, because of the semantics described in \u00a72.3.3, we can be assured \nthat |s(a)|=|s(b)|will always be true. Thus, because t(a)= t(b)is also true, and a is available at the \nde.nition of b, S(a) :S(b) will hold by Relation 1. Hence, a can share the same storage as b. If the \nstorage sizes of both a and b are statically ines\u00adtimable, a resizing check would have to be inserted \nbefore the code generated for each of the IR statements. If a and b have statically estimable sizes, \nboth will be stack allocated within a maximal sized storage. These two situations are shown in the IR \nbelow where the \u00b1superscript indicates a de.ned array that may need resizing and the + superscript indicates \na de.ned array that if resized, will be grown. The superscripts are applicable only in the dynamic case. \ns \u00b1 . eye(x, y); s + . subsasgn(s,1, i1, i2); There exists one situation where a and b won t share the \nsame storage even if they don t interfere; this will happen if the size of only one of them can be statically \nestimated.  3.3 Decomposing a Color Class into Groups Adirected graph G =(V, E) is used to represent \nthe storage-size partial order :on a color class V .A directed edge u . v is introduced between a pair \nof distinct vari\u00adables u and v in V if and only if S(u) :S(v). The algorithm Decompose-color-class given \nbelow performs the decom\u00adposition. Decompose-color-class(G) 1 Find the component graph GSCC of G. The \ncomponent graph GSCC =(V SCC,ESCC)of a di\u00adrected graph G =(V, E) has a node for each strongly connected \ncomponent (SCC) in G,and a directed edge x .y if there is a directed edge from a node in the SCC in G \ncorresponding to x toanodein the SCC in G corresponding to y [10]. 2 Decompose GSCC into a forest of \ntrees by invoking either depth-.rst search (DFS) or bread-.rst search (BFS) on nodes with in-degrees \nof 0. All variables in an SCC of G have the same storage size. Thus, the root of each tree returned by \nDecompose-color\u00adclass corresponds to an SCC in G whose variables have a maximal storage size under :. \nThis is because the roots have in-degrees of 0 in GSCC . Note that in-degrees of 0 will exist because \nGSCC has the important property of being acyclic [10]. Therefore, variables in all those SCCs in G that \ncorrespond to a returned tree s nodes have storage sizes that are bounded by a maximal element in :. \nHence, the collec\u00adtion of trees returned by Decompose-color-class forms a decomposition of a color class \ninto groups. Lemma 1. If every node in a color class belongs to a unique maximal chain9 under :,then \nDecompose-color\u00adclass assigns all nodes in a maximal chain to the same group. Proof. Suppose there exists \na maximal chain in which nodes belong to di.erent groups. Because both BFS and DFS touch all nodes reachable \nfrom a root, this means that there exists a u such that S(u) is bounded by at least two distinct maximal \nelements, which is a contradiction. 9A chain is maximal if no other chain properly subsets it. Hence, \nfrom Lemma 1, Decompose-color-class auto\u00ad matically fosters the closeness property mentioned in \u00a73.2.2 \nexcept in one situation. That exceptional situation occurs if a node is common to two or more maximal \nchains. The im\u00ad plementation currently assigns nodes common to two maxi\u00ad mal chains wholly to one of \nthem. In terms of complexity, the running time of Decompose\u00ad color-class is O(V +E)since Step 1 can be \ndone in O(V + E) time [10], and because Step 2 can also be done in O(V + E) time since V SCC = O(V )and \nESCC = O(E).  4. PERFORMANCE EVALUATION The e.cacy of the GCTD pass was evaluated by collecting metrics \nrelating to memory footprints and execution times over a set of 11 MATLAB programs obtained from di.er\u00adent \nsources. These benchmarks are listed in Table 1 along with brief descriptions, their origins, and their \nsizes in terms of the number of .les that constitute their source codes (called M-.les in MATLAB jargon) \nand the total number of nonempty noncomment lines in them. 4.1 Benchmarks Organization Programs in the \nset were organized along the lines of the FALCON benchmark suite in which the main function in a program \nis invoked from a driver routine. Typical tasks performed by a driver are the preparation of arguments \nfor an invocation, the display of results from an invocation and the timing of the entire execution. \nThough can handle built-in functions like dispand fprintfthat produce out\u00adputs to external .les, no \nsupport currently exists for loading data from external .les. Some of the drivers in their original form \ndid read data from external .les; these were modi.ed by the inclusion of the loaded data within the driver. \nThis wasn t found to be a problem because the loaded data was con.ned to only a few scalars. Nevertheless, \nthe current in\u00adability to handle MATLAB s loadbuilt-in function is a lim\u00aditation of our implementation. \nPrograms that produce data by alternate means, such as by using the MATLAB random number generator rand, \ncan be handled by our system.  4.2 Platform Speci.cations All measurements were done on a 440 MHz UltraSPARC-IIi \nworkstation running Solaris 7 and having 128MB of main memory. The version of the MATLAB interpreter \nused was 6.1 (Release 12), while the version of mcc (The MathWorks MATLAB-to-C compiler) used was 2.2.10 \nVersion 5.1 of the Sun Workshop C compiler was used for back-end compila\u00adtion by both mcc and mat2c (the \n program). Recall from \u00a73.1 that an external engine is used to infer the pro\u00ad gram variable types; this \nis invoked transparently by mat2c and was executed on version 4.1 of the Mathematica kernel. The back-end \nC compiler in both cases was passed the -xO4 and -xlibmil options that turn on a host of global and local \noptimizations, and inline certain library routines for faster execution. All optimizations o.ered by \nmcc were turned on. To minimize memory usage, the MATLAB in\u00ad terpreter was always run with the -nojvm \noption. This suppresses the loading of a Java virtual machine that allows a MATLAB session to draw on \nJava s capabilities. 10In July of this year, latest versions of both 6.5 (Release 13) and 3.0 were announced. \n Benchmark Synopsis Origin M-Files Lines adpt Adaptive Quadrature by Simpson s Rule . FALCON 2 79 capr \nTransmission Line Capacitance Chalmers University of Technology, Sweden 5 68 clos Transitive Closure \n. OTTER 2 30 crni Crank-Nicholson Heat Equation Solver FALCON 3 48 diff Young s Two-Slit Di.raction Experiment \nThe MathWorks Central File Exchange 2 40 dich Dirichlet Solution to Laplace s Equation FALCON 2 49 edit \nEdit Distance The MathWorks Central File Exchange 2 34 . fdtd Finite Di.erence Time Domain (FDTD) Technique \nChalmers University of Technology, Sweden 2 47 fiff Finite-Di.erence Solution to the Wave Equation FALCON \n2 32 nb1d One-Dimensional N-Body Simulation OTTER 2 53 . nb3d Three-Dimensional N-Body Simulation Modi.ed \nnb1d 2 46 . Benchmarks involve three-dimensional arrays. . FALCON MATLAB Compiler Test Suite [13]. . \nOTTER Parallel MATLAB Compiler Test Suite [20]. Table 1: Benchmark Suite Description Benchmark Static/Dynamic \nVariable Reduction Original Variable Count Storage Reduction (KB) adpt 127/74 271 0.96 capr 84/75 301 \n0.68 clos 24/0 46 1216.14 crni 73/0 113 4055.85 diff 48/1 93 12.77 dich 82/0 107 144.90 edit 25/21 108 \n0.21 fdtd 111/0 168 4374.61 fiff 51/0 77 12712.92 nb1d 66/63 235 0.55 nb3d 58/54 191 0.59 4500 4000 \n  Stack+Heap Segments Size (KB) 3500 3000 2500 2000 1500 1000 500 0 adpt capr clos crni diff dich \nedit fdtd fiff nb1d nb3d Table 2: Array Storage Coalescing Reductions  4.3 Storage Reductions Table \n2 shows reductions in variable count and corre\u00adsponding reductions in storage size due to the GCTD al\u00adgorithm. \nEntries in the Static/Dynamic Variable Reduc\u00adtion column are of the form s/d where s is the number of \nvariables whose array sizes are statically estimable and that get subsumed in another array by the GCTD \npass, and where d is the number of variables whose sizes are statically inestimable (thus requiring dynamic \nallocation) but which can still be statically subsumed within another dynamically allocated variable \nbecause of the storage-size partial order :. The Original Variable Count column indicates the total number \nof variables in the CFG on entry to the GCTD pass. This includes variables in the program source, as \nwell as temporaries introduced in preceding transformations like the SSA and SO form (see \u00a7 2.3) conversions. \nOther kinds of variables that also come under the coalescing regime of the GCTD pass, such as those assigned \nin symbolic shape tuple and rank expressions, are included in the above count. Observe that for .ve benchmarks, \nd is 0. This is because manages to explicitly infer all the shape tuples in them, causing all of their \nstorage to be stack allocated by the GCTD algorithm. The corresponding reduction in bytes due to the \ncoalescing of stack allocated variables is displayed Benchmarks Figure 2: Average Stack, and Stack+Heap \nLevels in the Storage Reduction column. This .gure is thus con\u00adservative in the savings in storage that \nit re.ects because reductions due to the coalescing of heap allocated variables aren t included in it. \nStill, in 4 out of 11 programs, static re\u00adductions are seen to be over a megabyte, and close to 13 MB \nin fiff. This is on account of the large coalescent arrays ( 451 \u00d7 451) that are operated upon by this \nbenchmark. This is also the reason why with coalescing turned on, code generated by mat2cfor fiffruns \nnearly two orders of mag\u00adnitude faster than code generated by mcc(see Figure 5), and why without coalescing, \ncode generated by mat2c is nearly six orders of magnitude slower (see Figure 6)! The key con\u00adtributors \nto the relatively large reductions in variable count are the temporaries introduced as part of the SO \nform con\u00adversion process. Since these temporaries only serve to break long expressions into smaller ones, \nthey have considerable potential for reuse both within and across expressions.  4.4 How mccHandles Arrays \nThe mcc compiler relies on run-time type determination for its generated C code. In essence, the approach \nis to rep\u00adresent every array by a C struct called mxArray,which besides embedding the contents of the \narray, has a number of .elds that contain meta information such as the array s 4 x 10  Benchmarks \nFigure 3: Average Virtual Memory Levels shape and intrinsic type. These .elds are set up at run time \nwhen arrays get created, and are examined and modi\u00ad.ed as conformance checks are performed and arrays \nevolve. A consequence is that all arrays in the mcc-generated C code are allocated on the heap. As a \nresult, memory us\u00adage conservation also happens at run time where the library versions of each of the \nMATLAB operators have the onus of creating and deleting arrays. Array duplication due to copies is also \nminimized dynamically through sharing and a copy-on-write scheme. The way the automated memory management \nworks is that mxArray structures created and returned within nested calls of library functions are deallo\u00adcated \nimmediately after being used. Another consequence of this run-time approach is that an mxArraystructure, \nwhose size in version 2.2 of mcc is 88 bytes, will be allocated for scalars that don t get folded at \ncompile time. This library-based model of compilation isn t unique to MATLAB; it is also used in compilers \nfor other similar typeless array-based languages like APL.  4.5 Run-Time Memory Footprints Figures 2 \nto 4 display the average memory levels in the stand-alone C codes automatically generated by mat2cwith \nthe GCTD pass turned on, and by mcc. Reductions in the dynamic program data sizes (stack plus heap space) \nrelative to the mat2c C codes are shown as percentages above the bars in Figure 2. Relative reductions \nin other categories of memory are also shown as percentages in Figures 3 and 4. 4.5.1 Average Stack Trends \nWhen a C program begins execution, there is already one stack frame on its run-time stack that contains \nthe initial process environment such as the argcand argvparameters and the array of strings representing \nthe process s environ\u00adment variables. Thus, the stack segment, which grows in units of pages, will initially \nbe at least one page in size, which is 8KB on the Solaris 7 UltraSPARC-IIi platform on which we ran our \nexperiments. Further procedure invoca\u00adtions from main can cause the stack segment to grow de\u00adpending \non how local variables are allocated and referenced in the invoked functions. Because the mcc C codes \nbank on the heap for all array allocations and use function interfaces only for the passing and allocation \nof handles to these ar\u00adrays, the high watermark of their run-time stacks shouldn t be expected to be \nlarge. Indeed, the mcc Ccodes for all benchmarks were found to have a stack segment size that grows to \n16KB and stays at that. This is why the average stack segment size of the mcc C codes in Figure 2 is \n16KB. Figure 2 also shows four prominent peaks for the aver\u00adage stack segment size of the mat2c Ccodes \nfor the clos, crni, fdtd,and fiffbenchmarks. This is because mat2c allocates all arrays in these benchmarks \non the stack. In the other benchmarks, signi.cant percentages of the array shapes were symbolic; this \nled to their storage being primar\u00adily allocated on the heap. The exception was dich in which though 100% \nof the array shapes were statically inferred, the overall average stack segment size was only about 17.5KB \nbecause most of the arrays in this benchmark were small.  4.5.2 Average Heap Trends Figure 2 also shows \nthe average stack and heap space sums across all benchmarks. All average memory sizes, be it stack size, \nvirtual memory level or resident set level, were calcu\u00adlated using a weighted time-averaged formula. \nIf mi was the observed memory size in some small duration .ti, then the average memory size M was calculated \nby the expression i mi.ti M = . (2) i .ti To ensure the interception of rapid .uctuations in memory \nlevels in Equation (2), .ti was made as small as possible. In our measurements, .ti was between 250 to \n350 microsec\u00adonds, which was usually about a thousandth to a hundredth thousandth of a C code s execution \ntime. In general, the average dynamic program data sizes of mat2c C codes were smaller than that of mcc \nCcodes; the relative reductions were over 20% in 7 out of 11 cases, being over 100% in over half of the \ncases. In the case of capr, though the mcc C code fared better by a small mar\u00adgin, it still has a far \nhigher kcore-min value as discussed in \u00a7 4.5.2.1 below. (For capr, the average dynamic program data sizes \nfor the mcc and mat2c C codes were 2428.02KB and 2506.75KB.) 4.5.2.1 KCore-Min Reductions. An important \npoint that the average .gures don t uncover is the duration of consumption of a memory resource. If pro\u00adcesses \nP1 and P2 both consume x kilobytes of memory, P1 for t seconds and P2 for 2t seconds, then both will \nhave the same time-averaged memory consumption level but clearly P2 will be the bigger memory hog. To \ntake into consider\u00adation the e.ect of time, UNIX systems use a metric called the kcore-min value of a \nprocess that is computed as kcore-min = M \u00d7 T where M is the mean memory size in kilobytes and T is the \nduration of memory usage in minutes. Thus, even though the average dynamic program data size of the mat2c \nC code was close to that of the mcc Ccode in capr and nb3d,the mat2c versions have lower kcore-min values \nbecause of their shorter execution times. The rela\u00adtive kcore-min reductions were 102.3% and 71.5% in \nthese RS Size (KB) 16000 mcc mat2c 100.0% 5.5% 57.1% 147.7% 68.0% 279.6% 27.1% 64.3% 124.0% 22.9% \n11.0% 14000 12000 10000 8000 6000 4000 2000 0 adpt capr clos crni diff dich edit fdtd fiff nb1d \nnb3d  Time (second) mat2c without GCTD 5 mat2c with GCTD 10 4 3.13 39.30 195.41 142.87 1.00 356.69 \n1075.76 3.61 359206.10 1.25 1.41 10 3 10 2 10 1 10 0 10 -1 10 -2 10 adpt capr clos crni \ndiff dich edit fdtd fiff nb1d nb3d  Benchmarks Benchmarks Figure 4: Average Resident Set Levels Figure \n6: E.ect of Coalescing on Execution Times 10 2 1.1 2.1 mcc mat2c intrp 1.3 82.6 2.4 257.9 6.2 2.5 91.1 \n11.4 1.7 10 1 10 0Time (second) 10 -1 10 -2 adpt capr clos crni diff dich edit fdtd fiff nb1d nb3d \nBenchmarks Figure 5: Comparative Execution Times two cases and are shown using arrow markers in Figure \n2. Though not shown in Figure 2, the mat2c Ccodes for all other benchmarks also exhibit kcore-min reductions \nbecause besides having lower size averages, they also usually have signi.cantly lower execution times \nas shown in Figure 5.  4.5.3 Overall Memory Levels To obtain the complete picture, the average virtual \nmem\u00adory consumption levels of the mat2cand mccCcodes, which includes all swapped-out pages, mapped .les \nand devices, is shown in Figure 3. The average resident set sizes (RSS) for all benchmarks are also shown, \nin Figure 4, which describe the amount of physical memory used by a process. The RSS numbers are signi.cant, \nespecially in an embedded setting, because non-resident pages don t task a RAM. Note that the sizes of \nthe compiled images of the mat2c and mcc C codes also a.ect both these levels. However, it should be \nmentioned that the binary image size of a mat2c Ccode is nearly always larger than that of an mccC code. \nThe reason for this is that mat2cinlines out most of the language oper\u00adations (this can be controlled \nby a pass-.le option) whereas mcc always translates them to calls into a library. Hence, at the heart \nof the resource usage di.erences are the disparate approaches to compilation used by mat2c and mcc. \n 4.6 Execution Time Improvements Figure 5 compares execution times of the mat2c and mcc C codes on a \nlog scale. The .gure also shows the times taken by the MATLAB interpreter to execute the benchmarks. \nIn\u00addicated above the bars are the performance speedups of the mat2c C codes over the mcc C codes. In \nonly one case was the speedup marginal 10% for the adpt benchmark. The adpt benchmark has a parameter \ncalled tol that speci.es the error tolerance in the quadrature values produced. De\u00adcreasing this value \ndecreases the fractional overhead due to type checking, making numerical computation the dominant work \nin the benchmark. Measurements for adpt were taken with tol set at 10-12 , which is the setting in the \nFAL-CON benchmark suite. Because increasing tol increases the relative overhead due to type checking, \nthe mat2c C code performs much better than the mcc Ccode athigher values of tol. In all other benchmarks, \nthe speedups ranged from 30% and 74% in two cases, to over 100% in the remain\u00ading. In fact, in 4 out \nof 11 benchmarks, the speedups were dramatic, being over an order of magnitude. 4.6.1 Impact of the GCTD \nAlgorithm Figure 6 exhibits the in.uence of the GCTD pass on the execution times of the mat2c C codes. \nAll other opti\u00admizations o.ered by the system, like common subex\u00adpression elimination, constant folding, \ndead-code elimina\u00adtion and loop unrolling, were active in both cases. Numbers shown above the bars indicate \nthe relative speedups with and without the GCTD pass. These relative speedups, in the context of the \ntimings displayed in Figure 5, show that without it, the mat2cC codes would have performed poorly with \nrespect to the mcc C codes in 8 out of 11 cases. This demonstrates the pivotal role that the GCTD pass \nplays in improving performance through the better static manage\u00adment of storage.  5. SUMMARY This paper \npresented an algorithm for the e.cient static management of storage in MATLAB through the coalescing \nof arrays. The algorithm consists of two phases the .rst of which uses the classic notion of interference \nto form classes of variables that don t compete for storage and the second of which decomposes those \nclasses on the basis of program variable types and control .ow. Unique aspects of the phases are the \nconsideration of interference due to operator seman\u00adtics, its resolution using types, and the use of \nsymbolic type information in the decomposition of color classes. Rather than clumping together all arrays \nhaving symbolic shapes within a color class, which we have observed to be a poor storage management policy, \nthe second phase uses a partial order to facilitate better spatial reuse characteristics. The algorithm \nis also applicable to other typeless array-based languages like IDL and APL that present similar issues. \nThe algorithm is also nonoptimal. The simplest example is an interference graph that consists of three \nnodes A, B and C representing variables with identical intrinsic types, whose corresponding storage sizes, \nassuming all are stati\u00adcally estimable, are say 4, 2 and 3 units respectively. If the only edge in the \ninterference graph is between A and B, its chromatic number is 2. However, the aggregate of the coalesced \nstorages will di.er depending on which minimal coloring is actually used if B and C share the same color, \nthe aggregate will be 7 units, whereas if A and C share the same color, the aggregate will be 6 units. \nThus, arriving at an optimal solution to the problem even in the simpler case of all array shapes being \ncompile-time knowns would require an exploration of all possible colorings, a point that Fabri had also \nnoted [15]. However, our experiments show that even with a conservative approach to the problem like \nours, considerable savings in storage space and improvements in execution performance can be achieved. \n 6. REFERENCES [1] P.S.Abrams. An APL Machine. Ph.D. dissertation, Stanford University, Feb. 1970. Available \nas Technical Report SLAC 114 from the Stanford Linear Accelerator Center. [2] G. Alm\u00b4asi and D. A. Padua. \nMAJIC: Compiling MATLAB for Speed and Responsiveness. In Proceedings of the ACM SIGPLAN Conference on \nProgramming Language Design and Implementation, pages 294 303, June 2002. [3] P. Briggs. Register Allocation \nvia Graph Coloring. Ph.D. dissertation, Rice University, Apr. 1992. [4] P. Briggs, K. D. Cooper, K. \nKennedy, and L. Torczon. Coloring Heuristics for Register Allocation. In Proceedings of the ACM SIGPLAN \nConference on Programming Language Design and Implementation, pages 275 284, July 1989. [5] T. Budd. \nAn APL Compiler. Springer-Verlag, Inc., New York City, NY 10010, USA, 1988. [6] G. J. Chaitin, M. A. \nAuslander, A. K. Chandra, J. Cocke, M. E. Hopkins, and P.W.Markstein. Register Allocation via Coloring. \nComputer Languages, 6(1):47 57, Jan. 1981. [7] W.-M. Ching. Program analysis and code generation in an \napl/370 compiler. IBM Journal of Research and Development, 30(6):594 602, Nov. 1986. [8] F. Chow and \nJ. Hennessy. Register Allocation by Priority-Based Coloring. In Proceedings of the ACM SIGPLAN Symposium \non Compiler Construction, pages 222 232, June 1984. [9] D. Cociorva, G. Baumgartner, C.-C. Lam, P. Sadayappan, \nJ. Ramanujam, M. Nooijen, D. E. Bernholdt, and R. Harrison. Space-Time Tradeo. Optimization for a Class \nof Electronic Structure Calculations. In Proceedings of the ACM SIGPLAN Conference on Programming Language \nDesign and Implementation, pages 177 186, June 2002. [10] T. H. Cormen, C. E. Leiserson, and R. L. Rivest. \nIntroduction to Algorithms. The MIT Electrical Engineering and Computer Science Series. The MIT Press, \n1995. [11] R. Cytron and J. Ferrante. What s in a Name? The Value of Renaming for Parallelism Detection \nand Storage Allocation. In Proceedings of the 1987 International Conference on Parallel Processing, pages \n19 27, Aug. 1987. [12] R. Cytron, J. Ferrante, B. K. Rosen, and M. N. Wegman. E.ciently Computing Static \nSingle Assignment Form and the Control Dependence Graph. ACM Transactions on Programming Languages and \nSystems, 13(4):451 490, Oct. 1991. [13] L. A. De Rose and D. A. Padua. Techniques for the translation \nof matlab programs into fortran 90. ACM Transactions on Programming Languages and Systems, 21(2):286 \n323, Mar. 1999. [14] A. P. Er.sov. Reduction of the Problem of Memory Allocation in Programming to the \nProblem of Coloring the Vertices of Graphs. Doklady Akademii Nauk SSSR, 142:785 787, Jan. 1962. English \ntranslation in Soviet Mathematics, Vol., 3, No., 1, July 1962, pages 163 165. [15] J. Fabri. Automatic \nStorage Optimization. In Proceedings of the ACM SIGPLAN Symposium on Compiler Construction, pages 83 \n91, Aug. 1979. [16] L. J. Guibas and D. K. Wyatt. Compilation and Delayed Evaluation in APL. In Proceedings \nof the 5th ACM SIGPLAN Symposium on Principles of Programming Languages, pages 1 8, Jan. 1978. [17] P. \nG. Joisha and P. Banerjee. :A Software Tool for Inferring Types in MATLAB. Technical  Report CPDC TR \n2002 10 004, Center for Parallel and Distributed Computing, Department of Electrical and Computer Engineering, \nNorthwestern University, Oct. 2002. [18] P. G. Joisha and P. Banerjee. Implementing an Array Shape Inference \nSystem for MATLAB Using Mathematica. Technical Report CPDC TR 2002 10 003, Center for Parallel and Distributed \nComputing, Department of Electrical and Computer Engineering, Northwestern University, Oct. 2002. [19] \nS. S. Lavrov. Store Economy in Closed Operator Schemes. Zhurnal vychislitel noi matematiki i matematicheskoi \n.ziki, 1(4):687 701, 1961. English translation in U.S.S.R. Computational Mathematics and Mathematical \nPhysics, Vol., 1, No., 3, 1962, pages 810 828. [20] A. Malishevsky. Implementing a Run-Time Library for \na Parallel MATLAB Compiler. M.S. report, Oregon State University, Apr. 1998. [21] The MathWorks, Inc. \nMATLAB: The Language of Technical Computing, Jan. 1997. Using MATLAB (Version 5). [22] P. Pineo and M. \nL. So.a. A Practical Approach to the Symbolic Debugging of Parallelized Code. In Proceedings of the 5th \nInternational Conference on Compiler Construction, pages 339 356, Apr. 1994. [23] M. Poletto and V. Sarkar. \nLinear scan register allocation. ACM Transactions on Programming Languages and Systems, 21(5):895 913, \nSept. 1999. [24] Y. Zhang and R. Gupta. Data Compression Transformations for Dynamically Allocated Data \nStructures. In Proceedings of the 11th International Conference on Compiler Construction, pages 14 28, \nApr. 2002.  \n\t\t\t", "proc_id": "781131", "abstract": "Static array storage optimization in MATLAB.", "authors": [{"name": "Pramod G. Joisha", "author_profile_id": "81100489083", "affiliation": "Northwestern University, Evanston, IL", "person_id": "P328656", "email_address": "", "orcid_id": ""}, {"name": "Prithviraj Banerjee", "author_profile_id": "81100147290", "affiliation": "Northwestern University, Evanston, IL", "person_id": "PP15024230", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/781131.781160", "year": "2003", "article_id": "781160", "conference": "PLDI", "title": "Static array storage optimization in MATLAB", "url": "http://dl.acm.org/citation.cfm?id=781160"}