{"article_publication_date": "05-09-2003", "fulltext": "\n Compile-time Composition of Run-time Data and Iteration Reorderings Michelle Mills Strout Larry Carter \nJeanne Ferrante UC, San Diego UC, San Diego UC, San Diego 9500 Gilman Dr., Dept 0114 9500 Gilman Dr., \nDept 0114 9500 Gilman Dr., Dept 0114 La Jolla, CA 92037-0114 La Jolla, CA 92037-0114 La Jolla, CA 92037-0114 \nmstrout@cs.ucsd.edu carter@cs.ucsd.edu ferrante@cs.ucsd.edu ABSTRACT Many important applications, such \nas those using sparse data structures, have memory reference patterns that are unknown at compile-time. \nPrior work has developed run\u00adtime reorderings of data and computation that enhance lo\u00adcality in such \napplications. This paper presents a compile-time framework that al\u00adlows the explicit composition of run-time \ndata and iteration\u00adreordering transformations. Our framework builds on the iteration-reordering framework \nof Kelly and Pugh to rep\u00adresent the e.ects of a given composition. To motivate our extension, we show \nthat new compositions of run-time re\u00adordering transformations can result in better performance on three \nbenchmarks. We show how to express a number of run-time data and iteration-reordering transformations \nthat focus on improv\u00ading data locality. We also describe the space of possible run-time reordering transformations \nand how existing trans\u00adformations .t within it. Since sparse tiling techniques are included in our framework, \nthey become more generally ap\u00adplicable, both to a larger class of applications, and in their composition \nwith other reordering transformations. Finally, within the presented framework data need be remapped \nonly once at runtime for a given composition thus exhibiting one example of overhead reductions the framework \ncan express. Categories and Subject Descriptors D.3.4 [Processors]: Optimization General Terms Performance, \nExperimentation Keywords optimization, run-time transformations, data remapping, it\u00aderation reordering, \ninspector/executor, sparse tiling Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. PLDI 03, June 9 11, 2003, San Diego, California, USA. Copyright 2003 \nACM 1-58113-662-5/03/0006 ...$5.00.  1. INTRODUCTION Data locality and parallelism are essential for \nimproving the performance of applications on current architectures. Data and loop transformations can \nfurther both goals. Un\u00adtil recently the focus has been primarily on compile-time transformation frameworks \n[19, 27, 18, 3, 16, 17, 32, 14, 31] restricted to a.ne loop bounds and a.ne array references. These frameworks \nallow for the uniform representation, the composition, the legality determination, and sometimes a bene.t \nmodel of various compile-time transformations. One such framework that of Kelly and Pugh [16] represents \nloop nests as iteration spaces. A compiler can use this framework to transform iteration spaces. The \nle\u00adgality of a transformation is determined by the transformed data dependences of the program. Previous \ncompile-time frameworks (including theirs) conservatively assume depen\u00addence when faced with non-a.ne \nmemory references, which occur in many important applications such as sparse ma\u00adtrix and unstructured \nmesh computations [21]. Fortunately, the Kelly and Pugh framework describes non-a.ne mem\u00adory references \n(such as indirect memory references A[B[i]]) by using Presburger arithmetic with uninterpreted function \nsymbols [23]. We exploit this ability to specify data map\u00adpings between loop iterations and data locations, \nand de\u00adpendences between loop iterations, when non-a.ne memory references are involved. We can also express \nrun-time data and iteration-reordering transformations for locality, which include consecutive packing \n[7], graph partitioning [12], bucket-tiling [21], lexicographical grouping [7], full sparse tiling [29], \nand cache blocking [9]. Describing the e.ect of run-time data and iteration re\u00adorderings in a compile-time \nframework has several advan\u00adtages. First, both run-time and compile-time transforma\u00adtions are uniformly \ndescribed. Secondly, the transformation legality checks provide constraints on the run-time reorder\u00ading \nfunctions. Finally, the overhead involved in generating the run-time reordering functions can be reduced \nwith vari\u00adous optimizations, such as moving the data to new locations only once and traversing fewer \ndependences. Run-time reordering transformations are implemented with inspectors and executors, originally \ndeveloped for paralleliza\u00adtion [6]. In this setting, the inspector traverses the index ar\u00adrays that describe \nthe data mappings and/or dependences. Based on the values in these arrays, it generates data and/or iteration-reordering \nfunctions. The executor is a transformed version of the original loop that uses the reordered data and/or \nnew schedule based on the reordering functions. Our key insight is that given a composition of run-time \nreorder\u00adings, the modi.ed data mappings and/or dependences are used by inspectors for subsequent run-time \nreordering trans\u00adformations. While this paper focuses on data locality, our framework can also be used \nto describe run-time reordering transformations for parallelism.  Formalizing run-time data and iteration \nreorderings is only one step toward our goal of automating the creation of com\u00adposite run-time transformations \nfor computations with non\u00ada.ne memory references. Still needed are methods of au\u00adtomatically generating \nrun-time inspectors for a variety of reordering heuristics. Another key component is guidance mechanisms \nthat decide when to apply which sequence of transformations. These decisions should probably be made \nat runtime based on the characteristics of the actual data mappings and dependences. Nevertheless, we \nbelieve that the framework presented here helps make such a system pos\u00adsible, and our experiments (on \nthe irreg, nbf, and moldyn benchmarks [12]) illustrate that, in some cases, large per\u00adformance improvements \nresult from composite run-time re\u00adordering transformations. Summarizing, we make the following contributions: \n We give experimental results (using hand-coded ver\u00adsions which we believe can ultimately be automatically \ngenerated) that show that signi.cant performance im\u00adprovements can result from composing run-time re\u00adordering \ntransformations.  We show how to use an existing compile-time frame\u00adwork to describe a number of existing \nrun-time data and iteration-reordering transformations that improve data locality. We also describe the \nspace of possible run-time reordering transformations and how existing transformations .t within it. \n We show how sparse tiling techniques, which improve locality in loops with data dependences, can be \nde\u00adscribed in this framework. As a result, sparse tiling can be applied to a larger class of programs \n(until now, it has only been applied to Gauss-Seidel).  We give experimental results that show moving \nthe data to new locations only once reduces the overhead of composed run-time reordering transformations. \n Section 2 motivates the composition of run-time reorder\u00ading transformations by describing experimental \nresults where full sparse tiling composed with other transformations can result in signi.cant performance \nimprovements. Section 3 reviews the terminology for the Kelly and Pugh framework using an example irregular \nkernel (used throughout the pa\u00adper). In Section 4, we describe how to formally express run\u00adtime reordering \ntransformations and the space of possibili\u00adties for such transformations. Section 5 formally describes \nan example composed inspector. Section 6 shows examples of using the framework to reduce the overhead \nof run-time reordering transformations. Finally, sections 7, 8 and 9 are future work, related work, and \nconclusions.  2. MOTIVATION FOR COMPOSITIONS Our experiments compare the performance resulting from \nvarious run-time transformation compositions on the moldyn, nbf, and irreg benchmarks. The goal is to \nmotivate a  Figure 2: Example mapping of iterations in the j loop to locations in the data arrays x \nand fx. Here, circles represent iterations of the j loop inside one iteration of the outer time-stepping \nloop. j  Figure 3: Example of Figure 2 mapping after the CPACK data reordering. on the iteration to \ndata mapping in the j loop. Partitioning algorithms like Gpart [12] logically operate on a graph where \neach data location is a node. There is an edge between two nodes whenever their associated data is accessed \nwithin a loop iteration. By partitioning the nodes (ie. data) of the graph so that the data associated \nwith each partition .ts into (some level of) cache and ordering the data consecutively within a partition, \nGpart improves the spatial locality of the computation. 2.2 Run-time Iteration Reordering Often it is \nbene.cial for an iteration reordering of a loop with non-a.ne memory accesses to follow a data reorder\u00ading \n[7, 12]. The goal of iteration reordering is to reorder the iterations based on the order in which the \nloop touches the data. One such iteration-reordering transformation is lexicograph\u00adical grouping (lexGroup) \n[7]. For the simple moldyn exam\u00adple, Figure 3 shows how lexGroup further changes the data mapping between \nthe iterations in the j loop to locations in the data arrays x and fx. Notice that iterations which touch \nthe same or adjacent data locations now execute consecu\u00adtively, therefore the computation exhibits better \ntemporal and spatial locality. We experimented with the iteration-reordering transfor\u00admations bucket \ntiling [21] and lexicographical sorting [12] as well. However, lexicographical grouping (lexGroup) consis\u00adtently \nexhibited the best performance to overhead trade-o. on our benchmarks; therefore, the results in this \npaper al\u00adways use lexGroup for reordering the iteration of the j loop. Figure 4: Example of Figure 2 \nmapping after the CPACK data reordering followed by a lexGroup it\u00aderation reordering. 2.3 Sparse Tiling \nSparse tiling programming techniques, full sparse tiling [29] and cache blocking [9], were developed \nfor an important ker\u00adnel used in Finite Element Methods, Gauss-Seidel. Sparse tiling results in run-time \ngenerated tiles or iteration slices [24] that cut between loops or across an outer loop and that only \naccess a subset of the total data. By performing an iteration reordering based on sparse tiling, locality \nbetween loops or iterations of an outer loop improves. Sparse tiling di.ers from other iteration-reordering \ntrans\u00adformations in four ways. Sparse tiling improves the locality between loops and across iterations \nof outer loops even when there are data dependences. Other run-time iteration-reordering transformations \nfor data locality are not applicable when there are data dependences.  Whereas existing run-time iteration \nreorderings for lo\u00adcality are realized with inspectors which traverse the data mappings, sparse tiling \ninspectors traverse the dependences.  Until now, sparse tiling transformations have only been applied \nto Gauss-Seidel. By specifying the e.ect of sparse tiling within our composition framework, the legality \nof applying sparse tiling in any program can be determined.  Sparse tiling can also be used to provide \na coarser gran\u00adularity of parallelism than other run-time reordering transformations for parallelism \n[30].  Sparse tiling starts with a seed partitioning of iterations in one of the loops (or in one iteration \nof an outer loop). If other data and iteration-reordering transformations have been applied to the loop \nbeing partitioned, then consecutive iterations in the loop have good locality and a simple block partitioning \nof the iterations is su.cient to obtain an e.ec\u00adtive seed partitioning. From this seed partitioning tiles \nare grown to the other loops involved in the sparse tiling by a traversal of the data dependences between \nloops (or between iterations of an outer loop). The main di.erence between full sparse tiling and cache \nblocking is how tile growth oc\u00adcurs. In cache blocking [9], the seed partitioning occurs on the .rst \niteration of an outer loop and then tiles are grown by shrinking each partition for later iterations \nof that outer loop. The remaining iteration points are assigned to one tile. Full sparse tiling allows \nthe seed partitioning to occur   Figure 5: We highlight the iterations of one sparse tile for the code \nin Figure 1. The j loop has been blocked to provide a seed partitioning. In the full sparse tiled executor \ncode, the iterations within a tile are executed atomically. at any loop or iteration within an outer \nloop, and tiles are grown side-by-side. For the simpli.ed moldyn example, Figure 5 shows the status of \nthe data dependences between iterations of the i, j, and k loops after applying the data reordering trans\u00adformation \nCPACK and iteration-reordering transformation lexGroup. A full sparse tiling iteration reordering causes \nsubsets of all three loops to be executed atomically as sparse tiles. Figure 5 highlights one such sparse \ntile where the j loop has been blocked to create a seed partitioning. Fig\u00adure 14 shows the executor that \niterates over tiles and then within the i, j, and k loops. Since iterations within all three loops touch \nthe same or adjacent data locations, locality be\u00adtween the loops is improved in the new schedule. In \nour experiments, we also apply a data reordering trans\u00adformation, tile packing (tilePack), after applying \nfull sparse tiling. TilePack reorders the data arrays based on how data is accessed within tiles. For \nexample, in Figure 5 tilePack creates the data ordering 4, 2, 5, 6, 3, 1, resulting in the con\u00adsecutive \nordering of data accessed within the highlighted tile.  2.4 Experimental Results The sizes of the data \nsets we use in terms of nodes and edges in a representative graph are as follows. Data set nodes edges \nmol1 131072 1179648 mol2 442368 3981312 foil 144649 1074393 auto 448695 3314611 The baseline benchmarks \nand the executors for the run\u00adtime reordering transformation compositions use inter-array data regrouping \n[8] to leverage shared memory reference patterns between data arrays. All compositions we con\u00adsider consist \nof a data reordering transformation (CPACK or Gpart) followed by the iteration-reordering transformation \nlexicographical grouping (lexGroup) for the j loop. We also perform the composition CPACK, lexGroup, \nCPACK, lex-Group. Finally, we apply full sparse tiling (FST) after the other compositions to see if improving \nthe locality between the i, j, and k loops results in better performance. IBM Power3, 375MHz, 64KB L1 \ncache irreg nbf moldyn Figure 6: Normalized execution time without over\u00adhead on the Power3. Intel Pentium \n4, 1.7GHz, 8KB L1 cache irreg nbf moldyn Figure 7: Normalized execution time without over\u00adhead on the \nPentium 4. The composed run-time reordering transformations are executed on two architectures: a 375MHz \nPower3 (64KB L1 cache)1 and a 1.7GHz Pentium 4 (8KB L1 cache). On the Power3 we use the compilation command \nxlc -bmaxdata:0x80000000 -bmaxstack:0x10000000 -O3 -DNDEBUG , and on the Pentium 4 we use gcc -O2 -DNDEBUG \n2 . In our experiments, we target the L1 cache when selecting parameters for Gpart and full sparse tiling \nsuch as partition size. Figures 6 and 7 show the normalized execution times for the various compositions \nwithout overhead. The number of outermost loop iterations (time steps) required to amortize the run-time \noverhead are shown in Figures 8 and 9. We calculate this number by taking the execution time of the inspector \nand dividing by the savings per time step observed in the executor. When we apply our full sparse tiling \ntechnique in compo\u00adsition with existing run-time data and iteration-reordering 1 A single node of the \nIBM Blue Horizon at the San Diego Supercomputer Center. 2 gcc version 2.96 on Red Hat Linux 7.2 2.96-108.7.2 \nIBM Power3, 375MHz, 64KB L1 cache 3. FRAMEWORK TERMINOLOGY 10MB 31MB 11MB 37MB 18MB 61MB irreg nbf \nmoldyn Figure 8: Amortization of the overhead on the Power3 in number of outer loop iterations based \non the savings per iteration of the outer loop. Intel Pentium 4, 1.7GHz, 8KB L1 cache irreg nbf moldyn \nFigure 9: Amortization of the overhead on the Pen\u00adtium 4 in number of outer loop iterations based on \nthe savings per iteration of the outer loop. transformations, we observe mixed results on the Power3. \nOn the Pentium 4, using full sparse tiling in composition with the other data and iteration-reordering \ntransformations results in improved performance for all our benchmarks and data sets. The results for \nthe moldyn benchmark are espe\u00adcially impressive. The moldyn benchmark does more computation and ac\u00adcesses \nmore data than the other two benchmarks. In fact, for each molecule 72 bytes of data are stored. On the \nPen\u00adtium 4, the cache line is only 64 bytes long. Therefore, the data reordering transformations which \nimprove spatial locality have less e.ect than iteration-reordering transfor\u00admations like full sparse \ntiling. Full sparse tiling improves the performance in this benchmark to such an extent that it is actually \neasier to amortize the inspectors that include full sparse tiling (see Figure 9).  In this section, \nwe review the Kelly-Pugh iteration-reordering framework terminology using the simpli.ed moldyn code in \nFigure 1 as an example. Pugh and Wonnacott introduced the idea of using uninterpreted function symbols \nto stati\u00adcally describe the values in index arrays and other non-a.ne memory references [23]. Here we \nuse this idea to describe some of the data mappings and dependences in the simpli.ed moldyn example (Figure \n1), which involve indirect memory references. 3.1 Loops, Statements, and Data The traditional literature \non loop transformations repre\u00adsents each iteration within a loop nest as an integer tuple, pd=[p1 , ..., \npn], where pq is the value of the iteration variable for the qth loop. Thus, a loop s iteration space \nis a set of integer tuples with constraints indicating the loop bounds. {[p1 , ..., pn]|lb1 . p1 . ub1 \n.\u00b7 \u00b7\u00b7. lbn . pn . ubn} This representation is not very convenient for represent\u00ading transformations \nthat operate on a collection of loops that are not perfectly nested. For instance, there are three traditional \niteration spaces in the code shown in .gure 1, and it is awkward to express how the sparse tiling run-time \nreordering transformation operates across all three. Ahmed et al. [1] and Kelly-Pugh [16] give two di.erent \nmethods for building a uni.ed iteration space. In this paper, we use the Kelly-Pugh method. For the simpli.ed \nmoldyn example in Figure 1, they would use a four-dimensional space. Each loop corresponds to a pair \nof dimensions, where the .rst di\u00admension of the pair is the numerical order of the loop as a statement, \nand the second dimension is a value of the index variable. A program executes its iterations in lexicographic \norder of the uni.ed iteration space. For instance, using this representation, the [s, k]-th itera\u00adtion \nof S4 is denoted [s, 3, k, 1] since S4 is in the third state\u00adment (loop j) of the outer loop, and its \nthe .rst statement within the k loop. The uni.ed iteration space I0 for the (untransformed) program is \nthe following set: I0 = {[s, 1, i, 1] | (1 . s . num steps) . (1 . i . num nodes)} . {[s, 2, j, q] | \n(1 . s . num steps) . (1 . j . num inter) . (1 . q . 2)} . {[s, 3, k, 1] | (1 . s . num steps) . (1 . \nk . num nodes)} Next we will de.ne data mappings and dependences for this uni.ed iteration space. 3.2 \nData Mappings Each array has an associated data space represented with a integer tuple set with the same \ndimensionality as the array. The simpli.ed moldyn example contains 4 data spaces: x0 = {[m] | 1 . m . \nnum nodes} vx0 = {[m] | 1 . m . num nodes} fx0 = {[m] | 1 . m . num nodes} left0 = {[m] | 1 . m . num \ninter} right0 = {[m] | 1 . m . num inter} The subscripts 0 are used here since these are the data spaces \nfor the original, untransformed program. De.ne a data mapping MI a from iterations to sets of storage \nlocations in an array a, so that for each iteration pd= I, MI a(pd) is the set of locations that are \nreferenced by iteration tuple pd. Notice that the subscript I . a gives the domain and range of the mapping. \nThe moldyn example has the following data mappings: MI0 x0 = {[s, 1, i, 1] . [i]} U {[s, 2, j, q] . [left(j)]} \nU {[s, 2, j, q] . [right(j)]} U {[s, 3, k, 1] . [k]} MI0 fx0 = {[s, 1, i, 1] . [i]} U {[s, 2, j, 1] . \n[left(j)]} U {[s, 2, j, 2] . [right(j)]} U {[s, 3, k, 1] . [k]} MI0 vx0 = {[s, 1, i, 1] . [i]} U {[s, \n3, k, 1] . [k]} MI0 left0 = {[s, 2, j, q] . [j]} MI0 right0 = MI0 left0 Here we use uninterpreted function \nsymbols to abstractly represent the data mappings for which Figure 2 shows con\u00adcrete examples.  3.3 \nDependences De.ne the dependences DI I to be the set of directed edges between iterations dp = I that \nrepresent dependent computations. This set consists of the data dependence re\u00adlations between statement \nSV and SW denoted as dVW , with 1 . V, W . 4 in the simpli.ed moldyn example. For exam\u00adple, the dependences \nbetween statements S1 ([s, 1, i, 1]), S2 ([s, 2, j, 1]), and S3 ([s, 2, j, 2]) due to the x and fx arrays \nare speci.ed with the following dependence relation. ff d12 U d13 = {[s, 1, i, 1] . [s, 2, j, q] | (s \n. s) . (1 . q . 2) . (i = left(j) . i = right(j))} The dependences between statements S2 ([s, 2, j, \n1]), S3 ([s, 2, j, 2]), and S4 ([s, 3, k, 1]) due to the fx arrays are speci.ed with the following dependence \nrelations. ff d24 U d34 = {[s, 2, j, q] . [s, 3, k, 1] | (s . s) . (1 . q . 2) . (k = left(j) . k = \nright(j))} The arrows in Figure 5 represent concrete examples of these dependences. Notice that the \ndependences d12 U d13 are symmetric to the dependences d24 Ud34 since both sets of dependences have constraints \ninvolving the left and right index arrays. 4. RUN-TIME REORDERING TRANSFOR-MATIONS With run-time reordering \ntransformations, an inspector traverses mappings or data dependences transformed by reorderings produced \nby earlier inspectors. Thus, we want our frame\u00adwork to describe the data mappings and dependences in \nef\u00adfect at all stages of the transformation process. At compile\u00adtime the data and iteration reorderings \nare expressed with uninterpreted function symbols. At run-time the inspectors traverse and generate index \narrays to store the reordering functions. Formally, a data reordering transformation is expressed with \na mapping Ra a' , where the data that was originally stored in location m is relocated to Ra a' (m). \nWedonot need to consider the legality of a data mapping since data mappings do not a.ect data dependences \n-any one-to-one data remapping is legal. The result of remapping an array a is a new data mapping. MI \na' = {pd. Ra a' (m)|m = MI a(pd) . pd= I} An iteration-reordering transformation is expressed with a \nmapping TI I' that assigns each iteration pdin iteration space I to iteration TI I' (pd) in a new iteration \nspace If. The new execution order is given by the lexicographic order of the iterations in If. For iteration-reordering \ntransformations, the new execu\u00adtion order must respect all the dependences of the original. Thus for \neach {pd. qd}= DI I , TI I'(pd) must be lexico\u00adgraphically earlier than TI I' (dq ). Vdq : dq = DI I \n. TI I' (pd) . TI I' (qd) p,d p . d Lexicographical order on integer tuples can be de.ned as follows \n[15]: [p1 , ..., pn] . [q1 , ..., qn] . :m : (Vi :1 . i<m . pi = qi) . (pm <qm) The dependences of the \ntransformed iteration space are DI' I' = {TI I' (pd) . TI I' (dq ) | pd. dq = DI I } and the new data \nmapping MI' a for each array a is MI' a = {TI I' (pd) . MI a(pd) | pd= I} Given the new dependences \nand data mappings, we can plan further run-time transformations. Within this framework run-time reordering \ntransforma\u00adtions operate on subspaces within the uni.ed iteration space. For each mapping of statements \nto uni.ed iteration space a subspace can be speci.ed by selecting a subset of dimen\u00adsions in the mapping. \nA subspace is a candidate for run-time reordering transformations whenever the statements within the \nsubspace involve non-a.ne memory references. Data run-time reordering transformations are always legal \nsince they do not a.ect dependences. It is not possible to perform iteration reordering if dependences \nbetween itera\u00adtions in the subspace completely order the execution3 . Some run-time iteration-reordering \ntransformations (eg. lexico\u00adgraphical grouping, lexicographical ordering, bucket tiling) 3 Reduction \ndependences are the exception, because they al\u00adlow some reordering can only be applied when there are \nno dependences be\u00adtween iterations in the selected subspace. When the sub\u00adspace has dependences involving \nnon-a.ne memory refer\u00adences, run-time iteration-reordering transformations such as run-time partial parallelization \nand sparse tiling satisfy the constraints ordained by the dependences by inspecting the dependences. \nRun-time reordering transformations for partial parallelism traverse all the data dependences within \nan iteration sub\u00adspace and create a run-time parallel schedule with maximal parallelism [25]. Parallelism \nis expressed within our frame\u00adwork by mapping parallel iterations to the same point in the uni.ed iteration \nspace. Sparse tiling transformations partition a portion of the subspace and then grow tiles that respect \nthe data depen\u00addences throughout the rest of the subspace. Since a loop is typically the portion of the \nsubspace initially partitioned, sparse tiles are grown across dependences between loops or between iterations \nof an outer loop. By mapping all inde\u00adpendent tiles to the same tile number, parallelism between tiles \ncan be expressed.  5. COMPOSING TRANSFORMATIONS This section illustrate how to specify the e.ects of \napply\u00ading several run-time data and iteration-reordering transfor\u00admations for the simpli.ed moldyn example. \n5.1 Run-time Data Reordering CPACK M I0 to x0(left,right) // initialize alreadyOrdered bit vector // \nto all false count = 0 do j=1 to num inter mem loc1 = left[j] mem loc2 = right[j] if not alreadyOrdered(mem \nloc1) sigma cp inv[count] = mem loc1 alreadyOrdered(mem loc1) = true count = count + 1 endif if not alreadyOrdered(mem \nloc2) sigma cp inv[count] = mem loc2 alreadyOrdered(mem loc2) = true count = count + 1 endif enddo do \ni=1 to num nodes if not alreadyOrdered(i) sigma cp inv[count] = i count = count + 1 endif enddo return \nsigma cp inv Run-time data reordering inspectors traverse data map\u00adpings and generate a data reordering \nfunction. Figure 10 shows the CPACK inspector code specialized for the origi\u00adnal data mapping MI0 x0 \n(speci.ed in section 3.2) in the simpli.ed moldyn example. This specialized CPACK inspec\u00adtor is called \nby the composed inspector in .gure 11. The e.ect of CPACK can be speci.ed at compile-time by changing \nall the data mappings which involve the array being reordered. In the simpli.ed moldyn example, it makes \nsense to construct the same reordering for the x, fx, and vx arrays. Let Rx0 x1 = {m . m1 | m1 = Ocp(m)} \nspecify the run-time data reordering on the x array, where x0 is the data space for the x array in its \noriginal order, and x1 is the data space for the reordered array x1. The new data mapping is speci.ed \nas follows: MI0 x1 = {[s, 1, i, 1] . [Ocp(i)]} U{[s, 2, j, q] . [Ocp(left(j))]} U{[s, 2, j, q] . [Ocp(right(j))]} \nU{[s, 3, k, 1] . [Ocp(k)]} A data reordering Ogp based on Gpart, orders data within the same partition \nconsecutively. In the simple moldyn ex\u00adample, the abstract speci.cation of the iteration to data mappings \nafter applying Gpart is obtained by replacing Ocp with Ogp.  5.2 Run-time Iteration Reordering If an \niteration-reordering transformation on the j loop follows a data reordering transformation (as in our \nexperi\u00adments), then the inspector for the iteration-reordering trans\u00adformation will traverse the updated \ndata mappings. In the simpli.ed moldyn example, lexGroup will iterate over the data mappings that include \nthe Ocp function if lexGroup is Figure 10: First CPACK inspector for moldyn called from composed inspector \nin Figure 11. performed after CPACK. The iteration reordering of the i, j, and k loops is speci.ed as \nfollows: TI0 I1 = {[s, 1, i, 1] . [s, 1,i1 , 1] | i1 = Ocp(i)} U{[s, 2, j, q] . [s, 2,j1 ,q] | j1 = .lg(j)} \nU{[s, 3, k, q] . [s, 3,k1 , 1] | k1 = Ocp(k)} Since each iteration of the i and k loops directly maps \nto the x, fx, and vx arrays, the data reordering function generated for them, Ocp, can be used for reordering \nthe i and k loops as well. The transformation TI0 I1 is legal because the only loop-carried dependences \nwithin the i, j, or k loops are reduction dependences between iterations of the j loop. Due to the iteration \nreordering, the data mappings and dependences become: MI1 x1 = {[s, 1,Ocp(i), 1] . [Ocp(i)]} U{[s, 2,.lg(j),q] \n. [Ocp(left(j))]} U{[s, 2,.lg(j),q] . [Ocp(right(j))]} U{[s, 3,Ocp(k),q] . [Ocp(k)]} (df U df U df U \ndf 12 13 24 34 ) . DI1 I1 ff f d12 U d13 = {[s, 1,Ocp(i), 1] . [s, 2,.lg(j),q] | (s . s f) . (1 . q . \n2) . (i = left(j) . i = right(j))} df U dff 24 34 = {[s, 2,.lg(j),q] . [s, 3,Ocp(k), 1] | (s . s f) . \n(1 . q . 2) . (k = left(j) . k = right(j))}  5.3 Subsequent Transformations When composing run-time \nreordering transformations, spe\u00adcialized instances of the relevant inspectors can be created that account \nfor changes to the data mappings and de\u00adpendences incurred by any previously planned inspectors. The \nexplicit abstract description of how run-time reorder\u00ading transformations a.ect each other allows new \nrun-time reordering transformation compositions. For example, it is possible to generate another CPACK \ndata reordering and lexGroup iteration reordering after generating the .rst CPACK and lexGroup reorderings. \nFigure 12 shows how the second CPACK inspector is spe\u00adcialized to traverse the data mappings resulting \nfrom the .rst data and iteration-reordering functions, MI1 x1 . The array delta lg inv stores the inverse \nof the iteration re\u00adordering function .lg . The second CPACK inspector speci\u00ad.es a data reordering Rx1 \nx2 = {m1 . m2 |m2 = Ocp2 (m1 )}. A second iteration-reordering transformation for loop j traverses the \ndata mapping MI1 x2 and generates the re- CPACK M I1 to x1(left,right,sigma cp, sigma lg inv) // initialize \nalreadyOrdered bit vector // to all false count = 0 do j1=1 to num inter mem loc1 = sigma cp[left[delta \nlg inv[j1]]] mem loc2 = sigma cp[right[delta lg inv[j1]]] // same as CPACK M I0 to x0 except creating \n// sigma cp2 inv instead of sigma cp inv ... return sigma cp2 inv  Figure 12: Second CPACK inspector \nfor moldyn called from composed inspector in Figure 11. do s = 1 to num steps do i2 = 1 to num nodes \nx2[i2] = x2[i2] + vx2[i2] + fx2[i2] enddo do j2 = 1 to num inter fx2[left2[j2]] += g(x2[left2[j2]], x2[right2[j2]]) \nfx2[right2[j2]] += g(x2[left2[j2]], x2[right2[j2]]) enddo do k2 = 1 to num nodes vx2[k2] += fx2[k2] enddo \nenddo Figure 13: Executor for simple moldyn example when inspector applies CPACK, lexGroup, CPACK, lex-Group \ncomposition. ordering function .lg2 to implement the transformation TI1 I2 . MI1 x2 = {[s, 1,Ocp(i), \n1] . [Ocp2 (Ocp(i))]} U{[s, 2,.lg(j),q] . [Ocp2 (Ocp(left(j)))]} U{[s, 2,.lg(j),q] . [Ocp2 (Ocp(right(j)))]} \nU{[s, 3,Ocp(k),q] . [Ocp2 (Ocp(k))]} TI1 I2 = {[s, 1,i1 , 1] . [s, 1,i2 , 1] | i2 = Ocp(i1 )} U{[s, 2,j1 \n,q] . [s, 2,j2 ,q] | j2 = .lg2 (j1 )} U{[s, 3,k1 ,q] . [s, 3,k2 , 1] | k2 = Ocp2 (k1 )} With a compile-time \ndescription of the e.ects of a run\u00adtime data or iteration reordering, it is possible to plan com\u00adpositions \nof run-time transformations and generate code for the composed data remappings at the end of all inspection. \nThis is done by manipulating reordering function arrays (sigma cp, delta lg, etc.) at run-time. The composed \nin\u00adspector in .gure 11 remaps and updates the data and index arrays accordingly after all data and iteration \nreorderings have been computed.  do s = 1 to num steps do t=1 to num tiles do i4 in sched(t,1) x3[i4] \n= x3[i4] + vx3[i4] + fx3[i4] enddo do j4 in sched(t,2) fx3[left3[j4]] += g(x3[left3[j4]], x3[right3[j4]]) \nfx3[right3[j4]] += g(x3[left3[j4]], x3[right3[j4]]) enddo do k4 in sched(t,2) vx3[k4] += fx3[k4] enddo \nenddo enddo Figure 14: Sparse tiled executor when the composed inspector performs CPACK, lexGroup, CPACK, \nlexGroup, full sparse tiling, and tilePack.  5.4 Sparse Tiling As iteration-reordering transformations, \nsparse tiling trans\u00adformations can also be composed with other run-time re\u00adordering transformations. \nThe main di.erence between sparse tiling transformations and other run-time reordering trans\u00adformations \nfor locality is that sparse tiling is applicable within subspaces of the uni.ed iteration space that \nhave data de\u00adpendences. This is possible because sparse tiling inspectors traverse the data dependences. \nIn the simpli.ed moldyn example, applying sparse tiling after the CPACK, lexGroup, CPACK, lexGroup series \nof run-time transformations described in Section 5.3 can be speci.ed with the following transformation \nmapping. TI2 I3 = {[s, 1,i2 , 1] . [s, e(1,i3 ), 1,i3 , 1] | i3 = i2 } U{[s, 2,j2 ,q] . [s, e(2,j3 ), \n2,j3 ,q] | j3 = j2 } U{[s, 3,k2 , 1] . [s, e(3,k3 ), 3,k3 , 1] |k3 = k2 } The tiling function e assigns \na subspace of the uni.ed itera\u00adtion space to tile numbers. The subspace being sparse tiled in this example \nis {[1,i2 ] U [2,j2 ] U [3,k2 ]}, with the seed partitioning occurring on the [2,j2 ] portion of the \nsubspace. Figure 5 illustrates an instance of sparse tiled moldyn that uses full sparse tiling for tile \ngrowth. In Figure 5, the computation exhibits better spatial lo\u00adcality if the data arrays are remapped \nafter sparse tiling. Speci.cally if the data item (and corresponding iteration) numbered as 6 is put \nbefore 3, and 2 before 5, there is bet\u00adter locality. We refer to reordering the data and iterations in \nloops i and k based on the tiling function as tile pack\u00ading (tilePack). TilePack uses an inspector that \ntraverses the tiling function to generate a data reordering and iteration\u00adreordering transformation. \n Rx2 x3 = {[m2 ] . [m3 ] | m3 = Otp(m2 )} TI3 I4 = {[s, t, 1,i3 , 1] . [s, t, 1,i4 , 1] | i4 = Otp(i3 \n)} U{[s, t, 2,j3 ,q] . [s, t, 2,j4 ,q] | j4 = j3 } U{[s, t, 3,k3 , 1] . [s, t, 3,k4 , 1] | k4 = Otp(k3 \n)} Figure 14 shows the executor for the simple moldyn exam\u00adple when the iteration-reordering composition \nTI0 and I4 the data reordering compositions Rx0 x3 are generated by composing the transformation mappings \ndiscussed in this section and previous sections. Since the transformed code must traverse the .nal iteration \nspace in lexicographical or\u00adder, a schedule (indexed by the tile and all but the last dimension in the \nsubspace being sparse tiled) is created to Power3 Pentium 4 indicate the subset of iterations within \neach tile. 15 sched(t, 1) = {[i4 ] | i4 = Otp(i3 ) . e(1,i3 )= t} sched(t, 2) = {[j4 ] | j4 = j3 . e(2,j3 \n)= t} sched(t, 3) = {[k4 ] | k4 = Otp(k3 ) . e(3,k3 )= t}  % Overhead Reduction 10 10 5 5 0 0 irreg \nmoldyn irreg moldyn Figure 16: Percent reduction in inspector overhead for compositions with two or \nmore data reorderings and data is only remapped once.  6. REDUCING THE OVERHEAD The overhead of executing \nany inspector must be amor\u00adtized to make run-time reordering transformations bene.\u00adcial. In our experiments, \nwe take advantage of the frame\u00adwork in two ways to generate e.cient inspectors. First, in the benchmarks \nthere are cases where two sets of data de\u00adpendences satisfy the same constraints. Therefore, the full \nsparse tiling inspector need only traverse one set of data dependences while generating a legal tile \nfunction. Second, our experimental results indicate that remapping the data arrays after all run-time \nreordering functions have been gen\u00aderated reduces the execution time of inspectors that perform more \nthan one data reordering. Whenever two sets of data dependences satisfy the same constraints, it is only \nnecessary to traverse one set at run\u00adits position in a composed inspector should result in less overhead \nthan an inspector implemented in a run-time li\u00adbrary, since the latter must be generally applicable. \nThe need for specialized inspectors has been described in work for data locality [20] and parallelism \n[11].  Automatically generating code for the inspector and ex\u00adecutor can leverage the work in [7], which \ndescribes compiler support for dynamic data packing, and the work in [16], which generates optimized \ncode for compile-time transfor\u00admations. Speci.cally, the techniques described in [16] can be used to \ngenerate the transformed executor code and the inspector code that traverses the data mappings and depen\u00addences. \nAutomatically generating specialized and optimized versions of the various data and iteration-reordering \nalgo\u00adrithms such as CPACK, lexGroup, and full sparse tiling will be more challenging. To completely automate \nthe usage of composed run-time reordering transformations, more performance modeling work is needed to \nselect between various compositions and the time. In the simpli.ed moldyn example, the data depen\u00addences \nbetween statement S1 and the statements in the j loop, S2 and S3, are symmetric to the dependences between \nthe statements in the j loop and statement S4. Therefore, our full sparse tiling inspector need only \ntraverse one set of these dependences to grow the tiles from a seed partitioning of the j loop to the \ni and k loops. A similar situation occurs in all the benchmarks we use for experiments. The framework \nallows the compiler to choose when to remap a data array. Figure 11 illustrates a composed in\u00adspector \nthat performs data remapping and index array ad\u00adjustments after all reordering functions are generated, \nand Figure 15 shows an inspector performing the same composi\u00adtion of transformations, but remapping and \nadjusting after each reordering function is generated. Notice that many of the functions, like CPACK \nM I1 to x1 B, take fewer parame\u00adters in Figure 15 than in Figure 11. Since the index ar\u00adrays left and \nright are remapped and adjusted after every transformation in Figure 15, the index arrays which main\u00adtain \nthe reordering functions are not needed. This results in fewer indirect memory references in the composed \ninspector and can have an a.ect on its performance. Our experience suggests that remapping and adjusting \nthe index arrays after each transformation and remapping the data arrays after all data reordering transformations \nleads to the most e.cient inspectors. Figure 16 shows the percentage execution time reduction when the \ndata arrays are remapped after all transformations. The results are shown only for the irreg and moldyn \nbenchmarks because nbf does not bene.t from the tilePack data reordering transformation. Therefore, most \nof the compositions involving nbf do not use two or more data reordering transformations. Optimal generation \nof composed inspectors is an open question. Our framework allows the expression of diverse possibilities. \n 7. FUTURE WORK An obvious extension of our work is the automatic gen\u00aderation of specialized run-time \ninspectors. Specializing an inspector for a reordering transformation in the context of parameters for \ncompositions. Figure 17 shows how the per\u00adformance of the executor di.ers as the Gpart and full sparse \ntiling parameters are selected to target di.erent cache sizes. The performance varies depending on the \nbenchmark and dataset. The machine is also a factor. Since characteristics of the dataset are not available \nuntil runtime, the selection and order of run-time reordering transformations depend on information available \nat runtime as well as compile time. In the domain of data and iteration reordering, [22, 33] pro\u00adpose \nmethods for guidance when some information such as the data access pattern is not available until runtime. \n    8. RELATED WORK Run-time reordering transformations di.er from dynamic compilation techniques \nsuch as those described in [10], be\u00adcause reordering transformations do not change the code at runtime. \nInstead the code has already been transformed and inspectors create data and iteration-reordering func\u00adtions, \nwhich are stored in index arrays. Researchers have developed run-time data dependence analysis to handle \nnon-a.ne memory references [23, 26]. In [23] constraints for disproving dependences are evaluated at \nrun-time. Rus et al. [26] take this further adding the ability to traverse all data dependences at run-time \nif neces\u00adsary. They perform a hybrid (static and dynamic) data de\u00adpendence analysis inter-procedurally. \nAs we have described in this paper, traversing data dependences at run-time is necessary for some run-time \nreordering transformations. Many run-time data reordering transformations [4, 2, 21, 7, 12] .t within \nour framework. Space .lling curves and reg\u00adister tiling for sparse matrix vector multiply are two types \nof data reordering transformations that are more specialized. Data reorderings generated from space-.lling \ncurves [28, 20] traverse data mappings and mappings of data to spatial co\u00adordinates. The programmer must \nspecify how data maps to spatial coordinates, therefore, such data reorderings can not be fully automated. \nIm and Yelick [13] have developed the SPARSITY code generator that improves the locality for the dx and \ndb vectors in the sparse matrix-vector multi\u00ad d plication Adx = b. The dynamic register blocking techniques \nare useful for many application domains that use sparse ma\u00adtrices, but the work focuses on a single algorithm. \nThere has been a de.nite progression toward complete automation of run-time reordering transformations. \nIni\u00adtially such transformations were incorporated into appli\u00adcations manually for parallelism [5]. Next, \nlibraries with run-time transformation primitives were developed so that a programmer or compiler could \ninsert calls to such prim\u00aditives [6]. Currently, there are many run-time reordering transformations for \nwhich a compiler can automatically an\u00adalyze and generate the inspectors [25, 7, 21, 12]. However, each \ntransformation or composition of transformations are treated separately. Our framework provides a uniform \nrep\u00adresentation for these transformations and describes how to compose any number of them at compile-time. \n9. CONCLUSIONS This paper motivates compositions of run-time data and iteration reordering transformations \nwith experimental re\u00adsults showing signi.cant performance improvements for the moldyn, nbf, and irreg \nbenchmarks. We show how to use an existing compile-time framework to formally express the changes in \ndependences and data mappings which occur when a composition of data and iteration reorderings are performed. \nRepresenting the abstract e.ect of run-time data and iteration-reordering transformations at compile-time \nis an important step toward the automatic generation of spe\u00adcialized inspectors. By showing that sparse \ntiling can be represented in our framework, we demonstrate its general applicability to other irregular \ncodes; until now, it has been used only on Gauss-Seidel. We also use two di.erent opti\u00admizations to improve \nthe performance of our composed in\u00adspectors, thus reducing the overhead of composed run-time reordering \ntransformations. 10. ACKNOWLEDGMENTS This work was supported by an AT&#38;T Labs Graduate Research Fellowship, \na Lawrence Livermore National Labs LLNL grant, and in part by NSF Grant CCR-9808946. Equip\u00adment used \nin this research was supported in part by the Na\u00adtional Partnership for Computational Infrastructure \n(NPACI). We used Rational PurifyPlus as part of the SEED program. We would like to thank Hwansoo Han \nfor making the benchmarks and run-time inspector code available. We would also like to thank the students \nof CSE238 at UCSD and anonymous reviewers for comments and suggestions. 11. REFERENCES [1] N. Ahmed, \nN. Mateev, and K. Pingali. Synthesizing transformations for locality enhancement of imperfectly-nested \nloop nests. In Proceedings of the 2000 International Conference on Supercomputing, pages 141 152, May \n2000. [2] I. Al-Furaih and S. Ranka. Memory hierarchy management for iterative graph structures. In \n Proceedings of the 1st Merged International Parallel Processing Symposium and Symposium on Parallel \nand Distributed Processing, pages 298 302, Los Alamitos, March 30 April 3 1998. [3] S. Carr, K. S. McKinley, \nand C. Tseng. Compiler optimizations for improving data locality. In Proceedings of the Sixth International \nConference on Architectural Support for Programming Languages and Operating Systems, pages 252 262, November \n1994. [4] E. Cuthill and J. McKee. Reducing the bandwidth of sparse symmetric matrices. In Proceedings \nof the 24th National Conference ACM, pages 157 172, 1969. [5] R. Das, D. Mavriplis, J. Saltz, S. Gupta, \nand R. Ponnusamy. The design and implementation of a parallel unstructured Euler solver using software \nprimitives, AIAA-92-0562. In Proceedings of the 30th Aerospace Sciences Meeting, 1992. [6] R. Das, M. \nUysal, J. Saltz, and Yuan-Shin S. Hwang. Communication optimizations for irregular scienti.c computations \non distributed memory architectures. Journal of Parallel and Distributed Computing, 22(3):462 478, 1994. \n [7] C. Ding and K. Kennedy. Improving cache performance in dynamic applications through data and computation \nreorganization at run time. In Proceedings of the ACM SIGPLAN 1999 Conference on Programming Language \nDesign and Implementation, ACM SIGPLAN Notices, pages 229 241, May 1 4, 1999. [8] C. Ding and K. Kennedy. \nInter-array data regrouping. In Twelfth International Workshop on Languages and Compilers for Parallel \nComputing. Springer-Verlag, August 1999. [9] C. C. Douglas, J. Hu, M. Kowarschik, U. R\u00a8ude, and C. Weiss. \nCache Optimization for Structured and Unstructured Grid Multigrid. Electronic Transaction on Numerical \nAnalysis, 10:21 40, February 2000. [10] B. Grant, M. Mock, M. Philipose, C. Chambers, and S. J. Eggers. \nDyC: an expressive annotation-directed dynamic compiler for C. Theoretical Computer Science, 248(1 2):147 \n199, 2000. [11] E. Guti\u00b4errez, R. Asenjo, O. Plata, and E. L. Zapata. Automatic parallelization of irregular \napplications. Parallel Computing, 26(13 14):1709 1738, 2000. [12] H. Han and C. Tseng. A comparison of \nlocality transformations for irregular codes. In 5th International Workshop on Languages, Compilers, \nand Run-time Systems for Scalable Computers. Springer, 2000. [13] E. Im and K. Yelick. Optimizing sparse \nmatrix computations for register reuse in sparsity. In V.N.Alexandrov, J.J. Dongarra, B.A.Juliano, R.S.Renner, \nand C.J.K.Tan, editors, Computational Science -ICCS 2001, Lecture Notes in Computer Science, pages 127 \n136. Springer, May 28-30, 2001. [14] M. Kandemir, A. Choudhary, J. Ramanujam, and P. Banerjee. Improving \nlocality using loop and data transformations in an integrated framework. In Proceedings of the 31st Annual \nACM/IEEE International Symposium on Microarchitecture, pages 285 296, November 1998. [15] W. Kelly and \nW. Pugh. Finding legal reordering transformations using mappings. In K. Pingali, U. Banerjee, D. Gelernter, \nA. Nicolau, and D. Padua, editors, Proceedings of the 7th International Workshop on Languages and Compilers \nfor Parallel Computing, volume 892 of Lecture Notes in Computer Science, pages 107 124. Springer, August \n8 10, 1994. [16] W. Kelly and W. Pugh. A unifying framework for iteration reordering transformations. \nTechnical Report CS-TR-3430, University of Maryland, College Park, February 1995. [17] Induprakas Kodukula \nand Keshav Pingali. Transformations for imperfectly nested loops. In Proceedings of 1996 Conference on \nSupercomputing. ACM Press, 1996. [18] W. Li and K. Pingali. A singular loop transformation framework \nbased on non-singular matrices. International Journal on Parallel Processing, 22(2), April 1994. [19] \nLee-Chung Lu. A uni.ed framework for systematic loop transformations. In Proceedings of the 3rd Annual \nACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, pages 28 38, 1991. [20] J. \nMellor-Crummey, D. Whalley, and K. Kennedy. Improving memory hierarchy performance for irregular applications. \nIn Proceedings of the 1999 International Conference on Supercomputing, ACM SIGARCH, pages 425 433, June \n20 25 1999. [21] N. Mitchell, L. Carter, and J. Ferrante. Localizing non-a.ne array references. In Proceedings \nof the 1999 International Conference on Parallel Architectures and Compilation Techniques, pages 192 \n202, October 12 16, 1999. [22] N. Mitchell, L. Carter, and J. Ferrante. A modal model of memory. In \nV.N.Alexandrov, J.J. Dongarra, B.A.Juliano, R.S.Renner, and C.J.K.Tan, editors, Computational Science \n-ICCS 2001, Lecture Notes in Computer Science. Springer, May 28-30, 2001. [23] B. Pugh and D. Wonnacott. \nNonlinear array dependence analysis. Technical Report CS-TR-3372, Dept. of Computer Science, Univ. of \nMaryland, nov 1994. [24] W. Pugh and E. Rosser. Iteration space slicing for locality. In Proceedings \nof the 12th Workshop on Languages and Compilers for Parallel Computing (LCPC), August 1999. [25] L. Rauchwerger, \nN. M. Amato, and D. A. Padua. Run-time methods for parallelizing partially parallel loops. In Proceedings \nof the 1995 ACM International Conference on Supercomputing, 1995. [26] S. Rus, L. Rauchwerger, and J. \nHoe.inger. Hybrid analysis: Static &#38; dynamic memory reference analysis. In Proceedings of the 16th \nAnnual ACM International Conference on Supercomputing, June 2002. [27] V. Sarkar and R. Thekkath. A general \nframework for iteration-reordering loop transformations. In Proceedings of the ACM SIGPLAN 92 Conference \non Programming Language Design and Implementation, ACM SIGPLAN Notices, pages 175 187, June 1992. [28] \nJ. P. Singh, C. Holt, T. Totsuka, A. Gupta, and J. Hennessy. Load balancing and data locality in adaptive \nhierarchical N-body methods: Barnes-Hut, fast multipole, and radiosity. Journal of Parallel and Distributed \nComputing, 27(2):118 141, June 1995. [29] M. M. Strout, L. Carter, and J. Ferrante. Rescheduling for \nlocality in sparse matrix computations. In V.N.Alexandrov, J.J. Dongarra, B.A.Juliano, R.S.Renner, and \nC.J.K.Tan, editors, Computational Science -ICCS 2001, Lecture Notes in Computer Science. Springer, May \n28-30, 2001. [30] M. M. Strout, L. Carter, J. Ferrante, J. Freeman, and B. Kreaseck. Combining performance \naspects of irregular gauss-seidel via sparse tiling. In Proceedings of the 15th Workshop on Languages \nand Compilers for Parallel Computing (LCPC), July 2002. [31] W. Thies, F. Vivien, J. Sheldon, and S. \nAmarasinghe. A uni.ed framework for schedule and storage optimization. In Proceedings of the ACM SIGPLAN \n01 Conference on Programming Language Design and Implementation, ACM SIGPLAN Notices, pages 232 242, \nJune 2001. [32] M. E. Wolf, D. E. Maydan, and D. Chen. Combining loop transformations considering caches \nand scheduling. In Proceedings of the 29th Annual ACM/IEEE International Symposium on Microarchitecture, \npages 274 286, December 2 4, 1996. [33] H. Yu, F. Dang, and L. Rauchwerger. Parallel reductions: An application \nof adaptive algorithm selection. In Proceedings of the 15th Workshop on Languages and Compilers for Parallel \nComputing (LCPC), July 2002.  \n\t\t\t", "proc_id": "781131", "abstract": "Many important applications, such as those using sparse data structures, have memory reference patterns that are unknown at compile-time. Prior work has developed run-time reorderings of data and computation that enhance locality in such applications.This paper presents a compile-time framework that allows the explicit composition of run-time data and iteration-reordering transformations. Our framework builds on the iteration-reordering framework of Kelly and Pugh to represent the effects of a given composition. To motivate our extension, we show that new compositions of run-time reordering transformations can result in better performance on three benchmarks.We show how to express a number of run-time data and iteration-reordering transformations that focus on improving data locality. We also describe the space of possible run-time reordering transformations and how existing transformations fit within it. Since sparse tiling techniques are included in our framework, they become more generally applicable, both to a larger class of applications, and in their composition with other reordering transformations. Finally, within the presented framework data need be remapped <i>only once</i> at runtime for a given composition thus exhibiting one example of overhead reductions the framework can express.", "authors": [{"name": "Michelle Mills Strout", "author_profile_id": "81100191330", "affiliation": "UC, San Diego, La Jolla, CA", "person_id": "PP39031703", "email_address": "", "orcid_id": ""}, {"name": "Larry Carter", "author_profile_id": "81392591424", "affiliation": "UC, San Diego, La Jolla, CA", "person_id": "P168418", "email_address": "", "orcid_id": ""}, {"name": "Jeanne Ferrante", "author_profile_id": "81100357275", "affiliation": "UC, San Diego, La Jolla, CA", "person_id": "P137070", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/781131.781142", "year": "2003", "article_id": "781142", "conference": "PLDI", "title": "Compile-time composition of run-time data and iteration reorderings", "url": "http://dl.acm.org/citation.cfm?id=781142"}