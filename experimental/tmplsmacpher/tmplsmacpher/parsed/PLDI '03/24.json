{"article_publication_date": "05-09-2003", "fulltext": "\n Region-based Hierarchical Operation Partitioning for Multicluster Processors Michael Chu Kevin Fan \nScott Mahlke Advanced Computer Architecture Laboratory University of Michigan Ann Arbor, MI 48109 {mchu, \nfank, mahlke}@umich.edu  ABSTRACT Clustered architectures are a solution to the bottleneck of centralized \nregister .les in superscalar and VLIW proces\u00adsors. The main challenge associated with clustered architec\u00adtures \nis compiler support to e.ectively partition operations across the available resources on each cluster. \nIn this work, we present a novel technique for clustering operations based on graph partitioning methods. \nOur approach incorporates new methods of assigning weights to nodes and edges within the data.ow graph \nto guide the partitioner. Nodes are as\u00adsigned weights to re.ect their resource usage within a clus\u00adter, \nwhile a slack distribution method intelligently assigns weights to edges to re.ect the cost of inserting \nmoves across clusters. A multilevel graph partitioning algorithm, which globally divides a data.ow graph \ninto multiple parts in a hi\u00aderarchical manner, uses these weights to e.ciently generate estimates for \nthe quality of partitions. We found that our algorithm was able to achieve an average of 20% improve\u00adment \nin DSP kernels and 5% improvement in SPECint2000 for a four-cluster architecture. Categories and Subject \nDescriptors D.3.4 [Programming Languages]: Processors code gen\u00aderation, retargetable compilers; C.1.1 \n[Processor Archi\u00adtectures]: Single Data Stream Architectures RISC/CISC, VLIW architectures General Terms \nAlgorithms, Experimentation, Performance  Keywords clustering, instruction-level parallelism, instruction \nschedul\u00ading, multicluster processor, operation partitioning, region\u00adbased compilation Permission to make \ndigital or hard copies of all or part of this work for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for pro.t or commercial advantage and that copies bear \nthis notice and the full citation on the .rst page. To copy otherwise, to republish, to post on servers \nor to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI 03, June 9 11, 2003, \nSan Diego, California, USA. Copyright 2003 ACM 1-58113-662-5/03/0006 ...$5.00. 1. INTRODUCTION Superscalar \nand VLIW processors achieve high perfor\u00admance by exploiting instruction-level parallelism (ILP) to issue \nmultiple operations each cycle. As the number of op\u00aderations issued each cycle grows, the demands to \nsupply operands to these operations increases in turn. In a con\u00adventional processor, a centralized register \n.le is responsible for operand supply. Supplying a larger and larger number of operands each cycle from \na centralized register .le can quickly become the bottleneck in a processor design. The bottleneck results \ndue to the combined e.ects of: register .le cost and access time growing with the square of the number \nof register ports; a larger number of registers being necessary as issue width increases to maintain \nmore temporary values; register bypass logic growing quadratically with the number of operations issued \nper cycle; and the distance separating function units (FUs) from the register .le increasing with a larger \nnumber of FUs [8] [7]. A natural solution to these problems is to remove the cen\u00adtralized register .le \nand create a decentralized architecture with several smaller register .les. Each of the smaller reg\u00adister \n.les supplies operands to a subset of the FUs. These smaller register .les can be e.ciently designed, \nthereby al\u00adleviating the register .le bottleneck while maintaining the desired level of ILP. This strategy \nis generally referred to as a clustered architecture or a multicluster processor [10]. One of the .rst \nclustered architectures was the Multi.ow Trace. Clustered architectures are becoming increasingly popular \nin many recent processor designs including the Al\u00adpha 21264, TI C6x series, and Analog Tigersharc. Each \nof these processors is a two-cluster design. The central challenge with clustered architectures is com\u00adpilation \nsupport. The compiler must e.ectively partition operations across the resources available on each cluster \nto maximize ILP. However, this goal must be achieved while carefully considering the implications of \ninter-cluster com\u00admunication. Communication of values between clusters is both slow and bandwidth-limited. \nThus, operations must be partitioned to ensure that ILP is not constrained by frequent inter-cluster \ncommunication. A common rule of thumb is that breaking a processor into two identical clus\u00adters reduces \nprogram performance by around 20%. Further\u00admore, a four cluster processor loses around 30% performance \nover the equivalent single cluster processor [7]. Generally, these numbers get worse when the clusters \nare not identical. While clustering makes sense from an architectural perspec\u00adtive, a large amount of \nperformance is left on the table with this choice. Examining common operation partitioning algorithms \nin more depth reveals two recurring problems. First, partition\u00ading algorithms are modeled closely after \noperation schedul\u00ading. They make local, greedy decisions to optimize the place\u00adment of an operation based \non the placement of its neigh\u00adbors. This strategy makes sense as clustering and schedul\u00ading are heavily \nintertwined. However, locally optimal deci\u00adsions may actually be poor decisions when the global picture \nis considered. The second problem is that clustering algo\u00adrithms are notoriously slow due to the detailed \nmodeling of processor resource constraints. Resource models similar to (or often identical to) those \nused during operation schedul\u00ading are repeatedly evaluated for each candidate operation placement. The \n.nal code schedule is indeed very sensi\u00adtive to the partition chosen, so this seems like the proper strategy. \nHowever, detailed modeling of a particular place\u00adment can be counterproductive when it limits the number \nof choices that can be considered. Furthermore, as proces\u00adsors have more resources and their resource \nusage patterns become more complex, detailed modeling of each placement choice may become infeasible \nfor production compilers. We use an approach opposite to this scheduler-centric methodology. Operation \npartitioning is performed at a glo\u00adbal scope with the view of all operations in a region (a group of \nclosely related basic blocks is referred to as a region [11]). We adapt two powerful techniques that \nare commonly used in VLSI design: multilevel graph partitioning and slack dis\u00adtribution. Multilevel graph \npartitioning divides the data.ow graph into multiple parts in a hierarchical manner. Op\u00aderations are \niteratively partitioned from a coarse level of groups of related operations down to a .ne-grained level \nof individual operations. Slack distribution identi.es available scheduling slack within a region and \nallocates it to speci.c data.ow edges. In this manner, the cost of cutting speci.c data.ow edges to create \na partition is determined. Graph partitioning also requires a processor resource mod\u00adel to determine \nthe quality of a partition. Again, we take a non-scheduler-centric approach. We employ a simple es\u00adtimation \nstrategy that is similar to the RESMII (resource minimum initiation interval) calculation used with modulo \nscheduling [24]. However, we focus on scalar scheduling as opposed to software pipelining. Resource usage \nestimates are computed prior to partitioning and used to estimate the resource load for each candidate \npartition. While this method su.ers inaccuracies, it is both more e.cient and accurate enough to provide \na suitable guide to the opera\u00adtion partitioning algorithm. Our proposed approach is re\u00adferred to as region-based \nhierarchical operation partitioning, or RHOP. The remainder of this paper is organized into .ve sec\u00adtions. \nSection 2 provides an overview of architectural model and the basics of operation partitioning. Section \n3 presents RHOP algorithm itself. A preliminary experimental evalu\u00adation is given in Section 4. We compare \nand contrast our work with previous work in Section 5. Conclusions and fu\u00adture work are discussed in \nthe last section. 2. OVERVIEW OF CLUSTERING This section introduces the clustered architectural model \nthat is assumed for this paper and the basic process of partitioning a data.ow graph (DFG) for this architecture. \nInterconnection Network Figure 1: A heterogeneous two-cluster machine. Next, we present a high-level \nclassi.cation of the common approaches for clustering and break them down by four cat\u00adegories: phase \nordering, scope, desirability metric, and op\u00aderation grouping. Last, we conclude with a discussion of \nthe limitations of scheduler-centric approaches to motivate the work in this paper. 2.1 Basics The architectural \nmodel assumed in this paper is as in Figure 1. Each cluster consists of a tightly connected set of register \n.les (RFs) and function units (FUs). FUs in a cluster may only address those registers within the same \ncluster. Transfers of values between clusters are accom\u00adplished through explicit move operations that \ngo through an interconnection network. The interconnection network is assumed to have a uniform connection \nto all clusters with a .xed bandwidth. Though this assumption is not necessary, it simpli.es the compiler \nalgorithms. Clusters in a machine may be homogeneous, each containing the same types and numbers of RFs \nand FUs, or heterogeneous, each having a unique mix of resources. The machine in Figure 1 is het\u00aderogeneous \nand has three FUs and one RF in cluster 1, and two FUs and a RF in cluster 2. The goal of clustering \nis to obtain a balanced workload that takes advantage of parallelism available within the ma\u00adchine. The \nnotion of balance on a cluster relates to the resources available on that cluster and the operations \nsched\u00aduled on it. For example, given a machine with two hetero\u00adgeneous clusters such that cluster 1 has \ntwice as many FUs as cluster 2, a balanced workload would tend to have twice as many operations scheduled \non cluster 1 as on cluster 2. Data is transferred from cluster to cluster via explicit inter-cluster \nmove operations. Inter-cluster moves have a non-zero latency (1 cycle is assumed in this paper) and thus \ncan lengthen the schedule. However, if the latency of the move can be overlapped with the execution of \nother oper\u00adations, then the inter-cluster moves will not a.ect perfor\u00admance by much. A good partitioning \nof operations mini\u00admizes overall schedule length by simultaneously maximizing the number of operations \nexecuted in parallel while minimiz\u00ading the number of moves that negatively a.ect performance. Figure \n2 shows two possible clusterings of an example DFG. For simplicity, the machine that will be used for \nthe example is a homogeneous two-cluster machine, with one RF and one FU per cluster. Each FU is capable \nof executing any operation. The latencies of all operations are assumed cluster 1 cluster 2 (a) (b) \nFigure 2: Example data.ow graph with (a) locally greedyand (b) region-aware cluster assignment. to be \none cycle. Furthermore, the interconnection network is capable of sustaining one inter-cluster move per \ncycle. For the example in Figure 2, the clustering algorithm must partition the operations into two sets \nwith each set executing on a particular cluster. Any time an edge is broken, an inter\u00adcluster move is \nrequired to copy the data from the producing cluster to the consuming cluster. The critical path through \nthis example graph is 5 cycles going through operations 1, 2, 6, 10, and 12. Cutting edges along the \ncritical path increases the critical path length; thus, most clustering algorithms avoid such cuts. \n2.2 Approaches to Clustering A large number of algorithms to perform clustering have been proposed by \nthe research community. To better un\u00adderstand the operation and relative strengths of these al\u00adgorithms, \nit is useful to understand the major characteris\u00adtics that di.erentiate them. We have identi.ed four \npri\u00admary characteristics of clustering algorithms: phase order\u00ading, scope, desirability metric, and grouping. \nPhase Ordering. In the compilation process, cluster as\u00adsignment can take place as a separate phase before \nschedul\u00ading, where the scheduler is constrained by the decisions made in the clustering phase. This isolates \nthe clustering prob\u00adlem from the scheduling problem. Alternatively, clustering can be integrated with \nthe scheduling phase; in this case, more information is available for making decisions, but the complexity \nof the problem increases, limiting the number of options that can be considered during the process. As \na compromise, clustering and scheduling can be done iter\u00adatively, such that decisions made by one phase \nare used to guide the decisions made in the other, until a suitable result is obtained. Scope. The scope \nof the clustering algorithm can be lo\u00adcal, region-based, or global. A local algorithm generally ex\u00adamines \none operation at a time and decides which cluster it should be assigned to based on its immediate neighbors. \nAs with scheduling, operation priorities may guide the order in which operations are considered. A region-based \nalgorithm, on the other hand, considers all of the operations within a region such as a basic block or \nset of basic blocks at once. Finally, a global algorithm uses knowledge of the entire pro\u00adgram or function \nto make intelligent decisions. The com\u00adplexity of the algorithm increases with the scope, but better \ndecisions can be made with higher level knowledge. DesirabilityMetric. The cluster assignment algorithm \ncan use one of several ways to measure the quality of a can\u00addidate partition. It can perform an actual \nscheduling of the code, which gives the most accurate measure since the per\u00adformance is then known. It \ncan generate a pseudo-schedule using an approximate machine model to provide a reason\u00adable estimate of \nthe actual schedule. It can use quantitative resource usage estimates to project the load a set of opera\u00adtions \nplaces on a cluster. Finally, it can use a simple count of how many operations are on each cluster and \nhow many moves are required to get an idea of the desirability of a partition. Grouping. Another property \nof clustering algorithms is whether they employ a hierarchical or .at partitioning scheme. In the case \nof region-based or global clustering, a hierarchical approach means that decisions are made on multiple \nlevels, with information available in a .ner-grained view of the operation graph being used to re.ne \nprevious decisions made from a coarser view of the graph. 2.3 Pitfalls of Scheduler-Centric Approaches \nScheduler-centric approaches to clustering employ a nat\u00adural extension of the scheduling process to perform \ncluster assignment. This does not mean clustering is done during scheduling. In fact, any phase ordering \nis possible. The two distinguishing characteristics of scheduler-centric ap\u00adproaches are local scope \nand .at grouping. These are the primary characteristics of operation scheduling where opera\u00adtions are \ngreedily placed into the schedule one by one consid\u00adering the placement of those operations with higher \npriority that have already been scheduled. The desirability metric for scheduler-centric approaches is \nmost often through the use of an actual schedule. But again, this is not a require\u00adment. The most well \nknown scheduler-centric clustering algo\u00adrithm is Bottom-Up Greedy, or BUG [6]. BUG occurs before scheduling; \nother algorithms such as [19], [22], and [23] are similarly scheduler-centric though they take place \nduring or interleaved with scheduling. BUG proceeds by recursing depth-.rst along the DFG, critical paths \n.rst. It assigns each operation to a cluster based on estimates of when the operation and its predeces\u00adsors \ncan complete earliest. These estimates are based on resource usage information from the scheduler, and \nBUG queries this information twice whenever it considers each operation on each cluster once before and \nonce after its predecessors have been bound. This works well for simple graphs, but when the graph becomes \nmore complex such that locally good decisions may have negative e.ects on future decisions, the algorithm \ncan be fooled into making a bad partition. Figure 2(a) shows a likely partition generated by BUG or other \nsimilar local, scheduler-centric algorithms. The crit\u00adical path (1, 2, 6, 10, 12) is considered .rst, \nand nodes 1, 2, and 6 are placed together on one cluster. Nodes 3 and 7 are placed on the other cluster, \nsince this allows node 10 to begin and complete executing soonest. However, the right subtree is now \nconstrained by the decisions that were locally optimal for the left subtree. As a result, in our example \nma\u00adchine which executes one operation per cycle per cluster, this code would take 8 cycles to complete. \nThe optimal partitioning requires only 7 cycles and is shown in Figure 2(b). Operations 4, 5, 8, 9, and \n11 should be placed on one cluster, with the remaining operations on the other cluster, as the shading \nin the .gure indicates. With this partition, one inter-cluster move is needed along the edge from 11 \nto 12. This partitioning minimizes the resul\u00adtant schedule length by balancing the workload e.ectively \nand introducing only one move operation, which is not on the critical path. Another limitation of BUG \nis that it keeps track of which resources are busy as it proceeds, and at every step of the algorithm \nit performs checks to see if a cluster is free at a certain time to perform a certain operation. Therefore, \nthe number of queries to the resource information grows with the number of clusters in the machine and \nwith the number of nodes in the graph. In order to avoid the potential pitfalls of local decision\u00admaking \nand the compiler overhead of using detailed schedul\u00ading information for resources, our approach is to \nview the graph more globally and to use estimates for determining resource load balance.  3. REGION-BASED \nHIERARCHICAL OPERATION PARTITIONING Our region-based hierarchical operation partitioning al\u00adgorithm consists \nof two distinct phases: weight calculation and partitioning. Each operation is represented by a node \nin a DFG, and node weights are created to represent the resource utilization of the operation. The edges \nconnecting the nodes are given edge weights, which represent the cost on the schedule length for adding \nan inter-cluster move between those operations. Both node and edge weights are used to guide the partitioning \nphase. The node weights are used to balance the workload among the clusters, while the edge weights are \nused to minimize the communication required between them. The partitioning phase consists of a coars\u00adening \nprocess, where highly related operations are combined together and placed into clusters, and a re.nement \nprocess, which improves the initial partitioning. The re.nement pro\u00adcess uses the calculated weights \nto consider moving opera\u00adtions between clusters, then iteratively improves the parti\u00adtion by weighing \nthe bene.ts of improving load balance and reducing inter-cluster communication. The rest of this section \nincludes a more detailed explana\u00adtion of the clustering process. The example code and DFG shown in Figure \n3 will be used throughout this section to demonstrate the process. 3.1 Weight Calculation Phase 3.1.1 \nNode Weights Node weights enable the algorithm to calculate an esti\u00admate of how many cycles it will take \nto execute a set of operations on a cluster, ignoring dependencies, when it is determining the quality \nof a partition under consideration. Thus, the weights re.ect the quantity of resources an oper\u00adation \nuses in the machine. Resources can be characterized as being used by an indi\u00advidual operation, such as \nFUs, or shared between operations, such as buses or RF ports. In general, resource usage in a machine \nforms a spectrum between these two extremes. We use the two endpoints to compute an individual node weight \nand a shared node weight for each operation. The worst case of these provides an approximation of the \noperation s resource usage. Individual node weight, or op wgtc, is calculated per node for each cluster \nc, and is a measure of the resources used by this particular node. Note that, in the case of heterogeneous \nclusters, the weight of a node is dependent on which cluster it is being considered on. For example, \nan ADD operation on a cluster with one adder carries more weight than an ADD operation on a cluster with \ntwo adders, because in the second case it only uses up half of the available resources. To determine \nthe weight of a node on a cluster, the num\u00adber of times that the resources available on that cluster \nwill support the execution of that operation in a single cycle is counted. The weight is the inverse \nof this number: 1 op wgtc = #ops supported on c in 1 cycle Since the example machine executes one operation \nper cycle, the individual weight of all of the nodes in the graph is 1.0 for both clusters. To account \nfor shared resources, a shared node weight value is calculated per cluster for the region being clustered. \nThis shared node weight on a cluster, shared wgtc, is deter\u00admined by placing all of the operations in \na region on clus\u00adter c, and dividing the resulting resource-limited minimum schedule length by the number \nof operations. The mini\u00admum schedule lengthissimilar to RESMII usedinmod\u00adulo scheduling. Since this is \ndone only to determine re\u00adsource availability and ignores data dependencies, no actual scheduling is \ndone and the calculation is fast. resource limited sched length on c shared wgtc = #ops In the example, \nplacing the 14 operations on either cluster reveals that a minimum of 14 cycles is required. Thus the \nshared weight is 1.0 on each cluster. Due to the simplicity of the machine, these numbers are somewhat \ntrivial. Given a real machine with more and varied resources available, as was used in our experiments, \nthe node weights become more interesting.  3.1.2 Edge Weights The weight of an edge is a measure of \nits criticalness. If an edge is critical, then placing the nodes on either end of the edge on di.erent \nclusters and inserting the required move will impact the schedule length. Therefore, edges on the critical \npath have a higher weight than other edges, and the graph partitioning algorithm attempts to minimize \nthe sum of the weights of the edges that are cut by the partition. Once critical edges are assigned a \nhigh weight, the remain\u00ading edges can be assigned a low weight. However, this can be dangerous as there \nmay be a non-critical path that, once a few moves are inserted, becomes critical. Therefore, a more intelligent \nsystem for assigning edge weights to non-critical edges is bene.cial. The concept of slack is a measure \nof how critical an edge is. An edge on a path where nodes can be delayed with\u00adout a.ecting the overall \nminimum schedule length has more slack, while a critical edge has no slack. We use a de.nition of slack \nsimilar to that of global slack in [9], though our measurement is per-edge rather than per-node. The \nslack of a directed .ow edge from src to dest is de.ned (estart, lstart) 1: ADD r1 <- r2, 4 (0,0) 2: \nSUB r3 <- r4, 2 (0,0) 3: SHR r5 <- r1, r3 (1,1) 4: ADD r6 <- r7, 8 (0,1) 5: ADD r8 <- r9, r10 (0,1) \n 6: SUB r11 <- r12, 2 (0,1) 7: SUB r13 <- r14, r15 (0,1) 8: ADD r16 <- r5, 8 (2,2) 9: MAC r17 <- r6, \nr8, r11 (1,2) 10: SHL r18 <- r19, r20 (0,2) 11: SHR r21 <- r13, 2 (1,2) 12: SUB r22 <- r16, r17 (3,3) \n 13: SUB r23 <- r18, r21 (2,3) 14: ADD r24 <- r22, r23 (4,4)   (a) (b) (c) Figure 3: The example code \nand corresponding DFG with slack distribution de.ning the edge weights. as: slackedge = lstartdest - \nlatedge - estartsrc Here, estart refers to the earliest cycle an operation can be\u00adgin executing (i.e. \nits inputs are available), and lstart refers to the latest cycle it can begin executing without delaying \nthe exit operation(s) of the region. The latency of the edge, latedge, is de.ned to be the latency of \nthe src operation. Thus, in the example DFG of Figure 3(a) with estart and lstart as shown next to the \nassembly code, the edges on the critical path 1 3 8 12 14 have zero slack; the edge from node 10 to 13 \nhas a slack of 3-1 -0 = 2; and the remaining edges have a slack of 1. A method of .rst-come .rst-serve \nslack distribution is used to account for paths that have some slack in them by taking up slack (by increasing \nlatency) starting from edges close to the critical path. This increased latency may lower the lstart \nof operations higher on the path, thereby decreas\u00ading the slack on their incoming edges. The process \ncontin\u00adues for the next edge on the path until all of the slack has been allocated. The edge weights \nare assigned depending on whether or not slack was allocated to the edge, based on the following numbers: \n. . 10 if critical edge wgt = 8 if no slack after distribution . 1 if slack allocated These numbers were \nchosen because cutting a critical edge will increase the schedule time so it is weighted high; cutting \nan edge that has slack allocated to it is essentially free, so it is weighted low. Cutting a non-critical \nedge that has no slack remaining after distribution is not guaranteed to increase the schedule, but it \nis likely especially if the free edges are cut; therefore, it is given a high weight but not as high \nas that of a critical edge. Using this slack distribution algorithm, edges closer to the critical path \nare more likely to be cut. This accomplishes the goal of o.oading as much work as possible from the critical \npath. It also discourages cutting a single non-critical path too many times such that it becomes critical. \nAs shown in Figure 3(b), the edges which initially had zero slack (indicating that they are critical) \nare assigned a weight of 10. Now, the non-critical edge from node 13 to 14 can have a unit of slack allocated \nto it, giving it a weight of 1. This lowers the lstart of node 13 by one cycle, with the result that \nthe slacks on the edges coming into node 13 are decreased. Similarly, the edge from node 9 to 12 receives \na weight of 1, and the slacks on the edges coming into node 9 are decreased. Figure 3(c) shows the .nal \nedge weights for this DFG, with the edges that had zero slack remaining after step 3(b) receiving a weight \nof 8, and the edge from node 10 to 13 receiving a weight of 1.  3.2 Partitioning Phase The partitioning \nphase of RHOP employs a multilevel graph partitioning algorithm to cluster the DFG of a region into distinct \ngroups. Multilevel graph partitioning, known for its e.ciency and good results, is available in many \nsoft\u00adware packages such as Chaco [12] and Metis [15]. A multilevel algorithm coarsens highly related \nnodes to\u00adgether and places them into partitions. The nodes within the graph are continually coarsened \nby grouping pairs of nodes together. At each level of coarsening, a snapshot of the currently coarsened \nnodes is taken. When the number of coarse nodes reaches the number of desired partitions, coars\u00adening \nstops. The coarse nodes are then assigned to di.erent clusters, and the uncoarsening process begins. \nDuring un\u00adcoarsening, the algorithm backtracks across the earlier snap\u00adshots of coarse nodes, considering \nmoving operations at each stage. A re.nement algorithm is used to decide the bene.ts of moving a node \nfrom one cluster to another in order to improve the partition. 3.2.1 Coarsening Coarsening takes a DFG \nrepresenting the region to be clustered and produces an initial partition for the graph. Producing a \ngood initial partition has been shown to have a large impact on how well the algorithm produces results \n[12]. The coarsening algorithm uses edge weights determined earlier during the weight calculation phase \nto intelligently group operations together. Operations separated by a high weight edge are thus .rst \ntargeted for coarsening. Each stage of the coarsening process groups together op\u00aderations into pairs \nbased on the weights of their edges. All operations are sorted based on the highest weight on any of \nits edges and considered for coarsening in that order. Op\u00aderations on the critical path will then most \nlikely be paired together. In order to try and coarsen as many nodes as pos\u00adsible at each stage, operations \nare coarsened from the out\u00adside of the DFG toward the center; thus, operations with a single neighbor \nhave higher priority for coarsening. Ties in preferences for coarsening are broken arbitrarily. Figure \n4 shows how coarsening progresses through the running example. At the .rst stage, the .rst priority is \nto  Cluster 2 Cluster 1 Figure 4: The coarsening process to group together highlyrelated operations \nand create the initial cluster  assignment. coarsen together critical edge paths, then the lower weight \nedges are considered. Each stage of the process only pairs up a single operation once, and every operation \nthat has an available neighbor to coarsen with will be paired up. Oper\u00adations that cannot be paired up, \neither because they have no neighbors or all of their neighbors have already coarsened with other operations, \nwill be left as is for the current coars\u00adening stage. For example, in the .rst coarsening stage of Fig\u00adure \n4, operation 8 no longer has any uncoarsened neighbors, so is not coarsened for this particular stage. \nWhen no more operations in a stage can be coarsened, the entire coarsening process is repeated with the \nresulting coarse nodes. The coarsening phase ends when the number of coarse nodes is equal to the number \nof desired clusters for the ma\u00adchine. The coarse nodes are then divided up between the clusters to form \nthe initial partition. For the running exam\u00adple, the .nal partition ends with operations 1, 2, 3, 4, \n5, 6, 8, 9, 12 and 14 on cluster 1; with the rest on cluster 2.  3.2.2 Re.nement The re.nement process \ntraverses back through the coars\u00adening stages, making improvements to the initial partition. At each \nuncoarsening stage, the coarsened nodes available at that point are considered for movement to another \ncluster. In order to properly improve the partition of the operations in the graph, the algorithm must \nhave metrics for deciding which cluster to move from, the desirability of the current partition, and \nthe bene.ts of an individual move. The re\u00ad.nement process uses the following to judge each of these: \n Cluster Weight: Thenodeweights for each opera\u00adtion are used to generate an estimate for the load per \ncluster; the cluster with the highest weight is denoted the imbalanced cluster.  System Load: Similar \nto the cluster weight, the system load uses the node weights of all the operations, but estimates the \nload across all clusters, generating a metric for the current cluster assignment desirability.  Gain: \nOnce the imbalanced cluster has been tar\u00adgeted, the gain of moving each operation to the other clusters \nis calculated using the change in system load and the change in edge cuts.  Since the process backtracks \nthrough the coarsening sta\u00adges, highly related operations are grouped together at each stage, and the \nalgorithm can then account for a group of op\u00aderations preferring to move together. This helps to alleviate \nsituations where moving one operation to another cluster is not bene.cial, but moving a group of related \noperations together will show improvements. The re.nement algorithm is a slightly modi.ed Kernighan-Lin \npartitioner, which is known to be a good algorithm for partitioning graphs. Traditionally, Kernighan-Lin \ntries to match pairs of operations from di.erent partitions to swap. Each swap incurs some cost upon \nthe system, and swaps are continually made until the overall cost gain is negative. This allows individual \nnegative moves, which may in fact allow future positive moves to occur. By allowing individ\u00adual negative \nmoves, the algorithm avoids falling into local minima. A modi.ed version of Kernighan-Lin is used which \ncon\u00adsiders node and edge weights to determine the gain of mov\u00ading an operation to another cluster. Unlike \nKernighan-Lin, which weighs the bene.ts of swapping nodes between par\u00adtitions, RHOP considers explicitly \nmoving each operation within the imbalanced cluster, rather than swapping. Like Kernighan-Lin, RHOP allow \nmoves with negative gain as long as the overall gain for the current re.nement step is positive. Cluster \nweight. To determine the cluster which is most imbalanced, cluster weights, the metric for the load per \nclus\u00adter, is calculated. In order to calculate the weight of a partic\u00adular cluster, a weight for each \nexecution cycle of the region is computed. To estimate the weight of each operation at each cycle, we \nuse the scheduling range, which is the estart of the operation to its lstart. The operation must be placed \nwithin this range in order to achieve minimum schedule length. The two important factors in regards to \nthe load of oper\u00adations on a cluster are: the individual resource constraints for the operations at each \ncycle, and the total node weight which is the constraint on the shared resources of a given cluster. \nThe individual resource constraint is the load put on any one speci.c resource. The shared resource weight \nis the load put on all the resources within the cluster as a whole. Since these individual resource and \nshared resource weights are competing with one another, the overall cluster weight is the max between \nthem. To compute the individual resource constraints, each op\u00aderation is placed in a single op group, \nwhich groups similar operations by their resource usage. For each operation in an op group, its total \nimpact to a particular cycle is the node weight for the operation, calculated earlier, divided by the \nslack+1 of the operation. This value, the individual weight, Iwgtc,t ,for cluster c at cycle t gives \na general approxima\u00adtion of the impact of those operations which use a similar resource, currently placed \nin that cycle on the cluster load. cycle Cluster 1 Cluster 2 max (Cwgti,cycle) Cwgt1,cycle Cwgt2,cycle \n 0  3.5 3.0 1 2 1.5 3 1.0 4 1.0 cluster_wgt1=5.0 0.83 3.5 1.33 3.0 1.33 1.5 0.5 1.0 0.0 1.0 cluster_wgt2=0.67 \nSL=5 Figure 5: The initial partition after coarsening and the cluster weights. The total node weight, \nTwgtc,t ,isthencalculated asthe total number of operations currently placed within cluster c at cycle \nt divided by the average slack for all the operations. This is then multiplied with the shared resource \nweight from the weight calculation phase to give an approximation of the constraints on the shared resources. \nThis gives an estimate as to how well the operations share the resources at a cycle. The total node weight \nis e.ective in situations where there is a lot of parallelism and the assumption of operations .n\u00adishing \nwithin the scheduling range breaks down. Thus, the desired partition focuses more on spreading the work \nout evenly among the clusters. The cycle weight Cwgtc,t in cluster c at cycle t is therefore determined \nby: op wgtcIwgtc,t = max o.opgroupsopslack +1 op.oat t #ops in c at t Twgtc,t = * shared wgtc slackave \n+1 Cwgtc,t =max(Iwgtc,t ,T wgtc,t ) (1) For example, at the end of the coarsening process, the graph \nreaches a partition as shown in Figure 5 with its corresponding cycle-by-cycle representation of all \noperation scheduling ranges (estart to lstart). For the simplicity of the example, we will not consider \nthe explicitly consider total node weight, as it has no e.ect in the result. The Cwgtc,t of each cluster \nis shown in the .gure. Cwgt1,1,the cycle weight of cluster 1 at cycle 0 is calculated as follows: ops \n1 and 2 each have a node weight of 1 and slack+1 of 1. Ops 4, 5, and 6 each have node weight of 1 and \na slack+1 of 2. Therefore, ops 1 and 2 each contribute 1 to the cycle weight while ops 4, 5, and 6 each \ncontribute 0.5, which forms a cycle weight, Cwgt1,1,of3.5. The weight of a cluster, cluster wgtc, is \nsimply the sum of all cycle weights from 0 until the max estart, minus one. One cycle is subtracted from \neach cycle weight to evaluate how overloaded each cycle is. Since every cycle can do one cycle s worth \nof work, any amount greater than one means the cycle has too much work assigned to it. Therefore, our \ncluster weight equation is: max estart cluster wgtc =( Cwgtc,t - 1) (2) t=0 System Load. While equation \n2 gives an estimate to the weight of any one cluster, it doesn t give a general estimate for the overall \ndesirability of the current chosen clustering. This is de.ned by the system load, SL,which gives a cy\u00adcle \nby cycle account for the clustering. At any given cycle, whenever one cluster s weight dominates that \nof the other cluster, the smaller load is subsumed by the larger. There\u00adfore equation 1, which calculated \nthe load every cycle in a cluster, is maxed it across all clusters and summed for the scheduling range. \nThe system load then results in the maximum any cluster is overloaded over all cycles in the scheduling \nrange. Since inter-cluster moves have a limited bandwidth, the system load is maxed with consideration \nof the inter-cluster moves required for this cluster. The inter-cluster move band\u00adwidth, icm bw, is used \nto determine the overhead, icmo,of making the inter-cluster moves required in the current par\u00adtition. \nThis inter-cluster move overhead is mostly used as a safeguard to prevent clusters from forming that \ncontain far too many inter-cluster moves. In general, the partitioner tries to minimize edge cuts, so \nthis simple estimate of to\u00adtal inter-cluster moves by the cluster is all that is necessary. The system \nload is therefore determined by: #icm icmo = - (max estart +1) icm bw max estart SL =max(( max Cwgti,t \n- 1),icmo) (3) i.cluster t=0 Gain. At each uncoarsening stage, our algorithm cal\u00adculates the weight of \nboth clusters using equation 2. The cluster with the higher weight, which we refer to as the imbalanced \ncluster, is chosen as the one to begin moving operations from. A metric is then needed for determining \nthe bene.ts of moving an operation to a di.erent cluster. Each time a node is moved to another cluster, \nthere is a shift in both the load balance, as a di.erent set of opera\u00adtions are now on each cluster, \nand also the cut edges, as there will now be di.erent edges between clusters requiring inter-cluster \nmoves. Thus, the load gain, Lgain, is de.ned as the di.erence in the system load before and after the \nproposed move is made. The edge gain, Egain, is the sum of the edge weights of the edges merged minus \nthe sum of the edge weights of the edges cut. The algorithm counts an increase of one on the load gain \nas equal importance to cutting a critical edge, as it will mean the cycle is so overloaded with work \nthat schedule length must be increased by one. Therefore, the di.erence in system load is multiplied \nby the cost of a critical edge, which is currently speci.ed as 10. The overall gain for a cycle Cluster \n1 Cluster 2 0 1 2 3 4  (a) (b) cycle Cluster 1 Cluster 2 (c) Figure 6: The re.nement process traveling \nback through coarsened states. (a) The bene.cial move of the coarsened node containing operations 4, \n5, 6 and 9 to cluster 2. (b) A situation where no positive moves exist and the move is not made. (c) \nMoving the coarsened node containing 7 and 11 to cluster 1 is now bene.cial. particular move, Mgain, \nis then determined by: Egain = edge wgti - edge wgtj i.merged edges j.cut edges Lgain = SL(before) - \nSL(after) Mgain = Egain +(Lgain * CRITICAL EDGE COST) Figure 6 shows the re.nement process and gain calcula\u00adtions \non the running example. In 6(a), the proposed move is to change the coarsened node containing operations \n4, 5, 6, and 9 from cluster 1 to cluster 2. This decreases the sys\u00adtem load from 5.0 to 4.5, a Lgain \nof 0.5. By moving this operation over, no edges are merged, and a weight 1 edge is cut (between operations \n9 and 12). Therefore the Mgain for this operation is -1 + 5 = 4. No other moves in this un\u00adcoarsening \nstep are bene.cial, so the graph is uncoarsened again. The next uncoarsened state is shown in Figure \n6(b), where operation 6 has been uncoarsened from 4, 5, and 9, all of which are now in cluster 2 after \nstep 6(a). Of interest is that even though moving operation 6 from cluster 1 to cluster 2 provides a \npositive Lgain by dropping the system load from 4.5 to 4.17, This is not enough to counteract the cost \nof cutting the weight 8 edge from operation 6 to 9, therefore this move is not made. Since no move in \nthis coarsening state is bene.cial, uncoarsening continues. Next the graph reaches the uncoarsening state \nin Figure 6(c). At this stage of uncoarsening, both the coarsened nodes containing 4 and 9 as well as \n7 and 11 decrease the system load from 4.5 to 3.5 if moved to cluster 1, as they both have the same node \nweights and a.ect the same cycles. The coarse node with 7 and 11 is chosen for moving, though, because \nit only cuts one weight 8 edge and thus remains a positive move, while the coarse node 4 and 9 cuts two \nweight 8 edges and merges one weight 1 edge, making it a negative Mgain.  Each uncoarsening stage .nishes \nwhen it can make no more moves and the same imbalanced cluster is chosen twice in a row. Then, it moves \non to the the next uncoarsened stage and the re.nement process is repeated. When the un\u00adcoarsening process \ncompletes its re.nement of the original, totally uncoarsened snapshot of the region, one .nal pass through \neach of the clusters is run, to ensure that no pos\u00aditive moves out of a cluster were ignored because \nanother cluster was extremely out of balance. In this .nal phase, each cluster allows only positive moves. \nWhen this .nal phase completes, the resulting partitions correspond to the desired clusters. The node \nweights and edge weights ensure that there exists a good load balance between the clusters as well as \na minimal cut set for inter\u00adcluster communication. For this example, the .nal uncoars\u00adening step yields \nno change from the partition after the move in Figure 6(c). Thus, the .nal partition is operations 1, \n2, 3, 7, 8, 11, 12 and 14 on cluster 1, with the remaining oper\u00adations on cluster 2. Even though there \nare three cuts in this partition, none are critical and this results in the optimal schedule length of \n8 cycles.   4. EXPERIMENTAL EVALUATION We implemented the RHOP algorithm using the Trimaran tool set \n[25], a retargetable compiler for VLIW processors. 4.1 Methodology To gauge the performance of our algorithm, \nwe compared our results to the BUG algorithm. We evaluated the per\u00adformance of both BUG and RHOP on several \nDSP ker\u00adnels and the SPECint2000 benchmark suite. DSP kernels Kernel 2-1111 2-2111 4-1111 4-2111 4-H \nadpcm -2.09 3.25 12.03 8.95 11.98 atmcell -0.32 3.34 34.86 32.58 14.04 channel -3.35 -0.73 11.20 20.44 \n6.50 dct -0.64 10.53 31.24 28.86 17.31 .r 4.75 15.74 30.90 12.34 11.62 fsed 4.39 6.52 22.87 27.90 10.65 \nhalftone 1.17 4.91 27.99 34.18 -2.12 heat -6.24 21.50 31.23 33.32 15.26 hu.man -4.84 -3.87 24.65 24.79 \n19.76 LU -2.65 -1.23 -1.42 12.44 4.51 lyapunov 1.83 9.43 13.26 6.63 13.41 rls -1.90 4.50 6.09 30.51 11.42 \nsobel -2.04 1.02 20.67 20.92 22.20 Average -0.92 5.75 20.43 22.60 12.04 SPEC 2-1111 2-2111 4-1111 4-2111 \n4-H 164.gzip -2.18 5.21 8.67 6.86 4.12 175.vpr -5.98 2.42 3.26 6.15 3.41 181.mcf -1.72 -1.49 3.99 -5.99 \n-3.44 197.parser -3.45 -2.76 -1.22 -1.40 -1.62 253.perl -3.16 0.00 6.25 5.13 1.84 254.gap -5.79 0.34 \n-0.76 0.47 -1.08 255.vortex -2.61 2.08 -5.29 7.59 1.84 256.bzip2 -1.02 -0.29 25.45 21.66 9.66 300.twolf \n-2.16 1.13 8.24 3.41 4.04 Average -3.11 0.73 5.40 4.87 2.28 Table 1: Percentage improvement byRHOP on \ncycle time over the BUG algorithm for several kernels and the SPECint2000 benchmarks on .ve di.erent \nmachine models. were investigated because of their characteristically high ILP that make them ideal candidates \nfor wide-issue processors. As a result, they provide a true measure of the cluster\u00ading algorithm s ability \nto exploit high levels of ILP. The SPECint2000 benchmarks1 were also used because of their generally \nlow and irregular ILP. These benchmarks provide the challenge of exploiting ILP when it is available, \nbut not over-partitioning when ILP is limited. Five di.erent machine con.gurations were used to com\u00adpare \nour performance with BUG. Common to all these ma\u00adchines are 64 registers per cluster, operation latencies \nsimi\u00adlar to those of the Itanium, and perfect caches. Four of the machine con.gurations have homogeneous \nclusters (i.e. the resources on each cluster are identical), and the last one is a heterogeneous machine. \nEach has varying numbers of in\u00adteger (I), .oat (F), memory (M) and branch (B) units. The di.erent machine \ncon.gurations are summarized below: Name Con.guration 2-1111 2 Homogeneous Clusters 1I, 1F, 1M, 1B per \ncluster 2-2111 2 Homogeneous Clusters 2I, 1F, 1M, 1B per cluster 4-1111 4 Homogeneous Clusters 1I, 1F, \n1M, 1B per cluster 4-2111 4 Homogeneous Clusters 2I, 1F, 1M, 1B per cluster 4-H 4 Heterogeneous Clusters \nIF, IM, IB, and IMF clusters For each benchmark, the dynamic cycle count was used as the evaluation \nmetric for how well the clustering algo\u00adrithm was able to partition the code into clusters. After clustering, \nprepass scheduling, register allocation and post\u00adpass scheduling are performed to generate the .nal assembly \ncode.  4.2 Analysis of Results Table 1 shows our improvement over BUG for 13 kernels and the SPECint2000 \nbenchmarks for the .ve di.erent ma\u00adchine models. For each kernel, we present the percentage 1 176.gcc, \n186.crafty, and 252.eon were not run due to limi\u00adtations of the current Trimaran compiler system. improvement \nin dynamic total cycles of RHOP over BUG. Positive results mean RHOP performed better than BUG, while \nnegative results mean BUG performed better. Overall, our results for a two cluster machine with one re\u00adsource \nof each type are rather poor, with an average increase in dynamic cycles of 0.92% on the kernels. As \nthe machine con.guration becomes more complex, by adding more re\u00adsources and additional clusters, results \ndramatically improve in the quality of our operation clustering. On the kernels, there was an average \nof 20% improvement on the 4-1111 machine, and a 23% improvement on the 4-2111 machine. The results for \nthe four-cluster heterogeneous machine fell between the two and four-cluster homogeneous machines. A \nsimilar trend is seen on the SPECint2000 programs in Table 1 except the improvements are more modest. \nIn gen\u00aderal, the SPECint2000 benchmarks have less ILP than the kernels, thus there is less opportunity \nfor distributing work across clusters. The data in the table shows that local, greedy methods for clustering \ncan perform quite well in constrained, resource limited situations. The poor results from the two-cluster \nmachine occur because of the inaccuracies of our resource model. Estimates, in general, can be wrong, \nand at times we observe one cluster gets more operations than it should. One major factor is that our \nresource load estimate ignores edges; thus, it also ignores dependencies between instructions, and assumes \nreordering is possible where in actuality, it may not be. We then have too many operations being placed \ninto a cluster and forced to execute serially. On the other hand, for four-cluster machines, the most \nim\u00adportant factor is carefully spreading out the workload among all the di.erent clusters. In such a \nsituation, the region-level scope used by RHOP becomes much more e.ective than a local, operation-centric \nscope. Thus, we are able to achieve a drastic improvement for four clusters. Figure 7 compares the performance \nof the 2-1111 and 4\u00ad1111 machines using BUG and RHOP with a single cluster machine containing the sum \nof the resources of all the clus\u00adters. The single cluster machine provides an upper-bound of performance. \nFor the two-cluster results, both BUG and RHOP achieve greater than 92% of the upper-bound. Con\u00adversely, \nfor a four-cluster machine, BUG only achieves 68% 1  1 0.9 0.9 0.8 0.8 BUG 0.7 RHOP 0.6 0.6 0.5 0.5 \n0.4 0.4   Benchmarks Benchmarks (a) (b) Figure 7: Comparison of BUG and RHOP clustering performance \ndegradations on (a) 2 cluster (2-1111) and (b) 4 cluster (4-1111) machine con.gurations versus a 1-cluster \nmachine with the sum of the resources of the clusters. to a good partition is properly spreading out \nwork across the 4.1 clusters, and the total node weight heuristic in RHOP intel\u00adligently balances the \nworkload. The middle ground, when regions are neither critical-path nor resource constrained, is where \nRHOP has the most di.culty. Since neither resources 1.1 7.1 7.7 3.0 22.2 4.8 12.7 9.1 8.1 1 4.1 10.0 \n0.9 4.2 0.8 0 < 5 < 10 < 15 < 20 < 30 < 40 < 50 < 60 < 80 < 120 < 160 > 160 % over CPL Figure 8: Histogram \ncomparing the performance of RHOP and BUG; each categoryis the achieved schedule length of the region \nwith respect to the critical path length. The numbers on top are the dynamic execution percentage of \nthe category. of the upper-bound. Again this is due to the local, greedy heuristics breaking down for \nwide machines. RHOP in\u00adcreases performance to 79% of the upper bound. Clearly, there is still room for \nimprovements to the RHOP algorithm. Since our desirability metrics assume that schedules .nish within \nthe critical-path length (CPL) number of cycles, we performed a study of RHOP performance as a function \nof schedule length relative to the CPL. In Figure 8, each bar represents the cumulative ratio of RHOP \ncycles over BUG cycles for all regions (across all benchmarks) in the range. The bars are annotated on \ntop with the percentage of dy\u00adnamic cycles that occur within these ranges. For regions very close to \nthe CPL, our algorithm performs modestly well. These regions are critical-path limited, and our system \nload estimates are quite accurate. For regions much higher than the CPL, where the regions are resource-constrained, \nour algorithm performs even better. In such regions, the key nor CPL dominate, our resource estimates \nlose substantial accuracy and thus bad clustering decisions can be made. In addition, the runtime of \nthe two algorithms was evalu\u00adated. For a research-oriented compiler like Trimaran, simply evaluating \nraw compute time is a rather inaccurate way to measure the speed of an algorithm. A more realistic mea\u00adsurement \nis the number of calls to the resource table, which gives an estimate on how often the algorithm is checking \nand rechecking its resource model. This is the heart of the scheduler, where most of the time is spent. \nThus, minimiz\u00ading entries into this function is a key metric to improving compiler run-time. The results \nfrom this experiment are presented in Table 2. Our algorithm shows signi.cant im\u00adprovement over BUG, \ntaking an average of 1.2 times the runtime of the scheduler alone, versus 3.0 times for BUG. This is \na result of the necessity of scheduler-centric algo\u00adrithms requiring a detailed model of the current \nresource constraints and repeatedly reevaluating the model for each step of the process.  5. RELATED \nWORK There has been a large body of research conducted in the area of clustering. In Table 3, we summarize \nour general categorization of many of them based on the four character\u00adistics of clustering algorithms \npresented in Section 2. The most closely related work to our clustering approach is with algorithms that \nuse graph partitioners, and those that use an estimate-based approach. Capitanio et al. [3] proposed \na graph partitioning method to clustering opera\u00adtions, but focused mainly on a Kernighan-Lin like approach \nto improving partitions. They focus their improvements strictly on a function of the partition cut set, \nand weigh\u00ading the bene.ts of making a cut with the probability that it will increase the schedule length. \nAlet`a et al. use a similar multilevel graph partitioner, but Kernel Sched BUG RHOP adpcm 4550 13777 \n(3.0) 6676 (1.5) atmcell 31560 109880 (3.5) 33244 (1.1) channel 12294 32094 (2.6) 14686 (1.2) dct 16646 \n47346 (2.8) 17148 (1.0) .r 9284 26434 (2.8) 10474 (1.1) fsed 13300 40244 (3.0) 13910 (1.0) halftone 14109 \n29519 (2.1) 15475 (1.1) heat 5159 13113 (2.5) 5667 (1.1) hu.man 22030 54974 (2.5) 25296 (1.1) LU 1935 \n4563 (2.4) 3105 (1.6) lyapunov 16256 43234 (2.7) 17940 (1.1) rls 25305 84413 (3.3) 26471 (1.0) sobel \n8138 23145 (2.8) 9414 (1.2) Average (2.8) (1.2) SPEC Sched BUG RHOP 164.gzip 385173 1303443 (3.4) 455795 \n(1.2) 175.vpr 1356211 4555987 (3.4) 1528947 (1.1) 181.mcf 220845 700958 (3.2) 245269 (1.1) 197.parser \n1238238 4045074 (3.3) 1434704 (1.2) 253.perl 2102449 7066202 (3.4) 2862355 (1.4) 254.gap 2046872 6754402 \n(3.3) 2813026 (1.4) 255.vortex 2133516 7199868 (3.4) 2635402 (1.2) 256.bzip2 489923 1580643 (3.2) 550493 \n(1.1) 300.twolf 1405475 4861800 (3.5) 1681433 (1.2) Average (3.3) (1.2) Table 2: Number of calls to \nthe resource table. For BUG and RHOP, the ratio of total calls over Scheduling-onlycalls is given in \nparentheses. focus on tightly integrating the clustering algorithm with the instruction scheduling and \nregister allocation [1]. Also stud\u00adied was clustering via a multilevel partitioner to determine the optimal \ninitiation interval (II) for a modulo scheduled loop using a pseudo-scheduler [2]. Their work focuses \non scheduling cyclic code in multicluster domains, while ours is targeted toward acyclic code. We also \nuse substantially di.erent models for computing node and edge weights. While not a heavily researched \narea, there has been some work on estimate-based approaches for clustering. Lapin\u00adskii et al. [17] base \ntheir estimate o. three major factors: the data transfer penalty, FU serialization penalty and bus serialization \npenalty. They use a local approach like BUG to minimize the data transfer penalties. The FU serialization \nis basically the load of the cluster, which is determined in a cycle by cycle approach. Partitioning \ncan also be approached by considering oper\u00adands rather than operations [13]. Research on partitioning \nfor multiprocessors has many similarities to clustering for multicluster processors. Yang and Gerasoulis \n[26] proposed a low-complexity method for clustering and scheduling par\u00adallel tasks for multiprocessors. \nLiou and Palis [20] improved upon the complexity of this algorithm. 6. CONCLUSION This paper proposes \na novel technique to cluster opera\u00adtions for multicluster processors. A slack distribution al\u00adgorithm \nis presented, which e.ectively weights edges based on their preference for being broken across clusters. \nWe introduce a new way to estimate the impact of clustering decisions, which is used to guide our graph \npartitioner. Our graph partitioner is able to consider an entire region of code and base its decisions \no. a view of the code as a whole, rather than what the best clustering is for a single operation. We \ncompared our results to a popular algorithm, BUG, and results show that for larger number of clusters, \nour algo\u00adrithm is able to e.ciently produce better partitions. Two\u00adcluster machines saw an average performance \ndecrease of 1.8% across all kernels and benchmarks. As we increased the number of clusters, there was \na dramatic increase in the performance of our partitioner. A four-cluster machine pro\u00advided an average \nimprovement of 14% in our experiments. In the future, we plan to improve our system load es\u00adtimation \nheuristic in situations where regions are neither resource nor critical path limited. This corresponds \nto the middle portion of Figure 8, where RHOP s performance de\u00adcreased in comparison to BUG. In addition, \nwe plan to inves\u00adtigate the e.ects of register allocation and register pressure on clustering decisions. \nExtending RHOP to e.ectively clus\u00adter modulo scheduled loops presents another area of future study. \n7. ACKNOWLEDGMENTS We thank K. V. Manjunath for his help in debugging the compiler infrastructure. We \nalso thank the anonymous ref\u00aderees for their comments and suggestions. This research was supported in \npart by the DARPA/MARCO C2S2 Research Center and equipment donated by Intel Corporation.  8. REFERENCES \n[1] A. Alet`a, J. Codina, J. S\u00b4anchez, and A. Gonz\u00b4alez. Graph-partitioning based instruction scheduling \nfor clustered processors. In Proceedings of the 34th Annual International Symposium on Microarchitecture, \nDec. 2001. [2] A. Alet`a, J. Codina, J. S\u00b4anchez,A.Gonz\u00b4alez, and D. Kaeli. Exploiting pseudo-schedules \nto guide data dependence graph partitioning. In Proceedings of the 2002 International Conference on Parallel \nArchitectures and Compilation Techniques, Sept. 2002. [3] A. Capitanio, N. Dutt, and A. Nicolau. Partitioned \nregister .les for VLIWs: A preliminary analysis of tradeo.s. In Proceedings of the 25th Annual International \nSymposium on Microarchitecture, pages 103 114, Dec. 1992. [4] J. Codina, J. S\u00b4anchez, and A. Gonz\u00b4alez. \nURACAM: A uni.ed register allocation, cluster assignment and modulo scheduling approach. In Proceedings \nof the 34th Annual International Symposium on Microarchitecture, Dec. 2001. [5] G. Desoli. Instruction \nassignment for clustered VLIW DSP compilers: A new approach. Technical Report HPL-98-13, Hewlett-Packard \nLaboratories, Feb. 1998. [6] J. Ellis. Bulldog: A Compiler for VLIW Architectures. MIT Press, Cambridge, \nMA, 1985. [7] P. Faraboschi, G. Desoli, and J. Fisher. Clustered instruction-level parallel processors. \nTechnical Report HPL-98-204, Hewlett-Packard Laboratories, Dec. 1998. [8] K. Farkas, P. Chow, N. Jouppi, \nand Z. Vranesic. The multicluster architecture: Reducing cycle time When (rel. to sched) Scope DesirabilityMetric \nGrouping Algorithm Beforev During Iterative Local v Region Sched v Pseudo Est Count Hier Flatv BUG [6] \n[21] v v v v v PCC [5] v v v v UAS [23] v v v v ABC [16] v v v v Eichenberger [22] v v v v Leupers [19] \nv v v v Capitanio [3] v v v v URACAM [4] v v v v GP(A) [1] v v v v GP(B) [2] v v v v B-ITER [17] v v \nv v CARS [14] v v v v Convergent [18] v v v v RHOP Table 3: A comparison of several di.erent clustering \ntechniques based on four important characteristics: when the clustering occurs in relation to scheduling, \nthe scope of the algorithm, the metric used in order to determine the qualityof the partition, and whether \noperations are considered individuallyor in groups. through partitioning. In Proceedings of the 30th \nAnnual International Symposium on Microarchitecture, Dec. 1997. [9] B. Fields, R. Bod\u00b4ik, and M. D. Hill. \nSlack: Maximizing performance under technological constraints. In Proceedings of the 29th Annual International \nSymposium on Computer Architecture, May 2002. [10] J. Fisher. Very long instruction word architectures \nand the ELI-52. In Proceedings of the 10th Annual International Symposium on Computer Architecture, pages \n140 150, June 13 17, 1983. [11] R. Hank, W. Hwu, and B. Rau. Region-based compilation: An introduction \nand motivation. In Proceedings of the 28th Annual International Symposium on Microarchitecture, pages \n158 168, Nov. 1995. [12] B. Hendrickson and R. Leland. The Chaco User s Guide. Sandia National Laboratories, \nJuly 1995. [13] J. Hiser, S. Carr, and P. Sweany. Global register partitioning. In Proceedings of the \n2000 International Conference on Parallel Architectures and Compilation Techniques, pages 13 23, Oct. \n2000. [14] K. Kailas, K. Ebcio.glu, and A. Agrawala. CARS: A new code generation framework for clustered \nILP processors. In Proceeding of the 2001 International Conference on High Performance Computer Architecture, \npages 133 142, Feb. 2001. [15] G. Karypis and V. Kumar. Metis: A Software Package for Partitioning Unstructured \nGraphs, Partitioning Meshes and Computing Fill-Reducing Orderings of Sparse Matrices. University of Minnesota, \nSept. 1998. [16] G. Krishnamurthy, E. Granston, and E. Stotzer. A.nity-based cluster assignment for unrolled \nloops. In Proceedings of the 2002 International Conference on Supercomputing, pages 107 116, June 2002. \n[17] V. Lapinskii, M. Jacome, and G. de Veciana. High-quality operation binding for clustered VLIW datapaths. \nIn Proceedings of the 2001 Design Automation Conference, June 2001. [18] W. Lee, D. Puppin, S. Swenson, \nand S. Amarasinghe. Convergent scheduling. In Proceedings of the 35th Annual International Symposium \non Microarchitecture, Nov. 2002. [19] R. Leupers. Instruction scheduling for clustered VLIW DSPs. In \nProceedings of the 2000 International Conference on Parallel Architectures and Compilation Techniques, \nOct. 2000. [20] J. Liou and M. Palis. A new heuristic for scheduling parallel programs on multiprocessor. \nIn Proceedings of the 1998 International Conference on Parallel Architectures and Compilation Techniques, \npages 358 365, Oct. 1998. [21] P. Lowney et al. The Multi.ow Trace Scheduling compiler. The Journal of \nSupercomputing, 7(1-2):51 142, 1993. [22] E. Nystrom and A. E. Eichenberger. E.ective cluster assignment \nfor modulo scheduling. In Proceedings of the 31th Annual International Symposium on Microarchitecture, \npages 103 114, Nov. 1998. \u00a8 [23] E. Ozer, S. Banerjia, and T. Conte. Uni.ed assign and schedule: A new \napproach to scheduling for clustered register .le microarchitectures. In Proceedings of the 31th Annual \nInternational Symposium on Microarchitecture, pages 308 315, Nov. 1998. [24] B. Rau. Iterative modulo \nscheduling: An algorithm for software pipelining loops. In Proceedings of the 27th Annual International \nSymposium on Microarchitecture, pages 63 74, Nov. 1994. [25] Trimaran. An infrastructure for research \nin ILP. http://www.trimaran.org. [26] T. Yang and A. Gerasoulis. DSC: Scheduling parallel tasks on an \nunbounded number of processors. IEEE Transactions on Parallel and Distributed Systems, 1994.  \n\t\t\t", "proc_id": "781131", "abstract": "Clustered architectures are a solution to the bottleneck of centralized register files in superscalar and VLIW processors. The main challenge associated with clustered architectures is compiler support to effectively partition operations across the available resources on each cluster. In this work, we present a novel technique for clustering operations based on graph partitioning methods. Our approach incorporates new methods of assigning weights to nodes and edges within the dataflow graph to guide the partitioner. Nodes are assigned weights to reflect their resource usage within a cluster, while a slack distribution method intelligently assigns weights to edges to reflect the cost of inserting moves across clusters. A multilevel graph partitioning algorithm, which globally divides a dataflow graph into multiple parts in a hierarchical manner, uses these weights to efficiently generate estimates for the quality of partitions. We found that our algorithm was able to achieve an average of 20% improvement in DSP kernels and 5% improvement in SPECint2000 for a four-cluster architecture.", "authors": [{"name": "Michael Chu", "author_profile_id": "81406596186", "affiliation": "University of Michigan, Ann Arbor, MI", "person_id": "PP14059873", "email_address": "", "orcid_id": ""}, {"name": "Kevin Fan", "author_profile_id": "81100488802", "affiliation": "University of Michigan, Ann Arbor, MI", "person_id": "P517416", "email_address": "", "orcid_id": ""}, {"name": "Scott Mahlke", "author_profile_id": "81100622742", "affiliation": "University of Michigan, Ann Arbor, MI", "person_id": "P260983", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/781131.781165", "year": "2003", "article_id": "781165", "conference": "PLDI", "title": "Region-based hierarchical operation partitioning for multicluster processors", "url": "http://dl.acm.org/citation.cfm?id=781165"}