{"article_publication_date": "05-09-2003", "fulltext": "\n A Region-Based Compilation Technique for a Java Just-In-Time Compiler Toshio Suganuma Toshiaki Yasue \nToshio Nakatani IBM Tokyo Research Laboratory 1623-14 Shimotsuruma, Yamato-shi, 242-8502 Japan {suganuma,yasue,nakatani}@jp.ibm.com \nABSTRACT Method inlining and data flow analysis are two major optimization components for effective program \ntransformations, however they often suffer from the existence of rarely or never executed code contained \nin the target method. One major problem lies in the as\u00adsumption that the compilation unit is partitioned \nat method boundaries. This paper describes the design and implementation of a region-based compilation \ntechnique in our dynamic compilation system, in which the compiled regions are selected as code por\u00adtions \nwithout rarely executed code. The key part of this technique is the region selection, partial inlining, \nand region exit handling. For region selection, we employ both static heuristics and dynamic profiles \nto identify rare sections of code. The region selection process and method inlining decision are interwoven, \nso that method inlining exposes other targets for region selection, while the region selection in the \ninline target conserves the inlining budget, leading to more method inlining. Thus the inlining process \ncan be performed for parts of a method, not for the entire body of the method. When the program attempts \nto exit from a region boundary, we trigger recompilation and then rely on on-stack re\u00adplacement to continue \nthe execution from the corresponding entry point in the recompiled code. We have implemented these tech\u00adniques \nin our Java JIT compiler, and conducted a comprehensive evaluation. The experimental results show that \nthe approach of re\u00adgion-based compilation achieves approximately 5% performance improvement on average, \nwhile reducing the compilation over\u00adhead by 20 to 30%, in comparison to the traditional function\u00adbased \ncompilation techniques.  Categories and Subject Descriptors D.3.4 [Programming Languages]: Processors \n incremental compilers, optimization  General Terms Performance, Design, Experimentation, Measurement, \nLanguages  Keywords Region-based compilation, dynamic compilers, partial inlining, on-stack replacement \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, \nto post on servers or to redistribute to lists, requires prior specific permission and/or a fee. PLDI \n03, June 9-11, 2003, San Diego, California, USA. Copyright 2003 ACM 1-58113-662-5/03/0006 $5.00. 1. \nINTRODUCTION Dynamic compilation systems can exploit the profile information from the current execution \nof a program. This allows dynamic compilers to find opportunities for better optimization, and thus is \na significant advantage over traditional static compilers. Many of today s Java Virtual Machines (JVMs) \nand Just-In-Time (JIT) compilers indeed use some form of online profile information, not only for focusing \nthe optimization efforts on programs hot spots, but also for better optimizing programs by exploiting \nvarious kinds of runtime information such as call edge frequency and bi\u00adased branch behaviors [1][2][10][21][24][25]. \nTypically these systems have multiple execution modes. They be\u00adgin the program execution using the interpreter \nor the base-line compiler as the first execution mode. When they detect the pro\u00adgram s frequently executed \nmethods or critical hotspots, they in\u00advoke the optimizing compiler for those selected methods to run \nin the next execution mode for higher performance. Some systems employ several different optimization \nlevels to select from based on the invocation frequency or relative importance of the target methods. \nMethod inlining and data flow analysis are two major components for effective program transformations \nin the higher optimization levels. However, methods often contain rarely or never executed paths, as \nshown in the previous studies [5][29], and this can cause some adverse effects that reduce the effectiveness \nof these optimi\u00adzations. For example, method inlining can be restricted due to the excessive code size \ncaused by the rarely executed code in the tar\u00adget method. Some methods may include a large portion of \nrarely executed code, and this may prevent them from being inlined at the corresponding call sites. Others \ncan grow large from the cumu\u00adlative effects of sections of rare code after several stages of inlin\u00ading, \nand this may prevent other hot methods from being inlined. Also data flow analysis is often hindered \nby kill points existing in those rarely executed paths, whose control flow may merge back to non-rare \npaths, and this can prevent the propagation of accurate data flow information on non-rare paths. The \nproblem here is that we implicitly assume methods are the units for compilation. Even if we perform inline \nexpansion, we ei\u00adther inline or do not inline the entire body of a target method, re\u00adgardless of the \nstructure and dynamic behavior of the target method. Method boundaries have been a convenient way to \nparti\u00adtion the process of compilation, but methods are not necessarily a desirable unit to perform optimizations. \nIf we can eliminate from the compilation target those portions that are rarely or never exe\u00adcuted, we \ncan focus the optimization efforts only on non-rare paths and this would make the optimization process \nboth faster and more effective. In this paper, we describe the design and implementation of a re\u00adgion-based \ncompilation (RBC) technique in our dynamic compila\u00adtion system. In this framework, we no longer treat \nmethods as the unit of compilation, as in traditional method-based or function\u00adbased compilation (FBC), \nand select only those portions that are identified as non-rare paths. The term region refers to a new \ncom\u00adpilation unit, which results from collecting code from several methods of the original program but \nexcludes all rarely executed portions of these methods. Regions are inherently inter-procedural, rather \nthan intra-procedural, and thus the region selection needs to be interwoven with the method inlining \nprocess. The notion of region-based compilation was first proposed in [14], as a generalization of the \nprofile-based trace selection approach [19], with some experimental evidence of the potential impact \nby allowing the compiler to repartition the program for desirable compilation units. An improved region \nformation algorithm was then proposed by combining region selection and the inlining process [26][27]. \nThe goal of this prior work was to expose as many scheduling and other optimization opportunities as \npossible for an ILP static compiler without creating an excessively large amount of code due to the aggressive \ninlining. In a dynamic com\u00adpilation environment, however, this technique is especially useful for several \nreasons. First, dynamic compilers can take advantage of runtime profile information from currently executing \ncode and use this information for the region selection process. Second, they are very sensitive to the \ncompilation overhead, and this technique can significantly improve the total compilation time and code \nsize. Third, they can avoid generating code outside the selected regions until the code is actually executed \nat runtime. At the same time, we have other kind of challenges that are differ\u00adent from those in static \ncompilers. For example, dynamic online profiles are not always available for all target methods, unlike \na static compilation model using offline profile results. Even if they are available, we cannot expect \ncomplete profile results that cover all basic blocks of the target method. The profiling is usually per\u00adformed \nonly for selected methods and on a sampling basis in order to reduce the runtime overhead. Thus we cannot \nrely solely on the profile information and need to come up with a different region selection strategy \nby combining some static heuristics. Also the cost of handling region exits can be high in a dynamic \ncompilation system, with recompilation and possibly stack frame replacement, so the region selection \nmay need to be more conservative. The key components for the RBC approach are region selection, partial \ninlining, and the region exit handling. For region selection, we employ both static heuristics and dynamic \nprofiles to identify seed blocks of rare code and then propagate them to form rare code sections. The \nregion selection process and method inlining can affect each other, in the sense that method inlining \nexposes another target for region selection, and the region selection proc\u00adess in turn conserves the \ninlining budget. Thus the inlining process can be performed for parts of a method, not for the entire \nbody of the method. When the program attempts to exit from a region boundary at runtime, we trigger recompilation \nand rely on on\u00adstack replacement (OSR) [15], which is a technique to dynami\u00adcally replace a stack frame \nfrom one form to another, in order to continue the execution from the corresponding entry point in the \nrecompiled code. Notions related to RBC include deferred compilation [7][13], un\u00adcommon traps [15][21], \nand partial method compilation [29]. The fundamental difference between these prior techniques and ours \nis in the integration of the region selection process with the method inlining decisions. That is, the \nprevious techniques first perform inlining normally and then eliminate rarely executed code from the \nresulting code. The major disadvantage with that approach is that possibly large portions of the body \nof a once inlined method can be simply eliminated as rare in later phases, and it may miss some optimization \neffects that could be achieved with more method inlining if those eliminated rare code sections were \nnot included in the inlining process. The following are the contributions of this paper: Design and \nimplementation of a region-based compilation technique for a dynamic compiler: We present a simple and \neffective technique consisting of region selection, partial inlin\u00ading, region exit handling, and other \nRBC-related optimizations (such as partial escape analysis and partial dead code elimina\u00adtion), as implemented \nin our dynamic compilation system.  Detailed experimental evaluation of the effectiveness of re\u00adgion-based \ncompilation in a dynamic compilation environ\u00adment: We present detailed evaluation results for the impact \non both performance and compilation overhead by this technique, based on the actual implementation on \na production-level Java JIT compiler.  The rest of this paper is organized as follows. The next section \nis an overview of our dynamic compilation system as used in this study, and describes our implementation \nof on-stack replacement for safely executing the exit path from the selected region. Section 3 describes \nour design and implementation of the region-based compilation technique, including intra-method region \nidentifica\u00adtion, partial inlining, and other optimization techniques that are aware of region information. \nSection 4 presents the experimental results on both performance and compilation overhead by apply\u00ading \nthis technique. Section 5 discusses the results and possible fu\u00adture research. Section 6 summarizes related \nwork, and finally Sec\u00adtion 7 presents our conclusions. 2. BACKGROUND In this section, we describe the \nbasic infrastructure required for the RBC approach in a dynamic compilation system. Section 2.1 gives \nthe brief overview of the recompilation system with dynamic pro\u00adfiling mechanism1. Section 2.2 describes \nour implementation of OSR as the basic support for handling region exit points. 2.1 System Overview Our \nsystem is a multilevel compilation system, with a mixed mode interpreter (MMI) and three compilation \nlevels (level-0 to level-2). Initially all methods are interpreted by the MMI. A counter for counting \nboth method invocation frequencies and loop iterations is provided for each method. When the counter \nexceeds a thresh\u00adold, the method is considered as frequently invoked or computa\u00adtionally intensive, and \nthe first compilation is triggered. The dynamic compiler has a variety of optimization capabilities. \nThe level-0 compilation employs only a very limited set of optimi\u00ad 1 The overall system architecture \nof our dynamic optimization framework is described in detail in [24][25]. zations. Method inlining is \nconsidered only for small target meth\u00adods. The devirtualization of method calls is applied based on class \nhierarchy analysis and type flow analysis, and produces either guarded or unguarded code [17]. Preexistence \nanalysis is also per\u00adformed to safely remove guard code and backup paths without re\u00adquiring OSR [12]. \nMost of the data flow optimizations are dis\u00adabled except for very basic copy and constant propagation. \nThe level-1 compilation enhances level-0 by adding additional optimi\u00adzations, including more aggressive \nfull-fledged method inlining, and a wider set of data flow optimizations. The level-2 compila\u00adtion is \naugmented with all the remaining optimizations available in our system, such as escape analysis and stack \nobject allocation, code scheduling, and DAG-based optimizations. The level-0 compilation is invoked from \nthe MMI and is executed as an application thread, while level-1 and level-2 compilations are performed \nby a separate compilation thread in the background. The priority of the compilation thread is set equal \nto that of the application threads. The upgrade recompilation from level-0 com\u00adpiled code to level-1 \nor level-2 optimized code is triggered on the basis of the compiled method hotness level as detected \nby a timer\u00adbased sampling profiler. The sampling profiler periodically moni\u00adtors the program counters \nof application threads, and keeps track of methods in threads that are using the most CPU time. Depend\u00ading \non the relative hotness level, the method can be promoted from level-0 compiled code to either level-1 \nor directly to level-2 opti\u00admized code. There is another profiler, the instrumenting profiler, used when \ndetailed information needs to be collected from the target method. The instrumentation code is initially \ngenerated in level-0 compiled code but disabled. When a method is identified as hot and is a candidate \nfor promotion to a higher optimization level, the con\u00adtroller enables the instrumentation code in the \ntarget method to collect the required information. After a sufficient number of sam\u00adples are collected, \nit is disabled to minimize the performance im\u00adpact. The compiler can then take advantage of this online \nprofile information in the higher optimization levels. This instrumenting profile mechanism is used for \ncollecting both virtual/interface call receiver type distributions and basic block execution frequencies. \nThe receiver type profile drives inlining if it identifies the call site as dynamically monomorphic, \nand the ba\u00adsic block frequency profile provides the runtime rare code infor\u00admation. We apply RBC-based \noptimization on level-1 and level-2, using the rare code profile information collected from level-0 compiled \ncode. 2.2 Region Exit Handling There are several options to handle region exit points. The sim\u00adplest \noption is code splitting [7], which was originally designed to reduce the overhead of message sends in \nthe SELF programming language. It eliminates control flow merges from rarely executed code blocks by \nperforming tail-duplication of conditional control flows to improve data flow information. To alleviate \nthe poten\u00adtially exponential code expansion problem of this technique, sev\u00aderal improved versions have \nbeen proposed, such as reluctant splitting [7] and feedback-directed splitting [2]. With this option, \nwe do not eliminate rarely executed paths, but keep that code in a separate section from non-rare paths. \nThe implementation at a re\u00adgion boundary can be very simple, just a single jump instruction, without \nrequiring a complex OSR mechanism. However, using this option in our framework means we lose the majority \nof the benefit expected from the integration of inlining and region selec\u00adtion. Recall that our motivation \nfor the RBC approach is not only to eliminate the control flow merges, but also to exploit mutual in\u00adteraction \nbetween method inlining and region selection, and this is possible by actually removing rare regions \nfrom the compilation scope. Thus we eliminated this simple option from consideration. Some other options \nare listed below, all of which assume on-stack replacement as a pre-requisite technique. 1. Simply fall \nback to the interpreter. This is the option taken by the HotSpot server compiler [21] when class loading \ninvali\u00addates inlining or other optimization assumptions. This relies on the underlying recompilation \nsystem to promote the de\u00adcompiled interpreted method once again. 2. Drive recompilation with deoptimization. \nThis is the policy used by the SELF-93 system [15] and Jikes RVM [13]. The deoptimized compiled code \nis used only for the current transi\u00adtion. If the optimization assumption turns out to be wrong and the \nuncommon cases happen frequently, the system reopti\u00admizes the method under the new assumption, replacing \nthe code for future invocations, by using the underlying recom\u00adpilation system. Thus this option can \nproduce two additional versions of the code. 3. Drive recompilation with the same optimization level. \nThe re\u00adcompilation is performed with the same scope for method inlining as in the original version. The \nrecompiled version has several entry points corresponding to region boundaries in the original version, \nand is used for both current transitions and future method invocations.  The information we need to \nkeep for restoring the JVM state at the region boundary should be basically the same for all three options \nabove, but the way to reconstruct the new stack frame is different. In particular, the first two options \nneed to create a set of stack frames according to the inlined context at the transfer point, while the \nthird option can simply create a single new stack frame. There are both advantages and disadvantages \nfor each of these three options, and which options we should choose depends on the meaning of rare code \n. If we stay very conservative, saying that only extremely rare cases are rare, then the first option \nis probably the best choice, since the current version can still serve for future invocations and it \nis only necessary to handle transitions that may happen very infrequently. However, this means the compiler \nmay miss some additional optimization opportunities for frequent cases due to the conservativeness. On \nthe other hand, if we use an ag\u00adgressive strategy for optimizing away rare code, these cases would actually \noccur frequently at runtime, and for the sake of overall performance it may be better to replace the \ncompiled code as quickly as possible to avoid too many expensive OSR operations. Ideally it might be \nbetter to choose from these options depending on the predicted execution frequency for each rare code \nblock identified. For example, we could use the branch prediction from the dynamic profile information. \nIn our current implementation, we use only the third option above, recompilation with the same optimization \nlevel. This is because we apply RBC-based compila\u00adtion only at the higher optimization levels (level-1 \nand level-2), and thus those methods where region exits occur are already known to be hot and performance \ncritical, and we do not want to Figure 1. An example of transitions from an RBC-optimized version to \nrecompiled version. After replacing the current stack frame (OSR), it jumps to the corresponding entry \npoint in the recompiled code. If the source and destination frames are of same shape, it directly jumps \nwithout an OSR operation. The OSR counter is used to determine when a fu\u00adture invocation is redirected \nto the recompiled version. deoptimize nor decompile them. In the recompilation, to avoid the recursive \nrecompilation we do not apply RBC optimization. Fig\u00adure 1 illustrates how a transition from a RBC-optimized \nversion to a recompiled version occurs. The recompiled version prepares all the entry points for future \npossible transitions, not only the entry point for the current transition. Since our region selection \nis speculative (based on static heuristics and dynamic profiles), it is possible that control frequently \nexits from certain region boundary points. However, if the frequency is small enough, we want the original \nRBC version to still serve for future invocations since it is better optimized. In order to deter\u00admine \nwhen we should switch from the RBC-optimized version to the new recompiled version for future invocations, \nwe provide a counter for each RBC version to count the number of actual OSR events. Initially the recompiled \nversion is used for the region exit transitions only, not for future invocations. When the OSR counter \nexceeds a certain threshold, we conclude that our speculative re\u00adgion selection did not work well for \nthis method, and redirect the control of future invocations to the recompiled version. As in the previous \nimplementation of OSR [13][29], we provide a first-class operator in our compiler s intermediate representation, \ncalled OPC_RECOMPILE, at each region exit point. We keep all live variables at that program point in \nthe given bytecode sequence as operands of this special operator for both local and stack vari\u00adables \nso as to be able to restore the JVM state. This is done in a very early stage in the compilation, so \nthat any optimizations in later phases can rely on the use of those variables across each re\u00adgion exit \nboundary. At the final stage of the compilation, we create a map indicating the final locations of those \nvariables as well as other information for frame conversion, such as frame size, and Figure 2. The flow \nof region selection and inlining in level-1 and level-2 RBC-based optimizations. Both static heuristics \nand dynamic information provided by the online profiler are used in the intra-method region selection \nprocess. callee-saved registers. We also create a runtime structure indicat\u00ading the inline context of \nthe given method, and a pointer to the ap\u00adpropriate node in this structure is also recorded as part of \nthe in\u00adformation for each exit point. All registers holding live variables are first spilled out into \nmem\u00adory at region exit points. One small optimization here is that when the source and destination stack \nframes turn out to be the same shape (frame size, callee saved registers, number and order of lo\u00adcal \nvariables, etc) in the first OSR after recompilation, we patch the instruction at the region exit points \nwith an unconditional jump instruction directly into the corresponding entry points, so that we can skip \nthe expensive OSR operation from the next time (see the shortcut line in Figure 1).  3. REGION-BASED \nCOMPILATION In this section, we provide a detailed description of our region\u00adbased compilation technique. \nSection 3.1 describes the intra\u00adprocedural region selection process using both static heuristics and \ndynamic profiles. Section 3.2 shows how the region identification and method inlining are interrelated \nto obtain more desirable tar\u00adgets as compilation units. Section 3.3 gives some useful optimiza\u00adtions \nwe implemented to take advantage of the target code in the selected region. As shown in Figure 2, region-based \ncompilation is performed only in level-1 and level-2 compilation, using the profile information resulting \nfrom the instrumentation on level-0 compiled code. Since only level-0 compiled methods that are identified \nas hot and hence are candidates for promotion to the next optimization level are instrumented, profile \ninformation is not always available for all target methods considered for inlining at level-1 and level-2. \nThis is in contrast to the static compilation model using offline profiles. Even if the information is \navailable, we cannot expect a complete profile that covers all the code within the target method. This \nis regarded as a common problem for dynamic compilers, where online profiles need to be collected selectively \nand on a sampling basis from the currently executing program to reduce the performance impact. Therefore \nwe combine some static heuristics with incomplete profile information available for region selection. \n3.1 Intra-Procedural Region Selection Unlike the region formation algorithm shown in [14], we identify \nrarely executed portions that we want to remove from the original code, rather than choosing a preferred \nportion to be optimized to\u00adgether. This is because we have to be more conservative for region selection \nin a dynamic compiler than in a static compiler, consid\u00adering the potentially high overhead of OSR that \nhas to be per\u00adformed when control exits from the selected region. This is also a different strategy from \nthat used in dynamic binary translation systems [3][6][9], where frequently executed paths (traces) are \nextracted as a unit for optimization to form a single\u00adentry multiple-exit contiguous sequence. These \nsystems dynami\u00adcally reoptimize code on top of already (statically) optimized ge\u00adneric executable, which \nis quite a different compilation model from our system environment. We need to avoid escaping from the \nselected regions as much as possible, and thus have to employ wider regions that can contain arbitrary \nnumbers of hot traces by removing rarely executed code blocks. The intra-procedural region selection \nprocess is shown in Figure 3. We assume each method is represented as a control flow graph (CFG) at this \npoint with a single entry block and a single exit block. The algorithm begins by marking a Gen set for \nsome seed basic blocks as either non-rare or rare by employing both heuris\u00adtics and dynamic profile results. \nWe currently use the following heuristics. A backup block generated by compiler versioning optimiza\u00adtion \n(such as devirtualization of method invocation2) is rare.  A block that ends with an exception throwing \ninstruction (OPC_ATHROW) is rare.  An exception handler block is rare.  A block containing unresolved \nor uninitialized class references is rare.  A block that ends with a normal return instruction is non-rare. \n If the dynamic profile information is available and it shows that a block is never executed, then we \nmark the block as rare. If the pro\u00adfile count value is above a predetermined threshold, the block is \nmarked as non-rare. The dynamic profile information is given pri\u00adority when there are conflicts with \nthe static heuristics. In the iteration phase, we propagate this information along back\u00adward data flow \nuntil it converges for all of the basic blocks. The basic block is marked non-rare in its Out set if \nany of the succes\u00adsor s In sets is marked non-rare, otherwise it is marked rare when any of the In sets \nis marked rare. If two flags conflict on the same path between Gen set and Out set, the flag in Gen set \nis selected to propagate further. Thus a region of rare code can grow backward until it encounters a \nnon-rare path, or a statically identified rare region can be blocked from growing by a profile-based \nnon-rare block along its path. When converged, the rare regions should have reached the points where \nthe branches are expected to be rarely taken from the non-rare paths. 2 Loop versioning is another versioning \noptimization, but we cur\u00adrently do not treat its backup block as rare due to our implemen\u00adtation. See \nSection 5. /* Rare-Static, Rare-Profile, NonRare-Static, and NonRare-Profile are all defined as a bit \nvector representation. */ /* initialization phase (set flags in seed blocks) */ for each basic block \nbb {  /* set flags in Gen set based on heuristics */ if ( matched to static heuristics ) { set (Gen \n(bb), Rare-Static) or set (Gen (bb), NonRare-Static); } /* set flags in Gen set based on dynamic profile \ninformation */ if ( profile_info_available (bb) ) { if ( profile_count (bb) == 0 ) { set (Gen (bb), \nRare-Profile ); unset (Gen (bb), NonRare-Static); } else if ( profile_count (bb) > threshold ) { set \n(Gen (bb), NonRare-Profile ); unset (Gen (bb), Rare-Static); } } } /* iteration phase (grow rare and \nnon-rare regions) */ do { changed = false for each basic block bb { /* compute Out set from all successors \nIn set */ Out (bb) = . In (succ (bb)) for all successors of bb; if ( is_set (Out (bb), NonRare-Static \n| NonRare-Profile) ) unset (Out (bb), Rare-Static | Rare-Profile); /* combine Out set and Gen set to \nupdate In set */ new_set = Gen (bb) | Out (bb); if ( is_set (Gen (bb), NonRare-Profile) ) unset (new_set, \nRare-Static | Rare-Profile);  else if ( is_set (Gen (bb), Rare-Static | Rare-Profile) )   unset ( \nnew_set, NonRare-Static | NonRare-Profile); if ( new_set . In (bb) ) { In (bb) = new_set changed = true \n } } } while ( ! changed ) /* final phase (remove rarely executed block) */ find boundary points from \na non-rare block to a rare block perform live variable analysis for each rare block entry bb { create \na new bb (RE-BB)  create a special instruction in the RE-BB holding live variables  redirect incoming \nedges to the RE-BB } Figure 3. Algorithm for intra-procedural region selection. The final phase simply \ntraverses the basic blocks to determine the transitions from non-rare blocks to rare blocks, and marks \nthose locations as rare block entry points. After performing live analysis to find the set of live variables \nat each rare block entry point, we generate a new region exit basic block (RE-BB) for each entry point \nand replace the original entry block by redirecting its incom\u00ading control flow edge to the new block. \nThis new RE-BB contains a single special instruction OPC_RECOMPILE, which holds all live variables at \nthe rare block entry point as its operands, in order to trigger recompilation and call a runtime routine \nthat performs OSR when it is executed. All the rare blocks that originally existed following rare block \nentry points are no longer reachable from the top of the method and thus eliminated in the succeeding \ncontrol flow cleanup phase. 3.2 Partial Inlining Partial inlining begins by performing region selection \nfor the root method as shown in Figure 4. It then builds a large call tree of possible inlined scopes \nfrom this root method with allowable sizes and depths using optimistic assumptions. The actual inlining \npass proceeds by checking each individual decision in the given call tree against the total cost to come \nup with a pruned final tree. Spe\u00adcifically, it tries to greedily incorporate as many methods as possi\u00adble \nusing static heuristics until the predetermined budget is used up. Tiny methods are always inlined without \nqualification [25]. Otherwise the target method is first processed by region selection, and then it is \ndetermined whether or not it is inlinable based on this reduced code size. If inlinable, it performs \ninlining only for the non-rare part of the code, and the current cost is updated with the reduced size \nof the method. Note that the devirtualization of the dynamically dispatched call sites is also performed \nduring the inlining process based on class hierarchy analysis and the receiver type distribution profile \ncol\u00adlected in the previous runs, and region selection can successfully remove the backup path which otherwise \nneeds to be generated. One tricky part of this process is to update the live variable infor\u00admation in \neach special instruction provided in the RE-BB for the method being inlined. There are two things that \nneed to be done. One is to rename those live variables by reflecting the mapping into the caller s context, \njust like other local variable conversions when inlined. The other is to add the live variables at the \ncall site to reflect the complete set of live variables at each point. This is necessary to automatically \nhide the effect of all optimizations such as copy propagation, constant propagation, and any other program \ntransformations. The direct advantage of the partial inlining in our framework is twofold: 1. Since we \nfirst remove the rarely executed paths before trying to find the next inlining candidate, we never inline \nmethods at call sites within rare portions of code, since they are no longer included in the current \nscope. This avoids performing inlining at performance-insensitive call sites and can conserve the inlining \nbudget. 2. Methods being inlined are first processed through intra\u00admethod region selection before actual \ninlining. Inlining is con\u00adsidered and carried out against the reduced target code after removing the \nrare portions of the code, and this contributes to conserving the inlining budget.  Assuming the current \nset of criteria for method inlining is reason\u00adable and thus fixed, we can then use the saved budget from \nthe above steps and try to inline other methods in the call tree, which is expected to be more effective \nand can contribute to further im\u00adproving performance. Other indirect benefits due to partial inlin\u00ading \ninclude that 1) instruction cache locality can be improved since non-rare parts of the code tend to be \nbetter packed into the same compilation unit, and 2) later optimizations in the compila\u00ad /*setup for \nroot method */ regionSelection ( root_method ) construct (a possibly large) call_graph G set total_budget \nB, set current_cost C = 0 /* actual inlining pass */ do { select a call edge E in call_graph G {  M \n= target method of E  if ( is_tiny (M) ) { /* tiny methods are always inlined */ perform inlining for \nM } else {   /* otherwise, decision is made after region selection */ M = regionSelection (M)  if \n( inlinable (M ) ) { perform inlining M update live information for each RE-BB in M C += cost (M ) } \n} } while ( C < B ) Figure 4. Algorithm for interacting intra-procedural region selection and inlining \nprocess, leading to partial inlining. tion process can be more effective with increased optimization \nscope. 3.3 Optimizations All of the analyses and optimizations that follow partial inlining can proceed \nnormally. The operands in the special instruction OPC_RECOMPILE provided in the RE-BB for each region \nexit point work as anchors to preserve the necessary variables, which can be renamed by another variables \nor replaced by constants dur\u00ading the optimization phases. The RE-BB serves as a placeholder for any optimizations \nthat require generating compensation code for an exiting path. As special optimizations that can take \nadvan\u00adtage of the selected region, we implemented partial dead code elimination and partial escape analysis, \nas described in [29]. Partial dead code elimination. We have to keep all live variables (both local and \nstack) in the bytecode-level code at each RE-BB to be able to restore the JVM state correctly when a \nregion exit oc\u00adcurs at runtime. This means the live range of some of those vari\u00adables becomes longer \nthan in the FBC approach. For example, common subexpression elimination can make some variables un\u00adreferenced, \nand they are eliminated as dead code in the FBC. But those variables may have to be passed on region \nexit if they are used later at the bytecode level. This problem is expressed in the final code as increased \nregister pressure, extra instructions to spill into memory in the non-rare paths, and larger frame sizes. \nPartial dead code elimination [18] can partly alleviate this prob\u00adlem by pushing computations that are \nonly live in region exit paths into the RE-BBs. Our implementation of partial dead code elimi\u00adnation \nis a simple code motion followed by dead code elimination. We maintain two sets of live variables, one \nfrom the RE-BBs and the other from the non-rare paths. Using a standard code motion algorithm, we then \nmove computations whose defined variables lv1 = opc_new class lv1 = opc_stack2heap lv1 opc_recompile \nlv1, ... region exit basic block (RE-BB) opc_putfield lv1, lv2  Figure 5. An example of partial escape \nanalysis and the compensation code generation. are included in the set from the RE-BBs but not included \nin the other set. The computations are copied once into both the appro\u00adpriate RE-BB and the non-rare \npath in the other branch direction, but the copy in the non-rare path can then be eliminated in the fol\u00adlowing \ndead code elimination phase. Partial escape analysis. Escape analysis and its application to stack object \nallocation, scalar replacement, and synchronization elimination is very effective for improving performance. \nHowever, quite often it suffers from the fact that objects escape from only rarely executed code, especially \nfrom the backup path of a devir\u00adtualized method call, and thus its effectiveness with the FBC ap\u00adproach \nhas been limited in practice. We modified the escape analysis to simply ignore region exit points, so \nthat it can analyze whether or not objects can be escaped only for non-rare paths in the target code. \nOur escape analysis is based on the algorithm described in [28]. It is a compositional analysis designed \nto analyze each method inde\u00adpendently and to produce a parameterized analysis summary result that can \nbe used at all of the call sites that may invoke the method. Hence the analysis result can be more precise \nand complete as more of the invoked methods are analyzed. However, if the analy\u00adsis result is summarized \nbased on the optimistic assumption with rare regions ignored, then some of its caller methods may con\u00adclude \nthat the object passed as a parameter is non-escaping. When the execution exits from a region boundary, \nwe have to recompile not only the current method but those caller methods that used the summary information \nas well. Therefore, we check whether argu\u00adments are included in the list of live variables at any region \nexit point within the method, and suppress generating summary infor\u00admation if that is the case. In the \nfinal stage of the analysis, we check each of the objects identified as stack allocatable or replaced \nby scalar variables as to whether it is included in the list of live variables at each region exit point, \nand if it is, generate a special instruction OPC_STACK2HEAP in the RE-BB as compensation code. Fig\u00adure \n5 shows a graphic example of this. When executed, this in\u00adstruction calls a runtime that performs 1) \nallocation of the object on the heap and its initialization, 2) copying of the object content from stack \nto heap or copying of the scalar-replaced variables which are also included in the list of live variables \nat the current exit point, and 3) synchronization on the allocated object, if that operation has been \neliminated in the non-rare paths but is neces\u00adsary at the exit point.  4. EXPERIMENTAL RESULTS This \nsection presents some experimental results showing the effec\u00adtiveness of the RBC in our dynamic compilation \nsystem. We out\u00adline our experimental methodology first, describing the configura\u00adtions used in the evaluation, \nand then present and discuss our measurement results. 4.1 Benchmarking Methodology All the performance \nresults presented in this section were obtained on an IBM IntelliStation M Pro 6850 (Pentium4 Xeon 2.8 \nGHz uni-processor with 1,024 MB memory), running Windows XP, and using the JVM of the IBM Developer Kit \nfor Windows, Java Technology Edition, version 1.3.1 prototype build. The bench\u00admarks we chose are SPECjvm98-1.04 \nand SPECjbb2000-1.02 [23]. SPECjvm98 was run in interactive mode with the default large input size, and \nin autorun mode 10 times for each test, with the initial and maximum heap sizes of 128 MB. Each distinct \ntest was run with a separate JVM. SPECjbb2000 was run in the fully compliant mode with 1 to 8 warehouses, \nwith the initial and maximum heap sizes of 256 MB. The threshold in the MMI to initiate level-0 compilation \nwas set to 500. The timer interval for the sampling profiler for detecting hot methods was 3 milliseconds. \nThe number of samples to be col\u00adlected in the instrumentation-based profiler was set to 10,000. The threshold \nof the numbers of OSRs to redirect the future method invocations to recompiled code is set to 10. The \nfive configurations are compared, one with the FBC approach, and the others are RBC approaches with variations \nin optimiza\u00adtions and the region selection process as listed below. The baseline of the comparison is \nwith the current FBC approach. 1. RBC-noopt: This is the RBC approach, but disabling partial escape analysis, \npartial dead code elimination, and partial inlining. 2. RBC-nopi: The same setting as RBC-noopt, but \nenabling the partial escape analysis and partial dead code elimination only. This is to evaluate the \neffectiveness of partial inlining. In this and the above cases, the region selection process is performed \nfor the resulting code after normal method inlining. 3. RBC-full: This is the RBC approach, with all \nthree optimiza\u00adtions enabled. 4. RBC-offline: This is the same as RBC-full, but using offline collected \nprofile results instead of online results. This is to evaluate the maximum potential of the RBC approach \nif we could have complete profile information. There should be no recompilation or OSR events during \nexecution.  In our current implementation of partial inlining, we perform re\u00adgion selection for an inline \ntarget method and estimate the cost for the reduced target code, but temporarily inline the entire body \nof the method. The rare portion of the code identified in the region selection is removed immediately \nafter the whole inlining process. This should not greatly affect the compilation time or the code size, \nbut the compile-time peak work memory usage can look larger than it should be with the complete implementation. \n Benchmarks mtrt jess compress db mpeg jack javac SPECjbb Total number of methods executed 455 734 325 \n321 495 561 1,085 2,818 Level-0 compiled methods 200 334 135 126 219 373 825 566 Level-1 / level-2 compiled \nmethods 33 41 7 7 56 81 157 166 Region-based optimized methods 28 22 1 6 19 58 101 123 Number of region \nexit points Profile identified rare path 3 12 1 7 2 7 14 35 Devirtualized backup path 483 36 0 19 16 \n136 644 1,244 Exception throwing path 19 22 0 7 33 114 169 171 Handler block 2 3 0 6 0 69 54 96 Uninitialized \ncode path 0 0 0 0 0 1 0 4 Recompiled methods due to region exit 0 0 0 0 1 9 4 1 Number of on-stack replacement \n0 0 0 0 6 52 34 10 Table 1. Statistics of the region-based compilation for benchmark runs with RBC-full. \nThe top three rows show execution and compilation statistics, the middle six rows are numbers of RBC \noptimized methods and how those rare region are identified, and the bottom two show runtime behavior \nfor recompilations and on-stack replacements. 24.7%  4.2 Statistics on Region-Based Compilation Table \n1 shows the statistics when running the benchmarks with 12  RBC-full. The second to fourth rows are \nexecution and compila- Percent improvement over FBC 10 tion statistics, showing the total number of methods \nexecuted for each test, the total number of methods compiled with level-0, and the total number of methods \ncompiled with level-1 and level-2, re\u00ad spectively. The last number, level-1 and level-2 compiled methods, \nare the target of RBC-based optimizations. Out of these methods, the fifth row shows the actual number \nof methods where rare re\u00adgions were identified and RBC optimizations were performed. The next five rows \nshow the breakdown of how those rare regions 8 6 4  2 were identified in the region selection process, \nthat is, the number of exit points for each rare type, based on profile results or on some heuristics. \nThe bottom two rows show the number of meth\u00adods recompilation has been triggered by region exits, and \nthe total number of OSR events that occurred, respectively. Except for compress, quite a few methods \nwere RBC optimized. The number was roughly 50% to 80% of the level-1 and level-2 compiled methods (except \nfor mpegaudio, which was about 30%), showing that many benchmarks contain rare regions even in hot methods \nand we can do some optimizations for these methods. The backup paths for the devirtualized method invocations \nis the most common kind of rare region identified, followed by the ex\u00adception throwing path. The majority \nof rare regions are identified on the basis of static heuristics, and the numbers of profile identi\u00adfied \nrare paths are relatively small. This is because profile results are not always available for RBC optimization \ntarget methods as described in Section 3, and because we remain conservative when working from the profile \nresults, considering the fact that our pro\u00adfiles are based on the samples collected for short intervals \nof pro\u00adgram execution. The numbers of recompiled methods and OSRs is relatively large in jack and javac \ncompared to the other benchmarks. These benchmarks are known to frequently raise exceptions during the \nprogram execution, and control frequently exited from certain re\u00adgion boundary points. But the number \nof recompiled methods was still kept within a reasonable level, around 15% of the total num\u00ad mtrt jess \ncomp db mpeg jack javac jbb G.M. -2 -4 Figure 6. Performance improvement with four RBC ap\u00adproaches over \nthe FBC. Taller bars show better scores. ber of RBC-optimized methods. The number of OSRs is con\u00adstrained, \nowing to the mechanism of dynamic OSR counting and the control of future invocations for the recompiled \nmethods based on those counts. The other benchmarks show none or very small numbers of recompilations \nand OSRs.  4.3 Performance Figure 6 shows the performance improvements of the four varia\u00adtions of the \nRBC over the current FBC approach. We took the best time from 10 repetitive autoruns for each test in \nSPECjvm98, and the best throughput from a series of successive executions from 1 to 8 warehouses for \nSPECjbb2000. The figure shows that both RBC-full and RBC-offline perform significantly better than FBC \nfor some benchmarks, with a 25% improvement for mtrt, and 5 to 7% improvement on average. The majority \nof the performance gain for mtrt comes from the elimination of backup paths for devirtualized method \ncalls and its exploitation by the partial escape analysis. Mtrt has many virtual 0 method invocations, \nmost of which are devirtualized and the target methods are inlined. These methods are never overridden \nand re\u00adtaining the backup paths for dynamic class loading just prevents the escape analysis from working \nwell. However, simply removing the backup paths as a part of region selection does not solve this problem. \nWithout partial escape analysis, the performance actually degrades as shown in RBC-noopt, because the \nescape analysis now has to treat region exit points as globally escaping points for all live variables, \nnot only for the variables passed as arguments as in the original virtual invocation call sites. As a \nresult, more ob\u00adjects are analyzed as escaping, and the number of stack allocated or scalar variable \nreplaced objects is decreased. For the other benchmarks, the performance difference with RBC\u00adnoopt and \nRBC-nopi seems not very significant, meaning that simply eliminating data flow merge points from rare \nregions is not generally very effective by itself for a wide range of benchmarks. The partial inlining, \non the other hand, can contribute to a signifi\u00adcant improvement for some benchmarks, especially for jess \nand jack, as the difference from RBC-nopi to RBC-full shows. The large improvement with partial inlining \nin jack, for example, results from the additional inlining performed in the partial inlin\u00ading process \nwhich then allows the escape analysis to recognize some frequently allocated objects as captured and \nmakes those ob\u00adjects stack allocated in one of the core methods of the benchmark. This is a good example \nof the indirect effect of partial inlining. 4.4 Compilation Overhead Figure 7 shows the ratio of compilation \noverhead with the four RBC variations over FBC using three metrics: the compilation time, the compiled \ncode size, and the compilation time peak work memory usage. Smaller bars mean better scores in these \nfigures. We measured only level-1 and level-2 overhead, since level-0 is shared among all configurations. \nAll these figures for the three RBC configurations using online profile include additional over\u00adhead \nthat results from the recompilations if region exits occur at runtime. The peak memory usage is the maximum \namount of memory allocated for compiling methods. Since our memory man\u00adagement routine allocates and \nfrees memory in 1 Mbyte blocks, this large granularity masks the minor differences in memory us\u00adage and \ncauses the results of the figure to form clusters. Overall the RBC approaches show significant advantages \nover the current FBC approach in most of the benchmarks. In particular, the reductions sometimes exceed \n60% (such as code size for jess and db), and are between 20% and 30% on average, depending on the benchmarks \nand metrics of the overhead. This significant re\u00adduction in compilation overhead is not surprising for \nRBC-noopt and RBC-nopi, since the rare regions identification and their re\u00admoval from the target code \nis performed after completing the nor\u00admal inlining process, and all the optimizations and code genera\u00adtion \nare executed for this smaller target code. The reduction of overhead for RBC-full is slightly less dramatic, \nbut it still shows a significant reduction from FBC overall. An increase in peak work memory usage with \nRBC-full and RBC-offline for some bench\u00admarks is caused by a problem in our current implementation of \npartial inlining, in temporarily inlining the entire body of target methods. We intend to fix this problem \nin the future. The middle graph of Figure 7 shows the code size reduction over FBC, but RBC actually \nrequires more runtime memory space for Ratio of peak work memory usage over FBC Ratio of compiled code \nsize over FBC Ratio of compilation time over FBC 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 1 0.9 0.8 0.7 \n0.6 0.5 0.4 0.3 0.2 0.1 0 1.2 1 0.8 0.6 0.4 0.2 0 Compilation Time Ratio mtrt jess comp db mpeg jack \njavac jbb G.M. Compiled Code Size Ratio mtrt jess comp db mpeg jack javac jbb G.M. Peak Work Memory \nUsage Ratio mtrt jess comp db mpeg jack javac jbb G.M. RBC-noopt RBC-nopi RBC-full RBC-offline Figure \n7. Ratio of three metrics of compilation overhead with four RBC approaches compared to FBC. Smaller bars \nmean better scores. The top figure shows the compilation time ratio, the middle shows the ratio of compiled \ncode size, and the bot\u00adtom shows the ratio of compilation time peak work memory usage. the map of each \nregion s exit points and for the runtime structure holding the inline context of each method. The structure \nshowing the inline context is shared for other purposes in our implementa\u00adtion, such as by the exception \nhandling system, so the additional overhead specific to the RBC is only the map. The size of the map \ndepends on the number of live local and stack variables for each region exit point, but it typically \nrequires around 50 to 70 bytes per exit point. Even if we take this map space into consideration, the \ntotal size is still well under the FBC code size for most of the benchmarks.  5. DISCUSSION Overall, \nthis study shows the advantages of the RBC approach in both performance and compilation overhead over \nthe traditional FBC approach. It shows the potential for significantly reducing the compilation overhead, \nmeasured in time, work memory, and code size, and for improving performance. In a dynamic compila\u00adtion \nenvironment, we have to be very careful in performing any optimizations that have significant impact \non compilation over\u00adhead, so RBC is a promising strategy for dynamic compilers. As expected, RBC-offline \nshows better performance with smaller compilation overhead over RBC-full for several benchmarks, but \noverall our current online RBC strategy seems to compete well against RBC using the offline profiles. \nThe online profiling com\u00adbined with static heuristics currently provides useful information to identify \nrarely or never executed blocks of code, as compared to the offline-collected profile results. Currently \nwe disable exception directed optimization (EDO) [20]. This is a technique to monitor frequently raised \nexception paths and to optimize them by inlining and converting exception throw\u00ading instructions to simple \njump instructions into their correspond\u00ading handlers. This is complementary to our RBC approach, since \nit effectively eliminates frequently excepting instructions from the current control flow, before treating \nthem as rare paths in our static region identification heuristics, and we can ensure the re\u00admaining exception \nthrowing instructions are truly rare. As shown in Section 4.2, most of the recompilation and OSR occurring \nin our current implementation is due to region exits from exception paths, so this optimization is expected \nto decrease the probability of region exits without reducing the effectiveness of RBC ap\u00adproach. As described \nin Section 2.2, it may be useful to support several options for OSR and employ them depending on the \ncharacteris\u00adtics of each region exit point how the rare paths are eliminated. For example, the current \nstrategy of recompilation with the same optimization level works fine for the exit points of a devirtualized \ncall site backup path, because once the control escapes from one of those exit points due to a dynamic \nclass loading, it will most likely escape from this exit point in subsequent executions. On the other \nhand, when the region exit occurs from an exception throw\u00ading path, it may be sufficient to fall back \nto the interpreter, since exception paths (especially after EDO) need not be optimized, considering the \ninherently high overhead of runtime exception handling. We can still collect the dynamic counts of OSRs \nto iden\u00adtify from which region exit points the control is frequently escap\u00ading, and thereby drive recompilation, \nrather than waiting for the promotion to be performed by the underlying recompilation sys\u00adtem. This mechanism \nmay allow more aggressive rare path elimi\u00adnation than we currently use. We can explore further opportunities \nfor identifying rarely exe\u00adcuted code to increase the effectiveness of the RBC approach. For example, \nloop versioning is an optimization technique for hoisting the array bound exception checking code for \nan individual array outside a loop by providing two copies of the loop: the safe loop, where exception \nchecking code is retained as in the original loop, and an unsafe loop, where all array exception checking \ncode is eliminated. Guard code is provided to examine the whole range of the index against the bound \nof the arrays accessed within the loop, and depending on the result of this test, either the safe or \nunsafe loop is selected at runtime. This is an effective optimization, but entails a significant code \nsize increase. It is expected that the safe loop is rarely or never executed in most of the cases, and \nwe could have a significant code size reduction if we can integrate this op\u00adportunity into the RBC strategy. \nWe could even use on-the-fly safe loop generation when the test in the entry guard code fails. Method \nsplitting, or procedure splitting [22] is a technique that can complement our RBC strategy. This is to \nplace relatively in\u00adfrequent code apart from common code, typically in a separate page, in order to improve \ninstruction cache locality. Our region se\u00adlection process does not identify these relatively infrequent \nbut still executed portions of the code, since over-aggressive region selection will lead to too many \nrecompilations and can degrade performance. In other word, the selected region still contains rela\u00adtively \ninfrequent code. We can increase the code locality even more by integrating the method splitting technique \ninto our framework. 6. RELATED WORK As described earlier, Hank et al. [14] first described the problems \nof the conventional function-based compilation strategy and dem\u00adonstrated the potential of the region-based \ncompilation technique by showing several experimental results, including static code size savings. The \nproposed region formation was to perform an aggres\u00adsive (possibly an over-aggressive) inlining pass first, \nfollowed by a partitioning phase that created new regions based on heuristics us\u00ading offline profile \nresults. Way et al. [26] improved this region formation algorithm to make it scalable by combining region \nse\u00adlection and the inlining process, and reduced the compilation time memory requirements considerably. \nThey also evaluated region\u00adbased partial inlining that was performed through partial cloning, and observed \nsmall performance improvements [27]. All this work was done in an ILP static compiler environment, so \nit required two additional steps, encapsulation and reintegration, to make regions look like ordinary \nfunctions for optimizations and then to reinte\u00adgrate them into the containing function. The SELF-91 system \n[7] uses a technique called deferred compila\u00adtion for uncommon branches where a skewed execution frequency \ndistribution can be expected. They use type information to defer compilation for messages sent to receiver \nclasses that are pre\u00adsumed to be rare. When the rare path is actually executed, the compiler generates \ncode for the uncommon branch extension, which is a continuation of the original compiled code from the \npoint of failure to the end of the method. The extension is unopti\u00admized to avoid recursive uncommon \nbranches, and reuses the stack frame created for the original common-case version. It was demonstrated \nthat the technique increases compilation speed sig\u00adnificantly, by nearly an order of magnitude, but there \nwere both performance and code size problems when the compiler s uncom\u00admon code predictions were wrong. \nThe SELF-93 system [15] fixed these problems by treating the occurrence of uncommon cases as another \nform of runtime feedback and replacing overly specialized code with less specialized code rather than \njust extend\u00ading the specialized code with an unoptimized extension code. This approach was made possible \nby introducing both an OSR tech\u00adnique and an adaptive recompilation system. The OSR allowed the dynamic \ndeoptimization of the target code and the replacement of the stack frame containing the uncommon trap \nwith several unop\u00adtimized frames. When it was found that this unoptimized compiled code was executed \nfrequently, the recompilation system could op\u00adtimize the method again based on the feedback from the \nunopti\u00admized code. The HotSpot server [21] is a JVM product implementing an adap\u00adtive optimization system \nwith an interpreter to allow a mixed exe\u00adcution environment. As in the SELF-93 system, it also employed \nthe uncommon trap mechanism to avoid code generation for un\u00adcommon cases. The system always falls back \nto the interpreter at a safe point after converting the stack frame when an uncommon path is actually \ntaken. Both the SELF-93 and HotSpot systems have some similarities to ours, but the important differences \nare that our technique deals with a broader set of rare regions using both static heuristics and profiling \nresults, not focusing only on uncommon virtual method targets and references to an uninitial\u00adized class, \nand that we perform partial inlining to make the region\u00adselection process affect inlining decisions. \nWhaley [29] described a technique for performing partial method compilation using basic-block-level offline \nprofile information. The technique allows most optimizations to completely ignore rare paths and fully \noptimize the common cases. This system also as\u00adsumes falling back to an interpreter when the rare path \nis taken. Since the technique is not fully implemented in a working system, he estimated the effectiveness \nof the technique by collecting basic\u00adblock-level profiles offline and then using this information to \nrefactor the affected classes with a Bytecode Engineering Library tool. The interpreter transfer points \nat rare block entry are replaced with method calls to synthetic methods that contain all the code separated \nfrom the transfer point, so that the compilation of those rarely executed blocks can be avoided. Thus \nthe result is an ideal case, since the compiler need not hold any information to restore the interpreter \nstate. Fink and Qian [13] described a new, relatively compiler inde\u00adpendent mechanism for implementing \nOSR, and apply this tech\u00adnique to integrate the deferred compilation strategy in the Jikes RVM adaptive \noptimization system. Since they do not yet imple\u00adment optimizations that take advantage of the deferred \ncompila\u00adtion, the performance improvement is small, but the compilation time and code size show modest \nimprovements. There are several binary translation systems for profile-based na\u00adtive-to-native reoptimization, \nsuch as Dynamo [3], its descendent DynamoRIO [6], HCO [11], and mojo [9]. These systems identify frequently \nexecuted paths (traces) and optimize them by exploiting code layout and other runtime optimization opportunities. \nSince the trace is a single-entry multiple-exit contiguous sequence, and can extend across static program \nboundaries, it has arbitrary sub\u00admethod and cross-method granularity as the unit of optimization, similar \nto the effect with our partial inlining. However, they oper\u00adate only on a single trace at a time, and \nthe optimization can be less effective when the selected trace is hot but not a dominant one. This may \nnot be a problem for these systems, since the statically optimized generic code is already there and \na specialized reopti\u00admized version is being generated. We need to be more conserva\u00adtive not to frequently \nexit from selected regions, and thus employ more general regions to contain arbitrary numbers of hot \ntraces by removing rarely executed code blocks. Trace scheduling [19] is a technique that predicts the \noutcome of conditional branches and then optimizes the code assuming the prediction is correct, but it \ncan suffer from the complexity in\u00advolved in the compensation code generation. Superblock schedul\u00ading \n[16] simplifies the complexity by using tail duplication to cre\u00adate superblocks, single-entry multiple-exit \nregions. Both of these techniques are to extend the scope of ILP scheduling beyond basic block boundaries \nto encompass larger units. Other classic optimi\u00adzations are also extended to exploit superblocks in [8]. \nBruening and Duesterwald [5] explored the issues of finding op\u00adtimal compilation unit shapes for an embedded \nJava JIT compiler. They demonstrated that method boundaries are a poor choice for compilation. They did \nnot implement a working JIT compiler for evaluation, and only provided estimates of the code size benefits \nwhen using trace-based and loop-based strategies for compilation units. They found that a majority of \nthe code in methods is rarely or never executed, and concluded that code size can be reduced drastically \nwith only a negligible change in performance. Ball and Larus [4] proposed a heuristic approach for static \nbranch prediction based on the data types and type of comparison used in the branch and the code in the \ntarget basic blocks. We also use program-based static heuristics for the region selection, and propagate \nthe rare/non-rare information along backward data flow to identify rarely or never executed regions. \n 7. CONCLUSIONS We have described the design and implementation of a region\u00adbased compilation technique \nin our dynamic compilation system for Java. We presented our design decisions for handling the re\u00adgion \nexit points and for the intra-procedural region selection and its integration in the inlining process. \nWe implemented this RBC framework and evaluated it using the industry standard bench\u00admarks. The experimental \nresults show the potential to achieve bet\u00adter performance and improved compilation overhead in compari\u00adson \nto the traditional FBC approach. In the future, we plan to fur\u00adther study the effectiveness of region-based \ncompilation tech\u00adniques, especially by exploring other optimization opportunities that can take advantage \nof selected regions and by providing sev\u00aderal options for region exit handling. 8. ACKNOWLEDGEMENTS \nWe would like to thank all the members of the Network Comput\u00ading Platform group in IBM Tokyo Research \nLaboratory for helpful discussions and comments. In particular, we thank Motohiro Kawahito for implementing \nthe partial dead code elimination rou\u00adtine. The anonymous reviewers also provided many valuable sug\u00adgestions \nand comments to improve the presentation of the paper.  REFERENCES [1] M. Arnold, S. Fink, D. Grove, \nM. Hind, and P.F. Sweeney. Adaptive Optimizations in the Jalape\u00f1o JVM. In Proceed\u00adings of the ACM SIGPLAN \nConference on Object-Oriented Programming, Systems, Languages &#38; Applications, pp. 47\u00ad65, Oct. 2000. \n[2] M. Arnold, M. Hind, and B.G. Ryder. Online Feedback-Directed Optimization of Java. In Proceedings \nof the ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages &#38; Applications, \npp. 111-129, Nov. 2002. [3] V. Bala, E. Duesterwald, and S. Banerjia. Dynamo: A Trans\u00adparent Dynamic \nOptimization System. In Proceedings of the ACM SIGPLAN Conference on Programming Language De\u00adsign and \nImplementation, pp. 1-12, Jun. 2000. [4] T. Ball, and J.R. Larus. Branch Prediction For Free. In Pro\u00adceedings \nof the ACM SIGPLAN Conference on Programming Language Design and Implementation, pp. 300-313, Jun. 1993. \n[5] D. Bruening, and E. Duesterwald. Exploring Optimal Compi\u00adlation Unit Shapes for an Embedded Just-In-Time \nCompiler. In Proceedings of the ACM SIGPLAN Workshop on Feed\u00adback-Directed and Dynamic Optimization, \n2000. [6] D. Bruening, T. Garnett, and S. Amarasinghe. An Infrastruc\u00adture for Adaptive Dynamic Optimization. \nIn Proceedings of the ACM SIGPLAN Conference on Code Generation and Op\u00adtimization, pp. 265-275, Mar. \n2003. [7] C. Chambers and D. Ungar. Making Pure Object-Oriented Languages Practical. In Proceedings of \nthe ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages &#38; Applications, pp. \n1-15, Oct. 1991. [8] P.P. Chang, S.A. Mahlke, and W.M. Hwu. Using Profile In\u00adformation to Assist Classic \nCode Optimizations. Software Practice and Experience, 21(12), pp. 1301-1321, Dec. 1991. [9] W.K. Chen, \nS. Lerner, R. Chaiken, and D.M. Gillies. Mojo: A Dynamic Optimization System. In Proceedings of the ACM \nSIGPLAN Workshop on Feedback-Directed and Dynamic Optimization, 2000. [10] M. Cierniak, G.Y. Lueh, and \nJ.M. Stichnoth. Practicing JUDO: Java Under Dynamic Optimizations. In Proceedings of the ACM SIGPLAN \nConference on Programming Lan\u00adguage Design and Implementation, pp. 13-26, Jun. 2000. [11] R. Cohn, and \nP.G. Lowney. Hot Cold Optimization of Large Windows/NT Applications. In Proceedings of 29th Interna\u00adtional \nConference on Microarchitecture, MICRO-29, pp. 80\u00ad89, Dec. 1996. [12] D. Detlefs, and O. Agesen. Inlining \nof Virtual Methods. In the 13th European Conference on Object-Oriented Program\u00adming, ECOOP, LNCS 1628, \npp. 258-277, 1999. [13] S. Fink, and F. Qian. Design, Implementation and Evaluation of Adaptive Recompilation \nwith On-Stack Replacement. In Proceedings of the ACM SIGPLAN Conference on Code Generation and Optimization, \npp. 241-252, Mar. 2003. [14] R.E. Hank, W. Hwu, and B.R. Ran. Region-Based Compila\u00adtion: An Introduction \nand Motivation. In Proceedings of 28th International Conference on Microarchitecture, MICRO-28, pp. 158-168, \nDec. 1995. [15] U. H\u00f6lzle. Adaptive Optimization for SELF: Reconciling High Performance with Exploratory \nProgramming. Ph.D. Thesis, Stanford University, CS-TR-94-1520, Aug. 1994. [16] W.M. Hwu, S.A. Mahlke, \nW.Y. Chen, P.P. Chang, N.J. Warter, R.A. Bringmann, R.G. Ouellete, R.E. Hank, T. Kiyohara, G.E. Haab, \nJ.G. Holm, and D.M. Lavery. The Su\u00adperblock: An Effective Technique for VLIW and Superscalar Compilation. \nThe Journal of Supercomputing, 7(1-2), pp. 229-248, May 1993. [17] K. Ishizaki, M. Kawahito, T. Yasue, \nH. Komatsu, and T. Nakatani. A Study of Devirtualization Techniques for a Java Just-In-Time Compiler. \nIn Proceedings of the ACM SIG-PLAN Conference on Object-Oriented Programming, Sys\u00adtems, Languages &#38; \nApplications, pp. 294-310, Oct. 2000. [18] J. Knoop, O. R\u00fcthing, and B. Steffen. Partial Dead Code Elimination. \nIn Proceedings of the ACM SIGPLAN Confer\u00adence on Programming Language Design and Implementation, pp. \n147-158, Jun. 1994. [19] P.G. Lowney, S.M. Freudenberger, T.J. Karzes, W.D. Lichtenstein, R.P. Nix, J.S. \nO Donnell, and J.C. Ruttenberg. The Multiflow Trace Scheduling Compiler. The Journal of Supercomputing, \n7(1-2), pp. 51-142, Jan. 1993. [20] T. Ogasawara, H. Komatsu, and T. Nakatani. A Study of Ex\u00adception \nHandling and Its Dynamic Optimization in Java. In Proceedings of the ACM SIGPLAN Conference on Object-Oriented \nProgramming, Systems, Languages &#38; Applications, pp. 83-95, Oct. 2001. [21] M. Paleczny, C. Vick, \nand C. Click. The Java HotSpot Server Compiler. In Proceedings of the Java Virtual Machine Re\u00adsearch \nand Technology Symposium (JVM 01), pp. 1-12, Apr. 2001. [22] K. Pettis and R.C. Hansen. Porfile Guided \nCode Positioning. In Proceedings of the ACM SIGPLAN Conference on Pro\u00adgramming Language Design and Implementation, \npp. 16-27, Jun. 1990. [23] Standard Performance Evaluation Corporation. SPECjvm98 available http://www.spec.org/osg/jvm98, \nand SPECjbb2000 available at http://www.spec.org/osg/jbb2000. [24] T. Suganuma, T. Yasue, M. Kawahito, \nH. Komatsu, and T. Nakatani. A Dynamic Optimization Framework for a Java Just-In-Time Compiler. In Proceedings \nof the ACM SIG-PLAN Conference on Object-Oriented Programming, Sys\u00adtems, Languages &#38; Applications, \npp. 180-194, Oct. 2001. [25] T. Suganuma, T. Yasue, and T. Nakatani. An Empirical Study of Method Inlining \nfor a Java Just-In-Time Compiler. In Proceedings of the Java Virtual Machine Research and Technology \nSymposium (JVM 02), pp. 91-104, Aug. 2002. [26] T. Way, B. Breech, and L. Pollock. Region Formation Analy\u00adsis \nwith Demand-driven Inlining for Region-based Optimiza\u00adtion. In Proceedings of the Conference on Parallel \nArchitec\u00adture and Compilation Technique, pp. 24-36, Oct. 2000. [27] T. Way, and L. Pollock. Evaluation \nof a Region-based Partial Inlining Algorithm for an ILP Optimizing Compiler. In Pro\u00adceedings of the Conference \non Parallel and Distributed Processing Techniques and Applications (PDPTA2002), pp. 552-556, Jun. 2002. \n[28] J. Whaley and M. Rinard. Compositional Pointer and Escape Analysis for Java Programs. In Proceedings \nof the ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages &#38; Applications, \npp. 187-206, Nov. 1999. [29] J. Whaley. Partial Method Compilation using Dynamic Pro\u00adfile Information. \nIn Proceedings of the ACM SIGPLAN Con\u00adference on Object-Oriented Programming, Systems, Lan\u00adguages &#38; \nApplications, pp. 166-179, Oct. 2001. \n\t\t\t", "proc_id": "781131", "abstract": "Method inlining and data flow analysis are two major optimization components for effective program transformations, however they often suffer from the existence of rarely or never executed code contained in the target method. One major problem lies in the assumption that the compilation unit is partitioned at method boundaries. This paper describes the design and implementation of a region-based compilation technique in our dynamic compilation system, in which the compiled regions are selected as code portions without rarely executed code. The key part of this technique is the region selection, partial inlining, and region exit handling. For region selection, we employ both static heuristics and dynamic profiles to identify rare sections of code. The region selection process and method inlining decision are interwoven, so that method inlining exposes other targets for region selection, while the region selection in the inline target conserves the inlining budget, leading to more method inlining. Thus the inlining process can be performed for parts of a method, not for the entire body of the method. When the program attempts to exit from a region boundary, we trigger recompilation and then rely on on-stack replacement to continue the execution from the corresponding entry point in the recompiled code. We have implemented these techniques in our Java JIT compiler, and conducted a comprehensive evaluation. The experimental results show that the approach of region-based compilation achieves approximately 5% performance improvement on average, while reducing the compilation overhead by 20 to 30%, in comparison to the traditional function-based compilation techniques.", "authors": [{"name": "Toshio Suganuma", "author_profile_id": "81100185644", "affiliation": "IBM Tokyo Research Laboratory, 1623-14 Shimotsuruma, Yamato-shi, 242-8502 Japan", "person_id": "PP14074580", "email_address": "", "orcid_id": ""}, {"name": "Toshiaki Yasue", "author_profile_id": "81100210831", "affiliation": "IBM Tokyo Research Laboratory, 1623-14 Shimotsuruma, Yamato-shi, 242-8502 Japan", "person_id": "PP14082638", "email_address": "", "orcid_id": ""}, {"name": "Toshio Nakatani", "author_profile_id": "81100311827", "affiliation": "IBM Tokyo Research Laboratory, 1623-14 Shimotsuruma, Yamato-shi, 242-8502 Japan", "person_id": "PP14113792", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/781131.781166", "year": "2003", "article_id": "781166", "conference": "PLDI", "title": "A region-based compilation technique for a Java just-in-time compiler", "url": "http://dl.acm.org/citation.cfm?id=781166"}