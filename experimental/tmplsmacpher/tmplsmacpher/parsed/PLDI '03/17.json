{"article_publication_date": "05-09-2003", "fulltext": "\n Automatically Proving the Correctness of Compiler Optimizations Sorin Lerner Todd Millstein Craig Chambers \nDepartment of Computer Science and Engineering University of Washington {lerns,todd,chambers}@cs.washington.edu \nABSTRACT We describe a technique for automatically proving compiler optimizations sound, meaning that \ntheir transformations are always semantics-preserving. We .rst present a domain\u00adspeci.c language, called \nCobalt, for implementing optimiza\u00adtions as guarded rewrite rules. Cobalt optimizations operate over a \nC-like intermediate representation including unstruc\u00adtured control .ow, pointers to local variables and \ndynami\u00adcally allocated memory, and recursive procedures. Then we describe a technique for automatically \nproving the sound\u00adness of Cobalt optimizations. Our technique requires an au\u00adtomatic theorem prover to \ndischarge a small set of simple, optimization-speci.c proof obligations for each optimiza\u00adtion. We have \nwritten a variety of forward and backward intraprocedural data.ow optimizations in Cobalt, includ\u00ading \nconstant propagation and folding, branch folding, full and partial redundancy elimination, full and partial \ndead assignment elimination, and simple forms of points-to analy\u00adsis. We implemented our soundness-checking \nstrategy using the Simplify automatic theorem prover, and we have used this implementation to automatically \nprove our optimiza\u00adtions correct. Our checker found many subtle bugs during the course of developing \nour optimizations. We also imple\u00admented an execution engine for Cobalt optimizations as part of the Whirlwind \ncompiler infrastructure. Categories and Subject Descriptors D.2.4 [Software Engineering]: Software/Program \nVeri\u00ad.cation correctness proofs, reliability, validation; D.3.4 [Programming Languages]: Processors \n compilers, op\u00adtimization; F.3.1 [Logics and Meanings of Programs]: Specifying and Verifying and Reasoning \nabout Programs mechanical veri.cation General Terms Reliability, languages, veri.cation.  Keywords \nCompiler optimization, automated correctness proofs. Permission to make digital or hard copies of all \nor part of this work for personal or classroom use is granted without fee provided that copies are not \nmade or distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. PLDI 03, June 9 11, 2003, San Diego, California, USA. Copyright \n2003 ACM 1-58113-662-5/03/0006 ...$5.00. 1. INTRODUCTION Compilers are an important part of the infrastructure \nre\u00adlied upon by programmers. If a compiler is faulty, then so are potentially all programs compiled with \nit. Unfortu\u00adnately, compiler errors can be di.cult for programmers to detect and debug. First, because \nthe compiler s output can\u00adnot be easily inspected, problems can often be found only by running a compiled \nprogram. Second, the compiler may appear to be correct over many runs, with a problem only manifesting \nitself when a particular compiled program is run with a particular input. Finally, when a problem does \nap\u00adpear, it can be di.cult to determine whether it is an error in the compiler or in the source program \nthat was compiled. For these reasons, it is very useful to develop tools and techniques that give compiler \ndevelopers and programmers con.dence in their compilers. One way to gain con.dence in the correctness \nof a compiler is to run it on various programs and check that the optimized version of each program pro\u00adduces \ncorrect results on various inputs. While this method can increase con.dence, it cannot provide any guarantees: \nit does not guarantee the absence of bugs in the compiler, nor does it even guarantee that any one particular \noptimized program is correct on all inputs. It also can be tedious to assemble an extensive test suite \nof programs and program inputs. Translation validation [26, 20] and credible compila\u00adtion [28, 27] improve \non this testing approach by automat\u00adically checking whether or not the optimized version of an input \nprogram is semantically equivalent to the original pro\u00adgram. These techniques can therefore guarantee \nthe correct\u00adness of certain optimized programs, but the compiler itself is still not guaranteed to be \nbug-free: there may exist programs for which the compiler produces incorrect output. There is little \nrecourse for a programmer if a compiled program can\u00adnot be validated. Furthermore, these approaches can \nhave a substantial impact on the time to run an optimization. The best solution would be to prove the \ncompiler sound, meaning that for any input program, the compiler always produces an equivalent output \nprogram. Optimizations, and sometimes even complete compilers, have been proven sound by hand [1, 2, \n16, 14, 8, 24, 3, 11]. However, manually prov\u00ading large parts of a compiler sound requires a lot of e.ort \nand theoretical skill on the part of the compiler writer. In addition, these proofs are usually done \nfor optimizations as written on paper, and bugs may still arise when the algo\u00adrithms are implemented \nfrom the paper speci.cation. We present a new technique for proving the soundness of compiler optimizations \nthat combines the bene.ts from the last two approaches: our approach is fully automated, as in credible \ncompilers and translation validation, but it also proves optimizations correct once and for all, for \nany input program. We achieve this goal by providing the com\u00adpiler writer with a domain-speci.c language \nfor implement\u00ading optimizations that is both .exible enough to express a variety of optimizations and \namenable to automated cor\u00adrectness reasoning. The main contributions of this paper are as follows: We \npresent a language, called Cobalt, for de.ning op\u00adtimizations over programs expressed in a C-like inter\u00admediate \nlanguage including unstructured control .ow, pointers to local variables and dynamically allocated memory, \nand recursive procedures. To implement an optimization (i.e., an analysis plus a code transfor\u00admation), \nusers provide a rewrite rule along with a guard describing the conditions that must hold for the rule \nto be triggered at some node of an input pro\u00adgram s control-.ow graph (CFG). The optimization also includes \na small predicate over program states, which captures the key insight behind the optimiza\u00adtion that justi.es \nits correctness. Cobalt also allows users to express pure analyses, such as pointer analysis. Pure analyses \ncan be used both to verify properties of interest about a program and to provide information to be consumed \nby later transformations. Optimizations and pure analyses written in Cobalt are directly exe\u00adcutable \nby a special data.ow analysis engine written for this purpose; they do not need to be reimplemented in \na di.erent language to be run.  We have used Cobalt to express a variety of intrapro\u00adcedural forward \nand backward data.ow optimizations, including constant propagation and folding, copy prop\u00adagation, common \nsubexpression elimination, branch folding, partial redundancy elimination, partial dead assignment elimination, \nand loop-invariant code mo\u00adtion. We have also used Cobalt to express several sim\u00adple intraprocedural \npointer analyses, whose results we exploited in the above optimizations.  We present a strategy for \nautomatically proving the soundness of optimizations and analyses expressed in Cobalt. The strategy requires \nan automatic theorem prover to discharge a small set of proof obligations for each optimization. We have \nmanually proven that if these obligations hold for any particular optimization, then that optimization \nis sound. The manual proof takes care of the necessary induction over program ex\u00adecution traces, which \nis di.cult to automate. As a result, the automatic theorem prover is given only non\u00adinductive theorems \nto prove about individual program states.  We have implemented our correctness checking strat\u00adegy using \nSimplify [31, 23], the automatic theo\u00adrem prover used in the Extended Static Checker for Java [6]. We \nhave written a general set of axioms that are used by Simplify to automatically discharge the optimization-speci.c \nproof obligations generated by our strategy. The axioms simply encode the seman\u00adtics of programs in our \nintermediate language. New op\u00adtimization programs can be written and proven sound  without requiring \nany modi.cations to Simplify s ax\u00adiom set. We have used our correctness checker to automatically prove \ncorrect all of the optimizations and pure analy\u00adses listed above. The correctness checker uncovered a \nnumber of subtle problems with earlier versions of our optimizations that might have eluded manual testing \nfor a long time.  We have implemented an execution engine for Cobalt optimizations as part of the Whirlwind \ncompiler infras\u00adtructure, and we have used it to successfully execute all of our optimizations.  By \nproviding greater con.dence in the correctness of com\u00adpiler optimizations, we hope to provide a foundation \nfor ex\u00adtensible compilers. An extensible compiler would allow users to include new optimizations tailored \nto their applications or domains of interest. The extensible compiler can protect it\u00adself from buggy \nuser optimizations by verifying their correct\u00adness using our strategy; any bugs in the resulting extended \ncompiler can be blamed on other aspects of the compiler s implementation, not on the user s optimizations. \nExtensible compilers could also be a good vehicle for research into new compiler optimizations. The next \nsection introduces Cobalt by example and sketches our strategy for automatically proving soundness of \nCobalt optimizations. Sections 3 and 4 formally de.ne Cobalt and our automatic proof strategy, respectively. \nSec\u00adtion 5 discusses our implementation of Cobalt s execution engine and correctness checker. Section \n6 evaluates our work, and section 7 discusses future work. Section 8 de\u00adscribes related work, and section \n9 o.ers our conclusions. 2. OVERVIEW In this section, we informally describe Cobalt and our technique \nfor proving Cobalt optimizations sound through a number of examples. A companion technical report [13] \ncontains the complete de.nitions of all the optimizations and analyses we have written in Cobalt. 2.1 \nForward Transformation Patterns 2.1.1 Semantics The heart of a Cobalt optimization is its transformation \npattern. For a forward optimization, a transformation pat\u00adtern has the following form: .1 followed by \n.2 until s . s . with witness P A transformation pattern describes the conditions under which a statement \ns may be transformed to s . . The formulas .1 and .2, which are properties of a statement such as x is \nde.ned and y is not used, together act as the guard indicating when it is legal to perform this transformation: \ns can be transformed to s . if on all paths in the CFG from the start of the procedure being optimized \nto s,there exists a statement satisfying .1, followed by zero or more statements satisfying .2, followed \nby s. Figure 1 shows this scenario pictorially. Forward transformation patterns codify a scenario com\u00admon \nto many forward data.ow analyses: an enabling state\u00adment establishes the conditions necessary for a transforma\u00adtion \nto be performed downstream, and any intervening state\u00adments are innocuous, i.e., do not invalidate the \nconditions. paths in the CFG boundary where.1 holds region where.2 holds Figure 1: CFG paths leading \nto a statement s which can be ' transformed to sby the transformation pattern .1 followed by .2 until \ns . s' with witness P. The shaded region can only be entered through a statement satisfying .1,and all \nstatements within the region satisfy .2. The statement s can only be reached by .rst passing through \nthis shaded region. The formula .1 captures the properties that make a state\u00adment enabling, and .2 captures \nthe properties that make a statement innocuous. The witness P captures the conditions established by \nthe enabling statement that allow the trans\u00adformation to be safely performed. Witnesses have no e.ect \non the semantics of an optimization; they will be discussed more below in the context of our strategy \nfor automatically proving optimizations sound. Example 1. A simple form of constant propagation re\u00adplaces \nstatements of the form X := Y with X := C if there is an earlier (enabling) statement of the form Y := \nC and each intervening (innocuous) statement does not modify Y . The enabling statement ensures that \nvariable Y holds the value C, and this condition is not invalidated by the innocu\u00adous statements, thereby \nallowing the transformation to be safely performed downstream. This sequence of events is ex\u00adpressed \nby the following transformation pattern (the witness is discussed in more detail in section 2.1.2): stmt(Y \n:= C) followed by \u00acmayDef (Y ) until X := Y . X := C with witness .(Y )= C The pattern variables X and \nY may be instantiated with any variables of the procedure being optimized, while the pat\u00adtern variable \nC may be instantiated with constants in the procedure. 2.1.2 Soundness A transformation pattern is sound, \ni.e., correct, if all the transformations it allows are semantics-preserving. Forward transformation \npatterns have a natural approach for under\u00adstanding their soundness. Consider a statement s trans\u00ad ' \nformed to s. Any execution trace of the procedure that ' contains swill at some point execute an enabling \nstatement, followed by zero or more innocuous statements, before reach\u00ad ' ing s. As mentioned earlier, \nexecuting the enabling state\u00adment establishes some conditions at the subsequent state of execution. These \nconditions are then preserved by the innocuous statements. Finally, the conditions imply that s '' and \nshave the same e.ect at the point where sis executed. As a result, the original program and the transformed \npro\u00adgram have the same semantics. Our automatic strategy for proving optimizations sound is based on \nthe above intuition. As part of the code for a for\u00adward transformation pattern, optimization writers \nprovide a forward witness P, which is a (possibly .rst-order) predicate over an execution state, denoted \n.. The witness plays the role of the conditions mentioned in the previous paragraph and is the intuitive \nreason why the transformation pattern is correct. Our strategy attempts to prove that the witness is \nestablished by the enabling statement and preserved by ' the innocuous statements, and that it implies \nthat s and shave the same e.ect.1 We call the region of an execution trace between the enabling statement \nand the transformed statement the witnessing region. In .gure 1, the part of a trace that is inside the \nshaded area is its witnessing region. In example 1, the forward witness .(Y )= C denotes the fact that \nthe value of Y in execution state . is C. Our imple\u00admentation proves automatically that the witness .(Y \n)= C is established by the statement Y := C, preserved by state\u00adments that do not modify the contents \nof Y , and implies that X := Y and X := C have the same e.ect. Therefore, the constant propagation transformation \npattern is automati\u00adcally proven to be sound. 2.1.3 Labels Each node in a procedure s CFG is labeled \nwith prop\u00aderties that are true at that node, such as stmt(x := 5)or mayDef (y). The formulas .1 and .2 \nin an optimization are boolean expressions over these labels. Users can de.ne a new kind of label by \ngiving a predicate over a statement, referred to in the predicate s body using the distinguished variable \ncurrStmt. As a trivial example, the stmt(S) label, which denotes that the statement at the current node \nis S, can be de.ned as: stmt(S) .currStmt = S As another example, syntacticDef (Y ), which stands for \nsyntactic de.nition of Y , can be de.ned as: syntacticDef (Y ) .case currStmt of . = Y X := ... X decl \nX X . = Y else false . endcase The label syntacticDef (Y ) holds at a node if and only if the current \nstatement is a declaration of or an assignment to 1The correctness of our approach does not depend on \nthe correctness of the witness, since our approach independently veri.es that the witness has the required \nproperties. Y .The case predicate is a convenience that provides a form of pattern matching, but it is \neasily desugared into an ordinary logical expression. Similarly, pattern variables and ellipses get desugared \ninto ordinary quanti.ed variables. Given the de.nition of syntacticDef , a conservative ver\u00adsion of the \nmayDef label from example 1 can be de.ned as: mayDef (Y ) .case currStmt of *X := Z . true X := P (Z) \n. true else . syntacticDef (Y ) endcase In other words, a statement may de.ne variable Y if the statement \nis either a pointer store (since our intermediate language allows taking the address of a local variable), \na procedure call (since the procedure may be passed pointers from which the address of Y is reachable), \nor otherwise a syntactic de.nition of Y . In addition to de.ning labels using predicates, users can also \nde.ne labels using the results of an analysis. Section 2.4 shows how such labels are de.ned and how they \ncan be used to make mayDef less conservative in the face of pointers.  2.2 Backward Transformation Patterns \nA backward transformation pattern is similar to a for\u00adward one, except that the direction of the .ow \nof analysis is reversed: .1 preceded by .2 since s . s ' with witness P The backward transformation pattern \nabove says that s may be transformed to s ' if on all paths in the CFG from s to the end of the procedure, \nthere exists a statement satisfying .1, preceded by zero or more statements satisfying .2, preceded by \ns. The witnessing region of a program execution trace consists of the states between the transformed \nstatement and the statement satisfying .1; P is called a backward witness. As with forward transformation \npatterns, the backward witness plays the role of an invariant in the witnessing re\u00adgion. However, in \na backward transformation the witness\u00ading region occurs after, rather than before, the point where the \ntransformed statement has been executed. Therefore, in general a backward witness must be a predicate \nthat re\u00adlates two execution states .old and .new , representing corre\u00adsponding execution states in the \nwitnessing region of traces in the original and transformed programs. Our automatic proof strategy attempts \nto prove that the backward witness is established by the transformation and preserved by the innocuous \nstates. Finally, we prove that after the enabling statement is executed, the witness implies that the \noriginal and transformed execution states become identical, imply\u00ading that the transformation is semantics-preserving. \nExample 2. Dead assignment elimination may be imple\u00admented in Cobalt by the following backward transformation \npattern: (stmt(X := ...) .stmt(return ...)) .\u00acmayUse(X) preceded by \u00acmayUse(X) since X := E . skip with \nwitness .old /X = .new /X We express statement removal by replacement with a skip statement.2 The removal \nof X := E is enabled by either a later assignment to X or a return statement, which signals the end of \nthe procedure. Preceding statements are innocuous if they don t use the contents of X. The backward witness \n.old /X = .new /X says that .old and .new are equal up to X: corresponding states in the wit\u00adnessing \nregion of the original and transformed programs are identical except for the contents of variable X. \nThis invari\u00adant is established by the removal of X := E and preserved throughout the region because X \nis not used. The witness im\u00adplies that a rede.nition of X or a return statement causes the execution \nstates of the two traces to become identical. 2.3 Pro.tability Heuristics If an optimization s transformation \npattern is proven sound, then it is legal to transform all matching occurrences of that pattern. For \nsome optimizations, including our two examples above, all legal transformations are also pro.table. However, \nin more complex optimizations, such as code mo\u00adtion and optimizations that trade o. time and space, many \ntransformations may preserve program behavior while only a small subset of them improve the code. To \naddress this distinction between legality and pro.tability, an optimiza\u00adtion is written in two pieces. \nThe transformation pattern de.nes only which transformations are legal. An optimiza\u00adtion separately describes \nwhich of the legal transformations are also pro.table and should be performed; we call this second piece \nof an optimization its pro.tability heuristic. An optimization s pro.tability heuristic is expressed \nvia a choose function, which can be arbitrarily complex and writ\u00adten in a language of the user s choice. \nGiven the set . of the legal transformations determined by the transformation pattern, and given the \nprocedure being optimized, choose returns the subset of the transformations in . that should actually \nbe performed. A complete optimization in Cobalt therefore has the following form, where Opat is a transfor\u00admation \npattern: Opat .ltered through choose This way of factoring optimizations into a transformation pattern \nand a pro.tability heuristic is critical to our abil\u00adity to prove optimizations sound automatically, \nsince only an optimization s transformation pattern a.ects soundness. Transformation patterns tend to \nbe simple even for com\u00adplicated optimizations, with the bulk of an optimization s complexity pertaining \nto pro.tability. Pro.tability heuris\u00adtics can be written in any language, thereby removing any limitations \non their expressiveness. Without pro.tability heuristics, the extra complexity added to guards to express \npro.tability information would prevent automated correct\u00adness reasoning. For the constant propagation \nand dead assignment elimi\u00adnation optimizations shown earlier, the choose function re\u00adturns all instances: \nchooseall (.,p) = .. This pro.tability heuristic is the default if none is speci.ed explicitly. Be\u00adlow \nwe give an example of an optimization with a nontrivial choose function. Example 3. Consider the implementation \nof partial redun\u00addancy elimination (PRE) [15, 10] in Cobalt. One way to 2An execution engine for optimizations \nwould not actually insert such skips. perform PRE is to .rst insert copies of statements in well\u00adchosen \nplaces in order to convert partial redundancies into full redundancies, and then to eliminate the full \nredundan\u00adcies by running a standard common subexpression elimina\u00adtion (CSE) optimization expressible \nin Cobalt. For example, in the following code fragment, the computation x:=a +b at the end is partially \nredundant, since it is redundant only when the true leg of the branch is executed: b := ...; if (...) \n{ a := ...; x:=a+ b; } else { ... // don t define a, b, or x, and don t use x. } x :=a +b; This partial \nredundancy can be eliminated by making a copy of the assignment x :=a +b in the false leg of the branch. \nNow the assignment after the branch is fully redundant and can be removed by running CSE followed by \nself-assignment removal (removing assignments of the form x:= x). The criterion that determines when \nit is legal to dupli\u00adcate a statement is relatively simple. Most of the complexity in PRE involves determining \nwhich of the many legal du\u00adplications are pro.table, so that partial redundancies will be converted to \nfull redundancies at minimum cost. The .rst, code duplication pass of PRE can be expressed in Cobalt \nas the following backward optimization: stmt(X := E) .\u00acmayUse(X) preceded by unchanged(E) .\u00acmayDef (X) \n.\u00acmayUse(X) since skip . X := E with witness .old /X = .new /X .ltered through ... Analogous to statement \nremoval, we express statement insertion as replacement of a skip statement.3 The label unchanged (E) \nis de.ned (by the optimization writer, as de\u00adscribed in section 2.1.3) to be true at a statement s if \ns does not rede.ne the contents of any variable mentioned in E. The transformation pattern for code duplication \nallows the insertion if, on all paths in the CFG from the skip, X := E is preceded by statements that \ndo not modify E and X and do not use X, which are preceded by the skip. In the code frag\u00adment above, \nthe transformation pattern allows x:=a+ b to be duplicated in the else branch, as well as other (unprof\u00aditable) \nduplications. This optimization s choose function is responsible for selecting those legal code insertions \nthat also are the latest ones that turn all partial redundancies into full redundancies and do not introduce \nany partially dead com\u00adputations. This condition is rather complicated, but it can be implemented in \na language of the user s choice and can be ignored when verifying the soundness of PRE. 3An execution \nengine for optimizations would conceptually insert skips dynamically as needed to perform insertions. \n 2.4 Pure Analyses In addition to optimizations, Cobalt allows users to write pure analyses that do not \nperform transformations. These analyses can be used to compute or verify properties of in\u00adterest about \na procedure and to provide information to be consumed by later transformations. A pure analysis de.nes \na new label, and the result of the analysis is a labeling of the given CFG. The new label can then be \nused by other analyses, optimizations, or label de.nitions. A forward pure analysis is similar to a forward \noptimiza\u00adtion, except that it does not contain a rewrite rule or a pro.tability heuristic. Instead, it \nhas a de.nes clause that gives a name to the new label. A forward pure analysis has the form .1 followed \nby .2 de.nes label with witness P The new label can be added to a statement s if on all paths to s, there \nexists an (enabling) statement satisfying .1, followed by zero or more (innocuous) statements satisfy\u00ading \n.2, followed by s. The given forward witness should be established by the enabling statement and preserved \nby the innocuous statements. If so, the witness provides the new label s meaning: if a statement s has \nlabel label, then the corresponding witness P is true of the program state just before execution of s. \nThe following example shows how a pure analysis can be used to compute a simple form of pointer information: \nExample 4. We say that a variable is tainted at a pro\u00adgram point if its address may have been taken prior \nto that program point. The following pure analysis de.nes the notTainted label: stmt(decl X) followed \nby \u00acstmt(... := &#38;X) de.nes notTainted (X) with witness notPointedTo(X, .) The analysis says that \na variable is not tainted at a state\u00adment if on all paths leading to that statement, the variable was \ndeclared, and then its address was never taken. The wit\u00adness notPointedTo(X, .) is a .rst-order predicate \nde.ned by the user that holds when no memory location in . contains apointer to X. The notTainted label \ncan be used to de.ne a more precise version of the mayDef label from earlier examples, which incorporates \nthe fact that pointer stores and procedure calls cannot a.ect variables that are not tainted: mayDef \n(Y ) . case currStmt of *X := Z .\u00acnotTainted (Y ) X := P (Z) . X = Y .\u00acnotTainted(Y ) else . syntacticDef \n(Y ) endcase With this new de.nition, optimizations using mayDef be\u00adcome less conservative in the face \nof pointer stores and calls. Cobalt currently has no notion of backward pure analyses. Although we anticipate \nno technical barrier to introducing such a notion, additional mechanisms would be required in order to \nde.ne the semantics of a label introduced by a back\u00adward analysis. So far we have not encountered a need \nfor backward analyses. Cobalt also currently only allows the results of a forward analysis to be used \nin a forward optimization, or in another forward analysis. Allowing a forward analysis to be used in \na backward optimization may result in interference,whereby a transformation triggered by the backward \noptimization invalidates the results of the forward analysis. This issue is discussed in more detail \nin section 4.1.  3. COBALT This section provides a formal de.nition of Cobalt and of the intermediate \nlanguage that Cobalt optimizations manip\u00adulate. The full formal details can be found in our technical \nreport [13]. 3.1 Intermediate Language A program p in our (untyped) intermediate language is described \nby the following grammar: Progs p ::= pr ... pr Procs pr ::= p(x) {s; ... ; s;} Stmts s ::= decl x|skip \n|lhs:= e|x:= new | x:= p(b) |if b goto . else . | return x Exprs e ::= b|*x|&#38;x|op b ... b Locatables \nlhs ::= x|*x Base Exprs b ::= x|c Ops op ::= various operators with arity =1 Vars x ::= x |y |z |... \nProc Names p ::= p |q |r |... Consts c ::= constants Indices . ::= 0 |1 |2 |... A program pis a sequence \nof procedures, and each procedure is a sequence of statements. We assume a distinguished pro\u00adcedure named \nmain. Statements include local variable dec\u00adlarations, assignments to local variables and through point\u00aders, \nheap memory allocation, procedure calls and returns, and conditional branches (unconditional branches \ncan be simulated with conditional branches). We assume that no procedure declares the same local variable \nmore than once. We assume that each procedure ends with a return state\u00adment. Statements are indexed consecutively \nfrom 0, and stmtAt(p,.) returns the statement with index . in p.Ex\u00adpressions include constants, local \nvariable references, pointer dereferences, taking the addresses of local variables, and n\u00adary operators \nover non-pointer values. A state of execution of a program is a tuple . = (.,.,s,.,M). The index . indicates \nwhich statement is about to be executed. The environment . is a map from variables in scope to their \nlocations in memory, and the store s describes the contents of memory by mapping locations to values \n(constants and locations). The dynamic call chain is represented by a stack .,and M is the memory allocator, \nwhich returns fresh locations as needed. The states of a program ptransition according to the state transition \nfunction .p.We denote by . .p . ' the fact that . ' is the program state that is stepped to when execution \nproceeds from state .. The de.nition of .p is standard and is given in our accompanying technical report \n[13]. We also de.ne an intraprocedural state transition function .p . This function acts like .p except \nwhen the statement to be executed is a procedure call. In that case, .p steps over the call, returning \nthe program state that will eventually be reached when control returns to the calling procedure. We model \nrun-time errors through the absence of state transitions: if in some state . program execution would \nfail with a run-time error, there won t be any . ' such that . .p . ' is true. Likewise, if a procedure \ncall does not return successfully, e.g., because of in.nite recursion, there won t be any . ' such that \n..p . ' is true.  3.2 Cobalt In this section, we .rst specify the syntax of a rewrite rule s original \nand transformed statements s and s ' .Then we de.ne the syntax used for expressing .1 and .2. Finally, \nwe provide the semantics of optimizations. The witness P does not a.ect the (dynamic) semantics of optimizations. \n3.2.1 Syntax of sand s ' Statements s and s ' are de.ned in the syntax of the ex\u00adtended intermediate \nlanguage, which augments the interme\u00addiate language with a form of free variables called pattern variables. \nEach production in the grammar of the original intermediate language is extended with a case for a pattern \nvariable. A few examples are shown below: Exprs e ::= \u00b7\u00b7\u00b7|E Vars x ::= \u00b7\u00b7\u00b7|X |Y |Z |... Consts c ::= \n\u00b7\u00b7\u00b7|C Statements in the extended intermediate language are instantiated by substituting for each pattern \nvariable a program fragment of the appropriate kind from the intermediate-language program being optimized. \nFor ex\u00adample, the statement X := E in the extended intermedi\u00adate language contains two pattern variables \nX and E,and this statement can be instantiated to form an intermediate\u00adlanguage statement assigning any \nexpression occurring in the intermediate program to any variable occurring in the intermediate program. \n 3.2.2 Syntax and Semantics of .1 and .2 The syntax for ., and also for label de.nitions, is de\u00adscribed \nby the following grammar: . ::= true |false |\u00ac. |... |... |l(t,...,t) |t= t|case t of t.. \u00b7\u00b7\u00b7 t.. else \n.. endcase In the above grammar, l ranges over label names and t ranges over terms, which are elements \ndrawn from the ex\u00adtended intermediate language as well as the distinguished term currStmt. The grammar \nconsists of propositional logic augmented with label predicates, term equality, and the case predicate. \nThe semantics of a formula . is de.ned with respect to a labeled CFG. Each node n in the CFG for procedure \np is labeled with a .nite set Lp(.), where . is n s index. Lp(.) includes labels l(t1,...,tn) where the \nterms do not contain pattern variables. For example, a node could be labeled with stmt(x := 3)and mayDef \n(x). The meaning of a formula .at a node depends on a substi\u00adtution . mapping the pattern variables in \n. to fragments of p. We extend substitutions to formulas and program frag\u00adments containing pattern variables \nin the usual way. We . to indicate that the node with index . satis-also does in p ' .Let p[p ' ] denote \nthe program identical .p p. write . |= .es . in the labeled CFG of p under substitution ..The ' to p \nbut with procedure p replaced by p . An optimization p. . is straightforward, with the base case O is \nsound if for all intermediate-language programs p and = .[OD(p)] is a semantically equivalent de.nition \nof . | being . |= complete de.nition of | p. l(t1,...,tn) .. .(l(t1,...,tn)) . Lp(.). The procedures \npin p, p[p p. is in our technical report [13]. transformation of p. =  3.2.3 Semantics of Optimizations \nWe de.ne the semantics of optimizations and analyses in several pieces. First, the meaning of a forward \nguard .1 fol\u00adlowed by .2 is a function that takes a procedure and returns a set of matching indices with \ntheir corresponding substitu\u00adtions: De.nition 1. The meaning of a forward guard Oguard of the form .1 \nfollowed by .2 is as follows: [Oguard D(p)= {(.,.) | for all paths .1,...,.j ,. in p s CFG such that \n.1 is the index of p s entry node To prove a Cobalt optimization sound, we prove the soundness of its \nassociated transformation pattern. We say that a transformation pattern Opat with rewrite rule s . s \n' is sound if, for all intermediate-language programs p and procedures p in p, for all subsets . . [Opat \nD(p), p[p ' ,p,.)] is a semantically equivalent transfor\u00ad . app(s mation of p. If a transformation pattern \nis sound, then any optimization O with that transformation pattern is sound, since the optimization will \nselect some subset of the transfor\u00admation pattern s suggested transformations, and each subset is known \nto result in a semantically equivalent transforma\u00adtion of p. Therefore, weneed notreasonatall aboutan \noptimization s pro.tability heuristic in order to prove that .k.(1 =k =j ..k |= p. .1 ..i.(k<i =j ..i \n|= p. .2))} The above de.nition formalizes the description of forward guards from Section 2. The meaning \nof a backward guard .1 preceded by .2 is identical, except that the guard is evaluatedonCFG paths .,.j,...,.1 \nthat start, rather than end, at .,where .1 is the index of the procedure s exit node. Guards can be seen \nas a restricted form of temporal logic formula, expressible in variants of both Linear Temporal Logic \n(LTL) [7] and Computation Tree Logic (CTL) [5]. Next we de.ne the semantics of transformation pat\u00adterns. \nA forward (backward) transformation pattern Opat = Oguard until (since) s . s ' with witness P simply \n.lters the set of nodes matching its guard to include only those nodes of the form s: the optimization \nis sound. First we discuss a property of Cobalt that simpli.es the obligations necessary for proving \na transformation pattern sound. Then we describe these obligations for forward and backward optimizations, \nrespectively. 4.1 Noninterference As described above, for a transformation pattern to be sound, it must \nbe possible to apply any subset of the suggested transformations without changing a procedure s semantics. \nTherefore, to prove a transformation pattern sound, we must argue that its suggested transformations \ncannot interfere with one another. Interference occurs when multiple transformations that are semantics-preserving \nin isolation cause a procedure s semantics to change when per\u00ad [Opat D(p)= {(.,.) |(.,.) .[Oguard D(p)and \n.|= p. stmt(s)} formed together. The meaning of an optimization is a function that takes a procedure \npand returns the procedure produced by applying In general it is possible for an optimization to interfere \nto p all transformations selected by the choose function. with itself. For example, consider an optimization \nthat per\u00ad forms both dead assignment elimination and redundant as\u00ad signment elimination. On the following \nprogram fragment De.nition 2. Given an optimization O of the form Opat .ltered through choose, where \nOpat has rewrite rule s . s ' , the meaning of O is as follows: [OD(p)= let .:= [Opat D(p) in app(s ' \n,p,choose(.,p) n.) where app(s ' ,p,. ' ) returns the procedure identical to p but with the node with \nindex . transformed to .(s ' ),for each (.,.) in . ' . 4 Finally, the meaning of a pure analysis Oguard \nde\u00ad.nes label with witness P applied to a procedure pisa new version of p s CFG where for each pair (.,.)in \n[Oguard D(p), the node with index . is additionally labeled with .(label).   4. PROVING SOUNDNESS AUTOMATICALLY \nIn this section we describe our technique for automatically proving soundness of Cobalt optimizations. \nThe full details, including the proofs of the theorems, are in our technical report [13]. We say that \nan intermediate-language program p ' is a semantically equivalent transformation of p if, whenever main(v1) \nreturns v2 in p,for some values v1 and v2,then it If there are multiple pairs in .' with the same index \n., then one of them is chosen nondeterministically. \u00b7\u00b7\u00b7 S1: x:= 5; S2: x:= 5; \u00b7\u00b7\u00b7 our hypothetical optimization \nwill suggest both S1 and S2 for removal: S1 is dead and S2 is redundant. Perform\u00ading either removal is \ncorrect, but performing both removals changes the program s semantics. Fortunately, it is possible to \nshow that a Cobalt transfor\u00admation pattern cannot interfere with itself: if each transfor\u00admation from \na set of suggested transformations is correct in isolation, then performing any subset of the transformations \nis correct. The optimization above cannot be directly writ\u00adten in Cobalt. Instead, it must be written \nas two separate optimizations, one forward and one backward.5 Because of Cobalt s noninterference property, \nthe optimization-speci.c obligations to be discharged as part of our proof strategy need only pertain \nto a single transforma\u00adtion. The theorems described below validate the su.ciency of these obligations \nfor proving Cobalt optimizations sound. 5The example illustrates a potential unsoundness from combining \nfor\u00adward and backward transformation patterns. This is the reason that we currently disallow employing \na forward pure analysis in a backward transformation. We can, however, prove that a forward transforma\u00adtion \npattern cannot interfere with any forward pure analysis. 4.2 Forward Transformation Patterns Consider \na forward transformation pattern of the follow\u00ading form: .1 followed by .2 until s . s ' with witness \nP As discussed in section 2, our proof strategy entails showing that the forward witness P holds throughout \nthe witness\u00ading region and that the witness implies s and s ' have the same semantics. This can naturally \nbe shown by induction over the states in the witnessing region of an execution trace leading to a transformed \nstatement. In general, it is di.cult for an automatic theorem prover to determine when proof by induction \nis necessary and to perform such a proof with a strong enough inductive hypothesis. Therefore we instead \nrequire an automatic theorem prover to discharge only non\u00adinductive obligations, which pertain to individual \nexecution states rather than entire execution traces. We have proven that if these obligations hold for \nany particular optimization, then that optimization is sound. We use index as an accessor on states: \nindex((.,.,s,.,M)) = .. The optimization-speci.c obligations, to be discharged by an automatic theorem \nprover, are as follows, where .(P) is the predicate formed by applying . to each pattern variable in \nthe de.nition of P: F1. If ..p . ' and index(.) |=.p .1,then .(P)(. ' ). F2. If .(P)(.) and ..p . ' and \nindex(.) |=p .2,then . .(P)(. ' ). F3. If .(P)(.) and ..p . ' and . = index(.) and stmtAt(p,.)= .(s) \nand stmtAt(p ' ,.)= .(s ' ),then ..p ' . ' . Condition F1 ensures that the witness holds at any state \nfollowing the execution of an enabling statement (one sat\u00adisfying .1). Condition F2 ensures that the \nwitness is pre\u00adserved by any innocuous statement (one satisfying .2). Fi\u00adnally, condition F3 ensures \nthat s and s ' have the same se\u00admantics when executed from a state satisfying the witness. As an example, \nconsider condition F1 for the constant propagation optimization from example 1. The condition looks as \nfollows: If ..p . ' and index(.) |=p stmt(Y := C), . then .(. ' (Y)= C). The condition is easily proven \nauto\u00admatically from the semantics of assignments and the stmt label. The following theorem validates \nthe optimization-speci.c proof obligations. Theorem 1. If O is a forward optimization satisfying con\u00additions \nF1, F2, and F3, then O is sound. The proof of this theorem uses conditions F1 and F2 as part of the base \ncase and the inductive case, respectively, in an inductive argument that the witness holds throughout \na witnessing region. Condition F3 is then used to show that s and s ' have the same semantics in this \ncontext. A pure analysis .1 followed by .2 de.nes label with witness P is proven sound similarly. We \nrequire conditions F1 and F2 to be satis.ed; F3 has no analogue. These con\u00additions allow us to show that \nlabel indeed has the semantics of the witness P. 4.3 Backward Transformation Patterns Consider a backward \ntransformation pattern of the fol\u00adlowing form: .1 preceded by .2 since s . s ' with witness P The optimization-speci.c \nobligations are similar to those for a forward transformation pattern, except that the ordering of events \nin the witnessing region is reversed: B1. If ..p .old and ..p ' .new and . = index(.) and stmtAt(p,.)= \n.(s) and stmtAt(p ' ,.)= .(s ' ),then .(P)(.old ,.new ). B2. If and . ' and = .(P)(.old ,.new ) .old \n.p old .old index(.old ) and .new = index(.new ) and .old |= p. .2 and stmtAt(p,.old )= stmtAt(p ' ,.new \n), then there '' exists some .new such that .new .p ' .new and '' .(P)(.old ,.new ). B3. If .(P)(.old \n,.new ) and .old .p . and .old = index(.old ) and .new = index(.new ) and .old |= p .1 . and stmtAt(p,.old \n)= stmtAt(p ' ,.new ),then .new .p ' .. Condition B1 ensures that the backward witness holds be\u00adtween \nthe original and transformed programs, after s and s ' are respectively executed.6 Condition B2 ensures \nthat the backward witness is preserved through the innocuous state\u00adments. Condition B3 ensures that the \ntwo traces become identical again after executing the enabling statement (and exiting the witnessing \nregion). Analogous to the forward case, the following theorem val\u00adidates the optimization-speci.c proof \nobligations for back\u00adward optimizations. Theorem 2. If O is a backward optimization satisfying conditions \nB1, B2, and B3, then O is sound. 5. IMPLEMENTING COBALT We have implemented a tool that automatically \nchecks the correctness of Cobalt optimizations as well as an exe\u00adcution engine for running them. Section \n5.1 describes our correctness checker, and section 5.2 describes our execution engine. 5.1 Correctness \nChecker We have implemented our strategy for automatically prov\u00ading Cobalt optimizations sound with the \nSimplify auto\u00admatic theorem prover. For each optimization, we ask Sim\u00adplify to prove the three associated \noptimization-speci.c obligations given a set of background axioms. There are two kinds of background \naxioms: optimization-independent ones and optimization-dependent ones. The optimization\u00adindependent axioms \nsimply encode the semantics of our in\u00adtermediate language and they need not be modi.ed in or\u00adder to prove \nnew optimizations sound. The optimization\u00addependent axioms encode the semantics of user-de.ned la\u00adbels \nand are generated automatically from the Cobalt label 6This condition assumes that s ' does not get stuck \nby causing a run-time error. That assumption must actually be proven, but for simplicity we elide this \nissue here. It is addressed by requiring a few ' additional obligations to be discharged that imply that \ns cannot get stuck if the original program does not get stuck. Details are in our technical report [13]. \nde.nitions. Our correctness checker translates label de.\u00adnitions into Simplify axioms by expanding case \nexpressions into ordinary boolean expressions and performing a few sim\u00adple transformations to produce \naxioms in a form accepted by Simplify. To encode the Cobalt intermediate language in Simplify, we introduce \nfunction symbols that represent term construc\u00adtors for each kind of expression and statement. For example, \nthe term assgn(var(x),deref (var(y)) represents the state\u00adment x := *y. Next we formalize the representation \nof pro\u00adgram states. Simplify has built-in axioms about a map data structure, with associated functions \nselect and update to ac\u00adcess elements and (functionally) update the map. This is useful for representing \nmany components of a state. For ex\u00adample, an environment is a map from variables to locations, and a \nstore is a map from locations to values. Given our representation for states, we de.ne axioms for a function \nsymbol evalExpr, which evaluates an expression in a given state. The evalExpr function represents the \nfunc\u00adtion .(\u00b7) used in section 2. We also de.ne axioms for a function evalLExpr which computes the location \nof a lhs ex\u00adpression given a program state. Then we provide axioms for the stepIndex, stepEnv, stepStore, \nstepStack,and stepMem functions, which together de.ne the state transition func\u00adtion .p from section \n3.1. These functions take a state and a program and return the new value of the state component being \nstepped. As an example, the axioms for stepping an index and a store through an assignment lhs := e are \nas follows: ..,p,lhs,e. stmtAt(p,index(.)) = assgn(lhs,e) . stepIndex (.,p)= index(.)+1 ..,p,lhs,e. stmtAt(p,index(.)) \n= assgn(lhs,e) . stepStore(.,p)= update(store(.),evalLExpr (.,lhs), evalExpr (.,e)) The .rst axiom says \nthat the new index is the current index incremented by one. The second axiom says that the new store \nis the same as the old one, but with the location of lhs updated to the value of e. Finally, the .p function \nis de.ned in terms of the .p function. In the context of intraprocedural analysis, we do not have access \nto the bodies of called procedures. There\u00adfore, we conservatively model the semantics of stepping over \na procedure call by a set of axioms that hold for any call. The primary axiom says that the store after \na call preserves the values of local variables in the caller whose locations are not pointed to before \nthe call. This axiom encodes the fact that locals not reachable from the store cannot be modi.ed by a \ncall. We have implemented and automatically proven sound a dozen Cobalt optimizations and analyses (which \nare given in our technical report [13]). On a modern workstation, the time taken by Simplify to discharge \nthe optimization\u00adspeci.c obligations for these optimizations ranges from 3 to 104 seconds, with an average \nof 28 seconds. 5.2 Execution Engine To run Cobalt optimizations without .rst rewriting them in some \nother language, we have implemented an execution engine for Cobalt as an analysis in the Whirlwind compiler, \nasuccessor toVortex[4]. This analysis stores at each program point a set of sub\u00adstitutions, with each \nsubstitution representing a potential witnessing region. Consider a forward optimization: .1 followed \nby .2 until s . s ' with witness P .ltered through choose The .ow function for our analysis works as \nfollows. First, if the statement being processed satis.es .1, then the .ow function adds to the outgoing \ndata.ow fact the substitution that caused .1 to be true. Also, for each substitution . in the incoming \ndata.ow fact, the .ow function checks if .(.2) is true at the current statement. If it is, then . is \npropagated to the outgoing data.ow fact, and otherwise it is dropped. Finally, merge nodes simply take \nthe intersec\u00adtion of the incoming data.ow facts. After the analysis has reached a .xed point, if a statement \nhas a substitution . in its incoming data.ow fact that makes .(stmt(s)) true and the choose function \nselects this statement, then the state\u00adment is transformed to .(s ' ). For example, in constant propagation \nwe have .1 = stmt(Y := C)and .2 = \u00acmayDef (Y). Below we show the data.ow facts propagated after a few \nexample statements: S1 : a := 2;[Y . a,C . 2] S2 : b := 3;[Y . a,C . 2],[Y . b,C . 3] S3 : c := a; S1 \nsatis.es .1, so its outgoing data.ow fact contains the sub\u00adstitution [Y . a,C . 2]. S2 satis.es .2 under \nthis substi\u00adtution, so the substitution is propagated; S2 also satis.es .1 so [Y . b,C . 3] is added \nto the outgoing data.ow fact. In fact, the data.ow information after S2 is very similar to the regular \nconstant propagation data.ow fact {a . 2,b . 3}. At .xed point, the statement c := a can be transformed \nto c := 2 because the incoming data.ow fact contains the map [Y . a,C . 2]. Note that this implementation \nevaluates all instances of the constant propagation transformation pattern simultaneously. Our analysis \nis implemented using our earlier framework for composable optimizations in Whirlwind [12]. This frame\u00adwork \nallows optimizations to be de.ned modularly and then automatically combines all-forward or all-backward \nopti\u00admizations in order to gain mutually bene.cial interactions. Analyses and optimizations written in \nCobalt are there\u00adfore also composable in this way. Furthermore, Whirlwind s framework automatically composes \nan optimization with it\u00adself, allowing a recursively de.ned optimization to be solved in an optimistic, \niterative manner; this property is likewise conferred on Cobalt optimizations. For example, a recursive \nversion of dead-assignment elimination allows X := E to be removed even if X is used before being rede.ned, \nas long as it is only used by other dead assignments (possibly including itself).7  6. DISCUSSION In \nthis section, we evaluate our system along three di\u00admensions: expressiveness of Cobalt, debugging value, \nand reduced trusted computing base. Expressiveness. One of the key choices in our approach is to restrict \nthe language in which optimizations can be 7Although Cobalt optimizations can be composed, we have not \nyet proved that the .ow function of our Cobalt engine satis.es the prop\u00aderties required in [12] for the \ncomposition to be sound. We plan to investigate this in future work. written, in order to gain automatic \nreasoning about sound\u00adness. However, Cobalt s restrictions are not as onerous as they may .rst appear. \nFirst, much of the complexity of an optimization can be factored into the pro.tability heuristic, which \nis unrestricted. Second, the pattern of a witnessing region beginning with a single enabling statement \nand passing through zero or more innocuous statements before reaching the statement to be transformed \n is common to many forward intraprocedural data.ow analyses, and simi\u00adlarly for backward intraprocedural \ndata.ow analyses. Third, optimizations that traditionally are expressed as having ef\u00adfects at multiple \npoints in the program, such as various sorts of code motion, can in fact be decomposed into several sim\u00adpler \ntransformations, each of which .ts Cobalt s transforma\u00adtion pattern syntax. The PRE example in section \n2.3 illustrates all three of these points. PRE is a complex code-motion optimiza\u00adtion [15, 10], and yet \nit can be expressed in Cobalt using simple forward and backward passes with appropriate prof\u00aditability \nheuristics. Our way of factoring complicated opti\u00admizations into smaller pieces, and separating the part \nthat a.ects soundness from the part that doesn t, allows users to write optimizations that are intricate \nand expressive yet still amenable to automated correctness reasoning. Even so, the current version of \nCobalt does have limita\u00adtions. For example, it cannot express interprocedural op\u00adtimizations or one-to-many \ntransformations. As mentioned in section 7, our future work will address these limitations. Also, optimizations \nand analyses that build complex data structures to represent their data.ow facts may be di.cult to express. \nFinally, it is possible for limitations in either our proof strategy or in the automatic theorem prover \nto cause a sound optimization expressible in Cobalt to be rejected. In all these cases, optimizations \ncan be written outside of our framework, perhaps veri.ed using translation valida\u00adtion. Optimizations \nwritten in Cobalt and proven correct can peacefully co-exist with optimizations written the nor\u00admal way. \nDebugging bene.t. Writing correct optimizations is di.cult because there are many corner cases to consider, \nand it is easy to miss one. Our system in fact found sev\u00aderal subtle problems in previous versions of \nour optimiza\u00adtions. For example, we have implemented a form of com\u00admon subexpression elimination (CSE) \nthat eliminates not only redundant arithmetic expressions, but also redundant loads. In particular, this \noptimization tries to eliminate a computation of *X if the result is already available from a previous \nload. Our initial version of the optimization pre\u00adcluded pointer stores from the witnessing region, to \nensure that the value of *X was not modi.ed. However, a failed soundness proof made us realize that even \na direct assign\u00adment Y := ... can change the value of *X,because X could point to Y. Once we incorporated \npointer information to make sure that direct assignments in the witnessing region were not changing the \nvalue of *X, our implementation was able to automatically prove the optimization sound. With\u00adout the \nstatic checks to .nd the bug, it could have gone undetected for a long time, because that particular \ncorner case may not occur in many programs. Reduced trusted computing base. The trusted com\u00adputing base \n(TCB) ordinarily includes the entire compiler. In our system we have moved the compiler s optimization \nphase, one of the most intricate and error-prone portions, outside of the TCB. Instead, we have shifted \nthe trust in this phase to three components: the correctness checker, in\u00adcluding the automatic theorem \nprover, the manual proofs done as part of our framework, and the engine that exe\u00adcutes optimizations. \nBecause all of these components are optimization-independent, new optimizations can be incor\u00adporated \ninto the compiler without enlarging the TCB. Fur\u00adthermore, as discussed in section 5, the execution engine \nis implemented as a single data.ow analysis common to all user-de.ned optimizations. This means that \nthe trustwor\u00adthiness of the execution engine is akin to the trustworthiness of a single optimization \npass in a traditional compiler. Trust can be further enhanced in several ways. First, we could use an \nautomatic theorem prover that generates proofs, such as the prover in the Touchstone compiler [22]. This \nwould allow trust to be shifted from the theorem prover to a simpler proof checker. The manual proofs \nof our frame\u00adwork are made public for peer review in [13] to increase con.dence. We could also use an \ninteractive theorem prover such as PVS [25] to validate these proofs. 7. FUTURE WORK There are many \ndirections for future work. We plan to ex\u00adtend Cobalt to handle interprocedural optimizations. One approach \nwould extend the scope of analysis from a single procedure to the whole program s control-.ow supergraph. \nA technical challenge for this approach is the need to express the witness P in a way that is robust \nacross procedure calls. For example, the predicate .(Y)= C does not make sense once a call is stepped \ninto, because Y has gone out of scope. We intend to extend the syntax for the witness to be more precise \nabout which location is being talked about. A dif\u00adferent approach to interprocedural analysis would use \npure analyses to de.ne summaries of procedures, which could be used in intraprocedural optimizations \nof callers. Currently Cobalt only supports transformations that re\u00adplace a single statement with a single \nstatement. It should be relatively straightforward to generalize the framework to handle one-to-many \nstatement transformations, allowing op\u00adtimizations like inlining to be expressed. Supporting many\u00adto-many \nstatement transformations, including various kinds of loop restructuring optimizations, would also be \ninterest\u00ading. We plan to try inferring the witnesses, which are currently provided by the user. It may \nbe possible to use some simple heuristics to guess a witness from the given transformation pattern. As \na simple example, in the constant propagation example of section 2, the appropriate witness, that Y has \nthe value C, is simply the strongest postcondition of the enabling statement Y := C. Many of the other \nforward optimizations that we have written also have this property. Our current notion of a semantically \nequivalent transfor\u00admation reasons only about computations in the original pro\u00adgram that terminate without \nan error. It would be straight\u00adforward to reason about computations that end in a run-time error by extending \nthe .p function to step to an explicit er\u00adror state in these situations. We would also like to extend \nthe notion of semantic equivalence to allow reasoning about nonterminating computations. We plan to explore \nmore e.cient implementation tech\u00adniques for the Cobalt execution engine, such as generating specialized \ncode to run each optimization [32]. Another di\u00adrection for improving e.ciency would be to allow analyses \nto be de.ned over a sparse representation such as a data.ow graph. Finally, an important consideration \nthat we have not ad\u00addressed is the interface between the optimization writer and our automatic correctness \nchecker. It will be critical to pro\u00advide useful error messages when an optimization cannot be proven \nsound. When Simplify cannot prove a given propo\u00adsition, it returns a counterexample context, which is \na state of the world that violates the proposition. An interesting approach would be to use this counterexample \ncontext to synthesize a small intermediate-language program that il\u00adlustrates a potential unsoundness \nof the given optimization. 8. RELATED WORK Temporal logic has previously been used to express data.ow \nanalyses and reason about them by hand [32, 33, 29, 30, 11]. Our language is inspired by recent work \nin this direction by Lacey et al. [11]. Lacey describes a language for writing optimizations as guarded \nrewrite rules evaluated over a labeled CFG, and our transformation patterns are modeled on this language. \nLacey s intermediate language lacks several constructs found in realistic languages, includ\u00ading pointers, \ndynamic memory allocation, and procedures. Lacey describes a general strategy, based on relating exe\u00adcution \ntraces of the original and transformed programs, for manually proving the soundness of optimizations \nin his lan\u00adguage. Three example optimizations are shown and proven sound by hand using this strategy. \nUnfortunately, the gen\u00aderality of this strategy makes it di.cult to automate. Lacey s guards may be arbitrary \nCTL formulas, while our guard language can be viewed as a strict subset of CTL that codi.es a particularly \ncommon idiom. However, we are still able to express more precise versions of Lacey s three ex\u00adample optimizations \n(as well as many others) and to prove them sound automatically. Further, Lacey s optimization language \nhas no notion of semantic labels nor of pro.tabil\u00adity heuristics. Therefore, expressing optimizations \nthat em\u00adploy pointer information (assuming Lacey s language were augmented with pointers) or optimizations \nlike PRE would instead require writing more complicated guards, and some optimizations we support may \nnot be expressible by Lacey. As mentioned in the introduction, much other work has been done on manually \nproving optimizations correct [14, 16, 1, 2, 8, 24, 3]. Transformations have also been proven correct \nmechanically, but not automatically: the transfor\u00admation is proven sound using an interactive theorem \nprover, which requires user involvement. For example, Young [35] has proven a code generator correct \nusing the Boyer-Moore theorem prover enhanced with an interactive interface [9]. Instead of proving that \nthe compiler is always correct, translation validation [26, 20] and credible compilation [28, 27] both \nattack the problem of checking the correctness of a given compilation run. Therefore, a bug in an optimiza\u00adtion \nonly appears when the compiler is run on a program that triggers the bug. Our work allows optimizations \nto be proven correct before the compiler is even run once. How\u00adever, to do so we require optimizations \nto be written in a special-purpose language. Our approach also requires the Cobalt execution engine to \nbe part of the TCB, while trans\u00adlation validation and credible compilation do not require trust in any \npart of the compiler. Proof-carrying code [19], certi.ed compilation [21], typed intermediate languages \n[34], and typed assembly lan\u00adguages [17, 18] have all been used to prove properties of programs generated \nby a compiler. However, the kinds of properties that these approaches have typically guaranteed are type \nsafety and memory safety. In our work, we prove the stronger property of semantic equivalence between \nthe original and resulting programs. 9. CONCLUSION We have presented an approach for automatically proving \nthe correctness of compiler optimizations. Our technique provides the optimization writer with a domain-speci.c \nlan\u00adguage, called Cobalt, for writing optimizations. Cobalt is both reasonably expressive and amenable \nto automated cor\u00adrectness reasoning. Using our technique we have proven cor\u00adrect implementations of several \noptimizations over a realistic intermediate language. We believe our approach is a promis\u00ading step toward \nthe goal of reliable and user-extensible com\u00adpilers. Acknowledgments This research is supported in part \nby NSF grants CCR\u00ad0073379 and ACI-0203908, a Microsoft Graduate Fellow\u00adship, an IBM Faculty Development \nAward, and by gifts from Sun Microsystems. We would also like to thank Keunwoo Lee, Andrew Petersen, \nMark Seigle and the anonymous re\u00adviewers for their useful suggestions on how to improve the paper. 10. \nREFERENCES [1] Patrick Cousot and Radhia Cousot. Abstract interpretation: A uni.ed lattice model for \nstatic analysis of programs by construction or approximation of .xpoints. In Conference Record of the \nFourth ACM Symposium on Principles of Programming Languages, pages 238 252, Los Angeles CA, January 1977. \n[2] Patrick Cousot and Radhia Cousot. Systematic design of program analysis frameworks. In Conference \nRecord of the Sixth ACM Symposium on Principles of Programming Languages, pages 269 282, San Antonio \nTX, January 1979. [3] Patrick Cousot and Radhia Cousot. Systematic design of program transformation frameworks \nby abstract interpretation. In Conference Record of the 29th ACM SIGPLAN-SIGACT Symposium on Principles \nof Programming Languages, Portland OR, January 2002. [4] Je.rey Dean, Greg DeFouw, Dave Grove, Vassily \nLitvinov, and Craig Chambers. Vortex: An optimizing compiler for object-oriented languages. In Proceedings \nof the 1996 ACM Conference on Object-Oriented Programming Systems, Languages, and Applications, pages \n83 100, San Jose CA, October 1996. [5] E.M. Clarke and E.A. Emerson. Synthesis of Synchronization Skeletons \nfor Branching Time Temporal Logic. In Logics of Programs: Workshop, volume 131 of Lecture Notes in Computer \nScience, Yorktown Heights, New York, May 1981. Springer-Verlag. [6] Cormac Flanagan, K. Rustan M. Leino, \nMark Lillibridge, Greg Nelson, James B. Saxe, and Raymie Stata. Extended static checking for Java. In \nProceedings of the ACM SIGPLAN 02 Conference on Programming Language Design and Implementation, June \n2002. [7] Dov Gabbay, Amir Pnueli, Saharon Shelah, and Jonathan Stavi. On the temporal analysis of fairness. \nIn Proceedings of the 7th ACM SIGPLAN-SIGACT symposium on Principles of programming languages, pages \n163 173, Las Vegas, Nevada, 1980. [8] J. Guttman, J. Ramsdell, and M. Wand. VLISP: a veri.ed implementation \nof Scheme. Lisp and Symbolic Compucation, 8(1-2):33 110, 1995. [9] M. Kau.mann and R.S. Boyer. The Boyer-Moore \ntheorem prover and its interactive enhancement. Computers and Mathematics with Applications, 29(2):27 \n62, 1995. [10] Jens Knoop, Oliver R\u00a8uthing, and Bernhard Ste.en. Optimal code motion: Theory and practice. \nACM Transactions on Programming Languages and Systems, 16(4):1117 1155, July 1994. [11] David Lacey, \nNeil D. Jones, Eric Van Wyk, and Carl Christian Frederiksen. Proving correctness of compiler optimizations \nby temporal logic. In Conference Record of the 29th ACM SIGPLAN-SIGACT Symposium on Principles of Programming \nLanguages, Portland OR, January 2002. [12] Sorin Lerner, David Grove, and Craig Chambers. Composing data.ow \nanalyses and transformations. In Conference Record of the 29th ACM SIGPLAN-SIGACT Symposium on Principles \nof Programming Languages, Portland OR, January 2002. [13] Sorin Lerner, Todd Millstein, and Craig Chambers. \nAutomatically proving the correctness of compiler optimizations. Technical Report UW-CSE-02-11-02, University \nof Washington, November 2002. [14] J. McCarthy and J. Painter. Correctness of a compiler for arithmetic \nexpressions. In T. J. Schwartz, editor, Proceedings of Symposia in Applied Mathematics,January 1967. \n[15] E. Morel and C. Renvoise. Global optimization by suppression of partial redundancies. Communications \nof the ACM, 22(2):96 103, February 1979. [16] F. Lockwood Morris. Advice on structuring compilers and \nproving them correct. In Conference Record of the 1st ACM SIGPLAN-SIGACT Symposium on Principles of Programming \nLanguages, Boston MA, January 1973. [17] Greg Morrisett, Karl Crary, Neal Glew, Dan Grossman, Richard \nSamuels, Frederick Smith, David Walker, Stephanie Weirich, and Steve Zdancewic. TALx86: A realistic typed \nassembly language. In 1999 ACM SIGPLAN Workshop on Compiler Support for System Software, pages 25 35, \nAtlanta GA, May 1999. [18] Greg Morrisett, David Walker, Karl Crary, and Neal Glew. From System F to \nTyped Assembly Language. ACM Transactions on Programming Languages and Systems, 21(3):528 569, May 1999. \n[19] George C. Necula. Proof-carrying code. In Conference Record of the 24th ACM SIGPLAN-SIGACT Symposium \non Principles of Programming Languages,Paris, France, January 1997. [20] George C. Necula. Translation \nvalidation for an optimizing compiler. In Proceedings of the ACM SIGPLAN Conference on Programming Language \nDesign and Implementation, pages 83 95, Vancouver, Canada, June 2000. [21] George C. Necula and Peter \nLee. The design and implementation of a certifying compiler. In Proceedings of the ACM SIGPLAN 98 Conference \non Programming Language Design and Implementation, Montreal, Canada, June 1998. [22] George C. Necula \nand Peter Lee. Proof generation in the Touchstone theorem prover. In Proceedings of the International \nConference on Automated Deduction, pages 25 44, Pittsburgh, Pennsylvania, June 2000. Springer-Verlag \nLNAI1831. [23] Greg Nelson and Derek C. Oppen. Simpli.cation by cooperating decision procedures. ACM \nTransactions on Programming Languages and Systems, 1(2):245 257, October 1979. [24] D. P. Oliva, J. Ramsdell, \nand M. Wand. The VLISP veri.ed PreScheme compiler. Lisp and Symbolic Computation, 8(1-2):111 182, 1995. \n[25] S. Owre, S. Rajan, J.M. Rushby, N. Shankar, and M.K. Srivas. PVS: Combining speci.cation, proof \nchecking, and model checking. In Computer-Aided Veri.cation, CAV 96, volume 1102 of Lecture Notes in \nComputer Science, pages 411 414, New Brunswick, NJ, July/August 1996. Springer-Verlag. [26] A. Pnueli, \nM. Siegel, and E. Singerman. Translation validation. In Tools and Algorithms for Construction and Analysis \nof Systems, TACAS 98, volume 1384 of Lecture Notes in Computer Science, pages 151 166, 1998. [27] Martin \nRinard. Credible compilation. Technical Report MIT-LCS-TR-776, Massachusetts Institute of Technology, \nMarch 1999. [28] Martin Rinard and Darko Marinov. Credible compilation. In Proceedings of the FLoC Workshop \nRun-Time Result Veri.cation, July 1999. [29] David A. Schmidt. Data.ow analysis is model checking of \nabstract interpretations. In Conference Record of the 25th ACM SIGPLAN-SIGACT Symposium on Principles \nof Programming Languages, San Diego CA, January 1998. [30] David A. Schmidt and Bernhard Ste.en. Data \n.ow analysis as model checking of abstract interpretations. In Giorgio Levi, editor, Proceedings of the \n5th International Symposium onStaticAnalysis(SAS), volume 1503 of LectureNotes in Computer Science(LNCS), \npages 351 380. Springer-Verlag, September 1998. [31] Simplify automatic theorem prover home page, http://research.compaq.com/SRC/esc/Simplify.html. \n[32] Bernhard Ste.en. Data .ow analysis as model checking. In T. Ito and A.R.Meyer,editors, Theoretical \nAspects of Computer Science(TACS), Sendai (Japan), volume 526 of LectureNotes in Computer Science(LNCS), \npages 346 364. Springer-Verlag, September 1991. [33] Bernhard Ste.en. Generating data.ow analysis algorithms \nfor model speci.cations. Science of Computer Programming, 21(2):115 139, 1993. [34] David Tarditi, Greg \nMorrisett, Perry Cheng, Chris Stone, Robert Harper, and Peter Lee. TIL: A type-directed optimizing compiler \nfor ML. In Proceedings of the ACM SIGPLAN 96 Conference on Programming Language Design and Implementation, \nPhiladelphia PA, May 1996. [35] William D. Young. A mechanically veri.ed code generator. Journal of Automated \nReasoning, 5(4):493 518, December 1989.    \n\t\t\t", "proc_id": "781131", "abstract": "We describe a technique for automatically proving compiler optimizations <i>sound</i>, meaning that their transformations are always semantics-preserving. We first present a domain-specific language, called Cobalt, for implementing optimizations as guarded rewrite rules. Cobalt optimizations operate over a C-like intermediate representation including unstructured control flow, pointers to local variables and dynamically allocated memory, and recursive procedures. Then we describe a technique for automatically proving the soundness of Cobalt optimizations. Our technique requires an automatic theorem prover to discharge a small set of simple, optimization-specific proof obligations for each optimization. We have written a variety of forward and backward intraprocedural dataflow optimizations in Cobalt, including constant propagation and folding, branch folding, full and partial redundancy elimination, full and partial dead assignment elimination, and simple forms of points-to analysis. We implemented our soundness-checking strategy using the Simplify automatic theorem prover, and we have used this implementation to automatically prove our optimizations correct. Our checker found many subtle bugs during the course of developing our optimizations. We also implemented an execution engine for Cobalt optimizations as part of the Whirlwind compiler infrastructure.", "authors": [{"name": "Sorin Lerner", "author_profile_id": "81100399150", "affiliation": "University of Washington", "person_id": "PP43119616", "email_address": "", "orcid_id": ""}, {"name": "Todd Millstein", "author_profile_id": "81100018064", "affiliation": "University of Washington", "person_id": "PP14019523", "email_address": "", "orcid_id": ""}, {"name": "Craig Chambers", "author_profile_id": "81100528252", "affiliation": "University of Washington", "person_id": "PP39047060", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/781131.781156", "year": "2003", "article_id": "781156", "conference": "PLDI", "title": "Automatically proving the correctness of compiler optimizations", "url": "http://dl.acm.org/citation.cfm?id=781156"}