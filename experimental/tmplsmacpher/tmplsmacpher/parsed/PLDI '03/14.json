{"article_publication_date": "05-09-2003", "fulltext": "\n Debugging Temporal Speci.cations with Concept Analysis Glenn Ammons * David Mandelin James R. Larus \n\u00a7 ammons@us.ibm.com Rastislav Bod\u00b4ik  larus@microsoft.com {mandelin,bodik}@cs.berkeley.edu ABSTRACT \nProgram veri.cation tools (such as model checkers and static ana\u00adlyzers) can .nd many errors in programs. \nThese tools need formal speci.cations of correct program behavior, but writing a correct speci.cation \nis dif.cult, just as writing a correct program is dif.\u00adcult. Thus, just as we need methods for debugging \nprograms, we need methods for debugging speci.cations. This paper describes a novel method for debugging \nformal, tem\u00adporal speci.cations. Our method exploits the short program execu\u00adtion traces that program \nveri.cation tools generate from speci.ca\u00adtion violations and that speci.cation miners extract from programs. \nManually examining these traces is a straightforward way to de\u00adbug a speci.cation, but this method is \ntedious and error-prone be\u00adcause there may be hundreds or thousands of traces to inspect. Our method \nuses concept analysis to automatically group the traces into highly similar clusters. By examining clusters \ninstead of individual traces, a person can debug a speci.cation with less work. To test our method, we \nimplemented a tool, Cable, for debug\u00adging speci.cations. We have used Cable to debug speci.cations produced \nby Strauss, our speci.cation miner. We found that us\u00ading Cable to debug these speci.cations requires, \non average, less than one third as many user decisions as debugging by examining all traces requires. \nIn one case, using Cable required only 28 deci\u00adsions, while debugging by examining all traces required \n224. Categories and Subject Descriptors D.2.5 [Software Engineering]: Testing and Debugging De\u00adbugging \naids; D.2.4 [Software Engineering]: Software/Program Veri.cation; D.3.4 [Programming Languages]: Processors \nDebuggers; I.5.3 [Pattern Recognition]: Clustering Similar\u00adity Measures *Department of Computer Sciences, \nUniversity of Wisconsin, Madison, Wisconsin, USA. Department of Electrical Engineering and Computer Sciences, \nUniversity of California, Berkeley, California, USA. IBM T.J. Watson Research Center, Hawthorne, New \nYork, USA. \u00a7Microsoft Research, Redmond, Washington, USA. Permission to make digital or hard copies of \nall or part of this work for personal or classroom use is granted without fee provided that copies are \nnot made or distributed for pro.t or commercial advantage and that copies General Terms Veri.cation, \nHuman Factors, Experimentation  Keywords temporal speci.cations, speci.cation debuggers, concept analy\u00adsis, \nhierarchical clustering 1. INTRODUCTION Program veri.cation tools [3 7, 12, 15, 16, 20, 23] can .nd \nmany errors in programs. These tools need formal speci.cations of correct program behavior, but writing \na correct speci.cation is dif.cult, just as writing a correct program is dif.cult. One partial solution \nis speci.cation mining [2], but speci.cation miners can produce buggy speci.cations. If program veri.cation \ntools are to be more effective and widely used, we need methods for de\u00adbugging speci.cations, because \nwithout these methods, too few speci.cations will be developed. Very small speci.cations can be debugged \nby inspection. A natural way to debug a more complicated formal speci.cation is by testing it. Conceptually, \nto test a speci.cation, the speci.ca\u00adtion s author uses a program veri.cation tool to check the speci\u00ad.cation \nagainst several programs. The tool .nds inconsistencies between the program and the speci.cation and \nreports them to the author. The author is supposed to look at each inconsistency and decide if the inconsistency \nis caused by a speci.cation error. If the cause is a speci.cation error, it should be .xed. In particular, \na temporal speci.cation can be expressed as a .\u00adnite automaton (FA) that accepts some program execution \ntraces and rejects others. A tool that veri.es temporal speci.cations generates short program execution \ntraces that appear to occur in the program but are not accepted by the FA. To debug a tempo\u00adral speci.cation \nby testing, the speci.cation author looks at each trace and decides whether the trace demonstrates an \nerror or not. If the trace is not erroneous, it should be added to the language of the FA. A similar \nmethod works for debugging temporal speci.cations found by a speci.cation miner. Given data collected \nduring a few runs of one or more programs, the miner generates a large number of short scenario traces \nand infers a speci.cation FA from them; if some of the runs contain errors (as often happens), some of \nthe scenario traces are also erroneous, and the miner learns an FA that accepts erroneous traces. Worse, \nthis FA is bear this notice and the full citation on the .rst page. To copy otherwise, to usually more \ncomplicated than an FA that accepts only correct republish, to post on servers or to redistribute to \nlists, requires prior speci.c traces, so it is hard to debug by inspection. To debug such a permission \nand/or a fee. speci.cation, a speci.cation expert looks at each scenario trace PLDI 03, June 9 11, 2003, \nSan Diego, California, USA. Copyright 2003 ACM 1-58113-662-5/03/0006 ...$5.00. and decides whether it \nis erroneous or not. If the scenario trace is erroneous, then the expert tells the miner to ignore it \nwhen inferring a correct speci.cation. These debugging methods are tedious and error-prone because a \nperson must inspect many traces some program veri.cation tools and miners generate hundreds or thousands \nof traces [2, 4, 15]. This paper describes a novel method for debugging formal, temporal speci.cations \nthat allows a person to take all of the traces into consideration without individually inspecting every \ntrace. In our method, an automatic tool .nds similarities within a set of program execution traces and \nuses concept analysis [24] to cluster similar traces together. The user inspects clusters of traces summarized \nin various ways instead of individual traces. Ideally, instead of looking at thousands of individual \ntraces, a speci.cation author can use our method to look at a few clusters of similar traces. For each \ncluster, the author views a summary of the cluster such as a .nite automaton that recognizes the cluster \ns traces and decides en masse whether to classify the cluster s traces as erroneous or not. Concept analysis \nclusters objects hierarchically, producing a concept lattice of small clusters and big clusters, with \nsmall clus\u00adters contained within big clusters. Moreover (and this is a key property), the traces in small \nclusters are more alike than the traces in big clusters. Hierarchical clustering is essential. The ideal \nclustering tool would divide the traces into two clusters: a cluster of traces that the author would \nclassify as erroneous and a cluster of traces that the author would classify as correct. Unfortunately, \nthis ideal can not be attained. Any real tool can produce mixed clusters, which contain both erroneous \ntraces and correct traces. Hierarchical clustering solves this problem: a speci.cation author who is \npre\u00adsented with a mixed cluster can choose to look at the smaller clusters within it. These clusters \nare less likely to be mixed be\u00adcause they are smaller and because the traces within them are more similar. \nHierarchical clustering has bene.ts beyond splitting mixed clus\u00adters: Small clusters are easier to understand \nand judge as correct or incorrect than large clusters, but it takes more small clusters than large clusters \nto cover the entire set of traces. Hierar\u00adchical clustering allows the user to choose to examine small \nclusters, large clusters, or a mixture of both.  Clusters overlap, so the user can check his classi.cation \ndeci\u00adsions by viewing summaries of the intersections and unions of clusters. For example, a speci.cation \nauthor who believes he has found a number of erroneous traces can view a summary of all erroneous traces \nin a particular cluster: the summary should be consistent with his belief.  Our method de.nes the similarity \nof a set of traces in terms of the transitions of an FA that recognizes traces. We regard traces that \nexecute many transitions in common as more similar than traces that execute fewer transitions in common. \nThis de.nition is .exible because the FA can be varied; it is also intuitive, be\u00adcause the user is debugging \na speci.cation that is itself expressed in terms of an FA. The de.nition also enables our use of concept \nanalysis, which clusters objects with discrete attributes. In our case, objects represent traces and \nattributes represent FA transi\u00adtions. To test our method, we implemented a tool, Cable, for debug\u00adging \nspeci.cations. We have used Cable to debug speci.cations produced by Strauss, our speci.cation-miner \n[2]. The corrected speci.cations found 199 bugs in widely distributed X11 pro\u00adgrams, including serious \nrace conditions and performance bugs. We found that using Cable to debug these speci.cations requires \nless than one-third as many user decisions as debugging by ex\u00adamining all traces requires. In one case, \nusing Cable required only 28 decisions, while debugging by examining all traces re\u00adquired 224. We also \nfound that concept analysis is affordable: it never took longer than about 22 seconds to compute the \nconcept lattice. 1.1 Contributions This paper describes the following contributions: A novel method \nfor debugging temporal speci.cations based on hierarchical clustering. The method applies not only to \nmined speci.cations where it .lls a large hole left unex\u00adplored by our previous work [2] but also to \ntemporal speci\u00ad.cations from any source.  A .exible, intuitive de.nition of similarity for traces that \nal\u00adlows hierarchical clustering via concept analysis.  A tool, Cable, that helps debug speci.cations \nby presenting users with a simple interface for classifying traces by explor\u00ading a cluster hierarchy. \n  1.2 Organization of the paper The rest of the paper is organized as follows. Section 2 presents two \nexamples, which demonstrate how to debug speci.cations by examining clusters of traces. Section 3 presents \nconcept anal\u00adysis and shows how to apply it to clustering traces. The Cable tool is described in Section \n4, as are strategies for using it ef\u00adfectively. Section 5 evaluates the usefulness of Cable for debug\u00adging \nspeci.cations mined by Strauss. Section 6 discusses related work. Section 7 concludes the paper.  2. \nTWO EXAMPLES This section presents two examples, which demonstrate how to debug temporal speci.cations \nwith concept analysis. The .rst example demonstrates debugging with the aid of a veri.cation tool by \ntesting a speci.cation against a program, while the sec\u00adond example demonstrates debugging a mined speci.cation \nby inspecting the traces from which the miner inferred the speci.\u00adcations. We will refer to several FAs \nin this section and in the rest of this paper. Note that, in this paper, the start state of an FA is \nalways state 0, and double lines indicate an accepting state. 2.1 Debugging by testing Figure 1 shows \na buggy temporal speci.cation. In general, a temporal speci.cation captures the control and data .ow \nof pro\u00adgram operations in an FA. This example attempts to formalize a rule about the C stdio library. \nIn that library, a call to fopen opens a .le and returns a .le pointer for reading and writing the .le. \nThe .le pointer should eventually be closed with a call to fclose. By contrast, a call to popen opens \na pipe for com\u00admunication with another process. Like fopen, popenreturns a .le pointer. Unlike fopen, \nthe .le pointer returned by popen should be closed with a call to pclose. The speci.cation in Figure \n1 gets this wrong: it allows a call to fcloseon any .le pointer, regardless of its source. For all calls \nX = fopen() or X = popen(): X = popen() X = fopen() fread(X) fwrite(X)  Figure 1: An incorrect temporal \nspeci.cation. X = popen(); fread(X); fwrite(X); pclose(X) X = popen(); fread(X); fread(X); pclose(X) \nX = popen(); pclose(X) X = fopen(); fwrite(X) X = popen(); fread(X) X = fopen() X = fopen(); fread(X); \nfread(X); pclose(X) X = fopen(); fwrite(X); fwrite(X); pclose(X) X = fopen(); pclose(X) Figure 2: Several \nviolation traces that could be reported by veri.cation of the speci.cation in Figure 1. Suppose that \na speci.cation author is debugging this speci.\u00adcation by testing it against a program. The author starts \nby using a program veri.cation tool to .nd inconsistencies between the speci.cation and the program. \nThe tool analyzes the program and reports violation traces, which are program execution traces that demonstrate \nan apparent violation of the speci.cation. Tra\u00additionally, the author looks at each violation trace, \ndecides why it was reported, and takes an appropriate action. For the speci.ca\u00adtion in Figure 1, the \nviolation traces (see Figure 2 for examples) might include Traces that begin with a call to popen and \nend with a call to pclose. These traces are correct, so the author should change the speci.cation to \naccept these traces.  Traces that begin with a call to fopenor with a call to popen and end without \na call to fcloseor a call to pclose. These traces are erroneous, so the author should not change the \nspec\u00adi.cation.  Traces that begin with a call to fopen and end with a call to pclose. Again, these traces \nare erroneous, so the author should not change the speci.cation.  Unfortunately, the veri.cation tool \ndoes not summarize the vi\u00adolation traces as neatly as we just did. Instead, the tool lists each trace \nwith all of the calls it makes (not just the relevant calls we picked out in the above list), and in \nno particular order. For a sim\u00adple example like the one in Figure 1, it may be easy for the author to \ninspect the violation traces and understand them well enough to decide how to .x the speci.cation. However, \nif the violation traces are more complicated, inspecting each trace is both te\u00addious and error-prone. \nIf the tool reports hundreds or thousands X = popen() X = fopen() fread(X) fwrite(X) pclose(X)  Figure \n3: A small FA that recognizes violation traces from ver\u00adi.cation of the speci.cation in Figure 1. X = \npopen() X = fopen() fread(X) fwrite(X) pclose(X) Figure 4: A very small FA that recognizes violation \ntraces from veri.cation of the speci.cation in Figure 1. of complicated violations (as some do [4, 15]), \nthe problem is daunting. Now let us see how the author would debug this speci.ca\u00ad tion with our method. \nOur method has three steps. Step 1 auto\u00ad matically builds a concept lattice that summarizes the violation \ntraces, before the speci.cation author sees them. This step has three substeps: Step 1a This step .nds \na small reference FA that recognizes the violation traces and will be used to de.ne trace similarity. \nAl\u00adgorithms to learn a small FA that recognizes (at least) a set of strings have been studied extensively \nsee Murphy [18] for a good survey. However, an FA learning algorithm that per\u00adforms well on traditional \nmeasures, such as training set accu\u00adracy, is not needed for concept analysis. We only require that dissimilar \ntraces execute different transitions in the automa\u00adton (see Step 1b). For example, we have had success \nwith FAs that recognize all possible traces over the API. Figure 3 shows a small FA that recognizes violation \ntraces from veri.cation of the speci.cation in Figure 1. Figure 4 shows an automaton that recognizes \nall traces over the API. Step 1b This step uses a reference FA M to de.ne a measure of similarity for \nviolation traces. M recognizes a trace oiff there is an accepting sequence of M-transitions for o, which \nis a sequence (a0,... ,an) such that each transition ai is labeled by the ith event in o, the head of \na0 is the start state of M, and the tail of an is an accepting state of M.If an M-transition a is on \nan accepting sequence of M-transitions for o, we say that o executes a. Given a set O of violation traces, \nthe common M-transitions of Oare the M-transitions that are executed by every violation trace in O. The \nsimilarity of O with respect to M is the number of common M-transitions of O. Note that we want a reference \nFA that is useful for classi.ca\u00adtion. In particular, erroneous traces and correct traces should execute \ndifferent transitions, so that they are not considered highly similar. It is also helpful, but not necessary, \nif cor\u00adrect (erroneous) traces execute many of the same transitions ... X = popen() pclose(X)   \n Figure 5: Part of a concept lattice that might be induced by vi\u00adolation traces from veri.cation of \nthe speci.cation in Figure 1, with respect to the FA in Figure 3. as other correct (erroneous) traces, \nso that they are considered highly similar. De.ning similarity with respect to an FA has two bene.ts. \nFirst, by varying parameters of the FA-learning algorithm, the author can choose to use a large FA that \nmakes very .ne dis\u00adtinctions among traces or a smaller FA that makes coarser distinctions. For example, \nthe FA in Figure 3 distinguishes between traces that call popen before calling pclose and traces that \ncall pclosebefore calling popen, since the lat\u00adter execute no transitions in the FA. If the order did \nnot matter, a very small FA, such as the one given in Figure 4, could be used to induce a simpler concept \nlattice. On the other hand, if the order of calls to fread and fwrite also mattered, then a larger FA \ncould be used to induce a concept lattice that distinguished different orders. The second bene.t is that, \nsince the speci.cation itself is ex\u00adpressed as an FA, summarizing violation traces with FAs makes it \neasier for the author to see how to .x the speci.cation. Step 1c This step uses concept analysis to build \na concept lat\u00adtice; the nodes of the lattice are called concepts. A concept pairs a set of violation \ntraces with a set of FA transitions that are executed by every trace in the set. Concepts at the top \nof the concept lattice contain more traces but fewer transitions than concepts at the bottom of the lattice. \nThat is, according to our de.nition of similarity, the sets of traces in concepts get smaller but more \nsimilar as one moves down in the lattice. Figure 5 shows part of a concept lattice that might be induced \nby violation traces from veri.cation of the speci.cation in Figure 1, with respect to the FA in Figure \n3. The concept lattice is a neat summary of the violation traces. In Step 2 of our method, the speci.cation \nauthor uses Cable to display the lattice and to track his decisions about the traces in concepts. Step \n2 has two substeps: Step 2a In this step, the author records his decisions about traces by labeling traces. \nHis goal is to partition the traces into cor\u00adrect traces, which should be accepted by the correct speci.ca\u00adtion, \nand erroneous traces, which should not be accepted. The former he labels good , while the latter he labels \nbad .  For all calls X = fopen() or X = popen(): X = fopen() fread(X) fread(X) fwrite(X) fwrite(X) \n pclose(X) Figure 6: The result of debugging the speci.cation in Figure 1. The author is free to inspect \nconcepts in any order, although a mostly top-down approach seems to work best in practice. Section 4 \nsuggest several strategies, which are evaluated in Section 5. Suppose that the author .rst looks at the \nconcept that contains traces that execute X = popen(). The author asks Cable to display an FA that is \ninferred from the traces in that con\u00adcept. Because this automaton contains both erroneous traces and \ncorrect traces, the author decides to look at the concepts immediately below this concept. Each of these \nchild concepts contains a proper subset of the traces in the parent concept. Suppose that the .rst child \nconcept he looks at contains just traces that execute both X = popen() and pclose(X). These traces are \ncorrect, so the author labels them as good . Finally, suppose that the author revisits the concept that \ncon\u00adtains traces that execute X = popen(). He asks Cable to display an FA that is inferred from the unlabeled \ntraces in that concept. These traces execute X = popen() but not pclose(X), so they are erroneous. The \nauthor labels these traces as bad . At this point, the author has come to a deci\u00adsion about all of the \ntraces that execute X = popen(). The traces that execute X = fopen()remain, and the author la\u00adbels these \nin a similar fashion. Step 2b In this step, the author checks his labeling. Once all traces have been \nlabeled, the author views an FA that is in\u00adferred from all good traces. These traces should be accepted \nby the correct speci.cation. If the author made a mistake in his labeling, it will be revealed as the \npresence or absence of certain traces in the FA s language. Note that if the FA for all good traces is \ntoo complicated, the author can choose to view an FA inferred from the good traces within concepts below \nthe top of the lattice. If there is a mistake, the author searches through the lattice for concepts that \ncontain only traces that are incorrectly la\u00adbeled good , just as earlier he searched through the lattice \nfor traces that should be labeled good . Once the author is satis.ed that his labeling is correct, he \n.xes his speci.cation so that it accepts all good traces and continues to reject all bad traces: Step \n3 In this step, the author .xes his speci.cation. Note that although the author has not inspected every \nviolation trace, he has taken every violation trace into consideration. Con\u00adsequently, he can be more \ncon.dent that he has the right .x Traces from test runs  Front end Scenario traces apply STM  extract \nscenarios Back end learn NFASpecification Figure 7: Architecture of Strauss. for his speci.cation. \nFigure 6 shows the result of .xing the speci.cation in Figure 1. To summarize, our method has the following \nbene.ts: The concept lattice neatly summarizes complicated traces that the veri.cation tool lists in \nno particular order.  De.ning similarity with respect to an FA is .exible because the FA can be varied \nand intuitive because the speci.cation itself is expressed in terms of an FA.  The concept lattice allows \nthe author to take every trace into consideration without inspecting every trace.  The author can use \nthe lattice to check that he has made the right decision about every trace.   2.2 Debugging a mined \nspeci.cation A speci.cation miner is a tool for learning speci.cations. Fig\u00adure 7 shows the architecture \nof our miner, Strauss. Strauss has a front end and a back end. The front end extracts scenario traces \nfrom a training set of program execution traces. The details of how this occurs are discussed in a previous \npaper [2]. The sce\u00adnario traces may have bugs, because the training set may have bugs. The back end uses \nmachine learning techniques to learn a temporal speci.cation that accepts the scenario traces. Suppose \nthat Strauss learns the buggy speci.cation in Figure 1 from a set of scenario traces that include Traces \nthat begin with a call to popenand end with a call to fclose. These traces are erroneous, so they should \nnot be included in the correct speci.cation.  Traces that begin with a call to fopenand end with a call \nto fclose. These traces are correct, and should be included in the correct speci.cation.  An expert \ncan produce a correct speci.cation by rerunning the back end of Strauss only on the latter of the two \nkinds of traces above. Unfortunately, the traces are not summarized so neatly as they are above. In general, \nit is tedious and error-prone for the expert to inspect every scenario trace. Our solution is to summarize \nthe scenario traces neatly, before the expert sees them. The method is very similar to the method we \ndiscussed in Section 2.1. The differences are in Steps 1a and 3. In Step 1a, the expert does not need \nto .nd an FA that rec\u00adognizes the traces. He already has one: namely, the FA from the miner s buggy speci.cation. \nOn the other hand, if the miner infers an FA that makes unnecessarily .ne distinctions among X = popen(); \nfread(X); fwrite(X); pclose(X) X = popen(); fread(X); fread(X); pclose(X) X = popen(); pclose(X) X = \nfopen(); fread(X); fwrite(X); fclose(X) X = fopen(); fread(X); fread(X); fclose(X) X = fopen(); fclose(X) \n Figure 8: Several scenario traces. traces, the expert may choose to use a different FA. In our expe\u00adrience, \nhowever, the inferred FA is usually a good starting point. Steps 1b and 1c are just as in Section 2.1: \nthe expert supplies a reference FA, which de.nes a measure of similarity for traces and a concept lattice. \nStep 2 is the same, too: the expert uses Cable to label as good the scenario traces that belong in the \ncorrect speci.cation and to label as bad the scenario traces that don t belong. The expert .xes the speci.cation \nin Step 3. In Section 2.1, the speci.cation author did this manually. In mining, the expert just runs \nthe back end of the miner on the traces that have been labeled good . There is a further problem, however. \nA useful miner also gen\u00aderalizes: the speci.cation accepts some traces that were not in its training \nset, but are similar to traces in the training set. For example, a miner given the good scenario traces \nin Figure 8 would ideally produce an FA that accepts any number of calls to freadand fwritebetween calls \nto popenand pcloseand between calls to fopenand fclose. Unfortunately, in gener\u00adalizing, the miner can \nmake mistakes: in this case, a miner might produce an FA that allows a call to popen to be followed by \na call to fclose. To address this problem, the expert can vary parameters on the miner, but a more frequently \nfruitful solution is to further subdivide the training set and apply the miner separately to each division. \nIn our example, if the expert observes that the miner overgeneralizes, he would redo Step 2 and assign \nseveral differ\u00adent kinds of good labels. Here, the expert would assign a label good fopen and another \nlabel good popen . Next, the expert would run the miner s back end twice, once on the good fopen traces \nand once on the good popen traces. Because the miner sees each class of traces separately, it can not \nconfuse them. The .nal speci.cation would be the union of the speci.cation for good fopen traces with \nthe speci.cation for good popen traces.  3. APPLYING CONCEPT ANALYSIS Concept analysis [24] is a hierarchical \nclustering technique for objects with discrete attributes. This section reviews concept analysis and \nexplains how to use it to cluster program execution traces with respect to a temporal speci.cation. In \nthe process, we de.ne a natural measure of the similarity of a set of traces and show that concept analysis \nbuilds a hierarchy of clusters of traces where small clusters are more similar than the large clus\u00adters \nthat contain them. This property allows a user of Cable to choose between labeling many small and highly \nsimilar clusters and labeling a few larger but less similar clusters. 3.1 Concept analysis The input \nto concept analysis is a set O of objects, a set A    4-legged hairy smart marine thumbed cats yes \nyes dogs yes yes dolphins yes yes gibbons yes yes yes humans yes yes whales yes yes Figure 9: A context \nwhere the objects are animals and the at\u00adtributes are adjectives that describe animals. of attributes, \nand a context R . O \u00d7 A that relates objects to attributes. Figure 9 shows an example where the objects \nare ani\u00admals and the attributes are adjectives that describe animals. 1 Given O, A, and R, concept analysis \n.nds concepts. A con\u00adcept pairs a set of objects X with a related set of attributes Y: Y is exactly the \nset of attributes enjoyed by all objects in X, and X is exactly the set of objects that enjoy all of \nthe attributes in Y. To de.ne concepts formally, the standard formulation de.nes two mappings sR :2O \n. 2A and tR :2A . 2O . For any X . O and Y . A, sR(X)= {a. A|.x. X.(x,a) . R}tR(Y)= {o . O|.y . Y.(o,y) \n. R} The formal de.nition of a concept is as follows: (X,Y) is a concept iff sR(X)= Y and tR(Y)= X. X \nis called the extent of the concept and Y is called the intent of the concept. c0 = ({cats, dogs, dolphins, \ngibbons, humans, whales}, {}) c1 = ({cats, dogs, gibbons}, {hairy}) c2 = ({dolphins, gibbons, humans, \nwhales}, {smart}) c3 = ({gibbons, humans}, {smart, thumbed}) c4 = ({cats, dogs}, {4-legged, hairy}) c5 \n= ({gibbons}, {hairy, smart, thumbed}) c6 = ({dolphins, whales}, {smart, marine}) c7 = ({}, {4-legged, \nhairy, smart, marine, thumbed}) c0 c7 The choice of O, A, and Runiquely de.nes a set of concepts. Concepts \nare partially ordered under the ordering =R, de.ned as follows: (X0,Y0) =R (X1,Y1) iff X0 . X1. This \npartial order induces a complete lattice on concepts, called the concept lattice. Figure 10 shows the \nconcept lattice for the example in Figure 9. In general, the top concept of a concept lattice is the \nconcept with all objects and the bottom concept is the concept with all attributes. In the example, the \ntop concept is c0, and the bottom concept is c7. 1We took this example from Michael Siff s thesis [22]. \nBy de.nition, the concept lattice is a subset lattice on objects. In fact, the concept lattice is also \na superset lattice on attributes. That is, (X0,Y0) =R (X1,Y1) iff Y0 . Y1. This fact allows the de.nition \nof a measure of similarity that increases as one moves down in the concept lattice. De.ne the similarity \nof X . O by sim(X)= |sR(X)|. That is, the similarity of X is simply the number of attributes shared by \nall objects in X. Because the concept lattice is a superset lattice on attributes, if (X0,Y0) and (X1,Y1) \nare concepts with X0 . X1, then sim(X0) = sim(X1). 3.1.1 Ef.ciency of concept analysis There are several \nalgorithms for building concept lattices. The algorithm we use is due to Godin and others [13] (we use \ntheir Algorithm 1). Let k be an upper bound on |sR({o})|, where o . O. That is, k is an upper bound on \nthe number of attributes enjoyed by any object in O. Then, their algorithm runs in time 2k O(2|O|) In \nour applications, kis typically less than ten, while |O| ranged up to the hundreds. Our measurements \n(see Section 5) show that the algorithm is practical, terminating in less than 22 seconds on our largest \ndata set, which contained 496 traces.  3.2 Clustering traces To cluster traces with concept analysis, \nwe need to de.ne O, A, and R: O The objects are the traces themselves. A We either have in hand or can \ninfer an FA M that recognizes the traces. The attributes are the transitions of M. R Let AS(o) be the \nset of all accepting sequences of M-transitions for the trace o. We de.ne Rby R = {(o,a) . O\u00d7 A|.s. AS(o).s \n=(... ,a,...)} That is, (o,a) . R iff o executes a. R can be computed by simulating each trace on the \n.nite automaton. This is a natural choice of R, which matches the intuition that traces with many common \ntransitions are more alike than traces with few common transitions. Also, Rprovides a direct connection \nbetween traces and the speci.cation that the user is debugging. This connection is useful for answering \nques\u00adtions such as Which parts of the speci.cation matter for these traces? and Which traces would be \naffected by a change to this part of the speci.cation? . Our de.nition of similarity with respect to \nan FA ignores the order in which transitions are executed. There are two good rea\u00adsons to ignore the \norder of transitions. First, the number of pos\u00adsible orders grows exponentially with the amount of history \nthat is tracked. Second, by changing the FA, our de.nition of similarity can simulate de.nitions that \ntrack the order of transitions. The FA already constrains the order in which transitions may execute. \nThus, by distinguishing traces that execute different sets of tran\u00adsitions, the FA also makes some distinctions \namong traces that execute transitions in different orders. If more ordering informa\u00adtion is desired, \nthe FA can be modi.ed to make .ner distinctions (see the discussion in Section 2.1, Step 1b).  4. CABLE \nThe section describes Cable, our tool for debugging speci.ca\u00adtions. Cable displays the concept lattices \nde.ned in Section 3.2 and enables a speci.cation author or other expert to view sum\u00admaries of concepts \nand to decide en masse whether the traces in a concept are erroneous or not. The rest of this section \nexplains Cable s interface, discusses strategies for using Cable effectively, and explains on which lattices \nCable works best. 4.1 The Cable interface Cable, which is based on the Dotty [11] and Grappa [17] graph \nvisualization tools, displays the concept lattice and allows the user to view summaries of concepts and \nto decide en masse whether the traces in a concept are erroneous or not. The user records his decision \nabout a set of traces by labeling the traces in the set. His goal is to partition the traces into a set \nof erroneous traces, labeled bad , and a set of correct traces, labeled good . For example, if a speci.cation \nauthor decides that certain vio\u00adlation traces do not demonstrate a program error, he gives those traces \nthe label good . On the other hand, the author gives vio\u00adlation traces that do demonstrate program errors \nthe label bad . The author s goal is to label every trace; when he is done, he uses Cable to view the \ntraces labeled good and .xes his speci.ca\u00adtion accordingly. An expert uses Cable to debug a miner s speci.cation \nin a sim\u00adilar fashion. If the expert decides that certain scenario traces are not erroneous, he labels \nthem good . Scenario traces that are erroneous are labeled bad . After the expert has labeled ev\u00adery \ntrace, he uses Cable to view the traces labeled good and reruns the miner on those traces. Labels are \na .exible mecha\u00adnism: as Section 2.2 discussed, the expert can avoid problems with overgeneralization \nby assigning several different kinds of good labels and running the miner several times. A user of Cable \ncan label all of a concept s traces at once. Be\u00adcause concepts belong to a lattice, labeling the traces \nin one con\u00adcept affects the labels on traces in other concepts. Labeling all of the traces in a descendant \nconcept also labels some of the traces in an ancestor concept, labeling all of the traces in an ancestor \nconcept also labels all of the traces in a descendant concept, and labeling all of the traces in a cousin \nor sibling concept might label some of the traces in another cousin or sibling concept. Consequently, \nCable keeps track of which traces have been labeled, ensures that each trace receives no more than one \nlabel, and gives the user visual feedback that makes it obvious which concepts still have unlabeled traces. \nAt any time, each concept in the lattice is in one of three states: Unlabeled The concept has unlabeled \ntraces, and no traces that are labeled. Cable displays Unlabeled concepts in green. PartlyLabeled The \nconcept has some unlabeled traces and some labeled traces. Cable displays PartlyLabeled concepts in yel\u00ad \nlow. FullyLabeled The concept has no unlabeled traces. Cable dis\u00adplays FullyLabeled concepts in red. \nNote that the empty concept (the concept with no traces) is always FullyLabeled. Cable s Label traces \ncommand allows the user to assign la\u00adbels to selected traces in a concept: Label traces If some traces \nalready have labels, then Cable asks the user which traces to label: the user may choose to label all \nof the traces, only the unlabeled traces, or only the traces with a given label. Then, Cable prompts \nthe user for a label and gives that label to the selected traces. Because no trace may have more than \none label, the new label replaces any existing labels. If no traces have labels, then Cable prompts the \nuser for a label and gives that label to all of the concept s traces. A Cable user bases his labeling \ndecisions on concept views. Cable supports the following views of a concept: FA Cable uses an FA learner \n(Raman and Patrick s sk-strings learner [21]) to construct a summary FA that accepts concept traces and \nthen displays this FA. In our experience with Cable, this was the most frequently used concept view. \nCable uses Raman and Patrick s sk-strings learner to construct FAs. If the concept is PartlyLabeled or \nFullyLabeled, then the user can choose which concept traces to include in the view: the user can choose \nto include all traces, only unlabeled traces, or only traces with a given label. Cable constructs the \nsummary FA only from included traces (we say that these traces are in the view). This feature is particularly \nuseful once all concepts are FullyLabeled: the user can obtain an FA for all traces with a particular \nlabel l by viewing the FA of l-labeled traces in the top concept. The FA view also provides selection \nby transitions, which en\u00adables the user to .nd traces that execute or do not execute selected transitions \nin the summary FA. By clicking on transi\u00adtions in the FA view, the user selects a set of included transi\u00adtions \nand a set of excluded transitions. These selections corre\u00adspond to a selection of traces: a trace is \nselected iff it is in the view and it executes all included transitions and no excluded transitions. \nSelection by transitions enables navigation by transitions. Af\u00adter transitions (and hence traces) are \nselected, Cable will nav\u00adigate the user to the smallest concept that contains a superset of the selected \ntraces. This concept is smaller and more simi\u00adlar than the current concept (unless the lattice forces \nCable to select the current concept). Transition selection thus equips the user with control over which \nof the smaller, more similar concepts he would like to examine next. Transitions This view displays the \ntransitions in the reference FA that belong to the concept. In our experience, this has been the second \nmost frequently used view because we often know that the label for a trace depends on whether the trace \nexecutes a certain set of transitions in the reference FA or not. As with the FA view, if the concept \nis PartlyLabeled or Fully-Labeled, the user can choose which concept traces to include in the view. Traces \nThis view displays the traces in the concept. We do not use this view very often because it usually generates \nmore output than we can understand. As with the FA view, if the concept is PartlyLabeled or Fully-Labeled, \nthe user can choose which concept traces to include in the view. Finally, the user can choose a new FA \nand use it to cluster concept traces: Focus Cable starts a sub-session, which focuses on a single con\u00adcept \ns traces. Cable prompts the user for a reference FA to use for the session, and clusters the traces in \nthe focused session with that FA. The user can end a focused session at any time, at which time any labels \nthat he assigned are automatically merged into the original session. In our experiments, we always started \nCable with a cluster de\u00adtermined by our miner s inferred FA. If this cluster appeared complicated, we \nsometimes started a focused session. The reference FAs that we used for focusing followed one of the \nfollowing three templates: Unordered FAs that follow this template distinguish among traces based on \nwhich events appear in traces, while com\u00adpletely ignoring the order in which events appear: (event0 | \nevent1 | ... | eventN)* where event0through eventNare the events that oc\u00adcur on transitions in the inferred \nFA. FAs that follow the unordered template work well when correct traces and erroneous traces often contain \ndifferent events. Name projection FAs that follow this template distinguish traces based on a single \nname, say X, that occurs in the inferred FA: (event0(... X ...) | event1(... X ...) | ... | eventN(... \nX ...) | wildcard)* where event0 through eventN are events that occur on transitions in the inferred \nFA, and wildcard matches any event. Name projections work well when the in\u00adferred FA mentions several \nnames, because they allow the user to check correctness with respect to one name at a time. The template \nabove pays no attention to the order of events; more generally, name projections can be any FA (that \naccepts the traces) whose transitions are all labeled by wildcard or by an event that refers to X. Seed \norder FAs that follow this template distinguish among traces based on which events appear before a distinguished \nseed event and which events appear after the seed event: (event0 | event1 | ... | eventN)*; event [seed]; \n(event0 | event1 | ... | eventN)* where event0through eventNare the events that oc\u00adcur on transitions \nin the inferred FA. Because FAs that follow the seed order template pay attention to ordering, they can \ndistinguish traces that cannot be distinguished by an unordered FA. However, the ordering is very lim\u00adited, \nso the concept lattice stays small. 4.2 Strategies for using Cable Cable allows the user to inspect \nand label concepts in any or\u00ad der. Some orders are better than others, however. To use Strauss effectively, \na user should have some strategy for selecting con\u00ad cepts to inspect and label. An important question \nis how much does the user s choice of strategy matter? . To answer that question, this section de.nes \nseveral common-sense strategies whose cost can be measured automatically, given a reference labeling \nfor the traces, and Sec\u00adtion 5 measures the relative performance of these strategies on several labelings. \nWe measure the cost of a strategy by counting the number of Cable operations inspecting concepts and \nlabeling traces that the strategy performs. We include the cost of inspecting con\u00adcepts, because otherwise \nan optimal strategy could inspect every concept and use that perfect information to minimize the number \nof labeling operations. Including the cost of labeling is not as es\u00adsential, but we include it because \notherwise an optimal strategy could include redundant labeling operations. Note that we do not allow \na strategy to label a concept without inspecting it .rst. The strategies are Top-down This strategy visits \nUnlabeled and PartiallyLabeled concepts in a random order, subject to the constraints that no concept \nmay be visited unless one its parents has already been visited and that no concept may be visited if \ncarries an in\u00adspected mark. At each visit, the strategy marks the concept as inspected and inspects the \nconcept s unlabeled traces. If all unlabeled traces should receive the same label, then the strategy \nlabels them. Labeling a concept can make it possi\u00adble to label the concept s ancestors, because labeling \nthe con\u00adcept changes the sets of unlabeled traces in the ancestors. For this reason, when a concept is \nlabeled, the strategy clears the marks on the concept s ancestors.. The advantage of this strategy is \nthat, because it is biased to\u00adward visiting concepts near the top of the lattice, it is likely to exploit \nopportunities to label many traces at once. The dis\u00adadvantage of this strategy is that it may visit many \nconcepts that cannot be labeled because they include traces that should receive different labels. Bottom-up \nThis strategy loops over the concept lattice until all concepts are FullyLabeled, visiting concepts in \na bottom-up order. On each iteration, the strategy visits a concept that is not FullyLabeled but whose \nchildren are all FullyLabeled. The advantage of this strategy is that it never visits concepts that cannot \nbe labeled because they are too general. The dis\u00adadvantage is that it misses most opportunities to label \nmany traces at once. Random This strategy visits concepts in random order, never visiting FullyLabeled \nconcepts and stopping when all con\u00adcepts are FullyLabeled. Optimal This strategy visits concepts in an \noptimal order. An optimal order is an order that minimizes the cost. Real users do not follow any of \nthese strategies exactly. One reason is that a real user is limited: for example, even when all of a \nconcept s traces should receive the same label, the user might need to inspect the concept s subconcepts \nto convince himself of that fact. Another reason is that a real user makes heuristic decisions: for example, \nhe may realize that a certain concept should be visited .rst because it has an interesting transition \nin its attribute set. 4.3 Well-formed lattices Because Cable only allows the user to label the traces \nin con\u00adcepts en masse (with the Label traces command), a bad con\u00adcept lattice can make it impossible \nfor the user to give traces a desired labeling. We say that such lattices are not well-formed for the \ndesired labeling. A well-formed lattice for a labeling is a lattice where every concept is well-formed \nfor that labeling. We de.ne a well-formed concept recursively; a concept c is well-formed for a labeling \niff one of the following cases holds: 1. The labeling gives the same label to every trace in c. 2. All \nof the children of c are well-formed for the labeling, and the labeling gives the same label to every \ntrace in c that is not in a child of c.  Intuitively, a concept c is well-formed for a labeling if there \nis a sequence of Label traces commands that puts c in the Fully-Labeled state with the desired labeling. \nThe .rst case says that c can be put in the FullyLabeled state simply by labeling its traces. The second \ncase says that c can be put in the FullyLabeled state by putting all of its children in the FullyLabeled \nstate and then labeling the unlabeled traces of c. If the concept lattice is not well-formed, it is impossible \nto label all of the traces with Cable, without changing the FA. In particular, none of the above strategies \nwould succeed. It is easy to see how lattices that are not well-formed could arise. For example, suppose \nthat it is correct to call a routine foo an even number of times and incorrect to call foo an odd number \nof times. Consider a buggy speci.cation whose FA has one accepting state and one transition to and from \nthat state on foo. This speci.cation accepts all sequences of foocalls. The concept lattice induced by \nthis speci.cation and any set of traces would put all sequences of calls to foo in the same concept, \nsince they all exercise the sole FA transition. That concept, and hence the whole lattice, would not \nbe well-formed for the label\u00ading that labels correct traces as good and incorrect traces as bad . The \nuser does have some options when presented with a lattice that is not well-formed. One option is to change \nthe FA and con\u00adstruct a better concept lattice, using Cable s Focus command. Another option is to label \nconcepts that are not well-formed as mixed ; the user can label the traces in those concepts by hand, \nor use our method again, with a different FA and with the set of traces restricted to the mixed traces. \n  5. EXPERIMENTAL RESULTS This section evaluates the usefulness of Cable for debugging speci.cations \nmined by Strauss [2]. We analyzed traces from full runs of 72 programs that use the Xlib and X Toolkit \nIntrinsics libraries for the X11 windowing system; in all, we collected 90 traces. The programs came \nfrom the X11 distribution, the X11 contrib directory, and from the pro\u00adgrams installed for general use \nat the University of Wisconsin. Each trace records events for all X library calls and for all call\u00adbacks \nfrom the X library to client code. Measurements of running time were taken on an Ultra Enter\u00adprise 6000 \nServer; the machine uses 248 Mhz SPARCv9 proces\u00adsors (we used one processor only) and runs Solaris 5.8. \n5.1 Speci.cations We used Cable to debug seventeen Strauss speci.cations. For each speci.cation, Table \n1 lists the number of states and tran\u00adsitions in the speci.cation s FA (after debugging) and translates \nthe speci.cation into English. These are important speci.cations. Using a dynamic checker (described \nin earlier work [2] and more completely in a disser\u00adtation by one of the authors [1]), we searched for \nviolations of these speci.cations in program execution traces and found a to\u00adtal of 199 bugs, including \nresource leaks, potential races, and performance bugs. All of these speci.cations are fairly simple, \nand none of them contain loops. Consequently, the longest trace through each FA is very short, usually \nless than ten events long. Debugging spec\u00adi.cations that accept such short traces is actually a worst \ncase for our method because when Strauss s front end generates short scenario traces, it does not generate \nvery many unique scenario traces. So, it is relatively easy to debug these speci.cations simply by looking \nat a representative from each set of identi\u00adcal traces. Nonetheless, the results in Section 5.3 show \nthat our method was better than this brute-force method. 5.2 Cost of concept analysis The statistics \nin Table 2 demonstrate that concept analysis is affordable. The times in the table do not include reading \nand parsing the traces, nor do they include writing the .nal lattice to disk. The reported time is the \nshortest time from three runs; the time for each run did not vary signi.cantly. Since Strauss extracted \nmany identical scenario traces, we built the lattice from representatives for classes of identical traces, \nrather than from all of the traces. Although concept lattices are potentially exponentially large in \nthe number of objects or attributes (whichever is lower), the size of the lattices generated for our \nspeci.cations varied roughly linearly with the number of FA transitions. The times seem to vary slightly \nworse than linearly, but it is hard to tell for sure, since many of the times were so short. These observations \nagree with the more thorough empirical evaluation that Godin and oth\u00aders did of their algorithm (which \nwe use) in their paper [13].  5.3 Traversal strategies Table 3 compares the cost of labeling by a variety \nof meth\u00adods, where cost is de.ned as in Section 4.2. One of the au\u00adthors (an expert user and developer \nof the tool) used Cable to debug each speci.cation and create an accurate labeling. Then, we measured \nthe cost of obtaining the same labeling with each method. Because the Top-down, Bottom-up, and Random \nstrate\u00adgies have non-deterministic costs, Table 3 reports the lowest cost for Bottom-up and the arithmetic \nmean and standard deviation of the cost of 1024 trials for Top-down and Random. We were un\u00adable to measure \nthe cost of the Optimal strategy for RegionsBig and XSaveContext, because the program we wrote to evaluate \nthe strategies on these speci.cations took too long to run. For those speci.cations, we report a lower \nbound on the cost of Op\u00adtimal. In addition to the strategies listed in Section 4.2, Table 3 lists two \nother methods: Expert This method measured the actual cost of labeling for the expert user. The expert \nused a mostly top-down approach, but sometimes directed his search based on transitions he found interesting. \nOn 5 speci.cations (RegionsBig, XSaveContext, XGContextFromGC, XtFree, and XtRealizeProc), the expert \nalso used Cable s Focus command, using reference FAs that matched the templates described in Section \n4.1. This was an advantage for the expert, since the automatic strategies did not use this feature of \nCable. Name FA |Q| |T| Bugs True False English description PrsAccelTbl 13 22 0 1 Accelerator tables \nmust be parsed with XtParseAcceleratorTablebefore they are used. PrsTransTbl 4 5 0 0 Translation tables \nmust be parsed with XtParseTranslationTablebefore they are used. Quarks 10 13 0 0 The name and the class \narguments of XrmQGetResource must come from calls to XrmPermStringToQuark. RegionsAlloc 30 35 16 0 Every \nRegion that is created by the program must eventually be destroyed by the program; every Region that \nis destroyed by the program must have been created by the program. RegionsBig 352 623 12 0 Every Region \nthat is created by the program must eventually be destroyed by the program; calls that accept Regions \nmust be passed Regions that were either created by the program or supplied by the library. RmvTimeOut \n5 6 0 0 XtRemoveTimeOutcan only remove time-outs added with XtAddTimeOut. XFreeGC 10 13 44 0 Every GC \nthat is created by the program must eventually be destroyed by the program; every GC that is destroyed \nby the program must have been created by the program. XGContextFromGC 38 48 44 1 XGContextFromGCmust \nbe passed a valid GC; every GCthat is created by the program must eventually be destroyed by the program; \nevery GCthat is destroyed by the program must have been created by the program. XGetSelOwner 5 5 9 0 \nAfter calling XSetSelectionOwner, selection ownership must be veri.ed by calling XGetSelectionOwner. \nXInternAtom 7 15 42 0 For good performance, XInternAtomshould not be called in the event loop. XPutImage \n7 9 2 2 The image and graphics context passed to XPutImagemust have been created on the same display. \nXSaveContext 66 86 33 0 An association installed with XSaveContextmust eventually be deleted with XDeleteContext; \nalso, the association must be used by a call to XFind-Contextat some point. XSetFont 30 40 44 1 XSetFontmust \nbe passed a valid GC; every GCthat is created by the program must eventually be destroyed by the program; \nevery GC that is destroyed by the program must have been created by the program. XSetSelOwner 5 9 7 0 \nThe timestamp passed to XSetSelectionOwner must come from an event received from the X server. XtFree \n29 35 45 0 Memory allocated with XtCalloc or XtMalloc must be deallocated with XtFree; memory deallocated \nwith XtFree must have been allocated with XtCallocor XtMalloc; memory must not be deallocated twice. \nXtOwnSel 5 10 1 0 The timestamp passed to XtOwnSelectionmust come from an event received from the X server. \nXtRealizeProc 57 64 0 0 If a XtRealizeProc callback calls XtCreateWindow, the call must pass the callback \ns widget and attributes arguments to XtCreateWindow. Also, XtCreateWindow must not be called except by \na XtRealizeProc callback. Table 1: Seventeen Strauss speci.cations, which we debugged with Cable. FA \nreports the number of states and transitions in each speci.cation s debugged FA and Bugs reports the \nnumber of true bugs and false positives that each speci.cation found. Spec. Traces FA |Q| |T| Lattice \n|C| |E| Time (ms) PrsAccelTbl 9 3 10 12 19 3.2 PrsTransTbl 3 3 4 6 7 1.6 Quarks 8 10 13 21 37 5.5 RegionsAlloc \n10 14 15 18 24 4.2 RegionsBig 270 375 611 680 1377 2.67 \u00d7 103 RmvTimeOut 2 3 3 4 4 1.3 XFreeGC 10 10 \n13 23 38 5.7 XGContextFromGC 25 22 36 73 151 32.0 XGetSelOwner 2 5 5 4 4 1.2 XInternAtom 10 3 11 13 21 \n4.1 XPutImage 6 3 7 10 14 2.6 XSaveContext 92 150 224 302 639 477 XSetFont 28 30 40 66 129 25.0 XSetSelOwner \n6 3 7 9 13 2.3 XtFree 112 95 171 380 869 1.51 \u00d7 103 XtOwnSel 7 3 8 10 15 2.7 XtRealizeProc 38 57 64 104 \n207 37.4 Table 2: Running time of concept analysis with respect to 17 mined speci.cations. Traces reports \nthe number of scenario traces in the lattice (none of which are identical), FA reports the number of \nstates and transitions in the reference FA that de.ned similarity, Lattice reports the number of concepts \nand edges in the concept lattice, and Time (ms) reports the running time of the analysis, in milliseconds. \nBaseline As mentioned above, many of the traces were identi\u00adcal. Instead of using Cable, this method \nsimply divides the traces into classes of identical traces and then counts the cost of inspecting and \nlabeling each class separately. That is, the cost of Baseline is two times the number of classes of identi\u00adcal \ntraces. Comparing the cost of Expert with the cost of Baseline indi\u00adcates the value of Cable in practice. \nBy this measure, the advan\u00adtage of using Cable increases as the number of different scenario traces increases. \nCable does not appear to have a large advantage for speci.\u00adcations built from less than 10 unique scenario \ntraces. For three of these speci.cations (XGetSelOwner, PrsTransTbl, and Rmv-TimeOut), the cost of Baseline \nwas very low. For these spec\u00adi.cations the cost for the Expert was also very low. For .ve other speci.cations \n(Quarks, XSetSelOwner, XtOwnSel, XInter\u00adnAtom, and PrsAccelTbl), the cost of Baseline was a bit higher, \nwhile the cost of Expert remained very low. Cable shows an ad\u00advantage here. Finally, for RegionsAlloc, \nXFreeGC, and XPutIm\u00adage, the cost of both methods was a bit higher, although the cost of Baseline was \nstill slightly higher than the cost of Expert. Cable was more useful for debugging speci.cations built \nfrom many tens or hundreds of scenario traces. The improvement was sometimes dramatic, as in the case \nof XtFree. Two speci.cations were hard to debug, even with Cable: RegionsBig was much easier to debug \nwith Cable than by hand, but still required 149 Cable operations; and XSetFont was just barely easier \nto debug with Cable than by hand. Some other observations: Expert never did much worse than Baseline, \nand sometimes did signi.cantly better.  Because Baseline labels each class of identical traces sep\u00adarately, \nit does not take into account generalization by the learner. The cost for Expert includes choosing labels \nto ensure good generalization and verifying that the learner generalized  well. The cost for Baseline \ndoes not include these actions, so it is an underestimate. Top-down and Random beat Baseline on every \nspeci.cation except XGetSelOwner, XPutImage, and XSaveContext. Only XPutImage and XSaveContext are signi.cant, \nsince the cost of labeling XGetSelOwner is very low for all strategies. The fact that these blind strategies \ndo well indicates that the con\u00adcept lattice clusters traces appropriately. XSaveContext is a special \ncase. This speci.cation s FA was large: 150 states and 224 transitions. In fact, we discovered while \ndebugging this speci.cation that most of these states and transitions are irrelevant. These irrelevant \ntransitions hurt the performance of Top-down and Random, because concept analysis found many spurious \nconcepts. On the other hand, these transitions did not hurt Expert very much, because the expert used \nthe Focus command to choose a better reference FA.  Top-down outperforms Random, which implies that \nTop-down was often able to label concepts near the top of the lattice. Top-down beat Random on all but \nfour speci.cations (XGet-SelOwner, XPutImage, XSaveContext, and XSetFont). Top\u00addown beats Random handily \nwhen no traces are erroneous (PrsTransTbl, Quarks, and RmvTimeOut). This is not sur\u00adprising, since the \ntop concept can be labeled immediately in this case. Top-down also beats Random signi.cantly on more \ninteresting speci.cations, particularly XtRealizeProc and Re\u00adgionsBig. By contrast, Random beat Top-down \nsigni.cantly on just one speci.cation: XSaveContext, which contained many irrelevant transitions.  Finally, \nnote that Bottom-up labeling is equivalent to Base\u00adline labeling on these speci.cations, but not in general. \nThese speci.cations have no loops, so each class of identical traces has a characteristic set of FA transitions. \nThese sets appear as concepts near the bottom of the concept lattice.  Spec. Opt Exp Cost of labeling \nTop-down Btm-up Mean Std. Dev. Rand Mean Std. Dev. Base PrsAccelTbl PrsTransTbl Quarks RegionsAlloc RegionsBig \nRmvTimeOut XFreeGC XGContextFromGC XGetSelOwner XInternAtom XPutImage XSaveContext XSetFont XSetSelOwner \nXtFree XtOwnSel XtRealizeProc 4 2 2 8 = 7 2 6 14 4 4 6 = 5 16 4 24 4 12 8 2 2 16 149 2 12 24 5 5 10 42 \n53 5 28 5 13 14.0 1.9 16 2.0 0.0 4 2.0 0.0 16 15.7 1.4 20 121 9.2 540 2.0 0.0 4 12.9 2.3 20 39.0 4.3 \n50 5.0 0.0 4 5.0 0.0 20 21.0 3.9 12 267 21.8 184 52.0 5.4 56 5.0 0.0 12 124 10.9 224 5.0 0.0 14 29.1 \n2.1 76 14.1 2.8 3.4 0.9 8.6 3.2 16.2 2.8 231 29.2 3.4 0.9 14.4 3.4 39.5 5.9 4.5 0.9 12.5 5.6 15.6 3.9 \n188 18.9 50.3 4.8 8.7 3.4 149 13.4 9.8 3.9 51.0 7.4 18 6 16 20 540 4 20 50 4 20 12 184 56 12 224 14 76 \n Table 3: The cost of labeling with various Cable strategies: Optimal (Opt), Expert (Exp), Top-down (Top-down), \nBottom-up (Btm\u00adup), and Random (Rand). These costs are compared to the Baseline method (Base), which \ndoes not use Cable. 5.4 Navigation by transitions In another set of experiments, we studied how long \nit took a second author (also an expert user and developer of the tool) to debug a speci.cation using \nCable. For 11 of the 17 speci\u00ad.cations, we measured the actual time elapsed during a Cable debugging \nsession. We compared the session s time to the time that the expert needed to debug the speci.cation \nwithout Cable, by examining and classifying individual scenario traces. This experiment used Cable s \nnavigation by transitions feature (see Section 4.1). The idea was to replace a traversal strategy (see \nSection 4.2) with navigation to concepts that contain only correct or incorrect traces. The expert s \nstrategy was as follows. Starting at the top concept, the expert examined the summary FA. If the FA described \nonly correct behavior or only erroneous behavior, the expert labeled the concept accordingly. Otherwise, \nthe expert selected transitions that were either clearly executed by all correct traces, or clearly executed \nonly by erroneous traces. Then, the expert asked Cable to .nd the smallest concept that contained all \ntraces that execute all of the selected transitions and applied this procedure recursively at the new \nconcept. After labeling a concept, the expert returned to the concept above it. If this concept still \ncontained unlabeled traces, the expert started over at this concept, but only with the unlabeled traces. \nFor this experiment, we used a modi.ed set of attributes for concept analysis. As in previous experiments, \nthe attributes of a trace included the transitions executed by the trace in the ref\u00aderence FA. Our modi.cation \nadded negative attributes: each trace had a negative attribute for each transition not executed by the \ntrace. That is, a trace that executed transition i but did not ex\u00adecute transition j would have attributes \nai and a\u00afj . The rationale for creating more attributes was to create a lattice with more con\u00adcepts. \nSpeci.cally, we wanted to ensure that Cable was always able to navigate to a concept closely matching \nthe transitions se\u00adlected by the expert. See Section 5.5 (below) for a discussion of how to de.ne suitable \nattributes. Table 4 shows the time to debug each speci.cation, together with the cost of labeling, as \nde.ned in Section 4.2. For four spec- Name Time Cost of labeling Cable Baseline Cable Baseline PrsAccelTbl \n136 42 5 18 PrsTransTbl 9 11 2 6 RmvTimeOut 9 6 2 4 XGContextFromGC 158 107 12 50 XGetSelOwner 32 9 5 \n4 XInternAtom 100 37 8 20 XPutImage 148 31 14 12 XSetFont 117 133 12 56 XSetSelOwner 27 33 5 12 XtFree \n251 254 42 224 XtOwnSel 98 21 10 14 Table 4: Time to debug speci.cations. Time reports the time to debug \na speci.cation using either Cable or the baseline method of classifying individual traces. Cost of labeling \nreports the cost of labeling as de.ned in Section 4.2. i.cations (PrsTransTbl, XSetFont, XSetSelOwner, \nand XtFree), using Cable with the navigation strategy was faster than the base\u00adline method of classifying \nindividual traces, but in general Cable was slower. However, Cable was often signi.cantly better in terms \nof labeling cost, which re.ects the number of concepts that the expert examined. Comparison with Table \n3 reveals three cases where one expert beat the other by a large margin: on XtFree, the expert who used \nthe traversal strategy beat the expert who used navigation (28 to 42); on XGContextFromGC and XSetFont, \nthe expert who used navigation won (12 to 24 and 12 to 53). On XGContextFromGC, the navigation expert \neven beat Optimal, which was possible be\u00adcause the navigation expert was working with a larger concept \nlattice. The fact that neither expert dominated the other indicates that both traversal and navigation \nare valuable, although more study is needed to settle this question. A natural question is why the sometimes \ndramatic reductions in labeling cost from Baseline to Cable did not always translate into shorter classi.cation \ntimes. In our experience, using Cable with navigation was slower than Baseline in those cases when the \nsummary FA for the top concept was hard to understand. If the summary FA lacked salient features, such \nas an obviously erro\u00adneous loop, the expert had to study all paths through the FA. The understanding \nof the top summary FA thus became even harder than the baseline method because the FA presented potentially \nconfusing overlapping paths. A possible solution is for the expert to start by traversing the lattice, \nusing navigation by transitions only when he .nds a summary FA with a salient feature. 5.5 Discussion \nWe presented and evaluated two approaches to debugging spec\u00adi.cations using concept analysis. In traversal \nstrategies, the user visits concepts in some order and attempts to label them (Sec\u00adtion 5.3). In navigation \nstrategies, the user selects features of a concept in order to navigate to a concept with those features \n(Section 5.4). These two strategies have complementary bene.ts but also con.icting requirements. In a \ntraversal strategy, the user searches the lattice for classi\u00ad.able concepts. The search is undirected \nin that the user can\u00adnot in.uence the traversal order by indicating which traces from the current concept \nhe would prefer to visit next. For example, in a top-down strategy, when the user encounters a concept \nthat cannot be classi.ed, he moves to a subconcept that represents a smaller, simpler subset of the traces. \nThe traversal is thus based only on the structure of the lattice, not on user-selected features of concepts. \nAs a result, the traversal strategy works best on a small lat\u00adtice because, otherwise, its undirected \nsearch often visits many ancestors (or descendants) of the classi.able concepts. In a navigation strategy, \nthe user skips over many concepts by navigating directly to a subconcept that focuses on an interesting \nfeature of a larger concept. For example, in our experiments, the user selected salient transitions in \na summary FA and navigated to a concept containing matching traces. The navigation strategy works best \nwhen concepts have salient features and there is a closely matching concept for any user se\u00adlection of \nsalient features. We call such a lattice navigable.A simple way to get a navigable lattice is to use \na complete subset lattice. Traversal and navigation have complementary bene.ts. Nav\u00adigation is very effective \nwhen concepts have salient features. However, in practice, concepts do not always have salient fea\u00adtures, \nin which case traversal is needed. Thus, we would like to combine traversal and navigation into a single \nstrategy with the bene.ts of both. Combining traversal and navigation is, however, dif.cult be\u00adcause \nthey have con.icting requirements. While traversal re\u00adquires a small lattice, navigation requires a navigable \nlattice. Therefore, a combined strategy would work best on a small, navigable lattice. As noted, a complete \nlattice is navigable, but not small. Conversely, while designing our experiments, we ob\u00adserved that small \nlattices are often not navigable. One solution to the problems with combining navigation and traversal \nis to use a single lattice, but with a different view for each approach. The lattice would be complete, \nor nearly so, to support navigation. However, the user would only see a coarse subset of the lattice, \nwhich could be traversed effectively. The research question is how to select the subset of attributes \nthat will de.ne the coarser lattice used in the traversal. Another solution is to select attributes judiciously \nto produce a lattice that is both small and navigable. If there were an attribute for each salient feature, \nthe lattice would be navigable, and not too large. Since the salient features are not known when the \nlat\u00adtice is computed, we would have to alter the lattice in response to user input. An interesting question \nis how to infer good attributes from user input. For example, with navigation by transitions, a tool \ncould add attributes as necessary to distinguish selected tran\u00adsitions. As another example, a tool could \ninfer useful attributes according to labels previously assigned by the user.  6. RELATED WORK This \npaper .lls a large hole left unexplored by our previous work on speci.cation mining [2]. That paper explained \nhow to extract speci.cations from program execution traces, but only offered a naive mechanism coring, \nor dropping low frequency transitions for removing errors from those speci.cations. This paper gives \na general method for debugging speci.cations, which applies not only to our miner s speci.cations but \nalso to temporal speci.cations from any source. Previous work on helping users work through the large \nnum\u00adbers of bug reports that can be produced by veri.cation tools has focused on ranking the bug reports, \nso that the user sees likely bugs before likely false positives, and severe bugs before minor bugs. An \nexample is the Xgcc system [15], which uses statistical and other heuristics to rank likely bugs and \nsevere, hard-to-.nd bugs above other bug reports. Xgcc also does some simple clus\u00adtering based on which \nfunctions appear in bug reports. Another tool, PRE.x [4], uses a number of .ltering and ranking heuris\u00adtics \nto reduce what they call noise . In our opinion, ranking and clustering are complementary: ranking tells \nthe user what reports to inspect .rst, while clustering helps the user avoid inspecting redundant reports. \nDaikon [8], a tool for dynamically discovering arithmetic in\u00advariants, uses statistical con.dence checks \nto suppress invariants that appear to have occurred by chance. In our case, we found that some buggy \ntraces occurred so frequently that suppressing them similarly would also suppress valid traces. One way \nto debug speci.cations is to assume that any part of a speci.cation that can not be veri.ed by a program \nveri\u00ad.cation tool is, in fact, wrong. This approach is used by the Houdini tool [9], which guesses many \ninvariants and then uses ESC/Java [10] to prune out those that do not always hold. A similar approach \nwas used to integrate the Daikon and ESC/Java tools [19]. Both tools still rely on a user to help debug \nspeci.ca\u00adtions, because programs are buggy: an invariant that should be true (and so should be checked) \nmay be unveri.able because of an error in the program. This paper concentrates on debugging temporal \nspeci.cations. Many program veri.cation tools rely on Hoare-style invariants, preconditions, and postconditions \n[14]. It would be interesting to see if clustering techniques such as ours apply to such speci.\u00adcations. \nThe concept lattice that we build is sensitive to the particular FA used to recognize traces. Strauss \nuses Raman and Patrick s sk-string algorithm [21] to infer the FAs in its speci.cations. There are many \nother algorithms in the literature; Murphy has written a good survey [18]. For Cable, it would be particularly \ninteresting to explore interactive algorithms, which would allow the user to .ne-tune the concept lattice \nas he uses it for labeling. 7. CONCLUSION This paper described a method for debugging temporal spec\u00adi.cations. \nOur method uses concept analysis to cluster the vio\u00adlation traces from a program veri.cation tool or \nscenario traces of a speci.cation miner, so that users can label them quickly. We found that our method \nis ef.cient, both in terms of machine resources and in terms of human resources. We also found a trade-off \nbetween small concept lattices, which are easier to tra\u00adverse, and large concept lattices, which are \nmore likely to con\u00adtain a desired concept. Future work should explore this trade-off, probably by changing \nthe lattice interactively. Acknowledgements This research was supported by NSF grants CCR-0093275, CCR\u00ad0105721, \nEIA-0103670, and awards from IBM Corporation and Microsoft Corporation. We are also grateful for helpful \ncom\u00adments from Mark D. Hill, Barton P. Miller, Thomas W. Reps, Vadim Shapiro, Manu Sridharan, and the \nanonymous reviewers. 8. REFERENCES [1] Glenn Ammons. Strauss: A Speci.cation Miner. PhD thesis, University \nof Wisconsin, Madison, May 2003. [2] Glenn Ammons, Rastislav Bod\u00b4ik, and James R. Larus. Mining speci.cations. \nIn Proceedings of the 2002 ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages (POPL \n02), pages 4 16, January 2002. [3] Thomas Ball and Sriram K. Rajamani. The SLAM project: debugging system \nsoftware via static analysis. In Proceedings of the 2002 ACM SIGPLAN-SIGACT Symposium on Principles of \nProgramming Languages (POPL 02), pages 1 3, January 2002. [4] William R. Bush, Jonathan D. Pincus, and \nDavid J. Sielaff. A static analyzer for .nding dynamic programming errors. Software Practice and Experience, \n30:775 802, 2000. [5] James C. Corbett, Matthew B. Dwyer, John Hatcliff, Shawn Laubach, Corina S. Pasareanu, \nRobby, and Hongjun Zheng. Bandera: Extracting .nite-state models from Java source code. In Proceedings \nof the 22nd International Conference on Software Engineering, June 2000. [6] Manuvir Das, Sorin Lerner, \nand Mark Seigle. ESP: Path-sensitive program veri.cation in polynomial time. In Proceedings of the ACM \nSIGPLAN 2002 Conference on Programming Language Design and Implementation (PLDI), pages 57 68, 2002. \n[7] Robert DeLine and Manuel F\u00a8ahndrich. Enforcing high-level protocols in low-level software. In Proceedings \nof the ACM SIGPLAN 2001 Conference on Programming Language Design and Implementation (PLDI), pages 59 \n69, June 2001. [8] Michael D. Ernst. Dynamically Discovering Likely Program Invariants. PhD thesis, University \nof Washington, August 2000. [9] Cormac Flanagan and K. Rustan M. Leino. Houdini, an annotation assistant \nfor ESC/Java. In International Symposium on FME 2001: Formal Methods for Increasing Software Productivity, \nLNCS, volume 1, 2001. [10] Cormac Flanagan, K. Rustan M. Leino, Mark Lillibridge, Greg Nelson, James \nB. Saxe, and Raymie Stata. Extended static checking for Java. In Proceedings of the ACM SIGPLAN 2002 \nConference on Programming Language Design and Implementation (PDLI), pages 234 245, June 2002. [11] Emden \nR. Gansner and Stephen C. North. An open graph visualization system and its applications to software \nengineering. Software Practice and Experience, 30(11):1203 1233, September 1999. [12] Patrice Godefroid. \nModel checking for programming languages using VeriSoft. In Proceedings of the 24th ACM ACM SIGPLAN-SIGACT \nSymposium on Principles of Programming Languages, pages 174 186, January 1997. [13] Robert Godin, Rokia \nMissaoui, and Hassan Alaoui. Incremental concept formation algorithms based on galois (concept) lattices. \nComputational Intelligence, 11(2):246 267, 1995. [14] David Gries. The Science of Programming. Springer-Verlag, \nNew York, New York, USA, 1981. [15] Seth Hallem, Benjamin Chelf, Yichen Xie, and Dawson Engler. A system \nand language for building system-speci.c, static analyses. In Proceedings of the ACM SIGPLAN 2002 Conference \non Programming Language Design and Implementation (PLDI), pages 69 82, May 2002. [16] Daniel Jackson. \nAlloy: a lightweight object modelling notation. In ACM Transactions on Software Engineering and Methodology \n(TOSEM), volume 11, pages 256 290, April 2002. [17] John Mocenigo. Grappa: A Java graph package. URL: \nhttp: //www.research.att.com/ john/Grappa. [18] Kevin P. Murphy. Passively learning .nite automata. Technical \nReport 96-04-017, Santa Fe Institute, 1996. [19] Jeremy W. Nimmer and Michael D. Ernst. Static veri.cation \nof dynamically detected program invariants: Integrating Daikon and ESC/Java. In Proceedings of RV 01, \nFirst Workshop on Runtime Veri.cation, Paris, France, July 2001. [20] David Y. W. Park, Ulrich Stern, \nJens U. Sakkebaek, and David L. Dill. Java model checking. In Proceedings of the Fifteenth IEEE International \nConference on Automated Software Engineering (ASE 00), pages 253 256, sep 2000. [21] Anand V. Raman and \nJon D. Patrick. The sk-strings method for inferring PFSA. In Proceedings of the workshop on automata \ninduction, grammatical inference and language acquisition at the 14th international conference on machine \nlearning (ICML97), 1997. [22] Michael Siff. Techniques for software renovation. PhD thesis, University \nof Wisconsin, Madison, 1998. [23] Willem Visser, Klaus Havelund, Guillaume Brat, and Seung Joon Park. \nModel checking programs. In Proceedings of the Fifteenth IEEE International Conference on Automated Software \nEngineering (ASE 00), pages 3 12, sep 2000. [24] R. Wille. Restructuring lattice theory: an approach \nbased on lattices of concepts. Ordered Sets, pages 445 470, 1982.  \n\t\t\t", "proc_id": "781131", "abstract": "Program verification tools (such as model checkers and static analyzers) can find many errors in programs. These tools need formal specifications of correct program behavior, but writing a correct specification is difficult, just as writing a correct program is difficult. Thus, just as we need methods for debugging programs, we need methods for debugging specifications.This paper describes a novel method for debugging formal, temporal specifications. Our method exploits the short program execution traces that program verification tools generate from specification violations and that specification miners extract from programs. Manually examining these traces is a straightforward way to debug a specification, but this method is tedious and error-prone because there may be hundreds or thousands of traces to inspect. Our method uses concept analysis to automatically group the traces into highly similar clusters. By examining clusters instead of individual traces, a person can debug a specification with less work.To test our method, we implemented a tool, Cable, for debugging specifications. We have used Cable to debug specifications produced by Strauss, our specification miner. We found that using Cable to debug these specifications requires, on average, less than one third as many user decisions as debugging by examining all traces requires. In one case, using Cable required only 28 decisions, while debugging by examining all traces required 224.", "authors": [{"name": "Glenn Ammons", "author_profile_id": "81546156556", "affiliation": "University of Wisconsin, Madison, Wisconsin and IBM T.J. Watson Research Center, Hawthorne, New York", "person_id": "PP39040589", "email_address": "", "orcid_id": ""}, {"name": "David Mandelin", "author_profile_id": "81100521702", "affiliation": "University of California, Berkeley, California", "person_id": "P517403", "email_address": "", "orcid_id": ""}, {"name": "Rastislav Bod&#237;k", "author_profile_id": "81100033082", "affiliation": "University of California, Berkeley, California and IBM T.J. Watson Research Center, Hawthorne, New York", "person_id": "P517421", "email_address": "", "orcid_id": ""}, {"name": "James R. Larus", "author_profile_id": "81100277326", "affiliation": "Microsoft Research, Redmond, Washington", "person_id": "P132790", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/781131.781152", "year": "2003", "article_id": "781152", "conference": "PLDI", "title": "Debugging temporal specifications with concept analysis", "url": "http://dl.acm.org/citation.cfm?id=781152"}