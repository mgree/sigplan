{"article_publication_date": "05-09-2003", "fulltext": "\n Stride Prefetching by Dynamically Inspecting Objects Tatsushi Inagaki Tamiya Onodera Hideaki Komatsu \nToshio Nakatani IBM Tokyo Research Laboratory 1623-14, Shimotsuruma, Yamato-shi, Kanagawa-ken 242-8502, \nJapan {e29253,tonodera,komatsu,nakatani}@jp.ibm.com  ABSTRACT Software prefetching is a promising technique \nto hide cache miss latencies, but it remains challenging to e.ectively prefetch pointer\u00adbased data structures \nbecause obtaining the memory address to be prefetched requires pointer dereferences. The recently proposed \nstride prefetching overcomes this problem, but it only exploits inter\u00aditeration stride patterns and relies \non an o.-line pro.ling method. We propose a new algorithm for stride prefetching which is in\u00adtended for \nuse in a dynamic compiler. We exploit both inter-and intra-iteration stride patterns, which we discover \nusing an ultra\u00adlightweight pro.ling technique, called object inspection. This is a kind of partial interpretation \nthat only a dynamic compiler can per\u00adform. During the compilation of a method, the dynamic compiler gathers \nthe pro.le information by partially interpreting the method using the actual values of parameters and \ncausing no side e.ects. We evaluated an implementation of our prefetching algorithm in a production-level \nJava just-in-time compiler. The results show that the algorithm achieved up to an 18.9% and 25.1% speedup \nin industry-standard benchmarks on the Pentium 4 and the Athlon MP, respectively, while it increased \nthe compilation time by less than 3.0%. Categories and Subject Descriptors D.3.4 [Programming Languages]: \nProcessors code generation, compilers, memory management, optimization  General Terms Algorithm, Design, \nExperimentation, Performance  Keywords Java just-in-time compiler, object inspection, stride prefetching \n 1. INTRODUCTION The performance gap between the processor and memory con\u00adtinues to widen with no indication \nof yet reaching a limit. Cache memories at several levels attempt to ameliorate the problem, ex\u00adploiting \ntemporal and spatial locality of program execution. They Permission to make digital or hard copies of \nall or part of this work for personal or classroom use is granted without fee provided that copies are \nnot made or distributed for pro.t or commercial advantage and that copies bear this notice and the full \ncitation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to \nlists, requires prior speci.c permission and/or a fee. PLDI 03, June 9 11, 2003, San Diego, California, \nUSA. Copyright 2003 ACM 1-58113-662-5/03/0006 ...$5.00. are quite e.ective in many programs for reducing \nmemory access latencies and thus improving performance. However, cache memories do not help very much \nfor programs which access a large amount of data and su.er from a signi.cant number of cache misses. \nWhile classical instances of such pro\u00adgrams are numerical applications accessing large data structures \nsuch as vectors and matrices, modern object-oriented programming also tends to result in such programs \nby creating a large number of objects in the heap and chasing references among the objects. It is not \nuncommon for Java [7] programs to create millions of objects in a heap that might be hundreds of megabytes \nin size [5]. Software prefetching is one of the promising techniques to ad\u00address the issue [3]. Assuming \na special prefetch instruction exists for moving data into a higher-level cache, it attempts to hide \ncache miss latencies by issuing a prefetch instruction for the data well before the data is accessed. \nHowever, improving performance with software prefetching is not a trivial task. First, the timing of \nissuing the prefetch instruction is tricky. It must not be issued too late, or the prefetched data may \nstill not have become available when the processor executes the memory operations using that data. On \nthe other hand, it must not be issued too early, or the cache may no longer contain the prefetched data \nwhen the processor executes the operations. Second, the prefetch instruction must be issued only when \nmemory bandwidth is not being fully used, since executing a prefetch instruction is not free. Finally, \nthe overhead of computing the address of the data to be prefetched should be small. In partic\u00adular, the \nnumber of memory operations executed for obtaining the address must be minimized. While many algorithms \nhave been successfully developed to prefetch array-based data references in numerical applications [16], \nit is more challenging to e.ectively prefetch pointer-based data refer\u00adences because of the much larger \noverhead to obtain the target ad\u00address. Consider a loop that iterates over the elements of a list. If \nwe prefetch in the i-th iteration the element accessed in the (i + c)-th it\u00aderation, we must make c pointer \ndereferences to obtain the address of the element for the prefetch instruction. Recently, Wu [23] and \nWu et al. [24] presented a new prefetch\u00ading algorithm that can uniformly handle both array-based and \npointer\u00adbased references. It is based on the observation that important load instructions could exhibit \nstride patterns even when they reference pointer-based data structures. Consider again a loop traversing \na list. If the program constructs the list by allocating and append\u00ading equal-sized elements without \nother intervening allocations, the load instruction for retrieving the next element in the loop probably \nhas constant strides. Wu developed an o.-line pro.ling method to e.ciently discover load instructions \nwith stride patterns, and used the obtained stride pro.les to guide compiler prefetching. Wu s stride \nprefetching class TokenVector { yields a signi.cant speedup because stride patterns allow prefetch- \nToken[] v; int ptr; ing without dereferences. While Wu handles both in-loop and out-of-loop loads, \nhis ex\u00ad void addElement (Token val) {...} perimental results show that the performance gain from prefetch\u00ad \nvoid removeElement (Token val) {...} ing out-of-loop loads was insigni.cant. Moreover, Wu attempted \n... to discover only what we call inter-iteration stride patterns for in\u00ad } loop loads. That is, given \na load instruction in a loop, Wu attempted to determine whether or not the sequence of the addresses \naccessed class Token { ValueVector[] facts; by the instruction over iterations exhibits a stride pattern. \nint size = 0; We propose a new approach to stride prefetching which is in- Token (ValueVector firstFact) \n{ tended for use in a dynamic compiler. Focusing on in-loop loads, facts = new ValueVector[5]; we attempt \nto discover and exploit not only inter-iteration patterns facts[size++] = firstFact; } but also intra-iteration \npatterns. Given a pair of load instructions in ... } a loop, we de.ne the stride between them as the \ndi.erence between the addresses accessed by the two instructions within one iteration, and two load instructions \nare said to show an intra-iteration stride class Node2 { pattern when the sequence of the strides shows \na pattern between Token findInMemory (TokenVector tv, Token t) { iterations. As we show later, pairs \nof important loads without inter- TokenLoop: for (int i = 0; i < tv.ptr; i++) { iteration stride patterns \ncould exhibit intra-iteration stride patterns, Token tmp = tv.v[i]; for (int j = 0; j < t.size; j++) \n allowing our approach to yield more performance gains. We discover inter-iteration and intra-iteration \nstride patterns with if (!t.facts[j].equals (tmp.facts[j])) an ultra-lightweight technique for dynamic \npro.ling called object continue TokenLoop; inspection. This is a kind of partial interpretation that \nonly a just-in\u00ad return tmp; time (JIT) compiler is able to perform. When invoked for a method } return \nnull; containing one or more loops, the JIT compiler partially interprets } ... the method with the \nactual values of the method s parameters and without generating any side e.ects, executing each loop \na small } number of times to discover the stride patterns. We build a dependence graph of load instructions \nto limit the number of candidate pairs of load instructions that must be consid- Figure 1: Simpli.ed \ncode fragments from 202 jess. ered for intra-iteration stride patterns. Load instructions for chasing \nreferences are likely to show intra-iteration stride patterns, since constructors in object-oriented \nlanguages tend to allocate a bunch our knowledge, this is the .rst application of such an ultra\u00adof related \nobjects. We build the dependence graph according to lightweight pro.ling technique for dynamic optimizations. \nreference-chasing sequences within a loop, resulting in candidate More concretely, we use the novel pro.ling \ntechnique for pairs of load instructions being represented as adjacent nodes in discovering both inter-iteration \nand intra-iteration stride pat\u00adthe graph. terns. We evaluated an implementation of our prefetching algorithm \nin the JIT compiler [22] of IBM Developer Kit for Windows, Java Evaluation on a production Java JIT \ncompiler. We imple-Technology Edition. We ran the SPECjvm98 benchmark [20] and mented our stride prefetching \nalgorithm on a production Java Section 3 of the JavaGrande v2.0 benchmark [12] on two machines virtual \nmachine and JIT compiler, and evaluated the e.ec\u00adwith di.erent IA-32 architectures, an Intel Pentium \n4 [9] and an tiveness on two di.erent IA-32 machines using industry stan-AMD Athlon MP [1]. The results \nshow that our stride prefetching dard benchmarks. achieved up to an 18.9% and 25.1% speedup on the Pentium \n4 and the Athlon MP, respectively. The rest of this paper is organized as follows. Section 2 shows a \nOur contributions in this paper are as follows. motivating example to discuss stride prefetching and \ngive an overview of our approach. Section 3 describes our stride prefetching algo\u00ad Discovery and exploitation \nof intra-iteration stride patterns. rithm. Section 4 presents performance results, while Section 5 dis-To \nthe best of our knowledge, this is the .rst attempt to dis\u00ad cusses the related work. Finally, Section \n6 o.ers conclusions. cover intra-iteration stride patterns and utilize them for soft\u00ad ware prefetching. \nGiven a pair of load instructions in a loop,  2. A MOTIVATING EXAMPLE the two instructions are said \nto show an intra-iteration stride We discuss stride prefetching using the motivating example shown pattern, \nif the sequence of the strides between them shows in Figure 1. The code is taken from the 202 jess benchmark \nin a pattern over iterations. We .rst build a dependence graph SPECjvm98. The findInMemory()method is \none of the time\u00ad of load instructions to limit the number of candidate pairs consuming methods in 202 \njess, spending most of the time execut\u00ad of load instructions, and attempt to discover intra-iteration \ning the doubly nested loop. Also, the execution pro.le shows that, stride patterns using object inspection. \nwhile the outer loop has a large trip count, the inner loop has a quite Object inspection. This is an \nultra-lightweight technique for small trip count. Figure 2 summarizes the data structures accessed dynamic \npro.ling which only a dynamic compiler is able in the method, representing as solid arrows the pointer \nreferences to use. When dynamically compiling a method, it gathers chased in the method. an execution \npro.le of the method by partially interpret-In-loop loads are major targets of software prefetching. \nThe dou\u00ading the method at compile time, using the actual values of bly nested loop contains the eleven \nload instructions listed in Ta\u00adthe parameters and causing no side e.ects. To the best of ble 1. The load \ninstructions for the length.elds of the arrays are Table 1: Load instructions in the findInMemory()method. \nLoad instructions Memory addresses L1 &#38;tv.ptr L2 &#38;tv.v L3 &#38;tv.v.length L4 &#38;tv.v[i] L5 \n&#38;t.size L6 &#38;t.facts L7 &#38;t.facts.length L8 &#38;t.facts[j] L9 &#38;tmp.facts L10 &#38;tmp.facts.length \nL11 &#38;tmp.facts[j]  not explicit in the Java source program, but are generated for array bound checks. \nWe now explain how Wu et al. [24] discover and exploit inter\u00aditeration stride patterns for in-loop loads. \nTheir approach is based on o.-line pro.ling. They select candidate loads using the execu\u00adtion frequency \npro.le and compiler s static analysis, instrument the code to collect stride pro.les for the candidate \nloads, and generate prefetch instructions based on the stride pro.les obtained. They select a load in \na loop as a candidate for stride pro.ling using the following criteria: The loop has a high trip count. \n The load is frequently executed.  The memory address of the load is not a loop invariant.  A load \nin a loop with a high trip count is selected since the load is likely to touch a large range of memory \nand to miss the cache. When a nested loop does not have a high trip count but the parent loop does, each \nload in the nested loop is considered as if it were in the parent loop. When applied to the findInMemory() \nmethod, their algo\u00adrithm selects L4, L9, L10, and L11 to collect stride pro.les, and the rest of the \nloads are all loop-invariants. However, the resulting stride pro.les show that only L4 has a stride pattern. \nThis is because the array of Tokenobjects referenced by tv.vis not constructed in the initialization \nphase. The Token objects are appended to and removed from the array during the execution of the 202 jess \nbenchmark. Furthermore, when the removeElement()method attempts to remove a Token object and .nds it \nstored as the ar\u00adray s i-th element, it removes the object by moving to the index i the array s last \nelement. As a result, by only discovering a stride pattern for L4, their algorithm generates a prefetch \ninstruction as in Figure 3, which is intended for prefetching data c iterations ahead, when L4 has d \nbytes of an inter-iteration constant stride. We extend their approach in two ways to capture more loads \nas targets for prefetching. First, although L9 does not exhibit an inter\u00aditeration stride pattern, we \ncould prefetch the data which L9 is likely to load after c iterations, since L9 is data dependent upon \nL4 and L4 shows an inter-iteration stride pattern. That is, we could specula\u00adtively execute L4 using \nthe memory address predicted based on the stride pattern, and prefetch the data for L9 using the result. \nThis re\u00adquires one pointer dereference, but the bene.ts may o.set the cost. Second, while none of L9, \nL10, and L11 is likely to show an inter\u00aditeration stride pattern, (L9, L10) is likely to exhibit an intra-iteration \nstride pattern. This is because the constructor for the Tokenclass allocates an array and stores a reference \nto the array into the facts .eld, not reassigning a new reference into the .eld after that. When Figure \n2: Data structures accessed and pointer references chased in the findInMemory()method. TokenLoop: for \n(int i = 0; i < tv.ptr; i++) { Token tmp = tv.v[i]; prefetch (&#38;tv.v[i] + c*d); for (int j = 0; \nj < t.size; j++) if (!t.facts[j].equals (tmp.facts[j])) continue TokenLoop; return tmp; } Figure 3: \nStride prefetching by Wu et al. s approach. the data loaded by the two instructions with an intra-iteration \nstride are farther apart than the size of a cache line, the stride pattern is exploitable for software \nprefetching. Assuming that we have al\u00adready obtained the memory address for prefetching the data loaded \nby L9, we could prefetch the data loaded by L10 based on the same memory address. Figure 4 shows the \nresulting optimized code using our approach, where o denotes the o.set of the facts.eld in the Tokenobject \nand s denotes the stride between L9 and L10. We also assume that the stride is longer than the cache \nline. We perform three types of prefetching in the optimized code, inter-iteration stride prefetching, \ndereference-based prefetching, and intra-iteration stride prefetch\u00ading. Notice that we do not necessarily \nassume hardware support for the speculative load, and we could obtain an equivalent e.ect with a sequence \nof ordinary instructions. TokenLoop: for (int i = 0; i < tv.ptr; i++) { Token tmp = tv.v[i]; tmp_pref \n= spec_load (&#38;tv.v[i] + c*d); prefetch (tmp_pref + o); prefetch (tmp_pref + o + s); for (int j = \n0; j < t.size; j++) if (!t.facts[j].equals (tmp.facts[j])) continue TokenLoop; return tmp; } Figure \n4: Stride prefetching by our approach.  3. OUR PREFETCHING ALGORITHM We describe our algorithm for stride \nprefetching intended for use in a JIT compiler. The JIT compiler is invoked for a method when the method \nis about to be executed. It may be compiling the method for the .rst time, or recompiling the method \nwith more aggressive optimizations. Thus, actual values for the parameters are available at compile time, \nand our algorithm fully exploits this information. As one of the optimization phases, our algorithm transforms \nthe input code in the intermediate representation of a method into code augmented with prefetch instructions. \nGiven a method, it .rst at\u00adtempts to identify loops, constructing a loop nesting forest. The algorithm \nthen traverses the loops in each tree in a postorder traver\u00adsal, walking the trees in the program order. \nFor each loop, the algorithm performs the following three steps. First, it constructs a dependence graph \nof the load instructions in the loop. As explained below, the reference-chasing sequences of load instructions \nare connected in the graph, limiting the num\u00adber of pairs of load instructions we must check for intra-iteration \nstride patterns. Second, the algorithm performs object inspection to detect both inter-iteration and \nintra-iteration stride patterns. It attempts to partially interpret the loop body a certain number of \ntimes, annotating the dependence graph with the stride patterns dis\u00adcovered. Finally, we generate prefetching \ncode based on stride pat\u00adterns recorded in the dependence graph. We prefetch data for a load instruction \nonly when it is e.ective. The more instructions there are which are data dependent upon a particular \nload instruc\u00adtion, the more e.ective prefetching is estimated to be for that load instruction. Before \nwe describe each step in detail in the subsequent subsec\u00adtions, we note that a nested loop with a small \ntrip count is handled in a manner similar to [24]. When we process the parent loop, all the load instructions \nin the nested loop are considered again as if they were in the parent loop. Our algorithm detects that \na loop has a small trip count when it is performing object inspection. Alter\u00adnatively, we could rely \non execution pro.les if the system supports online pro.ling. 3.1 Construction of a Load Dependence Graph \nWe utilize a directed graph, called a load dependence graph, to capture reference-chasing sequences of \nload instructions. Each node of the graph is a load instruction using a reference as an operand. A directed \nedge exists from node L1 to node L2 if and only if L2 is directly data dependent upon L1. That is, L2 \nloads data using the value loaded by L1, which must thus also be a reference. When the Java bytecode \nis used as an intermediate representa\u00adtion, the instructions that can be a node of a load dependence \ngraph include getfield, getstatic, aaload, iaload, daload, arraylength, and others. Only three instructions \ncan be non\u00adleaf nodes in the graph: getfieldand getstaticinstructions yielding reference values, and \naaload. For a given loop, we construct a load dependence graph of the load instructions in the loop. \nWhen it has a nested loop, the load instructions in the nested loop are also considered only if it has \na small trip count. We can construct the graph, for instance, by utilizing the use-def chains built for \nthe method containing the loop, but many other ways are possible. Figure 5 shows a part of the load dependence \ngraph constructed for the doubly nested loop in the findInMemory()method. The load instructions in the \nnested loop also appear in the graph since it has a small trip count.  3.2 Object Inspection After constructing \na load dependence graph for the target loop,  Figure 5: A load dependence graph for the candidate load \nin\u00adstructions in the findInMemory()method. we perform object inspection to detect stride patterns. We \nattempt to partially interpret the loop body a certain number of times (for example, 20 times) using \nthe actual values of the parameters, and record the memory addresses used by the load instructions in \nthe graph. After this partial interpretation, we analyze the trace of the memory addresses. While we \ncheck each of the load instructions for inter-iteration patterns, we also check each of the adjacent \npairs in the graph for intra-iteration patterns. If the majority (for exam\u00adple, over 75%) of the strides \nof a load or a pair of loads are the same, we recognize that they have stride patterns, and annotate \nthe corresponding node or edge with the constant stride value. Object inspection interprets the method \ns instructions in the in\u00adtermediate representation, starting from the method entry. Although the actual \nvalues for the parameters are available, there will still be some cases where the operand is not available, \nand when those cases happen we use a special value, unknown, for the operand. Any instruction that involves \nan unknown operand will have an un\u00adknown result at interpretation. Object inspection must be free of \nside e.ects. In particular, we must prevent the interpretation of store instructions from causing any \nvisible e.ects. To do this, we make a copy of the stack frame, and interpret each store instruction into \na local variable within the copied stack frame. Also, we interpret each store instructions into an object \nby recording the updated address and the value in a hash table, and accordingly interpret a load instruction \nfrom an object by .rst looking in the hash table. For a similar reason, we prepare a private heap, and \ninterpret object-creating instructions using the private heap. We may encounter one or more loops before \nreaching the entry point of the target loop. We interpret the body of such a loop only once. This is \nbecause the induction variable or the recurrent refer\u00adence variable of a loop is often initialized without \ndepending on the results of the preceding loops. For instance, it is often the case that the induction \nvariable is initialized to zero, and that the recurrent reference variable is initialized by chasing \none of the parameters. Also, since object inspection must be lightweight, we cannot a.ord to interpret \nother loops until we really exit from them at any rate. Finally, we interpret a method invocation by \nsimply skipping it and assuming that the return value, if any, is unknown. Alterna\u00adtively, we could step \ninto the callee method for a non-virtual invoca\u00adtion or all the methods potentially invoked for a virtual \ninvocation. Making object inspection inter-procedural might improve the ac\u00adcuracy of our analysis, but \nit would increase the compilation time, requiring the trade-o. to be carefully assessed.  3.3 Generation \nof Prefetching Code After object inspection, we generate the prefetching code based on the stride patterns \nrecorded in the dependence graph. We .rst explain the code sequences we generate to exploit stride patterns. \nAs shown below, the prefetching code for a load instruction varies depending on the types of stride patterns \nof the instruction and the adjacent nodes. We then explain a simple pro.tability anal\u00adysis to remove \nine.ective and redundant prefetching codes. Fi\u00adnally, we discuss mapping of two prefetch instructions \nwe assume, prefetchand spec load, to hardware instructions. Code Sequences Consider a node Lx with an \ninter-iteration stride d. If Lx has no adjacent node, or, if all of the adjacent nodes have inter-iteration \nstride patterns, we generate the following code for prefetching data accessed by Lx in c iterations after \nthe current iteration, where we denote as A(L) the memory address of data loaded by L in the cur\u00adrent \niteration. prefetch (A(Lx) + d*c); Otherwise, there exists a node, Ly, which is adjacent to Lx and does \nnot exhibit an inter-iteration stride pattern. We then generate the following code for prefetching data \naccessed by Lx and Ly in c iterations after the current iteration, a = spec load (A(Lx) + d*c); prefetch \n(F[Lx,Ly](a)); where F[Lx, Ly] denotes a function which maps the memory ad\u00address produced by Lx to the \nmemory address used by Ly. Typi\u00adcally, the function simply adds a constant o.set to the input address. \nThe code sequence performs both (inter-iteration) stride prefetch\u00ading and dereference-based prefetching. \nNotice that Lx and Ly can never have an intra-iteration stride pattern. The existence of the intra-iteration \npattern between the two implies that Ly has an inter\u00aditeration pattern, which contradicts that Ly does \nnot exhibit such a stride pattern. The case that Ly does not have an inter-iteration stride pattern opens \nopportunities for exploiting intra-iteration stride patterns. For each node Lz which has an intra-iteration \nstride pattern with Ly di\u00adrectly or transitively, we generate the following prefetching code, prefetch \n(F[Lx,Ly](a) + S[Ly,Lz]); where S [Ly, Lz] denotes the stride between Ly and Lz. The actual value for \nthe scheduling distance c depends on the processor s cache parameters and the amount of computation and \nnumber of memory accesses in the loop body. While we cannot change the cache parameters, we can increase \nthe amount of com\u00adputation by unrolling the loop.  Pro.tability Analysis Since the prefetch instructions \nconsume processor resources and memory bandwidth, we must be selective in issuing them. Ideally, we should \ngenerate prefetching codes for those load instructions that frequently cause cache misses. However, it \nis quite di.cult to predict the frequency of cache misses by a load instruction at compile-time, because \nit subtly depends on many factors, includ\u00ading cache parameters and static and dynamic instruction streams \nsurrounding the load instruction. Instead, we perform a simple but e.ective pro.tability analy\u00adsis. We \ngenerate the prefetching code for a load instruction L only when it satis.es the following three conditions. \nFirst, one or more instructions must be data dependent on L. Second, data accessed by L must not apparently \nshare the same cache line with data for which the prefetch code is already issued. Finally, when L has \nan inter-iteration stride pattern, the stride must be larger than half of the cache line. Prefetching \nfor such a load instruction will not be pro.table, especially on processors with hardware prefetching \n[13]. Table 2: Parameters related to prefetching on the Pentium 4 and the Athlon MP. L1 size L1 line \nL2 size L2 line #DTLB Processor (KB) size (B) (KB) size (B) entries Pentium 4 8 64 256 128 64 Athlon \nMP 64 64 256 64 256 Table 3: Description of the SPECjvm98 and the JavaGrande v2.0 Section 3. Programs \nDescription Compiled code (%) mtrt Two threaded ray tracing 75.1 jess Java expert shell system 70.3 compress \nModi.ed Lempel-Ziv method 93.6 db Memory resident database 92.3 mpegaudio MPEG Layer-3 audio decompression \n87.0 jack Java parser generator 36.2 javac Java compiler from JDK1.0.2 51.9 Euler Computational .uid \ndynamics 79.5 MolDyn Molecular dynamics simulation 85.4 MonteCarlo Monte Carlo simulation 48.0 RayTracer \n3D ray tracer 79.8 Search Alpha-beta pruned search 73.4 Mapping to Hardware Instructions We can realize \neach of the prefetch and spec load instruc\u00adtions in two ways, the hardware instruction and a load instruction \nguarded by a software exception check. If the underlying processor provides the hardware support, we \nshould obviously use the hard\u00adware instruction. It takes less processor resources, and imposes less impact \non bandwidth since the processor cancels the execution of the instruction when a data translation lookaside \nbu.er (DTLB) miss will occur. Currently, the prefetch instructions are supported by most of the modern \ncommercial processors, while the specula\u00adtive load instruction only receives support in the Intel IA-64 \n[8] and the SPARC-V9 [19] architectures. However, a guarded load instruction is sometimes preferable \neven in the presence of hardware support, since we can use the guarded load instruction to .ll a missing \nDTLB entry in advance (called TLB priming in [9]). Thus, when the stride is larger than half of the page \nsize, the guarded load instruction might be better than the hardware instruction. A more important case \nis to prefetch the address obtained by a dereference. In the above example, it is not surprising if the \ndi.erence between A(Lx) and A(Ly) is often larger than half of the page size. Thus, it might be better \nto use the guarded instruction for prefetching A(Ly).   4. EXPERIMENTAL RESULTS We implemented our \nprefetching algorithm and evaluated the following two algorithms: INTER This option enables only inter-iteration \nstride prefetching. Note that this con.guration is a limited emulation of Wu s stride prefetching using \nour prefetching algorithm. The limi\u00adtations are that 1) we use object inspection instead of o.-line pro.ling, \nand 2) we apply prefetching only to in-loop loads. INTER+INTRA This option enables both inter-and intra-iteration \nstride prefetching as described in Section 3.  Figure 6: Speedup ratios on the Pentium 4. We investigated \nthe .rst 20 iterations of a given loop to collect the constant strides. We recognize that a constant \nstride is dom\u00adinant when it matches 75% of the all collected strides. We .xed the scheduling distance \nas one iteration for both inter-and intra\u00aditeration stride prefetching because our primary concern was \nnot to optimally tune up both kinds of stride prefetching, but to examine the e.ectiveness of intra-iteration \nstride prefetching. We prototyped our prefetching algorithm as an extension to the JIT compiler [11] \nof the IBM Developer Kit for Windows, Java Technology Edition, Version 1.3.1. The JVM runs in a mixed\u00admode, \nmeaning it selectively compiles methods that are executed frequently [22]. The garbage collector [6] \nof the JVM uses a tradi\u00adtional mark-and-sweep algorithm. Live objects are packed by slid\u00ading compaction, \nwhich does not change their internal order on the heap. Thus, the garbage collector usually preserves \nconstant strides among the live objects. We set the initial and the maximum heap sizes to 128 MB. The \nmeasurements were done on two workstations, one with a 2 GHz Intel Pentium 4 [9] processor and 1 GB of \nmemory, and the other with a 1.2 GHz AMD Athlon MP [1] processor and 512 MB of memory. Both of these \nprocessors provide out-of-order su\u00adperscalar execution, and software and hardware prefetching mech\u00adanisms. \nTable 2 shows the parameters of the Pentium 4 and the Athlon MP related to prefetching [9, 1]. The major \ndi.erences be\u00adtween the two processors that a.ect this research are that 1) the Pentium 4 provides a \nsmaller number of DTLB entries, and 2) the target cache levels for software prefetching are the L2 cache \non the Pentium 4 and the L1 cache on the Athlon MP. We used a load in\u00adstruction guarded by a software \nexception check for intra-iteration stride prefetching on the Pentium 4 in order to .ll a missing DTLB \nentry. Otherwise, we used a prefetch instruction provided by the processor. We used the Microsoft Windows \n2000 Professional op\u00aderating system on both workstations. We used two benchmark suites, the SPECjvm98 \nbenchmark [20] and the JavaGrande v2.0 [12] benchmark Section 3. To evaluate the performance improvement, \nwe report the best run times reported by the benchmarks themselves. The scores of the SPECjvm98 are usually \nmeasured by their best run times under automatic continu\u00adous execution. This means it tends to exclude \nthe JIT compilation time because after several runs, the benchmark is in the steady state where the JIT \ncompilation rarely occurs. We iterated each bench\u00admark ten times in its auto run mode. We set the problem \nsize to 100. For the JavaGrande benchmarks, we ran each benchmark once and therefore that run includes \nthe JIT compilation time in the best run time. We set the problem size to Size A . To evaluate the overhead \nof compilation time, we report the total execution time and the total Figure 7: Speedup ratios on the \nAthlon MP. JIT compilation time of each benchmark. Table 3 shows the de\u00adscription of the benchmarks in \nthe SPECjvm98 and the JavaGrande v2.0 Section 3. The last column shows the ratios of the execution time \nof the compiled code against the total execution times on the Pentium 4. The benchmarks are suitable \nfor evaluating the opti\u00admizations by a JIT compiler because the compiled code consumes over 70% of the \ntotal execution time except for jack, javac, and MonteCarlo. 4.1 Performance Figure 6 and Figure 7 show \nthe speedup ratios on the Pentium 4 and on the Athlon MP, respectively. The baseline is the execution \ntime without stride prefetching. Overall, the combination of inter-and intra-iteration stride prefetch\u00ading \ngives a performance improvement better than only using inter\u00aditeration stride prefetching. The algorithm \nINTER was e.ective for Euler on both processors, and was e.ective for MolDyn on the Athlon MP. The algorithm \nINTER+INTRA improved those pro\u00adgrams, as well as jess and db on both processors, and also improved RayTracer \non the Pentium 4. In particular, INTER+INTRA achieved an 18.9% speedup of db on the Pentium 4, and also \nachieved a 25.1% speedup on the Athlon MP, while INTER was ine.ective on both processors. This pro\u00adgram \nspends more than 85% of its execution time in a shell sort loop that reorders a number of large records \nand frequently causes cache misses and DTLB misses [18]. Each record contains a number of Vector and \nString objects, and they only have intra-iteration constant strides between the containing records in \nthe sorting loop. And also, INTER+INTRA achieved a 2.0% speedup for jess on the Pentium 4, and achieved \na 2.9% speedup on the Athlon MP, while INTER had very small or negative speedups. The improve\u00adments of \nthe jess benchmark by INTER+INTRA were rather small for two reasons. First, the method findInMemory() \nis hot, but not dominant. The hottest method, which the findInMemory() method is inlined into, uses only \nabout 25% of the compiled code execution time, while the compiled code takes about 70% of the to\u00adtal \nexecution time. Second, the cache line size is su.ciently large to contain both the Tokenobject and the \narray object pointed to by the facts.eld. The speedup on the Athlon MP was slightly larger than the Pentium \n4 because the Athlon MP has a larger number of DTLB entries. Since the benchmark Euler has inter-iteration \nconstant strides in its main data structures, large two-dimensional arrays of vectors, both algorithms \nachieved similar speedups on the Pentium 4 and the Athlon MP. The algorithm INTER+INTRA achieved a 15.4% \nspeedup on the Pentium 4, and 14.0% speedup on the Athlon MP.  Figure 8: L1 cache load MPIs on the Pentium \n4. While the MolDyn benchmark also has inter-iteration constant strides, neither algorithm improved it \non the Pentium 4. This is because the main data structure of MolDyn is a one-dimensional array of molecule \nobjects that .ts in the L2 cache given the prob\u00adlem size in this experiment. In contrast, both algorithms \nachieved small speedups on the Athlon MP, since the molecule objects are prefetched into the L1 cache. \nThe RayTracer benchmark showed an anomaly in that the algo\u00adrithm INTER+INTRA improves the performance \non the Pentium 4 and degrades it on the Athlon MP. One of the target loops of Ray-Tracer contains an \ninvocation of a recursive method. On the Pen\u00adtium 4, stride prefetching in that target loop also reduces \nthe cache misses in the other methods where prefetches are not inserted. We need further investigation \nof stride prefetching for this method in\u00advocation. Both algorithms slightly degraded the mpegaudio benchmark \non the Pentium 4. This is because the cache miss ratios and the DTLB miss ratio were quite small, as \nwe will see in Section 4.2. The benchmarks compress, javac, and Search do not contain code frag\u00adments \nwhere either intra-or inter-iteration stride prefetching are applicable.  4.2 Cache Misses and DTLB \nMisses In this section, we investigate the e.ect on cache misses and DTLB misses, comparing the execution \nwithout stride prefetch\u00ading (labeled as BASELINE ) and using our prefetching algorithm, INTER+INTRA. \nWe measured these results on the Pentium 4 us\u00ading the Intel VTune Performance Analyzer [10] Version 5.0. \nWe use a metric, misses per instruction (MPI), which is the number of dynamic miss events divided by \nthe number of retired instructions. Note that in the out-of-order superscalar execution, cache misses \nor DTLB misses are observed by all of the concurrently executed load instructions that access the same \ncache line or the same TLB entry. Thus, prefetching and TLB priming do not reduce the number of .lls, \nbut can reduce the number of miss events. Inserting prefetch instructions increases the number of retired \ninstructions, but they are relatively few compared to the reduction of the number of miss events. Our \nprefetching algorithm increases the number of retired instructions for db by 9.7%, for RayTracer by 6.9%, \nfor jess by 2.2%, and for the other benchmarks by less than 2%. Figure 8 shows the L1 cache load MPIs. \nOur prefetching algo\u00adrithm greatly decreased the L1 cache load MPI of db, and slightly decreased the \nL1 cache load MPIs of jess and RayTracer. This is because intra-iteration constant stride prefetching \nis applicable to these benchmarks, and an additional load instruction for prefetch- Figure 9: L2 cache \nload MPIs on the Pentium 4. ing brings the target cache line into the L1 cache. The L1 cache MPIs of \nmpegaudio and MonteCarlo are quite small, and thus prefetch\u00ading is not pro.table for these benchmarks. \nFigure 9 shows the L2 cache load MPIs. Our prefetching algo\u00adrithm greatly decreased the L2 cache load \nMPI of RayTracer, but this bene.t was not fully predicted at compile-time, because it was caused by the \ne.ects across multiple methods, as described above. Our prefetching algorithm also decreased the L2 cache \nload MPIs of db, Euler, and mtrt. Since inter-iteration stride prefetching is ap\u00adplicable to the Euler \nbenchmark, we can reduce its L2 cache load MPI more by a longer scheduling distance. Figure 10 shows \nthe DTLB load MPIs. Our algorithm greatly decreased the DTLB load MPIs of RayTracer and db, and slightly \ndecreased the DTLB load MPI of jess. The reduction of the L2 cache misses for mtrt is slightly better \nthan that for jess, but the speedup ratio of jess was better than that of mtrt. It suggests the importance \nof reducing the DTLB misses on the Pentium 4. 4.3 Compilation Time The left-hand bars in Figure 11 show \nadditional compilation time for our prefetching algorithm (INTER+INTRA) normalized by the total JIT compilation \ntime (indexed by the left axis). The baseline is compilation time without stride prefetching. The additional \ncom\u00adpilation time for our prefetching algorithm were less than 3.0% of the total JIT compilation time, \nand exceeded 2.0% for mpegaudio, Euler, and MolDyn, since these benchmarks contain a number of loops \nthat refer to arrays of objects. For the other benchmarks, the overheads were smaller than 1.5%. The \nright-hand bars in Figure 11 show total JIT compilation time normalized by the total execution time of \neach benchmark (indexed by the right axis). The total JIT compilation time was less than 13% of the total \nexecution time. Thus, by multiplying these numbers, we observed that the additional JIT compilation time \nrequired for our prefetching algorithm, in other words the runtime overhead, was less than 0.4% of the \ntotal execution time.  5. RELATED WORK Luk and Mowry [14, 15] studied software prefetching for recur\u00adsive \ndata structures (RDSs) such as linked lists, trees, and graphs. At a given RDS node ni, they wish to \nprefetch the node ni+d that will be visited d nodes after ni, and they propose three schemes for com\u00adputing \nthe address of ni+d , Ai+d, at node ni. While greedy prefetch\u00ading approximates Ai+d as one of the pointers \nfrom ni, history-pointer prefetching adds a jump-pointer at ni for this purpose that contains the observed \nvalue of Ai+d during a recent traversal. Data lineariza\u00adtion attempts to map the nodes of a RDS onto \nan array, so that one can easily predict Ai+d without any pointer dereference. Data linearization exploits \nthe same stride pattern information as (inter\u00aditeration) stride prefetching. In contrast to the attempts \nof stride prefetching to discover patterns, data linearization attempts to cre\u00adate them, which is almost \nimpossible for a compiler to do automati\u00adcally. Indeed, they only simulate data linearization by hand \nfor two benchmarks, where the RDSs already exhibit stride patterns, thus requiring no mapping for linearization. \n Stoutchinin et al. [21] proposed an automatic approach for prefetch\u00ading data for RDSs, which relies \nsolely on compiler analysis. They .rst identify pointer-chasing recurrences in loops with a low com\u00adplexity \nalgorithm. They then perform a pro.tability analysis to in\u00advestigate whether there are enough processor \nresources and avail\u00adable memory bandwidth for pro.table prefetching. Only if there are, they issue prefetch \ninstructions for the data accessed through induction pointers, which are the addresses used by the loads \nin\u00advolved in the pointer-chasing recurrences. In other words, they as\u00adsume that the loads exhibit constant \nstride patterns, and rely on the pro.tability analysis to avoid performance degradation when the assumption \ndoes not hold. However, their experimental results show that performance is actually degraded in six \nof the eleven pro\u00adgrams they tested. Wu [23] and Wu et al. [24] attempted to discover and exploit the \ninter-iteration stride patterns of load instructions through e.cient o.-line pro.ling. They exploit three \nstride patterns, strong single stride, phased multiple-stride, and weak single stride, by issuing di.erent \nprefetching code sequences. As expected, the majority of loads prefetched have strong single stride patterns. \nAlso, they han\u00addle both in-loop and out-of-loop loads, but the performance gain from prefetching out-of-loop \nloads was insigni.cant. We extend their approach in two signi.cant ways. First, we attempt to dis\u00adcover \nand exploit intra-iteration patterns as well as inter-iteration patterns. Second, we provide stride prefetching \nin a dynamic com\u00adpiler. Since the overhead of o.-line stride pro.ling is still too high for such a dynamic \nenvironment, we have invented an ultra\u00adlightweight pro.ling technique called object inspection. Mainly \nbecause we must use such a lightweight pro.ler, we focus on dis\u00adcovering single stride patterns in in-loop \nloads, but we believe that we are capturing most of the opportunities for performance gain that stride \nprefetching can provide. Chilimbi and Hirzel [4] developed a dynamic prefetching scheme for general-purpose \nprograms that involve extensive pointer deref\u00aderencing. They gather a temporal data reference pro.le \nand extract the hot data streams, which are data reference sequences that are frequently repeated in \nthe same order. The system then dynamically Figure 11: Compilation time for prefetching and total JIT \ncom\u00adpilation time. injects code at appropriate program points to detect and prefetch these hot data streams. \nPrefetching hot data streams does not re\u00adsult in stride prefetching. In other words, loads with stride \npatterns are not captured as hot data streams. Thus, the two approaches can work e.ectively together. \nCahoon and McKinley [2] proposed an e.ective data .ow anal\u00adysis technique for identifying RDS traversals \nin Java. The analysis contains intra-and inter-procedural components and .nds recurrent pointer variables \nthat occur both in loops and in recursive func\u00adtion calls. They use the analysis to drive greedy prefetching \nand history-pointer prefetching. They do not include stride prefetching, although it is possible as in \nStoutchinin et al. [21]. Their approach is based on whole-program analysis, and thus cannot be used for \na dynamic compilation system.  6. CONCLUDING REMARKS We have proposed a new algorithm for stride prefetching \nwhich is intended for use in a dynamic compiler. Our algorithm attempts to discover and exploit both \ninter-and intra-iteration stride pat\u00adterns. Intra-iteration patterns often result since the constructors \nin an object-oriented language tend to allocate a bunch of related objects and store references to them \ninto the objects being con\u00adstructed. We discover stride patterns using object inspection, an ultra\u00adlightweight \ntechnique for dynamic pro.ling which only a dynamic compiler is able to use. During the compilation of \na method, the dynamic compiler gathers an execution pro.le of the method by partially interpreting the \nmethod using the actual values of param\u00adeters and while causing no side e.ects. Also, in order to dis\u00adcover \nintra-iteration patterns e.ciently, we build a load depen\u00addence graph which represents as adjacent nodes \nthe load instruc\u00adtions which chase references and thus limits the number of pairs we must check for intra-iteration \npatterns. We evaluated an implementation of our prefetching algorithm in a production-level Java JIT \ncompiler. We measured the SPECjvm98 benchmark and Section 3 of the JavaGrande v2.0 benchmark on two di.erent \nIA-32 processors, an Intel Pentium 4 and an AMD Athlon MP. The results show that our prefetching algorithm \nachieved up to an 18.9% and 25.1% speedup on the Pentium 4 and the Athlon MP, respectively, while it \nincreased the compilation time by less than 3.0%. One of the future goals is to extend the scope of load \ninstructions for prefetching in our algorithm. In particular, handling out-of-loop loads in recursive \nmethods is important and expected to be as re\u00adwarding as in-loop loads, but discovering exploitable stride \npatterns for out-of-loop loads still remains as an open problem. Also, it might be interesting to combine \nstride prefetching with other types of prefetching, such as history-pointer prefetching and prefetching \nhot data streams. Acknowledgments We thank the members of the Network Computing Platform group of Tokyo \nResearch Laboratory for their support and valuable com\u00adments. We also thank the anonymous reviewers for \ntheir helpful comments and suggestions. 7. REFERENCES [1] Advanced Micro Devices, Inc. AMD Athlon Processor \nx86 Code Optimization Guide, Aug. 2001. Document Number 22007J. [2] B. Cahoon and K. S. McKinley. Data \nFlow Analysis for Software Prefetching Linked Data Structures in Java. In Proc. of the International \nConference on Parallel Architectures and Compiler Techniques, Sept. 2001. [3] D. Callahan, K. Kennedy, \nand A. Porter.eld. Software Prefetching. In Proc. of the Fourth International Conference on Architectural \nSupport for Programming Languages and Operating Systems, pages 40 52, Apr. 1991. [4] T. M. Chilimbi and \nM. Hirzel. Dynamic Hot Data Stream Prefetching for General-Purpose Programs. In PLDI 02 [17], pages 199 \n209. [5] S. Dieckmann and U. H\u00a8 olzle. A Study of the Allocation Behavior of the SPECjvm98 Java Benchmarks. \nIn Proc. of the 13th European Conference of Object Oriented Programming, pages 92 115, 1999. LNCS 1628. \n[6] R. Dimpsey, R. Arora, and K. Kuiper. Java Server Performance: A Case Study of Building E.cient, Scalable \nJVMs. IBM Systems Journal, 39(1):151 174, 2000. [7] J. Gosling, B. Joy, and G. Steele. The Java Language \nSpeci.cation. Addison-Wesley Publishing Co., Reading, MA, 1996. [8] Intel Corporation. Intel Itanium \nArchitecture Software Developer s Manual Volume 3: Instruction Set Reference, 2001. Revision 2.0, Document \nNumber 245319-003. [9] Intel Corporation. Intel Pentium 4 Processor Optimization Reference Manual, 2001. \nDocument Number 248966. [10] Intel Corporation. VTune Performance Analyzer. http://www.intel.com/software/products/vtune, \n2002. [11] K. Ishizaki, M. Kawahito, T. Yasue, M. Takeuchi, T. Ogasawara, T. Suganuma, T. Onodera, H. \nKomatsu, and T. Nakatani. Design, Implementation, and Evaluation of Optimizations in a Just-In-Time Compiler. \nIn Proc. of the ACM JavaGrande Conference, pages 119 128, June 1999. [12] Java Grande Benchmarking Project. \nJava Grande Forum Benchmark Suite, Version 2.0. http://www.epcc.ed.ac.uk/javagrande, 1999. [13] N. P. \nJouppi. Improving Direct-Mapped Cache Performance by the Addition of a Small Fully-Associative Cache \nand Prefetch Bu.ers. In Proc. of the 17th Annual International Symposium on Computer Architecture, pages \n364 373, 1990. [14] C.-K. Luk and T. C. Mowry. Compiler-Based Prefetching for Recursive Data Structures. \nIn Proc. of the 7th International Conference on Architectural Support for Programming Languages and Operating \nSystems, pages 222 233, Oct. 1996. [15] C.-K. Luk and T. C. Mowry. Automatic Compiler-Inserted Prefetching \nfor Pointer-Based Applications. IEEE Transactions on Computers, 48(2), 1999. [16] T. C. Mowry, M. S. \nLam, and A. Gupta. Design and Evaluation of a Compiler Algorithm for Prefetching. In Proc. of the Fifth \nInternational Conference on Architectural Support for Programming Languages and Operating Systems, pages \n62 73, Oct. 1992. [17] Proc. of the ACM SIGPLAN Conference on Programming Language Design and Implementation, \nJune 2002. [18] Y. Shuf, M. J. Serrano, M. Gupta, and J. P. Singh. Characterizing the Memory Behavior \nof Java Workloads: A Structured View and Opportunities for Optimizations. In Proc. of the ACM SIGMETRICS \nInternational Conference on Measurement and Modeling of Computer Systems, pages 194 205, June 2001. [19] \nSPARC International, Inc. The SPARC Architecture Manual Version 9, 2000. Document Number SAV09R1459912. \n[20] Standard Performance Evaluation Corporation (SPEC). JVM Client98 (SPECjvm98). http://www.spec.org/osg/jvm98, \n1998. [21] A. Stoutchinin, J. N. Amaral, G. R. Gao, J. Dehnert, S. Jain, and A. Douillet. Speculative \nPrefetching of Induction Pointers. In Proc. of the 10th International Conference on Compiler Construction, \npages 289 303, Apr. 2001. LNCS 2027. [22] T. Suganuma, T. Ogasawara, M. Takeuchi, T. Yasue, M. Kawahito, \nK. Ishizaki, H. Komatsu, and T. Nakatani. Overview of the IBM Java Just-In-Time Compiler. IBM Systems \nJournal, 39(1):175 193, Feb. 2000. [23] Y. Wu. E.cient Discovery of Regular Stride Patterns in Irregular \nPrograms and Its Use in Compiler Prefetching. In PLDI 02 [17], pages 210 221. [24] Y. Wu, M. Serrano, \nR. Krishnaiyer, W. Li, and J. Fang. Value-Pro.le Guided Stride Prefetching for Irregular Code. In Proc. \nof the 11th International Conference on Compiler Construction, pages 307 324, Apr. 2002. LNCS 2304. \n \n\t\t\t", "proc_id": "781131", "abstract": "Software prefetching is a promising technique to hide cache miss latencies, but it remains challenging to effectively prefetch pointer-based data structures because obtaining the memory address to be prefetched requires pointer dereferences. The recently proposed stride prefetching overcomes this problem, but it only exploits <i> inter-iteration</i> stride patterns and relies on an off-line profiling method.We propose a new algorithm for stride prefetching which is intended for use in a dynamic compiler. We exploit both <i>inter-</i> and <i>intra-iteration</i> stride patterns, which we discover using an ultra-lightweight profiling technique, called <i>object inspection</i>. This is a kind of partial interpretation that only a dynamic compiler can perform. During the compilation of a method, the dynamic compiler gathers the profile information by partially interpreting the method using the actual values of parameters and causing no side effects.We evaluated an implementation of our prefetching algorithm in a production-level Java just-in time compiler. The results show that the algorithm achieved up to an <i>18.9%</i> and <i>25.1%</i> speedup in industry-standard benchmarks on the Pentium 4 and the Athlon MP, respectively, while it increased the compilation time by less than <i>3.0%</i>.", "authors": [{"name": "Tatsushi Inagaki", "author_profile_id": "81100024923", "affiliation": "IBM Tokyo Research Laboratorym Shimotsuruma, Yamato-shi, Kanagawa-ken 242-8502, Japan", "person_id": "P497399", "email_address": "", "orcid_id": ""}, {"name": "Tamiya Onodera", "author_profile_id": "81100474003", "affiliation": "IBM Tokyo Research Laboratorym Shimotsuruma, Yamato-shi, Kanagawa-ken 242-8502, Japan", "person_id": "PP31043832", "email_address": "", "orcid_id": ""}, {"name": "Hideaki Komatsu", "author_profile_id": "81100557247", "affiliation": "IBM Tokyo Research Laboratorym Shimotsuruma, Yamato-shi, Kanagawa-ken 242-8502, Japan", "person_id": "PP39048455", "email_address": "", "orcid_id": ""}, {"name": "Toshio Nakatani", "author_profile_id": "81100311827", "affiliation": "IBM Tokyo Research Laboratorym Shimotsuruma, Yamato-shi, Kanagawa-ken 242-8502, Japan", "person_id": "PP14113792", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/781131.781161", "year": "2003", "article_id": "781161", "conference": "PLDI", "title": "Stride prefetching by dynamically inspecting objects", "url": "http://dl.acm.org/citation.cfm?id=781161"}