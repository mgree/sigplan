{"article_publication_date": "05-09-2003", "fulltext": "\n Linear Analysis and Optimization of Stream Programs Andrew A. Lamb, William Thies and Saman Amarasinghe \n{aalamb, thies, saman}@lcs.mit.edu Laboratory for Computer Science Massachusetts Institute of Technology \nABSTRACT As more complex DSP algorithms are realized in practice, there is an increasing need for high-level \nstream abstractions that can be compiled without sacri.cing e.ciency. Toward this end, we present a set \nof aggressive optimizations that target linear sections of a stream program. Our input lan\u00adguage is StreamIt, \nwhich represents programs as a hierar\u00adchical graph of autonomous .lters. A .lter is linear if each of \nits outputs can be represented as an a.ne combination of its inputs. Linearity is common in DSP components; \nexam\u00adples include FIR .lters, expanders, compressors, FFTs and DCTs. We demonstrate that several algorithmic \ntransformations, traditionally hand-tuned by DSP experts, can be completely automated by the compiler. \nFirst, we present a linear ex\u00adtraction analysis that automatically detects linear .lters from the C-like \ncode in their work function. Then, we give a procedure for combining adjacent linear .lters into a single \n.lter, as well as for translating a linear .lter to operate in the frequency domain. We also present \nan optimization se\u00adlection algorithm, which .nds the sequence of combination and frequency transformations \nthat will give the maximal bene.t. We have completed a fully-automatic implementation of the above techniques \nas part of the StreamIt compiler, and we demonstrate a 450% performance improvement over our benchmark \nsuite. Categories and Subject Descriptors D.3.4 [Programming Languages]: Processors; D.3.2 [Pro\u00adgramming \nLanguages]: Language Classi.cations; D.2.2 [Software Engineering]: Software Architectures; D.3.3 [Programming \nLanguages]: Language Constructs and Features General Terms Languages, Performance, Design, Algorithms \nKeywords Stream Programming, StreamIt, Optimization, Embedded, Linear Systems, Algebraic Simpli.cation, \nDSP, FFT Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI \n03, June 9 11, 2003, San Diego, California, USA. Copyright 2003 ACM 1-58113-662-5/03/0006 ...$5.00 1. \nINTRODUCTION Digital computation is a ubiquitous element of modern life. Everything from cell phones \nto HDTV systems to satel\u00adlite radios require increasingly sophisticated algorithms for digital signal \nprocessing. Optimization is especially impor\u00adtant in this domain, as embedded devices commonly have high \nperformance requirements and tight resource constraints. Consequently, there are often two stages to \nthe development process: .rst, the algorithm is designed and simulated at a high level of abstraction, \nand second, it is optimized and re\u00adimplemented at a low level by an expert DSP programmer. In order to \nachieve high performance, the DSP programmer needs to take advantage of architecture-speci.c features \nand constraints (usually via extensive use of assembly code) as well as global properties of the application \nthat could be ex\u00adploited to obtain algorithmic speedups. Apart from requir\u00ading expert knowledge, this \ne.ort is time-consuming, error\u00adprone, and costly, and must be repeated for every change in the target \narchitecture and every adjustment to the high\u00adlevel system design. As embedded applications continue \nto grow in complexity, these factors will become unmanageable. There is a pressing need for high-level \nDSP abstractions that can be compiled without any performance penalty. In this paper, we develop a set \nof optimizations that lower the entry barrier for high-performance stream programming. Our work is done \nin the context of StreamIt [7, 19], which is a high-level language for signal processing applications. \nA program in StreamIt is comprised of a set of concurrently ex\u00adecuting .lters, each of which contains \nits own address space and communicates with its neighbors using FIFO queues. Our analysis focuses on \n.lters which are linear: their outputs can be expressed as an a.ne combination of their inputs. Linear \n.lters are common in DSP applications; examples in\u00adclude FIR .lters, expanders, compressors, FFTs and \nDCTs. In practice, there are a host of optimizations that are applied to linear portions of a stream \ngraph. In partic\u00adular, neighboring linear nodes can be combined into one, and large linear nodes can \nbene.t from translation into the frequency domain. However, these optimizations require detailed mathematical \nanalysis and are tedious and com\u00adplex to implement. They are only bene.cial under cer\u00adtain conditions \nconditions that might change with the next version of the system, or that might depend on neighbor\u00ading \ncomponents that are being written by other develop\u00aders. To improve the modularity, portability, and extensibil\u00adity \nof stream programs, the compiler should be responsible for identifying linear nodes and performing the \nappropriate optimizations. Toward this end, we make the following con\u00adtributions: Figure 1: Block diagram \nof two FIR .lters. /* perform two consecutive FIR filters with weights w1, w2 */ void two_filters(float* \nw1, float* w2, int N) { int i; float data[N]; /* input data buffer */ float buffer[N]; /* inter-filter \nbuffer */ for (i=0; i<N; i++) { /* initialize the input data buffer */ data[i] = get_next_input(); } \nfor (i=0; i<N; i++) { /* initialize inter-filter buffer */ buffer[i] = filter(w1, data, i, N); data[i] \n= get_next_input(); } i = 0; while(true) { /* generate next output item */ push_output(filter(w2, buffer, \ni, N)); /* generate the next element in the inter-filter buffer */ buffer[i] = filter(w1, data, i, N); \n/* get next data item */ data[i] = get_next_input(); /* update current start of buffer */ i = (i+1)%N; \n} } /* perform N-element FIR filter with weights and data */ float filter(float* weights, float* data, \nint pos, int N) { int i; float sum = 0; /* perform weighted sum, starting at index pos */ for (i=0; i<N; \ni++, pos++) { sum += weights[i] * data[pos]; pos = (pos+1)%N; } return sum; } Figure 2: Two consecutive \nFIR .lters in C. Channels are represented as circular bu.ers, and the schedul\u00ading is done by hand. A \nlinear data.ow analysis that extracts an abstract linear representation from imperative C-like code. \n An automated transformation of neighboring linear nodes into a single collapsed representation.  An \nautomated translation of linear nodes into the fre\u00adquency domain.  An optimization selection algorithm \nthat determines which transformations are most bene.cial to apply.  A fully-automatic implementation \nof these techniques in the StreamIt compiler, demonstrating an average speedup of 450% and a best-case \nspeedup of 800%.  In the rest of this section, we give a motivating example and background information \non StreamIt. Then we present our linear node representation (Section 2) and our supporting data.ow analysis \n(Section 3). Next we describe structural transformations on linear nodes (Section 4), a frequency do\u00admain \noptimization (Section 5) and an optimization selec\u00adtion algorithm (Section 6). Finally, we present results \n(Sec\u00adtion 7), related work (Section 8) and conclusions (Section 9). 1.1 Motivating Example To illustrate \nthe program transformations that our tech\u00adnique is designed to automate, consider a sequence of .nite \nimpulse response (FIR) .lters as shown in Figure 1. The float->float pipeline TwoFilters(float[N] w1, \nfloat[N] w2) { add FIRFilter(w1); add FIRFilter(w2); } float->float filter FIRFilter(float[N] weights) \n{ work push 1 pop 1 peek N { float sum = 0; for (int i=0; i<N; i++) { sum += weights[i] * peek(i); } \npush(sum); pop();  } } Figure 3: Two consecutive FIR .lters in StreamIt. Bu.er management and scheduling \nare handled by the compiler. float->float filter CollapsedTwoFilters(float[N] w1, float[N] w2) { float[N] \ncombined_weights; init { /* calculate combined_weights from w1 and w2 */ } work push 1 pop 1 peek N { \nfloat sum = 0; for (int i=0; i<N; i++) { sum += combined_weights[i]*peek(i); } push(sum); pop();  } \n} Figure 4: Combined version of the two FIR .lters. Since each FIR .lter is linear, the weights can be \ncombined into a single combined weights array. float->float pipeline FreqTwoFilters(float[N] w1, float[N] \nw2) { float[N] combined_weights = ... ; // calc. combined weights complex[N] H = fft(combined_weights); \n// take FFT of weights add FFT(); // add FFT stage to stream add ElementMultiply(H); // add multiplication \nby H add IFFT(); // add inverse FFT } Figure 5: Combined version of two FIR .lters in the frequency domain. \nimperative C style code that implements this simple DSP application is shown in Figure 2. The program \nlargely de\u00ad.es many standard compiler analysis and optimization tech\u00adniques because of its use of circular \nbu.ers and the muddled relationship between data, buffer and the output. Figure 3 shows the same .ltering \nprocess in StreamIt. The StreamIt version is more abstract than the C version. It indicates the communication \npattern between .lters, shows the structure of the original block diagram and leaves the complexities \nof bu.er management and scheduling to the compiler. Two optimized versions of the FIR program are shown \nin Figures 4 and 5. In Figure 4, the programmer has combined the weights arrays from the two .lters into \na single, equiv\u00adalent array. This reduces the number of multiply operations by a factor of two. In Figure \n5, the programmer has done the .ltering in the frequency domain. Computationally in\u00adtensive streams are \nmore e.cient in frequency than in time. Our linear analysis can automatically derive both of the implementations \nin Figures 4 and 5, starting with the code in Figure 3. These optimizations free the programmer from \nthe burden of combining and optimizing linear .lters by hand. Instead, the programmer can design modular \n.lters at the natural granularity for the algorithm in question and rely on the compiler for combination \nand transformation. (a) A pipeline. (b) A splitjoin. (c) A feedbackloop. Figure 6: Stream structures \nsupported by StreamIt.  1.2 StreamIt StreamIt is a language and compiler for high-performance signal \nprocessing [6, 7, 19]. In a streaming application, each data item is in the system for only a small amount \nof time, as opposed to scienti.c applications where the data set is used extensively over the entire \nexecution. Also, stream programs have abundant parallelism and regular commu\u00adnication patterns. The StreamIt \nlanguage aims to expose these properties to the compiler while maintaining a high level of abstraction \nfor the programmer. StreamIt programs are composed of processing blocks called .lters. Each .lter has \nan input tape from which it can read values and an output tape to which it can write values. Each .lter \nalso contains a work function which describes the .l\u00adter s atomic execution step in the steady state. \nIf the .rst invocation of the work function has di.erent behavior than other executions, a special prework \nfunction is de.ned. The work function contains C-like imperative code, which can access .lter state, \ncall external routines and produce and consume data. The input and output channels are treated as FIFO \nqueues, which can be accessed with three primitive operations: 1) pop(), which returns the .rst item \non the input tape and advances the tape by one item, 2) peek(i), which returns the value at the ith position \non the input tape, and 3) push(v), which pushes value v onto the output tape. Each .lter must declare \nthe maximum element it will peek at, the number of elements it will pop, and the number of elements that \nit will push during an execution of work. These rates must be resolvable at compile time and constant \nfrom one invocation of work to the next. A program in StreamIt consists of a hierarchical graph of .lters. \nFilters can be connected using one of the three prede.ned structures shown in Figure 6: 1) pipelines \nrep\u00adresent the serial computation of one .lter after another, 2) splitjoins represent explicitly parallel \ncomputation, and 3) feedbackloops allow cycles to be introduced into the stream graph. A stream is de.ned \nto be either a .lter, pipeline, splitjoin or feedbackloop. Every subcomponent of a struc\u00adture is a stream, \nand all streams have exactly one input tape and exactly one output tape. It has been our experience that \nmost practical applications can be represented using StreamIt s hierarchical structures. Though sometimes \na program needs to be reorganized to .t into the structured paradigm, there are bene.ts for both the \nprogrammer and the compiler in having a structured language [19]. In particular, the linear analyses \ndescribed in this paper rely heavily on the structure of StreamIt since they focus on each hierarchical \nprimitive rather than dealing with the complexity of arbitrary graphs. = y Figure 7: Representation \nof a linear node.  2. REPRESENTING LINEAR NODES There is no general relationship that must hold between \na .lter s input data and its output data. In actual appli\u00adcations, the output is typically derived from \nthe input, but the relationship is not always clear since a .lter has state and can call external functions. \nHowever, we note that a large subset of DSP operations produce outputs that are some a.ne function of \ntheir in\u00adput, and we call .lters that implement such operations lin\u00adear. Examples of such .lters are \n.nite impulse response (FIR) .lters, compressors, expanders and signal processing transforms such as \nthe discrete Fourier transform (DFT) and discrete cosine transformation (DCT). Our formal de.\u00adnition \nof a linear node is as follows (refer to Figure 7 for an illustration). Definition 1. (Linear node) A \nlinear node A = {A, bb, e, o, u} represents an abstract stream block which performs an a.ne transformation \nby = bxA + bb from input elements bx to output elements by. A is an e \u00d7 u matrix, bb is a u-element row \nvector, and e, o and u are the peek, pop and push rates, respectively. A .ring of a linear node A corresponds \nto the following series of abstract execution steps. First, an e-element row vector bx is constructed \nwith bx[i]= peek(e - 1 - i). The node computes by = bxA + bb, and then pushes the u elements of by onto \nthe output tape, starting with by [u - 1] and proceeding through by [0]. Finally, o items are popped \nfrom the input tape. The intuition of the computation represented by a linear node is simply that speci.c \ncolumns generate speci.c out\u00adputs and speci.c rows correspond to using speci.c inputs. The values found \nin row e - 1 - i of A (i.e., the ith row from the bottom) and column u - 1 - j of A (i.e., the jth column \nfrom the right) represents a term in the formula to compute the jth output item using the value of peek(i). \nThe value in column u - 1 - j of bb is a constant o.set added to output j. Figure 7 shows a concrete \nexample of a work function and its corresponding linear node. 3. LINEAR EXTRACTION ALGORITHM Our linear \nextraction algorithm can identify a linear .lter and construct a linear node A that fully captures its \nbe\u00adhavior. The technique, which appears as Algorithm 1 on Algorithm 1 Linear extraction analysis. proc \nToplevel(.lter F ) returns linear node for F 1. Set globals Peek, Pop, Push to I/O rates of .lter F . \n 2. Let A0 + new .oat[Peek, Push] with each entry = = 3. Let bb0 + new .oat[Push] with each entry = \n= 4. (map, A,bb, popcount, pushcount) + b  Extract(Fwork , (Ax.=),A0,b0 , 0, 0) 5. if A and bb contain \nno T or = entries then return linear node A = {A,bb, Peek, Pop, Push} else fail endif proc BuildCoe.(int \npos) returns bv for peek at index pos bv = b0 bv[Peek - 1 - pos]=1 return bv the next page, is a .ow-sensitive, \nforward data.ow analysis similar to constant propagation. Unlike a standard data.ow analysis, we can \na.ord to symbolically execute all loop iter\u00adations, since most loops within a .lter s work function have \nsmall bounds that are known at compile time (if a bound is statically unresolvable, the .lter is unlikely \nto be linear and we disregard it). During symbolic execution, the algorithm computes the following for \neach point of the program (refer to Figure 8 for notation): A map between each program variable y and \na linear form .bv, c. where bv is a P eek-element column vector and c is a scalar constant. In an actual \nexecution, the value of y would be given by y = bv \u00b7 bx + c, where bx represents the input items.  Matrix \nA and vector bb, which will represent the linear node. These values are constructed during the opera\u00adtion \nof the algorithm.  pushcount, which indicates how many items have been pushed so far. This is used to \ndetermine which column of A and bb correspond to a given push statement.  popcount, which indicates \nhow many items have been popped so far. This is used to determine the input item that a given peek or \npop expression refers to.  We now brie.y discuss the operation of Extract at each program node. The \nalgorithm is formulated in terms of a simpli.ed set of instructions, which appear in Figure 8. First \nare the nodes that generate fresh linear forms. A constant assignment y = c creates a form .b0,c. for \ny, since y has con\u00adstant part c and does not yet depend on the input. A pop operation creates a form \n.BuildCoe.(popcount), 0., where BuildCoe. introduces a coe.cient of 1 for the current in\u00addex on the input \nstream. A peek(i) operation is similar, but o.set by the index i. Next are the instructions which combine \nlinear forms. In the case of addition or subtraction, we simply add the com\u00adponents of the linear forms. \nIn the case of multiplication, the result is still a linear form if either of the terms is a known constant \n(i.e., a linear form .b0,c.). For division, the result is linear only if the divisor is a non-zero constant1 \nand Note that if the dividend is zero and the divisor has a non-zero coe.cients vector, we cannot conclude \nthat the result is zero, since certain runtime inputs might cause a singularity. b proc Extract(code, \nmap, A, b, int popcount, int pushcount) b returns updated map, A, b, popcount, and pushcount for i + \n1 to code.length do switch code[i] case y := const map.put(y, (b0, const)) case y := pop() map.put(y, \n.BuildCoe.(popcount), 0.) popcount++ case y := peek(i) map.put(y, .BuildCoe.(popcount + i), 0.) case \npush(y) .bv, c.+ map.get(y) if pushcount = T then fail A[., Push - 1 - pushcount] + bv b b[Push - 1 \n- pushcount] + c pushcount++ case y1 := y2 op y3, for op .{+, -} .bv2 ,c2 .+ map.get(y2) .bv3 ,c3 .+ \nmap.get(y3) map.put(y1, .bv2 op bv3 ,c2 op c3 .) case y1 := y2 . y3 .bv2 ,c2 .+ map.get(y2) .bv3 ,c3 \n.+ map.get(y3) if bv2 = b0 then map.put(y1 , (c2 . bv3 ,c2 . c3 )) else if bv3 = b0 then map.put(y1 \n, (c3 . bv2 ,c3 . c2 )) else map.put(y1 , T) case y1 := y2/y3 .bv2 ,c2 .+ map.get(y2) .bv3 ,c3 .+ map.get(y3) \nif b= b8 v3 0 . c3 =0 then map.put(y1 , ( 1 . bv2 ,c2 /c3 )) c3 else map.put(y1 , T) case y1 := y2 op \ny3, for op .{&#38;, |, ., &#38;&#38;, ||, !, etc.} .bv2 ,c2 .+ map.get(y2) .bv3 ,c3 .+ map.get(y3) map.put(y1, \n(b0 U bv2 U bv3 ,c2 op c3 )) case (loop N code') for j + 1 to N do (map, A,bb, popcount, pushcount) + \nExtract(code, map, A,bb, popcount, pushcount) case (branch code1 code2) b (map1 ,A1,b1 , popcount1 , \npushcount1 ) + Extract(code1 , map, A,bb, popcount, pushcount) b (map2 ,A2,b2 , popcount2 , pushcount2 \n) + Extract(code2 , map, A,bb, popcount, pushcount) map + map1 U map2 A + A1 U A2 bb + bb1 U bb2 popcount \n+ popcount1 U popcount2 pushcount + pushcount1 U pushcount2 end for b return (map, A, b, popcount, pushcount) \ny . program-variable T c . constant b bv,b . vectorT .bv, c.. linear-formT map . program-variable -linear-form \n(a hashtable) A . matrixT code . list of instructions, each of which can be: y1 := const push(y1) y1 \n:= pop() (loop N code) y1 := peek(i)(branch code1 code2 ) y1 := y2 op y3 Figure 8: Data types for the \nextraction analysis. for non-linear operations (e.g., bit-level and boolean), both operands must be known \nconstants. If any of these condi\u00adtions are not met, then the LHS is assigned a value of T, which will \nmark the .lter as non-linear if the value is ever pushed. The .nal set of instructions deal with control \n.ow. For loops, we resolve the bounds at compile time and execute the body an appropriate number of times. \nFor branches, we have to ensure that all the linear state is modi.ed consistently on both sides of the \nbranch. For this we apply the con.uence operator U, which we de.ne for scalar constants, vectors, matrices, \nlinear forms, and maps. c1 Uc2 is de.ned according to the lattice constantT. That is, c1 U c2 = c1 if \nand only if c1 = c2 ; otherwise, c1 U c2 = T. For vectors, matrices, and linear forms, U is de.ned element-wise; \nfor example, A ' = A1 U A2 is equivalent to A ' [i, j]= A1 [i, j] U A2[i, j]. For maps, the join is taken \non the values: map1 U map2 = map , where map .get(x)= map1.get(x) U map2.get(x). Our implementation of \nlinear extraction is also interproce\u00addural. It is straightforward to transfer the linear state across \na call site, although we omit this from the pseudocode for the sake of presentation. Also implicit in \nthe algorithm de\u00adscription is the fact that all variables are local to the work function. If a .lter \nhas persistent state, all accesses to that state are marked as T.  4. COMBINING LINEAR FILTERS A primary \nbene.t of linear .lter analysis is that neighbor\u00ading .lters can be collapsed into a single matrix representa\u00adtion \nif both of the .lters are linear. This transformation can automatically eliminate redundant computations \nin linear sections of the stream graph, thereby allowing the program\u00admer to write simple, modular .lters \nand leaving the combi\u00adnation to the compiler. In this section, we .rst describe a linear expansion operation \nthat is needed to match the sizes of A and bb for di.erent linear nodes and is therefore an es\u00adsential \nbuilding block for the other combination techniques. We then give rules for collapsing pipelines and \nsplitjoins into linear nodes; we do not deal with feedbackloops as they re\u00adquire linear state, which \nwe do not describe here. 4.1 Linear Expansion In StreamIt programs, the input and output rate of each \n.lter in the stream graph is known at compile time. The StreamIt compiler leverages this information \nto compute a static schedule that is, an ordering of the node executions such that each .lter will have \nenough data available to atom\u00adically execute its work function, and no bu.er in the stream graph will \ngrow without bound in the steady state. A gen- U' UU UU u' e' -(e + o ( -1)) * u e e e e' e '' ' Figure \n9: Expanding a linear node to rates (e ,o ,u ). eral method for scheduling StreamIt programs is given \nby Karczmarek [10]. A fundamental aspect of the steady-state schedule is that neighboring nodes might \nneed to be .red at di.erent fre\u00adquencies. For example, if there are two .lters F1 and F2 in a pipeline \nand F1 produces 2 elements during its work function but F2 consumes 4 elements, then it is necessary \nto execute F1 twice for every execution of F2 . Consequently, when we combine hierarchical structures \ninto a linear node, we often need to expand a matrix repre\u00adsentation to represent multiple executions \nof the correspond\u00ading stream. Expansion allows us to multiply and interleave columns from matrices that \noriginally had mismatching di\u00admensions. The transformation can be done as follows. Transformation 1. \n(Linear expansion) Given a linear node A = {A,bb, e, o, u}, the expansion of A to a rate of '' ' b'' \n' (e ,o ,u ' ) is given by expand(A, e ' ,o ,u ' )= {A ' ,b ' ,e ,o ,u }, '' b ' where A ' is a e \u00d7 u \nmatrix and bis a u ' -element row vec\u00adtor: shift(r, c) is a u ' \u00d7 e ' matrix : A[i - r, j - c] shift(r, \nc)[i, j]=if i - r . [0,e - 1] . j - c . [0,u - 1] 0 otherwise l A '*u/u8' ' = Im=0 shift(u - u - m . \nu, e - e - m . o) bb ' b' [j]= b[u - 1 - (u - 1 - j) mod u] The intuition behind linear expansion is \nstraightforward (see Figure 9). Linear expansion aims to scale the peek, pop and push rates of a linear \nnode while preserving the functional relationship between the values pushed and the values peeked on \na given execution. To do this, we construct a new matrix A ' that contains copies of A along the diagonal \nstarting from the bottom right. To account for items that are popped between invocations, each copy of \nA is o.set by o from the previous copy. The complexity of the de.nition is due to the end cases. If the \nnew push rate u ' is nota multiple of the old push rate u, then the last copy of A includes only some \nof its columns. Similarly, if the new peek rate e ' exceeds that which is needed by the diagonal of As, \nthen A ' needs to be padded with zeros at the top (since it peeks at some values without using them in \nthe computation). Note that a sequence of executions of an expanded node A ' might not be equivalent \nto any sequence of executions of the original node A, because expansion resets the push and pop rates \nand can thereby modify the ratio between them. However, if u ' = k . u and o ' = k . o for some integer \nk, then A ' is completely interchangeable with A. In the combination rules that follow, we utilize linear \nexpansion both in contexts that do and do not satisfy this condition.  4.2 Collapsing Linear Pipelines \nThe pipeline construct is used to compose streams in se\u00adquence, with the output of stream i connected \nto the input of stream i + 1. The following transformation describes how to collapse two linear nodes \nin a pipeline; it can be applied re\u00adpeatedly to collapse any number of neighboring linear nodes. Transformation \n2. (Pipeline combination) Given two linear nodes A1 and A2 where the output of A1 is connected to the \ninput of A2 in a pipeline construct, the combination '' ' pipeline(A1,A2 )= {A ' ,bb ' , e , o , u } \nrepresents an equiva\u00adlent node that can replace the original two. Its components are as follows: chanPop \n= lcm(u1 ,o2 ) chanPeek = chanPop + e2 - o2 chanPeek Ae 1 = expand(A1, ( rl- 1) . o1 + e1, u1 chanPop \n. o1 , chanPeek) u1 Ae 2 = expand(A2, chanPeek, chanPop, chanPop . u2 ) A ' = Ae 1 Ae 2 o2 bb ' = bb1 \ne Ae 2 + bbe 2 ' e e = e 1 ' e o = o 1 ' e u = u 2 The basic forms of the above equations are simple \nto de\u00adrive. Let bxi and byi be the input and output channels, respec\u00adtively, for Ai. Then we have by \nde.nition that by1 = bx1A1 +bb1 and by2 = bx2A2 +bb2 . But since A1 is connected to A2, we have that \nbx2 = by1 and thus by2 = by1A2 +bb2 . Substituting the value of by1 from our .rst equation gives yb2 \n= bx1A1A2 +bb1A2 +bb2 . Thus, the intuition is that the two-.lter sequence can be represented by matrices \nA ' = A1A2 and bb ' = bb1 A2 + bb2 , with peek and pop rates borrowed from A1 and the push rate borrowed \nfrom A2 . However, there are two implicit assumptions in the above analysis which complicate the equations \nfor the general case. First, the dimensions of A1 and A2 must match for the ma\u00adtrix multiplication to \nbe well-de.ned. If u1 8= e2, we .rst construct expanded nodes Ae 1 and Ae 2 in which the push and peek \nrates match so Ae 1 and Ae 2 can be multiplied. The second complication is with regards to peeking. If \nthe downstream node A2 peeks at items which it does not consume (i.e., if e2 >o2), then there needs to \nbe a bu.er to hold items that are read during multiple invocations of A2 . However, in our current formulation, \na linear node has no concept of internal state, such that this bu.er cannot be incorporated into the \ncollapsed representation. To deal with this issue, we adjust the expanded form of A1 to re\u00adcalculate \nitems that A2 uses more than once, thereby trad\u00ading computation for storage space. This adjustment is \nev\u00adident in the push and pop rates chosen for Ae 1: though A1 pushes u1 items for every o1 items that \nit pops, Ae 1 pushes chanPeek . u1 for every chanPop . o1 that it pops. When chanPeek > chanPop, this \nmeans that the outputs of Ae 1 are overlapping, and chanPeek - chanPop items are being regenerated on \nevery .ring.  Note that although Ae 1 performs duplicate computations in the case where A2 is peeking, \nthis computation cost can be amortized by increasing the value of chanPop. That is, though the equations \nset chanPop as the least common mul\u00adtiple of u1 and o2, any common multiple is legal. As chanPop grows, \nthe regenerated portion chanPeek-chanPop becomes smaller on a percentage basis. It is the case that some \ncollapsed linear nodes are always less e.cient than the original pipeline sequence. The worst case is \nwhen Ae 1 is a column vector and Ae 2 is a row vec\u00adtor, which requires O(N) operations originally but \nO(N 2 ) operations if combined (assuming vectors of length N). To avoid such performance-degrading combinations, \nwe employ an automated selection algorithm that only performs bene\u00ad.cial transformations (see Section \n6). Figure 10 illustrates the combination of back to back FIR .lters. Since the push rate of the .rst \n.lter (u1 = 1) di.ers from the peek rate of the second (e2 = 3), the .rst .lter must be expanded to Ae \n1 = expand(A1, 4, 1, 3). There is no need to expand the second .lter, so A2 e = A2. By construction, \nwe can now form the matrix product of Ae 1 and Ae 2, which corresponds to the matrix for the overall \nlinear node. 4.3 Collapsing Linear SplitJoins The splitjoin construct allows the StreamIt programmer \nto express explicitly parallel computations. Data elements that arrive at the splitjoin are directed \nto the parallel child streams using one of two pre-de.ned splitter constructs: 1) duplicate, which sends \na copy of each data item to all of the child streams, and 2) roundrobin, which distributes items cyclically \naccording to an array of weights. The data from the parallel streams are combined back into a single \nstream by means of a roundrobin joiner with an array of weights w. First, w0 items from the leftmost \nchild are placed onto the overall output tape, then w1 elements from the second leftmost child are used, \nand so on. The process repeats itself n-1 after Ii=0 wi elements has been pushed. In this section, we \ndemonstrate how to collapse a splitjoin into a single linear node when all of its children are linear \nnodes. Since the children of splitjoins in StreamIt can be parameterized, it is often the case that all \nsibling streams are linear if any one of them is linear. However, if a splitjoin contains only a few \nadjacent streams that are linear, then these streams can be combined by wrapping them in a hi\u00aderarchical \nsplitjoin and then collapsing the wrapper com\u00adpletely. Our technique also assumes that each splitjoin \nad\u00admits a valid steady-state schedule; this property is veri.ed by the StreamIt semantic checker. Our \nanalysis distinguishes between two cases. For dupli\u00adcate splitters, we directly construct a linear node \nfrom the child streams. For roundrobin splitters, we .rst convert to a duplicate splitter and then rely \non the transformation for duplicate splitters. We describe these translations below. 4.3.1 Duplicate \nSplitter Intuitively, there are three main steps to combining a duplicate splitjoin into a linear node. \nSince the combined node will represent a steady-state execution of the splitjoin construct, we .rst expand \neach child node according to its multiplicity in the schedule. Secondly, we ensure that each child s \nmatrix representation has the same number of rows that is, that each child peeks at the same number of \nitems. Once these conditions are satis.ed, we can construct a ma\u00adtrix representation for the splitjoin \nby simply arranging the columns from child streams in the order speci.ed by the joiner. Reordering columns \nis equivalent because with a du\u00adplicate splitter, each row of a child s linear representation refers \nto the same input element of the splitjoin. The trans\u00adformation is described in mathematical terms below. \nTransformation 3. (Duplicate splitjoin combination) Given a splitjoin s containing a duplicate splitter, \nchildren that are linear nodes A0 ...An-1 , and a roundrobin joiner with weights w0 ...wn-1 , the combination \nsplitjoin(s)= ''' {A ' ,bb ' , e , o , u } represents an equivalent node that can replace the entire \nstream s. Its components are as follows: lcm(u0 ,w0 ) lcm(un-1 ,wn-1 ) joinRep = lcm( w0 ,..., w) n-1 \nmaxPeek = maxi(oi . repi + ei - oi) 'k . [0,n - 1] : k-1 wSumk = Ii=0 wi wk *joinRep repk = uk Ae k = \nexpand(Ak, maxPeek,ok . repk,uk . repk) 'k . [0,n - 1], 'm . [0, joinRep - 1], 'p . [0,uk - 1] : A '' \ne [.,u - 1 - p - m . wSumn - wSumk]= Aek[.,u k - 1 - p] bb ' [u ' e - 1 - p - m . wSumn - wSumk]= bek[uk \n- 1 - p] ' ee e = e = \u00b7\u00b7\u00b7 = e 0 n-1 ' ee o = o = \u00b7\u00b7\u00b7 = o 0 n-1 u ' = joinRep . wSumn The above formulation \nis derived as follows. The joinRep variable represents how many cycles the joiner completes in an execution \nof the splitjoin s steady-state schedule; it is w1 w1 w1 w2w2 w2 ww w  the minimal number of cycles \nrequired for each child node to execute an integral number of times and for all of their output to be \nconsumed by the joiner. Similarly, repk gives the execution count for child k in the steady state. Then, \nin keeping with the procedure described above, Aek is the expansion of the kth node by a factor of repk \n, with the peek value set to the maximum peek across all of the expanded children. Following the expansion, \neach Aei has the same number of rows, as the peek uniformization causes shorter matrices to be padded \nwith rows of zeros at the top. The .nal phase of the transformation is to re-arrange the columns of the \nchild matrices into the columns of A ' and bsuch that they generate the correct order of outputs. b ' \n Though the equations are somewhat cumbersome, the con\u00adcept is simple (see Figure 11): for the kth child \nand the mth cycle of the joiner, the pth item that is pushed by child k will appear at a certain location \non the joiner s output tape. This location (relative to the start of the node s execution) is p + m . \nwSumn + wSumk, as the reader can verify. But since the right-most column of each array A holds the for\u00admula \nto compute the .rst item pushed, we need to subtract this location from the width of A when we are re-arranging \nthe columns. The width of A is the total number of items pushed u ' in the case of A ' and uke in the \ncase of Aek. Hence the equation as written above: we copy all items in a given column from Aek to A ' \n, de.ning each location in A ' exactly once. The procedure for bb is analogous. It remains to calculate \nthe peek, pop and push rates of the combined node. The peek rate e ' is simply maxP eek, which we de.ned \nto be equivalent for all the expanded child nodes. The push rate joinRep . wSumn is equivalent to the \nnumber of items processed through the joiner in one steady-state execution. Finally, for the pop rate \nwe rely on the fact that the splitjoin is well-formed and admits a schedule in which no bu.er grows without \nbound. If this is the case, then the pop rates must be equivalent for all the expanded streams; otherwise, \nsome outputs of the splitter would accumulate in.nitely on the input channel of some stream. These input \nand output rates, in combination with the values of A ' and b, de.ne a linear node that exactly rep\u00ad \nb ' resents the parallel combination of linear child nodes fed with a duplicate splitter. Figure 12 \nprovides an example of splitjoin combination. 4.3.2 Roundrobin Splitter In the case of a roundrobin \nsplitter, items are directed to each child stream si according to weight vi: the .rst v0 items are sent \nto s0, the next v1 items are sent to s1, and so on. Since a child never sees the items that are sent \nto sibling streams, the items that are seen by a given child form a periodic but non-contiguous segment \nof the splitjoin s input tape. Thus, in collapsing the splitjoin, we are unable to directly use the columns \nof child matrices as we did with a duplicate splitter, since with a roundrobin splitter these matrices \nare operating on disjoint sections of the input. Instead, we collapse linear splitjoins with a roundrobin \nsplitter by converting the splitjoin to use a duplicate splitter. In order to maintain correctness, we \nadd a decimator on each branch of the splitjoin that eliminates items which were intended for other streams. \nTransformation 4. (Roundrobin to duplicate) Given a splitjoin s containing a roundrobin splitter with \nweights v0 ... vn-1 , children that are linear nodes A0 ...An-1 , and a round\u00adrobin joiner j, the transformed \nrr-to-dup(s) is a splitjoin with a duplicate splitter, linear child nodes A ' 0 ...A ' n-1, and roundrobin \njoiner j. The child nodes are computed as fol\u00adlows: k-1 vSumk = Ii=0 vi vTot = vSumn 'k . [0,n - 1] : \ndecimate[k] is a linear node {A,b0, vTot, vTot,vk} 1 if i = vTot - vSumk+1 + jwhere A[i, j]= 0 otherwise \nA ' k = pipeline(decimate[k],Ak) In the above translation, we utilize the linear pipeline combinator \npipeline to construct each new child node Aei as a composition of a decimator and the original node Ai. \nThe kth decimator consists of a vTot \u00d7 vk matrix that con\u00adsumes vTot items, which is the number of items \nprocessed in one cycle of the roundrobin splitter. The vk items that are intended for stream k are copied \nwith a coe.cient of 1, while all others are eliminated with a coe.cient of 0.  4.4 Applications of \nLinear Combination There are numerous instances where the linear combina\u00adtion transformation could bene.t \na programmer. For exam\u00adple, although a bandpass .lter can be implemented with a low pass .lter followed \nby a high pass .lter, actual imple\u00admentations determine the coe.cients of a single combined .lter that \nperforms the same computation. While a simple bandpass .lter is easy to combine manually, in an actual \nsystem several di.erent .lters might be designed and imple\u00admented by several di.erent engineers, making \noverall .lter combination infeasible. Another common operation in discrete time signal pro\u00adcessing is \ndownsampling to reduce the computational re\u00adquirements of a system. Downsampling is most often im\u00adplemented \nas a low pass .lter followed by an M compres\u00adsor which passes every Mth input item to the output. In \npractice, the .lters are combined to avoid computing dead items in the low pass .lter. However, the system \nspeci.ca\u00adtion contains both elements for the sake of understanding. Our analysis can start with the speci.cation \nand derive the e.cient version automatically. A .nal example is a multi-band equalizer, in which N dif\u00adferent \nfrequency bands are .ltered in parallel (see our FM-Radio benchmark). If these .lters are time invariant, \nthen they can be collapsed into a single node. However, design\u00ading this single overall .lter is di.cult, \nand any subsequent changes to any one of the sub .lters will necessitate a total redesign of the .lter. \nWith our automated combination pro\u00adcess, any subsequent design changes will necessitate only a recompile \nrather than a manual redesign.   5. TRANSLATION TO FREQUENCY In this section, we demonstrate how we \ncan leverage our linear representation to automatically perform a common domain-speci.c optimization: \ntranslation to the frequency domain. First, we show that a linear node is equivalent to a set of convolution \nsums, which can bene.t from algo\u00adrithmic gains if performed in frequency rather than time. We then present \nan optimized code generation strategy for transforming linear nodes to frequency. 5.1 Basic Frequency \nImplementation Our .rst goal is to show that the computation of a linear node can be represented as a \nconvolution sum. Consider executing m iterations of a linear node A = {A,b0, e, 1, 1} that is, a node \nwith bb = b0 and push = pop = 1 (these bith value that is pushed during execution, let bassumptions will \nbe relaxed below). Let out[i] represent the in[i] hold the value of peek(i) as seen before the execution \nbegins, and let by be the convolution of the only column of A with the byA[., 0] . bb vector in (that \nis, b= in). Note that out is an b m-element vector, A[., 0] is an e-element vector, in is an (m + e - \n1)-element vector, and by is an (m +2e - 2)-element vector. Then, we make the following claim: 'i . [0,m \n- 1] : by [i + e - 1] out[i]= b(1) To see that this is true, recall the de.nition of convolution: o bin[i]= \nin[i - k]y [i]= A[i, 0] . bA[k, 0] b k=-o Substituting b in by its de.nition, and restricting k to range \nover the valid rows of A, we have: e-1 bA[k, 0]peek(i - k) y [i]= k=0 Remapping the index i to i + e \n- 1 makes the right hand side equivalent to out[i], by De.nition 1. Claim 1 follows. b In other words, \nvalues pushed by a linear node can be calculated by a convolution of the input tape with the co\u00ade.cients \nA. The signi.cance of this fact is that a convolu\u00adtion operation can be implemented very e.ciently by \nusing the Fast Fourier Transform (FFT) to translate into the fre\u00adquency domain. To compute the convolution, \nthe N-point bbb FFTs of in and A[., 0] are calculated to obtain X and H, respectively, each of which \nis a complex-valued vector of length N. Element-wise multiplication of Xband Hbyields a vector Yb, to \nwhich the inverse transform (IFFT) is applied to obtain by. Convolution in the frequency domain requires \nO(N lg(N)) operations, as each FFT and IFFT has a cost of O(N lg(N)) and the vector multiplication is \nO(N). By contrast, the complexity is O(N 2 ) in the time domain, as each of the N output values requires \nO(N) operations. For more details, refer to [15]. We can use the procedure described above to implement \na linear node in the frequency domain. We simply calculate bin, and extract values by [m+(e-1)-1] y = \nA[., 0].by [e-1] ...bas the m values pushed by the node. Note that by [i] is also de.ned for i . [0,e \n- 2] and i . [m + e - 1,m +2e - 2]; these values represent partial sums in which some coe.cients were \nexcluded. Our na\u00a8ive implementation simply disregards these values. However, in the next section, we \ngive an optimized implementation that takes advantage of them. The only task remaining for the implementation \nis to choose N, the FFT size, and m, the number of iterations to execute at once in the frequency domain. \nAccording to Fourier s theorem, an N-point FFT can exactly represent any discrete sequence of N numbers, \nso the only constraint on N and m is that N . m +2e - 1. For performance rea\u00adsons, N should be a power \nof two and as large as possible. In our implementation, we set N to the .rst power of two that is greater \nthan or equal to 2e, and then set m = N - 2e + 1. This strikes a reasonable compromise between storage \nspace and performance for our uniprocessor benchmarking plat\u00adform; the choice of N should be adjusted \nfor the particular resource constraints of the target architecture. The transformation below gives a \nna\u00a8ive translation of a linear node to the frequency domain. In addition, it relaxes all of the assumptions \nthat we made above. The algorithm allows for a non-zero value of bb by simply adding bb after returning \nfrom the frequency domain. To accommodate a push rate greater than one, the algorithm generates matri\u00ad \n b ces for Y and yband alternates pushing values from each column of by in turn. Finally, to accommodate \na pop rate greater than one, the algorithm proceeds as if the pop rate was one and adds a special decimator \nnode that discards the extra outputs. Though this introduces ine.ciency by calcu\u00adlating values that are \nnever used, it still leaves room for large performance improvements, as the frequency transformation \ncan improve performance by a large factor (see Section 7). Transformation 5. (Na\u00a8ive frequency implementation) \nGiven a linear node A = {A,bb, e, o, u}, the following stream is a na\u00a8ive implementation of A in the \nfrequency domain: float -float pipeline naiveFreq (A,Hb,e,o,u) { add float -float filter { N + 2*lg(2e)8 \nm + N - 2e +1 init { for j =0 to u - 1 H [*,j] + FFT(N, A[*,u - 1 - j]) } work peek m + e - 1 pop m \npush u * m { H x + peek(0 ...m + e - 2) Hx) X + FFT(N, H for j =0 to u - 1 { YH[*,j] + H X. * H [*,j] \nHy [*,j] + IFFT(N, YH[*,j]) } for i =0 to m - 1 { pop() for j =0 to u - 1 push(Hy [i + e - 1,j]+ Hb[j]) \n} } } add FreqDecimator(o, u) } float -float filter freqDecimator (o, u) { work peek u * o pop u * o \npush u { for i =0 to u - 1 push(pop()) for i =0 to u - 1 for j =0 to o - 2 pop() } }  5.2 Optimized \nFrequency Implementation The na\u00a8ive frequency implementation discards e - 1 ele\u00adments from the beginning \nand end of each column of by that it computes. These values represent partial sums in which some of the \ncoe.cients of A are excluded. However, for i . [0,e - 2], by [i, j] in one iteration contains the missing \nterms from by [m + e - 1+ i, j] in the previous iteration. The sum of these two elements gives a valid \noutput for the .l\u00adter. This symmetry arises from the convolution of A o. the edges of the input block \nthat we consider in a given iteration. Reusing the partial sums which is exploited in the transformation \nbelow is one of several methods that use blocking to e.ciently convolve a short .lter with a large amount \nof input [15]. Transformation 6. (Optimized frequency implementa\u00adtion) Given a linear node A = {A,bb, \ne, o, u}, the following stream is an optimized implementation of A in the frequency domain: float -float \npipeline optimizedFreq (A,Hb,e,o, u) { add float -float filter { N + 2*lg(2e)8 m + N - 2e +1 H partials \n+ new array[0 ...e - 2, 0 ...u - 1] r + m + e - 1 init { for j =0 to u - 1 H H[*,j] + FFT(N, A[*,u \n- 1 - j]) } prework peek r pop r push u * m { Hx + pop(0 ...m + e - 2) H X + FFT(N, Hx) for j =0 to \nu - 1 { HH Y [*,j] + X. * HH[*,j] HY [*,j]) y[*,j] + IFFT(N, H H partials[*,j] + Hy [m + e - 1 ...m \n+2e - 3,j] } for i =0 to m - 1 for j =0 to u - 1 push(Hy [i + e - 1,j]+ Hb[j]) } work peek r pop r push \nu * r { Hx + pop(0 ...m + e - 2) H X + FFT(N, Hx) for j =0 to u - 1 { HH Y [*,j] + X. * HH[*,j] HY \n[*,j]) y[*,j] + IFFT(N, H } for i =0 to e - 1 for j =0 to u - 1 { push(Hy [i, j]+ partials[i, j]) H \nH partials[i, j] + Hy [m + e - 1+ i, j] } for i =0 to m - 1 for j =0 to u - 1 push(Hy [i + e - 1,j]+ \nHb[j]) } } add Decimator(o, u) // see Transformation 5 }  5.3 Applications of Frequency Transformation \nThe transformation to the frequency domain is straight\u00adforward in theory and very common in practice. \nHowever, the detailed record keeping, transform size selection, and state management make an actual implementation \nquite in\u00advolved. Further, as the complexity of DSP programs con\u00adtinues to grow, manually determining \nthe disparate regions across which to apply this optimization is an ever more daunting task. For example, \nseveral .lters individually may not perform su.ciently large convolutions to merit a fre\u00adquency transformation, \nbut after a linear combination of multiple .lters the transformation could be bene.cial. Dif\u00adfering levels \nof architectural support for various convolu\u00adtion and frequency operations further complicates the task \nof choosing the best transform. Our compiler automatically determines all the necessary information and \ntransforms the computation into the frequency domain.  6. OPTIMIZATION SELECTION To reap the maximum \nbene.t from the optimizations de\u00adscribed in the previous two sections, it is important to apply them \nselectively. There are two components of the optimiza\u00adtion selection problem: .rst, to determine the \nsequence of optimizations that will give the highest performance for a given arrangement of the stream \ngraph, and second, to de\u00adtermine the arrangement of the stream graph that will give the highest performance \noverall. In this section, we explain the relevance of each of these problems, and we present an e.ective \nselection algorithm that relies on dynamic program\u00adming to quickly explore a large space of con.gurations. \n6.1 The Selection Problem First, the selection of optimizations for a given stream graph can have a large \nimpact on performance. As alluded to in Section 4, linear combination can increase the num\u00adber of arithmetic \noperations required, e.g., if combining a two-element pipeline where the second .lter pushes more items \nthan it peeks. However, such a combination might be justi.ed if it enables further combination with other \ncompo\u00adnents and leads to a bene.t overall. Another consideration is that as the pop rate grows, the bene.t \nof converting to frequency diminishes; thus, it might be preferable to trans\u00adform smaller sections of \nthe graph to frequency, or to per\u00adform linear combination only. This occurs in our Vocoder and FMRadio \nbenchmarks, in which the selection algorithm improves performance by choosing plain linear combination \nover frequency translation. Second, the arrangement of the stream graph can dic\u00adtate which transformations \nare possible to apply. Since our transformations operate on an entire pipeline or splitjoin construct, \nthe graph often needs to be refactored to put lin\u00adear nodes in their own hierarchical unit. For example, \nin our TargetDetect benchmark, we cut a splitjoin horizontally and collapse the top piece before converting \nto the frequency do\u00admain, thereby amortizing the cost of the FFT on the input items. In our Vocoder benchmark, \nwe cut a splitjoin ver\u00adtically in order to separate the non-linear .lters on the left and combine all \nof the .lters on the right into a single linear node. 6.2 Dynamic Programming Solution Our optimization \nselection algorithm, shown in Figures 13 and 14, automatically derives the example transformations described \nabove. Intuitively, the algorithm works by esti\u00admating the minimum cost for each structure in the stream \ngraph. The minimum cost represents the best of three con\u00ad.gurations: 1) collapsed and implemented in \nthe time do\u00admain, 2) collapsed and implemented in the frequency do\u00admain, and 3) uncollapsed and implemented \nas a hierarchical unit. The cost functions for the collapsed cases are guided by pro.ler feedback, as \ndescribed below. For the uncollapsed case, the cost is the sum of each child s minimum cost. How\u00adever, \ninstead of considering the children directly, the children are refactored into many di.erent con.gurations, \nand the cost is taken as the minimum over all con.gurations. This allows the algorithm to simultaneously \nsolve for the best set of transformations and the best arrangement of the stream graph. The key to the \nalgorithm s e.ciency is the manner in which it considers refactoring the children of hierarchical nodes. \nInstead of considering arbitrary arrangements of the // types of transformations we consider for each \nstream enum Transform { ANY, LINEAR, FREQ, NONE } // a tuple representing a cost and a stream struct \nConfig { int cost : cost of the configuration Stream str : Stream corresponding to the lowest cost } \n // a hierarchical stream element struct Stream { int height : number of rows in the container int width[y] \n: number of columns in row y int child[x, y] : child in position (x, y) [column x, row y] } Figure 13: \nType declarations for code in Figure 14. stream graph, it considers only rectangular partitions, in which \na given splitjoin is divided into two child splitjoins by either a horizontal or vertical cut. This approach \nworks well in practice, because splitjoins often have symmetrical child streams in which linear components \ncan be separated by a straight line. Moreover, as the child splitjoins are decom\u00adposed and evaluated, \nthere are overlapping sub-problems that enable us to search the space of child con.gurations in polynomial \ntime, using dynamic programming. A subtlety in the pseudocode of Figure 14 is with regards to the translation \nfrom a StreamIt graph to a set of hi\u00aderarchical Stream objects. In the pseudocode, each Stream corresponds \nonly to a pipeline; adjacent splitjoin objects are wrapped in a pipeline and their children are considered \nas direct children of the pipeline. This enables di.erent parts of neighboring splitjoins to be combined. \nHowever, it im\u00adplies that a Stream might have a di.erent width at di.er\u00adent points (since neighboring \nsplitjoins could have di.ering widths); this necessitates the addition of the width array to the algorithm. \n 6.3 Cost Functions The pseudocode in Figure 14 refers to functions getDirect-Cost and getFrequencyCost \nthat estimate a node s execution time if implemented in the time domain or the frequency domain. These \ncost functions can be tailored to a speci.c architecture and code generation strategy. For example, if \nthere is architecture-level support for convolution operations (such as the the FIRS instruction in the \nTMS320C54x [18]), then this would e.ect the cost for certain dimensions of matrices; similarly, if a \nmatrix multiplication algorithm is available that exploits symmetry or sparsity in a matrix, then this \nbene.t could be accounted for where it applies. In our implementation, we use the following versions \nof the cost functions (let A =(A,bb, e, o, u) be the linear node cor\u00adresponding to stream s): * (if s \nis roundrobin splitjoin) 185 + 2 . u + (otherwise) getDirectCost(s)= ! !! |{i s.t. bbi =08}| + 3 . |{(i, \nj) s.t. Ai,j 8 =0}| getFrequencyCost(s)= 1+4 . e 185 + 2 . u + u . ln .max(o, 1)+dec(s) 1+ 2rlg(2*e)* \n 50 ) ] dec(s)=(o - 1) . (185 + 4 . u) // global variable holding the lowest-cost Config for nodes // \n(x1..x2, y1..y2) of Stream <s> if Transform <t> is applied Config memoTable[s, t, x1, x2, y1, y2] // \ngiven original Stream <s>, return optimized stream Stream toplevel (Stream s) initialize all entries \nof memoTable to Config(-1, null) return getCost(s, ANY).str // returns lowest-cost Config for Stream \n<s> under Transform <t> Config getCost (Stream s, Transform t) if (t = ANY) c1 getCost(s, LINEAR) c2 \ngetCost(s, FREQ) c3 getCost(s, NONE) return ci s.t. ci.cost = min(c1.cost, c2.cost, c3.cost) else if \n(s is Node) return getNodeCost(s, t) else // s is Container maxWidth max(s.width[0], ..., s.width[s.height-1]) \nreturn getContainerCost(s, t, 0, maxWidth-1, 0, s.height-1) // returns lowest-cost Config for Node <s> \nunder Transform <t> Config getNodeCost (Stream s, Transform t) // scale cost by the number of times <s> \nexecutes in the steady-state schedule scalingFactor executionsPerSteadyState(s) if (t = LINEAR) if (isLinear(s)) \nreturn Config(scalingFactor getDirectCost(s), makeLinearImplementation(s)) else return Config( , s) else \nif (t = FREQ) if (isLinear(s) canConvertToFrequency(s)) return Config(scalingFactor getFrequencyCost(s), \nmakeFreqImplementation(s)) else return Config( , s) else // t = NONE if (isLinear(s)) return Config(scalingFactor \ngetDirectCost(s), s) else return Config(0, s) // don t tally up costs of non-linear nodes // returns \nlowest-cost Config for children (x1..x2, y1..y2) of <s> under <t> Config getContainerCost (Stream s, \nTransform t, int x1, int x2, int y1, int y2) // if we've exceeded the width of this node, then trim down \nto actual width x2 min (x2, max (width[y1], ..., width[y2]) -1) // if value is memoized, return it if \n(memoTable[s, t, x1, x2, y1, y2] -1) return memoTable[s, t, x1, x2, y1, y2] if (x1 =x2 y1= y2) // if \ndown to one child, descend into it result getCost(s.child[x1, y1], t) // if the transform will collapse \nchildren, then treat them as a single node if (t = LINEAR t = FREQ) result getNodeCost(extractSubstream(s, \nx1, x2, y1, y2), t) if (t = NONE) result = Cost ( , s) // try horizontal cut for yPivot y1 to y2-1 do \n// get cost of 2-element Pipeline; remember Config if it is best so far c1 getCost(s, ANY, x1, x2, y1, \nyPivot) c2 getCost(s, ANY, x1, x2, yPivot+1, y2) if (c1.cost + c2.cost < result.cost) result Config(c1.cost+c2.cost, \nPipeline(c1.str, c2.str)) // can only do vertical cut if all child streams belong to same splitjoin \nif (sameSplitJoinParent(s.child[x1, y1], s.child[x2, y2])) for xPivot = x1 to x2-1 do // get cost of \n2-element SplitJoin; remember Config if it is best so far c1 getCost(s, ANY, x1, xPivot, y1, y2) c2 getCost(s, \nANY, xPivot+1, x2, y1, y2) if (c1.cost + c2.cost < result.cost) result Config(c1.cost+c2.cost, SplitJoin(c1.str,c2.str)) \n memoTable[s, t, x1, x2, y1, y2] result return result Figure 14: Algorithm for optimization selection. \n Originally After Optimizations Benchmark Filters Pipelines SplitJoins Average Filters Pipelines SplitJoins \n(linear) (linear) (linear) vector size  FIR 3 (1) 1(0) 0 (0) 256 3 1 0 RateConvert 5 (3) 2 (1) 0 (0) \n102 4 1 0 TargetDetect 10 (4) 6 (0) 1 (0) 300 7 1 1 FMRadio 26 (22) 11 (9) 2 (2) 40 5 1 0 Radar 76 (60) \n17 (0) 2 (0) 4412 38 17 2 FilterBank 27 (24) 17 (9) 4 (4) 52 3 1 0 Vocoder 17 (13) 10 (8) 2 (1) 60 6 \n2 1 Oversampler 10 (8) 1 (1) 0 (0) 33 3 1 0 DToA 14 (10) 3 (1) 0 (0) 52 7 2 0 Table 1: Characteristics \nof benchmarks before and after running optimizations with automatic selection. For the direct cost, we \nconsider the cost to be in.nite for splitjoins with roundrobin splitters. This is because the splitjoin \ncombination does not eliminate any arithmetic op\u00aderations, and for the roundrobin case it introduces \nextra overhead (the duplicate case is di.erent because the input items are shared between streams). For \nother streams, we count the number of multiplies and adds required to per\u00adform the matrix multiplication, \ngiving more weight to the multiplies since they are more expensive. We do not count the zero entries \nof the arrays, since our matrix multiply rou\u00adtines take advantage of the sparsity of the matrix. We add \n185 +2 . u to all costs as a measure of the overhead of the node s execution. For the frequency cost, \nour cost curve is guided by pro\u00ad.ler feedback. The speedup gained by translating to the frequency domain \ndepends on the peek rate of the .lter. To obtain an n-fold speedup of the frequency code over the direct \ncode, the .lter has to peek 64 . n items. Mathemati\u00adcally, this translates to a logarithmic expression \nfor the cost of the frequency node in terms of the number of items e (we actually use the next power \nof two above e, as that is the size of the FFT). We multiply the above cost by the number of items pushed, \nadd the constant o.set of 185 + 2 . u for a node, and then multiply by max(o, 1) because only one out \nof o items represents a valid output from the frequency domain. Finally, we add dec(s), the cost of the \ndecimator for the frequency node. We estimate an extra cost of 2 per push operation in the decimator, \nas it must read from the input tape. The other pop operations in the decimator are free, because they \ncan be performed as a single adjustment of the tape position. Of course, both cost functions are unde.ned \nif s is non\u00adlinear (i.e., if there is no corresponding As). If this is the case, then the selection algorithm \nassigns an in.nite cost.  7. RESULTS We have completed a fully automatic implementation of the linear \ncombination, frequency replacement, and opti\u00admization selection algorithms described in the previous \nsec\u00adtions. The implementation is part of the StreamIt compiler, and works for both the uniprocessor and \nRaw [13] backends. In this section, we evaluate three con.gurations of linear op\u00adtimizations for the \nuniprocessor backend: Linear replacement, which transforms maximal linear sections of the stream graph \ninto a single linear node, which we implement as a matrix multiply. For small nodes (less than 256 operations), \nthis takes the form of an unrolled arithmetic expression, whereas for large nodes we implement an indexed \nmatrix multiply that avoids zero entries at the top and bottom of each col\u00adumn. Frequency replacement, \nwhich transforms maximal lin\u00adear sections of the stream graph into a single node in the frequency domain. \nTo implement the necessary ba\u00adsis conversions, we use FFTW [5], which is an adaptive and high-performance \nFFT library.  Automatic selection, which employs both of the previ\u00adous transformations judiciously in \norder to obtain the maximal bene.t. This works according to the algo\u00adrithm in Section 6.  Below we \ndescribe experiments and results that demonstrate substantial performance improvements due to our methods. \nFor full results, stream graphs, and source code, please visit http://cag.lcs.mit.edu/linear/. 7.1 Measurement \nMethodology Our measurement platform is a Dual Intel P4 Xeon sys\u00adtem with 2GB of memory running GNU/Linux. \nWe compile our benchmarks using StreamIt s uniprocessor backend and generate executables from the resulting \nC .les using gcc -O2. To measure the number of .oating point operations, we use an instruction counting \nDynamoRIO[1] client. Since StreamIt is a new language, there are no external sources of benchmarks. Thus, \nwe have assembled the fol\u00adlowing set of representative streaming components and have rewritten them in \nStreamIt: 1) FIR, a single 256 point low pass FIR .lter; 2) RateConvert, an audio down sam\u00adpler that \nconverts the sampling rate by a non-integral factor ( 23 ); 3) TargetDetect, four matched .lters in parallel \nwith threshold target detection; 4) FMRadio, an FM software radio with equalizer; 5) Radar, the core \nfunctionality in modern radar signal processors, based on a system from the Polymorphic Computing Architecture \n[12]; 6) FilterBank, a multi-rate signal decomposition processing block common in communications and \nimage processing; 7) Vocoder,a channel voice coder, commonly used for speech analysis and compression; \n8) Oversampler,a 16x oversampler, a func\u00adtion found in many CD players, 9) DToA, an audio post\u00adprocessing \nstage prior to a 1-bit D/A converter with an over\u00adsampler and a .rst order noise shaper. Some statistics \non our benchmarks appear in Table 1.  7.2 Performance One interesting aspect of our optimizations is \nthat they eliminate .oating point operations (FLOPS) from the pro\u00adgram, as shown in Figure 15. The removal \nof FLOPS rep\u00adresents fundamental computation savings that is indepen\u00ad  Benchmark dent of the streaming \nruntime system and other (FLOPS\u00adpreserving) optimizations in the compiler. As shown in the .gure, each \noptimization leads to a signi.cant reduction of FLOPS. Linear replacement eliminates FLOPS for all except \nfor FIR, TargetDetect, and Radar, while frequency replace\u00adment eliminates FLOPS for all except for Radar. \nThe automatic selection option eliminates more FLOPS than either of the other options for TargetDetect, \nFMRadio, Radar, and Vocoder. The e.ect is especially pronounced in Radar, where linear and frequency \nreplacement increase the number of FLOPS, but automatic selection decreases FLOPS; the selection algorithm \nchooses to combine only some of the .lters in Radar, transforming none to the fre\u00adquency domain. Automatic \nselection always performs at least as well as the other two options, which implies that our cost functions \nhave some level of accuracy. Execution speedups are shown in Figure 16. With au\u00adtomatic selection, our \nbenchmarks speed up by an average factor of 450% and by a factor of 800% in the best case. While the \ngraph suggests that frequency replacement al\u00admost always performs better than linear replacement, this \nis not strictly the case; in FMRadio, Radar, and Vocoder, the automatic selection algorithm obtains its \nspeedup by using linear replacement instead of frequency replacement for part of the stream graph. However, \nlinear replacement does re\u00adduce performance for FIR, TargetDetect, and DToA despite reducing the number \nof FLOPS. We believe that this is due to ine.ciencies in our implementation of the matrix multi\u00adplication \nroutine, as well as auxiliary e.ects on the runtime overhead in the StreamIt library. We have experimented \nwith using the machine-tuned ATLAS library for the ma\u00adtrix multiply [20], but performance varies widely: \nlinear re\u00adplacement with ATLAS performs anywhere from -36% (on FMRadio) to 58% (on Oversampler) better \nthan it does with our own matrix multiply routine, and average performance with ATLAS is 4.3% lower. \nNote that these numbers re\u00ad.ect our overhead in interfacing with ATLAS rather than the performance of \nATLAS itself. In the future, we plan to further investigate how best to perform the matrix multiply. \nPerhaps the most interesting benchmark is Radar2 . Max- This is the same Radar application as in [7], \nwith some .lters adjusted to work at a coarser level of granularity. This elimi\u00adnates persistent state \nin exchange for increased I/O rates. Also, 900%  -200% Benchmark imal linear and frequency replacement \nlead to abysmal per\u00adformance on Radar, due to a vector-vector multiply .lter named Beamform at the top \nof a pipeline construct. The Beamform .lter pushes 2 items, but pops and peeks 24; thus, when the replacement \nalgorithms combine it with a down\u00adstream FIR .lter, much of its work is duplicated. Moreover, the frequency \nreplacement option su.ers from the large pop rates in the application (as high as 128 for some .lters), \nthereby increasing FLOPS and execution time by more than a factor of 30. The automatic selection algorithm \nis essential in this case: it averts the performance-degrading combina\u00adtions and bene.ts from linear \ncombinations elsewhere in the program, resulting in a signi.cant reduction in FLOPS and a 5% performance \ngain.  8. RELATED WORK Several groups are researching strategies for e.cient code generation for DSP \napplications. SPIRAL is a system that generates libraries for signal processing algorithms[8, 9, 4]. \nUsing a feedback-directed search process, DSP transforms are optimized for the underlying architecture. \nThe input language to SPIRAL is SPL[22, 21], which provides a param\u00adeterizable way of expressing matrix \ncomputations. Given a matrix representation in SPL, SPIRAL generates formulas that correspond to di.erent \nfactorizations of the matrix. It searches for the most e.cient formula using several tech\u00adniques, including \ndynamic programming and stochastic evo\u00adlutionary search. We consider our work to be complementary to \nSPIRAL. While SPIRAL starts with a matrix representation in SPL, we start with general StreamIt code \nand use linear data.ow analysis to extract a matrix representation where possible. Our linear combination \nrules are distinct from the factor\u00adizations of SPIRAL, as StreamIt nodes can peek at items that they \ndo not consume. In the future, SPIRAL could be integrated with StreamIt to optimize a matrix factorization \nfor a given architecture. The ATLAS project [20] also aims to produce fast libraries for linear algebra \nmanipulations, focusing on adaptive li\u00adbrary generation for varying architectures. FFTW [5] is a runtime \nlibrary of highly optimized FFT s that dynam\u00ad frequency replacement caused an internal error in gcc for \nthis program, so we used egcs 2.91 instead. ically adapt to architectural variations. StreamIt is again \ncomplementary to these packages: it allows programmers to interface with them using general user-level \ncode. ADE (A Design Environment) is a system for specify\u00ading, analyzing, and manipulating DSP algorithms \n[3]. ADE includes a rule-based system that can search for improved arrangements of stream algorithms \nusing extensible trans\u00adformation rules. However, the system uses prede.ned signal processing blocks that \nare speci.ed in mathematical terms, rather than the user-speci.ed imperative code that appears in a StreamIt \n.lter. Moreover, ADE is intended for algo\u00adrithm exploration, while StreamIt includes support for code \ngeneration and whole-program development. In addition to ADE, other work on DSP algorithm development \nis surveyed in [14]. Karr [11] and Cousot and Halbwachs [2] describe general methods for detecting linear \nrelationships among program variables. Karr maintains an a.ne representation (simi\u00adlar to ours) for each \nprogram variable, while Cousot and Halbwachs use a polyhedral model in which each dimen\u00adsion corresponds \nto a program variable. For general pro\u00adgrams, the analyses described by these authors is more gen\u00aderal \nthan ours. In fact, the novelty of our linear data.ow analysis is in its specialization for the streaming \ndomain. Rather than tracking general relationships, we only track relationships to items on the input \ntape. This restriction in combination with the atomic, .ne-grained nature of .lter work functions makes \nit feasible to symbolically execute all loops, thereby obtaining more precise linearity information. \nA number of other programming languages are oriented around a notion of a stream; see [17] for a survey. \nAlso note that the linear data .ow analysis of Ryan [16] is completely unrelated to our work; it aims \nto do program analysis in linear time. 9. CONCLUSION This paper presents a set of automated analyses \nfor de\u00adtecting, analyzing, and optimizing linear .lters in streaming applications. Though the mathematical \noptimization of lin\u00adear .lters has been a longtime focus of the DSP community, our techniques are novel \nin the automated application of these techniques to programs that are written in a .exible and high-level \nprogramming language. We demonstrate that using our linear data.ow analysis, linear combination, fre\u00adquency \ntranslation and automated optimization selection we can improve execution speed by an average factor \nof 450%. The ominous rift between the design and implementation of signal processing applications is \ngrowing by the day. Algo\u00adrithms are designed at a conceptual level utilizing modular processing blocks \nthat naturally express the computation. However, in order to obtain good performance, each hand\u00adtuned \nimplementation is forced to disregard the abstraction layers and painstakingly consider specialized whole-program \noptimizations. The StreamIt project aims to reduce this process to a single stage in which the designers \nand imple\u00admentors share a set of high-level abstractions that can be e.ciently handled by the compiler. \nThe linear analysis described in this paper represents a .rst step toward this goal. By automatically \nperforming linear combination, frequency translation, and optimization selection, it allows programmers \nto write linear stream op\u00aderations in a natural and modular fashion without any per\u00adformance penalty. \n 10. ACKNOWLEDGEMENTS We are very grateful to David Maze, Michal Karczmarek, Jasper Lin, and Michael \nGordon for extensive support with the StreamIt infrastructure, and to Alex Salcianu for his helpful comments. \nThe StreamIt project is supported by DARPA, NSF, and the MIT Oxygen Alliance. 11. REFERENCES [1] V. Bala, \nE. Duesterwald, and S. Banerjia. Dynamo: A transparent dynamic optimization system. In PLDI, 1999. [2] \nP. Cousot and N. Halbwachs. Automatic discovery of linear restraints among variables of a program. In \nPOPL, 1978. [3] M. M. Covell. An Algorithm Design Environment for Signal Processing. PhD thesis, MIT, \n1989. [4] S. Egner, J. Johnson, D. Padua, M. P\u00a8uschel, and J. Xiong. Automatic derivation and implementation \nof signal processing algorithms. SIGSAM Bulletin, 35(2):1 19, 2001. [5] M. Frigo. A Fast Fourier Transform \nCompiler. In PLDI, 1999. [6] M. Gordon. A stream-aware compiler for communication-exposed architectures. \nMaster s thesis, MIT Laboratory for Computer Science, August 2002. [7] M. Gordon, W. Thies, M. Karczmarek, \nJ. Lin, A. S. Meli, A. A. Lamb, C. Leger, J. Wong, H. Ho.mann, D. Maze, and S. Amarasinghe. A Stream \nCompiler for Communication-Exposed Architectures. ASPLOS, 2002. [8] J. Johnson, R. W. Johnson, D. A. \nPadua, and J. Xiong. SPIRAL Home Page. http://www.ece.cmu.edu/~spiral/. [9] J. Johnson, R. W. Johnson, \nD. A. Padua, and J. Xiong. Searching for the best FFT formulas with the SPL compiler. LNCS, 2017, 2001. \n [10] M. A. Karczmarek. Constrained and phased scheduling of synchronous data .ow graphs for the streamit \nlanguage. Master s thesis, MIT LCS, October 2002. [11] M. Karr. A.ne relationships among variables of \na program. Acta Informatica, pages 133 155, 1976. [12] J. Lebak. Polymorphous Computing Architecture \n(PCA) Example Applications and Description. External Report, Lincoln Laboratory, Mass. Inst. of Technology, \n2001. [13] M.B. Taylor et. al . The raw microprocessor: a computational fabric for software circuits \nand general-purpose programs. IEEE Micro, 22(2):25 35, March/April 2002. [14] A. V. Oppenheim and S. \nH. Nawab, editors. Symbolic and Knowledge-Based Signal Processing. Prentice Hall, 1992. [15] A. V. Oppenheim, \nR. W. Shafer, and J. R. Buck. Discrete-Time Signal Processing. Prentice Hall, second edition, 1999. [16] \nS. Ryan. Linear data .ow analysis. ACM SIGPLAN Notices, 27(4):59 67, 1992. [17] R. Stephens. A Survey \nof Stream Processing. Acta Informatica, 34(7), 1997. [18] Texas Instruments. TMS320C54x DSP Reference \nSet, volume 2: Mnemonic Instruction Set. 2001. [19] W. Thies, M. Karczmarek, and S. Amarasinghe. StreamIt: \nA Language for Streaming Applications. In Proc. of the Int. Conf. on Compiler Construction (CC), 2002. \n[20] R. C. Whaley, A. Petitet, and J. J. Dongarra. Automated empirical optimizations of software and \nthe ATLAS project. Parallel Computing, 27(1 2):3 35, 2001. [21] J. Xiong. Automatic Optimization of DSP \nAlgorithms. PhD thesis, Univ. of Illinois at Urbana-Champaign, 2001. [22] J. Xiong, J. Johnson, R. W. \nJohnson, and D. A. Padua. SPL: A language and compiler for DSP algorithms. In PLDI, 2001.  \n\t\t\t", "proc_id": "781131", "abstract": "As more complex DSP algorithms are realized in practice, there is an increasing need for high-level stream abstractions that can be compiled without sacrificing efficiency. Toward this end, we present a set of aggressive optimizations that target linear sections of a stream program. Our input language is StreamIt, which represents programs as a hierarchical graph of autonomous filters. A filter is linear if each of its outputs can be represented as an affine combination of its inputs. Linearity is common in DSP components; examples include FIR filters, expanders, compressors, FFTs and DCTs.We demonstrate that several algorithmic transformations, traditionally hand-tuned by DSP experts, can be completely automated by the compiler. First, we present a linear extraction analysis that automatically detects linear filters from the C-like code in their work function. Then, we give a procedure for combining adjacent linear filters into a single filter, as well as for translating a linear filter to operate in the frequency domain. We also present an optimization selection algorithm, which finds the sequence of combination and frequency transformations that will give the maximal benefit.We have completed a fully-automatic implementation of the above techniques as part of the StreamIt compiler, and we demonstrate a 450% performance improvement over our benchmark suite.", "authors": [{"name": "Andrew A. Lamb", "author_profile_id": "81100353841", "affiliation": "Massachusetts Institute of Technology", "person_id": "P414677", "email_address": "", "orcid_id": ""}, {"name": "William Thies", "author_profile_id": "81100276340", "affiliation": "Massachusetts Institute of Technology", "person_id": "P335208", "email_address": "", "orcid_id": ""}, {"name": "Saman Amarasinghe", "author_profile_id": "81100533031", "affiliation": "Massachusetts Institute of Technology", "person_id": "PP14184970", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/781131.781134", "year": "2003", "article_id": "781134", "conference": "PLDI", "title": "Linear analysis and optimization of stream programs", "url": "http://dl.acm.org/citation.cfm?id=781134"}