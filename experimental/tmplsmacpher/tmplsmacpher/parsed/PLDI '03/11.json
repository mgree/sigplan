{"article_publication_date": "05-09-2003", "fulltext": "\n Bug Isolation via Remote Program Sampling * Ben Liblit Alex Aiken <liblit@cs.berkeley.edu> <aiken@cs.berkeley.edu> \n Alice X. Zheng Michael I. Jordan , < alicez@cs.berkeley.edu> < jordan@cs.berkeley.edu> Department of \nElectrical Engineering and Computer Science Department of Statistics University of California, Berkeley \nBerkeley, CA 94720-1776  ABSTRACT We propose a low-overhead sampling infrastructure for gath\u00adering information \nfrom the executions experienced by a pro\u00adgram s user community. Several example applications illus\u00adtrate \nways to use sampled instrumentation to isolate bugs. Assertion-dense code can be transformed to share \nthe cost of assertions among many users. Lacking assertions, broad guesses can be made about predicates \nthat predict program errors and a process of elimination used to whittle these down to the true bug. \nFinally, even for non-deterministic bugs such as memory corruption, statistical modeling based on logistic \nregression allows us to identify program behaviors that are strongly correlated with failure and are \ntherefore likely places to look for the error. Categories and Subject Descriptors D.2.5 [Software Engineering]: \nTesting and Debugging distributed debugging;G.3 [Mathematics of Computing]: Probability and Statistics \ncorrelation and regression anal\u00adysis; I.5.2 [Pattern Recognition]: Design Methodology feature evaluation \nand selection General Terms Experimentation, Performance, Reliability * This research was supported \nin part by NASA Grant No. NAG2-1210; NSFGrant Nos. EIA-9802069, CCR\u00ad0085949, ACI-9619020, and IIS-9988642; \nDOE Prime Con\u00adtract No. W-7405-ENG-48 through Memorandum Agree\u00adment No. B504962 with LLNL; and a Lucent \nGRPW Fel\u00adlowship. The information presented here does not necessar\u00adily re.ect the position or the policy \nof the Government and no o.cial endorsement should be inferred. Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. PLDI 03, June 9 11, 2003, San Diego, California, \nUSA. Copyright 2003 ACM 1-58113-662-5/03/0006 ...$5.00.  Keywords bug isolation, random sampling, assertions, \nfeature selec\u00adtion, statistical debugging, logistic regression 1. INTRODUCTION It is an unfortunate fact \nthat essentially all deployed soft\u00adware systems have bugs, and that users often encounter these bugs. \nThe resources (measured in time, money, or people) available for improving software are always limited, \nand the normal case is that through sheer numbers the user community brings far more resources to bear \non testing a piece of software than the team responsible for producing that software. This paper is about \nmaking lemonade from lemons. Given that deployed software has problems, perhaps we can speed up the process \nof identifying and eliminating those problems by learning something from the enormous number of exe\u00adcutions \nperformed by the software s user community. We propose an infrastructure where some information about \neach user execution of a program is transmitted to a central database. The data gathered from all executions \nis analyzed to extract information that helps engineers .nd and .x prob\u00adlems more quickly; we call this \nautomatic bug isolation.In our view, such an infrastructure has several bene.ts: For deployed software \nsystems, the number of execu\u00adtions in actual use dwarfs the number of executions produced in testing \nby orders of magnitude. For many software systems today, essentially all of the informa\u00adtion from user \nexecutions is discarded, because there is no mechanism for feedback. Retaining even a small portion of \nthat information could be valuable.  Gathering information from all, or at least a repre\u00adsentative sample, \nof user executions gives an accurate picture of how the software is actually used, allowing better decisions \nabout how to spend scarce resources on modi.cations. In particular, bugs that a.ect a large number of \nusers are a higher priority than bugs that are very rare. This kind of information is almost im\u00adpossible \nto obtain from anywhere other than actual user executions.  While our primary interest is in .nding \nand .xing qual\u00adity problems, information gathered from user execu\u00adtions could be useful for other purposes. \nFor exam\u00ad  ple, software authors may simply wish to know which features are most commonly used, or we \nmay be in\u00adterested in discovering whether code not covered by in-house testing is ever executed in practice, \netc. Traditional user feedback about problems often con\u00adsists of a call from a relatively unsophisticated \nuser to a perhaps only somewhat more sophisticated techni\u00adcal support center. In a networked world, it \nis simply more e.cient and accurate to gather this information automatically.  Many bugs sit on open \nbug lists of products for an ex\u00adtended period of time before an engineer is available to work on the \nbug. Automatically gathering data from user executions allows for automated analysis without human intervention. \nThus, when an engineer is .nally available to work on a problem, the results of auto\u00admated analyses done \nin the interim may help the engi\u00adneer to identify and .x even relatively simple problems more quickly. \n The idea of gathering data from actual user executions is not new. Commercial databases, for example, \nroutinely produce extensive log .les, and the .rst action when a user reports a problem is to inspect \nthose logs. Similarly, each .ight of the Boeing 777 generates logs that are subsequently combed for signs \nof possible problems [14]. There are many other similar examples in the world of commercial software. \nA more recent development is the result of ubiquitous In\u00adternet connectivity. Netscape/Mozilla, Microsoft, \nGNOME, and KDE have all deployed automated, opt-in crash report\u00ading systems. These systems gather key \ninformation about program state after a failure has occurred: stack trace, reg\u00adister contents, and the \nlike. By sending this information back to the development organization, the user community helps developers \ne.ectively triage bugs that cause crashes and focus on the problems experienced by the most users. We \nbelieve crash reporting is progress in the right di\u00adrection, but we also believe that existing approaches \nonly scratch the surface of what is possible when developers and users are connected by a network. For \nexample, the crash\u00adreporting systems do not gather any information about what happened before the crash. \nTrace information leading up to the failure may contain critical clues to the actual problem. Also, crash \nreporting systems report no information for suc\u00adcessful runs, which makes it di.cult to distinguish anoma\u00adlous \n(crash-causing) behavior from innocuous behavior com\u00admon to all executions. In general, the information \ngathered by crash-reporting systems is not very systematic, and in all feedback systems of which we are \naware (crash-reporting or otherwise) the subsequent data analysis is highly manual. We present one approach \nto systematically gathering in\u00adformation about program runs from a large, distributed user community \nand performing subsequent automatic analysis of that information to help in isolating bugs. Initially, \nwe believed that the interesting problem was the analysis of the data, and that gathering the data was \nrelatively straightfor\u00adward. However, we discovered that designing a data gath\u00adering infrastructure that \nwould scale is non-trivial. As a result, this paper is as much about the design and imple\u00admentation of \nthe system that gathers the data from user executions (Section 2) as it is about the subsequent data \nanalysis (Section 3). Our infrastructure is designed to gather information about a large number of program \nexecutions taking place remotely from a central site where data is collected. Any such design must solve \ntwo problems. The .rst problem is that the method for gathering in\u00adformation must have only a modest \nimpact on the perfor\u00admance of the user s program. Our approach, discussed in Section 2, is based on sampling. \nClassical sampling for mea\u00adsuring program performance searches for the elephant in the haystack : it \nis looking for the biggest consumers of time. In contrast, we are looking for needles (bugs) that may \noccur very rarely, and furthermore our sampling rates may be very low to maintain client performance. \nThis leads us to be concerned with guaranteeing that the sampling is statistically fair, so that we can \nrely on the reported frequen\u00adcies of rare events. We also develop new ways to reduce the overhead of \nthe necessary additional code that determines whether to take a sample or not. The second problem is \nthat information from the client must be transmitted over the network to a central database. Gathering \neven a relatively small amount of data periodi\u00adcally from a large number of clients creates signi.cant \nscal\u00adability problems. We have found it necessary to discard information about the order in which program \nstatements execute to achieve su.ciently compact representations of sampled data (Section 2.5). Section \n3 presents three applications of increasing sophis\u00adtication: We show how to share the cost of program \nassertions over a large user base through sampling. Each user only executes a fraction of the assertions, \nand thus sees good performance, but in the aggregate bugs due to assertion failures are still extremely \nlikely to be de\u00adtected (Section 3.1).  We show how to isolate deterministic bugs without the bene.t \nof explicit assertions. A bug is determin\u00adistic with respect to a predicate P if whenever P is true, \nthe program is guaranteed to crash at some fu\u00adture point. An initially large set of predicates hypoth\u00adesized \nto capture the cause of the crash is gradually reduced over time as sampled executions reveal which predicates \npredict program failure (Section 3.2).  We generalize our approach to the isolation of non\u00addeterministic \nbugs. A bug is non-deterministic with respect to a set of program predicates if it is not de\u00adterministic \nfor any predicate in the set (i.e., none of the considered predicates perfectly predicts program crashes). \nWe use statistical regression techniques to identify predicates that are highly correlated with pro\u00adgram \nfailure (Section 3.3).  Finally, monitoring of user executions raises privacy and security concerns. \nThe problems are both social and techni\u00adcal; a discussion of these issues appears in Section 5. 2. SAMPLING \nFRAMEWORK This section describes our sampling framework. We begin with sampling of basic blocks and gradually \nadd features until we can describe how to perform sampling for entire programs. Suppose we start with \nthe following C code: { check(p != NULL); p = p->next; check(i < max); total += sizes[i]; } Our sampling \nframework can be con.gured to sample ar\u00adbitrary pieces of code, which may be either portions of the original \nprogram or instrumentation predicates added sepa\u00adrately. For this particular example, assume that the \nital\u00adicized check() calls have been tagged for sampling. A check() call might conditionally halt the \nprogram (as with assert()), or it might append an event to a trace stream, or it might update a per-predicate \ncounter to record how often the predicate is true. The precise behavior of the instrumen\u00adtation code \nis of no concern to the sampling transformation itself. 2.1 Sampling the Bernoulli Way Suppose that we \nwish to sample one hundredth of these checks. Maintaining a global counter modulo one hundred is simple, \nbut has the disadvantage of being trivially periodic. If the above fragment were in a loop, for example, \none of the checks would execute on every .ftieth iteration while the other would never execute. We wish \nsampling to be fair and uniformly random: each check should independently have a 1/100 chance of being \nsampled each time it occurs. This is a so-called Bernoulli process, the most common example of which \nis repeatedly tossing a coin. We wish to sample based on the outcome of tossing a coin that is biased \nto come up heads only one time in a hundred. Ana\u00a8ive approach would be to use a simple random num\u00adber \ngenerator. Suppose rnd(n) yields a random integer uni\u00adformly distributed between 0 and n- 1. Then the \nfollowing code gives us fair random sampling at the desired density: { if(rnd(100) == 0) check(p != NULL); \np = p->next; if(rnd(100) == 0) check(i < max); total += sizes[i]; } This strategy has some practical \nproblems. Random num\u00adber generation is not free: tossing the coin may be slower than simply doing the \ncheck unconditionally. Furthermore, what was previously straight-line code is now dense with branches \nand joins, which may impede other optimizations. Sampling is sparse. Each of the conditionals has a 99/100 \n= 99% chance of not sampling. On any run through this block, there is a (99/100)2 98% chance that both \ninstrumentation sites are skipped. If we determine, upon reaching the top of a basic block, that no site \nin that block is sampled, then we can branch into fast-path code with all conditionally\u00adguarded checks \nremoved. This requires two versions of the code: one with sampled instrumentation, one without. It also \nrequires that we can predict how many future sampling opportunities are skipped before the next one is \ntaken. Anticipating future samples requires a change in random\u00adization strategy. Consider a sequence \nof biased coin tosses, with 0 indicating no sample and 1 indicating that a sample is to be taken. Temporarily \nincreasing the sampling density to 1/5,wemight have: (0,0,0,0,0,1,0,0,0,1, 0,1 ,0,0,1,...) ' -v \"'-v \n\"'-v\" '-v\" 6 423 An equivalent representation counts the number of tosses until (and including) the next \nsampled check: (6,4,2,3,...). This representation is predictive: the head of the sequence can be treated \nas a countdown, telling us how far away the next sample is. If we are at the top of a basic block contain\u00ading \nonly two checks, and the next sampling countdown is 6, we know in advance that neither of those sites \nis sampled on this visit. Instead, we merely discard two tosses and proceed directly to the instrumentation-free \nfast path: { if(countdown > 2) { /* fast path: no sample ahead */ countdown -= 2; p = p->next; total \n+= sizes[i]; } else { /* slow path: sample is imminent */ if(countdown--== 0) { check(p != NULL); countdown \n= getNextCountdown(); } p = p->next; if(countdown--== 0) { check(i < max); countdown = getNextCountdown(); \n} total += sizes[i]; } } The instrumented code does extra work to manage the next-sample countdown, but \nthe fast path is much improved. The only overhead is a single compare/branch and a con\u00adstant decrement, \nand this overhead is amortized over the entire block. In larger blocks with more instrumentation sites, \nthe initial countdown check has a larger threshold, but that one check su.ces to predict a larger number \nof skipped sampling opportunities. Consider the distribution of countdown values. With a sampling density \nof 1/100, there is a 1/100 chance that we sam\u00adple at the very next opportunity. There is a (99/100) \u00d7 \n(1/100) that the next chance is skipped but that the one after that is taken. A countdown of three appears \non a (99/100)2 \u00d7 (1/100) chance, and so on. These numbers form a geometric dis\u00adtribution whose mean value \nis the inverse of the sampling density (that is, 100). Numbers in a geometric distribution characterize \nthe expected inter-arrival times of a Bernoulli process. However, repeated tossing of a biased coin is \nnot necessary: geometrically distributed random numbers can be generated directly using a standard uniform \nrandom gen\u00aderator and some simple .oating-point operations. (In theory, a countdown may need to be arbitrarily \nlarge. However, the odds of a 1/100 countdown exceeding 232 - 1are less than onein10107 .) As canbeseen \nin theinstrumentedslowpath, thecount\u00addown is reset once it reaches zero. Thus, we consume next\u00adsample \ncountdowns gradually over time. However, the rate of consumption is slower than that for raw coin tosses: \nn countdowns for 1/d sampling encode, on average, nd tosses. A bank of pre-generated random countdowns, \nthen, is quite reasonable and will exhaust or repeat d times more slowly than would a similar bank of \nraw coin tosses. 2.2 From Blocks to Functions The scheme for blocks outlined above generalizes to an \nar\u00adbitrary control .ow graph as follows. Any region of acyclic code has a .nite number of possible paths. \nLet the maxi\u00admum number of instrumented sites on any path be the re\u00adgion s weight. A countdown threshold \ncheck can be placed at the top of each acyclic region. If the next-sample count\u00addown exceeds the weight \nof an acyclic region r on entry to r, then no samples are taken on that execution of r. Any cycle in \na control-.ow graph without instrumenta\u00adtion is weightless and may be disregarded. Any cycle with instrumentation \nmust also contain a threshold check, which guarantees that if we start at any threshold check and ex\u00adecute \nforward, we cross only a .nite number of instrumen\u00adtation sites before reaching the next threshold check. \nThus, we can compute a .nite weight for each threshold check. There is some .exibility regarding exactly \nwhere a thresh\u00adold check is placed, but computing an optimal solution is NP-hard [18]. For simplicity, \nour present system places one threshold check at function entry and one along each loop back edge. Weights \nmay be computed in a single bottom\u00adup traversal of each function s control .ow graph. If any threshold \ncheck is found to have zero weight, it is simply discarded. We produce two complete copies of the function \nbody. One contains full instrumentation, with each possible sam\u00adple guarded by a decrement and test of \nthe next-sample countdown. The other copy, the fast path, merely decre\u00adments the countdown where appropriate, \nbut otherwise has all instrumentation removed. We stitch the two variants to\u00adgether at threshold check \npoints: at the top of each acyclic region, we decide whether a sample is imminent. If it is, we branch \ninto the instrumented code. If the next sample is far o., we continue in the fast path code instead. \nFigure 1 shows an example of code layout for a function containing one conditional and one loop. Dotted \nnodes rep\u00adresent instrumentation sites; these are reduced to countdown decrements in the fast path. The \nboxed nodes represent threshold checks; we have added one at function entry and one along the back edge \nof the loop. This code layout strat\u00adegy is a variation on that used by Arnold and Ryder to reduce the \ncost of code instrumented for performance pro\u00ad.ling [2]. The principal change in our transformation is \nthe use of geometrically distributed countdowns in conjunction with acyclic region weights to choose \nbetween the two code variants. Arnold and Ryder use .xed sampling periods (e.g., exactly once per n opportunities, \nor exactly once per n in\u00adstructions) and do not apply region-speci.c weighting. Our approach imposes \nmore overhead, but o.ers greater statis\u00adtical rigor in the resultant sampled data. Arnold and Ryder have \nstudied several variations with di.erent trade-o.s of code size versus overhead; the same choices apply \nhere. 2.3 Function Calls New optimization opportunities arise in the presence of function calls. A conservative \ntreatment assumes any func\u00adtion call changes the countdown arbitrarily. Therefore, a Figure 1: Example \nof instrumented code layout new threshold check must appear immediately after each function call. This \ntreatment is appropriate if, e.g., the callee is being compiled separately. However, if the callee is \nknown and available for exam\u00adination, a simple interprocedural analysis can be used. A weightless function \nhas the following properties: The function contains no instrumentation sites.  The function only calls \nother weightless functions.  The set of weightless functions can be computed via a standard iterative \nalgorithm. For purposes of identifying acyclic regions and placing threshold checks, calls to weightless \nfunctions are invisible. Acyclic regions can extend below such calls, and no addi\u00adtional threshold check \nis required after such a call returns. A further bene.t is that the bodies of weightless functions may \nbe compiled with no modi.cations. They have no thresh\u00adold checks, no instrumented code, and therefore \nrequire no cloning or transformation of any kind. 2.4 Global Countdown Management Our initial experience \nsuggests that having the next-site countdown in a global variable can be expensive. Our sys\u00adtem is implemented \nas a source-to-source transformation for C, with gcc as our native compiler. We .nd that gcc treats the \nmany countdown-- decrements along the fast path quite poorly. It will not, for example, coalesce a sequence \nof .ve such decrements into a single countdown -= 5 adjust\u00adment. This apparently stems from conservative \nassumptions about aliasing of global variables. E.cient countdown management requires that the na\u00adtive \nC compiler take greater liberties when optimizing these decrements. We assist the native compiler by \nmaintaining the countdown in a local variable within each function: 1. At function entry, import the \ncurrent global count\u00addown into a local variable. 2. Use this local copy for all decrements, threshold \nchecks, and sampling decisions. 3. Just before function exit, export this local copy back out to the \nglobal.  To maintain agreement across all functions, we must also export just before each function call \nand import again after each call returns. Again, though, calls to weightless func\u00adtions may simply be \nignored, as they do not change or even inspect the countdown. Similarly, the bodies of weightless functions \nneed not import and export at entry and exit, since they always leave the countdown unchanged. With this \nchange, the conventional native C compiler can coa\u00adlesce decrements and perform other standard but important \noptimizations. 2.5 Issues in Remote Sampling Our framework for statistically fair sampling can be used \nfor any program monitoring application. As discussed in Section 1, there are issues peculiar to monitoring \na large number of remote sites. Here we brie.y discuss the main problems and a particular solution that \nwe adopt. Remote monitoring can harm performance in several ways. As usual the performance penalty imposed \nby the extra monitoring code is a concern, but so are the use of local storage to hold results (even \ntemporarily) on a user s ma\u00adchine, the use of network bandwidth to transmit results, and .nally the storage \nneeded to hold results on a central server for analysis. For example, if we wish to retain all sampled \ndata, then the storage requirements for the central server grow linearly with the number of executions \neven if the data collected from each execution is constant size. Our approach is to sample the number \nof observations of each of a very large, but .xed, set of predicates (see Sec\u00adtions 3.2 and 3.3). The \n.nal form of the data is a vector of integers, with position icontaining the number of times we observed \nthat the ith predicate was true. For example, a typical entry might be that the predicate x>yat a partic\u00adular \nprogram point was observed to be true 42 times in one execution. Maintaining a vector of counters produces \ndata for an ex\u00adecution whose size is largely independent of the sampling density or running time. The \nloss of information, however, is signi.cant, as the order of the observations is discarded. While our \nresults can be interpreted as showing that one can go a long way ignoring ordering, we expect there are \ninter\u00adesting applications that require ordering information. We leave the problem of determining how \nto e.ciently gather, store and analyze partial traces (with ordering information) for future work.  \n 3. APPLICATIONS AND EXPERIMENTS As outlined in Section 1, we report on three applications of our framework. \nFrom the least to the most sophisticated, these are: sharing the cost of assertions among many users \n(Sec\u00adtion 3.1);  isolating a bug that is deterministic with respect to a predicate (Section 3.2);  \nusing statistical regression techniques to isolate a bug that is non-deterministic with respect to every \nconsid\u00adered predicate (Section 3.3). For each application we report on the overhead of instru\u00admentation. \nFor the last two applications we also quantify how e.ectively and e.ciently the bugs are isolated. While \nthe bug isolation examples presented here are based on .nd\u00ading particular bugs in speci.c programs, the \ntechniques are general. 3.1 Sharing the Cost of Assertions In conventional usage, C assert() calls are \nused dur\u00ading program development but are disabled when code ships to boost performance. However, deployed \nprograms fail in unanticipated ways, and it would be helpful to retain some level of assertion checking \nif the performance penalty were not excessive. The CCured translator analyzes programs written in C and \nattempts to prove that pointer operations are mem\u00adory safe. Where this cannot be done, CCured inserts \ndy\u00adnamic checks to enforce memory safety at run time [21]. For our purposes, CCured is simply a source \nof assertion-dense code. The individual assertions are quite small and fast (array bounds checks, testing \nfor null, etc.) but their per\u00adformance impact can be signi.cant. We wish to use random sampling to spread \nthis cost among many users. We have applied sampling to CCured versions of several Olden [10] and SPECINT95 \n[23] benchmarks. All programs run to completion and we are simply measuring the overhead of performing \nthe dynamic checks. 3.1.1 Whole-Program Sampling Table 1 summarizes static aspects of the sampling trans\u00adformation \nwhen applied to the entirety of each benchmark. For each program, we give the total number of non-library \nfunctions and the number of these that are weightless. As CCured is a whole-program analysis, weightless \nfunction identi.cation has the advantage of being able to examine every function body. We also count \nthe number of functions that directly contain at least one instrumentation site. (The remainder are functions \nthat have no sites of their own but that call other non-weightless functions.) Considering just the functions \nthat directly contain at least one instrumentation site, Table 1 also presents the av\u00aderage number of \nsites per function, the average number of threshold check points per function, and the average thresh\u00adold \nweight for all such points. (Note that the product of the last two of these metrics may exceed the .rst, \nas a single in\u00adstrumentation site may fall under more than one threshold check point. This can be seen \nin the example in Figure 1 as well.) The average site count shows the density of assertions in the code. \nThe average threshold weight measures how ef\u00adfective our transformation has been in amortizing the cost \nof countdown checks over multiple sites. Single-site func\u00adtions are not uncommon; thus, an average threshold \nweight above two is encouraging because it suggests that overall amortization rates are good. Table 2 \nshows the performance impact of unconditional in\u00adstrumentation as well as sampled instrumentation at \nvarious densities. The baseline for comparison is code translated by CCured and from which all dynamic \nmemory safety checks are removed. We report the speedup (> 1) or slowdown (<1) relative to this baseline \nwhen sampling at various den\u00ad benchmark function counts total weightless has sites average for functions \nwith sites sites threshold checks threshold weight bh 64 15 48 11.9 3.8 9.5 bisort 13 3 10 4.1 1.9 2.6 \nem3d 15 5 10 5.5 3.1 4.7 health 16 2 14 6.1 2.9 3.1 mst 16 5 11 6.2 2.5 3.9 perimeter 11 4 6 7.8 2.7 \n2.1 power 19 4 15 5.8 3.0 2.8 treeadd 7 2 5 3.6 2.0 2.5 tsp 14 5 8 15.2 3.9 3.5 compress 20 4 15 7.1 \n2.9 3.9 go 380 12 359 14.8 6.0 4.7 ijpeg 314 34 267 18.7 5.0 7.3 li 375 16 336 6.2 3.2 2.9 Table 1: \nStatic metrics for CCured benchmarks. Olden benchmarks are listed .rst, followed by SPECINT95. benchmark \nalways 10-2 10-3 10-4 10-6 bh 2.81 1.30 1.10 1.07 1.07 bisort 1.08 1.07 1.05 1.05 1.04 em3d 2.14 1.121.04 \n1.021.04 health 1.02 1.03 1.021.021.02 mst 1.25 1.06 1.04 1.03 1.04 perimeter 1.08 1.19 1.13 1.13 1.12 \npower 1.36 1.07 1.05 1.04 1.04 treeadd 1.13 1.09 1.09 1.09 1.11 tsp 1.05 1.17 1.16 1.15 1.14 compress \n2.01 1.21 1.14 1.14 1.14 go 1.17 1.46 1.26 1.22 1.22 ijpeg 2.46 1.17 1.05 1.04 1.03 li 1.58 1.24 1.18 \n1.16 1.16 Table 2: Relative performance of CCured bench\u00admarks with unconditional or sampled instrumenta\u00adtion. \nItalics marks cases where sampled instrumen\u00adtation outperforms unconditional instrumentation. sities. \nAll benchmarks were compiled using gcc 3.2 using optimization level -O2. Times were collected on a 1.3 \nGHz Pentium 4 Linux workstation with 512 megabytes of RAM. Reported speedups represent the average of \nfour runs; each run used a di.erent pre-generated bank of 1024 geometri\u00adcally distributed random countdowns. \nUnconditional instrumentation imposes slowdowns that vary widely from (health:2%) to (bh: 181%; ijpeg: \n146%). Even at a fairly high sampling density of 1/100,more than two thirds of our benchmarks run faster \nthan when all checks are always performed. Because each single check is small and fast, this suggests \nthat we have been successful in amor\u00adtizing the sampling overhead. On the other hand, three benchmarks \nrun slower, with go showing the largest penalty. In these cases, the time recovered by skipping 99/100 \nchecks is not enough to mask the added overhead of sampling. Furthermore, in all benchmarks, the overhead \nrelative to instrumentation-free code remains large. Only .ve bench\u00admarks have less than a 10% slowdown, \nand only one is below 5%. Performance improves as we reduce the sampling density to 1/1000. Most benchmarks \nsu.er less than a 10% penalty relative to uninstrumented code, and half are below 5%. Further reducing \nthe sampling density to 1/10,000 shows lit\u00adtle change, and by the time we reach 1/1,000,000 it is clear \nthat we have reached a performance .oor. Three bench\u00admarks (perimeter, tsp, go) are unable to compensate \nfor their sampling overhead relative to unconditional instrumen\u00adtation, while the remaining ten do run \nfaster. Among these, a few benchmarks (treeadd, compress, li)retainhighover\u00adhead relative to instrumentation-free \ncode, but in most cases the penalty is quite modest. Some benchmarks that perform the worst using unconditional \ninstrumentation perform quite well with sampling: ijpeg, for example, moves from an un\u00adconditional instrumentation \noverhead of 146% to only 3% with sparse sampling.  3.1.2 Statically Selective Sampling It is not necessary \nto put all instrumentation into a sin\u00adgle executable; one can easily create multiple executables where \neach contains a subset of the complete instrumen\u00adtation. Partitioning instrumentation by site, by module, \nby function, or by object .le are all reasonable schemes. Any individual executable contains less instrumentation \nand therefore incurs a smaller performance penalty. Fewer sites mean more weightless functions, and therefore \nbetter inter\u00adprocedural optimization per Section 2.3. Functions with\u00adout instrumentation sites require \nno code duplication, which limits executable growth. Known trusted code can be ex\u00adempted from instrumentation, \nor especially suspect code can be farmed out to a larger proportion of users for more in\u00adtensive study. \nGiven a suitable dynamic instrumentation infrastructure, sites can be added or removed over time as debugging \nneeds and intermediate results warrant. We have experimented with variants of the CCured benchmarks in \nwhich only a single function is instrumented at a time. Whereas fully instrumented executables range \nfrom 13%-149% larger than their non-sampling counter\u00adparts, average growth for single-function instrumented \nexe\u00adcutables is just 12% for the small Olden benchmarks and 6% for the larger SPECINT95 applications. \nPerformance is uni\u00adformly good: at 1/1000 sampling, 94% of site-containing func\u00adtions incur less than \n5% slowdown versus instrumentation\u00adfree code, while even the worst single function has less than a 12% \npenalty. 3.1.3 The Effectiveness of Sampling From these benchmarks and the examples in Sections 3.2 \nand 3.3, we conclude that realistic deployments will use sam\u00adpling densities between 1/100 and 1/1000. \nBut how e.ective is 1/1000 sampling at observing rare program behavior? Sup\u00adpose we are interested in \nan event occurring once per hun\u00addred executions. To achieve 90% con.dence of observing this event in \nat least one run, we need at least . . log (1 - 0.90)/ log 1 - 1 = 230,258 runs. 100 \u00d7 1000 While this \nis a large number, consider that sixty million O.ce XP licenses were sold in its .rst year on the mar\u00adket \n[19]. Assuming each licensee runs Microsoft Word twice per week, then this user base produces 230,258 \nruns every nineteen minutes. Achieving 99% con.dence of observing an event that occurs on one in a thousand \nruns requires 4,605,168 runs, which takes less than seven hours to gather. For smaller deployments, we \nmust either wait longer for su.cient data or increase the sampling density. As we shall see in Sections \n3.2 and 3.3, at least for restricted classes of bugs we can perform useful analysis with a few thousand \nexecutions. Thus, our techniques are likely most suited to applications where it is possible to gather \ndata with at least 1/1000 sampling from thousands of executions per day.  3.2 Bug Isolation Using Predicate \nElimination In this section we consider automatic isolation of deter\u00administic bugs. Recall from Section \n1 that for a deterministic bug there is a predicate that becomes true if the program must crash at some \npoint in the future. Deterministic bugs are quite common, though they are generally easier to .nd and \n.x using any method than non-deterministic bugs (see Section 3.3). 3.2.1 Instrumentation Strategy As \na case study in .nding deterministic bugs we take re\u00adlease 1.2 of the ccrypt encryption tool. This version \nis known to contain a bug that involves overwriting existing .les. If the user responds to a con.rmation \nprompt with EOF rather than yes or no, ccrypt crashes. The EOF sensitivity suggests that the problem \nhas some\u00adthingtodowith ccrypt s interactions with standard .le operations. In C, these functions commonly \nreturn values to indicate success or failure. Many C application program\u00admers follow the same model for \ntheir own error reporting. Thus, randomly sampling function return values may iden\u00adtify key operations \nthat behave di.erently in successful ver\u00adsus crashed runs. We group function return values into three \nclasses: negative values, zero, and positive values. This re\u00adduces the amount of information we must \ntrack while still making distinctions consistent with common C program\u00adming style. We instrument ccrypt \nas follows. Consider each syntactic call site that returns scalar values, including both arithmetic types \nas well as pointers. After each such call, update one of three counters depending upon the sign of the \nresult: one for negative values, one for zeros, and one for positive values. Each call site has its own \ntriple of counters. Thus, when the program terminates, we can examine any function call of interest and \nask how often that call returned a negative, zero, or positive value. For ccrypt, there are 570 call \nsites of interest, for 570\u00d73= 1710 counters. Each counter corresponds to a single predi\u00adcate that is \nhypothesized to behave di.erently in successful versus crashed runs. Speci.cally, we pose the problem \nas follows: Assume that predicates capture incorrect behav\u00adior. That is, assume that each predicate P \nshould always be false during correct execution. When P is true, the program either fails (a determinis\u00adtic \nbug) or is at increased risk of failing (a non\u00addeterministic bug). If we eliminate all predicates for \nwhich this hypothesis is disproved by observed runtime behavior, then the predi\u00adcates that remain describe \nthe conditions under which the program fails. 3.2.2 Elimination Strategies We make no e.ort to restrict \ninstrumentation to known system or library calls, nor do we distinguish functions that return status \ncodes from those that do not. Most of those 1710 predicates, then, have no bearing on program success \nor failure. Given a set of runs, we can discard irrelevant predicates using a set of elimination strategies: \n(Elimination by universal falsehood): Disregard any counter that is zero on all runs. These likely repre\u00adsent \npredicates that can never be true. (Elimination by lack of failing coverage): Disregard any triple of \ncounters all three of which are zero on all failed runs. Because one counter in each triple must always \nbe true for any sample, these likely represent instrumentation sites that are not even reached in fail\u00ading \nexecutions. (Elimination by lack of failing example): Disregard any counter that is zero on all failed \nruns. These likely represent predicates that need not be true for a failure to occur. (Elimination by \nsuccessful counterexample): Disregard any counter that has a non-zero value on any successful run. These \nmust represent predicates that can be true without a subsequent program failure. We characterize these \nas strategies because they are sub\u00adject to noise from random sampling, and also because not all are equally \napplicable to all bugs. For example, elimina\u00adtion by (successful counterexample) assumes that the bug \nis deterministic. The other three strategies do not make this assumption, but do require enough runs \nso that any predi\u00adcate that is ever true is likely to have been observed true at least once. Note that \nthese strategies are also not indepen\u00addent: (universal falsehood) and (lack of failing coverage) each \neliminate a subset of the counters identi.ed by (lack of failing example). Elimination strategies also \nvary in which kinds of runs they exploit: (successful counterexample) considers only successful runs; \n(lack of failing example) and (lack of failing coverage) consider only failures; (universal falsehood) \nuses both. 3.2.3 Data Collection and Analysis One function call, with one update to one of three coun\u00adters, \nis considered one instrumentation site. We transform the instrumentation to be sampled rather than uncondi\u00adtional \nusing the framework described in Section 2. In lieu of a large user community, we generate many runs \narti.\u00adcially in the spirit of the Fuzz project [20]. Each run uses a randomly selected set of present \nor absent .les, randomized command line .ags, and randomized responses to ccrypt prompts including the \noccasional EOF. We have collected 2990 trial runs at a sampling rate of 1/1000; 88 of these end in a \ncrash. Applying each elimination strategy independently to the counter traces: (Universal falsehood) \ndiscards 1569 counters that are zero on all runs, leaving 141 candidate predicates. (Lack of failing \ncoverage) discards 526 counter triples that are all zero on all crashes, leaving 132 candidate pred\u00adicates. \n(Lack of failing example) discards 1665 counters that are zero on all crashes, leaving 45 candidate predicates. \n(Successful counterexample) discards 139 counters that are non-zero on any successful run, leaving 1571 \ncan\u00addidate predicates. At .rst glance, elimination by (universal falsehood) is quite e.ective while elimination \nby (successful counterexample)seems rather poor. However, these two strategies test dis\u00adjoint properties \nand can be combined to good e.ect. The combination leaves only those predicates that are sometimes observed \nto be true in failed runs but never observed to be true in successful runs. For our ccrypt trials, only \ntwo pred\u00adicates meet this criterion: 1. traverse.c:320: file_exists() return value > 0 2. traverse.c:122: \nxreadline() return value == 0  Examining the corresponding code shows that these predi\u00adcates are consistent \nwith the circumstances under which the bug is reported to occur. This call to file_exists() re\u00adturns \n1 when an output .le already exists. A con.rmation prompt is presented, and this call to xreadline() \nreturns the user s reply, or null if the input terminal is at EOF.Inspec\u00adtion of the code immediately \nfollowing the xreadline() call shows that the programmer forgot to check for the EOF case: he assumes \nthat xreadline() returns a non-null string, and immediately inspects its contents. We have successfully \niso\u00adlated this (known) bug in ccrypt,and the.xisclear. While the file_exists() predicate is not itself \nthe cause of the bug, the fact that it appears on our list is useful in\u00adformation. It represents a necessary \ncondition under which crashes occur. That may be helpful, for example, if the engi\u00adneer wishes to reproduce \nthe bug in-house for further study. Of course, there should be some runs where file_exists() reports \nthat the .le exists but xreadline() returns a valid response from the user and therefore the program \ndoes not crash. If the file_exists() call is sampled on any such run, elimination by (successful counterexample) \ncorrectly de\u00adtermines that this predicate does not imply failure. It will be eliminated from further \nconsideration, and only the true smoking gun, the call to xreadline(), will remain. Thus we have the \nability to identify not only the direct cause of a bug but also related behaviors that are strongly but \nimper\u00adfectly correlated with failure. We further explore this idea of broad correlation in Section 3.3, \nwhere even the buggy lineofcodeitselfdoes not always causeacrash. As previously noted, the .rst three \nelimination strategies partially overlap, whereas the last, (successful counterexam\u00adple), is distinct. \n(Universal falsehood) and (successful coun\u00adterexample) only look at successful runs, hence are easily \n140 120 100 80 60 40 20 0 Number of successful trials used Figure 2: Progressive elimination by (successful \ncoun\u00adterexample) as successful runs accumulate. Crosses mark means; error bars mark one standard devia\u00adtion. \nanalyzed together. (Lack of failing example) in general elim\u00adinates the most features, and therefore \nis also a good can\u00addidate to combine with (successful counterexample).Doing so in the case of ccrypt \nleaves us with exactly the same two features, though in general one might .nd di.erent re\u00adsults. Elimination \nby (lack of failing coverage), on the other hand, is an inherently weaker strategy: when combined with \n(successful counterexample), we are still left with 86 features. 3.2.4 Re.nement over time In order \nto gain a better understanding of how the elimi\u00adnation strategies bene.t from increasing the number of \nruns, we have experimented with randomized subsets of our com\u00adplete run suite. We have seen that elimination \nby (successful counterexample) is quite e.ective when given a few thousand successful runs; how well \ndoes it perform with a smaller suite? We start with the 141 candidate predicates that are ever nonzero \non any run. We assemble a random subset of .fty successful runs and .lter the predicate set using elimi\u00adnation \nby (successful counterexample). We then add another .fty runs, and another .fty, and so on in steps up \nto the full set of 2902 successful runs. We repeat this entire process one hundred times to gauge how \nrapidly one can expect the predicate set to shrink as more runs arrive over time. Figure 2 shows the \nresults. The crosses mark the mean number of predicates remaining, while the vertical bars ex\u00adtend one \nstandard deviation above and below the mean. The short vertical bars in this case tells us that there \nis relatively little diversity in each of the hundred random subsets at any given size. The results show \nthat, on average, 1750 runs are enough to isolate twenty candidate features, another 500 runs reduces \nthat count by half, and a total of 2600 runs is enough to narrow the set of good features down to just \n.ve. One would expect more variety in runs collected from real users rather than an automated script. \nGreater diver\u00adsity can only bene.t the analysis, as it would provide more novel counterexamples and therefore \nmay eliminate more uninteresting predicates more rapidly. Number of \"good\" features left  3.2.5 Performance \nImpact Instrumenting function return values confounds several of the optimizations proposed in Section \n2. If most function calls are instrumentation sites, and if most function calls terminate acyclic regions, \nthen most acyclic regions contain only a single site and we have poor amortization of sampling overhead. \nFurthermore, ccrypt is built one object .le at a time, and we must conservatively assume that any cross\u00adobject \nfunction call is not weightless. Thus, for much of ccrypt, our sampling transformation devolves to a \nsimpler but slower pattern of checking the next-sample countdown at each and every site. In spite of \nthis, the performance impact of sampled instru\u00admentation is minimal. Using an experimental setup similar \nto that described earlier in Section 3.1.1, we .nd that the overhead for 1/1000 sampling is less than \n4%, and progres\u00adsively sparser sampling rates shrink this still further. Un\u00adconditional instrumentation \nalso performs well here, making either reasonable for this particular application. In the next section, \nthough, we consider a more invasive instrumenta\u00adtion strategy that requires sampling to keep overhead \nunder control.  3.3 Statistical Debugging In this section we consider the automatic isolation of non\u00addeterministic \nbugs. Recall from Section 1 that a bug is non\u00addeterministic with respect to a set of program predicates \nif no predicate in the set is perfectly correlated with pro\u00adgram crashes. For this case study we use \nversion 1.06 of the GNU implementation of bc. We .nd that feeding bc nine megabytes of random input causes \nit to crash roughly one time in four from, as it turns out, a previously unknown bu.er overrun error. \nSince bc sometimes terminates suc\u00adcessfully even when it overruns the bu.er, this bug is non\u00addeterministic. \nWe instrument bc using a variation on our previous strat\u00adegy of counter triples. We abandon elimination \nby (successful counterexample)1 in favor of statistical modeling to identify behavior that is broadly \ncorrelated with failure. 3.3.1 Instrumentation Strategy We instrument bc to guess and randomly check \na large number of predicates. As before, our goal is to identify predicates that capture bad behavior: \nfalse when the pro\u00adgram succeeds and true when the program crashes. We cast an extremely broad net, but \nwith an eye toward pointer and bu.er errors. For pointers, null pointers are of interest. Rel\u00adative addresses \nof pointers may be interesting as well, as this may capture cases where one pointer scans within a second \npointed-to bu.er. Checking pointer/pointer equality may reveal aliasing that, when not anticipated by \nthe program\u00admer, can lead to dangling wild pointer bugs. Scalar vari\u00adables serve as array indexes, pointer \no.sets, and in many other roles; relationships among scalars may reveal bu.er overruns, unanticipated \nconsequences of negative values, in\u00advalid enumeration constants, or a variety of other problems. At any \ndirect assignment to a scalar variable a, we iden\u00adtify all other local or global variables {b1,b2,...,bn} \nthat arealsoinscope andthathavethe same type. Wethen com\u00adpare the updated a to each bi, and note whether \na was less 1Because the bug is non-deterministic, if we have enough runs no predicates will satisfy elimination \nby (successful counterexample). than, equal to, or greater than bi. We compare pointers to same-typed \npointers as well, and additionally compare each pointer for equality with null. One comparison between \na and bi, which bumps one of three counters, is considered to be one instrumentation site subject to \nrandom sampling. When an instrumented application terminates, it emits the vector of counter triples \nalong with a .ag indicating whether it completed successfully or was aborted by a fatal signal. For bc \nthere are 10,050 counter triples, or 30,150 counters in all. The vast majority of these are of no interest: \neither they compare completely unrelated variables, or they ex\u00adpress relationships that behave identically \nin both successful and failed runs. The challenge is to .nd the few predicates that matter. 3.3.2 Crash \nPrediction Using Logistic Regression To .nd the important predicates, we recast bug isolation as a statistical \nanalysis problem. Each run of bc consti\u00adtutes one sample point consisting of 30,150 observed features \n(counters) and one binary outcome (0 = succeeded,1= crashed). Given numerous data points (sampled runs), \nwe want to identify a subset of our 30,150 features that pre\u00addict the outcome. This is equivalent to \nthe machine learning problem of learning a binary classi.er with feature selection, i.e., using as few \ninput features as possible. In the classi.cation setting, we take a set of data with known binary output \n(a training set), and attempt to learn a binary classi.er that gives good predictions on a test set. \nThe learning process usually involves additional parameters whose values can be determined using a cross-validation \nset. In our case, the end goal is to narrow down the set of fea\u00adtures. Hence our method must balance \ngood classi.cation performance with aggressive feature selection. A binary classi.er takes feature values \nas inputs, and out\u00adputs a prediction of either 0 or 1. Logistic regression [17] is a method of learning \na binary classi.er where the output function is assumed to be logistic. The logistic function is a continuous \nS -shaped curve approaching 0 on one end, and 1 on the other. The output can be interpreted as a prob\u00adability \nmeasure of how likely it is that the data point falls within class 0 or 1. Quantizing the logistic function \nout\u00adput then gives us a binary classi.er: if the output is greater than 1/2, then the data point is classi.ed \nas class 1 (a crash), otherwise it falls under class 0 (a successful run). Feature selection can be achieved \nby regularizing the function pa\u00adrameters to ignore most input features, forcing it to form a model that \npredicts success or failure using just a small se\u00adlection of sampled features. Regularization is important \nfor our purposes because we expect that most of our features are wild guesses, but that there may be \njust a few that correctly characterize the bug. While other techniques for combined classi.cation and \nfeature selection exist, few of them are particularly well\u00adsuited for this problem. Some methods [15, \n24] calculate a univariate correlation coe.cient independently for each feature; other methods, such \nas decision trees [6], are more computationally intensive. In our dataset, the features are clearly not \nindependent of each other, and the size of the problem can potentially be too large for more computation\u00adally \nintensive methods. Furthermore, logistic regression is a discriminative classi.cation method, and thus \ndoes not make any assumptions about the underlying distribution of the in\u00adput. This is crucial since \nour features arise from a decidedly arti.cial process and would be di.cult to characterize using simple \ndistributions. Suppose our training set D consists of M data points (x1,y1),...,(xM ,yM ), where each \nxi . RN denotes a vector of input predicate counters, and each yi = {0,1} denotes the corresponding output \nlabel. To learn a good classi.er, we can maximize the log likelihood of the training set. M LL(D)= [yi \nlog P(Y =1|x) i=1 +(1 - yi)log(1 - P(Y =1|x))]. Here the output labels yi are used as indicator functions \nto zero out exactly one of the two terms in each summand. In logistic regression, the distribution P(Y \n=1|x) is modeled as the logistic function \u00b5 (x) with parameters \u00df = (\u00df0 . \u00df R,\u00df . RN ). 1 P(Y =1|x)= \n\u00b5\u00df (x)= . 1+exp(-\u00df0 - \u00dfT x) The logistic parameters \u00df0 and \u00df take on the respective roles as the intercept \nand slope of the classi.er, and essen\u00adtially weigh the relative importance of each feature in the .nal \noutcome. We expect most of the input features to have no in.uence over the success or failure of the \nprogram, so we place an additional constraint that forces most of the \u00df s toward zero. This is accomplished \nby subtracting a penalty M term basedonthe \u00a31 norm 1\u00df 11 =|\u00dfj |. We can tune j=0 the importance of this \nregularization term through a regular\u00adization parameter .. The penalized log likelihood function is: \nM LL(\u00df |D,.)= [yi log \u00b5 (xi)+(1 - yi)log(1 - \u00b5 (xi))] \u00df\u00dfi=1 - .1\u00df 11. An assignment of \u00df coe.cients \nthat maximizes this func\u00adtion represents a model that maximizes the .delity of its predictions while \nstill limiting itself to form those predic\u00adtions on the basis of only a small number of features from \nthe complete feature set. 3.3.3 Data Collection and Analysis Our bc data set consists of 4390 runs with \ndistinct ran\u00addom inputs and distinct randomized 1/1000 sampling. We randomly chose 2729 runs for training, \n322 runs for cross\u00advalidation, and 1339 runs for testing. Although there are 30,150 raw features, many \ncan be discarded immediately us\u00ading elimination by (universal falsehood): in the training set 27,242 \nfeatures are always zero. Hence the e.ective number of features used in training is 2908. (Elimination \nby (lack of failing example) can eliminate another 647 features that are zero for all failed runs. However \nwe .nd that the presence or absence of these 647 features does not signi.cantly a.ect the quality of \nthe regularized logistic regression results.) To make the magnitude of the \u00df parameters comparable, the \nfeature values must be on the same scale. Hence all the input features are shifted and scaled to lie \non the in\u00adterval [0,1], then normalized to have unit sample variance. A suitable value for the regularization \nparameter . is de\u00adtermined through cross-validation to be 0.3. The model is then trained using stochastic \ngradient ascent to reach a local maximum of the penalized log likelihood. Using a step size 152 void \n153 more_arrays () 154 { 155 int indx; 156 int old_count; 157 bc_var_array **old_ary; 158 char **old_names; \n159 160 /* Save the old values. */ 161 old_count = a_count; 162 old_ary = arrays; 163 old_names = a_names; \n164 165 /* Increment by a fixed amount and allocate. */ 166 a_count += STORE_INCR; 167 arrays = (bc_var_array \n**) bc_malloc (a_count*si... 168 a_names = (char **) bc_malloc (a_count*sizeof(ch... 169 170 /* Copy \nthe old arrays. */ 171 for (indx = 1; indx < old_count; indx++) 172 arrays[indx] = old_ary[indx]; 173 \n174 175 /* Initialize the new elements. */ 176 for (; indx < v_count; indx++) 177 arrays[indx] = NULL; \n178 179 /* Free the old elements. */ 180 if (old_count != 0) 181 { 182 free (old_ary); 183 free (old_names); \n184 } 185 } Figure 3: Suspect bc function more_arrays().All top-ranked crash-predicting features point \nto large values of indx on line 176. of 10-5, the model usually converges within sixty iterations through \nthe training set. This takes roughly thirty minutes in MATLAB on a 1.8 GHz Pentium 4 CPU with 1 GB of \nRAM. Once the model has been trained, predicates with the largest \u00df coe.cients suggest where to begin \nlooking for the bug. In our case, the top .ve ranked coe.cients are well\u00adseparated in magnitude from \nthe rest, and show an unmis\u00adtakable trend: 1. storage.c:176: more_arrays(): indx > scale 2. storage.c:176: \nmore_arrays(): indx > use_math 3. storage.c:176: more_arrays(): indx > opterr 4. storage.c:176: more_arrays(): \nindx > next_func 5. storage.c:176: more_arrays(): indx > i_base  Thesourcecodefor more_arrays() appears \nin Figure 3. A comment earlier in the same .le suggests that this one of a suite of three functions for \nincreasing the number of functions, variables, or arrays that are needed. The logic is a fairly clear \ninstance of the bu.er reallocation idiom, even to one unfamiliar with the code: line 167 allocates a \nlarger chunk of memory; line 171 is the top of a loop that copies values over from the old, smaller array; \nline 176 completes the resize by zeroing out the new extra space. As the comment suggests, there are \ntwo similar functions (more_functions() and more_variables())nearby that do largely the same thing with \ndi.erent storage pools. The text of these three functions is nearly identical, but each uses di.erent \nglobal variables (such as a_count versus f_count versus v_count). The top ranked predicates seem bizarre \non .rst exami\u00adnation, because the variables they relate do not appear to have any real connection to \neach other or to more_arrays(). For example, scale tracks signi.cant digits for .oating point calculations, \nwhile use_math records whether an initial math library is to be loaded. Why would crashes tend to happen \nwhen local variable indx exceeds these seemingly unrelated globals on this particular line? An obvious \nhypothesis is that indx is simply unusually large in such cases. If indx is large, then it will tend \nto be larger than any number of otherwise unrelated variables. Perhaps crashes occur when the input to \nbc de.nes unusually large numbers of arrays. Closer scrutiny of more_arrays() quickly reveals this to \nbe the case. The allocation on line 167 requests space for a_count items. The copying loop on line 171 \nranges from 1 through old_count -1. The zeroing loop on line 176 continues on from old_count through \nv_count -1.And here we .nd the bug: the new storage bu.er has room for a_count elements, but the second \nloop is incorrectly bound by v_count instead. After a glimpse at the neighboring more_variables() function \nit is clear that more_arrays() was created by copying and pasting more_variables() and then changing \nnames like v_count and v_names to a_count and a_names. The loop bound on line 176 was missed in the renaming. \nThe logistic regression model points us at the buggy line, the buggy variable, and even reveals something \nof the condi\u00adtions under which the bug appears. Having found the bug, it is reasonable to ask whether \nthe statistical analysis could have pointed at it even more directly. The mistaken use of v_count instead \nof a_count on line 176 means that a bu.er overrun occurs when indx > a_count on line 176. This does correspond \nto a predicate sampled by our system, but this predicate is ranked 240th in the trained model. Why was \nthis, the smoking gun, not ranked .rst? There are several reasons to consider. Samples are taken randomly, \nwhile the model itself is trained using stochastic gradient ascent. Thus, a degree of noise is fundamental \nto the process. Even crashing is not guaranteed: out of 320 runs in which sampling spotted indx > a_count \nat least once, 66 did not crash. Thus, C programs can get lucky , meaning that this is not a strict overrun \n=. crash impli\u00adcation. Manual inspection of the data reveals a high degree of redundancy among many instrumentation \nsites within more_arrays(), meaning that the model has several fea\u00adtures to choose from that have equivalent \npredictive power. This suggests that our counters may be too .ne-grained: we are distinguishing many \nbehaviors that are in fact so tightly interrelated as to be equivalent. This bug seems clear enough once \nfound. However it has been present and undiscovered at least since 1992 (the time stamp on this .le in \nthe oldest version of GNU bc that we can .nd). Many bugs are obvious only once one knows where to look. \nThe logistic regression results directed us to one misbehaving variable on one line of code, out of 8910 \nlines in bc as a whole. Our approach does not automatically .nd and .x bugs. But it does suggest where \nto start looking, and what sort of scenarios (e.g., unusually large indx)to consider. Although we are \nstill learning about the capa\u00adbilities of this system and how to interpret its results, we believe that \nstatistically guided debugging has the potential to make the process of .nding and .xing bugs more e.cient \nand more responsive to the needs of end users. 1.15 1.1 1.05 1 0.95 Sampling density Figure 4: Relative \nperformance of bc with uncondi\u00adtional or sampled instrumentation 3.3.4 Performance Impact Our bc instrumentation \nis fairly dense. The leftmost bar in Figure 4 shows that if this instrumentation is added with\u00adout sampling, \nthe performance penalty is 13%. A sampling density of 1/100 cuts this in half (6%). At the 1/1000 density \nused in our statistical debugging experiment, the penalty is barely measurable (0.5%). Still lower densities \nshow small, unexpected speedups relative to uninstrumented code. This is apparently due to e.ects such \nas changes in relative code alignment, cache behavior, measurement noise, and other unpredictable factors. \n  4. RELATED WORK Sampling has a long history, with most applications focus\u00ading on performance pro.ling \nand optimization. Any sam\u00adpling system must de.ne a trigger mechanism that signals when a sample is to \nbe taken. Typical triggers include pe\u00adriodic hardware timers/interrupts [8, 25, 27], periodic soft\u00adware \nevent counters (e.g., every nth function call) [3], or both. In most cases, the sampling interval is \nstrictly peri\u00adodic; this may su.ce when hunting for large performance bottlenecks, but may systematically \nmiss rare events. The Digital Continuous Pro.ling Infrastructure [1] is un\u00adusual in choosing sampling \nintervals randomly. However, the random distribution is uniform, such as one sample ev\u00adery 60K to 64K \ncycles. Samples thus extracted are not inde\u00adpendent. If one sample is taken, there is zero chance of \ntak\u00ading any sample in the next 1 59,999 cycles and zero chance of not taking exactly one sample in the \nnext 60K 64K cy\u00adcles. We trigger samples based on a geometric distribution, which correctly models the \ninterval between successful in\u00addependent coin tosses. The resulting data is a statistically rigorous \nfair random sample, which in turn grants access to a large domain of powerful statistical analyses. Recent \nwork in trace collection has focused on program understanding. Techniques for capturing program traces \nconfront challenges similar to those we face here, such as minimizing performance overhead and managing \nlarge quan\u00adtities of captured data. Dynamic analysis in particular must Relative performance encode, \ncompress, or otherwise reduce an incoming trace stream in real time, as the program runs [12, 22]. It \nmay be di.cult to directly adapt dynamic trace analysis techniques to a domain where the trace is sampled \nand therefore in\u00adcomplete. Our e.ort to understand and debug programs by selecting predicates is partly \ninspired by Daikon [13]. Like Daikon, we begin with fairly unstructured guesses and eliminate those that \ndo not appear to hold. Unlike Daikon, we are concerned with gathering data from production code, which \nleads us to use sampling of a large number of runs and statistical mod\u00adels; the Daikon experiments are \ndone on a smaller number of complete traces. We are also interested in detecting bugs, while Daikon focuses \non the somewhat di.erent problem of detecting program invariants. The DIDUCE project [16] also attempts \nto identify bugs using analysis of executions. Unlike Daikon, most processing does take place within \nthe client program. As in our study, DIDUCE attempts to relate changes in predicates to the manifestation \nof bugs. However, DIDUCE performs com\u00adplete tracing and focuses on discrete state changes, such as the \n.rst time a predicate transitioned from true to false. Our approach is more probabilistic: we wish to \nidentify broad trends over time that correlate predicate violations with in\u00adcreased likelihood of failure. \nSoftware tomography as realized through the GAMMA system [5] shares our goal of low-overhead distributed \nmon\u00aditoring of deployed code. Applications to date have focused on code coverage and traditional performance \nmonitoring tasks, whereas our primary interest is bug isolation. 5. PRIVACY AND SECURITY As noted in \nSection 1, the most important program behav\u00adiors are those exhibited by deployed software in the hands \nof users. However, any scheme for monitoring software post\u00addeployment necessarily raises privacy and \nsecurity concerns. Theissues arecomplex andasmuchsocialastechnical. However, our approach can only succeed \nif users feel safe contributing to the shared data pool. Thus, addressing these concerns is both a moral \nand a practical imperative. The experiences of Netscape/Mozilla with crash feedback systems may be illustrative. \nWe have met with members of the Netscape Talkback Team, a group of quality assur\u00adance engineers who manage \ncrash reports from the auto\u00admated feedback system. Considerable e.ort has gone into designing the client \nside of this system so that users are fully informed. The system is strictly opt-in on a per-failure \nba\u00adsis, or may be disabled entirely. The user may optionally examine the contents of the crash report, \nand no informa\u00adtion is ever sent to Netscape without explicit authorization. Figure 5 shows the sort \nof information presented each time Netscape or Mozilla has crash data to submit. Not all users will read \nor understand these assurances. Even so, there are some technical measures we can take to protect the \nprivacy of even non-technically savvy users. The very nature of the sampling process itself a.ords a \ndegree of anonymity. We collect a small bit of information from many, many users; any single run has \nlittle revelatory power. Some data, or some parts of execution, may be so sensitive that even this di.use \ninformation leakage is unacceptable. Several type-based analyses under the broad heading of se\u00adcure information \n.ow [7, 26, 28] may be helpful here. Such systems statically identify parts of a program that manipu- \nThe Netscape Quality Feedback Agent is a feature that gathers prede.ned technical information about Communi\u00adcator \nand sends it back to the Netscape software develop\u00adment team so they can improve future versions of Commu\u00adnicator. \n... No information is sent until you can examine exactly what is being sent. ... Information gathered \nby this agent is limited to information about the state of Communicator when it has an error. Other sensitive \ninformation such as web sites visited, email messages, email addresses, passwords, and pro.les will not \nbe collected. All information Netscape collects via this agent will be used only for the purposes of \n.xing product defects and improv\u00ading the quality of Netscape Communicator. This data is for internal \ndiagnostic purposes only and will not be shared with third parties. For more information on Netscape \ns general privacy pol\u00adicy, go to: <http://home.netscape.com/legal_notices/ privacy.html> Communicator \nactivates the agent dialog box when a prob\u00adlem occurs, or when it has gathered information that Netscape \nneeds to improve future versions of Communi\u00adcator. ... If you prefer to disable the agent, you may do \nso here: Figure 5: Privacy assurances as used in Netscape Quality Feedback Agent late sensitive data; \nwe can avoid inserting instrumentation that reveals such values. Of course, this will make it di.\u00adcult \nto track bugs in security-sensitive parts of an applica\u00adtion, but that trade-o. is always present: one \ncan only .x bugs about which the customer is willing to provide useful information. When using statistical \nmodels such as that of Section 3.3, an attractive mechanism for protecting user anonymity be\u00adcomes available. \nMany statistical analyses are characterized by a set of su.cient statistics: a collection of values that \ncompletely capture the internal state of the analysis. For example, if one wanted to compute the mean \nof a stream of numbers, then a running total and a count would be su.\u00adcient statistics: the mean can \nbe computed from these with\u00adout retaining the individual numbers in the stream. Simi\u00adlarly, once the \nlogistic regression parameters have been up\u00addatedwithanew trace, thetrace itself maybediscarded. If the \nanalysis host is compromised, an attacker cannot recover the precise details of any single past trace. \nA statistical approach designed to cope with noise o.ers some protection against malicious users who \nmight try to poison the central database with bogus data, or overwhelm it with data representing the \nparticular bugs they wish to see .xed. Recent work on protecting privacy and preventing abuse in collaborative \n.ltering systems may also be applica\u00adble [9, 11]. 6. CONCLUSIONS We have described a sampling infrastructure \nfor gathering information about software from the set of runs produced by its user community. To ensure \nthat rare events are ac\u00adcurately represented, we use a Bernoulli process to do the sampling, and we have \ndescribed an e.cient implementa\u00adtion of that process. We have also presented several sample applications: \nsharing the overhead of assertions, predicate guessing and elimination to isolate a deterministic bug, \nand regularized logistic regression to isolate a non-deterministic memory corruption error. 7. REFERENCES \n[1] J.M.Anderson,L.M.Berc, J. Dean,S.Ghemawat, M. R. Henzinger, S.-T. A. Leung, R. L. Sites, M. T. Vandevoorde, \nC. A. Waldspurger, and W. E. Weihl. Continuous pro.ling: Where have all the cycles gone? ACM Transactions \non Computer Systems, 15(4):357 390, Nov. 1997.  [2] M. Arnold and B. Ryder. A framework for reducing \nthe cost of instrumented code. ACM SIGPLAN Notices, 36(5):168 179, May 2001. [3] M. Arnold and P. F. \nSweeney. Approximating the calling context tree via sampling. Research report RC 21789 (98099), IBM T.J. \nWatson Research Center, Yorktown Heights, New York, July 7 2000. [4] Association for Computing Machinery. \nProceedings of the International Conference on Software Engineering, Buenos Aires, Argentina, May 2002. \n[5] J. Bowring, A. Orso, and M. J. Harrold. Monitoring deployed software using software tomography. In \n M. B. Dwyer, editor, Proceedings of the 2002 ACM SIGPLAN-SIGSOFT workshop on Program analysis for software \ntools and engineering (PASTE-02), volume 28, 1 of SOFTWARE ENGINEERING NOTES, pages 2 9. ACM Press, 2002. \n[6] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classi.cation and Regression Trees. Statistics/Probability \nSeries. Wadsworth Publishing Company, Belmont, California, U.S.A., 1984. [7] P. Broadwell, M. Harren, \nand N. Sastry. Scrash: A system for generating secure crash information. In Proceedings of the 11th USENIX \nSecurity Symposium, Washington, DC, Aug. 4 8 2003. To appear. [8] M. Burrows, U. Erlingson, S.-T. Leung, \n M. Vandevoorde, C. Waldspurger, K. Walker, and B. Weihl. E.cient and .exible value sampling. ACM SIGPLAN \nNotices, 35(11):160 167, Nov. 2000.  [9] J. Canny. Collaborative .ltering with privacy. In Proceedings \nof the IEEE Symposium on Research in Security and Privacy, pages 45 57, Oakland, CA, May 2002. IEEE Computer \nSociety, Technical Committee on Security and Privacy, IEEE Computer Society Press. [10] M. C. Carlisle. \nOlden: Parallelizing Programs with Dynamic Data Structures on Distributed-Memory Machines. PhD thesis, \nDepartment of Computer Science, Princeton University, June 1996. [11] C. Dellarocas. Immunizing online \nreputation reporting systems against unfair ratings and discriminatory behavior. In Proceedings of the \n2nd ACM Conference on Electronic Commerce (EC-00), pages 150 157. ACM, 2000. [12] B. Demsky and M. C. \nRinard. Role-based exploration of object-oriented programs. In Proceedings of the International Conference \non Software Engineering [4]. [13] M. D. Ernst, J. Cockrell, W. G. Griswold, and D. Notkin. Dynamically \ndiscovering likely program invariants to support program evolution. IEEE Transactions on Software Engineering, \n27(2):1 25, Feb. 2001. [14] D. Esler. Welcome to the virtual ramp. Overhaul &#38; Maintenance, VII(2):55, \nMar. 2001. [15] T. R. Golub, D. K. Slonim, P. Tamayo, C. Huard, M. Gaasenbeek, J. P. Mesirov, H. Coller, \nM. L. Loh, J. R. Downing, M. A. Caligiuri, C. D. Bloom.eld, and E. S. Lander. Molecular classi.cation \nof cancer: class discovery and class prediction by gene expression monitoring. Science, 286(5439):531 \n537, 1999. [16] S. Hangal and M. S. Lam. Tracking down software bugs using automatic anomaly detection. \nIn Proceedings of the International Conference on Software Engineering [4], pages 291 301. [17] T. Hastie, \nR. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Stats. Springer, 2001. [18] M. \nHirzel and T. Chilimbi. Bursty tracing: A framework for low-overhead temporal pro.ling. In 4th ACM Workshop \non Feedback-Directed and Dynamic Optimization, Austin, Texas, Dec. 1 2001. [19] Microsoft Corp. Microsoft \n2002 annual report and form 10-K. Available at <http://www.microsoft.com/msft/ar02/>, Redmond, Washington, \n2002. [20] B. Miller, D. Koski, C. P. Lee, V. Maganty, R. Murthy, A. Natarajan, and J. Steidl. Fuzz revisited: \nA re-examination of the reliability of UNIX utilities and services. Technical report, Computer Science \nDepartment, University of Wisconsin, Madison, WI, 1995. [21] G. Necula,S.McPeak, andW.Weimer. CCured: \nType-safe retro.tting of legacy code. In C. Norris and J. James B. Fenwick, editors, Proceedings of the \n2002 ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages (POPL-02), volume 37, 1 of ACM \nSIGPLAN Notices, pages 128 139. ACM Press, 2002. [22] S. P. Reiss and M. Renieris. Encoding program executions. \nIn Proceedings of the 23rd International Conference on Software Engeneering (ICSE-01), pages 221 232. \nIEEE Computer Society, 2001. [23] SPEC 95. Standard Performance Evaluation Corporation Benchmarks. <http://www.spec.org/osg/cpu95/CINT95/> \n,July 1995. [24] R. Tibshirani, T. Hastie, B. Narasimhan, and G. Chu. Diagnosis of multiple cancer types \nby shrunken centroids of gene expression. PNAS, 99(10):6567 6572, 2002. [25] O. Traub, S. Schechter, \nand M. D. Smith. Ephemeral instrumentation for lightweight program pro.ling. Unpublished technical report, \nDepartment of Electrical Engineering and Computer Science, Hardward University, Cambridge, Massachusetts, \nJune 2000. [26] D. M. Volpano and G. Smith. A type-based approach to program security. In M. Bidoit and \nM. Dauchet, editors, TAPSOFT 97: Theory and Practice of Software Development, volume 1214 of Lecture \nNotes in Computer Science, pages 607 621. Springer-Verlag, 1997. [27] J. Whaley. A portable sampling-based \npro.ler for Java virtual machines. In Proceedings of the ACM 2000 conference on Java Grande, pages 78 \n87. ACM Press, 2000. [28] S. Zdancewic, L. Zheng, N. Nystrom, and A. C. Myers. Untrusted hosts and con.dentiality: \nSecure program partitioning. In Proceedings of the 18th ACM Symposium on Operating Systems Principles \n(SOSP 01), pages 1 14. Chateau Lake Louise, Ban., Alberta, Canada, Oct. 2001. Appeared as ACM Operating \nSystems Review 35.5.  \n\t\t\t", "proc_id": "781131", "abstract": "We propose a low-overhead sampling infrastructure for gathering information from the executions experienced by a program's user community. Several example applications illustrate ways to use sampled instrumentation to isolate bugs. Assertion-dense code can be transformed to share the cost of assertions among many users. Lacking assertions, broad guesses can be made about predicates that predict program errors and a process of elimination used to whittle these down to the true bug. Finally, even for non-deterministic bugs such as memory corruption, statistical modeling based on logistic regression allows us to identify program behaviors that are strongly correlated with failure and are therefore likely places to look for the error.", "authors": [{"name": "Ben Liblit", "author_profile_id": "81100555854", "affiliation": "University of California, Berkeley, Berkeley, CA", "person_id": "PP38025922", "email_address": "", "orcid_id": ""}, {"name": "Alex Aiken", "author_profile_id": "81100399954", "affiliation": "University of California, Berkeley, Berkeley, CA", "person_id": "PP39041079", "email_address": "", "orcid_id": ""}, {"name": "Alice X. Zheng", "author_profile_id": "81538301756", "affiliation": "University of California, Berkeley, Berkeley, CA", "person_id": "P338603", "email_address": "", "orcid_id": ""}, {"name": "Michael I. Jordan", "author_profile_id": "81339507945", "affiliation": "University of California, Berkeley, Berkeley, CA", "person_id": "PP42051464", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/781131.781148", "year": "2003", "article_id": "781148", "conference": "PLDI", "title": "Bug isolation via remote program sampling", "url": "http://dl.acm.org/citation.cfm?id=781148"}