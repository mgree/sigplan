{"article_publication_date": "05-09-2003", "fulltext": "\n A Comparison of Empirical and Model-driven Optimization Kamen Yotov2 Xiaoming Li1 Gang Ren1 Michael \nCibulskis1 kyotov@cs.cornell.edu xli15@uiuc.edu gangren@students.uiuc.edu cibulski@uiuc.edu Gerald DeJong1 \nMaria Garzaran1 dejong@cs.uiuc.edu garzaran@cs.uiuc.edu Keshav Pingali2 Paul Stodghill2 pingali@cs.cornell.edu \nstodghil@cs.cornell.edu David Padua1 padua@uiuc.edu Peng Wu3 pengwu@us.ibm.com 1University of Illinois \nat Urbana-Champaign 2Cornell University 3IBM T.J. Watson Research Center ABSTRACT Empirical program optimizers \nestimate the values of key optimi\u00adzation parameters by generating different program versions and running \nthem on the actual hardware to determine which values give the best performance. In contrast, conventional \ncompilers use models of programs and machines to choose these parameters. It is widely believed that \nmodel-driven optimization does not com\u00adpete with empirical optimization, but few quantitative compari\u00adsons \nhave been done to date. To make such a comparison, we replaced the empirical optimization engine in ATLAS \n(a system for generating a dense numerical linear algebra library called the BLAS) with a model-driven \noptimization engine that used de\u00adtailed models to estimate values for optimization parameters, and then \nmeasured the relative performance of the two systems on three different hardware platforms. Our experiments \nshow that model-driven optimization can be surprisingly effective, and can generate code whose performance \nis comparable to that of code generated by empirical optimizers for the BLAS. Categories and Subject \nDescriptors D.3.4 [Programming Languages]: Processors compilers, code generation, optimization; I.2.2 \n[Artificial Intelligence]: Auto\u00admatic programming program transformation; G.4 [Mathemati\u00adcal Software] \nGeneral Terms Algorithms, Measurement, Performance, Experimentation. Keywords Compilers, Memory hierarchy, \nTiling, Blocking, Unrolling, Pro\u00adgram transformation, Code generation, Empirical optimization, Model-driven \noptimization, BLAS This work was supported by NSF grants ACI-9870687, EIA-9972853, ACI-0085969, ACI-0090217, \nACI-0103723, and ACI-012140 Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are not made or distributed for \nprofit or commercial advantage and that copies bear this notice and the full citation on the first page. \nTo copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific \npermission and/or a fee. PLDI 03, June 9-11, 2003, San Diego, California, USA. Copyright 2003 ACM 1-58113-662-5/03/0006 \n$5.00.  1. INTRODUCTION The essential form of knowledge is nothing but a representation of truth: for \nthe truth of being and the truth of knowing are one, differing no more than the direct beam and the beam \nreflected. Francis Bacon, Advancement of Learning (1605) High-level program transformations such as \nloop tiling, loop un\u00adrolling, and software pipelining are critical for compiling efficient code for modern \narchitectures. Many of these transformations have numerical parameters whose values must be chosen carefully \nto obtain optimal performance. For example, to apply loop unroll\u00ading, it is necessary to determine how \nmany times the loop must be unrolled; too little unrolling may result in inefficient use of proc\u00adessor \nresources while too much unrolling may cause instruction cache overflow, or register spills to memory. \nTo compute good values for transformation parameters, most compilers use simple architectural models \nthat are tractable ab\u00adstractions of the complex hardware of modern computers. When applying loop tiling \nfor example, tile sizes are usually determined by assuming that the cache is fully associative, whereas \nmost hardware caches have limited set-associativity and use a pseudo-LRU replacement policy. Although \nthere is a substantial body of work on restructuring compilers, it is fair to say that even for a simple \nkernel like matrix multiplication, most current compilers do not generate code that can compete with \nhand-written code in efficiency. To circumvent this difficulty, some library writers are using empirical \noptimiza\u00adtion to generate highly tuned libraries automatically. Well-known library generators that employ \nempirical optimization are ATLAS [18] which generates highly tuned Basic Linear Algebra Subrou\u00adtine (BLAS), \nFFTW [8] and SPIRAL [21] which generate FFT libraries. To choose a good tile size for a loop, a system \nthat uses empirical optimization generates multiple versions of the tiled loop, runs all of them on the \nactual machine, and selects the tile size that results in the best performance. Simple architectural \nmodels can be useful to prune the size of the search space, but there is no need for precise models. \nThese library generators pro\u00adduce better code on a wide range of architectures than native compilers \nusing model-driven optimization. Why are current compilers unable to transform a simple matrix multiplication \nloop into code that performs as well as the code generated by ATLAS? One possibility is that compilers \nare at a disadvantage because they are general-purpose and must be able to optimize any program, whereas \nATLAS is a library generator that can focus on a particular problem domain. The trouble with this argument \nis that the problem domain of ATLAS is dense nu\u00admerical linear algebra, which is precisely the area that \nhas been studied most intensely by the compiler community! Another pos\u00adsibility is that systems like \nATLAS are performing certain optimi\u00adzations that compilers do not know about. Yet another possibility \nis that these systems incorporate the same optimizations as com\u00adpilers do, but perform them in a different \norder (the so-called phase-ordering problem ). Finally, if phase-ordering is not an issue, perhaps the \narchitectural models used by compilers are overly simplistic compared to the complex hardware of modern \ncomputers, so they are unable to estimate optimal transformation parameters accurately. To the best of \nour knowledge, no studies exist to provide clear answers to these questions. Figure 1. Empirical and \nmodel-driven optimizers This paper provides the first quantitative evaluation of the differ\u00adences between \nempirical and conventional, model-driven optimi\u00adzation. Figure 1 shows our experimental set-up. Like \nall systems that use empirical optimization, ATLAS has (i) a module that performs empirical search to \ndetermine certain parameter values (MMSearch), and (ii) a module that generates code, given these values \n(MMCase). We first studied the code generation module, and determined that the code it produces can be \nviewed as the end result of applying standard compiler transformations to high-level BLAS code. We then \nbuilt a modified version of ATLAS in which the search module was replaced with a module (MMModel) that \nuses architectural models to estimate values for the same parame\u00adters that ATLAS normally searches for. \nFinally, we measured the performance of the code generated by the two systems on three architectures, \nand studied the generated code itself to understand performance differences. Since both ATLAS and our \nmodified version use the same code generator, we are assured that any dif\u00adferences in performance arise \nsolely from differences between empirical optimization and model-driven optimization as imple\u00admented \nin the two systems, and not from different code generation strategies. This paper summarizes our findings. \nIn Section 2, we use the framework of restructuring compilers to describe the code genera\u00adtion strategy \nof ATLAS. In Section 3, we describe how ATLAS determines the values of the optimization parameters by \nusing an extensive empirical search. In Section 4, we describe novel pro\u00adgram and machine models that \nwe use to estimate values for these parameters without doing any empirical searches. In Section 5, we \ncontrast the two approaches, comparing the time spent to determine the parameter values,  the values \nof the parameters, and  the relative performance of generated code.  In Section 6, we show how performance \nchanges as parameter values are changed. This sensitivity analysis is useful for deter\u00admining which parameters \nmust be estimated most accurately for efficient code. We conclude in Section 7 with a discussion of our \nmain findings, and suggest future directions of research.  2. HIGH-PERFORMANCE BLAS In this section, \nwe use the framework of restructuring compilers to describe how ATLAS produces highly optimized code \nfor the Basic Linear Algebra Subroutines (BLAS). This description is based solely on what we have been \nable to deduce by studying the source code of the ATLAS system. The code produced by ATLAS depends on \ncertain optimization parameters, which are assumed to be given to the code generator by an oracle for \nnow. 2.1 High-level BLAS code We restrict our attention to the Level-3 BLAS, which are by far the most \ncomplex and time-consuming of the BLAS routines. The simplest Level-3 BLAS performs the following computation: \nC =aA\u00d7 B +\u00dfC (1) In this equation, A, B and C are input matrices of appropriate shape, while a and \u00df \nare scalars. Note that matrix multiplication is a special case of this computation in which a = 1 and \n\u00df = 0. We will consider the case a = \u00df = 1, for which it is straightforward to provide an implementation \nin a high-level, C-like language, as shown in Figure 2. Other BLAS-3 routines perform the same computation \nwith trans\u00adposed versions of either A or B or both. Level-2 BLAS codes perform variations of matrix-vector \nmultiplication, while Level-1 BLAS codes perform vector operations such as inner product and sum. for \n(int j = 0; j < M; j++) for (int i = 0; i < N; i++) for (int k = 0; k < K; k++) C[i][j]= C[i][j]+A[i][k]*B[k][j] \n Figure 2. Matrix Multiplication It is well known that the code in Figure 2 will perform poorly if the \nmatrices A, B, and C are large. This is because modern com\u00adputers have deep processor pipelines and multi-level \nmemory hierarchies consisting of caches and registers. In principle, matrix multiplication has excellent \nalgorithmic data reuse because it per\u00adforms O(N3) operations on O(N2) data. In practice, the program \nmay run poorly if a lot of data is touched between successive ac\u00adcesses to a given cache line, evicting \nthe cache line before it can be accessed again. For example, in the code shown in Figure 2, successive \naccesses to a given element of B are separated by ac\u00adcesses to O(N2) data, so every access to B may miss \nin the cache. Registers may be considered to be the highest level of the memory hierarchy. To use them \neffectively in matrix multiplication, it is necessary to register-allocate array elements, which many \ncompil\u00aders do not normally do. Finally, on pipelined processors that issue instructions in order, there \nwill be little overlap in the execution of different iterations of the k-loop, so instruction-level parallel\u00adism \n(ILP) will not be exploited. 2.2 Optimized BLAS codes To address these performance problems, it is necessary \nto restruc\u00adture the code to exploit features of modern architectures. ATLAS is not a restructuring compiler, \nbut the code it produces can be viewed as the end result of performing the following sequence of code \ntransformations on the high-level code in Figure 2. Cache-level tiling: The standard approach to exploiting \ndata reuse in loops such as matrix multiplication is to tile (or block) the loops in the loop nest [1]. \nIn effect, the matrix multiplication is converted to a sequence of smaller matrix multiplications whose \nworking sets fit in the cache. Each of the small matrix multiplications multiplies an MBxKB sub\u00admatrix \nof A by a KBxNB sub-matrix of B and accumulates the result in a MBxNB sub-matrix of C. In this paper, \nwe call these operations mini-MMMs. ATLAS tiles only for the L1 data cache, and it uses only square tiles \n(NB=MB=KB). The code for a mini-MMM is shown in Figure 3, assuming the JIK loop order. for (int j = 0; \nj < NB; j++) for (int i = 0; i < NB; i++) for (int k = 0; k < NB; k++) C[i][j] += A[i][k] * B[k][j] \nFigure 3. Mini-MMM code The value of NB is an optimization parameter. Choosing too large or too small \na value of NB increases the L1 cache miss ratio and leads to inefficient cache utilization. Register-level \ntiling: The code for the mini-MMM in the previous step is itself tiled to make effective use of the gen\u00aderal-purpose \nregisters. Each of the smaller matrix multiplica\u00adtions multiplies a MUx1 sub-matrix of A with a 1xNU \nsub\u00admatrix of B and accumulates the result in a MUxNU sub\u00admatrix of C. In this paper, we call these micro-MMMs. \nThe loops of a micro-MMM are unrolled completely to produce a straight-line code segment. The pseudo-code \nfor a mini-MMM after register-level tiling and unrolling of the micro-MMM is shown in Figure 4. In this \ncode, we assume for simplicity that the elements in the A, B, and C tiles touched in the mini-MMM are \nindexed starting at (0, 0). A pictorial view of this code is shown in Figure 5. The shaded regions in \nthis figure correspond to a micro-MMM. for (int j = 0; j < NB; j += NU) for (int i = 0; i < NB; i += \nMU) load C[i..i+MU-1, j..j+NU-1] into registers for (int k = 0; k < NB; k++) load A[i..i+MU-1,k] into \nregisters load B[k,j..j+NU-1] into registers multiply A s and B s and add to C s store C[i..i+MU-1, \nj..j+NU-1] Figure 4. Mini-MMM Code after Register-level Tiling After register-level tiling, the k loop \nin Figure 4 is unrolled completely if NB is small enough. Otherwise, this loop is un\u00adrolled by a factor \nof KU. Unrolling together with scheduling of operations, as described in the next step, gives the effect \nof software-pipelining the innermost loop of the mini-MMM. The values of MU, NU, and KU are optimization \nparameters. If MU and NU are too small, registers are not fully utilized, but if they are too large, \nthe compiler may generate many spills to memory. Unrolling by KU reduces loop control overhead, but too \nmuch unrolling can lead to instruction cache overflow, which reduces performance.   K A  Figure 5: \nPictorial View of Code in Figure 4 Scheduling: The body of the innermost loop that results from the \nprevious step is straight-line code that contains KU cop\u00adies of the micro-MMM code; each copy has MU*NU \nmulti\u00adply-add operations and the corresponding MU loads from A and NU loads from B. The operations in \nthe loop body can be divided into two groups: computation and memory ac\u00adcesses. We first focus on the \nscheduling of the computation opera\u00adtions in the loop body, assuming that the appropriate loads have \nbeen done. If the MMSearch routine in ATLAS detects that the processor has a combined multiply-add instruction \n(that is, the parameter MulAdd is true), the code generation module generates code that increases the \nlikelihood that the C compiler will uses that instruction. We confine our discus\u00adsion to the more complicated \ncase when such an instruction is not available. In this case, each multiply instruction writes its result \nto a temporary register, which is read by the corre\u00adsponding add instruction. For efficient pipelining, \nit is desirable that a multiplication instruction and its corresponding addition instruction be separated \nby independent instructions. To accomplish this, instruction scheduling considers separately the MU*NU \nmul\u00adtiply instructions (M1M2 MMU*NU) and the MU*NU add in\u00adstructions (A1A2 AMU*NU) in a single micro-MMM. \nIt inter\u00adleaves the two lists after skewing them by Latency, a parame\u00adter related to FP multiplier latency, \nto produce a schedule like this: M1 M2  MLatency A1 MLatency+1 A2 MLatency+2  MMU*NU-1 AMU*NU-Latency \nMMU*NU AMU*NU-Latency+1 AMU*NU-Latency+2  AMU*NU Such a schedule requires Latency number of extra registers \nto hold the results of the multiplications. In particular, if NR is the number of registers, the following \ninequality must hold for efficiency: MU * NU + MU + NU + Latency = NR (2) It is obvious that the final \nLatency adds can overlap with the initial Latency multiplies of the next iteration of the K loop. We \nnow consider where the loads of A and B elements must be inserted into this computation schedule. A na\u00efve \napproach is to schedule all the loads as early in this schedule as de\u00adpendences allow. However, if the \nCPU supports only a small number of outstanding loads, the instruction pipeline might stall. To avoid \nthis, ATLAS schedules blocks of NFetch loads at a time, where NFetch is an optimization parameter that \ndepends on the number of outstanding loads that the CPU supports. In this way, the executions of memory \nand ALU operations are interleaved. Notice that in each micro-MMM, there are MU+NU loads but MU*NU multiplies \nand adds. Therefore, the last part of this schedule for the loop body is likely to have only arithme\u00adtic \noperations. To make better use of memory, ATLAS tries to overlap some of the initial loads for one iteration \nof the k loop with the final computations of the previous iteration. This step can be viewed as a simple \nkind of software pipelin\u00ading of the unrolled loop. The number of these initial loads is determined by \nan optimization parameter called IFetch. Finally, there is a parameter called FFetch whose precise role \nis somewhat unclear to us. It appears to be used to suppress the initial loads for C from memory under \nsome circum\u00adstances. The values of MulAdd, FFetch, IFetch, NFetch, and Latency are optimization parameters. \n 2.3 Versioning As in many BLAS libraries, the library generated by ATLAS ac\u00adtually contains several \nversions of each high-level BLAS-3 algo\u00adrithm. For example, it is often the case that A, B, and C are \nsub\u00admatrices of larger matrices. In that case, it may be beneficial to copy these matrices into contiguous \nstorage to reduce conflict misses during the execution of the operation. However, the over\u00adhead of copying \nmay not be worthwhile if the matrix sizes are small, or if copying requires more memory than what is \navailable. Most BLAS libraries therefore have both a copying and a non\u00adcopying version of each code; \nat runtime, matrix size information is used to determine which version should be executed. Decisions \nthat ATLAS makes at runtime include (i) whether to copy the tiles for mini-MMM into continuous memory, \n(ii) the loop order of the mini-MMM (only JIK or IJK are considered), and (iii) how boundary sub-matrices \nwill be multiplied. Boundary sub-matrices arise because a matrix dimension may not necessarily be an \ninte\u00adger multiple of the tile size. Skinny matrices left-over from til\u00ading can be multiplied by specialized \ncode called clean-up code, in which the exact size of operand sub-matrices is used to fully un\u00adroll loops. \nAt runtime, ATLAS decides whether to call the generic MMM code or the clean-up code for handling the \nboundary sub\u00admatrices.  2.4 Summary The code produced by ATLAS can be viewed as the end-result of applying \na sequence of well-known program transformations to high-level BLAS codes. The optimization parameters \nused in these transformations are NB, MU, NU, KU, MulAdd, Latency, IFetch, NFetch, and FFetch. Although \nthese optimization parameters are specific to ATLAS, we would argue that most of them would arise naturally \nin the context of conventional restructuring compilers. Any compiler that tiles for the data cache will \ncompute a parameter similar to NB. Parameters similar to MU, NU, and KU are used for register tiling. \nInstruction selection and scheduling would require parame\u00adters like MulAdd, Latency and something similar \nto NFetch or IFetch. FFetch is the odd man out. 3. HOW ATLAS FINDS PARAMETER VALUES As mentioned earlier, \nATLAS does its work in two phases. The installation phase of ATLAS is shown in Figure 1. First, ATLAS \ncomputes machine parameters such as L1 data cache size and the number of registers. Then, it performs \nempirical search to determine values for the optimization parameters, using machine parameters to limit \nthe size of the search space. Finally, it generates all the mini-MMM versions for the library.  At run-time, \nan application program calls ATLAS s general interface routine. This interface routine takes care of \nsome trivial cases such as empty input matrices and the case when a= 0 (which means the result is just \n\u00dfC), and then makes a sequence of calls to the appropriate mini-MMM to perform the matrix multiplication. \n  3.1 Estimating machine parameters Since ATLAS is self-tuning, it does not require the user to pro\u00advide \nthe values of machine parameters. Instead, it runs micro\u00adbenchmarks to determine approximate values for \nmost of these parameters. Among these are the size of L1 data cache (L1Size),  the number of floating-point \nregisters (NR),  the availability of a multiply-add instruction (MulAdd), and  the latency of the floating-point \nmultiply unit (Latency).  These micro-benchmarks have nothing to do with matrix multipli\u00adcation; for \nexample, the micro-benchmark for estimating the size of the L1 data cache is similar to the one in Hennessy \nand Patter\u00adson [9]. As described in Section 2, two other architectural parameters are critical for performance: \n(i) the L1 instruction cache size, and (ii) the number of outstanding loads that the machine supports. \nATLAS does not determine these explicitly; instead, they are con\u00adsidered implicitly during the optimization \nof matrix multiplication code. For example, the size of the L1 instruction cache may limit the amount \nof unrolling of the k loop of Figure 4 (parameter KU) that is beneficial. Rather than estimating the \nsize of the instruction cache by running a micro-benchmark and then using that to de\u00adtermine the amount \nof unrolling, ATLAS generates a suite of MMM kernels with different amounts of unrolling, and deter\u00admines \nthe best one experimentally. 3.2 Estimating optimization parameters Once machine parameters have been \nestimated, ATLAS estimates optimization parameters using an extensive search to find optimal values. \nThe optimization sequence is as follows. 1. Find best NB.  2. Find best MU and NU.  3. Find best KU. \n 4. Find best Latency.  5. Find best Fetch factors.  6. Find non-copy version crossover.  7. Find \noptimal cleanup codes. We now discuss each of these steps in greater detail.  3.2.1 Find Best NB In \nthis step, ATLAS generates a number of mini-MMMs for ma\u00adtrix sizes NBxNB where NB is a multiple of 4 \nthat satisfies the following inequalities: 16 = NB = 80; NB2 = L1Size (3) A phase-ordering problem in \ngenerating these mini-MMM codes is that ATLAS does not as yet have optimal values for the other optimization \nparameters. Therefore, it uses rough estimates for the values of these parameters. The values of MU and \nNU are set to the values closest to each other that satisfy (2). For each matrix size, ATLAS tries two \nextreme cases for KU no unrolling (KU=1) and full unrolling (KU=NB). Suitable Latency and all Fetch \nparameters are obtained from running the micro\u00adbenchmarks. The NB that produces highest MFLOPS is chosen \nas best NB value, and it is used from this point on in all experiments as well as in the final versions \nof the optimized mini-MMM code. 3.2.2 Find Best MU and NU This step is a straightforward search that \nrefines the reference values of MU and NU that were used to find the best NB . ATLAS tries all possible \ncombinations of MU and NU that satisfy Inequality (2). The cases when MU or NU is 1 are treated spe\u00adcially. \nA test is performed to see if 1x9 unrolling or 9x1 unrolling is better than 3x3 unrolling. If not, unrolling \nfactors of the form 1xU and Ux1 for values of U greater than 3 are not checked. 3.2.3 Find Best KU This \nstep is another simple search. Unlike MU and NU, KU does not depend on the number of available registers, \nso technically we can make it as large as we want to without causing register spills. The main constraint \nhere is instruction cache size. ATLAS tries values for KU between 4 and NB/2 as well as the special values \n1 and NB. The value that gives best performance in terms of MFLOPS (based on NB, MU and NU as determined \nfrom the previous steps) is declared the optimal value for KU.  3.2.4 Find Best Latency In this step, \nATLAS tries different schedules for the computations in the unrolled k loop of Figure 4 to determine \nif there is a skew that generates a better schedule than the one obtained by using the hardware Latency \nvalue. It checks all the values between 1 and 6, and selects the one that performs best, using parameter \nvalues determined from the previous steps. It also ensures that the chosen value divides MU*NU*KU to \nfacilitate instruction scheduling. 3.2.5 Find Best Fetch In this step ATLAS searches for the values \nof FFetch, IFetch and NFetch. First, ATLAS determines the value of FFetch (0 or 1). Then, it searches \nfor the best value of the pair (IFetch, NFetch) where IFetch is in the interval [2,MU+NU] and NFetch \nis in the interval [1,MU+NU-IFetch].  3.2.6 Find Non-Copy Version of NB ATLAS generates both a copy \nand a non-copy version of the mini-MMM code. Without copying, the sub-matrices touched by a mini-MMM \nare not stored contiguously in memory, so the prob\u00adability of conflict misses is higher than in the copy \nversion. To avoid excessive conflict misses, ATLAS uses a smaller tile size for the non-copy version \nthan for the copy version. It searches for tile sizes from NB down to 16 in steps of 4, until performance \ndeteriorates by 20% or more. The tile size that yields highest per\u00adformance is selected to be the value \nof NCNB (Non-Copy NB). ATLAS also does a very restricted search for unroll factors and latencies (between \n2 and 9 in this case). These searches are very much along the lines of the corresponding copy versions. \nAt runtime, ATLAS invokes the copy version whenever the col\u00adlective size of the matrices is big enough \nthat the cost of copying can be amortized by computation. The non-copy version is used when the dimensions \nof the matrices (see Figure 2) satisfy the following constraint: M * N * K = NB3 (4) One other case \nin which ATLAS decides to use the generated non-copy version is for matrices larger than (218-2)/NB in \none dimension.  3.3 Generate Optimal Cleanup Codes If the tile size is not a multiple of the original \nmatrix size, there may be left-over rows and columns in the matrices to be multi\u00adplied that are too few \nto form a tile. ATLAS generates clean-up code for handling these left-over rows and columns. These special \ntiles have one dimension of size NB and another dimension be\u00adtween 1 and NB-1. The size of this other \ndimension will be called L in this discussion. ATLAS generates cleanup codes as follows. For each value \nof L from NB-1 on down, ATLAS generates a specialized version of the code in which some of the loops \nare fully unrolled. Full un\u00adrolling is possible because the shapes of the operands are com\u00adpletely known. \nWhen the performance of the general version falls within 1% of the performance of the current specialized \nversion, the generation process is terminated. The current L is declared to be the Crossover Point. At \nruntime, the specialized versions are invoked when the dimension of the left-over tile is greater than \nL, while the general version is invoked for tile sizes smaller than L. 4. ESTIMATING PARAMETERS USING \nMODELS We now discuss the use of architectural models to estimate the values of optimization parameters \nwithout empirical search. 4.1 Estimating NB There is a large body of work in the compiler community \non esti\u00admating good tile sizes in the context of general-purpose compil\u00aders. Dongarra and Schreiber [6] \ndetermine tile sizes and orienta\u00adtions such that the amount of data touched by the tile is bounded by \nthe size of the cache, while minimizing the surface to volume ratio of the tile. Their solution modeled \nthe problem as a constrained minimization problem. Boulet et al [2] suggest that the surface to volume \nratio is not the appropriate metric to be minimized and present alternative metrics one might choose \nto optimize. Neither of these two approaches addresses the question touches all of A. To stay in LCM, \nall misses must be cold misses, so we need to be able to store A completely in the cache. This will require \nstorage for MB*KB=NB2 elements. Matrix B on the other hand is accessed by the control variables of the \noutermost loop j and the innermost loop k. Therefore once j is fixed, we will need to access the entire \njth column of B in every iteration of loop i. After this computation, we will not access this column \nagain, so we need storage for KB=NB elements of B in the cache. K of conflict misses. Lam, Rothberg and \nWolf present strategies to determine square tiles while minimizing capacity and conflict misses [12]. \nColeman and McKinley generalized this analysis to rectangular tiles [5]. Ramanujam and Sadayappan have \nconsid\u00adered tiling in the context of distributed-memory computers. Wolf, Chen and Maydan [20] discuss \nhow tile sizes can be determined in the setting of production compilers. Clauss has used Ehrhart polynomials \nto provide exact values for the working set of a loop [4]. Unfortunately, few if any of these papers \nhave comparisons with hand-optimized BLAS code, so it is difficult to determine the accuracy of these \nmethods and their impact on performance. Our model for estimating NB is quite intricate, so we describe \nit in stages. Although the model works for any level of the memory  Figure 6. Matrix Indexing Scheme \nand Cache Usage for JIK Finally, matrix C is indexed by the control variables of the outer\u00adhierarchy, \nwe consider only the L1 cache (like ATLAS). most two loops j and i. This means that we fix a single \nelement First, we assume a simple cache model: of C, reuse it in all the iterations of the innermost \nloop k, and fully-associative cache (no conflict misses);  line size is one element (no spatial locality); \n optimal replacement strategy (not LRU). Consider the code for matrix multiplication in Figure 2 in \nwhich the loops are executed in the JIK order. We can distinguish be\u00adtween three different scenarios, \ndepending on how large the ma\u00adtrices are compared to the cache size. Large Cache Model (LCM): This model \nis valid when the size of the matrices is small compared to the cache size. In this model, the only misses \nare cold misses because nothing ever needs to be evicted from the cache.  Small Cache Model (SCM): This \nmodel is valid when the cache is small compared to the size of the matrices; intui\u00adtively, it is too \nsmall to hold even one row of a matrix. Spe\u00adcifically, if two different accesses to an array element \nare separated by accesses to O(1) other elements, we assume that the second access hits in the cache; \notherwise, the number of accesses to other elements grows with matrix size, so we as\u00adsume that the second \naccess misses in the cache.  Medium Cache Model (MCM): This model is valid when the cache is big enough \nto hold one (or several) rows of a matrix, but is not enough to hold a full matrix.  never touch it \nagain after that. Therefore we need storage for one element of C in the cache. Summarizing, we need NB2 \n+ NB +1 lines in the cache to satisfy the requirements for LCM in this simple cache model. Because we \nknow the capacity C of the cache, we can set NB to be the largest value consistent with this inequality: \nNB2 + NB +1 = C (5) For loop orders other than JIK, the reasoning is very similar. The only difference \nis that the matrices A, B and C change their roles, thus contributing different terms on the left side \nof (5), but the final inequality is the same. In summary, we always need to keep one full matrix in the \ncache, a row or a column of another matrix and a single element of the third matrix. 4.1.1 Modeling NB \nfor Caches with Larger Lines We now refine our cache model by allowing lines to hold some number of elements \ngreater than 1 (say B). Spatial locality be\u00adcomes important, and we must consider the layout of matrices \nin the storage space. If we now return to our example (loop order JIK), and take into account that we \nare dealing with FORTRAN matrices stored in column-major layout, we can correct Inequality (5) as follows: \nNB2 NB C .... These models can be used to make quantitative performance pre\u00ad + 1 = + (6) B B B dictions, \nbut we will not pursue that line of investigation in this paper. Our objective here is only to choose \nthe largest NB that makes the LCM valid during the execution of the mini-MMM The reasoning behind Inequality \n(6) is that we still need to cache the full matrix A, which contains MB*KB=NB2 elements. With B code \nshown in Figure 3. To keep things simple in the discussion elements per line and assuming that A is \nlaid out sequentially in that follows, we will use the word matrix instead of matrix tile. memory, we \nneed .NB2/B. cache lines. We need also to cache a Matrix A is indexed by the control variables of the \ninnermost two full column of B (KB=NB elements). Because matrices are laid out in column major order \nin memory, one such column will re\u00ad quire .NB /B. cache lines. Finally the single element of C that we \nneed to store in cache is part of a single cache line. The same reasoning also works for loop orders \nJKI and KJI. For other loop orders (IJK, IKJ, KIJ) we need to store in the cache a full matrix, a matrix \nrow from another matrix, and a single ele\u00adment from the third matrix (e.g. these matrices in the IJK \ncase are B, A and C respectively). Since we have non-unit line sizes, stor\u00ading a row in the cache requires \nmore storage than a column does. Because of the column major layout, each element of the row will most \nlikely be part of a different cache line, which makes the row require NB cache lines instead of the .NB \n/ B. , normally required by a column. Therefore Inequality (6) changes to the following: cache replacement \npolicy is LRU, the line containing A1,1 will be in the cache only if everything in the history so far \nis in the cache. The history contains matrix A (NB*NB elements), a column of B and a column of C. In \ncontrast, in a cache with optimal replace\u00adment, we need only one element of C. Continuing this argument, \nwe see that we need space for two col\u00adumns of B in the cache. With the space for the second column of \nB, it becomes possible for the elements of A to remain in the cache while the elements of the jth column \nof B are evicted. Applying the same argument again, but this time for the columns of C, it is easy to \nshow that we need storage for one extra element of C to ensure that no elements of A are evicted as a \nconsequence of LRU replacement. .. . Putting all this together, we get the following inequality: .. \n= NB 2 C + NB + 1 = (7) .. .... .. . 2 B B NB NB B NB NB C ..... 2 1 + + + or .... ... .. .... B B B \n 4.1.2 Modeling NB for LRU Replacement Caches (10) Finally, we analyze the effect of cache replacement \npolicy. Most NB2 C = .... + 3 + 1 ........ caches implement (pseudo) Least Recently Used (LRU) replace- \nB B B ment policy. As we shall see, this has a substantial impact on the optimal NB value. To find the \nimpact of replacement policy, we must reason about the history of data accesses. A well-known approach \nfor doing this is the stack algorithm [13]. In this approach, the history of memory references is represented \nas a stack that is updated after every memory reference M by pushing the line containing M onto the top \nof the stack and removing it from its old location if the line was already in the stack. An LRU cache \nwould obviously keep only the top L lines, where L is the size of the cache. We can rewrite the pseudo-code \nof the JIK loop order in the fol\u00adlowing way: for jth column B (BXj) for iit row of A (AiX) multiply AiX \nby BXj and add to Cij The top of the stack after one execution of the inner multiply will look like: \nAi ,1 B1, j Ai ,2 B2, j L Ai , NB BNB , jCi , j (8) Ci,j is used repeatedly (for each element multiply \nof AiX and BXj), so it stays at the top of the stack. This innermost multiply is performed NB times using \nthe different In summary, for an LRU cache and loop order JIK, the optimal NB ensures that the full matrix \nA, two columns of B, and one column and one element of C fit in the cache. In general, for any other \nloop order, the optimal NB ensures that one full matrix, two columns or rows of another matrix, and one \ncolumn or row and one element of the third matrix fit in the cache. Table 1. Equations for Optimal NB \nin the General Case Equation IJK, IKJ B CNB B NB =++... ...13 2 JIK, JKI B C B NB B NB =+... ...+... \n...13 2 KIJ B C B NBNB B NB . =. .. .. .. +... ...++... ...12 2 KJI ( ) B CNB B NB B NB =++... ...+... \n...12 2 The inequalities for all the different loop orders are summarized in rows of A and the same \ncolumn of B. So the top of the stack after Table 1. The only variable in all these equations is NB, therefore \none full execution of the middle loop looks like this: we can find the largest integer solution for it \nusing binary search. AA LAC 1,1 1,2 1,NB 1, j AA L AC 2,1 2,2 2,NB 2, j M (9) AA LAC NB-1,1 NB-1, 2 \nNB-1,NB NB-1, j ABA B LA BC NB,1 1, j NB,2 2, j NB,NB NB, j NB, j Notice that the elements of the jth \ncolumn of B appear only in the last row of the history because they are accessed in each iteration of \nthe i loop. At the next iteration of the outer loop, we need to access A1,1 again. Since A1,1 is the \noldest reference in the history and the  4.1.3 Modeling NB for Set-Associative Caches In practice, caches \nare not fully associative. In spite of this, the models developed so far are relevant for the copy version \nof mini-MMM code, which copies tiles into a sequential region of mem\u00adory to reduce conflict misses. To \ndetermine the optimal value for NB when copying is not per\u00adformed, we need to estimate the impact of \nconflict misses. Our estimates are based on the model developed by Fraguela et al. [7], which uses a \nstatistical approach to predict approximately the number of cache misses caused by a loop nest with constant \nbounds. For lack of space, we do not address this issue further in the paper.  4.2 Finding MU, NU, and \nKU Register tiling can be looked at as a special case of tiling (or equivalently unroll and jam [1]) \nfor a cache that has the following properties: fully-associative any register can contain a value loaded \nfrom any memory address;  unit line size each register contains a single value and is a line in this \nL0 cache by itself;  optimal replacement policy the code generator is free to schedule any register \nfill or spill at any time. We assume that the register tile has K as its outermost loop, as shown in \nFigure 4. If all three parameters (MU, NU and KU) are equal to some value U, we can use inequality (5) \nto constrain U: U 2 + U +1 = NR (11) Here NR is the number of floating point registers. Intuitively, \nU2 registers are required for the register tile of C, U registers are re\u00adquired for B and 1 register \nis required for A. Because NR is usu\u00adally small, it may be sub-optimal to unroll equally in all direc\u00adtions. \nFor example NR=6 forces us to make U=1 (no unrolling). However if we can unroll by different amounts \nin the three direc\u00adtions, we can choose to unroll by 2 in exactly one direction and get better performance. \nIf we think about the three unroll factors separately, (11) is re\u00adplaced with: MU * NU + NU +1 = NR (12) \nNotice that while MU and NU are constrained by the number of registers, unrolling along the K direction \nis not. 4.2.1 ATLAS-Compatible MMM Unroll Model The code generation strategy described in Section 2.2 \nactually requires MU registers rather than just one register for A (this al\u00adlows more elements of A to \nbe prefetched into registers). Fur\u00adthermore, in the absence of a combined multiply-add instruction we \nneed extra registers for storing the result of floating-point mul\u00adtiplications. Taking into account all \nthese considerations, the ap\u00adpropriate constraint is given by Inequality (2), where Latency is replaced \nby the number of temporary registers (TR) required for scheduling: MU * NU + MU + NU + TR = NR Now if \nwe assume MU=NU we get: 2 NU + 2NU +(TR - NR)= 0 (13) which we can solve for NU. Having obtained NU, \nwe can solve (4) for MU: NR - TR - NU MU = (14) NU +1 and adjust MU and NU so that they are both at least \none and MU is the bigger one. For KU, our approach is to unroll along the K direction as much as possible \nwithout overflowing the L1 instruction cache. We can do this because we know the size of the instruction \ncache. When generating code for a specific KU value, we measure the size of the loop in bytes, using \na special feature of the C language, pre\u00adsent in the GCC compiler that allows us to compute addresses \nof goto-style labels. Here is an example: printf( code size = %d\\n , &#38;&#38;l2 - &#38;&#38;l1); return; \n l1: // mini-MMM code generated for fixed KU l2:;  This code prints the number of bytes of generated \nbinary code between the two labels.  4.3 Finding Fetch, Latency, and MulAdd ATLAS has three fetch parameters: \nFFetch, IFetch and NFetch. We choose FFetch=1 (to prefetch the portion of C into registers). We believe \nthat IFetch and NFetch should both be set to the num\u00adber of supported outstanding loads (OL). However \nwe do not yet have a benchmark that estimates OL; and we also found that per\u00adformance was not sensitive \nto the values of these parameters, as we discuss in Section 6. Therefore we set both parameters to 2. \nFor the optimization parameters Latency and MulAdd, our model uses the machine parameters determined \nby the hardware parame\u00adter detection module. 4.4 Summary We have developed a model-driven approach for \nchoosing all optimization parameters used by ATLAS: NB, MU, NU, KU, Mu\u00adlAdd, Latency, FFetch, IFetch \nand NFetch.  5. PERFORMANCE ANALYSIS In this section, we describe the results of our experiments with \nATLAS and with the modified version of ATLAS (called Model from this point on) in which empirical optimization \nhas been re\u00adplaced with model-driven optimization. We compare both the installation time of the two versions, \nand the execution times of double-precision matrix multiplications of various sizes on the three architectures \nshown in Table 2. 5.1 The Installation Phase Table 2. Test Platforms Hardware SGI Sun Intel CPU R12000 \nUltraSparcIII PIII-Xeon Frequency 270MHz 900MHz 550MHz Registers 32 32 8 L1 Cache 32KB/32KB 64KB/32KB \n16KB/16KB L2 Cache 4MB 8MB 512KB Memory 1GB 2GB 1GB OS IRIX64 v6.5 SunOS 5.8 RedHat 7.3 ATLAS Compiler \nMIPSpro CC v7.3.1.1m Forte 7 C v5.4 gcc v3.2 ATLAS Options -O3 -64 -OPT:Olimit=15000 -TARG:platform=IP27 \n-LNO:blocking=OFF -LOPT:alias=typed -dalign -fsingle -xO2 -native -fomit-frame\u00adpointer -O Native Compiler \nMIPSpro F77 v7.3.1.1m Forte 7 F95 v7.0 g77 v3.2 Native Options -O3 -64 -OPT:Olimit=15000 -TARG:platform=IP27 \n-dalign -native -xO5 pad -O3 -fno-inline -funroll-all-loops -funroll-loops The installation phase can \nbe divided into four parts. The first part, Detecting Machine Parameters, takes 6%-12% longer in Model, \nmainly because of the way the code is organized. The original version of ATLAS detects the cache size \nas part of the empirical search while Model performs this task in this part. The second part, Estimating \nOptimization Parameters, is where 5.2.1 Mini-MMM Performance Table 5 shows that on both the SGI and \nIntel machines, the per\u00adformance of the codes generated by the two approaches is similar. On the Sun, \nthere is roughly a 20% difference in performance. Table 5. Mini-MMM Performance Comparison ATLAS performs \nthe empirical search. In Model, this part takes almost no time because no search is performed. The final \ntwo parts of the installation phase generate the final source, and then compile it to make the library. \nCurrently, we generate more ver\u00adsions of the clean-up code (for multiplying boundary sub\u00admatrices) than \nATLAS does, so there are minor differences be\u00adtween ATLAS and Model in the time they take to execute \nthese two parts. Figure 7 presents this breakdown of installation times. Difference (%) SGI 457 453 1 \nSun 1287 1052 20 Intel 394 384 1  5.2.2 MMM Performance 10000 9000 8000 7000 6000 5000 4000 3000 2000 \n1000 0 Detect Machine Parameters Optimize MMM Generate Final Code Build Library Figure 7. Installation \nTime time (s)  5.1.1 Optimization Parameter Values Table 3 and Table 4 show the values of the optimization \nparame- Next, we compare the performance of complete MMM using: mini-MMMs generated by ATLAS (with \nempirical search),  mini-MMMs generated by Model,  hand-tuned BLAS routines, and  high-level matrix \nmultiplication compiled using the most powerful optimizations available in the native compiler.  We \ncompare performance for square matrices of size 100 to 5000. On SGI and Sun, both ATLAS and Model use \nthe non-copy ver\u00adsions of mini-MMM for multiplying large matrices. These data\u00adpoints are shown as unfilled \nmarkers on the plots. F77 ATLAS Model BLAS 600 500 400 ters that are determined by ATLAS and by Model \nrespectively. The parameter values determined by the two systems are very similar on the Intel and SGI \nmachines. On the Sun, tile size and MFLOPS 300 KU values are significantly different, but other parameter \nvalues 200 are close. The impact of these differences will be discussed later 100 in this paper. Table \n3. ATLAS Estimated Parameters 0 - - Latency SGI 64 / 64 4 / 4 / 64 0 / 5 / 1 3 Sun 48 / 48 5 / 3 / 48 \n0 / 3 / 5 5 Intel 40 / 40 2 / 1 / 40 0 / 3 / 1 4 Table 4. Model Estimated Parameters - - Latency SGI \n62 / 45 4 / 4 / 62 1 / 2 / 2 6 Sun 88 / 78 4 / 4 / 88 1 / 2 / 2 4 Intel 42 / 39 2 / 1 / 42 1 / 2 / 2 \n3 5.2 Comparison of Performance 0 1000 2000 3000 4000 5000 Martix Size 0 1000 2000 3000 4000 5000 Matrix \nSize Model ATLAS In this section, we compare the execution times of both the mini-MMM routines (Table \n5) as well as complete MMM for various Figure 8. MMM Performance Comparison on SGI matrix sizes (Figure \n8, Figure 9 and Figure 10). On the SGI machine, the best performer is the native BLAS li\u00adbrary. On the \nmatrix sizes we tested, Model is always within 2% of ATLAS in performance. For matrix sizes larger than \n4000, Model outperforms ATLAS by roughly 30%, but both are much slower than BLAS. For these matrix sizes, \nboth ATLAS and Model decide to use the non-copy version, and this causes TLB misses to go up, as can \nbe seen in Figure 8. ATLAS finds a tile size of 64, which is also the size of the TLB on the SGI machine. \nThe model predicts a tile size of 45, so it requires fewer TLB en\u00adtries, and thus performs better. These \nexperiments demonstrate the well-known fact that for large data sizes, TLB effects can be im\u00adportant. \n Compiler ATLAS Model BLAS 1800 1600 1400 1200 MFLOPS 1000 800 600 400 200 0 Figure 9. MMM Performance \nComparison on Sun parameters to the high-level code for matrix multiplication that was given to these \ncompilers. We found that if the matrix sizes are hard-coded constants in this code, the performance obtained \nby the native compilers on SGI and Sun is close to that of ATLAS and Model. We do not yet understand \nthis issue.  5.3 Summary Two surprising conclusions can be drawn from the experimental results in this \nsection. First, we found that handwritten BLAS libraries perform better than either ATLAS-generated or \nModel\u00adgenerated code on all three machines; on the Sun and Intel ma\u00adchines, the difference in performance \nis 25%-33%. This suggests there is considerable room for improvement in both empirical and model-driven \noptimization techniques for generating the BLAS. Second, we found that on the SGI and Intel machines, \nthe code generated by model-driven optimization is similar in performance to the code generated by ATLAS. \nOn the Sun, only the values selected for tile size and KU by the two systems were significantly different, \nand the performance of Model-generated code is about 20% worse than ATLAS-generated code. It would appear \nthat for generating optimized BLAS, empirical search is not as important as is commonly believed. We \nalso repeated these experiments with rectangular matrices of different sizes, but reached the same conclusions. \n 6. SENSITIVITY ANALYSIS The results of the previous section show that the performance of code produced \nby model-driven optimization can be comparable to that of code generated by empirical optimization. An \ninteresting question at this point is the following: On the Sun, the best performer is again the native \nBLAS library. The codes generated by ATLAS and by Model are between 25% and 50% slower than the BLAS. \nATLAS-generated code performs How sensitive is the performance of the code to changes in the values \nof optimization parameters? about 20% better than Model-generated code for matrix sizes less This question \nis of interest for several reasons. In our context, the than 3000. problem of generating efficient \ncode can be viewed as a multi- As on the other machines, the native BLAS library performs best on the \nPentium. Both ATLAS-generated code and Model\u00adgenerated code perform about 20% worse than the BLAS, and \nare within 3% to 10% of each other. G77 ATLAS Model BLAS 600 500 400 dimensional optimization problem \nin which the independent vari\u00adables are the optimization parameters such as NB, MU, NU, etc., and the \ndependent variable is performance. When the parameter values determined by the two systems are different, \nsensitivity analysis is useful to understand which of these differences af\u00adfected performance the most. \nIn addition, if it turns out that near the optimal point, performance is relatively insensitive to changes \nin one of the parameters, we can spend less time and effort in optimizing the value of that parameter, \nwhich would benefit both empirical and model-driven optimization. Such insights would also help in developing \nhybrid optimization strategies that com\u00adbine model-driven and empirical optimization; if performance \nis 200 100 0 Figure 10. MMM Performance Comparison on Intel The native compilers on all three machines \ndid not produce very insensitive to the value of some parameter, we can use simple models to choose \nits value, and use complex models or empirical optimization only for determining values for high-sensitivity \npa- MFLOPS rameters. In the limit, if performance near the optimal point is relatively insensitive to \nchanges in any of the parameters, a simple model-driven optimization strategy is adequate. Because of \nthe large number of optimization parameters, it is im\u00adpractical to vary all of them simultaneously. Instead, \nwe set all optimization parameters to the values found by ATLAS, and then measured how performance of \nthe mini-MMM code changes when we vary one parameter at a time. On some graphs presented in this section, \nwe mark three impor\u00ad tant points: A shows the parameter value selected by ATLAS, M shows the parameter \nvalue selected by the Model, and B shows the best parameter choice among those tested. It is impor\u00adtant \nto note that M does not represent the performance achieved by the model since all parameters other than \nthe one being varied are set to ATLAS-selected values. 6.1 Sensitivity to tile size (NB) Figure 11, \nFigure 12, and Figure 13 show how performance changes when tile size is varied on the SGI, Sun and Intel \nma-On the Sun machine, performance is relatively unchanged for tile sizes between 40 and 80. The best \nperformance of 1336 MFLOPS is obtained for a tile size of 48. The model predicts a somewhat larger tile \nsize (88), and this accounts for roughly 10% of the 20% difference in the performance of Model-generated \nand ATLAS\u00adgenerated code. We are doing more detailed cache simulations to understand this issue. 450 \nM A, B chines respectively. 400 350 M A B MFLOPS 300 250 200400 MFLOPS 150 300 100 50200 0 100 20 \n70 120 170 220 270 320 Tile Size (B: Best, A: ATLAS, M: Model) 0 20 220 420 620 820 Figure 13: Performance \nvs. tile size on Intel Tile Size (B: Best, A: ATLAS, M: Model) Sensitivity to tile size is most pronounced \non the Intel machine; a 10% change in the tile size from the optimal tile size can reduce Figure 11: \nPerformance vs. tile size on SGI performance by 10% (400 MFLOPS down to 350 MFLOPS). On the SGI machine, \nATLAS and our model choose almost the However, both ATLAS and Model choose good tile sizes. same tile \nsize (64 vs. 62). Notice that the best performance is ob\u00ad tained when the tile size is roughly 450; \nat that point, performance is roughly 525 MFLOPS, which is about 15% better than the per\u00ad formance obtained \nby ATLAS or the model. Since ATLAS uses We believe that the sensitivity of performance to tile size on \nthe Intel machine arises from the relatively small L1 cache size. In contrast, performance on the SGI \nand Sun machines is relatively insensitive to tile size. the size of the L1 cache to limit the search \nspace for NB, it does not explore such large tile sizes.  6.2 Sensitivity to register tile size (MU, \nNU) Our conjecture is that this large tile size is appropriate for the L2 cache. If we use our model \nto determine NB for the L2 cache, we obtain 722, which is close to the point at which there is a sudden \ndrop in performance in Figure 11. Multi-level caching is outside the scope of this paper because we use \nthe ATLAS infrastructure for code generation. We are investigating this matter further. 5  1  1600 \nMU6 2 7 1 8 7 6 400 5 8 NU 300 74 6 200 53 1 4 NU 2 3 2 3 4  B A M 1400 Figure 14: \nPerformance vs. register tile size on SGI 1200 8 7   MFLOPS 400 3 200 8 MU 0 20 40 60 80 100 120 \n140 Figure 14, Figure 15, and Figure 16 show how performance Tile Size (B: Best, A: ATLAS, M: Model) \n changes when the register tile size is changed on the SGI, Sun, and Intel machines respectively. Both \nATLAS and Model choose Figure 12: Performance vs. tile size on Sun the best register tiles on the SGI \nand the Intel machines. On the Figure 18 shows that performance of the code increases slightly as the \nunroll factor KU is increased. The performance points for Model lie outside the range shown, so they \ndo not appear in this graph. Although ATLAS and Model choose very different values for KU on the Sun, \nit can be seen that performance is insensitive to this difference. The instruction caches on all the \nmachines are large enough that the k loop in the mini-MMM code of Figure 4 can be unrolled completely \nwithout overflowing the cache. This  will not be the case on a machine with a small instruction cache \nbut a large data cache, because NB for such a machine would be too large to permit the loop to be unrolled \ncompletely. Similarly, 8 MU  MU x 1 1 x NU 450 400 350 300 250 200 150 100 full unrolling can be \nsuboptimal on a machine with many regis\u00adters, since the values of MU and NU will be large; the code size \nfor each micro-MMM might be big enough that full unrolling (even for small NB) might overflow the instruction \ncache. 6.4 Sensitivity to Latency Latency is important only when the code generator is not using combined \nmultiply-add instructions. SGI has a combined multi\u00adply-add, and the impact on performance of changes \nin the Latency parameter is less than 20 MFLOPS so we do not show it graphi\u00adcally. 1600 MFLOPS Figure \n17 shows that the shape of the register tile can affect per-400 formance significantly. For example, \nthe value of the expression (MU*NU+MU+NU) is the same for (MU, NU) = (3, 1) and for 200 (MU, NU) = (1, \n3), but it can be seen that the performance of the 0 code produced in the two cases differs by 33% on \nthe Pentium. M A B 50 1400 0 1200 1000 MU / NU Figure 17: Performance vs. Register Tile Shape on Intel \nMFLOPS 800 600 The dependence of performance on the shape of the register tile disappears when we use \nicc rather than gcc to generate code. It 1 3 57 911 Latency (B: Best, A: ATLAS, M: Model) appears that \nicc uses RISC-like instructions such as load reg1,@mem followed by fmul reg1,reg2 instead of CISC-like \ninstructions such as fmul reg,@mem.  6.3 Sensitivity to KU SGI Sun Intel 1600 B A   A B B A  \n  0 10 20 30 40 50 60  1400 1200 1000 800 600 400 MFLOPS 12345678 Latency (B: Best, A: ATLAS, M: Model) \nFigure 20: Performance vs. latency on Intel We now consider machines without a multiply-add instruction. \nIf Latency is too small, the multiplications do not finish in time for the corresponding additions, so \nthe pipeline stalls. If the Latency is too big, the ATLAS code generator needs that many temporary registers, \nwhich limits the MU and NU unroll factors, reducing performance. Therefore we expect an inverse U shape \nfor the graph of performance vs. Latency, which is what we see in Figure 200 0 KU (B: Best, A: ATLAS, \nM: Model) Figure 18: Performance vs. KU 19 and Figure 20. On the Sun, the Model chooses a sub-optimal \nvalue for Latency, which though close to the value ATLAS chooses, results in a 10% drop in performance. \n 6.5 Sensitivity to Fetch parameters Our experiments showed that performance on the three machines is \ninsensitive to the values of FFetch, IFetch and NFetch parame\u00adters. 6.6 Summary Our experiments show \nthat performance on all three machines is most sensitive to the register tiling parameters MU and NU. \nThere are relatively few registers on most machines, so it is important to use them effectively. However, \neven for these sensitive parameters model-driven optimization was competitive. On the Sun, perform\u00adance \nis also very sensitive to the Latency parameter. Paradoxi\u00adcally, although performance is relatively insensitive \nto cache tile size on machines with large L1 data caches, there appear to be some subtleties in choosing \ngood values for this parameter, per\u00adhaps having to do with multiple memory hierarchy levels. Sub\u00adoptimal \ntile size and Latency choices contributed roughly equally to the 20% difference in performance between \nATLAS-generated and Model-generated code on the Sun.  7. CONCLUSIONS AND FUTURE WORK In this paper, \nwe compared the relative effectiveness of empirical and model-driven optimization in producing optimized \nBLAS libraries. To isolate the contribution of empirical optimization, we modified ATLAS so that it used \noptimization parameters derived from model-driven optimization, and compared the performance of code \ngenerated by the two approaches on three architectures. A surprising conclusion from our experiments \nis that model\u00addriven optimization can be very effective on the SGI and Intel machines, performance of \nthe generated code was very close to that of ATLAS-generated code. On the Sun, the model chose sub\u00adoptimal \nvalues for two parameters (tile size and Latency), and this led to a 20% difference in performance. It \nremains to be seen if more accurate modeling can eliminate this difference. Empirical optimization is \nused in FFTW and SPIRAL to choose an optimal algorithm from a suite of algorithms, and not just to choose \nvalues for transformation parameters as in ATLAS. It is an open question whether model-driven optimization \nis effective in this context as well. It would be interesting to see if empirical search can be speeded \nup by using modeling to bound the size of the search space. Perhaps the most important conclusion of \nthis work is that empiri\u00adcal search may not be necessary to generate high quality code, given the effectiveness \nof the model-driven approach. It is more difficult to determine what needs to be done to make compilers \nbridge the gap with library generators. Although all techniques used by ATLAS loop tiling, unrolling, \ninstruction scheduling etc. have been part of the compiler writer s lore for many years, we cannot claim \nthat it is easy to make compilers competitive with library generators. The developers of library generators \nknow in advance the code to be generated and therefore can hardwire the search space and performance \nequations into their systems. With today s technology, it is feasible to automate the identification \nof the search space, but the development of performance equations which were the main component of our \nstrategy is not well un\u00adderstood and cannot be automated at this time. There are, how\u00adever, some promising \nresults in this area [3]. A second perhaps equally important conclusion of this study is that there \nis still a significant gap in performance between the code generated by ATLAS and the BLAS routines. \nAlthough we do not understand the reason for this gap very well, it is clear that the problem of automating \nlibrary generation remains open. The high cost of library and application tuning makes this one of the \nmost important questions we face today. APPENDIX A In this appendix we present in pseudo-code an optimized \nmini-MMM code generated by ATLAS. The optimization parameter values chosen are MU=4, NU=2, KU=1; IFetch=NFetch=2; \nFFetch=1; Latency=2; and MulAdd=false. To produce a compact representation, we define some notation. \n There will be MU=4 temporary registers devoted to A (rAi). LAi will mean load ith such register from \nL1D$.  There will be NU=2 temporary registers devoted to B (rBj). LBj will mean load jt such register \nfrom L1D$.  There will be MU*NU temporary registers devoted to C (rCij). Sij will mean store register \nrCij into L1D$. *ij will mean: multiply rAi by rBj and store the value in a temporary register.  +ij \nwill mean: add the temporary value of the previous multi\u00adplication of rAi by rBj to rCij. We start with \nmini-MMM code similar to Figure 4: loop on N, step NU=2 loop on M, step MU=4 prefetch C; // FFetch=1 \nloop K, step KU=1 LA0; LB0; // IFetch=2 *00; LA1; LA2; // NFetch=2 *10; LA3; LB1; // NFetch=2 +00; \n*20; +10; *30; +20; *01; +30; *11; +01; *21; +11; *31; +21; +31; end K S00; S10; S20; S30; S01; S11; \nS21; S31; end M end N  Although this is not the final code ATLAS generates, from here we can make some \nimportant observations: As expected the main loop body contains MU*NU=4*2=8 multiplies and 8 corresponding \nadds. Each pair is Latency=2 FP instructions apart;  Fetch parameters work on a per K-iteration basis, \nissuing IFetch loads from rAi and rBj, followed by several groups of NFetch loads with computation in-between. \n The final transformation step ATLAS performs on the code is to software pipeline the K loop. The result \nlooks as follows: loop on N, step NU=2 loop on M, step MU=4 prefetch C; // FFetch=1 // start the software \npipeline LA0; LB0; // IFetch=2 *00; LA1; LA2; // NFetch=2 *10; LA3; LB1; // NFetch=2 loop K, step \nKU=1 // software pipeline pattern +00; *20; +10; *30; +20; *01; +30; *11; +01; *21; +11; *31; LA0; \nLB0; // IFetch=2 LA1; LA2; // NFetch=2 +21; *00; LA3; LB1; // NFetch=2 +31; *10; end K // drain \nthe software pipeline +00; *20; +10; *30; +20; *01; +30; *11; +01; *21; +11; *31; +21; +31; S00; S10; \nS20; S30; S01; S11; S21; S31; end M end N When Latency is large, ATLAS also tries to schedule the multi\u00adplies \nof the current iteration with the adds from the previous itera\u00adtion and the data loads of the next iteration, \neffectively spanning three original iterations in the pipeline pattern.  8. REFERENCES [1] R. Allan \nand K. Kennedy, Optimizing Compilers for Modern Architectures, Morgan Kaufman Publishing, 2002 [2] P. \nBoulet, A. Darte, T. Risset, and Y. Robert, Pen-ultimate tiling?, INTEGRATION, the VLSI Journal, Volume \n17, pages 33-51, 1994 [3] G. Cascaval and D. Padua, Estimating Cache Misses and Locality using Stack \nDistances. To appear in the Interna\u00adtional Conference on Supercomputing (ICS), 2003 [4] P. Clauss, Counting \nsolutions to linear and nonlinear con\u00adstraints through Ehrhart polynomials: Applications to ana\u00adlyze \nand transform scientific programs, In proceedings of the International Conference on Supercomputing (ICS), \n1996 [5] S. Coleman and K. S. McKinley. Tile size selection using cache organization and data layout. \nIn proceedings of Pro\u00adgramming Languages Design and Implementation (PLDI), 1995 [6] J. Dongarra and R. \nSchreiber, Automatic blocking of nested loops, Technical Report UT-CS-90-108, Department of Computer \nScience, University of Tennessee, May 1990 [7] B. B. Fraguela, R. Doallo, and E. Zapata: Automatic Analyti\u00adcal \nModeling for the Estimation of Cache Misses. In pro\u00adceedings of Parallel Architectures and Compilation \nTech\u00adniques (PACT), pages 221-231, 1999 [8] M. Frigo and S. G. Johnson. FFTW: An adaptive software architecture \nfor the FFT. In proceedings of the IEEE Inter\u00adnational Conference on Acoustics, Speech, and Signal Proc\u00adessing \n(ICASSP), volume 3, pages 1381-1384, 1998 [9] D. A. Patterson and J. L. Hennessy, Computer Architecture: \nA Quantitative Approach, 2 Edition, Morgan Kaufman, 1996 [10] T. Kisuki, P. M. W. Knijnenburg, M. F. \nP. O'Boyle, and H. A. G. Wijshoff . Iterative compilation in program optimi\u00adzation. In proceedings of \nCompilers for Parallel Computers, pages (CPC), pages 35-44, 2000 [11] T. Kisuki, P. M. W. Knijnenburg, \nM. F. P. O'Boyle, F. Bodin, and H. A. G. Wijshoff. A feasibility study in itera\u00adtive compilation. In \nproceedings of the International Sympo\u00adsium on High Performance Computing (ISHPC), 1999 [12] M. S. Lam, \nE. E. Rothberg, and M. E. Wolf. The cache per\u00adformance and optimizations of blocked algorithms. In pro\u00adceedings \nof the International Conference on Architectural Support for Programming Languages and Operating Systems \n(ASPLOS), pages 63-74, 1999 [13] R. L. Mattson, J. Gecsei, D. R. Slutz, and I. L. Traiger. Evaluation \ntechniques for storage hierarchies. IBM Systems Journal 9, 2, 78-117, 1970 [14] A. C. McKellar and E. \nG. Coffman. Organizing matrices and matrix operations for paged memory systems. Commu\u00adnications of the \nACM 12, 3, pages 153-165 [15] K. S. McKinley, S. Carr, and C. Tseng, Improving data lo\u00adcality with loop \ntransformations, In ACM Transactions on Programming Languages and Systems, 18(4):424-453, 1996 [16] J. \nJ. Navarro, T. Juan, and T. Lang, MOB forms: a class of multilevel block algorithms for dense linear \nalgebra opera\u00adtions, In proceedings of the International Conference on Supercomputing (ICS), pages 354-362, \n1994 [17] J. Ramanujam and P. Sadayappan, Tiling multidimensional iteration spaces for multicomputers, \nJournal of Parallel and Distributed Computing, 16(2):108-120, 1992 [18] R. Whaley and J. Dongarra. Automatically \nTuned Linear Algebra Software. Technical Report UT CS-97-366, LAPACK Working Note No.131, University \nof Tennessee, 1997 [19] M. Wolfe, Iteration space tiling for memory hierarchies, In SIAM Conference on \nParallel Processing for Scientific Computing, 1987 [20] M. E. Wolf, D. E. Maydan, and D. Chen, Combining \nloop transformations considering caches and scheduling, In pro\u00adceeding of the International Symposium \non Microarchitec\u00adture (MICRO 29), pages 274-286, 1996 [21] J. Xiong, J. Johnson, R. Johnson, and D. Padua, \nSPL: A Language and its Compiler for DSP Algorithms, In proceed\u00adings of Programming Languages Design \nand Implementation (PLDI), 2001 \n\t\t\t", "proc_id": "781131", "abstract": "Empirical program optimizers estimate the values of key optimization parameters by generating different program versions and running them on the actual hardware to determine which values give the best performance. In contrast, conventional compilers use models of programs and machines to choose these parameters. It is widely believed that model-driven optimization does not compete with empirical optimization, but few quantitative comparisons have been done to date. To make such a comparison, we replaced the empirical optimization engine in ATLAS (a system for generating a dense numerical linear algebra library called the BLAS) with a model-driven optimization engine that used detailed models to estimate values for optimization parameters, and then measured the relative performance of the two systems on three different hardware platforms. Our experiments show that model-driven optimization can be surprisingly effective, and can generate code whose performance is comparable to that of code generated by empirical optimizers for the BLAS.", "authors": [{"name": "Kamen Yotov", "author_profile_id": "81100239394", "affiliation": "Cornell University", "person_id": "PP30026642", "email_address": "", "orcid_id": ""}, {"name": "Xiaoming Li", "author_profile_id": "81492656485", "affiliation": "University of Illinois at Urbana-Champaign", "person_id": "PP14169255", "email_address": "", "orcid_id": ""}, {"name": "Gang Ren", "author_profile_id": "81100597816", "affiliation": "University of Illinois at Urbana-Champaign", "person_id": "P517409", "email_address": "", "orcid_id": ""}, {"name": "Michael Cibulskis", "author_profile_id": "81100626887", "affiliation": "University of Illinois at Urbana-Champaign", "person_id": "P517419", "email_address": "", "orcid_id": ""}, {"name": "Gerald DeJong", "author_profile_id": "81100619998", "affiliation": "University of Illinois at Urbana-Champaign", "person_id": "PP14213859", "email_address": "", "orcid_id": ""}, {"name": "Maria Garzaran", "author_profile_id": "81100525327", "affiliation": "University of Illinois at Urbana-Champaign", "person_id": "P517417", "email_address": "", "orcid_id": ""}, {"name": "David Padua", "author_profile_id": "81452612804", "affiliation": "University of Illinois at Urbana-Champaign", "person_id": "P63208", "email_address": "", "orcid_id": ""}, {"name": "Keshav Pingali", "author_profile_id": "81100554731", "affiliation": "Cornell University", "person_id": "PP39048331", "email_address": "", "orcid_id": ""}, {"name": "Paul Stodghill", "author_profile_id": "81100219663", "affiliation": "Cornell University", "person_id": "P222278", "email_address": "", "orcid_id": ""}, {"name": "Peng Wu", "author_profile_id": "81408599112", "affiliation": "IBM T.J. Watson Research Center", "person_id": "PP39052835", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/781131.781140", "year": "2003", "article_id": "781140", "conference": "PLDI", "title": "A comparison of empirical and model-driven optimization", "url": "http://dl.acm.org/citation.cfm?id=781140"}