{"article_publication_date": "05-09-2003", "fulltext": "\n Predicting Whole-Program Locality through Reuse Distance Analysis Chen Ding Yutao Zhong Computer Science \nDepartment University of Rochester Rochester, New York {cding,ytzhong}@cs.rochester.edu ABSTRACT Pro.ling \ncan accurately analyze program behavior for select data inputs. We show that pro.ling can also predict \nprogram locality for inputs other than pro.led ones. Here locality is de.ned by the dis\u00adtance of data \nreuse. Studying whole-program data reuse may reveal global patterns not apparent in short-distance reuses \nor local con\u00adtrol .ow. However, the analysis must meet two requirements to be useful. The .rst is ef.ciency. \nIt needs to analyze all accesses to all data elements in full-size benchmarks and to measure distance \nof any length and in any required precision. The second is predication. Based on a few training runs, \nit needs to classify patterns as regular and irregular and, for regular ones, it should predict their \n(chang\u00ading) behavior for other inputs. In this paper, we show that these goals are attainable through \nthree techniques: approximate analy\u00adsis of reuse distance (originally called LRU stack distance), pattern \nrecognition, and distance-based sampling. When tested on 15 inte\u00adger and .oating-point programs from \nSPEC and other benchmark suites, our techniques predict with on average 94% accuracy for data inputs \nup to hundreds times larger than the training inputs. Based on these results, the paper discusses possible \nuses of this analysis. Categories and Subject Descriptors D.3.4 [Programming Languages]: Processors optimization, \ncompilers General Terms Algorithms, Measurement, Languages  Keywords Program locality, reuse distance, \nstack distance, data locality, train\u00ading, sampling, pro.ling, pattern recognition and prediction Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI 03, June 9 \n11, 2003, San Diego, California, USA. Copyright 2003 ACM 1-58113-662-5/03/0006 ...$5.00. 1. INTRODUCTION \nCaching is widely used in many computer programs and systems, and cache performance increasingly determines \nsystem speed, cost, and energy usage. The effect of caching depends on program lo\u00adcality or the pattern \nof data reuse. Many applications may have a consistent recurrence pattern at the whole-program level, \nfor ex\u00adample, reusing a large amount of data across the time steps of an astronomical simulation, the \noptimization passes of a compiler, or the moves of a game-playing program. To exploit program local\u00adity, \nnew cache designs are adding more cache levels and dynamic con.guration control. As the memory hierarchy \nbecomes deeper and more adaptive, its performance will increasingly depend on our ability to predict \nwhole-program locality. The past work provides mainly three ways of locality analysis: by a compiler, \nwhich analyzes loop nests but is not as effective for dynamic control .ow and data indirection; by a \npro.ler, which an\u00adalyzes a program for select inputs but does not predict its behavior change in other \ninputs; or by run-time analysis, which cannot af\u00adford to analyze every access to every data. The inquiry \ncontinues for a prediction scheme that is ef.cient, accurate, and applicable to general-purpose programs. \nIn this paper, we predict locality in programs that have consis\u00adtent reuse patterns. Since different \nruns of the same program may use different data and go through different control .ow, our analy\u00adsis is \nnot based on program code nor its data but on a concept we call reuse distance. In a sequential execution, \nreuse distance is the number of distinct data elements accessed between two consecu\u00adtive references to \nthe same element. It measures the volume of the intervening data not the time between two accesses. While \ntime distance is unbounded in a long-running program, reuse distance is always bounded by the size of \nphysical data. In 1970, Mattson et al. studied stack algorithms in cache management and de.ned the concept \nof stack distance [30]. Reuse distance is the same as LRU stack distance or stack distance using LRU \n(Least Recently Used) replacement policy. In this paper, we use a different (and shorter) name to re.ect \nour purpose in program analysis, not cache manage\u00adment. We later show that reuse distance is measured \nmuch faster using a tree instead of a stack. Reuse distance is a powerful basis for pattern analysis \nfor three reasons. First, reuse distance is at most a linear function of program data size. The search \nspace is therefore much smaller for pattern recognition and prediction. Second, reuse distances reveal \ninvari\u00adance in program behavior. Most control .ow perturbs only short access sequences but not the cumulative \ndistance over millions of data. Long reuse distances suggest important data and signal ma\u00adjor phases \nof a program. For example, a distance equal to 50% of program data is likely to span a signi.cant program \nphase. Finally and most importantly for this work, reuse distance allows direct comparison of data behavior \nin different program runs. Distance\u00adbased correlation does not require two executions to have the same \ndata or execute the same function. Therefore, it can identify consis\u00adtent patterns in the presence of \ndynamic data allocation and input\u00addependent control .ow. This paper presents three new techniques. The \n.rst is approx\u00adimate reuse distance analysis, which bounds the relative error to arbitrarily close to \nzero. It takes O(log log M) time per access and O(log M) total space, where M is the size of program \ndata. The second is pattern recognition, which pro.les a few training runs and extracts regular patterns \nas a function of program data size. The last one, distance-based sampling, predicts reuse pattern for \nan unknown data input at run time by sampling at the beginning of the execution when needed. Together \nthese three techniques provide a general method that predicts locality patterns in whole or parts of \na program or its data. We should note two important limitations of this work. First, our goal is not \ncache analysis. Cache performance is not a direct measure of a program but a projection of a particular \nexecution on a particular cache con.guration. Our goal is program analysis. We .nd patterns consistent \nacross all data inputs. We analyze the reuses of data elements instead of cache blocks. The element-level \nbehavior is harder to analyze because it is not amortized by the size of cache blocks or memory pages \n(element miss rate is much higher than cache-block miss rate). We analyze the full distance, not its \ncomparison with .xed cache sizes. Per-element, full-length anal\u00adysis is most precise and demands highest \nef.ciency and accuracy. Program analysis, however, does not directly improve a program or a machine. \nIn Section 4, we discuss current and future uses of this analysis in compiler, .le system, and memory \nsystem design. We do not .nd all patterns in all programs. Not all programs have a consistent pattern, \nnor are all patterns predictable, let alone by our method. Our goal is to de.ne common recurrence patterns \nand measure their presence in representative programs. As depen\u00addence analysis analyzes loops that can \nbe analyzed, we predict pat\u00adterns that are predictable. We now show that, in many cases, reuse distance \ncan extend the scope of locality analysis to the whole pro\u00adgram.  2. REUSE PATTERN ANALYSIS This section \ndescribes the three components of reuse pattern anal\u00adysis: approximate reuse distance analysis, reuse \npattern recogni\u00adtion, and distance-based sampling. 2.1 Approximate reuse distance analysis In a distance \nanalysis, we view program execution as a sequence of accesses to data. Measuring reuse distance between \ntwo data accesses means counting the number of distinct data between them. In the worst case, the measurement \nneeds to examine all preceding accesses for each access in the trace. So a naive algorithm would need \nO(N2) time and O(N) space for a trace of length N. This cost is impractical for real programs, which \nhave up to hundreds of billions of memory accesses. The time and space costs can be improved, as shown \nby the ex\u00adample in Figure 1. Part (a) shows that we need to count accesses to distinct data. Part (b) \nshows that instead of storing the whole trace, we can store (and count) just the last access of each \ndata. Part (c) shows that we can organize the last-access time of all data in a search tree. The counting \ncan be done in a single tree search if we maintain the weight or the number of nodes in all sub-trees. \nFor a balanced tree, reuse-distance measurement takes O(log M) time per access and O(M) total space, \nwhere M is the size of program data. For a program with a large amount of data, however, the space requirement \nbecomes a limiting factor. Each data element needs a tree node, which stores the last-access time, pointers \nto its children, and the weight of its sub-tree. Since the tree data at least quadru\u00ad ple program data, \nthey easily over.ow physical memory and even the 32-bit address space for a program with more than 100 \nmillion data. To reduce the space cost, we introduce approximate analysis for long reuse distances. If \nthe length of a distance is in the order of millions, the accuracy of the last couple of digits rarely \nmatters. The main feature of our analysis is approximating a block of data in a tree node, as shown in \nPart (d) of Figure 1. The space requirement is reduced by a factor equal to the average block size. Using \na large block size, the approximate analysis can make its tree data small enough to .t in not only the \nphysical memory but also the processor cache. In our discussion, we do not consider the cost of .nding \nthe last access time. This requires a hashtable with one entry for each data. The space cost is O(M). \nHowever, Bennett and Kruskal showed that hashing can be done in a pre-pass, using blocked algorithms \nto reduce the memory requirement to arbitrarily low [6]. The time complexity of hashing is constant per \naccess, a well-studied prob\u00ad lem compared to distance measurement. In the rest of this paper, we will \nfocus our attention only on reuse distance measurement. We present two approximation algorithms, with \ndifferent guaran\u00ad tee on the accuracy of the measured distance, dmeasured, compared to the actual distance, \ndactual, as shown below. 1. bounded relative error e, 1 = e> 0 and dactual-dmeasured = e dactual 2. \nbounded absolute error B, B> 0 and dactual - dmeasured = B Both methods also guarantee dmeasured = dactual. \nThe rest of this section describes them in more detail. 2.1.1 Analysis with a bounded relative error \nThe analysis guarantees a bounded error rate that can be arbitrar\u00adily close to zero. Figure 2 shows the \nmain algorithm. Given the cur\u00adrent and last access time, the main routine uses TreeSearchDelete to search \nthe block tree and calculate reuse distance using sub-tree weights. Once the node containing the last \naccess time is found, the subroutine TreeSearchDelete updates the capacity of the node. The e ' new capacity \nis distance* 1-e . To simplify the notation, we use e e to represent 1-e . The value of distance is the \nnumber of distinct data accessed after this node. The subroutine uses distance as the approximate distance. \nThe approximation is never greater than the actual distance. The maximal relative error e happens when \nthe ac\u00ad '' tual distance is distance * (1 + e). The formula of eassumes 1 >e> 0. The algorithm is not \nvalid if e =0, which means no approximation. It is trivial to approximate if e =1: we simply report all \nreuse distance as 0. After capacity update, the subroutine TreeSearchDelete deletes the last access and \ninserts the current access. The tree insertion and deletion will rebalance the tree and update sub-tree \nweights. These two steps are not shown because they depend on the type of the tree being used, which \ncan be an AVL, red-black, splay, or B-tree. The most important part of the algorithm is dynamic tree \ncom\u00ad pression by subroutine TreeCompression. It scans tree nodes in re\u00ad verse time order, updates their \ncapacity as in TreeSearchDelete, and merges adjacent tree nodes when possible. The size of the merged \nnode must be no more than the smaller capacity of the two nodes; otherwise, the accuracy cannot be guaranteed. \nTree compression is triggered when the tree size exceeds 4 * log1+el M +4, where M is the number of accessed \ndata. It guarantees that the tree size is cut by at least a half, as proved by the following proposition. \ntime: 1 2 3 4 5 6 7 8 9 10 11 12 access: d a c b c c g e f a f b distance: | 5 distinct accesses | (a) \nAn example access sequence. The reuse distance between two b's is 5. tree nodeg (time ,weight) (7,7) \n  b a (4,3) c efd (11,1) (c) Organize last access times as a tree. Each node represents a distinct \nelement. Attribute time is its last access time, weight is the number of nodes in the subtree. The tree \nsearch for the first b finds the reuse distance, which is the number of nodes whose last access time \nis greater than 4. time: 1 2 3 4 5 6 7 8 9 10 11 12 access: d a c b c c g e f a f b /// / distance: \n| 5 last accesses | (b) Store and count only the last access of each data. tree node (time, weight, capacity, \nsize) e a (10,7,2,2) d b c g f (7,4,6,4) (11,1,1,1) (d) Use an approximation tree with 33% guranteed \naccuracy. Attributes capacity and size are the maximal and current number of distinct elements represented \nby a node, time is their last access time, and weight is the total size of subtree nodes. The approximate \ndistance of two b's is 3 or 60% of the actual distance. Figure 1: Reuse distance example PROPOSITION \n2.1. For a trace of N accesses to M data ele\u00adments, the approximate analysis with a bounded relative \nerror e (1 >e> 0) takes O(N log log M) time and O(log M) space, assuming it uses a balanced tree. PROOF. \nThe maximal tree size cannot exceed 4*log1+el M +4, or, O(log M), because of tree compression. Here e \n' = e .We 1-e now show that TreeCompression is guaranteed to reduce the tree size by at least a half. \nLet n0, n1, ..., and nt be the sequence of tree nodes in reverse time order. Consider each pair of nodes \naf\u00adter compression, n2i and n2i+1. Let sizei be the combined size of the two nodes. Let sumi-1 be the \ntotal size of nodes before n2i, that is sumi-1 = sizej. The new capacity of j=0,...,i-1 n2i.capacity \nis Lsumi-1 * e 'J. The combined size, sizei, must be at least n2i.capacity +1 and consequently no smaller \nthan sumi-1 * e ' ; otherwise the two nodes should have been com\u00adpressed. We have size0 = 1 and sizei \n= sumi-1 * e ' . By in\u00adduction, we have sumi = (1 + e ' )i or i = log1+el sumi.For a tree holding M data \nin Tcompressed tree nodes after compres\u00adsion, we have i = LTcompressed/2J and sumi = M. Therefore, Tcompressed \n= 2 * log1+el M +2. In other words, each compres\u00adsion call must reduce tree size by at least a half. \nNow we consider the time cost. Assume that the tree is balanced and its size is T . The time for tree \nsearch, deletion, and insertion is O(log T ) per access. Tree compression happens periodically after \na tree growth of at least 2 * log1+el M +2 or T/2 tree nodes. Since at most one tree node is added for \neach access, the number of accesses between successive tree compressions is at least T/2 accesses. Each \ncompression takes O(T ) time because it examines each node in a constant time, and the tree construction \nfrom an ordered list takes O(T ). Hence the amortized compression cost is O(1) for each access. The total \ntime is therefore O(log T +1),or O(log log M) per access.   2.1.2 Analysis with a bounded absolute \nerror For a cut-off distance C and a constant error bound B, the sec\u00adond approximation algorithm guarantees \nprecise measurement of distance shorter than C and approximate measurement of longer distances with a \nbounded error B. It keeps the access trace in two parts. The precise trace keeps the last accessed C \nelements. The approximate trace stores the remaining data in a tree with tree nodes having capacity B. \nPeriodically, the algorithm transfers data from the precise trace to the approximate trace. Our earlier \npa\u00adper describes a detailed algorithm and its implementation using a B-Tree in both precise and approximate \ntraces [44]. In this work, we generalize the previous algorithm. In addition to using B-Tree, the precise \ntrace can use a list, a vector, or any type of trees, and the approximate trace can use any type of trees, \nas long as two minimal requirements are met. First, the size of precise trace is bounded by a constant. \nSecond, the minimal occupancy of the approximate tree is guaranteed. Invoking a transfer when the precise \ntrace exceeds a pre-set size can satisfy the .rst requirement. For the second requirement, we dynamically \nmerge a tree node with any of its neighbors when the combined size is no more than B. The merge operation \nguarantees at least half utilization of the tree capacity. Therefore, the maximal size of the approximate \ntree is 2M . B We implemented a splay tree [37] version of the algorithm in this work. We will use only \nthe approximate trace (the size of precise trace is set to 0) in distance-based sampling because it runs \nfastest among all analyzers, as shown in Section 3. 2.1.3 Comparison The past 30 years have seen a steady \nstream of work in measur\u00ading reuse distance. We categorize previous methods by their orga\u00adnization of \nthe data access trace. The .rst three rows of Table 1 show methods using a list, a vector, and a tree. \nIn 1970, Mattson et al. published the .rst measurement algorithm [30]. They used a list-based stack. \nBennett and Kruskal showed that a stack was too slow to measure long reuse distances. They used a vector \nand built an m-ary tree on it [6]. They also showed how to use blocked hash\u00ading in a pre-pass. In 1981, \nOlken implemented the .rst tree-based method using an AVL tree [34]. Olken also showed how to com\u00adpress \nthe trace vector in Bennett and Kruskal s method and improve the time and space ef.ciency to those of \ntree-based algorithms. In 1994, Sugumar and Abraham showed that a splay tree [37] has bet\u00adter memory \nperformance [41]. Their analyzer, Cheetah, is widely data declarations TreeNode = structure(time, weight, \ncapacity, size, left, right, prev) root: the root of the tree representing the trace e: the upper bound \nto the error rate algorithm ReuseDistance(last, current) // inputs are the last and current access time \n1. T reeSearchDelete(last, distance) 2. new = T reeNode(current, 1, 1, 1, ., ., .) T reeInsert(new) \n 3. if (tree size = 4 * log1+e root.weight +4) T reeCompression(new)  Assert(compression more than halves \nthe tree) end if 4. return distance end algorithm subroutine T raceSearchDelete(time, distance) // time \nis last access time of the current data. // distance will be returned. node = root; distance =0 while \ntrue node.weight = node.weight - 1 if (time < node.time and node.prev exists and time = node.prev.time) \nif (node.right exists) distance = distance + node.right.weight if (node.left not exists) break distance \n= distance + node.size node = node.left else if (time > node.time) if (node.right not exists) break node \n= node.right else break end if end while node.capacity = max(distance * e , 1) 1-e node.size = node.size \n- 1 return distance end subroutine T reeSearchDelete subroutine T reeCompression(n) // n is the last \nnode in the trace distance =0 n.capacity =1 while (n.prev exist) if (n.prev.size + n.size = n.capacity) \n// merge n.prev into n n.size = n.size + n.prev.size n.prev = n.prev.prev deallocate n.prev else distance \n= distance + n.size n = n.prev e n.capacity = max(distance * 1-e , 1) end if end while Build a balanced \ntree from the list and return the root end subroutine T reeCompression Figure 2: Approximate analysis \nwith a bounded relative error Analysis methods Time Space trace as a stack (or list) [30] O(NM) O(M) \ntrace as a vector [6, 2] O(N log N) O(N) trace as a tree [34, 41, 2] O(N log M) O(M) list-based aggregation \n[25] O(NS) O(M) block tree [44] O(N log M B ) O( M B ) dynamic tree compression O(N log log M) O(log \nM) N is the length of execution, M is the size of program data Table 1: Asymptotic complexity of measuring \nfull distance available from the SimpleScalar tool set. Recently, Almasi et al. gave an algorithm that \nrecords the empty regions instead of non\u00adempty cells in the trace. Although the asymptotic complexity \nre\u00admains the same, the actual cost of trace maintenance is reduced by 20% to 40% in vector and tree based \ntraces. They found that the modi.ed Bennett and Kruskal method was much faster than meth\u00adods using AVL \nand red-black trees [2]. Kim et al. gave the .rst imprecise (but accurate) analysis method in 1991 [25]. \nTheir method stores program data in a list, marks S ranges in the list, and counts the number of distances \nfell inside each range. The time cost per access is proportional to the number of markers smaller than \nthe reuse distance. The space cost is O(C), where C is the furthest marker. The method is ef.cient if \nS and C are bounded and not too large. It is not suitable for measuring the full length of reuse distance, \nwhere S and C need to be propor\u00adtional to M. Unlike approximate analysis, this method is accurate in \ncounting the reuse distance within a marked range. In comparison, approximation methods, shown in the \nlast two rows in Table 1, trade accuracy for ef.ciency especially space ef\u00ad.ciency. They can analyze \nlarger data and longer reuse distances. They are adjustable because the cost is proportional to accuracy. \nThe analysis with bounded relative error has the lowest asymptotic space and time cost, for any error \nrate that is greater than (and can be arbitrarily close to) zero. Reuse distance is no longer a favorable \nmetric in low-level cache design because it cannot model the interaction between cache and CPU such as \ntiming. However, at the high level, reuse distance de\u00adtermines the number of capacity misses for all \ncache sizes. Earlier work has also extended it to analyze interference in various types of set-associative \ncache [22, 30]. Section 4 will discuss the uses of reuse distance analysis in cache optimization.  \n 2.2 Pattern recognition Pattern recognition detects whether the recurrence pattern is pre\u00addictable across \ndifferent data inputs. Example recurrence patterns at the whole-program level include ocean simulation \nin a series of time steps, compilation of a collection of .les, and computer chess\u00adplaying in a number \nof moves. Based on two or more training runs, pattern recognition constructs a parameterized reuse pattern. \nThe main parameter is the size of data involved in program recurrences. This is not the same as the size \nof data touched by a program. The next section will show how to obtain an estimate of this number through \ndistance-based sampling. In this section, we assume it ex\u00adists and refer to it indistinctively as program \ndata size. We de.ne the reuse, recurrence or locality pattern as a histogram showing the percentage of \nmemory accesses whose reuse distance falls inside consecutive ranges divided between 0 and the data size \n(maximal distance). We will use ranges of both logarithmic and linear sizes. Our approach is not speci.c \nto a particular accuracy of the histogram. We now describe the three steps of pattern recogni\u00adtion. 2.2.1 \nCollecting reference histograms A reference histogram is a transpose of the reuse distance his\u00adtogram. \nIt sorts all memory accesses based on their reuse distance and shows the average distance of each k percent \nof memory ref\u00aderences. For example, when k is 1, the reference histogram .rst gives the average distance \nfor 1% shortest reuse distances, then the average for the next 1% shortest reuse distances, and so on. \nWe use the reference histogram for two purposes. First, we iso\u00adlate the effect of non-recurrent parts \nof the program. Some instruc\u00adtions are executed per execution; some are repeated per program data. When \nthe data size becomes suf.ciently large, the effect of the former group diminishes into at most a single \nbin of the his\u00adtogram. Second, the size of the bin controls the granularity of prediction. A bin size \nof 1% means that we do not predict .ner distribution of distances within 1% of memory references. A reference \nhistogram is computed from a reuse-distance his\u00adtogram by traversing the latter and calculating the average \ndistance for each k% of memory references. Getting a precise histogram incurs a high space cost. We again \nuse approximation since we do not measure precise distances anyway. In the experiment, we collect reuse-distance \nhistogram using log-linear scale bins. The size of bins is a power of 2 up to 1024 and then it is 2048 \nfor each bin. To improve precision, we calculate the average distance within each bin and use the average \ndistance as the distance of all refer\u00adences in the bin when converting it to the reference histogram. \nThe cost and accuracy of the approximation scheme can be adjusted by simply changing the size of bins \nin both types of histograms. 2.2.2 Recognizing patterns Given two reference histograms from two different \ndata inputs ( we call them training inputs), we construct a formula for each bin. Let d1i be the distance \nof the ith bin in the .rst histogram, d2i be the distance of the ith bin in the second histogram, s1 \nbe the data size of the .rst training input, and s2 the data size of the second input. We use linear \n.tting to .nd the closest linear function that maps data size to reuse distance. Speci.cally, we .nd \nthe two coef.cients, ci and ei, that satisfy the following two equations. d1i = ci + ei * fi(s1) d2i \n= ci + ei * fi(s2) Assuming the function fi is known, the two coef.cients uniquely determine the distance \nfor any other data size. The formula there\u00adfore de.nes the reuse-distance pattern for memory accesses \nin the bin. The overall pattern is the aggregation of all bins. The pattern is more accurate if more \ntraining pro.les are collected and used in linear .tting. The minimal number of training inputs is two. \nIn a program, the largest reuse distance cannot exceed the size of program data. Therefore, the function \nfi can be at most linear, not a general polynomial function. In this work, we consider the fol\u00adlowing \nchoices of fi. The .rst is the function pconst(x)=0.We call it a constant pattern because reuse distance \ndoes not change with data size. The second is plinear(x)= x. We call it a lin\u00adear pattern. Constant and \nlinear are the lower and upper bound of the reuse distance changes. Between them are sub-linear patterns, \n1/21/3 for which we consider three: p1/2(x)= x , p1/3(x)= x , and p2/3(x)= x 2/3 . The .rst happens in \ntwo-dimensional prob\u00adlems such as matrix computation. The other two happen in three\u00addimensional problems \nsuch as ocean simulation. We could consider higher dimensional problems in the same way, although we \ndid not .nd a need in our test programs. For each bin of the two reference histograms, we calculate the \nratio of their average distance, d1i/d2i, and pick fi to be the pattern function, pt, such that pt(s1)/pt(s2) \nis closest to d1i/d2i. Here t is one of the patterns described in the preceding paragraph. We take care \nnot to mix sub-linear patterns from a different number of dimensions. In our experiments, the dimension \nof the problems was given as an input to the analyzer. This can be automated by trying all dimension \nchoices and using the best overall .t. 2.2.3 Limitations Although the analysis can handle any sequential \nprogram, the generality comes with several limitations. The pro.ling inputs should be large enough to \nfactor out the effect of non-recurring accesses. The smallest input we use in our experiment has four \nmillion memory accesses. For linear and sub-linear patterns, our analysis needs inputs of different data \nsizes. The difference should be large enough to separate pattern functions from each other. For high-dimensional \ndata, pattern prediction requires that different in\u00adputs have a similar shape, in other words, their \nsize needs to be proportional or close to proportional in all dimensions. Otherwise, a user has to train \nthe analyzer for each shape. In our future work, we will combine the pattern analyzer with a compiler \nto predict for all shapes. All high-dimensional data we have seen come from sci\u00adenti.c programs, for \nwhich a compiler can collect high-level infor\u00admation. Finally, predicting reuse pattern does not mean \npredicting execution time. The prediction gives the percentage distribution but not the total number \nof memory accesses, just as loop analysis can know the dependence but not the total number of loop iterations. \nOnce the pattern is recognized from training inputs, we can pre\u00addict constant patterns in another input \nstatically. For other patterns, we need the data size of the other input, for which we use distance\u00adbased \nsampling.  2.3 Distance-based sampling The purpose of data sampling is to estimate data size in a pro\u00adgram \nexecution. For on-line pattern prediction, the sampler cre\u00adates a twin copy of the program and instruments \nit to generate data access trace. When the program starts to execute, the sampling version starts to \nrun in parallel until it .nds an estimate of data size. Independent sampling requires that the input \nof the program be replicated, and that the sampling run do not produce side effects. The sampling is \ndistance-based. It uses the reuse distance ana\u00adlyzer and monitors each measured distance. If a distance \nis greater than a threshold, the accessed memory location is taken as a data sample. The sampler collects \nmore data samples in the same way except that it requires data samples to have between each other a spatial \ndistance of a fraction of the .rst above-threshold distance. The sampler records above-threshold reuse \ndistances to all data samples. We call them time samples. Given the sequence of time samples of a data \nsample, the sampler .nds peaks, which are time samples whose height (reuse distance) is greater than \nthat of its preceding and succeeding time samples. The sampler runs until seeing the .rst k peaks of \nat least m data samples. It then takes the appropriate peak as the data size. The peak does not have \nto be the actual data size. It just needs to be proportional to the data size in different inputs. We \nuse the same sampling scheme to determine data size in both training and predic\u00adtion runs. For most programs \nwe tested, it is suf.cient to take the .rst peak of the .rst two data samples. An exception is Apsi. \nAll its runs initialize the same amount of data as required by the largest input size, but smaller inputs \nuse only a fraction of the data in the computation. We then use the second peak as the program data size. \nMore complex cases happen when early peaks do not show a consistent relation with data size, or the highest \npeak appears at the end of a program. We identify these cases during pattern recogni\u00adtion and instruct \nthe predictor to predict only the constant pattern. The sampling can be improved by more intelligent \npeak .nding. Speed and scalability For example, we require the peak and the trough differ by a certain \nfactor, or use a moving average to remove noises. The literature on statistics and time series is a rich \nresource for sample analysis. For pattern prediction, however, we do not .nd a need for sophis\u00ad ticated \nmethods yet because the (data-size) peak is either readily recognizable at the beginning or it is not \nwell de.ned at all. The cost of distance-based sampling is signi.cant since it needs to measure reuse \ndistance of every memory reference until peaks are found. The analysis does not slow the program down \nsince it uses a separate copy. It only lengthens the time taken to make a prediction. For minimal delay, \nit uses the fastest approximation an\u00ad data size (reuse distance=1% data) alyzer. It can also use selective \ninstrumentation and monitor only distinct memory references to global and dynamic data [19]. For long-running \nprograms, this one-time cost is insigni.cant. In ad- Accuracy dition, many programs have majority of \nmemory references reused in constant patterns, which we predict without run-time sampling. Another use \nof distance-based sampling is to detect phases in a program. For this purpose, we continue sampling through \nthe entire execution. Time segments between consecutive peaks are phases. A temporal graph of time samples \nshows recurrent accesses in time order and the length and shape of each recurrence. The evaluation section \nwill use phase graphs to understand the results of pattern prediction. % memory references 3 2.5 2 1.5 \n1 0.5 Finding the .rst few peaks of the .rst few data samplings is an unusual heuristic because it is \nnot based on keeping track of a par\u00adticular program instruction or a particular data item. The peaks \nfound by sampling in different program executions do not have to be caused by the same memory access \nto the same data. Very likely they are not. In programs with input-dependent control .ow, one cannot \nguarantee the execution of a function or the existence of a dynamic data item. Distance-based sampling \nallows correlation across data inputs without relying on any pre-assumed knowledge about program code \nor its data.  3. EVALUATION 3.1 Reuse distance measurement Figure 3 compares the speed and accuracy \nfor eight analyzers, which we have described in Section 2.1. BK-2, BK-16, and BK-256 are Bennett and \nKruskal s k-ary tree analyzers with k equal to 2, 16, and 256 [6]. KHW is list-based aggregation with \nthree markers at distance 32, 16K, and the size of analyzed data [25]. We re\u00adimplemented it since the \noriginal no longer exists. Cheetah uses a splay-tree, written by Sugumar [41]. ZDK-2k and Sampling are \napproximate analysis with the error bound B = 2048, as described in Section 2.1.2. ZDK-2k uses a B-tree \nand a mixed trace [44]. Sampling uses a splay tree and only the approximate trace. 99% is the analysis \nwith the bounded relative error e =1%. The input program traverses M data twice with a reuse distance \nof M/100. To measure only the cost of reuse-distance analysis, the hashing step was bypassed by pre-computing \nthe last access time (except for KHW, where hashing was inherent). We did not separately measure the \ncost of hashing since we did not implement blocked hashing [6]. The timing was collected on a 1.7 GHz \nPentium 4 PC with 800 MB of main memory. The programs were compiled with gcc with optimization .ag -O3. \nCompared to accurate methods, approximate analysis is faster and more scalable with data size and distance \nlength. The vector\u00adbased methods have the lowest speed. KHW with three markers is fastest (7.4 million \nmemory references per second) for small and medium distances but is not suited for measuring very long \nreuse distances. Cheetah achieves an initial speed of 4 million 0 55K 57K 59K 61K 63K 66K reuse distance \nFigure 3: Comparison of analyzers memory references per second. All accurate analyses run out of physical \nmemory at 100 million data. Sampling has the highest speed, around 7 million memory references per second, \nfor large data sizes. ZDK-2k runs at a speed from 6.7 million references per second for 100 thousand \ndata to 2.9 million references per second for 1 billion data. Sampling and ZDK-2k do not analyze beyond \n4 billion data since they use 32-bit integers. The most scalable performance is obtained by the analyzer \nwith 99% accuracy (e =1%), shown by the line marked 99%. We use 64-bit integers in the program and test \nit for up to 1 trillion data. The asymptotic cost is O(log log M) per access. In the experi\u00adment, the \nanalyzer runs at an almost constant speed of 1.2 million references per second from 100 thousand to 1 \ntrillion data. The consistent high speed is remarkable considering that the data size and reuse distance \ndiffers by eight orders of magnitude. The speed is so predictable that when we .rst ran 1 trillion data \ntest, we esti\u00admated that it would .nish in 19.5 days: It .nished half a day later, a satisfying moment \nconsidering that prediction is the spirit of this work. If we consider an analogy to physical distance, \nthe precise methods measure the distance in miles of data, the approximation method measures light years. \nThe lower graph of Figure 3 compares the accuracy of approx\u00adimation on a partial histogram of FFT. The \ny-axis shows the per\u00adcentage of memory references, and the x-axis shows the distance in a linear scale \nbetween 55 thousand and 66 thousand with an in\u00adcrement of 2048. 99.9% and 99% approximation (e =0.1% \nand e =1% respectively), shown by the second and third bar, closely match the accurate distance. Their \noverall error is about 0.2% and 2% respectively. The bounded absolute error with a bound 2048, shown \nby the last bar, has a large misclassi.cation near the end, although the error is no more than 4% of \nthe actual distance. In terms of the space overhead, accurate analyzers need 67 thousand tree or list \nnodes, ZDK-2k needs 2080 tree nodes, 99% needs 823, and 99.9% needs 5869. The analysis accuracy is adjustable, \nso is the cost.  3.2 Pattern prediction Figure 4 shows the result of pattern prediction for Lucas from \nSpec2K, which is representative in our test suite. The graph has four groups of bars. The .rst two are \nfor two training inputs. Their data access traces are feed into our analyzer for pattern recognition. \nThe third input is the target of prediction. The analyzer samples 0.4% of its execution, .nds the data \nsize, and predicts the reuse distance histogram shown by the third bar. The prediction matches closely \nwith the measured histogram shown by the fourth bar. The two histograms overlap by 95%. The accuracy \nis remarkable con\u00adsidering that the target execution has 500 times more data and 300 times more data \naccesses than the training runs. The correct pre\u00addiction of the peaks on the far side of the histograms \nis especially telling because they differ from the peaks of the training inputs not only in position \nbut also in shape and height. Table 2 shows the effect of pattern prediction on 15 benchmarks, including \n7 .oating-point programs and 6 integer programs from SPEC95 and SPEC2K benchmark suites, and 2 additional \nprograms, SP from NASA and a two-dimensional FFT kernel. We reduce the number of iterations in a program \nif it does not affect the overall pattern. We compile the tested programs with DEC compiler with the \ndefault compiler optimization (-O3). Different compiler opti\u00admization levels may change the reuse pattern \nbut not the accuracy of our prediction. We use Atom [39] to instrument the binary code to collect the \naddress of all loads and stores and feed them to our analyzer, which treats each distinct memory address \nas a data ele\u00adment. Column 1 and 2 of the table in Figure 2 give the name and a short description of \nthe test programs. The programs are listed in the decreasing order of the average reuse distance. Their \ndata in\u00adputs are listed by the decreasing order of the data size. For each input, Column 5 shows the \ndata size or the number of distinct data, and Column 6 and 7 give the number of data reuses and average \nreuse distance normalized by the data size. The programs have up to 36 million data, 130 billion memory \nreferences, and 5 million average reuse distance. The table shows that these are a diverse set of programs: \nno two programs are similar in data size or execution length. Although not shown in the table, the programs \nhave differ\u00adent reuse distance histograms (even though the average distance is a similar fraction of \nthe data size in a few programs). In addition, the maximal reuse distance is very close to the data size \nin each program run. The third column lists the patterns in benchmark programs, which can be constant, \nlinear, or sub-linear. Sub-linear patterns include 2nd root (x 1/2) and 3rd roots (x 1/3 and x 2/3). \nFloating-point pro\u00adgrams generally have more patterns than integer programs. The prediction accuracy \nis shown by the second to the last col\u00adumn of the table. Let xi and yi be the size of ith bar in predicted \nand measured histograms. The cumulative difference, E, is the sum of |yi - xi| for all i. In the worst \ncase, E is 200%. We use 1 - E/2 as the accuracy. It measures the overlap between the two histograms, \nranging from 0% or no match to 100% or complete match. The accuracy of Lucas is 95%, shown in Figure \n4. We use three different input sizes for all programs except for Gcc. Based on two smaller inputs, we \npredict the largest input. We call this forward prediction. The prediction also works back\u00adwards: based \non the smallest and the largest inputs, we predict the middle one. In fact, the prediction works for \nany data input over a reasonable size. The table shows that both forward and back\u00adward predictions are \nvery accurate. Backward prediction is gener\u00adally better except for Lucas because the largest input is \nabout 500 times larger than the middle input and for Li because only the constant pattern is considered \nby prediction. Among all prediction results, the highest accuracy is 98.7% for the train input of Gcc, \nthe lowest is 81.8% for the train input of Lucas. The average accuracy is 93.7%. The last column shows \nthe prediction coverage. The coverage is 100% for programs with only constant patterns because they need \nno sampling. For others, the coverage starts after the data-size peak is found in the execution trace. \nLet N be the length of ex\u00adecution trace, P be the logical time of the peak, then the coverage is 1 - \nP/N. For programs using a reduced number of iterations, N is scaled up to be the length of full execution. \nTo be consistent with other SPEC programs, we let SP and FFT to have the same num\u00adber of iterations as \nTomcatv. Data sampling uses the .rst peak of the .rst two data samples for all programs with non-constant \npatterns except for Compress and Li. Compress needs 12 data samples. It is predictable only because it \nrepeats compression multiple times, an unlikely case in real uses. Li has random peaks that cannot be \ncon\u00adsistently sampled. We predict Li based on only the constant pattern. The average coverage is 98.8%. \nThe reported coverage is for predicting simulation results. In\u00adstead of measuring reuse distance for \nthe whole program, we can predict it by sampling on average 1.2% of the execution. To predict a running \nprogram, the coverage is smaller because the sampled version of a program runs much slower than the program \nwithout sampling. Our fastest analyzer causes a slowdown by factors rang\u00ading from 20 to 100. For a slowdown \nof 100, we need coverage of at least 99% to .nish prediction before the end of the execution! For\u00adtunately, \nthe low coverage happens only in Compress and the train input of Swim. Without them, the average coverage \nis 99.88%, indi\u00adcating a time coverage over 88%. Even without a fast sampler, the prediction is still \nuseful for long running programs and programs with mainly constant patterns. Six programs or 40% of our \ntest suite do not need sampling at all. Most inputs are test, train, and reference inputs from SPEC. \nFor GCC, we pick the largest and two random ones from the 50 input .les in its ref directory. SP and \nFFT do not come from SPEC, so we randomly pick their input sizes (FFT needs a power of two matrix). We \nchange a few inputs for SPEC programs, shown in Column 4. Tomcatv and Swim has only two different data \nsizes. We add in more inputs. All inputs of Hydro2d have a similar data size, but we do not make any \nchange. The test input of Twolf has 26 cells and is too small. We randomly remove half of the cells in \nits train data set to produce a larger test input. Finally, Apsi uses different-shape inputs of high-dimensional \ndata, which our current predictor cannot accurately predict. We change the shape of its largest input. \n 3.2.1 Comparisons Most pro.ling methods use the result from training runs as the prediction for other \nruns. An early study by Wood measured the accuracy of this scheme in .nding the most frequently accessed \nvariables and executed control structures in a set of dynamic pro\u00adgrams [43]. We call this scheme constant \nprediction, which in our case uses the reuse-distance histogram of a training run as the pre\u00addiction \nfor other runs. For programs with only constant patterns, constant prediction is the same as our method. \nFor the other 11 programs, the worst-case accuracy is the size of the constant pat\u00adtern, which is 57% \non average. The largest is 84% in Twolf, and the smallest 28% in Apsi. The accuracy can be higher if \nthe lin\u00ad Benchmarks Description Patterns Inputs Data elements Avg. reuses per element Avg. dist. data \nsize Accura\u00adcy(%) Cover\u00adage(%) Lucas (Spec2K) Lucas-Lehmer test for primality const linear ref train \ntest 20.8M 41.5K 6.47K 621 971 619 2.49E-1 2.66E-1 2.17E-1 95.1 81.8 99.6 100 Applu (Spec2K) solution \nof .ve coupled nonlinear PDE s const 3rd roots linear ref(603) train(243) test(123) 22.8M 1.29M 128K \n185 178 176 1.37E-1 1.37E-1 1.33E-1 83.6 94.7 99.6 99.4 Swim (Spec95) .nite difference approximations \nfor shallow water equation const 2nd root linear ref(5122) 4002 2002 3.68M 2.26M 572K 46.9 46.6 46.2 \n2.47E-1 2.47E-1 2.47E-1 99.0 99.3 99.9 88.1 SP (NAS) computational .uid dynamics (CFD) simulation const \n3rd roots linear 503 323 283 4.80M 1.26M 850K 132 124 125 1.05E-1 1.01E-1 9.78E-2 90.3 95.8 99.9 99.9 \nTomcatv (Spec95) vectorized mesh generation const 2nd root linear ref(5132) 4002 train(2572) 1.83M 1.12M \n460K 208 104 104 1.71E-1 1.67E-1 1.67E-1 92.4 99.2 99.5 99.3 Hydro2d (Spec95) hydrodynamical equations \ncomputing galactical jets const ref train test 1.10M 1.10M 1.10M 13.4K 1.35K 139 2.23E-1 2.23E-1 2.20E-1 \n98.5 98.4 100 100 FFT fast Fourier transformation const 2nd root linear 5122 2562 1282 1.05M 265K 66.8K \n72.9 69.0 61.4 6.41E-2 6.76E-2 7.60E-2 84.3 94.0 99.7 99.6 Mgrid (Spec95) multi-grid solver in 3D potential \n.eld const 3rd roots linear ref(643) test(643) train(323) 956K 956K 132K 35.6K 1.42K 32.4K 6.81E-2 6.76E-2 \n7.15E-2 96.4 96.5 100 99.3 Apsi (Spec2K) pollutant distribution for weather prediction const 3rd roots \nlinear 128x1x128 train(128x1x64) test(128x1x32) 25.0M 25.0M 25.0M 6.35 146 73.6 1.60E-3 2.86E-4 1.65E-4 \n91.6 92.5 97.8 99.1 Compress (Spec95) an in-memory version of the common UNIX compression utility const \nlinear ref train test 36.1M 279K 142K 628 314 147 4.06E-2 6.31E-2 9.73E-2 85.9 92.3 92.2 86.9 Twolf (Spec2K) \ncircuit placement and global routing, using simulated annealing const linear ref(1888-cell) train(752-cell) \n370-cell 734K 402K 227K 177K 111K 8.41K 2.08E-2 1.82E-2 1.87E-2 94.2 96.6 100 100 Vortex (Spec95) (Spec2K) \nan object oriented database const ref test train 7.78M 2.58M 501K 4.60K 530 71.3K 4.31E-4 3.25E-4 4.51E-4 \n95.1 97.2 100 100 Gcc (Spec95) based on the GNU C compiler version 2.5.3 const expr cp-decl explow train(amptjp) \ntest(cccp) 711K 705K 321K 467K 456K 137 190 68.3 221 233 2.75E-3 2.65E-3 3.69E-3 3.08E-3 3.25E-3 98.2 \n98.6 96.1 98.7 100 100 100 100 Li (Spec95) Xlisp interpreter const linear ref train test 87.9K 44.2K \n14.5K 328K 1.86K 37.0K 2.19E-2 3.11E-2 2.56E-2 82.7 86.0 100 100 Go (Spec95) an internationally ranked \ngo-playing program const ref test train 109K 104K 86.1K 124K 64.6K 2.68K 3.78E-3 3.78E-3 2.02E-3 96.5 \n96.9 100 100 average 93.7 98.8 Table 2: Prediction accuracy and coverage for 15 programs  reuse distance \nFigure 4: Pattern prediction for Spec2K/Lucas ear and sub-linear patterns overlap in training and target \nruns. It is also possible that the linear pattern of a training run overlaps with a sub-linear pattern \nof the target run. However, the latter two cases are not guaranteed; in fact, they are guaranteed not \nto happen for certain target runs. The last case is a faulty match since those ac\u00adcesses have different \nlocality. For several programs, the average reuse distance is of a similar fraction of the data size \n1. For example in Swim, the average dis\u00adtance is 25% of the data size in all three runs. This suggests \nthat we can predict the average reuse distance of other runs by the data size times 25%. This prediction \nscheme is in fact quite accurate for programs with a linear pattern (although not for other programs). \nWhen the data size is suf.ciently large, the total distance will be dominated by the contribution from \nthe linear pattern. The average distance is basically the size of the linear pattern, which in Swim is \n25% of all references. This scheme, however, cannot predict the overall distribution of reuse distance. \nIt also needs to know the input data size from distance-based sampling. The total data size is not always \nappropriate. For example, Apsi touches the same amount of data regardless of the size of the data input. \nAs a reality check, we compare with the accuracy of random prediction. If a random distribution matches \nthe target distribution equally well, our method would not be very good. A distribution is an n-element \nvector, where each element is a non-negative real number and they sum to 1. Assuming any distribution \nis equally likely, the probability of a random prediction has an error a or less is equal to the number \nof distributions that are within a error to the target distribution divided by the total number of possible \ndistribu\u00adtions. We calculate this probability using n-dimensional geometry. The total number of such \nvectors is equal to the surface volume on a corner cut of an n-dimensional unit-size hypercube. The number \nof distributions that differ by a with a given distribution equals to the surface volume of a perpendicular \ncut through 2n-1 corner cuts of an a-size hypercube. The probability that a random prediction 1The observation \ncame from two anonymous reviewers of PLDI 03 yields at least 1 - a accuracy is an-1, the ratio of the \nlatter vol\u00adume to the former. For the program Lucas shown in Figure 4, n is 26 and the probability of \na random prediction achieving over 95% accuracy is 0.0525 or statistically impossible. 3.2.2 A case study \nThe program Gcc compiles C functions from an input .le. It has dynamic data allocation and input-dependent \ncontrol .ow. A closer look at Gcc helps to understand the strength and limitation of our approach. We \nsample the entire execution of three inputs, Spec95/Gcc compiling cccp.iand amptjp.iand Spec2K/Gcc compiling \n166.i. The three graphs in Figure 5 show the time samples of one data sample. Other data samples produce \nsimilar graphs. The upper two graphs, cccp.iand amptjp.i, link time samples in vertical steps, where \nthe starting point of each horizontal line is a time sample. The time samples for 166.i are shown directly \nin the bottom graph. The two upper graphs show many peaks, related to 100 func\u00adtions in the 6383-line \ncccp.iand 129 functions in the 7088-line amptjp.i. Although the size and location of each peak appear \nrandom, their overall distribution is 96% to 98% identical between them and to three other input .les \n(shown previously in Table 2). The consistent pattern seems to come from the consistency in pro\u00adgrammers \ncoding, for example, the distribution of function sizes. Our analyzer is able to detect such consistency \nin logically unre\u00adlated recurrences. On the other hand, our prediction is incorrect if the input is unusual. \nFor example for 166.i, Gcc spends most of its time on two functions consisting of thousands lines of \ncode. They dominate the recurrence pattern, as shown by the lower graph in Figure 5. Note the two orders \nof magnitude difference in the range of x-and y-axes. Our method cannot predict such unusual pattern. \nOur analyzer is also able to detect the similarity between differ\u00adent programs. For example, based on \nthe training runs of Spec95/Gcc, we can predict the reuse pattern of Spec2K/Gcc on its test input (the \nsame as the test input in Spec95) with 89% accuracy. Spec95/Gcc, compiling cccp.i, sampled 331 times \n0.0E0 2.0E7 4.0E7 6.0E7 8.0E7 1.0E8 logical time of data access Spec95/Gcc, compiling amptjp.i, sampled \n352 times 1.0E5 8.0E4 6.0E4 4.0E4 2.0E4 0.0E0 8.0E4 While our initial goal was to predict programs with \nregular recur\u00adrence patterns, Gcc and other programs such as Li and Vortex took us by surprise. They \nshowed that our method also captured the cumulative pattern despite the inherent randomness in these \npro\u00adgrams. High degree of consistency was not uncommon in applica\u00adtions including program compilation, \ninterpretation, and databases. In addition, Gcc showed that our method could predict the behavior of \na later version of software by pro.ling its earlier version.   4. USES OF PATTERN INFORMATION A program \nhas different portions of accesses with different reuse distances, as shown graphically by the peaks \nin the histogram of Lucas in Figure 4. The peaks differ in number, position, shape, and size among different \nprograms and among different inputs to the same program. Since our method predicts these peaks, it helps \nto better model program locality and consequently improves program and machine optimization as well as \nprogram-machine co-design. Compiler design Reuse distance provides much richer informa\u00adtion about a program \nthan a cache miss rate does. For this reason, at least four compiler groups have used reuse distance \nfor differ\u00adent purposes: to study the limit of register reuse [29] and cache reuse [17, 44], to evaluate \nthe effect of program transformations [2, 7, 17, 44], and to annotate programs with cache hints to a \nproces\u00adsor [8]. In the last work, Beyls and D Hollander used reuse distance pro.les to generate hints \nin SPEC95 FP benchmarks and improved performance by 7% on an Itanium processor [8]. The techniques in \nthis paper will allow compiler writers to analyze larger programs faster and with adjustable accuracy \nand to predict analysis results on data inputs other than analyzed ones. Another potential use is to \n.nd related data in a program based on their usage pattern, for example, arrays or structure .elds that \ncan be grouped to improve cache performance. Recon.gurable memory system A recent trend in memory system \ndesign is adaptive caching based on the usage pattern of a running program. Balasubramonian et al. described \na system that can dy\u00adnamically change the size, associativity, and the number of levels of on-chip cache \nto improve cache speed and save energy [4]. They used an on-line method that tries different choices \nand searches for an appropriate cache con.guration. Since our pattern analysis di\u00adrectly determines the \nbest cache size for capacity misses, it should reduce the search space (and overhead) of run-time adaptation. \nFor FPGA-based systems, So et al. showed that a best design can be found by examining only 0.3% of design \nspace with the help of program information [38], including the balance between compu\u00adtation and memory \ntransfer as de.ned by Callahan et al [9]. So et al. used a compiler to adjust program balance in loop \nnests and to enable software and hardware co-design. While our analysis can\u00adnot change a program to have \na particular balance (as techniques such as unroll-and-jam do [10]), it can measure memory balance and \nsupport hardware adaptation for general programs. File caching Two recent studies by Zhou et al. [45] \nand by Jiang and Zhang [24] have used reuse distance in .le caching. The com\u00admon approach is to partition \ncache space into multiple buffers, each holding data of different reuse distances. Both studies showed \nthat reuse-distance based methods well adapt to the access pattern in server and database traces and \ntherefore signi.cantly outperform single-buffer LRU and frequency-based multi-buffer schemes. Zhou et \nal. used run-time statistics to estimate the peak distance [45]. Our work will help in two ways. The \n.rst is faster analysis, which reduces management cost for large buffers (such as server cache), handles \nlarger traces, and provides faster run-time feedbacks. The sampled reuse distance sampled reuse distance \nsampled reuse distance 6.0E4 4.0E4 2.0E4 0.0E0 4.0E7 0.0E0 2.0E7 4.0E7 6.0E7 8.0E7 1.0E8 logical time \nof data access Spec2K/Gcc, compiling 166.i, sampled 1083 times 3.0E7 2.0E7 1.0E7 0.0E0 0.0E0 5.0E9 1.0E10 \n1.5E10 2.0E10 logical time of data access Figure 5: Sampling results of Gcc for three inputs second is \npredication, which gives not only the changing pattern but also a quantitative measure of the regularity \nwithin and between different types of workloads.  5. RELATED WORK The preceding sections have discussed \nrelated work in the mea\u00adsurement and use of reuse distance. This section compares our work with program \nanalysis techniques. We focus on data reuse analysis. Compiler analysis Compiler analysis has achieved \ngreat success in understanding and improving locality in basic blocks and loop nests. A basic tool is \ndependence analysis. Dependence summa\u00adrizes not only the location but also the distance of data reuse. \nWe refer the reader to a recent, comprehensive book on this subject by Allen and Kennedy [1]. Cascaval \ngave a compiler algorithm that measures reuse distance directly [11]. Because dependence analy\u00adsis is \nstatic, it cannot accurately analyze input-dependent control .ow and dynamic data indirection, for which \nwe need pro.ling or run-time analysis. However, dynamic analysis cannot replace compiler analysis, especially \nfor understanding high-dimensional computation. Bala et al. used training sets in performance prediction \non a parallel machine [5]. They ran test programs to measure the cost of primitive operations and used \nthe result to calibrate the perfor\u00admance predictor. While their method trains for different machines, \nour scheme trains for different program inputs. Compiler analysis can differentiate .ne-grain locality \npatterns. Recent source-level tools use a combination of program instrumentation and pro.ling analysis. \nMcKinley and Temam carefully measured various types of reference locality within and between loop nests \n[31]. Mellor-Crummey et al. measured .ne-grained reuse and program balance through their HPCView tool \n[32]. Reuse distance can be included in these or binary-level tools to recognize reuse-distance pattern \nin smaller code or data units. Since our current analyzer can analyze all data in complete executions, \nit can de.nitely handle program fragments or data subsets. Data pro.ling Access frequency has been used \nsince the early days of computing. In early 80s, Thabit measured how often two data el\u00adements were used \ntogether [42]. Chilimbi recently used grammar compression to .nd longer sequences of repetition called \nhot data streams [12]. To measure both CPU and cache behavior, many studies have tried to identify representative \nsegments in an exe\u00adcution trace. The most recent (and very accurate) is reported by Lafage and Seznec \n[28] and by Sherwood et al. [36]. The two tech\u00adniques cut instruction traces into .xed size windows (10 \nand 100 million instructions) and .nd representative windows through hier\u00adarchical and k-means clustering \nrespectively. While previous studies .nd repeating sequences by measuring frequency and individual similarity, \nwe .nd recurrence patterns by measuring distance and overall accumulation. Reuse distance anal\u00adysis does \nnot construct frequent sub-sequences as other techniques do. On the other hand, it discovers the overall \npattern without re\u00adlying on identical sequences or .xed-size trace windows. Repeti\u00adtion and recurrence \nare orthogonal and complementary aspects of program behavior. Recurrence helps to explain the presence \nor ab\u00adsence of repetition. Lafage and Seznec found that Spec95/Gcc was so irregular that they needed \nto sample 33% of program trace [28], while Sherwood et al. found that Spec2K/Gcc (compiling 166.i) consisted \nof two identical phases with mainly four repeating pat\u00adterns [36]. Being a completely different approach \nthan cycle\u00adaccurate CPU simulation, data sampling shown in Figure 5 con\u00ad.rms both of their observations \nand suggests that the different re\u00adcurrence pattern is the reason for this seemingly contradiction. On \nthe other hand, clustering analysis like theirs provides a broader framework than ours does. They can \ninclude reuse distance as one of the clustering parameters. Lafage and Seznec mentioned this possibility \nbut chose not to use reuse distance because it was too time consuming to measure. Phalke and Gopinath \nused a Markov model to predict the time distance of data reuses inside the same trace [35]. Our focus \nis to predict behavior changes in other inputs. After a few training inputs, it predicts locality pattern \nin other inputs, including those that are too large to run, let alone to simulate. Correlation among \ndata inputs Early analysis of execution fre\u00adquency included sample-and counter-based pro.ling by Knuth \n[27] and static probability analysis by Cocke and Kennedy [15]. Most dynamic pro.ling work considered \nonly a single data input. Wall presented an early study of execution frequency across multiple runs [43]. \nRecently, Chilimbi examined the consistency of hot streams [13]. Since data may be different from one \ninput to an\u00adother, Chilimbi used the instruction PC instead of the identity of data and found that hot \nstreams include similar sets of instructions if not the same sequence. The maximal stream length he showed \nwas 100. Hsu et al. compared frequency and path pro.les in dif\u00adferent runs [23]. Eeckhout et al. studied \ncorrelation in 79 inputs of 9 programs using principal components analysis followed by hier\u00adarchical \nclustering [20]. They considered data properties including access frequency of global variables and the \ncache miss rate. All these techniques measure rather than predict correlation. Instead of using program \ndata or code like most previous work, we correlate program data by their reference histogram. The direct \ncorrelation of data recurrence allows us to predict the changing be\u00adhavior in other program inputs, a \nfeature that we do not know any previous work has attempted. Run-time data analysis Saltz and his colleagues \npioneered dy\u00adnamic parallelization with an approach known as inspector-executor, where the inspector \nexamines and partitions data (and computa\u00adtion) at run time [16]. Similar strategies were used to improve \ndy\u00adnamic locality, including studies by Ding and Kennedy [18], Han and Tseng [21], Mellor-Crummey et \nal. [33], and Strout et al [40]. Knobe and Sarkar included run-time data analysis in array static\u00adsingle \nassignment (SSA) form [26]. To reduce the overhead of run\u00adtime analysis, Arnold and Ryder described a \ngeneral framework for dynamic sampling [3], which Chilimbi and Hirzel extended to dis\u00adcover hot data \nstreams to aid data prefetching [14]. Their run-time sampling was based on program code, while our run-time \nsam\u00adpling is based on data (selected using reuse distance). The two schemes are orthogonal and complementary. \nDing and Kennedy used compiler and language support to mark and monitor impor\u00adtant arrays [18]. Ding \nand Zhong extended it to selectively moni\u00adtor structure and pointer data [19]. Run-time analysis can \nidentify patterns that are unique to a program input, while training-based prediction cannot. On the \nother hand, pro.ling analysis like ours is more thorough because it analyzes all accesses to all data. \n 6. CONCLUSIONS The paper has presented a general method for predicting program locality. It makes three \ncontributions. First, it builds on the 30\u00adyear long series of work on stack distance measurement. By \nusing approximate analysis with arbitrarily high precision, it for the .rst time reduces the space cost \nfrom linear to logarithmic. The new an\u00adalyzer achieves a consistently high speed for practically any \nlarge data and long distance. Second, it extends pro.ling to provide pred\u00adication for data inputs other \nthan pro.led ones. It de.nes common locality patterns including the constant, linear, and a few sub-linear \npatterns. Finally, it enables correlation among different executions with distance-based histogram and \nsampling, which overcomes the limitation of traditional code or data based techniques. When tested on \nan extensive set of benchmarks, the new method achieves 94% accuracy and 99% coverage, suggesting that \npattern prediction is practical for use by locality optimizations in compilers, architec\u00adture, and operating \nsystems. 7. ACKNOWLEDGEMENTS Mark Hill pointed us to several related work in stack distance analysis. \nThe original splay-tree implementation was provided by Daniel Sleator. Grant Farmer extended it to maintain \nsub-tree weights. We also thank our colleagues in and outside Rochester for their valuable inputs. Our \nexperiments were mainly based on local machines purchased by several NSF Infrastructure grants and equipment \ngrants from Compaq/HP and Intel. The D System group at Rice University provided access to their Alpha \nclusters at the critical time before the submission deadline. The authors were supported by a DoE Young \nInvestigator grant as well as funding from DARPA and NSF. Finally, we wish to thank the anonymous reviewers \nof the LCR 02 workshop and PLDI 03 conference for their comments, especially the fourth reviewer of PLDI \n03 for his or her excellent .ve-page review. 8. REFERENCES [1] R. Allen and K. Kennedy. Optimizing Compilers \nfor Modern Architectures: A Dependence-based Approach. Morgan Kaufmann Publishers, 2001. [2] G. Almasi, \nC. Cascaval, and D. Padua. Calculating stack distances ef.ciently. In Proceedings of the .rst ACM SIGPLAN \nWorkshop on Memory System Performance, Berlin, Germany, 2002. [3] M. Arnold and B. G. Ryder. A framework \nfor reducing the cost of instrumented code. In Proceedings of ACM SIGPLAN Conference on Programming Language \nDesign and Implementation, Snowbird, Utah, 2001. [4] R. Balasubramonian, D. Albonesi, A. Buyuktos, and \nS. Dwarkadas. Dynamic memory hierarchy performance and energy optimization. In Proceedings of the 27th \nAnnual International Symposium on Computer Architecture, 2000. [5] V. Balasundaram, G. Fox, K. Kennedy, \nand U. Kremer. A static performance estimator to guide data partitioning decisions. In Proceedings of \nthe Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, Williamsburg, VA, \nApr. 1991. [6] B. T. Bennett and V. J. Kruskal. LRU stack processing. IBM Journal of Research and Development, \npages 353 357, 1975. [7] K. Beyls and E. D Hollander. Reuse distance as a metric for cache behavior. \nIn Proceedings of the IASTED Conference on Parallel and Distributed Computing and Systems, 2001. [8] \nK. Beyls and E. D Hollander. Reuse distance-based cache hint selection. In Proceedings of the 8th International \nEuro-Par Conference, Paderborn, Germany, 2002. [9] D. Callahan, J. Cocke, and K. Kennedy. Estimating \ninterlock and improving balance for pipelined machines. Journal of Parallel and Distributed Computing, \n5(4):334 358, Aug. 1988. [10] S. Carr and K. Kennedy. Improving the ratio of memory operations to .oating-point \noperations in loops. ACM Transactions on Programming Languages and Systems, 16(6):1768 1810, 1994. [11] \nG. C. Cascaval. Compile-time Performance Prediction of Scienti.c Programs. PhD thesis, University of \nIllinois at Urbana-Champaign, 2000. [12] T. M. Chilimbi. Ef.cient representations and abstractions for \nquantifying and exploiting data reference locality. In Proceedings of ACM SIGPLAN Conference on Programming \nLanguage Design and Implementation, Snowbird, Utah, 2001. [13] T. M. Chilimbi. On the stability of temporal \ndata reference pro.les. In Proceedings of International Conference on Parallel Architectures and Compilation \nTechniques, Barcelona, Spain, 2001. [14] T. M. Chilimbi and M. Hirzel. Dynamic hot data stream prefetching \nfor general-purpose programs. In Proceedings of ACM SIGPLAN Conference on Programming Language Design \nand Implementation, Berlin, Germany, 2002. [15] J. Cocke and K. Kennedy. Pro.tability computations on \nprogram .ow graphs. Technical Report RC 5123, IBM, 1974. [16] R. Das, M. Uysal, J. Saltz, and Y.-S. Hwang. \nCommunication optimizations for irregular scienti.c computations on distributed memory architectures. \nJournal of Parallel and Distributed Computing, 22(3):462 479, Sept. 1994. [17] C. Ding. Improving Effective \nBandwidth through Compiler Enhancement of Global and Dynamic Cache Reuse. PhD thesis, Dept. of Computer \nScience, Rice University, January 2000. [18] C. Ding and K. Kennedy. Improving cache performance in dynamic \napplications through data and computation reorganization at run time. In Proceedings of the SIGPLAN 99 \nConference on Programming Language Design and Implementation, Atlanta, GA, May 1999. [19] C. Ding and \nY. Zhong. Compiler-directed run-time monitoring of program data access. In Proceedings of the .rst ACM \nSIGPLAN Workshop on Memory System Performance, Berlin, Germany, 2002. [20] L. Eeckhout, H. Vandierendonck, \nand K. D. Bosschere. Workload design: selecting representative program-input pairs. In Proceedings of \nInternational Conference on Parallel Architectures and Compilation Techniques, Charlottesville, Virginia, \n2002. [21] H. Han and C. W. Tseng. Locality optimizations for adaptive irregular scienti.c codes. Technical \nreport, Department of Computer Science, University of Maryland, College Park, 2000. [22] M. D. Hill. \nAspects of cache memory and instruction buffer performance. PhD thesis, University of California, Berkeley, \nNovember 1987. [23] W. Hsu, H. Chen, P. C. Yew, and D. Chen. On the predictability of program behavior \nusing different input data sets. In Proceedings of the Sixth Workshop on Interaction Between Compilers \nand Computer Architectures (INTERACT), 2002. [24] S. Jiang and X. Zhang. LIRS: an ef.cient low inter-reference \nrecency set replacement to improve buffer cache performance. In Proceedings of ACM SIGMETRICS Conference \non Measurement and Modeling of Computer Systems, Marina Del Rey, California, 2002. [25] Y. H. Kim, M. \nD. Hill, and D. A. Wood. Implementing stack simulation for highly-associative memories. In Proc. ACM \nSIGMETRICS Conference on Measurement and Modeling of Computer Systems, pages 212 213, May 1991. [26] \nK. Knobe and V. Sarkar. Array SSA form and its use in parallelization. In Proceedings of Symposium on \nPrinciples of Programming Languages, San Diego, CA, January 1998. [27] D. Knuth. An empirical study of \nFORTRAN programs. Software Practice and Experience, 1:105 133, 1971. [28] T. Lafage and A. Seznec. Choosing \nrepresentative slices of program execution for microarchitecture simulations: a preliminary application \nto the data stream. In Workload Characterization of Emerging Applications, Kluwer Academic Publishers, \n2000. [29] Z. Li, J. Gu, and G. Lee. An evaluation of the potential bene.ts of register allocation for \narray references. In Workshop on Interaction between Compilers and Computer Architectures in conjuction \nwith the HPCA-2, San Jose, California, February 1996. [30] R. L. Mattson, J. Gecsei, D. Slutz, and I. \nL. Traiger. Evaluation techniques for storage hierarchies. IBM System Journal, 9(2):78 117, 1970. [31] \nK. S. McKinley and O. Temam. Quantifying loop nest locality using SPEC 95 and the perfect benchmarks. \nACM Transactions on Computer Systems, 17(4):288 336, 1999. [32] J. Mellor-Crummey, R. Fowler, and D. \nB. Whalley. Tools for application-oriented performance tuning. In Proceedings of the 15th ACM International \nConference on Supercomputing, Sorrento, Italy, 2001. [33] J. Mellor-Crummey, D. Whalley, and K. Kennedy. \nImproving memory hierarchy performance for irregular applications. International Journal of Parallel \nProgramming, 29(3), June 2001. [34] F. Olken. Ef.cient methods for calculating the success function of \n.xed space replacement policies. Technical Report LBL-12370, Lawrence Berkeley Laboratory, 1981. [35] \nV. Phalke and B. Gopinath. An inter-reference gap model for temporal locality in program behavior. In \nProceedings of ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, Ottawa, Ontario, \nCanada, 1995. [36] T. Sherwood, E. Perelman, G. Hamerly, and B. Calder. Automatically characterizing \nlarge scale program behavior. In Proceedings of International Conference on Architectural Support for \nProgramming Languages and Operating Systems, San Jose, CA, 2002. [37] D. D. Sleator and R. E. Tarjan. \nSelf adjusting binary search trees. Journal of the ACM, 32(3), 1985. [38] B. So, M. W. Hall, and P. C. \nDiniz. A compiler approach to fast hardware design space exploration in FPGA-based systems. In Proceedings \nof ACM SIGPLAN Conference on Programming Language Design and Implementation, Berlin, Germany, 2002. [39] \nA. Srivastava and A. Eustace. ATOM: A system for building customized program analysis tools. In Proceedings \nof ACM SIGPLAN Conference on Programming Language Design and Implementation, Orlando, Florida, June 1994. \n[40] M. M. Strout, L. Carter, and J. Ferrante. Compile-time composition of run-time data and iteration \nreorderings. In Proceedings of ACM SIGPLAN Conference on Programming Language Design and Implementation, \nSan Diego, CA, 2003. [41] R. A. Sugumar and S. G. Abraham. Multi-con.guration simulation algorithms for \nthe evaluation of computer architecture designs. Technical report, University of Michigan, 1993. [42] \nK. O. Thabit. Cache Management by the Compiler. PhD thesis, Dept. of Computer Science, Rice University, \n1981. [43] D. W. Wall. Predicting program behavior using real or estimated pro.les. In Proceedings of \nACM SIGPLAN Conference on Programming Language Design and Implementation, Toronto, Canada, 1991. [44] \nY. Zhong, C. Ding, and K. Kennedy. Reuse distance analysis for scienti.c programs. In Proceedings of \nWorkshop on Languages, Compilers, and Run-time Systems for Scalable Computers, Washington DC, March 2002. \n[45] Y. Zhou, P. M. Chen, and K. Li. The multi-queue replacement algorithm for second level buffer caches. \nIn Proceedings of USENIX Technical Conference, 2001.   \n\t\t\t", "proc_id": "781131", "abstract": "Profiling can accurately analyze program behavior for select data inputs. We show that profiling can also predict program locality for inputs other than profiled ones. Here locality is defined by the distance of data reuse. Studying whole-program data reuse may reveal global patterns not apparent in short-distance reuses or local control flow. However, the analysis must meet two requirements to be useful. The first is efficiency. It needs to analyze all accesses to all data elements in full-size benchmarks and to measure distance of any length and in any required precision. The second is predication. Based on a few training runs, it needs to classify patterns as regular and irregular and, for regular ones, it should predict their (changing) behavior for other inputs. In this paper, we show that these goals are attainable through three techniques: approximate analysis of reuse distance (originally called LRU stack distance), pattern recognition, and distance-based sampling. When tested on 15 integer and floating-point programs from SPEC and other benchmark suites, our techniques predict with on average 94% accuracy for data inputs up to hundreds times larger than the training inputs. Based on these results, the paper discusses possible uses of this analysis.", "authors": [{"name": "Chen Ding", "author_profile_id": "81309499457", "affiliation": "University of Rochester, Rochester, New York", "person_id": "PP43124106", "email_address": "", "orcid_id": ""}, {"name": "Yutao Zhong", "author_profile_id": "81100049521", "affiliation": "University of Rochester, Rochester, New York", "person_id": "PP14028644", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/781131.781159", "year": "2003", "article_id": "781159", "conference": "PLDI", "title": "Predicting whole-program locality through reuse distance analysis", "url": "http://dl.acm.org/citation.cfm?id=781159"}