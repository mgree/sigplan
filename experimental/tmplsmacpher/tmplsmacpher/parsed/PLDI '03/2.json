{"article_publication_date": "05-09-2003", "fulltext": "\n Taming the IXP Network Processor * Lal George Matthias Blume Network Speed Technologies, Inc Toyota \nTechnological Institute at Chicago lg@network-speed.com blume@tti-c.org ABSTRACT We compile Nova, a \nnew language designed for writing network processing applications, using a back end based on integer-linear \nprogramming (ILP) for register allocation, optimal bank assign\u00adment, and spills. The compiler s optimizer \nemploys CPS as its in\u00adtermediate representation; some of the invariants that this IR guar\u00adantees are \nessential for the formulation of a practical ILP model. Appel and George used a similar ILP-based technique \nfor the IA32 to decide which variables reside in registers but deferred the actual assignment of colors \nto a later phase. We demonstrate how to carry over their idea to an architecture with many more banks, \nregister aggregates, variables with multiple simultaneous register assignments, and, very importantly, \none where bank-and register\u00adassignment cannot be done in isolation from each other. Our ap\u00adproach performs \nwell in practise without causing an explosion in size or solve time of the generated integer linear programs. \nCategories and Subject Descriptors D.3.4 [Programming Languages]: Processors code generation; D.3.3 [Programming \nLanguages]: Language Constructs and Fea\u00adtures data types and structures General Terms Algorithms, Performance, \nLanguages. Keywords network processors, Intel IXA, integer linear programming, reg\u00adister allocation, \nbank assignment, programming languages, code generation. 1. INTRODUCTION Network processors are designed \nto meet the demands of next generation networks: cost, scalability, and performance of packet\u00admanipulation \napplications. To achieve very high execution speed * This works was done at Lucent Technologies, Bell \nLabs. Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI \n03, June 9 11, 2003, San Diego, California, USA. Copyright 2003 ACM 1-58113-662-5/03/0006 ...$5.00. LD \nL SD S Figure 1: Micro-engine architecture when processing data at gigabit line rates, network processors \nsuch as the Intel IXP employ fairly unusual designs which make it hard to write programs for them. Our \nwork is an attempt to address this problem. We have focused on the IXP1200, but all of the ideas carry \nover to newer generations of the architecture. The IXP1200 consists of a StrongARM core and six micro-engines \nwith hardware supported multi-threading. Figure 1 shows the basic architecture as seen from the vantage \npoint of a single micro-engine thread. There are six register banks: two general purpose banks (A and \nB); two banks forming the interface to external SRAM mem\u00adory (L and S); and two as the interface to external \nSDRAM memory (LD and SD). The L and LD transfer banks are the destinations for all memory loads, S and \nSD the sources of all stores. Input to the ALU can come from L, LD, A,or B, but each of A, B, and L . \nLD can supply at most one operand. Results from the ALU can go to A, B, S,or SD. There is no direct path \nfrom any register in a transfer bank to another register in the same transfer bank. Not shown in the \n.gure is an on-chip scratch memory M, also accessed via L and LD. 1.1 IXP programming issues To the compiler, \nthe IXP hardware presents a combination of several dif.cult problems for which there are no good published \nheuristics. As a result, the state-of-the-art in programming the IXP is still (a very quirky) assembly. \nA high-level programming lan\u00adguage for the IXP and its compiler must address: Few registers: Because \nof the penalty for memory accesses, lack of data caches, and real-time constraints, spilling (not to \nmen\u00adtion the use of a stack) is nearly intolerable. Bank assignment: The IXP has many different register \nbanks with quite different characteristics. Which program variable should be allocated to which bank \nand when? Register aggregates: Transactions to memory are performed in sets of adjacent registers called \naggregates. Several aggre\u00adgate sizes anywhere in the range 1...8 occur in most pro\u00adgrams. Where should \nan aggregate be allocated within a bank? If the bank is fragmented, which variables should be moved out \nto accommodate the aggregate, where should the evicted values go, and when should this happen? Limited \ndata paths: After a variable has been moved to S or SD, it cannot be moved back to another bank without \ngoing through memory. If its value is required elsewhere in the program, then it should have been duplicated \nbefore being moved. When and how should such duplication occur? Data structures and alignment: Access \nto SDRAM memory is restricted to 8-byte boundaries and access to SRAM to 4-byte boundaries. Real-world \npacket data does not respect these alignment requirements. How can one effectively deal with misalignment \nin conjunction with header .eld extraction? Fine control: How should one provide the necessary knobs \nto ac\u00adcess specialized hardware registers for I/O and concurrency control in a high level language? \n1.2 Our Approach and Contributions The Nova language and its compiler address all of these issues with \ngood results: Optimal bank assignment: Our formulation of integer-linear pro\u00adgramming (ILP) generates \nan optimal bank assignment in\u00adcluding spill considerations. Allocation of aggregates: Allocation of aggregates \nstrongly inter\u00adacts with bank assignment and is dif.cult to solve heuristi\u00adcally. Therefore, we use ILP \nto solve the two problems to\u00adgether. Static single use: Our compiler makes use of a static single use \nproperty enforced for certain variables, enabling the register allocator to place these variables into \nmultiple registers at the same time when doing so is bene.cial. Typed language: Nova s static type system \nis strati.ed into two layers: types and layouts. Record-and tuple types describe collections of word-size \ndata that typically would be stored in collections of registers. Other types are related to the man\u00adagement \nof Nova s control structures. The typing rules guar\u00adantee that no memory allocation (stack or heap) is \nrequired for implementing control. Layouts: Layouts elegantly deal with alignment issues and bit\u00adlevel \ndata access, resulting in ef.cient and easy-to-maintain code. Practical demonstration: Experimentation \nwith our prototype pro\u00advides evidence that the techniques are successful in compil\u00ading real-life programs \nwith compile times short enough to accommodate an edit-compile-debug cycle.  2. RELATED WORK 2.1 Code \nGeneration Imagine a mini-IXP where the transfer banks have four regis\u00adters, and consider a program that \n.rst .lls the S bank with values u, v, w, and x. Along some control path starting at this instruction, \nthe variables v and x might go dead, leaving the register .le with holes. Later on, the program might \nrequire suf.cient contiguous space for allocating y and z: u,v,w,x = sram(addr1) . . . register .le v&#38;x \nare dead y, z =sram(addr2) u w Which variable should be evicted to accommodate the new al\u00adlocation? \nEither u or w would do, as suf.cient space would then be made available. But the best choice might also \ndepend on life\u00adtime, location, and usage constraints of the other variables in the program. The literature \nis .lled with both heuristic and ILP-based techniques for similar problems. However, none of them apply \ndi\u00adrectly to the IXP, nor can they easily be adapted. For example, past work on handling .oating point \nregister pairs or overlapping registers (such as AL, AX, EAX on the IA32) makes the assumption that something \nto be allocated to the larger register will always stay together. This results in a simpler picture since \nindividual smaller-sized pieces will have identical live ranges and never get considered in isolation \nfrom each other [19, 21]. In their work on compiling for irregular architectures, Kong and Wilken [19] \nas well as Scholz and Eckstein [21] insert register\u00adregister copies at strategic points to start a new \nlive range. This gives their algorithm more freedom in the choice of register banks or memory. Their \noptimization technology is ultimately responsi\u00adble for determining if the copies are necessary. Given \nthat there are six register banks on the IXP, it is not clear where such copies should be inserted and \nwhat they would accomplish. In contrast, we do not insert copies into our input program, but our ILP \nformu\u00adlation is free to insert an inter-bank move at any program point as necessary. Tallman and Gupta \n[24] describe a bit-width aware register allo\u00adcation algorithm where static analysis is used to compute \nthe varia\u00adtions in the bit-width over the lifetime of a variable. This informa\u00adtion is used to construct \nan interference graph that aids in packing several variables into a single register. The allocation of \naggre\u00adgates to transfer banks can be seen as a similar packing problem (at register-granularity). Unfortunately, \nthe analogy does not carry very far since IXP aggregates do not have to be stored in adjacent registers \nall of the time. There are only a few speci.c program points where aggregation matters; at all other \npoints it does not have to be taken into account (at least not directly). Not even the live ranges of \nindividual members of any given aggregate will nec\u00adessarily coincide. Fu and Wilken [15] investigated \nthe problem of reducing the number of variables and constraints in an ILP model for the IA32 without \ncompromising optimality with the goal of achieving faster solve times. As described later (Sections 8 \nand 9), we had to deal with similar problems, arriving at similar solutions. Dealing with multiple register \n.les in clustered VLIW architec\u00adtures dates back to the Bulldog compiler [12]. The problem in clus\u00adtered \narchitectures is that of keeping related registers together. On the IXP we face the dual problem of keeping \nthe operands of a sin\u00adgle instruction in different banks, taking the architecture s limited data paths \nbetween banks into account. The register allocation and bank assignment on the IXP is dif.\u00adcult enough \nthat we do not consider the integration of scheduling, or memory bank assignment. This is a shortcoming \nthat will have to be addressed in future work. Since memory latencies differ from bank to bank, a solution \nto this problem will have to take the multi\u00adthreaded nature of IXP applications into account. Let us \ncome back to our mini-IXP with transfer banks of size four. Consider a following program that contains \ntwo SRAM store instructions with a common operand x: sram(addr1 )= u, v, x,w . . . sram(addr2 )= a, \nx,b,c The different positions of x in the list of operands creates con\u00ad.icting constraints for its allocation \nto a register in the transfer bank. We could address this problem by making copies of x, us\u00ading a different \ncopy in each of the con.icting instructions. Since making a physical copy increases register pressure, \noptimal place\u00adment of the copy instruction is intertwined with optimal register\u00adand bank assignment itself. \nNot always are all copies required. Which ones are, however, depends on other decisions of the register \nallocator. For example, suppose the last use of x is in an instruction like: sram(addr3 )= y, x,z This \nuse of x is compatible with one of the other two uses (but not with both), so an existing copy of x might \nbe able to do double-duty here. But not before register allocation is done do we know which one, if any. \nThe traditional approach is to make suf.ciently many copies when x is de.ned and to rely on subsequent \ncoalescing to eliminate the ones that are unneeded [16]. The idea of cloning (see Section 10) makes it \npossible for the same technique to work in an ILP-based allocator. 2.2 Programming Packet Manipulation \nFrom the programmer s point of view, one of the most tedious and error-prone aspects of packet manipulation \nis that of header .eld extraction. Depending on sizes and offsets, a .eld might be aligned with the start \nor the end of a word, it might reside inside a word, or it could straddle a word boundary. To extract \na .eld requires a different sequence of instructions in each case, and the slightest change to one of \nthe sizes has the potential of changing the situation for many .elds, making it dif\u00ad.cult to maintain \nor enhance hand-crafted machine code. The few existing compilers for higher-level languages on the IXP \n(such as Intel-C) do not provide the language support necessary to improve the situation. The next section \npresents an overview of the Nova programming language with emphasis on the salient design choices, and \nthe se\u00admantics of layout and overlay speci.cations. Section 4 gives an overview of the CPS-based front \nend of our prototype implemen\u00adtation. The bulk of the paper (Sections 5 through 10) presents our formulation \nof the ILP optimization problem, and lastly we .nish with performance results on stock hardware and conclusions. \n 3. THE PROGRAMMING LANGUAGE Nova is a modern, lexically-scoped, strict, statically typed, left\u00adto-right \norder, call by-value programming language. It has many of the familiar control constructs including functional \nabstraction and exceptions, but can be compiled to a FORTRAN-like runtime model. Nova provides value \naggregation through record-and tuple-types as well as arrow-and exception-types for describing arguments \nthat are functions or exceptions. There is no polymorphism. 3.1 Inexpensive to Implement Compiled Nova \ncode must be able to .t within one or two thou\u00adsand words of IXP microengine instruction cache. The code \nwill typically implement the fast path of some network processing ap\u00adplication and must be as ef.cient \nas possible. Our language design aims at helping in this regard by avoiding constructs that could become \nexpensive. In particular, unless reg\u00adister allocation requires spills (which is very rare), memory alloca\u00adtion \nand memory access are never implicit but always syntactically apparent. This means that a number of popular \nfeatures have been intentionally left out: no recursive types Nova does not have support for recursive \nalge\u00adbraic types such as lists or trees because, in general, values of these types would have to be memory-allocated. \nno stack Nova functions can be mutually recursive, but all recur\u00adsive calls are restricted to be in tail-call \nposition. Further\u00admore, the absence of recursive types prohibits self-application, making it impossible \nto de.ne a .xpoint combinator that would bring unrestricted recursion through the back-door. no memory-allocated \nclosures Nova functions can be nested so that free occurrences of variables in an inner function refer \nto their corresponding de.nitions in the outer scope. But no two function values created by instantiating \na single de.ni\u00adtion can ever be alive simultaneously in Nova programs, so closures do not have to be \nmemory-allocated. .attening of records Our language has records and tuples with .elds of arbitrary types. \nRecords are .nite collections of la\u00adbeled values, written by enclosing them in square brackets (e.g., \n[x=4, y=3]), while tuples are sequences of values, written by enclosing them in parentheses (e.g., (4,3)). \nThe grouping of conceptually related values into larger values is a useful organizing tool for the programmer, \nbut it does not have to be re.ected in the runtime model. Our compiler han\u00addles tuples and records by \ncompile-time bookkeeping; only leaf .elds have a runtime counterpart, and each of them is treated as \nan independent variable by the register allocator. 3.2 Layouts The analysis of incoming and the construction \nof outgoing net\u00adwork packets is at the heart of network processing. Therefore, Nova programs perform \nmany bit-or byte-level operations, i.e., masking and shifting. The instruction set of the IXP microengine \nprovides adequate support for dealing with these situations, but hand-crafting the nec\u00adessary code is \ntedious and error-prone. Moveover, even a small update such as the insertion of a new .eld at the beginning \nof a header typically means that any code using these low-level opera\u00adtions must be rewritten from scratch. \nThe Nova language automates the task of generating shift and mask operations by providing a sublanguage \nfor de.ning layouts that statically describe the arrangement of bit.elds within a byte stream. For every \nlayout l Nova de.nes two types: packed(l) and unpacked(l). The former is a suf.ciently long word tuple \ntype de\u00adscribing raw, packed data with all bits in their correct positions. The latter is a record type \nwhose structure follows the structure of l s de.nition where all bit.elds have been spread out, each \ninto its own record component of type word. For example, the layout de.nition of the IPv6 header lists \nits .elds and makes use of another layout called ipv6 address: layout ipv6_address = {a1 : 32, a2 :32, \na3 : 32, a4 :32 }; layout ipv6_header ={ version : 4, priority : 4, flow_label : 24, payload_length \n: 16, next_header : 8, hop_limit : 8, src_address : ipv6_address, dst_address : ipv6_address }; Given \nthis de.nition, the type name packed(ipv6 header) is a syn\u00adonym for word[10] and we have the following \ntype equalities: type unpacked(ipv6_address)= [a1: word,a2: word,a3: word, a4: word ] type unpacked(ipv6_header)= \n[ version : word, priority : word, flow_label : word, payload_length : word, next_header : word, hop_limit \n: word, src_address : unpacked(ipv6_address), dst_address : unpacked(ipv6_address)] On the operational \nside, packed(l) and unpacked(l) are connected by operations unpack[l](x) and pack[l](x). The unpack[l] \noper\u00adator maps a value of type packed(l) to the corresponding value of type unpacked(l). For example, \none could write something like:1 let pdata : packed(ipv6_header)= ... let udata = unpack[ipv6_header](pdata); \nif (udata.version == 6 &#38;&#38; udata.hop_limit > 0) ... else ... Although formally every bit.eld mentioned \nin l gets extracted by unpack[l](x), no actual machine instructions will be generated for those .elds \nthat are ignored by the rest of the program. A formal de.nition for pack[l], which acts as the inverse \nof unpack[l], is complicated by the possibility of overlays within lay\u00adouts. Overlays consist of two \nor more alternative sub-layouts, each covering the same bit range. Unpacking generates all bit.elds in \na given layout, including every possible alternative of each of its overlays, but packing takes input \ncorresponding to precisely one alternative of each overlay. As an example, consider the aforementioned \nIPv6 address lay\u00adout. In most programs it makes sense to consider version and priority .elds together, \nforming a larger 8-bit .eld that is cheaper to extract. A revised version of our layout could provide \nthe two competing views side-by-side in the form of an overlay: 1The let keyword binds a new variable. \nThis construct can appear at any point within {... }-enclosed blocks. The new binding is in scope until \nthe end of the block. One can add a type constraint using the : notation as shown in the example. layout \nipv6_header ={ verpri : overlay { whole : 8 | parts : { version: 4, priority: 4 } }, flow_label : 24, \n... } ... let x= pack[ipv6_header] [ verpri = [ whole = 0x60 ], ... ] let y= pack[ipv6_header] [ verpri \n= [ parts = [ version = 6, priority = 0 ] ], ... ] Layouts can be composed on the .y in layout expressions. \nThis can be useful when several small variations of the same basic layout are required. The following \nexample assumes a 56-bit layout lyt that can appear at offsets 0, 16, or 24 within a 3-word (96 bit) \ntuple of packed data. Sequential layouts can be concatenated using the in.x operator ## and the notation \n{n} speci.es a small sequential layout that consists of nothing more than an unnamed n-bit gap: layout \nlyt ={x:16,y:32,z: 8} //size=56 bits ... let udata = // pdata: word[3] if (...) // alignment is 0 unpack[lyt \n##{40}](pdata) else if (...) // alignment is 16 unpack[{16}## lyt ##{24}](pdata) else // alignment is \n24 unpack[{24}## lyt ##{16}](pdata); if (udata.x == 0x3456) ... Notice that completely different instruction \nsequences must be gen\u00aderated for each of the three branches in order to correctly extract the values \nof bit .elds like udata.x, but we are able to use the same layout de.nition lyt in each case. 3.3 Exposing \nHardware Features The IXP microengine instruction set is rather quirky, and Nova tries to shield the \nprogrammer from much of that. But some fea\u00adtures are useful and we incorporated special syntax into Nova \nto let the programmer have access to them. Examples include inter\u00adthread communication (on the same micro-engine, \non different mi\u00adcroengines on the same chip, or across chips), locking and unlock\u00ading (mutual exclusion), \nconcurrency control, access to FIFOs, a number of special-purpose operations such as hashing, and access \nto different kinds of memory. 3.4 No GOTO Nova provides syntax for the usual set of structured control \nconstructs such as if-then-else or loops. However, it does not provide a general goto construct that \nwould let the programmer specify arbitrary control .ows. Nevertheless, most legitimate uses of goto have \nan alternative formulation in Nova in terms of function calls and exceptions. The Nova compiler translates \nall tail-calls into unconditional branches. In fact, the main difference between a tail call and a goto \nis that the tail-call respects scoping discipline and can pass arguments. Programs running on an IXP \nmicroengine often implement no more than the fast path of a particular network protocol. As a result, \nthe program must detect all cases that cannot be handled on the fast path. Once such a situation has \noccured, control should be transferred to some error handler immediately. The error handler could, for \nexample, hand the current packet to the main CPU for slow-path processing. To express such behavior in \nNova, it is convenient to make use of the try-handle construct. Example: fun g [..., x1, x2 ]{ if (...) \nraise x2 () else if (...) raise x1[ b =..., c= ... ]; ... } ... try { if (x.a == A1) { ... raise X1 [b \n=..., c= ... ] } else g[..., x2 =X2, x1=X1 ] } handle X1 [b, c ]{ ... } handle X2 () {... } Each try-handle \nblock introduces in a lexically scoped manner the names of the exceptions (X1 and X2 in the example) \nthat can be used within. These blocks can be nested, and exception values can be passed as arguments \nto function calls, thus enabling these functions (such as g in the example) to directly jump back out \nto the corresponding handler. Nova s type system guarantees that no computation that might raise an exception \ne can escape the try\u00adhandle-block that handles e.  4. THE FRONT END Nova programs can never become \nvery big because generated machine code has to .t within only a few thousand instruction words. Therefore, \nour compiler can easily afford to do whole\u00adprogram analysis. The front end of the compiler performs the \nusual lexical and syn\u00adtactic analyses, elaboration and type-checking, and then converts to a continuation-passing \nstyle intermediate representation (CPS). It then further transforms the code into static single assignment \n(SSA) form for temporaries and performs a host of CPS optimizations, de\u00adproceduralization, transformation \nto a new static single use form for temporaries participating in memory output operations, and, .\u00adnally, \nIXP instruction selection. The phases after instruction selec\u00adtion are considered part of the back end. \n4.1 Continuation-Passing Style Continuation-passing style [23, 2] has been criticized as overkill in \nthe case of a traditional stack-based implementation of languages with recursive procedures. Indeed, \nCPS makes it somewhat harder to determine which closures can be stack-allocated. On the other hand, some \nof the desirable properties of CPS (e.g., the explicit naming of all intermediate values) are shared \nwith direct-style rep\u00adresentations such as A-Normal Form [13]. Other studies point to heap-allocated \ncontinuation closures like those used by some ex\u00adisting CPS-based compilers such as SML/NJ [5] as being \nexpen\u00adsive [7]. However, none of these problems matter in the case of Nova: Nova does not have general \nrecursion and its implementation is not stack-based. Due to the restrictions that we placed on the source \nlanguage, closures do not require memory allocation and all free variables can simply remain in registers. \nAlternatives to CPS such as A-Normal Form do not capture control .ow as concisely as does CPS. While \nwe do not claim CPS to be strictly superior to direct\u00adstyle representations, we still found it to be \nan excellent match for the problem of compiling the Nova language. Our CPS does not have aggregate types; \nall variables concep\u00adtually correspond to single machine registers. The CPS converter takes advantage \nof information produced by the type checker and .attens all records, representing each leaf .eld by its \nown CPS variable. From that point on, each record .eld has its own inde\u00adpendent representation and is \nsubject to subsequent optimizations without regard of the conceptual relationship to other variables \nthat had been expressed by types in the source program. The converter also tries as long as it is cheap \nto do so to en\u00adcode boolean values as control .ow. In particular, functions return\u00ading a bool take two \nreturn continuations instead of one. This can make life somewhat easier for the CPS optimizer. 4.2 Static \nSingle Assignment Soon after CPS conversion the compiler eliminates all assign\u00adments to temporaries, \neffectively bringing the code into static sin\u00adgle assignment form (for temporaries) [9]. Luckily, CPS \nis already powerful enough to express SSA directly without requiring addi\u00adtional constructs such as f-nodes \n[18, 3]. As we will explain later (see Section 9), SSA is not only useful when it comes to performing \ndata .ow analyses. In our compiler its use guarantees an essential property: no variable will appear \nas the target of two different memory read instructions, ruling out the possibility of con.icting constraints \nthat would make consistent colorings impossible. Although in principle it is possible to handle inconsistent \ncolorings (i.e., the use of different colors at different program points), the resulting ILP models turned \nout to have too many variables and to cause solve times that are too long. The simpli.cation of the ILP \nmodel resulting from the ability to rely on SSA is what makes our approach feasible. 4.3 De-proceduralization \nThe current prototype of our ILP-based back end cannot han\u00addle general interprocedural bank-and register-assignment. \nAs a workaround we implemented a CPS phase that fully inlines all pro\u00adcedure calls in non-tail position. \n 4.4 CPS Optimization Phases Our CPS optimizer is far from complete, but even now its out\u00adput is good. \nWe have implemented constant folding, global con\u00adstant propagation, local value propagation, eta reduction, \nsimple hoisting of arithmetic operations, simple contractions (e.g., inlining of called-once functions), \nuseless variable elimination, dead code elimination, and trimming of memory reads. In particular, the \ncombination of .attened records, dead code elimination, and useless variable elimination makes programming \nwith records and tuples (especially pack and unpack) inexpensive. Consider the following example: fun \nf (p1, p2) { layout p={a:16,b: 32,c:16 }; let u1 = unpack[p](p1); let u2 = unpack[p](p2); (if (u1.c > \n10) u1 else u2).b } Our compiler will determine that .elds u1.a, u2.a, and u2.c are never used and that, \ntherefore, their values do not even have to be extracted.  4.5 Static Single Use As mentioned in section \n4.2, SSA form solves a potential color\u00ading problem for temporaries de.ned by memory read operations. \nA dual of the same problem arises for variables participating in mem\u00adory write operations. Therefore, \njust before going into instruction selection our com\u00adpiler brings the program into static single use \n(SSU) form, a dual of SSA form where cloning plays the role of SSA s f-nodes. For our purposes, static \nsingle use means that any use of a variable x as an operand in a memory-write operation is the only use \nof that variable in the entire program. , , . model : set T ; set R; var x {T,R}; param cost{T }; ....: \n.t....,r.... ... =... _ data : set T ={t1 t2} .  .. ... xt1,r1 +xt1,r2 +xt1,r3 =3  AMPL xt2,r1 +xt2,r2 \n+xt2,r3 =4 . ... ... set R ={r1 r2 r3}param cost ={(t13)(t24)} Figure 2: Modeling with AMPL. The input \nto AMPL consists of an abstract model and concrete data for that model. AMPL instantiates the model with \nthe data, generates the (integer-)linear program to be solved by some off-the-shelf solver, and .nally \ninterprets the solution in terms of the original model. SSU form can be generated simply by making suf.ciently \nmany copies of each variable. Since the program was in SSA form al\u00adready, no variable will ever be written \nto after its creation, so origi\u00adnals and copies are guaranteed to be consistent. We added a new primitive \noperation clone to the CPS language and to the IXP machine description used by the back end of the compiler. \nCloning is semantically equivalent to copying. As we describe later in more detail, the difference is \nin how clone oper\u00adations are handled by the ILP model: variables that are clones of each other may but \ndo not necessarily have to be allocated to the same register, so cloning does not always imply physical \ncopying. Minimizing the amount of such physical copying is part of the ILP solver s job, made possible \nby the idea that clones are copies that do not interfere with each other. The example on the left is \nrewritten to that on the right where the variable x is cloned with y such that there is a single use \nof all members of the cloned set. x . \u00b7\u00b7\u00b7 x . \u00b7\u00b7\u00b7 y . clone(x) ... ... sram() . x sram() . y ... ... \n. x . x   5. ILP-BASED OPTIMIZATION We model the problem of optimal bank-assignment and coloring of \naggregates as a 0-1 integer-linear program, i.e., an optimization problem with constraints that are linear \ninequalities, a linear cost function, and the additional constraint that every variable must take the \nvalue 0 or 1. We use AMPL [14] to describe, generate, and solve the linear program. The AMPL compiler \nderives an instance of the optimization problem by instantiating a mathematical model with data speci.c \nto the task being solved, and feeds the resulting system to a standard off-the-shelf simplex solver. \nThe AMPL model consists of several variable-, set-, and parameter\u00addeclarations, plus templates to generate \nthe constraints for the lin\u00adear program. Sets can simply be symbolic enumerations, or they can be built \nup from other sets using constructive set operations. Related ILP variables are grouped together and \nreferred to using a single name and a set that acts as an index range for that name. For example, if \nT and R are sets, then a declaration var x {T,R}; introduces variables xi,j where i ranges over T and \nj over R. Figure 2 outlines an example of a model, data, and the generated system of linear equations. \n5.1 Overview In what follows, we refer to program variables in the code to be compiled as temporaries \nand reserve the word variable for the description of the AMPL model. For every instruction of the program \nwe want to .nd an assign\u00adment of temporaries to register banks. If a temporary is required in different \nbanks for adjacent instructions i1 and i2, then an inter\u00adbank move has to be inserted at the point between \ni1 and i2. In our model, there is a move for every live temporary at every point; if a temporary remains \nin a bank, then source and destination banks of the move are identical and its cost is zero. A feasible \nsolution is one that does not exceed any of the physical resource limits of the IXP such as number of \nregisters in a bank. An optimal solution is one that has the lowest weighted cost of inter\u00adbank moves. \n 5.2 The Model We develop the model in three phases: basic constraints to ex\u00adpress operand requirements \nand resource bounds, colors and aggre\u00adgation of registers, and cloning. After we have covered the basics, \neach later progression will require additional or somewhat modi.ed sets and constraints which we will \nthen introduce on-the-.y. Variables: The set V is the set of temporaries in the program, P denotes the \nset of program points within the .owgraph. Each in\u00adstruction of the program s original .owgraph is located \nbetween two such points. A branch instruction is followed by a single point that is connected to all \npoints at the targets of the branch. Using Pand V, we then de.ne sets related to liveness properties \nof the temporaries in the program. For any v1 . V that is live at a point p1 . P, we write (p1,v1). Exists. \nThe Exists set is similar to the live set but not identical: if an instruction between points p1 and \np2 produces a result v that is immediately dead, then v is nowhere live but (p2,v). Exists. If a temporary \nv1 is live and carried unchanged from point p1 to p2, then we say that (p1,p2,v1).Copy. set Exists . \nP\u00d7V ; set Copy . P\u00d7P\u00d7V ; Our model starts with three sets of 0-1 variables: Movep,v,b1,b2 has the value \n1 if the temporary v needs to be moved from bank b1 to bank b2 at the point p; and is zero otherwise. \nBeforep,v,b has the value 1 if the temporary v is in the bank b before the point p; and is zero otherwise. \nAfterp,v,b has the value 1 if the temporary v is in the bank b after the point p; and is zero otherwise. \nIn AMPL these variables would be declared using: p1 let (a, b, c, d) = sram(100); p2 let (e, f, g, h, \ni, j) = sram(200); p3 let u=a+ c; p4 let v=g+ h; p5 sram(300) . (b, e, v, u);  p6 sram(500) . (f, j, \nd, i);  p7 setP := { p1 p2 ... p7} set V :={a bcdefghijuv} set DefL4 := {( p1, p2,a,b,c,d)} set DefL6 \n:= {( p2, p3,e,f,g,h,i,j)} set DefABW := {( p3, p4,u) ( p4, p5,v)} set Arith := {( p3, p4,a,c) ( p4, \np5,g,h)} set UseS4 := {( p5, p6,b,e,v,u) ( p6, p7,f,j,d,i)} set Exist := {( p2,a), ( p2, b), ... ( p3,e), \n( p3, f), ... ( p4,u), ... set Copy := {( p2, p3,a) ( p2, p3,b) ... Figure 3: Sample source code and \nAMPL data. AMPL set de.nitions on the right capture the essence of the bank-and register-allocation problem \nfor the program on the left. set XBank := {L, LD, S, SD}; set GBank := {A, B, M}; set Banks := XBank \n. GBank ; var Move {Exists, Banks, Banks} binary; var Before {Exists, Banks} binary; var After {Exists, \nBanks} binary; where the set declarations enumerate the transfer banks in XBank and general purpose \nbanks in GBank. The declaration for Move de.nes a variable indexed over points p and temporaries v such \nthat (p,v) . Exists, and a pair of banks representing the source and destination banks. Instruction operands: \nLike Appel and George [4], we charac\u00adterize instructions by the resources they require and de.ne. For \nexample, the output of the ALU can either be an A/B register or one of the write-side transfer registers. \nThe operands can be taken from any disjoint set of input banks. Thus an instruction of the form x := \ny .z between the points p1 and p2 will be modeled as (p1,p2,x) .DefABW, and (p1,p2,y,z) .Arith, and constraints \non these sets will ensure that the necessary conditions for the in\u00adstruction are met. set DefABW . P\u00d7P\u00d7V \nset Arith . P\u00d7P\u00d7V\u00d7V In a similar manner we have a number of sets to characterize the various operand-and \ndestination constraints of other IXP instruc\u00adtions. On the IXP, multiple memory locations can be read \nor written us\u00ading a single instruction, and all the registers used in these operations must be consecutive. \nThe number of registers used in SDRAM memory operations is always a multiple of two. We declare the sets \nDefLi, UseSi :1 =i =8, and DefLDj, UseSDj : j .2,4,6,8 to associate the points before and after the instruction \nwith the vari\u00adables de.ned or consumed by it. For example: i ,, set DefLi .P\u00d7P\u00d7V\u00d7\u00b7\u00b7\u00b7\u00d7V Others: Our model \nuses several other sets that, for brevity, are not shown in this extended abstract. The purpose of these \nsets is to deal with, e.g., instructions that mutate an operand or situations where it would be illegal \nto insert move instructions at certain program points. Example: Figure 3 shows a sample program and the \nAMPL data generated for it. The .rst two lines read four and six SRAM mem\u00adory locations from addresses \n100 and 200; the last two lines, write values to addresses 300 and 500 in SRAM. The AMP data declares \n7 programs points and 12 variables; ( p3, p4,u) is a member of defABW since there is an arithmetic operation \nbetween p3 and p4, and the destination of the instruction is u, etc. 6. CONSTRAINTS Linear constraints \ndeal with liveness, operand constraints, and K constraints which guarantee that at no program point the \nnumber of temporaries assigned to a bank exceeds the capacity of that bank. Finally, there are constraints \nthat bind together the redundant vari\u00adables of the model. (Redundant variables are variables that exist \nmerely to make it easier to specify the model but whose values are uniquely determined by the values \nof other variables.) In-before and in-after: If at some point p a temporary v is moved from some source \nbank b1 to some other bank b2 (even when b1 = b2), then v must have existed in b1 before p and it must \nexist in b2 after p. The constraints relate Before and After to Moves: .(p,v) .Exists, .b .Banks: Beforep,v,b \n= .d.BanksMovep,v,b,d .(p,v) .Exists, .b .Banks: Afterp,v,b = .s.BanksMovep,v,s,b In one place only: \nWe place the restriction that if a temporary exists at a point it must exist in precisely one bank. Therefore, \nthe sum over all banks must be one. This constraint must be relaxed when cloning is involved, however \nfor non-cloned variables, the in\u00adone-place assumption simpli.es the modelling, and does not appear to \nimpact optimality in practise: .(p,v) . Exists: . Beforep,v,b = 1 b.Banks Copy propagation: If a temporary \nis copied unchanged between p1 and p2, then its location after p1 must be the same as its location before \np2: This constraint propagates liveness information and is expressed as: .(p1,p2,v) .Copy, .b .Banks: \nAfterp1 ,v,b = Beforep2 ,v,b Operand de.nition: Sets like DefABWdescribe results of instruc\u00adtions with \nmore than one possible destination bank. Here, the des\u00adtination may be in the A, B, S,or SD bank just \nbefore the point following the instruction. Since an operand can only be in one bank at a time, the corresponding \nsummations must equal one. .(p1,p2,v) .DefABW: Beforep2,v,A + Beforep2,v,B + Beforep2,v,S + Beforep2,v,SD \n= 1 Arithmetic: If x and y are involved in an ALU operation, then they must come from one of the input \nregister banks A, B, L,or LD: .(p1, p2,x,y) . Arith: .b.{A,B,L,LD} Afterp1 ,x,b = 1 .(p1, p2,x,y) . Arith: \n.b.{A,B,L,LD} Afterp1 ,y,b = 1 However, x and y cannot be in the same register bank: .(p1, p2,x,y) . \nArith,b .{A, B, L,LD} : Afterp1 ,x,b + Afterp1,y,b = 1 Furthermore, if one of the operands is in a transfer \nbank, then the other operand cannot be in a transfer bank: .(p1, p2,x,y) . Arith: Afterp1,x,L + Afterp1,y,LD \n= 1 .(p1, p2,x,y) . Arith: Afterp1,x,LD + Afterp1,y,L = 1 Aggregate de.nition and use: The aggregate \nde.nition and use constraints such as DefLi and UseSi are similar; the constraints merely cycle through \nthe variables involved in the aggregate, spec\u00adifying where they should exist. We only show the case i \n= 4: .(p1, p2,v1,v2,v3,v4) . DefL4, .i . 1..4: Beforep2,vi,L = 1 .(p1, p2,v1,v2,v3,v4) . UseS4, .i . \n1..4: Afterp1 ,vi,L = 1 K and Spilling for A/B: The ILP model does not pick colors for temporaries in \nA/B banks; this is left to a subsequent color\u00ading phase. To prevent that phase from having to insert \nadditional inter-bank moves or spills, the model makes sure that no more than 16 A/B registers are needed \nat any time. We leave room for one extra register in A to be able to implement cycles in parallel copies \nthat might be needed during optimistic coalescing [4]. The K con\u00adstraints are needed both before and \nafter each point; for brevity we only show the former kind: .p . P : .(p,v).ExistsBeforep,v,A = 15 .p \n. P : .(p,v).ExistsBeforep,v,B = 16 The K constraints for transfer banks is deferred to Section 9.  \n 7. OBJECTIVE FUNCTION The objective is to minimize the weighted cost of moves. For each point we compute \na static frequency estimation based on loop nesting and branch probabilities using the Dempster-Shaffer \ntheory to combine probabilities. (Our own variation of the Wu-Larus fre\u00adquency estimation [25] can cope \nwith irreducible .owgraphs.) The objective function uses the following parameters: param weight{P}; /* \nexecution freq */ param mvC := 1; /* move cost */ param ldC := 200; /* load cost */ param stC := 200; \n/* store cost */ param bias := 1.01; mvC is the cost of a register-register move, and stC and ldC are \nthe cost of accessing spill memory. If Movep,v,A,M = 1, then the move from A to M will be im\u00adplemented \nas a register-register move from A . S, followed by a (S . M). This is re.ected in the objective function \nby using the term (mvC + stC) \u00b7 Movep,v,A,M. (We also added a small bias to\u00adwards using A registers over \nB registers since we found that this speeds up the ILP solver.) Here is the portion of the objective \nfunc\u00adtion that is related to moves from A and B. .(p,v).Exists( / * from A bank */ weightp \u00b7 mvC\u00b7 .b.{B,S,SD} \nMovep,v,A,b + weightp \u00b7 (mvC+ stC) \u00b7 Movep,v,A,M + weightp \u00b7 (mvC+ stC+ ldC) \u00b7 Movep,v,A,L  / * from \nB bank */ + bias\u00b7 weightp \u00b7 mvC\u00b7 .b.{A,S,SD}\u00b7Movep,v,B,b + \u00b7\u00b7\u00b7)  8. A MILLION VARIABLES The above description \nof the model was a simpli.cation. In prac\u00adtice, if de.ned this way, problem sizes grow too large and \ncannot be solved with reasonable resources. Since there are 7 banks, the number of Move variables for \neach temporary at each point is 72. Since there are 64 available registers, we could theoretically have \nthat many live temporaries at every program point. Even if we as\u00adsumed just an average of 20 live variables \nat every point and a full instruction cache of 1000 instructions, then we would have approx\u00adimately one \nmillion Move variables (72 \u00b7 20 \u00b7 1000), not to mention Before, After, and so on. To reduce the number \nof variables we perform a static analysis on the use of temporaries. For example, if a temporary is loaded \nfrom SRAM memory and is never stored back anywhere, then there is no reason for it to ever be in S, SD,or \nLD. We will therefore rule out all transitions to and from these banks. Of course, if the temporary is \nspilled, then it will have to make a brief appearance in S. Ruling out these banks implies that spilling \nwill move the temporary either from {L, A, B} directly to M, and reloading will move from M di\u00adrectly \nto {L, A, B}. Since spilling occurs very rarely, these restric\u00adtions, which result in dramatically smaller \noptimization problems, are not a problem in practise. 9. AGGREGATES AND COLORING In the work of Appel \nand George the program generated from the results of integer-linear programming satis.ed the K constraints, \nand subsequent coloring phases were used to assign registers using a variation of the Park and Moon [20] \noptimistic coalescing. We use the same approach for the A and B bank, but for transfer registers this \nis not possible because of aggregation. In the example shown in Figure 3, two temporaries from the .rst \nread must be moved out of the transfer bank to make room for the next read. If a na\u00a8ive optimal solution \nto the coloring problem does this in a manner resulting in fragmentation of the bank, then the second \nread cannot be performed even though a K-constraint was satis.ed. Our approach is to let the ILP solver \nderive a coloring of ag\u00adgregates directly in conjunction with the bank assignment problem. For this we \nadded the following declarations to the model: set XRegs := 0..7; var Color {V, XBank, XRegs} binary; \n XRegs is an enumeration of register numbers, Colorv,b,r = 1if whenever v is in the transfer bank b, \nthen it is in the register r of that transfer bank. There are a couple of important properties of the \nColor variable that are in the spirit of Fu and Wilken[15], in that the number of variables and constraints \nare manageable, (but optimality, in our case, may be compromized): 1. Color is point-independent: whenever \na variable is in a spe\u00adci.c bank, it will always reside in the same registers (even after spilling/reloading). \n2. The objective function is color-independent. 3. In general, if the .owgraph were not in static single \nassign\u00adment, the problem could become unsolvable. Consider a pro\u00adgram:  (a,b,X,Y) . sram(...) (Y,X,u,v) \n. sram(...) In the .rst read operation Y must be in a higher numbered register than X, the opposite \nis true for the second operation, so there is no feasible solution. But programs like this are not in \nSSA form, and cannot occur. 4. The analogous problem on the write-side is avoided by the use of static \nsingle use form (Section 10). Assuming a bank of size four, without static single use form, there would \nbe no solution for: sram(...) . (X,a,b,c); sram(...) . (a,b,c,X); For continuity we express the constraints \nbelow using .(p,v) . Exists and do not burden the reader with narrowing this down as described in Section \n8. Color: There are three constraints related to colors. A color must exist for a temporary that can \nlive in a transfer bank: .v .V b .XBank: .r.XRegsColorv,b,r = 1 If two interfering temporaries are simultaneously \nlive in a transfer bank, then they must not have the same color. Our model lists inter\u00adferences explicitly \nin a set Interferes.V\u00d7Vbecause (due to cloning see Section 10) it is not always true that two temporaries \ninterfere just because they are both live at the same time: .(p,v1),(p,v2) .Exists,(v1,v2) .Interferes \n.b .XBank .r .XRegs: Beforep,v1,b + Beforep,v2,b + Colorv1,b,r + Colorv2,b,r =3 An analogous constraint \ncovers the After case. Aggregation: The bulk of the constraints in our system have to do with aggregation, \nand they are all fairly simple and similar in nature. Adjacency is expressed pairwise: .(p1,p2,...,vk,vk+1,...) \n.DefLi .r =7 -i + k : Colorvk ,L,r = Colorvk+1,L,r+1 We found that adding a redundant set of constraints \nthat immedi\u00adately rules out a number of impossible allocations for an aggregate speeds up the solver. \nFor example, the .rst temporary in an aggre\u00adgate of three cannot possibly have colors 6 or 7. Same register: \nSome instructions use one register number to re\u00adfer to two different registers in two different banks. \nWhen coloring the corresponding temporaries we must make sure that they end up having the same color. \nConsider: dst . hash(src) dst . (sram[addr=ea,bit_test_set] . src) In the .rst line dst gets the hash \nof src, and in the second the memory at effective address ea is modi.ed by src, and the old value returned \nin dst. Each of these operations correspond to in\u00adstructions on the IXP, and in each case dst and src \nhave the same register number but are in different transfer banks. We use a set p0 x, y = clone(z) p1 \n   sdram(...) <- z Figure 4: An example of cloning. x has clones y and z. All three start out at the \nsame location but eventually have to split up to satisfy other con\u00adstraints. SameReg whose members are \npairs of temporaries together with the two points just before and after the instruction in question. \nsetSameReg.P\u00d7P\u00d7V\u00d7V .(p1,p2,d,s) .SameReg .r .XRegs: Colord,L,r = Colors,S,r K and Spilling for transfer \nbanks: We do not need K constraints for LD or SD because the fact that a color must be picked for each \noccupant of the bank effectively limits their number. Even though the model also colors L and S, the \nsituation here is slightly differ\u00adent because we sometimes need an extra register for implementing spills. \nTo handle this, we added a set of 0-1 variables colorAvailp,b,r for b .{L,S}which indicate whether or \nnot at point p one could allocate some variable to register r in b: .v : Colorv,b,r + Beforep,v,b =1 \n+ colorAvailp,b,r The K constraint for L and S then becomes . colorAvailp,b,r = 8 -needsSpillp,b r.XRegs \nwhere needsSpillp,b indicates that spilling requires a spare register in bank b at point p. needsSpillp,b \nitself is constrained by inequali\u00adties of the form needsSpillp,b =Movep,v,b1,b2 needsSpillp,b =.b1,b2:... \nMovep,v,b1,b2 for all relevant combinations of b1 and b2. We found that the second constraint (which \nis not necessary for correctness) improves solve times by tightening the model somewhat: 10. THE ROLE \nOF STATIC SINGLE USE In Figure 4, x has clones y and z. All of them eventually get used in different \ncontexts. Immediately after a clone instruction, the original and all its clones are still in the same \nregister. For our model this means that they are in the same bank b and, if b is a transfer bank, their \nb-colors are the same. The members (p1,p2,d,s) of the Cloneset indicate that a clone instruction ex\u00adists \nbetween p1 and p2, cloning the source s resulting in d. With this we can write: .(p1,p2,d,s) .Clone: \nBeforep2,d,b = Afterp1,s,b .(p1,p2,d,s) .Clone,r1 = r2,b .XBank: Colord,b,r1 + Colors,b,r2 =2 -Beforep2,d,b \nOther than at the point of cloning itself, there is no requirement for cloned temporaries to exist in \nthe same bank or the same register. In our example (Figure 4) all three variables eventually must and \nwill move to different banks to satisfy other constraints. In par\u00adticular, z could get moved to its .nal \ndestination even before p5 so that y and z exist in different banks simultaneously at that point (y in \nS and z in SD). But y and z represent the same program variable, so the cloning device effectively allows \nfor such a program variable to be in more than one place at the same time. We need to make the following \nadjustments in our model to ac\u00adcount for the possibility of cloning: When coloring, we require different \ncolors for live tempo\u00adraries that interfere with one another. By de.nition, tempo\u00adraries that are each \nother s clones do not interfere.  In K constraints for A/B we should count only one represen\u00adtative \nfor each set of mutual clones that is in the respective bank. For example, if x, y and z are all in A \nat p2, then they will all be in the same register.  In the objective function we should count as one \nany col\u00adlection of moves that involve members of the same clone set, moving them from identical sources \nto identical destinations.  We take care of the .rst point by guaranteeing that whenever x and y are \nclones of one another, then neither (x,y) .Interferes nor (y,x) .Interferes. The other two points require \na different way of counting. For this we de.ne three new sets of 0-1 variables referred to as cloneBefore, \ncloneAfter, and cloneMove and include constraints that tie them to Before, After, and Move as follows: \nFirst we look at each program point p and consider the sets {x1,...,xn} of temporaries that are clones \nof each other and live at p. From {x1,...,xn}we pick a representative xr for the point p and require \nthat cloneBeforep,xr ,b = 1 if and only if there is at least one x .{x1,...,xn}so that Beforep,x,b = \n1. This can be expressed roughly as follows: .x .{x1,...,xn}: cloneBeforep,xr ,b = Beforep,x,b cloneBeforep,xr \n,b = .x.{x1,...,xn}Beforep,x,b The same de.nition, mutatis mutandis, works for cloneAfter and also for \ncloneMove. With this, K constraints can be adjusted to look at cloneBefore and cloneAfter instead of \nBefore or After when dealing with clones. A similar change is done in the objective function using cloneMove. \nIn the example (see Figure 4) we have x to be the representative for x, y, and z between p1 and p2. Later \non, between p4 and p5, x is not live any more, so a different representative gets used there. 11. RESULTS \nNetwork processors are so new that a set of standard benchmarks does not exist. We have exercised our \ncompiler on a large number of problems, however three programs stand out both in size and in real-world \nrelevance. AES Rijndael: implements the NIST standard for encryption based on Rijndael [10, 11]. Our \nimplementation has the following variation from the fast C reference implementation available from http://www.nist.gov. \n We keep the encryption state in registers at all times, sometimes exploding 4 registers holding the \nstate into 16 registers containing the individual bytes.  The ethernet, IP, and TCP headers are shifted \nbefore en\u00adcryption so that plaintext is read potentially quad-word misaligned, but the ciphertext is \nwritten out quad-word aligned.  The code maintains the TCP checksum .eld.  All tables reside in SRAM \nmemory, resulting in con\u00adtention; we did not investigate distributing the tables over the different memory \nbanks.  The key expansion was statically computed.  We did not implement CBC, so the data size must \nbe a multiple of 16 bytes.  Kasumi: implements the Kasumi encryption algorithm[1] used in the ETSI 3GPP \nstandard. Like Rijndael, this implementation shifts headers, statically computes the subkey expansion, \nand maintains the TCP checksum. All tables are stored in scratch memory, except the S9 table, which is \nstored in SRAM mem\u00adory. By interleaving and packing all the subkey tables, each iteration performs one \nscratch read to access all the 16 sub\u00adkey elements. IPv6-IPv4 NAT This program implements network address \ntrans\u00adlation (NAT) between IPv6 and IPv4 headers [17]. Because of the different header sizes, the start \nof the packet must be moved to a new location and care is required in updating the new checksum .eld. \nIn all cases the code for the application must be compiled with code that synchronizes with the receive \nscheduler, reads in the packet from the receive FIFOs to SDRAM memory, synchronizes with the transmit \nscheduler, and contains the logic to invoke the worker thread. Figure 5 summarizes the characteristics \nof these programs. Line counts are those reported by wc and includes whitespaces and com\u00adments. The numbers \nfor NAT are those for an older (and now obso\u00adlete) version of Nova that was lower-level and did not have \nlayouts. Figure 6 shows the part of the AMPL statistics related to the number of variables that participate \nin coloring. All the variables mentioned in DefLi were added up and included in the .rst col\u00adumn. The \ntable shows that the model has to deal with a fair deal of coloring. Next we show the time it takes to \nsolve the root relaxation (op\u00adtimal linear solution), the time it takes to .nd the optimal integer solution \n(within 0.01% of optimal), the size of the optimization problem, and the number of inter-bank moves and \nspills. The num\u00adbers are in Figure 7; there are too few data points at the present time to plot a trend, \nbut our experimentation gives reason to be op\u00adtimistic about solve times. All numbers are for CPLEX [8] \non an 800MHz dual pentium-III processor, 2Gbyte, Linux machine. We have experimented with another objective \nfunction that lets us determine whether spills are required at all, and if so, where. Once this has been \ndetermined many of the variables and con\u00adstraints involving memory can be eliminated, resulting in a \nmuch smaller linear program. We have not found it necessary to follow this route (which gave solve times \nof 9 seconds for AES and 19.2 Line count Layouts Exce ptions Nova instructions specs pack unpack raise \nhandle AES 541 588 7 8 5 3 1 Kasumi 587 538 7 7 4 2 2 NAT 839 740 - - - - - Figure 5: Static benchmark \nprogram statistics DefLi DefLDj Total UseSi UseSDj Total AES68 16 84 4 10 14 Kasumi 44 14 58 4 14 18 \n8 60 64 NAT 43 22 65 Figure 6: AMPL statistics  Solve TRoot ime (sec) Integer Variables \u00d71000 Constraints \n\u00d71000 Terms in Objective \u00d71000 SolutMoves ion Spills AES 30.4 35.9 108 102 37 25 0 Kasumi 48.2 59.2 138 \n131 50 20 0 NAT 69.2 155.6 208 203 72 60 0 Figure 7: Solver statistics seconds for NAT), and a detailed \ndescription is beyond the scope of this paper. Lastly we have exercised the output of our compiler on \nreal in\u00adhouse hardware [22] consisting of a 233 MHz IXP1200 with data from a hardware packet generator. \nFor Rijndael we measured 270Mbs for payloads of 16 bytes, and 320, 210, and 60 Mbs for 8, 16, and 256 \nbyte payloads using Kasumi. None of these programs were written to be highly optimized for bit-rate processing \nspeeds.  12. FUTURE WORK / OPEN PROBLEMS There are many opportunities to improve the current design \nand implementation and more open research questions to address: newer IXP chips Our project focused on \nthe IXP1200. While most of the ideas will carry over to more recent versions of the IXP hardware, there \nare also a number of new features (e.g., near\u00adest neighbor registers, and signals) that need to be addressed. \nmultithreading Nova provides a key piece of the puzzle, however a complete solution must address the \nissues of co-operative multithreading among micro-engines, threads, and host pro\u00adcessor. module system \nThe addition of a module system together with a more powerful type system (e.g., one that lets the program\u00admer \ncreate new abstract types) will become more important as programmers begin to collect larger utility \nlibraries of IXP code. re-materialization A compiler with aggressive constant folding and propagation \nsuch as ours tends to create intermediate code with many residual constants. Loading a constant is not \nwithout cost: on the IXP it takes 1 or 2 instructions, depend\u00ading on the value. To avoid wasting cycles \nthis way one can keep frequently used constants in registers. But this can be costly, too, because it \nwill increase register pressure. Thus, the problem of loading constants should be solved by the reg\u00adister \nallocator, and a simple trick makes this straightforward. We treat every individual constant as a temporary \nand invent a virtual register bank C. C has unlimited capacity and can hold constants (but nothing else). \nA move to C represents the operation of discarding a constant from a physical register; it has zero cost. \nA move from C represents the load operation of the corresponding constant; its cost depends on the value \nof the constant (which is statically known). This scheme can be further re.ned by paying attention to \npairs (c1,c2) of con\u00adstants where calculating c2 from c1 is cheaper than loading c2 from scratch. (We \nhave an AMPL model that takes all this into account, but we did not .nd the time to complete the rest \nof our compiler infrastructure to take advantage of it.) global register allocation Our compiler fully \ninlines all procedure calls, and Nova was designed for this to always be possible. However, excessive \ninlining can easily cause code explosion. A better compilation scheme would be to compile to a run\u00adtime \nmodel that permits genuine procedure calls. But on the IXP1200 we cannot afford to use a stack. The challenge \nis then to .nd an ILP formulation of register allocation that de\u00adrives globally optimal calling conventions \nand callee-save as\u00adsignments for every procedure [6]. 13. CONCLUSIONS Unconventional architectures with \ncompilation problems that do not have good heuristic solutions require unconventional compila\u00adtion techniques. \nOur results indicate that 0-1 integer linear pro\u00adgrams can provide an excellent solution for an architecture \nsuch as the Intel IXP. Our ILP formulation addresses three open problems without good known heuristics: \nbank assignment, coloring of ag\u00adgregates in conjunction with bank assignment, and the management of variables \nin multiple locations. The formulation of our model turns out to be straightforward. Although not optimal \nin the strict sense, we have evidence to be\u00adlieve that the solutions are very good. Finding a correct \nmathemat\u00adical model is relatively easy, but we found that engineering such a model to reduce the number \nof redundant variables and constraints is extremely important. Making the right set of assumptions to \nlimit the number of variables and constraints is critical as illustrated by the color-and clone-related \nvariables. More experimental results are required to evaluate the perfor\u00admance of the model for problems \ngenerated by the compiler. But since the model is similar in .avor to that used by Appel and George (where \nthey show that solve times for over 600 .owgraphs, some having thousands of instructions, were always \nwithin 30 sec), we can be cautiously optimistic. Our situation is more complex be\u00adcause we deal with \na considerably more dif.cult problem. On the positive side, though, we know that IXP programs cannot \ngrow much bigger than the ones we tried successfully. Our programming language provides the tools for \nwriting robust and maintainable programs, especially when comparing it with the current state of the \nart: assembler or Intel s C compiler for the IXP. Tuples, layouts, loops, functions and exceptions appear \nto provide the right balance of expressiveness and ef.ciency. CPS turned out to be a great intermediate \nrepresentation for the kind of language that does not require memory-allocated closures. 14. ACKNOWLEDGEMENTS \nWe thank Satish Chandra for one of the earlier Nova front ends; John Reppy for the control .ow graph \nfrequency estimation and as one of the initiators of the Nova project; David Gay for help with AMPL and \nsolvers; Ron Sharp and Mike Coss for comments and help with the IXP architecture, simulator, and Tadpole \nboard[22]; and Andrew Appel, Cliff Young, and the anonymous referees for comments on an earlier draft. \n 15. REFERENCES [1] 3GPP. Speci.cation of the 3GPP con.dentiality and integrity algorithms. Version 1.2, \nSept. 2000. [2] A. W. Appel. Compiling with Continuations. Cambridge University Press, Cambridge, England, \n1992. [3] A. W. Appel. SSA is functional programming. ACM SIGPLAN Notices, 33(4):17 20, April 1998. \n [4] A. W. Appel and L. George. Optimal spilling for CISC machines with few registers. In SIGPLAN Conference \non Programming Language Design and Implementation, pages 243 253, 2001. [5] A. W. Appel and D. B. MacQueen. \nA Standard ML compiler. In G. Kahn, editor, Functional Programming Languages and Computer Architecture \n(LNCS 274), pages 301 24, New York, 1987. Springer-Verlag. [6] U. Boquist. Interprocedural register allocation \nfor lazy functional languages. In Proceedings of the 1995 Conference on Functional Programming Languages \nand Computer Architecture (FPCA), La Jolla, California, USA, June 1995. [7] W. D. Clinger, A. Hartheimer, \nand E. Ost. Implementation strategies for .rst-class continuations. Higher-Order and Symbolic Computation, \n12(1):7 45, 1999. [8] CPLEX mixed integer solver. www.cplex.com, 2000. [9] R. Cytron, J. Ferrante, B. \nK. Rosen, M. N. Wegman, and F. K. Zadeck. Ef.ciently computing static single assignment form and the \ncontrol dependence graph. ACM Trans. Prog. Lang. Syst., 13(4):451 490, October 1991. [10] J. Daemen and \nV. Rijmen. The block cipher rijndael, pages 288 296. LNCS 1820. Springer-Verlag, 2000. J.-J.Quisquater \nand B.Schneier, Eds. [11] J. Daemen and V. Rijmen. Rijndael, the advanced encryption standard. Dr. Dobb \ns journal, 26(3):137 139, March 2001. [12] J. R. Ellis. Bulldog: A compiler for VLIW architectures. The \nMIT Press, 1986. [13] C. Flanagan, A. Sabry, B. F. Duba, and M. Felleisen. The essence of compiling with \ncontinuations. In Proceedings of the ACM SIGPLAN 93 Conference on Programming Language Design and Implementation, \npages 237 247, New York, 1993. ACM Press. [14] R. Fourer, D. M. Gay, and B. W. Kernighan. AMPL: A Modeling \nLanguage for Mathematical Programming. Scienti.c Press, South San Francisco, CA, 1993. [15] C. Fu and \nK. Wilken. A faster optimal register allocator. In The 35th Annual IEEE/ACM International Symp. on Microarchitecture. \nIEEE/ACM, November 2002. [16] L. George and A. W. Appel. Iterated register coalescing. ACM transacations \non programming languages and systems., 18(3):300 324, May 1996. [17] E. Grosse and Lakshman Y. N. Network \nprocessors applied to IPv4/IPv6 transition. Bell Labs Report. [18] R. A. Kelsey. A correspondence between \ncontinuation passing style and static single assignment form. In Proceedings ACM SIGPLAN Workshop on \nIntermediate Representations, volume 30, pages 13 22. ACM Press, Mar. 1995. [19] T. Kong and K. D. Wilken. \nPrecise register allocation for irregular architectures. In 31st International Microarchitecture Conference. \nACM, December 1998. [20] J. Park and S.-M. Moon. Optimistic register coalescing. In Proceedings of the \n1998 International Conference on Parallel Architecture and Compilation Techniques, pages 196 204, 1998. \n[21] B. Scholz and E. Eckstein. Register allocation for irregular architectures. In LCTES/SCOPES. ACM, \nJune 2002. [22] R. Sharp, M. Blott, M. Coss, B. Ellis, D. Majette, and V. Purohit. Starburst: Building \nnext-generation internet devices. Bell Labs Technical Journal, 6(2):6 17, 2001. [23] C. Strachey and \nC. Wadsworth. Continuations: A mathematical semantics which can deal with full jumps. Technical Monograph \nPRG-11, Programming Research Group, Oxford University, 1974. [24] S. Tallam and R. Gupta. Bitwidth aware \nglobal register allocation. In 30th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming \nLanguages, pages 85 96. ACM, January 2003. [25] Y. Wu and J. R. Larus. Static branch frequency and program \npro.le analysis. In 27th IEEE/ACM Inter l Symp. on microarchitecture (MICRO-27). IEEE/ACM, Nov. 1994. \n  \n\t\t\t", "proc_id": "781131", "abstract": "We compile Nova, a new language designed for writing network processing applications, using a back end based on integer-linear programming (ILP) for register allocation, optimal bank assignment, and spills. The compiler's optimizer employs CPS as its intermediate representation; some of the invariants that this IR guarantees are essential for the formulation of a practical ILP model.Appel and George used a similar ILP-based technique for the IA32 to decide which variables reside in registers but deferred the actual assignment of colors to a later phase. We demonstrate how to carry over their idea to an architecture with many more banks, register aggregates, variables with multiple simultaneous register assignments, and, very importantly, one where bank- and register-assignment cannot be done in isolation from each other. Our approach performs well in practise---without causing an explosion in size or solve time of the generated integer linear programs.", "authors": [{"name": "Lal George", "author_profile_id": "81100355594", "affiliation": "Network Speed Technologies, Inc", "person_id": "PP31038673", "email_address": "", "orcid_id": ""}, {"name": "Matthias Blume", "author_profile_id": "81100215091", "affiliation": "Toyota Technological Institute at Chicago", "person_id": "PP43117671", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/781131.781135", "year": "2003", "article_id": "781135", "conference": "PLDI", "title": "Taming the IXP network processor", "url": "http://dl.acm.org/citation.cfm?id=781135"}