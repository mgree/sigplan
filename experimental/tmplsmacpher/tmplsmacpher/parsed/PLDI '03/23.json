{"article_publication_date": "05-09-2003", "fulltext": "\n A Compiler Framework for Speculative Analysis and Optimizations Jin Lin Tong Chen  Wei-Chung Hsu \n  Pen-Chung Yew Department of Computer Science and Engineering University of Minnesota Minneapolis, \nMN 55455 U.S.A {jin, tchen, hsu, yew}@cs.umn.edu ABSTRACT Speculative execution, such as control speculation \nand data speculation, is an effective way to improve program performance. Using edge/path profile information \nor simple heuristic rules, existing compiler frameworks can adequately incorporate and exploit control \nspeculation. However, very little has been done so far to allow existing compiler frameworks to incorporate \nand exploit data speculation effectively in various program transformations beyond instruction scheduling. \nThis paper proposes a speculative SSA form to incorporate information from alias profiling and/or heuristic \nrules for data speculation, thus allowing existing program analysis frameworks to be easily extended \nto support both control and data speculation. Such a general framework is very useful for EPIC architectures \nthat provide checking (such as advanced load address table (ALAT) [10]) on data speculation to guarantee \nthe correctness of program execution. We use SSAPRE [21] as one example to illustrate how to incorporate \ndata speculation in those important compiler optimizations such as partial redundancy elimination (PRE), \nregister promotion, strength reduction and linear function test replacement. Our extended framework allows \nboth control and data speculation to be performed on top of SSAPRE and, thus, enables more aggressive \nspeculative optimizations. The proposed framework has been implemented on Intel's Open Research Compiler \n(ORC). We present experimental data on some SPEC2000 benchmark programs to demonstrate the usefulness \nof this framework and how data speculation benefits partial redundancy elimination. Categories and Subject \nDescriptors D.3.4 [Programming Languages]: Processors compiler, optimization. General Terms Algorithms, \nPerformance, Design, Experimentation. Keywords Speculative SSA form, speculative weak update, data speculation, \npartial redundancy elimination, register promotion. Permission to make digital or hard copies of all \nor part of this work for personal or classroom use is granted without fee provided that copies are not \nmade or distributed for profit or commercial advantage and that copies bear this notice and the full \ncitation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to \nlists, requires prior specific permission and/or a fee. PLDI 03, June 9-11, 2003, San Diego, California, \nUSA. Copyright 2003 ACM 1-58113-662-5/03/0006 $5.00. Roy Dz-Ching Ju  Tin-Fook Ngai Sun Chan Microprocessor \nResearch Lab. Intel Corporation Santa Clara, CA 95052 U.S.A  {roy.ju, tin-fook.ngai, sun.c.chan}@intel.com \n1. INTRODUCTION Data speculation refers to the execution of instructions on most likely correct (but \npotentially incorrect) operand values. Control speculation refers to the execution of instructions before \nit has been determined that they would be executed in the normal flow of execution. Both types of speculation \nare effective techniques to improve program performance. Many existing compiler analysis frameworks have \nalready incorporated and used edge/path information to support control speculation. Considering the program \nin Figure 1, the frequency/probability of the execution paths can be collected by edge/path profiling \nat runtime and represented in the control flow graph. If the branch-taken path (i.e. the condition being \ntrue) has a high probability, the compiler can move the load instruction up and execute it speculatively \n(ld.s) before the branch instruction. A check instruction (chk.s) is inserted at its home location to \ncatch and recover from any invalid speculation. The ld.s and chk.s are IA64 instructions that support \ncontrol speculation [10]. Since the execution of the speculative load may overlap with the execution \nof other instructions, the critical path can be shortened along the speculated path. . ld.s x =[y] if \n(c){if (c){ ld x =[y]chk.s x, recovery next: }  . } recovery: ld x=[y] br next (a) original program \n(b) speculative version Figure 1. Using control speculation to hide memory latency. However, so far there \nhas been little work on how to incorporate information for data speculation into existing compiler analysis \nframeworks to help more aggressive speculative optimizations beyond instruction scheduling. Traditional \nalias analysis is non\u00adspeculative and thus cannot facilitate aggressive speculative optimizations. For \nexample, elimination of redundant loads can sometimes be inhibited by an intervening aliasing store. \nConsidering the program in Figure 2(a), the traditional redundancy elimination cannot remove the second \nload *p unless the compiler analysis proves that the expressions *p and *q do not access the same location. \nHowever, through profiling or simple heuristic rules, if we know that there is a small probability that \n*p and *q will access the same memory location, the second load of *p can be speculatively removed as \nshown in Figure 2(b). The first load *p is replaced with a speculative load instruction (ld.a), and a \ncheck load instruction (ld.c) is added to replace the second load instruction. If the store of *q does \nnot access the same location as the load *p, the value in register r32 is used directly without re-loading \n*p. r31 = p = *p *q = ..ld.a r32=[r31] = *p *q = ld.c r32=[r31] = r32 (a) original program (b) speculative \nversion Figure 2. Redundancy elimination using data speculation. In this paper, we address the issues \nof how to incorporate information for data speculation into an existing compiler analysis framework and \nthus enable aggressive speculative optimizations. We use profiling information and/or simple heuristic \nrules to supplement traditional non-speculative compile-time analysis. Such information is then incorporated \ninto the SSA form. One important advantage of using data speculation is that it allows us to use useful \nbut imperfect information or to apply aggressive but uncertain heuristic rules. For example, if we find \n*p and *q are not aliases in the current profiling, it does not guarantee that they are not aliases under \ndifferent program inputs (i.e. input sensitivity). We can only assume speculatively that they are not \naliases when we exploit such profiling information in program optimizations. This requires data speculation \nsupport. Our extended compiler analysis framework supports both control and data speculation. Like traditional \ncompiler analysis, control speculation is supported by examining program control structures and estimating \nlikely execution paths through edge/path profiling and/or heuristic rules [1]. In this paper, we will \nfocus on the data speculation support in the extended framework Edge/path profile Heuristic rules Control \nflow Speculative analysis alias and dataflow analysis Alias profile Heuristic rules Control flow graph \nSpeculative use-def chain/ SSA form (control speculation) (data speculation) Speculative optimizations \n. SSAPRE based optimizations: PRE for expressions register promotion, strength reduction, . Instruction \nscheduling . Figure 3. A framework of speculative analyses and optimizations. Figure 3 depicts our framework \nof speculative analysis and optimization. This is built on top of the existing SSA framework in the ORC \ncompiler. While the general framework we proposed is not restricted to this particular design, we choose \nit to exemplify our framework because it includes a set of compiler optimizations often known as SSAPRE \n[21]. Many optimizations problems, such as redundancy elimination, strength reduction, and register promotion, \nhave been modeled and resolved as PRE problems. The existing SSAPRE in ORC already supports control speculation. \nWe extend it by adding data speculation support and speculative optimizations (see components highlighted \nin bold in Figure 3). In our experimental results, we study the effectiveness of speculative PRE as applied \nto register promotion. The rest of this paper is organized as follows: We first give a survey on related \nwork on data and control speculation, alias analysis and PRE optimization in section 2. Then, in section \n3, we present our speculative analysis framework in detail. Next, in section 4, we propose an algorithm \nthat extends SSAPRE to perform both data and control speculation using the speculative analysis results. \nSection 5 presents some experimental results on the speculative PRE. Finally, section 6 concludes this \npaper by summarizing our contributions.  2. RELATED WORK Several recent studies have tried to use control \nand data speculation to help program analysis and compiler optimizations such as instruction scheduling, \nPRE, register promotion and alias analysis. For example, Ju et al. [17] proposed a unified framework \nto exploit both data and control speculation targeting specifically for memory latency hiding. The speculation \nis exploited by hoisting load instructions across potentially aliasing store instructions or conditional \nbranches, thus allow memory latency of the load instruction to be hidden. In contrast, our proposed framework \nis a general framework that can exploit a larger set of optimizations, such as those in PRE and instruction \nscheduling, by utilizing general profiling information or incorporating general heuristic rules. PRE \nis a powerful optimization technique first developed by Morel et al [26]. The technique removes partial \nredundancy in programs by solving a bi-directional system of data flow equations. Knoop et al. [23] proposed \nan alternative PRE algorithm called lazy code motion that improves on Morel et al s results by avoiding \nunnecessary code movement and removing the bi-directional nature of the original PRE data flow equations. \nThe system of equations suggested by Dhamdhere in [8] is weak bi\u00addirectional and have the same low computational \ncomplexity as uni-directional ones. Chow et al. [6, 21] was the first one to propose an SSA framework \nto perform PRE, which used the lazy code motion formulation for expressions. Lo et al. [25] extended \nthe SSAPRE framework to handle control speculation and register promotion. Bodik et al. [3] proposed \na path profile guided PRE algorithm to handle control speculation so that it enables the complete removal \nof redundancy along more frequent paths at the expense of additional computation along less frequently \nexecuted paths. Kenendy et al. [20] used the SSAPRE framework to perform strength reduction and linear \nfunction test replacement. A recent study by Dulong et al. [11] suggested that PRE can be extended to \nremove redundancy using both control and data speculation, but no systematic design were given. In our \nprior work [24], we studied the use of the Advanced Load Address Table (ALAT), a hardware feature to \nsupport data speculation defined in the IA-64 architecture, for speculative register promotion. An algorithm \nfor speculative register promotion based on PRE was presented. In this paper, we show that using our \nproposed framework, SSAPRE can be extended to handle both data and control speculation using alias profiling \ninformation and/or simple heuristic rules. Some of our extensions to handle data speculation in SSAPRE \nare similar to the approach used in [20] for strength reduction. The speculative weak update concept \ndescribed in this paper corresponds to the injuring definition and the generation of speculative check \ninstructions corresponds to the repair code in [20]. On the other hand, our work builds a unified speculation \nframework to allow SSAPRE be easily extended to incorporate both data and control speculation. Most of \nthe proposed alias analysis algorithms [13, 29, 28, 14, 9] categorize aliases into two classes: must \nalias or definite points-to relation, which holds for all execution paths, and may aliases or possible \npoints-to relation, which may hold for at least one execution path. However, they did not include the \ninformation of how likely such may aliases may occur during the program execution. Such information is \nvery crucial in data speculative optimizations. Recently, there has been some studies on speculative \nalias analysis and probabilistic memory disambiguation. Fernandez [12] described some approaches that \nuse speculative may alias information to optimize code. They gave some experimental data on the precision \nand the mis-speculation rates in their speculative analysis results. Ju. et al.[16] proposed a method \nto calculate the alias probability among array references in application programs. Hwang et al. [15] \nproposed a probabilistic point-to analysis technique to compute the probability of each point-to relation. \nIt could be used to guide the calculation of alias probability among pointer references. The memory reference \nprofiling proposed by Wu et al [30] can also be used to calculate the alias probability based on a particular \ninput. However, such profiling can be very expensive since every memory reference needs to be monitored \nand compared pair-wise. It could slow down the program execution by order of magnitude during the profiling \nphase. Compared to their approaches, we use a lower cost alias profiling scheme to estimate the alias \nprobability. In addition, when alias profiling is unavailable, we use a set of heuristic rules to quickly \napproximate alias probabilities in common cases. 3. SPECULATIVE ANALALYSIS FRAMEWORK In this study, \nwe assume the result of the dataflow analysis is in the SSA form. In SSA, each definition of a variable \nis given a unique version number. Different versions of the same variable can be regarded as different \nprogram variables. Each use of the variable can only refer to a single reaching definition of some version \nof that variable. When several definitions of a variable, a1, a2, ,an-1, reach a merge point in the control \nflow graph, a f function is inserted to merge them into the definition of a new version an, i.e. an .f \n(a1, a2, , an-1). Thus, the semantic of single assignment is preserved. The introduction of a new version \nas the result of a f function can factor the set of use-def edges over merge nodes, and thus reduce the \nnumber of use-def edges needed. The basic SSA form was originally crafted for scalar variables in sequential \nprograms. Recently, it has been extended to cover indirect pointer references [5] and arrays [22]. In \nthis paper, we further specify how likely an alias relation may exist at runtime among a set of scalar \nvariables and indirect references. Such information is then incorporated into an extended SSA form to \nfacilitate data speculation in later program optimizations. The reader is referred to [5, 19] for a full \ndiscussion on the SSA form for indirect memory accesses. 3.1 Basic Concepts Our speculative SSA form \nis an extension of the HSSA form proposed by Chow et al [5, 19]. The traditional SSA form [7] only provides \nuse-def factored chain for the scalar variables. In order to accommodate pointers, Chow et al proposed \nthe HSSA form which integrates the alias information directly into the intermediate representation using \nexplicit may modify operator (.) and may reference operator (\u00b5). In the HSSA form, virtual variables \nare first created to represent indirect memory references. The rule that governs the assignment of virtual \nvariables is that all indirect memory references that have similar alias behaviors in the program are \nassigned a unique virtual variable. Thus, an alias relation could only exist between real variables (i.e. \noriginal program variables) and virtual variables. In order to characterize the effect of such alias \nrelations, the . assignment operator and the \u00b5 assignment operator are introduced to model the may modify \nand the may reference relations, respectively. In our proposed framework, we further introduce the notion \nof likeliness to such alias relations, and attach a speculation flag to the . and \u00b5 assignment operators \naccording to the following rules: Speculative update .s: A speculation flag is attached to a . assignment \noperator if the . assignment operator is highly likely to be substantiated at runtime. It indicates that \nthis update is highly likely and can t be ignored. Speculative use \u00b5s: A speculation flag is attached \nto a \u00b5 assignment operator if the \u00b5 operator is highly likely to be substantiated at runtime. It indicates \nthat the variable in the \u00b5 assignment operator is highly likely to be referenced during the program execution. \nThe compiler can use the profiling information and/or some heuristic rules to specify the degree of likeliness \nfor an alias relation. For example, the compiler can regard an alias relation as highly likely if it \nexists during profiling, and attach speculation flags to the . and \u00b5 assignment operators accordingly. \nThese speculation flags can help to expose opportunities for data speculation. Example 1 shows how to \nbuild a use-def chain speculatively by taking such information into consideration. In this example, v \nis a virtual variable to represent *p, and the numerical subscript of each variable indicates the version \nnumber of the variable. Assume variables a and b are potential aliases of *p. The fact that the variables \na and b could be potentially updated by the *p store reference in s1 is represented by the . operations \non a and b after the store statement. We further assume that according to the profiling information, \nthe indirect memory reference *p is highly likely to be an alias of the variable b, but not of the variable \na, at runtime. Hence, we could attach a speculation flag for .s(b1) in s3 because the update to b caused \nby the potential store *p is also highly likely to be executed. Similarly, *p in s8 will also be highly \nlikely to reference b, and we can attach a speculation flag for \u00b5s(b2) in s7. Example 1 s0: a1 = s0: \na1 = s1: *p1 = 4 s1: *p1 = 4 s2: a2.. ( a1 ) s2: a2.. ( a1 ) s3: b2.. ( b1 ) s3: b2..s ( b1 ) s4: v2.. \n( v1 ) s4: v2.. ( v1 ) s5: = a2 s5: = a2 s6: a3= 4 s6: a3= 4 s7:    \u00b5(a3), \u00b5(b2), \u00b5(v2) s7:   \n \u00b5(a3), \u00b5 s(b2), \u00b5(v2) s8: = *p1 s8: = *p1 (a) traditional SSA graph (b) speculative SSA graph The \nadvantage of having such likeliness information is that we could speculatively ignore those updates that \ndo not carry the speculation flag, such as the update to a in s2, and consider them as speculative weak \nupdates. When the update to a in s2 is ignored, the reference of a2 in s5 becomes highly likely to use \nthe value defined by a1 in s0. Similarly, because *p is highly likely to reference b in s8 (from \u00b5s(b2) \nin s7), we can ignore the use of a3 and v3 in s7, and conclude that the definition of *p in s1 is highly \nlikely to reach the use of *p in s8. From this example, we could see that the speculative SSA form could \ncontain both traditional compiler analysis information and speculation information. The compiler can \nuse the speculation flags to conduct speculative optimizations. 3.2 Speculative Alias Analysis and Dataflow \nAnalysis . Equivalence class based alias analysis . Create . and \u00b5 list . Generate the .s and \u00b5s list \nbased on alias profile . In the absence of alias profile, generate the .s and \u00b5s list based on heuristic \nrules . Construct speculative SSA form . Flow sensitive pointer alias analysis  Figure 4. A Framework \nof Speculative Alias and Dataflow Analysis. Figure 4 shows a basic framework of the alias analysis and \nthe dataflow analysis with the proposed extension to incorporate speculation flags to the . and \u00b5 assignment \noperators using profiling information and/or heuristic rules. In this framework, we can use the equivalence \nclass based alias analysis proposed by Steensgard [28] to generate the alias equivalence classes for \nthe memory references within a procedure. Each alias class represents a set of real program variables. \nNext, we assign a unique virtual variable for each alias class. We also create the initial \u00b5 list and \n. list for the indirect memory references and the procedure call statements. The rules of the construction \nof \u00b5 and . lists are as follows: (1) For an indirect memory store reference or an indirect memory load \nreference, its corresponding . list or \u00b5 list is initialized with all the variables in its alias class \nand its virtual variable. (2) For a procedure call statement, the \u00b5 list and the . list represent the \nref and mod information of the procedure call, respectively. Using alias profiling information and/or \nheuristic rules, we construct the .s and \u00b5s lists. In the next step, all program variables and virtual \nvariables are renamed according to the standard SSA algorithm [7]. Finally, we perform a flow sensitive \npointer analysis using factored use-def chain to refine the \u00b5s list and the .s list. We also update the \nSSA form if the \u00b5s and .s lists have any change. In the following sections we give more detailed description \non how to construct speculative SSA form using alias profile and heuristic rules. 3.2.1 Construction \nof Speculative SSA Form Using Alias Profile We use the concept of abstract memory locations (LOCs) [13] \nto represent the points-to targets in the alias profile. LOCs are storage locations that include local \nvariables, global variables and heap objects. Since heap objects are allocated at runtime, they do not \nhave explicit variable names in the programs1. Before profiling, the heap objects are assigned a unique \nname according to a naming scheme. Different naming schemes may assume different storage granularities \n[4]. For each indirect memory reference, there is a LOC set to represent the collection of memory locations \naccessed by the reference at runtime. In addition, there are two LOC sets to represent the side effect \ninformation, such as modified and referenced locations, respectively, at each procedure call site. The \nrules of assigning a speculation flag for . and \u00b5 list are as follows: .s: Given an indirect memory store \nreference and its profiled LOC set, if any of the member in its profiled LOC set is not in its . list, \nadd the member to the . list using the speculation update .s. If the member is in its . list, then a \nspeculation flag is attached to its . operator (thus becoming a speculative update .s). \u00b5s: Given an \nindirect memory load reference and its profiled LOC set, if any of the member in its profiled LOC set \nis not in its \u00b5 list, add the member to the \u00b5 list using the speculative use \u00b5s. If the member is in \nits \u00b5 list, then a speculation flag is attached to its \u00b5 operator (thus becoming a speculative use \u00b5 \ns). 1 For this same reason, the \u00b5 and . lists may not contain a heap object. 3.2.2 Construction of Speculative \nSSA Form Using Heuristic Rules In the absence of alias profile, compiler can also use some heuristic \nrules to assign the speculation flags. The heuristic rules discussed here are based on the pattern matching \nof syntax tree. We present three possible heuristic rules used in this approach: 1. The two indirect \nmemory references with an identical address expression are assumed highly likely to hold the same value. \n 2. The two direct memory references of the same variable are assumed highly likely to hold the same \nvalue. 3. Since we do not perform speculative optimization across procedure calls, the side effects \nof procedure calls obtained from compiler analysis are all assumed highly likely. Hence, all . definitions \nin the procedure call are changed into .s. The \u00b5 list of the procedure call remains unchanged.  The \nabove three heuristic rules imply that all updates caused by statements other than call statements between \ntwo memory references with the same syntax tree can be speculatively ignored. Using a trace analysis \non SPEC2000 integer benchmark, we found that these three heuristic rules are quite satisfactory with \nsurprisingly few mis-speculations.  4. SPECULATIVE SSAPRE FRAMEWORK In this section, we show how to \napply the speculative SSA form for speculative optimizations. We use SSAPRE [21] because it includes \na set of optimizations that are important in most compilers. The set of optimizations in SSAPRE include: \npartial redundancy elimination for expressions, register promotion, strength reduction and linear function \ntest replacement. We first give a quick overview of the SSAPRE framework. Then, we present an extension \nto incorporate both data and control speculation. 4.1 Overview Most of the work in PRE is focused on \ninserting additional computations in the least likely execution paths. These additional computations \ncause partial redundant computations in most likely execution paths to become fully redundant. By eliminating \nsuch fully redundant computations, we can then improve the overall performance. We assume all expressions \nare represented as trees with leaves being either constants or SSA renamed variables. For indirect loads, \nthe indirect variables have to be in SSA form in order for SSAPRE to handle them. Using an extended HSSA \nform presented in [5], it can uniformly handle indirect loads together with other variables in the program. \nSSAPRE performs PRE one expression at a time, so it suffices to describe the algorithm with respect to \na given expression. In addition, the SSAPRE processes the operations in an expression tree using a bottom-up \norder. The SSAPRE framework consists of six separate steps [21]. The first two steps, f -Insertion and \nRename, construct an expression SSA form using a temporary variable h to represent the value of an expression. \nIn the next two steps, DownSafety and WillBeAvailable, we select an appropriate set of merge points for \nh that allow computations to be inserted. In the fifth step, Finalize, additional computations are inserted \nin the least likely paths, and redundant computations are marked after such additional computations are \ninserted. The last step, CodeMotion, transforms the code and updates the SSA form in the program. In \nstandard SSAPRE, control speculation is suppressed in order to ensure the safety of code placement. Control \nspeculation is realized by inserting computations at the incoming paths of a control merge point f whose \nvalue is not downsafe (e.g. its value is not used before it is killed) [25]. The symbol f is used to \ndistinguish the merge point in the expression SSA form which is different from the merge point f in the \noriginal SSA form. Since control speculation may or may not be beneficial to overall program performance, \ndepending on which execution paths are taken frequently, the edge profile of the program can be used \nto select the appropriate merge points for insertion. The Rename step plays an important role in facilitating \nthe identification of redundant computations in the later steps. In the original SSAPRE without data \nspeculation, such as the example shown in Figure 5(a), two occurrences of an expression a have the same \nvalue, hence, its temporary variable h are assigned the same version number for those two references. \nSince they have the same value, the second occurrence is redundant to the first one, thus, the second \nload can be replaced with a register access. h1. in Figure 5(a) means a value is to be stored into h1. \nHowever, if there is a store *p that may modify the value of the expression a, the second occurrence \nof a is not redundant and should be assigned a different version number, as shown in Figure 5(b). In \nFigure 5(b), the traditional alias analysis will report that this assignment to *p may kill the value \nof the first occurrence of a. Now, as in Figure 5(c), if the speculative SSA form indicates that the \nalias relation between the expression of a and *p is not likely, we can speculatively assume that the \npotential update to a due to the alias relationship to *p can be ignored. The second occurrence of a \nis regarded as speculatively redundant to the first one, and a check instruction is inserted to check \nwhether the value of a is changed before the second occurrence of a (This can be done, for example, by \ninserting a ld.c instruction on the IA-64 architecture [10]). The register that contains the value in \nthe first occurrence can be used in the second occurrence, instead of reloading it. By speculatively \nignoring those updates, we expose speculative redundancy between those two occurrences of the expression \na. a [h1.] a [h1.] a [h1.]  *p . may modify a;*p. may modify aa check statement to check whether a is \nchanged;  a [h1] a [h2.] a [h1] (a) redundant (b) not redundant (c)speculatively redundant Figure 5. \nTypes of occurrence relationships ( h is temporary variable for a ). Thus, the SSA form built for the \nvariable a by the f -Insertion and Rename steps can exhibit more opportunities for redundancy elimination \nif it is enhanced to allow data speculation. The generation of check statements is performed in CodeMotion. \nThe CodeMotion step also generates the speculative load flags for those occurrences whose value can reach \nthe check statements along the control flow paths. The changes to support data speculation in SSAPRE \nframework are confined to f -Insertion, Rename and CodeMotion steps. We now give more detailed description \non the extension to incorporate data speculation in the SSAPRE framework. The reader is referred to [21] \nfor a full description of the foundation of SSAPRE. 4.2 f -Insertion Step One purpose of inserting f \ns for the temporary variable h of an expression is to capture all possible insertion points for the expression. \nInserting too few f s will miss some PRE opportunities. On the other hand, inserting too many f s will \nhave an unnecessarily large SSA graph to be dealt with. As described in [21,6], f s are inserted according \ntwo criteria. First, f s are inserted at the Iterated Dominance Frontiers (DF+) of each occurrence of \nan expression [21]. Secondly, a f can be inserted where there is a f for a variable contained in the \nexpression, because it indicates a change of value for the expression that reaches the merge point. The \nSSAPRE framework performs this type of f insertion in a demand-driven way. An expression at a certain \nmerge point is defined as not anticipated if the value of the expression is never used before it is killed, \nor reaches an exit. A f is inserted at a merge point only if its expression is partially anticipated \n[21], i.e. the value of the expression is used along one control flow path before it is killed. For a \nnot-anticipated expression at a merge point, if we could recognize that its killing definition is a speculative \nweak update, the expression can become partially-anticipated speculatively, thus its merge point could \npotentially be a candidate for inserting computations to allow a partial redundancy to become a speculative \nfull redundancy. s0: = a1 s0: = a1 [h] s0: = a1 [h] s1: if ( ){ s1: if ( ){ s1: if ( ){ s2: *p1= \ns2: *p1= s2: *p1= s3: a2 .. (a1) s3: a2 .. (a1) s3: a2 .. (a1) s4: b2 ..s (b1) s4: b2 ..s (b1) s4: b2 \n..s (b1) s5: v2 ..s (v1) s5: v2 ..s (v1) s5: v2 ..s (v1) } } } s6: a3 .f (a1, a2) s6: a3 .f (a1, a2) \ns6: h .ff (h, h) s7: b3 .f (b1, b2) s7: b3 .f (b1, b2) s7: a3 .f (a1, a2) s8: v3 .f (v1, v2) s8: v3 .f \n(v1, v2) s8: b3 .f (b1, b2) s9: *p1= s9: *p1= s9: v3 .f (v1, v2) s10: a4 .. (a3) s10: a4 .. (a3) s10: \n*p1= s11: b4 ..s (b3) s11: b4 ..s (b3) s11: a4 .. (a3) s12: v4 ..s (v3) s12: v4 ..s (v3) s12: b4 ..s \n(b3) s13: = a4 s13: = a4 [h] s13: v4 ..s (v3) (a) original program (b) after traditional f insertion \ns14: = a4 [h] (c) after enhanced f insertion Figure 6. Enhanced f insertion allows data speculation. \nFigure 6 gives an example of this situation. In this example, a and b are may alias to *p. However, b \nis highly likely to be an alias of *p, but a is not likely to be an alias of *p. Hence, without any data \nspeculation in Figure 6(a), the value of a3 in s6 cannot reach a4 in s13 because of the potential *p \nupdate in s9, i.e. a3 is not anticipated at the merge point in s6. Hence, the merge point in s6 is no \nlonger considered as a candidate to insert computations along the incoming paths as shown in Figure 6(b). \nSince a is not likely to be an alias of *p, the update of a4 in s10 can be speculatively ignored, the \nexpression a3 can now reach a4 in s13. Hence, a3 in s6 becomes speculatively anticipated, and we could \ninsert a f for temporary variable h as shown in Figure 6(c). Appendix A gives the extended version of \nthe f -Insertion step that handles data speculation. The parts that differ form the original algorithm \n[21] are highlighted in bold. 4.3 Rename Step In the previous subsection, we show how the f -insertion \nstep inserts more f s at the presence of may-alias stores, creating more opportunities for inserting \nmore computations. In contrast, the Rename step assigns more occurrences of an expression to the same \nversion of temporary variable h and allows more redundancies to be identified. The enhancement to the \nRename step is to deal with speculative weak updates and speculative uses. Like traditional renaming \nalgorithms, the renaming step keeps track of the current version of the expression by maintaining rename \nstack while conducting a preorder traversal of the dominator tree of the program. Upon encountering a \nnew expression occurrence q, we trace the use-def chain to determine whether the value of the expression \np on top of rename stack can reach this new occurrence. If so, we assign q with the same version as that \nof the expression p. Otherwise, we check whether q is speculative redundant to p by ignoring the speculative \nweak update and continuing tracing upward along the use-def chain. If we eventually reach the expression \np, we speculatively assign q with the same version as given by the top the rename stack and annotate \nq with a speculation flag in order to enforce the generation of check instruction for expression q later \nin the code motion step. If the value of p cannot reach q, we stop and assign a new version for q. Finally, \nwe push q onto the rename stack and proceed. = a1 [h1] *p1 = v2 .. (v1), a2 .. (a1) b2 .. (b1) = a2 \n[h2] (a) traditional renaming = a1 [h1] *p1 = v4 .. (v3), a2 .. (a1) b2 ..s (b1) = a2 [h1<speculation>] \n(b) speculative renaming Figure 7. Enhanced renaming allows data speculation. Figure 7 gives an example \nthat shows the effect of enhanced renaming. In this example, there are two occurrences of the expression \na that are represented by the temporary variable h. The alias analysis shows that expression *p and a \nmay be aliases. Variable a may be updated after the store of *p, and is represented by the . operation \nin the SSA form. These two occurrences of a are assigned with different version numbers in the original \nRename step. However, in our algorithm, if p does not point to a (either by alias profile and/or heuristic \nrules), the . operation with a is not marked with .s, so this update can be ignored in the Rename step. \nIn Figure 7 (b), the second occurrence of a is speculatively assigned with the same version number as \nthe first one. In order to generate the check instruction in the CodeMotion step, the second occurrence \nof a is annotated with a speculation flag. So our algorithm successfully recognizes that the first and \nthe second real occurrences of a are in the same version by ignoring the speculative weak update caused \nby the indirect reference *p. 4.4 CodeMotion Step The CodeMotion step introduces a new temporary variable \nt, which is used to realize the generation of assignment statements and uses of temporary variable h \n[21]. With data speculation, this step is also responsible for generating speculative check statements. \nThe speculative check statements can only occur at places where the occurrences of an expression are \npartially anticipated speculatively. At the same time, multiple speculative check statements to the same \ntemporary variable should be combined into as few check statements as possible. The speculative check \nstatements are generated in the main pass of CodeMotion. Starting from an occurrence a with a speculation \nflag in a use-def chain (shown as a2 [h1 <speculation flag>] in Figure 8(a)), we reach the first speculatively \nweak update (i.e. a2 ..(a1) in Figure 8(a)). A speculative check statement is generated if it has not \nbeen generated yet. In our ORC implementation, actually an advance load check flag is attached to the \nstatement first as shown in Figure 8(b), and the real speculative check instruction, i.e. ld.c, is generated \nlater in the code generation phase. The occurrences of the temporary variable h that are marked with \n. are annotated with an advanced load flag (as shown in Figure 8(b)) if the value of those occurrences \ncan reach their speculative check statements. An actual ld.a instruction will be then generated in the \nlater code generation phase. In Appendix B we give the extended version of the CodeMotion step that handles \ndata speculation. = a[h1.] t = a (advance load flag) 1 11 *p1 =  = t1 v4 .. (v3) *p1 = a.. (a) v.. \n(v) 2143 b.. (b) a.. (a) 4s321 = a[h<speculation flag>] b.. (b) 214s3 t = a (advance load check flag) \n42 = t(a)Before Code Motion 4 (b) Final Output Figure 8. An example of speculative load and check generation. \n 5. EXPERIMENTAL RESULTS We have implemented our speculative PRE algorithm in the Open Research Compiler \n(ORC) [18], version 1.1. The SSAPRE with control speculation is already implemented in ORC. Our implementation \nextends their work by including data speculative analysis and optimizations. In this section, we study \nthe effectiveness of speculative PRE as applied to register promotion. Speculation in software pipelining \nis not included in the current implementation. We measure the effectiveness of our techniques using eight \nSPEC2000 benchmarks executed with the reference inputs. The benchmarks are compiled at the O3 optimization \nlevel with type-based alias analysis [9]. The measurements were performed on an HP workstation i2000 \nequipped with one 733MHz Itanium processor and 2GB of memory running Redhat Linux 7.1. We report on the \nreduction of dynamic loads, the execution time speedup over O3 performance, and the data mis\u00adspeculation \nratio collected by the pfmon tool [27]. 5.1 The Performance Opportunity Exhibited in a Procedure We first \nuse a relatively simple but time critical procedure, smvp, in the equake program to demonstrate the performance \nopportunity of our implemented speculative PRE optimization. Procedure smvp shown in Figure 9 takes nearly \n60% of the total execution time of equake. There are many memory references of similar patterns in the \ninner loop, and we show three statements in this example to illustrate the speculative register promotion \nopportunities. In this example, the load operations of array ***A (i.e. A[][][]) and **v are not promoted \nto registers because the **w references are possibly aliased with them as reported in the compiler alias \nanalysis. However, these load operations can be speculatively promoted into registers. void smvp(int \nnodes, double ***A, int *Acol, int *Aindex, double **v, double **w) { . . . for (i = 0; i < nodes; i++) \n{ . . . while (Anext < Alast) { col = Acol[Anext]; sum0 += A[Anext][0][0] * sum1+= A[Anext][1][1] * \nsum2+= A[Anext][2][2] * w[col][0] += A[Anext][0][0]*v[i][0] + w[col][1] += A[Anext][1][1]*v[i][1] + w[col][2] \n+= A[Anext][2][2]*v[i][2] + Anext++; } } } Figure 9. Example code extracted from procedure smvp. According \nto our alias profile feedback, these potential aliasing never actually occur at runtime. Hence, it would \nbe profitable to speculatively promote ***A and **v to registers. Furthermore, all **v references can \nbe treated as loop invariants and speculatively hoisted out of the inner loop. As a result, 39.8% of \nall load operations in this procedure can be replaced by check instructions. After our speculative register \npromotion transformation, procedure smvp is 6% faster than the base version. As a reference point, a \nmanually tuned smvp, which allocates the aforementioned candidates to registers without generating any \ncheck instructions2, can be 14% faster than the base version. This indicates that we should be able to \ngain a lot more from procedure smvp. After inspecting the generated Itanium code sequence, we learn that \nour transformation replaces regular floating point loads by check instructions (ldfd.c), and the current \ninstruction scheduler in ORC does not effectively schedule floating point load check instructions. Some \ntuning in the instruction scheduler could significantly boost the performance of the transformed smvp. \n 5.2 Experimental Data for Eight SPEC2000 Benchmarks operations. Among the eight programs, art, ammp, \nequake, mcf, and twolf have between 5% to 14% reduction on load operations. The reduction of loads in \nturn reduces data access cycles and CPU cycles. As can be observed from Figure 10, the reduction of loads \nmay not directly translate into execution time improvement. For example, 6% of loads reduction in mcf \nonly achieves 2% of execution time speedup. This is because the reduced loads are often cache-hit operations, \nthus having a smaller impact on performance for programs suffering from frequent data cache misses. In \nFigure 11, we report the percentage of dynamic check loads over the total loads retired, which indicates \nthe amount of data speculation opportunities having been exploited in each program. We also report the \npercentage of load checks that failed during runtime, and this metric is called the mis-speculation ratio. \nA high mis-speculation ratio can decrease the benefit of speculative We now examine the effectiveness \nof speculative register promotion for each benchmark relative to its base case, which is already highly \noptimized with the O3 compiler option and type\u00adbased alias analysis. In general, speculative register \npromotion shortens the critical paths by promoting the values of load operations into registers and replacing \nredundant loads by data speculation checks. Since an integer load has a minimal latency of 2 cycles (L1 \nDcache hit on Itanium), and a floating-point load has a minimal latency of 9 cycles (L2 Dcache hit)3, \nand a successful check (ld.c or ldfd.c) cost 0 cycles, the critical path could be significantly reduced \nas long as the speculations are successful. The first metric is the percentage of load operations reduced \nby speculative register promotion at runtime. We also measure the reduction in total CPU cycles and the \ncycles attributed to data access. Figure 10 shows results from eight SPEC 2000 programs from our implementation \n(more results to be included in the final version). The data show that our speculative register promotion \nbased on alias profiles can significantly reduces retired load 2 Since there is no aliasing observed \nduring run-time, the optimistic code runs correctly when the same input set is used. 3 On Itanium, floating \npoint loads fetch data from the L2 data cache optimization or even degrade performance. In Figure 11, \nwe observe that the mis-speculation ratio is generally very small. For gzip, although the mis-speculation \nratio is almost 6%, the total number of check instructions is nearly negligible compared to the total \nnumber of load instructions. Therefore, there is little performance impact from the high mis-speculation \nratio. Figure 11. The mis-speculation ratio in speculative register promotion. Speculation has a tendency \nto extend the lifetime of registers. Register promotion increases the use of registers. The combination \nof both effects might increase register pressure, which could cause more stack registers to be allocated \nfor the enclosing procedure. More allocated registers may in turn cause memory traffic from register \nstack overflows. We have measured the RSE (Register Stack Engine) stall cycles, but have not observed \nany notable increase. Hence, register pressure has not been an issue from our speculative optimizations \nin these experiments. In the absence of alias profile, we apply heuristic rules in our speculative analysis \nframework and use this information for speculative register promotion. We have performed similar experiments \nto evaluate the heuristic version. We found that the performance of the heuristic version is comparable \nto that of the profile-based version. 5.3 Potential Load Reduction We also evaluate the potential of \nspeculative register promotion by comparing the number of load reduction currently exploited in our implementation \nto the number of speculatively redundant loads visible at runtime. We used two methods to estimate the \npotential load reduction. The first method is simulation-based, similar to the method used in [2]. It \ncan measure the amount of load reuses in programs. By analyzing the dynamic stream of memory references, \nwe can identify all potential speculative reuses available under a given input. The second method uses \nthe existing register promotion algorithm, but aggressively allocates memory references into registers \nwithout considering any potential alias. In the simulation-based method, we gathered the potential reuses \nby instrumenting the compiled program after register promotion but before code generation and register \nallocation. In the simulation, every redundant load is presumed to have its value already been allocated \nto a register. The simulation algorithm assumes a redundant load can reuse a result of another load \nor itself. These loads have identical names (if they are scalars) or identical syntax tree structures. \nThe memory references with identical names or syntax trees are classified into the same equivalent classes. \nredundancies are detected by tracking the access behavior of each static memory reference. A redundant \nload is detected when two consecutive loads with the same address in an equivalence class load the same \nvalue within the same procedure invocation. We track these loads by instrumenting every memory reference \nand recording its address, value and equivalence class during execution. Figure 12 shows the numbers \nof potential load reduction by the simulation\u00adbased method and aggressive register promotion, respectively. \nWe observe that the trend of potential load reduction correlates well with that of the load reduction \nachieved by our speculative register promotion (c.f. Figure 10.) For example, after seeing the limited \npotential of gzip in Figure 12, we may not expect a significant performance gain from speculative register \npromotion.  6. CONCLUSION In this paper, we propose a compiler framework for speculative analysis and \noptimizations. Although control speculation has been well exploited in existing compiler frameworks, \nlittle work has been done so far to systematically incorporate data speculation into various program \noptimizations beyond hiding memory latency. The contributions of this paper are as follows: Firstly, \nwe presented a general compiler analysis framework based on a speculative SSA form to incorporate speculative \ninformation for both data and control speculation. Secondly, we demonstrate the use of the speculative \nanalysis in PRE optimizations, which include not only partial redundancy elimination but also register \npromotion and strength reduction. As a result, many optimizations can be performed aggressively under \nthe proposed compiler analysis framework. Thirdly, this is one of the first attempts to feed the alias \nprofiling information back into the compiler to guide optimizations. The speculative analysis can be \nassisted by both alias profile and heuristic rules. Finally, we have implemented the speculative SSA \nform, and the corresponding speculative analysis, as well as the extension of the SSAPRE framework to \nuse the speculative analysis results. Through the experimental results on speculative register promotion, \nwe have demonstrated the usefulness of this speculative compiler framework and promising performance \npotential of speculative optimizations. As for future work, we would like to enable more optimizations \nunder the speculative SSA form to ensure the generality of the framework and exploit additional performance \nopportunities. We would also like to conduct more empirical studies to further understand the factors \nthat impact the effectiveness of speculative optimizations. 7. ACKNOWLGEMENTS The authors wish to thank \nRaymond Lo, Shin-Ming Liu (Hewlett-Packard) and Peiyi Tang (University of Arkansas at Little Rock) for \ntheir valuable suggestions and comments. This work was supported in part by the U.S. National Science \nFoundation under grants EIA-9971666, CCR-0105571, CCR\u00ad0105574, and EIA-0220021, and grants from Intel. \n 8. REFERENCE [1] T. Ball and J. Larus. Branch prediction for free. In Proceedings of the ACM SIGPLAN \nSymposium on Programming Language Design and Implementation, pages 300-313, June 1993. [2] R. Bodik, \nR. Gupta, and M. Soffa. Load-reuse analysis: design and evaluation, In Proceedings of the ACM SIGPLAN \nConference on Programming Language Design and Implementation, pages 64-76, Atlanta, Georgia, May 1999. \n[3] R. Bod\u00edk, R. Gupta, and M. Soffa. Complete removal of redundant expressions. In Proceedings of the \nACM SIGPLAN Conference on Programming Language Design and Implementation, pages 1-14, Montreal, Canada, \n17-19 June 1998. [4] T. Chen, J. Lin, W. Hsu, P.C. Yew. An Empirical Study on the Granularity of Pointer \nAnalysis in C Programs, In 15th Workshop on Languages and Compilers for Parallel Computing, pages 151-160, \nCollege Park, Maryland, July 2002. [5] F. Chow, S. Chan, S. Liu, R. Lo, and M. Streich. Effective representation \nof aliases and indirect memory operations in SSA form. In Proceedings of the Sixth International Conference \non Compiler Construction, pages 253--267, April 1996. [6] F. Chow, S. Chan, R. Kennedy, S. Liu, R. Lo, \nand P. Tu. A new algorithm for partial redundancy elimination based on SSA form. In Proceedings of the \nACM SICPLAN Conference on Programming Language Design and Implementation, pages 273-286, Las Vegas, Nevada, \nMay 1997. [7] R. Cytron, J. Ferrante, B. Rosen, M. Wegman, and K. Zadeck. Efficiently computing static \nsingle assignment form and the control dependence graph. ACM Transactions on Programming Languages and \nSystems, 13(4): 451--490, 1991. [8] D. M. Dhamdhere. Practical Adaptation of the Global Optimization \nAlgorithm of Morel and Renovise, ACM Trans. on Programming Languages and Systems, 13(2): 291\u00ad294, 1991. \n[9] A. Diwan, K. McKinley, and J. Moss. Type-based alias analysis, In Proceedings of the ACM SIGPLAN \nConference on Programming Language Design and Implementation, pages 106-117, Montreal, Canada, 17-19 \nJune 1998. [10] C. Dulong. The IA-64 Architecture at Work, IEEE Computer, Vol. 31, No. 7, pages 24-32, \nJuly 1998. [11] C. Dulong, R. Krishnaiyer, D. Kulkarni, D. Lavery, W. Li, J. Ng, and D. Sehr. An overview \nof the Intel IA-64 compiler. Intel Technology Journal, November 1999. [12] M. Fernande, and R. Espasa. \nSpeculative alias analysis for executable code, In Proceedings of International Conference on Parallel \nArchitectures and Compilation Techniques, pages 222-231, Charlottesville, Virginia, Sept 2002. [13] R. \nGhiya, D. Lavery, and D. Sehr. On the Importance of Points-To Analysis and Other Memory Disambiguation \nMethods for C Programs. In Proceedings of the ACM SIGPLAN 2001 Conference on Programming Language Design \nand Implementation, pages 47-58, Snowbird, Utah, June 2001. [14] M. Hind. Pointer analysis: Haven't we \nsolved this problem yet? In ACM SIGPLAN-SIGSOFT Workshop on Program Analysis for Software Tools and Engineering, \npages 54-61, Snowbird, Utah, June 2001. [15] Y.-S. Hwang, P.-S. Chen, J.-K. Lee, and R. D.-C. Ju, Probabilistic \nPoints-to Analysis, In Proceeding of the Workshop of Languages and Compilers for Parallel Computing, \nAug. 2001. [16] R. D.-C. Ju, J. Collard, and K. Oukbir. Probabilistic Memory Disambiguation and its Application \nto Data Speculation, Computer Architecture News, Vol. 27, No.1, March 1999. [17] R. D.-C. Ju, K. Nomura, \nU. Mahadevan, and L.-C. Wu. A Unified Compiler Framework for Control and Data Speculation, In Proceedings \nof 2000 International Conf. on Parallel Architectures and Compilation Techniques, pages 157 - 168, Oct. \n2000. [18] R. D.-C. Ju, S. Chan, and C. Wu. Open Research Compiler (ORC) for the Itanium Processor Family. \nTutorial presented at Micro 34, 2001. [19] R. D.-C. Ju, S. Chan, F. Chow, and X. Feng. Open Research \nCompiler (ORC): Beyond Version 1.0, Tutorial presented at PACT 2002. [20] R. Kennedy, F. Chow, P. Dahl, \nS.-M. Liu, R. Lo, and M. Streich. Strength reduction via SSAPRE. In Proceedings of the Seventh International \nConference on Compiler Construction, pages 144--158, Lisbon, Portugal, Apr. 1998. [21] R.Kennedy, S. \nChan, S. Liu, R. Lo, P. Tu, and F. Chow. Partial Redundancy Elimination in SSA Form. ACM Trans. on Programming \nLanguages and systems, v.21 n.3, pages 627-676, May 1999. [22] K. Knobe and V. Sarkar. Array SSA form \nand its use in parallelization. In Proceedings of ACM Symposium on Principles of Programming Languages, \npages 107--120, San Diego, California, January 1998. [23] J. Knoop, O. Ruthing, and B. Steffen. Lazy \ncode motion. In Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation, \npages 224-234, San Francisco, California, June 1992. [24] J. Lin, T.Chen, W.C. Hsu, P.C. Yew, Speculative \nRegister Promotion Using Advanced Load Address Table (ALAT), In Proceedings of First Annual IEEE/ACM \nInternational Symposium on Code Generation and Optimization, pages 125-134, San Francisco, California, \nMarch 2003 [25] R. Lo, F. Chow, R. Kennedy, S. Liu, P. Tu, Register Promotion by Sparse Partial Redundancy \nElimination of Loads and Stores, . In Proc. of the ACM SIGPLAN Conf. on Programming Language Design and \nImplementation, pages 26--37, Montreal, 1998 [26] E. Morel and C. Renvoise. Global optimization by suppression \nof partial redundancies. Communications of the ACM, 22(2): 96--103, 1979. [27] pfmon: ftp://ftp.hpl.hp.com/pub/linux-ia64/pfmon-1.1\u00ad0.ia64.rpm \n[28] B. Steensgaard. Points-to analysis in almost linear time. In Proceedings of ACM Symposium on Principles \nof Programming Languages, pages 32--41, Jan. 1996. [29] R.P. Wilson and M.S. Lam. Efficient context-sensitive \npointer analysis for C program. In Proceedings of the ACM SIGPLAN Conference on Programming Language \nDesign and Implementation, pages1-12, La Jolla, California, Jun 18\u00ad21, 1995. [30] Y. Wu and Y. Lee. Accurate \nInvalidation Profiling for Effective Data Speculation on EPIC processors, In 13th International Conference \non Parallel and Distributed Computing Systems, Las Vegas, Nevada, Aug 2000. Appendix A Algorithm for \nthe enhanced f insertion which allows data speculation procedure f-Insertion(E) DF_phis. {}; for each \noccurrence v of E in program do { DF_phis. DF_phis . DF+(v) var_phi_list . {} while (v is defined by \n. without speculation flags) v . the operand of . if (v is defined by f){ set_def_phi_recursive(phi(v),var_phi_list); \nDF_phis. DF_phis . var_phi_list; } Insert f for E according to DF_phis } end f-Insertion procedure set_def_phi_recursive(par_phi, \nvar_phi_list) if (par_phi . var_phi_list){ var_phi_list . var_phi_list . {par_phi} for each operand v \nin par_phi do{ while (v is defined by . without speculation flags) v . the operand of . if (v is defined \nby f) set_def_phi_recursive(phi(v), var_phi_list); } } end set_def_phi_recursive Appendix B  Algorithm \nfor the enhanced CodeMotion step which handles data speculation procedure CodeMotion(E) { ... for each \noccurrence p of expression E in post-order DT traversal order do{ if ( speculative(p) is true &#38;&#38; \n(p is marked with reload or p is a f operand of a f occurrence marked with will_be_avialbe)) Set_speculative_check_flag \n( p ) } } end CodeMotion procedure Set_speculative_check_flag (p) q . avail_def (p) D . defining statement \nof p if (the check statement for p not yet generated for D) { generate an assignment statement stmt \nwhich is a save of the computation E after D; speculative_check(stmt) . advance load check flag for ld.c \nif (E is an indirect reference){ a . the address expression of p if (a is defined by a speculative check \nstatement s) speculative_check(s) . advance load check flag for chk.a } if (q is real occurrence or inserted \noccurrence ) speculative_load(q) . advance load flag } else if (q is f occurrence){ phi_list . {} Set_speculative_load_flag(phi(q), \nphi_list) } end Set_speculative_check_flag procedure Set_speculative_load_flag (par_phi,phi_list) phi_list \n. phi_list . {par_phi} for (each operand q of par_phi){ if avail_def(q) is a f occurrence and q . phi_list \nSet_speculative_load_flag(q, phi_list) else if avail_def(q) is a real or inserted occurrence{ r . avail_def \n(q) speculative_load(r) . advance load flag } } end Set_speculative_load_flag  \n\t\t\t", "proc_id": "781131", "abstract": "Speculative execution, such as control speculation and data speculation, is an effective way to improve program performance. Using edge/path profile information or simple heuristic rules, existing compiler frameworks can adequately incorporate and exploit control speculation. However, very little has been done so far to allow existing compiler frameworks to incorporate and exploit data speculation effectively in various program transformations beyond instruction scheduling. This paper proposes a speculative SSA form to incorporate information from alias profiling and/or heuristic rules for data speculation, thus allowing existing program analysis frameworks to be easily extended to support both control and data speculation. Such a general framework is very useful for EPIC architectures that provide checking (such as <i>advanced load address table</i> (ALAT) [10]) on data speculation to guarantee the correctness of program execution. We use SSAPRE [21] as one example to illustrate how to incorporate data speculation in those important compiler optimizations such as partial redundancy elimination (PRE), register promotion, strength reduction and linear function test replacement. Our extended framework allows both control and data speculation to be performed on top of SSAPRE and, thus, enables more aggressive speculative optimizations. The proposed framework has been implemented on Intel's Open Research Compiler (ORC). We present experimental data on some SPEC2000 benchmark programs to demonstrate the usefulness of this framework and how data speculation benefits partial redundancy elimination.", "authors": [{"name": "Jin Lin", "author_profile_id": "81451598863", "affiliation": "University of Minnesota, Minneapolis, MN", "person_id": "PP14087362", "email_address": "", "orcid_id": ""}, {"name": "Tong Chen", "author_profile_id": "81100108514", "affiliation": "University of Minnesota, Minneapolis, MN", "person_id": "PP14047970", "email_address": "", "orcid_id": ""}, {"name": "Wei-Chung Hsu", "author_profile_id": "81416596404", "affiliation": "University of Minnesota, Minneapolis, MN", "person_id": "P297226", "email_address": "", "orcid_id": ""}, {"name": "Pen-Chung Yew", "author_profile_id": "81100518451", "affiliation": "University of Minnesota, Minneapolis, MN", "person_id": "P223123", "email_address": "", "orcid_id": ""}, {"name": "Roy Dz-Ching Ju", "author_profile_id": "81451594942", "affiliation": "Microprocessor Research Lab., Intel Corporation, Santa Clara, CA", "person_id": "P249896", "email_address": "", "orcid_id": ""}, {"name": "Tin-Fook Ngai", "author_profile_id": "81100430856", "affiliation": "Microprocessor Research Lab., Intel Corporation, Santa Clara, CA", "person_id": "P282900", "email_address": "", "orcid_id": ""}, {"name": "Sun Chan", "author_profile_id": "81100177245", "affiliation": "Microprocessor Research Lab., Intel Corporation, Santa Clara, CA", "person_id": "P271348", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/781131.781164", "year": "2003", "article_id": "781164", "conference": "PLDI", "title": "A compiler framework for speculative analysis and optimizations", "url": "http://dl.acm.org/citation.cfm?id=781164"}