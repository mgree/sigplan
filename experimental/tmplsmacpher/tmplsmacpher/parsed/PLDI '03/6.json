{"article_publication_date": "05-09-2003", "fulltext": "\n Meta Optimization: Improving Compiler Heuristics with Machine Learning Mark Stephenson and Saman Amarasinghe \nMassachusetts Institute of Technology Laboratory for Computer Science Cambridge, MA 02139 {mstephen, \nsaman}@cag.lcs.mit.edu ABSTRACT Compiler writers have crafted many heuristics over the years to approximately \nsolve NP-hard problems e.ciently. Find\u00ading a heuristic that performs well on a broad range of ap\u00adplications \nis a tedious and di.cult process. This paper in\u00adtroduces Meta Optimization, a methodology for automat\u00adically \n.ne-tuning compiler heuristics. Meta Optimization uses machine-learning techniques to automatically search \nthe space of compiler heuristics. Our techniques reduce com\u00adpiler design complexity by relieving compiler \nwriters of the tedium of heuristic tuning. Our machine-learning system uses an evolutionary algorithm \nto automatically .nd e.ec\u00adtive compiler heuristics. We present promising experimental results. In one \nmode of operation Meta Optimization creates application-speci.c heuristics which often result in impres\u00adsive \nspeedups. For hyperblock formation, one optimization we present in this paper, we obtain an average speedup \nof 23% (up to 73%) for the applications in our suite. Further\u00admore, by evolving a compiler s heuristic \nover several bench\u00admarks, we can create e.ective, general-purpose heuristics. The best general-purpose \nheuristic our system found for hy\u00adperblock formation improved performance by an average of 25% on our \ntraining set, and 9% on a completely unrelated test set. We demonstrate the e.cacy of our techniques \non three di.erent optimizations in this paper: hyperblock for\u00admation, register allocation, and data prefetching. \nCategories and Subject Descriptors D.1.2 [Programming Techniques]: Automatic Program\u00adming; D.2.2 [Software \nEngineering]: Design Tools and Techniques; I.2.6 [Arti.cial Intelligence]: Learning General Terms Design, \nAlgorithms, Performance Keywords Machine Learning, Priority Functions, Genetic Program\u00adming, Compiler \nHeuristics Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI \n03, June 9 11, 2003, San Diego, California, USA. Copyright 2003 ACM 1-58113-662-5/03/0006 ...$5.00. Martin \nMartin and Una-May O Reilly Massachusetts Institute of Technology Arti.cial Intelligence Laboratory Cambridge, \nMA 02139 {mcm, unamay}@ai.mit.edu 1. INTRODUCTION Compiler writers have a di.cult task. They are expected \nto create e.ective and inexpensive solutions to NP-hard problems such as register allocation and instruction \nschedul\u00ading. Their solutions are expected to interact well with other optimizations that the compiler \nperforms. Because some optimizations have competing and con.icting goals, adverse interactions are inevitable. \nGetting all of the compiler passes to mesh nicely is a daunting task. The advent of intractably complex \ncomputer architectures also complicates the compiler writer s task. Since it is im\u00adpossible to create \na simple model that captures the intrica\u00adcies of modern architectures and compilers, compiler writers \nrely on inaccurate abstractions. Such models are based upon many assumptions, and thus may not even properly \nsimulate .rst-order e.ects. Because compilers cannot a.ord to optimally solve NP\u00adhard problems, compiler \nwriters devise clever heuristics that quickly .nd good approximate solutions for a large class of applications. \nUnfortunately, heuristics rely on a fair amount of tweaking to achieve suitable performance. Trial-and-error \nexperimentation can help an engineer optimize the heuris\u00adtic for a given compiler and architecture. For \ninstance, one might be able to use iterative experimentation to .gure out how much to unroll loops for \na given architecture (i.e.,with\u00adout thrashing the instruction cache or incurring too much register pressure). \nAfter studying several compiler optimizations, we found that many heuristics have a focal point. A single \npriority or cost function often dictates the e.cacy of a heuristic. A priority function a function of \nthe factors that a.ect a given problem measures the relative importance of the di.erent options available \nto a compiler algorithm. Take register allocation for example. When a graph col\u00adoring register allocator \ncannot successfully color an interfer\u00adence graph, it spills a variable to memory and removes it from \nthe graph. The allocator then attempts to color the reduced graph. When a graph is not colorable, choosing \nan appropriate variable to spill is crucial. For many allocators, this decision is bestowed upon a single \npriority function. Based on relevant data (e.g., number of references, depth in loop nest, etc.), the \nfunction assigns weights to all uncolored variables and thereby determines which variable to spill. Fine-tuning \npriority functions to achieve suitable perfor\u00admance is a tedious process. Currently, compiler writers \nman\u00adually experiment with di.erent priority functions. For in\u00adstance, Bernstein et al. manually identi.ed \nthree priority functions for choosing spill variables [3]. By applying the three functions to a suite \nof benchmarks, they found that a register allocator s e.ectiveness is highly dependent on the priority \nfunction the compiler uses. The importance of priority functions is a key insight that motivates Meta \nOptimization, a method by which a machine\u00adlearning algorithm automatically searches the priority func\u00adtion \nsolution space. More speci.cally, we use a learning al\u00adgorithm that iteratively searches for priority \nfunctions that improve the execution time of compiled applications. Our system can be used to cater a \npriority function to a speci.c input program. This mode of operation is essen\u00adtially an advanced form \nof feedback directed optimization. More importantly, it can be used to .nd a general-purpose function \nthat works well for a broad range of applications. In this mode of operation, Meta Optimization can perform \nthe tedious work that is currently performed by engineers. For each of the three case studies we describe \nin this paper, we were able to at least match the performance of human\u00adgenerated priority functions. \nIn some cases we achieved con\u00adsiderable speedups. While many researchers have used machine-learning tech\u00adniques \nand exhaustive search algorithms to improve an ap\u00adplication, none have used learning to search for priority \nfunc\u00adtions. Because Meta Optimization improves the e.ective\u00adness of the compiler itself, in theory, we \nneed only apply the process once (rather than on a per-application basis). The remainder of this paper \nis organized as follows. The next section introduces priority functions. Section 3 de\u00adscribes genetic \nprogramming, a machine-learning technique that is well suited to our problem. Section 4 discusses our \nmethodology. We apply our technique to three separate case studies in Section 5, Section 6, and Section \n7. Results of our experiments are included in the case study sections. Sec\u00adtion 8 discusses related work, \nand .nally Section 9 concludes.  2. PRIORITY FUNCTIONS This section is intended to give the reader a \nfeel for the utility and ubiquity of priority functions. Put simply, pri\u00adority functions prioritize the \noptions available to a compiler algorithm. For example, in list scheduling, a priority function assigns \na weight to each instruction in the scheduler s dependence graph, dictating the order in which to schedule \ninstructions. A common and e.ective heuristic assigns priorities using latency-weighted depths [10]. \nEssentially, this is the instruc\u00adtion s depth in the dependence graph, taking into account the latency \nof instructions on all paths to the root nodes: . latency(i):if i is independent. P (i)= max latency(i)+ \nP (j):otherwise. i depends on j The list scheduler proceeds by scheduling ready instructions in priority \norder. In other words, if two instructions are ready to be scheduled, the algorithm will favor the instruc\u00adtion \nwith the higher priority. The scheduling algorithm hinges upon the priority function. Apart from enforcing \nthe legality of the schedule, the scheduler entirely relies on the priority function to make all of its \ndecisions. This description of list scheduling is a simpli.cation. Pro\u00adduction compilers use sophisticated \npriority functions that account for many competing factors (e.g., how a given sched\u00adule may a.ect register \nallocation). The remainder of the section lists a few other priority functions that are amenable to \nthe techniques we discuss in this paper. We will explore three of the following priority functions in \ndetail later in the paper. \u00a8 proach to scheduling for architectures with clustered register .les [20]. \nThey note that the choice of priority function has a strong e.ect on the schedule. They also investigate \n.ve di.erent priority functions [20]. Clustered scheduling: Ozer et al. describe an ap\u00ad  Hyperblock \nformation: Later in this paper we use the formation of predicated hyperblocks as a case study.  Meld \nscheduling: Abraham et al. rely on a priority function to schedule across region boundaries [1]. The \npriority function is used to sort regions by the order in which they should be visited.  Modulo scheduling: \nIn [22], Rau states that there is a limitless number of priority functions that can be devised for modulo \nscheduling. Rau describes the tradeo.s involved when considering scheduling priori\u00adties.  Data Prefetching: \nLater in this paper we investigate a priority function that determines whether or not to prefetch an \naddress.  Register allocation: Many register allocation algo\u00adrithms use cost functions to determine \nwhich variables to spill if spilling is required. We use register allocation as a case study later in \nthe paper.  This is not an exhaustive list of applications. Many im\u00adportant compiler optimizations \nemploy cost functions of the sort mentioned above. The next section introduces genetic programming, which \nwe use to automatically .nd e.ective priority functions. 3. GENETIC PROGRAMMING Of the many available \nmachine-learning techniques, we chose to employ genetic programming (GP) because its at\u00adtributes best \n.t the needs of our application. The following list highlights the suitability of GP to our problem: \n GP is especially appropriate when the relationships among relevant variables are poorly understood [13]. \nSuch is the case with compiler heuristics, which often feature uncertain tradeo.s. Today s complex systems \nalso introduce uncertainty.  GP is capable of searching high-dimensional spaces. Many other learning \nalgorithms are not as scalable.  GP is a distributed algorithm. With the cost of com\u00adputing power at \nan all-time low, it is now economically feasible to dedicate a cluster of machines to searching a solution \nspace.  GP solutions are human readable. The genomes on which GP operates are parse trees which can \neasily be converted to free-form arithmetic equations. Other machine-learning representations, such as \nneural net\u00adworks, are not as comprehensible.  (a) (b) (c) (d) Figure 1: GP Genomes. Part (a) and (b) \nshow examples of GP genomes. Part (c) provides an example of a random crossover of the genomes in (a) \nand (b). Part (d) shows a mutation of the genome in part (a). Figure 2: Flow of genetic programming. \nGenetic pro\u00adgramming (GP) initially creates a population of expres\u00adsions. Each expression is then assigned \na .tness, which is a measure of how well it satis.es the end goal. In our case, .tness is proportional \nto the execution time of the compiled application(s). Until some user-de.ned cap on the number of generations \nis reached, the algorithm probabilistically chooses the best expressions for mat\u00ading and continues. To \nguard against stagnation, some expressions undergo mutation. Like other evolutionary algorithms, GP is \nloosely pat\u00adterned on Darwinian evolution. GP maintains a popula\u00adtion of parse trees [13]. In our case, \neach parse tree is an expression that represents a priority function. As with natu\u00adral selection, expressions \nare chosen for reproduction (called crossover) according to their level of .tness. Expressions that best \nsolve the problem are most likely to have progeny. The algorithm also randomly mutates some expressions \nto innovate a possibly stagnant population. Figure 2 shows the general .ow of genetic programming in \nthe context of our system. The algorithm begins by creat\u00ading a population of initial expressions. The \nbaseline heuris\u00adtic over which we try to improve is included in the initial population; the remainder \nof the initial expressions are ran\u00addomly generated. The algorithm then determines each ex\u00adpression s \nlevel of .tness. In our case, compilers that pro\u00adduce the fastest code are .ttest. Once the algorithm \nreaches a user-de.ned limit on the number of generations, the pro\u00adcess stops; otherwise, the algorithm \nproceeds by probabilis\u00adtically choosing the best expressions for mating. Some of the o.spring undergo \nmutation, and the algorithm continues. Unlike other evolutionary algorithms, which use .xed\u00adlength binary \ngenomes, GP s expressions are variable in length and free-form. Figure 1 provides several examples of \ngenetic programming genomes (expressions). Variable\u00adlength genomes do not arti.cially constrain evolution \nby setting a maximum genome size. However, without special consideration, genomes grow exponentially \nduring crossover and mutation. Our system rewards parsimony by selecting the smaller of two otherwise \nequally .t expressions [13]. Parsimonious expressions are aligned with our philosophy of using GP as \na tool for compiler writers and architects to identify impor\u00adtant heuristic features and the relationships \namong them. Without enforcing parsimony, expressions quickly become unintelligible. In Figure 1, part \n(c) provides an example of crossover, the method by which two expressions reproduce. Here the two expressions \nin (a) and (b) produce o.spring. Crossover works by selecting a random node in each parent, and then \nswapping the subtrees rooted at those nodes1.In theory, crossover works by propagating good subexpressions. \nGood subexpressions increase an expression s .tness. Because GP favors .t expressions, expressions with \nfavor\u00adable building blocks are more likely selected for crossover, further disseminating the blocks. \nOur system uses tourna\u00adment selection to choose expressions for crossover. Tourna\u00adment selection chooses \nN expressions at random from the population and selects the one with the highest .tness [13]. N is referred \nto as the tournament size. Small values of N reduce selection pressure; expressions are only compared \nagainst the other N - 1 expressions in the tournament. Finally, part (d) shows a mutated version of the \nexpression in (a). Here, a randomly generated expression supplants a randomly chosen node in the expression. \nFor details on the mutation operators we implemented, see [2]. 1 Selection algorithms must use caution \nwhen selecting random tree nodes. If we consider a full binary tree, then leaf nodes comprise over 50% \nof the tree. Thus, a naive selection algorithm will choose leaf nodes over half of the time. We employ \ndepth-fair crossover, which equally weighs each level of the tree [12]. Real-Valued Function Representation \nReal1 + Real2 (add Real1 Real2) Real1 -Real2 (sub Real1 Real2) Real1 \u00b7Real2 (mul Real1 Real2) Real1 \n(sqrt Real1) Real1 :ifBool1 (tern Bool1 Real1 Real2) Real2 :if notBool1 Real1 \u00b7Real2 :ifBool1 (cmul \nBool1 Real1 Real2) Real2 :if notBool1 Returns real constant K (rconst K) Boolean-Valued Function Representation \nBool1 and Bool2 (and Bool1 Bool2) Bool1 or Bool2 (or Bool1 Bool2) not Bool1 (not Bool1) Real1 <Real2 \n(lt Real1 Real2) Real1 >Real2 (gt Real1 Real2) Real1 = Real2 (eq Real1 Real2) Returns Boolean constant \n(bconst {true, false}) Returns Boolean value from environment of arg (barg arg) Table 1: GP primitives. \nOur GP system uses the prim\u00aditives and syntax shown in this table. The top segment represents the real-valued \nfunctions, which all return a real value. Likewise, the functions in the bottom seg\u00adment all return a \nBoolean value. To .nd general-purpose expressions (i.e., expressions that work well for a broad range \nof input programs), the learning algorithm learns from a set of training programs. To train on multiple \ninput programs, we use the technique described by Gathercole in [9]. The technique called dynamic subset \nselection (DSS) trains on subsets of the training programs, concentrating more e.ort on programs that \nperform poorly compared to the baseline heuristics. DSS reduces the num\u00adber of .tness evaluations that \nneed to be performed in order to achieve a suitable solution. Because our system must compile and run \nbenchmarks to test an expression s level of .tness, .tness evaluations for our problem are costly. The \nnext section describes the methodology that we use throughout the remainder of the paper.  4. METHODOLOGY \nCompiler priority functions are often based on assump\u00adtions that may not be valid across application \nand architec\u00adtural variations. In other words, who knows on what set of benchmarks, and for what target \narchitecture the priority functions were designed? It could be the case that a prior\u00adity function was \ndesigned for completely orthogonal circum\u00adstances than those under which you use your compiler. Our system \nuses genetic programming to automatically search for e.ective priority functions. Though it may be possible \nto evolve the underlying algorithm, we restrict our\u00adselves to priority functions. This drastically reduces \nsearch space size, and the underlying algorithm ensures optimiza\u00adtion legality. Furthermore, this technique \nis still very pow\u00aderful; even small changes to the priority function can dras\u00adtically improve (or diminish) \nperformance. We optimize a given priority function by wrapping the iterative framework of Figure 2 around \nthe compiler and ar\u00adchitecture. We replace the priority function that we wish Parameter Setting Population \nsize 400 expressions Number of generations 50 generations Generational replacement 22 expressions Mutation \nrate 5% Tournament size 7 Elitism Best expression is guaranteed sur\u00ad vival. Fitness Average speedup over \nthe baseline on the suite of benchmarks.  Table 2: GP parameters. This table shows the GP pa\u00adrameters \nwe used to collect the results in this section. to optimize with an expression parser and evaluator. \nThis allows us to compile the benchmarks in our training suite using the expressions which are priority \nfunctions in the population. The expressions that create the fastest executa\u00adbles for the applications \nin the training suite are favored for crossover. Our system uses total execution time to assign .tnesses. \nThis approach focuses on frequently executed procedures, and therefore, may slowly converge upon general-purpose \nsolutions. However, when one wants to specialize a compiler for a given input program, this evaluation \nof .tness works extremely well. Table 1 shows the GP expression primitives that our sys\u00adtem uses. Careful \nselection of GP primitives is essential. We want to give the system enough .exibility to potentially \n.nd unexpected results. However, the more leeway we give GP, the longer it will take to converge upon \na general solution. Our system creates an initial population that consists of 399 randomly generated \nexpressions; it randomly grows expressions of varying heights using the primitives in Table 1 and features \nextracted by the compiler writer. Features are measurable program characteristics that the compiler writer \nthinks may be important for forming good priority functions (e.g., latency-weighted depth for list scheduling). \nIn addition to the randomly generated expressions, we seed the initial population with the compiler writer \ns best guess. In other words, we include the priority function distributed with the compiler. For two \nof the three opti\u00admizations presented in this paper, we found that the seed is quickly obscured and weeded \nout of the population as more favorable expressions emerge. In fact, for hyperblock selec\u00adtion and data \nprefetching, which we discuss later, the seed had no impact on the .nal solution. These results suggest \nthat one could use Meta Optimization to construct priority functions from scratch rather than trying \nto improve upon preexisting functions. In this way, our tool can reduce the complexity of compiler design \nby sparing the engineer from perfunctory algorithm tweaking. Table 2 summarizes the parameters that we \nuse to col\u00adlect results. We chose the parameters in the table after a moderate amount of experimentation. \nWe give our GP sys\u00adtem 50 generations to .nd a solution. For the benchmarks that we surveyed, the time \nrequired to run for 50 genera\u00adtions is about one day per benchmark in the training set2 . Our system \nmemoizes benchmark .tnesses because .tness evaluations are so costly. After every generation the system \nrandomly replaces 22% of the population with new expressions created via the crossover 2 We ran on 15 \nto 20 machines in parallel for the experiments in Section 5 and Section 6, and we used 5 machines for \nthe experiments in Section 7.  Figure 3: Control .ow v. predicated execution. Part (a) shows a segment \nof control-.ow that demonstrates a sim\u00adple if-then-else statement. As is typical with multime\u00addia and \ninteger applications, there are few instructions per basic block in the example. Part (b) is the corre\u00adsponding \npredicated hyperblock. If-conversion merges disjoint paths of control by creating predicated hyper\u00adblocks. \nChoosing which paths to merge is a balancing act. In this example, branching may be more e.cient than \npredicating if p3 is rarely true. operation presented in Section 3. Only the best expression is guaranteed \nsurvival. Typically, GP practitioners use much higher replacement rates. However, since we use dynamic \nsubset selection, only a subset of benchmarks is evaluated in a generation. Thus, we need a lower replacement \nrate in order to increase the likelihood that a given expression will be tested on more than one subset \nof benchmarks. The mutation operator, which is discussed in the same section, mutates roughly 5% of the \nnew expressions. Finally, we use a tournament size of 7 when selecting the .ttest expressions. This setting \ncauses moderate selection pressure. The following three sections build upon the methodology described \nin this section by presenting individual case stud\u00adies. Results for each of the case studies are included \nin their respective sections. 5. CASE STUDY I: HYPERBLOCK FORMATION This section describes the operation \nof our system in the context of a speci.c compiler optimization: hyperblock for\u00admation. Here we introduce \nthe optimization, and then we discuss factors that might be important when creating a pri\u00adority function \nfor it. We conclude the section by presenting experimental results for hyperblock formation. Architects \nhave proposed two noteworthy methods for de\u00adcreasing the costs associated with control transfers3:im\u00adproved \nbranch prediction, and predication. Improved branch prediction algorithms would obviously increase processor \nuti\u00adlization. Unfortunately, some branches are inherently unpre\u00addictable, and hence, even the most sophisticated \nalgorithm would fail. For such branches, predication may be a fruitful alternative. The Pentium r It \nsquashes up to . 4 architecture features 20 pipeline stages. 126 in-flight instructions when it mispredicts. \n Rather than relying on branch prediction, predication al\u00adlows a multiple-issue processor to simultaneously \nexecute the taken and fall-through paths of control .ow. The processor nulli.es all instructions in the \nincorrect path. In this model, a predicate operand guards the execution of every instruc\u00adtion. If the \nvalue of the operand is true, then the instruction executes normally. If however, the operand is false, \nthe pro\u00adcessor nulli.es the instruction, preventing it from modifying processor state. Figure 3 highlights \nthe di.erence between control-.ow and predicated execution. Part (a) shows a segment of control-.ow. \nUsing a process dubbed if-conversion, the IM-PACT predicating compiler merges disjoint paths of execu\u00adtion \ninto a predicated hyperblock. A hyperblock is a predi\u00adcated single-entry, multiple-exit region. Part \n(b) shows the hyperblock corresponding to the control-.ow in part (a). Here, p2 and p3 are mutually exclusive \npredicates that are set according to the branch condition in part (a). Though predication e.ectively \nexposes ILP, simply pred\u00adicating everything will diminish performance by saturating machine resources \nwith useless instructions. However, an appropriate balance of predication and branching can dras\u00adtically \nimprove performance. 5.1 Feature Extraction In the following list we give a brief overview of several \ncri\u00adteria that are useful to consider when forming hyperblocks. Such criteria are often referred to as \nfeatures. In the list, a path refers to a path of control .ow (i.e.,asequence of basic blocks that are \nconnected by edges in the control .ow graph): Path predictability: Predictable branches incur no misprediction \npenalties, and thus, should probably re\u00admain unpredicated. Combining multiple paths of ex\u00adecution into \na single predicated region uses precious machine resources [15]. In this case, using machine resources \nto parallelize individual paths is typically wiser.  Path frequency: Infrequently executed paths are \nprobably not worth predicating. Including the path in a hyperblock would consume resources, and could \nnegatively a.ect performance.  Path ILP: If a path s level of parallelism is low, it may be worthwhile \nto predicate the path. In other words, if a path does not fully use machine resources, combining it with \nanother sequential path probably will not di\u00adminish performance. Because predicated instructions do not \nneed to know the value of their guarding pred\u00adicate until late in the pipeline, a processor can sustain \nhigh levels of ILP.  Number of instructions in path: Long paths use up machine resources, and if predicated, \nwill likely slow execution. This is especially true when long paths are combined with short paths. Since \nevery instruc\u00adtion in a hyperblock executes, long paths e.ectively delay the time to completion of short \npaths. The cost of misprediction is relatively high for short paths. If the processor mispredicts on \na short path, the pro\u00adcessor has to nullify all the instructions in the path, and the subsequent control-independent \ninstructions fetched before the branch condition resolves.  Number of branches in path: Paths of control \nthrough several branches have a greater chance of mis\u00adpredicting. Therefore, it may be worthwhile to \npredi\u00adcate such paths. On the other hand, including several such paths may produce large hyperblocks \nthat satu\u00adrate resources.  Compiler optimization considerations: Paths that contain hazard conditions \n(i.e., pointer dereferences and procedure calls) limit the e.ectiveness of many compiler optimizations. \nIn the presence of hazards, a compiler must make conservative assumptions. The code in Figure 3(a) could \nbene.t from predication. Without architectural support, the load from *inp can\u00adnot be hoisted above the \nbranch. The program will behave unexpectedly if the load is not supposed to ex\u00adecute and it accesses \nprotected memory. By removing branches from the instruction stream, predication af\u00adfords the scheduler \nfreer code motion opportunities. For instance, the predicated hyperblock in Figure 3(b) allows the scheduler \nto rearrange memory operations without control-.ow concerns.  Machine-speci.c considerations: A heuristic \nshould account for machine characteristics. For instance, the branch delay penalty is a decisive factor. \n Clearly, there is much to consider when designing a heuris\u00adtic for hyperblock selection. Many of the \nabove considera\u00adtions make sense on their own, but when they are put to\u00adgether, contradictions arise. \nFinding the right mix of criteria to construct an e.ective priority function is nontrivial. That is why \nwe believe automating the decision process is crucial.  5.2 Trimaran s Heuristic We now discuss the \nheuristic employed by Trimaran s IM-PACT compiler for creating predicated hyperblocks [15, 16]. The IMPACT \ncompiler begins by transforming the code so that it is more amenable to hyperblock formation [15]. IMPACT \ns algorithm then identi.es acyclic paths of con\u00adtrol that are suitable for hyperblock inclusion. Park \nand Schlansker detail this portion of the algorithm in [21]. A pri\u00adority function which is the critical \ncalculation in the predi\u00adcation decision process assigns a value to each of the paths based on characteristics \nsuch as the ones just described [15]. Some of these characteristics come from runtime pro.ling. IMPACT \nuses the priority function shown below: . 0.25 : if pathi contains a hazard. hi = 1:if pathi is hazard \nfree. dep heighti d ratioi = maxj=1.N dep heightj num opsi o ratioi = maxj=1.N num opsj priorityi = exec \nratioi \u00b7 hi \u00b7 (2.1 - d ratioi - o ratioi)(1) The heuristic applies the above equation to all paths in \na predicatable region. Based on a runtime pro.le, exec ratio is the probability that the path is executed. \nThe prior\u00adity function also penalizes paths that contain hazards (e.g., Feature Description Registers \n64 general-purpose registers, 64 .oating\u00adpoint registers, and 256 predicate registers. Integer units \n4 fully-pipelined units with 1-cycle la\u00adtencies, except for multiply instructions, which require 3 cycles, \nand divide instruc\u00adtions, which require 8. Floating-point units 2 fully-pipelined units with 3-cycle \nlaten\u00ad cies, except for divide instructions, which require 8 cycles. Memory units 2 memory units. L1 \ncache accesses take 2 cycles, L2 accesses take 7 cycles, and L3 accesses require 35 cycles. Stores are \nbu.ered, and thus require 1 cycle. Branch unit 1 branch unit. Branch prediction 2-bit branch predictor \nwith branch misprediction penalty. a 5-cycle Table 3: Architectural characteristics. This table de\u00adscribes \nthe EPIC architecture over which we evolved. This model approximates the Intel Itanium architecture. \npointer dereferences and procedure calls). Such paths may constrain aggressive compiler optimizations. \nTo avoid large hyperblocks, the heuristic is careful not to choose paths that have a large dependence \nheight (dep height)with respect to the maximum dependence height. Similarly it penalizes paths that contain \ntoo many instructions (num ops). IMPACT s algorithm then merges the paths with the high\u00adest priorities \ninto a predicated hyperblock. The algorithm stops merging paths when it has consumed the target archi\u00adtecture \ns estimated resources. 5.3 Experimental Setup This section discusses the experimental results for opti\u00admizing \nTrimaran s hyperblock selection priority function. Trimaran is an integrated compiler and simulator for \na pa\u00adrameterized EPIC architecture. Table 3 details the speci.c architecture over which we evolved. This \nmodel resembles Intel s Itanium r . architecture. We modi.ed Trimaran s IMPACT compiler by replacing \nits hyperblock formation priority function (Equation 1) with our GP expression parser and evaluator. \nThis allows IM-PACT to read an expression and evaluate it based on the values of human-selected features \nthat might be important for creating e.ective priority functions. Table 4 describes these features. The \nhyperblock formation algorithm passes the features in the table as parameters to the expression evaluator. \nFor instance, if an expression contains a reference to dep height, the path s dependence height will \nbe used when the expres\u00adsion is evaluated. Most of the characteristics in Table 4 were already available \nin IMPACT. Equation 1 has a local scope. To provide some global information, we also extract the minimum, \nmaximum, mean, and standard deviation of all path-speci.c characteristics in the table. We added a 2-bit \ndynamic branch predictor to the sim\u00adulator and we modi.ed the compiler s pro.ler to extract branch predictability \nstatistics. Lastly, we enabled the fol\u00adlowing compiler optimizations: function inlining, loop un\u00adrolling, \nbackedge coalescing, acyclic global scheduling [6], modulo scheduling [25], hyperblock formation, register \nallo\u00adcation, machine-speci.c peephole optimization, and several classic optimizations. Feature Description \ndep height The maximum instruction dependence height over all instructions in path. num ops The total \nnumber of instructions in the path. exec ratio How frequently this path is executed com\u00ad pared to other \npaths considered (from pro.le). num branches The total number of branches in the path. Table 4: Hyperblock \nselection features. The compiler writer chooses interesting attributes, and the system evolves a priority \nfunction based on them. We rely on pro.le information to extract some of these parameters. We also include \nthe min, mean, max, and standard devi\u00adation of path characteristics. This provides some global information \nto the greedy local heuristic.  5.4 Experimental Results We use the familiar benchmarks in Table 5 to \ntest our sys\u00adtem. All of the Trimaran certi.ed benchmarks are included in the table4 [24]. Our suite \nalso includes many of the Media\u00adbench benchmarks [14]. The build process for ghostscript proved too di.cult \nto compile. We also exclude the remain\u00adder of the Mediabench applications because the Trimaran system \ndoes not compile them correctly5 . We begin by presenting results for application-specialized heuristics. \nFollowing this, we show that it is possible to use Meta Optimization to create general-purpose heuristics. \n 5.4.1 Specialized Priority Functions Specialized heuristics are created by optimizing a prior\u00adity function \nfor a given application. In other words, we train the priority function on a single benchmark. Figure \n4 shows that Meta Optimization is extremely e.ective on a per-benchmark basis. The dark bar shows the \nspeedup (over Trimaran s baseline heuristic) of each benchmark when run with the same data on which it \nwas trained. The light bar shows the speedup attained when the benchmark processes a data set that was \nnot used to train the priority function. We call this the novel data set. 4 Due to preexisting bugs in \nTrimaran, we could not get 134.perl to execute correctly, though [24] certified it. 5 We exclude cjpeg, \nthe complement of djpeg, because it does not execute prop\u00aderly when compiled with some priority functions. \nOur system can also be used to uncover bugs!   Benchmark Suite Description  codrle4 See [4] RLE type \n4 encoder/decoder. decodrle4 hu. enc See [4] A Hu.man encoder/decoder. hu. dec djpeg Mediabench Lossy \nstill image decompressor. g721encode Mediabench CCITT voice g721decode compressor/decompressor. mpeg2dec \nMediabench Lossy video decompressor. rasta Mediabench Speech recognition application. rawcaudio Mediabench \nAdaptive di.erential pulse code rawdaudio modulation audio encoder/decoder. toast Mediabench Speech transcoder. \nunepic Mediabench Experimental image decompressor. 085.cc1 SPEC92 gcc C compiler.    osdemo Mediabench \nPart of a 3-D graphics library mipmap Mediabench similar to OpenGL. 129.compress 132.ijpeg SPEC95 SPEC95 \nIn-memory .le compressor and decompressor. JPEG compressor and decompressor. 130.li SPEC95 Lisp interpreter. \n124.m88ksim SPEC95 Processor simulator. 147.vortex SPEC95 An object oriented database. Table 5: Benchmarks \nused. The set includes applica\u00adtions from the SpecInt, SpecFP, and Mediabench bench\u00admark suites, as well \nas a few miscellaneous programs. Intuitively, in most cases the training input data achieves a better \nspeedup. Because Meta Optimization is performance\u00addriven, it selects priority functions that excel on \nthe training input data. The alternate input data likely exercises dif\u00adferent paths of control .ow paths \nwhich may have been unused during training. Nonetheless, in every case, the application-speci.c priority \nfunction outperforms the base\u00adline. Figure 5 shows .tness improvements over generations. In many cases, \nMeta Optimization .nds a superior priority function quickly, and .nds only marginal improvements as the \nevolution continues. In fact, the baseline priority func\u00adtion is quickly obscured by GP-generated expressions. \nOf\u00adten, the initial population contains at least one expression that outperforms the baseline. This means \nthat by simply creating and testing 399 random expressions, we were able to .nd a priority function that \noutperformed Trimaran s for the given benchmark. Once GP has discovered a decent solution, the search \nspace and operator dynamics are such that most o.spring will be worse, some will be equal and very few \nturn out to be better. This seems indicative of a steep hill in the solution space. In addition, multiple \nreruns using di.erent initializa\u00adtion seeds reveal minuscule di.erences in performance. It might be a \nspace in which there are many possible solutions associated with a given .tness. 5.4.2 General-Purpose \nPriority Functions We divided the benchmarks in Table 5 into two sets6:a training set, and a test set. \nInstead of creating a priority 6 We chose to train mostly on Mediabench applications because they compile \nand run faster than the Spec benchmarks. Speedup 3 2.5 2 1.5 1 0.5 0 Train data set 1.54 1.23 Novel data \nset  129.compress g721encode g721decode huff_dec huff_enc rawcaudio rawdaudio toast mpeg2dec Average \ndecodrle4codrle4g721decodeg721encoderawdaudiorawcaudiotoastmpeg2dec124.m88ksim129.compresshuff_enchuff_decAverage \nFigure 4: Hyperblock specialization. This graph shows speedups obtained by training on a per-benchmarks \nba\u00adsis. The dark colored bars are executions using the same data set on which the specialized priority \nfunction was trained. The light colored bars are executions that use an alternate, or novel data set. \n Figure 6: Training on multiple benchmarks. A sin\u00adgle priority function was obtained by training over \nall thebenchmarksinthisgraph. Thedark barsrepresent speedups obtained by running the given benchmark \non the same data that was used to train the priority func\u00adtion. The light bars correspond to a novel \ndata set.  refers to this as cross validation. Since the benchmarks in the test set are not related \nto the benchmarks in the training set, this is a measure of the priority function s generality. The results \nof the cross validation are shown in Figure 7. This experiment applies the best priority function on \nthe training set to the benchmarks in the test set. The av\u00aderage speedup on the test set is 9%. In three \ncases (un\u00adepic, 023.eqntott, and 085.cc1) Trimaran s baseline heuris\u00adtic marginally outperforms the GP-generated \npriority func\u00adtion. For the remaining benchmarks, the heuristic our sys\u00adtem found is better. 5.4.3 The \nBest Priority Function 0 5 101520253035404550 Generation Figure 5: Hyperblock formation evolution. This \n.gure graphs the best .tness over generations. For this prob\u00adlem, Meta Optimization quickly .nds a priority \nfunction that outperforms Trimaran s baseline heuristic. function for each benchmark, in this section \nwe aim to .nd one priority function that works well for all the benchmarks in the training set. To this \nend, we evolve over the training set using dynamic subset selection [9]. Figure 6 shows the results of \napplying the single best pri\u00adority function to the benchmarks in the training set. The dark bar associated \nwith each benchmark is the speedup over Trimaran s base heuristic when the training input data is used. \nThis data set yields a 44% improvement. The light bar shows results when novel input data is used. The \noverall improvement for this set is 25%. It is interesting that, on average, the general-purpose pri\u00adority \nfunction outperforms the application-speci.c priority function on the novel data set. The general-purpose \nsolution is less susceptible to variations in input data because it was trained to be more general. We \nthen apply the resulting priority function to the bench\u00admarks in the test set. The machine-learning community \n Figure 8 shows the best general-purpose priority function our system found for hyperblock selection. \nBecause par\u00adsimony pressure favors small expressions, most of our sys\u00adtem s solutions are readable. Nevertheless, \nthe expressions presented in this paper have been hand simpli.ed for ease of discussion. Notice that \nsome parts of the expression have no im\u00adpact on the overall result. For instance, removing the sub\u00adexpression \non line 2 will not a.ect the heuristic; the value is invariant to a scheduling region since the mean \nexecution ratio is the same for all paths in the region. Such use\u00adless expressions are called introns. \nIt turns out that introns are actually quite useful for preserving good building blocks during crossover \nand mutation [13]. The conditional multiply statement on line 4 does have a direct e.ect on the priority \nfunction: it favors paths that do not have pointer dereferences (because the sub-expression in line 5 \nwill always be greater than one). Pointers inhibit the e.ectiveness of the scheduler and other compiler \nopti\u00admizations, and thus dereferences should be penalized. The IMPACT group came to the exact same conclusion, \nthough the extent to which they penalize dereferences di.ers [15]. The sub-expression on line 8 favors \nbushy parallel paths, where there are numerous independent operations. This re\u00adsult is somewhat counterintuitive \nsince highly parallel paths will quickly saturate machine resources. In addition, paths unepicdjpegrasta023.eqntott132.ijpeg052.alvinn147.vortex085.cc1art \n130.liosdemomipmapAverage Figure 7: Cross validation of the general-purpose prior\u00adity function. The best \npriority function found by training on the benchmarks in Figure 6 is applied to the bench\u00admarks in this \ngraph. (1) (add (2) (sub (mul exec ratio mean 0.8720) 0.9400) (3) (mul 0.4762 (4) (cmul (not mem hazard) \n (5) (mul 0.6727 num paths) (6) (mul 1.1609  (7) (add (8) (sub (9) (mul  (10) (div num ops dep height) \n10.8240) (11) exec ratio ) (12) (sub (mul (cmul has unsafe jsr (13) predict product mean  (14) 0.9838) \n(15) (sub 1.1039 num ops max)) (16) (sub (mul dep height mean (17) num branches max )  (18) num paths \n))))))) Figure 8: The best priority function our system found for hyperblock scheduling. with higher \nexec ratio s are slightly penalized, which also de.es intuition. The conditional multiply expression \non line 12 penalizes paths with unsafe calls (i.e., calls to subroutines that may have side e.ects). \nOnce again this agrees with the IMPACT group s reasoning [15]. Because Trimaran is such a large and complicated \nsys\u00adtem, it is di.cult to know exactly why the priority function in Figure 8 works well. This is exactly \nthe point of us\u00ading a methodology like Meta Optimization. The bountiful complexities of compilers and \nsystems are di.cult to un\u00adderstand. Also worthy of notice is the fact that we get such good speedups, \nparticularly on the training set, by changing such a small portion of the compiler. The next section \npresents another case study, which we also test on Trimaran. 6. CASE STUDY II: REGISTER ALLOCATION The \nimportance of register allocation is well-known, so we will not motivate the optimization here. Many \nregister al\u00adlocation algorithms use cost functions to determine which variables to spill when spilling \nis required. For instance in priority-based coloring register allocation, the priority func\u00adtion is an \nestimate of the relative bene.ts of storing a given variable in a register [7]. Priority-based coloring \n.rst associates a live range with every variable. A live range is the composition of code segments (basic \nblocks), through which the associated vari\u00adable s value must be preserved. The algorithm then pri\u00adoritizes \neach live range based on the estimated execution savings of register allocating the associated variable: \nsavingsi = wi \u00b7 (LDsave \u00b7 usesi + ST save \u00b7 defsi) (2) priority(lr)= i.lr savingsi N (3) Equation 2 \nis used to compute the savings of each code segment. LDsave and ST save are estimates of the execu\u00adtion \ntime saved by keeping the associated variable in a reg\u00adister for references and de.nitions respectively. \nusesi and defsi represent the number of uses and de.nitions of a vari\u00adable in block i. wi is the estimated \nexecution frequency for the block. Equation 3 sums the savings over the N blocks that com\u00adpose the live \nrange. Thus, this priority function represents the savings incurred by accessing a register instead of \nre\u00adsorting to main memory. The algorithm then tries to assign registers to live ranges in priority order. \nPlease see [7] for a complete description of the algorithm. For our purposes, the important thing to \nnote is that the success of the algorithm depends on the priority function. The priority function described \nabove is intuitive it as\u00adsigns weights to live ranges based on the estimated execution savings of register \nallocating them. Nevertheless, our system .nds functions that improve the heuristic by up to 11%.  \n6.1 Experimental Results We collected these results using the same experimental setup that we used for \nhyperblock selection. We use Tri\u00admaran and we target the architecture described in Table 3. However, \nto more e.ectively stress the register allocator, we only use 32 general-purpose registers and 32 .oating-point \nregisters. We modi.ed Trimaran s Elcor register allocator by re\u00adplacing its priority function (Equation \n2) with an expres\u00adsion parser and evaluator. The register allocation heuristic described above essentially \nworks at the basic block level. Equation 3 simply sums and normalizes the priorities of the individual \nbasic blocks. For this reason, we stay within the algorithm s framework and leave Equation 3 intact. \nFor a more detailed description of our experiments with register allocation, including the features we \nextracted to perform them, please see [23]. 6.1.1 Specialized Priority Functions These results indicate \nthat Meta Optimization works well, even for well-studied heuristics. Figure 9 shows speedups obtained \nby specializing Trimaran s register allocator for a given application. The dark bar associated with each \nappli\u00adcation represents the speedup obtained by using the same input data that was used to specialize \nthe heuristic. The light bar shows the speedup when the benchmark processes a novel data set. 1 1.15 \n1.1 1.15 1.05 1.1 Figure 10: Register allocation evolution. This .gure graphs .tness over generations. \nUnlike the hyperblock selection evolution, these .tnesses improve gradually. Once again, it makes sense \nthat the training input data outperforms the alternate input data. In the case of reg\u00adister allocation \nhowever, we see that the disparity between speedups on training and novel data is less pronounced than \nit is with hyperblock selection. This is likely because hyper\u00adblock selection is extremely data-driven. \nAn examination of the general-purpose hyperblock formation heuristic reveals two dynamic factors (exec \nratio and predict product mean) that are critical components in the hyperblock decision pro\u00adcess. Figure \n10 graphs .tness improvements over generations. It is interesting to contrast this graph with Figure \n5. The fairly constant improvement in .tness over several genera\u00adtions seems to suggest that this problem \nis harder to op\u00adtimize than hyperblock selection. Additionally, unlike the hyperblock selection algorithm, \nthe baseline heuristic typi\u00adcally remained in the population for several generations. Figure 9: Register \nallocation specialization. This graph shows speedups obtained by training on a per\u00adbenchmarks basis. \nThe dark colored bars are executions using the same data set on which the specialized pri\u00adority function \nwas trained. The light colored bars are executions that use a novel data set. 0.95 1 0.9 0.95 0.85 0.9 \n0 5 101520253035404550 Generation Figure 12: Cross validation of the general-purpose reg\u00adister allocation \npriority function. The best priority func\u00adtion found by the DSS run is applied to the benchmarks in this \ngraph. Results from two target architectures are shown. 6.1.2 General-Purpose Priority Functions Just \nas we did in Section 5.4.2, we divide our benchmarks into a training set and a test set7 . The benchmarks \nin Fig\u00adure 11 show the training set for this experiment. The .gure also shows the results of applying \nthe best priority function (from our DSS run) to all the benchmarks in the set. The dark bar associated \nwith each benchmark is the speedup over Trimaran s baseline heuristic when using the training input data. \nThe average for this data set is 3%. On a novel data set we attain an average speedup of 3%, which indicates \nthat register allocation is not as susceptible to variations in input data. Figure 12 shows the cross \nvalidation results for this ex\u00ad 7 This experiment uses smaller test and training sets due to preexisting \nbugs in Trimaran. It does not correctly compile several of our benchmarks when target\u00ading a machine with \n32 registers. Figure 11: Training a register allocation priority func\u00adtion on multiple benchmarks. Our \nDSS evolution trained on all the benchmarks in this .gure. The single best pri\u00adority function was applied \nto all the benchmarks. The dark bars represent speedups obtained by running the given benchmark on the \nsame data that was used to train the priority function. The light bars correspond to an alternate data \nset.   Speedup mpeg2dec rawcaudio 129.compress huff_enc huff_dec g721decode Average Training data set \n1.081.06 Novel data set decodrle4 codrle4 124.m88ksim unepic djpeg 023.eqntott132.ijpeg147.vortex 085.cc1 \n130.li Average 129.compress  g721decode g721encode huff_enc huff_dec rawcaudio rawdaudio mpeg2dec Average \nSpeedup Train data set 1.031.03 Novel data set 2.50 2.00 Training data 1.351.40 Novel data  Speedup \n1.00 0.50 0.00 101.tomcatv 102.swim 103.su2cor 125.turb3d 146.wave5 093.nasa7 015.doduc 034.mdljdp2 107.mgrid \n141.apsi Average 0 5 1015202530354045 50 Generation Figure 13: Prefetching specialization. This graph \nshows speedups obtained by training on a per-benchmarks ba\u00adsis. The dark colored bars are executions \nusing the same data set on which the specialized priority function was trained. The light colored bars \nare executions that use a novel data set. periment. The .gure shows the speedups (over Trimaran s baseline) \nachieved by applying the single best priority func\u00adtion to a set of benchmarks that were not in the training \nset. The learned priority function outperforms the baseline for all benchmarks except decodrle4 and 132.ijpeg. \nAlthough the overall speedup on the cross validation set is only 3%, this is an exciting result. Register \nallocation is well-studied optimization which our technique is able to improve. 7. CASE STUDY III: DATA \nPREFETCHING This section describes another memory hierarchy opti\u00admization. Data prefetching is an optimization \naimed at re\u00adducing the costs of long-latency memory accesses. By mov\u00ading data from main memory into cache \nbefore it is accessed, prefetching can e.ectively reduce memory latencies. However, prefetching can degrade \nperformance in many cases. For instance, aggressive prefetching may evict useful data from the cache \nbefore it is needed. In addition, adding unnecessary prefetch instructions may hinder instruction ca\u00adche \nperformance and saturate memory queues. The Open Research Compiler (ORC) [19] uses an exten\u00adsion of Mowry \ns algorithm [18] to insert prefetch instruc\u00adtions. ORC uses a priority function that assigns a Boolean \ncon.dence to prefetching a given address. Subsequent passes use this value to determine whether or not \nto prefetch the address. Currently, the priority function is simply based upon how well the compiler \ncan estimate loop trip counts.  7.1 Experimental Setup This case study is di.erent from those already \npresented in two important ways. First, we collected the results of this section in the context of a \nreal machine: we use the Open Research Compiler, and we target an Itanium I architecture. Just as with \nthe previous two case studies, the .tness of an expression is the speedup over the baseline priority \nfunction. However, unlike simulated execution which is perfectly re\u00adproducible, real environments are \ninherently noisy. Even on an unloaded system, back-to-back runs of a program may vary. Figure 14: Prefetching \nevolution. This .gure graphs .t\u00adness over generations. The baseline expression is quickly weeded out \nof the population. Fortunately, GP can handle noisy environments, as long as the level of noise is smaller \nthan attainable speedups us\u00ading our technique. For the Itanium processor, this is in\u00addeed the case. Since \nit is a single threaded, statically sched\u00aduled processor, our measurements are fairly consistent; vari\u00adations \ndue to noise are well within the range of our attained speedups. Another major divergence from the methodology \nemployed in the last two case studies is the format of the priority func\u00adtion that we aim to optimize. \nWhereas the priority functions for register allocation and hyperblock formation are real\u00advalued, the \nprefetching priority function is Boolean-valued. This case study emphasizes GP s .exibility. As the ORC \nwebsite recommends, we compile all bench\u00admarks with -O3 optimizations enabled, and we use pro.le\u00addriven \nfeedback. For additional details such as the features we extracted for this optimization, please see \n[23]. 7.2 Experimental Results Prefetching is known to be an e.ective technique for .oat\u00ading point benchmarks, \nand for this reason we train on various SPECFP benchmarks in this case study. 7.2.1 Specialized Priority \nFunctions Figure 13 shows the results of the ten di.erent application\u00adspecialized priority functions. \nCloser examination of the GP solutions reveals that ORC overzealously prefetches and that by simply taming \nthe compiler s aggressiveness, one can sub\u00adstantially improve performance (on this set of benchmarks). \nThe GP solutions rarely prefetched. In fact, shutting o. prefetching altogether achieves gains within \n7% of the spe\u00adcialized priority functions. Figure 14 graphs .tness over generation for the application\u00adspeci.c \nexperiments. Just as with hyperblock selection, the baseline priority function has no impact on the .nal \nsolutions it is quickly obscured by superior expressions. As is the case with hyperblock selection, it \nappears that in many cases, GP solutions get stuck in a local minimum in the solution space; the .tnesses \nstop improving early in the evolution. One plausible explanation for this is our use of parsimony pressure \nin the selection process. For application\u00adspeci.c evolutions, it is often the case that very small expres\u00adsions \nwork well. While these small expressions are e.ective, Speedup 3.0 2.5 2.0 1.5 1.0 0.5 0.0 Train data \nset Novel data set  1.311.36    101.tomcatv102.swim103.su2cor125.turb3d146.wave5093.nasa7015.doduc034.mdljdp2107.mgrid141.apsiAverage \n 168.wupwise171.swim172.mgrid173.applu178.galgel183.equake187.facerec188.ammp189.lucas200.sixtrack301.apsi191.fma3dAverage \nFigure 15: Training a prefetching priority function on multiple benchmarks. Our DSS evolution trained \non all the benchmarks in this .gure. The single best priority function was applied to all the benchmarks. \nThe dark bars represent speedups obtained by running the given benchmark on the same data that was used \nto train the priority function. The light bars correspond to an alter\u00adnate data set. they limit the genetic \nmaterial available to the crossover operator. Furthermore, since we always keep the best ex\u00adpression, \nthe population soon becomes inbred with copies of the top expression. Future work will explore the impact \nof parsimony pressure and elitism.  7.2.2 General-Purpose Priority Functions The performance of the \nbest DSS-generated prefetching priority function is shown in Figure 15. The priority function was trained \non the same benchmarks in the .gure, which are a combination of SPEC92 and SPEC95 .oating point benchmarks. \nData prefetching, like hyperblock selection, is extremely data-driven. By applying the same input data \nthat we used to train the priority function we achieve a 31% speedup. Somewhat surprisingly, the novel \ninput data set achieves a better speedup of 36%. Because the priority function learned to prefetch infrequently, \nit is simply the case that the novel data set is more sensitive to prefetching than the training data \nset is. Figure 16 shows the cross validation results for this opti\u00admization, and prompts us to mention \na caveat of our tech\u00adnique. GP s ability to identify good general-purpose so\u00adlutions is based on the \nbenchmarks over which they are evolved. For the SPEC92 and SPEC95 benchmarks that were used to train \nour general-purpose heuristic, aggressive prefetching was debilitating. However, for a couple of bench\u00admarks \nin the SPEC2000 .oating point set, we see that ag\u00adgressive prefetching is desirable. Thus, unless designers \ncan assert that the training set provides adequate problem cov\u00aderage, they cannot completely trust GP-generated \nsolutions.  8. RELATED WORK Many researchers have applied machine-learning methods to compilation, and \ntherefore, only the most relevant works are cited here. Calder et al. used supervised learning techniques \nto .ne\u00adtune static branch prediction heuristics [5]. They employ Figure 16: Cross validation of the \ngeneral-purpose prefetching priority function on SPEC2000. The best priority function found by the DSS \nrun is applied to the benchmarks in this graph. Results from two target ar\u00adchitectures are shown. neural \nnetworks and decision trees to search for e.ective static branch prediction heuristics. While our methodology \nis similar, our work di.ers in several important ways. Most importantly, we use unsupervised learning, \nwhile they use supervised learning. Unsupervised learning is used to capture inherent orga\u00adnization in \ndata, and thus, only input data is required for training. Supervised learning attempts to match training \ninputs with known outcomes, called labels. This means that their learning techniques rely on knowing \nthe optimal out\u00adcome, while ours does not8 . In their case determining the op\u00adtimal outcome is trivial \nthey simply run the benchmarks in their training set and note the direction that each branch favors. \nIn this sense, their method is simply a classi.er: classify the data into two groups, either taken or \nnot-taken. Priority functions cannot be classi.ed in this way, and thus they demand an unsupervised method \nsuch as ours. We also di.er in the end goal of our learning techniques. They use misprediction rates \nto guide the learning process. While this is a perfectly valid choice, it does not necessarily re.ect \nthe bottom line: execution time. Monsifrot et al. use a classi.er based on decision tree learning to \ndetermine which loops to unroll [17]. Like [5], this supervised methodology relies on extracting labels, \nwhich is not only di.cult, in many cases it is simply not feasible. Cooper et al. use genetic algorithms \nto solve compilation phase ordering problems [8]. Their technique is quite ef\u00adfective. However, like \nother related work, they evolve the application, not the compiler9 . Thus, their compiler itera\u00adtively \nevolves every program it compiles. By evolving com\u00adpiler heuristics, and not the applications themselves, \nwe need only apply our process once as shown in Section 5.4.2. The COGEN(t) compiler creatively uses \ngenetic algorithms to map code to irregular DSPs [11]. This compiler, though interesting, also evolves \non a per-application basis. Nonethe\u00adless, the compile-once nature of DSP applications may war\u00adrant the \nlong, iterative compilation process. 8 This is a strict requirement both for decision trees and the gradient \ndescent method they use to train their neural network. 9 However, they were able to manually construct \na general-purpose sequence us\u00ading information gleaned from their application-specific evolutions. 9. \nCONCLUSION Compiler developers have always had to contend with complex phenomenon that are not easy modeled. \nFor ex\u00adample, it has never been possible to create a useful model for all the input programs the compiler \nhas to optimize. However until recently, most architectures the target of compiler optimizations were \nsimple and analyzable. This is no longer the case. A complex compiler with multiple in\u00adterdependent optimization \npasses exacerbates the problem. In many instances, end-to-end performance can only be eval\u00aduated empirically. \nOptimally solving NP-hard problems is not practical even when simple analytical models exist. Thus, heuristics \nplay a major role in modern compilers. Borrowing techniques from the machine-learning community, we created \na gen\u00aderal framework for developing compiler heuristics. We ad\u00advocate a machine-learning based methodology \nfor automat\u00adically learning e.ective compiler heuristics. The techniques presented in this paper show \npromise, but they are still in their infancy. For many applications our techniques found excellent application-speci.c \npriority func\u00adtions. However, the disparity in some cases between the application-speci.c performance \nand the general-purpose per\u00adformance tells us that our techniques can be improved. We also note disparities \nbetween the performance of train\u00ading set applications and the cross validation performance. In some cases \nour solutions over.t the training set. If compiler developers use our technique but only train using \nbench\u00admarks on which their compiler will be judged, the generality of their compiler may actually be \nreduced. Our .edgling research has a few shortcomings that future work will address. For instance, the \nsuccess of any learn\u00ading algorithm hinges on selecting the right features. We will explore techniques \nthat aid in extracting features that best re.ect program variability. While genetic programming is well-suited \nto our application, it too has shortcomings. The overriding goal of our research is to free humans from \nte\u00addious parameter tweaking. Unfortunately, GP s success is dependent on parameters such as population \nsize and mu\u00adtation rate, and .nding an adequate solution relies on some experimentation (which fortunately \ncan be performed with a minimal amount of user interaction). Future work will experiment with di.erent \nlearning techniques. We believe the bene.ts of using a system like ours far out\u00adweighs the drawbacks. \nWhile our techniques do not always achieve large speedups, they do reduce design complexity considerably. \nCompiler writers are forced to spend a large portion of their time designing heuristics. The results \npre\u00adsented in this paper lead us to believe that machine-learning techniques can optimize heuristics \nat least as well human de\u00adsigners. We believe that automatic heuristic tuning based on empirical evaluation \nwill become prevalent, and that design\u00aders will intentionally expose algorithm policies to facilitate \nmachine-learning optimization. A toolset that can be used to evolve compiler heuristics will be available \nat: http://www.cag.lcs.mit.edu/metaopt 10. ACKNOWLEDGMENTS We would like to thank the PLDI reviewers \nfor their in\u00adsightful comments. In general we thank everyone who helped us with this paper, especially \nKristen Grauman, Michael Gordon, Sam Larsen, William Thies, Derek Bruening, and Vladimir Kiriansky. Finally, \nthanks to Michael Smith and Glenn Holloway at Harvard University for lending us their Itanium machines \nin our hour of need. This work is funded by DARPA, NSF, and the Oxygen Alliance. 11. REFERENCES [1] S.G.Abraham, \nV.Kathail, andB.L.Deitrich. Meld Scheduling: Relaxing Scheduling Constaints Across Region Boundaries. \nIn Proceedings of the 29th Annual International Symposium on Microarchitecture (MICRO-29), pages 308 \n321, 1996. [2] W. Banzhaf, P. Nordin, R. Keller, and F. Francone. Genetic Programming : An Introduction \n: On the Automatic Evolution of Computer Programs and Its Applications. Morgan Kaufmann, 1998. [3] D. \nBernstein, D. Goldin, and M. G. et. al. Spill Code Minimization Techniques for Optimizing Compilers. \nIn Proceedings of the SIGPLAN 89 Conference on Programming Language Design and Implementation, pages \n258 263, 1989. [4] D. Bourgin. Losslessy compression schemes. http://hpux.u-aizu.ac.jp/hppd/hpux\u00ad/Languages/codecs-1.0/. \n [5] B. Calder, D. G. ad Michael Jones, D. Lindsay, J. Martin, M. Mozer, and B. Zorn. Evidence-Based \nStatic Branch Prediction Using Machine Learning. In ACM Transactions on Programming Languages and Systems \n(ToPLaS-19), volume 19, 1997. [6] P. Chang, D. Lavery, S. Mahlke, W. Chen, and W. Hwu. The Importance \nof Prepass Code Scheduling for Superscalar and Superpipelined processors. In IEEE Transactions on Computers, \nvolume 44, pages 353 370, March 1995. [7] F. C. Chow and J. L. Hennessey. The Priority-Based Coloring \nApproch to Register Allocation. In ACM Transactions on Programming Languages and Systems (ToPLaS-12), \npages 501 536, 1990. [8] K. Cooper, P. Scheilke, and D. Subramanian. Optimizing for Reduced Code Space \nusing Genetic Algorithms. In Languages, Compilers, Tools for Embedded Systems, pages 1 9, 1999. [9] \nC. Gathercole. An Investigation of Supervised Learning in Genetic Programming.PhD thesis, University \nof Edinburgh, 1998.  [10] P. B. Gibbons and S. S. Muchnick. E.cient Instruction Scheduling for a Pipelined \nArchitecture. In Proceedings of the ACM Symposium on Compiler Construction, volume 21, pages 11 16, 1986. \n [11] G. W. Grewal and C. T. Wilson. Mappping Reference Code to Irregular DSPs with the Retargetable, \nOptimizing Compiler COGEN(T). In International Symposium on Microarchitecture, volume 34, pages 192 202, \n2001 . [12] M. Kessler and T. Haynes. Depth-Fair Crossover in Genetic Programming. In Proceedings of \nthe ACM Symposium on Applied Computing, pages 319 323, February 1999. [13] J. Koza. Genetic Programming: \nOn the Programming of Computers by Means of Natural Selection.The MIT Press, 1992. [14] C. Lee, M. Potkonjak, \nand W. H. Mangione-Smith. MediaBench: a tool for evaluating and synthesizing multimedia and communication \nsystems. In International Symposium on Microarchitecture, volume 30, pages 330 335, 1997. [15] S. A. \nMahlke. Exploiting instruction level parallelism in thepresenceofbranches. PhD thesis, University of \nIllinois at Urbana-Champaign, Department of Electrical and Computer Engineering, 1996. [16] S. A. Mahlke, \nD. Lin, W. Chen, R. Hank, and R. Bringmann. E.ective Compiler Support for Predicated Execution Using \nthe Hyperblock. In International Symposium on Microarchitecture, volume 25, pages 45 54, 1992. [17] A. \nMonsifrot, F. Bodin, and R. Quiniou. A Machine Learning Approach to Automatic Production of Compiler \nHeuristics. In Arti.cial Intelligence: Methodology, Systems, Applications, pages 41 50, 2002. [18] T. \nC. Mowry. Tolerating Latency through Software-Controlled Data Prefetching.PhD thesis, Stanford University, \nDepartment of Electrical Engineering, 1994 . [19] Open Research Compiler. http://ipf-orc.sourceforge.net. \n[20] E. Ozer, S. Banerjia, and T. Conte. Uni.ed Assign and Schedule: A New Approach to Scheduling for \nClustered Register Filee Microarchitectures. In Proceedings of the 27th Annual International Symposium \non Microarchitecture (MICRO-24), pages 308 315, 1998. [21] J. C. H. Park and M. S. Schlansker. On Predicated \nExecution. Technical Report HPL-91-58, Hewlett Packard Laboratories, 1991. [22] B. R. Rau. Iterative \nModulo Scheduling: An Algorithm for Software Pipelining Loops. In Proceedings of the 27th Annual International \nSymposium on Microarchitecture (MICRO-24), November 1994. [23] M. Stephenson, S. Amarasinghe, U.-M. O \nReilly, and M. Martin. Compiler Heuristic Design with Machine Learning. Technical Report TR-893, Massachusetts \nInstitute of Technology, 2003. [24] Trimaran. http://www.trimaran.org. [25] N. Warter. Modulo Scheduling \nwith Isomorphic Control Transformations. PhD thesis, University of Illinois at Urbana-Champaign, Department \nof Electrical and Computer Engineering, 1993.  \n\t\t\t", "proc_id": "781131", "abstract": "Compiler writers have crafted many heuristics over the years to approximately solve NP-hard problems efficiently. Finding a heuristic that performs well on a broad range of applications is a tedious and difficult process. This paper introduces Meta Optimization, a methodology for automatically fine-tuning compiler heuristics. Meta Optimization uses machine-learning techniques to automatically search the space of compiler heuristics. Our techniques reduce compiler design complexity by relieving compiler writers of the tedium of heuristic tuning. Our machine-learning system uses an evolutionary algorithm to automatically find effective compiler heuristics. We present promising experimental results. In one mode of operation Meta Optimization creates application-specific heuristics which often result in impressive speedups. For hyperblock formation, one optimization we present in this paper, we obtain an average speedup of 23% (up to 73%) for the applications in our suite. Furthermore, by evolving a compiler's heuristic over several benchmarks, we can create effective, general-purpose heuristics. The best general-purpose heuristic our system found for hyperblock formation improved performance by an average of 25% on our training set, and 9% on a completely unrelated test set. We demonstrate the efficacy of our techniques on three different optimizations in this paper: hyperblock formation, register allocation, and data prefetching.", "authors": [{"name": "Mark Stephenson", "author_profile_id": "81100072491", "affiliation": "Massachusetts Institute of Technology, Cambridge, MA", "person_id": "P191158", "email_address": "", "orcid_id": ""}, {"name": "Saman Amarasinghe", "author_profile_id": "81100533031", "affiliation": "Massachusetts Institute of Technology, Cambridge, MA", "person_id": "PP14184970", "email_address": "", "orcid_id": ""}, {"name": "Martin Martin", "author_profile_id": "81100426075", "affiliation": "Massachusetts Institute of Technology, Cambridge, MA", "person_id": "P517418", "email_address": "", "orcid_id": ""}, {"name": "Una-May O'Reilly", "author_profile_id": "81100434482", "affiliation": "Massachusetts Institute of Technology, Cambridge, MA", "person_id": "P286796", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/781131.781141", "year": "2003", "article_id": "781141", "conference": "PLDI", "title": "Meta optimization: improving compiler heuristics with machine learning", "url": "http://dl.acm.org/citation.cfm?id=781141"}