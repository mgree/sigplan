{"article_publication_date": "10-01-2001", "fulltext": "\n Functioning without Closure: Type-Safe Customized Function Representations for Standard ML Allyn Dimock* \n Ian Westmacott* Robert Muller \u00a7 Harvard University Boston University Boston College dimock@das.harvard.edu \nianw@bu.edu muller@cs.bc.edu Franklyn Turbak J. B. Wells*\u00b6 . Wellesley College Heriot-Watt University \nfturbak@wellesley.edu http://cee.hw.ac.uk/ jbw/ ABSTRACT The CIL compiler for core Standard ML compiles \nwhole ML programs using a novel typed intermediate language that supports the generation of type-safe \ncustomized data repre\u00adsentations. In this paper, we present empirical data compar\u00ading the relative e.cacy \nof several di.erent .ow-based cus\u00adtomization strategies for function representations. We de\u00advelop a cost \nmodel to interpret dynamic counts of operations required for each strategy. In this cost model, customizing \nthe representation of closed functions gives a 12 17% im\u00adprovement on average over uniform closure representations, \ndepending on the layout of the closure. We also present data on the relative e.ectiveness of various \nstrategies for re\u00adducing representation pollution, i.e., situations where .ow constraints require the \nrepresentation of a value to be less e.cient than it would be in ideal circumstances. For the benchmarks \ntested and the types of representation pollu\u00adtion detected by our compiler, the pollution removal strate\u00adgies \nwe consider often cost more in overhead than they gain via enabled customizations. Notable exceptions \nare selective defunctionalization, a function representation strategy that often achieves signi.cant \ncustomization bene.ts via aggres\u00adsive pollution removal, and a simple form of .ow-directed inlining, \nin which pollution removal allows multiple func\u00adtions to be inlined at the same call site. * Partially \nsupported by NSF CCR grant 9417382. Partially supported by Sun grant EDUD-7826-990410-US. Partially supported \nby NSF EIA grant 9806745/9806746/ 9806747/9806835. \u00a7Partially supported by a Faculty Fellowship of the \nCarroll School of Management, Boston College. \u00b6 Partially supported by NSF CCR grant 9988529. Partially \nsupported by EPSRC grants GR/L 36963 and GR/L 15685. Permission to make digital or hard copies of all \nor part of this work for personal or classroom use is granted without fee provided that copies are not \nmade or distributed for profit or commercial advantage and that copies bear this notice and the full \ncitation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to \nlists, requires prior specific permission and/or a fee. ICFP 01, September 3-5, 2001, Florence, Italy. \nCopyright 2001 ACM 1-58113-415-0/01/0009 ...$5.00. 1. INTRODUCTION The e.ciency of the object code generated \nby a compiler depends to a large extent on the compiler s ability to select e.cient target language representations \nof values manipu\u00adlated by the source program. For modern programming lan\u00adguages that make heavy use of \nfunctions and methods, such as Standard ML or Java, e.cient representation of functions and function \ncalling protocols is particularly important. In this paper, we report on the performance of the CIL1 \ncompiler for Standard ML [14, 16]. The CIL compiler is a type-and .ow-directed whole-program compiler \nwhich is designed to generate type-safe function representations that are customized for the contexts \nin which they are created and applied. The compiler employs a novel typed inter\u00admediate language [44] \nthat integrates polyvariant .ow in\u00adformation directly in the intermediate representation. The compiler \ngenerates code for the SPARC V8 architecture. The CIL compiler has the following key features: It performs \n.ow-based representation customizations in a type-directed compiler. In this paper, we restrict our at\u00adtention \nto customizing function representations, though the CIL framework supports customizing any type of data. \nThe intermediate representations generated within the compiler are guaranteed to be well-typed with respect \nto a type system that encodes .ow information. Thus, we address both e.ciency and reliability.  It is \nparameterized with respect to a function represen\u00adtation strategy. We have implemented seven such strate\u00adgies. \nIn this paper, we evaluate the relative e.cacy of these strategies on a variety of benchmarks based on \na cost model for function representations. We focus on se\u00adlective strategies in which closed functions \n(those with\u00adout free variables) are represented more e.ciently than open functions (those with free variables). \nOther cus\u00adtomizations supported by our framework are tagged en\u00advironment representations of functions \n(in the context of inlined open functions and defunctionalization) and .ow\u00adbased optimizations of known \nfunction calls.  1 CIL is an acronym for Church Intermediate Language. The authors are members of the \nChurch Project (http: //types.bu.edu/). The Church Project is investigating ap\u00adplications of sophisticated \ntype systems in the implementa\u00adtion of higher-order typed programming languages. It is parameterized \nwith respect to a .ow analysis. We have implemented four typed .ow analyses that vary with respect to \nprecision. All are polvariant on type and one is polyariant on variable occurrences. All of the data \nre\u00adported in this paper is derived from instrumented code generated from runs of the compiler using only \nthe typed source split analysis. This is our most accurate analysis that is polyvariant on types but \nnot on variable occur\u00adrence. It is comparable to a typed 0CFA on monomor\u00adphized code [4] restricted by \na shallow subtyping rule [44].  It can generate customized function representations even in the presence \nof representation pollution. Pollution of function representations occurs when a function is con\u00adstrained \nto have a less e.cient representation than it oth\u00aderwise would because it shares an application site \nwith an ine.ciently represented function. A complementary phenomenon occurs for function representations \nwith pol\u00adlution of application sites. Although we focus on func\u00adtions here, representation pollution \ncan occur with any data type. The CIL compiler can remove pollution by  (1) generating multiple and \nmutually incompatible cus\u00adtomized representations of value de.nition and use forms and (2) introducing \nsu.cient plumbing to ensure that only compatible representations .ow together. Recently, there has been \ngrowing interest in using typed intermediate languages to ensure the integrity of complex program transformations \nsuch as closure conversion [25, 26]. Our approach is similar to the customization strategies used by \ntype-based compilers that remove polymorphic higher\u00adorder functions via monomorphization and defunctionaliza\u00adtion \n[39, 8]. These compilers maintain type correctness dur\u00ading closure conversion by injecting closures with \ndi.erent free variables that .ow to the same application site into a sum-of-products datatype, and performing \na case dispatch on the constructed value at the application site.2 As in the CIL compiler, these compilers \nuse .ow analysis to cus\u00adtomize function representations for particular application sites. These .ow analyses \nare not integrated into their type systems, although after monomorphization and defunction\u00adalization \na .ow analysis can be implicit in their types. Customization by duplication of value construction points \nis sometimes called cloning [11]. Among other applications, it has been used for implementing lazy functional \nlanguages [17], for resolving overloading in Haskell [23], for compiling NESL [6], and particularly for \noptimizing method invocation in object-oriented languages [9, 10, 1, 13, 31, 30]. Our Results The CIL \ncompiler emits instrumented code that tracks the creation and application of functions as well as the \nplumbing associated with pollution removal. We have developed a cost model that assigns a dynamic cost \nto these points and have used it to compare the costs of our function representation strategies on a \nbenchmark suite.3 We have experimented 2Some defunctionalizing compilers avoid this run-time cost by \nusing the appropriate code pointer as a tag in the generated object code and replacing the case dispatch \nby a jump, but their type systems do not support this as a well typed operation and hence this must be \ndone in the code generator after types are erased. 3We report on compile-time space costs of the CIL \ncompiler in [16]. with two basic closure representations: (1) paired closures that pair a code pointer \nwith an environment record; and (2) .at closures that combine the code pointer and environment values \nin a single record. We consider two function representation strategies that perform no pollution removal: \n(1) a uniform strategy in which all functions are represented by closures; and (2) a polluted selective \nstrategy that represents closed functions as code pointers only when no additional plumbing is needed \nto do so. Under our cost model, our data show that compil\u00ading whole SML programs with the polluted selective \nstrategy yields SPARC code in which function representation costs are 12% lower with .at closures, and \n17% lower with paired closures, than those observed with the uniform strategy. For the benchmarks tested \nand the types of representa\u00adtion pollution detected by our compiler, our data show that the plumbing \ncosts associated with removing pollution often outweigh the bene.ts of using more e.cient function repre\u00adsentations. \nIn particular, three pollution-removing strate\u00adgies (what we call selective source split, selective sink \nsplit, and uniform defunctionalization) rarely perform better than and in many cases perform signi.cantly \nworse than the polluted selective strategy. In two other strategies, pollution removal often results \nin a net gain, sometimes signi.cant, compared to polluted se\u00adlective.In selective defunctionalization, \naggressive splitting of application sites enables many customizations, and the function representation \ncosts are often 10% or more bet\u00adter (and in one case 46% better) than polluted selective.In the inlining \nstrategy, .ow-directed inlining [45] allows cer\u00adtain functions to be represented as tagged environments \nand splitting of application sites allows multiple functions to be inlined at the same site. As with \nselective defunctionaliza\u00adtion, the gains for inlining can be impressive. However, in some cases these \ntwo strategies have much higher costs than polluted selective. In general, pollution costs are relatively \nlarger in .at than in paired closures. The remainder of this paper is organized as follows. Sec\u00adtion \n2 provides an overview of the function representations used in the CIL compiler. Section 3 gives a brief \noverview of the CIL compiler. Section 4 presents our cost model. Section 5 presents run-time statistics \nfor several standard benchmark programs. Section 6 summarizes our conclusions and describes future work. \n 2. REPRESENTATION CUSTOMIZATION AND POLLUTION An essential invariant maintained by any compiler is that \nthe representation chosen for a run-time value at its point of de.nition must be consistent with the \nrepresentation ex\u00adpected at every point where that value is used. We call this the representation invariant. \nThe simplest way to satisfy this invariant is to adopt a uniform representation assump\u00adtion (URA), under \nwhich the representation of any value is determined by the type constructor of its type and all values \nare accessed through a .xed-size interface (achieved by box\u00ading values larger than one machine word). \nThe URA sim\u00adpli.es the task of compiler writing by using a type system as a crude form of .ow analysis. \nType soundness guarantees that a value reaching an elimination form for a type con\u00adstructor must have \nbeen de.ned at an introduction form for that constructor. If the uniform representations chosen for the \nintroduction and elimination forms are consistent, then the representation invariant will automatically \nbe satis.ed. A classic example of the URA is the representation of functions. Compilers for languages \nwith higher-order func\u00adtions must at some point in the compilation process convert every open function \n(one with free variables) into a closed function (one with no free variables). One way to accom\u00adplish \nthis in a uniform manner is to represent every function uniformly as a closure, which pairs (1) a code \npointer to a closed function with (2) an environment containing the val\u00adues of the free variables of \nthe function. Each code pointer addresses a function that expects an argument that pairs (1) the argument \nof the original function with (2) the en\u00advironment of the closure. The process of transforming all functions \ninto closures is known as closure conversion. In this discussion, we will compare closures with several \nother function representations in the context of a simple concrete example. Figure 1(a) illustrates a \nfragment from a higher-order functional program.4 The fragment shows three function abstractions (.) \nand two application sites (@). These pieces are represented graphically to highlight how the function \nvalues created at abstraction sites .ow to application sites. As in CIL, some sites are annotated with \n.ow labels that approximate the .ow of values in the pro\u00adgram. 5 In the .l notation, l is a source label \nidentifying a . function de.nition point and . isaset of sink labels conser\u00advatively approximating those \ncall sites at which the function labeled l may be used. Dually, the notation @fk speci.es a sink label \nk identifying a function use point and a set of source labels f conservatively approximating the functions \nthat may reach that point. In the .gure, the abstraction bodies P , Q, R are superscripted with the set \nof free vari\u00adables they reference (not including the abstraction param\u00adeter). Thus, .3 {5} is an open \nfunction with free variables a and b, while .1 and .2 are closed functions. {4}{4,5} The result of closure \nconverting the fragment in Figure 1(a) is shown in Figure 1(b). Both closures and environments are represented \nas tuples (parenthesized sequences of elements separated by commas). Projections from these tuples are \nimplicit in the destructuring notation for . and let bind\u00adings. Although we show only untyped terms in \nthe example, the terms are typable, albeit with some di.culty. In a naive approach, closure converting \ntwo functions of the same type can yield closures whose types di.er due to di.erences in free variables \nexposed by the transformation. This problem is usually addressed with existential types [25, 27, 12]. \nIn contrast, CIL uses intersection and union types to solve this problem; as argued in [44], this has \nadvantages over existen\u00adtial quanti.ers for customizing data representations. The URA simpli.es compiler \nwriting and facilitates the support of features like type polymorphism, separate compi\u00adlation, and dynamic \nlinking. However, because it e.ectively 4The syntactic notation used in the example is a stylized notation \nchosen to simplify the presentation of the example. This notation di.ers signi.cantly from real CIL, \nas de\u00adscribed in [16]. In particular, typed CIL terms would require signi.cantly more annotations in \norder to be type correct. Even untyped CIL terms would be more verbose than the notation in the .gure, \nsince CIL does not support destruc\u00adturing in variable binding positions. 5In the CIL compiler, every \nintroduction and elimination form, as well as every type constructor, is annotated with .ow labels. To \nreduce visual clutter, we only highlight func\u00adtion value labels in the example. requires worst-case \nrepresentations to be used for all values, the URA stands in the way of customized representations and \nclassical optimizations. For instance, in Figure 1(b), the closed functions .1 {4} and .2 {4,5} must \nbe represented as closures with empty environments, because all application sites have been compiled \nassuming this representation. Ide\u00adally, we would prefer to use what Wand and Steckler call a selective \nfunction representation, in which closed functions are represented as code pointers and function invocations \nare implemented via a jump to this code pointer [41]. This avoids allocating and projecting from a closure \npair. The key di.culty with using such customized representa\u00adtions is that care must be taken that they \ndo not invalidate the representation invariant. One approach to customiza\u00adtion is to relax the URA by \nadopting single representation assumption (SRA), in which a single representation is in\u00addependently chosen \nfor each de.nition point and each use point. Although the SRA would appear to allow .exibility in choosing \ncustomized representations, it has two problems: 1. The single representation at a de.nition point must \nbe consistent with all use points where it could be used. Du\u00adally, the single representation at a use \npoint must be con\u00adsistent with de.nition points de.ning a value that could be used there. These constraints \nimply that there must be a more precise notion of where a value .ows than all sites at which a value \nof the same type is used . Thus, some form of .ow analysis [33, 21, 28, 22] is required to perform representation \ncustomization. 2. An ine.cient representation at de.nition point D1 of a program can propagate through \na program in a way that constrains the choice of representation at a distant de.\u00adnition point D2,where \nD1 and D2 do not even directly share a use point. (A dual problem holds for use points that do not share \na de.nition point.) De.ne a colleague of a de.nition point D recursively as either the point it\u00adself, \nor any de.nition point that shares a use point with a colleague of D. Then by the constraints of the \nSRA and the representation invariant, all colleagues must share the same representation. Figure 1(b) \nprovides a simple ex\u00adample of this. Even though closed abstractions .1 {4} and  {1,2} .2 {4,5} share \nan application site (@), they cannot be 4 represented as code pointers because .2 {4,5} shares an ap\u00adplication \nsite with open function .3 {5}. By the SRA, this constrains .2 {4,5} to be represented as a closure, \nwhich in turn forces .1 {4} to be represented as a closure. We call the ability of one bad representation \nto spoil the representation of all its colleagues the representa\u00adtion pollution problem.6 Representation \npollution is a serious obstacle to customizing representations in a com\u00adpiler. For instance, the selective \ncustomization suggested by Wand and Steckler [41] can only be applied when all colleagues of a closed \nfunction are closed. In the CIL compiler, our solution to pollution is to break the SRA constraints \nby splitting some de.nition and/or use points to use multiple representations. Figure 1(c) shows the \n6Other names for this problem are the poisoning problem [42] and the W problem [Jens Palsberg, personal \ncommunica\u00adtion]. The latter is named from the shape of a simple .ow diagrams, like Figure 1, that illustrate \nthe problem. .1 {4}x.P {} .2 {4,5}y.Q{} .3 {5}z.R{a,b} . @{1,2}4 M . @{2,3}5 N (a) Source program (.1 \n{4}(x, ()).P {}, ()) (.2 {4,5}(y, ()).Q{}, ()) (.3 {5}(z, (a, b)).R{a,b}, (a, b)) let (c, e)= . in c \n@{1,2}4 (M, e) end let (c, e)= . in c @{2,3}5 (N, e) end (b) Uniform code/environment representation \n.1 {4}x.P {} p1. (.2; {4}y.Q{}, (.2;; {5}(y, ()).Q{}, ())) p2. (.3 {5}(z, (a, b)).R{a,b}, (a, b)) . @{1,2;}4 \nM let (c, e)= . in c @{2;; ,3}5 (N, e) end (c) Selective source splitting .1 {4}x.P {} .2 {4,5;}y.Q{} \n.1. .2. (.3 {5;;}(z, (a, b)).R{a,b}, (a, b)) . @{1,2}4 M case . of .1f . f @{2}5; N .2(c, e) . c @{3}5;; \n(N, e) (d) Selective sink splitting .1() .2. () .1. .2(a, b) case . of .1() . g1 @{1}4; (M, ()) .2() \n. g2 @{2}4;; (M, ()) case . of .1() . g2 @{2}5; (N, ()) .2(a, b) . g3 @{3}5;; (N, (a, b)) where g1 bound \nto .1 {4;}(x, ()).P {}, g2 bound to .2 {4;; ,5;}(y, ()).Q{}.and g3 bound to .3 {5;;}(z, (a, b)).R{a,b} \n(e) Uniform defunctionalization .1() .2. () .1. .2(a, b) case . of .1() . g1 @{1}4; M .2() . g2 @{2}4;; \nM case . of .1() . g2 @{2}5; N .2(a, b) . g3 @{3}5;; (N, (a, b)) where g1 bound to .1 {4;}x.P {}, g2 \nbound to .2 {4;; ,5;}y.Q{}.and g3 bound to .3 {5;;}(z, (a, b)).R{a,b} (f) Selective defunctionalization \n.1() .2. () .1. .2(a, b) case . of .1() . let x = M in P {} end .2() . let y = M in Q{} end case . of \n.1() . let y = N in Q{} end .2(a, b) . let z = N in R{a,b} end (g) Inlining Figure 1: Transformations \non a W-shaped .ow diagram. e.ect of splitting the source point .2 {4,5} into two represen\u00adtations: (1) \na code pointer that .ows to call site 4, which is shared with another closed function; and (2) a closure \nthat .ows to call site 5, which is shared with an open function. This permits call site 4 to use an optimized \ncalling sequence even though call site 5 uses the ine.cient one. The costs of this customization are \npairing the two representations and extracting them (via explicit projections pi) from the pair. An alternative \napproach for breaking pollution constraints is to split a sink point, as illustrated in Figure 1(d). \nSink splitting involves injecting (via .i) inconsistent representa\u00adtions into a tagged variant value, \nand then dispatching o. the tag (via case) to an appropriate handler at a sink site. Again, breaking \nthe pollution constraints allows the two closed functions to share a call site with an optimized call\u00ading \nconvention. The costs of this approach are the injections and case analyses necessary at the other call \nsite. Source and sink splitting enable customizations by elimi\u00adnating some pollution, but also introduce \nmanipulations of tuples or variants that were not in the original program. In the balance, are such customizations \na good idea? This ques\u00adtion is a complex one that is addressed in Sections 4 and 5, which investigate \nthe conditions under which the customiza\u00adtions can improve the performance of a program. It is important \nto note that closure conversion is not the only uniform strategy for transforming higher-order func\u00adtions \nto .rst-order functions. Another uniform strategy is defunctionalization, in which every function value \nis repre\u00adsented as an element of an algebraic datatype whose con\u00adstructor uniquely identi.es the abstraction \nof the function and whose components are the values of the free variables of the abstraction [32, 5]. \nThis is similar to a closure, except that the environment is paired not with a code pointer but with \nan abstract tag denoting the function. Call sites are transformed to dispatch o.this tag to a direct \ncall of an ap\u00adpropriate global closed function. In the simplest approach, there is one algebraic datatype \nfor all functions of a given monomorphic type, and one constructor for each abstraction appearing in \nthe program with this type.7 However, .ow analysis can signi.cantly reduce the size of the datatypes \nintroduced by defunctionalization [39, 8]. Uniform defunctionalization is illustrated for our example \nin Figure 1(e). The code portions of the functions have been lifted to global functions g1, g2,and g3, \nleaving behind only environment injections. Call sites dispatch o.the injection tags to invoke an appropriate \nglobal function. Uniform de\u00adfunctionalization can be viewed as maximally splitting each sink according \nto all the functions that might .ow there. This maximal splitting e.ectively eliminates all represen\u00adtation \npollution, and allows representations to be chosen completely independently for each abstraction. Figure \n1(f) shows how selective representations can be used in conjunc\u00adtion with defunctionalization to improve \nthe representations of g1 and g2 and the three sites at which these are invoked. Inlining can be viewed \nas a variant of defunctionalization in which the known call to a global function within a branch of a \ncase dispatch has been reduced at compile time (see Fig\u00adure 1(g)). This perspective covers the .ow-directed \ninlining of open functions [45], as well as the inlining of multiple functions at a single call site. \n7Polymorphic functions are handled by a monomorphiza\u00adtion process that specializes a polymorphic de.nition \nto each monomorphic type at which it is used.  3. CIL COMPILER OVERVIEW 3.1 Compiler Architecture We \nhave constructed a whole-program compiler for core SML based on CIL, our typed intermediate language. \nThe key features that distinguish CIL from other such languages are its use of .ow labels in conjunction \nwith intersection and union types to encode polyvariant .ow analyses in the type system of the language \n[44]. The resulting .ow types support the customizations presented in this report and serve as an important \nsanity check in the compiler implementation. In CIL, customization opportunities are represented by virtual \nrecords (introduction forms for terms of intersection type) and virtual case expressions (elimination \nforms for terms of union type). The compiler enables customization by reifying some of the virtual forms \ninto real forms, as seen in the above examples of source and sink splitting. The core compiler implementation \nis based on the archi\u00adtecture speci.ed in [14]. We have extended the simple in\u00adtermediate language described \nthere with numerous features (e.g., references, arrays, exceptions, standard library func\u00adtions, etc.) \nnecessary to support the compilation of SML. In implementing the compiler, we took advantage of exist\u00ading \ntools and other freely available SML compilers. The CIL compiler uses the MLton source-to-source defunctorizer \n[8] as a prepass to convert SML into Core SML. It then uses the front end of the SML/NJ 110.03 compiler \n(somewhat modi\u00ad.ed) to produce FLINT code. The FLINT code is translated to untyped CIL code, keeping \ndatatype information on the side to avoid reinference of recursive types. The untyped CIL code is then \nprocessed by one of four .ow analyses we have implemented as part of the type inference/.ow analysis \nstage that transforms it into .ow-typed CIL code. The representation transformations detailed in [14] \nare parameterized over seven function representation strategies that heuristically choose the representation \nfor each abstrac\u00adtion and call site. The CIL compiler uses .ow types to man\u00adage the plumbing of the chosen \nrepresentations, splitting sources and sinks as needed to ensure that all representa\u00adtions are used consistently \nrelative to type and .ow infor\u00admation. Despite complex types powerful enough to encode polyvariant .ow \nanalyses and term representations that du\u00adplicate the components of virtual records and the branches \nof virtual case expressions, the size of the intermediate rep\u00adresentations of CIL programs is tractable \nin practice [16]. The CIL compiler back-end transforms typed CIL pro\u00adgrams into assembly code for the \nSPARC processor. It does not currently add any type annotations, or assertions, to the assembly code, \nalthough this is planned for future work. The produced assembly code is linked with a runtime library \nproviding the environment in which CIL programs are exe\u00adcuted. Theback-endisbasedonMLRISC, aframework \nfor building portable optimizing code generators [18]. The runtime library is written in C and provides \nmem\u00adory management, exception handling, basis functions and a foreign function interface for CIL programs \nat runtime. The runtime library currently manages memory using the Boehm-Demers-Weiser conservative garbage \ncollector for C [7]. CIL programs use stack-allocated activation records, which have a layout similar \nto C stack frames. The code generator does not yet implement tail recursion. CIL data representations \nare straightforward. Records, arrays, references, and strings are heap-allocated and in\u00adclude size headers8 \n. Exception identi.ers and all other con\u00adstants are immediate. Injections may either be immediate or \nheap allocated, depending on the number and types of summands in their type. 3.2 Function Representation \nStrategies The CIL compiler currently implements the following seven function representation strategies: \n uniform (uni): represents all functions as closures. We investigate two uniform closure representations: \n(1) a paired closure that is a record of a code pointer and en\u00advironment, which is itself a record of \nfree variable values; and (2) a .at closure (see [2]) that is a single record con\u00adtaining both code pointer \nand free variable values.  polluted selective (pse): represents a closed abstraction as a code pointer \nif and only if all the colleagues of the abstraction are closed. This implements the approach proposed \nby Wand and Steckler in [41].  selective source splitting (src): generates a code-only rep\u00adresentation \nfor a closed function .owing to call sites that are not shared with open functions. If a closed function \nshares some application sites with other closed functions but shares other application sites with open \nfunctions, then the framework will split the source by generating a record containing multiple copies \nof the function.  selective sink splitting (snk): generates a selective repre\u00adsentation when the function \nhas no free variables. This representation is called sink splitting because if the function shares call \nsites with open functions, the trans\u00adformation framework will inject the function representa\u00adtions into \na sum type and the application site will be split into multiple sites governed by a case dispatch.  \ndefunctionalization (dfn): represents all functions as in\u00adjected environments and all call sites as dispatches \nto an invocation of an appropriate global function on the ar\u00adgument and the environment. Functions not \nsharing call sites with other functions are not injected.  selective defunctionalization (sdf): like \ndfn, but repre\u00adsents closed functions as code pointers to functions that expect just an argument (but \nno environment).  inlining (inl): inlines (possibly open) functions at the call site. The run-time representation \nof an inlined function is a record of the function s free variable values. Call sites may be split to \nallow non-inlined and (possibly multiple) inlined representations at the same call site. Thus far, we \nhave investigated only one point in a huge space of possible inlining heuristics: any non-recursive function \n.owing to two or fewer call sites will be inlined; when inlining is not possible, the strategy falls \nback to pse.  The results of using these strategies to transform the exam\u00adple in Figure 1(a) are shown \nin Figures 1(b)-(g). Note that the pse strategy does not have its own .gure because the result is the \nsame as that for uni due to pollution. 8Such headers are currently unnecessary since we use con\u00adservative \nGC. But we expect to develop customized memory management in the future.  3.3 Optimizations The CIL \ncompiler is currently a research tool and is far from an industrial-strength program. In particular, \nbeyond the function customizations described in this report, some standard partial-evaluation style optimizations, \nand some back-end optimizations performed by MLRISC, there are few optimizations performed in CIL. One \nimportant optimization we do implement is known function variable elimination (KVFE). If at representation \nchoice time it is guaranteed that a function f will not have a closure, then we do not consider a reference \nto f to be free in g for any other function g in the program. Since invocations of known functions can \nbe compiled as a jump to an address known at compile-time, the name of a known function ap\u00adpearing in \nthe rator position of an application need not be treated as a free variable. This optimization signi.cantly \nincreases the number of closed functions in our compiler, which in turn creates more customization opportunities. \nIn early versions of our compiler, the free variables of a func\u00adtion included every externally de.ned \nfunction name used within the body of the function. This hobbled customiza\u00adtion because very few functions \nwere closed. We use a slightly di.erent version of KFVE than other compilers. First, we use a .ow-based \ncalculation for KFVE rather than the standard syntax-based algorithm, so we need not worry about tracking \nnames through copy propagation or about functions escaping their scope of de.nition. In this respect \nour algorithm is similar to one hinted at in [3]. Sec\u00adond, we do not currently use KFVE to eliminate \nclosures where the function pointer is known and the environment is empty. We have postponed this work \nuntil we have de\u00adveloped a more general .ow-based known value elimination algorithm that handle constants, \nunused values, and records of known values in a single framework. The KFVE algorithm enables the compiler \nto .nd more empty environments and to decrease the environment size in many cases. For uni, KFVE has \nno e.ect on the cost, since all variables are bound to closures and none is bound to a known function. \nFor dfn, KFVE has been disabled to maintain its status as a straw man strategy comparable to uni. In \nother strategies, KFVE results in a roughly 13% decrease in cost for paired closures under our cost model \n(see the next section), depending on both the strategy and the benchmark.  4. COST MODEL An important \ngoal of this paper is to evaluate and com\u00adpare the seven function representation strategies presented \nin Section 3.2 using benchmark programs. Although one way to do this is to measure running times, our \ncompiler does not yet implement numerous optimizations (see Sec\u00adtion 3.3) so such measurements would \nnot represent the pos\u00adsible costs and bene.ts of the various representations. Fur\u00adthermore, we want some \nway to dissect the costs of function creation, function application, and pollution removal in or\u00adder \nto better understand where the time charged to function representations is being spent. Based on these \nconsiderations, our approach in this paper is to instrument the CIL compiler to track intermediate code \npoints related to function representation, count the dynamic number of times these points are reached \nwhen executing the compiled code, and use a cost model to attribute a cost to these points. Our approach \nmodels only those costs directly related to packaging, unpackaging, and otherwise manipu\u00ad lating the \ncode pointer, argument, and free variables of a function when creating or invoking it. It does not model \ncosts of manipulating values of non-function type, nor does it account for other operations associated \nwith functions, such as preserving live registers on the stack across a function call. Moreover, our \nmodel does not re.ect many optimiza\u00ad tions commonly associated with functions, such as storing some free \nvariables on the stack or stack-allocating some clo\u00ad sure and environment records, nor does it re.ect \nmany other compiler optimizations (e.g., loop optimizations, tuple .at\u00ad tening, cheap representations \nof certain variant values) that could a.ect the costs we charge to function representations. In light \nof these disclaimers, we stress that there may be little correlation between the relative costs of function \nrepre\u00ad sentation strategies under our cost model and those obtained in optimizing compilers. Nevertheless, \nwe believe that our model gives some sense for the bene.ts and drawbacks of various function representation \nstrategies and suggests that certain strategies are worthy of further exploration. This section develops \nthe cost model for comparing func\u00ad tion representation choices made by the CIL compiler. The model abstracts \nlow-level implementation details, and fo\u00ad cuses on cost as the number of abstract assembly language instructions \nof three classes: register-to-register (r), register\u00ad to-memory (m), and memory allocation (a). This \nmetric captures the relative cost of customized representations, and can be applied to a particular implementation \nto account for architectural details such as cache and branch prediction (see section 4.4 below). Our \nmodel handles both paired and .at closures. For paired closures we assume that the code references a \nsingle argument register holding a pointer to a tuple of the function argument and the environment. For \n.at closures we assume that the code references two argument registers holding (1) the function argument \nand (2) the closure record itself.9 4.1 Function De\u00a3nition Record Creation Environments and closures \nare repre\u00adsented as heap-allocated records. The cost of creating a record includes an allocation cost \na plus a store cost m for each .eld of the record. We assume that a record includes a one-word header \n(to facilitate garbage collection); it costs r + m to store the header. A zero-.eld record can be rep\u00adresented \nspecially as a null pointer.10 The total cost RC(n) of creating an n-.eld record is: RC(0) = r RC(n)= \nr +(n +1) * m + a, if n> 0 Closure Creation Paired closures require both a pair record and an environment \nrecord. Flat closures require only a sin\u00adgle record. The cost of creating a closure with n free variable \n9Whether the function argument and record of free variable values are passed in separate registers or \nas a tuple in one register is orthogonal to the paired vs. .at distinction. To avoid considering four \ncombinations, we associate the more e.cient two-register strategy with the more e.cient .at rep\u00ad resentation \nto yield the two extreme combinations. 10A one-.eld record can sometimes be represented as the .eld itself. \nCIL s parallel contexts [44] prevent performing this optimization in all cases. Our cost model (but not \nour compiler) implementation assumes this optimization for one\u00ad element environments (where it is always \nsafe). values is: CCpaired (n)= RC(2) + RC(n) CC.at (n)= RC(n +1)  4.2 Function Invocation Known Function \nCall Calling a function known at com\u00adpile time jumps directly to the function address with a cost 11 \n of KF =3r . Unknown Function Call Calling a function unknown at compile time loads the function address \ninto a register and jumps indirectly with a cost of UF =4r. Record Projection Extracting closure and \nenvironment components requires accessing a slot of a heap-allocated record with cost RP = m. Closure \nApplication The cost of applying a closure de\u00adpends on the paired vs. .at representation, the number \nof free variable values, and whether or not the function is known. Paired closures with unknown functions \nrequire pro\u00adjecting both the code pointer and environment and invoking an unknown function; for known \nfunctions, the code pointer projection is avoided. In either case, an argument/environment record is \nconstructed and deconstructed, and the n environ\u00adment components are projected. The cost of applying \na paired closure with n free variables is: CApaired/known (n)= RC(2) + KF +(3+ n) * RP CApaired/unknown \n(n)= RC(2) + UF +(4+ n) * RP For .at closures with unknown functions, the code pointer is projected \nbefore the call and free variables are projected after the call; the code pointer projection is avoided \nfor a known function. By assumption, there are no allocation or projection costs for passing the argument \nand closure: CA.at/known (n)= KF + n * RP CA.at/unknown (n)= UF +(n +1) * RP  4.3 Pollution Removal \nVirtual Tuples Virtual tuples that are rei.ed into real tu\u00adples incur the cost of record creation (RC) \nand record pro\u00adjection (RP). We see an example of pollution removal cost due to virtual tuples in Figure \n1(c) (selective source split\u00adting). Here the .2 {4,5} abstraction has been split into two di.erent representations, \nincurring the cost of a record cre\u00adation at de.nition and record projection at invocation.12 Virtual \nVariants Virtual variants which are rei.ed into real variants incur the cost of injection (IN) and case \ndis\u00adpatch (CD). An example of pollution removal cost due to virtual variants is Figure 1(d) (selective \nsink splitting). Injections are allocated on the heap, and contain the in\u00adjection tag and the injected \nvalue. Assuming headers on records as above, then injections have the same cost as 1\u00ad.eld records (where \nthe tag is the header and the .eld is the value): IN = RC(1).13 11 The actual cost of known and unknown \nfunction calls is implementation-dependent. For example, on the Ultra- SPARC a branch misprediction can \ncost as many as 18 cy\u00adcles. But since KF and UF are taken to be constant in the model, they can be de.ned \naccording to the implementation. 12It is possible to lift abstractions (but not necessarily clo\u00adsures) \nout of tuples to top-level and remove the correspond\u00ading projections. The CIL compiler does not yet do \nthis, nor does the cost model implementation, except in the defunc\u00adtionalization strategies. 13In some \ncases the injection may be represented only as an The cost of a case dispatch with n clauses (CD(n)) \nin\u00adcludes m to load the discriminant tag, r + m to load and bind the discriminant value, r to branch \nafter a clause, and the cost of dispatching using the better of a conditonal tree or jump table (depending \non the number of clauses; see [40]): CD(n)=2r +2m + cases(n), where cases(0|1) = 0r; cases(2) = 3r; cases(3) \n= 4r; cases(4) = 5r; cases(5) = 7r; cases()=4r +1m.  4.4 Example Consider the representations of the \nexample program in Figure 1. Assuming paired closures, we can use the cost model to calculate the static \ncosts of di.erent representa\u00adtions along particular paths in the program. For example, {1,2} along the \npath from .1 {4} to @in Figure 1(d), we have 4 no function de.nition cost, a function invocation cost \nof UF, and no pollution removal cost. This results in a cost of 4r. Along the same path in Figure 1(f), \nwe have no de.nition cost, an invocation cost of KF, and a pollution removal cost of IN + CD(2)14, for \na total cost of 9r +4m + a. Using this cost model and reasonable weights wr, wm,and wa for the r, m and \na operations for a particular implemen\u00adtation, we can calculate the actual cost of di.erent represen\u00adtations. \nFor example, on a typical SPARC implementation [35] with a fast memory allocator we might choose: wr \n=1, wm =3 and wa = 16 to represent machine cycles. In the example, we .nd that the selective sink splitting \nrepresen\u00adtation costs 4 cycles using these weights, and the selective defunctionalization representation \ncosts 37 cycles. The costs for each of the four paths in the program, and for each of the representations \nin Figure 1 are given in the following table, using the weights given above. Representation (1,4) (2,4) \n(2,5) (3,5) uni src snk dfn sdf inl 88 7 4 69 37 34 88 33 4 69 37 34 88 117 37 69 37 34 100 103 130 101 \n69 66 Cache e.ects may be accounted for by adding an expected miss cost to the base cost for a memory \noperation. The ex\u00ad pected miss cost is the product of the miss rate and miss penalty for the implementation \n[29]. For example, a typi\u00ad cal UltraSPARC implementation has a .rst level cache miss penalty of 6 cycles \nand a second level cache miss penalty of 13 cycles [37]. So in the example above, selective sink splitting \nhas no data miss cost (having no memory opera\u00ad tions), and selective defunctionalization has a total \ncost of 9r+4m+a+((4/14)*l1missrate*6)+((4/14)*l1missrate* l2missrate * 13). The results reported in the \nnext section are based on the simpler instantiation of the cost model, without modeling cache misses \nor branch mispredictions. -*-LaTeX-*\u00ad immediate tag, and not allocated on the heap. The CIL compiler \nsupports this, but we do not account for this opti\u00ad mization in the cost model. An optimization in defunctionalization \nwith uniform rep\u00adresentation at an invocation site is to use the code pointers themselves as the tag, \navoiding the injection altogether. The CIL compiler does not currently use this optimization.  5. MEASUREMENTS \nTo determine the e.ect of customizations and pollution removal on the dynamic costs of function representations, \nwe use the cost model to measure the performance of our function representation strategies for a set \nof kernel codes and small benchmarks of 50 to 3000 lines of source code. The SML benchmarks that we use \nin this paper are: Source dynamic dynamic Name lines From function function of code creations applications \nmsort 55 TIL 1.8m 13m church 73 Church 2.7k 188m soli 115 O Caml 1.7m 3.1m quicksort 120 O Caml 1.6m \n4.7m life 147 SML/NJ 643k 6.5m matmult 156 TIL 40k 40.m .t 194 O Caml 1.4m 2.8m tsp 249 SML/NJ 4.9m 10m \nbarnes-hut 401 SML/NJ 502k 9.2m kb 467 O Caml 7.3m 13m frank 487 TIL 36m 60m ratio-regions 505 SML/NJ \n36m 162m tyan 856 * TIL 29k 594k boyer2 856 Church 551k 1.2m lexgen 1067 SML/NJ 1.5m 5.2m simple 1228 \n* SML/NJ 2.2m 22m pia 2081 * TIL 2.5m 5.1m nucleic 2923 O Caml 1.1m 2.5m  The source lines count does \nnot include comments or blank lines. Source lines marked with an asterisk are the out\u00adput of the MLton \ndefunctorizer, which tends to insert more line breaks than a human programmer. The dynamic func\u00adtion \ncreations column shows the number of closure cre\u00adations when the program is run after being compiled \nwith the uni strategy. The dynamic function applications col\u00adumn shows the number of function applications \nwhen the program is run after being compiled with the uni strategy. The letter k means 103 and m means \n106 . Most of the benchmarks are from standard benchmark suites. Some occur in slight variations in more \nthan one suite. We have contributed two new benchmarks showing extremes of programming style. The boyer2 \nbenchmark is a variant of the O Caml boyer benchmark modi.ed to be entirely .rst-order; church is a library \nof arithmetic on Church numerals, a simple example of a program built out of many very small higher-order \nfunctions. Table 1 and the bar charts in Figure 2 summarize the results of running the benchmarks using \nthe seven repre\u00adsentation strategies presented in Section 3.2. Table 1 lists the average costs of each \nstrategy in our cost model relative to that for uni, which is given a cost of 100. Because the church \nbenchmark is such an extreme case, averages for the benchmarks without church are also presented. Fig\u00adure \n2 presents the costs of each strategy for each benchmark for the two closure representations, and displays \nthe cost as a bar broken into three segments: 1. the segment .lled with represents the (relative) cost \nof function creation; 2. the segment .lled with represents the (relative) cost of calling a function, \nincluding the cost of unpacking the environment if an environment is used; closures uni pse src snk inl \ndfn sdf paired 100 83 83 90 73 111 86 w/o church 100 76 76 76 62 87 66 .at 100 88 88 157 89 180 171 w/o \nchurch 100 86 86 87 88 100 89 Table 1: Average relative costs of function represen\u00adtation strategies \nas % of cost of uni strategy. 3. the segment .lled with represents the (relative) cost of pollution \nremoval, accounting for the new record cre\u00adations/projections and variant creations/case dispatches introduced \nby splitting. Based on data not shown here, the uni strategy using .at closures on average incurs only \n68% of the cost of the uni strategy using paired closures. The actual costs of pollu\u00adtion removal are \nindependent of the closure representation. As seen in Figure 2, the lower total costs when using .at \nclosures makes the costs of pollution removal proportionally larger than when using paired closures. \nThere is a wide variation in the relative cost of using any of the selective representations vs. the \nuniform representa\u00adtion: from 9% for msort to > 99% for church, matmult, and quicksort. In the cases \nof matmult and quicksort, most of the functions are open, so we always have to accept the cost of using \nenvironments. In the case of church,a substantial amount of representation pollution precludes us\u00ading \noptimized representations in the most frequently called functions. In both church and matmult, the cost \nof build\u00ading closures is not visible in the bar charts due to the high ratio of function calls to closure \ncreations. Because it never pays any plumbing costs but in some cases still enjoys the bene.ts of more \ne.cient closed function representations, pse is never worse than, and is sometimes signi.cantly better \nthan, uni. On average, the cost of using pse is 83% (for paired closures) and 88% (for .at closures) \nof the cost of using uni. There does not seem to be any strong correlation between the size of the benchmark \nand the cost savings, although there is an obvious correlation between small size and large variation. \nThe actual speedup in a program would be less, since we are only modeling costs of function closing and \ninvocation. But it seems that the advantage of specialized function representations is clear, and that \neven a simple representation customization such as selective closure conversion is worthwhile. Our experimental \nresults show that for the simple function representations and cost models we consider, several pollu\u00adtion \nbreaking strategies are rarely more e.ective than pse and can be far worse due to the cost of breaking \npollution. What follows is an explanation of these results. For certain programs in our benchmark set, \nour .ow anal\u00adysis .nds that no call site has more than a single function .owing to it. In this case there \nis no pollution to remove. There is also no overhead involved in using a defunctionaliza\u00adtion strategy. \nBenchmarks matmult, .t, tsp,and boyer2 fall into this category. There are some benchmarks in which some \ncall sites have more than a single function .owing to them so there is some overhead involved in a defunctionalization \nstrategy but all functions .owing to a given call site have the same representation. In this case there \nis no pollution to remove in src and snk. Benchmarks in this category are soli, quick\u00adsort, frank, ratio-regions,and \nlexgen. Paired closures: Excluding pollution removal costs Including pollution removal costs benchmark \nsrc snk src snk msort 0.00 1.37 0.00 -8.20 church 0.00 46.66 0.00 -26.67 life 0.68 1.15 0.65 0.28 barnes-hut \n0.00 0.00 0.00 0.02 kb 0.10 0.11 0.08 0.11 tyan 0.37 0.84 0.33 -3.09 simple 4.99 5.33 4.55 3.40 pia 0.60 \n0.60 0.60 0.60 nucleic 0.00 3.11 0.00 0.57  Table 2: Improvements and costs of pollution re\u00admoval as \n% of cost of the pse strategy. The src strategy will clone a copy of a closed function if it shares one \ncall site with an open function and another call site with only closed functions. If every closed function \nhas an open function as an immediate colleague at every shared application site, then src is identical \nto pse.Benchmarks in this category are msort, barnes-hut,and nucleic. This leaves church, life, kb, tyan, \nsimple,and pia to explore as to whether src gives improved performance over pse. To evaluate snk, we \nneed to additionally investigate msort, barnes-hut,and nucleic. Table 2 summarizes the improvements due \nto pollution removal for src and snk, using paired closures, as well as the total cost of pollution removal. \nThe .rst two columns of numbers show the percentage improvement in the cost over pse if the cost of pollution \nremoval itself is excluded. The last two columns show the percentage improvement when the pollution removal \ncosts are included. The .rst two data columns show that if pollution removal were free , it could reduce \n(in some cases, signi.cantly) the cost of function creation and application. However, when the cost of \ncreating and manipulating the values required by pollution removal is included, these bene.ts vanish \nin almost all cases. Indeed, the cost of pollution removal typ\u00adically makes the snk strategy less e.ective \nthan pse.This result is clearest with church, where a lot of representation specialization is blocked \nby pollution but the cost of remov\u00ading pollution turns out to be prohibitive. There is one ex\u00adception: \nsimple shows non-trivial gains for paired closures, even when the costs of pollution removal are factored \nin. While the conservative partial pollution removal by src never loses for paired closures on our benchmark \nset, it sel\u00addom improves performance by much, and the very aggressive snk may actually signi.cantly degrade \nperformance. For .at closures, the data in Figure 2(b) shows that the larger rela\u00adtive cost of pollution \nremoval can cause the src strategy to have higher cost than the pse strategy in some cases, even in simple. \nClearly the space of heuristics for pollution re\u00admoval requires further exploration. There are some record \ncreations and projections that the CIL compiler creates for pollution removal that can be eliminated. \nHowever, much of the cost of pollution removal for snk is due to injections and case dispatches, which \nare not obviously removable. Our inlining strategy generally results in lower costs than pse for both \npaired and .at closures. The fact that inl ex\u00adhibits pollution removal costs in some benchmarks indicates \nthat sinks are being split to allow inlined functions to share call sites with other functions representations. \nWhen using .at closures, the cost of pollution removal sometimes out\u00ad msortuni pse src snk inl dfn sdf \n100 10 10 10 6 87 9 churchuni pse src snk inl dfn sdf 100 100 100 127 100 171 136 soliuni pse src snk \ninl dfn sdf 100 64 64 64 62 77 59 quicksortuni pse src snk inl dfn sdf 100 99 99 99 97 89 88 lifeuni \npse src snk 100 96 96 96 matmultuni pse src snk 100 100 100 100 .tuni pse src snk 100 83 83 83 tspuni \npse src snk 100 56 56 56 inl 53 inl 32 inl 82 inl 40 dfn 83 dfn 93 dfn 82 dfn 73 sdf 79 sdf 93 sdf 67 \nsdf 41 barnes-hutuni pse src snk inl dfn sdf 100 66 66 66 59 92 60 kbuni pse src snk inl dfn sdf 100 \n92 92 92 62 86 78 frankuni pse src snk inl dfn sdf 100 79 79 79 71 78 63 ratio-regionsuni pse src snk \ninl dfn sdf 100 74 74 74 68 90 66 tyanuni pse src snk 100 47 47 48 boyer2uni pse src snk 100 81 81 81 \nlexgenuni pse src snk 100 67 67 67 simpleuni pse src snk 100 77 74 75 inl 43 inl 61 inl 68 inl 64 dfn \n91 dfn 77 dfn 82 dfn 97 sdf 34 sdf 66 sdf 54 sdf 73 piauni pse src snk 100 65 65 65 nucleicuni pse src \nsnk 100 71 71 71 inl 54 inl 53 dfn 76 dfn 80 sdf 48 sdf 50 (a) Benchmark costs using paired closures. \nmsortuni pse src snk inl dfn sdf 100 32 32 36 24 88 36 churchuni pse src snk inl dfn sdf 100 100 100 \n564 100 642 642 soliuni pse src snk inl dfn sdf 100 71 71 71 67 71 69 quicksortuni pse src snk inl dfn \nsdf 100 100 100 100 98 108 108 lifeuni pse src snk 100 99 99 102 matmultuni pse src snk 100 100 100 100 \n.tuni pse src snk 100 92 92 92 tspuni pse src snk 100 68 68 68 inl 69 inl 67 inl 91 inl 30 dfn 92 dfn \n100 dfn 94 dfn 45 sdf 90 sdf 100 sdf 84 sdf 37 barnes-hutuni pse src snk inl dfn sdf 100 83 83 83 79 \n98 81 kbuni pse src snk inl dfn sdf 100 89 89 89 99 112 101 frankuni pse src snk inl dfn sdf 100 83 83 \n83 79 82 71 ratio-regionsuni pse src snk inl dfn sdf 100 89 89 89 100 111 100 tyanuni pse src snk 100 \n75 74 83 boyer2uni pse src snk 100 78 78 78 lexgenuni pse src snk 100 81 81 81 simpleuni pse src snk \n100 89 91 95 inl 73 inl 62 inl 87 inl 106 dfn 103 dfn 74 dfn 91 dfn 138 sdf 74 sdf 64 sdf 73 sdf 128 \npiauni pse src snk 100 77 76 76 nucleicuni pse src snk 100 87 87 92 inl 67 inl 71 dfn 80 dfn 90 sdf 62 \nsdf 61 (b) Benchmark costs using .at closures. Figure 2: Relative costs of function representation strategies. \nIn each benchmark, the number is the total cost as % of uni cost, and bar lengths are normalized to the \nlength of the longest of the seven bars. weighs the bene.ts of inlining (kb,ratio-regions,simple). The \nlexgen benchmark shows a case where inl appears more costly than pse even before pollution removal; this \nis a re\u00adsult of variations of the KFVE algorithm for various strate\u00adgies. For the pse strategy, which \nintroduces no splitting, and for sdf, which introduces maximal splitting, the KFVE algorithm is more \naggressive than for the src, snk,and inl strategies, which may or may not introduce splitting at a given \nfunction de.nition or invocation. The defunctionalization strategies dfn and sdf split all call sites, \nso it is easily possible to have a specialized represen\u00adtation per function de.nition. The dfn strategy \nrealizes two bene.ts from splitting: (1) record manipulations involving code pointers are eliminated; \nand (2) all function calls be\u00adcome known calls. It does not distinguish between closed and open functions, \nand passes empty environments at call sites rather than discarding them. In many cases, dfn can beat \nuni because its bene.ts outweigh the costs of the injec\u00adtions and case dispatches due to splitting. Occasionally, \ndfn even beats pse,asin tsp, frank,and boyer2. Unlike dfn,the sdf strategy handles closed functions spe\u00adcially. \nOur data show that this leads to signi.cant gains over dfn and, in many cases, over pse. The costs of \nsdf are between 70% and 93% of the pse strategy in 16 of the 18 benchmarks using paired closures and \nbetween 54% and 91% in 8 of the 18 benchmarks using .at closures. If we do not consider the outlier church \nbenchmark, then the sdf strat\u00adegy out-performs pse on the average for paired closures, and is close to \npse s performance for .at closures. We expect the combination of inlining and selective de\u00adfunctionalization \nto prove a very cost-e.ective strategy for most of the benchmarks, except for cases where the high cost \nof pollution removal would make pse the most e.ective strategy (such as (1) the church benchmark for \nboth closure representations and (2) the simple,and quicksort bench\u00admarks with .at closures). However, \nwe have not actually collected numbers for this combination. We emphasize that our results include only \nthe costs of function representations in our cost model, and that our con\u00adclusions are dependent on the \narchitecture being modeled, on the presence or absence of compiler optimizations, and on the closure \nrepresentations used by the compiler.  6. CONCLUSIONS AND FUTURE WORK Our results suggest that .ow-based \ncustomization of func\u00adtions based on the presence or absence of free variables (our polluted selective \nstrategy) is worthwhile. It remains to be seen how e.ective this customization is in a production com\u00adpiler, \nand whether it (and other .ow-based customizations) can be adapted to frameworks for separate compilation. \nOur experiments with removing function representation pollution are less conclusive. In three strategies \n(selective source splitting, selective sink splitting,and uniform defunc\u00adtionalization), there is little \nor no function representation pollution to remove in many benchmarks, and for bench\u00admarks with pollution, \nthe costs of removing the pollution often outweigh the bene.ts. In the selective defunctional\u00adization \nstrategy, pollution removal leads to signi.cant gains for many benchmarks, but there are still cases \nwhere it does not pay o.. This suggests that it would be worthwhile to characterize situations where \npollution removal is bene.cial and use it only in these situations. Note that we have not yet explored \noptimization opportunities enabled by our cus\u00adtomizations that might give rise to additional bene.ts \nnot re.ected in our current data; these might a.ect our conclu\u00adsions regarding pollution removal. Our \ninlining strategy is very e.ective, but it is unclear how much this depends on the .ow-based nature of \nthe in\u00adlining and the fact that pollution removal allows multiple functionstobeinlined at thesamecallsite. \nIn afuture study, we plan to compare various heuristics for inlining in our framework (varying fan-in, \nfan-out, and fall-back strat\u00adegy) with each other and with classical syntax-based inlining techniques \n(e.g., [2, 38]) to gain insight into the factors that make this strategy e.ective. While pollution removal \ndoes not seem very helpful in the context of selecting closed vs. open functions, it may very well be \ne.ective for other representation decisions. In terms of function representations, we are currently investigating: \n(1) uncurrying [19], which can increase the number of closed functions; (2) closure representations that \nexclude free vari\u00adables from an environment if their values are available at all call sites [36]; and \n(3) register allocation and calling con\u00adventions informed by .ow information. There are numerous other \nclosure representation tricks (e.g., those discussed in [24, 2, 34]) to investigate in the context of \nour framework. We have yet to explore customized representations for other structures, but CIL is rich \nenough to support .ow-directed customizations for all types of data. For instance, we believe it can \nbe used to treat certain data structures as .ctitious, as in [34], and can be extended to encode polyvariant \nusage\u00adbased customizations, such as Haskell s used-once thunk op\u00adtimization [42]. Much work remains to \nbe done to optimize customized representations and develop heuristics for choos\u00ading between allowable \nrepresentations. There are many areas for improvement in the CIL com\u00adpiler as a whole. The compiler can \nbene.t from numerous standard optimizations not yet implemented (e.g., tuple .at\u00adtening, loop optimizations, \npassing arguments in registers) as well as some important non-standard optimizations (e.g., the complete \nremoval of polymorphic equality, a type system to formalize the code-pointer-as-tag optimization, general\u00adized \nknown value elimination). Several existing algorithms can be more e.ciently implemented: e.g., our algorithm \nto produce clones and multiway dispatches for pollution re\u00admoval currently fails to re-combine identical \nclones. There are also many opportunities for improvement in the repre\u00adsentation of the intermediate \nlanguage: e.g., although we can model the e.ect of multi-argument functions, we have yet to implement \nthem in the compiler. The CIL compiler currently maintains type and .ow in\u00adformation through code generation, \nbut its output is un\u00adtyped. We plan to eventually produce typed assembly lan\u00adguage (TAL) [27] from the \nCIL compiler. Acknowledgments We thank other Church Project members for their advice and support. We \nespecially acknowledge Jef Considine s im\u00adplementation of cyclic hash-consing and the contributions of \nSantiago Pericas-Geersten and Glenn Holloway to early ver\u00adsions of the CIL compiler. We thank the referees \nfor their excellent questions and suggestions; we have incorporated as many as space permits. Thanks \nto Nausheen Eusuf for helping us to achieve closure on our title. 7. REFERENCES [1] O. Agesen. The Cartesian \nproduct algorithm. In Proceedings of ECOOP 95, Seventh European Conference on Object-Oriented Programming, \nvol. 952, pp. 2 26. Springer-Verlag, 1995. [2] A.W.Appel. Compiling with Continuations. Cambridge University \nPress, 1992. [3] J. M. Ashley. Flexible And Practical Flow Analysis for Higher-Order Programming Languages.Ph.D. \nthesis, Indiana University, May 1996. [4] A. Banerjee. A modular, polyvariant, and type-based closure \nanalysis. In ICFP 97 [20]. [5] J. M. Bell, F. Bellegarde, and J. Hook. Type-driven defunctionalization. \nIn ICFP 97 [20], pp. 25 37. [6] G. E. Blelloch. NESL: A nested data-parallel language. Technical Report \nCMU-CS-93-129, School of Computer Science, Carnegie Mellon University, Apr. 1993. [7] H.-J. Boehm. Space \ne.cient conservative garbage collection. In Proc. ACM SIGPLAN 93 Conf. Prog. Lang. Design &#38; Impl., \npp. 197 206, 1993. [8] H. Cejtin, S. Jagannathan, and S. Weeks. Flow-directed closure conversion for \ntyped languages. In Programming Languages &#38; Systems, 9th European Symp. Programming, vol. 1782 of \nLNCS, pp. 56 71. Springer-Verlag, 2000. [9] C. Chambers and D. Ungar. Customization: Optimizing compiler \ntechnology for Self, a dynamically-typed object-oriented programming language. In Proc. ACM SIGPLAN 89 \nConf. Prog. Lang. Design &#38; Impl., pp. 146 160, 1989. [10] C. Chambers and D. Ungar. Iterative type \nanalysis and extended message splitting: Optimizing dynamically-typed object-oriented programs. In Proc. \nACM SIGPLAN 90 Conf. Prog. Lang. Design &#38; Impl., 1989. [11] K. D. Cooper, M. W. Hall, and K. Kennedy. \nA methodology for procedure cloning. Computer Languages, 19(2):105 118, 1993. [12] K. Crary, S. Weirich, \nand G. Morrisett. Intensional polymorphism in type erasure semantics. In Proc. 1998 Int l Conf. Functional \nProgramming, pp. 301 312. ACM Press, 1998. [13] J. Dean,C.Chambers, andD.Grove.Selective specialization \nfor object-oriented languages. In Proc. ACM SIGPLAN 95 Conf. Prog. Lang. Design &#38; Impl., 1995. [14] \nA. Dimock, R. Muller, F. Turbak, and J. B. Wells. Strongly typed .ow-directed representation transformations. \nIn ICFP 97 [20], pp. 11 24. [15] A. Dimock, I. Westmacott, R. Muller, F. Turbak, J. B. Wells, and J. \nConsidine. Space issues in compiling with intersection and union types. In Preliminary Proceedings of \nthe Third Workshop on Types in Compilation (TIC 2000), 2000. Superseded by [16]. [16] A. Dimock, I. Westmacott, \nR. Muller, F. Turbak, J. B. Wells, and J. Considine. Program representation size in an intermediate language \nwith intersection and union types. In Proceedings of the Third Workshop on Types in Compilation (TIC \n2000), vol. 2071 of LNCS, pp. 27 52. Springer-Verlag, 2001. Supersedes [15]. [17] K.-F. Fax\u00b4en. The costs \nand bene.ts of cloning in a lazy functional language. In Trends in Functional Programming, Volume 2. \nIntellect, 2001. [18] L. George. MLRISC: Customizable and reusable code generators. Technical report, \nBell Labs, 1997. [19] J. Hannan and P. Hicks. Higher-order uncurrying. In Conf. Rec. POPL 98: 25th ACM \nSymp. Princ. of Prog. Langs., pp. 1 11, 1998. [20] Proc. 1997 Int l Conf. Functional Programming.ACM \nPress, 1997. [21] S. Jagannathan and S. Weeks. A uni.ed treatment of .ow analysis in higher-order languages. \nIn Conf. Rec. 22nd Ann. ACM Symp. Princ. of Prog. Langs., pp. 393 407, 1995. [22] S. Jagannathan, S. \nWeeks, and A. Wright. Type-directed .ow analysis for typed intermediate languages. In Proc. 4th Int l \nStatic Analysis Symp., vol. 1302 of LNCS. Springer-Verlag, 1997. [23] M. P. Jones. Dictionary-free overloading \nby partial evaluation. In PEPM 94 ACM SIGPLAN Workshop Partial Eval. &#38; Semantics-Based Prog. Manipulation. \nACM, 1994. [24] D. Kranz, R. Kelsey, J. A. Rees, P. Hudak, J. Philbin, and N. I. Adams. Orbit: An optimizing \ncompiler for Scheme. In Proc. SIGPLAN 86 Symp. Compiler Construction, pp. 219 233, 1986. [25] Y. Minamide, \nG. Morrisett, and R. Harper. Typed closure conversion. In Conf. Rec. POPL 96: 23rd ACM Symp. Princ. of \nProg. Langs., 1996. [26] G. Morrisett, D. Walker, K. Crary, and N. Glew. From System F to typed assembly \nlanguage. ACM Trans. on Prog. Langs. &#38; Systs., 21(3):528 569, may 1999. [27] G. Morrisett, D. Walker, \nK. Crary, and N. Glew. From System F to typed assembly language. ACM Trans. on Prog. Langs. &#38; Systs., \n21(3):528 569, May 1999. [28] F. Nielson and H. R. Nielson. In.nitary control .ow analysis: A collecting \nsemantics for closure analysis. In Conf. Rec. POPL 97: 24th ACM Symp. Princ. of Prog. Langs., pp. 332 \n345, 1997. [29] D. A. Patterson and J. L. Hennessy. Computer Organization &#38; Design. Morgan Kaufmann, \n1998. [30] J. Plevyak. Optimization of Object-Oriented and Concurrent Programs. Ph.D. thesis, University \nof Illinois at Urbana-Champaign, 1996. [31] J. Plevyak and A. Chien. Type directed cloning for object-oriented \nprograms. In Workshop for Languages and Compilers for Parallel Computers, Aug. 1995. [32] J. Reynolds. \nDe.nitional interpreters for higher-order programming languages. In ACM Annual Conference, pp. 717 740, \n1972. [33] O. Shivers. Control Flow Analysis of Higher Order Languages. Ph.D. thesis, Carnegie Mellon \nUniversity, 1991. [34] J. M. Siskind. Flow-directed lightweight closure conversion. Technical Report \n99-190R, NEC Research Institute, Inc., Dec. 1999. [35] SPARC International Inc., Menlo Park, CA. The \nSPARC Architecture Manual, Version 8. [36] P. Steckler and M. Wand. Lightweight closure conversion. ACM \nTrans. on Prog. Langs. &#38; Systs., 19(1):48 86, Jan. 1997. [37] Sun Microsystems, Inc., Palo Alto, \nCA. UltraSPARC User s Manual. [38] D. Tarditi. Design and Implementation of Code Optimizations for a \nType-Directed Compiler for Standard ML. Ph.D. thesis, Carnegie Mellon University, Dec. 1996. [39] A. \nP. Tolmach and D. Oliva. From ML to Ada: Strongly-typed language interoperability via source translation. \nJ. Funct. Programming, 8(4):367 412, 1998. [40] G.-R. Uh and D. B. Whalley. E.ectively exploiting indirect \njumps. Software Practice and Experience, 29(12):1061 1101, 1999. [41] M. Wand and P. Steckler. Selective \nand lightweight closure conversion. In Conf. Rec. 21st Ann. ACM Symp. Princ. of Prog. Langs., pp. 435 \n445, 1994. [42] K. Wansbrough and S. P. Jones. Once upon a polymorophic type. In Conf. Rec. POPL 99: \n26th ACM Symp. Princ. of Prog. Langs., pp. 15 28, 1999. [43] J. B. Wells, A. Dimock, R. Muller, and F. \nTurbak. A typed intermediate language for .ow-directed compilation. In Proc. 7th Int l Joint Conf. Theory \n&#38; Practice of Software Development, pp. 757 771, 1997. Superseded by [44]. [44] J. B. Wells, A. Dimock, \nR. Muller, and F. Turbak. A calculus with polymorphic and polyvariant .ow types. J. Funct. Programming, \n200X. To appear. Supersedes [43]. [45] A. Wright and S. Jagannathan. Polymorphic splitting: An e.ective \npolyvariant .ow analysis. ACM Trans. on Prog. Langs. &#38; Systs., 20:166 207, 1998.  \n\t\t\t", "proc_id": "507635", "abstract": "The CIL compiler for core Standard ML compiles whole ML programs using a novel typed intermediate language that supports the generation of type-safe customized data representations. In this paper, we present empirical data comparing the relative efficacy of several different flow-based customization strategies for function representations. We develop a cost model to interpret dynamic counts of operations required for each strategy. In this cost model, customizing the representation of closed functions gives a 12-17% improvement on average over uniform closure representations, depending on the layout of the closure. We also present data on the relative effectiveness of various strategies for reducing representation pollution, i.e., situations where flow constraints require the representation of a value to be less efficient than it would be in ideal circumstances. For the benchmarks tested and the types of representation pollution detected by our compiler, the pollution removal strategies we consider often cost more in overhead than they gain via enabled customizations. Notable exceptions are selective defunctionalization, a function representation strategy that often achieves significant customization benefits via aggressive pollution removal, and a simple form of flow-directed inlining, in which pollution removal allows multiple functions to be inlined at the same call site.", "authors": [{"name": "Allyn Dimock", "author_profile_id": "81100363224", "affiliation": "Harvard Univ.", "person_id": "P15557", "email_address": "", "orcid_id": ""}, {"name": "Ian Westmacott", "author_profile_id": "81339536103", "affiliation": "Boston Univ.", "person_id": "PP42053197", "email_address": "", "orcid_id": ""}, {"name": "Robert Muller", "author_profile_id": "81407593283", "affiliation": "Boston College", "person_id": "PP43117034", "email_address": "", "orcid_id": ""}, {"name": "Franklyn Turbak", "author_profile_id": "81339533353", "affiliation": "Wellesley College", "person_id": "PP42051187", "email_address": "", "orcid_id": ""}, {"name": "J. B. Wells", "author_profile_id": "81341498152", "affiliation": "Heriot-Watt Univ.", "person_id": "PP42052708", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/507635.507640", "year": "2001", "article_id": "507640", "conference": "ICFP", "title": "Functioning without closure: type-safe customized function representations for standard ML", "url": "http://dl.acm.org/citation.cfm?id=507640"}