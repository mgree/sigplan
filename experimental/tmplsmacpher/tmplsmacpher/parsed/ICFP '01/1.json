{"article_publication_date": "10-01-2001", "fulltext": "\n Conti.cation Using Dominators Matthew Fluet Stephen Weeks Cornell University InterTrust STAR Laboratories \n.uet@cs.cornell.edu sweeks@intertrust.com ABSTRACT Conti.cation is a compiler optimization that turns \na func\u00adtion that always returns to the same place into a continua\u00adtion. Compilers for functional languages \nuse conti.cation to expose the control-.ow information that is required by many optimizations, including \ntraditional loop optimizations. This paper gives a formal presentation of conti.cation in MLton, a whole-program \noptimizing Standard ML compiler. We present two existing algorithms for conti.cation in our framework, \nas well as a new algorithm based on the domina\u00adtor tree of a program s call graph. We prove that the \ndom\u00adinator algorithm is optimal. We present benchmark results on realistic SML programs demonstrating \nthat conti.cation has minimal overhead on compile time and signi.cantly im\u00adproves run time. 1. INTRODUCTION \nCompiler writers for traditional imperative languages can choose from a vast array of well-understood \noptimizations to improve the quality of generated code. To avoid reinventing the wheel, compiler writers \nfor functional languages should use these known techniques, or variants of them, whenever possible. In \norder to do this, they should use intermediate languages (ILs) that allow traditional optimizations to \nbe applied with minimal changes. They should also ensure that their compiler translates source programs \ninto the IL in a way that makes traditional optimizations applicable. Traditional optimizations [1, 19] \nsuch as common-subex\u00adpression elimination, loop-invariant code motion, and global register allocation \noperate on a control-.ow graph that rep\u00adresents intraprocedural information. Many traditional opti\u00admizations \ncan be implemented e.ciently using Static Single-Assignment form (SSA) [8], a convenient representation \nof def-use information in control-.ow graphs. In contrast to traditional compilers, functional-language \ncompilers typi\u00adcally use a .-calculus based IL like Continuation-Passing Style (CPS) [2] or A-normal \nform [11]. Fortunately, the con\u00adtrast is not as large as it would seem. It has been shown that Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. ICFP 01, September \n3-5, 2001, Florence, Italy. Copyright 2001 ACM 1-58113-415-0/01/0009 ...$5.00. functional programs and \nCPS are closely related to SSA [4, 14]. It is even possible to translate between a subset of CPS and \nSSA [14]. Functional languages use function calls to express all control-.ow, including loops. In order \nto apply traditional optimizations, a functional language compiler must trans\u00adlate function calls in \nthe source program into an IL that exposes the same intraprocedural control-.ow information that is available \nin traditional SSA form. In particular, recursive functions used to implement loops in the source should \nbe recognizable as loops in the IL. Section 8 of [14], which describes how to translate from CPS to SSA, \ncontains the key to exposing intraprocedural control-.ow information in functional languages. The idea \nis to .nd a set of procedures all of which are always called with the same contin\u00aduation, and then to \nsubstitute that continuation for the procedures continuation variables. The procedures are then themselves \ncontinuations. What this means is that if a function always returns to the same place, then that function \ns calls and returns can be viewed as describing intraprocedural instead of interproce\u00addural control-.ow. \nAs an example, consider the following functions: fun gy =y -1 fun fb =(if b then g13 else g 15) +1 and \ntheir translation into CPS: fun g(y, k) = k(y -1) fun f(b, k) = letfun k (x) =k (x +1) in if b then g \n(13, k ) else g (15, k ) end The declaration of k de.nes a continuation that increments its argument \nand passes the result to the continuation of f. Since g s continuation is always k , we can transform \ng into a continuation within f and eliminate the continuation argument from the de.nition of g and from \ncalls to g. fun f(b, k) = letfun k (x) =k (x +1) fun g (y) =k (y -1) in if b then g (13) else g (15) \nend This transformation exposes intraprocedural control .ow in\u00adformation (g now directly calls k ), \nand enables subsequent optimization, such as inlining k . In this paper, we coin the term conti.cation \nto mean turn\u00ading functions into continuations. We also use the verb con\u00adtify. Conti.cation (under various \nnames) has been used in functional-language compilers for over ten years [15] and has recently received \nattention in compilers for Dylan [20] and Moby [21]. Conti.cation is also used in MLton [18], a com\u00adpiler \nfor Standard ML. A simple form of conti.cation, tail\u00adcall optimization [19], has been used in traditional \ncompilers for decades. This paper gives a formal presentation of conti.cation in MLton. First, we present \nan overview of the MLton compiler and of FOL, the .rst-order intermediate language on which conti.cation \nis performed. Next, we formalize conti.cation as the combination of an analysis and a transformation. \nOur framework allows a variety of analyses, and de.nes a single transformation that works for any of \nthem. An analysis de\u00adtermines which functions in the program can be viewed as a continuation and the \ncontinuations to which they always re\u00adturn. The transformation uses the results of the analysis to rewrite \nthe program, turning functions into continuations. The main contribution of the paper is the presentation \nof analyses derived from the algorithms used in a previous ver\u00adsion of MLton and in the Moby compiler, \nas well as a new conti.cation analysis that uses the dominator tree of a pro\u00adgram s call graph. We prove \nthat the dominator analysis is optimal, in the sense that it conti.es any function that is conti.able \nby any other approach expressible in our frame\u00adwork. Finally, we describe an implementation of all of \nthese algorithms as part of MLton, and provide measurements of their running time and e.ectiveness. \n2. MLTON This section presents an overview of MLton and describes where conti.cation .ts in the compilation \nprocess. MLton is a whole-program optimizing compiler for SML where the main focus has been the generation \nof e.cient code. It does not support separate compilation. MLton is freely available under the GPL from \nhttp://www.sourcelight.com/MLton. There is a large gap between SML and traditional ILs that use control \n.ow because of SML features like parametric modules, polymorphism, and .rst-class functions. MLton relies \non having the whole program to translate these fea\u00adtures into a simply-typed .rst-order intermediate \nlanguage, FOL, that is similar to SSA. MLton performs most opti\u00admizations on FOL, including conti.cation. \nWe now describe the relevant compiler passes of MLton. First, MLton eliminates module level constructs \nfrom the input SML program. It removes all uses of structures and signatures by moving declarations to \nthe top level and ap\u00adpropriately renaming variables. It removes functors by ap\u00adplying them at compile \ntime, duplicating their bodies for each use [10]. This produces a program in a polymorphic, higher-order \nIL (the XML of [12]). Next, MLton eliminates polymorphism by duplicating each polymorphic expression \nfor each monotype at which it is used. This produces a program in a simply typed, higher-order IL. MLton \nthen performs a monovariant whole-program .ow analysis [13] to determine where each function could be \ncalled. MLton uses the results of the .ow analysis to elim\u00adinate higher-order functions by the closure \nconversion al\u00adgorithm described in [6]. First-class functions become data structures and calls to parameters \nof functional type become ordinary function calls, possibly preceded by a case dispatch. c . Const p \n. Prim k . Cont x . Var f . Func P ::= let fun f(xx)= e...in fm () end e ::= let val x= sin eend | let \ncont k(xx)= e...in eend | k(xx) | if xthen k1() else k2() | f(xx) | k(f(xx)) | xx s ::= c | x | (xx) \n| #ix | p(xx) Figure 1: FOL syntax The resulting program is in FOL, and is simply typed and .rst order. \nNext, MLton performs optimizations including conti.cation. All of MLton s ILs up to and including FOL \nare typed ILs [22]. All of the transformations and optimizations to this point, including conti.cation, \ntake well-typed programs as input and produce well-typed programs as output. In debugging mode, MLton \nruns a type checker on each IL after each optimization. After FOL optimization, MLton translates to a \nlow level untyped IL, and then into C or native x86 instructions. MLton is similar in structure to Tolmach \nand Oliva s RML-to-3GL translator [24]. However, this structure dif\u00adfers from that of most other functional \nlanguage compilers such as Orbit [16], SML/NJ [2], and TIL [22] in three crucial ways. First, MLton is \na whole-program compiler. This en\u00adables certain optimizations like monomorphisation and en\u00adsures that \nall optimization passes have access to more infor\u00admation. Second, MLton performs closure conversion early \nin the compilation process, before most optimization occurs. As a consequence, optimizations (including \nconti.cation) operate on a very simple IL that is closer to more tradi\u00adtional ILs. Third, MLton uses \nwhole-program .ow analysis to drive closure conversion, and the results are expressed di\u00adrectly in FOL. \nThus, optimizations (including conti.cation) bene.t from the control-.ow information computed by the \nanalysis. Optimizations need not recompute control .ow information as is done in many analyses (e.g. \n[25]). Nor do they need to introduce imprecisions based on escaping functions, as is done in [21].  \n3. FOL This section describes a slightly simpli.ed version of MLton s FOL (exception handling constructs \nare omitted) and gives some examples of conti.cation. FOL is very sim\u00adilar to Tolmach and Oliva s First-order \nSIL with jump points [24], which they use as the target language for tail recursion elimination. Figure \n1 presents the syntax of FOL, which is lexically scoped, .rst order, and simply typed. We do not present \nthe typing rules. We assume mutually dis\u00adjoint sets of constants Const, primitives Prim, continuation \nlabels Cont, variables Var, and function names Func. A pro\u00adgram P declares a collection of mutually recursive \nfunctions and calls a distinguished nullary main function fm. Func\u00adtions can take multiple arguments \nand return multiple re\u00adsults. We use xx to indicate a sequence of variables. We assume all variable, \nfunction, and continuation names are unique. An expression e either binds a simple value, declares a \ncon\u00adtinuation, or transfers control to another expression. Simple values are constants, variables, tuples, \nselections from tu\u00adples, or applications of primitives. The syntax cont k(xx)= e declares continuation \nk with arguments xx and body e. Con\u00adtinuations can be declared simultaneously, in which case they are \nmutually recursive. Every expression .nishes by transferring control, either via a jump to a local continu\u00adation \nk(xx), an if-then-else, a tail call f(xx), a nontail call k(f(xx)), or a return to its caller. To keep \nour examples easy to read, we assume the ex\u00adistence of needed constants and primitives, write primitive \napplications in in.x notation, and elide val bindings for con\u00adstants and primitive applications. The \nexample from Sec\u00adtion 1 would be written in FOL as follows. fun g (y) =y -1 fun f (b) = let cont k (x)= \nx+ 1 cont l1 () = k (g (13)) cont l2 () = k (g (15)) inif b then l1() else l2 () end FOL is similar to \nSSA in the manner described in [4]. An FOL continuation declaration cont k(xx)= e is like a basic block \nwith label k that executes the sequence of val bindings in e and transfers control according to the last \nexpression in e. There two di.erences between SSA and FOL are that FOL uses lexical scoping to enforce \nthe SSA condition that a variable de.nition dominate all of its uses and that FOL uses continuation call \nk(xx) to express the f function that assigns the actuals xx to the formals of k. FOL is also similar \nto CPS, although this may not be ap\u00adparent at .rst sight. After all, FOL has returns and nontail calls, \nand functions are not passed continuations as argu\u00adments. However, these are only minor syntactic di.erences, \nas was observed in [11]. The above example might be writ\u00adten in more traditional CPS as follows. fun \ng (y, k)= k(y -1) fun f (b, k)= let cont k (x)= k(x +1) cont l1 () = g (13, k ) cont l2 () = g (15, k \n) inif b then l1() else l2 () end In FOL, the only use of continuations passed as arguments to functions \nwould be to return as in k(x +1) or in tail calls, which are not shown above. Also, FOL nontail calls \nsuch as k (g 13) exactly correspond to traditional CPS tail calls with a nontrivial continuation g (13, \nk ), thus explaining our choice of the cont keyword. The FOL syntax simply elides redundant places where \na continuation variable might be written (formal parameter, return, tail call) and writes nontail calls \nin a di.erent order. Continuing with the example in FOL, the result of conti\u00adfying g within f would look \nlike the following. fun f(b) = letcont k (x) = x+ 1 cont g(y) =k (y -1) cont l1() =g (13) cont l2() =g \n(15) inif bthen l1() else l2 () end Conti.cation has transformed nontail calls to the function g into \njumps to the continuation g. As a .nal example, here is an SML function to sum the elements in a vector. \nfun sum (v) = let fun loop (i, s) = if i = length (v) then s else loop (i +1, s+ sub (v, i)) in loop \n(0, 0) end Here are the FOL functions that MLton would produce. fun sum (v) = loop (v, 0, 0) fun loop \n(v , i, s) = letcont l1() =s cont l2() =let val x =sub (v , i) in loop (v , i +1, s+ x) end val n = length \n(v ) inif i= nthen l1 else l2 end Contifying loop within sum yields the following, which dem\u00adonstrates \nhow recursive continuations express loops. fun sum (v) = let cont loop (v , i, s) = let cont l1() = s \ncont l2() = letval x =sub (v , i) inloop (v , i+1, s +x) end val n = length (v ) inif i =nthen l1else \nl2 end in loop (v, 0, 0) end Although conti.cation may appear to be like inlining, it is di.erent. Inlining \na function replaces a call to a function by its body, substituting actual arguments for formal param\u00adeters. \nOn the other hand, conti.cation does not duplicate code it only moves code from one place to another, \nex\u00adposing control-.ow information. Conti.cation of the above example exposes the information that both \nthe inner and outer call to loop always return to sum. From the above example, conti.cation might appear \nto be a clean-up optimization that is undoing the e.ects of MLton s previous conversion of the program \ninto FOL. Af\u00adter all, conti.cation has just moved loop back inside sum after closure conversion took \nit out. Nevertheless, once we explain the semantics of FOL in the next section, we can show that conti.cation \nis expressing a nontrivial fact about the program needed in order to perform some optimizations, and \nis independent of closure conversion. v . Value = Const + Value * . . Env = Var . Value . . Stack =(Cont \n\u00d7 Env) * s . State = Exp \u00d7 Env \u00d7 Stack -. . State \u00d7 State (const) ( let val x = c in e end,.,.) -. ( \ne,.[x. c],.) (var) ( let val x = y in e end,.,.) -. ( e,.[x. .(y)],.) (tuple) ( let val y)in -. ( e,.[x. \n.(xy)],.) x =(xe end,.,.) (select) ( let val x =#iy in e end,.,.) -. ( e,.[x. pi(.(y))],.) (primapp) \n( let val x = p(xx)in e end,.,.) -. ( e,.[x. d(p,.(x ))],.) '' (cont) ( let cont k(xx)= e ... in eend,.,.)-.( \ne,.,.) (jump) ( k(xx),.,.) -. ( e,.[xy . .(xx)],.) cont k(yx)= e. P (if-true) ( if x then k1() else k2(),.,.) \n-. ( e,.,.) .(x)= true and cont k1() = e. P (if-false) ( if x then k1 else k2,.,.) -. ( e,.,.) .(x)= \nfalse and cont k2() = e . P y . .(xfun f(x(tail) (f(x ),.,.) -. ( e,[xx)],.) y)= e. P (nontail) ( k(f(xx)),.,.) \n-. ( e,[xy . .(xx)],( k,.) :: .) fun f(xy)= e. P (return) (xx,.,( k,.') :: .) -. ( e,.'[xy . .(xx)],.) \ncont k(xy)= e. P Figure 2: FOL operational semantics In the remainder of this paper, we write program \nfrag\u00adments in which we show declarations and calls of relevant functions, but elide continuation declarations. \nWe will al\u00adways write the full expression for a control transfer, distin\u00adguishing between tail and nontail \ncalls. For example, in the following program fragment, fm includes a nontail call to f and a tail call \nto g. fun fm () =... k(f ()) ... g() ... fun f () = ... f() ... fun g () = ... 3.1 Operational semantics \nFigure 2 presents an operational semantics for programs in FOL via a transition relation -. , written \nin in.x. Techni\u00adcally speaking, -. is dependent on the program and should be written -. P , but since \nthe program is always clear from context, we will drop the P. Values v are either constants or tuples \nof other values. An environment . is a map from variables to values. We write .[x . v] to denote the \nenvi\u00adronment .' such that .'(x)= v and .'(y)= .(y)if y x. = We extend this notation to sequences of variables \nand val\u00adues as in .(xx) and .[x . xv], in which the sequences xx and xv must be of the same length. A \nstate (e,.,.) corresponds to the evaluation of expression ein environment . with a stack of callers . \nto which e should return when done. A stack is a sequence of frames written with an in.x cons operator \nas ( k,.) :: ., where ( k,.) is the top frame on the stack and . is the rest of the stack. We write fun \nf(xx)= e . P or cont k(xx)= e. P to mean that the function or continuation declaration is in the program \nP. The rules for simple expressions (const, var, tuple, select, and primapp) are all straightforward \n each adds a binding to the environment and continues with the rest of the ex\u00adpression. The primapp rule \nassumes the existence of a func\u00adtion d : Prim \u00d7 Value * . Value, which de.nes the meaning of primitives. \nThe rule for a continuation declaration does nothing, since continuations are just labels. The rules \nfor local control .ow transfers (jump, if-true, if-false) switch to the body of the desired continuation, \npossibly modifying the current environment. The rules rely on on the fact that all variable names are \ndistinct in order to avoid capture of free variables in continuations. The rules for function call (tail \nand nontail) switch to the body of the desired function in a new environment, binding only the formal \nparameters of the function. The nontail rule adds a frame to the stack; the tail rule does not. The return \nrule pops the top frame o. the stack, and continues with the expression corresponding to the continuation \non the top frame. The transition rules bring out an important di.erence be\u00adtween continuations and functions. \nJumps to continuations maintain in the current environment while function calls (tail and nontail) create \na new environment. Also, unlike traditional CPS, no closure needs to be created for a contin\u00aduation, \nsince its free variables can be found in the current environment (section 5.3 of [15]). The reason that \ncontinua\u00adtions do not need their own environment is that the syntax of FOL guarantees that the environment \nat their point of de.nition always coincides with the environment at a point of use. Therefore, one can \nrepresent the environment as a stack frame and think of FOL variables as stack slots and continuations \nas basic blocks. With this in mind, we can see that the e.ect of conti.cation is to turn function calls \ninto jumps, thus exposing intraprocedural control-.ow in\u00adformation. Also, when a function g is conti.ed \nwithin a function f, it will be able to share the same stack frame with f. Finally, invocations of the \nconti.ed function can be optimized as intraprocedural control-.ow transfers, where live variables can \nbe passed in registers, rather than as in\u00adterprocedural control-.ow transfers, where standard calling \nconventions must be followed. With the semantics in hand, we can now see that in the loop example of \nthe previous section, conti.cation expresses the fact that loop always returns to sum. Hence, it can \nbe implemented as a loop that shares the same stack frame as sum. The fact that loop always returns to \nsum is apparent in the source program, because loop always calls itself in tail position and there is \nonly one outer call. This information is neither obscured nor clari.ed by MLton s translation of the \nsource program into FOL. In fact, the conti.cation analysis in Section 5.1 uses exactly that reasoning \nto contify loop. It is possible to perform conti.cation analysis on the source program, before closure \nconversion. However, in do\u00ading so one would encounter two problems: what to do about higher-order functions \nand how to express the results of the analysis. The .rst problem could be solved by introducing imprecision \ninto the analysis whenever higher-order func\u00adtions obscure control .ow or by doing control-.ow analysis \nalong with conti.cation. In MLton, we take the approach of performing control-.ow analysis before conti.cation, \nthus making its results available to conti.cation. The second problem could be solved by annotating the \nprogram with the results of conti.cation analysis in some way. This then leads to the question of how \nsubsequent optimizations would use the annotation. We do not take this approach in MLton. As a practical \nmatter in compiler construction, we .nd it easier to use an analysis if its results are expressed not \nas an an\u00adnotation on the source program but instead via a program transformation into an IL whose semantics \ndirectly re.ect the results of the analysis. Thus, in MLton and in this pa\u00adper, we use conti.cation analysis \nto transform the program into FOL, which has a very di.erent semantics for function call versus continuation \ncall.  4. CONTIFICATION In this section, we present the two components of conti.\u00adcation: analysis and \ntransformation. A conti.cation analy\u00adsis speci.es what is conti.ed, and where. The transforma\u00adtion rewrites \nthe program based on the analysis. To simplify the presentation, we abstract from FOL and represent a \npro\u00adgram by its call graph. A program P =(fm , N, T) consists of a main function, a multiset of nontail \ncalls, and a multiset of tail calls. A nontail call consists of a caller, callee, and a continuation. \nA tail call consists of a caller and a callee. Formally, we have the following, where we write M(S) for \nthe set of all multisets of S. P . Program = Func \u00d7M(Nontail ) \u00d7M(Tail ) Nontail = Func \u00d7Func \u00d7Cont Tail \n= Func \u00d7Func For each k .Cont, there is a unique f .Func such that k is declared in the body of f; we \nwrite this f as D(k). We de.ne the predicate R: Func .Bool such that R(f)ifand only if there is a path \nof calls from fm to f in P . 4.1 Analysis An analysis is a map from functions to abstract return points. \n. . Return = {Uncalled, Unknown}.Cont .Func A. Analysis = Func .Return The following table describes \nthe intended meaning of A(f), for f .F unc. A(f) Meaning The meaning of A(f)= k is that whenever the \nbody of f is evaluating, the top frame on the stack will have k as its continuation. The meaning of A(f)= \ng is that g is always responsible for calling f, either directly or through an inter\u00advening sequence \nof tail calls. We do not allow analyses to express information about sets of continuations other than \nto use Unknown, which represents the set of all continua\u00adtions. Uncalled f is never called during evaluation \nof P k .Cont f always returns to continuation k g .Func f always returns to function g Unknown f returns \nto multiple k s and/or g s The conti.cation transformation uses an analysis Aas fol\u00adlows. The transformed \nprogram has as its functions those f with A(f)= Unknown. Functions with A(f)= Uncalled are removed from \nthe program. Functions with A(f) . Cont .Func are conti.ed in other functions. For g .Func, the functions \nf such that A(f)= g are conti.ed as the .rst declaration in the body of g.For k .Cont, the functions \nf such that A(f)= k are conti.ed in D(k) as the .rst decla\u00adration after the declaration of k. We now \nintroduce a condition on an analysis that ensures it will lead to a sensible transformation. De.nition \n1. An analysis A is safe for a program P = (fm, N, T) if all of the following hold. *1 if \u00acR(f), then \nA(f)= Uncalled. *2 A(fm )= Unknown. *3 for all nontail calls (f, g, k) .N, if R(f) then A(g) .{k, Unknown}. \n *4 for all tail calls (f, g) .T with f = g, if R(f) then A(g) .{f, A(f), Unknown}. Another way to think \nof safety is that it ensures that the analysis conservatively approximates the actual run time be\u00adhavior \nof the program, in the sense of abstract interpreta\u00adtion [7]. Condition *1 forces unreachable functions \nto be marked Uncalled, which causes the transformation to remove them from the program. We need *1 to \nensure that the transformation is well-de.ned. Condition *2 forces the main function to be marked Unknown, \nwhich prevents it from be\u00ading conti.ed. Condition *3 forces the callee of a nontail call to return to \nthe continuation of the call or be Unknown. Con\u00addition *4 forces the callee of a tail call to return \nto the caller, have the same continuation as the caller, or be Unknown. In the remainder of this paper, \nwe will drop the phrase for a program P from de.nitions and theorems. Any two anal\u00adyses mentioned in \nthe same context will be considered anal\u00adyses for the same program. The following lemma shows that a \nsafe analysis never labels a reachable function Uncalled. Lemma 1. If Ais a safe analysis, then \u00acR(f) \ni. A(f)= Uncalled. Although we can transform a program based on any safe analysis, there are many safe \nanalyses with a wide range of utility. For example, the following trivial analysis is safe. Uncalled \nif \u00acR(f) ATriv(f)= Unknown if R(f) The transformation based on ATriv fails to do anything other than \neliminate dead code. In order for the transformation to be useful (i.e., actually contify functions), \nit must be based on an analysis A with A(f)= Unknown. Hence, we are motivated to search for an analysis \nwhere A(f)= Unknown for as many functions as possible. This leads us to introduce the de.nition of a \nmaximal analysis. P: Program .Program P(let {fun fi(xxi)= ei |i .I}in fm() end)= let {fun fi(xxi)= F(fi,.) \n|i .I and A(fi)= Unknown}in fm() end F: Func \u00d7Cont. .Exp ' F(g,k)= let {cont K(f)(xx)= F(f,k) |A(f)= \ng and fun f(xx)= e .P}in E(e,k) end where fun g(xx)= e.P E: Exp \u00d7Cont. .Exp E(let val x = s in e end,k)= \nlet val x = s in E(e,k) end E(let {cont ki(xxi)= ei |i .I}in e end,k)= let {cont ki(xxi)= E(ei,k) |i.I}. \n i.I {cont K(fij )(xxij )= F(fij ,ki) |j .Ji}in E(e,k) end where for all i .I, {fij |j .Ji}= {f |A(f)= \nki} and for all j .Ji, fun fij (xxij )= eij .P E(k(x ),k)= k(xx) E(if x then k1() else k2(),k)= if x \nthen k1() else k2() E(f(xx),k) =if A(f) = Unknown then K(f)(xx) else if k = k then k(f(xx)) else f(xx) \nE(k(f(xx)),k) =if A(f) = Unknown then K(f)(xx) else k(f(xx)) E(xx,k) =if k = k then k(xx) else x Figure \n3: The conti.cation transformation De.nition 2. A safe analysis A is maximal if for all safe analyses \nB and all functions f . Func, B(f)= Unknown . A(f)= Unknown. In Section 5.3, we will prove by construction \nthat for all programs there exists a maximal safe analysis. A maximal analysis need not be unique the \nfollowing program frag\u00adment shows why. fun fm () =... k(f ()) ... fun f () = ... g() ... fun g () = ... \nFor this program, the analyses A1 and A2 below are both safe and maximal. A1 A2 fm f g Unknown k f Unknown \nk k  4.2 Transformation Figure 3 presents the formal de.nition of the conti.cation transformation. \nP(P ) removes uncalled and conti.ed func\u00adtions and processes the remaining ones. We abuse FOL syn\u00adtax \nslightly and use set notation to indicate collections of si\u00admultaneous function (or continuation) declarations. \nF(g, k) pre.xes the set of functions that are conti.ed in g onto the transform of g s body. We de.ne \nCont. = Cont . {.} and use k to range over Cont.. For a function f that is conti.ed, we need a fresh \nelement of Cont to replace it; we write this element as K(f). E(e, k) transforms expression e, changing \ncalls and returns according to context k.If k = k, then the expression is transformed so that it transfers \ncontrol to the continuation k.If k = ., then control can be transferred as before. The rule for transforming \na set of continuation declarations translates the bodies of the continuations and inserts all functions \nthat are conti.ed at one of the continuations. The rule for a tail call depends on the context and whether \nor not the callee is conti.ed. If the callee is conti.ed (A(f)= Unknown), then the tail call is replaced \nby a jump to the conti.ed function. In this case, the context is irrel\u00adevant because the body of the \ncallee was transformed with the proper context. On the other hand, if the callee is not conti.ed, then \nthe transformation depends on the context k. If k = k, then there is a continuation to which f must return, \nso the tail call becomes a nontail call with continuation k. On the other hand, if k = ., then there \nis no continuation to which f must return, so the tail call remains a tail call. The rule for a nontail \ncall is based on *3, which ensures that if f is a nontail callee and A(f)= Unknown, then A(f)= k. The \nrule for a return replaces the return by a jump when the context requires it. 4.3 Related Transformations \nConti.cation is super.cially similar to lambda drop\u00adping [9], which also nests functions. However, they \nare com\u00adplementary optimizations. Roughly speaking, the block sinking component of lambda dropping uses \nthe call graph to nest a function f in another function g if all calls to f are from within g. Block \nsinking does not approximate the returns of a function and does not change calls from tail to nontail \nor vice versa. Nesting f within g does not imply that f only expresses intraprocedural control .ow within \ng and can share the same stack frame as g. Conti.cation is also super.cially similar to Appel s loop \nheaders [3]. However, once again, they are complementary. The introduction of a loop header is a transformation \nlo\u00adcal to a particular function that allows self tail calls to be turned into jumps and loop invariant \ncode to be moved into the header. It is a useful local optimization, which can take advantage of conti.cation, \nbut does not expose any new in\u00adtraprocedural contro-.ow information. Appel relies on in\u00adlining to do \nthat. MLton introduces loop headers as one of the many FOL optimizations.  4.4 Well-de.nedness of the \ntransformation We state but do not prove in this paper that for safe analy\u00adses, the result of the transformation \nobeys the lexical scoping rules and type system of FOL. In this section, we focus on the more fundamental \nissue of showing that the transforma\u00adtion is well-de.ned, again, only for safe analyses. The rules in \nFigure 3 are not de.ned by induction on the structure of expressions, since the mutually recursive calls \nof E and F use sets of functions determined by the analysis. Conse\u00ad . Uncalled if \u00acR(f) quently, for \nsome nonsensical analyses, the transformation . . . . Unknown if f = fm is not well-de.ned. ACall(f)= \nl if OuterN(f) . OuterT(f)= {l} As an example of a nonsensical analysis, suppose an anal-. . and InnerN(f)= \n\u00d8 . . ysis Aspeci.es A(f)= g and A(g)= f. This would force f Unknown otherwise to be conti.ed in g and \ng to be conti.ed in f, causing Fand E to be unde.ned. The problem with such an analysis is OuterN(f)= \n{k | (g,f, k) . N and R(g) and g = f} OuterT(f)= {g | (g, f) . T and R(g) and g = f} that there is a \ncycle from f to g back to f through A.In InnerN(f)= {k | (f, f,k) . N} order to rule out such analyses, \nwe de.ne a directed graph G =(Node,Edge) based on analysis A. Figure 4: Call analysis Node = Return \nEdge = {(A(f),f) |f .Func}.{(D(k),k) |k .Cont} 5. ANALYSES In this section, we present three safe analyses: \nthe origi- The mutually recursive calls to F and E in Figure 3 cor\u00adnal conti.cation analysis used in \nMLton, the analysis used respond to a traversal of G starting at Unknown. Thus, if in Moby, and our new \ndominator analysis. These analyses there are no cycles reachable from Unknown in G, the trans\u00advary in \ntheir complexity and their utility in guiding trans\u00ad formation is well-de.ned. This leads us to introduce \nthe formations. Their range demonstrates the generality of our de.nition of an acyclic analysis, and \nto prove that a safe framework and the ease with which analyses can be de.ned analysis is always acyclic, \nand hence always leads to a well\u00adon FOL. As in Section 4, we assume a .xed, but arbitrary, de.ned transformation. \nprogram P =(fm,N,T) when de.ning the analyses. De.nition 3. An analysis Ais cyclic if there exists a \nse-5.1 The ACall Analysis quence l0,...,ln . Cont .Func such that l0 = ln and for Our .rst analysis, \nthe call analysis, is a syntax driven all 0 =i<n, either (1) li .Func and A(li)= li+1 or (2) analysis \nthat has been used in MLton since September 1998. li .Cont and D(li)= li+1. An analysis is acyclic ifitis \nnot The analysis is based on the observation that a function has cyclic. one return location if there \nis exactly one reachable call to the function from outside its body and if there are only tail The key \nto proving that safety implies acyclicity is the obser\u00adcalls to the function within its body. For example, \nthis was vation is that any purported cycle in a safe analysis must be the case with the loop function \nused to sum the elements in composed of unreachable functions, which must be marked a vector in Section \n3. Uncalled by *1, contradicting the de.nition of a cycle. We We formally de.ne the call analysis in \nFigure 4. We de.ne formalize this reasoning in the following theorem. the multisets OuterN(f), OuterT(f), \nand InnerN(f), corre\u00adsponding to the continuations of reachable nontail calls to Theorem 2. If Ais a \nsafe analysis, then Ais acyclic. f from outside its body, the reachable tail callers of f from outside \nits body, and the continuations of nontail calls to f Proof. Suppose, by way of contradiction, that \nthere ex-from inside its body, respectively. For a function f, there ists l0,...,ln .Cont .Func such \nthat l0 = ln and for each is one reachable call from outside its body if and only if 0 = i<n, either \n(1) li . Func and A(li)= li+1 or (2) OuterN(f) . OuterT(f)= {l}. Further, all calls from f to li . Cont \nand D(li)= li+1. Condition (2) implies that itself are in tail position if and only if InnerN(f)= \u00d8. \nthere exists li .Func. Condition (1) implies that for each The proof of safety of the call analysis is \nstraightforward. li .Func, A(li)= Uncalled. Hence R(li), by Lemma 1, and Theorem 3. Ais safe. there \nexists a path of calls from fm to li. Call Consider a .xed path pfrom fm to li, composed of nontail Proof. \nWe show that ACall satis.es each of the safety calls (gj,gj+1,kj) .N and tail calls (gj,gj+1) .T. There \nconditions. is a least j such that gj .{l0,...,ln},say gj = li. . ' *1 If \u00acR(f), then ACall(f)= Uncalled \nby the .rst clause. We show that A(gj. )= li.+1 for all j = j by induc\u00adtion on j ' . If j ' = 0, then \ngj. = fm and A(gj. )= *2 ACall(fm )= Unknown by the second clause. Unknown by *2, because A is safe. \nIf j ' > 0, then ei\u00adther (gj.-1,gj. ,kj.-1) . N or (gj.-1,gj. ) . T is on p. *3 Suppose (f,g,k) . N such \nthat R(f). Therefore, In the .rst case, A(gj. ) .{kj.-1,Unknown} by *3,be-R(g) and ACall(g)= Uncalled, \nbecause the .rst clause cause Ais safe. By the minimality of j and condition (2), does not apply. If \nf = g, then InnerN(g)= \u00d8 and kj.-1 = li.+1. Hence, A(gj. )= li.+1. In the second case ACall(f)= Unknown \n.{k,Unknown}.If f = g, then A(gj. ) .{gj.-1,A(gj.-1),Unknown} by *4, because A is OuterN(g) .{k}and ACall(g) \n.{k,Unknown}. safe. By the minimality of j and condition (1), gj.-1 = li.+1 *4 Suppose (f,g) .T such \nthat f = g and R(f). There\u00adand, by the induction hypothesis, A(gj.-1)= li.+1. Hence, fore, R(g) and ACall(g)= \nUncalled, because the .rst A(gj. )= li.+1. clause does not apply. Also, OuterT(g) .{f} andThus A(li. \n)= li.+1, which is a contradiction. ACall(g) .{f,Unknown}.{f,ACall(f),Unknown}. Theorem 2 guarantees \nthat G is an acyclic graph if Ais Although the call analysis is useful in practice, it fails safe. In \nfact, G is a forest of exactly two trees, rooted at to contify functions in many simple cases. For example, \nUncalled and Unknown respectively. consider the following program fragment. ACont = lfp(C) C : Analysis \n.Analysis C(A)(f)= if f = fm then Unknown ( ) . {k |(g, f, k) .N and R(g)} else .{A(g) |(g, f) .T and \nR(g)} Figure 5: Continuation analysis fun fm () =... k(f ()) ... k(g ()) ... fun f () = ... g() ... h() \n... fun g () = ... f() ... h() ... fun h () = ... In this case, f, g, and h always return to the continuation \nk, either directly or by returning to a function which always returns to the continuation k. Thus we \ncould contify f, g, and h within fm. However, because each function is called at multiple places outside \nits body, the call analysis is useless: ACall(f)= ACall(g)= ACall(h)= Unknown. We would like an algorithm \nto compute the safe analysis A(f)= A(g)= A(h)= k. The analysis in the next section will do just that. \n 5.2 The ACont Analysis Our second analysis, the continuation analysis, is based on the analysis in \nMoby, which computes an approximation of return continuations of each known function [21]. Unlike the \ncall analysis, which gives up if a function is called from many places, the continuation analysis conti.es \na function if the function returns to a single continuation through one or more (possibly disjoint) sequences \nof tail calls. Viewed from the point of view of CPS, ACont determines when a continuation variable takes \non a constant value. To de.ne the continuation analysis, we arrange the ele\u00adments of Return in a lattice. \nWe de.ne Uncalled c l c Unknown for any l .Cont .Func and de.ne .1 U.2 to be the least upper bound of \n.1 and .2. We extend cand U pointwise to form a lattice on Analysis. The continuation analysis is de.ned \nvia the least .xpoint in Figure 5. The idea behind the analysis is that a function f returns to continuation \nk if all reachable nontail calls to f use k and if all tail callers of f also return to continuation \nk. The least .xpoint ties the recursion in the previous sentence. It ensures that ACont(f)= k if and \nonly if all paths of reachable tail calls to f start with a function that returns to continuation k. \nTheorem 4. ACont is safe. Proof. We show that ACont satis.es each of the safety conditions. *1 Recall \nATriv de.ned in Section 4.1. If R(f), then C(ATriv )(f) cUnknown = ATriv (f). If \u00acR(f), then C(ATriv \n)(f)= Uncalled = ATriv(f). Hence, we have C(ATriv ) cATriv and ACont = lfp(C) cATriv. There\u00adfore, if \n\u00acR(f), then ACont(f) cATriv (f)= Uncalled. Therefore, ACont(f)= Uncalled. *2 ACont(fm )= C(ACont)(fm) \n=if f = fm then Unknown else ... = Unknown. *3 Suppose (f, g, k) .N such that R(f). ACont(g) = C(ACont)(g) \n=if g = fm then Unknown ( ) . {k |(f, g,k) .N and R(f)} else .{ACont(f) |(f, g) .T and R(f)}=if g = \nfm then Unknown else U({k}....) .{k, Unknown}. *4 Suppose (f, g) .T such that R(f). Then ACont(g) = C(ACont)(g) \n=if g = fm then Unknown ( ) . {k |(f, g,k) .N and R(f)} else .{ACont(f) |(f, g) .T and R(f)}=if g = fm \nthen Unknown else U(ACont(f) ....) .{ACont(f), Unknown}.{f, ACont(f), Unknown}. The continuation analysis \ndi.ers from the analysis in [21] in several ways. First, our analysis operates over the en\u00adtire program, \nwhile Reppy s analyzes a module in isolation. Second, ours operates over a .rst-order IL, while Reppy \ns op\u00aderates over a higher-order language. Third, ours runs after MLton s control-.ow analysis, which \nexposes control-.ow information in the .rst order FOL program. Reppy s anal\u00adysis introduces imprecision \nwhen escaping function cause un\u00adknown control-.ow. Finally, since our language is already in continuation-passing \nstyle, there is no need for local CPS conversion in order to apply the conti.cation transforma\u00adtion. \nThe example at the end of Section 5.1 demonstrates that on some programs ACont conti.es more functions \nthan ACall. One can also construct programs on which ACall will con\u00adtify more functions than ACont. Furthermore, \nthere are pro\u00adgrams with conti.able functions that are found by neither analysis. For example, consider \nthe following program frag\u00adment. fun fm ()= ... k1(f ()) ... k2 (f()) ... fun f() = ... g1() ... g2() \n... fun g1 ()= ... h() ... fun g2 ()= ... h() ... fun h() = ... The following three analyses, ACall, \nACont, and A, are safe. ACall ACont A fm Unknown Unknown Unknown f Unknown Unknown Unknown g1 f Unknown \nf g2 f Unknown f h Unknown Unknown f The analysis Acaptures our intuition that h can be conti\u00ad.ed along \nwith g1 and g2 within f. The call analysis fails to contify h because it is called from more than one \nplace. Further, although h always returns to f, the continuation analysis fails to contify h because \nit determines that f, g1, g2, and h all have the same set of multiple return locations, {k1, k2}. The \nanalysis in the next section will compute A. ADom(f)= if parentD(f)= Root then if R(f) then Unknown else \nUncalled else l, where l .ancestorsD(f) and parentD(l)= Root D is the dominator tree of G =(Node,Edge) \nNode = {Root}.Cont .Func Edge = {(Root,fm)}. 1 {(Root,k) |k .Cont}. 2 {(Root,f) |\u00acR(f)}. 3 {(f, g) |(f, \ng) .T and R(f)}. 4 {(k, g) |(f, g, k) .N and R(f)} 5 Figure 6: Dominator analysis  5.3 The ADom Analysis \nOur .nal analysis, the dominator analysis, fully utilizes the control .ow information available in FOL \nto deter\u00admine exactly how far a function should be allowed to return through tail calls. The analysis \ncan contify a function f in some cases even if f s continuation is not constant. Af\u00adter de.ning the dominator \nanalysis, we will prove that it is both safe and maximal. That is, it conti.es at least as many functions \nas ACall, ACont, or any other safe analysis. The dominator analysis, ADom, is de.ned in Figure 6. It \nuses a directed graph G similar to the call graph of the pro\u00adgram, but which contains the return information \nneeded for conti.cation. For f . Func, each edge (l, f) . Edge in\u00addicates that f returns to the location \nl.If l = Root, then the edge indicates that f has no return location; either f is uncalled or f is the \nmain function. The dominator analysis is de.ned using the dominator tree D of G. As a reminder, anode \nn dominates anode n ' in a graph if and only if every path from the root to n ' goes through n [17]. \nFurther, node n '' immediately dominates node n if n dominates n and every dominator of n ' (other than \nn ' ) dominates n. The nodes of the graph can be arranged into the dominator tree, D, where n is the \nparent of n ' if n is the immediate domina\u00adtor of n ' . Figure 7 shows the graph G and the dominator \ntree D corresponding to the program fragment at the end of Section 5.2. To ensure that the dominator \ntree of G exists, we need the following lemma, which shows that G is a connected graph rooted at Root. \nLemma 5. For all l .Cont .Func, there is a path from Root to l in G. Proof. If l . Cont, then (Root,l) \n. Edge by 2.If l .Func and \u00acR(l), then (Root,l) .Edge by 3. Finally, if l .Func and R(l), then there \nis a path of calls from fm to l. We proceed by induction on the length of the path. If n = 0 then l = \nfm and (Root,l) .Edge by 1.If n> 0, then there exists f .Func such that there exists a path of n -1 calls \nfrom fm to f and (f, l, k) .N or (f, l) .T.If (f, l, k) .N, then (k, l) .Edge by 5 and (Root,k) .Edge \nby 2.If(f, l) .T, then (f, l) .Edge by 4 and there is a path from Root to f in G by the induction hypothesis. \nThe set of dominators of a node f . Func is the set of locations to which f always returns in any execution \nof the Dominator tree D Graph G Figure 7: Dominator example program. However, we cannot de.ne ADom(f)tobean \nar\u00adbitrary dominator of f; this could easily lead to an unsafe analysis. Instead, for l .Cont .Func, \nlet parentD(l) be the parent of l in D and let ancestorsD(l) be the set of ancestors of l in D. The dominator \nanalysis de.nes ADom(f) as the dominator of f closest to Root. Thus, we see in Figure 7 that ADom(g1)= \nADom(g2)= ADom(h)= f. Theorem 6. ADom is safe. Proof. We show that ADom satis.es each of the safety conditions. \n*1 If \u00acR(f), then (Root,f) .Edge by 3. Hence, parentD(f)= Root and ADom(f)= Uncalled. *2 Note (Root,fm \n) . Edge by 1 and R(fm). Hence, parentD(fm )= Root and ADom(fm )= Unknown. *3 Suppose (f, g, k) .N and \nR(f). Hence, R(g), (Root,k) . Edge by 2, and (k, g) . Edge by 5. Therefore, parentD(g) .{Root,k} and \nparentD(k)= Root. Hence, ADom(g) .{k, Unknown}. *4 Suppose (f, g) . T and R(f). Hence, R(g) and (f, g) \n. Edge by 4.If parentD(g)= Root, then ADom(g)= Unknown .{f, ADom(f), Unknown}.If parentD(g)= Root, then \nlet l . ancestorsD(g) such that parentD(l)= Root. Thus, ADom(g)= l.If l = f, then ADom(g)= f .{f, ADom(f), \nUnknown}.If l = f, then l . ancestorsD(f) and parentD(f)= Root,be\u00adcause l dominates f in G. Therefore, \nADom(f)= l and ADom(g)= ADom(f) .{f, ADom(f), Unknown}. To show that ADom is maximal, we need the following \nlemma that relates a safe analysis to paths in the graph G. Lemma 7. Let A be safe and let Root,l, f0,...,fn \nbe a path in G. Then A(fn) .{fn-1,...,f0,l, Unknown}. Proof. By induction on the length of the path. \nn = 0: Suppose l . Func. First, note that (Root,l) . Edge either by 1 (with l = )orby (with \u00acR(l)). fm \n3 Second, note that (l, f0) .Edge by 4 (with (l, f0) .T and R(l)). Thus, l = fm. Therefore, A(l)= Unknown \nby *2 and A(f0) .{l, A(l), Unknown}= {l, Unknown},by *4. Program Lines Contify time (seconds) Compile \ntime (seconds) Executable sizes (bytes, normalized) none ACall ACont ADom Run times (normalized) ACall \nACont ADom barnes-hut 1,262 0.06 4.68 67,641 0.93 0.93 0.94 0.68 0.97 0.81 count-graphs 538 0.05 2.82 \n66,012 0.85 0.83 0.83 0.73 0.80 0.71 hamlet 22,895 2.54 103.59 1,107,661 0.95 0.92 0.90 0.79 0.86 0.94 \nkit 73,490 13.24 682.18 5,729,035 1.02 0.99 0.97 0.72 0.93 0.72 lexgen 1,327 0.28 10.08 170,578 0.89 \n0.90 0.85 0.73 0.76 0.68 mlton 92,134 13.26 572.55 5,242,115 0.94 0.91 0.90 0.72 0.88 0.74 mlyacc 7,295 \n0.67 37.91 472,514 0.96 0.94 0.93 0.59 0.91 0.59 raytrace 2,378 0.36 19.89 235,058 0.89 0.93 0.84 1.05 \n1.05 1.05 tensor 2,947 0.17 5.42 75,418 0.88 0.87 0.90 0.14 0.14 0.15 vliw 3,694 0.65 24.20 278,006 1.03 \n0.97 1.00 0.59 0.81 0.59 zern 595 0.02 1.49 45,375 0.89 0.89 0.91 0.29 0.29 0.31 Table 1: Compilation \nand run time statistics Suppose l . Cont. Note that (Root,k) . Edge by 2 and (l,f0) . Edge by 4 (with \n(g,f0,l) . N and R(g)). Therefore, A(f0) .{l,Unknown},by *3. n> 0: Note that (fn-1,fn) . Edge by 4 with \n(fn-1,fn) . T and R(fn-1). By *4, A(fn) . {fn-1,A(fn-1),Unknown}. From the induction hypothesis, we know \nthat A(fn-1) .{fn-2,...,f0,l,Unknown}.Thus we know that A(fn) .{fn-1,fn-2,...,f0,l,Unknown}. A simple \ncorollary of Lemma 7 is the key to proving that ADom is maximal. Corollary 8. If Ais safe and A(f) .Cont \n.Func then A(f) dominates f in G. Theorem 9. ADom is maximal. Proof. Let Bbe an arbitrary safe analysis \nand f .Func be arbitrary. We consider the possible values of B(f). If B(f)= Unknown, we are done. If \nB(f)= Uncalled, then \u00acR(f) and ADom(f)= Uncalled by Lemma 1, because B and ADom are safe analyses. Finally, \nsuppose B(f)= l . Cont .Func. Hence, f = fm and R(f)by *2 and Lemma 1, because Bis safe. By examining \nthe cases in the de.nition of Edge, we see that every path from Root to f in G has length greater than \n1. By Corollary 8 applied to B, l = B(f) dominates f in G. Therefore, parentD(f)= Root and ADom(f) .Cont \n.Func. The argument above proves that ADom is a maximal anal\u00adysis, but we can be more precise about the \nrelationship be\u00adtween ADom and other safe analyses. One can extend the argument above to prove the following \ntheorem. Theorem 10. Let Abe safe. If A(f)= k .Cont, then ADom(f)= k.If A(f) . Func, then ADom(f) . Cont \n. Func. This theorem shows that the dominator analysis favors con\u00adti.cation at continuations over conti.cation \nin functions. It also implies that the conti.cation and dominator analyses should agree on many functions. \nIn fact, k dominates f in the graph de.ned in Figure 6 if and only if all paths of reachable tail calls \nstart with a function that returns to k; so, ACont is equivalent to an analysis that assigns A(f)= k \nif k dominates f.  6. EXPERIMENTS As described in Section 1, many compiler optimizations are enabled \nby exposing the intraprocedural control-.ow of a program through the conti.cation transformation. It \nis therefore not surprising that a conti.cation pass (similar to the call analysis) was the .rst FOL \noptimization added to MLton in September 1998. The conti.cation pass now runs at three places in the \nFOL optimizer, with intervening opti\u00admization passes including constant propagation, dead-code elimination, \ninlining, raise-to-jump transformation, loop op\u00adtimizations, and shrink reductions [5]. To demonstrate \nthe practicality and bene.ts of the trans\u00adformation and analyses described in this paper, we imple\u00admented \na new conti.cation pass in the MLton compiler. Paralleling the presentation in Section 4, in which we \nsepa\u00adrate conti.cation into analysis and transformation, our new pass consists of an analysis phase that \nproduces an anno\u00adtation and a transformation phase that conti.es based on the annotation. We have implemented \neach of the analyses described in Section 5. The implementations of ACall and ACont are straightforward. \nThe implementation of ADom uses the Lengauer-Tarjan dominator algorithm [17] as pre\u00adsented in [19]. We \nmeasured the impact of conti.cation on compile times and running times for a representative sample of \nbenchmarks with sizes up to 92K lines. Among the benchmarks, lexgen, mlyacc, and vliw are standard [2]; \nbarnes-hut, tensor, and zern are .oating-point intensive and count-graphs is mostly symbolic.1 The raytrace \nbenchmark was the win\u00adning entry in the Third Annual ICFP Programming Con\u00adtest.2 The mlton benchmark \nis the compiler itself; kit is the ML-Kit (Version 3) [23]; hamlet is the HaMLet SML in\u00adterpreter.3 The \nbenchmarks were compiled with the native x86 backend and executed on an 800 MHz Intel Pentium III 1Juan \nJose Garcia Ripoll (worm@arrakis.es) wrote tensor, David McClain (dmcclain@azstarnet.com ) wrote zern, \nand Henry Cejtin (henry@sourcelight.com) wrote count-graphs. 2Team PLClub (http://www.cis.upenn.edu/~sumii/icfp) \nwrote the original version of raytrace in O Caml. Stephen Weeks translated it to SML, and John Reppy \n(jhr@research.bell-labs.com ) made further modi.ca\u00adtions. 3Andreas Rossberg (rossberg@ps.uni-sb.de ) \nwrote hamlet. Analysis barnes-hut count-graphs hamlet kit none 0/118 0/96 0/43 0/102 0/82 0/48 0/1689 \n0/1625 0/943 0/7381 0/7192 0/3083 ACall 67/118 6/49 0/ 7 61/102 7/41 0/11 384/1689 14/1273 2/648 2776/7381 \n69/4527 2/1691 ACont 64/118 6/52 0/10 62/102 8/40 0/10 370/1689 13/1287 0/702 2570/7381 54/4736 3/1900 \nADom 72/118 5/44 0/ 9 69/102 6/33 0/ 9 653/1689 12/1004 0/578 3402/7381 50/3911 3/1585 Analysis lexgen \nmlton mlyacc raytrace none 0/245 0/229 0/125 0/9921 0/8435 0/2389 0/818 0/779 0/453 0/301 0/277 0/121 \nACall 121/245 3/124 0/ 37 4207/9921 439/4695 2/1153 419/818 9/396 1/111 124/301 10/177 0/ 39 ACont 109/245 \n2/136 0/ 51 3656/9921 544/5247 0/1456 360/818 7/455 1/172 117/301 10/184 0/ 46 ADom 139/245 2/106 0/ \n34 4749/9921 438/4156 0/1049 473/818 7/342 0/103 172/301 9/129 0/ 32 Analysis tensor vliw zern none 0/156 \n0/129 0/59 0/501 0/447 0/247 0/49 0/41 0/25 ACall 98/156 10/ 57 0/ 7 229/501 20/268 0/ 92 34/49 3/15 \n0/ 2 ACont 94/156 9/ 61 0/11 209/501 15/288 0/115 36/49 1/13 0/ 2 ADom 102/156 10/ 53 0/ 6 254/501 16/245 \n0/ 83 37/49 1/12 0/ 1 Table 2: Number of functions conti.ed in each conti.cation pass with 256 MB of \nmemory, with the exceptions of mlton and kit, which were compiled on a 733 MHz Intel Pentium III with \n512 MB of memory. All benchmarks are available at http://www.sourcelight.com/MLton. In the .rst part \nof Table 1, we report the number of lines of SML for each benchmark, the total amount of time spent in \nconti.cation, and the total compile time in seconds. The number of lines does not include approximately \n8000 lines of basis library code that MLton pre.xes to each program. The contify time is the sum over \nall three conti.cation passes, and includes the time to compute all three analyses and to perform the \ntransformation based on ADom. The total con\u00adti.cation time was typically about 2% of the total compile \ntime, and was never more than 4%. In the second part of Table 1, we report the absolute size in bytes \nfor each benchmark compiled with conti.cation dis\u00adabled. We also report the sizes when compiled using \neach analysis, normalized to the absolute size. The results show that conti.cation almost always has \na bene.cial e.ect on executable size. In the third part of Table 1, we report the running time of each \nbenchmark, compiled using each conti.cation analysis, normalized to the running time with conti.cation \ndisabled. Unsurprisingly, these results show that conti.cation typically yields a signi.cant speedup. \nIn Table 2, we report the number of functions conti.ed by each analysis. For each benchmark, there are \nfour rows, one for each analysis (none means conti.cation was disabled). Each row contains the counts \nfor each of the three contif\u00adication passes. Each cell in the table reports the number functions conti.ed, \nfollowed by a / , followed by the total number of functions in the program input to the conti.ca\u00adtion \npass. The counts show that for most of the benchmarks, by the last round of conti.cation, there are no \nconti.able functions detectable by the analysis. They also show that conti.ca\u00adtion, by any analysis, \nhas a signi.cant impact on the total number of functions in the program. Although other opti\u00admizations \nreduce the number of functions (e.g., inlining), in these examples, a conti.ed program generally has \nat most half the number of functions of an unconti.ed program. The counts also show that in almost all \ncases, because ADom is maximal, it produces a result with fewer functions than ACall or ACont. The only \nexception is barnes-hut, where ADom yields a program with 9 functions, but ACall yields a program with \n7 functions. In this case, ADom conti\u00ad.ed a function that ACall did not, increasing its size beyond the \nlimit used by the inliner that runs between the second and third rounds of conti.cation. This interaction \nwith the inliner also explains why conti.cation, which should enable more optimizations, does not always \nlead to smaller code size. It also partially explains why conti.cation with ADom does not always lead \nto better running times than contif\u00adication with ACall and ACont. We also suspect that this discrepancy \nin running times is an artifact of the evolution of the FOL optimizer, which developed around an original \nconti.cation pass similar to ACall. 6.1 Mutual recursion in FOL This section contains a minor note about \na di.erence be\u00adtween the FOL language described in this paper and as im\u00adplemented in MLton. The conti.cation \ntransformation is complicated by the fact that MLton s current implementation of FOL does not allow continuations \nto be simultaneously declared as mu\u00adtually recursive. Recall that the conti.cation transforma\u00adtion described \nin Section 4.2 requires the set of functions {f .Func |A(f)= l}to be declared simultaneously. The limitations \nof the implementation of FOL are problematic when this set contains multiple functions. However, because \ncontinuation declarations can be nested, it is possible to de\u00ad.ne mutually recursive continuations by \nnesting one within another (this approach is used in [9]). Unfortunately, this nesting is not su.cient \nto contify all sets of mutually recur\u00adsive de.nitions. For example, we cannot contify g1 and g2 in the \nfollowing code fragment. fun f (b) = let cont k (x) =... cont l1 () =k (g1 ()) cont l2 () =k (g2 ()) \ninif b then l1 () else l2 () end fun g1 () = ... g2 ()... fun g2 () = ... g1 ()... Fortunately, the limitations \nof FOL did not signi.cantly a.ect the results in this section. When our full bench\u00admark suite (including \n18 additional benchmarks not re\u00adported here) is compiled with the dominator analysis, there are over \n11,000 functions marked conti.able, only 78 of which must be nested and only 25 of which cannot be con\u00adti.ed \ndue to the absence of mutual recursion. The absence of mutually recursive continuations in FOL is a historical \naccident, and is not fundamental. We are considering improving MLton to handle them.  7. CONCLUSION \nConti.cation is not a new concept in functional-language compiler optimizations, but all previous work \nin this area has focused on presenting a single conti.cation analysis and transformation. We have presented \na simple, yet general, framework for expressing conti.cation analyses. This gen\u00aderality has allowed us \nto de.ne a maximality criterion for analyses and to introduce a single transformation that can be applied \nto any analysis satisfying a safety condition. We have shown how to express a number of existing analyses \nin our framework and presented a new maximal analysis based on the dominator tree of a program s call \ngraph. Finally, our implementation in MLton has shown that con\u00adti.cation is e.cient, taking a small percentage \nof compile time, and leads to improved run times. Although we have veri.ed that the dominator analysis \nconti.es more functions than existing analyses, we have not been able to show that this leads to consistently \nbetter run times. Nevertheless, the increased conti.cation has convinced us to switch to the dominator \nanalysis MLton. We believe that the unde\u00adsirable interaction with inlining can be .xed and that the improved \nintraprocedural control-.ow information provided by the dominator analysis will provide more bene.ts \nthan other analyses to existing and planned optimizations. ACKNOWLEDGEMENTS We would like to thank John \nReppy for providing helpful information about conti.cation in Moby. We would like to thank Henry Cejtin, \nNeal Glew, Suresh Jagannathan, Nathaniel Nystrom and the anonymous referees for helpful comments on the \npaper. 8. REFERENCES [1] A. V. Aho, R. Sethi, and J. D. Ullman. Compilers: Principles, Techniques, and \nTools. Addison Wesley, 1986. [2] A. W. Appel. Compiling with Continuations. Cambridge University Press, \n1992. [3] A. W. Appel. Loop headers in .-calculus or CPS. Lisp and Symbolic Computation, 7:337 343, 1994. \n[4] A. W. Appel. SSA is functional programming. ACM SIGPLAN Notices, 33(4):17 20, 1998. [5] A. W. Appel \nand T. Jim. Shrinking lambda expressions in linear time. Journal of Functional Programming, 7(5):515 \n540, 1997. [6] H. Cejtin, S. Jagannathan, and S. T. Weeks. Flow-directed closure conversion for typed \nlanguages. In European Symposium on Programming, pages 56 71, Mar. 2000. [7] P. Cousot and R. Cousot. \nAbstract interpretation: A uni.ed lattice model for static analysis of programs by construction or approximation \nof .xpoints. In Symposium on Principles of Programming Languages, pages 238 252, 1977. [8] R. Cytron, \nJ. Ferrante, B. K. Rosen, M. N. Wegman, and F. K. Zadeck. E.ciently computing static single assignment \nform and the control dependence graph. Transactions on Programming Languages and Systems, 13(4):451 490, \nOctober 1991. [9] O. Danvy and U. P. Schultz. Lambda-dropping: Transforming recursive equations into \nprograms with block structure. Theoretical Computer Science, 248(1 2):243 287, 2000. [10] M. Elsman. \nStatic interpretation of modules. In International Conference on Functional Programming, pages 208 219, \nSept. 1999. [11] C. Flanagan, A. Sabry, B. F. Duba, and M. Felleisen. The essence of compiling with continuations. \nIn Conference on Programming Language Design and Implementation, pages 237 247, June 1993. [12] R. Harper \nand J. C. Mitchell. On the type structure of Standard ML. Transactions on Programming Languages and Systems, \n15(2):211 252, April 1993. [13] N. D. Jones. Flow analysis of lambda expressions. In International Colloquium \non Automata, Languages, and Programming, volume 115, pages 114 128. Springer-Verlag, 1981. [14] R. A. \nKelsey. A correspondence between continuation passing style and static single assignment form. In Workshop \non Intermediate Representations, pages 13 22, Jan. 1995. [15] R. A. Kelsey and P. Hudak. Realistic compilation \nby program transformation. In Symposium on Principles of Programming Languages, pages 281 292, 1989. \n[16] D. Kranz, R. A. Kelsey, J. Rees, P. Hudak, J. Philbin, and N. Adams. ORBIT: An optimizing compiler \nfor Scheme. In Symposium on Compiler Construction, pages 219 233, June 1986. [17] T. Lengauer and R. \nE. Tarjan. A fast algorithm for .nding dominators in a .owgraph. Transactions on Programming Languages \nand Systems, 1(1):121 141, 1979. [18] MLton, a whole program optimizing compiler for Standard ML. http://www.sourcelight.com/MLton/. \n[19] S. S. Muchnick. Advanced Compiler Design and Implementation. Morgan Kaufmann Publishers, 1997. [20] \nJ. E. Piazza. System for conversion of loop functions in continuation-passing style. US Patent 5881291, \nMar. 1999. [21] J. Reppy. Local CPS conversion in a direct-style compiler. In Workshop on Continuations, \npages 1 5, Jan. 2001. [22] D. Tarditi, J. G. Morrisett, P. Cheng, C. Stone, R. Harper, and P. Lee. TIL: \nA type-directed optimizing compiler for ML. In Conference on Programming Language Design and Implementation, \npages 181 192, 1996. [23] M. Tofte, L. Birkedal, M. Elsman, N. Hallenberg, T. H. Olesen, and P. S. Bertelsen. \nProgramming with regions in the ML Kit (for version 3). Technical Report 98/25, University of Copenhagen, \n1998. [24] A. P. Tolmach and D. Oliva. From ML to Ada: Strongly-typed language interoperability via source \ntranslation. Journal of Functional Programming, 8(4):367 412, 1998. [25] K. Yi and S. Ryu. A cost-e.ective \nestimation of uncaught exceptions in Standard ML programs. Theoretical Computer Science, 237(1), 2000. \n  \n\t\t\t", "proc_id": "507635", "abstract": "Contification is a compiler optimization that turns a function that always returns to the same place into a continuation. Compilers for functional languages use contification to expose the control-flow information that is required by many optimizations, including traditional loop optimizations. This paper gives a formal presentation of contification in MLton, a whole-program optimizing Standard ML compiler. We present two existing algorithms for contification in our framework, as well as a new algorithm based on the dominator tree of a program's call graph. We prove that the dominator algorithm is optimal. We present benchmark results on realistic SML programs demonstrating that contification has minimal overhead on compile time and significantly improves run time.", "authors": [{"name": "Matthew Fluet", "author_profile_id": "81100181338", "affiliation": "Cornell Univ.", "person_id": "PP17009813", "email_address": "", "orcid_id": ""}, {"name": "Stephen Weeks", "author_profile_id": "81100226831", "affiliation": "InterTrust STAR Laboratories", "person_id": "PP31032926", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/507635.507639", "year": "2001", "article_id": "507639", "conference": "ICFP", "title": "Contification using dominators", "url": "http://dl.acm.org/citation.cfm?id=507639"}