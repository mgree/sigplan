{"article_publication_date": "10-01-2001", "fulltext": "\n Automatic Generation of Staged Geometric Predicates Aleksandar Nanevski School of Computer Science \nCarnegie Mellon University Pittsburgh, PA 15213-3891  aleks@cs.cmu.edu Guy Blelloch School of Computer \nScience Carnegie Mellon University Pittsburgh, PA 15213-3891  blelloch@cs.cmu.edu ABSTRACT Algorithms \nin Computational Geometry and Computer Aid\u00aded Design are often developed for the Real RAM model of computation, \nwhich assumes exactness of all the input argu\u00adments and operations. In practice, however, the exactness \nimposes tremendous limitations on the algorithms even the basic operations become uncomputable, or prohibitively \nslow. When the computations of interest are limited to de\u00adtermining the sign of polynomial expressions \nover .oating\u00adpoint numbers, faster approaches are available. One can evaluate the polynomial in .oating-point \n.rst, together with some estimate of the rounding error, and fall back to exact arithmetic only if this \nerror is too big to determine the sign reliably. A particularly e.cient variation on this approach has \nbeen used by Shewchuk in his robust implementations of Orient and InSphere geometric predicates. We extend \nShewchuk s method to arbitrary polynomial expressions. The expressions are given as programs in a suitable \nsource language featuring basic arithmetic opera\u00adtions of addition, subtraction, multiplication and squaring, \nwhich are to be perceived by the programmer as exact. The source language also allows for anonymous functions, \nand thus enables the common functional programming technique of staging. The method is presented formally \nthrough several judgments that govern the compilation of the source expres\u00adsion into target code, which \nis then easily transformed into SML or, in case of single-stage expressions, into C. 1. INTRODUCTION \nAlgorithms in Computational Geometry and Computer Aided Design are often created for the Real RAM model \nof computation. Real RAM model assumes exactness of all the arguments and operations involved in the \ncalculations, thus making it easy to carry out the mathematical arguments behind the algorithm. Unfortunately, \nthis very fact implies that the computations have to be done with unbounded or in.nite precision, which \ncan render the basic operations and predicates prohibitively slow or even uncomputable. Permission to \nmake digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. ICFP 01, September \n3-5, 2001, Florence, Italy. Copyright 2001 ACM 1-58113-415-0/01/0009 ...$5.00. Robert Harper School of \nComputer Science Carnegie Mellon University Pittsburgh, PA 15213-3891 rwh@cs.cmu.edu A practical and \nvery useful compromise, when applicable, is to assume that the input arguments are of .oating-point type. \nIt is also very common that the required functional\u00adity involves computation of only the sign of a given \npolyno\u00admial expression. Such calculations are, for example, used in the geometric predicates for determining \nwhether a point is in/out/on a given line, circle, plane, sphere... These predi\u00adcates are, in turn, fundamental \nbuilding blocks of algorithms for some basic geometric structures such as convex hulls and Delaunay triangulations. \nHowever, .oating-point alone is not su.cient to guarantee that the evaluation of a polynomial expression \nwill correctly obtain its sign. The rounding error accumulated during the computation, if su.ciently \nlarge, can perturb and change the .nal result. If this computation is part of a geometric algorithm, \nit can present the program with an inconsistent view of the data set, and cause it to produce incoherent \nresults, diverge, or even crash. On the other hand, under the stated assumptions, the exact sign can \nalways be com\u00adputed, albeit slowly, by .rst converting the .oating-point arguments into rational numbers, \nand then carrying out the prescribed operations in rational arithmetic. One method that has been proposed \nas an e.ciency im\u00adprovement to the exact rational arithmetic involves the use of .oating-point .lters. \nA .oating-point .lter carries out the given computation in .oating-point .rst, together with some sort \nof estimate of the rounding error, and falls back to exact arithmetic only if the estimated error is \ntoo big to reliably determine the sign [1,2, 6,3,9]. Thus,it .lters out the easy computations whose sign \ncan be quickly determined and only leaves the hard ones for the exact arithmetic. A particularly e.cient \nvariation of this approach has been de\u00adscribed by Jonathan Shewchuk in his PhD thesis [11]. Aside from \nperforming the .oating-point part of the computation as the .rst phase of the .lter, it introduces additional \n.l\u00adtration phases of ever-increasing precision. The phases are attempted in order, each phase building \non the result from the previous one, until the correct sign is obtained. There are two di.culties related \nto Shewchuk s method that this paper addresses: 1. Developing robust geometric predicates in this style \ncan be very cumbersome and error prone. For exam\u00adple, the basic InSphere predicate which tests whether \na point is in/out/on the sphere determined by three other points is represented by a simple 4 \u00d7 4matrix. \nHowever, Shewchuk s implementation of InSphere con\u00adsists of about 580 lines of C code. In addition, one \nneeds to perform the error analysis of the given poly\u00adnomial expression, which is also a tedious procedure. \nA solution is to automate this process by using an ex\u00adpression compiler [2, 5]. However, to the best \nof our knowledge, none of the existing expression compilers is capable of performing the analysis required \nby the multi-phase .oating-point .lters. 2. We are also interested in designing predicates for func\u00adtional \nlanguages and exploiting the common functional programming technique of staging to speed up the computation. \nFor example, consider .ltering a set of points to see on what side of a plane de.ned by three points \nthey lie. The test can be staged by .rst form\u00ading the plane and then checking the position of each point \nfrom the set. This obviates the need to repeat the part of the computation pertinent to the plane whenever \na new point is tested, and can potentially save a lot of work. Such staging of programs is natu\u00adrally \nexploited in functional programming languages, but unfortunately, the expression compilers available \nto date work only with C. This paper reports on an expression compiler that han\u00addles these shortcomings. \nThe input to the compiler is a function written in an appropriate source language o.er\u00ading the basic \narithmetic operations of addition, subtraction, multiplication and squaring, and allowing for nested \nanony\u00admous functional expressions (stages). All the operations in the source language are perceived as \nexact. The output of the compiler is a program in the target language designed to be easily converted \ninto Standard ML (SML) or, in the case of single-stage programs, to C. The resulting SML or C program \nwill determine the sign of the source function at the given .oating-point arguments, using a .oating-point \n.lter with several phases, when exact computation needs to be performed. In particular, in the case of \nShewchuk s ba\u00adsic geometric predicates, the expression compiler will gener\u00adate code that, to a considerable \nextent, reproduces that of Shewchuk. The rest of the paper is organized as follows. Section 2 summarizes \nthe main ideas behind .oating-point .lters and arbitrary precision .oating-point arithmetic. The source \nand target languages are presented in Section 3, and the pro\u00adgram transformation process is described \nin Section 4. Per\u00adformance comparison with Shewchuk s predicates is given in Section 5 and a de.nition \nof selected judgments governing the compilation follows in the Appendix. 2. BACKGROUND From here on \nwe assume .oating-point arithmetic as pre\u00adscribed by the IEEE standard and the to-nearest rounding mode \nwith the round-to-even tie-breaking rule [4]. We also assume that no over.ows or under.ows occur. One \nof the most important properties of a .oating-point arithmetic is the correct rounding of the basic arithmetic \noperations. It requires that the computed result always look as if it were .rst computed exactly, and \nthen rounded to the number of bits determined by the precision of the arithmetic. If x and y are .oating-point \nnumbers, ~is the rounded .oating-point version of the operation *.{+,-,\u00d7},and x~y is a .oating-point \nnumber with a normalized mantissa (i.e. is not a denormalized number), a consequence of the correct rounding \nis that |x*y-x~y|=E|x~y| and |x*y-x~y|=E|x*y| The quantity E in the above inequality is called machine \nepsilon . If m is the precision of the arithmetic, i.e. the number of bits reserved for the normalized \nmantissa (with\u00adout the hidden leading bit), then E=2-(m+1) . In the IEEE standard for double precision, \nfor example, E =2-53.By abuse of notation, the above inequalities are often stated respectively as x*y \n=(1 \u00b1E)(x~y)= x~y\u00b1E|x~y| (1) and x*y = x~y\u00b1E|x*y| The equation (1) provides a bound on absolute error \nof the expression when the expression consists of only a single .oating point operation. Notice that \nthe error is composed of two multiples, E and |x*y|, the .rst of which does not depend on the arguments \nx and y. The rounding error for a composite expression can also be split into two multiples, one of which \ndoes not depend on the arguments of the expres\u00adsion. This part of the error need not be computed in run\u00adtime \nwhen all the arguments of the expression are supplied, but can rather be completely obtained while preprocessing \nthe expression. To this end, assume that the exact values Xi are approximated in .oating-point as xi \nwith absolute error dipi, i.e. that for i =1,2we have Xi = xi \u00b1dipi Assume in addition that the quantities \ndi do not depend on any run-time arguments and that the invariant |xi|=pi holds. This is clearly true \nin the base case when xi is obtained from a single operation on exact arguments, as can be seen from \n(1) where di = E and pi = |xi|. The quantities di are rational numbers, and the values pi are .oating-point. \nDiverging slightly from the customary nomenclature, we call these two multiples respectively the relative \nerror and the permanent of the approximation xi. Using the inequalities for rounded .oating-point arith\u00admetic \nfrom above, we can derive |(X1 + X2) -(x1 .x2)| = = |(X1 + X2) -(x1 + x2)+(x1 + x2) -(x1 .x2)| =|(X1 \n+ X2) -(x1 + x2)|+ |(x1 + x2) -(x1 .x2)| = (d1p1 + d2p2)+ E|x1 .x2| = max(d1,d2)(p1 + p2)+ E(p1 .p2) \n= max(d1,d2)(1 + E)(p1 .p2)+ E(p1 .p2) = (E+max(d1,d2)(1 + E))(p1 .p2) The above inequality is, by abuse \nof notation, customarily written as () X1 + X2 = x1 .x2 \u00b1 E+max(d1,d2)(1 + E)(p1 .p2) The relative error \nof the composite expression X1 + X2 is then E +max(d1,d2)(1 + E) and its permanent is p1 .p2. Notice \nthat the relative error again does not depend on the run-time arguments, and that the invariant |x1 .x2|=p1 \n. p2 is preserved. Similar derivations produce X1 -X2 = x1 ex2 \u00b1(E+max(d1,d2)(1 + E))(p1 .p2) X1X2 = \nx1 .x2 () \u00b1 E+(d1 + d2 + d1d2)(1 + E)(p1 .p2) (2) () X12 = x1 .x1 \u00b1E+(2d1 + d12)(1 + E)(p1 .p1) The above \nformulas provide a quick test for the sign of Xi. Obviously, xi and Xi have the same sign if |xi|>dipi.How\u00adever, \nthis test is not completely satisfactory since it contains exact multiplication of a rational number \ndi and a .oating\u00adpoint number pi. A simpler, although less tight test is |xi|> i(1 + E)dil.pi (3) fp \nwhere iQlfp denotes the smallest .oating-point value above the rational number Q. This is indeed the \ninequality we use in our expression compiler to test the sign of an evaluated expression. Another important \nfeature of round-to-nearest arithmetic complying with the IEEE standard is that the roundo. error of \nthe basic operations is always representable as a .oating\u00adpoint number and can be recovered from the \nresult and the arguments of the operation. Theorem 1 (Knuth). In .oating-point arithmetic with precision \nm = 3,if x = a.b and c = xea then a+ b = x+((ae(xec)) .(bec)). Knuth s theorem is signi.cant since it \nprovides a way to quickly perform exact addition of two .oating-point num\u00adbers. First the addition x \n= a. b is performed approxi\u00admately, and then the roundo. error e=(ae(xec)).(bec)is recovered. This takes \nonly 6 .oating-point operations, which is generally much faster than .rst converting a and b into rational \nnumbers, and then adding them in rational arith\u00admetic. The two values (x,e) put together represent the \nexact sum of aand b. One can view this pair as a sparse represen\u00adtation of the sum in a digit system \nwith a radix 2m+1.Clos\u00ading up the set of sparse representations under addition leads to a very e.cient \ndata structure for exact computation. The values of this data type are lists of .oating-point numbers \nsorted by magnitude and satisfying certain technical condi\u00adtions about the alignment of their mantissas. \nThese lists are called expansions, and each expansion represents the exact sum of its elements. For the \nsake of illustration, here we only picture the process of adding a .oating-point number to an expansion \n(Figure 1) and of summing up two expan\u00adsions (Figure 2). Quick algorithms for other basic arithmetic \noperations on this data type have been devised as well [8, 11]. Another consequence of Knuth s theorem \nis a convenient ordering of operations that makes it possible to separate the computation into a sequence \nof .ltering phases. Each phase is attempted after the previous one had failed to determine the sign reliably, \nand each computes with increasing preci\u00adsion, building on the result of the previous one. The following \nexample, although admittedly a bit con\u00adtrived, is illustrative. Consider the expression X =(ax - bx)2 \n+(ay - by )2 where ax, ay, bx and by are .oating\u00adpoint values. To .nd the sign of X,let vx = ax ebx and \nvy = ay eby,and let ex and ey be the roundo.s from the ee e e 12 3 4 sum b hh h h h5 12 3 4 Figure 1: \nAdding a .oating-point number to an expansion. The .oat b is added to the expansion e1+e2 +e3+e4, to \nproduce a new expansion h1 +\u00b7\u00b7\u00b7+h5. gg g g 12 3 4g5 sum hh h 1 2h3 4h5 Figure 2: Summing two expansions. \nThe compo\u00adnents of e1 + e2 + e3 and f1 + f2 are .rst merged by decreasing magnitude into the list [g1,... \n,g5],which is then normalized into an expansion h1 + \u00b7\u00b7\u00b7+ h5. two subtractions. Then X =(vx + ex)2 +(vy \n+ ey)2 22 22 =(v + v )+ (2vxey +2vyex)+(e + e ) xyxy In this sum, the summand vx 2 + vy 2 is dominant, \nsince |ex|=E|vx| and |ey|= E|vy| by (1). It is then a good heuristic to .rst compute vx 2 + vy 2 and \ntest it for sign before proceed\u00ading, because it is likely that vx 2 + vy 2 will already have the same \nsign as X. The process can be sped up even more if this expression is .rst computed approximately to \nobtain XA =(vx .vx) .(vy .vy). Then only if XA has too big an error bound, as determined by the test \n(3), the compu\u00adtation of XB = vx 2 + vy 2 is undertaken exactly using the data type of expansions. If \nXB is also too crude an approxima\u00adtion of X, we can correct it by adding up the smaller terms (2vxey \n+2vyex), .rst approximately, and then correctly. Fi\u00adnally, if all of these approximations fail to give \nan answer, we can compute the exact result by adding the last summand ex 2 + ey 2 to the expansion computed \nin the previous phase. Using this approach, we will compute the exact value only if absolutely necessary, \nand even then, the e.orts spent on previous phases will not be wasted, but will rather be reused to obtain \nthe exact result in an e.cient way. This idea to generalize .oating-point .lters into a hierar\u00adchy of \nadaptive precision .ltering phases is due to Shewchuk. While the number and type of adaptive phases, \nstrictly speaking, can vary with the expression, his experiments pointed to a scheme with four phases \nas the optimal in prac\u00adtice for the basic geometric predicates that he considered. We adopt this scheme \nand present its formalization in Sec\u00adtion 4. The arbitrary precision .oating-point arithmetic and the \ndata type of expansions is invented by Priest, and fur\u00adther optimized by Shewchuk. Detailed description \nand anal\u00adysis of adaptive precision arithmetic and of the algorithms for basic operations can be found \nin their respective PhD theses ([8] and [11]). The whole method described above relies on the fact that \nphrases f ::= x| c| e expressions e ::= f1 + f2 | f1 -f2 | f1 \u00d7 f2 | ~f | sq f assignment lists a ::= \nval x = e | val x = ea programs p ::= fn [x1,... ,xn] => let a end | fn [x1,... ,xn]=>let ap end Figure \n3: Source language .. ax - cx bx - cx .. .. orient2(A,B,C)= sgn .. ay - cy by - cy fn [ax, ay, cx, cy] \n=> letvalacx= ax -cx val acy=ay-cy fn [ bx,by ] => letval d = acx \u00d7 (by-cy)\u00adacy \u00d7 (bx-cx) end end Figure \n4: Orient2D predicate: de.nition and imple\u00admentation in the source language. the required .oating-point \noperations will execute without any exceptions, i.e. that neither over.ow nor under.ow will occur during \nthe computation. If exceptions do happen, the expansions holding the exact intermediate values may lose \nbits of precision and produce a distorted answer. A possible solution in such cases is to rerun the computation \nin some other, slower, form of exact arithmetic (for example in in.nite precision rational numbers). \n 3. SOURCE AND TARGET LANGUAGES The source language of the compiler is shown in Figure 3. Its syntax \nsupports the basic arithmetic operations (includ\u00ading squaring), assignments and staged functional expres\u00adsions. \nThe arguments of the functions should be perceived as .oating-point values, while the intermediate results \nare as\u00adsumed to be computed exactly. Squaring is included among the arithmetic operations because it \ncan often be executed quicker than the multiplication of two equal exact values, and has a better error \nbound. In addition, it provides the compiler with the knowledge that its result is non-negative, which \ncan be used in some cases to optimize the code. In order to simplify the compilation process, the source \nlan\u00adguage requires that all the assignments are non-trivial, i.e. it disallows assignments of variables \nor constants. A func\u00adtion de.ned in the source language is designed to compute the sign of the last expression \nin the assignment list of the function s last stage. Of course, a staged source function can be partially \ninstantiated with an appropriate subset of the arguments in order to return a new function that encodes \nthe rest of the computation. The source language does not have any syntactic constructs for the sgn function, \nbut this function is always assumed at the last assignment of the last stage of the program. As an example, \nconsider the Orient2D geometric predi\u00adcate and its implementation in Figure 4. Orient2D deter\u00ad reals \nr ::= x | c | r1 * r2 | r1 ~r2 | sq r | ~r| abs r | double r | approx r | tail*(r1,r2,r3) | tailsq(r1,r2) \nassignment lists .::= val (x1,... ,xn) = lforce x. | val (x1,... ,xn) = rforce x. | val x = susp . in \n((x1,... ,xn), (x1,... ,xm)) end . | val x = r. | empty sign tests s ::= sign r | signtest (r1 \u00b1 r2) \nwith . in s end functions .::= fn (x1,... ,xn)=> let . in s end | fn (x1,... ,xn)=> let . in . end Figure \n5: Target language. mines the position (in/out/on) of point B =(bx,by )with respect to the line from \nA =(ax,ay)to C =(cx,cy). The implementation in Figure 2 is staged in the coordinates of A and C. Once \nthe predicate is applied to these two points, its result is a new function specialized to compute relative \nto the line AC, without recomputing the intermediate results acx and acy. The target language of the \ncompilation is presented in Figure 5. It is designed to be easily converted to SML, so its semantics \nis best explained by referring to SML. In the syntactic category of reals, the symbol * varies over the \nop\u00aderations {+,-,\u00d7}. The values of the syntactic category of reals are translated either into .oating-point \nnumbers, or ex\u00adpansions. Each of the operations in the target language has a very de.nite notion of which \nof the two types it expects (and .oats are considered subtypes of expansions). How\u00adever, we chose not \nto make this distinction explicit and did not introduce separate types for .oats and expansions in the \ntarget language. The reason is that we do not plan to do any programming in this language directly, but \nrather just use it for intermediate representation of programs before they are converted into SML or \nC. The target language operations ., e and . are inter\u00adpreted as corresponding .oating-point operations. \nThey ex\u00adpect .oating-point input, and produce .oating-point out\u00adput. The exact operations +, - and \u00d7 \nare translated into the appropriate exact operations on the data type of expan\u00adsions. Constants are always \n.oating-point values. The tail constructs compute the roundo.s from their correspond\u00ading .oating-point \noperation. For example, tail+ (a,b,a. b) will compute the roundo. from the addition a. b following Knuth \ns theorem. The construct double is multiplication by 2 on expansions, and approx returns a .oating-point \nnumber approximating the passed expansion with a relative error of 2E. To describe the role of susp, \nlforce and rforce con\u00adstructs, we need to make a clear distinction between stages and phases of computation \nin the target language. The source program contains nested functional expression which we refer to as \nstages. Once it is compiled into the target lan\u00adguage, every stage gets transformed into a stage of the \ntar\u00adget language, which consists of four computational phases of increased precision. The .rst phase \ncarries out the computa\u00ad STAGE 0 STAGE 1 Phase C previous phases previous phases Phase C V = rforce \nX CC . . . Phase D (exact phase) V = rforce X DD . . . Figure 6: Passing intermediate results between \nphases and stages using susp, lforce and rforce. tion in .oating-point, and the other phases mix in elements \nof exact computation as hinted in the previous section. The computation of these other phases have to \nbe suspended, since their results are needed only when the .oating-point calculations carried out by \nthe .rst phase of the .nal stage failed to determine the sign. Thus, the notion of stages refers to partial \nevaluation of code, while the notion of phases refers to lazy evaluation of code. Going back to the target \nlanguage, susp creates a piece of code, a suspension, to be evaluated when requested by rforce or lforce. \nIt gives a mechanism to pass intermedi\u00adate results between di.erent stages, and between di.erent phases \nof the same stage. The output from a suspension contains two tuples of intermediate values. The .rst \ntuple consists of intermediate values to be passed to some later phase of the current stage, and the \nsecond tuple consists of intermediate values intended for the following stage. The .rst tuple can be \nrecovered by lforce-ing the suspension, and the second tuple by rforce-ing it (see Figure 6). The sign \nfunction returns a sign of an expansion. The construct signtest .rst checks whether the magnitude |r1|of \nthe tested value is bigger than the magnitude |r2| of the roundo. error. If so, it returns the sign of \nr1.Otherwise, it cannot determine the sign of r1 with certainty, so it under\u00adtakes the computation of \nthe next phase ., followed by sign test s.Values r1 and r2 are assumed to be .oating-point.  4. COMPILATION \nTo describe the compilation process, .rst notice that the source program, according to the grammar of \nthe source lan\u00adguage (Figure 3), can be viewed as a nonempty sequence of assignment lists, each representing \na single stage of compu\u00adtation. Each of these stages is separately compiled into four target phases which \nare meant to perform the computation of the stage with increasing precision, as described in Section \n2. At the end, these pieces of target code are pasted together in a target program, according to speci.c \ntemplates, so that sign checks are performed between subsequent phases, while respecting the staging \nspeci.ed in the source program. The whole process is formalized using .ve judgments four for compiling \nsource stages into their target counter\u00adparts, and one judgment to compose all the obtained tar\u00adget stages \nand phases together into a target program. This section describes the compilation process in more detail, \nexplains some decisions in designing the compilation judg\u00adments and illustrates representative rules \nof the judgments through several examples. Selected subset of rules is pre\u00adsented in the appendix. The \ncomplete de.nition of the judg\u00adments can be found in [7]. Before proceeding further, there is one technicality \nto no\u00adtice. Namely, it can be assumed, without loss of generality, that the source program to be compiled \nis in a very speci.c format. First, we require that all of its assignments consist of a single binary \noperation acting on two other variables (rather then on two arbitrary source expressions), or of a single \nunary operation acting on another variable. The sec\u00adond requirement is that the source program does not \ncontain any .oating-point constants all the constants are replaced by fresh variables for error analysis \npurposes, and then put back into the target code at the end of the compilation. It is trivial to transform \nthe source program so that it complies with these two prerequisites, so we do not present the formalization \nof this procedure. In our implementation it is carried out in parallel with the parsing of the source \nprogram. In the following section, we illustrate the various phases of the process with the compilation \nof the source expression F = fn [a,b] => letvalab= a-b val ab2= sq ab fn [c] => letvald= c \u00d7 ab2 end \nend 4.1 Compiling the stages The .rst phase, phase A, of the predicate performs all the source operations \napproximately in .oating-point. The ex\u00adpression compiler determines the appropriate error bounds for \nthe generated code following the equations (2). As can be noticed from these formulas, the relative error \nis a ra\u00adtional quantity that depends solely on the structure of the source program, while its permanent \ndepends on the input arguments as well, and hence must be computed at run\u00adtime. The job of the expression \ncompiler is to determine the relative error of the expression, and insert code into the target program \nthat will compute the permanent and per\u00adform checks to see whether the obtained results correctly determine \nthe sign of the source expression. The rest of this section presents a formalization of the er\u00adror analysis \nand program transformation mentioned above. In order to describe the compilation for the phase A, we \nrely on the judgment E1 A a .; r1,r2E2 This judgment relates a list a of source assignments to the list \n.of corresponding phase A target assignments. Expres\u00adsions r1 and r2 are from the syntactic category \nof reals in the target language (Figure 5). The expression r1 isto be tested for sign at the end of the \nphase, and the expression r2 is an upper bound on the roundo. error. The assignments source code target \ncode context phase A A ebA val ab = a -b val abA = a abA : OA(E) abP abs(abA) val ab2 = sq ab val ab2A \n= abA .abA ab2A : OA(3E +3E2 + E3) phase B val ab = a -b abB : OB (E) abB abA val ab2 = sq ab val ab2B \n= sq abA ab2B : OB(2E +3E2 + E3) phase C A , bA val ab = a -b val abC = tail-(a , abA) abC : OC (0,E, \n0) A , 2E 1+ val ab2 = sq ab val ab2C = double(ab.abC ) ab2C : OC (3E2 +3E31- ,E) phase D val ab = a \n-b abD abC val ab2 = sq ab val ab2D = double(abA \u00d7abC )+ sq(abC ) Table 1: Compilation of the .rst stage \nof F . target code testing value error estimate context phase A val dA = c A .ab2A dA i(1+ )2(3 +3 2+ \n3) 1- lfp .abs(dA) dA : OA(4E +6E2 +4E3 + E4) dP abs(dA) phase B val dB = c A \u00d7ab2B approx(dB ) i(1+ \n)2(2 +3 2+ 3) 1-2 lfp .abs(dA) dB = OB (2E +5E2 +4E3 + E4) phase C val dC = c A .ab2C dC i5 2+12 3+6 \n4-4 5-3 6 (1- )2 lfp .abs(dA) dC : OC ( 5 2+2 3-3 4 1- , 2E( 1+ 1- )2 , 2E + E2) phase D val dD = c A \n\u00d7ab2D dB + dD Table 2: Compilation of the second stage of F . The source code for this stage consists \nof the single assignment val d = c \u00d7ab2. in . will perform the phase A calculations and compute the appropriate \npermanent. The contexts E1 and E2 deserve special attention. They are sets relating target language variables \nwith their error estimates. The grammar for their generation is presented below. contexts E ::= \u00b7|x : \nt, E |x r, E errors t ::= OA(d) |OB (d) |OC (d, ., .) |OD |P Each variable in a context is bound in one \nof the four phases of the computation (A, B, C or D), and will have error es\u00adtimates that are appropriate \nfor that phase of the compu\u00adtation (OA(d), OB (d), OC (d, .,.)and OD), where d, . and . are rational \nnumbers (Figure 7). For example, if the er\u00adror relation x : OA(d) . E, that means that the variable x \nwhich is bound in phase A, has been estimated by the compiler to have a relative error bounded from above \nby the rational number d. Similar meaning can be ascribed to the error assignments x : OB(d) for phase \nB. Phase C, on the other hand, is a mix of approximate and exact computa\u00adtions, and there are three rational \nvalues d, . and . that govern phase C error estimations. We don t describe their meaning in this paper, \nas it would take too much space, but selected formulas for their derivation can be found in the appendix \nand the complete de.nition is in [7]. Phase D is the exact phase, so there are no error estimates to \nassociate with phase D variables. Finally, the temporary variables in\u00adtroduced to hold parts of the permanent \nare not analyzed for error. We still place them into the error contexts, just for clarity, but with the \nerror tag P. To reduce clutter, the error estimate of a variable x in a context E will be denoted simply \nas E(x), as it will always be clear from the rule in which phase the variable is bound. In addition to \nthe error estimates, the contexts contain substitutions of variables by target language real expressions \n(x r). If some compila\u00adtion rule needs to emit into target code a variable for which there is a substitution \nin the context, the substituting ex\u00adpression will be emitted instead. This serves two purposes. First, \nwe can use it to express that certain variables in the code are just placeholders for .oating-point constants \na situation occurring, as explained before, because of an as\u00adsumed stricter form of the source programs. \nSecond, it lets us optimize, in a single pass of the compiler, the code for computing the permanent of \nthe expression -a process that will be illustrated below. Now that we have lain out the structure of \nthe contexts E1 and E2 in the judgment we are de.ning, we can describe their purpose. Simply, the compilation \nwith the judgment starts with the context E1, and ends with E2.So, E2 is in fact E1 enlarged with the \nnew variables, error estimates and substitutions introduced during the compilation. The context E2 is \nreturned so that it can be threaded into other rules. Going back to the analysis for the expression F \n,given on the previous page, we illustrate how its phase A can be compiled using the above judgment. \nFirst of all, the expression F is speci.ed as two stages: the one executing val ab = a - b and val ab2 \n= sq ab, and the other one ex\u00adecuting the assignment val d = c \u00d7 ab2. Each of the stages will be compiled \ninto four phases of assignments. The com\u00adpilation for phase A starts by breaking down each stage of the \nsource program into individual assignments. The rule is the following. E'E1 A valx = e.H ; s1,s2 E. A \na.T ; r1,r2 E2 E1 A valx = ea .H .T ; r1,r2 E2 The rule folds the functionality of the compiler across \nthe list of source assignments, carrying the context from one assignment to the next. Notice that the \nexpressions s1 and s2 are never used only the last expression in the assignment list is ever tested for \nsign. Now, to compile the assignment val ab = a - b, we need a rule applicable to subtraction of input \narguments. Input arguments are assumed to be error\u00adless, so the following rule applies. AA E(x1 )= E(x2 \n)=0 A AA E A val y = x1 -x2 val y = x1 e x2 ; A PPA y,0 E,yA : OA(E),y : P,y abs(y ) When applied to \nthe assignment val ab = a - b,the meta variables y, x1 and x2 are instantiated to ab, a and b respec\u00adtively. \nThe rule then emits the target code for the assign\u00adment to abA (the superscripts A indicate that the \ntarget variable is bound in the phase A of the predicate). For the purpose of bookkeeping, the rule must \nalso extend the context with information about the relative error and the permanent of abA . The relative \nerror of abA is E,so the rule generates the error estimate abA : OA(E). Finally, the per\u00admanent abP of \nabA is equal to |abA|, and the substitution context is extended with abP abs(abA). Next in the assignment \nlist from our example is the assign\u00adment val ab2 = sq ab. The squaring operation is handled by the rule \nPAP A xx or x abs(x1 ) . E 11 1 A AA E A val y = sq(x1) val y = x1 . x1 ; 22 A (1+)(2d1+d1) A y,i 1- \nlfp . y E,yA : OA(E+(1 + E)(2d1 + d12)), P PA y : P,y y In the assignment to ab2,the meta variables x1 \nand y of this rule are instantiated to ab and ab2 respectively. Then the meta variable x P 1 becomes \nabP .But abP has already been introduced into the context with a substitution abP abs(abA). Thus, the \npremises of this rule are satis.ed, and it can be applied. The meta variable d1 from the rule refers \nA to the relative error of the variable x1 as read from the con- A text, i.e. OA(d1)= E(x1 ). In our \nexample of assignment A to ab2,the variable x1 is instantiated to abA and d1 is in\u00adstantiated to E. The \nproduced permanent for ab2A is ab2A itself, explaining why we avoided emitting any code for per\u00admanent \ncomputation so far. Any separate computation of the permanent for ab2 would have been just a waste of \nef\u00adfort, since it is already computed by the main thread of the .lter. For future use, however, this \nrule stores the substi\u00adtution ab2P ab2A into the context. The relative error for ab2 is computed as E+(1 \n+ E)(2d1 + d12)=(3E+3E2 + E3) andis storedinto the context. That .nishes the compilation of phase A of \nthe .rst stage. The next stage contains the assignment val d = c \u00d7 ab2, and its phase A target code is \nobtained by the rule A PAP A E(x1 )=0 x2 x2 or x2 abs(x2 ) . E A AA E A val y = x1 \u00d7 x2 val y = x1 . \nx2 ; A ,i (1+ )2d2 A y 1- lfp . abs(y ) E,yA : OA(E+(1 + E)d2),y P : P, y P abs(y A) The rule compiles \nthe source assignment into val dA = c A . ab2A and expands the current context with the error es\u00adtimate \ndA : OA(4E +6E2 +4E3 + E4) and the substitution dP abs(dA). The remaining phases for F are obtained in \na similar way, and the reader is refered to the Appendix for a selected set of rules for the described \njudgments. The complete de.nition of the rules can be found in [7]. The steps in the derivation for the \ntwo stages, including the changes in the judgment contexts, are presented in Table 1 and Table 2 respectively. \nIn addition to the target code and the contexts, Table 2 also shows, for each of the phases, the testing \nvalue and error estimate (recall that only the testing value and the error es\u00adtimate of the last stage \nare actually emitted into the target code). As can be seen from Table 2, the testing values for the four \nphases of the second stage are dA , approx(dB), dC and dB +dD, respectively. In the .rst three phases, \nthese will be checked against the rounding errors to determine if they have the correct sign. In phase \nD, the testing value is actu\u00adally the exact value of the expression. The error estimates for the second \nstage are obtained from the corresponding rounding errors using (3), producing a quick .oating-point \ntest for the sign of the testing value. The error estimates are represented in the table in a symbolic \nform. It is important to notice that all of them are known in compile time, and are emitted into target \ncode as .oating-point constants1 . 2 i (1+ )2(3 3 + 3) So, for example, 1- lfp = 3.33067e-16 and i (1+ \n)2(2 +3 2+ 3) 1-2 lfp = 2.22045e-16.  4.2 Compiling the program Once all the stages of the source program \nhave been com\u00adpiled, they need to be pasted together into a target program, but in such a way that the \nphases can communicate their intermediate results. For illustration, the target code result\u00ading from \nthe compilation of the expression F is presented in Figure 7. The translation is done through a new judgment \nE1 P p; xB,xC,xD .VB ,VC ,VD which takes a source program p and compiles it into a target program .. \nThis judgment works in a bottom-up manner the later stages are pasted in .rst (recall that a source \npro\u00adgram is a list of stages; the judgment .rst processes the tail of the list, and then pastes in the \nhead stages). Thus, it is possible that the target program . will not have all of its variables bound \nsome of them might have been introduced in one of the previous stages, and thus will be compiled and \nbound by the judgment only later. The meta variables xB, xC and xD hold object-code variables, freshly \nallocated in the previous stage to hold that stage s suspensions, and then passed to the current stage \nto be rforce d if needed. The 1In the actual SML and C implementations, these values are calculated in \nan initialization routine, rather than placed in the code as decimal constants, in order to avoid rounding \nerrors in the decimal-to-binary conversion. fn [aA ,bA]=> let val abA =a A e bA val ab2A =abA . abA val \nyB = susp val ab2B =sqabA in ((), ab2) end val yC = susp val abC = tail-(aA,bA,abA) val ab2C = double(abA \n. abC) in ((abC), (ab2C)) end val yD = susp val (abC) = lforce yC val ab2D = double(abA \u00d7 abC) +sq abC \nin ((), ab2D) end in fn [cA]=> letval =c A . ab2A dA in signtest (dA \u00b1 (3.33067e-16 . abs(dA))) with \nval (ab2B) = rforce yB dBA val =c \u00d7 ab2B val y BX = approx(dB) in signtest (y BX \u00b1 (2.22045e-16 . abs(dA))) \nwith val (ab2C) = rforce (yC) val =c A . ab2C dC in signtest BX . (dC \u00b1 (2.22045e-16 . y 6.16298e-32 \n. abs(dA))) with val (ab2D) = rforce yD val =c A \u00d7 ab2D dD in sign(dB + dD) end end end end end Figure \n7: Target code for the example expression F. variables VB, VC and VD hold the object-code variables that \nthe mentioned suspensions should populate with intermedi\u00adate values. They are passed back to the previous \nstage so that the stage can be correctly constructed. To determine which object-code variables will be \npassed via suspensions to a particular phase, we use the following function. fv(.,S)=(S. free variables \nof .) \\ bound variables of . For example, if .D is the assignment list for the exact phase (phase D) \nof the last stage in a program p,its free variables will be VD =fv(.D,\u00d8). Some of these free variables \nwill be bound in the .A, .B or .C list of the same stage, but some will have to be passed by a suspension \nfrom the exact phase of the previous stage (see Figure 6). The variables to be placed in this suspension \nare therefore all characterized by the fact that they are introduced in the exact phase of some previous \nstage. Thus, their set is VD n domD E,where domD E is the set of variables from the context E that have \nphase D error estimates. We can similarly determine the suspensions for the phase C of the last stage. \nSince phase C needs to bind some of the variables from .D, we don t just consider the free variables \nof .C , but rather set VC =fv(.C,VD). As before, some of these variables will be bound in .A and .B, \nbut those that are not will need to be passed via suspension from the phase C of the preceding stage. \nThese variables are in the set VC n domC E, where domC E is, analogously to the phase D case, the set \nof object-code variables from context E bound in some of the previous C phases. In a similar way, the \nphase B will request the set VB n domB E where VB =fv(.B,VC) passed as a suspension from phase B of the \npreceding stage. Finally, phase A doesn t require any variable passing, since the computations of this \nphase are always carried out immediately in each stage, and are never suspended. The above discussion \nmotivates the following rule of the P judgment. The rule applies only if p is a single-stage program, \nand since the judgment is recursively applied, it serves to compile the last stage of the source program. \nE,xAi : OA(0) A a.A; r1 A ,r2 A E1 E1 B a.B; r1 B ,r2 B E2 E2 C a.C; r1 C ,r2 C E3 E3 D a.D; r1 D E4 \nE P fn [x1,... ,xn] => let a end; xB,xC,xD F VB n domB E,VC n domC E,VD n domD E where F is de.ned as \nfn [x1,...,xn]=> let .A in signtest (r1 A \u00b1 r2 A) with val (VB n domB E) = rforce(xB) .B val y BX = \nr B 1 BX \u00b1 r B in signtest (y 2 ) with val (VC n domC E) = rforce(xC) .C lm 2 C 2 (1+ )BX . r C in signtest \n(r1 \u00b1 1- . y 2 ) fp with val (VD n domD E) = rforce(xD) .D in sign (r1 D) end end end end Similar analysis \nof variable passing can be performed if the stage considered is not the last one. Then one only needs \nto take into account that some object-code variables might be requested from the subsequent stage, and \nfactor them in when creating the suspensions. The rule that handles this case is E,xiA : O(0) A a.A; \nr1 A ,r2 A E1 E1 B a.B; r1 B ,r2 B E2 E2 C a.C; r1 C ,r2 C E3 E3 D a.D; r1 D E4 E4 P p; yB,yC,yD .UB,UC,UD \nE P fn [x1,... ,xn]=>let ap end; xB,xC,xD F VB n domB E,VC n domC E,VD n domD E where VD =fv(.D ,UD), \nVC =fv(.C ,UC . VD), VB = fv(.B,UB . VC ), and the program F is de.ned as follows. fn [x1,... ,xn]=> \nlet .A val yB = susp val (VB n domB E) = rforce xB .B in (VD n domB E, UB ) end val yC = susp val (VC \nn domC E) = rforce xC .C in (VD n domC E, UC ) end val yD = susp val (VD n domD E) = rforce xD val (VD \nn domB E) = lforce yB val (VD n domC E) = lforce yC .D in ((), UD) end in . end Finally, if p is a source \nprogram, then as described before, it can be assumed that all its assignment expressions consist of a \nsingle operation acting only on variables,and that its constants ci are replaced by free variables yi.The \ntarget program . for p is obtained through the judgment after all these new variables are placed into \ncontext with relative error 0 together with their substitutions with constants. E,yiA : OA(0),yiA ciP \np; xB,xC ,xD .VB ,VC ,VD Notice how the pieces of target code shown in Tables 1 and 2, which represent \nvarious stages and phases of computation, are pasted together into the target program from Figure 7. \nFor clarity, the empty suspensions and forcings have been deleted from this target program.  5. PERFORMANCE \nWe have already mentioned that our automatically gen\u00aderated code for 2-and 3-dimensional Orient, InCircle \nand InSphere predicates to a large extent resembles that of Shew\u00adchuk [11]. Of course, this similarity \nis hard to quantify, if for no other reason than because our predicates are gen\u00aderated in our target \nlanguage, while Shewchuk s predicates are in C. Nevertheless, we wanted to measure the extent to which \nthe logical and mathematical di.erences in the code in.uence the e.ciency of our predicates. For that \npur\u00adpose we translated (automatically) the generated predicates from target language into C and compared \nthe translations against Shewchuk s C implementations. The .rst test con\u00adsisted of running the compared \npredicates on a common set of input entries. Each set had 1000 entries, and each entry was a list of \npoint coordinates, in cardinality and dimension appropriate for the particular predicate. The coordinates \nof the points were drawn with a uniform random distribu\u00adtion from the set of .oating-point numbers with \nexponents between -63 and 63. The summary of the results is rep\u00adresented in Table 3. As can be seen, \nour C predicates are of comparable speed with Shewchuk s, except in the case of InSphere where Shewchuk \ns hand-tuned version is about 2.4 times faster. The InSphere predicate is the most complex of all and \nit is only natural that it can bene.t the most from optimizations. Shewchuk s Automatically Ratio version \ngenerated version Orient2D 0.208 ms 0.249 ms 1.197 Orient3D 0.707 ms 0.772 ms 1.092 InCircle 6.440 ms \n5.600 ms 0.870 InSphere 16.430 ms 39.220 ms 2.387 Table 3: Performance comparison with Shewchuk s predicates. \nThe presented results are times for an average run of a predicate on random inputs. Shewchuk s Automatically \nRatio version generated version random 1187.1 ms 1410.3 ms 1.19 tilted grid 2060.4 ms 3677.5 ms 1.78 \nco-circular 1190.2 ms 1578.3 ms 1.33 Table 4: Performance comparison with Shewchuk s predicates for \n2d divide-and-conquer Delaunaytri\u00adangulation. In particular, one of the most visible di.erences between \nour InSphere predicate and Shewchuk s is the number of variables declared in the program. Our version \nof InSphere declares a new double array (which can be of considerable size) for every local variable \nin the target code intended to hold an exact value of an expansion type. However, a lot of this memory \ncan actually be reused, because only a minor portion of the exact values needs to be accessible through\u00adout \nthe run of the program. This will improve the cache management of the automatically generated programs \nand certainly increase their time e.ciency. However, it is im\u00adportant to notice that this problem is \nnot inherent to the automatically generated predicates, but is due to the naive translation from our \ntarget language into C. A better trans\u00adlator could probably decrease these di.erences considerably. For \nthe second test we modi.ed Triangle, Shewchuk s 2d Delaunay triangulator [10] to use automatically generated \npredicates. The testing included triangulations of three dif\u00adferent sets of 50,000 sample points: uniformly \nrandom in a unit square, tilted grid and uniformly random on a unit circle. The summary of the results \nis represented in Ta\u00adble 4. As can be seen, our predicates are a bit slower in the degenerate cases of \ntilted grid and co-circular points. Triangulation of such point-distributions often requires the higher \nphases of the .lter, which are better optimized in Shewchuk s hand-tuned versions. All the results are \nobtained on a Pentium II on 266 MHz and 96 MB of RAM. 6. FUTURE WORK The most immediate extensions of \nthe compiler should fo\u00adcus on exploiting the paradigm of staging even better. Stag\u00ading of expressions \nprevents recomputing already obtained intermediate results. However, each stage in the source pro\u00adgram \ntranslates into four phases of the target program, with four approximations of di.erent precision to \nthe given inter\u00admediate result. If a computation ever carries out its phase D it will obtain the exact \nvalue of this intermediate result, and could potentially use it to increase the accuracy of the approximations \nfrom the inexact phases. It would be in\u00adteresting and useful to devise a scheme that would exploit both \nthe adaptive precision arithmetic and the staging in this broader manner. A longer term goal could be \nto exploit the structure of the computation to obtain better error bounds. Priest has derived su.cient \nconditions which guarantee that the re\u00adsult from a certain .oating-point operation will actually be computed \nexactly, i.e. will not incur any roundo. error [8]. While putting this idea in practice will likely require \na non\u00adtrivial amount of theorem proving, it might still be feasible, since geometric predicates are typically \nshort expressions, and that the time for their compilation is not really crucial. Finally, one may wonder \nhow to extend the source lan\u00adguage with the standard programming constructs such as products, coproducts \nand functions. Adding functions for the sake of structuring the code will most likely require that every \nsingle intermediate variable in the program be replaced with a tuple containing that variable phase A \nvalue and a suspension for the other three phases. This is required since now functions in the language \ncan test signs of arbi\u00adtrary values, even those produced by other functions, so the values have to be \nequipped with means to compute them\u00adselves exactly. But this is likely to be too slow, defeating the \nwhole purpose of the expression compiler. On the other hand, adding recursive functions is even less \nrealistic. Per\u00adforming error analysis for recursive functions is hard it is one of the main goals of \nthe whole mathematical .eld of numerical analysis. Therefore, it seems to be more useful to just add \ncoproducts, since products lose much of their purpose if functions are not around. 7. CONCLUSION This \npaper has presented an expression compiler for au\u00adtomatic generation of functions for testing the sign \nof a given arithmetic expression over .oating-point constants and variables. In addition to the basic \noperations of addition, subtraction, multiplication, squaring and negation, our ex\u00adpressions can contain \nanonymous functions and thus exploit the optimization technique of staging, that is well-known in functional \nprogramming. The output of the compiler is a target program in a suitably designed intermediate lan\u00adguage, \nwhich can be easily converted to SML or, in case of single-stage programs, to C. Our method is an extension \nto arbitrary expressions of the idea of Shewchuk [11], which he employed to develop quick robust predicates \nfor the Orient and InSphere geo\u00admetric test. In particular, when applied to source expres\u00adsions for these \ngeometric predicates, our compiler generates code that, to a large extent, resembles that of Shewchuk. \nThe idea behind the approach is to split the computation into several phases of increasing precision \n(but decreasing speed), each of which builds upon the result of the previous phase, while using forward \nerror analysis to achieve reliable sign tests. There remain, however, two caveats when generating pred\u00adicates \nwith this general approach the produced code works correctly (1) only if no over.ow or under.ow happen, \nand (2) only in round-to-nearest, tie-to-even .oating-point arith\u00admetic complying with the IEEE standard. \nIf over.ow or under.ow happens in the course of the run of some predicate, the expansions holding exact \nintermedi\u00adate results may lose bits of information and distort the .nal outcome. Thus, we need to recognize \nsuch situations and, in those supposedly rare cases, rerun the computation in another form of exact arithmetic \n(say in in.nite precision rational numbers). Unfortunately, even though the IEEE standard prescribes \n.ags that can be read to check for over\u00ad.ow and under.ow, the Standard Basis Library of ML does not provide \nany functions for their testing. As concerning the second requirement, the IEEE standard is implemented \non most modern processors. Unfortunately, on the Intel x86 family this is not a default setup. This family \nuses internal .oating-point registers that are larger than 64-bits reserved for values of .oating-point \ntype. This property can occasionally make them round incorrectly in the to-nearest mode (for an example, \nsee [8] page 103) and thus destroys the soundness of the language semantics. This default can be changed \nby setting a processor .ag, but again, the Standard Basis Library does not provide any means for it. \nWe believe that these two described insu.ciencies can easily be remedied, and should be if SML is to \nbecome a language with serious applications in numerical analysis and scienti.c computing. 8. REFERENCES \n[1] M. O. Benouamer, P. Jaillon, D. Michelucci, and J. M. Moreau. A lazy exact arithmetic. In E. E. Swartzlander, \n M. J. Irwin, and J. Jullien, editors, Proceedings of the 11th IEEE Symposium on Computer Arithmetic, \npages 242 249, Windsor, Canada, June 1993. IEEE Computer Society Press, Los Alamitos, CA. [2] S. Fortune \nand C. J. V. Wyk. E.cient exact arithmetic for computational geometry. In Ninth Annual Symposium on Computational \nGeometry, pages 163 172. Association for Computing Machinery, May 1993. [3] S. Funke and K. Mehlhorn. \nLOOK a lazy object-oriented kernel for geometric computation. In Proceedings of the 16th Symposium on \nComputational Geometry, pages 156 165. ACM, June 2000. [4] IEEE. IEEE standard for binary .oating-point \narithmetic. ACM SIGPLAN Notices, 22(2):9 25, Feb. 1985. [5] K. Mehlhorn and S. N\u00a8aher. LEDA:A Platform \nfor Combinatorial and Geometric Computing. Cambridge University Press, 1999. [6] D. Michelucci and J. \nM. Moreau. Lazy arithmetic. IEEE Transactions on Computers, 46(9), September 1997. [7] A. Nanevski, \nG. Blelloch, and R. Harper. Automatic generation of staged geometric predicates. Technical Report CMU-CS-01-141, \nSchool of Computer Science, Carnegie Mellon University, June 2001. [8] D. M. Priest. On Properties of \nFloating Point Arithmetics: Numerical Stability and the Cost of Accurate Computations. PhD thesis, University \nof California at Berkeley, Berkeley, California, November 1992. [9] S. A. Seshia, G. E. Blelloch, and \nR. W. Harper. A performance comparison of interval arithmetic and error analysis in geometric predicates. \nTechnical Report CMU-CS-00-172, School of Computer Science, Carnegie Mellon University, December 2000. \n [10] J. R. Shewchuk. http://www.cs.cmu.edu/~quake/triangle.html. [11] J. R. Shewchuk. Delaunay Re.nement \nMesh Generation. PhD thesis, Carnegie Mellon University, 1997. APPENDIX A. COMPILATION RULES The expression \ncompiling is governed by .ve judgments. Four of them correspond to the four phases of adaptive computation. \nThey take lists of source language assignment in context and produce lists of target language assignment. \nThey also return a target .oating-point expression (an expression in the syntactic category of reals) \nto be tested for sign and a target .oating-point expression representing the upper bound on the relative \nerror (or a part of it in the case of phase C). The .fth judgment compiles the whole program by putting \ntogether all the pieces of target code obtained by the other judgments. It takes a source program and \nthree variables naming suspensions for B, C and D phases, and returns a target program plus lists of \nvariables to be bound in those suspensions, as described Section 4. In the following text, concatenation \nof lists of assignments is represented by their juxtaposition. The relative error of a variable x in \ncontext is E is refered to as E(x). Only selected rules of each judgement are presented. For the complete \nde.nition, the reader is refered to [7]. A.1 First phase Phase A of the compilation is handled by the \njudgment E1 fA / a .; r1,r2E2. We abbreviate d1 = E(xA)and d2 = E(xA) when the quantities on the right \nare de.ned. judgment followbelow. 1 The ru 2 les for the E1 fA valx= e .H; s1,s2/E' E' fA a .T ; r1,r2/E2 \n E1 fA valx= ea .H.T ; r1,r2 / E2 A.1.0.1 Addition. Denote errA(d1,d2)= E+(1 + E)max(d1,d2). + AA E(x1 \n)= E(x2 )=0 A AA E fA val y /= x1 + x2 val y= x1 . x2 ; AP y,0E,yA : OA(E),y: P,yP abs(yA) A E(x1 )=0 \nE fA val y = x1 + x2 A AA val y= x1 . x2 P AP val y= abs(x) . x2 ; 1 / A , 1+ PP yd2lfp . yE,yA : OA(E+ \nd2),y: P 1- A Symmetrically if E(x2 )= 0. PA PA x x. Ex x. E 11 22 A AA E fA val y = x1 + x2 val y= x. \nx 12 ; , (1+ )2 / AA ymax(d1,d2)lfp . y 1- PA E,yA : OA(errA(d1,d2)),y: P,yP y + E fA val y = x1 + x2 \n AAA PPP val y= x. xval y= x. x2 ; 12 1 , (1+ )2 / AP ymax(d1,d2)lfp . y 1- P E,yA : OA(errA(d1,d2)),y: \nP + A.1.0.2 Multiplication. Denote errA(d1,d2)= E+(1+ E)(d1 + d2 + d1d2). \u00d7 AA E(x1 )= E(x2 )=0 A AA \nE fA val y /= x1 \u00d7 x2 val y= x1 . x2 ; AP y,0E,yA : OA(E),y: P,yP abs(yA) A PAP A E(x)=0 x xor x abs(x) \n. E 1 2222 A AA E fA val y = x1 \u00d7 x2 val y= x. x; 12 , (1+ )2d2 / A ylfp . abs(yA) 1- P E,yA : OA(E+(1 \n+ E)d2),y: P, yP abs(yA) A E(x1 )=0 E fA val y = x1 \u00d7 x2 AAAP AP val y= x. xval y= abs(x) . x2 ; 12 1 \n2/ A (1+ )d2 P y, lfp . y 1- P E,yA : OA(E+(1 + E)d2),y: P A Symmetrically if E(x2 )= 0. PA PA x x. Ex \nx. E 11 22 A AA E fA val y = x1 \u00d7 x2 val y= x1 . x2 ; (1+ )2(d1+d2+d1d2) AA y, 1- lfp . y/ P PA E,yA \n: OA(errA(d1,d2)),y: P,y y \u00d7 PAP A x xor x abs(x) . E 1111 PAP A x xor x abs(x) . E 2222 A AA E fA val \ny = x1 \u00d7 x2 val y= x. x 12 ; 2/ A (1+)(d1+d2+d1d2) A y, 1- lfp . y PP E,yA : OA(errA(d1,d2)),y: P,y abs(yA) \n\u00d7 E fA val y = x1 \u00d7 x2 AAA PPP val y= x. xval y= x. x 12 12 ; (1+ )2(d1+d2+d1d2) 1- yA , P / lfp . y \nP E,yA : OA(errA(d1,d2)),y: P \u00d7  A.2 Second phase / The judgment handling phase B is E1 fB a .; r1,r2E2. \nBB As before, we denote d1 = E(x1 )and d2 = E(x2 ). / E1 fB valx = e .H; s1,s2E' / E' fB a .T ; r1,r2E2 \nE1 fB valx = ea .H .T ; r1,r2 / E2 A.2.0.3 Addition. Denote errB(d1,d2)= (1+ E)max(d1,d2). + E(xA 1 )= \nE(xA 2 )=0 /E fB val y = x1 + x2 empty;0,0 A E,yB : OB(E),yB yA E(x1 )=0 BAB E fB val y = x1 + x2 val \ny= x+ x; lm 12 / 1+ P approx(yB), . xE,yB : OB(d2) 1-2 d2 fp 2 A Similarly if E(x2 )= 0. BBB E fB val \ny = x1 + x2 val y= x+ x2 ; lm 1 1+ BP approx(yB), err(d1,d2) . y/ 1-2+ fp E,yB : OB (errB(d1,d2)) + \nA.2.0.4 Multiplication. Denote errB(d1,d2)= (1+ E)(d1 + d2 + d1d2). \u00d7 E(xA 1 )= E(xA 2 )=0 / E fB val \ny = x1 \u00d7 x2 empty;0,0 A E,yB : OB(E),yB y A E(x1 )=0 BA B E fB val y = x1 \u00d7 x2 val y= x\u00d7 x2 ; 1 / (1+ \n)2 P approx(yB), l d2 m . y 1-2 fp E,yB : OB ((1 + E)d2) A Similarly if E(x2 )= 0. BB B E fB val y = \nx1 \u00d7 x2 val y= x1 \u00d7 x2 ; / 1+ BP approx(yB), l err(d1,d2) m . y 1-2 \u00d7 fp E,yB : OB(errB (d1,d2)) \u00d7  \nA.3 Third phase / The judgment for phase C is E1 fB a.; r1,r2E2.The expression r2 is nowjust one summand \nin the bound on the ab\u00adsolute error. See the de.nition of the judgment fP for its use in the target program. \nNotational abbreviation for this section are CC .1 =(d1,.1,.1)= E(x1 )and .2 =(d2,.2,.2)= E(x2 )when \nCC the context E contains variables xand x 2 . 1 ' E1 fC valx= e.H; s1,s2/E ' E fC a.T ; r1,r2/E2 E1 \nfC valx= ea .H .T ; r1,r2/E2 A.3.0.5 Addition. To simplify the presentation, we introduce the following \nnota\u00adtion. err C ((d1,.1,.1),(d2,.2,.2)) + A =(d+C , 1+ E max(.1,.2),err (.1,.2)) 1 - E + where dC =(1+ \nE)(Emax(.1,.2)+ max(d1,d2)) + AA E(x1 )= E(x2 )=0 E fC val y = x1 + x2 / C AA val y= tail+(x1 ,x2 ,yA); \n0,0E,yC : OC(0,E,0) A E(x1 )=0 E fC val y = x1 + x2 empty; 2 / (1+ ) xC 2 , l d2 m . xP 1- 2 fp CC E,yC \n: OC (.2),yx 2 A Similarly for E(x2 )= 0. C CC E fC val y = x1 + x2 val y= x1 . x2 ; lm / C (1+ )2 dCP \ny, 1- + . y fp E,yC : OC(errC (.1,.2)) + A.3.0.6 Multiplication. Here, the error functions are as follows. \n1+ E err C0(d,.,.)=(E.+ d, .,E+(1+ E).) \u00d7 1 - E err C ((d1,.1,.1),(d2,.2,.2)) \u00d7 A =(d\u00d7C , 1+ E (.1 + \n.2),err (.1,.2)) 1 - 2E- E2 \u00d7where dC = h (2E+ E2)(.1 + .2)+ \u00d7 +(.1.2 + .1.2)+ d1(1 + .2 + .2)+ i + d2(1 \n+ .1 + .1)+ .1.2 + d1d2 (1 + E) AA E(x1 )= E(x2 )=0 E fC val y = x1 \u00d7 x2 / C AA val y= tail\u00d7(x1 ,x2 ,yA) \n.;0,0E,yC : OC(0,E,0) A E(x1 )=0 C AC E fC val y = x1 \u00d7 x2 val y= x. x 12 ; lm 2 / C (1+ )P y, 1- (E.2 \n+ d2) . y fp E,yC : OC (errC0(.2)) \u00d7 Similarly for E(xA)= 0. 2 E fC val y = x1 \u00d7 x2 C AC CA val y= (x. \nx) . (x. x); 12 12 2 / C (1+ )P y, l dC m . y 1-\u00d7 fp E,yC : OC(errC (.1,.2)) \u00d7  A.4 Fourth phase The \nphase D of the .lter is exact, so there is no need for error functions or estimates in the judgment. \nThus, the judgment has the form E1 fD a.; r/E2, and is de.ned below. '' E1 fD valx= e.H; s/EE fD a.T \n; r/E2 E1 fD valx= ea .H .T ; r/E2 A.4.0.7 Addition. AA E(x1 )= E(x2 )=0 / E fD val y = x1 + x2 a empty;0 \nDC E,yD : OD,yy A E(x1 )=0 / E val y = x1 + x2 empty; yB + xD fD 2 DD E,yD : OD,yx 2 A Similarly if E(x2 \n)= 0. E fD val y = x1 + x2 / DDD D val y= x+ x; yB + yE,yD : OD 12 A.4.0.8 Multiplication. AA E(x1 )= \nE(x2 )=0 / E fD val y = x1 \u00d7 x2 empty;0 DC E,yD : OD,yy A E(x1 )=0 E fD val y = x1 \u00d7 x2 / DAD D val y= \nx\u00d7 x; yB + yE,yD : OD 12 A Similarly if E(x2 )= 0. E fD val y = x1 \u00d7 x2 D BD DB DD val y=(x\u00d7 x)+(x\u00d7 x)+(x\u00d7 \nx); /12 1212 BD y+ yE,yD : OD   \n\t\t\t", "proc_id": "507635", "abstract": "Algorithms in Computational Geometry and Computer Aided Design are often developed for the Real RAM model of computation, which assumes exactness of all the input arguments and operations. In practice, however, the exactness imposes tremendous limitations on the algorithms --- even the basic operations become uncomputable, or prohibitively slow. When the computations of interest are limited to determining the sign of polynomial expressions over floating point numbers, faster approaches are available. One can evaluate the polynomial in floating-point first, together with some estimate of the rounding error, and fall back to exact arithmetic only if this error is too big to determine the sign reliably. A particularly efficient variation on this approach has been used by Shewchuk in his robust implementations of Orient and InSphere geometric predicates. We extend Shewchuk's method to arbitrary polynomial expressions. The expressions are given as programs in a suitable source language featuring basic arithmetic operations of addition, subtraction, multiplication and squaring, which are to be perceived by the programmer as exact. The source language also allows for anonymous functions, and thus enables the common functional programming technique of staging. The method is presented formally through several judgments that govern the compilation of the source expression into target code, which is then easily transformed into SML or, in case of single-stage expressions, into C.", "authors": [{"name": "Aleksandar Nanevski", "author_profile_id": "81100503327", "affiliation": "Carnegie Mellon Univ., Pittsburgh, PA", "person_id": "PP17010203", "email_address": "", "orcid_id": ""}, {"name": "Guy Blelloch", "author_profile_id": "81100282539", "affiliation": "Carnegie Mellon Univ., Pittsburgh, PA", "person_id": "PP15028138", "email_address": "", "orcid_id": ""}, {"name": "Robert Harper", "author_profile_id": "81100140064", "affiliation": "Carnegie Mellon Univ., Pittsburgh, PA", "person_id": "PP39029370", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/507635.507662", "year": "2001", "article_id": "507662", "conference": "ICFP", "title": "Automatic generation of staged geometric predicates", "url": "http://dl.acm.org/citation.cfm?id=507662"}