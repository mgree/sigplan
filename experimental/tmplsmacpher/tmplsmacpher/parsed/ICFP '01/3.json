{"article_publication_date": "10-01-2001", "fulltext": "\n Optimizing Pattern Matching Fabrice Le Fessant, Luc Maranget INRIA Roquencourt, B.P. 105, 78153 Le \nChesnay Cedex, France (Email: {Fabrice.Le fessant, Luc.Maranget}@inria.fr) ABSTRACT We present improvements \nto the backtracking technique of pattern-matching compilation. Several optimizations are in\u00adtroduced, \nsuch as commutation of patterns, use of exhaus\u00adtiveness information, and control .ow optimization through \nthe use of labeled static exceptions and context information. These optimizations have been integrated \nin the Objective-Caml compiler. They have shown good results in increasing the speed of pattern-matching \nintensive programs, without increasing .nal code size. 1. INTRODUCTION Pattern-matching is a key feature \nof functional languages. It allows to discriminate between the values of a deeply structured type, binding \nsubparts of the value to variables at the same time. ML users now routinely rely on their com\u00adpiler for \nsuch a task; they write complicated, nested, pat\u00adterns. And indeed, transforming high-level pattern-matching \ninto elementary tests is a compiler job. Moreover, because it considers the matching as a whole and that \nit knows some intimate details of runtime issues such as the representation of values, compiler code \nis often better than human code, both as regards compactness and e.ciency. There are two approaches to \npattern-matching compila\u00adtion, the underlying model being either decision trees [5] or backtracking automata \n[1]. Using decision trees, one pro\u00adduces apriori faster code (because each position in a term is tested \nat most once), while using backtracking automata, one produces apriori less code (because patterns never \nget copied, hence never get compiled more than once). The price paid in each case is losing the advantage \ngiven by the other technique. This paper mostly focuses on producing faster code in the backtracking \nframework. Examining the code generated by the Objective-Caml compiler [11], which basically used the \nAugustsson s original algorithm, on frequent pieces of code, such as a list-merge function, or on large \nexamples [14], we found that the backtracking scheme could still be improved. Permission to make digital \nor hard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro\u00a3t or commercial advantage and that copies bear this notice \nand the full citation on the \u00a3rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci\u00a3c permission and/or a fee. ICFP 01, September 3-5, 2001, Florence, Italy. \nCopyright 2001 ACM 1-58113-415-0/01/0009 ...$5.00. Our optimizations improve the produced backtracking \nau\u00adtomaton by grouping elementary tests more often, removing useless tests and avoiding the blind backtracking \nbehavior of previous schemes. To do so, the compiler uses new in\u00adformation and outputs a new construct. \nNew information include incompatibility between patterns, exhaustiveness in\u00adformation and contextual \ninformation at the time of back\u00adtracking. As to the new construct, previous schemes used a lone exit \nconstruct whose e.ect is to jump to the nearest enclosing trap-handler ; we enrich both exits and traps\u00adhandlers \nwith labels, resulting in .ner control of execution .ow. Our optimizations also apply to or-patterns, \na convenient feature to group clauses with identical actions. Unsharing of actions is avoided by using \nour labelled exit construct. As or-patterns may contain variables, the exit construct is also extended \nto take arguments. All our optimizations are now implemented in the latest version of the Objective-Caml \ncompiler, whose language of accepted patterns has been extended by allowing variables in or-patterns. \nThe structure of this article is the following: we .rst in\u00adtroduce some theoretical basics on pattern-matching \nin sec\u00adtion 2 and describe the compilation scheme to backtracking automata in section 3. Then, we brie.y \nintroduce our op\u00adtimizations and or-pattern compilation in an intuitive way in sections 4 and 5, while \nsection 6 is a formalization of our complete compilation scheme. Finally, some experimental results are \nshown in section 7, and a comparison with other approaches is discussed in section 8.  2. BASICS In \nthis section, we introduce some notations and de.ni\u00adtions. Most of the material here is folklore, save, \nperhaps, or-patterns. 2.1 Patterns and Values ML is a typed language, where new types of values can be \nintroduced using type de.nitions such as: typet=Nil|Oneofint |Consofint*t This de.nition introduces a \ntype t, with three constructors that build values of type t. These three constructors de.ne the complete \nsignature of type t. Every constructor has an arity, i.e. the number of arguments it takes. Here arity \nof Nil is zero, while the arities of One and Cons are one and two respectively. A constructor of arity \nzero is called a constant constructor, while other constructors are non\u00adconstant constructors. Most native \ndata types in ML such as integers, records, arrays, tuples can be seen as particular instances of such \ntype de.nitions. For example, in the following we will con\u00adsider lists (nil being the constant constructor \n[] and cons the in.x constructor ::), and tuples (the type of n-tuples de.nes one constructor of arity \nn, pairs being written with the in.x constructor , ). For our purpose, integers are constant constructors, \nand the signature of the integer type is in.nite. More formally, patterns and values are de.ned as follows: \np::= Patterns wildcard x variable c(p1,p2,...,pa) constructor pattern (p1 |p2) or-pattern v ::= Values \nc(v1,v2,...,va) constructor value In the following, we freely replace variables by wild-cards  when \ntheir names are irrelevant. While describing compila\u00adtion, convenient tools are vectors of values (vv \n=(v1 v2 ...vn) and vvn.m =(vn...vm)), vectors of patterns (vp=(p1 p2 ...pn) and vpn.m =(pn...pm)) and \nmatrices of patterns (P =(pji )). In this paper, we present pattern-matching compilation as a transformation \non an intermediate code in the compiler, called lambda-code. Here, another useful object is the clause \nmatrix (P .L): . . 11 1 l1 22 2 p1 p2 \u00b7\u00b7\u00b7 pn . . p1 p2 \u00b7\u00b7\u00b7 pn . l2 . . . (P .L)= . . . . . . . mm mm \np1 p2 \u00b7\u00b7\u00b7 pn . l i i i A clause matrix associates rows of patterns (p1 p2 ...pn)to lambda-code actions \nli .  2.2 Pattern Matching in ML A pattern can be seen as representing a set of values shar\u00adingacommonpre.x. \nDefinition 1 (Instance). Let p be a pattern and v be a value belonging to a common type. The value v \nis an instance of the pattern p or p matches v, written p -v when one of the following rules apply: \n-v x -v (p1 |p2) -v i. p1 -v or p2 -v c(p1,...,pa) -c(v1,...,va) i. (p1 ...pa) -(v1 ...va) (p1 ...pa) \n-(v1 ...va) i. pi -vi,.i.[1..a] Seeing a pattern as the set of its instances, it is clear that or-patterns \nexpress set union. In ML, patterns are a binding construct, more speci.cally, a successful match p -v, \nbinds the variables of p to some sub-terms of v. Such bindings can be computed while check\u00ading that p \nmatches v, provided that the following set V(p) of variables de.ned by p is well-de.ned: V()= \u00d8 V(x)= \n{x} V(c(p1,...,pa)) = V(p1 ...pa) V(p1 ...pa)= V(p1) .... .V(pa) if for all i = j,V(pi) nV(pj )= \u00d8 V(p1 \n|p2)= V(p1), if V(p1)= V(p2) The .rst if condition above is the linearity of patterns. The second condition \nis speci.c to or-patterns, it means that matching by either side of the or-pattern binds the same variables \n(additionally, homonymous variables should possess the same type). We then de.ne the now dominant, textual \npriority scheme to disambiguate the case when several rows in a matrix match: Definition 2 (Matching \npredicate). Let P be a pat\u00adtern matrix and vv =(v1 ...vn) be a value vector. The value v matches line \nnumber i in P, if and only if the following two conditions are satis.ed: (p1 i ...p i ) -(v1 ...v nn) \n.j<i, (p1 ...pn) n)jj -(v1 ...v We will not give a full semantics for evaluating pattern\u00admatching expressions, \nand more generally lambda-code. In\u00adtuitively, given a clause matrix P . L and a value vec\u00adtor vv such \nthat line number i in P matches vv, evaluating the matching of vv by P .L in some environment . is eval\u00aduating \nli in . extended by the bindings introduced while matching vv by (p i 1 ...p i ). If vv is not matched \nby any line in n P, we say that the pattern-matching P fails. Ifnosuch vv exists, then the pattern-matching \nis said exhaustive. Like pattern vectors, pattern matrices represent sets of value vectors. More speci.cally, \nwhen some line in P matches vv we simply say that P matches vv. This looks obvious, but representing \nsets using matrices is at the core of our opti\u00ad mizations. One easily checks that the instances of P \nare the union of the instances of the lines of P.That is, when considering a matrix globally, the second \ncondition in de.\u00ad nition 2 above is irrelevant. More important, row order is also irrelevant. Finally, \nthe instance relation induces relations on the pat\u00ad terns themselves. Definition 3 (Relations on patterns). \nWe de.ne the following three relations: 1. Pattern p is less precise then pattern q, written p -q, when \nall instances of q are instances of p. 2. Pattern p and q are equivalent, written p=q, when their instances \nare the same. 3. Patterns p and q are compatible when p and q share a common instance.  Here some remarks \nare to be made. Because of typing, checking the precision relation is neither obvious nor cheap. More \nprecisely, there is no simple way to decide whether p-holds or not. For instance, ([]| :: ) -holds, while \n(Nil|One ) -does not. Or-patterns are not responsi\u00adble for this complication, since we also have (,) \n-. In such cases one should expand p and consider, whether signatures are complete or not (see [13, \nSection 5.1]). By contrast, compatibility can be checked by a simple recur\u00adsive algorithm. When compatible, \npatterns p and q admit a least upper bound, written p.q, which can be computed while checking compatibility: \n. (p1 ...pa) .(q1 ...qa)=(p1 .q1 ...pa .qa) . . . . .q = q p. = p . . c(p1,...,pa) .c(q1,...,qa)= c(r1,...,ra) \n. . where (r1 ...ra)is(p1 ...pa) .(q1 ...qa) With the following additional rules for or-patterns: . . \np1 .q, when p2 and q not compatible (p1 |p2) .q = p2 .q, when p1 and q not compatible . (p1 .q|p2 .q), \notherwise p.(q1 |q2)=(q1 |q2) .p Proving that p.q is indeed the least upper bound of p and q is easy, \nby considering patterns as sets of their instances. Note that p.q is de.ned up to =-equivalence, and \nthat it encodes instance intersection.  3. COMPILATION In this section, we present a compilation scheme \nclose to the one described in [20, 1], and implemented in compilers such as the hbc compiler or the Objective \nCaml compiler. This classical scheme will be re.ned later into an optimized scheme, using same notations \nand concepts. 3.1 Output of the match compiler The compilation of pattern-matching is described by the \nscheme C that maps a clause matrix to a lambda-code expres\u00adsion. We now describe the speci.c lambda-code \nconstructs that the scheme C outputs while compiling patterns. Let-bindings: let (xlx) l, nested let-bindings \nare abbre\u00adviated as: let (x1 l1)(x2 l2) \u00b7\u00b7\u00b7(xn ln) l Static exceptions, exit and traps, catch l1 with \nl2.If, when evaluating the body l1, exit is encountered, then the result of evaluating catch l1 with \nl2 is ther result evaluating the handler l2, otherwise it is the result of eval\u00aduating l1. By contrast \nwith dynamic exceptions, static exceptions are directly compiled as jumps to the associ\u00adated handlers \n(plus some environment adjustment, such as stack pops), whereas traps do not generate any code.  Switch \nconstructs:  switch l with case c1: l1 \u00b7\u00b7\u00b7 case ck: lk default: d The result of a switch construct \nis the evaluation of the li corresponding to the constructor ci appearing as the head of the value v \nof l. If the head constructor of v doesn t appear in the case list, the result is the evaluation of the \ndefault d expression. The default clause default: d can be omitted. In such a case the switch behavior \nis unspeci.ed on non-recognized values. Scheme C can thus omit the default clause when it is known that \ncase lists will cover all possibilities at runtime. We use the keyword switch* to highlight switch constructs \nwith no default clause. Those switch constructs are quite sophisticated, they com\u00adpile later into more \nbasic constructs: tests, branches and jump tables. We in fact modi.ed the Objective Caml compiler to \nimprove the compilation of switch constructs, using techniques .rst introduced in the context of compil\u00ading \nthe case statement of Pascal [3]. The key points are using range tests, which can typically be performed \nby one single (unsigned) test and branch plus possibly one addition, cutting sparse case lists into denser \nones, and deciding which of jump tables or test sequence is more ap\u00adpropriate to each situation. A survey \nof these techniques can be found in [19]. Accessors: field nx,where x is a variable and n is an integer \no.set. By convention, the .rst argument of non\u00adconstant constructors stands at o.set zero.  Sequences: \nl1; l2 and units: ()  3.2 Initial state Input to the pattern matching compiler C consists of two arguments: \na vector of variables vx of size n and a clause matrix P .L of width n and height m. . . 11 1 l1 22 2 \np1 p2 \u00b7\u00b7\u00b7 pn . . p1 p2 \u00b7\u00b7\u00b7 pn . l2 . . . vx =(x1 x2 ...xn),P .L= . . . . . . . mm mm p1 p2 \u00b7\u00b7\u00b7 pn . l \nThe initial matrix is generated from source input. Given a pattern-matching expression (in Caml syntax): \n1122 mm match x with | p -> e | p -> e ... | p -> e The initial call to C is: catch .. p 1 . l1 . 2 l2 \n. p . .. C((x), ) .. . p m . lm with (failwith \"Partial match\") Where the li s are the translations \nto lambda-code of the e i s, and (failwith \"Partial match\") is a runtime failure that occurs when the \nwhole pattern matching fails.  3.3 Classical scheme By contrast with previous presentations, we assume \nthat matrix P . L has at least one row (i.e. m> 0). This condition simpli.es our presentation, without \nrestricting its generality. Hence, scheme C is de.ned by cases on non\u00adempty clause matrices: 1. If n \nis zero (i.e. when there are no more columns), then the .rst row of P matches the empty vector (): .. \nl1 . . l2 . . .. 1 C((),. . .)= l . . . . . lm 2. If n is not zero, then a simple compilation is possible, \nusing the following four rules. (a) If all patterns in the .rst column of p are variables, y 1 , y 2 \n, ..., y m, then: '' C(vx,P .L)= C((x2 x3 ...xn),P.L) where . 11 1 . p2 \u00b7\u00b7\u00b7 pn . let (yx1) l1 22 2 . \np2 \u00b7\u00b7\u00b7 pn . let (yx1) l2 . '' . . P .L= . . .) . . . . mm mm p2 \u00b7\u00b7\u00b7 pn . let (yx1) l We call this rule, \nthe variable rule. This case also han\u00addles wild-card patterns: they are treated like variables except \nthat the let-binding is omitted. (b) If all patterns in the .rst column of P are constructor patterns \nc(q1,...,qa), then let C be the set of matched constructors, that is, the set of the head constructors \nof the p1i s. Then, for each constructor c in C,we de.ne the spe\u00adcialized clause matrix S(c,P .L) by \nmapping the fol\u00adlowing transformation on the rows of P. i p S(c,P .L) 1 iii i c(q1i ,...,qai ) q1 \u00b7\u00b7\u00b7qa \np2 \u00b7\u00b7\u00b7pn . li ' ii ' c(q1,...,qa/ )(c= c) No row (Matrices S(c,P . L)and P . L de.ne the same matching \npredicate when x1 is bound to some value c(v1,...,va).) Furthermore, for a given constructor c of arity \na,let y1,...,ya be fresh variables. Then, for any constructor cin C, we de.ne the lambda-expression r(c): \n(let (y1 (field 0 x1)) ... (ya (field (a-1) x1)) C((y1,...,ya,x2,...,xn),S(c,P .L))) Finally, assuming \nC = {c1,...,ck}, the compilation re\u00adsult is: switch x1 with case c1: r(c1)\u00b7\u00b7\u00b7case ck: r(ck) default: \nexit (Note that the default clause can be omitted when C makes up a full signature.) We call this rule, \nthe con\u00adstructor rule. (c) If P has only one row and that this row starts with an or-pattern: ( ) P =(q1 \n|...|qo) p2 \u00b7\u00b7\u00b7 pn . l, Then, compilation result is: .. q1 . () . . . C((x1),. . .); C((x2 ...xn),(p2 \n...pn .l)) . qo . () This rule is the orpat rule. Observe that it does not duplicate any pattern nor \naction. However, variables in or-patterns are not supported, since, in clause qi . (), the scope of qi \nvariables is the action () . (d) Finally, if none of the previous rules applies, the clause matrix P \n. L is cut in two clause matrices P1 . L1 and P2 . L2, such that P1 . L1 is the largest pre.x of P . \nL for which one of the variable, constructor or orpat rule applies. Then, compilation result is: catch \nC(vx,P1 .L1) with C(vx,P2 .L2) This rule is the mixture rule. This paper doesn t deal with optimizing \nlet-bindings, which are carelessly introduced by scheme C. This job is left to a later compilation phase. \n  4. OPTIMIZATIONS We now describe some improvement to the classical com\u00adpilation scheme. For simplicity, \nwe present examples and defer the full presentation of our scheme to section 6. In all these examples, \nwe focus on pattern-matching compi\u00adlation, replacing potentially arbitrary actions more simple ones, \nsuch as integers or variables. 4.1 Optimizing the mixture rule In this section and in the following, \nour running example is the classical list-merge: let merge lx ly = match lx,ly with |[],_->1 |_,[]->2 \n| x::xs, y::ys -> 3 Such a matching on pairs encodes matching on two argu\u00adments. As a consequene, we \nconsider the following initial call to scheme C: C((lx ly),(P .L)) Where (P .L)is: . . [] . 1 (P .L)= \n[] . 2 . . x::xs y::ys . 3 Applying the mixture rule twice yields three matrices: () P1 .L1 = [] . 1 \n() P2 .L2 = [] . 2 ( ) P3 .L3 = x::xs y::ys . 3 Now, consider another clause matrix (P' .L'): . . [] \n. 1 (P' .L')= . x::xs y::ys . 3 . [] . 2 Both clause matrices de.ne the same matching function, namely \nthey both map ([] v)to 1,(v1::v2 [])to 2 and '' (v1::v2 v1::v2)to 3. Furthermore, (P' . L') can be ob\u00adtained \nfrom (P .L) by swapping its second and third row. More generally, one easily checks that swapping two \ncontigu\u00adous incompatible rows is legal. Then applying the mixture rule to (P' .L'), yields two matrices \nonly: [] . 1 P1'.L'1 = , x::xs y::ys . 3 () P2'.L'2 = [] . 2 catch (catch (switch lx with case []: 1 \ndefault: exit) with (catch (switch ly with case []: 2 default: exit) with (catch (switch lx with case \n(::): (switch ly with case (::) : 3 default: exit) default: exit)))) with (failwith \"Partial match\") \ncatch (catch (switch* lx with case []: 1 case (::) : (switch ly with case (::): 3 default: exit)) with \n(switch ly with case []: 2 default: exit) with (failwith \"Partial match\") Figure 1: Mixture optimization \n' . L ' Final outputs for P . L and P are displayed on ' . L ' Figure 1. Hence, as a result of replacing \nP . L by P , the two tests on lx that were performed separately on the left code are now merged in a \nsingle switch in the right code. Also notice that one trap disappears. More generally, an optimized mixture \nrule should take advantage of pattern-matching semantics to swap rows when possible, so that as few cuts \nas possible are performed.  4.2 Using exhaustiveness information The Objective Caml compiler checks \nthe exhaustiveness of pattern matching expressions and issues a warning be\u00adfore compiling non-exhaustive \npattern matchings. However, the exhaustiveness information can also be used for avoid\u00ading tests. Matrix \nP ' of the previous section is exhaus\u00adtive; this means that there will be no \"Partial match\" fail\u00adure \nat runtime. As an immediate consequence, the switch: (switch ly with case []: 2 default: exit) always \nsuc\u00adceeds (this switch is the last one performed by the optimized code in .gure 1). Thus, we replace \nit by 2. We can also sup\u00adpress the outermost trap. Hence, applying both optimiza\u00adtions described up to \nnow, compilation of P . L .nally yields: catch (switch* lx with case []: 1 case (::): (switch ly with \ncase []: 3 default: exit)) with 2 In the general case, exhaustiveness information is exploited by slightly \nmodifying scheme C. Itsu.cestoavoid emitting default clauses in switch constructs, when it is known that \nno exit should escape from produced code. This property holds initially for exhaustive pattern matchings, \nand trans\u00admits to all recursive calls, except for the call on P1 . L1 in the mixture rule.  4.3 Optimizing \nexits The two previous optimizations yield optimal code for the merge example. Hence we complicate the \nrunning example by considering a matching on objects of type t from sec\u00adtion 2: .. Nil . 1 . Nil . 2 \n. .. P . L = . One x . 3 . .. One y . 4 Cons (x,xs) Cons (y,ys) . 5 The optimized mixture rule yields \nfour matrices: Nil . 1 P1 . L1 = Cons (x,xs) Cons (y,ys) . 5 () P2 . L2 = Nil . 2 () P3 . L3 = One x \n. 3 () P4 . L4 = One y . 4 For reasons that will appear immediately, we apply the mixture rule from bottom \nto top, thereby nesting trap han\u00addlers. The match being exhaustive, compilation yields the code displayed \non the left part of Figure 2. Now, consider what happens at run-time when (lx ly)is (Cons (v1, v2) One \nv). A .rst switch on lx leads to line 7, where a switch on ly is performed. This switch fails, and the \ndefault action jumps to the nearest enclosing handler (line 13), where ly is tested against Nil resulting \nin another switch failure. Here, in our case, control goes to line 17, where another switch on lx (against \nOne x) fails, resulting in .nal jump to line 20. Hence, it would be appropriate to jump to line 20 right \nfrom the .rst test on ys. To do so, both exits and trap handlers are now labelled by integers. Note that \nthis new feature does not really complicate the compilation of static exceptions. Then, it becomes possible \nto jump to di.erent trap handlers from the same point and a better compilation of P . L is displayed \nin the right part of .gure 2. The code above maps vectors (Cons (v1, v2) One v)to 4 by executing two \nswitches, while previous code needed four switches to perform the same task. Hence, exit optimization \nhas a noticeable bene.t as regards run-time e.ciency. How\u00adever, code size may be increased, since some \nswitches may have larger case lists, but still remains under control, since no extra switches are generated. \nHence, .nal code size crit\u00adically depends on how switches translate to machine-level constructs. For \ninstance, machine-level code size obviously does not increase when switches are translated to jump ta\u00adbles. \nGiven the Objective Caml encoding of constructors, 1 catch 1 catch 2 (catch 2 (catch 3 (catch 3 (catch \n4 (switch lx with 4 (switch lx with 5 case Nil: 1 5 case Nil: 1 6 case Cons: 6 case Cons: 7 (switch ly \nwith 7 (switch* ly with 8 case Cons: 5 8 case Cons: 5 9 default: exit) 9 case Nil: (exit 2) 10 10 case \nOne: (exit 4)) 11 default: exit) 11 default: (exit 2)) 12 with 12 with (2) 13 (switch ly with 13 (switch \nly with 14 case Nil: 2 14 case Nil: 2 15 default: exit)) 15 default: (exit 3))) 16 with 16 with (3) 17 \n(switch lx with 17 (switch lx with 18 case One: 3 18 case One: 3 19 default: exit)) 19 default: (exit \n4))) 20 with 4 20 with (4) 4 Unoptimized code Optimized code Figure 2: Exit optimization we are here \nin the same desirable situation where the com\u00adpilation of apparently larger switches does not result \nin pro\u00adducing more code. Surprisingly, performing exit optimization is quite simple and cheap: the needed \ninformation is available at compile\u00adtime by inspecting pattern matrices only. Reachable trap handlers \nare de.ned as pairs (P, e) of a pattern matrix and an integer. Reachable trap handlers originate from \nthe di\u00advision performed by the mixture rule. Here, P1 . L1 is compiled with the reachable trap-handlers \n(P2, 2), (P3, 3) and (P4, 4). Then, the constructor rule specializes reachable trap handlers. Here, in \nthe case where lx is Cons (v1, v2), specializing reachable trap handlers results in ((Nil), 2) and ((One \ny), 4) (note that specializing P3 yields an empty ma\u00adtrix, which is discarded). Hence, while generating \nthe .rst switch on ly (line 7), it is known that the code produced by compiling trap handlers number \n2 and 3 will surely exit when ly is One v, and a jump to trap handler number 4 can be generated by the \ncompiler in that case.  4.4 Aggressive control \u00a4ow optimization The code produced by exit optimization \nstill contains re\u00addundant tests, some of which can be removed without al\u00adtering the handler structure \nintroduced by the mixture rule. More speci.cally, we consider trap handler number 3 (line 16). It results \nfrom compiling P3 and is a switch of lx against One. The only (exit 3) lies in trap handler number 2 \n(line 15) and results from ly not being Nil, this gives us no direct information on lx. Now, looking \nupwards for (exit 2),we can infer that trap handler number 2 is entered from two di.erent points. In \nthe .rst case (line 9), (lx ly) is fully known as (Cons (v1, v2) Nil), in the second case (line 11), \nonly lx is know to be One v.As (exit 3) on line 15 gets ex\u00adecuted only when ly is not Nil, we can .nally \ndeduce that the .rst case never results in entering trap handler num\u00adber 3. As a consequence, trap handler \nnumber 3 is executed in a context where lx necessarily is One v, the switch it per\u00adforms is useless and \nline 16 can be simpli.ed into 3 . This elimination of useless tests[4] is usually performed at a lower \nlevel by combining dead code elimination[9] and conditional constant propagation[21, 6]. Finally, after \nall optimizations, there remains one redun\u00addant switch in produced code, in trap-handler number 2 (line \n12). As a result, vectors (Cons (v1, v2) Nil)are mapped to 2 by testing ly twice. One should notice that \nthis is pre\u00adcisely the test that would get duplicated by compilation to decision trees. Describing what \nis known on values while entering trap handlers is slightly involved. The key idea is representing set \nof value vectors as pattern matrices. We call such a set a context. Contexts for the three trap handlers \nof our example are: Trap number Context 2 One Cons ( , ) Nil 3 4 ( One ( (One Cons ( , | Cons ( ) One \n, )) ) ) If precise enough and exploited fully, we conjecture that con\u00adtexts subsume exhaustiveness \ninformation. However as in\u00adtuition suggests and experience con.rms, contexts get larger while compilation \nprogresses, potentially reaching huge sizes at the end of matrices. We cure this by safely approximating \ncontexts when they get too large, replacing some patterns in them by wild-cards. Hence the optimizations \nof section 4.2 is still worth considering, as being cheap and always appli\u00adcable.  5. COMPILING OR-PATTERNS \nUntil now, the code produced for or-patterns is ine.cient, because only one or-pattern can be compiled \nat a time, re\u00adquiring multiple applications of the mixture rule before and after each or-pattern. Thanks \nto integer labelled exits, one easily avoids dividing matrices before or-patterns. Consider a car function \nfor our three-constructors list: let car list = match list with | Nil -> -1 | (One x | Cons (x,_)) -> \nx Compilation proceeds by allocating a new trap-handler number 2 and expanding the clause One x | Cons \n(x,_) into two clauses with patterns One x and Cons (x,_) . Actions for the new clauses are exits to \n2: catch .. Nil . -1 . . C((list), One x . (exit 2 x ) ) Cons (x , ) . (exit 2 x ) () with (2 x) C((),. \nx ) Note that both exits and trap handlers now take yet an\u00adother extra argument, the occurrences of x \nin exits are non\u00adbinding and refer to pattern variables, while the occurrence of x in handler is binding. \nThis new construct allows the compilation of or-patterns with variables. Implementation is not very tricky: \nthe catch... with (2 x) ... construct allocates one mutable variable; an exit updates this variable, \nwhich is read before entering the handler. In a native code compiler, such a variable is a temporary \nand ultimately a machine register. The generated lambda-code is as follow: catch switch* list with case \nNil: -1 case One: (exit 2 (field 0 list)) case Cons: (exit 2 (field 0 list)) with(2x) x Moreover, by \nthe semantics of pattern-matching, cuts af\u00adter or-patterns can also be avoided in many situations. In \nthe case of one column matrices, where the expanded or\u00adpatterns express the full matching performed, \nall cuts can be avoided. Things get a bit more complicated when ma\u00adtrices have more than one column. \nConsider the following clause matrix, (1|2) p2 . l1 P .L = (3|4) q2 . l2 We further assume a match on \n(xy) and that match fail\u00adure should result in (exit 1) (the static exception label corresponding to match \nfailure can be given as a third argu\u00adment to the compilation scheme). Writing p1 = (1|2) and q1 = (3|4), \nthere are obviously no value vectors (v1 v2)such that v1 is an instance of both p1 and q1. As a consequence, \nthe following compilation is correct: catch (catch (switch x with case 1: (exit 2) case 2: (exit 2) case \n3: (exit 3) case 4: (exit 3) default: (exit 1)) () l1 with (2) C((y),p2 . ,1)) () l2 with (3) C((y),q2 \n. ,1) Intuitively, once x is checked, the choice between .rst and second row is made. Depending on the \nvalue of y,matching may still fail, but then, the whole matching fails. Conversely, matrix division cannot \nbe avoided when match\u00ading by p1 does not exclude matching by q1,thatis, when p1 and q1 are compatible. \nThis is the case, for instance, when p1 = (1|2) and q1 = (2|3). Then, a correct compilation is: catch \n(catch (switch x with case 1: (exit 2) case 2: (exit 2) default: (exit 3)) () l1 with (2) C((y),p2 . \n,3)) with (3) (catch (switch x with case 2: (exit 4) case 3: (exit 4) default: (exit 1)) () l2 with (4) \nC((y),q2 . ,1)) Note that the third argument to the .rst recursive call to the compilation scheme is \n3 and not 1 . As a conse\u00adquence, vectors (2 v2) such that p2 does not match v2 while q2 matches v2 get \nmapped correctly to l2 . A slight inne.ency shows up, since x is tested twice. More striking, perhaps, \nvectors (1 v2) such that p2 does not match v2 also lead to testing x twice. An alternative compilation \nrule for or-pattern would sim\u00adply expand or-patterns in a pre-processing phase, yielding the matrix: \n.. 1 p2 . l1 . . l1 . 2 p2 .. . . l2 . 2 q2 3 q2 . l2 Then, there are no extra run-time tests on x,since \nthecon\u00adstructor rule applies. However, patterns p2 and q2 are now compiled twice. Note that there is \nno simple solution for avoiding this duplication of e.ort, since, once the construc\u00adtor rule is applied, \nthe two occurences of these patterns occur in di.erent contexts. More generally, code size is now out \nof control, a clear contradiction with the spirit of bactracking automata.  6. OUR COMPILATION SCHEME \nThe new scheme C * takes .ve arguments and a typical call is C * (vx,P . L,ex,def,ctx), where vx =(x1 \n...xn)and P .L is a clause matrix of width n: . p 1 1 \u00b7\u00b7\u00b7 p 1 n . l1 . . p 2 1 \u00b7\u00b7\u00b7 p 2 n . l2 . P .L= \n. . . . . . . . . m mm p1 \u00b7\u00b7\u00b7 pn . l Extra arguments are: The exhaustiveness argument ex is either \npartial or total depending on whether compilation can produce escaping exit constructs or not.  Reachable \ntrap handlers def are sequences (P1,e1); \u00b7\u00b7\u00b7; (Pt,et), where the ei s are integers (trap handler numbers) \nand the Pi s are pattern matrices of width n.  The context ctx is a pattern matrix of width k+n, equiva\u00adlent \nto a pair of matrixes P Q, where each row is divided  Figure 3: Operations on contexts (a) Context \nspecialization P Q row S(c,P Q)row i 1 iki 1iaini 1 iki 1 iaiain c(q c( ,..., ) q) \u00b7\u00b7\u00b7 p \u00b7\u00b7\u00b7 q \u00b7\u00b7\u00b7 p \n\u00b7\u00b7\u00b7 q \u00b7\u00b7\u00b7 q p ,...,q p q +1 i 1 ikini 1 ikiain c( ,..., ) \u00b7\u00b7\u00b7 p \u00b7\u00b7\u00b7 q \u00b7\u00b7\u00b7 p \u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7 q p p q +1 ' c \n(q i 1 iki 1iain ) \u00b7\u00b7\u00b7 p \u00b7\u00b7\u00b7 q p ,...,q no row (b) Context collection P Q row COL(P Q)row \u00b7\u00b7\u00b7 q in i \n1 iki 1 iaiai 1 iki 1iain ) q c(qc( ) \u00b7\u00b7\u00b7 p \u00b7\u00b7\u00b7 q \u00b7\u00b7\u00b7 p \u00b7\u00b7\u00b7 q p q p ,...,q,..., +1 -1 (c) Context \npushing and popping P Q row .(P Q)row . (P Q)row p i 1 \u00b7\u00b7\u00b7 p i k q i 1 \u00b7\u00b7\u00b7 q i n p i 1 \u00b7\u00b7\u00b7 p i k q i \n1 q i 2 \u00b7\u00b7\u00b7 q i n pi 1 \u00b7\u00b7\u00b7 p i k -1 p i k q i 1 \u00b7\u00b7\u00b7 q i n into a pre.x (in P)ofwidth k and a fringe \n(in Q)of appropriate number of arguments standing at beginning width n. of thefringe(see.gure 3-(b)). \n11 1 k11 1 n . . . . q\u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7 p p q (c) Context pushing . and popping . move the fringe limit 21 2 k21 \n2 n q\u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7 p p q one step forward and backward, without examining any P Q = . . . . . . pattern (see \n.gure 3-(c)). . . . q mn Informally, at any point in compilation, contexts are pre\u00ad m 1 mkm 1 q\u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7 \np p As contexts are used to represent set of values, we natu\u00adrally de.ne union and intersection over \ncontexts. Context order representations of what is known about matched values. The fringe records the \npossible values for vx, while the pre.x records the same information for other sub\u00adterms which are relevant \nto pending calls to C * . Transfers of patterns from fringe to pre.x are performed on the ar\u00adguments \nof recursive calls, while transfers in the opposite direction are performed as results are collected. \nThe initial call to C * for an exhaustive match is: .. l1 p 1 . union P Q.P ' Q ' yields a new matrix \nwhose rows are the rows of P Q and P ' Q ' . Row order is not relevant. Con\u00adtext intersection P QnP ' \nQ ' is de.ned as a context whose rows are the least upper bounds of the compatible rows of P Q and P \n' Q ' .Context extraction EX is a particular case of context intersection. '' '' EX(p,P Q )=( ... p \n... ) nP Q For example, when p is c( ,..., ), context extraction re\u00ad ' tains those value vectors represented \nby P ' Q whose k+1th 2 l2 . . components admit cas head constructor. Observe that such . p .. C * ((x),. \n. .,total,\u00d8,( )) a computation involves extracting or-pattern arguments and . . . . making wild-cards \nmore precise. p m . l m For a non-exhaustive match, ex is partial, def is the one\u00ad element sequence (( \n),1) and a trap handler is added as in section 3.3. The context argument remains the same: it expresses \nthat nothing is known yet about the value of vx. The new scheme returns a lambda-code land a jump sum\u00ad \nmary, . = {...,i ..ctx,...}, which is a mapping from trap numbers to contexts. Jump summaries describe \nwhat is known about matched values at the places where (exit i ...) occur in l. 6.1 Operations on contexts \nWe de.ne the following four operations on contexts : (a) Context specialization, S, by a constructor \nc of arity a is de.ned by mapping the transformation of .gure 3-(a) on context rows. (b) Context collection, \nCOL, is the reverse of specialization.  Except for collection and popping, which consume pre\u00ad.x elements, \nall these operations can be extended to simple matrices, by using an empty pre.x in input, and taking \nthe fringe for output. Doing so, we obtain exactly the operations of section 3.3 used to compute pattern \nmatrices (specializa\u00adtion S in particular). Operations on contexts are extended to jump summaries in \nthe natural manner. For instance, the union of . and . ' is de.ned as: ... ' = {...,i ...(i) .. ' (i),...} \nOperations on matrices are extended to reachable trap handlers in a similar manner: for instance, pushing \ntrap handlers is de.ned as pushing all matrices in them : .((P1,e1); ...;(Pt,et)) = (.(P1),e1); ...;(.(Pt),et) \n 6.2 Compilation scheme We now describe scheme C * by considering cases over the It combines the the \nlast element of the pre.x with the typical call. 1. If n is zero. then we have: .. l1 . . l2 . . * .. \nC ((),. . .,ex,def,ctx)= l1 ,\u00d8 . . . . . lm Observe that the jump summary is empty since no exit is outputed. \n 2. With respect to section 3.3, the variable rule only changes as regards the extra arguments ex, def \nand ctx.We only describe these changes. The performed recursive call re\u00adturns code l and jump summary \n. : l,.= C * (...,...,ex,.(def),.(ctx)) Exhaustiveness information ex does not change, while def and \nctx are pushed. The variable rule returns l unchanged and . popped.  3. In the constructor rule, let \nC = {c1,...,ck}be the matched constructors, let also S be the signature of their type. For a given constructor \nc.C, the performed recursive call is: C * (...,...,ex,S(c,def),S(c,ctx)) Exhaustiveness information ex \nis passed unchanged, while the other two extra arguments are specialized (specializa\u00adtion of trap handlers \nbeing the natural extension of matrix specialization). Each recursive call returns a lambda-code l(c) \nand a jump summary .c. Lambda-code l(c) gets wrapped into let\u00adbindings like in section 3.3, yielding \nthe .nal lambda-code r(c). We then de.ne a case list L and a jump summary .rec as follows: L= case c1: \nr(c1) \u00b7\u00b7\u00b7case ck: r(ck)  .rec = {...,i ..COL(.c(i)),...} c.C The case list is as before, while the jump \nsummary is the union of the the jump summaries produced by recursive calls, once collected. Optimizations \nare then performed. For clarity, optimiza\u00adtions are described as a two phase process: .rst, extend (or \nnot extend) the case list L with constructors taken from S \\C, and add (or not add) a default case; then, \ncompute the .nal jump summary. A .rst easy case is when S \\C is empty or when ex is total. Then, the \ncase list L is not augmented. Otherwise, we distinguish two cases : (a) IfS \\C is .nite, then for all \nconstructors c in this set we consider the context Qc Q ' c = EX(c( ,..., ),ctx) Then, trap handlers \n(P1,e1); ...;(Pt,et) are scanned left-to-right, stopping at the smallest i, such that the intersection \nQ ' c nPi is not empty. That is, we .nd the trap handler where to jump to when the head construc\u00adtor \nof x1 is c, in order to extend the case list as follows : L= L case c: (exit ei) It is possible that \nei does not exist (when Q ' c is empty). This means that x1 head constructor will never be c at runtime. \n(b) IfS \\C is in.nite (as in the case of integers) or consid\u00adered too large (as it might be in the case \nof characters), then, a default case is added to the case list : L= L default: (exit e1) That is, all \nnon-recognized constructors lead to a jump the nearest enclosing reachable trap-handler. However it is \nstill possible to extend the case list for particular constructors, applying the previous proce\u00addure \n(a) to the constructors that appear in the .rst column of reachable trap handler matrices and not in \nC. The .nal jump summary is computed by considering the .nal case list L. For a given trap handler number \nei let '' ' {c 1,...,c k/ }be the set of constructors such that case cj : (exit ei) appears in L. Then \nthe jump summary .ei is de.ned as: .ei = {ei .1' ( ,..., ) |\u00b7\u00b7\u00b7 |ck' / ( ,..., .EX(c )),ctx) } Moreover, \nif there is a default clause, the jump sum\u00admary .d is de.ned as: .d = {e1 ..ctx} Finally the constructor \nrule returns a switch on case list L and the jump summary built by performing the union of .rec,ofall \n.ei s and, when appropriate, of .d. The constructor rule performs many context unions, so that contexts \nmay become huge. Fortunately, contexts can be made smaller using a simple observation. Namely, let pvand \nvq be two rows in a context, such that pvis less precise than vq (i.e., all instances of vq are instances \nof pv). Then, row vq can be removed from the context, without modifying its meaning as a set of value \nvectors. Hence, while performing context union, one can leave aside some pattern rows. If the produced \ncontext is still too large, then contexts are safely approximated by .rst replacing some patterns in \nthem by wild-cards (typically all the pat\u00adtern in a given column) and then removing rows using the previous \nremark. Rough experiments lead us to set the maximal admissible context size to 32 rows, yielding sat\u00adisfactory \ncompilation time in pathological examples and exact contexts in practical examples. 4. Or-pattern compilation \noperates on matrices whose .rst column contains at least one or-pattern. Additionally, when p1 i is a \nor-pattern, then for all j, i<j = m one of the following, mutually exclusive, conditions must hold: (a) \np1 i and p1 j are not compatible. ij ii (b) p1 and p1 are compatible, and (p2 ...pn) is less precise \njj than (p2 ...pn) Conditions (a) and (b) guarantee that, whenever p1 i matches the .rst value vector \nv1 of a value v , but row i does not match v , then no further row in P matches v either. This is necessary \nsince further rows of P won t be reachable in case of failure in the or-pattern trap handler. Now, consider \none row number i, such that p1 i is the or\u00adpattern q1 |\u00b7\u00b7\u00b7 |qo. Further assume that this or-pattern binds \nthe variables y1,...,yv. First, we allocate a fresh trap number e and divide P . L into the following \nor\u00adbody P ' . L ' and or-trap P '' . L '' clauses: .. . . . . . . i-1 i-1 li-1 . . p1 ... pn . . .. . \nq1 ... . (exit ey1... yv) . .. '' . .. P . L =. . . . .. . qo ... . (exit ey1... yv) . .. i+1 i+1 lj+1 \n. p1 ... pn . . .. . . . () '' '' ii P . L = p2 ... pm . li In the or-body matrix, observe that the or-pattern \nis ex\u00adpanded, while the other patterns in row number i are replaced by wild-cards and the action is replaced \nby exits. Recursive calls are performed as follows: ' C * (v' l ' ,. = x,P ' . L,ex,def,ctx) '' '' l \n,. = ... x2.n,P '' '' ...C * (v. L,ex,.(EX(p,def)),.(EX(p,ctx))) Outputed code .nally is catch l ' with \n(ey1... yv) l '' and the returned jump summary is . = . ' ..(. '' ). 5. The mixture rule is responsible \nfor feeding the other rules with appropriate clause matrices. We .rst consider the case of a random division. \nHence let us cut P . L into Q. M and R . N at some row. Then a fresh trap num\u00adber e is allocated and \na .rst recursive call is performed: lq,.q = C * (vx,Q. M,partial,(R,e); def,ctx) The exhaustiveness information \nis partial, since nothing about the exhaustiveness of Q derives from the exhaus\u00adtiveness of P. Reachable \ntrap handlers are extended. Then, a second recursive call is performed: lr,.r = C * (vx,R. N,ex,def,.q(e)) \nIt is no surprise that the context argument to the new call is extracted from the jump summary of the \nprevious call. Argument ex does not change. Indeed, if matching by P cannot fail, then matching by R \nneither can. Then, the scheme can output the code l = catch lq with (e) lr and return the jump summary \n(.q \\{e})..r,where .q \\{e}stands for .q with the binding for e removed. Of course, our optimizing compiler \ndoes not perform a random division into two matrices. It instead divides P . L right away into several \nsub-matrices. This can be described formally as several, clever, applications of the random mixture rule, \nso that one of the three previ\u00adous rules apply to each matrix in the division. The aim of the optimizing \nmixture rule is thus to perform a division of P into as few sub-matrices as possible. We present a simple, \ngreedy, approach that scans P downwards. We only describe the case when p 1 is a constructor pat\u00ad 1 tern. \nThus, having performed the classical mixture rule, we are in a situation where the i topmost rows of \nP have a constructor pattern in .rst position (i.e. are construc\u00adtor rows for short) and where p1 i+1 \nis not a constructor pattern. At that point, a matrix C has been built, which encompasses all the rows \nof P from 1 to i. Let us fur\u00adther write P ' for what remains of P,and let O and R be two new, initially \nempty matrices. We then scan the rows of P ' from top to bottom, appending them at the end of C, O or \nR.That is, given row number j in P ' : 'j (a) If p 1 is a variable, then append row j at the end of R. \n'j (b) If p 1 is a constructor pattern, then. . . i. If row j is not compatible with all the rows of \nboth R and O, then append row j at the end of C (i.e., move row j above all the rows that have been extracted \nfrom P ' at previous stages). ii. If row j is not compatible with all the rows of R and that one of conditions \n(a) or (b) for applying the or\u00adpattern rule are met by O with row j appended at the end, then do such \nan append. iii. Otherwise, append row j at the end of R. 'j (c) If p 1 is a or-pattern, then consider \ncases (ii) and (iii). When the scan of P ' is over, three matrices, C, O and R have been built. In the \ncase where Ois empty, matrix C is valid input to the constructor rule; otherwise, appending the rows \nof O at the end of C yields valid input for ap\u00adplying (maybe more than once) the or-pattern rule, which \nwill in turn yield valid input to the constructor rule (pro\u00advided that (_ |... ) or patterns have been \nreplaced by semantically equivalent wild-cards in a previous phase). Thus, the matrix built by appending \nO at the end of C is recorded into the overall division and the division process is restarted with input \nR, unless R is empty. Finally, the full process divides the input matrix P into several matrices, each \nof which is valid input to the other rules of the compilation scheme.  7. EXPERIMENTAL RESULTS We compare \nthe performance of the code generated by the Objective-Caml compilers version 3.00 and 3.01, where the \nformer implements the scheme of section 3.3 and the latter implements our new optimizing scheme (there \nare other di.erences of minor relevance to our purpose). For most programs there is little di.erence; \nthis is natural since pattern-matching usually accounts for a small fraction of most programs running \ntime. A full analysis of the e.\u00adciency of our optimizations would in fact require counting relevant instructions \n(test, branches and indirect branches through jump tables), both statically and dynamically. By lack \nof time, we only present some programs that demon\u00adstrate signi.cant improvement. Our .rst benchmark is \nthe traditionnal fib, that we write using a or-pattern. let rec fibn=match n with | (0|1) ->1|_->fib \n(n-1) + fib (n-2) Here, we simply measure the execution time of computing fib 38. Our second benchmark, \npcf,is a byte-code com\u00adpiler and interpreter for PCF. We compute the geometric mean of the execution \ntime for a set of .ve di.erent PCF programs. The time-consuming part of this program is the byte-code \nmachine which we coded in the style of the byte\u00adcode machine included in [14], the winning entry of the \n2000 ICFP programming contest. (we also give .gures for this program under the name raytrace). Experiments \nwere performed on a lightly loaded 366Mhz Pentium Pro Linux PC. The tables show wall-clock times (in \nseconds) and ratios: fib rayt race pcf V 3.00 5.36 100 1.69 100 8.12 100 V 3.01 3.74 71 1.62 96 5.08 \n63 Obviously, as demonstrated by the fib example, compila\u00adtion of or-patterns has much improved. Testing \nsimilar ex\u00adamples con.rms that fact. Improvements also comes from the better compilation of switches. \nThe pcf example is more interesting, it shows that our optimizations yield a 37% speed-up, in the case \nof a typical ML application (a quickly written, compact, prototype implementation of some pro\u00adgramming \nlanguage). The raytrace example exhibits less important improvements on the whole test suite of the con\u00adtest; \nhowever, improvements are noticeable for some inputs. It should also be noticed that the new compiler \nsomehow equates the runtime performance of various coding styles, a feature that is important for a high-level \nconstruct such as pattern-matching. Variations in coding style include the rel\u00adative ordering of non-overlapping \npatterns and on the order of arguments in pairs. We also performed measurements on a 500Mhz Dec Alpha \nserver. They suggest that the e.ects of our optimization do not depend on the targeted architecture. \n The raytrace example is is omitted because it relies on IEEE .oating point arithmetic, which is not \nimplemented in the Objective Caml compiler for this architecture. More detailed information on these \nbenchmarks is avail\u00adable at http://caml.inria.fr/pattern/speed.html. 8. RELATED WORK 8.1 Decision Trees \nvs Backtracking Compiling to decision trees is the original approach to pattern matching compilation; \nit .rst appeared in the Hope compiler and is described in [5]. It is currently used in the SML-NJ compiler \n[7]. In this approach, there is no mixture rule: instead, the constructor rule applies as soon as there \nis at least one con\u00adstructor in the .rst column, and a specialization matrix is created for each matched \nconstructor, plus one additional matrix for the remaining constructors in the signature of the types \nof matched values, if any. Specialization is done by following the rules of section 6.1. This means that \nrows whose .rst pattern is a variable get copied several times. On the one hand, this approach guarantees \nthat one con\u00adstructor test is never performed twice. On the other hand, copied pattern rows are compiled \nindependently and this result in potentially large code size. Namely, examples ex\u00adist that make the SML-NJ \ncompiler produce exponential code [12]. Compilation to backtracking automata is the classical scheme \nof section 3.3 (see also [1, 20]). It is currently in use in the Haskell-HBC and Objective-Caml compiler \n[11]. As we al\u00adready argued, its main advantage is that patterns are never copied, yielding linear output \nsize. Of course, the price paid is potentially testing the same sub-term several times, re\u00adsulting in \npotentially poor runtime performance. In that aspect, our new compilation scheme shows that this price \ncan be reduced signi.cantly. Compilation to decision trees easily detects unused match cases and non-exhaustive \nmatchings, since there is no dead code in a decision tree. Detecting these situations is impor\u00adtant, \nas programmers should be warned about them. How\u00adever, those problems are NP-complete [17] and this gives \nus a hint about the potential size of decision trees. More concretely, a decision tree may have many \nleafs correspond\u00ading to non-matched values, whereas knowing that one such values exist is the needed \ninformation. Rather, we check unused match cases and exhaustiveness before compilation with a simple \nalgorithm [13] that solves the used matched case problem by basically traversing the decision tree with\u00adout \ngenerating it. Advantages are not generating the tree, stopping search as soon as used match cases are \nfound and applying various heuristics and matrix simpli.cations which are not relevant to direct compilation. \nThen, one of our optimizations uses exhaustiveness information. 8.2 Compiling or-patterns From available \nML or Haskell compilers, we only found two compilers dealing with or-patterns: the (old) Objective-Caml \ncompiler and the SML-NJ compiler. Our technique makes the old Objective-Caml scheme (see section 3.3) \nob\u00adsolete, by both producing more e.cient code and allowing variables in or-patterns. The SML-NJ approach \nis very simple to understand and implement: or-patterns are expanded during a pre-processing phase. However, \nas we already discussed at the end of sec\u00adtion 5, this may lead to many duplications of patterns. Such \na risk is compatible with the very philosophy of compilation to decision trees and is natural in that \ncontext. 8.3 Optimizations Most optimizations dealing with pattern-matching in the literature try to \nimprove the order in which tests are per\u00adformed. In the matrix-based description, one considers al\u00adternatives \nto systematically choosing the .rst column of ma\u00adtrices in the constructor rule. Hence, such an approach \ncan be characterized as column optimization , while our ap\u00adproach would rather be row optimization . \nSince choos\u00ading the best column is thought to be NP-complete (to our knowledge, there is no published \nproof), most approaches describe heuristics. A typical and early work on such heuris\u00adtics is [2], a more \nrecent and thorough study is [16]. Another, related in practice, approach relies on sequentiality theory \nto identify directions that are columns that must be tested by all possible matchers [10, 15, 17, 13]. \nHowever, comput\u00ading directions is expansive, and one can consider relying on cheaper heuristics. These \nworks rather apply to the decision trees, with a primary focus on reducing code size. It is unclear to \nus how to combine column and row optimization in practice and whether this would yield noticeable improvements \nor not. There also exists a partial-evaluation based approach to pattern-matching optimization. [8] and \nlater [18] specialize an ultra-naive pattern-matching interpreter to create an e.\u00adcient pattern-matching \ncompiler. Both authors use context information as we do. By contrast, their target is decision trees. \nIn the end, the automatic process of partial evaluation does not .nd as many optimizations as we do. \n9. CONCLUSION This paper contribution is twofold. First, we propose an improvement on the classical technique \nof compiling pattern\u00admatching expressions into backtracking automata, a tech\u00adnique that has remained \nvirtually the same for about 15 years. Our improvements yield automata which run faster, thereby alleviating \nthe disadvantage of backtracking automata in practical cases. Moreover the very structure of the produced \nautomata is not altered and hence the highly desirable prop\u00aderty that output size is linear in the input \nsize is preserved. As a second contribution, we propose a technique for e.\u00adciently compiling or-patterns \nwith variables, still preserv\u00ading the linearity of output size. Using or-patterns in place of catch-all \nwild-cards results in more robust programs, while using one clause with a or-pattern in place of sev\u00aderal \nclauses with identical actions results in more compact, sometime clearer, programs. ML programmers can \nnow en\u00adjoy these bene.ts, without being afraid of degraded runtime e.ciency or code size explosion. We \nwould have wished to make a clear statement on com\u00adparing bactracking automata and decision trees. However, \nsophisticated compilation techniques exist that minimize the drawbacks of both approaches. Those are \nour techniques for backtracking automata, and hash-consing and column opti\u00admizations for decision trees. \nIn the absence of a practical comparison of full-.eged algorithms, choosing one technique or the other \nre.ects one s commitment to guaranteed code size or guaranteed runtime performance. 10. REFERENCES [1] \nAugustsson, L. Compiling pattern matching. In Functional Programming Languages and Computer Architecture. \n1985. [2] Baudinet, M., and MacQueen, D. Tree pattern matching for ML,. unpublished paper, Dec. 1985. \n [3] Bernstein, R. L. Producing good code for the case statement. Software Practice and Experience 15,10 \n(Oct. 1985), 1021 1024. [4] Bod\u00b4ik, R., Gupta, R., and Soffa, M. L. Interprocedural conditional branch \nelimination. In Conference on Programming Language Design and Implementation (PLDI) (1997). [5] Cardelli, \nL. Compiling a functional language. In Conference Record of the 1984 ACM Symposium on Lisp and Functional \nProgramming (Aug. 1984), ACM, ACM, pp. 208 217. [6] Fraser, C. W. A compact, machine-independent peephole \noptimizer. In Symposium on Principles of Programming Languages (1979). [7] Harper, R. W., MacQueen, \nD. B., and Milner, R. Standard ML. Report ECS-LFCS-86-2, Department of Computer Science, University of \nEdinburgh, Edinburgh, UK, 1986. Also CSR-209-86. [8] J\u00f8rgensen, J. Generating a pattern matching compiler \nby partial evaluation. In Glasgow Workshop on Functional Programming, Ullapool (1990). [9] Knoop, J., \nR\u00a8uthing, O., and Steffen, B. Partial dead code elimination. In Conference on Programming Language Design \nand Implementation (1994). [10] Laville, A. Implementation of lazy pattern matching algorithms. In ESOP \n88 (1988), H. Ganzinger, Ed., vol. 300, pp. 298 316. [11] Leroy, X. The objective caml system: Documentation \nand user s manual, 2000. With Damien Doligez, Jacques Garrigue, Didier R\u00b4emy, and J\u00b4r ome Vouillon. Available \nfrom http://caml.inria.fr. [12] Maranget, L. Compiling lazy pattern matching. In Proc. of the 1992 conference \non Lisp and Functional Programming (1992), ACM Press. [13] Maranget, L. Two techniques for compiling \nlazy pattern matching. Research Report 2385, INRIA Rocquencourt, Oct. 1994. [14] PLClub, and Caml R Us. \nObjective-caml: Winner of the .rst and second prizes of the p rogramming contest. ACM SIGPLAN International \nConference on Functional Programming (ICFP 2000). [15] Puel, L., and Su\u00b4 A. Compiling pattern arez, matching \nby term decomposition. Journal of Symbolic Computation 15, 1 (Jan. 1993), 1 26. [16] Scott, K., and Ramsey, \nN. When do match-compilation heuristics matter? Tech. Rep. CS-2000-13, Department of Computer Science, \nUniversity of Virginia, May 2000. [17] Sekar, R. C., Ramesh, R., and Ramakrishnan, I. V. Adaptive pattern \nmatching. In Automata, Languages and Programming, 19th International Colloquium (1992), vol. LNCS 623. \n[18] Sestoft, P. ML pattern match compilation and partial evaluation. Lecture Notes in Computer Science \n1110 (1996), 446 ?? [19] Spuler, D. A. Compiler code generation for multiway branch statements as a static \nsearch problem. Tech. Rep. 94/3, Department of Computer Science, James Cook University, 1994. [20] Wadler, \nP. Compilation of pattern matching. In The Implementation of Functional Programming Languages, S. L. \nPeyton Jones, Ed. Prentice-Hall International, 1987, ch. 7. [21] Wegman, M., and Zadeck, F. K. Constant \npropagation with conditional branches. In Symposium on Principles of Programming Languages (1985).  \n  \n\t\t\t", "proc_id": "507635", "abstract": "We present improvements to the backtracking technique of pattern-matching compilation. Several optimizations are introduced, such as commutation of patterns, use of exhaustiveness information, and control flow optimization through the use of labeled static exceptions and context information. These optimizations have been integrated in the Objective-Caml compiler. They have shown good results in increasing the speed of pattern-matching intensive programs, without increasing final code size.", "authors": [{"name": "Fabrice Le Fessant", "author_profile_id": "81100431247", "affiliation": "INRIA Roquencourt, Le Chesnay, France", "person_id": "PP40027191", "email_address": "", "orcid_id": ""}, {"name": "Luc Maranget", "author_profile_id": "81100574739", "affiliation": "INRIA Roquencourt, Le Chesnay, France", "person_id": "PP37029165", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/507635.507641", "year": "2001", "article_id": "507641", "conference": "ICFP", "title": "Optimizing pattern matching", "url": "http://dl.acm.org/citation.cfm?id=507641"}