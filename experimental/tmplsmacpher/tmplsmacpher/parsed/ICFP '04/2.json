{"article_publication_date": "09-19-2004", "fulltext": "\n Searching for Deadlocks while Debugging Concurrent Haskell Programs Jan Christiansen Frank Huch Christian-Albrechts-University \nof Kiel Institute of Computer Science Olshausenstr. 40, 24118 Kiel, Germany {jac,fhu}@informatik.uni-kiel.de \nAbstract This paper presents an approach to searching for deadlocks in Con\u00adcurrent Haskell programs. \nThe search is based on a rede.nition of the IO monad which allows the reversal of Concurrent Haskells \nconcurrency primitives. Hence, it is possible to implement this search by a backtracking algorithm checking \nall possible schedules of the system. It is integrated in the Concurrent Haskell Debugger (CHD), and \nautomatically searches for deadlocks in the background while debugging. The tool is easy to use and the \nsmall modi.ca\u00adtions of the source program are done by a preprocessor. In the tool we use iterative deepening \nas search strategy which quickly detects deadlocks close to the actual system con.guration and utilizes \nidle time during debugging at the best. Categories and Subject Descriptors Software [Software Engineering]: \nTesting and Debugging De\u00adbugging aids General Terms Languages  Keywords Concurrent Haskell, debugging, \ndeadlock, detecting deadlocks 1 Introduction Developing concurrent applications is a dif.cult task. Beside \nthe bugs a programmer can make in the sequential parts of his pro\u00adgram, he can also produce bugs related \nto concurrency and thread synchronization, like deadlocks, livelocks, or not guaranteed mu\u00adtual exclusion. \nOne approach to prevent programmers from such bugs is formal veri.cation, like model checking or theorem \nprov\u00ading, combined with a formal system speci.cation and re.nement to Permission to make digital or hard \ncopies of all or part of this work for personal or classroom use is granted without fee provided that \ncopies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. ICFP 04, September 19 21, 2004, Snowbird, Utah, \nUSA. Copyright 2004 ACM 1-58113-905-5/04/0009 ...$5.00 obtain the executable program. However, in practice, \nthis approach does not scale well for larger concurrent applications. The commu\u00adnication (and synchronization) \npart of the application may interfere with the computation. Often, it is even the case that concurrency \nis a means to express algorithms. In these cases, formal veri.ca\u00adtion is often not applicable, e.g. because \nof in.nite state spaces or state space explosion problems. As a consequence, validating an implementation \nis in many cases restricted to testing. In this paper, we mainly focus on the detection of deadlocks. \nDur\u00ading the development of a concurrent application, it is often the case that the execution runs into \na deadlock and there is no hint, in which con.guration of the program this deadlock occurs. To visualize \nthe deadlock and the schedule causing it, debuggers can be helpful. To also detect deadlocks related \nto other schedules, a debugger for a concurrent language should also give the possibility to in.uence \nthe scheduling to detect possible, hidden deadlocks not reached in nor\u00admal system executions. These hidden \ndeadlocks can become real deadlocks when for instance, the system is executed with a differ\u00adent scheduler \nor the scheduling is effected by adding further threads to the application. However, debugging is not \na good technique for detecting these hidden deadlocks, since it only performs a single schedule (although \nselectable by the user) like the non-debugged execution. To detect deadlocks in other schedules it would \nbe nice to search in all possible schedules during debugging. Since the state space of the application \nmay be in.nite, this search should be re\u00adstricted in depth and should consider the input/output behavior \nof the program in a sensible way. We have implemented this idea of searching for deadlocks for Con\u00adcurrent \nHaskell [19], an extension of the pure, lazy functional pro\u00adgramming language Haskell 98 [18]. The search \nis based on a re\u00adde.nition of the IO monad which allows the reversal of Concurrent Haskells concurrency \nprimitives. The approach is conveniently in\u00adtegrated into our graphical Concurrent Haskell Debugger (CHD) \n[2]. The debugger performs the search for deadlocks in the back\u00adground, without any debugging restrictions \nfor the user. When a deadlock is found the user is guided into the deadlock state. The implementation \nuses iterative deepening as search strategy. Hence, deadlocks close to the actual con.guration are quickly \ndetected and time until the user selects a thread to continue is utilized at the best. To keep the users \neffort for debugging as small as possible, the tool also provides a preprocessor which performs the necessary \nmodi.\u00adcations of the source program modules. The paper is organized as follows: in Section 2 we give \na short in\u00adtroduction to Concurrent Haskell and introduce previous work on the Concurrent Haskell Debugger \nin Section 3. Section 4 motivates the search for deadlocks during debugging. In Section 5 we dis\u00adcuss \nthe underlying idea of our approach with interpreting the IO monad as a data structure. Our algorithm \nfor search and an opti\u00admization based on partial order reduction is described in Section 6. The integration \nin CHD is shortly described in Section 7 and the practical applicability of the tool is discussed in \nSection 8. Finally, we discuss related work in Section 9 and conclude in Section 10. 2 Concurrent Haskell \nConcurrency is a useful feature for many real world applications. Examples are graphical user interfaces \nor web servers. Such reac\u00adtive systems (have to) interact with multiple interfaces at the same time. \nTherefore, a programming language should support concur\u00adrency for simultaneous serving of requests. The \nlazy functional programming language Haskell 98 [18] does not support concurrency. Therefore, the extension \nConcurrent Haskell [19] was proposed. Concurrent Haskell extends the monadic IO primitives of Haskell \n98 with a thread concept and inter-thread communication by means of shared variables. It is im\u00adplemented \nwithin the Glasgow Haskell Compiler (ghc) [7]. This section will give a short introduction to the possibilities \nand usage of Concurrent Haskell. 2.1 Threads The .rst extension is a thread concept. Multiple threads \ncan be exe\u00adcuted concurrently, where each thread can be seen as the execution of one Haskell program. \nThe function forkIO :: IO () -> IO ThreadId creates a new thread which starts with the execution of the \nargument of forkIO. The function forkIO itself is an IO action, which yields the unique thread identi.er \nof the created thread. This ThreadId can be used to kill the thread by killThread :: ThreadId -> IO () \nAll threads are executed concurrently. Their IO actions are inter\u00adleaved in a non-deterministic way. \nA thread terminates by perform\u00ading no more actions. 2.2 Communication For inter-thread communication \nConcurrent Haskell provides dif\u00adferent kinds of communication abstractions. The simplest commu\u00adnication \nabstraction are mutable variables (MVars). An MVar can either contain a value of a speci.ed type or be \nempty. data MVar a --abstract newEmptyMVar :: IO (MVar a) takeMVar :: MVar a -> IO a putMVar :: MVar \na -> a -> IO () The action newEmptyMVar creates a new mutual variable which is initially empty. putMVar \n.lls an empty MVar with a value. If the MVar is already full, then the thread performing putMVar is sus\u00adpended \nuntil the MVar is empty again1. takeMVar reads the con\u00adtents of a full MVar leaving it empty. Similarly \nto putMVar, the thread executing takeMVar is suspended, if the MVar is already 1This semantics has changed. \nOriginally, in [19], putMVar threw an exception in this case. empty, until another thread writes a value \nto it. In the case that mul\u00adtiple threads suspend on the same MVar, non-deterministically one thread \nis chosen to continue, when the MVar is emptied (putMVar) respectively .lled (takeMVar). The implementation \nguarantees that each of these functions (and all the other functions on com\u00admunication abstractions) \nare executed atomically. They cannot be interrupted by the scheduler. The library Concurrent implements \nmore actions on MVars such as testing for emptiness, swapping the contents, or just reading without emptying \nthe MVar. Furthermore, MVars are the base for other, more complex communication abstractions: a slight \nmodi.\u00adcation of MVars(SampleVar), two kinds of quantity semaphores (QSem and QSemN), and unbound channels \n(Chan). A Concurrent Haskell program is stuck in a deadlock if all threads are suspended on communication \nabstractions. If at least one thread is waiting for input from the outside world (e.g. like in a web \nserver), then this is not a deadlock, since the thread may awake the other threads after receiving input. \nNote, that systems may contain partial deadlocks in which only some threads are waiting for each other. \nIn the remaining paper we do not consider partial deadlocks, since they are very dif.cult to identify \nand many situations (e.g. an idle web server) can be seen as a partial deadlock. 2.3 An Example As a \nsmall example for Concurrent Haskell we present an imple\u00admentation of the well known dining philosophers \nproblem. The sticks the philosophers have to share for eating, are represented by MVars which are .lled \nif the corresponding stick is laying on the table. Hence, taking a stick is implemented by a takeMVar \naction. Since philosophers usually do not communicate, but only synchro\u00adnize for eating, it is suf.cient \nto use MVars of type (). A philoso\u00adpher can simply be de.ned by a non-terminating function: phil :: MVar \n() -> MVar () -> IO () phil left right = do --get hungry takeMVar left takeMVar right --eat putMVar left \n() putMVar right () --think phil left right  3 Concurrent Haskell Debugger In previous work [2], we \ndeveloped a debugger specialized for bugs related to concurrency in Concurrent Haskell, called the Concur\u00adrent \nHaskell Debugger (CHD). The basic idea of this debugger, is the extension of all concurrency primitives \nprovided by Concurrent Haskell with debugging output. The debugger consists of a mod\u00adule ConcurrentDebug \nwith the same interface as Concurrent. Calling a function of ConcurrentDebug .rst performs some de\u00adbug \noutput. Then the original function of the module Concurrent is executed and .nally, again, some debug \noutput is performed. The debug output is sent to a graphical user interface, which vi\u00adsualizes all threads, \ncommunication abstractions and performed ac\u00adtions. For example, if a thread performs a putMVar action, \nthen an arrow from the graphical representation of this thread to the corre\u00adsponding MVar is drawn and \nthe state of the MVar switches from empty to .lled. The different concurrency actions can easily be distinguished \nby the orientation and color of the drawn arrows. To Figure 1. Five dining philosophers observe the \n.ow of messages, CHD also indicates which thread has stored a value in a communication abstraction by \na thread identi\u00ad.er. A screenshot of CHD used in the dining philosophers program is presented in Figure \n1. In the presented state, only MVar5 is .lled and this value was written by Thread1 (T1). Thread2 is \nsuspended on MVar2 and Thread1 wants to perform a takeMVar action on MVar2 and will, since MVar2 is empty, \nsuspend in the next step. Thread3 will put a value into MVar3 in the next step, which will wake up Thread2. \nFor debugging a concurrent system, it is not suf.cient to just execute it step by step. It is also important \nto in.uence the scheduling, since not every schedule may contain a bug (e.g. dead\u00adlocks). In CHD the \nscheduling of the system can easily be in\u00ad.uenced by clicking on the thread that is supposed to continue \nits execution. This is implemented by additional synchroniza\u00adtion messages between the extended debug \nversion of the concur\u00adrent action and a control thread which also communicates with the graphical user \ninterface. The CHD provides further possibil\u00adities for convenient debugging, like naming threads or commu\u00adnication \nabstractions, con.guring the interaction behavior for the visualized objects as well as for special concurrent \nactions, and showing source code positions of threads. For more details see http://www.informatik.uni-kiel.de/~fhu/chd. \n 4 Motivation The Concurrent Haskell Debugger is a useful tool if you want to check whether a schedule \ncauses a deadlock. You can check this by just executing your program with exactly that schedule. However, \nif you do not .nd a deadlock you nevertheless do not know if there is any. You can also interactively \nmodify the scheduling and check other schedules that way, but you will never check every possible schedule. \nPerhaps if you just exchange the execution order of two threads you would run into a deadlock. To avoid \nthis undesirable situation it would be necessary to check every possible schedule of a program. This \npaper shows how we combine a search for deadlocks with the useful features of the Concurrent Haskell \nDebugger. The new de\u00adbugger should allow the user to execute one step of a particular thread just as \nCHD does. In the background it should also check the program for a deadlock by executing the program \nwith every possible scheduling. This causes the problem of restoring an old state after one computation \nhas been checked. One approach for solving this problem would be a restart of the whole system with serving \ninput actions with recorded input and ignoring output ac\u00adtions. However, this would be very inef.cient \nin combination with searching all possible schedules since the system has to be restarted for every possible \nschedule. Furthermore, the effort for restarting grows with the progress of the computation. Hence, search \nwill dramatically slow down after some debugging steps. An alternative would be to retract performed \nconcurrency primi\u00adtives. Inspecting the concurrency primitives, we see, that this is always possible, \nas the following examples show: action can be retracted by newEmptyMVar doing nothing putMVar m takeMVar \nm readMVar m doing nothing v <\u00ad takeMVar m putMVar m v To retract newEmptyMVar we have to do nothing \nbecause the garbage collector will eliminate the created MVar for us. Writing to an MVar by putMVar can \nbe retracted by takeMVar on the same MVar and discarding the taken value. Reading an MVar does not ef\u00adfect \nthe state of the concurrent system. Finally, to retract takeMVar we have to put the value, that has been \ntaken out of the MVar, back into it. Hence, it is not suf.cient to consider only a restore action, but \na restore function (of type a-> IO()) which takes the result of the retractable action (of type IO a) \nand retracts it by means of the restore action. Formally, the restore function that corresponds to a \nretractable action has to ful.ll the following property: (retractable action >>= restore function) . \nreturn () For the discussed concurrency actions we get: retractable action restore function newEmptyMVar \n\\_-> return() putMVar m v \\_-> takeMVar m >> return () readMVar m \\_-> return() takeMVar m \\v -> putMVar \nm v The creation of a thread could be retracted by killing this thread again. However, we want to control \nthe scheduling during search. Hence, our implementation uses a list of threads and the functions forkIO \nand killThread are implemented just in form of adding and removing elements of this list. We will discuss \nthis in more detail in the next section. With this insight we are able to undo every action that is performed \nby a concurrent program. So, the checker is able to execute the pro\u00adgram with a particular schedule and \nrestore the state before contin\u00aduing the execution. However, it is not suf.cient to be able to retract \nactions. The performed actions are dynamically combined during the execution of the system. Hence, we \nhave to collect the restore functions while searching for deadlocks. For this purpose we use a tree structure \nthat represents the actions of the program and their order. Every leaf in this tree contains a retractable \naction and the corresponding restore function. To generate this representation we rede.ne the IO datatype \nand all functions yielding it. Values of the new IO datatype are leafs in this tree. Inner nodes are \ncreated by interpreting the bind operator of the monad instance as a con\u00adstructor. The function checkThreads \ninterprets this structure and checks whether there is a schedule that causes a deadlock. Considering \nall possible schedules seams to result in a very large search space. Fortunately, we do not have to consider \nall possi\u00adble schedules: in Concurrent Haskell, the matter of communica\u00adtion and synchronization between \nthreads are the (retractable) ac\u00adtions for reading and writing communication abstractions de.ned in the \nmodule Concurrent. Hence, it is suf.cient to consider only schedulings which perform these actions in \nall possible orders. There is another possibility for threads to communicate: by other IO actions. For \nexample, one thread can write a .le and another thread can later read this .le. Fortunately, this is \nvery uncom\u00admon and we do not perform search when such communication is involved. Sequential IO operations \nproduce special nodes in the IO tree. Thus we can handle such actions separately, as we discuss in Section \n5.1.2. 5 Generating the IO Tree This section attends on the generation of a tree representation of the \nactions used in a concurrent program, similar to Hinzes Back\u00adtracking Monad transformers [11]. Before \nwe present how vari\u00adous schedules can be checked by means of this representation, we shortly discuss \nhow Haskell s module system provides the rede.ni\u00adtion of the IO datatype. For convenient use of our debuger, \nwe want to keep the name IO for the our newly de.ned datatype. Hence, the original IO type must be hidden \nexplicitely which has to be done explicitely since the prelude is always imported. import Prelude hiding \n( IO ) Furthermore, we also need the original IO datatype from the prelude and import it quali.ed, which \nmeans use it quali.ed as P.IO. import qualified Prelude as P To create the IO tree we have to rede.ne \nthe Concurrent Haskell functions. Since these rede.nitions will use the original ones, we import the \nmodule Concurrent quali.ed as well. import qualified Concurrent as C Hence, each function imported from \nthe original Concurrent Haskell library is preceded with C. . 5.1 IO Rede.nition The tree representation \nof a Concurrent Haskell program is gener\u00adated by rede.ning the P.IO datatype and the bind operator. The \nrepresentation consists of three categories each represented by a constructor. They classify all IO functions \nof (Concurrent) Haskell. Each constructor has its action and if required its restore function as argument. \nThe bind operator of the IO monad is de.ned as a constructor, generating a tree structure. Additionally, \nwe add a con\u00adstructor for return. 5.1.1 ConcAction The constructor ConcAction represents all functions \nof Concurrent Haskell, except for forkIO. Naturally, we do not want our checker to suspend when for example, \nexecuting takeMVar on an empty MVar. Therefore, the .rst argument of ConcAction, the retractable action, \nis of type P.IO (Maybe a). It yields Nothing if the re\u00adtractable action would suspend. Otherwise, it \nyields Just v where v is the result after performing the retractable action. The second argument of ConcAction \nis the corresponding restore function. data IOa =... | ConcAction (P.IO (Maybe a)) (a -> P.IO ()) 5.1.2 \nSeqAction A concurrent program does not only consist of functions de.ned in the Concurrent Haskell library. \nThe SeqAction constructor is used for all the rest of the P.IO functions. data IOa =... | SeqAction (ActionType \na) (P.IO a) It has an argument of type ActionType a that is de.ned as follows. data ActionType a = NonBackTrackable \n| Ignorable (P.IO a) This argument distinguishes between two types of sequential ac\u00adtions. NonBackTrackable \nare such actions that request an in\u00adput from the user. If such an action is used in a concurrent pro\u00adgram \nthe behavior can depend on that input. Thus, the debug\u00adger cannot check every possible schedule. Nevertheless, \nwe could check for a deadlock, but every time we execute a function of type NonBackTrackable we would \nhave to ask the user for input, which does not make sense during search. Thus, if a program uses a SeqActions \nof type NonBackTrackable, we stop the check un\u00adtil this action is executed. The SeqActions of type Ignorable \nare such actions that can be ignored by the search because they are of no relevance for the dead\u00adlock \nchecking. For instance, performing output will not effect the behavior of the concurrent system. On the \nother hand, such an ac\u00adtion may not be performed during search, because the system would produce too \nmuch output. Therefore, Ignorable has an argument which de.nes the action that is executed during search \ninstead of the output. Mostly, this will be return () because we just want to do nothing, but in principle \nother actions are possible. 5.1.3 ForkAction We use the constructor ForkAction to represent the result \nof forkIO. The .rst argument is the thread that is forked by forkIO, represented by a value of type IO \n(). The second argument is the retractable action of type P.IO a. Because the result type of forkIO is \nIO C.ThreadId the polymorphic datatype a will be uni\u00ad.ed with C.ThreadId. Thus, the retractable action \nis of type IO C.ThreadId and it yields the ThreadId of the forked thread. As mentioned before a thread \nis killed by removing it from the list of all threads. Thus ForkAction takes no restore function as param\u00adeter. \nA detailed discussion of the thread list follows in Section 6. data IOa =... | ForkAction (IO ()) (P.IO \na)  5.1.4 A Monad: Bind and Return The Bind constructor glues a value of IO b and one of b->IOa together \nto a tree structure, like (>>=) from the class Monad. Be\u00adcause the result type is IO a we have to existentially \nquantify the polymorphic datatype b. This is not possible in the type system of Haskell 98, but fortunately, \nghc supports existential quanti.ca\u00adtion in data type declarations, described in [16]. Finally, we need \na constructor representing return. data IOa =... | forall b. Bind (IO b) (b -> IO a) | Return a This \nresults in the following datatype for IO: data IOa = forall b. Bind (IO b) (b -> IO a) | ConcAction (P.IO \n(Maybe a)) (a -> P.IO ()) | ForkAction (IO ()) (P.IO a) | SeqAction (ActionType a) (P.IO a) | Return \na By de.ning our IO type as an instance of class Monad, IO functions in the program can be interpreted \nas functions generating an IO tree. instance Monad IO where (>>=) = Bind return = Return  5.2 Concurrent \nRede.nition As a next step, every IO function has to be rede.ned, so that it yields a value of the new \nIO type. In this section, we discuss the rede.nition of IO functions with respect to the different kinds \nof actions, as de.ned in the new IO datatype. 5.2.1 ConcAction Retracting newEmptyMVar is simple. We \njust do nothing because the garbage collector will remove the created MVar. Thus, the re\u00adstore function \nis \\ -> return (). Executing newEmptyMVar no suspension is possible. The retractable action always yields \na new MVar packed into the constructor Just. newEmptyMVar :: IO (MVar a) newEmptyMVar = ConcAction (do \nsearchMVar <-C.newEmptyMVar return (Just searchMVar)) (\\_ -> return ()) In the case of putMVar the retractable \naction works as follows. First, it tests if the MVar is empty. If this is the case, then it per\u00adforms \nthe C.putMVar and yields Just (). Otherwise, it yields Nothing because a putMVar on a full MVar would \nsuspend. In the context of search there will be no interleaving, so that the MVar can\u00adnot be .lled in \nbetween isEmptyMVar searchMVar and putMVar searchMVar (see Section 6). The restore function empties the \nMVar by calling C.takeMVar. putMVar :: MVar a -> a -> IO () putMVar searchMVar x = ConcAction (do b <-C.isEmptyMVar \nsearchMVar if b then do C.putMVar searchMVar x return (Just ()) else return Nothing) (\\_ -> do C.takeMVar \nsearchMVar return ()) The retractable action of takeMVar is similar to the one of putMVar. If the MVar \nis empty it yields Nothing because takeMVar would suspend on an empty MVar. Otherwise, it takes the value \nout of the MVar and yields it. When we check for a dead\u00adlock we execute the retractable action and if \nthe MVar can be taken we pass the result to the restore function. The restore function puts this value \nback into the MVar. takeMVar :: MVar a -> IO a takeMVar searchMVar = ConcAction (C.takeMVar searchMVar) \n(do b <-C.isEmptyMVar searchMVar if b then return Nothing else do v <-C.takeMVar searchMVar return (Just \nv)) (\\v -> C.putMVar searchMVar v) All the other concurrency primitives are rede.ned in the same way. \n 5.2.2 ForkAction There is only one function yielding a ForkAction, namely forkIO. The .rst argument \nof ForkAction is the argument of forkIO. This is the thread of type IO that should be forked. Again, \nwe refer to Section 6 for the explanation why we do not need a re\u00adstore action in ForkAction. That section \nwill also explain why the retractable action forks a thread that immediately terminates. forkIO :: IO \n() -> IO C.ThreadId forkIO newThread = ForkAction newThread (do threadId <-C.forkIO (return ()) return \nthreadId) As we have already seen, the .rst argument of SeqAction is the ActionType: NonBackTrackable \nor Ignorable. The second ar\u00adgument is the original action which is needed for performing the action in \nthe debugger. All functions that request an input from the user, like getChar, are of type NonBackTrackable. \nThe construc\u00adtor does not take a retractable action nor a restore action because we stop the search at \nthis point. getChar :: IO Char getChar = SeqAction NonBackTrackable (P.getChar) The search processes \nan Ignorable SeqAction by executing the action de.ned by the argument of Ignorable. In the example of \nputChar we do not want to perform any output. Instead of calling the original P.putChar function we do \nnothing. Thus, the action is return (). putChar :: Char -> IO () putChar chr = SeqAction (Ignorable (return \n())) (P.putChar chr) Our debugging library provides reimplementations of all IO func\u00adtions from the standard \nlibraries, returning the new IO datatype. Thus for every SeqAction we de.ne whether it is Ignorable or \nNonBackTrackable.   6 Deadlock Checking This section attends to searching for deadlocks given an IO \ntree as de.ned above. We cannot decide whether the concurrent program terminates or not. Thus, we do \nnot know whether the checker would terminate. So we have to limit the execution depth that we check. \nTo generate various schedules the checker has to coordinate the ex\u00adecution of the forked threads. It \nhas to execute the next action of one of the forked threads, then the next action of the same or an\u00adother \nthread and so forth. Thus, we need a mechanism that allows to execute only one action of one speci.c \nthread. Because we do not execute suspending actions, it is possible to map every schedule of various \nthreads to the execution of only one new thread. For every schedule this thread just executes the same \nac\u00adtions in the same order as the original threads would. To check for a deadlock we must know whether \nall threads are dead, that is sus\u00adpended or terminated. Thus, to check whether a schedule causes a deadlock \nwe just execute a thread that represents this schedule and check if a state exists in which all threads \nare dead. Therefore, we need the information which thread executes which action. We man\u00adage the IO trees \nof the threads in a list of Threads where a Thread is de.ned as follows. type Thread = (C.ThreadId,IO \n()) We assign its unique ThreadId to every thread. Thus the checker can yield a list of these ThreadIds \nas a deadlock path of threads to be executed. They are later required for the de.nition of killThread \ntoo. This idea of using a list of threads is similar to the cooperative scheduling [4] in Hugs [14]. \nNow the de.nition of forkIO makes sense. When we fork a thread we just generate a ThreadId and add the \ncorresponding Thread to the list. The only way to generate a new ThreadId is to fork a new thread. Because \nwe only want to generate a ThreadId the forked thread immediately terminates. forkIO :: IO () -> IO C.ThreadId \nforkIO newThread = ForkAction newThread (do threadId <- C.forkIO (return ()) return threadId) 6.1 General \nChecking So far, our new IO datatype and new variants of all IO functions are de.ned. As a next step, \nwe develop the interpreter for searching through all possible schedules of the concurrent system. We \nstart the interpretation with the de.nition of the function checkThread :: Thread -> P.IO Result where \nthe Result datatype is de.ned as data Result = Suspended | Stepped (Thread,P.IO ()) | Stop | Terminate \n| Fork Thread (Thread,P.IO ()) checkThread executes the next node in the IO tree, i.e. it executes the \nnext retractable action. If the next node is a NonBackTrackable SeqAction, then the result is Stop. The \nre\u00adsult Terminate is used, when a thread terminates, i.e. the node is Return ().Ifitis a ConcAction and \nthe retractable action yields Nothing, i.e. executing the action would suspend, then the result is Suspended. \nOtherwise, the result is Stepped with the performing thread and restore action as parameter. Similarly \nto Stepped,we yield Fork for ForkActions. On top of checkThread, the function checkThreads searches for \na deadlock in a given list of Threads up to a given depth. The function checkThreads yields a value of \ntype P.IO (Maybe [C.ThreadId]). The list of ThreadIds is a schedule that causes a deadlock. It is Nothing \nif no deadlock is found. You can think of the function checkThreads as an interpreter for the IO tree. \nIt executes the associated program with every possible scheduling up to a given execution depth. The \nfunction uses a depth .rst strategy, that is it .rst executes one scheduling up to the maximum depth. \nThen it undoes the last action and executes another thread instead and so on. The signature of the function \ncheckThreads is de.ned as follows. checkThreads :: [Thread] -> Int -> Bool -> [Int] -> P.IO (Maybe [C.ThreadId]) \nThe .rst argument ([Thread]) is a list of actually forked threads. When checkThreads is called for the \n.rst time the list contains only one Thread, namely the main thread. The second argument (Int) is the \ndepth to be checked. The third argument (Bool) indi\u00adcates if all checked threads in this depth are dead, \nthat is suspended or terminated. If all Threads in a depth are dead, then there is a deadlock. The last \nargument ([Int]) says which Threads still have to be checked in this depth. The numbers are positions \nin the Thread list. We will call this list position list. The .rst rule of checkThreads processes an \nempty position list. In this case all Threads in this depth have been checked. checkThreads ts@(_:_) \n_ dead [] = do if dead then return (Just []) else return Nothing If dead is True all threads are dead \nand a deadlock is found. There\u00adfore, it yields Just []. The list is empty because we are already in a \ndeadlock state. If dead is False we yield Nothing because after checking all Threads no deadlock was \nfound and so there is no schedule to a deadlock. If the position list is non-empty checkThreads calls \nthe function checkThread with the .rst Thread to be checked. Note, that by means of the as pattern n \nis bound to the value n1-1. checkThreads ts@(_:_) n1@(n+1) dead (m:ms) = do let thread = ts!!(m-1) threadId \n= fst thread resultT <-checkThread thread case resultT of ... Depending on the result of this function \nthere are .ve cases to con\u00adtinue, which we discuss in the following: Suspended This indicates that the \nthread would suspend after the next action. In this case we just check the other Threads in this depth. \nThis is done by calling checkThreads with the remaining position list: Suspended -> checkThreads ts n1 \ndead ms Stop This result indicates that the thread would next perform a SeqAction of type NonBackTrackable. \nAs already mentioned, in this case we stop the check and yield Nothing. Stop -> return Nothing  Figure \n2. Full Computation Tree Terminate In this case we know that the thread we checked terminated. First \nthis Thread is deleted from our Thread list. Then the search is continued with the new Thread list, a \ndecreased depth and a newly initialized position list because we have deleted one Thread.If this call \nyields a deadlock path, then we add the ThreadId of the Thread that terminated to this path. Otherwise, \nwe look for a dead\u00adlock in the original Thread list, with the original depth and indicate by False that \nno deadlock was found. Terminate -> do let ts = deleteWithPos m ts checkRes <-checkThreads ts n True \n[1..length ts ] case checkRes of Just path -> return (Just (threadId:path)) Nothing -> checkThreads ts \nn1 False ms Fork If the thread has performed a forkIO, then checkThread yields Fork. The .rst argument \nof Fork is the forked Thread. The second is a tuple consisting of the modi.ed Thread that forked the \nnew one and a restore function. First the old Thread is replaced with the modi.ed one and the new Thread \nis added to the Thread list. Then we look for a path in the new Thread list with a decreased depth. The \nold state is restored by executing restoreAction.Ifa path is found the ThreadId of the executed Thread \nis added to the path. Otherwise, the search is proceeded with the old Thread list and depth. Fork newT \n(t ,restoreAction) -> do let ts = replaceWithPos m t ts ++ [newT] checkRes <-checkThreads ts n True [1..length \nts ] restoreAction case checkRes of Just path -> return (Just (threadId:path)) Nothing -> checkThreads \nts n1 False ms Stepped If the thread has executed an action that doesn t .t in one of the cases discussed \nso far, then checkThread yields Stepped. All the concurrent functions like newEmptyMVar, putMVar and \ntakeMVar and the SeqActions of type Ignorable fall in this category. The argument of Stepped is a tuple \nconsisting of the modi.ed Thread and a restore function. In this case the old Thread is replaced by the \nmodi.ed one. Then the search is continued with the new Thread list and a decreased depth. the retractable \naction performed by checkThread is un\u00ad Figure 3. Minimized Computation Tree done by calling restoreAction. \nIf a deadlock path is found the ThreadId of the executed Thread is added to the path and the search ends \nwith this result. If the call yields Nothing the search is proceeded with the old Thread list and depth. \nStepped (t ,restoreAction) -> do let ts = replaceWithPos m t ts checkRes <-checkThreads ts n True [1..length \nts ] restoreAction case checkRes of Just path -> return (Just (threadId:path)) Nothing -> checkThreads \nts n1 False ms This search for deadlocks already .nds nearby deadlocks during debugging. Indeed, we .rst \nintegrated this search into CHD and used it successfully for debugging. However, there is the possibility \nto optimize the approach with ideas from partial order reduction in model checking [5]. Hence, we will \n.rst discuss this optimization, before we describe the integration into CHD in Section 7.  6.2 Search \nSpace Reduction To motivate the reduction of the search space, we consider a con\u00adcurrent program with \nthree threads, where the actions of all threads are independent from each other, e.g. all threads work \non disjoint sets of MVars. We search for deadlocks in that program with a maxi\u00admum depth of 3. Figure \n2 shows the structure of a tree representing the computation of checkThreads. Each node represents a \nstate of the checked program and each edge the execution of one action. The numbers at the edges indicate \nwhich Thread is executed in this step. Because the three threads are independent from each other, some \npaths are equivalent with respect to searching for a deadlock. All paths that execute the same multi-set \nof threads are equivalent. For instance, the path 2-1-1 is equivalent to the path 1-2-1. To .nd a deadlock \nin this case it would be suf.cient to check each multi-set of executions just once. Because the number \nof states to check is exponential in the depth we use this insight to minimize the number of states to \nbe checked. We are searching for a minimized tree that is equivalent with respect to detecting deadlocks \nto the one shown in Figure 2. Every path of that tree must represent a unique multi-set of executed threads \nand every multi-set of executed threads may only be represented by one path. Figure 3 shows a tree that \nful.lls this task. For example, the multi-set of executed threads {2,1,1} is represented only once in \nthe tree by the path 1-1-2. In every state we only check the positions in the thread list that are less \nor equal to the current position. We just have to change the calls of checkThreads that go down the tree. \nInstead of checking the whole position list from 1 to the length of the Thread list we have to check \nonly the positions from 1 to the position of the current Thread. Because only in this very arti.cal example \nthe actions of all threads are independent from each other there is some work to do. For ex\u00adample, if \nthe next action of two threads, say 1 and 2, are dependent on each other (e.g. because they perform takeMvar \non the same MVar) we have to check both execution orders, 1-2 and 2-1. We de.ne that two actions are \ndependent on each other if they work on the same communication abstraction, that is the same MVar, Chan \nand so forth. Otherwise, they are independent. Therefore, we add an argument to the ConcAction constructor \nof the IO datatype that gives us the communication abstraction on which the action operates. data IOa \n=... | ConcAction ComAbstr (P.IO (Maybe a)) (a -> P.IO ()) | ... In the type ComAbstrs all MVars, Chans, \nQSems, QSemNs and SampleVars are subsumed and every ComAbstr obtains a unique identi.er. Now it is possible \nto write a function that yields all threads that operate on the same ComAbstr as the current thread. \nWe add the positions of all Threads that operate on the same ComAbstr as the current one to the position \nlist. The function dependentOn takes a ComAbstr and a Thread list and yields all positions in that list \nthat operate on this ComAbstr in the next step. Because we do not want to check a Thread twice, we delete \nall positions that are already in the position list. This is done by .ltering only the positions that \nare greater than m: checkThreads ts@(_:_) n1@(n+1) dead (m:ms) = do let thread = ts!!(m-1) threadId = \nfst thread nextComAbs = nextComAbstr (snd thread) dependList = dependentOn nextComAbs ts list = filter \n(>m) dependList resultT <-checkThread thread case resultT of ... Again we discuss the different cases \nfor the result of the call to checkThread: Suspended/Stop In both cases there are no changes because \nthe search stops or just the rest of the position list has to be checked. Terminate In the case of Terminate \nwe just check all threads that have a num\u00adber less than the one of the current thread. We do not check \nthe m-th Thread because it just terminated and we have deleted it from the list. We additionally check \nall Threads that are dependent on the current one. Terminate -> do let ts = deleteWithPos m ts checkRes \n<-checkThreads ts n True ([1..m-1]++list) case checkRes of Just path -> return (Just (threadId:path)) \nNothing -> checkThreads ts n1 False ms Fork When we fork a thread we do not know which actions it will \nper\u00adform. Thus, we do not know which other threads are dependent on the actions of the new thread. We \nhave to assume that the ForkAction is dependent on all other threads and we still check all Threads in \nthis case. Thus there is no change in this case. Stepped The case of Stepped completes the changes in \nthe program code. It is similar to the Terminate case except that we do not delete the thread but replace \nit with the modi.ed one. We only check the threads with positions that are less or equal to the current \nposition and add the list of the dependent thread positions. Stepped (t ,restoreAction) -> do let ts \n= replaceWithPos m t ts checkRes <-checkThreads ts n True ([1..m]++list) restoreAction case checkRes \nof Just path -> return (Just (threadId:path)) Nothing -> checkThreads ts n1 False ms With this reduction \nthe search space is decreased noticeably, as shown in Figure 3. Unfortunately, we can not .nd deadlocks \nany\u00admore: As mentioned above, if the list of thread positions to be checked is empty, then we test for \na deadlock as follows: checkThreads ts@(_:_) _ dead [] = do if dead then return (Just []) else return \nNothing Because we now do not check all threads in a depth we do not know if all threads are dead. Therefore, \nwe have to keep in mind which threads we have checked. In the case that the list of positions is empty \nwe have to check all threads that we have not checked so far. We do this by calling the function checkSuspended: \ncheckSuspended :: [Thread] -> [Int] -> P.IO Bool checkSuspended _ [] = return True checkSuspended ts \n(m:ms) = do let thread = ts!!m resultT <-checkThread thread case resultT of Terminate -> checkSuspended \nts ms (1,[]) (2,[]) (3,[]) (1,[3]) (2,[]) (3,[]) (2,[]) (1,[2]) (1,[3])  (1,[]) (2,[3]) (1,[3]) 2 \n   Figure 4. Three Dining Philosophers Suspended -> checkSuspended ts ms Stepped (_,restoreAction) \n-> do restoreAction return False Fork _ (_,restoreAction) -> do restoreAction return False Kill _ (_,restoreAction) \n-> do restoreAction return False Stop -> return False checkSuspended takes a Thread list and a position \nlist pointing to the threads to be checked for detecting the deadlock. It starts with the .rst Thread \nposition and checks whether the thread is dead or not. If the next action is a Stepped, Fork, Kill or \nStop then we just yield False (non deadlock) after performing the restore action if necessary. In the \ncase that the next action is Terminate or Suspended, we check the remaining Threads. A check of the empty \nlist yields True. Figure 4 shows a tree representing the computation of the checkThreads function when \nchecking a system of three dining philosophers, as de.ned in Section 2.3. The root node of the tree belongs \nto a system state in which all philosopher threads are al\u00adready forked. The numbers within the nodes \nrepresent the states of the three Threads. For example, if the .rst number in a node is 2, then this \nmeans that Thread 1 will next operate on MVar 2. The empty nodes represent states checked by checkSuspended. \nThe edges are labeled with tuples. The .rst component indicates which Thread is executed in this path. \nThe second component is the list of positions that is additionally added in that speci.c call of checkThreads. \nThe dashed edges are part of the deadlock path.  7 Integration in CHD This section will give some information \nabout our implementation of the presented approach on top of CHD. Additionally to the retractable action \nand the restore function which are executed during the check, we add a third argument for actions to \nConcAction and ForkAction. These actions are similar to the retractable actions but do not check whether \nthe execution of this action would suspend. Thus, in the case of ConcAction their type is not IO (Maybe \na) but IO a. Furthermore, these actions use the functions provided by CHD instead of the original Concurrent \nHaskell functions. A function interprets the IO tree by executing these actions like the normal program. \nBecause we use the func\u00adtions de.ned in CHD we get the control of the scheduling of the program. The \nCHD blocks every function and allows the user to decide which thread to unblock. Every time CHD blocks \nthe func\u00adtion checkThreads is called with the current Thread list. Thus, the resulting program works \nlike CHD but searches for deadlocks in the background. The screenshot in Figure 1 shows the debugger \napplied to the dining philosophers problem, introduced in Section 2.3. The black circle marks the thread \nthe execution should be continued with (by click\u00ading this thread) to reach the found deadlock (here Thread5). \nIf you allow this thread to continue the next thread of the deadlock path will be marked. If you select \nanother Thread the program uses it\u00aderative deepening to .nd a new deadlock path based on the new Thread \nlist. The program increases the depth until a deadlock is found or the search tree contains more states \nthan a maximum se\u00adlectable by the user, initially 50,000 states. If the user unblocks a thread before \ncheckThreads has terminated, checkThreads is signaled to stop. It restores the state before the check \nand yields Nothing. The selected thread is unblocked, performs a step, and then a new check is started \nwith the new Thread list. Now we explain which modi.cations in the Concurrent Haskell program have to \nbe made, to apply our debugger. First, we have to make some changes in our program. Instead of importing \nthe original module Concurrent we import our new debugger import ConcurrentSearch Because the prelude \nis always imported we have to hide its IO datatype and all functions using it. import Prelude hiding \n(IO,putStr,getLine,... ) Furthermore, we import the prelude quali.ed to access the original IO monad. \nimport Prelude qualified as P The type of the main function of our program is IO now instead of P.IO. \nThus, we rename the original main function of the program to stepperMain. Then we add a new main function \nwhich just calls the function startCheck that takes a value of type IO and starts the check with this \nvalue. main :: P.IO () main = startCheck stepperMain Now the main function has the proper type P.IO (). \nWe have also implemented a preprocessor which takes a program and makes all these changes automatically \nfor the program and all imported modules. Furthermore, it adds position informations to every Con\u00adcurrent \nHaskell function. This information is later used by CHD to present the user actual source code positions \nof the threads. Additionally to the functions presented in this paper we have re\u00adde.ned the remaining \n(Concurrent) Haskell IO functions. Thus, the presented IO type contains an extra constructor namely KillAction \nfor the rede.nition of the function killThread of the Concurrent Haskell library. Furthermore, we have \nrede.ned all functions on QSems, QSemNs, SampleVars, Chans and the IO func\u00adtions de.ned in the System.IO \nmodules of the ghc. Because of the modular design of the debugger it is easy to extend it with further \nconcurrency primitives or to hide the concurrency primitives behind your own complex communication abstractions. \n 8 Practical Application We have not evaluated our tool in a large case study yet but to show the practical \nusability we applied it to different versions of the din\u00ading philosopher problem. First, we used it for \na system of ten din\u00ading philosophers. Already in the initial con.guration, before the philosophers are \nforked and the sticks are created, our debugger is able to .nd the deadlock and guide the user through \nmore than 40 steps into the deadlock con.guration. This was surprising to us, since initially, we were \nonly interested in .nding deadlocks very close to the actual con.guration. As a simulation of a more \ncomplex application, we rede.ned the application such that the sticks are administrated by a server. \nIn this system, the initial con.guration is too far away from the deadlock position and the search space \ntoo large to .nd a deadlock. The same holds even for some later states of the system. However, testing \nshowed that it is very probable to reach a con.guration in which the debugger is able to detect a deadlock \npath with about 22 steps. For larger applications, we expect our approach to scale well for the following \nreasons: If the path to a deadlock is too long, then the user can still debug the program by using CHD \nfeatures like stepwise ex\u00adecution. Search stops if the state space contains more then a speci.ed number \nof states.  The search space is independent of the number of used com\u00admunication abstractions. Only \nthe number of active (not sus\u00adpended) threads is relevant.  The degree of dependent concurrent actions \nis usually low. Communication is usually only performed between a small amount of threads. Hence, it \nis suf.cient to consider all pos\u00adsible interleavings for only these threads and partial order re\u00adduction \nshould reduce the state space ef.ciently, as known from using it for improving model checking. Furthermore, \nin larger applications many threads are waiting for the results of other applications, e.g. client threads \nwait for results from a server thread. These threads do not effect the search space until the server \nproduces the result. Then the server is usu\u00adally suspended until another client submits a request. Hence, \nparts of the system are sequentialized and the state space is not blown up.  If the system waits for \nexternal input, e.g. from the keyboard or via a TCP socket, then the existence of a deadlock may depend \non the concrete input. Hence, search does not make sense in this case and as long as one thread waits \nfor input, it is not possible to detect a deadlock in the system. The same problem holds for formal veri.cation. \nHere search is done with respect to all possible inputs which is either an abstrac\u00adtion of the real program \nbehavior (and may contain mistakes) or results in a state space explosion making veri.cation im\u00adpossible. \n However, it would make sense that the user can specify sub\u00adsets of threads which should not suspend \naltogether in one system state. This could give a hint to partial deadlocks, al\u00adthough it is very dif.cult \nto identify partial deadlocks, since other threads may later communicate with one of these threads and \nreactivate these suspended threads.  A slowdown during debugging is usually no problem for con\u00adcurrent \nsystems, since these systems in most cases do not per\u00adform complex computations. Instead, concurrency \nis used as a means for guaranteeing reactivity of a system. Larger ini\u00ad  tializations or computations \ncan be skipped during debugging by introducing break points in the source code. No search is performed \nuntil the break point is reached. However, using our debugger consumes memory, especially stack space \nsince we perform backtracking, while debug\u00adging. Using ghc this is no problem, since stack space can \nbe increased when starting the application. For example, for debugging the server based implementation \nof the dining philosophers, 5 MB stack space is suf.cient. 9 Related Work To the best of our knowledge, \nthere is no similar approach to im\u00adproving debugging of concurrent systems. The only work related to \nours, is Java MultiPathExplorer (JMPaX) [1, 20], which also ex\u00adtracts properties of other schedules during \nthe execution of concur\u00adrent Java programs. However, they only consider one execution of the system, \nand verify properties for all schedules obtained by switching independent actions. These are exactly \nthe schedules, we are not interested in, because they contain a deadlock if and only if the execution \nyields into a deadlock. However, they are not inter\u00adested in detecting deadlocks but in other unexpected \ncon.gurations which (in most cases only) might yield into e.g. a deadlock. For instance, they consider \nlock reversals [15] which means two locks (in our context usually MVars) are once taken in one order \nand later they are taken in the other order. We think, that our approach would also improve their Tool. \nHowever, applying our approach to Java is dif.cult, since Javas sequence operator (;) is a language feature \nwhich cannot be rede.ned like the function (>>=) in Haskell. A modi.cation of Javas virtual machine would \nbe necessary, imple\u00admenting the presented search on the level of Java byte code. Our work is also related \nto the area of software model checking where formal veri.cation of system implementations is proposed \nby applying model checkers, like SPIN [12], to concurrent pro\u00adgramming languages. For instance, Java \nPath.nder [10] and the Bandera Tool [6] translate concurrent Java programs into Promela, the speci.cation \nlanguage of SPIN. System properties can be de\u00ad.ned in a temporal logic and then be checked using SPINs \nmodel checker which uses a similar partial order reduction as presented in this paper. To avoid state \nspace explosion and to make the ap\u00adproach applicable for in.nite state systems, the user can also de.ne \ndata abstractions. However, .nding good data abstractions can be dif.cult and error-prone. Although these \nworks are considered for software veri.cation, they are only applied to toy examples in the case studies. \nThe veri.cation of real concurrent systems still seems to fail because of the state space explosion problem. \nAs a conse\u00adquence, the developers of Java Path.nder started the development of Java PathExplorer [9], \nwhich performs runtime model checking for concurrent Java programs. The goal is not formal veri.cation \nany more but applying the formal techniques of specifying system properties in temporal logics and model \nchecking for more power\u00adful testing. The further development of Java PathExplorer resulted in Java MultiPathExplorer, \ndiscussed in the last paragraph. Other work related to this approach to debugging are debuggers for sequential \nHaskell. First, we should mention the Haskell Object Observation Debugger HOOD [8] which is also integrated \nin ghc. In this system the user is able to observe the evaluation of interme\u00addiate data structures without \nin.uencing lazy evaluation. However, this approach does not help searching for deadlocks, since usually, \ndeadlocks do not depend on the values but the order of taking and putting values to resp. from a communication \nabstraction. For in\u00adstance, in the philosophers examples, the value () will not be eval\u00aduated at all, \nbecause there is no pattern matching on it. Hence, debugging by observation cannot help. Other approaches \nto debug\u00adging Haskell programs are the algorithmic debugger Freja [17] and the Redex Trail System [21] \nwhich was later extended to Hat [3] covering the ideas of HOOD and Freja as well. However, Freja and \nHat are not even applicable to programs written in Concurrent Haskell. Furthermore, these approaches \ndo not fully support the IO monad and reactive, non-terminating functions. There are also many debuggers \nfor non-functional languages sup\u00adporting concurrency. However, these debuggers usually work with a modi.ed \ncompilation/abstract machine and are hence, not compa\u00adrable with our lightweight approach as a library. \nFurthermore, there is no approach providing search for deadlocks available. 10 Conclusions and Future \nWork We have presented a new approach for improving debugging of Concurrent Haskell programs. The system \nis conveniently in\u00adtegrated in the Concurrent Haskell Debugger. The basic idea is searching for deadlocks, \nwhenever the computation is idle, while the user steps through the system states in the debugger. If \na deadlock is found, then the user is guided into this dead\u00adlock. We have presented the basic ideas of \nthe implementation of the search and an optimization based on partial order reduc\u00adtion. Furthermore, \nwe sketched the integration of the search into CHD. We have mostly restricted the presentation to MVars, \nbut all concurrency primitive of Concurrent Haskell are provided as well as all common IO functions. \nThe system is available at http://www.informatik.uni-kiel.de/~fhu/chd. We implemented the search for \ndeadlocks for Concurrent Haskell. However, a similar approach should also be useful for debuggers for \nother programming languages providing concurrency, although in most cases such a lightweight, library \nbased implementation will not be possible. The main opportunity of Concurrent Haskell is the possibility \nto rede.ne the sequence operator (>>=), which is not possible for the imperative sequence operator (;). \nHere different so\u00adlutions on the level of code generation or more complex debugging engines have to be \nfound. Further opportunities of (Concurrent) Haskell which would also be useful for transferring the \napproach to other languages are communication abstractions, that make an identi.cation of inter-thread \ncommunication possible so that all schedules with respect to these communication actions can be searched, \n the possibility of rede.ning the communication functions,  being able to retract concurrent actions, \nand  having an expressive type system which allows the de.nition of IO actions as data structures, like \nexistential quanti.cation in data type declarations [16].  The idea of the unoptimized search works \nindependently of the syn\u00adchronization and communication concepts of the language since all possible schedulings \nare performed and retracted during backtrack\u00ading. However, it is important to reduce the possible thread \nswitches to selected positions during the computation by identifying inter\u00adthread communication. For \nexample, this could be a problem, when threads communicate in an unsynchronized way via shared mem\u00adory, \nlike it is possible in Java. Fortunately, the same problems have to be solved in the veri.cation tools \nfor Java. Hence, these results should help transferring our approach to languages with a less ade\u00adquate \ncommunication concept. The application of partial order reduction should be applicable to other synchronization \nand communication concepts as well, but this optimization depends on the identi.cation of independent \nactions, which can be more complicated for other concepts. For future work, we have different directions \nin mind. At the mo\u00adment the implemented search is only considering deadlocks. How\u00adever, programmers are \nalso interested in other bugs like partial deadlocks, lifelocks and not guaranteed mutual exclusion. \nFor .nd\u00ading other bugs, we developed an environment for checking Linear Time Logic (LTL) formulas while \nexecuting a Concurrent Haskell program [13]. We want to integrate both approaches to improve the search. \nHowever, using search with integrated LTL run-time check\u00ading is not as simple as searching for deadlocks. \nAdditional effort of the programmer is needed to specify properties of the system in LTL and annotate \nthe program by state propositions. On the other hand, we showed that some .xed properties (in [13] we \npresented lock reversal) can easily be integrated in this approach. We want to .nd more common bugs and \nintegrate this into this extended search. Another direction for future work, is a larger case study. \nThis should give hints for improving our debugger, especially adding different views. We also want to \ncheck whether a more precise distinction of independent actions will improve the search. At the moment \nwe distinguish them only by the communication abstrac\u00adtion they operate on. As an improvement, we could \nalso distinguish the different operations. For example, two isEmptyMVar actions performed on the same \nMVar are independent as well. However, being more precise yields additional costs and it is not clear \nthat this will really allow deeper search in practice. Finally, we will investigate, if a combination \nof our debugger with Hood could be useful. Although the values in the communication abstractions are \nnot directly relevant for deadlocks, it can be interesting to view the values inside a concurrency abstraction \n(and perhaps also how much they are evaluated). At the moment the programmer can only label these values \nby hand e.g. using putMVarLabel instead of putMVar, but an automatic labeling by means of Hood could \nbe more convenient. 11 References [1] Gul Agha, Grigore Rosu, and Koushik Sen. Runtime safety analysis \nof multithreaded programs. In Proceedings of the 9th European software engineering conference held jointly \nwith 10th ACM SIGSOFT international symposium on Foun\u00addations of software engineering, pages 337 346. \nACM Press, 2003. [2] T. B\u00a8ottcher and F. Huch. A Debugger for Concurrent Haskell. In Proceedings of the \n14th International Workshop on the Implementation of Functional Languages, Madrid, Spain, September 2002. \n[3] Olaf Chitil, Colin Runciman, and Malcolm Wallace. Freja, Hat and Hood A comparative evaluation of \nthree systems for tracing and debugging lazy functional programs. In Pro\u00adceedings of the 12th International \nWorkshop on the Imple\u00admentation of Functional Languages, volume 2011 of Lecture Notes in Computer Science, \npages 176 193, 2001. [4] Koen Claessen. A poor man s concurrency monad. Journal of Functional Programming, \n9(3):313 323, 1999. [5] Edmund M. Clarke, Orna Grumberg, Marius Minea, and Doron Peled. State space reduction \nusing partial order tech\u00adniques. Software Tools for Technology Transfer, 2, 1998. [6] James Corbett, \nMatthew Dwyer, John Hatcliff, Corina Pasare\u00adanu, Robby, Shawn Laubach, and Hongjun Zheng. Ban\u00addera: Extracting \n.nite-state models from Java source code. In 22nd International Conference on Software Engineering, pages \n439 448, Limerick, Ireland, June 2000. IEEE Com\u00adputer Society. [7] The Glasgow Haskell compiler. http://www.haskell. \norg/ghc/. [8] Andy Gill. Debugging Haskell by observing intermediate data structures. In Proceedings \nof the 2000 ACM SIGPLAN Haskell Workshop, volume 41-1 of ENTCS. Elsevier, Septem\u00adber 2000. [9] K. Havelund \nand G. Rosu. Java PathExplorer A runtime veri.cation tool. In In Proceedings 6th International Sym\u00adposium \non Arti.cial Intelligence, Robotics and Automation in Space, ISAIRAS 01, Montreal, Canada, June 18 22, \n2001., 2001. [10] Klaus Havelund and Thomas Pressburger. Model checking Java programs using Java PathFinder. \nInternational Journal on Software Tools for Technology Transfer, 2(4), April 1998. [11] Ralf Hinze. Deriving \nbacktracking monad transformers. ACM SIGPLAN Notices, 35(9):186 197, 2000. [12] Gerard J. Holzmann. Proving \nproperties of concurrent sys\u00adtems with SPIN. In Proceedings of the Sixth International Conference on \nConcurrency Theory, volume 962 of Lecture Notes in Computer Sience, pages 453 455, 1995. [13] F. Huch \nand V. Stolz. Runtime veri.cation of Concurrent Haskell programs. In Proceedings of the Fourth Workshop \non Runtime Veri.cation, to appear in ENTCS. Elsevier Science Publishers, 2004. [14] The Haskell interpreter \nHugs. http://www.haskell.org/ hugs/. [15] K.Havelund. Using Runtime Analysis to Guide Model Check\u00ading \nof Java Programs. In W.Visser, K.Havelund, G.Brat, and S.Park, editors, SPIN Model Checking and Software \nVeri.\u00adcation (7th International SPIN Workshop), volume 1885 of Lecture Notes in Computer Science, Stanford, \nCA, USA, Au\u00adgust/September 2000. Springer. [16] K. L\u00a8aufer. Type classes with existential types. J. of \nFunctional Programming, 6(3):485 517, May 1996. [17] Henrik Nilsson and Jan Sparud. The evaluation dependence \ntree as a basis for lazy functional debugging. Automated Soft\u00adware Engineering, 4(2):121 150, April 1997. \n[18] Simon Peyton Jones et al. Haskell 98 report. Technical report, http://www.haskell.org, 1998. [19] \nSimon Peyton Jones, Andrew Gordon, and Sigbjorn Finne. Concurrent Haskell. In Conference Record of POPL \n96: The 23rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, pages 295 308, St. \nPetersburg Beach, Florida, 21 24 January 1996. [20] Grigore Rosu and Koushik Sen. An instrumentation \ntechnique for online analysis of multithreaded programs. In to appear as invited Paper at Parallel and \nDistributed Systems: Testing and Debugging (PADTAD-2), 2004. [21] Jan Sparud and Colin Runciman. Complete \nand partial redex trails of functional computations. In Proceedings of the 9th International Workshop \non the Implementation of Functional Languages, volume 1467 of Lecture Notes in Computer Sci\u00adence, pages \n160 174, 1998.   \n\t\t\t", "proc_id": "1016850", "abstract": "This paper presents an approach to searching for deadlocks in Concurrent Haskell programs. The search is based on a redefinition of the IO monad which allows the reversal of Concurrent Haskells concurrency primitives. Hence, it is possible to implement this search by a backtracking algorithm checking all possible schedules of the system. It is integrated in the Concurrent Haskell Debugger (CHD), and automatically searches for deadlocks in the background while debugging. The tool is easy to use and the small modifications of the source program are done by a preprocessor. In the tool we use iterative deepening as search strategy which quickly detects deadlocks close to the actual system configuration and utilizes idle time during debugging at the best.", "authors": [{"name": "Jan Christiansen", "author_profile_id": "81100396528", "affiliation": "Christian-Albrechts-University of Kiel, Kiel, Germany", "person_id": "P693061", "email_address": "", "orcid_id": ""}, {"name": "Frank Huch", "author_profile_id": "81100099102", "affiliation": "Christian-Albrechts-University of Kiel, Kiel, Germany", "person_id": "PP37023706", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1016850.1016858", "year": "2004", "article_id": "1016858", "conference": "ICFP", "title": "Searching for deadlocks while debugging concurrent haskell programs", "url": "http://dl.acm.org/citation.cfm?id=1016858"}