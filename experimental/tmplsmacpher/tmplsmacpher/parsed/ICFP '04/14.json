{"article_publication_date": "09-19-2004", "fulltext": "\n Veri.cation of Safety Properties for Concurrent Assembly Code. Dachuan Yu and Zhong Shao Department \nof Computer Science, Yale University New Haven, CT 06520-8285, U.S.A. {yu, shao}@ cs.yale.edu Abstract \nConcurrency, as a useful feature of many modern programming lan\u00adguages and systems, is generally hard \nto reason about. Although existing work has explored the veri.cation of concurrent programs using high-level \nlanguages and calculi, the veri.cation of concur\u00adrent assembly code remains an open problem, largely \ndue to the lack of abstraction at a low-level. Nevertheless, it is sometimes neces\u00adsary to reason about \nassembly code or machine executables so as to achieve higher assurance. In this paper, we propose a logic-based \ntype system for the static veri.cation of concurrent assembly programs, applying the invari\u00adance proof \ntechnique for verifying general safety properties and the assume-guarantee paradigm for decomposition. \nIn particular, we introduce a notion of local guarantee for the thread-modular veri.cation in a non-preemptive \nsetting. Our system is fully mechanized. Its soundness has been veri.ed using the Coq proof assistant. \nA safety proof of a program is semi\u00adautomatically constructed with help of Coq, allowing the veri.ca\u00adtion \nof even undecidable safety properties. We demonstrate the us\u00adage of our system using three examples, \naddressing mutual exclu\u00adsion, deadlock freedom, and partial correctness respectively. Categories and \nSubject Descriptors F.3.1 [Logics and Meanings of Programs]: Specifying and Verifying and Reasoning about \nPrograms assertions, invariants, mechanical veri.cation; D.2.4 [Software Engineering]: Soft\u00adware/Program \nVeri.cation correctness proofs, formal methods; D.3.1 [Programming Languages]: Formal De.nitions and \nThe\u00adory *This research is based on work supported in part by DARPA OA-SIS grant F30602-99-1-0519, NSF \nITR grant CCR-0081590, and NSF grant CCR-0208618. Any opinions, .ndings, and conclusions contained in \nthis document are those of the authors and do not re\u00ad.ect the views of these agencies. Permission to \nmake digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. ICFP 04, September \n19 22, 2004, Snowbird, Utah, USA. Copyright 2004 ACM 1-58113-905-5/04/0009 ...$5.00  General Terms Languages, \nVeri.cation Keywords Assembly, concurrency, local guarantee 1 Introduction The Verifying Compiler, as \na Grand Challenge [20] to the pro\u00adgramming languages community, solicits a programming frame\u00adwork which \nguarantees the correctness of a program before run\u00adning it. The criterion of correctness ranges from \ntype-safety, which has been widely applied in programming practice, to total correct\u00adness of critical \ncomponents, whose application is often limited by an evaluation of the cost and bene.ts of accurate and \ncomplete for\u00admalization. Whereas it is dif.cult to ensure correctness for sequential systems, it is even \nmore so for concurrent systems due to the interaction and interference between multiple threads. Much \nexisting work has explored the veri.cation of concurrent programs using high\u00adlevel languages and calculi \n(e.g., CSP [19], CCS [26], CML [37], TLA [24]). Unfortunately, the veri.cation of concurrent assembly \ncode remains an open problem. On the one hand, the low-level ab\u00adstraction is arduous to work with; On \nthe other hand, the extreme .exibility offered by assembly languages is tough to manage. Nevertheless, \nany high-level program or computation must be trans\u00adlated before it is carried out on actual machines, \nand the translation process in practice is invariably anything but trivial. It is at least suspectable \nthat the guarantee established at the high-level may be invalidated by tricky compilation and optimization \nbugs. Therefore, it is not only useful, but sometimes necessary to reason about as\u00adsembly code or machine \nexecutable directly so as to achieve higher assurance. Furthermore, certain critical applications, such \nas core system libraries and embedded software, are sometimes directly written in assembly code for ef.ciency \nand expressiveness; their veri.cation should not be overlooked. Our previous work [46, 47] investigated \na logic-based approach for verifying assembly code. We demonstrated that a simple low-level language \nbased on Hoare logic [17, 18], namely a language for certi.ed assembly programming (CAP), can be used \nto certify more than type safety; and with help of a proof assistant, semi-automatic proof construction \nis not unduly dif.cult, allowing the veri.cation of even undecidable program properties. Moreover, the \nmachine\u00adcheckable proofs for the safety of the assembly code directly en\u00ad  (Program)]::=(q,\u00a7,I) (CodeHeap)q::={f.I}* \n(State)\u00a7::=(J,l) (Heap)J::={1.w}* (RegFile)l::={r.w}* ::={rk}kE{0...7} (Register)r (Labels)f,1::=n(nat \nnums) (WordVal)w::=n(nat nums) (InstrSeq)I::=;IIjd f (Command)::=add rd ,rs,rt Imovi rd ,wIbgt rs,rt \n,f Ild rd ,rs(w)Ist rd (w),rs (CdHpSpec). ::={f.a}* (Assert)aEState-Prop Figure 1. Syntax and veri.cation \nconstructs of Simple CAP. able the encapsulation of veri.ed CAP programs as Foundational Proof-Carrying \nCode (FPCC) [4, 15] packages. In this paper, we extend the previous work on CAP to the domain of concurrent \ncomputation, presenting a concurrent language for certi.ed assembly programming (CCAP). The computation \nlayer ports the non-deterministic interleaved execution of two threads. The type layer of CCAP, as is \nthe case of CAP, engages the cal\u00adculus of inductive constructions (CiC) [35] to support essentially higher-order \npredicate logic reasonings. To effectively model safety properties and the interaction between threads, \nwe adapt and apply established approaches for reasoning about (high-level) concurrent programs, namely \nthe invariance proof technique [24] and the assume-guarantee paradigm [27, 23]. The thread control in \nCCAP is non-preemptive a thread s execu\u00adtion will not be interrupted until it explicitly yields control. \nThis non-preemptive setting is very useful in clearly separating thread in\u00adteraction from other orthogonal \nissues of the veri.cation of assem\u00adbly programs. Furthermore, besides facilitating simpler presenta\u00adtion \nand allowing easier understanding, the non-preemptive setting is also more fundamental, because even \npreemptive thread controls are essentially implemented with help of non-preemption, such as the disabling \nof interrupts. Non-preemption also introduces extra dif.culties which have not been addressed by previous \nresearches that assume preemption. Since the thread control will not be interrupted arbitrarily, programs \ncan be written in a more .exible manner (for most safety proper\u00adties, a program that works correctly \nin a preemptive setting will also work correctly in a non-preemptive setting, but not vice versa). In \nthis paper, we generalize the assume-guarantee paradigm by in\u00adtroducing a notion of local guarantee to \naccommodate such extra .exibility. It is worth noting that CCAP is strictly more expressive than CAP \nit is a generalization to handle more problems in practice, rather than a specialization for a particular \nproblem domain. We also wish to point out that generating FPCC packages for CCAP is as easy as that for \nCAP, although a detailed account of which is omitted due to space constraints. We have developed the \nlanguage CCAP and proved its soundness using the Coq proof assistant [42]. The implementation in Coq \nis available for download [43]. We illustrate the usage of CCAP by discussing three example programs, \nwhose safety properties of  (q,(J,l),I) >-]where if I= then ]= jd f (q,(J,l),I )where q(f)=I add rd \n,rs,rt ;I (q,(J,l{rd .l(rs)l(rt )}),I ) movi rd ,w;I (q,(J,l{rd .w}),I ) bgt rs,rt ,f;I (q,(J,l),I )when \nl(rs):l(rt ); and (q,(J,l),q(f))when l(rs)>l(rt ) ld rd ,rs(w);I (q,(J,l{rd .J(l(rs)w)}),I ) where (l(rs)w)Edom(J) \nst rd (w),rs;I (q,(J{l(rd )w.l(rs)},l),I ) where (l(rd )w)Edom(J) Figure 2. Operational semantics of \nSimple CAP. concern are mutual exclusion, deadlock freedom, and partial cor\u00adrectness respectively. For \nease of understanding, we present the central idea by modeling a simpli.ed abstract machine, and give \na generalized account in the appendix. 2 Background In this section, we review some key techniques used \nin the design of CCAP. Section 2.1 presents a simpli.ed CAP in a nutshell. Sec\u00adtion 2.2 discusses informally \nthe technique of invariance proof for proving safety properties, along with its connection with the syn\u00adtactic \napproach to type soundness [44]. A brief introduction to the assume-guarantee paradigm is given in Section \n2.3. 2.1 Simple CAP As suggested by its name (a language for assembly programming), CAP is an assembly \nlanguage designed for writ\u00ading certi.ed programs , i.e., programs together with their formal safety proofs. \nHere we introduce Simple CAP with a limited in\u00adstruction set for ease of understanding. In particular, \nwe omit the support for higher-order code pointers (realized by an indirect jump instruction in the original \nCAP), instead discussing its handling in Section 6.3. This language is best learned in two steps. Step \none puts aside the certifying part and focus on a generic assembly language (see the upper part of Figure \n1). A complete program consists of a code heap, a dynamic state component made up of the register .le \nand data heap, and an instruction sequence. The instruction set is mini\u00admal but extensions are straightforward. \nThe register .le is made up of 8 registers and the date heap is potentially in.nite. The opera\u00adtional \nsemantics of this language (Figure 2) should pose no surprise. Note that it is illegal to access a heap \nlocation (label) which does not exist, in which case the execution gets stuck. In step two, we equip \nthis language with a construct . (Code Heap Speci.cation) for expressing user-de.ned safety requirements \nin Hoare-logic style (see the lower part of Figure 1). A code heap speci.cation associates every code \nlabel with an assertion, with the intention that the precondition of a code block is de\u00adscribed by the \ncorresponding assertion. CAP programs are writ\u00adten in continuation-passing style because there are no \ninstructions directly in correspondence with the calling and returning in a high\u00adlevel language. Hence \npostconditions in Hoare logic do not have an explicit counterpart in CAP; they are interpreted as preconditions \nof the continuations. To check the validity of these assertions mechanically, we im\u00ad  . iq. i{a}I(a\u00a7) \n(PROG) . i{a}(q,\u00a7,I) . ={f1 .a1 ...fn .an} . i{ai}Ii Vi E{1...n} (CODEHEAP) . i{f1 .I1...fn .In} V\u00a7.a\u00a7=a1 \n\u00a7where .(f)=a1 (JD) . i{a}jd f VJ.Vl.a(J,l)=a(J,l{rd .l(rs)l(rt )}) . i{a}I (ADD) . i{a}add rd ,rs,rt \n;I Figure 3. Sample inference rules of Simple CAP. plement all CAP language constructs and their semantics \nusing the calculus of inductive constructions (CiC) [42, 35], which is a calculus that corresponds to \na variant of higher-order predicate logic via the formulae-as-types principle (Curry-Howard isomor\u00adphism \n[22]). We further de.ne our assertion language (Assert) to contain any CiC terms which have type State \n-Prop, where Prop is a CiC sort corresponding to logical propositions, and the various syntactic categories \nof the assembly language (such as State) have been encoded using inductive de.nitions. Although CiC is \nunconventional to most programmers, we implement the CAP language using the Coq proof assistant [42] \nwhich implicitly handles the formulae-as-types principle and hence programmers can reason as if they \nwere directly using higher-order predicate logic. (Logic is something a programmer has to learn if formal \nreasoning is desired. Higher-order logic appears to be fairly close to what people usually use to reason.) \nFor example, a precondition specifying that the registers r1, r2 and r3 store the same value is written \nin Coq as the assertion (state predicate) [s:State]1et(H,R)=sinR(r1)=R(r2)/\\R(r2)=R(r3) where [s:State]is \na binding of variable sof type State, 1etis a pattern matching construct, and /\\is the logical connective \nA. Proving the validity of propositions is done by using tactics which correspond to logical inference \nrules such as A-introduction. To reason about the validity of an assertion as a precondition of a code \nblock, we de.ne a set of inference rules for proving speci.ca\u00adtion judgments of the following forms: \n. i{a}](well-formed program) . iq(well-formed code heap) . i{a}I(well-formed instruction sequence) The \nintuition of well-formed instruction sequence, for example, is that if the instruction sequence Istarts \nexecution in a machine state which satis.es the assertion a, then executing Iis safe with respect to \nthe speci.cation .. In Figure 3, we give some sample inference rules which the pro\u00adgrammers use to prove \nthe safety of their programs. A program is well-formed (rule PROG) under assertion aif both the code \nheap and the instruction sequence are well-formed, and the machine state satis.es the assertion a. A \ncode heap is well-formed (rule CODE-HEAP) if every code block is well-formed under the associated as\u00adsertion. \nA direct jump is safe (rule JD) if the current assertion is no weaker than the assertion of the target \ncode block as speci.ed by the code heap speci.cation. Here the symbol = is used to de\u00adnote logical implication. \nAlso note the slight notational abuse for convenience state meta-variables are reused as state variables. \n An instruction sequence preceded by an addition is safe (rule ADD) if we can .nd an assertion awhich \nserves both as the postcondition of the addition (aholds on the updated machine state after execut\u00ading \nthe addition, as captured by the implication) and as the precon\u00addition of the tail instruction sequence. \nA programmer s task, when proving the well-formedness of a program, involves mostly apply\u00ading the appropriate \ninference rules, .nding intermediate assertions like a, and proving the logical implications. The soundness \nof these inference rules is established with respect to the operational semantics following the syntactic \napproach of prov\u00ading type soundness [44]. The soundness theorem below guarantees that given a well-formed \nprogram, the current instruction sequence will be able to execute without getting stuck (normal type-safety); \nin addition, whenever we jump to a code block, the speci.ed asser\u00adtion of that code block (which is an \narbitrary state predicate given by the user) will hold. Theorem 1 (CAP Soundness) If . i{a}(q,\u00a7,I), then \nfor all natural number n, there exists a program ]such that (q,\u00a7,I)>-n ], and if (q,\u00a7,I)>-*(q,\u00a7,jd f), \nthen .(f)\u00a7; if (q,\u00a7,I)>-*(q,(J,l),(bgt rs,rt ,f))and l(rs)>l(rt ), then .(f)(J,l). Interested readers \nare referred to the previous work [46, 47] for a complete modeling of CAP and a certi.ed malloc/free \nlibrary whose correctness criterion involves more than type-safety. 2.2 Invariance Proof The code heap \ntype of CAP allows the speci.cation of certain pro\u00adgram properties in addition to type safety. However, \nit appears insuf.cient in modeling general safety properties, which typically disallow undesired things \nfrom happening at any program points. Directly accommodating such a property in CAP would require in\u00adspection \nof preconditions at all program points, resulting in an un\u00adwieldy speci.cation. Fortunately, many methods \nhave been proposed for proving safety (invariance) properties for both sequential and concurrent pro\u00adgrams \n[12, 17, 33, 25, 24]. As observed by Lamport [24], all of these methods are essentially the same when \napplied to the same program, they involve the same proof, though perhaps in different orders and with \ndifferent notation. In particular, the execution of a program can be viewed as transitions of states, \nand a general in\u00advariance proof of a program satisfying a safety property P reduces to .nding an invariant \nI satisfying three conditions: 1. The initial state of the program satis.es I; 2. I implies P; 3. If \na state satis.es I, then the next state satis.es I.  Interestingly, this invariance proof technique \nalso characterizes certain principles found in the syntactic approach to type sound\u00adness [44], as engaged \nby many modern type systems. In fact, the re\u00ad (1)f1: movi r1,1 (2) ... (3) yield (4) st r2(0),r1 (5) \njd f2  (6)f2: movi r1,2 (7) st r2(1),r1 (8) yield (9) ... (10) jd f3 Figure 4. Example: Non-preemptive \nthread control. semblance is clear once we describe the type-safety proofs of these systems as follows: \n1. The program is initially well-typed; 2. If a program is well-typed, then it makes a step (usually \nre\u00adferred to as a lemma of progress ); 3. If a program is well-typed, then it is also well-typed af\u00adter \nmaking a step (usually referred to as a lemma of type\u00adpreservation ).  It is not dif.cult to observe \nthat the syntactic approach to type soundness is in essence a special application of the invariance proof \nwhere the invariant I is well-typedness of the program and the safety property P is type-safety (i.e., \nnon-stuckness). Since the progress and preservation lemmas of such type systems are proved once and for \nall, all that remains when writing a program is to verify the (initial) well-typedness by typing rules. \nThe story of CAP is similar the invariant I is the well-formedness of the program and the safety property \nP is type-safety and satisfac\u00adtion of the code heap speci.cation. The problem we encountered applying \nCAP directly to general safety properties now becomes apparent: type-safety and satisfaction of the code \nheap speci.ca\u00adtion is not suf.cient in modeling arbitrary safety properties. On the bright side, a solution \nbecomes also apparent: it suf.ces to enhance the judgment of well-formed programs so that it implies \nthe validity of a user speci.ed predicate Inv (referred to as a global invariant in the remainder of \nthis paper), which in turn implies a safety prop\u00aderty of concern. Such a CAP-like language maintains \nthe .avor of a type system, but the typing rules are practically proof rules of a logic system.  2.3 \nAssume-Guarantee A last piece of the puzzle is a compositional method for reason\u00ading about the interaction \nbetween threads. Since Francez and Pnueli [13] developed the .rst method for reasoning composi\u00adtionally \nabout concurrency, various methods have been proposed, the most representative ones of which are assumption-commitment \ninvented by Misra and Chandy [27] for message-passing sys\u00adtems and rely-guarantee by Jones [23] for shared-memory \npro\u00adgrams. These two methods have thereupon been carefully stud\u00adied [32, 36, 40, 41, 1, 45, 2, 5, 16, \n11] and often referred to as assume-guarantee. Under this assume-guarantee paradigm, every thread (or \nprocess) is often associated with a pair (A,G)consisting of a guarantee G that the thread will satisfy \nprovided the environment satis.es the assumption A. Under the shared-memory model, the assumption A (Prog)]::=(\u00a7,q1 \n,q2 ,I1,I2,i)where i E{1,2} (State)\u00a7::=(M,l) (Mem)M::={1.w}* (RegFile)l::={r.w}* {rk}kE{0...7} (Register)r::= \n(Labels)f,1::=n(nat nums) (WordVal)w::=n(nat nums) (CdHeap)q::={f.I} * (InstrSeq)I::=;IIjd f (Comm)::=yield \nIadd rd ,rs,rt Isub rd ,rs,rt Imovi rd ,wIbgt rs,rt ,fIbe rs,rt ,f Ild rd ,rs(w)Ist rd (w),rs Figure \n5. Syntax of CCAP. of a thread describes what atomic transitions may be performed by other threads, \nand the guarantee G of a thread must hold on every atomic transition performed by the thread itself. \nThey are typically modeled as predicates on a pair of states, which are often called actions. Reasoning \nabout a concurrent program then reduces to reasoning about each thread separately, provided that the \nguarantee of each thread be no weaker than the assumption of every other thread. We apply this approach \nat an assembly level, using assumptions and guarantees to characterize the interaction between threads. \nIn Sec\u00adtion 3, our abstract machine adopts a non-preemptive thread model, in which threads yield control \nvoluntarily with a command yield. An atomic transition in a preemptive setting then corresponds to a \nsequence of commands between two yield in our setting. Figure 4, for example, shows two code blocks that \nbelong to the same thread; the state transition between the two yield at lines (3)and (8)must satisfy \nthe guarantee of the current thread, and the potential state transition at either yield, caused by other \nthreads, is expected to sat\u00adisfy the assumption. A dif.culty in modeling concurrency in such a setting \nis that one has to look inside an atomic operation. It is different from mod\u00adeling an atomic operation \nas a whole, as is the case of some previous work on concurrency veri.cation, where the state transition \ncaused is immediately visible when analyzing the atomic operation. When an atomic operation is made up \nof a sequence of commands, its ef\u00adfect can not be completely captured until reaching the end. For ex\u00adample, \nin Figure 4, the state change caused by the st at line (4)need not immediately satisfy the guarantee; \ninstead, it may rely on its fol\u00adlowing commands (i.e., lines (6) and (7)) to complete an adequate state \ntransition. Hence when verifying the safety of a command us\u00ading CAP-style inference rules, it is insuf.cient \nto simply check the guarantee against the state change cause by that command. In our modeling, we introduce \na local guarantee gfor every program point to capture further state changes that must be made by the \nfol\u00adlowing commands before it is safe for the current thread to yield control. Such a non-preemptive \nsetting helps to separate thread interaction from other orthogonal issues of assembly veri.cation. For \ninstance, from the example of Figure 4, it is apparent that the thread inter\u00adaction using yield is orthogonal \nfrom the handling of control .ow transfer (e.g., direct jump jd or indirect jump jmp). This also pro\u00advides \ninsights on the veri.cation of preemptive threads, because a preemptive model can be considered as a \nspecial case of the non\u00adpreemptive model in which explicit yielding is used at all program points. ((M,l),q1 \n,q2 ,I1,I2,1)>-]where if I1 = then ]= jd f ((M,l),q1 ,q2 ,I,I2,1)where q1 (f)=I yield;I ((M,l),q1 ,q2 \n,I,I2,i)where i E{1,2} bgt rs,rt ,f;I ((M,l),q1 ,q2 ,I,I2,1)when l(rs):l(rt ); and ((M,l),q1 ,q2 ,I,I2,1)when \nl(rs)>l(rt )where q1 (f)=I be rs,rt ,f;I ((M,l),q1 ,q2 ,I,I2,1)when l(rs) =l(rt ); and ((M,l),q1 ,q2 \n,I,I2,1)when l(rs)=l(rt )where q1 (f)=I ;Ifor remaining cases of (Next(,(M,l)),q1 ,q2 ,I,I2,1) ((M,l),q1 \n,q2 ,I1,I2,2)>-]de.ned similarly Figure 6. Operational semantics of CCAP. if = then Next(,(M,l))= add \nrd ,rs,rt (M,l{rd .l(rs)l(rt )}) sub rd ,rs,rt (M,l{rd .l(rs)>l(rt )}) movi rd ,w (M,l{rd .w}) ld rd \n,rs(w) (M,l{rd .M(l(rs)w)}) where (l(rs)w)Edom(M) st rd (w),rs (M{l(rd )w.l(rs)},l) where (l(rd )w)Edom(M) \n Figure 7. Auxiliary state update macro.  3 CCAP CAP supports mechanical veri.cation on sequential \nassembly code; invariance proof can be applied to reason about general safety properties; and assume-guarantee \nallows the decomposition of a concurrent program into smaller pieces which in turn can be rea\u00adsoned about \nsequentially. The remaining task is to accommodate these techniques in a single language CCAP. In particular, \nassume\u00adguarantee must be adapted to work for non-preemptive assembly code, as brie.y explained in the \nprevious section. In this section, we present CCAP based on an abstract machine that supports the concurrent \nexecution of two threads using a shared memory. A generalized version supporting more threads is given \nin Appendix A. 3.1 Abstract Machine The abstract machine is a straightforward extension from CAP (see \nFigure 5 for the syntax). A program ]consists of a shared state component \u00a7made up of a memory Mand a \nregister .le l,two code heaps q(one for each thread), two current instruction se\u00adquences I(one for each \nthread), and an indicator i indicating the current thread. Threads may interact using the shared memory, \nand yield control (yield) voluntarily. Only a small set of commands are modeled for simplicity, but extensions \nare straightforward. Note that there is no need for instructions such as test-and-set because of non\u00adpreemption. \nThe operational semantics is de.ned in Figures 6 and 7. Figure 7 de.nes a next state macro relation detailing \nthe effect of some commands on the machine state. For memory access or update commands (ld, st), the \nmacro is de.ned only if the side conditions are met. In Figure 6, we only show the cases where thread \n1 is the current thread; the other cases are similar. At a yield command, the machine picks a thread \nnon-deterministically without affecting the (ProgSpec) F ::= ( Inv,.1,.2,A1 ,A2 ,G1 ,G2 ) (CdHpSpec) \n. ::={f.(p,g)}* (ThrdSpec) T ::= ( Inv,.,A,G) (Invariant) Inv E State-Prop (Assertion) p E State-Prop \n(Assumption) A E State-State-Prop (Guarantee) G,g E State-State-Prop Figure 8. Veri.cation constructs \nof CCAP. state. Control .ow commands (jd, bgt and be) do not affect the state either. Implicit from \nthese two .gures is that the machine gets stuck when executing a memory access or update but the side \ncondition as speci.ed by the macro is not met. 3.2 Inference Rules Veri.cation constructs of CCAP are \nintroduced in Figure 8. A pro\u00adgram speci.cation F consists of a global invariant (Inv), assump\u00adtions \n(A) and guarantees (G), and code heap speci.cations (.). A programmer must .nd a global invariant Inv \nfor the program; Inv should be no weaker than the safety property of interest and hold throughout the \nexecution of the program. A programmer must also .nd for each thread a guarantee Gand an assumption Adescribing \nallowed atomic state transitions of the thread and its environment respectively. Every thread also has \nits own code heap speci.cation .. Asisthe case of CAP, . is a mapping from code labels to preconditions. \nWhat s different now is that a precondition in CCAP contains not only an assertion pdescribing valid \nstates, but also a guarantee g describing valid state transitions it is safe for the current thread to \nyield control only after making a state transition allowed by g. We call ga local guarantee to distinguish \nit from G. To prove the safety of a program, the programmer needs to .nd appropriate preconditions for \nall program points, and apply the inference rules to be introduced later. CCAP s reasoning is thread-modular: \nthe veri.cation of a concur\u00adrent program can be decomposed into the veri.cation of its com\u00adponent threads, \nand each thread is reasoned about separately with respect to its own thread speci.cation. We use T to \ndenote such a thread speci.cation, which consists of the global invariant Inv, and the code heap speci.cation \n., assumption Aand guarantee Gof the thread. The program speci.cation can then be considered as a union \nof the thread speci.cation of every component thread. We use the following judgment forms to de.ne the \ninference rules: F;(p1,p2,g)i](well-formed program) T iq (well-formed code heap) T;(p,g)iI(well-formed \ninstr. seq.) These judgment forms bear much resemblance to those of CAP, noting that preconditions have \nevolved to include local guarantees. When verifying for a well-formed program, an extra assertion is \nused to describe a state expected by the idling thread; however, no extra local guarantee is needed. \nWell-formed program There are two rules for checking the well-formedness of a program. Rule PROG1 is \nused when thread 1 is the current thread, and Rule PROG2 is used when thread 2 is the current thread. \nF =(Inv,.1,.2,A1 ,A2 ,G1 ,G2 ) (Inv Ap1 \u00a7)V\u00a7.(g\u00a7\u00a7)=(p2 \u00a7) V\u00a7.V\u00a7.(Inv Ap2 \u00a7)=(A2 \u00a7\u00a7)=(p2 \u00a7) (Inv,.1,A1 \n,G1 )iq1 (Inv,.1,A1 ,G1 );(p1,g)iI1 (Inv,.2,A2 ,G2 )iq2 (Inv,.2,A2 ,G2 );(p2,G2 )iI2 F;(p1,p2,g)i(\u00a7,q1 \n,q2 ,I1,I2,1) (PROG1) F =(Inv,.1,.2,A1 ,A2 ,G1 ,G2 ) (Inv Ap2 \u00a7)V\u00a7.(g\u00a7\u00a7)=(p1 \u00a7) V\u00a7.V\u00a7.(Inv Ap1 \u00a7)=(A1 \n\u00a7\u00a7)=(p1 \u00a7) (Inv,.2,A2 ,G2 )iq2 (Inv,.2,A2 ,G2 );(p2,g)iI2 (Inv,.1,A1 ,G1 )iq1 (Inv,.1,A1 ,G1 );(p1,G1 \n)iI1 F;(p1,p2,g)i(\u00a7,q1 ,q2 ,I1,I2,2) (PROG2) Since these two rules are symmetrical, we only explain Rule \nPROG1, where thread 1 is the current thread. In this rule, the well-formedness of a program is determined \nwith respect to not only the program speci.cation F, but also an assertion p1 describ\u00ading the current \nstate, an assertion p2 describing the state in which thread 2 may take control, and a relation gdescribing \nthe discrep\u00adancy between these two states. The premises of Rule PROG1 may appear complex, but are rather \nintuitive. Line 1 gives us the components of F. Lines 2 requires that both the global invariant Inv and \nthe current assertion p1 must hold on the current state; it also restricts the assertion p2 of thread \n2 and the current guarantee gof thread 1: if some operations satisfying g is performed on the current \nstate, then the result state satis.es p2. This indicates that it is suf.cient to refer to gfor the expectation \nof thread 2 when checking thread 1. When thread 1 is executing, thread 2 should be in a safe state whose \nproperties described by p2 will not be disturbed by allowed (assumed) transitions of thread 1. Hence \nline 3 restricts the as\u00adsertion p2 to preserve itself under state transitions that satisfy the assumption \nA2 . Note that = is right associative, i.e., a =b=c is interpreted as a=(b=c). The last two lines decompose \nthe veri.cation task into two parts, each performed independently for a single thread based on the cor\u00adresponding \nthread speci.cation. For thread 1 (line 4), the code heap must be well-formed, and the current instruction \nsequence must be well-formed under the current precondition. The requirements for thread 2 is similar \n(line 5), except that the guarantee G2 is used in the precondition of thread 2, indicating necessary \nstate transitions before thread 2 may yield. Another requirement is that the assumptions and the guarantees \nbe compatible, i.e., G1 =A2 and G2 =A1 . Noting that these relations are constants and concern only the \nspeci.cation, we do not include this requirement in the inference rules, but will use it as a premise \nof a soundness lemma. Well-formed code heap T =(Inv,.,A,G) (pj,gj)}jE{1...n} . ={fj . T;(pj,gj)iIj \nVj E{1...n} (CODEHEAP) Ij}jE{1...n} T i{fj . A code heap is well-formed if every code block is well-formed \nun\u00adder the associated precondition. Note that after the decomposition performed by Rules PROG1 and PROG2, \nevery thread is veri.ed separately under its own thread speci.cation. Well-formed instruction sequence \nT =(Inv,.,A,G) V\u00a7.(Inv Ap\u00a7)=(g\u00a7\u00a7) V\u00a7.V\u00a7.(Inv Ap\u00a7)=(A\u00a7\u00a7)=(p\u00a7) T;(p,G)iI (YIELD) T;(p,g)iyield;I Rule \nYIELD is used to verify an instruction sequence starting with yield. It is safe to yield control under \na state \u00a7only if required state transitions are complete, i.e., an identity transition from \u00a7satis.es \nthe local guarantee g. Furthermore, the assertion pmust preserve itself under transitions allowed by \nthe assumption A, indicating that it is safe to execute any number of atomic operations by the other \nthread. Lastly, one must verify the remainder instruction sequence Iwith the local guarantee reset to \nG. T =(Inv,.,A,G) V\u00a7.(Inv Ap\u00a7)=(Inv ApNext(,\u00a7)) V\u00a7.V\u00a7.(Inv Ap\u00a7)=(gNext(,\u00a7)\u00a7)=(g\u00a7\u00a7) T;(p,g)iIE{add,sub,movi} \n(SIMPL) T;(p,g)i;I Rule SIMPL covers the veri.cation of instruction sequences starting with a simple \ncommand such as add, sub or movi. In these cases, one must .nd an intermediate precondition (p,g)under \nwhich the tail instruction sequence Iis well-formed. Moreover, the global invariant Inv and the intermediate \nassertion pmust hold on the up\u00addated machine state, and the intermediate guarantee gapplied to the updated \nmachine state must be no weaker than the current guar\u00adantee gapplied to the current state. Consider a \nsequence of states \u00a71...\u00a7n between two safe points (i.e., between two yield commands). Suppose we have \na sequence of guarantees g1 ...gsuch that: n (g2 \u00a72 \u00a7n)=(g1 \u00a71 \u00a7n); (LOCAL-GUAR) (g3 \u00a73 \u00a7n)=(g2 \u00a72 \u00a7n); \n...; (gn.1 \u00a7n.1 \u00a7n)=(gn.2 \u00a7n.2 \u00a7n); (g\u00a7n \u00a7n)=(g nn.1 \u00a7n.1 \u00a7n). From these we know that the state transition \nfrom \u00a71 to \u00a7n satis.es the original guarantee g1 if only (gn \u00a7n \u00a7n), which will be estab\u00adlished at the \nyield command. T =(Inv,.,A,G) VM.Vl.(Inv Ap(M,l))=((l(rs)w)Edom(M)) V\u00a7.(Inv Ap\u00a7)=(Inv ApNext(,\u00a7)) V\u00a7.V\u00a7.(Inv \nAp\u00a7)=(gNext(,\u00a7)\u00a7)=(g\u00a7\u00a7) T;(p,g)iI=ld rd ,rs(w) (LD) T;(p,g)i;I T =(Inv,.,A,G) VM.Vl.(InvAp(M,l))=((l(rd)w)Edom(M)) \nV\u00a7.(InvAp\u00a7)=(InvApNext(,\u00a7)) V\u00a7.V\u00a7.(InvAp\u00a7)=(gNext(,\u00a7)\u00a7)=(g\u00a7\u00a7) T;(p,g)iI=st rd(w),rs (ST) T;(p,g)i;I \nVerifying memory access or update with Rules LD or ST is in spirit similar to verifying simple commands, \nexcept one must also estab\u00adlish the side condition as speci.ed by the macro Next. T =(Inv,.,A,G).(f)=(p,g) \nV\u00a7.(InvAp\u00a7)=(p\u00a7) V\u00a7.V\u00a7.(InvAp\u00a7)=(g\u00a7\u00a7)=(g\u00a7\u00a7) (JD) T;(p,g)ijd f Direct jump is easily veri.ed by fetching \nthe precondition from the code heap speci.cation, as shown in Rule JD. There is no need to re-establish \nthe global invariant Inv explicitly, because the machine state remains the same. T =(Inv,.,A,G).(f)=(p,g) \nVM.Vl.(l(rs)>l(rt))=(InvAp(M,l))=(p(M,l)) VM.Vl.V\u00a7.(l(rs)>l(rt))=(InvAp(M,l)) =(g(M,l)\u00a7)=(g(M,l)\u00a7) VM.Vl.(l(rs):l(rt))=(InvAp(M,l))=(p(M,l)) \nVM.Vl.V\u00a7.(l(rs):l(rt))=(InvAp(M,l)) =(g(M,l)\u00a7)=(g(M,l)\u00a7) T;(p,g)iI T;(p,g)ibgt rs,rt,f;I (BGT) T =(Inv,.,A,G).(f)=(p,g) \nVM.Vl.(l(rs)=l(rt))=(InvAp(M,l))=(p(M,l)) VM.Vl.V\u00a7.(l(rs)=l(rt))=(InvAp(M,l)) =(g(M,l)\u00a7)=(g(M,l)\u00a7) VM.Vl.(l(rs)=l(rt))=(InvAp(M,l))=(p(M,l)) \nVM.Vl.V\u00a7.(l(rs)=l(rt))=(InvAp(M,l)) =(g(M,l)\u00a7)=(g(M,l)\u00a7) T;(p,g)iI T;(p,g)ibe rs,rt,f;I (BE) Lastly, \na conditional command is safe if both branches can be veri\u00ad.ed, hence Rule BGT or BE can be understood \nas a combination of Rules JD and SIMPL, noting that the comparison result can be used in verifying the \nbranches.  3.3 Soundness The soundness of these inference rules with respect to the opera\u00adtional semantics \nof the machine is established following the syn\u00adtactic approach of proving type soundness. From the progress \nand preservation lemmas (proved by induction on I; see the im\u00adplementation for detailed Coq proofs [43]), \nwe can guarantee that given a well-formed program under compatible assumptions and guarantees, the current \ninstruction sequence will be able to execute without getting stuck. Furthermore, any safety property \nderivable from the global invariant will hold throughout the execution. Lemma 1 (Code Lookup) If ( ,., \n, )iqand .(f)=(p,g), then there exists Isuch that q(f)=I. Proof sketch: By de.nition of well-formed code \nheap. . Lemma 2 (Progress) F =(Inv,.1,.2,A1 ,A2 ,G1 ,G2 ).If F;(p1,p2,g)i(\u00a7,q1 ,q2 ,I1,I2,i)where i E{1,2}, \nthen (Inv \u00a7) and there exists a program ]such that (\u00a7,q1 ,q2 ,I1,I2,i)>-]. Proof sketch: By induction \non the structure of Ii. (Inv \u00a7)holds by de.nition of well-formed program. In the cases where Ii starts \nwith add, sub, movi or yield, the program can always make a step by the de.nition of the operational \nsemantics. In the cases where Ii starts with ld or st, the side conditions for making a step, as de.ned \nby the operational semantics, are established by the corresponding inference rules. In the cases where \nIi starts with bgt or be,or where Ii is jd, the operational semantics may fetch a code block from the \ncode heap; such a code block exists by Lemma 1. . Lemma 3 (Code Well-formedness) If T =( ,., , ), T iq, \n.(f)=(p,g)and q(f)=I, then T;(p,g)iI. Proof sketch: By de.nition of well-formed code heap. . Lemma 4 \n(Preservation) F =(Inv,.1,.2,A1 ,A2 ,G1 ,G2 ). Suppose V\u00a7.V\u00a7.(G1 \u00a7\u00a7)=(A2 \u00a7\u00a7)and V\u00a7.V\u00a7.(G2 \u00a7\u00a7)=(A1 \u00a7\u00a7). \nIf F;(p1,p2,g)i(\u00a7,q1 ,q2 ,I1,I2,i)where i E{1,2}, and (\u00a7,q1 ,q2 ,I1,I2,i)>-], then there exists p1, p2 \nand gsuch that F;(p1,p2,g)i]. Proof sketch: By induction on the structure of Ii. We must estab\u00adlish \nthe premises (lines 2 5) of Rule PROG1or PROG2. The .rst half of line 2 is easily established: for any \ncommand that updates the machine state, its corresponding inference rule dictates that the global invariant \nand the postcondition hold on the updated machine state. The second half of line 2 also follows naturally, \nbe\u00adcause the inference rules yield a sequence of strengthening local guarantees, as illustrated by the \nequations LOCAL-GUAR. Unless Ii starts with yield, line 3 is trivially preserved because the precondition \nof the idling thread remains unchanged; in the case where Ii starts with yield, the current thread may \nbecome the idling thread, and line 3 follows from a premise of Rule YIELD. The .rst half of lines 4 \nand 5 (well-formed code heaps) is not af\u00adfected by a program step, hence it is trivially preserved. For \nthe second half of lines 4 and 5 (well-formed instruction sequences), in the cases where Ii starts with \nadd, sub, movi, yield, ld or st, the well-formedness of the instruction sequence in the next state can \nbe directly derived from the corresponding inference rules; in the cases where Ii starts with bgt or \nbe,or is jd, we apply Lemma 3. .  4 Examples CCAP is a realization of established veri.cation techniques \nat the assembly level. The expressiveness of these techniques and their application to high-level programs \nare well-known. In this section, we give 3 simple examples to demonstrate the mechanized veri\u00ad.cation \nof interesting safety properties for assembly code and, in particular, illustrate the usage of local \nguarantees (g) as part of the preconditions. 4.1 Mutual Exclusion Dekker s algorithm [10] is the .rst \nsolution to mutual exclusion problem for the two-process case. Flanagan et al. [11] have given a modeling \nunder the assume-guarantee paradigm for a high-level parallel language of atomic operations. In this \nsection, we show that the subtle synchronization used in Dekker s algorithm can be rea\u00adsoned about as \neasily in CCAP, where atomic operations are com\u00adposed of sequences of instructions. The algorithm is \nrevised using 4 boolean variables [11] (Figure 9). It is easy to observe that variable sis trueif thread \ni is in its crit\u00ad . Variables: Initially: boo1eana,a,s,s; -sA-s .... .. Thread1: Thread2 : whi1e(true){ \nwhi1e(true){ a . :=true; a . :=true; s . :=-a . ; s . :=-a . ; if(s){ if(s){ .. //ritia1se. //ritia1se. \ns . :=fa1se; s . :=fa1se; }} a . :=fa1se; a . :=fa1se; }} Figure 9. Dekker s mutual exclusion algorithm. \nInv =M(s .),M(s .),M(a .),M(a .)E{0,1} A-(M(s .)AM(s .)) A(M(s .)=M(a .))A(M(s .)=M(a .)) A1 ,G2 =(M(a \n.)=M(a .))A(M(s .)=M(s .)) A2 ,G1 =(M(a .)=M(a .))A(M(s .)=M(s .)) Initial M={s ..0,s ..0,a .. ,a .. \n} Initial thread: i where i E{1,2} Initial instruction sequences: Ii =jd loopi Initial precondition triple: \n(p1,p2,g)=(True,True,Gi ) loop1: >{(True,G1 )}csec1: >{(True,G1 )} yield yield movi r1,a >{(True,G1 \n)} movi r2,1. movi r1,s . st r1(0),r2 >{(l(r1)=s .,G1 )} >{(J(a .)=1,G1 )}movi r2,0 yield >{(l(r1)=sA \nmovi r1,s . l(r2)=0,G. 1 )} movi r2,1 st r1(0),r2 movi r3,a >{(J(s)=0,G1 )} ld r4,r3(0. ) yield . sub \nr5,r2,r4 >{(J(s .)=0,G1 )} st r1(0),r5 jd reset1 >{(True,G1 )} yield reset1: >{(J(s)=0,G1 )} . movi \nr1,s yield ld r2,r1(0) . movi r1,a movi r3,0 movi r2,0. bgt r2,r3,csec1 st r1(0),r2 >{(J(s .)=0,G1 )}>{(True,G1 \n)} yield yield jd reset1 jd loop1 Figure 10. CCAP implementation for Dekker s algorithm. ical section; \nhence the mutual exclusion property of this algorithm can be expressed as -(sAs). .. To decompose the \nveri.cation problem into two sequen\u00adtial ones, thread i makes the assumption that the other thread not \nmodify variables aand s. Furthermore, the .. mutual exclusion property is strengthened to the invariant \n-(sAs)A(s=a)A(s=a). ...... Some key components of a corresponding CCAP implementation of the algorithm \nare shown in Figure 10. Only the code of thread 1 is given, since that of thread 2 is similar. Yielding \nis inserted at all the intervals of the atomic operations of the original program. Variables: Initially: \nnatfork,fork; fork=0Afork=0 .. .. Thread1: Thread2 : whi1e(true){whi1e(true){ aquire(fork,1); aquire(fork,2); \n.. aquire(fork,1); aquire(fork,2); .. //eat //eat re1ease(fork); re1ease(fork); .. re1ease(fork); re1ease(fork); \n.. //think //think }} Figure 11. Dining philosophers algorithm. The code heap speci.cation is separated \nand given as preconditions (surrounded by >{}) at the beginning of each code block for ease of reading. \nExtra preconditions are spread throughout the code as an outline of the proof the inference rules can \nbe easily applied once preconditions are given at all program points. For expository convenience, we \nuse variable names as short-hands for memory locations. The boolean values fa1seand trueare encoded using \n0 and 1 respectively. We also omit the variable bind\u00adings from the predicates. To be exact, a .\u00a7. is \nomitted from the global invariant and every assertion; a .\u00a7..\u00a7. is omitted from every assumption and \nguarantee. In these predicates, Mand lre\u00adfer to the memory component and the register .le component of \nthe bounded variable \u00a7respectively, and Mand lrefer to those of the bounded variable \u00a7. For this example, \nstate predicate True is used as the assertion pat some of the yield instructions, because Inv alone is \nsuf.cient in characterizing the states at these points. Moreover, since thread 1 never modi.es the value \nof aor s, the guarantee G1 can be .. used as the local guarantee gthroughout the code of thread 1. Take \nthe code block labeled csec1 as an example. There are two move instructions used to put intermediate \nvalues into registers; their ef\u00adfect is apparent from the associated assertions. A following store instruction \nwrites into the memory location s, after which the . global invariant is still maintained. It is a simple \ntask to check the well-formedness of this code block with respect to the precondi\u00adtions, because every \nassembly instruction only incur little change to the machine state. The use of a proof assistant helps \nautomate most simple steps and keep the would-be arduous task manageable. 4.2 Deadlock Freedom One way \nto prevent deadlock is to assign a total ordering to all resources and stick to it when acquiring them. \nWe give a sim\u00adpli.ed example of dining philosophers in Figure 11, where two philosophers (Thread1 and \nThread2) share two forks (represented by memory locations forkand fork). A philosopher picks up .. (aquire) \na fork by writing the thread id (1 or 2) into the mem\u00adory location representing the fork, and puts down \n(re1ease) a fork by writing 0. Both philosophers acquire forkbefore acquiring . fork. . The deadlock \nfreedom property for this example can be expressed as (fork=0)V(fork=fork), indicating that either a \nfork is . .. free or a philosopher is holding both forks. In the more general case of three or more \nphilosophers, the deadlock freedom property can be expressed as (fork=0)V(:i.fork=fork), where n is the \n. .. number of philosophers and forkis the greatest fork. Note that . Inv =(M(fork .)=0)V(M(fork .)=M(fork \n.)) A1 ,G2 =(M(fork .)=2 =M(fork .)=2) A(M(fork .)=1 BM(fork .)=1) A(M(fork .)=1 BM(fork .)=1) A2 ,G1 \n=(M(fork .)=1 =M(fork .)=1) A(M(fork .)=2 BM(fork .)=2) A(M(fork .)=2 BM(fork .)=2) Initial M={fork.0,fork \n..0} . Initial thread: iwhere iE{1,2} Initial instruction sequences: Ii =jd fai Initial precondition \ntriple: (p1,p2,g)=( M(fork .)=1 AM(fork )=1, . M(fork .)=2 AM(fork )=2,Gi) . fa1: >{(M(fork .)=1 AM(fork \n.)=1,G1 )} yield movi r1,fork ld r2,r1(0) . movi r3,0 bgt r2,r3,fa1 >{(M(fork)=0 AM(fork )=1 .. Al(r1)=fork,G1 \n)} movi r2,1 . >{(M(fork)=0 AM(fork )=1 .. Al(r1)=fork . Al(r2)=1, (M(fork .)=1 =M(fork .)=1) A(M(fork \n.)=2) A(M(fork .)=2 BM(fork .)=2))} st r1(0),r2 >{(M(fork)=1 AM(fork )=1, .. (M(fork .)=1 =M(fork .)=1) \nA(M(fork .)=2) A(M(fork .)=2 BM(fork .)=2))} yield >{(M(fork .)=1 AM(fork .)=1,G1 )} jd fb1 fb1: >{(M(fork \n.)=1 AM(fork .)=1,G1 )} yield movi r1,fork ld r2,r1(0) . movi r3,0 bgt r2,r3,fb1 movi r2,1 st r1(0),r2 \n >{(M(fork)=1 AM(fork )=1, .. (M(fork .)=1 =M(fork .)=1) A(M(fork .)=2) A(M(fork .)=2))} yield >{(M(fork \n.)=1 AM(fork .)=1,G1 )} movi r1,fork movi r2,0 . st r1(0),r2 yield >{(M(fork .)=1 AM(fork .)=1,G1 )} \nmovi r1,fork movi r2,0 . st r1(0),r2 yield >{(M(fork .)=1 AM(fork .)=1,G1 )} jd fa1 Figure 12. CCAP \nimplementation for dining philosophers algo\u00adrithm. Variables: Initially: nata,b; a=a Ab=\u00df Thread1: Thread2 \n: whi1e(a=b) whi1e(b=a) if(a>b) if(b>a) a:=a>b; b:=b>a; Figure 13. GCD algorithm. we cannot use CCAP \nas is to reason about livelock or starvation freedom, which are liveness properties and require different \nproof methods [34]. The corresponding CCAP speci.cation and program are shown in Figure 12, where the \nsame notational convention is used as estab\u00adlished in the previous section, and only the code for Thread1 \nis given. In this example, Inv is exactly the desired property. Thread 1 guarantees (see G1 ) to acquire \nforkonly after acquiring fork .. (M(fork)=1 =M(fork)=1). Due to the nature of our sim\u00ad ..ulation using \nthe values of fork . and fork . to indicate the oc\u00adcupiers of the resources, Thread 1 also guarantees \nthat it does not release a resource held by Thread 2 or acquire a resource for Thread 2(M(forki)=2 BM(forki)=2, \nwhere Bmeans if and only if ). The case for Thread 2 is similar. We explain the code block labeled fa1, \nwhich corresponds to the .rst acquiring operation of Thread1. The precondition of this block indicates \nthat Thread1 owns neither resource at the beginning of the loop. A busy waiting on forkis coded using \nthe branch instruc\u00ad . tion (bgt). During the waiting, the guarantee is trivially maintained because \nno state change is incurred. The thread executes past the branch instruction only if forkis free, i.e., \nM(fork )=0, as captured by the assertion immediately after the branch. At this point, based on the current \nvalue of fork, which does not equal .. . to 2, we can simplify the local guarantee at the next step \nfollow\u00ading Rule SIMPL. The difference between the new guarantee and the original G1 lies in the handling \nof fork the new guarantee . requires that M(fork)=2, because it is known that the original .value of \nfork . was not 2. The veri.cation of the remainder of this code block is straightforward. At the yielding \ninstruction before jumping to fb1, the local guarantee is instantiated twice on the cur\u00adrent state; its \nvalidity is trivial. After the yielding instruction, the local guarantee is reset to G1 as required by \nRule YIELD. 4.3 Partial Correctness We consider a program for computing a Greatest Common Divi\u00adsor (GCD) \nas shown in Figure 13. This program satis.es the par\u00adtial correctness property that aand bbecome equal \nto the GCD of their initial values when the program exits the loop. a and \u00df are rigid variables ranging \nover natural numbers; they are introduced for reasoning purposes. Figure 14 gives the corresponding CCAP \nspeci.cation and pro\u00adgram. Only the code of Thread1 is shown. The global invariant of this program is \nthat the GCD of aand bremains the same. Thread1, for example, makes the assumption that Thread2 may change \nthe values of aor bonly if M(a)<M(b)and only in such a way that the GCD can be preserved. The partial \ncorrectness property is ex\u00adpressed as the precondition of the code block labeled done1 if the execution \never reaches here, the value stored in aand bmust be the expected GCD. Let d be a shorthand for gcd(a,\u00df). \nInv =gcd(M(a),M(b))=d A1 ,G2 =(M(a).M(b)=(M(a)=M(a)AM(b)=M(b))) A(M(a)<M(b)=(gcd(M(a),M(b))=d)) A2 ,G1 \n=(M(b).M(a)=(M(a)=M(a)AM(b)=M(b))) A(M(b)<M(a)=(gcd(M(a),M(b))=d)) Initial M={a.a,b.\u00df} Initial thread: \ni where i E{1,2} Initial instruction sequences: Ii =jd loopi Initial precondition triple: (p1,p2,g)= \n (gcd(M(a),M(b))=d,gcd(M(a),M(b))=d,Gi ) calc1: >{(M(a)>M(b),G1 )} loop1: >{(True,G1 )}yield yield >{(M(a)>M(b),G1 \n)} movi r1,a movi r1,a ld r2,r1(0) >{(M(a)>M(b)Al(r1)=a, movi r3,b gcd(M(a),M(b))=d)} ld r4,r3(0) ld \nr2,r1(0) be r2,r4,done1 movi r3,b >{(True,G1 )}ld r4,r3(0) yield sub r5,r2,r4 movi r1,a st r1(0),r5 ld \nr2,r1(0) >{(True, movi r3,b gcd(M(a),M(b))=d)} ld r4,r3(0) yield bgt r2,r4,calc1 jd loop1 >{(True,G1 \n)} yield done1: >{(M(a)=M(b)=d,G1 )} jd loop1 yield jd done1 Figure 14. CCAP implementation for GCD algorithm. \nThe most interesting code block is that labeled calc1. After the .rst yielding instruction, the comparison \nresult of M(a)>M(b)can be used to simplify the next local guarantee to gcd(M(a),M(b))=d. The actual proof \nof this certi.ed program involves also apply\u00ading mathematical properties such as gcd(a,a)=a and a >b \n= gcd(a,b)=gcd(a >b,b).  5 Implementation In Section 2.1, we mentioned the use of Coq and the underlying \nCiC for the mechanical veri.cation of CAP programs. The same approach is applied to implementing CCAP. \nWe encode the syntax of CCAP using inductive de.nitions, and de.ne the operational se\u00admantics and inference \nrules as a collection of relations. Embedding the entire CCAP in Coq allows us to make use of Coq s full \nex\u00adpressiveness, including its facility for inductive de.nitions, to code the veri.cation constructs \nin Figure 8. This also allows us to write down the soundness lemmas as Coq terms and formally prove their \nvalidity as theorems using the Coq proof assistant. In particular, our implementation contains about \n350 lines of Coq code for the language de.nition of CCAP, and about 550 lines of Coq tactics for the \nsoundness proof. We have also veri.ed the examples of Section 4 using this Coq encoding (see the actual \ncode [43] for details). The proof con\u00adstruction of each of these examples, with help of the Coq proof \nassistant, took no more than a few hours. On average, 11.5 lines of proof in Coq tactics is written for \nevery assembly instruction. As we have expected, many premises of the CCAP inference rules can be automatically \nveri.ed given the intermediate preconditions. This is especially true in the cases of simple instructions \nwhich do not concern the memory. Human smartness is required occasion\u00adally, however, to .nd proper intermediate \npreconditions and apply mathematical properties such as those of the GCD. In principle, the programer \nshould possess informal ideas on why their programs work when programming. Our system simply requires \nthe pro\u00adgramer to present those in logic, something necessary if formal rea\u00adsoning is desired. Keen readers \nmay have observed that these examples do not involve any data structures, hence the simplicity in verifying \nthem is not surprising. Previous work [46, 47] has studied similar veri.cation on list-like data structures, \nand shown that the veri.cation task is also straightforward after developing proper lemmas detailing \nthe interaction between various instructions (in particular st) and data structures. 6 Related and Future \nWork 6.1 Program Veri.cation The CAP languages (i.e., CAP and CCAP) approach the verify\u00ading compiler \ngrand challenge from the aspect of program veri.ca\u00adtion. They are developed as a complementary approach \nto Proof-Carrying Code (PCC) [29, 28] where the safety proof of a program is developed semi-automatically \nby the programmer with help of a proof assistant. The human factor involved achieves safety in a more \ngeneral sense, allowing the veri.cation of even undecidable program properties. Some useful applications \ninclude core system libraries and critical software components. Much further work is required, however, \non improving the modularity of the CAP lan\u00adguages before they can be applied to large scale applications. \nThis work includes modular support for higher-order code pointers (see Section 6.3) and safe linking \nof software components [14]. In the long run, it is conceivable to develop veri.ed software components \none by one and eventually have even a complete operation system veri.ed. The modularity support for the \nCAP languages will be crucial in realizing this ambition. The CAP languages address program safety directly \nat the level of machine code. The code being veri.ed is so close to the actual ex\u00adecutable that the translation \nbetween them is very trustworthy. This contrasts with model checking, which operates on models of pro\u00adgrams. \nThe non-trivial abstraction step to construct a model from a program indicates that a veri.ed model may \nnot necessarily imply a veri.ed program. On the other hand, focusing on high-level models enables model \nchecking to be effective in practice. Type systems are another popular approach of reasoning about pro\u00adgrams. \nTypes document programmers intent and enable compil\u00aders to detect certain programming errors automatically. \nIn practice, type systems have only been applied to reasoning about a limited scope of properties, including \nnon-stuckness of programs, infor\u00admation .ow, .nite-state rules, races and deadlocks, and atomicity of \nmultithreaded programs. In essence, types are simply restricted predicates, and the complexity of a type \nsystem is determined by the complexity of the underlying property that it enforces. This makes it hard \nto apply type systems to low-level code such as those found in storage allocation libraries, whose properties \noften demand very specialized type systems. The CAP languages, in contrast, directly employ higher-order \npredicate logic, intending to formally present the reasoning of a programmer. Proof construction is not \nunduly dif.cult with help of a proof assistant. Shao et al. [39] developed a type system for certi.ed \nbinaries (TSCB). An entire proof system (CiC) is integrated into a compiler intermediate language so \nas to perform reasoning about certi.ed programs in a calculus but essentially using higher-order predicate \nlogic via the formulae-as-types principle [22]. Being a type system, the soundness of TSCB only guarantees \ntype-safety (non-stuckness of well-typed programs). In comparison, the CAP languages allow programmers \nto write arbitrary predicates in the speci.cation to ac\u00adcount for more general safety properties. Another \nconvenient trait of the CAP languages is that, using a proof assistant like Coq, the well-formedness \nreasoning can be performed without the knowl\u00adedge about the CiC calculus, which may be desirable to some \npro\u00adgrammers.  6.2 Concurrency Veri.cation There has been much work on concurrency veri.cation (see \n[9, 21] for a systematic and comprehensive introduction). This paper adapts and applies established techniques \non this issue for assembly code, a domain not covered by existing work. The proof methods pertinent to \nthis paper have been discussed in Sections 2.2 and 2.3. In particular, our modeling has bene.ted from \nprevious work by Lamport [24] and Flanagan et al. [11], as elaborated below. Lamport [24] proposed the \nTemporal Logic of Actions (TLA) as a logic for specifying and reasoning about concurrent systems. He \npointed out a uni.ed view of existing methods for proving invari\u00adance (safety) properties, which can \nbe described formally in TLA as applications of a temporal inference rule named INV1. Some simple temporal \nreasoning following INV1shows that a general in\u00advariance proof can be reduced to .nding an invariant \nI satisfying three conditions. All these conditions are assertions about predi\u00adcates and actions, rather \nthan temporal formulas; hence the proof for invariance properties can be completed in ordinary mathematics \nfollowing the three conditions. CCAP engaged a syntactic approach usually found in type sys\u00adtems. Well-formedness \nof the program is used as the invariant I, and the soundness lemmas, namely progress and preservation, \ncover two of the conditions. The last condition, i.e., initial well\u00adformedness of the program, is left \nfor the programmer to establish using typing rules (inference rules). It is not dif.cult to see that \nthe lack of temporal reasoning in CCAP does not limit its expres\u00adsiveness in proving for safety properties, \nfollowing the observation from TLA. Proving liveness properties, on the other hand, requires further \nwork. Existing work handles liveness properties using counting\u00addown arguments or proof lattices [34]. \nAs observed by Lam\u00adport [24], although liveness properties are expressed by a variety of temporal formulas, \ntheir proofs can always be reduced to the proof of leads-to properties formulas of the form P .Q.It is \nan interesting future work to investigate how to apply these existing approaches to the mechanical veri.cation \nof assembly code. Flanagan et al. [11] investigated the mechanical checking of proof obligations for \nJava programs. Their automatic checker takes as input a program together with annotations describing \nappropriate assumptions, invariants and correctness properties. The key tech\u00adniques used by the checker, \nincluding assume-guarantee decompo\u00adsition, are derived from a parallel language of atomic operations. \n 6.3 Higher-Order Code Pointers Focusing on concurrency veri.cation, we have left out from this paper \nthe orthogonal issue of higher-order code pointers. Higher\u00adorder code pointers are the re.ection of higher-order \nprocedures at the assembly level, and a common example is the return pointers which are frequently used \nin most programs. Unfortunately, to the authors knowledge, the modularity support for higher-order code \npointers (or procedures) in Hoare-logic systems has been a long\u00adstanding open problem. A preliminary \nsupport for higher-order code pointers in CAP can be found in previous work [46, 47]. However, the solution \nthere is not suf.ciently modular. Take a library function as an example, both the caller s resource and \nthe callee s resource need to be explicitly written in the code heap speci.cation for a CAP routine. \nSince a library function is to be called at various places, its code heap spec\u00adi.cation entry in CAP \nis essentially a template to be instantiated upon used. Yu et al. [48] encountered the problem of functional \nparameters when checking the correctness of MC68020 object code programs. They handle it by asserting \nthe correctness of the functional param\u00adeters using constraints. The correctness of the program is proved \nassuming these constraints, and the correctness theorem of the pro\u00adgram is used repeatedly by substituting \nthe functional parameters with speci.c functions as long as these functions meet the imposed constraints. \nAlthough plausible, the actual mechanization of this idea, as explained by Yu et al. [48], is extremely \ndif.cult. Fur\u00adthermore, this approach is not truly satisfactory because the speci.\u00adcation of a program \nand its correctness theorem are essentially tem\u00adplates which need to be instantiated upon used. CAP s \napproach, as describe earlier, is very close to this: the actual code heap speci.\u00adcation is used when \nforming the constraints. In practice, we .nd it awkward when certifying programs with extensive use of \ncode pointers, especially when these uses are unconventional compared with return pointers. Similar problems \nsurfaced in some PCC systems as well. In par\u00adticular, Con.gurable PCC (CPCC) systems, as proposed by \nNec\u00adula and Schneck [30], statically check program safety using sym\u00adbolic predicates which are called \ncontinuations. For checking the safety of an indirect jump instruction which transfers the program control \ngiven a code pointer, a trusted decoder generates an in\u00addirect continuation whose safety needs to be \nveri.ed; this continu\u00adation is indirect because the target address cannot be determined by the decoder \nstatically. For veri.cation purpose, an untrusted VC-Gen extension is responsible for proposing some \ndirect contin\u00aduations (direct meaning that the target addresses are known stati\u00adcally) whose safety implies \nthe safety of the indirect continuation given by the decoder. In practice, the extension works by listing \nall the possible values of the code pointer (essentially replacing the code pointer in the continuations \nwith all concrete functions that it could stand for), which requires whole-program analysis and hence \nis contradictory with the goal of modularity. Chang et al. [6] presented a re.ned CPCC system in which \nlo\u00adcal invariants re.ne continuations. A local invariant essentially consists of two related components \nan assumption of the cur\u00adrent state and a list of progress continuations which are used for handling \ncode pointers. To allow the VCGen extension to manipu\u00adlate predicates using .rst-order logic, only a \nsyntactically restricted form of invariants are used. Although this is necessary for auto\u00admatic proof \nconstruction for type-safety, it is insuf.cient in han\u00addling higher-order code pointers in general. As \na result, these local invariants are only used to handle more gracefully certain .xed pat\u00adterns of code \npointers, such as return pointers. Other situations, such as virtual dispatch, would still require whole-program \nanal\u00adysis for the VCGen extension to discharge the decoder s indirect continuations. In particular, it \nis unclear how this approach extends to support arbitrary safety policies and nested continuations. Reynolds \n[38] identi.ed a similar problem in separation logic and referred to it as embedded code pointers, which \nare dif.cult to describe in the .rst-order world of Hoare logic. He specu\u00adlated that a potential solution \nlies in marrying separation logic with continuation-passing style. The idea was only described brie.y \nand informally, and a convincing development remains yet to come forth. Recently, O Hearn et al. [31] \ninvestigated proof rules for modularity and information hiding for .rst-order procedures using separation \nlogic. However, it is unclear how their approach extends to support higher-order features. It is also \nworth mentioning that related problems on higher-order procedures in Hoare logic can be traced back to \nthe late seventies [7, 8]. We are currently working on a system which addresses this issue more properly. \nIt involves a uni.ed framework for type systems and Hoare logic, and allows reasoning using both types \nand predicates. A similar idea on deploying a logic in a type system for an assembly language is due \nto Ahmed and Walker [3]. We intend to present this work, which is suf.ciently interesting and self-contained, \nin a separate paper.  6.4 Other Future Work CCAP is based on an abstract concurrent machine which shields \nus from the details of thread management such as creation and scheduling. However, many concurrent programs \nare executed on sequential machines, relying on thread libraries to take charge in thread management. \nOne interesting possibility is to implement a certi.ed thread library using a sequential CAP language \nby veri\u00adfying the implementation of thread primitives, which are really se\u00adquential programs with advanced \nhandling of code pointers. This work helps bring CCAP to a more solid ground regarding trustwor\u00adthy computing. \nWe are also working on the application of the CAP languages to a realistic machine model, such as the \nIntel Architecture (x86). The intention is to verify the very same assembly code as those running on \nactual processors. Our experience indicates that the tasks in\u00advolved here are mostly engineering issues, \nincluding the handling of .xed-size integers, byte-aligned (as opposed to word-aligned) addressing mode, \n.nite memory model (restricted by word-size), and encoding and decoding of variable-length instructions. \nWe be\u00adlieve, however, that these aspects are orthogonal to the reasoning of most program properties of \ninterest and hence their handling can be facilitated with proper proof libraries.  7 Conclusion We have \npresented a language CCAP for verifying safety proper\u00adties of concurrent assembly code. The language \nis modeled based on a concurrent abstract machine, adapting established techniques for concurrency veri.cation \nat an assembly level. CCAP has been developed using the Coq proof assistant, alone with a formal sound\u00adness \nproof and the veri.ed example CCAP programs. Only small examples are given in this paper for illustrative \npurposes, but prac\u00adtical applications are within reach, especially for small-scale soft\u00adware such as \ncore libraries, critical components or embedded sys\u00adtems. Its potential application to large-scale software \ncalls for a further development on modularity.  8 Acknowledgment We thank the anonymous referees for \nsuggestions on improving the presentation. 9 References [1] M. Abadi and L. Lamport. Composing speci.cations. \nACM Trans. on Programming Languages and Systems, 15(1):73 132, 1993. [2] M. Abadi and L. Lamport. Conjoining \nspeci.cations. ACM Trans. on Programming Languages and Systems, 17(3):507 535, 1995. [3] A. Ahmed and \nD. Walker. The logical approach to stack typing. In Proceedings of the 2003 ACM SIGPLAN international \nworkshop on Types in languages design and implementation, pages 74 85. ACM Press, 2003. [4] A. W. Appel. \nFoundational proof-carrying code. In Proc. 16th An\u00adnual IEEE Symposium on Logic in Computer Science, \npages 247 258. IEEE Computer Society, June 2001. [5] A. Cau and P. Collette. Parallel composition of \nassumption\u00adcommitment speci.cations. Acta Informatica, 33(2):153 176, 1996. [6] B.-Y. E. Chang, G. C. \nNecular, and R. R. Schneck. Extensible code veri.cation. Unpublished manuscript, 2003. [7] E. M. Clarke. \nProgramming language constructs for which it is im\u00adpossible to obtain good Hoare axiom systems. In Journal \nof the Asso\u00adciation for Computing Machinery, pages 129 147, Jan. 1979. [8] E. M. Clarke. The characterization \nproblem for Hoare logics. In C. A. R. Hoare and J. C. Shepherdson, editors, Mathematical Logic and Programming \nLanguages, pages 89 106. Prentice Hall, 1985. [9] W.-P. de Roever, F. de Boer, U. Hannemann, J. Hooman, \nY. Lakhnech, M. Poel, and J. Zwiers. Concurrency Veri.cation: Introduction to Compositional and Noncompositional \nMethods. Cambridge Univer\u00adsity Press, Cambridge, UK, 2001. [10] E. W. Dijsktra. Cooperating sequential \nprocesses. In F. Genuys, editor, Programming Languages, pages 43 112. Academic Press, 1968. [11] C. Flanagan, \nS. N. Freund, and S. Qadeer. Thread-modular veri.cation for shared-memory programs. In Proc. 2002 European \nSymposium on Programming, pages 262 277. Springer-Verlag, 2002. [12] R. W. Floyd. Assigning meanings \nto programs. In A. M. Society, editor, Proceedings of the Symposium on Applied Math. Vol. 19, pages 19 \n31, Providence, R.I., 1967. [13] N. Francez and A. Pnueli. A proof method for cyclic programs. Acta Informatica, \n9:133 157, 1978. [14] N. Glew and G. Morrisett. Type-safe linking and modular assembly language. In Proc. \n26th ACM Symp. on Principles of Prog. Lang., pages 250 261. ACM Press, Jan. 1999. [15] N. A. Hamid, Z. \nShao, V. Trifonov, S. Monnier, and Z. Ni. A syntactic approach to foundational proof-carrying code. In \nProc. Seventeenth Annual IEEE Symposium on Logic In Computer Science (LICS 02), pages 89 100. IEEE Computer \nSociety, July 2002. [16] T. A. Henzinger, S. Qadeer, S. K. Rajamani, and S. Tasiran. An assume-guarantee \nrule for checking simulation. In Formal Methods in Computer-Aided Design, pages 421 432, 1998. [17] C. \nA. R. Hoare. An axiomatic basis for computer programming. Com\u00admunications of the ACM, 12(10):576 580, \nOct. 1969. [18] C. A. R. Hoare. Proof of a program: FIND. Communications of the ACM, 14(1):39 45, Jan. \n1971. [19] C. A. R. Hoare. Communicating sequential processes. Commun. ACM, 21(8):666 677, 1978. [20] \nT. Hoare. The verifying compiler: A grand challenge for computing research. In Proc. 2003 International \nConference on Compiler Con\u00adstruction (CC 03), LNCS Vol. 2622, pages 262 272, Warsaw, Poland, Apr. 2003. \nSpringer-Verlag Heidelberg. [21] J. Hooman, W.-P. de Roever, P. Pandya, Q. Xu, P. Zhou, and H. Schep\u00aders. \nA compositional approach to concurrency and its applications. Incomplete manuscript. http://www.informatik.uni-kiel.de/ \ninf/deRoever/books/, Apr. 2003. [22] W. A. Howard. The formulae-as-types notion of constructions. In \nTo H.B.Curry: Essays on Computational Logic, Lambda Calculus and Formalism, pages 479 490. Academic Press, \n1980. [23] C. B. Jones. Tentative steps toward a development method for interfer\u00ading programs. ACM Trans. \non Programming Languages and Systems, 5(4):596 619, 1983. [24] L. Lamport. The temporal logic of actions. \nACM Trans. on Program\u00adming Languages and Systems, 16(3):872 923, May 1994. [25] L. Lamport and F. B. \nSchneider. The Hoare logic of CSP, and all that. ACM Trans. on Programming Languages and Systems, 6(2):281 \n296, Apr. 1984. [26] R. Milner. A Calculus of Communicating Systems. Springer-Verlag New York, Inc., \n1982. [27] J. Misra and K. M. Chandy. Proofs of networks of processes. IEEE Transactions on Software \nEngineering, 7(7):417 426, 1981. [28] G. Necula. Proof-carrying code. In Proc. 24th ACM Symp. on Prin\u00adciples \nof Prog. Lang., pages 106 119, New York, Jan. 1997. ACM Press. [29] G. Necula and P. Lee. Safe kernel \nextensions without run-time check\u00ading. In Proc. 2nd USENIX Symp. on Operating System Design and Impl., \npages 229 243, 1996. [30] G. C. Necula and R. R. Schneck. A sound framework for untrustred veri.cation-condition \ngenerators. In Proceedings of IEEE Symposium on Logic in Computer Science, pages 248 260. IEEE Computer \nSoci\u00adety, July 2003. [31] P. W. O Hearn, H. Yang, and J. C. Reynolds. Separation and informa\u00adtion hiding. \nIn Proc. 29th ACM Symp. on Principles of Prog. Lang., pages 268 280, Venice, Italy, Jan. 2004. ACM Press. \n[32] M. Ossefort. Correctness proofs of communicating processes: Three illustrative examples from the \nliterature. ACM Trans. on Programming Languages and Systems, 5(4):620 640, Oct. 1983. [33] S. Owicki \nand D. Gries. An axiomatic proof technique for parallel programs. Acta Informatica, 6(4):319 340, 1976. \n[34] S. Owicki and L. Lamport. Proving liveness properties of concur\u00adrent programs. ACM Trans. on Programming \nLanguages and Systems, 4(3):455 495, July 1982. [35] C. Paulin-Mohring. Inductive de.nitions in the system \nCoq rules and properties. In M. Bezem and J. Groote, editors, Proc. TLCA, vol\u00adume 664 of LNCS. Springer-Verlag, \n1993. [36] A. Pnueli. In transition from global to modular temporal reasoning about programs. Logics \nand models of concurrent systems, pages 123 144, 1985. [37] J. H. Reppy. CML: A higher concurrent language. \nIn Proc. 1991 Conference on Programming Language Design and Implementation, pages 293 305, Toronto, Ontario, \nCanada, 1991. ACM Press. [38] J. C. Reynolds. Separation logic: A logic for shared mutable data structures. \nIn Proceedings Seventeenth Annual IEEE Symposium on Logic in Computer Science, pages 55 74, Los Alamitos, \nCalifornia, July 2002. IEEE Computer Society. [39] Z. Shao, B. Saha, V. Trifonov, and N. Papaspyrou. \nA type system for certi.ed binaries. In Proc. 29th ACM Symp. on Principles of Prog. Lang., pages 217 \n232, Portland, OR, Jan. 2002. ACM Press. [40] E. W. Stark. A proof technique for rely/guarantee properties. \nIn S. N. Maheshwari, editor, Proc. 5th Conference on Foundations of Software (Program)]::=(\u00a7,[11...1n],i)where \ni E{1...n} (State)\u00a7::=(M,l) (Memory)M::={1.w}* (RegFile)l::={r.w}* {rk}kE{0...7} (Register)r::= (Thread)1::=(q,I) \n(CdHeap)q::={f.I} * (Labels)f,1::=n(nat nums) (WordVal)w::=n(nat nums) (InstrSeq)I::=;IIjd f (Comm)::=yield \nIadd rd ,rs,rt Isub rd ,rs,rt Imovi rd ,wIbgt rs,rt ,fIbe rs,rt ,f Ild rd ,rs(w)Ist rd (w),rs (ProgSpec)F \n::=(Inv,[.1 ....n],[A1 ...An ],[G1 ...Gn ]) (CHSpec). ::={f.(p,g)}* (ThrdSpec)T ::=(Inv,.,A,G) (Invariant)Inv \nEState -Prop (Assert)pEState -Prop (Assume)AEState -State-Prop (Guarantee)G,gEState -State-Prop Figure \n15. Syntax of a generalized CCAP. Technology and Theoretical Computer Science, volume 206 of LNCS, pages \n369 391, New Delhi, 1985. Springer-Verlag. [41] C. Stirling. A generalization of Owicki-Gries s Hoare \nlogic for a con\u00adcurrent while language. Theoretical Computer Science, 58(1-3):347 359, 1988. [42] The \nCoq Development Team. The Coq proof assistant reference man\u00adual. The Coq release v7.1, Oct. 2001. [43] \nThe FLINT Project. Coq (v7.3.1) implementation for CCAP language, soundness and examples. http://flint.s.yale.edu/flint/ \npubliations/vsa.html(17k), Mar. 2004. [44] A. K. Wright and M. Felleisen. A syntactic approach to type \nsound\u00adness. Information and Computation, 115(1):38 94, 1994. [45] Q. Xu, A. Cau, and P. Collette. On \nunifying assumption-commitment style proof rules for concurrency. In International Conference on Con\u00adcurrency \nTheory, pages 267 282, 1994. [46] D. Yu, N. A. Hamid, and Z. Shao. Building certi.ed libraries for PCC: \nDynamic storage allocation. In Proc. 2003 European Symposium on Programming, LNCS Vol. 2618, pages 363 \n379, Warsaw, Poland, Apr. 2003. Springer-Verlag. [47] D. Yu, N. A. Hamid, and Z. Shao. Building certi.ed \nlibraries for PCC: Dynamic storage allocation. Science of Computer Program\u00adming, 50(1-3):101 127, 2004. \n[48] Y. Yu. Automated proofs of object code for a widely used micropro\u00adcessor. PhD thesis, University \nof Texas at Austin, Austin, TX, 1992. A CCAP with Multiple Threads The CCAP presented in Section 3 supports \nonly two threads, which is easy to understand yet suf.cient in demonstrating all the key ideas. This \nappendix gives a generalized account. Figures 15 and 16 give the syntax and operational semantics of \na generalized CCAP supporting more than two threads. The auxiliary state update macro is the same as \nshown in Figure 7. Inference rules and soundness lemmas follow. In particular, a generalized inference \nrule for well\u00adformed program is given to support multiple threads. The inference rules for well-formed \ncode heap and well-formed instruction se\u00adquence, in contrast, remain exactly the same as in the two-threaded \nversion (these rules are gathered here for ease of reference). This demonstrates that CCAP is indeed \nthread-modular. ((M,l),[...(qi ,Ii)...],i)>-]where if Ii = then ]= jd f ((M,l),[...(qi ,I)...],i)where \nqi (f)=I yield;I ((M,l),[...(qi ,I)...],i )where i E{1...n} bgt rs,rt,f;I ((M,l),[...(qi ,I)...],i)when \nl(rs):l(rt); and ((M,l),[...(qi ,I)...],i)when l(rs)>l(rt)where qi (f) = I be rs,rt,f;I ((M,l),[...(qi \n,I)...],i)when l(rs)=l(rt); and ((M,l),[...(qi ,I)...],i)when l(rs)=l(rt)where qi (f) = I ;Ifor remaining \ncases of (Next(,(M,l)),[...(qi ,I)...],i) Figure 16. Operational semantics of a generalized CCAP.  Note \nthat we do not support the dynamic creation and termination of threads in this language, but extensions \non them are natural and have little to do with the concurrency veri.cation techniques pre\u00adsented in this \npaper. To be speci.c, even for a dynamically created thread, the code has to be written beforehand, and \nthe behavior has to obey the global invariant, the assumptions and the guarantees. Hence the thread s \ncounterparts exist statically in the code heap and the speci.cation. The safety reasoning used for a \nCCAP program remains unchanged, no matter how many individual threads are cre\u00adated for the same code. \nTherefore, the addition of thread creation and termination mostly affects the operational semantics, \nand the inference rules can be adapted with little effort. Well-formed program F;([p1 ...pn],g)i] F =(Inv,[.1 \n....n],[A1 ...An ],[G1 ...Gn ]) Tk =(Inv,.k,Ak ,Gk )Tk iqk Vk E{1...n} (InvApi \u00a7)V\u00a7.(g\u00a7\u00a7)=(pk \u00a7)Vk =i \nV\u00a7.V\u00a7.(InvApk \u00a7)=(Ak \u00a7\u00a7)=(pk \u00a7)Vk =i Ti;(pi,g)iIi Tk;(pk,Gk )iIk Vk =i (PROG) F;([p1 ...],g)i(\u00a7,[(q1 \n,I1)...(qn ,In)],i) pn Well-formed code heap T iq T =(Inv,.,A,G) (pj,gj)}jE{1...m} . ={fj . T;(pj,gj)iIj \nVj E{1...m} (CODEHEAP) Ij}jE{1...m} T i{fj .  Well-formed instruction sequence T;(p,g)iI T =(Inv,.,A,G) \nV\u00a7.(InvAp\u00a7)=(g\u00a7\u00a7) V\u00a7.V\u00a7.(InvAp\u00a7)=(A\u00a7\u00a7)=(p\u00a7) T;(p,G)iI (YIELD) T;(p,g)iyield;I T =(Inv,.,A,G) V\u00a7.(InvAp\u00a7)=(InvApNext(,\u00a7)) \nV\u00a7.V\u00a7.(InvAp\u00a7)=(gNext(,\u00a7)\u00a7)=(g\u00a7\u00a7) T;(p,g)iIE{add,sub,movi} (SIMPL) T;(p,g)i;I T =(Inv,.,A,G) VM.Vl.(InvAp(M,l))=((l(rs)w)Edom(M)) \nV\u00a7.(InvAp\u00a7)=(InvApNext(,\u00a7)) V\u00a7.V\u00a7.(InvAp\u00a7)=(gNext(,\u00a7)\u00a7)=(g\u00a7\u00a7) T;(p,g)iI=ld rd,rs(w) (LD) T;(p,g)i;I \nT =(Inv,.,A,G) VM.Vl.(InvAp(M,l))=((l(rd)w)Edom(M)) V\u00a7.(InvAp\u00a7)=(InvApNext(,\u00a7)) V\u00a7.V\u00a7.(InvAp\u00a7)=(gNext(,\u00a7)\u00a7)=(g\u00a7\u00a7) \nT;(p,g)iI=st rd(w),rs (ST) T;(p,g)i;I T =(Inv,.,A,G).(f)=(p,g) V\u00a7.(InvAp\u00a7)=(p\u00a7) V\u00a7.V\u00a7.(InvAp\u00a7)=(g\u00a7\u00a7)=(g\u00a7\u00a7) \n(JD) T;(p,g)ijd f T =(Inv,.,A,G).(f)=(p,g) VM.Vl.(l(rs)>l(rt))=(InvAp(M,l))=(p(M,l)) VM.Vl.V\u00a7.(l(rs)>l(rt))=(InvAp(M,l)) \n=(g(M,l)\u00a7)=(g(M,l)\u00a7) VM.Vl.(l(rs):l(rt))=(InvAp(M,l))=(p(M,l)) VM.Vl.V\u00a7.(l(rs):l(rt))=(InvAp(M,l)) =(g(M,l)\u00a7)=(g(M,l)\u00a7) \nT;(p,g)iI T;(p,g)ibgt rs,rt,f;I (BGT) T =(Inv,.,A,G).(f)=(p,g) VM.Vl.(l(rs)=l(rt))=(InvAp(M,l))=(p(M,l)) \nVM.Vl.V\u00a7.(l(rs)=l(rt))=(InvAp(M,l)) =(g(M,l)\u00a7)=(g(M,l)\u00a7) VM.Vl.(l(rs)=l(rt))=(InvAp(M,l))=(p(M,l)) VM.Vl.V\u00a7.(l(rs)=l(rt))=(InvAp(M,l)) \n=(g(M,l)\u00a7)=(g(M,l)\u00a7) T;(p,g)iI T;(p,g)ibe rs,rt,f;I (BE) Lemma 5 (Progress) Let F = ( Inv,[.1 . . . \n.n],[A1 . . . An ],[G1 . . . Gn ]).If F;([p1 . . . pn],g)i(\u00a7,[(q1 ,I1). . . (qn ,In)],i)where i E { 1. \n. . n}, then (Inv \u00a7)and there exists a program ]such that (\u00a7,[(q1 ,I1). . . (qn ,In)],i)>-]. Lemma 6 \n(Preservation) Let F = ( Inv,[.1 . . . .n],[A1 . . . An ],[G1 . . . Gn ]). Suppose V\u00a7.V\u00a7 .(G j \u00a7 \u00a7 )=(Ak \n\u00a7 \u00a7 )for all j =k.If F;([p1 . . . pn],g)i(\u00a7,[(q1 ,I1). . . (qn ,In)],i)where i E { 1. . . n}, and (\u00a7,[(q1 \n,I1)...(qn ,In)],i)>-], then there exists p1,...,pn and gsuch that F;([p1 ...pn],g)i].  \n\t\t\t", "proc_id": "1016850", "abstract": "Concurrency, as a useful feature of many modern programming languages and systems, is generally hard to reason about. Although existing work has explored the verification of concurrent programs using high-level languages and calculi, the verification of concurrent assembly code remains an open problem, largely due to the lack of abstraction at a low-level. Nevertheless, it is sometimes necessary to reason about assembly code or machine executables so as to achieve higher assurance.In this paper, we propose a logic-based \"type\" system for the static verification of concurrent assembly programs, applying the \"invariance proof\" technique for verifying general safety properties and the \"assume-guarantee\" paradigm for decomposition. In particular, we introduce a notion of \"local guarantee\" for the thread-modular verification in a non-preemptive setting.Our system is fully mechanized. Its soundness has been verified using the Coq proof assistant. A safety proof of a program is semi-automatically constructed with help of Coq, allowing the verification of even undecidable safety properties. We demonstrate the usage of our system using three examples, addressing mutual exclusion, deadlock freedom, and partial correctness respectively.", "authors": [{"name": "Dachuan Yu", "author_profile_id": "81100471723", "affiliation": "Yale University, New Haven, CT", "person_id": "PP28016610", "email_address": "", "orcid_id": ""}, {"name": "Zhong Shao", "author_profile_id": "81351597965", "affiliation": "Yale University, New Haven, CT", "person_id": "PP14127817", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1016850.1016875", "year": "2004", "article_id": "1016875", "conference": "ICFP", "title": "Verification of safety properties for concurrent assembly code", "url": "http://dl.acm.org/citation.cfm?id=1016875"}