{"article_publication_date": "09-19-2004", "fulltext": "\n Improving the Static Analysis of Embedded Languages via Partial Evaluation David Herman Philippe Meunier \ndherman@ccs.neu.edu meunier@ccs.neu.edu College of Computer and Information Science Northeastern University \n360 Huntington Ave #202 WVH Boston, MA 02115 Abstract Programs in embedded languages contain invariants \nthat are not au\u00adtomatically detected or enforced by their host language. We show how to use macros to \neasily implement partial evaluation of embed\u00added interpreters in order to capture invariants encoded \nin embedded programs and render them explicit in the terms of their host lan\u00adguage. We demonstrate the \neffectiveness of this technique in im\u00adproving the results of a value .ow analysis. Categories and Subject \nDescriptors D.2.5 [Software Engineering]: Testing and Debugging debug\u00adging aids; D.3.3 [Programming Languages]: \nLanguage Con\u00adstructs and Features macros; D.3.4 [Programming Languages]: Processors code generation, \ndebuggers, interpreters General Terms Reliability, Languages Keywords Partial evaluation, macros, embedded \nlanguages, value .ow analy\u00adsis 1 One Language, Many Languages Every practical programming language contains \nsmall program\u00adming languages. For example, C s printf [18] supports a string\u00adbased output formatting \nlanguage, and Java [3] supports a declara\u00adtive sub-language for laying out GUI elements in a window. \nPLT Scheme [9] offers at least .ve such languages: one for formatting console output; two for regular \nexpression matching; one for send\u00ading queries to a SQL server; and one for laying out HTML pages. Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. ICFP 04, September \n19 21, 2004, Snowbird, Utah, USA. Copyright 2004 ACM 1-58113-905-5/04/0009 ...$5.00 In many cases, though \nnot always, programs in these embedded special-purpose programming languages are encoded as strings. \nLi\u00adbrary functions consume these strings and interpret them. Often the interpreters consume additional \narguments, which they use as in\u00adputs to the little programs. Take a look at this expression in PLT Scheme: \n(regexp-match \"http://([a-z.]*)/([a-z]*)/\" line) The function regexp-match is an interpreter for the \nregular ex\u00adpression language. It consumes two arguments: a string in the regu\u00adlar expression language, \nwhich we consider a program, and another string, which is that program s input. A typical use looks like \nthe example above. The .rst string is actually speci.ed at the call site, while the second string is \noften given by a variable or an expression that reads from an input port. The interpreter attempts to \nmatch the regular expression and the second string. In PLT Scheme, the regular expression language allows \nprogram\u00admers to specify subpatterns via parentheses. Our running example contains two such subexpressions: \n([a-z.]*) and ([a-z]*).If the regular expression interpreter fails to match the regular expres\u00adsion and \nthe string, it produces false (#f); otherwise it produces a list with n + 1 elements: the .rst one for \nthe overall match plus one per subexpression. Say line stands for \"http://aaa.bbb.edu/zzz/\" In this \ncase, the regular expression matches the string, and regexp-match produces the list (list \"http://aaa.bbb.edu/zzz/\" \n\"aaa.bbb.edu\" \"zzz\") The rest of the Scheme program extracts the pieces from this list and computes \nwith them. The regexp-match expression above is a simpli.ed excerpt from the PLT Web Server [12]. Here \nis a slightly larger fragment: (let ([r (regexp-match \"http://([a-z.]*)/([a-z]*)/\" line)]) (if r (process-url \n(third r) (dispatch (second r))) (log-error line))) Notice how the then-clause of the if-expression \nextracts the second and third elements from r without any checks to con.rm the length of the list. After \nall, the programmer knows that if r is not false, then it is a list of three elements. The embedded program \nsays so; it is a regular expression and contains two subexpressions. Unfortunately, the static analysis \ntools for PLT Scheme cannot reason on both levels. MrFlow [20], a static debugger, uses a constraint-based \nanalysis [22], a version of set-based analy\u00adsis [2, 13, 10], to analyze the program and discover potential \ner\u00adrors. If it .nds one it can draw a .ow graph from the source of the bad value to the faulty primitive \noperation. For the let-expression above, MrFlow .nds that both (second r) and (third r) may raise runtime \nerrors because r may not contain enough elements. In this paper, we show how using Scheme macros to partially \neval\u00aduate calls to embedded interpreters such as regexp-match greatly increases the precision of the \nstatic analysis. Since we use macros, library designers can easily implement the partial evaluation, \nrather than relying on the host language implementor as they must for ad\u00adhoc solutions. In Section 2 \nwe give a brief overview of set-based analysis and Mr-Flow. In the next section we explain three examples \nof embedded languages and the problems they cause for MrFlow s static anal\u00adysis. We then present in Section \n4 our general approach to solv\u00ading those problems, based on macros. An overview of the macro system we \nuse is given in Section 5. Section 6 then presents a gen\u00aderal technique for translating embedded interpreters \ninto macros. In Section 7, we explain the properties of the static analysis that en\u00adable it to .nd more \nresults in partially evaluated code. Finally, in Section 8, we show how partially evaluating Scheme programs \nthat contain embedded programs helps MrFlow in our three examples. Section 9 presents related work and \nwe conclude in Section 10. 2 Set-Based Analysis To explain how the results of a static analysis can \nbe improved by using partial evaluation of embedded languages, we .rst need to describe such an analysis. \nMrFlow, a static analyzer for DrScheme, uses a set-based value .ow analysis to compute an approximation \nof the values that each subexpression of a program might evaluate to at runtime [22]. The approximation \ncomputed for each expression is a set of abstract values that can be displayed on demand. The debugger \ncan also draw arrows showing the .ow of values through the program. Figure 1 displays an example of analyzing \na simple program. In the box next to the term 3 is the abstract value for that term, meaning that at \nruntime the term 3 might evaluate to the value 3. The arrow starting from the term 3 shows that at runtime \nthe value 3 might .ow into the argument x of the function f and from there .ow into the reference to \nthe variable x in the body of f. There is a second reference to x in f the corresponding arrow is not \nshown in this example. In the box next to the call to the Scheme primitive gcd is the abstract value \nfor the result of that call. Since the analysis never tries to evaluate expressions, it uses the abstract \nvalue integer to represent the result of the primitive call, if any, which is a conservative approximation \nof the actual value that that call might compute at runtime. The biggest box displays the type of the \nadjacent if-expression, which is the union of the integer abstract value computed by the gcd primitive \nand of the string hello . Arrows show that the result of the if-expression can come from both the then-and \nelse-branches: the analysis does not attempt to apply the number? predicate to the variable x, so it \nconservatively assumes that both branches of the if-expression may be evaluated at runtime. 3 Three \nEmbedded Languages We now turn to embedded languages, which are a useful technique for establishing abstraction \nlayers for a particular design space. Functional languages are well-suited to writing interpreters for \nem\u00adbedded languages, in which the higher-level embedded language is implemented as a set of functions \nin the general purpose host lan\u00adguage and has access to all of its features [15, 16, 24]. But these abstractions \ncome at a cost for program analysis. In particular, tools built to examine programs of the host language \ncannot derive infor\u00admation for the programs in the embedded languages because they do not understand \nthe semantics of those languages. In this section we demonstrate three examples of practical embed\u00added \nlanguages for Scheme and show their negative effects on static analysis. In the .rst example, properties \nof the embedded language create the possibility of errors that can go undetected by the analy\u00adsis. In \nthe next two examples, undetected properties lead to analyses that are too conservative, resulting in \nmany false positives; that is, the analysis reports errors that can never actually occur. 3.1 Format \nStrings The PLT Scheme library provides a format function, similar to C s sprintf, which generates a \nstring given a format speci.er and a variable number of additional arguments. The format speci.er is \na string containing some combination of literal text and formatting tags. These tags are interpreted \nalong with the remaining arguments to construct a formatted string. The format function is thus an interpreter \nfor the format speci.er language. The format speci.er is a program in this language and the additional \narguments are its inputs. To construct its output, the format function requires the number of extra arguments \nto match the number of format tags, and these arguments must be of the appropriate type. Consider the \nexample of displaying an ASCII character and its encoding in hexadecimal: (format \" c = 0x x\" c n) In \nthis example, the format speci.er, which contains the format tags \" c\" and \" x\" and some literal text, \nexpects to consume exactly two arguments. These arguments must be a character and an in\u00adteger, respectively. \nAn incorrect number of arguments or a type mismatch results in a runtime error. Unfortunately analysis \ntools for Scheme such as MrFlow have no a priori knowledge of the semantics of embedded languages. The \nanalysis cannot infer any information about the dependencies be\u00adtween the contents of the format string \nand the rest of the arguments without knowledge of the syntax and semantics of the format lan\u00adguage. \nAs a result the analysis cannot predict certain categories of runtime errors, as shown in Figure 2. The \napplication of format is not underlined as an error, even though its arguments appear in the wrong order \nand the analysis correctly computes the types of both c and n. Figure 1. Analyzing a simple program \nwith MrFlow. 3.2 Regular Expressions Regular expressions are used in all kinds of Scheme programs. The \nlanguage of regular expression patterns is embedded in Scheme as strings. A library of functions interpret \nthese strings as programs that consume additional arguments as input strings and return either a list \nof matched subpatterns or #f to indicate failure. Consider again the excerpt from the PLT Web Server \nfrom Sec\u00adtion 1. Programmers know that if the match succeeds, then the result list contains exactly three \nelements: the result of the entire match, and the results of the two subpattern matches. Again the analysis \nis unable to discover this invariant on its own. Figure 3 shows the results of analyzing the sample code \nwith MrFlow. The list accessors second and third are underlined in red because the analysis cannot prove \nthat their arguments are suf.ciently long lists. Programmers then must either go through each of these \nfalse pos\u00aditives and prove for themselves that the errors can never occur, or else learn to ignore some \nresults of MrFlow. Neither option is de\u00adsirable. The former creates more work for the programmer, rather \nthan less; the latter is unsafe and easily leads to overlooked errors. 3.3 SchemeQL SchemeQL [28] is \nan embedded language for manipulating rela\u00adtional databases in Scheme. Unlike the string-based format \nlan\u00adguage, SchemeQL programs consist of special forms directly em\u00adbedded inside Scheme. The SchemeQL \nimplementation provides a set of macros that recognize these forms and expand them into Scheme code. \nA typical database query in SchemeQL might look like this: (direct-query (name age phone) directory) \n corresponding to the SQL statement SELECT name, age, phone FROM directory The result of executing a \nquery is a lazy stream representing a cur\u00adsor over the result set from the database server. Each element \nin the stream is a list of values representing a single row of the result set. The cursor computes the \nrows by need when a program selects the next sub-stream. Programmers know that the number of elements \nin each row of a cursor is equal to the number of columns in the original request. Our analysis, however, \ncannot discover this fact automatically. Fig\u00adure 4 shows the results of an analysis of a SchemeQL query \nin the context of a trivial Scheme program. The example query consists of exactly three columns, and \nthe code references the third element of the .rst row. This operation can never fail, but the analysis \nis unable to prove this. Instead, it conservatively computes that row is a list of unknown length: rec-type \ndescribes a recursive abstract value, which in the present case is the union of null and a pair consisting \nof any value (top) and the abstract value itself, creating a loop in the abstract value that simulates \nall possible list lengths. MrFlow there\u00adfore mistakenly reports an error by underlining the primitive \nthird in red, since, according to the analysis, row might have fewer than three elements at runtime. \n 4 Macros for Partial Evaluation All the embedded languages presented in the previous section have one \nthing in common: they can encode invariants that are not vis\u00adible to any analysis of the general purpose \nlanguage in which they are embedded. These invariants can be exposed to analyses in two ways: by extending \nthe analyses in an ad-hoc manner for each em\u00adbedded language so that they understand its semantics, or \n by partially evaluating the embedded interpreters with regard to the embedded programs to make the \ninvariants in the em\u00adbedded programs explicit as invariants in the host language, whenever possible. \n The .rst solution requires modifying each analysis to support each embedded language. The second solution \ncan simply be imple\u00admented from within the host language through the old Lisp trick of using compiler \nmacros [25] as a light-weight partial evaluation mechanism. In the present case, instead of using partial \nevaluation to optimize programs for speed, we use it to increase the precision of program analyses. While \nLisp s compiler macros are different from regular Lisp macros, Scheme s macro system is powerful enough \nthat the equiv\u00adalent of Lisp s compiler macros can be implemented as regular Scheme macros. The partial \nevaluation of embedded interpreters then simply involves replacing the libraries of functions imple\u00ad \n Figure 2. Imprecise analysis of the format primitive. Figure 3. Imprecise analysis of regexp-match. \n Figure 4. Imprecise analysis of a SchemeQL query. menting the interpreters with libraries of semantically \nequivalent macros1. This has the additional advantage that it can be done by the author of the library \nof functions, as opposed to the compiler s or analyzer s implementor in the case of ad-hoc extensions. \nOf course, the partial evaluation of embedded interpreters is only possible when their input programs \nare known statically. For ex\u00adample, it is not possible to expand a call to format if the format\u00adting \nstring given as its .rst argument is computed at runtime. The programmer therefore makes a trade-off \nbetween the precision of analyses and how dynamic the code can be. In practice, though, the embedded \nprograms are often speci.ed statically in user code. Combined with the simplicity of implementing partial \nevaluation with macros, this makes for a useful technique for improving the precision of analyses at \na low cost. In the next two sections, we describe some of the important features of the Scheme macro \nsystem and then explain how we make use of this system to partially evaluate the interpreters of these \nembedded languages to improve the results of static analysis. 5 Macros in Scheme Scheme has a powerful \nmacro system for extending the language with derived expression forms that can be rewritten as expressions \nin the core language. Macros serve as a means of syntactic abstrac\u00adtion. Programmers can generalize syntactic \npatterns in ways that are not possible with functional abstraction. This technology also provides a hook \ninto the standard compiler tool chain by allowing programmers to implement additional program transformations \nbe\u00adfore compilation. In this section we describe the basics of standard Scheme macros and introduce identi.er \nmacros, a generalization of the contexts in which macros can be matched. 5.1 Rule-Based Macros The define-syntax \nspecial form allows the programmer to extend Scheme with derived expression forms. Before compilation \nor exe\u00adcution of a Scheme program, all occurrences of these derived forms are replaced with their speci.ed \nexpansions. The syntax-rules form speci.es macro expansions as rewrite rules. Consider the following \nsimple macro, which de.nes a short\u00adcircuit logical or as a derived form: (define-syntax or (syntax-rules \n() [(or e1 e2) (let ([tmp e1]) (if tmp tmp e2))])) The macro de.nes a single rewrite rule, consisting \nof a pattern and a template. The pattern matches the or keyword in operator posi\u00adtion followed by two \npattern variables e1 and e2, each matching an arbitrary subexpression in argument position. The template \ndi\u00adrects the macro expansion to replace occurrences of the matched pattern with a let-expression constructed \nfrom the matched subex\u00adpressions. 1The transformation is not strictly speaking partial evaluation: the \nreductions performed by the macros are not exactly the ones per\u00adformed by the embedded interpreters. \nHowever, the macros share the techniques and issues of partial evaluation since they simulate parts of \nthe interpreters, and it is therefore useful to describe them as such. Notice that this or form cannot \nbe de.ned as a regular function in Scheme. The second argument is only evaluated if the .rst ar\u00adgument \nevaluates to false. Since Scheme has a strict evaluation semantics, a functional or would necessarily \nevaluate both of its arguments before computing a result. Controlling the evaluation of expressions is \nan important use of Scheme macros. Macros can also abstract over other syntactic forms in ways that functions \ncannot by expanding into second-class language constructs such as define. 5.2 Lexical Scope Macros written \nwith the standard Scheme syntax-rules mech\u00adanism are both hygienic and referentially transparent. Hygienic \nmacro expansion guarantees that binding forms inside the de.ni\u00adtion of the macro template do not capture \nfree variables in macro arguments. Consider the following use of our or macro:2 (or other tmp) . (let \n([tmp1 other]) (if tmp1 tmp1 tmp)) Hygienic expansion automatically renames the variable bound in\u00adside \nthe expanded macro template to avoid capturing the free vari\u00adable in the macro argument. Referential \ntransparency complements hygiene by ensuring that free variables inside the macro template cannot be \ncaptured by the context of the macro call site. For example, if the context that in\u00advokes orrebinds the \nifname, the expansion algorithm renames the binding in the caller s context to avoid capturing the variable \nused in the template body: (let ([if 3]) (or if #f)) . (let ([if1 3]) (let ([tmp if1]) (if tmp tmp #f))) \n The combination of hygiene and referential transparency produces macros that are consistent with Scheme \ns rules of lexical scope and can be invoked anywhere in a program without the danger of unex\u00adpected variable \ncapture.3 5.3 Identi.er Macros The syntax-rules form only matches expressions in which the macro name \noccurs in application position, i.e., as the operator in an application expression. References to a syntax-rules \nmacro in other contexts result in syntax errors: (fold or #f ls) . syntax error PLT Scheme s syntax-id-rules \nform is similar to syntax-rules but matches occurrences of the macro key\u00adword in arbitrary expression \ncontexts: in operator position, operand position, or as the target of an assignment. 2We use the convention \nof representing macro expansion with a double-arrow (.) and ordinary (runtime) evaluation with a single\u00adarrow \n(.). 3Macros can also be de.ned in and exported from modules in PLT Scheme [11]. The following macro \ndemonstrates a hypothetical use of syntax-id-rules: (define-syntax clock (syntax-id-rules (set!) [(set! \nclock e) (set-clock! e)] [(clock e) (make-time-stamp (get-clock) e)] [clock (get-clock)])) The list \nof identi.ers following syntax-id-rules, which was empty in our previous examples, now includes the set! \nidenti\u00ad.er, indicating that set! is to be treated as a keyword rather than a pattern variable. The .rst \nrewrite rule matches expressions in which the clock name occurs as the target of an assignment. The second \nrule is familiar, matching the macro in application position. The .\u00adnal rule matches the identi.er clock \nin any context not matched by the previous two rules. In addition to the usual application context, we \ncan use the clock macro in an argument position: (+ clock 10) . (+ (get-clock) 10) or as a set! target: \n(set! clock 5) . (set-clock! 5) 5.4 Programmatic Macros The language of patterns and templates recognized \nby syntax-rules and syntax-id-rules is actually a special case of Scheme macros. In general, the define-syntax \nform binds a transformer procedure (define-syntax name (lambda (stx) etc. The argument to the transformer \nprocedure is a syntax object, which is similar to an S-expression representing quoted code, but which \nalso encapsulates information about the lexical context of the code, such as source .le location and \nvariable bindings. This context information is essential in allowing DrScheme s language tools to trace \nerrors and binding relationships back to the original source lo\u00adcation in the user s code where a macro \nis invoked. Because syntax objects are so similar to quoted data, the standard library includes the syntax-object->datum \nprocedure, which strips the lexical in\u00adformation from a syntax object and returns its corresponding datum. \nFor example, the datum corresponding to a syntax object represent\u00ading a literal number is its numeric \nvalue, the datum corresponding to an identi.er is a symbol representing the identi.er s name, and so \non. A syntax transformer procedure accepts as its argument a syn\u00adtax object representing the expression \nthat invoked the macro, and produces a new syntax object, which the macro expansion algorithm uses to \nreplace the original expression. All Scheme macros are syntax transformers; although the syntax-rules \nand syntax-id-rules forms do not use the lambda notation, they are themselves implemented as macros that \nexpand to syntax trans\u00adformer procedures. The syntax-case facility allows the construction of macros \nwith pattern matching, as with syntax-rules and syntax-id-rules, but with arbitrary expressions in place \nof templates for the result expressions. For example, the above ormacro would be de.ned as: (define-syntax \nor (lambda (stx) (syntax-case stx () [(or e1 e2) # (let ([tmp e1]) (if tmp tmp e2))]))) The macro is \nalmost the same as before, but for two re.nements. First, the syntax-case form takes the argument stx \nexplicitly, whereas syntax-rules implicitly de.nes a transformer procedure and operates on the procedure \nargument. Second, the result ex\u00adpression is pre.xed by the syntax-quoting # operator, which is analogous \nto Scheme s quote operator . Whereas an expression pre.xed with evaluates to a quoted S-expression, \na # expression becomes a quoted syntax object that also includes lexical informa\u00adtion. Similarly, the \nquasisyntax operator # and unsyntax operator #, behave for syntax objects like the quasiquote and unquote \noper\u00adators for S-expressions, respectively. The use of arbitrary computations in the result expression \nallows macros to expand differently based on the results of actual compu\u00adtations: (define-syntax swap \n (lambda (stx) (syntax-case stx () [(swap a b) (if (and (identifier? # a) (identifier? # b)) # (let \n([tmp b]) (set! b a) (set! a tmp)) (raise-syntax-error swap \"expects identifiers\" stx))]))) In \nthis example, if swap is not given identi.ers as arguments, the raise-syntax-error function uses the \nlexical information in the stx syntax object to highlight the original swap expression in the user s \ncode. Conditional matching can also be achieved using pattern guards, which can inspect a matched expression \nand determine whether to accept a match: (define-syntax swap (lambda (stx) (syntax-case stx () [(swap \na b) (and (identifier? # a) (identifier? # b)) # (let ([tmp b]) (set! b a) (set! a tmp))]))) The \npattern guard is a new expression, inserted between the pattern and the result expressions. A guarded \nmatch only succeeds if its guard does not evaluate to false; when a guard fails, the pattern matcher \nfalls through to attempt the next pattern in the list.  6 Macros for Interpreters In this section, we \npresent a general technique for specializing em\u00adbedded interpreters with macros, and explain how we apply \nthis technique to the three embedded languages described in Section 3. The technique can be summarized \nin the following steps: 1. Write the interpreter compositionally as a module of library functions. 2. \nReplace the interpreter s main function with a macro that un\u00adfolds the case dispatch on the input (the \nembedded program) when it is known statically. 3. Default to the original function when the input is \nnot known at compile time.  Writing the interpreters compositionally serves two purposes. First, by \ndelegating the interpretation of the program constructs that make up an embedded program to separate \nfunctions, it becomes possi\u00adble to share code between the original interpreter and the macro that replaces \nit. This effectively limits the macro s responsibility to a simple dispatch. Second, compositionality \nmakes it easier to guarantee that unfolding terminates, since the recursive macro calls always operate \non smaller terms. 6.1 Format Strings The implementation of a string formatter involves a number of sim\u00adple \nlibrary functions to convert each possible type of argument to strings. Each formatting tag corresponds \nto one of these combi\u00adnators. For example, the \" c\" tag corresponds to a combinator, format/char, which \naccepts a character and converts it to a string, the \" x\" tag corresponds to format/hex, which converts \nintegers to their hexadecimal representation, and so forth. The string for\u00admatter then simply dispatches \nto these combinators based on the content of the formatting string: (define (format s . args) (cond \n [(string=? s \"\") \"\"] [(string=? (substring s 0 2) \" c\") (string-append (format/char (car args)) (apply \nformat (substring s 2) (cdr args)))] etc. )) The interpreter accepts the formatting string s and, \nbased on for\u00admatting tags like \" c\" that it .nds, decomposes the string into a series of applications \nof the corresponding combinators to succes\u00adsive arguments of format (represented by args). It reassembles \nthe transformed pieces with the standard string-append function. In order to specialize the format interpreter, \nwe replace it with a macro that re-uses its associated combinators: (define (format/dynamic s . args) \n as before ) (define-syntax format (lambda (stx) (syntax-case stx () [(format s-exp a1 a2 ...) (string? \n(syntax-object->datum # s-exp)) (let ([s (syntax-object->datum # s-exp)]) (cond [(string=? s \"\") # \n\"\"] [(string=? (substring s 0 2) \" c\") # (string-append (format/char a1) (format #,(substring s 2) \na2 ...))] etc. ))] [(format s-exp a1 a2 ...) # (format/dynamic s-exp a1 a2 ...)] [format (identifier? \n# format) # format/dynamic]))) The partial evaluation works by unfolding the interpreter s top-level \ncase dispatch on the program text. Rather than delaying the inspec\u00adtion of the string to runtime, the \nmacro precomputes the result of the decomposition statically whenever the string is given as a literal. \nWe can identify literal strings through the use of a pattern guard. More precisely, the macro can inspect \nthe syntax object s-exp, corresponding to format s .rst argument, and determine whether it can be converted \nto a string via syntax-object->datum. When the conversion succeeds, the pattern guard allows the match \nto suc\u00adceed, and partial evaluation proceeds. After the macro expansion, the resulting program text consists \nof the application of string-append to the calls to the library func\u00adtions, with no references to the \ninterpreter: (format \" c = 0x x\" c n) . (string-append (format/char c) \" = 0x\" (format/hex n)) In order \nfor the replacement of the original function with a macro to be unobservable, the macro must behave exactly \nlike the origi\u00adnal function in all contexts. When format is applied to a dynamic formatting string, the \nmacro defaults to the original functional im\u00adplementation. Similarly, when format is passed as an argument \nto a higher-order function, we use the technique of identi.er macros to refer to the original function.4 \n 6.2 Regular Expressions One of PLT Scheme s regular expression engines uses the two\u00adcontinuation model \nof backtracking [1]. A regular expression matcher is represented as a function that accepts a success \ncon\u00adtinuation and a failure continuation. When a matcher succeeds in matching its input, it applies its \nsuccess continuation to the accepted input, and when it fails to match, it invokes its failure continuation. \nThis allows the interpretation of the alternation operator | to try each alternate pattern sequentially: \nan alternation matcher tries to match its .rst pattern with a failure continuation to try the second \npattern. Thus if the .rst pattern fails, the matcher invokes the failure continuation, which tries the \nsecond pattern. Otherwise, the failure continuation is disregarded and the matcher applies its success \ncon\u00adtinuation, which skips the second pattern and returns the result of the .rst match. Each of the regular \nexpression constructions corresponds to a func\u00adtional combinator that produces a matcher. These combinators \ncan express the standard operators of regular expressions: suc\u00adcess, failure, alternation, concatenation, \nand repetition (i.e., Kleene star). There is also a submatch combinator for the parenthe\u00adsized subpatterns \nin the original regular expression. A successful regexp-match returns a list with the entire matched \nstring fol\u00adlowed by each submatch corresponding to a parenthesized subpat\u00adtern. Any subpattern that does \nnot match corresponds to an entry of false (#f) in the result list. For example, the following successful \n4The case of set! is not critical since, in PLT Scheme, imported module references cannot be the target \nof an assignment. match contains a failed submatch: (regexp-match \"a((b)|(c))\" \"ac\") . (list \"ac\" \"c\" \n#f \"c\") Regardless of the contents of the second argument, there is always exactly one element in the \nresult list for each parenthesized sub\u00adpattern in the regular expression. The submatch operator accom\u00adplishes \nthis by wrapping a given matcher with continuations that add either the result of a successful match \nor false to a list of in\u00addexed submatches accumulated during the match. The initial (suc\u00adcess) continuation \nfor regexp-match sorts the accumulated list of indexed submatches, adding false entries for all submatches \nthat were never reached because of backtracking. Partial evaluation of the regular expression library \nworks by un\u00adfolding the de.nitions of the combinators as well as the contents of the initial continuation. \nEach application of a combinator gets re\u00adplaced by an application of a copy of the body of the combinator \ns de.nition.5 The recursive code that constructs the result list in the success continuation gets expanded \ninto an explicit chain of cons expressions: (regexp-match \"a((b)|(c))\" input) . ((build-matcher input) \n(lambda (subs) (cons (lookup subs 0) (cons (lookup subs 1) (cons (lookup subs 2) (cons (lookup subs \n3) null))))) (lambda () #f)) Since the size of the result list is known, it is possible to unfold recursive \nde.nitions, such as the initial continuation that constructs the match result, to make the structure \nof the result explicit. Finally, in the cases where the embedded program is not known stat\u00adically, or \nwhen regexp-match is used in non-application contexts, the macro expands to the original functional de.nition. \n 6.3 SchemeQL The SchemeQL language differs from the other examples in that its programs are not embedded \nas strings but rather as special forms recognized by a library of macros. This means that for queries \nthat select from a .xed set of columns, the length of cursor rows is always known statically; the column \nnames are speci.ed as a sequence of identi.ers in the syntax of the query form. Just as the interpreters \nfor the string-based embedded programs perform a case dispatch on the contents of program strings, the \nSchemeQL macros dispatch on the shape of the query expressions. The cases where partial evaluation is \npossible can be captured by inserting additional rules into the original library s macros. Partial evaluation \nof SchemeQL queries uses the same technique as for the regular expression library: the recursive function \nthat con\u00adstructs a cursor row is unfolded into an explicit chain of cons ex\u00adpressions. Since we know \nthe length of the cursor row statically, the unfolding is guaranteed to terminate. 5It is convenient \nto de.ne the Kleene star operator recursively by p * =(pp *)|e. However, this non-compositional de.nition \nleads to an in.nite macro expansion, so the macro must carefully avoid unfolding such a de.nition. Since \nthe SchemeQL library is implemented as macros, there is no need to capture the cases where the query \nforms are used in non\u00adapplication contexts. Adding special cases to the existing macro does not affect \nits set of allowable contexts. Similarly, the cases where the row length is not known statically are \nalready handled by the existing SchemeQL macros.  7 Static Analysis for Scheme MrFlow s value .ow analysis \nis an extension of an ordinary set\u00adbased closure analysis like Palsberg s [22]. For every expression \nin a program, MrFlow statically computes a conservative approxima\u00adtion of the set of values to which \nthe expression might evaluate at runtime. From a given expression it creates a graph that simulates the \n.ow of values inside the expression. The analysis simulates evaluation by propagating abstract values \nin this graph until reach\u00ading a .xed point. From the set of abstract values that propagate to a given \nnode, the analysis reconstructs a type that is then displayed to the user through DrScheme s graphical \ninterface. Extensions to the basic analysis include, among other things: an\u00adalyzing functions that can \ntake any number of arguments, analyz\u00ading assignments to variables (set!), and analyzing generative data \nstructure de.nitions. MrFlow also supports all the primitives de\u00ad.ned in R5RS [17]. The vast majority \nof these primitives are de\u00ad.ned using a special, type-like language embedded inside the an\u00adalyzer. For \na given primitive, the corresponding type translates to a graph that simulates the primitive s internal \n.ows. The analysis then proceeds just like for any other expression. The few remaining primitives need \nspecial handling because of their imperative nature (set-car! or vector-fill!) and are analyzed in an \nad-hoc man\u00adner. By default, MrFlow analyzes the format primitive based on the following pseudo-type description: \n(string top *-> string) The * in the *-> constructor means that the primitive is a function that can \ntake any number of arguments as input beyond the ones explicitly speci.ed. In the present case, the function \nmust receive a string as its .rst argument, followed by any number of arguments of any type (represented \nby the pseudo-type top), and returns a string. Given such a description, the only errors MrFlow detects \nare when the primitive is given something other than a string as .rst argument, or if it is given no \nargument at all. After partial evaluation, the application of format is replaced by calls to its individual \nlibrary functions such as format/char and format/hex. These functions have respectively the pseudo-types \n(char -> string) and (integer -> string) Using this more precise information, MrFlow can detect arguments \nto the original format call that have the wrong type. Checking that the format primitive receives the \nright number of arguments for a given formatting string happens during partial evaluation, so the analyzer \nnever sees arity errors in the expanded code. Since DrScheme s syntax object system keeps track of program \nterms through the macro expansions [11], MrFlow is then able to trace detected errors back to the original \nguilty terms in the user s program and .ag them graphically. Arrows representing the .ow of values can \nalso be displayed interactively in terms of the original program, allowing the user to track in the program \nthe sources of the values that triggered the errors. In essence, the only requirement for MrFlow to analyze \nthe partially evaluated code of format is to specify the pseudo-types for the library functions introduced \nby the transformations, like format/char6. Similarly, it is enough to de.ne pseudo-types for the functions \nused in the partially evaluated form of SchemeQL s query to have MrFlow automatically compute precise \nresults without any further modi.cations. The partial evaluation for regular expressions is more challenging. \nConsider the example from Section 1: (let ([r (regexp-match \"http://([a-z.]*)/([a-z]*)/\" line)]) (if \nr (process-url (third r) (dispatch (second r))) (log-error))) After the call to regexp-match, the \nvariable r can be either a list of three elements or false. Based on its conservative pseudo-type speci.cation \nfor regexp-match, MrFlow computes that r can be either a list of unknown length or false. This in turn \ntriggers two errors for each of the second and third primitives: one error be\u00adcause the primitive might \nbe applied to false when it expected a list, and one error because it might be applied to a list that \nis too short. The second kind of false positives can be removed by partially eval\u00aduating regexp-match \nto make the structure of the result more ex\u00adplicit to MrFlow, as described in Section 6.2. The analysis \nthen determines that the primitive returns either a list of three elements or false and in turn checks \nthat second and third are applied to a list with enough elements. Still, the possible return values of \nregexp-match may contain false. Indeed, false will be the value returned at runtime if the line given \nto regexp-match does not match the pattern. The program\u00admer has to test for such a condition explicitly \nbefore processing the result any further. The only way for MrFlow not to show a false positive for secondand \nthird, because of the presence of this false value, is to make the analysis aware of the dependency between \nthe test of r and the two branches of the if-expression. This form of .ow-sensitive analysis for if-expressions \nis dif.cult to implement in general since there is no bound to the complexity of the tested ex\u00adpression. \nIn practice, however, an appreciable proportion of these tests are simple enough that an ad-hoc solution \nis suf.cient. In the case where the test is simply a variable reference it is enough to create two corresponding \nghost variables, one for each branch of the if, establish .ltering .ows between the variable r and the \ntwo ghost variables, and make sure each ghost variable binds the r variable references in its respective \nbranch of the if-expression. The .ltering .ows prevent the false abstract value from .owing into the \nthen branch of the if-expression and prevent everything but the false value from .owing into the else \nbranch. Only the combination of this .ow sensitivity for if-expressions with the partial evaluation of \nregexp-match gives analysis results with no false positives. 6Specifying such pseudo-types will not even \nbe necessary once MrFlow knows how to analyze PLT Scheme contracts. This is the subject of a forthcoming \npaper. Once .ow-sensitive analysis of if-expressions is added and pseudo-type descriptions of the necessary \nprimitives are provided to the analysis, partial evaluation makes all the false positives de\u00adscribed \nin Section 3 disappear, as we illustrate in the next section. 8 Improvement of Static Analysis Partially \nevaluating format eliminates the possibility of runtime arity errors, since the macro transformations \ncan statically check such invariants. It also allows MrFlow to detect type errors that it could not detect \nbefore, since the corresponding invariants were described only in the embedded formatting language. These \nin\u00advariants are now explicit at the Scheme level in the transformed program through the use of simpler \nprimitives like format/char or format/integer. Figure 5 shows the same program as in Fig\u00adure 2, but after \napplying partial evaluation. The format primitive is now blamed for two type errors that before could \nbe found only at runtime. The error messages show that the user simply gave the arguments n and c in \nthe wrong order. Similarly, specializing the regular expression engine with respect to a pattern eliminates \nfalse positives. The length of the list re\u00adturned by regexp-match cannot be directly computed by the \nanal\u00adysis since that information is hidden inside the regular expression pattern. As a result, the applications \nof second and third in Fig\u00adure 3 are .agged as potential runtime errors (we have omitted the fairly large \nerror messages from the .gure). After specialization, the structure of the value returned by regexp-match \nis exposed to the analysis and MrFlow can then prove that if regexp-match re\u00adturns a list, it must contain \nthree elements. The false positives for second and third disappear in Figure 6. Of course, regexp-match \ncan also return false at runtime, and the analysis correctly predicts this regardless of whether partial \neval\u00aduation is used or not. Adding .ow sensitivity for if-expressions as described in Section 7 removes \nthese last spurious errors in Fig\u00adure 6. Partial evaluation now allows the precise analysis of SchemeQL \nqueries as well. Figure 7 shows the precise analysis of the same program as in Figure 4, this time after \npartial evaluation. As with regexp-match, the analysis previously computed that cursor-car could return \na list of any length, and therefore .agged the call to third as a potential runtime error. This call \nis now free of spurious errors since the partial evaluation exposes enough structure of the list returned \nby cursor-car that MrFlow can compute its exact length and verify that third cannot fail at runtime. \nWhile the results computed by the analysis become more precise, partially evaluating the interpreters \nfor any of the three embedded languages we use in this paper results in code that is bigger than the \noriginal program. Bigger code in turn means that analyses will take more time to complete. There is therefore \na trade-off between preci\u00adsion and ef.ciency of the analyses. We intend to turn that trade-off into a \nuser option in MrFlow. The user might also exercise full control over which embedded languages are partially \nevaluated and where by using either the functional or macro versions of the em\u00adbedded languages interpreters, \nswitching between the two through the judicious use of a module system, for example [11]. Note that partial \nevaluation does not always bene.t all analyses. In the regexp-match example from Figure 6, spurious errors \ndisap\u00adpear because MrFlow has been able to prove that the list r is of length three and therefore that \napplying the primitives second or Figure 5. Precise analysis of the format primitive. Figure 6. Precise \nanalysis of regexp-match. Figure 7. Precise analysis of a SchemeQL query. third to r cannot fail. If \nthe analysis were a Hindley-Milner-like type system, though, no difference would be seen whether partial \nevaluation were used or not. Indeed, while such a type system could statically prove that the arguments \ngiven to second or third are lists, is would not attempt to prove that they are lists of the required \nlength and a runtime test would still be required. Using partial eval\u00aduation to expose such a property \nto the analysis would therefore be useless. Simply put, making invariants from embedded programs explicit \nin the host language only matters if the system analyzing the host language cares about those invariants. \nThis does not mean partial evaluation is always useless when used in conjunction with a Hindley-Milner \ntype system, though. Par\u00adtially evaluating format, for example, would allow the type system to verify \nthat the formatting string agrees with the types of the re\u00admaining arguments. This is in contrast to \nthe ad-hoc solution used in OCaml [19] to type check the printf primitive, or the use of dependent types \nin the case of Cayenne [4]. 9 Related Work Our work is analogous to designing type-safe embedded languages \nsuch as the one for printf [21, 4]. Both problems involve de\u00adtermining static information about programs \nbased on the values of embedded programs. In some cases, designers of typed lan\u00adguages simply extend \nthe host language to include speci.c embed\u00added languages. The OCaml language, for example, contains a \nspe\u00adcial library for printf [19] and uses of printf are type-checked in an ad-hoc manner. Similarly, \nthe GCC compiler for the C lan\u00adguage uses ad-hoc checking to .nd errors in printf format strings. Danvy \n[7] and Hinze [14] suggest implementations of printf in ML and Haskell, respectively, that obviate the \nneed for dependent types by recasting the library in terms of individual combinators. In our system, \nthose individual combinators are automatically in\u00adtroduced during macro expansion. The C++ language [26] \nlikewise avoids the problem of checking invariants for printf by breaking its functionality into smaller \noperations that do not require the use of an embedded formatting language. A work more closely related \nto ours is the Cayenne language [4]. Augustsson uses a form of partial evaluation to specialize depen\u00addent \ntypes into regular Haskell-like types that can then be used by the type system to check the user s program. \nOur macro system uses macro-expansion time computation to specialize expressions so that the subsequent \n.ow analysis can compute precise value .ow results. Augustsson s dependent type system uses computation \nper\u00adformed at type-checking time to specialize dependent types so that the rest of the type checking \ncan compute precise type information. The specialization is done in his system through the use of type\u00adcomputing \nfunctions that are speci.ed by the user and evaluated by the type system. The main difference is that \nhis system is used to compute special\u00adized types and verify that the program is safe. Once the original \nprogram has been typed it is just compiled as-is with type check\u00ading turned off. This means that in the \ncase of format, for example, the formatting string is processed twice: once at type checking time to \nprove the safety of the program, and once again at run time to compute the actual result. Our system \nis used to compute special\u00adized expressions. This means that the evaluation of the format s string needs \nto be done only once. Once specialized, the same pro\u00adgram can either be run or analyzed to prove its \nsafety. In both cases the format string will not have to be reprocessed since it has been completely \nreplaced by more specialized code. Another difference is that in our system, non-specialized programs \nare still valid programs that can be analyzed, proved safe, and run (though the result of the analysis \nwill probably be more conser\u00advative than when analyzing the corresponding partially evaluated program, \nso proving safety might be more dif.cult). This is not possible in Cayenne since programs with dependent \ntypes cannot be run without going through the partial evaluation phase .rst. Much work has gone into \noptimization of embedded languages. Hudak [15], Elliott et al [8], Backhouse [5], Christensen [6], and \nVeldhuizen [27] all discuss the use of partial evaluation to improve the ef.ciency of embedded languages, \nalthough none makes the connection between partial evaluation and static analysis. In Back\u00adhouse s thesis \nhe discusses the need to improve error checking for embedded languages, but he erroneously concludes \nthat syntactic analyses cannot be used due to the embedded nature of domain\u00adspeci.c embedded languages. \nThe Lisp programming language ([25], Section 8.4) provides for compiler macros that programmers can use \nto create optimized versions of existing functions. The compiler is not required to use them, though. \nTo our knowledge, there is no literature showing how to use these compiler macros to improve the results \nof static analyses. Lisp also has support for inlining functions, which might help monovariant analyses \nby duplicating the code of a function at all its call sites, thereby simulating polyvariant analyses. \nBigloo [23] is a Scheme compiler that routinely implements em\u00adbedded languages via macros and thus probably \nprovides some of the bene.ts presented in this paper to the compiler s internal anal\u00adyses. The compiler \nhas a switch to enable optimization by macro expansion, though there does not seem to be any documentation \nor literature describing the exact effect of using that switch. 10 Conclusion Programs in embedded languages \ncontain invariants that are not au\u00adtomatically enforced by their host language. We have shown that using \nmacros to partially evaluate interpreters of little languages embedded in Scheme with respect to their \ninput programs can re\u00adcapture these invariants and convey them to a .ow analysis. Be\u00adcause it is based \non macros, this technique does not require any ad-hoc modi.cation of either interpreters or analyses \nand is thus readily available to programmers. This makes it a sweet spot in the programming complexity \nversus precision landscape of pro\u00adgram analysis. We intend to investigate the relationship between macros \nand other program analyses in a similar manner. Acknowledgments We thank Matthias Felleisen, Mitchell \nWand, and Kenichi Asai for the discussions that led to this work and for their helpful feed\u00adback. Thanks \nto Matthew Flatt for his help with the presentation of Scheme macros. Thanks to Dale Vaillancourt for \nproofreading the paper and to Ryan Culpepper for his macrological wizardry. 11 References [1] H. Abelson \nand G. J. Sussman. The Structure and Interpre\u00adtation of Computer Programs. MIT Press, Cambridge, MA, \n1985. [2] A. Aiken. Introduction to set constraint-based program anal\u00adysis. Science of Computer Programming, \n35:79 111, 1999. [3] K. Arnold, J. Gosling, and D. Holmes. The Java Program\u00adming Language. Addison-Wesley, \n3d edition, 2000. [4] L. Augustsson. Cayenne a language with dependent types. In Proceedings of the third \nACM SIGPLAN international con\u00adference on Functional programming, pages 239 250. ACM Press, 1998. [5] \nK. Backhouse. Abstract Interpretation of Domain-Speci.c Embedded Languages. PhD thesis, Oxford University, \n2002. [6] N. H. Christensen. Domain-speci.c languages in software de\u00advelopment and the relation to partial \nevaluation. PhD the\u00adsis, DIKU, Dept. of Computer Science, University of Copen\u00adhagen, Universitetsparken \n1, DK-2100 Copenhagen East, Denmark, July 2003. [7] O. Danvy. Functional unparsing. Journal of Functional \nPro\u00adgramming, 8(6):621 625, 1998. [8] C. Elliott, S. Finne, and O. de Moor. Compiling embedded languages. \nIn SAIG, pages 9 27, 2000. [9] R. B. Findler, J. Clements, M. F. Cormac Flanagan, S. Kr\u00adishnamurthi, \nP. Steckler, and M. Felleisen. DrScheme: A progamming environment for scheme. Journal of Functional Programming, \n12(2):159 182, March 2002. [10] C. Flanagan and M. Felleisen. Componential set-based anal\u00adysis. ACM Trans. \non Programming Languages and Systems, 21(2):369 415, Feb. 1999. [11] M. Flatt. Composable and compilable \nmacros: you want it when? In Proceedings of the seventh ACM SIGPLAN interna\u00adtional conference on Functional \nprogramming, pages 72 83. ACM Press, 2002. [12] P. Graunke, S. Krishnamurthi, S. V. D. Hoeven, and M. \nFelleisen. Programming the web with high-level pro\u00adgramming languages. In Programming Languages and Sys\u00adtems, \n10th European Symposium on Programming, ESOP 2001, Proceedings, volume 2028 of Lecture Notes in Com\u00adputer \nScience, pages 122 136, Berlin, Heidelberg, and New York, 2001. Springer-Verlag. [13] N. Heintze. Set \nBased Program Analysis. PhD thesis, Carnegie-Mellon Univ., Pittsburgh, PA, Oct. 1992. [14] R. Hinze. \nFormatting: a class act. Journal of Functional Programming, 13(5):935 944, 2003. [15] P. Hudak. Modular \ndomain speci.c languages and tools. In Proceedings of Fifth International Conference on Software Reuse, \npages 134 142, June 1998. [16] S. N. Kamin. Research on domain-speci.c embedded lan\u00adguages and program \ngenerators. In R. Cleaveland, M. Mis\u00adlove, and P. Mulry, editors, Electronic Notes in Theoretical Computer \nScience, volume 14. Elsevier, 2000. [17] R. Kelsey, W. Clinger, and J. R. [editors]. Revised5 report \non the algorithmic language Scheme. Higher-Order and Sym\u00adbolic Computation, 11(1):7 104, August 1998. \nAlso appeared in SIGPLAN Notices 33:9, September 1998. [18] B. W. Kernighan and D. M. Ritchie. The C \nprogramming lan\u00adguage. Prentice Hall Press, 1988. [19] X. Leroy. The Objective Caml System, release 3.07, \n2003. http://caml.inria.fr/ocaml/htmlman. [20] P. Meunier. http://www.plt-scheme.org/software/ mrflow. \n[21] M. Neubauer, P. Thiemann, M. Gasbichler, and M. Sperber. Functional logic overloading. In Proceedings \nof the 29th ACM SIGPLAN-SIGACT symposium on Principles of programming languages, pages 233 244. ACM Press, \n2002. [22] J. Palsberg. Closure analysis in constraint form. Proc. ACM Trans. on Programming Languages \nand Systems, 17(1):47 62, Jan. 1995. [23] M. Serrano and P. Weis. Bigloo: A portable and optimizing compiler \nfor strict functional languages. In Static Analysis Symposium, pages 366 381, 1995. [24] O. Shivers. \nA universal scripting framework, or Lambda: the ultimate little language . In Proceedings of the Second \nAsian Computing Science Conference on Concurrency and Paral\u00adlelism, Programming, Networking, and Security, \npages 254 265. Springer-Verlag, 1996. [25] G. L. Steele. COMMON LISP: the language. Digital Press, 12 \nCrosby Drive, Bedford, MA 01730, USA, 1984. With contri\u00adbutions by Scott E. Fahlman and Richard P. Gabriel \nand David A. Moon and Daniel L. Weinreb. [26] B. Stroustrup. The C++ Programming Language, Third Edi\u00adtion. \nAddison-Wesley Longman Publishing Co., Inc., 1997. [27] T. L. Veldhuizen. C++ templates as partial evaluation. \nIn Par\u00adtial Evaluation and Semantic-Based Program Manipulation, pages 13 18, 1999. [28] N. Welsh, F. \nSolsona, and I. Glover. SchemeUnit and SchemeQL: Two little languages. In Proceedings of the Third Workshop \non Scheme and Functional Programming, 2002.  \n\t\t\t", "proc_id": "1016850", "abstract": "Programs in embedded languages contain invariants that are not automatically detected or enforced by their host language. We show how to use macros to easily implement partial evaluation of embedded interpreters in order to capture invariants encoded in embedded programs and render them explicit in the terms of their host language. We demonstrate the effectiveness of this technique in improving the results of a value flow analysis.", "authors": [{"name": "David Herman", "author_profile_id": "81337490109", "affiliation": "Northeastern University, Boston, MA", "person_id": "PP37029163", "email_address": "", "orcid_id": ""}, {"name": "Philippe Meunier", "author_profile_id": "81336491637", "affiliation": "Northeastern University, Boston, MA", "person_id": "PP36024562", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1016850.1016857", "year": "2004", "article_id": "1016857", "conference": "ICFP", "title": "Improving the static analysis of embedded languages via partial evaluation", "url": "http://dl.acm.org/citation.cfm?id=1016857"}