{"article_publication_date": "09-19-2004", "fulltext": "\n Types, Potency, and Idempotency: Why Nonlinearity and Amnesia Make a Type System Work * Peter M\u00f8ller \nNeergaard Computer Science Department Brandeis University Waltham, MA 02454, USA turtle@achilles.linearity.org \n The online title of this paper was changed according to the author's instructions. The previous title \nwas, \"Types, Potency, and Impotency: Why Nonlinearity and Amnesia Make a Type System Work Harry G. Mairson \nComputer Science Department Brandeis University Waltham, MA 02454, USA mairson@cs.brandeis.edu Abstract \nUseful type inference must be faster than normalization. Otherwise, you could check safety conditions \nby running the program. We an\u00adalyze the relationship between bounds on normalization and type inference. \nWe show how the success of type inference is funda\u00admentally related to the amnesia of the type system: \nthe nonlinearity by which all instances of a variable are constrained to have the same type. Recent work \non intersection types has advocated their usefulness for static analysis and modular compilation. We \nanalyze System-I (and some instances of its descendant, System E), an intersection type system with a \ntype inference algorithm. Because System-I lacks idempotency, each occurrence of a variable requires \na distinct type. Consequently, type inference is equivalent to normalization in every single case, and \ntime bounds on type inference and normal\u00adization are identical. Similar relationships hold for other \nintersec\u00adtion type systems without idempotency. The analysis is founded on an investigation of the relationship \nbe\u00adtween linear logic and intersection types. We show a lockstep cor\u00adrespondence between normalization \nand type inference. The latter shows the promise of intersection types to facilitate static analyses \nof varied granularity, but also belies an immense challenge: to add amnesia to such analysis without \nlosing all of its bene.ts. Categories and Subject Descriptors D.3.1 [Programming Languages]: Formal \nDe.nitions and The\u00adory; F.3.3 [Logics and Meanings of Programs]: Studies of Pro\u00adgram Constructs Program \nand recursion schemes; F.4.1 [Compu\u00ad * Supported by the Danish Research Agency grants 1999-114-0027 and \n642-00-0062 and the NSF grant CCR-9806718. Supported by NSF Grants CCR-9619638, CDA-9806718, and the \nTyson Foundation. Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. ICFP 04, September 19 21, 2004, Snowbird, Utah, USA. Copyright 2004 ACM 1-58113-905-5/04/0009 \n...$5.00 tation by Abstract Devices]: Mathematical Logic Computability theory, Computational logic, Lambda \ncalculus and related systems General Terms Languages, theory, performance  Keywords Intersection types, \ntype inference, proofnet, normalization, com\u00adplexity, idempotence 1 Introduction Those who forget the \npast are condemned to repeat it. George Santayana Type inference is a true success story of programming \nlanguage the\u00adory and pragmatics: added to languages like ML, Haskell, OCaml, it works without the programmer \nworrying (too much) how. Type inference works so well that it is easy to forget the ingenuity which lead \nto its success. Like other forms of static safety analysis, type inference predicts at compile-time the \nbehavior of the program at run-time recall Mil\u00adner s well-typed programs do not go wrong [35]. A crucial \nre\u00adquirement for type inference is that it is substantially faster than running the program otherwise, \nyou could get the same result as follows: run the program, and watch! This ef.ciency has been achieved \nfor simple types [43], the Hindley/Milner ML type sys\u00adtem [18, 35, 32, 25], and some intersection type \nsystems [24]. In these systems, there is an exponential or super-exponential leap from the worst-case \nbound on type inference to the worst-case bound on normalization (running time). As a nonobvious conse\u00adquence, \neven programs with small types can run long enough to do something interesting; this explains why ML \nworks so well. We explain the super-exponential gap via an investigation of in\u00adtersection types, where \npolymorphism is itemized explicitly using a type constructor .: a term of type t . s can have both type \nt and type s. Intersection types have gained recent interest for such varied purposes as .ow analysis \n[4], strictness analysis [20], dead-code elimination [10, 13], and totality analysis [8]. More\u00adover, \nintersection types have been advocated for modular compila\u00adtion [6, 26, 12, 4] as they (usually) have \nprincipal typings [21] that is, there exists a single typing from which all other typings of a term can \nbe derived.1 A signi.cant, interesting, and representative example of an intersection type system is \nthe prototype system Sys\u00adtem-I [28, 26] (and its descendant, System E [6]), which has both principal \ntypings, and automatic type inference [26, 7]. These type systems serve as a foundation of the Church \ncompiler project [48]. We analyze here the relationship in System-I between the cost of type inference, \nand the cost of normalization. We focus on the cru\u00adcial fact that System-I lacks type idempotency: a \n.a = a. The anal\u00adysis establishes the following: A formal connection between intersection types and linear \nlogic originally suggested by Regnier [39]. Most notably, the expansion variables [28, 6] of System-I \nare shown to be a syntactic rendition of the boxes from proofnets [29]. Further, the . in types are sharing \nnodes, and normalization bounds mimic normalization techniques from elementary linear logic. An exact \ncomplexity analysis of the type inference required for System-I. The analysis formalizes the relationship \nbetween type inference and normalization, since the inference rules correspond to the reduction rules \nfor proofnets. A corollary of this analysis is that normalization and type inference are identical in \nevery single case. This is evidenced by the Church Project s experimentation tool [46]: inference for \na seemingly inno\u00adcent ML-program like (fn s=>fn z=> s(s(s z)))(fn s=>fn z=> s(s z))(fn s=>fn z=>s(s z)) \ntimes out. The I in Sys\u00adtem-I therefore reminds us of its computational impotence. With\u00adout idempotency, \nyou need a much bigger type to get a much longer runtime in fact, a type potentially as big as the runtime. \nThis holds for other intersection type systems without idempotency. Compelling evidence that amnesia \nis crucial for type systems. Contrasting the expressiveness results for System-I with the re\u00adsults mentioned \nabove for simply-typed .-calculus and ML, the fundamental difference is in the simply-typable terms of \nthe sys\u00adtems. Simply-typed .-calculus has amnesia (idempotence) and nor\u00admalization is not bound by any \nelementary function [43], i.e., the simply-typed .-calculus has nonelementary power.2 In contrast, simple \n(.-free) types in System-I have only linear power as nor\u00admalization can be done in linear time in the \nsize of the term. More complex types for example, ML polymorphism versus the intro\u00adduction of .-types \namplify this distinction. In the rest of this section, we introduce the concepts mentioned above, yet \nstill using a fairly broad, informal brush. In Sec. 2 we give some required technical preliminaries. \nIn Sec. 3 and 4 we prove that without amnesia (idempotency), intersection type sys\u00adtems have identical \nbounds on normalization and type inference. In Sec. 5 we go one step further and show that for System-I \ntype inference is normalization a result that should hold for other in\u00adtersection type systems without \nidempotency. 1.1 Intersection types Intersection types provide an alternative to the ML/System F paradigm \nof parametric polymorphism. They implement a .nite 1This should not be confused with the weaker notion \nof princi\u00adpal types which is present in the Hindley/Milner type system [35]. With principal types the \ntype environment is .xed. 2Recall that an elementary function is any function bounded by 2x a stack of \nexponentials, i.e., 22\u00b7\u00b7\u00b7. polymorphism which is captured by the following typing rules: G1 I M : a1 \nG2 I M : a2 . x : a1 . a2 I x : ai G1 . G2 I M : a1 . a2 The .rst rule states that if x has type a1 . \na2,any occurrence of x may have either type a1 or type a2. The second rule states that if term M can \nhave both type a1 and type a2 in environments G1 and G2, then M also has intersection type a1 . a2 in \nthe combined environment G1 . G2. When combining types and environments a seemingly technical question \nis whether . is associative, commu\u00adtative, or idempotent (ACI). In other words, which of the following \nequations hold? a . (b . c)=(a . b) . ca . b = b . aa . a = a Even though leaving out ACI simpli.es some \nmatters (such as type inference), the choice, as we shall see, has substantial consequences for normalization \nbounds. The recent interest in intersection types is motivated by 3 concerns: Typing more terms: In full \ngenerality, type inference with both universal types and intersection types is undecidable Wells proves \nthis for universal types [47]; for intersection types, observe that such types characterize exactly the \nstrongly normalizing terms [2]3; see also Cor. 5.9. Practical implementations therefore employ a restriction. \nFor example, ML allows only outermost quanti.cation and therefore rejects fn f => (f 3, ftrue) since \nf must have monomorphic type, but type int does not unify with type bool. For intersection types, rank-bounds, \nwhich bound the depth of a .\u00adtype in an .-type by a constant (the rank), are used. Already with rank \n2 which, by an oddity in the way of counting, is the lowest non-trivial rank all ML programs and more \ncan be typed, e.g., the program above has intersection type ((int . a) . (bool . b)) . a \u00d7 b. This is \nwell-studied by Damiani [11]. Modular compilation, where the different parts of a software sys\u00adtem are \ndeveloped and compiled independently, becomes increas\u00adingly important as program size increases. Intersection \ntypes are a strong candidate for supporting type inference under modular com\u00adpilation. To avoid reanalyzing \nthe code, we make use of the prin\u00adcipal typing. Intersection type systems are advantageous because they \nin most cases have a principal typing for all programs. Static analysis: Flow analysis is a typical kind \nof static analysis does some .xed call site in a program ever call some other proce\u00addure? Since each \nvariable occurrence has a unique non-ACI inter\u00adsection type, typed-based .ow analysis between call sites \nand func\u00adtions can optimize speci.c procedure applications. This has been exploited for instance by Mossin \n[37]. 1.2 Comparing expressiveness, type inference We observe that without idempotency, linearity is \nenforced in the types: if a variable occurs twice, it has type a . b,evenif a = b. Consequently, in a \nterm with a simple type (i.e., without .), any variable (free or bound) occurs at most once. Such terms \nnormalize in linear time vastly less powerful than the nonelementary bounds found in the simply-typed \n.-calculus. We use this observation as the base case of a variant of Statman s theorem [43]. The linearity \n3The statement has been presented many places in the literature, but with incorrect proofs. B. Venneri \nin unpublished work made the .rst correct proof. The reference here appears to be the .rst correct proof \nin the published literature [23, Footnote 8]. spills over when we introduce ., and we prove that normalization \nof rank-bounded fragments of intersection type systems without idem\u00adpotency take at most elementary time. \nIn lower bounds on type inference, the fundamental technical ques\u00adtion is how to iterate a linear function \nwith different domain and range types. As shown by Henglein and Mairson [17], any Turing machine transition \nfunction can be encoded as a linear .-term; we can therefore represent the transition function by a simple \nSystem-I term. Since the Turing machine state is implicitly coded in the type, the domain and range types \nmust be different. By polymorphically iterating this function, we can prove that for every rank we add \nto the type system, expressiveness is increased by an exponential, as is the cost of type inference. \n1.3 Relating type inference, expressiveness in every case The shared upper bound on type inference and \nnormalization is however only the start: unlike most languages you can think of, in System-I type inference \nis equal to normalization for every single term. To relate type inference and normalization, we recast \n.-terms as sharing graphs for .-calculus [15]. We refer to the graphs as in\u00adteraction nets (or just nets) \nwhen untyped and proofnets when con\u00adtaining typing information. The sharing nodes of nets (indicating \ncontraction in linear logic) represent intersections. System-I uses the technology of expansion variables \nto facilitate compositionality, by delaying the application of a typing rule. Ex\u00adpansion variables are \nthe System-I syntax for boxes from interac\u00adtion nets (though they are not identical with the exponentials \nof linear logic). The boxes delimite a piece of code where we later may apply an intersection typing \nrule: if a box has typing (G;a), we might decide later that it has typing (G1 .\u00b7\u00b7\u00b7.Gn;a1 .\u00b7\u00b7\u00b7.an) where \nGi (ai) are derived from G (a) by picking fresh names for the type variables. Furthermore, by simply \nignoring the expansion variables, we have a standard ACI-less intersection type systems. Using standard \ninteraction net machinery, associativity and com\u00admutativity can be added to the system. The results therefore \nspeak not only about System-I, but in general about intersection type sys\u00adtems without idempotency. The \ndelaying of typing rules seems a promising way to avoid an\u00adalyzing a term more than once. Nonetheless, \nthe lack of idempo\u00adtency forces the exact number of copies needed to be made explicit when the box is \npassed in as an argument. The end result is that the set of typings is invariant under reduction. Principal \ntypings and normal forms are consequently isomorphic, following the in\u00adtuitive understanding all functional \nprogrammers have of reading a function from its type [45, 33] and vice versa. Combining these insights, \nwe conclude that type inference is normalization: we can either normalize a term (which terminates due \nto the normalization bounds) and read back the principal typing, or .nd the principal typing and return \nthe normal form. On the operational level, this correspondence is realized by observing that the uni.cation \nrules of the original presentation of System-I [27] are exactly the rules for global reduction of interaction \nnets. How may we interpret these technical results to yield practical in\u00adsights? Worst-case lower bounds \n(in ML, for example) have tradi\u00adtionally been wished away since type inference works in practice: in \nother words, programs with small types are good enough. Why? Programmers can usually keep the type in \ntheir heads, goes one saying because the bounds on normalization show that there is a tremendous expressive \npower, even with those small types. But you cannot keep a System-I type in your head, because you would \nthen know in advance exactly what your program is computing. The result is however not all negative: \nIt stresses that intersection types facilitate an exact program analysis you do not get more precise \nthan running the program. If combined with imprecise analysis as suggested by the non-linear ! present \nin System E it provides a powerful tool that allows the programmer to get the precision that she needs \nin the analysis of her program. 1.4 Relating .ow analysis and the geometry of interaction We observe \nthat in the interpretation of intersection-typed programs as nets, .ow analysis for programs becomes \nessentially a reformu\u00adlation of Girard s geometry of interaction (GoI), made mundane for computer scientists \nin the guise of context semantics [16, 15, 34]. From the context semantics of a term, we may recover \nintersection typing information. The method is an elaboration of a readback algorithm we have used to \nprove the correctness of optimal reduc\u00adtion [30, 15, 3, 34], where we not only want to read back a .-term, \nbut also the location of linear logic boxes (representing expansion variables), and the existence, commutativity, \nand associativity of sharing nodes. For space reasons we cannot provide further details, but hope to \nin a longer version of this paper. 1.5 Related work Coppo, Dezani, and Venneri [9], Ronchi della Rocca \nand Ven\u00adneri [41], van Bakel [44], and Sayag and Mauny [42] have all es\u00adtablished a connection between \ninferring the principal intersection typing and normalization as they (essentially) infer the principal \ntyping through normalization. In fact, we share the overall idea with Sayag and Mauny: establish a connection \non normal forms and show subject reduction and expansion. However, as we through the connection to linear \nlogic also account for the expansion vari\u00adables introduced by System-I, our result is signi.cantly stronger \nthan the previous results. The relationship between intersection types and linear logic was .rst observed \nby Regnier [39]. The present work formalizes his ob\u00adservations and extends his contribution, by relating \nnormalization and type inference from complexity-theoretic as well as observa\u00adtional perspectives. Kfoury \net al. [24] analyze the computational dif.culty of type infer\u00adence for intersection types with ACI, comparing \nit to the expressive power of the language. They develop a na\u00a8ive type inference al\u00adgorithm based on \nlet-expansion [36]. Let the Kalmar elementary functions be K(0,n)= n and K(t + 1,n)= 2K(t,n); they prove \nthat type-inference for a term M of rank r is in dtime[K(r,|M|)], while the expressiveness is in dtime[K(K(r, \n|M|),1)]. In this paper, we show that when idempotency is removed from the type system, the expressiveness \nbound reverts to the bound for type inference. Carlier et al. have recently supplanted System-I by System \nE [6], which overcomes a number of technical de.ciencies of System-I.It also introduces a !-modality \nto allow inexact analysis. The current type inference for System E [7, 6], however, uses only the linear \nfragment of System E. It therefore corresponds to the type infer\u00adence in System-I. Since the present \nwork was .rst presented at the working meeting on intersection types in the Summer 2003, Car\u00adlier and \nWells have subsequently proved the step-wise correspon\u00addence between type inference and normalization \nin the setting of System E [7]. Carlier et al. also show how System E can do the analysis corresponding \nto L\u00b4evy labels [31]. 1.6 Acknowledgements We thank Joe Wells for explaining the intricate details of \nSystem-I, and for his invitation to visit Heriot-Watt University during summer 2003, as well as S\u00b4ebastien \nCarlier for a very inspiring seminar on \u00df-uni.cation and proofnets both have done a tremendous effort \nto answer our questions. For their technical comments and criti\u00adcisms, we thank Alan Bawden, Vincent \nDanos, Jakob Grue Simon\u00adsen, Joe Hallett, Assaf Kfoury, Laurent Regnier, Simona Ronchi Della Rocca, Sebastian \nSkalberg, Franklyn Turbak, Pawel Urzy\u00adczyn, and Steffen van Bakel. We thank the anonymous referees, in \nparticular #4, whose detailed astute technical comments caught several howlers and vastly improved the \ntechnical presentation.  2 Preliminaries Before starting the real party proving the relationship between \ntype inference for intersection types and normalization, let us do a warm\u00adup waltz recalling the central \nconcepts. This is not self-contained, but aims at getting the reader back in rhythm with the main concepts \nof this paper. We use N to denote the positive integers and N0 for the non-negative integers. We use \nf : A .B to denote a complete function f from A to B and f : A '.B to denote a partial function. Disjoint \nunion is described with . We use log for logarithm base 2. We then in turn introduce the two major systems \nwe will use in this paper System-I, a rigid (non-ACI) intersection type system for the .-calculus, and \ninteraction nets, an alternative, more precise syntax for the .-calculus. We consider the typing problem \nof the basic .-calculus. The set of .-terms .is given by following grammar: .3M,N,P,Q ::=x |.x.M |MN \n(1) where the variables x are drawn from the countable set V..We adopt Barendregt s variable convention \n[5]. We use #x(M) for the number of free occurrences of the variable x in M. We use the standard size \nmeasure |\u00b7| on terms. We use xn y to denote n ap\u00adplications of x to y; e.g., s3 z = s (s (sz)); the Church \nnumeral n of n is .s..z.sn z. The capture-free substitution of Q for x in P is written P[Q/x]. We de.ne \nthe usual notion of \u00df-reduction: (.x.P)Q \u00dfP[Q/x]. When R is a notion of reduction (e.g., \u00df)we use .R \nfor the compatible closure and -R for the re.exive transi\u00adtive closure of .R. We de.ne subterm indirectly \nto have a unique path identifying the subterm: (M,e) . M, (P,lp) . .x.M when (P,p). M, (P,lp). MN when \n(P,p) . M, and (P,rp) . MN when (P,p) . N.We call P a subterm of M, written P .M, if, and only if, (P,p). \nM for some p. The subterm of path p is sub(M,p)=P when (P,p). M for some P and unde.ned otherwise. 2.1 \nSystem-I: types, expansions, rank We motivate System-I by .rst considering an intersection type sys\u00adtem \nin the style of van Bakel [44]. Types t,s,.. T and simple types t.T are given by the grammar t ::=a \n|t.t t::=t |t.t where a,b,c,... are type variables. Idempotency, associativity, and commutativity are \nthe equivalences t.t= t, t.(t'.t'')= '' (t.t').t, t.t' =t'.t; when these do not hold, we call the . operator \nrigid. The typing rules are Var x : t II x : t  ' G,x : tII P : t GII P : t.I .K K ' GII .Ix.P : t.t \nGII .x.P : t.t' GII P : tG'II P : tGII P : t.tG 'II Q : t . @ ' G.G'II P : t.tG.G'II PQ : t Rule .I \n(.K ) types abstraction .x.P with x .fv(P)(x .fv(P)), and G0 .G1 ={x : t|x : t.Gi,x .dom G1-i } .{x : \nt0 .t1 |x : ti .Gi } . Without the .-rule, the typing rules would be syntax-directed. Note that the rigidity \nof . and the formulation of rule @ implies that a variable x with n occurrences in a term has at least \nn-1 top-level .. The duplication of the term P in the premises of .highlights a ver\u00adbose feature of the \ntype system. We call (G;t).(V.'.T )\u00d7T a typing of a term M if, and only if, GIM : t. Due to the rule \n. there might be several derivations of a type for a subterm in a type derivation. We use typ(.,p) for \nall the types derived for the sub\u00adterm identi.ed by the subterm path p in the type derivation .; when \nthe subterm N is clear from the context we write typ(.,N). System-I attempts to restrain the above verbosity, \nwhile retaining principal typings [28]. It avoids the redundancy of rule . by infer\u00adring a principal \ntyping once, and deferring the choice of how many instances until a reasonable choice can be made. This \nis accom\u00adplished using expansion variables. Expansion variables are motivated by the fact that for intersection \ntypes, substitution is often not enough to obtain an instance of a principal typing. For instance, in \nthe term M =(.x.xx)(.y.y) the subterm N =.y.y has principal type tN =a .a. In a principal typ\u00ading for \nM, the subterm N has type (tN .tN ).tN which cannot be obtained by substitution into tN as the top-level \ntype constructors are different. Instead, we .rst expand by duplicating (parts of) the principal type \nwith fresh names to obtain (a..a.).(a . a ). We then apply the simultaneous substitution {a. = tN ,aa}. \n= This is formalized in numerous ways [41, 40, 44, 42], but unfortu\u00adnately with a very heavy notation. \nKfoury and Wells made a signi.\u00adcant simpli.cation by extending the types with expansion variables which \nmarks where the expansions should take place. Concretely, System-I introduces expansion variables F,G \nranging over expansions e .E, and extends the de.nition of types to include expansion variables: e .E \n::=D |e .e |F e s,t.T ::=t |t.t|F t. and extend the typing rules with GII P : t E F GII P :F t where \nF G={x :Fa|x : a.G}. The expansion variables come to use when doing type inference. The type inference \n.rst derives a typing (G;t) and a set of con\u00ad 'straints C on the form t=t. It then .nds a substitution \nS that solves the constraints C. The inferred type is obtained by apply\u00ading S to (G;t). The inference \nis syntax-directed and straightforward except for the application where we use4 GII P : tG'II Q : t AppInfer \n' G.(F G')II PQ : t '' where F and tare fresh. We add the constraint that t=F t. t' and modify each constraint \nt=t' found for Q to be F t=F t. To meaningfully talk about solutions, we need to specify what it means \nto substitute an expansion for an expansion variables. The idea is that when e is substituted for F, \napplications F tare replaced by e with each hole .lled with a fresh instance of t. The fresh instances \nare chosen such that if the same expansion is applied to we use the functions rank : (T .T ).N0 and rank \n: T .N0: ' rank(a)=0 rank(t.t )=max{rank t,rank t}rank(F t)=rank t rank(t.t)=max{rank t,rank t}rank (t)=rank \nt rank (F t)=rank t ' rank (t.t )=1 +max{rank t,rank t} (2) A term M with typing (G;t) derived by .has \nas rank the maximal rank of an abstracted variable (either explicitly or in the environ\u00adment): rank(.)=max({rank \ns| z : s. G}.{rank s|.p.(s. typ(., p)&#38; sub(M, p)=.z.Q)}).  2.2 Interaction nets and proofnets Interaction \nnets provide a graphical representation of .-calculus terms, while avoiding problems of variable capture, \npreserving sub\u00adject reduction, and providing a link to concepts in linear logic. The .-calculus can be \nrepresented using nets (here, we have used the call-by-name encoding i\u00b7l, presented inductively in Fig. \n1); to rep\u00adresent System-I typings we add typing information to the nets and obtain proofnets. The external \nvertices of a net are either free ports ( ) represent\u00ading the free variables, or the distinguished root \nport for the whole term. Weakening nodes ( ) mark unused function arguments; ap\u00adplication (@) and function \nnodes (.) mark the de.nition and use of procedures; sharing nodes ( ) code the multiplicity of variable \noccurrences; and croissant nodes ( ) mark variable occurrences. A global construction, the box, delineates \nthe (sharable) arguments of an application. Edges are wires; their endpoints are attached to 4Strictly \nspeaking, the System-I algorithm has an extra step where the term is decorated with expansion variables \nto form a so\u00adcalled skeleton. In principle this can be done in many ways, but the algorithm decorates \nthe arguments to applications as shown here. .. IK x .x.P .x.P Figure 1. The inductive encoding i\u00b7l \nof a .-term as a net. The port ( ) on top is the root, representing the whole term, while the ports \nin the bottom correspond to the free variables. In the application case the left group of wires is the \nvariables solely in P, the middle group the variables occurring in both P and Q, and the right group \nis the variables solely in Q.  Figure 2. Reduction rules for interaction nets. ports of either a node, \nor the entire graph. Each node has a principal port (marked with a black dot), and possibly other auxiliary \nports. Equivalent to doing \u00df-reduction, a net can be reduced using the rules presented in Fig. 2. An \ninteraction takes place between an @\u00adand .-node connected on their principal ports, between a sharing \nnode and box connected on their principal ports, or between two boxes, connected from a principal to \nan auxiliary port. These rules are the graphical equivalent of \u00df-uni.cation, the constraint solving rules \nused in System-I [27]. We omit discussion of necessary weakening rules, since our primary concern is \ntype inference where erasing an argument can change typability. The system is Church-Rosser, and we write \nNF. for its normal forms; it simulates \u00df-reduction up to trivial permutations on the sharing nodes, as \nin the following proposition. Proposition 2.1. LetMandN be .-terms such that M .\u00dfN. There is a net I \nsuch that iMl.I, and I is equivalent to the nodes of iNl reachable from the root up to left/right orientation \nof auxiliary ports on sharing nodes, and their location inside or outside boxes. We restrict our consideration \nto the nets that arise from encoding and reducing .-terms. At this point, we have a language corresponding \nto the untyped .\u00adcalculus. We recover the equivalent of a type system by annotating the wires of the \nnet with types, oriented in a .xed direction along the wire, where the type system enforces constraints \naround the ports of nodes and boxes. We call an annotated net a proofnet. For example, with simple types, \nthe wire on the principal port of an @-node has an incoming type a. \u00df, and the right (left) auxiliary \nport has an S (e t0) S (e t ) S (e(e ' t0' )) S (e (e ' t1' )) S (e (e ' tk' )) \u00b7\u00b7\u00b7 S (e t) S (e t ) \nS (e t ) S (e t1) \u00b7\u00b7\u00b7 S (e tk) with t0 = e 't0' ,...,tn = e 't' n S (e (t.t)) S (e t') S (e t) S (e \nt ) S (e t) S (e t) @ . S (e (t.t))  S (e (t.t')) S (e t ) Figure 3. The constraints on proofnet nodes \nin order to be well-typed. Note that S (e (e ' t' i)) is the same as S ( e ti') where ' e is e with e \n(with renamed expansion variables) substituted for each D. The annotations S (e (e ' t' i)) thus consist \nof the sub\u00adstitution S, the expansion e and the type t' i so the case for box is well-de.ned. incoming \n(outgoing) type a (\u00df). Observe that each of the typing rules Var, .I, .K, and AppInfer correspond na\u00a8ively \nand directly to our inductive cases in the encoding. However, the .-rule is a little problematic; we \nhave two options either duplicate the structure of the box contents, or .gure out a way to share them. \nThe .rst option amounts to typing a boxed net (with output of type a . \u00df) by explicitly representing \nthe two subderivations as distinct proofnets, and pairing their input to the application.5 But then the \nsharing nodes are unpairing nodes (with the linear logic equiva\u00adlent of '), and the multiple proofnets \nare paired together with .. However, the resulting proofnet would be the same size as that of the normal \nform, and all reductions would be linear. We therefore take the alternative option, where we do not duplicate \nthe term. In\u00adstead, we use expansions in the spirit of System-I to represent the different typings. With \nthese considerations in mind, we de.ne the intersection typing of a proofnet ` ala System-I: De.nition \n2.2. A proofnet is an interaction net where (1). Each wire is oriented, and annotated with a triple consisting \nof a type t, an expansion e, and a variable substitution S. The ex\u00adpansion of the type followed by the \nsubstitution, S (e t),gives the different types of the wire; we therefore write the annota\u00adtions as (S \n(e t)). (2). Exactly one free port has its wire oriented toward the port, denoting the root of the .-term \nof interest. This port is the conclusion of the proof . (holding the type of the .-term); the remaining \nports are the assumptions (corresponding to the types of the free variables). (3). The triples on the \nwires incident with each node and box sat\u00adis.es the constraints in Fig. 3. (4). The conclusion and the \nassumptions have the same expan\u00adsion e. (5). A switching is the graph derived by replacing each .-node \nwith a wire from the principal port to one of the auxiliary ports. Every switching results in a forest \nof connected, acyclic 5Note that the type environment of System-I contains exactly the free variables \nof the term consequently the two subderivations have the same variables in the type environment. graphs \ncontaining either the root port, or the port marking a weakening from a K-abstraction. (This is in essence \nthe cor\u00adrectness criterion of Danos and Regnier [14].) If . without annotations and orientations of the \nwires is the net I, we call . a typing of I. We say that I has typing (x1: t1,...,xn : tn;t)where t is \nthe type of the conclusion of . and ti is the type on the free port corresponding to the variable xi. \nTheorem 2.3. LetM be a .-term. Then M has System-I\u00adtyping (G;t) if, and only if, iMl has typing (G;t). \nEvery System-I typable term has a principal typing [28]: Lemma 2.4 (implicit in [28]). In a principal \nSystem-I typing, ev\u00adery type variable occurs at most twice; if it occurs twice it occurs once positively \nand once negatively.6 In the syntax tree of any type, each occurrence of a type variable has the same \nexpansion vari\u00adables along any path from the root of the syntax tree to the occur\u00adrence. Every expansion \nvariable occurs at most once positively. The lemma facilitates a translation to the net formulation: \nDe.nition 2.5. Let . be a proofnet typing a net I; . is principal if for any other proofnet .' typing \nI there exists a substitution S ' and an expansion substitution E ' such .' can be obtained from . by \nreplacing each of the annotations S (e t) with (S '. S)((E ' e) t).  3 Expressiveness bounds We can \nnow engage in our main motif of understanding what makes type inference worthwhile for intersection type \nsystems. In this sec\u00adtion we derive bounds on the expressiveness of rigid intersection types. We consider \nthis decision problem: how hard is it to tell whether two typed programs are equivalent? (A decision \nproblem avoids any question of irrelevant costs due to the choice of out\u00adput format). We prove that given \ntwo .-terms M and N typable with rank at most r it is dtime[(K(r,|M|))]-hard to decide whether the two \nterms are equal. This should be compared with the time .(K(r,|M|)) lower bound on deciding whether term \nM is typable in rank r which we derive in the following section. We shall see that the function K(r,n) \nappears in both bounds because of the lack of idempotency: simply-typed terms have to be linear. These \nworst\u00adcase bounds are a forewarning for the last section, where we show that speci.cally for System-I, \nit is not merely worst case, but for every case, type inference and normalization are the same. We obtain \nthe upper bound on normalization by tracing how the typing of a term is transformed into a typing of \nthe contractum. Recall the insight of Kfoury et al. [24] that a complete develop\u00adment reduces the rank \nof the redexes in a term by at least one; consequently a term with redexes of rank r is turned into a \nterm where all redexes are rank 0 in r - 1 complete developments; a term where all redexes are rank 0 \nterm is af.ne and can therefore be reduced in linear time. The key insight comes from observation .Pt \n)(t 1.\u00b7\u00b7\u00b7.t n of a redex of form ((.x.).t Qt 1.\u00b7\u00b7\u00b7.t n )t ; it contracts to Pt [Q/x] where each occurrence \nof Q has one of the types t1,...,t n. New redexes are only created when Q is an abstraction, and sub\u00adstituted \nas the operator of an application; all such redexes satisfy rank(t i) <rank((t 1 .\u00b7\u00b7\u00b7. t n) . t ) for \nsome i. Only when the re\u00ad 6Positive and negative occurrence are used in the standard way: count the number \nof times we choose the left child (the argument) of an arrow in the syntax tree when going from the root \nto the oc\u00adcurrence. If the number is even, the occurrence is positive; if it is odd, the occurrence is \nnegative. dex is linear, i.e., n =1, does reduction fail to reduce the rank of the redex. We therefore \nremove all linear redexes before each round of complete developments. Since System-I does not have subject \nreduction due to the rigidity of ., it is non-trivial to relate the System-I derivations of a term and \nits contractum. We therefore introduce a relaxed type system IACW. De.nition 3.1. The ACW type system \nis as System-I with the fol\u00adlowing changes: (1). The types are t ::=a |t . t and t ::={ t,..., t}. (2). \nFor n,m = 1, take {t 1,...,t n }.{t n+1,...,t n+m } = {t1,...,tn+m }. (3). The variable rule is Var where \n1 =i =n G,x : {t1,...,tn }Ix : ti  (4). The rank of an ACW-type is: rank(a)=0 rank (t . t )=max{rank \nt,rank t }rank({t1,...,tn })=max{ rank t1,...,rank t n } rank ({ t})=rank t rank ({t 1,t2,...,tn })=1+ \nmax{rank t 1,...,rank t n } (5). We de.ne the following mappings from System-I-types to ACW-types: .a.T \n=a, .t . t .T =.t...t .T , .t . = {.t .T }, and .t .t'.=.t...t'. and extend it trivially to environments. \nREMARK 3.2. The differences between II and IACW are that the latter allows weakening and strengthening, \nhas an associative and commutative ., and that we allow intersections as the argument type of a K-abstraction. \nMoreover, it is straightforward to prove that typability and rank are preserved under .\u00b7.. We now introduce \nlinear reductions and the necessary machinery to expose implicit redexes. De.nition 3.3. We de.ne the \nfollowing notions of reduction (.x.P)Q l P[Q/x] when #x(P)=1 (.x.P)QR T (.x.PR)Q Lemma 3.4. (1) Tl-reduction \nis strongly normalizing, (2) The Tl\u00adnormal form can be computed in dtime[|M|3]. (3) If M .Tl N for terms \nM and N then there is a term O such that M -\u00df O and M -\u00df O We will trace how the rank of redexes in the \nterm is decreased by reduction; we de.ne . as the maximal rank of a redex. De.nition 3.5. Given a derivation \n. of G IACW M : t, the maximal rank of a \u00df-redex is .(.). Formally, we take .(.)=max{rank t |.p.(t .typ(.,pl) \n&#38; sub(M,p)=(.x.P)Q)} when M .NF\u00df and .(M)=0 otherwise. Clearly, .(.)=rank(.). We .rst establish a \nsubstitution lemma: Lemma 3.6. Let .0 derive G0,x : {t 1,..., t n }I M : t0 and for i = 1,...,n let .i \nderive Gi I N : ti. We then have an ACW-derivation .' of G0 .\u00b7\u00b7\u00b7.Gn IM[N/x]: t0 where .(.')= max{.(.0),...,.(.n),rank \nt 1,...,rank t 1 }. We now show that Tl-reductions have subject reduction and only affects the rank in \nbenign way: Lemma 3.7. Let M and N be terms such that M .Tl N and let M have the ACW-typing (G;t)derived \nwith the derivation .. (1) There is an ACW-derivation .' of G IN : t. (2) .(.')=.(.). (3) If M =.z.S \nand N =.z.T, then rank t =.(.). To bound overall normalization, we follow a fairly standard cal\u00adculation \nthat pairs Tl developments (maximal sequences of Tl re\u00adductions) with complete developments; after a \nTl-development, a complete development must reduce the redex rank by 1, while in\u00adcreasing term size by \nat most an exponential. A complete development of a term is intuitively this: take a term, and underline \nevery existing redex. Now \u00df-reduce each of them, where redexes copied by a \u00df-step (called residuals) \nremain under\u00adlined, but new redexes (caused by substituting a .-abstraction for x in some application \nxP) are not underlined. More formally, de.ne D(.x.P)=.x.D(P), D(xP1 \u00b7\u00b7\u00b7Pn)=x D(P1)\u00b7\u00b7\u00b7D(Pn), and the interesting \ncase (where n =0 is chosen maximally) D((.x.P)Q0 \u00b7\u00b7\u00b7Qn)=D(P)[D(Q0)/x]D(Q1)... D(Qn) . Proposition 3.8. \nLet M .NFTl be .-term without Tl-redexes and with an ACW-derivation . of the typing (G;t). We have (1) \nThere is a ACW-derivation of G ID(M): t. (2) .(.')< .(.)if, and only if, .(.)> 0. (3) |D(M)|=2|M| (4) \nM -\u00df D(M). Proof. Part (1) and Part (2) are proven by induction on the height of the derivation. The \nonly interesting case is the last one above for complete developments: As the term is in Tl-normal form, \nwe can only have D((.x.P)Q)=D(P)[D(Q)/x]with #x(P)=2. All t .typ(.,(.x.P)Q)are therefore on the form \nt ={ t1,...,t n }.t ' and new redexes will have rank ti < rank t for some i. Part (3) is proven by induction \non M; in the only interesting case (see above) we have |D((.x.P)Q)|=|D(P)[D(Q)/x]|=|D(P)||D(Q)| =2|P|+|Q|=2|(.x.P)Q| \n. The last part is [5, Lem. 11.2.28(i)]. Corollary 3.9. Given a term M, its complete development D(M) \ncan be computed in dtime[2|M|]. Theorem 3.10. Let M be a term typable in rank r. Its nor\u00admal form has \nlength O(K(r -1,|M|)) and can be computed in dtime[K(r -1,|M|)]. Proof. It follows from Lem. 3.7 and \nProp. 3.8 that Tl-normaliza\u00adtion followed by a complete development of a term P produces a term Q with \n|Q|=2|P|. There is a term R such that P -\u00df R .\u00df Q. If P is typable in redex rank r, then Q is typable \nin redex rank k -1. By induction, at most r -1 repetitions produce a redex rank 0 type which by Tl-normalization \nbecomes a \u00df normal form N. Using the Church-Rosser property, N =NF\u00df(M). This theorem shows how to construct \nuntypable terms: just vio\u00adlate the normalization bounds. We recall n as the Church numeral for n, the \napplication 2 2 2 is not typable in rank 3. Note that P =(.x.x 2)n -\u00df 2n: observe that for any rank, \n.k.kP 2 cannot be typed for almost all k. Terms untypable in ML, System F, etc. are of\u00adten anomalous \nand peculiar; those untypable in rank-bounded7 Sys\u00adtem-I are the standard ones that are the mainstay \nof programming with inductive datatypes in the absence of .xpoint recursion see the Conclusions.  4 \nLower bounds on type inference To derive a lower bound on type inference, we simulate a Turing Machine \nin the type system. Following an idea by Henglein and Mairson [17], we observe that when a term is linear, \nits principal typing is isomorphic to its normal form. Consider for instance linear .-terms representing \nboolean true, false, and not: .x.y.x, .x.y.y, and .p.x.y.pyx. Their principal types are a .b . a, a .b \n.b, and (a . b .c).b . a .c; it is easy to see that there is only one linear normal form with each of \nthese types. More interesting is to consider the application not true; it has principal type b .a .a \nwhich is a principal type of false. Incidentally, (.p.x.y.pyx).x.y.x -\u00df.x.y.y. Type in\u00adference has effectively \nsimulated normalization a technique fur\u00adther elaborated in Section 5 In order to derive a lower bound \non type inference, we rep\u00adresent Turing machine IDs as closed, linear .-terms. A Tur\u00ading machine going \nthrough IDs s0,s1,...,sn is then simulated by terms M0,M1,...,Mn of types t0,t1,...,tn. We represent \nthe state transition function as a closed, linear .-term dwhere dMi -\u00df Mi+1 and similar in types dMi \n: ti+1. We tie the knot by proving the fol\u00adlowing lemma: Lemma 4.1 (Polymorphic Iteration). Let N =K(r,n), \nand mbe the Church numeral for m. Let fbe a term that can be given any of the simple types ti .ti+1 for \n0 =i <N, where the ti are arbitrary. De.ne Jn =.z.2n z; the term F =Jn 2 \u00b7\u00b7\u00b7 2 f (with r -1 applica\u00adtions \non 2s) reduces to .x.fN x, and has simple type t0 . tN in rank r. The proof is essentially as by Kfoury \net al. [24] with a more tedious packing of the types due to the rigidity. When we choose f=d the term \nFM0 of length O(r +n)simulates a Turing machine computa\u00adtion of up to N steps in both the types and in \nthe terms. It therefore provides a lower bound on both type inference and normalization; this shows that \nour upper bounds given above are tight. We use the following conventions regarding Turing machines; it \nshould be clear to the reader how to represent her favorite machine. Convention 4.2. Consider Turing \nmachines which decide predi\u00adcates, coded on a right-in.nite tape over alphabet , 0, and 1. The program \nnever goes beyond the leftmost symbol, never writes a blank symbol, and moves the head exactly when it \nreads a 0 or a 1. A given Turing machine has l states q1,...,ql . When the program is done, it loops \nin the designated failure state ql-1 or acceptance state ql . We build the transition function and states \nwith the linear .-terms in Fig. 4: We use the standard representation of pairs to build lists as 7Recall \nfrom the introduction that rank-unbounded System-I is irrelevant for practice as type inference is undecidable. \n         Pairs and lists: (.;f)=.zz .f nil =.z.z [x1,....xk]=(x1,(x2,(\u00b7\u00b7\u00b7(xk,nil)\u00b7\u00b7\u00b7)) Sequencing \n(right associative): ((v1,...,vk):=.;f)=.(.v1.\u00b7\u00b7\u00b7.vk.f) State, symbols, head movement: PCi =.q0.\u00b7\u00b7\u00b7.ql \n.qi =.B.Z.O.B 0 =.B.Z.O.Z 1 =.B.Z.O.O .=.x.y.z.x .=.x.y.z.y .=.x.y.z.z  State transition function d \n'' .I.(q,L,R):=I;(l,L):=L;(r,R):=R;(q ,r ,d '):=dqr; ( '' d ' (.q '.L.l.r '.R.(q ,L),(l,(r ,R))) '' (.q \n'.L.l.r '.R.(q ,(l,L),(r ,( ,nil))) ) '' ' (.q '.L.l.r '.R.(q ,(r ,(l,L)),R))q Llr ' ,R Figure 4. The \nbuilding blocks of a Turing machine encoding. The types are omitted: as the terms are linear they are \nisomor\u00adphic to the terms. pairs (in contrast with the usual inductive representation); this im\u00adplies \nthat there is no uniform type for list. We represent a machine state as a tuple (q,L,R) consisting of \nthe program counter q, the tape L to the left of the head in reverse, and the tape R to the right of \nthe head. The transition function is a table lookup: using q and the symbol under the head r, .nd the \nnew state q ', the new symbol under the head r, and the direction of the head d. This is represented \nby d: by our conventions, a blank is only read at the right end of the tape; in this case the tape is \nexplicitly extended with a blank (the third line of d). The Polymorphic Iteration Lemma is originally \nproved in a setting with ACI [24]. One application of this lemma with ACIisto let F be the term .x.x2, \nwhich is not linear then the normal form takes an enormous leap to K(K(t,n),1). But without ACI, the \nbase calculus is the linear .-calculus, not the simply-typed one, and there is no such boost. Finally, \nusing standard machinery (e.g., [22, 17]) we may conclude: Theorem 4.3. Let Pbea .-term of length n; \nthen deciding if P is typable in rank t is complete for dtime[K(t,n)]. 5 Type inference is never cheaper \nthan nor\u00admalization We have now reached the high tide of the paper, and prove that due to the lack of \nidempotency, type inference is the same as nor\u00admalization. Thus type inference cannot be faster than \nrunning the program, and types can be inferred without nominally solving constraints they can be read \noff of the normal form. It also sug\u00adgests a Curry-Howard isomorphism for System-I (and possible in\u00adtersection \ntypes in general) through interaction nets. Furthermore, it shows that the type inference bounds are, \nliterally, Statman s the\u00adorem for this type system. The result is obtained through an isomorphism between \nthe normal form of a net, and its principal typing.8 We .rst establish the iso\u00admorphism for the restricted \ncase of normal nets. In this case, we can read the net as its own principal typing, and dually, construct \na normal net from a given principal typing; we obtain algorithms similar to Sayag and Mauny [42]. In \nthe general case, we show that the set of typings is unchanged under reduction. As System-I is strongly \nnormalizing for .xed rank, it follows that any net has the same principal typing as its normal form. \n5.1 Principal typing from a normal form We obtain the principal typing from the normal form by reading \nsharing nodes as ., function and application nodes as ., and boxes as expansion variables. We collect \nthe principal typing using the re\u00adcursive algorithm outlined in Fig. 5; it is called tp : I '. T . The \nbase case corresponds to the normal form .x1.\u00b7\u00b7\u00b7.xk.v with the typing (0/,.1 .\u00b7\u00b7\u00b7 . .k . .i) when v = \nxi and (v : a,.1 . \u00b7\u00b7\u00b7. .k .a) when v is free. The algorithm returns the typing by anno\u00adtating the root \nand the free ports with the type information, e.g., in the second case, the root gets type .1 .\u00b7\u00b7\u00b7..k \n.a, and the free port gets type a. Otherwise, the graph represents .x1.\u00b7\u00b7\u00b7.xk.vN1 \u00b7\u00b7\u00b7Nl . Apply the algorithm \nrecursively to each subnet .i (corresponding to Ni); the result is type annotations of the root and the \nfree ports of each .i. We assign a fresh expansion variable Fi to each of the boxes. We assign type Fl \ntl 0 . \u00b7\u00b7\u00b7 . F1 t10 . a to the head variable and type Fi pij to each wire from .i with pij the type returned \nby the recursive call. Finally, we .nd the intersection type of each shared variable, by a bottom-up \ntraversal of the sharing forest. Each shar\u00ading node is given the principal port type t..t , where t. \n(t )isthe type of the white (black) auxiliary port. 5.2 Normal form from a principal typing Dually, the \nfunction net : (V. '.T ) \u00d7T . NF. produces a net normal form of any given principal System-I typing. \nAlgorithm net is founded on the intuition all functional program\u00admers have when gazing the structure \nof a function from its type: she immediately knows that a function of type (a . \u00df) . a . \u00df has two outermost \nabstractions and that the .rst argument appears in the function position of an application. What sets \nSystem-I apart from other functional languages is the rigidity: without idempo\u00adtency, we have a type \nspeci.cation for each variable occurrence; lack of commutativity gives the orientation of sharing nodes, \nand non-associativity tells us the boxes; .nally, the principal typing re\u00adveals the sources of the arguments \nof applications. For instance, 2 can be typed (a .a) .(a .a) .a .a; it is not clear whether it is the \n.rst or the second occurrence of the .rst argument that is applied to the second argument. However, when \nlooking at the prin\u00adcipal type, ((F \u00df . .) .F (G a . \u00df)) . FG a . ., it is clear that it is the second \noccurrence. De.nition 5.1. The function net : (V. '.T ) \u00d7T '.NF. is de\u00ad.ned on any principal typing \n(G;t) (see Lemma 2.4). We proceed as follows: (1) Let net+ and net- be the mutual recursive func\u00adtions \nin Fig. 6. Produce a forest of graphs by applying net+ to t and net- to each type in G. For example, \nnet+(t .t') constructs 8Strictly speaking, there is a plethora of principal typings. They differ, however, \nonly in the choice of names for the type and expan\u00adsion variables. net+(F t)= net+(t.t')= net+(a)= F \n. net-(t) a net+(t) net+(t') net-(F t)= net-(t.t')= net-(t.t')= net-(a)= net-(t') net-(t') net-(t) a \nnet-(t) @F net+(t) Figure 6. The mutual recursive de.nitions of the functions net+ and net- building \nthe skeleton of a net from a type. aa a. @ \u00df \u00df \u00df' {a=\u00df }{\u00df=..a} 1 a\u00df \u00df1 ... \u00dfka .\u00df {a = F a' ,\u00df1 = F \n\u00df' 1,...,\u00dfk = F \u00df' k } .{F t = F . |t = . .CI } . {a=\u00df..}{.=a.\u00df} . Figure 7. The typing constraints \nfor the various interaction nodes. In the right sub.gure, F is the expansion variable of the box. proofnets \nfrom net+(t') and net-(t), connecting them with a .\u00adnode. (2) Connect the ports of each type variable \nthat is mentioned twice. Connect the remaining ports to a weakening node. Proposition 5.2. Let I .NF. \nbe a net with principal typing (G;t). Then I = net((G;t)) and tp(I)= (G;t). 5.2.1 Typings Are Preserved \nUnder Reduction Going from the easier case of normal forms to arbitrary nets, we es\u00adtablish that the \nset of typings is invariant under reduction. From this analysis we derive both subject reduction, and \nsubject expansion.9 We .rst recast the question of typability as a constraint problem. We show that every \ninteraction net has a set of typing constraints, which exactly characterizes its typings. Second, we \nshow that the set of solutions is invariant under reduction. It follows that a net and its normal form \nhave the same set of typings. As a principal typing is a typing from which all other typings can be obtained, \na net and its normal form then have the same principal typing. De.nition 5.3. Let I . I be a net. Let \neach wire in I be labeled with a unique type variable and each box be labelled with a unique expansion \nvariable. (1). The typing constraints CI of I is a set of equations t = s; some occurrences of type variable \na can be labelled a con\u00adstraining a to types from T . The nodes induce the typing 9Recall subject expansion \nas the feature that if M reduces to N then N : (G;t) implies M : (G,t).  .1.\u00b7\u00b7\u00b7.\u00b7\u00b7\u00b7..k..i Fl tl 0.\u00b7\u00b7\u00b7.F1 \nt10.a . .1.\u00b7\u00b7\u00b7..k .a . . . . . . . .. . . . .k .1 . .k .1 .ia  Figure 5. Inductive de.nition of the \nfunction tp that derives a principal typing of a net. In the right sub.gure, the hatched oval is the \nwiring between the sharing forest at the bottom and the wires coming from the croissant and the free \nports of the subnets .1,...,.l . In the two cases to the left, the variables a,.1,...,.k are fresh. \nIn the last case, the variables a,F1,...,Fl are fresh. The algorithm is applied recursively to the darked \nsubnets .1,...,.l to obtain the types ti 0,...,ti for all i. The result types .1,...,.m,s1,...,sk are \nni  built by processing the sharing trees bottom-up as described in the text. constraint to the left \nof Fig. 7. For a box labelled F, let C be union of the typing constraints for the nodes and boxes in\u00adside \nthe box; the typing constraints for F are {a = F a' ,\u00df1 = F \u00df' 1,...,\u00dfk = F \u00df' k }.{F t = F s | t = s \n. C} where a (a') is the type variable of the root outside (inside) the box and \u00dfi (\u00df' i) is the type \nvariable of the ith auxiliary port outside (inside) the box. CI is the union of the constraints from \nthe outermost boxes and the nodes outside boxes. (2). A solution U to a set of typing constraints C is \npair consisting of an expansion substitution E and a variable substitution T such that T (E t)= T (E \ns) for all t = s . C. We restrict E and T to the variables occurring in C. Also, T respects labels, i.e., \nT(a) . T if a has a labelled occurrence in C. To prove an isomorphism between typings of a .xed net I, \nand solu\u00adtions to CI , we need to relate proofnets with solutions U to CI . Both specify an intersection \ntype for each wire: The proofnet through the annotation S (e t) of each wire. A solution U =(E,T) in\u00adserts \nexpansions from the substitution E for the expansion vari\u00adables G.=(G1,...,Gk), which annotate boxes \nsurrounding a wire (listed from outermost to innermost); the substitution T provides a type for the expanded \nderivatives of ., the wire s type variable (cre\u00adated by applying E .G.to .). We consider the proofnet \n. and the solution U equivalent on a wire exactly when S (e t)= T (E (G..)). Proposition 5.4. Let I be \na net and C its typing constraints. There exists a solution (T,E) to C for all proofnets . typing I such \nthat (T,E) and . are equivalent on all wires. Furthermore, given a so\u00adlution (T,E) to C there exists \na proofnet . typing I such that (T,E) and . are equivalent on all wires. It is now suf.cient to prove \nthat the set of solutions are preserved under reduction. At the intuitive level, the preservation holds \nbe\u00adcause CI capture what happens under reduction. As an example consider a croissant dissolving an outermost \nbox: looking at the wire with the croissant there are 3 relevant type variables in the constraint set: \na above the croissant, \u00df between the box and the croissant, and . on the inside of the box. The constraints \nare a = \u00df and \u00df = F .. Any solution (T,E) will have T (E a)= T (E (F .)); as a, \u00df, and . are type variables \nthe expansion substitution E has no effect and we have: T a = T \u00df and T \u00df = T (F .)). To respect the \nlabellings, T must substitute a T -type for \u00df so we have F = D. Consequently, all solutions have T a \n= T \u00df = T .. This is in ac\u00adcordance with the fact that after dissolving the box we have a sin\u00adgle wire. \nFurthermore, it allows any solution to the reduced net to be used on the original net. (If the redex \nis inside a box we have T (E (G.a)) = T (E (G.\u00df)) and T (E (G.\u00df)) = T (E (G.(F .))) where G.are the expansion \nvariables of the boxes; we essentially get a list of constraints similar to the outermost one.) In establishing \nthe preservation, the main technical dif.culty is that a redex and its contractum have a different number \nof wires. We solve this by noting the redundancy of solutions in the example above, choosing the substitution \nfor the instances of a leaves only one choice for the instances of \u00df and .. It is thus suf.cient to relate \nthe two typing constraints on a subset of variables which can be extended uniquely to a solution. We \ncall such a set a basis.  De.nition 5.5. (1). A basis B of a set of typing constraints C is a subset \nof the variables occurring in C such that: For any type substitution R and expansion substitution D speci.ed \non the variables of B, there is at most one solution (T,E) of C such that R(x)= T(x) for any x . dom \nR and D(F)= E(F) for any F . dom D. (2). Given a basis B and solution U =(S,E) to a set of typing constraints \nC, we write U(B) to denote the restriction of a solution to the variables in B.  (3). Let C be a set \nof typing constraints, B a basis of C, and (R,D) a pair of substitutions on B. If it exists, we write \ni(R,D)lC for the unique solution to C coinciding with R and D on B. We prove that the set of solutions \nto the typing constraints is invari\u00adant under reduction: for each reduction rule, choose two bases, one \nfor the net and one for the reduced net, with an immediate connec\u00adtion between the wires and boxes in \nthe two bases. The solutions are related by restricting them to the bases, using the mapping, and extending \nthem uniquely. Proposition 5.6. Let I and J be nets such that I . J. Let CI(CJ) be the typing constraints \nof I (J). There exists bases BI of CI and BJ ofCJ, such that 1) iUI (BI )lCJ exists and is a solution \nto CJ when UI is a solution to CI and 2) iUJ (BJ)lCI exists and is a solution to CI for any solution \nUJ of CJ. Strictly speaking we cannot be sure that the variables in CI and CJ overlap; we have however \nwithout loss of generality assumed that the unchanged wires have the same type variables. Subject reduc\u00adtion \nand expansion follow as does completeness using the strong normalization result in Sec. 3. Corollary \n5.7. The set of typings is invariant under proofnet re\u00adduction. Corollary 5.8. If I is a net with principal \ntyping (G;t) and NF(I)= J, then net((G;t))=J, and tp(J)=(G;t). Since net reduction simulates \u00df-reduction, \nthe following folklore theorem follows from subject expansion and the fact that all normal forms are \ntypable. Corollary 5.9. Let M be a strongly normalizable .-term. Then M is typable in System-I. 6 Conclusions \nIdempotency is crucial. Without idempotency, the rank 0 types form only the linear .-calculus, expressiveness \nof the language col\u00adlapses to the same complexity as type inference, and type inference becomes synonymous \nwith normalization. Previously [24], one of us conjectured that the complexity of type inference, and \nthat of normalization, are related by the log* function for suitably simple and reasonable type systems. \nThe reasonable characteristic is clearly whether the type system is built on simply-typed .-calculus. \nWithout idempotency, it can t be, and so the conjecture fails. Without idempotency, every Church numeral \nhas a different type, and generalizations of this observation to similar data representa\u00adtions (lists, \ntrees, etc.) are clear. As a consequence, typing functions on those datatypes makes no sense. A conceivable \nrebuttal: nor\u00admal people don t program with Church numerals they program with real numbers. But iteration \nis one of the functional program\u00admer s weapons of mass construction: for example iter 0s z =z, iter n+1 \nsz =s(iter ns z) but now, if iter is to have type Int . a, what is a? Even worse, typing simple program\u00adming \nexamples, such as Abelson and Sussman s square-root pro\u00adgram via iterative approximation [1], become \ncompiler exercises in numerical analysis. The interaction net vernacular underlines the similarities \nbetween intersection types and linear logic. Sharing nodes capture the be\u00adhavior of non-ACI features \nof .. Boxes capture the essence of expansion variables the renaming of expansions is similar to the copying \nof a box. Brackets capture the essence of absorption, and the propagation of an expansion through a type \nformula. An obvious question is how to preserve important aspects of this type analysis, while allowing \nthe expressiveness of the language to increase substantially beyond the cost of typing. A very related \nproblem is to design type systems whose expressiveness allow in\u00adtermediate levels of static analysis, \nbetween all and nothing. Could static analysis, for example, learn anything from limited forms of polymorphism? \n7 References [1] H. Abelson and G. J. Sussman. Structure and Interpretation of Computer Programs. MIT \nPress, 1985. [2] R. Amadio and P.-L. Curien. Domains and Lambda Calculi, volume 46 of Cambridge tracts \nin theoretical computer sci\u00adence. Cambridge University Press, 1998. [3] A. Asperti and S. Guerrini. The \nOptimal Implementation of Functional Programming Languages. Cambridge University Press, 1998. [4] A. \nBanerjee. A modular, polyvariant, and type-based closure analysis. In Proc. 1997 Int l Conf. Functional \nProgramming. ACM Press, 1997. [5] H. P. Barendregt. The Lambda Calculus: Its Syntax and Se\u00admantics. North-Holland, \nrevised edition, 1984. [6] S. Carlier, J. Polakow, J. B. Wells, and A. J. Kfoury. Sys\u00adtem E: Expansion \nvariables for .exible typing with linear and non-linear types and intersection types. In Programming \nLan\u00adguages &#38; Systems, 13th European Symp. Programming, vol\u00adume 2986 of LNCS, pages 294 309. Springer-Verlag, \n2004. [7] S. Carlier and J. B. Wells. Type inference with expansion vari\u00adables and intersection types \nin System E and an exact corre\u00adspondence with \u00df-reduction. Technical Report HW-MACS\u00adTR-0012, Heriot-Watt \nUniv., School of Math. &#38; Comput. Sci., Jan. 2004. [8] M. Coppo, F. Damiani, and P. Giannini. Strictness, \ntotal\u00adity, and non-standard type inference. Theoret. Comput. Sci., 272(1-2):69 111, Feb. 2002. [9] M. \nCoppo, M. Dezani-Ciancaglini, and B. Venneri. Func\u00adtional characters of solvable terms. Z. Math. Logik \nGrundlag. Math., 27(1):45 58, 1981. [10] F. Damiani. A conjunctive type system for useless-code elim\u00adination. \nMath. Structures Comput. Sci., 13:157 197, 2003. [11] F. Damiani. Rank 2 intersection types for local \nde.nitions and conditional expressions. ACM Trans. on Prog. Langs. &#38; Systs., 25(4):401 451, 2003. \n[12] F. Damiani. Rank 2 intersection types for modules. In Proc. 5th Int l Conf. Principles &#38; Practice \nDeclarative Program\u00adming, pages 67 78, 2003. [13] F. Damiani and P. Giannini. Automatic useless-code \ndetec\u00adtion and elimination for HOT functional programs. J. Funct. Programming, pages 509 559, 2000. [14] \nV. Danos and L. Regnier. The structure of multiplicatives. Arch. Math. Logic, 26, 1989. [15] G. Gonthier, \nM. Abadi, and J.-J. L\u00b4evy. The geometry of op\u00ad timal lambda reduction. In Conf. Rec. 19th Ann. ACM Symp. \nPrinc. of Prog. Langs., pages 15 26, 1992. [16] G. Gonthier, M. Abadi, and J.-J. L\u00b4evy. Linear logic \nwithout boxes. In Proc. 7th Ann. IEEE Symp. Logic in Comput. Sci., pages 223 34. IEEE Comput. Soc. Press, \n1992. [17] F. Henglein and H. G. Mairson. The complexity of type in\u00adference for higher-order typed lambda \ncalculi. J. Funct. Pro\u00adgramming, 4(4):435 478, Oct. 1994. [18] J. R. Hindley. The principal type scheme \nof an object in combinatory logic. Trans. American Mathematical Society, 146:29 60, Dec. 1969. [19] J. \nR. Hindley and J. P. Seldin, editors. To H. B. Curry: Es\u00adsays on Combinatory Logic, Lambda Calculus, \nand Formal\u00adism. Academic Press, 1980. [20] T. Jensen. Inference of polymorphic and conditional strictness \nproperties. In Conf. Rec. POPL 98: 25th ACM Symp. Princ. of Prog. Langs., 1998. [21] T. Jim. What are \nprincipal typings and what are they good for? In Conf. Rec. POPL 96: 23rd ACM Symp. Princ. of Prog. Langs., \n1996. [22] P. Kanellakis, H. Mairson, and J. C. Mitchell. Uni.cation and ML type reconstruction. In J.-L. \nLassez and G. Plotkin, ed\u00aditors, Computational Logic: Essays in Honor of Alan Robin\u00adson. MIT Press, 1991. \n[23] A. J. Kfoury. A linearization of the lambda-calculus. J. Logic Comput., 10(3), 2000. [24] A. J. \nKfoury, H. G. Mairson, F. A. Turbak, and J. B. Wells. Relating typability and expressibility in .nite-rank \nintersec\u00adtion type systems. In Proc. 1999 Int l Conf. Functional Pro\u00adgramming, pages 90 101. ACM Press, \n1999. [25] A. J. Kfoury, J. Tiuryn, and P. Urzyczyn. ML typability is DEXPTIME complete. In 15th Colloq. \nTrees in Algebra and Programming, volume 431 of LNCS, pages 206 220. Springer-Verlag, 1990. [26] A. J. \nKfoury, G. Washburn, and J. B. Wells. Implementing compositional analysis using intersection types with \nexpan\u00adsion variables. In Proceedings of the 2nd Workshop on Inter\u00adsection Types and Related Systems, \n2002. [27] A. J. Kfoury and J. B. Wells. Principality and decidable type inference for .nite-rank intersection \ntypes. In Conf. Rec. POPL 99: 26th ACM Symp. Princ. of Prog. Langs., pages 161 174, 1999. [28] A. J. \nKfoury and J. B. Wells. Principality and type infer\u00adence for intersection types using expansion variables. \nTheoret. Comput. Sci., 311(1 3):1 70, 2004. [29] Y. Lafont. From proof-nets to interaction nets. In J.-Y. \nGi\u00adrard, Y. Lafont, and L. Regnier, editors, Advances in Linear Logic, Proceedings of the 1993 Workshop \non Linear Logic, London Math. Soc. Lecture Note Series 222, pages 225 247. Cambridge University Press, \n1995. [30] J. Lamping. An algorithm for optimal lambda-calculus reduc\u00adtions. In POPL 90 [38], pages 16 \n30. [31] J.-J. L\u00b4evy. Optimal reductions in the lambda-calculus. In Hindley and Seldin [19], pages 159 \n191. [32] H. G. Mairson. Deciding ML typability is complete for deter\u00administic exponential time. In POPL \n90 [38], pages 382 401. [33] H. G. Mairson. Outline of a proof theory of parametricity. In FPCA 91, Conf. \nFunct. Program. Lang. Comput. Arch., vol\u00adume 523 of LNCS, pages 313 327, Cambridge, MA. U.S.A., 1991. \nSpringer-Verlag. [34] H. G. Mairson. From Hilbert spaces to Dilbert spaces: Con\u00adtext semantics made simple. \nIn 22nd Conference on Founda\u00adtions of Software Technology and Theoretical Computer Sci\u00adence, 2002. [35] \nR. Milner. A theory of type polymorphism in programming. J. Comput. System Sci., 17:348 375, 1978. [36] \nJ. C. Mitchell. Type systems for programming languages. In J. van Leeuwen, editor, Handbook of Theoretical \nComputer Science, pages 365 458. North-Holland, 1990. [37] C. Mossin. Exact .ow analysis. Math. Structures \nComput. Sci., 13:125 156, 2003. [38] Conf. Rec. 17th Ann. ACM Symp. Princ. of Prog. Langs., 1990. [39] \nL. Regnier. Lambda calcul et r\u00b4eseaux. PhD thesis, University Paris 7, 1992. [40] S. Ronchi Della Rocca. \nPrincipal type schemes and uni.ca\u00adtion for intersection type discipline. Theoret. Comput. Sci., 59(1 \n2):181 209, Mar. 1988. [41] S. Ronchi Della Rocca and B. Venneri. Principal type schemes for an extended \ntype theory. Theoret. Comput. Sci., 28(1 2):151 169, Jan. 1984. \u00b4 tion type discipline through principal \ntypings of normal forms. Technical Report RR-2998, INRIA, Oct. 16, 1996. [42] E. Sayag and M. Mauny. \nA new presentation of the intersec\u00ad [43] R. Statman. The typed lambda-calculus is not elementary re\u00adcursive. \nTheoret. Comput. Sci., 9(1):73 81, July 1979. [44] S. J. van Bakel. Intersection Type Disciplines in \nLambda Cal\u00adculus and Applicative Term Rewriting Systems. PhD thesis, Catholic University of Nijmegen, \n1993. [45] P. Wadler. Theorems for free! In Proceedings 4th Int. Conf. on Funct. Program. Languages and \nComputer Architecture, pages 347 359. ACM, 1989. [46] G. Washburn, A. Kfoury, J. Wells, B. Alan, B. Yates, \nO. Schwartz, and S. Carlier. System I experimentation tool. http://types.bu.edu/modular/compositional/ \nexperimentation-tool/. [47] J. B. Wells. Typability and type checking in System F are equivalent and \nundecidable. Ann. Pure Appl. Logic, 98(1 3):111 156, 1999. [48] J. B. Wells, A. Dimock, R. Muller, and \nF. Turbak. A calcu\u00adlus with polymorphic and polyvariant .ow types. J. Funct. Programming, 12(3):183 227, \nMay 2002.  \n\t\t\t", "proc_id": "1016850", "abstract": "Useful type inference must be faster than normalization. Otherwise, you could check safety conditions by running the program. We analyze the relationship between bounds on normalization and type inference. We show how the success of type inference is fundamentally related to the <i>amnesia</i> of the type system: the <i>nonlinearity</i> by which all instances of a variable are constrained to have the same type.Recent work on <i>intersection types</i> has advocated their usefulness for static analysis and modular compilation. We analyze System-I (and some instances of its descendant, System E), an intersection type system with a type inference algorithm. Because System-I lacks <i>idempotency</i>, each occurrence of a variable requires a distinct type. Consequently, type inference is equivalent to normalization in <i>every single case</i>, and time bounds on type inference and normalization are identical. Similar relationships hold for other intersection type systems without idempotency.The analysis is founded on an investigation of the relationship between <i>linear logic</i> and intersection types. We show a lockstep correspondence between normalization and type inference. The latter shows the promise of intersection types to facilitate static analyses of varied granularity, but also belies an immense challenge: to add amnesia to such analysis without losing all of its benefits.", "authors": [{"name": "Peter M&#248;ller Neergaard", "author_profile_id": "81100105547", "affiliation": "Brandeis University, Waltham, MA", "person_id": "P413661", "email_address": "", "orcid_id": ""}, {"name": "Harry G. Mairson", "author_profile_id": "81100061196", "affiliation": "Brandeis University, Waltham, MA", "person_id": "P107959", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1016850.1016871", "year": "2004", "article_id": "1016871", "conference": "ICFP", "title": "Types, potency, and idempotency: why nonlinearity and amnesia make a type system work", "url": "http://dl.acm.org/citation.cfm?id=1016871"}