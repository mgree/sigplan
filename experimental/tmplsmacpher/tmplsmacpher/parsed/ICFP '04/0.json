{"article_publication_date": "09-19-2004", "fulltext": "\n Making a Fast Curry: Push/Enter vs. Eval/Apply for Higher-order Languages Simon Marlow and Simon Peyton \nJones Microsoft Research, Cambridge Abstract Higher-order languages that encourage currying are implemented \nusing one of two basic evaluation models: push/enter or eval/apply. Implementors use their intuition \nand qualitative judgements to choose one model or the other. Our goal in this paper is to provide, for \nthe .rst time, a more sub\u00adstantial basis for this choice, based on our qualitative and quanti\u00adtative \nexperience of implementing both models in a state-of-the-art compiler for Haskell. Our conclusion is \nsimple, and contradicts our initial intuition: com\u00adpiled implementations should use eval/apply. Categories \nand Subject Descriptors: D.3.2 [Programming Lan\u00adguages]: Language Classi.cations Applicative (functional) \nlan\u00adguages General Terms: Languages, Performance 1 Introduction There are two basic ways to implement \nfunction application in a higher-order language, when the function is unknown: the push/enter model or \nthe eval/apply model [11]. To illustrate the difference, consider the higher-order function zipWith, \nwhich zips together two lists, using a function kto combine corresponding list elements: zipWith::(a->b->)->[a]->[b]->[] \nzipWithk[] [] =[] zipWithk(x:xs)(y:ys)=kxy:zipWithxsys Here kis an unknown function, passed as an argument; \nglobal .ow analysis aside, the compiler does not know what function kis bound to. What code should the \ncompiler generate to execute the call kxyin the body of zipWith? It can t blithely pass two argu\u00adments \nto k, because kmight in reality take just one argument and compute for a while before returning a function \nthat consumes the next argument; or kmight take three arguments, so that the result of the zipWithis \na list of functions. In the push/enter model, the call proceeds by pushing the arguments xand yon the \nstack, and entering the code for k. Every function s Permission to make digital or hard copies of all \nor part of this work for personal or classroom use is granted without fee provided that copies are not \nmade or distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. ICFP 04, September 19 21, 2004, Snowbird, Utah, USA. entry code \nis required to check how many arguments are on the stack, and behave appropriately: if there are too \nfew arguments, the func\u00adtion must construct a partial application and return. If there are too many arguments, \nthen only the required arguments are consumed, the rest of the arguments are left on the stack to be \nconsumed later, presumably by the function that will be the result of this call. In the eval/apply approach, \nthe caller .rst evaluates the function k, and then applies it to the correct number of arguments. The \nlatter step involves some run-time case analysis, based on information ex\u00adtracted from the closure for \nk.If ktakes two arguments, we can call it straightforwardly. If it takes only one, we must call it passing \nx, and then call the function it returns passing y; if it takes more than two, we must build a closure \nfor the partial application kxyand return that closure. The crucial difference between push/enter and \neval/apply is this. When a function of statically-unknown arity is applied, two pieces of information \ncome together at run-time: the arity of the function and the number of arguments in the call. The two \nmodels differ in whether they place responsibility for arity-matching with the func\u00adtion itself, or with \nthe caller: Push/enter: the function, which statically knows its own arity, ex\u00adamines the stack to .gure \nout how many arguments it has been passed, and where they are. The nearest analogy is C s varargs calling \nconvention. Eval/apply: the caller, which statically knows what the arguments are, examines the function \nclosure, .nds its arity, and makes an exact call to the function. Which of the two is best in practice? \nThe trouble is that the evalua\u00adtion model has a pervasive effect on the implementation, so it is too \nmuch work to implement both and pick the best. Historically, com\u00adpilers for strict languages (using call-by-value) \nhave tended to use eval/apply, while those for lazy languages (using call-by-need) have often used push/enter, \nbut either approach will work in both settings. In practice, implementors choose one of the two approaches \nbased on a qualitative assessment of the trade-offs. In this paper we put the choice on a .rmer basis: \n We explain precisely what the two models are, in a common notational framework (Section 4). Surprisingly, \nthis has not been done before.  The choice of evaluation model affects many other design choices in \nsubtle but pervasive ways. We identify and discuss these effects in Sections 5 and 6, and contrast them \nin Sec\u00adtion 7. There are lots of nitty-gritty details here, for which we make no apology they were far \nfrom obvious to us, and ar\u00adticulating these details is one of our main contributions.  In terms of its \nimpact on compiler and run-time system com- Copyright 2004 ACM 1-58113-905-5/04/0009 ...$5.00 plexity, \neval/apply seems decisively superior, principally be\u00ad cause push/enter requires a stack like no other: \nstack-walking is more dif.cult, and compiling to an intermediate language like C or C--is awkward or \nimpossible. We give the .rst detailed quantitative measurements (Sec\u00adtion 8) that contrast the two approaches, \nbased on a credible, optimising compiler (the Glasgow Haskell Compiler, GHC). We give both bottom-line \nresults such as wall-clock time, total instruction count and allocation, and also some more insightful \nnumbers such as breakdowns of call patterns. Our experiments show that the execution costs of push/enter \nand eval/apply are very similar, despite their pervasive differ\u00adences. What you gain on the swings you \nlose on the round\u00adabouts. Our conclusion is simple, and contradicts the abstract-machine her\u00aditage of \nthe lazy functional-language community: eval/apply is a clear win. We have now adopted eval/apply for \nGHC.  2 Background: ef.cient currying The choice between push/enter and eval/apply is only important \nif the language encourages currying. In a higher-order language one can write a multi-argument function \nin two ways: f ::(Int,Int)->Int f (x,y)=x*y g ::Int->Int->Int g xy=x*y Here, fis un-curried. It takes \na single argument that is a pair, un\u00adpacks the pair, and multiplies its components. On the other hand, \ng is curried. Notionally at least, gtakes one argument, and returns a function that takes a second argument, \nand multiplies the two. The type of gshould be read right-associatively, thus: g ::Int->(Int->Int) Currying \nappeals to our sense of beauty, because multi-argument functions come for free ; one does not need data \nstructures to sup\u00adport them. We said that notionally at least gtakes one argument , but suppose that, \ngiven the above de.nition of g, the compiler is faced with the call g34. The call is to a known function \n one whose de.ni\u00adtion the compiler can see . It would be ridiculous to follow the currying story literally. \nTo do that, we would call gpassing one argument, 3, get a function closure in return, and then call that \nfunc\u00adtion, again passing one argument, 4. No, in this situation, any decent compiler must load the arguments \n3and 4into registers, or on the stack, and call the code for gdirectly, and that is true whether the \nbasic evaluation model is push/enter or eval/apply. In the rest of this paper we will take it for granted \nthat saturated calls to known functions are compiled using an ef.cient argument-passing conven\u00adtion. \nThe push/enter and eval/apply models differ only in how they handle calls to unknown functions. In any \nhigher-order language one can write curried functions, simply by writing a function that returns a function, \nbut languages differ in the degree to which their syntax encourages it. For the purposes of this paper, \nwe assume that currying is to be regarded as the native way to de.ne multi-argument functions, and that \nwe wish to make multi-argument curried functions as fast as possible. Our measure\u00adments show that around \n40% of unknown function calls have more than one argument (Section 8).  3 Language To make our discussion \nconcrete we use a small, non-strict inter\u00admediate language similar to that used inside the Glasgow Haskell \nCompiler. Its syntax is given in Figure 1. In essence it is the STG language [11], but we have adjusted \nsome of the details for this pa\u00adper. Although the push/enter vs eval/apply question applies equally \nto strict and non-strict languages, we treat a non-strict one here because it is the slightly more complicated \ncase, and because our quantitative data is for Haskell. The idea is that each syntactic construct in \nFigure 1 has a direct operational reading. We give these operational intuitions here, and we will make \nthem precise in Section 4: A literal is an unboxed 32-bit integer, i, or 64-bit double\u00adprecision .oating-point \nnumber, d. We have more to say about unboxed values in Section 3.3.  A call, fk a1an, applies the function \nf to the arguments a1an. The application is in A-normal form [5] that is, each argument is an atom (literal \nor variable) so there is no argument preparation to perform .rst. The superscript k de\u00adscribes the statically-known \ninformation about the function s arity. It takes two forms:  fn, where n is an integer, indicates that \nthe compiler stat\u00adically knows the arity of f , usually because there is a lexically-enclosing binding \nfor f that binds it to a FUN object with arity n.  f .indicates that the compiler has no static information \nabout f s arity. It would be safe to annotate every appli\u00adcation with .   There is no guarantee that \nthe function s arity (whether stati\u00adcally known or not) matches the number of arguments supplied at the \ncall site. A letexpression (and only a let) allocates an object in the heap. We discuss the forms of \nheap object in Section 3.1. In this paper we will only discuss simple, non-recursive letex\u00adpressions. \nGHC supports a mutually-recursive letreas well, of course, but recursive bindings do not affect the issues \ndis\u00adcussed this paper, so we omit them to save clutter. The top\u00adlevel de.nitions of a program are recursive, \nhowever.  A aseevaluates a sub-expression, called the scrutinee, and optionally performs case analysis \non its value. More con\u00adcretely, asesaves any live variables that are needed in the case alternatives, \npushes a return address, and then evaluates the scrutinee. At the return address, it performs case analysis \non the returned value. All aseexpressions are exhaustive: ei\u00adther there is a default alternative as a \ncatch-all, or the patterns cover all the possibilities in the data type. We often omit the curly braces \nin our informal examples, using layout instead.  3.1 Heap objects The language does not provide a syntactic \nform of expression for constructor applications, or for anonymous lambdas; instead, they must be explicitly \nallocated using let. In general, letperforms heap allocation, and the right hand side of a letis a heap \nobject. There are exactly .ve kinds of heap objects: FUN(x1xn -e)is a function closure, with arguments \nxi and body e (which may have free variables other than the xi). The function is curried that is, it \nmay be applied to fewer than n, or more than n, arguments but it still has an arity of n. PAP(fa1an)represents \na partial application of function f to ar\u00adguments a1an. Here, f is guaranteed to be FUN object, and the \narity of that FUN is guaranteed to be strictly greater than n. CON(Ca1an)is a data value, the saturated \napplication of con\u00adstructor C to arguments a1an. Variables x,y,f ,g Constructors C De.ned in data type \ndeclarations Literals lit :: i d Unboxed integer or double Atoms a,v :: lit x Function arguments are \natomic Function arity k ::  Unknown arity n Known arity n ;1 Expressions e :: a Atom f k a1 an Function \ncall (n ;1) Ea1 an Saturated primitive operation (n ;1) letx =ob j ine asee of{alt1; ;altn} (n ;1) Alternatives \nalt :: Cx1 xn -e (n ;0) x \u00ade Default alternative Heap objects ob j :: FUN(x1 xn -e) Function (arity n \n;1) PAP(fa1 an) Partial application ( f is always a FUN with arity(f )>n ;1) CON(Ca1 an) Saturated constructor \n(n ;0) THUNK e Thunk BLACKHOLE [only during evaluation] Programs prog :: f1 =ob j1; ; fn =ob jn Figure \n1. Syntax THUNK e represents a thunk, or suspension. When its value is needed, e is evaluated, and the \nthunk overwritten with (an in\u00addirection to) the value of e. BLACKHOLE is used only during evaluation \nof a thunk, never in a source program. While a thunk is being evaluated, it is re\u00adplaced by BLACKHOLE \nto avoid space leaks and to catch cer\u00adtain forms of divergence [7]. Of these, FUN, PAP and CON objects \nare values, and cannot be evaluated any further. A top-level de.nition creates a statically-allocated \nobject, at a .xed address, whereas a letallocates a heap object dynamically.  3.2 Case expressions The \nlanguage offers conventional algebraic data type declarations, such as dataTreea=LeafaIBranh(Treea)(Treea) \ndataBool=FalseITrue dataLista=NilIConsa(Lista) Values of type Treeare built with the constructors Leafand \nBranh, and can be discriminated and taken apart with a aseex\u00adpression. The boolean type Boolis just a \nregular algebraic data type, so that a conditional is implemented by a aseexpression. Con\u00adstructors are \nalways saturated; unsaturated constructors can always be saturated by eta expansion. To give the idea, \nhere is the Haskell de.nition of the mapfunction: mapf[]=[] mapf(x:xs)=fx:mapfxs and here is its rendition \ninto our intermediate language: nil=CONNil map=FUN(fxs-> asexsof Nil->nil Consyys->leth=TRUNK(fy) t=TRUNK(mapfys) \nr=CON(Consht) inr ) The top-level de.nition of nilis automatically generated by GHC, so that there is \na value to hand for mapto return in the Nilcase al\u00adternative. A similar top-level de.nition is generated \nfor each nullary constructor. The scrutinee of a aseexpression is an expression rather than an atom. \nThis is important, because it lets us write, for example, ase(nullxs)of , rather than lety=TRUNK(nullxs)inaseyof \n There is no need to construct a thunk! 3.3 Unboxed values Another slightly unusual feature of our language \nis the use of un\u00adboxed values [12]. Supporting unboxed values is vital for perfor\u00admance, but it has signi.cant \nconsequences for the implementation: both heap objects and the stack may contain a mix of pointer and \nnon-pointer values. Most values are represented by a pointer to a heap object, including all data structures, \nfunction closures, and thunks. Our intermediate language also supports a handful of primitive, unboxed \ndata types, of which we consider only Int#and Double#here. An Int#is a 32-bit integer, in the native \nmachine representation; it is not a pointer. Sim\u00adilarly, a Double#is a 64-bit double-precision .oating-point \nvalue in IEEE representation. These unboxed values can be passed as a argu\u00adments to a function, returned \nas results, stored in data structures, and so on. For example, here is how the (boxed) type Intis de.ned, \nas an ordinary algebraic data type: dataInt=I#Int# That is, an Intvalue is a heap-allocated data structure, \nbuilt with the I#constructor, containing an Int#. Having explicit unboxed values allows us to make boxing \nand un\u00adboxing operations explicit in our intermediate language. For exam\u00adple, here is how Intaddition \nis de.ned: plusInt::Int->Int->Int plusIntab =aseaof{I#x-> asebof{I#y-> I#(x+#y) }} The .rst aseexpression \nevaluates the argument a(in case it is a thunk) and takes it apart; the second asedoes the same to b; \nthe asex+#yofadds the two unboxed values using the prim\u00aditive addition operator +#, while the .nal use \nof I#boxes the result back into an Int.  4 The two evaluation models It is now time to become precise \nabout what we mean by a push/enter or eval/apply model. We do so by giving an op\u00aderational semantics \nthat exposes the key differences between these models, while still hiding some representation details \nthat only con\u00adfuse the picture. Douence and Fradet give a completely different, combinator-based, formalism \nthat allows them to contrast push/enter with eval/apply [2], although their treatment only considers \nsingle\u00adargument functions whereas we are interested in how to perform multiple application without building \nintermediate function closures. Furthermore, the semantics we present here maps more directly to operational \nintuitions. Figure 2 gives the operational semantics for both evaluation models, using a small-step transition \nrelation of the form e1; s1; H1 e2; s2; H2 The components of the program state are: The code e, is the \nexpression under evaluation, in the syntax of Fig\u00adure 1. The stack s, is a stack of continuations that \nsays what to do when the current expression is evaluated. We use the notation : to means cons in the \ncontext of a stack. The heap H, is a .nite mapping from variables (which we treat as synonymous with \nheap addresses) to heap objects. The latter have the syntax given in Figure 1. To reduce clutter, we \nuse the convention that no binding is ever removed from the heap. For example, in rule CASECON the heap \nH on the right-hand side of the rule still has a binding for v. The stack continuations, ., take the \nfollowing forms: . :: ase of{alt1; ;altn} Upd t Update thunk t with returned value ( a1 an)Apply the \nreturned function to a1 an [eval/apply only] Arg a Pending argument [push/enter only] The meaning of \nthese continuations should become clear as we dis\u00adcuss the evaluation rules. The rules themselves are \nfairly dense, so the following subsections explain them in some detail. After that, we sketch how the \noperational semantics is mapped onto a real machine by the Glasgow Haskell Compiler. 4.1 Rules common \nto both models The .rst block of evaluation rules in Figure 2 are common to both push/enter and eval/apply. \nThe .rst rule, LET, says what happens when the expression to be evaluated is a letform. Following Launchbury \n[8], we simply allo\u00adcate the right-hand side ob j in the heap, using a fresh name x., extend the heap \nthus H[x. ob j]. The use of a fresh name corresponds to -allocating an unused address in the heap. Lastly, \nwe substitute x.for x in e, the body of the let, before continuing. In a real implementa\u00adtion this substitution \nwould be managed by keeping a pointer to the new object in a register, or accessing it by offset from \nthe allocation pointer, but we do not need to model those details here. The next group of four rules \ndeal with aseexpressions. Rule CASE, starts the evaluation of a aseexpression by pushing a asecontin\u00aduation \non the stack, and evaluating the scrutinee, e. When evaluation is complete, a value v (either a literal \nor a pointer to a heap value) is returned to the asecontinuation by RET. If v is (a pointer to) a constructor, \nrule CASECON applies; it resumes the appropriate branch of the ase, binding the constructor argu\u00adments \nto xi. If the returned value does not match any other ase alternative, the default alternative is used \n(rule CASEANY). These two rules precede CASE because they overlap it, and we use the con\u00advention that \nthe .rst applicable rule takes precedence. The next two rules deal with thunks. If the expression to \nbe evalu\u00adated is a thunk, we push an update continuation (or update frame), Upd t , which points to the \nthunk to be updated (rule THUNK). While the thunk t is being evaluated we update the heap so that t points \nto a BLACKHOLE. No left-hand sides match BLACKHOLE so evaluation will get stuck if we try to evaluate \na thunk during its own evaluation. This simple trick has been known for a long time, and is also crucially \nimportant to avoid space leaks [7]. When evaluation is complete, we overwrite the thunk with the value \n(rule UPDATE). The last two rules deal with saturated applications of known func\u00adtions, either primitive \noperations (PRIMOP) or user-de.ned ones (KNOWNCALL). Both are very simple and can be compiled ef.\u00adciently, \nwith fast parameter-passing mechanisms. Notice that the call to f is a tail call. No continuation is \npushed; instead control is simply transferred to f s body. The big remaining question is how function \napplication is handled when the function is unknown, or is applied to too many or too few arguments. \nAnd that is the key point at which the two evaluation models differ, of course. 4.2 The push/enter model \nThe rules in the second block of Figure 2 are the ones speci.c to the push/enter model. First consider \nrule PUSH, which deals with function applications. It simply pushes the arguments onto the stack, as \npending arguments, using the Arg continuation, and enters the function. The next three rules deal with \nwhat entering the function means: First, the function f might turn out to be a FUN object of arity n, \nand there might be n or more arguments on the stack. In that case (rule FENTER), we can proceed to evaluate \nthe body of the function, binding the actual arguments to the formal pa\u00adrameters as usual. Any excess \npending arguments are left on the stack, to be consumed by the function that e (presumably) evaluates \nto.  What if there aren t enough pending arguments on the stack? This could happen either because a \nfunction-valued thunk pushed an update frame, or because a aseexpression eval\u00aduated a function (see Section \n3.2). In either case, we must con\u00adstruct a value to return to the caller and that value is a partial \napplication, or PAP, as rule PAP1 shows.  What if f is a PAP and not a FUN? In that case, we simply \nunpack the PAP s arguments onto the stack, and enter the func\u00adtion (rule PENTER).  Rules common to \npush/enter and eval/apply letx =ob j ine; s; He[x./x]; s; H[x. -ob j](LET) x.fresh asev of{;Cx1 xn -e; \n}; s; e[a1/x1 an/xn]; s; H (CASECON) H[v -CON(Ca1 an)] asev of{;x -e}; s; He[v/x]; s; H (CASEANY) if \nv is a literal or H[v]is a value, and does not match any other case alternative asee of{}; s; He; ase \nof{}: s; H (CASE) v; ase of{}: s; H asev of{}; s; H (RET) if v is a literal or H[v]is a value x; s; H[x \n-THUNK e]e; Upd x : s; H[x -BLACKHOLE](THUNK) y; Upd x : s; Hy; s; H[x -H[y]] (UPDATE) if H[y]is a value \nfn a1 an; s; H[f -FUN(x1 xn -e)]e[a1/x1 an/xn]; s; H (KNOWNCALL) Ea1 an; s; Ha; s; H (PRIMOP) where a \nis the result of applying the primitive op\u00aderation Eto arguments a1 an Rules for push/enter fk a1 am; \ns; Hf ; Arg a1: : Arg am : s; H (PUSH) f ; Arg a1: : Arg an : s; H[f -FUN(x1 xn -e)]e[a1/x1 an/xn]; s; \nH (FENTER) f ; Arg a1: : Arg am : s; H[f -FUN(x1 xn -e)]p; s; H[p -PAP(fa1 am)] (PAP1) if m ;1; the top \nelement of s is not of the form Arg y; p fresh f ; Arg an+1: s; H[f -PAP(ga1 an)]g; Arg a1: : Arg an \n: Arg an+1: s; H (PENTER) Rules for eval/apply f .a1 an; s; H[f -FUN(x1 xn -e)] e[a1/x1 an/xn]; s; H \n(EXACT) f k a1 am; s; H[f -FUN(x1 xn -e)] e[a1/x1 an/xn]; ( an+1 am): s; H if m >n p; s; H[p \u00adPAP(fa1 \nam)] if m <n, p fresh (CALLK) (PAP2) f .a1 am; s; H[f -THUNK e] f ; ( a1 am): s; H (TCALL) f k an+1 am; \ns; H[f -PAP(ga1 an)] g.a1 an an+1 am; s; H (PCALL) f ; ( a1 an): s; H f .a1 an; s; H H[f ]is a FUN or \nPAP (RETFUN) Figure 2. The evaluation rules Info table Entry code Object type Layout info Type-specific \nfields Figure 3. A heap object The three cases above do not exhaust the possible forms of f .It might \nalso be a THUNK, but we have already dealt with that case (rule THUNK). It might be a CON, in which case \nthere cannot be any pending arguments on the stack, and rules UPDATE or RET apply.  4.3 The eval/apply \nmodel The last block of Figure 2 shows how the eval/apply model deals with function application. The \n.rst three rules all deal with the case of a FUN applied to some arguments: If there are exactly the \nright number of arguments, we behave exactly like rule KNOWNCALL, by tail-calling the function. Rule \nEXACT is still necessary and indeed has a direct coun\u00adterpart in the implementation because the function \nmight not be statically known.  If there are too many arguments, rule CALLK pushes a call con\u00adtinuation \non the stack, which captures the excess arguments. This is the essence of eval/apply. Given an application \nfxy where ftakes one argument, .rst call fx, and then apply the resulting function to y.  If there are \ntoo few arguments, we build a PAP (rule PAP2),  which becomes the value of the expression. These rules \nwork by dynamically inspecting the arity of the function closure in the heap, which works .ne for both \nknown and unknown calls, but we can do better for known calls. Rule KNOWNCALL has already dealt with \nthe saturated known case, and it is probably not worth the bother of treating under-and over-saturated \nknown calls specially because they are very uncommon (see Section 8). Another possibility is that the \nfunction in an application is a THUNK (rule TCALL). This case is very like the over-applied func\u00adtion \nof rule CALLK; we push a call continuation and enter the thunk. (This in turn will push an update frame \nvia rule THUNK.) Finally, the function in an application might be a partial application of another function \ng (rule PCALL). In that case we unpack the PAP and apply g to its new arguments. Since g is sure to be \na FUN, this willtakeusbacktooneofthecasesinrules EXACT, CALLKor PAP2. That concludes the rules for function \napplication. We need one last rule, RETFUN, which returns a function value (PAP or FUN)to a call continuation, \nin the obvious way. This rule re-activates a call continuation, exactly as rule RET re-activates a asecontinuation. \n 4.4 Heap objects To provide the context for our subsequent discussion, we now sketch brie.y how GHC \nmaps the operational semantics onto a real ma\u00adchine. Figure 3 shows the layout of a heap object. In GHC, \nthe .rst word of every object is called the object s info pointer, and points to an immutable, statically-allocated \ninfo table. The remainder of the object is called the payload, and may consist of a mixture of pointers \nand non-pointers. For example, the object CON(Ca1 an)would be represented by an object whose info pointer \nrepresented the con\u00adstructor C and whose payload is the arguments a1 an. The info table contains: Executable \ncode for the object. For example, a FUN object has code for the function body.  An object-type .eld, \nwhich distinguishes the various kinds of objects (FUN, PAP, CON etc) from each other.  Layout information \nfor garbage collection purposes, which de\u00adscribes the size and layout of the payload. By layout we mean \nwhich .elds contain pointers and which contain non\u00adpointers, information that is essential for accurate \ngarbage col\u00adlection.  Type-speci.c information, which varies depending on the ob\u00adject type. For example, \na FUN object contains its arity; a CON object contains its constructor tag, a small integer that distin\u00adguishes \nthe different constructors of a data type; and so on.  In the case of a PAP, the size of the object \nis not .xed by its info table; instead, its size is stored in the object itself. The layout of its .elds \n(e.g. which are pointers) is described by the (initial segment of) an argument-descriptor .eld in the \ninfo table of the FUN object which is always the .rst .eld of a PAP. The other kinds of heap object all \nhave a size that is statically .xed by their info table. A very common operation is to jump to the entry \ncode for the object, so GHC uses a slightly-optimised version of the representation in Figure 3. GHC \nplaces the info table at the addresses immediately before the entry code, and reverses the order of its \n.elds, so that the info pointer is the entry-code pointer, and all the other .elds of the info table \ncan be accessed by negative offsets from this pointer. This is a somewhat delicate hack, because it involves \njuxtaposing code and data, but (sadly) it does improve performance signi.cantly (on the order of 5%). \nAgain, however, is not germane to this paper and we ignore it from now on. 4.5 The evaluation stack \nIn GHC, the evaluation stack s, in Section 4, is represented by a con\u00adtiguous block of memory1. The abstract \nstack of Section 4 is a stack of continuations, .. These continuations are each represented con\u00adcretely \nby a stack frame. The stack frames for the two continuations common to both push/enter and eval/apply \nare these: An update continuation Upd x is represented by a small stack frame, consisting of a return \naddress and a pointer to the thunk to be updated, x. In the push/enter model, an update frame must contain \na second word, which points to the next update frame down in the stack (see Section 5). Having a return \naddress in the update frame means that a value can simply return to the topmost return address, without \nhaving to test whether the top frame is an update continuation or a asecontinuation. The return address \nfor every update frame can be identical, though; it points to a hand-written code fragment, part of the \nruntime system, that performs the update, pops the update frame, and returns to the next frame.  A asecontinuation \nase of{alts}is represented by a return address, together with the free variables of the alternatives \nalts, which must be saved on the stack across the evaluation of the scrutinee. For example, consider \nthis function:  1In fact, GHC supports lightweight concurrency, so there are many threads. Each has \nits own stack, of limited size. The compiler generates explicit stack-over.ow tests, and grows the stack \nwhen necessary. None of this is relevant to the discussion of this paper, so we do not discuss concurrency \nor stack over.ow any further. f::(Int,Int)->(Bool,Int)->Int fxy=asehixof ( ,b)->aseh2yof w->w+b Across \nthe call to hix, we must save yon the stack, because it is used later, but we need not save x; then across \nthe call to h2ywe must save b, but we need not save y. Unlike an update frame, the return address for \neach aseex\u00adpression is different: it points to code for the case alternatives of that particular aseexpression. \nIn both cases, the frame can be thought of as a stack-allocated func\u00adtion closure: the return address \nis the info pointer, and it knows the layout of the rest of the frame that is, where the pointers, non\u00adpointers \nand (in the case of asecontinuations) dead slots are. In our implementation, the stack grows downward, \nso the return address is at the lowest address, and a stack frame looks exactly like Figure 3. A return \naddress has an info table that the garbage collector uses to navigate over the frame. In the next sections \nwe describe how the other two continuations are implemented: the Arg continuation for push/enter (Section \n5) and the ( a1 an)continuation for eval/apply (Section 6).  5 Implementing push/enter The push/enter \nmodel uses the stack to store pending arguments, rep\u00adresented by continuations of form Arg a. Unlike \nthe other continu\u00adations, these have no return address. When a function with arity n is entered, it begins \nwork by grabbing the top n arguments from the stack (rule FENTER), not by returning to them! This is \nprecisely the difference alluded to in the Introduction: the function is in control. How does the function \nknow how many arguments are on the stack? It needs to know this so that it can perform rule FENTER or \nPAP1 re\u00adspectively. In GHC the answer is this: we dedicate a register2, called Su( u for update ), to \npoint to the topmost update frame or ase frame, rather like the frame pointer in a conventional compiler. \nThen the function can see if there are enough arguments by taking the dif\u00adference between the stack pointer \nand Su. (The function knows not only how many arguments it is expecting, but how many words they occupy.) \nThis is the so-called argument satisfaction check. Every function is compiled with two entry points. \nThe fast entry point is used for known calls; it expects its arguments in registers (plus some on the \nstack if there are too many to .t in registers). The slow entry point expects all its arguments on the \nstack, and begins by performing the argument-satisfaction check. If the argument-sat\u00adisfaction check \nfails, the slow entry point builds a PAP and returns to the return address pointed to by Su; if it succeeds, \nthe slow entry point loads the arguments from the stack into registers and jumps (or falls through, in \nfact) to the fast entry point. 5.1 Reducing the number of Su pushes In conventional compilers, the frame \npointer is really only needed to support debugging, and some compilers provide a .ag to omit it, thereby \nfreeing up a register. We cannot get rid of Sualtogether, but when pushing a new frame it is often unnecessary \nto save Suand make it point to the new frame. Consider: asexof{(a,b)-> } We know for sure that xwill \nevaluate to a pair, not to a function! There is no need to make Supoint to the aseframe during eval\u00aduation \nof x. The only time we need to do so is when the scrutinee cannot statically be determined to be a non-function \ntype. The clas\u00adsic example is the polymorphic seqfunction: 2or a memory location on register-starved \narchitectures Figure 4. Stack layout for push/enter seq::a->b->b seqab=aseaof{x->b} In some calls to \nseq, awill evaluate to a function, while in others it will not. In the former case we must ensure that \nSupoints to the aseframe, so that rule PAP1 applies. In principle, the same is true about update frames, \nbut in practice there are several reasons that we want to walk the chain of update frames (see Section \n7) so GHC always saves Suin every update frame. To avoid that some aseframes have a saved Suand some \ndo not, we instead never save Suin a aseframe. Instead, in the (rare) situation of a non-data-typed ase, \nwe push two continuations, a regular asecontinuation, and, on top of it, a seqframe containing Su.A seqframe \nis like an update frame with no update: it serves only to restore Subefore returning to the aseframe \nunderneath. 5.2 Accurate stack walking The most painful aspect of the push/enter model is the problem \nof representing Arg continuations, which hold pending arguments. Consider these functions: g::Int->Int->Int#->Double#->Int \ngx= f::Int->Int fx=gxx34 5 Under the push/enter model, we push the pending arguments x, 3, and 4 5onto \nthe stack before making the tail call gx. The function gmight compute for a very long time before returning \na function that consumes the pending arguments. During this period, the pending arguments simply sit \non the stack waiting to be consumed. An accurate garbage collector must be able to identify every pointer \nin the stack. The push/enter model leads to stack layout that looks like Figure 4. Update and asecontinuations, \nwhose representa\u00adtion was discussed in Section 4.5, are represented by regular stack frames, consisting \nof a return address (shown black) on top of a block of data (shown white) whose exact layout is known \nto the return address. The garbage collector can use the return address to access the info table for \nthe return address (Section 4.5 again), just as it does for a heap-allocated closure. The info table \ndescribes the lay\u00adout of the stack frame, including exactly where in the frame the (live) pointers are \nstored, so that the garbage collector can follow them; it also gives the size of the frame, so that the \ngarbage collector knows where to start looking for the next frame. These regular stack frames are the \neasy (and well-understood) part. However, between each regular stack frame are zero or more Arg continuations, \nor pending arguments (shown grey). The dif.culty is that there is no description of their number or layout \nin the stack data structure. The function that pushed them knows what they are, and the function that \nconsumes them knows too but an arbitrarily long period may elapse between push and consumption, and \nduring that time the garbage collector must somehow deal with them. There are two sub-problems: Identifying \nwhich are pointers and which are non-pointers; as the example above showed, there may be a mixture. \n Distinguishing the last pending argument from the next return address on the stack, which heralds a \nnew stack frame.  One alternative is to have a separate stack for pending arguments, which solves the \nsecond of these sub-problems, but not the .rst. Or, the separate stack could be for pending non-pointer \narguments only, which solves the .rst sub-problem, but not the second. However, a separate stack carries \nheavy costs of its own, to allocate it, maintain a pointer to the stack top, and check for over.ow. We \ndo not consider this alternative further. Another non-alternative is to use a conservative garbage collector. \nFirstly, to plug space leaks we would then have to use extra memory writes to stub off dead pointers, \nsomething the frame layout maps deal with automatically; this turns out to be very important in prac\u00adtice. \nSecond, there are other reasons that GHC s runtime system has to walk the stack accurately: to black-hole \nthunks under evaluation, and to raise exceptions. Third, stacks may have to move in order to grow; GHC \ns lightweight concurrency precludes simply allocating a gigantic stack for each thread. Failing these \nalternatives, the obvious approach is to add a tag word to each Arg continuation. The tag word distinguishes \npointer\u00adcarrying from non-pointer-carrying Arg continuations, speci.es the size of latter kind, and can \nbe distinguished from the return address that heralds the next regular stack frame. Easy enough, but \ninef.\u00adcient. In the following two sections we describe two optimisations that GHC uses to reduce the \ntagging cost. 5.2.1 Omitting tags on pointers Our .rst optimisation is to not to tag pointer arguments \nat all. This is attractive because pointer arguments dominate (see Section 8). Fur\u00adthermore it looks \nrelatively easy to distinguish a pointer from the re\u00adturn address that heralds the next stack frame, \nwhereas non-pointer arguments, which can hold any bit-pattern whatsoever, cannot be dis\u00adtinguished in \ngeneral. We were wrong to think it was easy, though: the problem of distinguishing pointers from return \naddresses is much trickier than it looks, as we now discuss. GHC allocates some heap objects statically, \ncompiling them directly into the binary. So we distinguish an object pointer from a return address in \ntwo steps: Step 1: distinguish a pointer to a dynamic heap object from a static pointer. Stack-walking \naside, the garbage collector needs to make this distinction frequently, because it needs to know whether \nto copy the object referenced by a given pointer or not. We could do this by examining the info table \nof the object, but it s more ef.cient if the test can be done without derefer\u00adencing the pointer and \npolluting the cache, especially if it turns out that we aren t otherwise going to touch the object that \nit points to (static objects are assumed to be in the old generation in GHC s generational collector, \nso they rarely get touched). GHC therefore implements the static/dynamic test without dereferencing the \npointer, using an address-based test we know exactly where the dynamic heap is and we re-use that test \nto perform Step 1 of the heap-object/return-address test when stack-walking. Step 2: distinguish a pointer \nto a static object from a return ad\u00address. In earlier versions of GHC we did this by keeping static objects \nin a separate linker segment from the code: static ob\u00adjects are data, whereas return addresses are in \nthe text segment. Determining the border between text and data can usually be done, although it is non-portable \nand usually needs to be imple\u00admented in a different way for each new platform the compiler is ported \nto. Furthermore, this breaks down when dynamic link\u00ading is added to the mix, because there may be many \ntext and data segments scattered throughout the address space. One al\u00adternative, which we used on Win32 \nsystems with DLLs, was to place a zero word before every static closure and use this to discriminate, \nmaking use of the fact that a return address is never preceded by a zero word. The problem with this \nis that it means dereferencing the pointer, which is something we were trying to avoid for ef.ciency \nreasons. The problem of distinguishing pointers from return addresses could be solved in another way: \nby saving Suin a known place every reg\u00adular frame. Then the stack-walker could rely on an Suchain linking \nevery regular frame, so it would always know where the next regular frame began. However, building a \nchain of all frames would impose a non-trivial run-time cost by increasing memory traf.c. 5.2.2 Lazy \ntagging Tagging non-pointer pending arguments carries only a modest run\u00adtime cost, because (in Haskell \nat least) it is rare to call a function that returns a function that consumes non-pointer arguments. \nGHC therefore tags non-pointer Arg continuations staightforwardly, with a tag word pushed on top of the \nnon-pointer argument, containing the length in words of the non-pointer argument (usually 1 or 2). A \ntag can always be distinguished from a pointer argument, because pointer arguments never point to very \nlow addresses. Even tagging non-pointers is tiresome. When calling the fast entry point of a function, \nwe can pass some arguments in registers, but when there are too many we pass them on the stack. It would \nmake sense for the stack layout of these over.ow parameters to be the same as the latter part of the \nstack layout expected by the slow entry point (which takes all its arguments on the stack). The latter \nhas tagged slots for non-pointers, so the former had better do so too. But we do not want to take the \ninstructions to explicitly tag the slots when making a fast call fast calls to functions taking non-pointer \narguments are not at all rare so we allocate space for the tags but do not .ll the tags in. (In a call \nto a known function when too many arguments are supplied, we must generate code to tag the extra arguments \nbut not the known ones.) So the invariant at the fast entry point is that there is space for the tags \nof the non-pointer arguments passed on the stack, but these slots are non necessarily initialised. The \nfast entry point typically starts with a heap-over.ow check; if it fails, it must remember to .ll in \nthe tags, so that the top frame of the stack is self-describing. The exact details are unimportant here. \nThe point is that, while tag\u00adging non-pointers in the stack is feasible and reasonably ef.cient, it imposes \na signi.cant complexity burden on both code generator and the the run-time system.  5.3 Generating c-- \nSome compilers generate native code directly, but a very popular alternative route is to generate code \nin C, or a portable assembly lan\u00adguage such as C--, leaving to another compiler the tasks of instruc\u00adtion \nselection, register allocation, instruction scheduling, and so on. A signi.cant disadvantage of the push/enter \nmodel is that it makes this attractive route much harder, or at least much less ef.cient. The problem, \nagain, is the pending arguments. Suppose that we want to generate C. We plainly cannot push the pending \narguments onto the C stack, because C controls its own stack layout. There is just no way to have C stack \nframes separated by chunks of pending ar\u00ad guments. The only way out of this is to maintain a separate \nstack for pending arguments. In fact, GHC uses C as a code generator, and it keeps everything on the \nseparately-maintained stack: pending arguments, saved variables, return addresses, and so on. Indeed, \nGHC does not use the C stack at all, so we only have to maintain a single stack. Unfortunately, we thereby \ngive up much of the bene.t of the portable assembly language. If we do not use the C stack, we cannot \nuse C s parameter-passing mechanisms. Instead, we pass arguments either in global variables that are \nexplicitly allocated in registers (using a gdirective) or on the explicit stack. We have to perform our \nown liveness analysis to .gure out what variables are live across a call, and generate code to save them \nto to the explicit stack. In short, we only use C to compile basic blocks, managing the entire call/return \ninterface manually. There are other reasons why we could not use C s stack, however. There is no easy \nway to check for stack over.ow, or to move stacks around (both important in our concurrent Haskell system). \nC may save live variables across a call, but does not generate stack descrip\u00adtors for the garbage collector \n(Section 5.2). Portable exception hand\u00ading is tricky. And so on. C--, on the other hand, is a portable \nassembly language designed speci.cally to act as a back end for high-level-language compilers. It provides \nexplicit and very general support for tail calls, garbage collection, exception handling, and concurrency, \nand so addresses many of C s de.ciencies. Yet, we have found no general or clean way to extend C-- s \ndesign to incorporate pending arguments. So, like C, C--provides no way to push an arbitrary number of \nwords on the stack that should persist beyond the end of the current call. The bottom line is this. The \npending arguments required by the push/enter model are incompatible with any portable assembly lan\u00adguage \nknown to us, except by using that language in a way that viti\u00adates many of its advantages. We count this \nas a serious strike against the push/enter model.  6 Implementing eval/apply Next, we turn our attention \nto the implementation details for eval/apply. The eval/apply model uses call continuations, of form ( \na1 an), which are represented by a stack frame consisting of a return address, together with the arguments \na1 an. This return address is entered when a function has evaluated to a value (FUN or PAP), and returns. \nThis is the moment when the complicated rules (EXACT, CALLK, PAP2, and so on) are needed, and that involves \nquite a lot of code. So we do not generate a fresh batch of code for each call site; instead, we pre-generate \na range of call-continuation return addresses, for 1, 2, 3, ...N arguments. What if we need to push a \ncall continuation for more than N argu\u00adments? Then we push a succession of call continuations, each for \nas many arguments as possible, given the range of pre-generated re\u00adturn addresses. In effect, this reverts \nto something more like the argument-at-a-time function application process, except that we deal with \nthe arguments N at a time. We can measure how often this hap\u00adpens, and arrange to pre-generate enough \ncall continuations to cover 99.9% of the cases (Section 8). The remainder are handled by push\u00ading multiple \ncall continuations. An important complication is that we need different call continua\u00adtions when some \nof the arguments are unboxed. Why? Because: (a) the calling convention for the function that the continuation \nwill call may depend on the types of its arguments (e.g. a .oating-point argument might be passed in \na .oating-point register); and (b) the call-continuation return address must (like any return address) \nhave layout information to guide the garbage collector. So cannot get stgApplyNP(f,a,b){ /*Applyftoargumentsaandb*/ \nswithTYPE(f){ aseTRUNK: fun ode=CODE(f); f=fun ode(f); /*a,bsavedarossthisall*/ jumpstgApplyNP(f,a,b) \naseFUN: swithARITY(f){ ase1:/*Toomanyargs*/ fun ode=CODE(f); f=fun ode(f,a); /*bsavedarossthisall*/ \njumpstgApplyP(f,b); ase2:/*Exatlyright!*/ fun ode=CODE(f); jumpfun ode(f,a,b); other:/*Toofewargs*/ ...hekforenoughheap \nspaetoalloatePAP... r=...buildPAPfor(fab)... return(r) } asePAP: swithPAP ARITY(f){ ase1:/*Toomanyargs*/ \nf=applyPapN(f,a); jumpstgApplyP(f,b); ase2:/*Justright*/ jumpapplyPapNP(f,a,b) other:/*Toofewargs*/ ...hekforenoughheap... \nr=...buildPAPfor(fab)... return(r) }} Figure 5. The generic apply function StgApplyNP away with just \nN continuations, but (in principle) we need 3N . The 3 comes from the three basic cases we deal with: \npointer, 32-bit non-pointer and 64-bit non-pointer. There might well be more if, for example, a 32-bit \n.oat was passed in a different register than a 32\u00adbit integer. Hence the importance of measurements, \nto identify the common cases. 6.1 Generic application in more detail To be more concrete, we will imagine \nthat we compile Haskell into C--[13] (we will introduce any unusual features of C--as we go along). Here \nis the code that the call f3x, where fis an unknown function, might generate: jumpstgApplyNP(f,3,x) This \ntransfers control the jump indicates a tail call to a pre-generated piece of run-time system code, \nstgApplyNP, where the NP suf.x means one 32-bit non-pointer, and one pointer . The .rst parameter is \nthe address of the closure for f. It s just as if the original Haskell call had been stgApplyNPf3x, where \nstgApplyNPis a known function, so we make a fast call to it. The run-time system provides a whole bunch \nof stgApplyfunc\u00ad tions, for various argument combinations. Indeed, we generate them by feeding the desired \nargument combinations to a generator pro\u00adgram. Figure 5 shows (approximately) is the code we generate \nfor stgApplyNP. In this code we assume that TYPE(f)is a macro that gets the type .eld from the info table \nof heap object f, ARITY(f) gets the arity from the info table of a FUNobject, and so on. CODE(f) gets \nthe fast entry point of the function, which takes the function ar\u00adguments in registers (plus stack if \nnecessary). First, the function might be a TRUNK; in that case, we evaluate it (by calling its entry \npoint, passing the thunk itself as an argument), before looping around to stgApplyNPagain. Next, consider \nthe FUNcase, which begins by switching on the arity of the function: ase2: if it takes exactly two arguments, \nwe just jump to the function s code, passing the arguments aand b. We also pass a pointer to f, the function \nclosure itself, because the free variables of the function are stored therein. Note that if we end up \ntaking this route, then the function ar\u00adguments might not even hit the stack: aand bcan be passed in \nregisters to stgApplyNP, and passed again in registers when performing the .nal call. This is an improvement \nover push/enter, where arguments to unknown function calls are al\u00adways stored on the stack.  asei: if \nthe function takes fewer arguments than the number required by f in this case there is just one such \nbranch we must save the excess arguments, make the call, and then apply the resulting function to the \nremaining arguments. The code for an N-ary stgApplymust have a case for each i <N.So we get a quadratic \nnumber of cases, but since it s all generated mechanically, and the smaller arities cover almost all \ncases, this is not much of a problem in practice.  other: otherwise the function is applied to too few \narguments, so we should build a partial application in the heap. The third case is that fmight be a partial \napplication. The three cases are similar to those for a FUN, but they make use of an auxiliary fam\u00adily \nof functions applyPapXetc which apply a saturated PAP. This apply operation is not entirely straightforward, \nbecause PAP con\u00adtains a statically-unknown number of arguments. One solution is to copy the argument \nblock from the PAP, followed by the argument(s) to applyPapXto a temporary chunk of memory, and call \na separate entry point for the function that expects its arguments in a contiguous chunk of memory. The \nadvantage of this approach is that it requires no knowledge of the calling convention. Another solution \n(currently used by GHC) is to exploit knowledge of the calling convention to make a generic call; in \nGHC s case we just copy the arguments onto the stack. There are several opportunities for optimisation. \nFirst, we can have specialised FUNtypes for functions of small arity (1, 2, 3, say); that way we could \ncombine the node-type and arity tests. Second, a top level function has no (non-constant) free variables, \nso there is no need to pass its function closure as its .rst argument. We would need another FUNnode \ntype to distinguish this case. At the time of writing, GHC does not implement either of these optimisations. \n 6.2 Too many arguments What do we do with an unknown call for which there is no pre\u00adgenerated stgApplyXfunction? \nAnswer, we just split it into two (or more) chunks. For example, suppose we only had stgApplyX functions \nfor a single argument. Then our call f3xwould compile to: fi=stgApplyN(f,3); jumpstgApplyP(fi,x); Of \ncourse, xmust be saved on the stack across the call to stgApplyN.  7 A qualitative comparison Having \ndescribed the two implementations, we now summarise the main differences. In favour of eval/apply: Much \neasier to map to a portable assembly language, such as C--or C.  No need to distinguish return addresses \nfrom heap pointers. This is a big win (Section 5.2.1).  No tagging for non-pointers; this reduces complexity \nand makes stack frames and PAPs a little smaller.  No need for the Supointer, perhaps saving a register; \nand up\u00addate frames become one word smaller, because there is no need to save Su.  Because the arity-matching \nburden is on the caller, not the callee, run-time system support functions, callable from Haskell, become \nmore convenient to write.  When calling an unknown function with the right number of arguments, the \narguments can be passed in registers rather than on the stack. Push/enter pretty much mandates passing \nargu\u00adments to unknown functions in memory, on the stack.  In favour of push/enter: Appears to be a \nnatural .t with currying.  Eliminates some PAPallocations compared to eval/apply.  The payload of a \nPAPobject can be self-describing because the arguments are tagged. In contrast, an eval/apply PAPobject \nrelies on its FUNto describe the layout of the payload; this re\u00adsults in some extra complication in the \ngarbage collector, and an extra global invariant: a PAPmust contain a FUN, it cannot contain another \nPAP.  Plain differences: Push/enter requires a slow entry point for each function, incor\u00adporating \nthe argument-satisfaction check. Eval/apply does not need this, but (in some renditions) may require \nan entry point in which the arguments are in a contiguous memory block.  The Supointer makes it easy \nto walk the chain of update frames. That is useful for two reasons. First, at garbage col\u00adlection time \nwe want to black-hole any thunks that are under evaluation [7]. Second, a useful optimisation is to collapse \nsequences of adjacent update frames into a single frame, by choosing one of the objects to be updated \nand making all the others be indirections to it. Under eval/apply, however, one can still .nd the update \nframes by a single stack walk; but it may take a little longer because the stack-walk must examine other \nframes on the stack in order to hop over them. Notice, though, that there is nothing to stop us adding \nan Suregister, pointing to the topmost update frame, to the eval/apply model, if that turned out to be \nfaster for the reasons just described. We have not tried this.  From this list we conclude two things. \nFirst, it is essentially impos\u00adsible to come to a rational conclusion about performance based on these \ndifferences. The only way is to build both both models and measure the difference. Second, the eval/apply \nmodel seems to have decisive advantages in terms of complexity. Yes, the stgApplyX generator is a new \ncomponent, but it is well isolated, and not too large (it amounts to some 580 lines of Haskell including \ncomments). The big wins are that complexity elsewhere is reduced, and it is eas\u00adier to map the code to \na portable assembly language. The bottom line is this: if eval/apply is no more expensive than push/enter, \nit is de.nitely to be preferred. Uneval Unknown (%) Known (%) Program (%) < > < > anna 0.8 0.0 25.5 0.0 \n0.6 73.8 0.0 cacheprof 0.3 0.0 25.2 0.0 0.2 74.5 0.0 compress 0.0 0.0 1.6 0.0 0.0 98.4 0.0 fem 0.0 0.0 \n5.4 0.0 0.0 94.6 0.0 fulsom 0.4 0.0 25.0 0.0 0.2 74.8 0.0 hidden 0.1 0.0 13.8 0.0 0.0 86.1 0.1 infer \n0.1 0.0 18.8 0.0 0.1 81.1 0.0 scs 0.5 0.0 17.3 0.0 0.0 82.5 0.2 circsim 0.0 0.0 14.5 0.0 0.0 85.5 0.0 \n.bheaps 5.1 5.8 8.3 0.0 0.0 85.3 0.6 typecheck 0.5 0.0 27.3 0.0 0.5 72.2 0.0 simple 0.0 0.0 49.2 0.0 \n0.0 50.8 0.0 Min 0.0 0.0 0.0 0.0 0.0 21.2 0.0 Max 18.7 8.3 78.8 1.1 3.9 100.0 1.6 Average 1.0 0.4 20.3 \n0.0 0.2 79.0 0.1 Figure 6. Anatomy of calls  8 Measurements Our measurements are made on the Glasgow \nHaskell Compiler ver\u00adsion 5.04 (approximately; it does not correspond exactly to any re\u00adleased version). \nWe made measurements across the entire nofib benchmark suite of 88 programs; we present detailed .gures \nfor a representative set of a dozen larger benchmarks, but the tables also give minimum, maximum and \nmean .gures across the whole suite. These extrema are often due to the micro-benchmarks in the suite, \nwhich we generally do not show individually. 8.1 The anatomy of calls First of all, we present data \non the dynamic frequency of the dif\u00adferent categories of function call. All these .gures are independent \nof evaluation model; they are simply facts about programs in our benchmark suite, as compiled by GHC. \nFigure 6 show the relative dynamic frequency of: Calls to an unknown (lambda-bound or case-bound) function \nwhich turned out to be unevaluated (as a percentage of the total calls),  Calls to unknown functions \nwith (a) too few arguments, (b) exactly the right number of arguments, and (c) too many argu\u00adments (each \nas a percentage of the total calls),  Calls to a known (let-bound) function with (a) too few argu\u00adments, \n(b) exactly the right number of arguments, and (c) too many arguments (again, each as a percentage of \nthe total calls).  The last six columns of the table together cover all calls, and add up to 100%. Note \nthat known simply means that a let(rec) binding for the function is statically visible at the call site; \nthe function may be bound at top level, or may be nested. GHC propagates arity informa\u00adtion across module \nboundaries, which greatly increases the number of known calls. Also notice that every over-saturated \napplication of a known or unknown function gives rise to a subsequent call to the unknown function returned \nas its result; these unknown calls are in\u00adcluded in one of the unknown calls columns. For example, each \nexecution of the call idfxwould count as one call to a known function (id) with too many arguments, and \none call to the unknown function returned by id. These numbers lead to three immediate conclusions. First, \nknown calls are common, and sometimes dominate, but unknown calls can be the majority in some programs. \nUnknown calls must be handled ef.ciently. Second, known calls are almost always saturated; the ef\u00ad.ciency \nof handling under-or over-saturated known calls is not im\u00adportant, and they can be treated like unknown \ncalls (c.f. Section 4.3). Eval/apply change (.%) Code Memory Run- Program size Alloc Instrs reads writes \ntime anna -5.1 +1.7 +2.0 +2.5 -3.2 -0.7 cacheprof -4.0 -0.0 +10.7 +10.3 +0.3 +4.1 circsim +0.2 +0.0 +0.2 \n+1.0 -9.4 -4.7 compress +2.2 -0.0 +1.8 +3.1 +3.6 +1.8 fem -0.8 +0.0 -5.5 -3.2 -7.7 - .bheaps +1.0 +0.9 \n+3.3 +4.5 -3.1 - fulsom -2.1 +0.1 -2.5 -2.3 -7.9 -3.6 hidden -2.4 +0.0 +3.3 +4.0 -6.1 +2.0 infer -1.6 \n+0.2 +2.4 +2.4 -0.9 - scs -2.3 +0.0 +0.6 +1.4 -2.4 -3.7 simple -1.8 +0.0 +3.5 +2.5 -4.7 +1.4 typecheck \n+4.6 +1.2 +6.8 +6.6 -4.7 +3.0 Min -5.1 -2.7 -10.1 -8.0 -13.6 -23.1 Max +7.6 +2.9 +11.6 +20.8 +21.4 +6.8 \nG. Mean +1.8 +0.1 +0.0 +1.0 -4.8 -2.4 Figure 8. Space and time Third, even unknown calls are almost \nalways to an evaluated func\u00adtion with the correct number of arguments, so it is worthwhile op\u00adtimising \nthis case. For example, we can pass the arguments to the generic apply function in registers, in the \nhope that it can just pass them directly to the function. Figure 7 classi.es the unknown calls of Figure \n6, by their argument patterns. This data is helpful in deciding how many different ver\u00adsions of stgApplyto \ngenerate. We don t care about known func\u00adtions because we generate inline code for their calls. The column \nheadings use one character per argument to indicate the pattern with the key: p= pointer, v= void. pp, \nfor example, means a call with two pointer arguments. A void argument is an argument of size zero; such \narguments are used for the state token used for implementing the IOmonad. The general conclusion is clear: \na double-handful of 9 argument patterns is enough to cope with 99.9% of all situations. 8.2 The bottom \nline What really matters in the end is time and space. Figure 8 shows the percentage change we measured \nin moving from push/enter to eval/apply. The runtime .gures are wall-clock times, averaged over 5 runs, \ndiscounting any programs that ran for less than 0.5 seconds on our 1GHz Pentium III (around half of the \nsuite). The machine was otherwise unloaded at the time of the test. Somewhat to our surprise, there is \nonly a small difference between the two models, with eval/apply edging out push/enter by around 2\u00ad3% \nof runtime on average. Under eval/apply, update frames are one word smaller due to not having to save \nthe Suregister in the frame. Thus, benchmarks which perform a large number of updates did well under \neval/apply. For example, the program which was 23.1% faster, exp3 8, is a micro-benchmark that spends \na lot of its time doing up\u00addates. In general, we believe that this change to updates is largely re\u00adsponsible \nfor the overall reduction in the number of memory writes, which in turn is responsible for the slight \nimprovement in perfor\u00admance of eval/apply compared to push/enter. Heap allocation is largely unaffected \nby the change from push/enter to eval/apply, as can be seen in the Alloc column of Figure 8. A small \nchange in allocation can be explained by two factors. First, eval/apply will allocate a PAP when returning \na function applied to too few arguments, whereas push/enter may get away without heap allocation because \nthe function can .nd its missing arguments on the stack. Second, the PAPs in eval/apply may be slightly \nsmaller because there is no need to tag their non-pointer components (Sec\u00adtion 4.4). Argument pattern \n(% of all unknown calls) Program v p pv pp ppv ppp pppv pppp ppppp OTRER anna 0.0 29.6 0.0 69.3 0.0 1.1 \n0.0 0.0 0.0 0.0 cacheprof 0.0 91.6 0.0 8.1 0.0 0.3 0.0 0.0 0.0 0.0 compress 0.4 73.9 0.0 12.9 0.0 12.7 \n0.0 0.0 0.0 0.0 fem 0.0 91.3 0.0 8.1 0.0 0.6 0.0 0.0 0.0 0.0 fulsom 0.0 17.5 0.0 82.5 0.0 0.0 0.0 0.0 \n0.0 0.0 hidden 0.2 48.7 0.0 14.3 0.0 36.8 0.0 0.0 0.0 0.0 infer 0.0 51.8 0.0 48.1 0.0 0.1 0.0 0.0 0.0 \n0.0 scs 1.4 19.6 0.0 79.0 0.0 0.0 0.0 0.0 0.0 0.0 circsim 0.0 70.2 0.0 8.6 0.0 21.2 0.0 0.0 0.0 0.0 .bheaps \n0.0 43.2 13.7 43.1 0.0 0.0 0.0 0.0 0.0 0.0 typecheck 0.0 89.5 0.0 10.5 0.0 0.0 0.0 0.0 0.0 0.0 simple \n0.0 20.1 0.0 79.9 0.0 0.0 0.0 0.0 0.0 0.0 Min 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 Max 58.6 100.0 \n13.7 100.0 15.5 98.9 6.2 11.3 0.3 0.1 Average 5.2 54.4 0.3 34.4 0.3 5.2 0.1 0.1 0.0 0.0 Figure 7. Argument \npatterns 9 Related work Two of the most popular and in.uential abstract machines for lazy languages, \nthe G-machine [6] and the Three Instruction Machine (TIM) [3], both use push/enter. As a result, many \ncompilers for lazy languages, including GHC and hb, use push/enter. However Fax\u00b4en s OCP compiler for \nthe lazy language Plain uses eval/apply [4]. Rather than have generic stgApplyXXapplication procedures, \nOCP creates specialised function entry points. For each function fof arity n, and for each i <n,j <n \n.i, OCP makes an entry point fij that expects to .nd i arguments in a PAP object, and j extra arguments \npassed in registers. That looks like an awful lot of entry points, but a global .ow analysis allows OCP \nto prune many entry points that cannot be used. The possibility of such speciali\u00adsation is an additional \nbene.t of eval/apply (Boquist [1] describes an extreme version). Eager Haskell, an unusual implementation \nof Haskell based on eager evaluation, also uses eval/apply [10]. Caml, a call-by-value language, uses \npush/enter for the interpreter [9], but eval/apply for the compiler, largely for the reasons outlined \nin Section 7. 10 Conclusions Our main conclusion is easy to state: for a high-performance, com\u00adpiled \nimplementation of a higher order language, use eval/apply! There is not much to choose between the two \nmodels on perfor\u00admance grounds, and eval/apply makes it noticeably easier to manage the complexity of \na compiler and runtime system for a higher order language, as Section 7 explained. We are con.dent of \nthis result for a non-strict language, and we believe that the bene.t is likely to be more pronounced \nfor a strict one. Many of the complexities of push/enter are caused by ef.ciency hacks, however. For \nan interpreter, where performance is not such an issue, these hacks are not important, and push/enter \nmay well be a more elegant solution. Acknowledgments. Many thanks to Robert Ennals, Karl-Filip Fax\u00b4en, \nXavier Leroy, Jan-Willem Maessen, Greg Morrisett, Alan Mycroft, Norman Ramsey, and Keith Wansbrough for \ngiving the paper a care\u00adful read. 11 References [1] U. Boquist. Code Optimisation Techniques for Lazy \nFunctional Languages. PhD thesis, Chalmers University of Technology, Sweden, April 1999. [2] R. Douence \nand P. Fradet. A systematic study of functional lan\u00adguage implementations. ACM Transactions on Programming \nLanguages and Systems, 20(2):344 387, March 1998. [3] J. Fairbairn and S. Wray. TIM -a simple lazy abstract \nma\u00adchine to execute supercombinators. In G. Kahn, editor, Proc IFIP conference on Functional Programming \nLanguages and Computer Architecture, Portland, pages 34 45. Springer Ver\u00adlag LNCS 274, Sept. 1987. [4] \nK.-F. Fax\u00b4en. Analysing, Transforming and Compiling Lazy Functional Programs. PhD thesis, Department \nof Teleinfor\u00admatics, Royal Institute of Technology, June 1997. [5] C. Flanagan, A. Sabry, B. Duba, and \nM. Felleisen. The essence of compiling with continuations. In ACM Conference on Pro\u00adgramming Languages \nDesign and Implementation (PLDI 93), pages 237 247. ACM, June 1993. [6] T. Johnsson. Ef.cient compilation \nof lazy evaluation. In Proc SIGPLAN Symposium on Compiler Construction, Montreal. ACM, June 1984. [7] \nR. Jones. Tail recursion without space leaks. Journal of Func\u00adtional Programming, 2(1):73 80, Jan 1992. \n[8] J. Launchbury. A natural semantics for lazy evaluation. In 20th ACM Symposium on Principles of Programming \nLanguages (POPL 93), pages 144 154. ACM, Jan. 1993. [9] X. Leroy. The Zinc experiment: an economical \nimplementa\u00adtion of the ML language. Tr 117, inria-rocquencourt, INRIA, Feb. 1990. [10] J.-W. Maessen. \nHybrid Eager and Lazy Evaluation for Ef.cient Compilation of Haskell. PhD thesis, Massachusetts Institute \nof Technology, June 2002. [11] S. Peyton Jones. Implementing lazy functional languages on stock hardware: \nThe spineless tagless G-machine. Journal of Functional Programming, 2(2):127 202, Apr. 1992. [12] S. \nPeyton Jones and J. Launchbury. Unboxed values as .rst class citizens. In R. Hughes, editor, ACM Conference \non Func\u00adtional Programming and Computer Architecture (FPCA 91), volume 523 of Lecture Notes in Computer \nScience, pages 636 666, Boston, 1991. Springer Verlag. [13] S. Peyton Jones, N. Ramsey, and F. Reig. \nC--: a portable as\u00adsembly language that supports garbage collection. In G. Na\u00addathur, editor, International \nConference on Principles and Practice of Declarative Programming, number 1702 in Lecture Notes in Computer \nScience, pages 1 28, Berlin, Sept. 1999. Springer.   \n\t\t\t", "proc_id": "1016850", "abstract": "Higher-order languages that encourage currying are implemented using one of two basic evaluation models: push/enter or eval/apply. Implementors use their intuition and qualitative judgements to choose one model or the other.Our goal in this paper is to provide, for the first time, a more substantial basis for this choice, based on our qualitative and quantitative experience of implementing both models in a state-of-the-art compiler for Haskell.Our conclusion is simple, and contradicts our initial intuition: compiled implementations should use eval/apply.", "authors": [{"name": "Simon Marlow", "author_profile_id": "81100515135", "affiliation": "Microsoft Research, Cambridge", "person_id": "P265492", "email_address": "", "orcid_id": ""}, {"name": "Simon Peyton Jones", "author_profile_id": "81100271851", "affiliation": "Microsoft Research, Cambridge", "person_id": "PP43121273", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1016850.1016856", "year": "2004", "article_id": "1016856", "conference": "ICFP", "title": "Making a fast curry: push/enter vs. eval/apply for higher-order languages", "url": "http://dl.acm.org/citation.cfm?id=1016856"}