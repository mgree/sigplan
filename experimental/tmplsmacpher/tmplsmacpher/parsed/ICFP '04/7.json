{"article_publication_date": "09-19-2004", "fulltext": "\n Implementing Functional Logic Languages Using Multiple Threads and Stores * Andrew Tolmach Sergio Antoy \nMarius Nita apt@cs.pdx.edu antoy@cs.pdx.edu marius@cs.pdx.edu Dept. of Computer Science, Portland State \nUniversity P.O.Box 751, Portland, OR 97201 Abstract Recent functional logic languages such as Curry and \nToy combine lazy functional programming with logic programming features in\u00adcluding logic variables, non-determinism, \nuni.cation, narrowing, fair search, concurrency, and residuation. In this paper, we show how to extend \na conventional interpreter for a lazy functional lan\u00adguage to handle these features by adding support \nfor reference cells, process-like and thread-like concurrency mechanisms, and a novel form of multi-versioned \nstore. Our interpretation scheme is prac\u00adtical, and can be easily extended to perform compilation. The \nlan\u00adguage speci.ed by our interpreter is designed so that programs are deterministic in a novel and useful \nsense. Categories and Subject Descriptors: D.3.2. [Programming Languages]: Language Classi.cations Multiparadigm \nlan\u00adguages, Applicative (functional) languages, Constraint and logic languages; D.3.4. [Programming Languages]: \nLanguage Processors Interpreters, Run-time environments General Terms: Languages, Design Keywords: Functional \nlogic languages, narrowing, residuation, multi-versioned stores 1 Introduction Functional logic programming \n(FLP) integrates the most impor\u00adtant features of functional and logic programming within a single programming \nmodel (see Hanus [8] for a detailed survey). Thus, functional logic languages provide pattern matching \nin the de.ni\u00adtion of functions and predicates as well as the use of logical vari\u00adables in expressions. \nThe latter feature requires some built-in search principle in order to guess the appropriate instantiations \nof logical * This work has been supported in part by the National Science Foundation under grants CCR-0110496 \nand CCR-0218224. Permission to make digital or hard copies of all or part of this work for personal or \nclassroom use is granted without fee provided that copies are not made or distributed for pro.t or commercial \nadvantage and that copies bear this notice and the full citation on the .rst page. To copy otherwise, \nto republish, to post on servers or to redistribute to lists, requires prior speci.c permission and/or \na fee. ICFP 04, September 19 21, 2004, Snowbird, Utah, USA. Copyright 2004 ACM 1-58113-905-5/04/0009 \n...$5.00 variables. There are a number of languages supporting FLP in this broad sense, including Curry \n[13], Escher [19], Life [1], Mercury [30], Oz [29], and Toy [20], among others. Narrowing is one promising \nbasis for a functional logic language. It is a combination of term reduction as in functional programming \nand (non-deterministic) variable instantiation as in logic program\u00adming. It exploits the presence of \nfunctions without transforming them into predicates, which yields a more ef.cient operational be\u00adhavior \n[7, 9]. As a simple example, consider this program (in Curry syntax, as are all the examples in this \nsection). data Color = Red | Yellow | Blue | Orange | Violet | Green mix Red Blue = Violet mix Yellow \nBlue = Green mix Yellow Red = Orange a1,a2,a3 :: Color a1 = mix Red Blue a2 = mix Yellow x where x free \na3 | (mix Yellow x =:= Green) = x where x free To compute a1, mix is used as an ordinary function, and \nreturns Violet.In a2, free declares x as a logic variable, so the call to mix narrows over the latter \ntwo clauses and produces two answers corresponding to the colors that result when Yellow is mixed with \nanother primary (namely Green and Orange). In a3, narrowing produces the same results from mix, but they \nare then .ltered by the equality constraint (=:=) appearing in the guard, and the return value is x itself \nwhich here will be Blue, the color that produces Green when mixed with Yellow. It is well known that \nnarrowing is an evaluation mechanism that enjoys soundness and completeness in the sense of functional \nand logic programming, i.e., all computed solutions are correct and all correct solutions are computed. \nBut narrowing is not able to deal with primitive (or external) functions. Therefore several authors (e.g., \nAit-Kaci [1], Lloyd [19]) have proposed alternative evaluation strategies based on residuation. The residuation \nprinciple delays a function call until the arguments are suf.ciently instantiated so that the call can \nbe deterministically reduced. To make this useful, residuation-based languages also support concurrent \nevaluation in order to deal with suspended computations. Hanus [11] proposes a seamless combination of \nneeded narrowing with residuation-based concurrency. This is the basis of the pro\u00adgramming language Curry \n[13], an attempt to provide a standard in the area of FLP languages. To illustrate the interplay between \nnarrowing and concurrency, suppose we have a primitive function hue::Color->Float that returns a numeric \nequivalent for each color (e.g., its position on the color wheel measured in radians), and assume that \nthe + operator on Floats is also primitive. The following expressions both sum hues for two colors: a4,a5 \n:: Float a4 = hue (mix x Blue) + hue x where x free a5 = hue x + hue (mix x Blue) where x free Both a4 \nand a5 should produce two answers, (hue Violet + hue Red) and (hue Green + hue Yellow), corresponding \nto the two narrowing choices for mix. But since hue is primitive, it cannot be invoked without an instantiated \nvalue for x. Thus, a4 requires the mix call in the left operand to + to be evaluated before the right \noperand; similarly, a5 requires the opposite order of evaluation. In general, primitive operands need \nto be evaluated concurrently in or\u00adder to guarantee that we get the same answer set independent of the \norder in which we write them. The evaluation strategy for the par\u00adallel and (&#38;) primitive operator \nin Curry, which is used to connect simultaneous constraints, is just an instance of this general policy. \nNarrowing can easily generate an in.nite space of alternatives to be explored. Faithful implementations \nof narrowing must employ a fair search strategy in order to guarantee completeness, i.e., they should \nnever get stuck inde.nitely computing in one narrowing al\u00adternative when there are other alternatives \nstill to be tried. Consider the following function to compute the color-wheel complements: complement \n:: Color -> Color complement Red = Green complement Yellow = Violet complement Blue = Orange complement \nx | (complement y =:= x) = y where y free a6 :: color a6 = complement Orange Here we thought to save \neffort by de.ning only half the cases ex\u00adplicitly, and using recursion and narrowing for the other half. \nThis is simple and elegant, but it is dangerous unless the language uses a fair strategy. In Curry, when \ntwo or more clauses match an ar\u00adgument, they are both explored, regardless of clause order. Since the \nlast clause for complement matches any argument, at least one computation path for a6 will always be \nin.nite. If the language search strategy is not fair, clause order is likely to matter. For ex\u00adample, \nif conventional backtracking is used, moving the last clause .rst is likely to get us stuck on an in.nite \nsequence of recursive calls. A fair strategy will guarantee that we eventually consider all possible \npaths (regardless of clause order), and so get an answer. (Of course, we would have to wait forever if \nwe wanted to make sure there were no other answers.) These examples illustrate that both the semantic \nformalization and the implementation of Curry-like languages must deal with ordi\u00adnary (lazy) functional \nevaluation, narrowing, and residuation in an integrated framework. This is a non-trivial task, especially \nbe\u00adcause functional and logic programming have rather different se\u00admantic and implementation traditions. \nThe of.cial semantics for Curry [13], largely based on work by Hanus [11, 10], is an op\u00aderational semantics \nbased on de.nitional trees and narrowing steps. Although fairly low-level, this semantics says nothing \nabout sharing behavior, which has an important impact on the ef.ciency of lazy functional programs, and \nis actually observable in the presence of logic variables. Moreover, this semantics is quite different \nin char\u00adacter from typical semantic formulations for functional languages, which are often given denotational \nor natural ( big step ) opera\u00adtional semantics. In more recent work [2], Albert, Hanus, Huch, Oliver, \nand Vidal propose a natural semantics, incorporating shar\u00ading, for the .rst-order functional and narrowing \naspects of Curry, and a small-step semantics that also covers residuation and concur\u00adrency. In previous \nwork [31], we extended the large-step semantics of Albert, et al., to handle concurrency, addressed some \nproblems with determinism in the presence of residuation, incorporated true higher-order functions, and \ncast the entire semantics in the form of a modular monadic interpreter written in Haskell. However, our \nlarge-step semantics is not suitable for use as an implementation. The small-step semantics of Albert, \net al. is more suitable, but still not very practical; for example, it treats substitution across an \nentire heap as a basic operation. On the implementation front, there have been a number of practical \nsystems, include Pakcs [12], and the M\u00a8unster Curry compiler [21]. These systems are fairly full-featured \nand deliver adequate per\u00adformance; their main drawback is that they use backtracking as a search strategy, \nand so are not complete. A more experimental, Java-based system that does use a fair strategy for narrowing \nwas reported in a previous paper [3]; that work did not address concur\u00adrency for arguments to primitives \n(except &#38;). The major contributions of the present paper are these: We describe a simple and parsimonious \nCCore language for functional logic programming that supports all the features mentioned above. Programs \nin Curry-like source languages can be desugared into CCore.  We show how a conventional functional language \nabstract\u00admachine interpreter can be extended in easy stages to support FLP with fair search, by using \nreference cells, process-like and thread-like concurrency mechanisms, and variant heaps. Viewed as a \nsemantics, our interpreter strongly resembles that of Albert, et al. [2], but our presentation is considerably \nmore detailed, making it a good basis for a practical interpreter.  To support narrowing, we de.ne a \nnovel kind of variant heaps that support a Unix-like fork operation; these may be of in\u00addependent interest \nfor applications requiring an analogue to callcc for stores.  The remainder of the paper is organized \nas follows. Section 2 gives a detailed description of our CCore language. Section 3 describes the translation \nphases that convert CCore to ACore, the form we actually interpret. Section 4 describes the de.nitional \ninterpreter in detail. Section 5 discusses practical implementation issues, in particular the ef.cient \nimplementation of variant heaps. Section 6 discussed ongoing and future work, and concludes. We assume \nreading knowledge of Standard ML [24], which is used throughout to describe the interpreter and its data \nstructures.  2 CCore Language Our starting point is an expression-based CCore language. This is essentially \na call-by-need, higher-order functional language with algebraic data types, extended with logic variables, \n.exible case expressions, (shallow) uni.cation, and a seq operator to force eval\u00aduation. We assume (but \ndo not describe) a pre-processing step that desugars richer source languages such as Curry into CCore. \nOur primary motivation in designing CCore has been to keep the basic facilities of the language as simple \nas possible, so they will have structure CCore = struct type var = string x (variable name) type dcname \n= string c (data constructor name) datatype prim = ... p (primitive operator) type pattern = dcname * \nvar list (case pattern) datatype flexibility = FLEXIBLE | RIGID f | r (case .exibility) datatype exp \n= e := Var of var x (variable) | Int of int i (integer) | Abs of var * exp .x.e (abstraction) | App of \nexp * exp e1 e2 (application) | Capp of dcname * exp list c(e1,...,en) (constructor application) | Primapp \nof prim * exp list p(e1,...,en) (primitive application) f|r | Case of flexibility * exp * case e of \n{ c(x1,...,xn)=> e } (pattern * exp) list (case: .exible or rigid) | Letrec of (var * exp) list * exp \nletrec { x = e }in e (recursive binding) | Let of var * exp * exp let x = e1 in e2 (local binding) | \nSeq of exp seq e (force evaluation) | Free free (logic variable) | Unify of exp * exp unify e1 e2 (shallow \nuni.cation) end Figure 1. CCore Language Abstract and Concrete Syntax. Curly braces delimit zero or \nmore repetitions. obvious, ef.cient implementations; more complicated operations can be encoded as macros \nor subroutines in CCore itself. Figure 1 de.nes the abstract syntax for the language, and a corresponding \nconcrete syntax used in examples. 2.1 Types and Values CCore is not explicitly typed, but we assume throughout \nthat we are dealing with typable expressions. In particular, programs are to be interpreted in the context \nof a set of algebraic datatype declara\u00adtions, .xing the names and arities of the data constructors, which \ncan be speci.ed in the usual Haskell or ML style. At a minimum, we assume these algebraic types: data \nBool = False | True data Success = Success data List a = Nil | Cons (a, List a) Success is used as the \ntype of constraints, which either succeed (but without returning any useful value) or fail (without returning \na value at all). For simplicity, the only primitive type used in this pa\u00adper is Int, which has the usual \nliterals; other primitive types along the same lines could easily be added. There are also the usual \narrow (function) types. In addition to the ordinary values, each non-arrow type includes an endless supply \nof distinct logic variables. These are initially unin\u00adstantiated; in the course of execution they may \nbecome instantiated to a value or to another logic variable of the same type. In the pres\u00adence of logic \nvariables, it is useful to distinguish between several different degrees of evaluation: Head-normal \nform (HNF) means the top-level constructor is known (for algebraic types), or the integer value is known \n(for the Int type), or the function closure is known (for an arrow type), or the value is an uninstantiated \nlogic variable.  Normal form (NF) means in HNF, and all subterms are also in NF.  Constructor-head-normal \nform (CHNF) means in HNF, but not an uninstantiated logic variable.  Constructor-normal form (CNF) means \nin CHNF, and all sub\u00adcomponents are also in CNF.  The basic evaluation mechanism provided by our interpreter \ncom\u00adputes HNF values; stronger degrees of normalization can be en\u00adcoded within CCore itself (see Section \n2.7). 2.2 Expressions The CCore expression language is very similar to that of Core Haskell [26], with \nthe addition of logic variables (free), .exible case expressions, unify, and seq. Function abstractions \nand ap\u00adplications have exactly one argument; multiple-argument functions can be coded as nested abstractions. \nData constructors (c) and prim\u00aditive operators (p) have .xed arity ar = 0. All primitive and con\u00adstructor \napplications and patterns (within Case expressions) must be saturated; unapplied or partially applied \nconstructors can be ex\u00adpressed by .-expanding them. By default, function and constructor arguments are \nevaluated lazily, but eager evaluation can be forced by wrapping an argument in seq. Primitive arguments \nare evaluated eagerly to CHNF. The arguments to a primitive or constructor are (conceptually) evaluated \nin parallel; this is important if evaluation of one or more arguments residuates (Section 2.5). A minimal \nset of primitive operators includes: == :: Int \u00d7 Int . Bool +,-,*,etc. :: Int \u00d7 Int . Int chnf :: .a.a \n. a pand :: Success \u00d7 Success . Success The last two operators are used only for their side-effects: \nchnf is just the identify function, which forces its argument to a CHNF value; pand forces parallel evaluation \nof its arguments (which are typically constraints) to CHNF but then ignores them. Let introduces a local \nbinding; it is evaluated lazily unless the right-hand-side is wrapped in a seq. Although, as we shall \nsee, \u00df-reduction is not generally valid for CCore, we do have (.x.e1) e2 = let x = e2 in e1. Letrec introduces \na set of (potentially) mutually recursive bindings, always evaluated lazily (seqs are not permitted on \nthe right-hand sides). Case expressions analyze values of algebraic type. Patterns are shallow; they \nconsist of a data constructor name c and a corre\u00adsponding list of variables. More complicated pattern \nmatching can be expressed using nested case expressions. The patterns for a sin\u00adgle case are assumed \nto be mutually exclusive, but not necessarily exhaustive; a computation that attempts to dispatch to \na missing case arm fails. Evaluating a case causes the scrutinee (the expres\u00adsion being cased over ) \nto be evaluated to HNF. Each case is stat\u00adically marked by the programmer as either .exible or rigid. \nThis controls what happens when the scrutinee evaluates to an uninstan\u00adtiated logic variable: .exible \ncases narrow (Section 2.4); rigid cases residuate (Section 2.5). A free expression evaluates to a fresh, \nanonymous, uninstantiated logic variable. Logic variables can be instantiated either by being used as \nthe scrutinee of a .exible case, or via evaluation of a unify (described in Section 2.7). Instantiation \ncan occur at most once; the instantiating value is always in HNF. Once instantiated, any use of the variable \ntransparently fetches the instantiating value. 2.3 Programs, Answers, and Determinism A program is just \na closed top-level expression. Each program eval\u00aduates to an unordered multiset of answers, each of which \nis an HNF value. (If NF values are desired, the program expression can be wrapped in a normalizing function; \nsee Section 2.7.) In general, there may be one answer corresponding to each combination of nar\u00adrowing \nchoices made during the evaluation of the program; how\u00adever, choices that lead to failure don t contribute \nan answer. The only possible sources of failure are a missing case arm, a failed uni.cation, or .oundering \n(Section 2.5). Determinism Property. Although we think of individual expres\u00adsions as being non-deterministic, \nwe would like CCore programs as a whole to be deterministic, in the speci.c sense that they al\u00adways produce \nthe same multiset of answers (neglecting order), no matter how the implementation schedules evaluation \nof narrowing alternatives and parallel evaluation of arguments, provided that the schedule is starvation-free. \nMaintaining the Determinism Property has been a key goal in our design of CCore, and has directed a number \nof important design decisions. However, we do not give a formal proof that it holds (indeed, the property \nwould require a more formal statement before a proof could be attempted). 2.4 Narrowing and Fairness \nNarrowing in CCore occurs when evaluating a .exible case, e.g. f case e of c1(x1,1,...,x1,ar(c1))=> e1 \n... cn(xn,1,...,xn,ar(cn))=> en if the HNF v of e is an uninstantiated logic variable. At such a point, \nevaluation splits into n parallel alternative evaluations, one corre\u00adsponding to each case arm. In the \ni-th alternative, v is instantiated to ci(w1,...,war(ci)), where the w values are fresh uninstantiated \nlogic variables; evaluation then proceeds by evaluating ei and con\u00adtinuing with the computation that \ndemanded the value of the case in the .rst place. Since the result of the program can depend on the value \nof v (and of the case), each of the n alternatives may con\u00adtribute a value to the overall multiset of \nprogram answers, or may terminate in failure. Note that a simple non-deterministic choice operator, which \nwe call amb (after McCarthy [22]), can easily be de.ned using a .exible case: amb e1 e2 = f case free \nof True => e1 False => e2 Since .exible cases can be nested, each alternative may split into further \nsub-alternatives, forming an or-parallel tree for the pro\u00adgram. Each leaf of the tree potentially contributes \nto the program answer, so all branches of the tree must be fully explored. The branches are completely \nindependent, so the strategy used to ex\u00adplore them is unimportant in principle, provided that it copes \nwith non-terminating branches. In other words, the strategy must be fair; it should guarantee that every \nanswer will ultimately be computed. Note that most implementations of (functional) logic languages use \nsimple backtracking, i.e., depth-.rst exploration, but this can get stuck in a non-terminating branch. \nTo avoid this problem, we use multiple concurrent or-parallel threads (or simply or-threads ) which are \nscheduled in a breadth-.rst fashion. Since each or-thread is logically independent from the others, it \nmust have its own heap; it thus resembles a process in a conventional operating system. 2.5 Residuation \nand Concurrency Sometimes evaluation is blocked because of an uninstantiated logic variable. For example, \nthe + primitive operation cannot proceed until both operands have been evaluated to integers. More gener\u00adally, \nall operands to primitives and scrutinees of caser expressions must evaluate to CHNF. What should happen \nwhen evaluation is blocked? The simplest idea might be to have blocked computations fail. But this would \nlead to very unattractive semantics. Recall ex\u00adpressions a4 and a5 from Section 1. Using left-to-right \nevaluation of the + operands, a4 produces answers, but a5 fails; using right\u00adto-left evaluation, the \nconverse is true. But there is nothing about the (intuitive) semantics of + that justi.es choosing one \norder over the other, so surely either both expressions should fail or (more use\u00adfully) both should succeed. \nMore generally, it might be the case that neither .xed order for operand evaluation works, but that an \ninterleaving of the two eval\u00aduations does. Consider sum = let y =free in let z =free in + (let dummy \n= unify y 1 in z, let dummy = unify z 1 in y) Here neither operand to + can be fully evaluated until \nevaluation of the other has started, but there is an interleaving of read and write operations on the \nvariables that produces an answer. Thus we are led naturally to the idea that operands to primitives \nshould be eval\u00aduated concurrently. The primitive application produces an answer if any interleaving of \nthe atomic variable read and write operations of the operand evaluations leads to an answer. Otherwise, \nthe com\u00adputation fails by .oundering. More operationally, we can think of each operand evaluation as \na separate and-parallel thread (or just and-thread ). Unlike or\u00adthreads, all and-threads must produce \nan answer, and they all share a single heap. Thus, they resemble lightweight (intra-process) threads \nin a conventional operating system. If any and-thread blocks (residuates) on an uninstantiated variable, \nevaluation of the other threads proceeds; with luck, one of these threads will instanti\u00adate the required \nvariable, allowing evaluation of the initial thread to proceed. If all the and-threads residuate simultaneously, \nthe compu\u00adtation .ounders; operationally, this corresponds to thread deadlock. The parallel and operator \nin Curry, which is ordinarily used to connect equational constraints, can now be seen as just a special \ncase of a primitive operator, which (just) requires its operands to be evaluated successfully (i.e., \nnot to fail). Parallel-and supports useful programming idioms originally developed in languages like \nConcurrent Prolog [28]. But it is interesting to note that even if we are not interested in using these \nidioms, the desire to maintain the Determinism Property drives us to evaluate primitive parameters concurrently: \nsupporting concurrency seems almost unavoidable in the presence of primitive operators that cannot narrow. \nParallel evaluations can be nested, e.g., when one primitive oper\u00adation consumes the result of another. \nIn such situations, the inner computation should not .ounder as long as there is a possibility that the \nouter computation might instantiate the necessary variables. Operationally, this means that all the and-threads \nfrom both inner and outer computations should be treated as a single pool; .ounder\u00ading occurs only when \nthe entire pool is deadlocked. In a general program, or-threads and and-threads coexist; the over\u00adall \nstate of the computation is always an or-parallel tree whose leaves are and-parallel sets (perhaps just \nsingleton sets if there is no concurrency). When a narrowing step occurs in any and-thread, the complete \nset of and-threads is replicated in each of the or-threads generated by the narrowing alternatives. As \nan example, consider the expression let x= free in let y=free in + (casef x of True => 0 | False => 1, \nf case yof True=>2|False=>4) Figure 2 illustrates how the computation state unfolds as this ex\u00adpression \nis evaluated. Initially (a), two and-threads are created for the two arguments to +. Assume that the \nleft argument is evaluated .rst. After the left narrowing step (b), there are two or-threads, each with \nits own pair of and-threads. After the right narrowing step (c), there are four or-threads and eight \nand-threads; at this point, the + operations can all evaluate, yielding four answers (2,3,4,5). 2.6 \nLogic Variables and Effects Creation and instantiation of logic variables are imperative effects. They \nmay therefore seem to be a poor ingredient to add to a lazy language. But the resulting language is reasonable \nto program in (and has the Determinism Property) because no program answer can depend on the order in \nwhich variable instantiations occur. In\u00adtuitively, this follows from the facts that no variable can be \ninstan\u00adtiated more than once, and that it is impossible to test whether a variable has been instantiated. \n(This last property is rather delicate; for example, small changes in the treatment of uni.cation could \nbreak it; see Section 2.7). (a) case y (c) y = F 0 4 Figure 2. Threads at various stages of evaluation. \nDashed lines connect groups of and-threads. However, logic variables do allow us to observe the sharing \nbehav\u00adior of the language. For example, given coin = amb01 coin1 = (.x. +(x,x)) coin coin2 = +(coin,coin) \ncoin1 produces the answer multiset {0,2}, whereas coin2 pro\u00adduces {0,1,1,2}. This example illustrates \nwhy \u00df-equivalence is not generally valid for CCore. 2.7 Full Uni.cation and Normalization The design \nof CCore deliberately excludes expression forms that would require ad-hoc polymorphic implementations. \nIn particular, uni.cation and full normalization both require traversal of arbitrary constructed values, \nso neither is provided as a built-in expression form. Instead, we unbundle these tasks: CCore provides \nonly shallow operations for uni.cation (unify) and forcing evaluation (seq), but these can be used to \nbuild type-indexed families of full equality and normalization functions in CCore itself. Uni.cation \nand equality constraints. The shallow unify opera\u00adtor evaluates its arguments in parallel to HNF, and \nthen attempts to unify the heads of the resulting values. If either argument evaluates to a logic variable, \nthe uni.cation succeeds (instantiating the vari\u00adable), and the unify expression evaluates to True, indicating \nthat there is no need to inspect sub-terms of either argument. Otherwise, the top-level constructors \nof the two values are compared but not the subterms. (In this context, integers are treated like nullary \ncon\u00adstructors.) If the heads are not equal, the entire computation fails without returning a value; if \nthey are equal, the unify expression evaluates to False, indicating that shallow uni.cation succeeded \nbut any sub-terms still need to be compared. Using unify as a building block, we can implement a deep \nstruc\u00adtural equality constraint operator somewhat similar to Curry s op\u00aderation =:= : .a.a -> a -> Success \nas a type-indexed family of CCore functions eqsInt, eqsBool, eqsList, etc. The basic idea is to use the \nunify expression to unify integers and head constructors and to construct explicit CCore functions to \ntraverse and unify subterms. For recursive types, the traversal functions will also be recursive. The \nfunctions for param\u00adeterized types are higher-order; they get additional arguments rep\u00adresenting the \nfunctions corresponding to the type parameters. Some examples: eqsInt e1 e2 = if (unify e1 e2) Success \nSuccess eqsList eqsA e1 e2 = letrec eqs = .x1..x2. if (unify x1 x2) Success (caser x1 of Nil => Success \n| Cons(h1,t1) => r case x2 of Cons(h2,t2) => pand(eqsA h1 h2, eqs t1 t2)) in eqs e1 e2 eqsIntList = \neqsList eqsInt eqsIntListList = eqsList eqsIntList ... A Curry-like front end could derive all the equality \nfunctions auto\u00admatically from the algebraic type de.nitions, and propagate them dynamically using dictionary \npassing. This version of equality differs from Curry s in that it is non-strict, i.e., a logic variable \nalways uni.es successfully against any other value (even .). Curry s =:= only returns Success if both \nargu\u00adments reach (uni.able) NFs, and it also performs an occurs check. We prefer the semantics of our \noperator, which seems more consis\u00adtent with ordinary lazy functional programming in that it permits de.nition \nof in.nite (circular) data structures via un.cation. There is another, more subtle, dif.culty with Curry \ns operator: to imple\u00adment it in an unbundled way seems to require an expression form that detects when \ntwo values are both uninstantiated logic variables. Including this in CCore would violate the Determinism \nProperty. Normalization. Similarly, to force evaluation of an expression to full CNF, we can use (another) \ntype-indexed family of CCore func\u00adtions. These functions make essential use of the seq operator and the \nchnf primitive. Examples: normInt e = chnf(e) normList normA e = letrec norm = r .x.case xof Nil => x \n| Cons(h,t) => Cons(seq (normA h), seq (norm t)) in norm e normIntList = normList normInt normIntListList \n= normList normIntList Note that the arguments to constructors (e.g., Cons) are evaluated in parallel, \nwhich may be essential to reach a normal form.  3 Translation Phases Although it would be possible \nto interpret CCore directly, it would require fairly heavy machinery within the interpreter to handle \nlazi\u00adness and multithreading. So instead, we pass it through two prelim\u00adinary translations, .rst to a \ncall-by-value language SCore, and then into an A-normal-form variant language ACore. ACore expressions \ncan be evaluated by an interpreter written in continuation-passing style, which makes multi-threading \nstraightforward. 3.1 Making Thunks Explicit We implement call-by-need by converting CCore programs into \na call-by-value language SCore that has explicit support for creating and resolving thunks. The syntax \nof Score is just a small extension of CCore: structure Score = datatype exp = ... ...just like CCore... \n| Delay of exp | Force of exp end But Score (implicitly) uses eager evaluation semantics. The use of \nthunks and the CCore to Score conversion are largely stan\u00addard [17], so we omit the details here. In \nprinciple, an advantage of using explicit delay and force is that we can express the results of various \npossible optimizations based on analysis of CCore programs. For example, we could use strict\u00adness analysis \nto remove delays (and the corresponding forces) on arguments that are guaranteed to be evaluated. We \ncan also re\u00admove provably redundant forces, omit delays for cheap expres\u00adsions (i.e., expressions whose \nevaluation guaranteed to terminate quickly) [5], etc. We have not implemented any such optimiza\u00adtions. \nSCore could also be used directly as the desugaring target of a call-by-value source language. 3.2 A-normal \nForm Acore is a variant of A-normal form [6] adapted to our language. It is essentially a subset of Score, \nstrati.ed into several kinds of ex\u00adpressions to support a continuation-passing-style interpreter.A class \nof trivial expressions that require no evaluation is explicitly called out as a separate syntactic class \n(vexp). All arguments to App, Primapp, Capp, Case etc., must be vexps; hence any non-trivial arguments \nin Score programs must be named. Order of evaluation is made explicit via Let and Letpar (parallel) bindings. \nOnly a restricted set of expressions (sexp) can be bound in Let expres\u00adsions, and only vexp s can be \nbound in Letrec expressions, which guarantees that the right-hand sides of recursive bindings can be \nevaluated without requiring the values of any left-hand sides. Acore also has two new expression forms. \nLetpar performs a set of two or more bindings in parallel; it is used to evaluate the argu\u00adments to Primapp, \nCapp, and Unify expressions. (Since parallel evaluation can have substantial overhead, we use sequential \neval\u00aduation instead when a simple analysis shows that at most one of the expressions instantiates a logic \nvariable.) CheckInstantiated blocks evaluation if its argument is an uninstantiated logic variable; it \nis used to ensure that the arguments to Primapps are in CHNF. Constructors SofV and EofS serve to inject \nvexps into sexps and sexps into exps, respectively. datatype vexp = Var of var | Int of int | Abs of \nvar * exp | Delay of exp and sexp = SofV of vexp | App of vexp * vexp | Primapp of prim * vexp list | \nCapp of dcname * vexp list | Unify of vexp * vexp | Force of vexp | CheckInstantiated of vexp | Free \nand exp = EofS of sexp | Case of flexibility * vexp * (pattern * exp) list | Let of var * sexp * exp \n| Letrec of (var * vexp) list * exp | Letpar of (var * exp) list * exp Figure 3. Acore Abstract Syntax \nThe translation of Score into Acore is also fairly standard [6], so again we omit the details. Since \nAcore.Case expressions can only appear in tail-position within functions, it is often necessary for the \ntranslation to create new functions representing join points follow\u00ading a case.  4 De.nitional Interpreter \nThe interpreter is fairly complex; to ease understanding, we present it in several stages. Section 4.1 \nshows the full code for interpreting just the functional substrate of ACore. Section 4.2 describes how \nlogic variables, narrowing, and or-threads can be added. Sec\u00adtion 4.3 adds support for concurrent and-threads \nand residuation. The code relies on several ADTs, whose signatures are given in Figure 4. 4.1 Functional \nSubstrate Figure 5 shows the interpreter code for the basic functional sub\u00adstrate. (A few standard features \nsuch as letrec are omitted to save space.) This is not novel; it is essentially a CaEK machine [6], with \nsupport added for thunks, and with explicit attention to heaps. The basic interpreter function, interp, \nevaluates a top-level expression exp. If the evaluation is successful, the result value is stored into \nthe global list answers. The real work of interpretation is done by functions evalE and evalS, which \nevaluate exps and sexps (re\u00adspectively) to HNF values. These functions are written in a form of continuation-passing \nstyle; they are explicitly parameterized by an environment (env), which is a mapping from program variables \nto values, and by a stack of continuation frames (kont), which tells what to do with the result value. \nEvaluation is also implicitly pa\u00adrameterized by the heap, which is a mutable map from heap point\u00aders \n(hptr)to values. The heap supports functions new, alloc, fetch, and update, with the obvious semantics, \nand a further oper\u00adator, fork, which is described in Section 4.2. Only mutable values (e.g., thunks) \nare allocated in the explicit heap; immutable values (e.g., records) live in the implicit (ML) heap. \nsignature E = sig type a env val empty : a env val extend : a env * (string * a) list -> a env val lookup \n: a env * string -> a end signature H = sig type a hptr type a heap val new : unit -> a heap val alloc \n: a heap -> a -> a hptr val fetch : a heap -> a hptr -> a val update : a heap -> a hptr * a -> unit val \nfork : a heap -> a heap end signature Q = sig type a q val empty : a q val enqueueRear : a q * a -> a \nq val enqueueFront : a q * a -> a q val dequeue : a q -> ( a * a q) option val dequeueIf : ( a -> bool) \n-> a q -> ( a * a q) option end Figure 4. Signatures for Environment, Heap, and Queue ADTs. The value \ntype is fundamental. The .rst three value constructors correspond roughly to CHNF s of CCore expressions; \nin particu\u00adlar, VClos represents a .-abstraction as a closure which includes the (entire) lexical environment. \nThe remaining value constructors have to do with the mutable heap. A thunk for expression e in environment \nenv is created by allocating a new heap entry contain\u00ading the value VThunk(env,e), say at heap pointer \np, and returning VDelay p as the value of the delay expression. VEmpty is used to black-hole thunks during \nforcing [18, 27]; this converts some non-terminating programs into failing ones, removes some possible \nspace leaks, and prevents certain synchronization problems in the presence of and-parallelism (see Section \n4.3). The continuation parameter to the evaluation functions tells them where to deliver the evaluation \nresult. Using continuations allows us to make these functions completely tail-recursive, so calls to \nthem are essentially just jumps, and no implicit control stack is needed. (This will prove to be valuable \nwhen we introduce mul\u00adtiple threads in the next section.)  4.2 Logic Variables and Narrowing To add \nsupport for narrowing and uni.cation to the interpreter, we introduce several new features: a value form \nrepresenting logic variables; a queue of othreads corresponding to narrowing alter\u00adnatives; and a mechanism \nfor representing variant versions of the heap, one for each othread. The added and altered code is shown \nin Figure 6. A free expression evaluates to VLogic p, where p is the heap pointer of a newly allocated \nentry in the (current) heap. To instanti\u00adate this logic variable, the interpreter updates the heap at \np. Narrow\u00ad ing (casef ) always causes instantiation to a CHNF, but uni.cation datatype value = and evalE \n(env:env,kont:kont,e:exp) : unit = VRecord of dcname * value list case e of | VClos of env * var * exp \nEofS s => evalS (env,kont,s) | VInt of int | Case(RIGID,v,pes) => | VDelay of hptr let val VRecord(c,ws) \n= evalV env v | VThunk of env * exp in case matchPattern pes c of | VEmpty SOME(xs ,e ) => withtype evalE \n(E.extend (env,zip (xs ,ws)), env = value E.env kont, e ) hptr = value H.hptr | NONE => fail() end data \nkontframe = | Let(x,s,e) => KBind of var * env * exp evalS (env,KBind(x,env,e)::kont,s) | KUpdate of \nhptr | Letrec(xvs,e) => ... type kont = kontframe list and evalS (env:env,kont:kont,s:sexp) : unit = \nval answers : value list ref = ref [] let val return = continue kont incase s of fun noteAnswer (w:value) \n: unit = SofV v => return (evalV env v) answers := w::(!answers) | App(vop,v) => fun fail () : unit = \n() let val VClos(env ,x ,e ) = evalV env vop val w = evalV(env,v) val heap : value H.heap ref = ref H.empty \nval env = E.extend (env ,[(x ,w)]) fun fetch h = H.fetch (!heap) h in evalE (env ,kont,e ) fun update \n(h,a) = H.update (!heap) (h,a) end fun alloc a = H.alloc (!heap) a | Force v => (case evalV env v of \nfun evalV (env:env) (v:vexp) : value = VDelay h => case v of (case fetch h of Var x => E.lookup (env,x) \nVThunk(env ,e ) => | Int i => VInt i (update (h,VEmpty); (* set BH *) | Abs(x,e) => VClos(env,x,e) evalE \n(env , KUpdate h::kont,e )) | Delay e => VDelay(alloc (VThunk(env,e))) | VEmpty => enterBH (env,kont,EofS \ns) | w => return w) fun matchPattern (pes:(pattern * exp) list) | w => return w) (c0:dcname) | Primapp(p,vs) \n=> : (var list * exp) option = ... return (doPrimapp (p,map (evalV env) vs)) | Capp (_,c,vs) => fun doPrimapp \n(p:prim,ws:value list) : value = ... return (VRecord(c,map (evalV env) vs)) end fun enterBH _ : unit \n= fail () and continue (kont:kont) (w:value) : unit = fun interp (e:exp) : unit = case kont of (heap \n:= H.new (); [] => noteAnswer w answers := []; | KBind(x,env ,e )::kont => evalE (E.empty,[],e)) evalE \n(E.extend (env ,[(x,w)]), kont ,e ) | KUpdate h ::kont => (update (h ,w); continue kont w) Figure 5. \nInterpreting Functional Subset of ACore. may cause instantiation to another VLogic value; auxiliary function \nchaseLogic (not shown) chases down chains of uni.ed variables until a CHNF or uninstantiated logic variable \nis found. Variant Heaps. Each narrowing variant requires an independent heap to store values that depend \non the narrowing choice. The key idea behind our narrowing implementation is that different threads see \ndifferent instantiations for a narrowed logic variable, even though they all use the same heap pointer \nto refer to that variable. Similarly, different threads may see different values for updated thunks, \nsince the value of the thunk may depend on a narrowed logic variable. Because logic variables are .rst-class \nentities that can be used interchangeably with ordinary values, it is not possible (in general) to determine \nstatically which thunks depend on variables in this way, so the interpreter assumes that every thunk \nmight. To support this per-thread state, the interpreter implements variant heaps, which differ from \neach other on some, but usually not many, entries. A new variant heap is created by applying the fork \nopera\u00adtion to an existing heap. (fork h) produces an independent copy of h that can subsequently be changed \n(by update or alloc op\u00aderations) without affecting h; its action on heaps is similar to the action of \nUnix fork on address spaces. Within the interpreter, all heap operations are interpreted with respect \nto the current variant stored in the global reference heap. Ef.cient implementation of fork is discussed \nin Section 5.1. datatype value = ... fun interp (e:exp) : unit = | VLogic of hptr (othreads := Q.empty; \nremainingSlice := initialSlice; fun mkLogic _ = VLogic (alloc VEmpty) ...) fun chaseLogic (w:value) \n: value = ... and evalE (env:env,kont:kont,e:exp) : unit = case e of val initialSlice : int = ... ... \nval remainingSlice : int ref = ref initialSlice | Case(flx,v,pes) => (case chaseLogic (evalV (env,v)) \nof type computation = env * kont * exp VRecord(c,ws) => ... type othread = value H.heap * computation \n| VLogic h0 => (case flx of val othreads : othread Q.q ref = ref Q.empty FLEXIBLE => (app (narrow (!heap,env,kont,h0)) \nfun onext () : unit = pes; case (Q.dequeue (!othreads)) of onext ()) SOME((heap ,comp ),rest) => | RIGID \n=> fail ())) (othreads := rest; remainingSlice := initialSlice; and evalS (env:env,kont:kont,s:sexp) \n: unit = heap := heap ; (remainingSlice := !remainingSlice -1; evalE comp ) if !remainingSlice = 0 then \n| NONE => () (* done! *) oyield (env,kont,EofS s) else fun ospawn (comp:computation) : unit = let val \nreturn = ... othreads := Q.enqueueFront (!othreads, in case e of (!heap,comp)) ... | Free => return (mkLogic \n()) fun oyield (comp:computation): unit = | Unify(v1,v2) => (othreads := Q.enqueueRear (!othreads, let \nval w1 = chaseLogic(evalV(env,v1)) (!heap,comp)); val w2 = chaseLogic(evalV(env,v2)) onext ()) in case \n(w1,w2) of (VLogic h1, _) => fun fail () : unit = onext () (update (h1,w2); return trueV) fun noteAnswer \n(w:value) : unit = | (_, VLogic h2) => (answers := w::(!answers); onext ()) (update (h2,w1); return trueV) \n| (VInt i1, VInt i2) => fun narrow (heap0:heap,env:env,kont:kont,h0:hptr) if i1 = i2 then return falseV \n((c,xs):pattern,e:exp) : unit = else fail() (heap := H.fork heap0; | (VRecord(c1,_),VRecord(c2,_)) => \nlet val ws = map mkLogic xs if c1 = c2 then return falseV val env = E.extend (env,zip (xs,ws)) else fail \n() in update (h0,VRecord(c,ws)); end ospawn (env ,kont,e) end) end) Figure 6. Interpreter changes for \nlogic features. New and altered de.nitions are marked in the margin. Or-thread queue. The execution state \nof the interpreter includes uation1 and a polling mechanism whereby a thread calls oyield a double-ended \nqueue othreads of or-threads that are waiting when its time slice is exhausted. Othreads thus behave \nessentially to execute. Each othread is represented by a pair containing a like engines [15]. The Determinism \nProperty requires that pro\u00adcomputation (environment, continuation, and expression) and a gram behavior \nis completely independent of the choice of value for heap in which the computation should be run. Evaluation \nof a initialSlice (except possibly for the order in which answers are top-level expression begins with \nan empty queue. New othreads produced). In general, performance will be enhanced by choosing generated \nby narrowing are enqueued at the front of the queue a very large value for initialSlice, so that most \ncomputations (ospawn). When an othread completes (by delivering an answer complete before they exhaust \ntheir .rst slice. This minimizes the or failing) or yields (oyield), the next othread to execute is taken \ncost of switching between computations; more importantly, it min\u00adfrom the front of the queue (onext). \nExecution terminates when no imizes the amount of live data held onto by computations pending othreads \nremain on the queue. on the queue. Choosing a large value for initialSlice essentially means that the \ninterpreter will perform depth-.rst exploration of the To implement fair search, we arrange to bound \nthe amount of com-narrowing options, except when it hits a very lengthy computation. putation any othread \ncan do at any one time. This bound is imple\u00admented using a time-slice counter decremented on each sexp \neval-1It would actually suf.ce to decrement and check the counter only for App and Force expressions, \nbecause any non-terminating loop must involve one of these expressions. Narrowing. Narrowing itself is \nimplemented by extending the evaluation code for casef expressions to cover the possibility that the \nscrutinee is a logic variable. Auxiliary function narrow is in\u00advoked for each case arm: it spawns a new \nothread with a forked copy of the current heap in which the scrutinized logic variable has been appropriately \nupdated.  4.3 Concurrency and Residuation We next describe how to support ACore s concurrency and residu\u00adation \nfeatures: The expression (letpar {xi = ei} in e) speci.es that the expressions ei should be evaluated \nin parallel, sharing the same heap. The resulting values are bound to the xi; all the values must be \nproduced before evaluation of e can proceed. r Evaluation of a caseor checkInstantiated expression residuates \nif the scrutinee is an uninstantiated logic variable (i.e., the scrutinee must evaluate to CHNF, not \nmerely to HNF). The residuated computation blocks until the logic vari\u00adable is instantiated by another \nparallel computation. If all par\u00adallel evaluations are blocked, the entire computation fails by .oundering. \n The implementation of enterBH is changed to cause resid\u00aduation rather than failure. The use of black-holing \nprevents two or more computations from attempting to update the same thunk simultaneously.  Figure 7 \nshows the added and altered interpreter code. Concur\u00adrency is implemented straightforwardly using a round-robin \nqueue of and-threads (athreads), each corresponding to an expres\u00adsion being evaluated in parallel. Each \nathread consists of a computation and an integer update count (explained below). The result of each athread \nis stored in the heap, where it can be ac\u00adcessed by the common continuation of the athreads. An othread \nis rede.ned to be a set of athreads, all sharing a common heap. A group of parallel athreads is spawned \nwhen a letpar expres\u00adsion is evaluated. Evaluation begins by creating a synchroniza\u00adtion counter hc in \nthe heap to track the number of uncompleted athreads, and a list of heap locations (all initialized to \nVEmpty) in which the athread results are to be stored. An athread is then spawned for each parallel expression. \nEach athread is given a KUpdate continuation frame that stores its answer in the heap. It then encounters \na special KSynch frame, which implements a bar\u00adrier synchronization with the other athreads in its spawn \ngroup; the last athread to complete continues by executing the body of the letpar in a suitably extended \nenvironment. An athread that has residuated onto the queue (by calling ayield) should be restarted when \nthe variable on which it is waiting has been instantiated. As a simple approximation to this, we ar\u00adrange \nthat the interpreter restarts an athread whenever any vari\u00adable update has been performed since it residuated. \nThe global updateCount tracks the number of updates so far, and the current value of this counter is \nrecorded in the integer .eld of each athread when it yields. The anext routine selects the .rst runnable, \ni.e., po\u00adtentially unblocked, athread from the queue. ((Q.dequeueIf p q) returns the .rst element a of \nqueue q for which p(a) is true.) Note that the routines for managing the othread queue are rede\u00ad.ned \nto save the entire current set of athreads when an othread is spawned or yields. Because the state of \nathread completion is recorded in the variant heap, behavior of a concurrent group is completely independent \nin each othread after an ospawn.  5 Implementation issues 5.1 Variant Heaps There are several possible \napproaches to implementing the variant Heap ADT. The most naive would be to represent heaps by (ex\u00adtensible) \narrays, and heap pointers by array indices, and implement fork by simply copying the array. However, \nthis approach is likely to be quite space-inef.cient, since much of the heap contents will be unchanged \nfrom variant to variant. A better tactic is to use a kind of copy-on-write. For example, if we represent \nheaps by immutable search trees, and heap pointers by integer keys, updating a vari\u00adant will typically \nrequire copying only O(log n) nodes. However, this approach has a garbage collection problem when implemented \nwithin ML: a heap entry becomes dead only when (all) the heap(s) containing it do. In particular, in \npurely functional programs with no narrowing, nothing ever becomes garbage! We could avoid this problem \nif we were generating compiled code to run under our own garbage collector (rather than ML s), but removing \ndead en\u00adtries from tree structures is still likely to be complicated and time\u00adconsuming. An alternative \napproach, which we currently favor, represents each heap pointer by an ML reference cell, containing \na list of possible values, each tagged with an integer heap number; the list is main\u00adtained in reverse \nsorted order by tag. Each time a heap is created by new or fork, it is assigned a new heap number from \na global counter. A heap itself is represented by a list of heap numbers, whose head is the heap s unique \nnumber and whose tail is the rep\u00adresentation of the heap s parent (hence empty for a heap created by \nnew). To perform (alloc hv), a new reference cell is created and initialized to the singleton list containing \nv tagged with the heap number of h.To do (update h ( p,v)), v is tagged with the heap number of h and \ninserted into the existing list stored in the reference cell for p. To perform (fetch hp), the list stored \nin the reference cell for p is searched for an entry tagged with some heap number that appears in the \nrepresentation of h. If the requested pointer is re\u00adally de.ned for heap h, its value must have been \nset either in h itself or in one of its ancestors, so the search is guaranteed to succeed. To illustrate \nhow this heap representation works, consider interpre\u00adtation of the following program: let x= free in \nlet y=free in f case xof True => x f | False => case y of True => x | False => x Evaluating the let \nbindings causes two heap pointers p0 and p1 to be allocated for x and y, respectively, in the initial \nheap h0, whose heap number list is [0]. The resulting heap state is shown in Figure 8(a). Since x is \nuninstantiated, evaluating (casef x) causes narrowing. Two othreads are created and two correspond\u00ading \nheaps, h1 and h2, are obtained by invoking (fork h0) twice. x is bound to True in h1 and False in h2, \nbringing the system to the state illustrated in Figure 8(b). Evaluation of the .rst othread causes a \nlookup of x in the corresponding heap, h1, whose heap number list is [1,0]. Searching the list pointed \nto by p0 for a value tagged with 1 successfully produces True. Evaluating (casef y) in the second othread \ncauses a lookup of y in heap h2, whose heap number list is [2,0]. Searching the list pointed to by p1 \nfails val updateCount : int ref := ref 0 fun interp (e:exp) : unit = (athreads := Q.empty; fun update \n(h,a) = updateCount := 0; (updateCount := !updateCount + 1; ...) H.update (!heap) (h,a)) and evalE (env:env,kont:kont,e:exp) \n: unit = datatype kontframe = ... case e of | KSynch of hptr * env * exp ... | Case(flx,v,pes) => type \nathread = computation * int (case chaseLogic (evalV (env,v)) of val athreads : athread Q.q ref = ref \nQ.empty ... | VLogic h0 => type othread = heap * athread Q.q (case flx of FLEXIBLE => ... fun onext () \n: unit = | RIGID => ayield (env,kont,e))) case (Q.dequeue (!othreads)) of | Letpar(xes,e) => SOME((heap \n,athreads ),rest) => let val hc = alloc (VInt(length xes)) (...; val (xs,es) = unzip xes athreads := \nathreads ; val hs = map(fn _=>alloc VEmpty) xs anext ()) val env = E.extend (env, | NONE => () (* done! \n*) map (fn (x,h) => (x,VLogic h)) (zip (xs,hs))) fun ospawn (comp:computation) : unit = val kont = KSynch(hc,env \n,e)::kont othreads := fun f (e,h) = Q.enqueueFront (!othreads, aspawn (env,KUpdate h::kont ,e) (!heap,Q.enqueueFront \n(!athreads,(comp,~1)))) in app f (zip (es,hs)); anext () fun oyield (comp:computation): unit = end (othreads \n:= Q.enqueueRear (!othreads, and evalS (env:env,kont:kont,s:sexp) : unit = (!heap,Q.enqueueRear (!athreads,(comp,~1)))); \n... onext ()) case s of ... fun anext () : unit = | CheckInstantiated v => let fun runnable (_,uc) = \n!updateCount > uc let val w = evalV (env,v) in case (Q.dequeueIf runnable (!athreads) of in case chaseLogic \nw of SOME((comp,_),rest) => VLogic _ => ayield (env,kont,EofS s) (athreads := rest; | w => return w evalE \ncomp) end | NONE => fail () end and continue (kont:kont) (w:value) : unit = case kont of fun aspawn (comp:computation) \n: unit = ... athreads := Q.enqueueFront (!athreads,(comp,~1)) | KSynch(hc,env ,e )::kont => let val VInt \nc = fetch hc in ifc =1 then fun ayield (comp:computation) : unit = evalE (env ,kont ,e ) (athreads := \nQ.enqueueRear (!athreads, else (comp,!updateCount)); (update (hc,VInt(c-1)); anext ()) anext ()) end \nfun enterBH (comp:computation) : unit = ayield comp Figure 7. Interpreter changes to support concurrency. \nNew and altered de.nitions are marked in the margin. to .nd a value tagged with 2, but does .nd a value \ntagged with 0, namely VEmpty. In other words, y is still uninstantiated, so a further narrowing step \noccurs. This produces two new othreads and corresponding heaps h3 and h4, forked from h2; y is bound \nto True in h3 and False in h4. The resulting state is illustrated in Figure 8(c). Now evaluating the \nothread corresponding to the left case arm causes of a lookup of x in heap h3, whose heap num\u00adber list \nis [3,2,0]; a value tagged with 2 is found, namely False. Evaluating the right arm in heap h4 is similar. \nIn this implementation, update and fetch take O(m) time in the worst case, where m is the number of distinct \nheaps containing the given pointer. They take only O(1) time in the frequent case when the operation \nis being performed on the most-recently-created heap. Functions alloc and fork are O(1). When implemented \ninside Standard ML, this scheme still has a garbage collection problem: the values associated with a \ngiven hptr becomes garbage as soon as that hptr is no longer live, but (a) case x x p0 0 VEmpty 0 VEmpty \n h0 y p1 (b) 0 x=T x=F case y h1 h2 x p0 y p1  0 VEmpty 2 F 1 T 0 VEmpty (c) 0 x=T x=F y=T y=F \nT h1 3 FF h3 h4  x p0 y p1  0 VEmpty 0 VEmpty Figure 8. Heap states, showing othread tree, heaps, \nand heap pointers. until then, all the values for that hptr stay live, even if the corre\u00adsponding heaps \nare no longer live. Because heaps are closely as\u00adsociated with othreads, the interpreter knows when they \nbecome garbage, so it is possible to implement a form of manual garbage collection within a version of \nML extended with weak pointers. The overheads of this scheme are .erce, but it does permit some examples \nthat would otherwise exhaust memory to run to comple\u00adtion. However, in a compiled implementation where \nwe can control memory management, this heap representation should be straight\u00adforward to garbage collect. \nThe problem of maintaining multiple variant stores has received occasional attention in the past, but \nthere does not appear to have been any systematic treatment of the ef.ciency and garbage collec\u00adtion \nissues. Johnson and Duggan [16] and Morrisett [25] proposed .rst-class stores as an analogue to .rst-class \ncontinuations. Tol\u00admach and Appel s ML debugger [32] implemented them to support roll-back. Most of these \nsystems have a notion of a current store, which can be checkpointed and subsequently restored. A common \nmechanism for doing this is to record all updates in a script that can be undone or redone on demand; \nas Haynes [14] noted, this effec\u00adtively generalizes the trail mechanism used in many backtracking\u00adbased \nsystems. The use of copy-on-write for Unix fork apparently dates back to System V [23].  5.2 Fast Interpretation \nThe de.nitional interpreter as describe in Section 4 is too inef.cient to be practical, but we can apply \nseveral well-known techniques to make it much more ef.cient. DeBruijn indices. Two of the most obvious \ninef.ciencies are per\u00adforming string-based lookups in environments and in pattern lists at runtime. It \nis trivial to .x these problems by changing to DeBruijn indices for variables, and converting constructor \nnames to numeric indices. Trimming and closures Both VClos and VThunk values contain captured environments, \nwhich are used to resolve references to free variables in the abstraction or thunk body. Currently, the \ninterpreter always captures the entire lexical environment at the point of def\u00adinition, even though this \nmay contain many entries in addition to the needed free variables. This policy makes environment capture \ncheap, and probably doesn t have much impact on lookup time but it can cause serious space leaks. Most \nfunctional language imple\u00admentations therefore trim the environment to contain just the free variables \n[27]. If the free variable sets are put into closure records, variable lookup time can be reduced still \nfurther over the DeBruijn model, although there is a considerable cost in building the records.  6 Conclusions \nand Ongoing Work The main goal of this work was to develop a practical FLP imple\u00admentation that provides \nfair search based on multi-threading. We believe this goal has been achieved, although more work needs \nto be done to improve performance of variant heaps. We are also working on an alternative interpreter \n(implemented in Java) which provides fair search, but uses substitution rather than variant heaps; it \nshould prove interesting to compare the performance of these approaches. The design of CCore and ACore \nalso sheds light on several dark corners of FLP semantics, and suggests some areas for improve\u00adment in \nthe de.nition of Curry, currently the most visible FLP lan\u00adguage. We expect to continue work on this \nfront as well; in particu\u00adlar, we would like to formalize the Determinism Property and prove that it \nholds for CCore. One important feature of Curry, which we have not discussed here for lack of space, \nis a facility for evaluating nested sub-programs and capturing their answer sets in a (lazy) list that \ncan be inspected by the enclosing program. A distinctive characteristic of this mech\u00adanism is that sub-programs \nmay read, but not write, heap entries created by the enclosing program. We have built an extended in\u00adterpreter \nthat faithfully implements this behavior, using an array of variant heaps. However, as currently speci.ed \nin Curry, this feature clearly violates the Determinism Property, because the enclosing program s behavior \ncan depend on the order in which answers to the sub-program are computed. We are therefore exploring \nalter\u00adnative mechanisms that provide some of the same power without abandoning determinism. We are also \nin the process of developing a compiler for CCore based on translation into a full continuation-passing \nstyle represen\u00adtation similar to that used in Standard ML of New Jersey [4]. This representation re.nes \nACore by performing closure conversion (re\u00admoving the need for environments) and introducing continuations \n(removing the need for a runtime stack and thus simplifying multi\u00adthreading). Queue and heap services \nare provided as part of the runtime system, using the same algorithms as in the interpreter. We plan \nto build a garbage collector tailored for both lazy evaluation and variant heap traversal. Our goal is \nto reach performance parity with the best current (non-fair) FLP compilers (such as the M\u00a8unster Curry \nCompiler [21]). Acknowledgements We thanks the anonymous referees for several helpful suggestions. 7 \nReferences [1] H. A\u00a8it-Kaci. An overview of LIFE. In J. Schmidt and A. Stogny, editors, Proc. Workshop \non Next Generation In\u00adformation System Technology, pages 42 58. Springer LNCS 504, 1990. [2] E. Albert, \nM. Hanus, F. Huch, J. Oliver, and G. Vidal. Oper\u00adational semantics for functional logic languages. In \nM. Co\u00admini and M. Falaschi, editors, Proc. Int l Workshop on Func\u00adtional and (Constraint) Logic Programming, \nvolume 76 of Electronic Notes in Theoretical Computer Science. Elsevier Science Publishers, 2002. [3] \nS. Antoy, M. Hanus, B. Massey, and F. Steiner. An implemen\u00adtation of narrowing strategies. In PPDP01, \npages 207 217. ACM Press, 2001. [4] A. W. Appel. Compiling with Continuations. Cambridge Uni\u00adversity \nPress, 1992. [5] K.-F. Fax\u00b4en. Analysing, Transforming and Compiling Lazy Functional Programs. PhD thesis, \nDepartment of Teleinfor\u00admatics, Royal Institute of Technology, June 1997. [6] C. Flanagan, A. Sabry, \nB. F. Duba, and M. Felleisen. The essence of compiling with continuations. In PLDI 93, pages 237 247, \n1993. [7] M. Hanus. Improving control of logic programs by using functional logic languages. In Proc. \nof the 4th Int. Symp. on Programming Language Implementation and Logic Program\u00adming, pages 1 23. Springer \nLNCS 631, 1992. [8] M. Hanus. The integration of functions into logic program\u00adming: From theory to practice. \nJournal of Logic Program\u00adming, 19&#38;20:583 628, 1994. [9] M. Hanus. Ef.cient translation of lazy functional \nlogic pro\u00adgrams into Prolog. In Proc. Fifth Int. Workshop on Logic Pro\u00adgram Synthesis and Transformation, \npages 252 266. Springer LNCS 1048, 1995. [10] M. Hanus. A uni.ed computation model for declarative pro\u00adgramming. \nIn Proc. 1997 Joint Conference on Declara\u00adtive Programming (APPIA-GULP-PRODE 97), pages 9 24, 1997. [11] \nM. Hanus. A uni.ed computation model for functional and logic programming. In Proc. POPL 97, 24st ACM \nSymp. on Principles of Programming Languages, pages 80 93, 1997. [12] M. Hanus, S. Antoy, K. H\u00a8oppner, \nJ. Koj, P. Niederau, R. Sadre, and F. Steiner. PAKCS: The Portland Aachen Kiel Curry System. Available \nat http://www.informatik. uni-kiel.de/~pakcs/, 2002. [13] M. Hanus, editor. Curry: An integrated functional \nlogic lan\u00adguage (version 0.8). Available at http://www.informatik. uni-kiel.de/~curry, Apr. 2004. [14] \nC. T. Haynes. Logic Continuations. Journal of Logic Pro\u00adgramming, 4(2):157 176, June 1987. [15] C. T. \nHaynes and D. P. Friedman. Engines build process ab\u00ad stractions. In Proc. 1984 ACM Conference on Lisp \nand Func\u00adtional Programming, pages 18 24, Aug. 1984. [16] G. F. Johnson and D. Duggan. First-class stores \nand partial continuations in a programming language and environment. Computer Languages, 20(1):53 68, \nMar. 1994. [17] T. Johnsson. Compiling lazy functional languages. Ph.D. the\u00adsis, Chalmers University, \n1987. [18] R. E. Jones. Tail recursion without space leaks. Journal of Functional Programming, 2(1):73 \n79, January 1992. [19] J. Lloyd. Programming in an integrated functional and logic language. Journal \nof Functional and Logic Programming, 1999(3):1 49, 1999. [20] F. L\u00b4opez-Fraguas and J. S\u00b4anchez-Hern\u00b4andez. \nTOY: A Mul\u00adtiparadigm Declarative System. In Proc. of RTA 99, pages 244 247. Springer LNCS 1631, 1999. \n[21] W. Lux. The M\u00a8unster Curry compiler. Available at http: //danae.uni-muenster.de/~lux/curry/, 2004. \n[22] J. McCarthy. A basis for a mathematical theory of computa\u00adtions. In P. Braffort and D. Hirschberg, \neditors, Computer Pro\u00adgramming and Formal Systems, pages 33 70. North-Holland, 1963. [23] R. Miller. \nA Demand Paging Virtual Memory Manager for System V. In USENIX Association Conference Proceedings, pages \n178 182, June 1984. [24] R. Milner, M. Tofte, R. Harper, and D. B. MacQueen. The Standard ML Programming \nLanguage (Revised). MIT Press, 1997. [25] J. G. Morrisett. Re.ning .rst-class stores. In Proc. Workshop \non State in Programming Languages, pages 73 87, Copen\u00adhagen, Denmark, June 1993. [26] S. Peyton Jones \nand S. Marlow. Secrets of the Glasgow Haskell Compiler inliner. Journal of Functional Program\u00adming, 12(4):393 \n434, July 2002. [27] P. Sestoft. Deriving a lazy abstract machine. Journal of Func\u00adtional Programming, \n7(3):231 264, May 1997. [28] E. Shapiro, editor. Concurrent Prolog, Collected Papers, Vol\u00adumes 1 and \n2. MIT Press, Cambridge, Massachusetts, 1987. [29] G. Smolka. The Oz programming model. In J. van Leeuwen, \neditor, Computer Science Today: Recent Trends and Develop\u00adments, pages 324 343. Springer LNCS 1000, 1995. \n[30] Z. Somogyi, F. Henderson, and T. Conway. The execution al\u00adgorithm of Mercury, an ef.cient purely \ndeclarative logic pro\u00adgramming language. Journal of Logic Programming, 29(1\u00ad3):17 64, 1996. [31] A. Tolmach \nand S. Antoy. A monadic semantics for core Curry. In G. Vidal, editor, Electronic Notes in Theoretical \nComputer Science, volume 86. Elsevier, 2003. [32] A. P. Tolmach and A. W. Appel. A debugger for Standard \nML. Journal of Functional Programming, 5(2):155 200, April 1995.  \n\t\t\t", "proc_id": "1016850", "abstract": "Recent functional logic languages such as Curry and Toy combine lazy functional programming with logic programming features including logic variables, non-determinism, unification, narrowing, fair search, concurrency, and residuation. In this paper, we show how to extend a conventional interpreter for a lazy functional language to handle these features by adding support for reference cells, process-like and thread-like concurrency mechanisms, and a novel form of multi-versioned store. Our interpretation scheme is practical, and can be easily extended to perform compilation. The language specified by our interpreter is designed so that programs are deterministic in a novel and useful sense.", "authors": [{"name": "Andrew Tolmach", "author_profile_id": "81100247872", "affiliation": "Portland State University, Portland, OR", "person_id": "P18423", "email_address": "", "orcid_id": ""}, {"name": "Sergio Antoy", "author_profile_id": "81100292463", "affiliation": "Portland State University, Portland, OR", "person_id": "PP14108529", "email_address": "", "orcid_id": ""}, {"name": "Marius Nita", "author_profile_id": "81100010495", "affiliation": "Portland State University, Portland, OR", "person_id": "P693062", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1016850.1016865", "year": "2004", "article_id": "1016865", "conference": "ICFP", "title": "Implementing functional logic languages using multiple threads and stores", "url": "http://dl.acm.org/citation.cfm?id=1016865"}