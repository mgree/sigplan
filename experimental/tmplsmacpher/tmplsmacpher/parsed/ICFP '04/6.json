{"article_publication_date": "09-19-2004", "fulltext": "\n Multi-return Function Call Olin Shivers Georgia Tech College of Computing shivers@cc.gatech.edu Abstract \nIt is possible to extend the basic notion of function call to al\u00adlow functions to have multiple return \npoints. This turns out to be a surprisingly useful mechanism. This paper conducts a fairly wide-ranging \ntour of such a feature: a formal semantics for a min\u00adimal .-calculus capturing the mechanism; a motivating \nexample; a static type system; useful transformations; implementation con\u00adcerns and experience with an \nimplementation; and comparison to related mechanisms, such as exceptions, sum-types and explicit continuations. \nWe conclude that multiple-return function call is not only a useful and expressive mechanism, both at \nthe source-code and intermediate-representation level, but is also quite inexpensive to implement. Categories \nand subject descriptors: D.3.3 [Program\u00adming languages]: Language Constructs and Features control structures, \nprocedures, functions, subroutines and recursion; F.3.3 [Logics and meanings of programs]: Studies of \nprogram constructs control primitives and functional constructs; D.1.1 [Programming techniques]: Applicative \n(Functional) Program\u00adming; D.3.1 [Programming languages]: Formal De.nitions and Theory semantics and \nsyntax General terms: Design, Languages, Performance, Theory Keywords: Functional programming, procedure \ncall, control structures, lambda calculus, compilers, programming languages, continuations 1 Introduction \nThe purpose of this paper is to explore a particular programming\u00adlanguage mechanism: adding the ability \nto specify multiple return points when calling a function. Let s begin by introducing this fea\u00adture in \na minimalist, essential core language, which we will call the multi-return .-calculus (MRLC). The MRLC \nlooks just like the standard .-calculus [2], with the addition of a single form: l . Lam ::= .x.e e . \nExp ::= x | n | l | e1 e2 | <er1...rm. | (e) r . RP ::= l | #i An expression is either a variable reference \n(x), a numeral (n), a .\u00adexpression (l, of the form .x.e), an application (e1 e2), or our new Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. ICFP 04, September \n19 21, 2004, Snowbird, Utah, USA. Copyright 2004 ACM 1-58113-905-5/04/0009 ...$5.00 David Fisher Georgia \nTech College of Computing d.sher@cc.gatech.edu addition, a multi-return form , which we write as <er1 \n...rm..1 Additionally, our expression syntax allows for parenthesisation to disambiguate the concrete \nsyntax. From here on out, however, we ll ignore parentheses, and speak entirely of the implied, unambiguous \nabstract syntax. We ll develop formal semantics for the MRLC in a following sec\u00adtion, but let s .rst \nde.ne the language informally. An expression is always evaluated in a context of a number of waiting \nreturn points (or ret-pts ). Return points are established with the ri elements of multi-return forms, \nand are speci.ed in our grammar by the RP pro\u00adductions: they are either . expressions, or elements of \nthe form #i for positive numerals i, e.g., #1 , #2 , etc. Here are the rules for evaluating the various \nkinds of expressions in the MRLC: x, n, .x.e Evaluating a variable reference, a numeral, or a .-expression \nsimply returns the variable s, numeral s, or . s value to the context s .rst return point, respectively. \ne1 e2 Evaluating an application .rst causes the function form e1 to be evaluated to produce a function \nvalue. Then, in a call-by\u00adname semantics, we pass the expression e2 off to the function. In a call-by-value \nsemantics, we instead evaluate e2 to a value, which we then pass off to the function. In either case, \nthe application of the function to the argument is performed in the context of the entire form s return \npoints. Note that the evaluation of e1 and, in call-by-value, e2 do not happen in the outer return-point \ncontext. These inner evalu\u00adations happen in distinct, single return-point contexts. So, if we evaluate \nthe expression (f 6)(g 3) in a context with .ve return points, then the f 6 and the g 3 ap\u00adplications \nthemselves are conducted in single ret-pt contexts. The application of f s return value to g s return \nvalue, how\u00adever, happens in the outer, .ve ret-pt context. <er1 ...rm. The multi-return form is how we \nestablish contexts with mul\u00adtiple return points. Evaluating such a form evaluates the inner expression \ne in a return-point context with m ret-pts, given by the ri. If e eventually returns a value v to a return \npoint of the form .x.e', then we bind x to value v, and evaluate expression e' in the original form s \nouter ret-pt context. If, however, e returns v to a ret-pt of the form #i, then v is, instead, passed \nstraight back to the ith ret-pt of the outer context. 1Strictly speaking, the addition of numerals means \nour language isn t as primitive as it could be, but we ll allow these so that we ll have something a \nlittle simpler than . expressions to use for arbi\u00adtrary constants in our concrete examples. Consider, \nfor example, evaluating the expression <(f 6)(.x .x+5)(.y .y*y) where we have slightly sugared the syntax \nwith the introduction of in.x notation for standard arithmetic operators. The function f is called with \ntwo return points. Should f return an integer j to the .rst, then the entire form will, in turn, return \nj +5 to its .rst ret-pt. But if f returns to its second ret-pt, then the square of j will be returned \nto the whole expression s .rst ret-pt. On the other hand, consider the expression <(f 6)(.x .x+5)#7 Should \nf return j to its .rst ret-pt, all will be as before: j +5 will be returned to the entire form s .rst \nret-pt. But should f return to its second ret-pt, the returned value will be passed on to the entire \nform s seventh ret-pt. Thus, #i notation gives a kind of tail-call mechanism to the language. One .nal \nquestion may remain: with the <er1 ...rm multi-ret form, we have a notation for introducing multiple \nreturn points. Don t we need a primitive form for selecting and invoking a cho\u00adsen return point? The \nanswer is that we already have the necessary machinery on hand. For example, if we wish to write an expression \nthat returns 42 to its third ret-pt, we simply write <42 #3 which means evaluate the expression 42 in \na ret-pt context with a single return point, that being the third return point of the outer context. \nThe ability of the #i notation to select return points is suf.cient. 2 An example To get a better understanding \nof the multi-return mechanism, let s work out an extended example that will also serve to demonstrate \nits utility. Consider the common list utility filter: (a.bool) . a list . a list which .lters a list \nwith a given element\u00adpredicate. Here is ML code for this simple function: fun filter f lis = let fun \nrecur nil = nil | recur (x::xs) = if f x then x :: (recur xs) else recur xs in recur lis end Now the \nchallenge: let us rewrite filter to be parsimonious, that is, to allocate as few new list cells as possible \nin the construction of the answer list by sharing as many cells as possible between the input list and \nthe result. In other words, we want to share the longest possible tail between input and output. We can \ndo this by changing the inner recursion so that it takes two return points. Our function\u00adcall protocol \nwill be: Ret-pt #1: unit output list =input list The call returns the unit value to its .rst return \npoint if every element of the input list satis.es the test f. Ret-pt #2: a list output list is shorter \nthan input list If some element of the input list does not satisfy the test f, the .ltered result is \nreturned to the second return point. fun filter f lis = let fun recur nil = () | recur (x::xs) = iff \nx then multi (recur xs) #1 (fn ans => multi (x::ans) #2) else multi (recur xs) (fn () => multi xs #2) \n#2 in multi (recur lis) fn() => lis #1 end Figure 1: The parsimonious .lter function, written with a \nmulti\u00adreturn recursion. We recommend that you stop at this point and write the function, given the recurrence \nspeci.cation above; it is an illuminating ex\u00adercise. We ll embed the multi-return form <er1 ...rm into \nML with the concrete syntax multi er1...rm. The result function is shown in .gure 1. Note the interesting \nproperty of this function: both recursive calls are semi-tail recursive, in the sense that one return \npoint requires a stack frame to be pushed, while the other is just a pre-existing pointer to someplace \nhigher in the call stack. However, the two calls differ in which ret-pt is which. In the .rst recursion, \nthe .rst ret-pt is tail-recursive, and the second ret-pt re\u00adquires a new stack frame. In the second, \nit is the other way around. Suppose we were using our parsimonious filter function to .lter even numbers \nfrom a list. What would the call/return pattern be for a million-element list of even numbers? The recursion \nwould perform a million-and-one calls. . . but only a single return! Every call would pass along the \nsame pointer to the base of the call stack as ret-pt one; the recur nil base case would return through \nthis pointer, jumping over all intermediate frames straight back to the stack base. Similarly, .ltering \neven numbers from a list containing only odd elements would also perform n calls and a single return, \ndriven by the tail-recursion through the second recursive call s second return point. Filtering mixed \nlists gives us the desired minimal-allocation prop\u00aderty we sought; contiguous stretches of elements not \nin the list are returned over in a single return. This is possible because multi\u00adple return points allow \nus to distribute code after the call over a conditional test contained inside the call. This combines \nwith the tail-recursive properties of the #i notation to give us the code im\u00adprovement. There s an alternate \nversion of this function that uses three re\u00adturn points, with the following protocol: return unit to \nret-pt #1 if output =input; return a list to ret-pt #2 if the output is a proper tail of the input; and \nreturn a list to ret-pt #3 if the output is neither. We leave this variant as an (entertaining) exercise \nfor the reader. 3 Formal semantics Having gained a reasonably intuitive feeling for the multi-return \nmechanism, it is fairly straightforward to return now to the mini\u00admalist MRLC and develop a formal semantics \nfor it. We can de.ne a small-step operational semantics as a binary relation . on Exp. We ll .rst designate \nintegers and .-expressions as values in our semantics: v . Val = Z + Lam. Then our core set of transition \nrules are de.ned as follows: [funapp] (.x.e) e2 . [x \u00ad. e2]e [rpsel] <vr1 r2 ... . <vr1 [retlam][ret1] \n<vl . lv <v #1 . v [rettail] 1 = i = m <<v #ir1 ...rm . <vri to which we add standard progress rules \nto allow reduction in any term context ' ' e1 . e e2 . e [funprog] 1 [argprog] 2 ' ' e1 e2 . e1 e2 e1 \ne2 . e1 e 2 ' e . e [retprog] ' <er1 ...rm . <er1 ...rm ' e . e [bodyprog] ' .x.e . .x.e l . l ' [rpprog] \n<er1 ...l ...rm . <er1 ...l ' ...rm funapp The funapp schema is the usual function application \u00df rule \nthat actually applies a . term to the argument. rpsel The rpsel schema describes how a value being returned \nse\u00adlects the context s .rst return point. retlam The retlam schema describes how a value is returned \nto a . return point the . expression is simply applied to the returned value. rettail The rettail schema \ndescribes how a value is returned tail\u00adrecursively through a #i return point. We simply select the ith \nreturn point from the surrounding context, collapsing the pair of nested multi-return contexts together. \nret1 Note that the rettail schema does not apply to all #i returns only those that appear nested within \nanother surrounding multi-return form providing the selected return point. With only this rule to de.ne \nreturns through #i ret-pts, expressions such as <(.x.e) #1 17 would be stuck. The ret1 rule allows such \nan expression to progress to (.x.e) 17. The necessity of this extra rule seems like a small blemish on \nthe semantics, especially since, as written, it is partially redun\u00addant with rettail on expressions of \nthe form <<v #1 r1 ...rm , whose transition is covered by both rettail and ret1. This mi\u00adnor non-determinancy \nis harmless, but we must either cover it in the details of any con.uence proof or eliminate it by in\u00adtroducing \nsurrounding context into the ret1 rule to restrict it to multi-ret forms appearing within application \nexpressions. Either way, it s a bit of extra work. Nonetheless, it s a necessary, and, we suggest, illuminating \nrule, rather than the sort of structural blemish we d prefer to just sweep under the rug. Part of the \npoint of the MRLC is to provide language-level access to the different continu\u00adations that underly the \nevaluation of the program albeit in a way that still manages to keep these continuations .rmly under \ncontrol. (We ll return to this theme later.) Consid\u00adered from the continuation perspective, evaluation \nof a func\u00adtion call hides an implicit continuation, the one passed to the evaluation of the application \ns function subexpression. In En\u00adglish, this continuation would be rendered as, Collect the .\u00adnal value \nfor this expression; this value must be a function. Then evaluate the application s argument, and pass \nits value to this function, along with the application s continuation. This implicit continuation is \nthe one indexed by the #1 in <(.x.e) #1 17. So the rettail rule indexes continuations given by the return \npoints of a multi-return expression, while ret1 allows us to index the continuation implicit in the recursive \nevaluation of the application s function expression. Note a pleasing anti-symmetry between function call \nand return in this calculus: application is strict in the function (i.e., we need to know where we are \ngoing), while return is strict in the value being passed back (i.e., we need to know what we are returning). \nWe can\u00adnot have a sort of normal-order return semantics allowing general non-value expressions to be \nreturned: the non-determinancy intro\u00adduced would destroy the con.uence of the calculus, giving us an \ninconsistent semantics. To see this, suppose we added a call-by\u00adname return rule of the form <elr2 ...rm \n. le allowing an arbitrary expression e rather than a value v to be re\u00adturned through a multi-return \nform. This would introduce semanti\u00adcally divergent non-determinism, as shown by the use of our new, bogus \nrule and the rettail rule to take the same expression in two very different directions: <<7#2 l1 l2 . \nl1 <7#2 (by bad rule) <<7#2 l1 l2 . <7 l2 (by rettail rule) Restricting the progress rules to just funprog \nand retprog gives us the call-by-name transition relation . n. The normal-order MRLC has some interesting \nand exotic behaviours, but exploring them is beyond the scope of this paper, so we will press on to the \napplicative-order semantics. For call-by-value, we simply restrict the function-application rule to require \nthe argument to be a value: [funappv] (.x.e) v . [x-. v]e To establish the MRLC as a reasonable semantics, \nwe need to en\u00adsure that the transition relations are con.uent. The call-by-value, call-by-name and full \nMRLC transition relations are all con.uent. The proofs are beyond the scope of this paper,2 but they \nare fairly straightforward variants of the standard con.uence proof for the .\u00adcalculus. 4 Types Our \nbasic untyped semantics in place, we can proceed to consider\u00adation of type systems and static safety. \nThe type system we develop is a static, monomorphic system. The key feature of this system is that expressions \nhave, not a single type t, but, rather, a vector of types (t1 ...tn) one for each return point. Further, \nwe allow a small degree of subtyping by allowing holes (written .) in the vector of result types, meaning \nthe expression will never return to the corresponding return point. So, if we extended the MRLC to have \nif/then forms, along with boolean and string values, then, as\u00adsuming that b is a boolean expression, \nif b then <3#2 else < three #4 2As with other omitted proofs, full details will be given in a forthcoming \ntechnical report. would have principal type vector (.,int,.,string), meaning, this expression either \nreturns an integer to its second ret-pt, or a string to its fourth ret-pt; it never returns to any other \nret-pt. For that matter, this expression has any type vector of the form (a,int,\u00df,string), for any types \na and \u00df. We lift this base form of subtyping to MRLC functions with the usual contravariant/covariant \nsubtyping rule on function types. Let us writet to mean a .nite vector of types with holes allowed \nfor some of the elements. More precisely,t is a .nite partial map from the naturals to types, where \nwe writet [i]=. to mean that i is not in the domain oft . Then our domain of types is t . T ::=int \n| t .t . (Notice that . is not a type itself.) Types and type vectors are ordered by the inductively-de.ned \nand subtype relations, respectively: tsup tsubt sub t sup int int tsub .t sub tsup .t sup  We de.net \nsub t sup to hold when .i . Dom(t sub). i . Dom(t sup).t sub[i] t sup[i]. In other words, type vectort \na is consistent with (is a sub-type\u00ad vector of) type vectort b ift a is pointwise consistent witht b. \nWe now have the machinery in place to de.ne a basic type sys\u00ad tem, given by the judgement G f e :t , \nmeaning expression e has type-vectort in type-environment G. Type environments are sim\u00adply .nite partial \nmaps from variables to types. The type-judgment relation is de.ned by the following schemata: G f n : \n(int) x . Dom(G) G f x : (Gx) G[x -t . t]f e : () G f .x.e : t .t ( ) G f e1: t .tG f e2:t2 t2 (t) \n G f e1 e2:tappt tapp G f e :t(e ) t j rj .Lam tec[j]= G f rj : t j .t j(.rj .Lam) t [i] rj =#i G f \n<er1 ...rm : tte tect j t The type system, as we ve de.ned it, is designed for the call-by\u00advalue semantics, \nand is overly restrictive for the call-by-name se\u00admantics. Development of a call-by-name type system \nis beyond the scope of this paper; we simply remark that it requires function types to take a type vector \non the left side of the arrow, as well as the right side. We have established the type-safety of the \ncall-by-value sys\u00adtem by the usual subject-reduction technique. The theorem guaran\u00adtees that a well-typed \nprogram will never attempt to return a value to a non-existent return point, or to one that expects a \nvalue of the wrong type. The proof is completely standard and contains no tech\u00adnical surprises or unusual \n(or even interesting) insights. The type system can be extended to handle parametric polymor\u00adphism with \nno great dif.culty, and algorithm W can be straightfor\u00adwardly adapted to infer types in a Hindley-Milner \nsystem for the MRLC [10]. We have also proved the correctness of this variant of algorithm W. The extension \namounts to using a variant of row polymorphism [18] on the types of the hidden, second-class contin\u00aduation \ntuples. As a .nal remark before leaving types, it s amusing to pause and note that one of the charms \nof this type system is that it provides a type for expressions whose evaluation never terminates: the \nempty 3 type vector (). 5 Transformations Besides the usual .-calculus transformations enabled by the \n\u00df and . rules in their various forms (general, CBN and CBV), the pres\u00adence of multi-return context as \nan explicit syntactic element in the MRLC provides for new useful transformations. For example, the ret-comp \ntransform allows us to collapse a pair of nested multi-ret forms together: ' ' '''' <<er1 ...rn r1 ...r \n=<er1 ...r [ret-comp] mn where ' r ri =# j '' j r = i '' .x.<(ri x)r1 ...r (x fresh) ri . Lam m This \nequivalence shows how tail-calls collapse out an intermediate stack frame. In particular, it illustrates \nhow a term of the form <e eats all surrounding context, freeing the entire pending stack of call frames \nrepresented by surrounding multi-return contexts. Thus a function call that takes no return points and \nso never returns can eagerly free the entire run-time stack. Another useful equivalence is the mirror \ntransform: le =<el [mirror] Note that the mirror transform does not hold for the normal-order semantics \nshifting e from its non-strict role as an application s argument to its strict role in a multi-ret form \ncan change a termi\u00adnating expression into a non-terminating one. Since both positions are strict in the \ncall-by-value semantics, the problem does not arise there. These equivalences are useful to allow tools \nsuch as compilers to manipulate and integrate terms in a .ne-grained manner (as we ll see in the following \nsection). We have established that these trans\u00adforms preserve meaning with respect to the CBV semantics. \nTo brie.y sketch the proof, we show that the transition relation that is the union of each transformation \n(performed at any subterm within a term) and the set of call-by-value transitions is con.uent; in addi\u00adtion, \na single transformation does not make a non-terminating ex\u00adpression terminating (in the CBV-only system), \nor vice-versa. In order to show that the combined semantics are con.uent, we invoke the Hindley-Rosen \nlemma, which states that if two commuting re\u00adlations are con.uent, then their union is con.uent. The \nrest of the proof is done simply by cases. 3Not every such expression can be assigned this type, of course. \n. . 6 Anchor pointing and encoding in the pc Consider compiling the programming-language expression \nx<5 in the two contexts f(x<5) and if x<5 then ...else ... In the .rst context, we want to evaluate the \nexpression, leaving a true/false value in one of the machine s registers. In the second context, we want \nto evaluate the inequality, branching to one or another location in the program based on the outcome \nin other words, rather than encode the boolean result as one of a pair of possible values in a general-purpose \nregister, we wish to encode it as a pair of possible addresses in the program counter. Compiler writers \nrefer to this distinction as eval-for-value and eval-for\u00adcontrol [4]. Not only do programs have these \ntwo ways of consuming booleans, they also have corresponding means of producing them. On many processors, \nthe conditional x<5 will be produced by a conditional-branch instruction thus encoded in the pc while \nthe boolean function call isLeapYear(y) will produce a boolean value in one of the general-purpose machine \nregisters thus en\u00adcoded as a value. Matching up and optimally interconverting between the different kinds \nof boolean producers and consumers is one of the standard tasks of good compilers. In the functional \nworld, the technique for doing so relies on a transformation called anchor pointing, [17, 9] de.ned for \nnested conditional expressions sometimes called if\u00adof-an-if. The transformation is if (if a then b else \nc) if a then d . then (if b then d else e) else e else (if c then d else e) although we usually also \nreplace the expressions d and e with calls to let-bound thunks . .d and . .e to avoid replicating large \nchunks of code (where we write  to suggest a fresh, unreferenced don t\u00adcare variable for the thunk, \nin the style of SML). In the original form, the b and c expressions are evaluated for value; in the trans\u00adformed \nresult, b and c are evaluated for control. In the MRLC, we can get this effect by introducing primitive \ncon\u00adtrol functions. The %if function consumes a boolean, and returns to a pair of unit return points: \n<(%if b) rthen relse . In other words, it is the primitive operator that converts booleans from a value \nen\u00adcoding to a pc encoding. The anchor-pointing transformation trans\u00adlates to this setting: <(%if <(%if \na) . .b . .c ) de . <(%if a) . .<(%if b) de . .<(%if c) de This transform, in fact, is easily derived \nfrom the basic ret-comp and mirror transforms, plus some simple constant-folding on %if applications \nto boolean constants. (You may enjoy working this out for yourself.) We can also de.ne n-way case branches \nwith multi\u00adreturn functions; for an intermediate representation for a language such as SML, we would \nprobably want to provide one such func\u00adtion for each sum-of-products datatype declaration, to case-split \nand disassemble elements of the introduced type. Recall that some boolean functions are primitively implemented \non the processor with instructions that encode the result in the pc integer comparison operations are \nan example. We can express this at the language level by arranging for the primitive de.nitions of these \nfunctions similarly to provide their results encoded in the pc. For example, the exported < function \ncan be de.ned in terms of an underlying primitive %< function that encodes its result in the pc using \nmultiple return points:    < = .xy. <(%< xy) (. .true) (. .false) With similar control-oriented de.nitions \nfor the short-circuiting boolean syntax forms x and y = <(%if x) . .y . .false x or y = <(%if x) . .true \n. .y not = .x. <(%if x) . .false . .true the anchor-pointing transform is capable of optimising the transi\u00adtions \nfrom encoded-as-value to encoded-as-pc. For example, suppose we start out with a conditional expression \nthat uses a short-circuit conjunction: if (0<= i) and (i< n) then e1 else e2 First, we expand the and \ninto its if/then/else form, and rewrite our in.x conditionals into canonical application syntax: if (if \n(<= 0 i) then (< i n) else false) then e1 else e2 (Note the tell-tale if-of-an-if that signals an opportunity \nto shift to evaluation for control.) Next, we translate the if/then/else syntax into its functional multi-return \nequivalent: <(%if <(%if (<= 0 i)) . .(< in) . .false  . .e1 . .e2 and \u00df-reduce the eval-for-control \nversions of the <= and < functions: <(%if <(%if <(%<= 0 i) . .true . .false ) . .<(%<i n) . .true . .false \n. .false ) . .e1 . .e2 Now we have a triply-nested conditional expression. Apply the anchor-pointing \ntransform to the second %if and the %<= condi\u00adtionals. This, plus a bit of constant folding, leads to: \n<(%if <(%<= 0 i) . .<(%< in) . .true . .false  . .false . .e1 . .e2  Now we apply anchor-pointing to \nthe .rst %if and the %<= applica\u00adtion, leading to: <(%<= 0 i) . .<(%if <(%< in) . .true . .false ) . \n.e1 . .e2 . .<(%if false) . .e1 . .e2 Applying anchor-pointing to the .rst arm of the %<= conditional, \nand constant-folding to the second arm gives us: <(%<= 0 i) . .<(%< in) . .<(%if true) . .e1 . .e2 . \n.<(%if false) . .e1 . .e2 . .e2 Some simple constant folding reduces this to the .nal simpli.ed form \nthat expresses exactly the control paths we wanted: <(%<= 0 i) . .<(%<in) . .e1 . .e2 . .e2 Note one \nof the nice effects of handling conditionals this way: we no longer need a special syntactic form in \nour language to handle conditionals; function calls suf.ce. The ability of multi-return func\u00adtion call \nto handle conditional control .ow in a functional manner suggests it would be a useful mechanism to have \nin a low-level in\u00adtermediate representation. CPS representations can also manage this feat, but at the \ncost of signi.cantly more powerful machin\u00adery: they expose continuations as denotable, expressible, .rst-class \nvalues in the language. The multi-return extension is a more con\u00adtrolled, limited linguistic mechanism. \n 7 Compilation issues Compiling a programming language that has the multi-return fea\u00adture is surprisingly \ntrouble-free. Standard techniques work well with only small modi.cations required to exploit some of \nthe op\u00adportunities provided by the new mechanism. 7.1 Stack management Calling subroutines involves managing \nthe stack allocating and deallocating frames. Typically, modern compilers distinguish be\u00adtween tail calls \nand non-tail calls in their management of the stack resource. The presence of multiple return points, \nhowever, intro\u00adduces some new and interesting possibilities: semi-tail calls and even super tail calls. \nIn the multi-return setting, there are three main cases for passing return points to a function call: \nAll ret-pts passed to called function E.g., <( f 5) #1 #3 #2 #1 If a function call simply passes along \nall of its context s return points, in a tail-call setting, then this is simply a straight tail call. \nThe current stack frame can be immediately recycled into f s frame, and thus there is no change in the \nnumber of frames on the stack across the call. Ret-pts are strict subset of caller s ret-pts E.g., <( \nf 5) #6 #4 However, we can have a tail call that drops some of the call\u00ading context s return points. \nIn this case, the caller can drop frames, collapsing the stack back to the highest of the sur\u00adviving \nframes. In this way, a call can be super tail recursive, with the stack actually shrinking across a call. \nThis aggressive resource reclamation does require a small amount of run-time computation: in order to \nshrink-wrap the stack prior to the call, the caller must compute the minimum of the surviving re\u00adturn \npoints, since there s no guaranteed order on their position in the stack. Some ret-pts are . expressions \nIf any return point is a . expression, then we must push stack frames to hold the pending state needed \nwhen these return points are resumed. However, we can still shrink-wrap the stack prior to allocating \nthese return frames, if some of the calling context s return points are also going dead at this call. \nThe ability to mix #i and . return points in a given call means we can have calls that are semi-tail \ncalls both pushing new frames and reclaiming existing ones.       7.2 Procedure-call linkage The \nMRLC makes it clear that multiple return points can be em\u00adployed as a control construct at different \nlevels of granularity, from .ne-grained conditional branching to coarse-grained procedure-call transfers. \nThis is analogous to the use of .-expressions in functional languages, which can be used across a wide \nspectrum of control granularity. Just as with .-expressions, a good compiler should be able to ef.ciently \nsupport uses of the multi-return construct across this entire spectrum. The most challenging case is \nthe least static and largest-grain one: passing multiple return points via a general-purpose procedure-call \nlinkage to a procedure. There are three cases determining the pro\u00adtocol used to pass return points to \nprocedures: 1 ret-pt (1 register + sp) In the normal, non-multi-return case, where we are only pass\u00ading \na single return point to a procedure, we need one register (or stack slot) for the return pc. Since the \npending frame to which we will return is the one just below the called proce\u00addure s current frame, the \nstack pointer does double duty, in\u00addicating both the location of the pending frame as well as the allocation \nfrontier for the current frame.  > 1 ret-pt (2n registers + sp) In general, however, we pass each return \npoint as a frame\u00adpointer/return-pc pair of values, either in registers or stack slots, just as with parameters \n(which should come as no sur\u00adprise to those accustomed to continuation-based compilers, since function-call \ncontinuations are just particular kinds of parameters).  However, if a procedure has more than one return \npoint, we cannot always statically determine which one will be the topmost pending frame on the stack \nwhen the function is executed in fact, this could vary from call to call. So we must separate the r ole \nof the stack pointer from that of the registers that hold the frame pointers of the return points. The \nstack pointer is used for allocation it indicates the frontier between allocated storage and unused, \navailable memory. The return frame pointers are for deallocation they indicate back to where the stack \nwill be popped on a return. Registers used by the function-call protocol for return points can be drawn \nfrom the same pool used for parameters, over\u00ad.owing into stack slots for calls with many return points \nor parameters. Thus a call that took many return points might still be accomplished in the register set, \nif the call did not take many parameters, and vice versa. We might wish to give pa\u00adrameters priority \nover ret-pts when allocating registers in the call protocol on the grounds that (1) only one of the ret-pt \nval\u00adues will be used and (2) invoking a ret-pt is the last thing the procedure will do, so the ret-pt \nwill most likely be referenced later than the parameters. (Neither of these observations is al\u00adways true; \nthey are merely simple and reasonable heuristics. For example, a procedure may access multiple ret-pts \nin order to pass them to a fully or partially tail-recursive call. If the call is only partially tail-recursive, \nthen the procedure may subsequently resume after the call, accessing other parame\u00adters. These issues \ncan be addressed by more globally-aware parameter-and register-management techniques.) 0 ret-pt (0 registers \n+sp) This singular case has a particularly ef.cient implementation: not only can we avoid passing any \nret-pc values, we can also reclaim the entire stack, by resetting sp to point to the original stack base! \nBesides being an interesting curiosity, we can actually use this property, in situations involving the \nspawning of threads, to indicate to the compiler the independence of a spawned thread from the spawning \nthread s stack. We have wished for this feature on multiple occasions when writing systems programs in \nfunctional languages. Note that ret-pt registers, being no different from parameter reg\u00adisters, are available \nfor general-purpose use inside the procedure body. Code that doesn t use multiple return points can use \nthe reg\u00adisters for other needs. Multi-return function call is a pay-as-you-go feature. 7.3 Static analyses \nThere are some interesting static-analysis possibilities that could reveal useful information about resource \nuse in this function-call protocol. For example, it might be possible to do a sort of live/dead analysis \nof return points to increase the aggressiveness of the pre\u00adcall shrink wrapping of stack frames. An analysis \nthat could order return points by their stack location could eliminate the min compu\u00adtation used to shrink-wrap \nthe stack over multiple live return points. We have not, however, done any signi.cant work in this direction. \n 7.4 Callee-saves register management One of the dif.culties with the ef.cient compilation of exceptions \nis the manner in which they con.ict with callee-saves register use. If a procedure P stores a callee-saves \nregister away in the stack frame, an exception raised during execution of a dynamically-nested pro\u00adcedure \ncall cannot throw directly to a handler above P s frame the saved register value would be lost. Either \nthe callee-saves regis\u00adters must be dumped out to the stack for retrieval after the handler\u00adprotected \ncode .nishes, or the control transfer to the exception s handler must instead unwind its way up from \nthe invoking stack frame, restoring saved-away callee-saves registers on the way out. The .rst technique \nraises the cost of establishing a handler scope, while the second raises the cost of invoking an exception. \nIn contrast, it s fairly simple to manage callee-saves registers in the multi-return setting. As with \nany function-call protocol (even the traditional single-return one) supporting constant-stack tail-calls, \nany tail call must restore the callee-saves registers to their entry values before transferring control \nto the called procedure (so tail\u00adcalls have some of the requirements of calls, and some of the re\u00adquirements \nof returns). Multi-return procedure calls allow for a new possibility beyond tail call and non-tail call: \nthe semi-tail call, which pushes frames and passes along existing return points, e.g., <(f 5)(.x.e)#1 \n.. We must treat this case with the tail-call restriction by restoring all callee-saves registers to \ntheir entry values prior to transferring control to f in order to keep from stranding callee-saves values \nin a skipped frame should f return through its second return point. So, in short, the simple tail-call \nrule for managing callee-saves reg\u00adisters applies with no trouble in the multi-return case. Note, how\u00adever, \nthat this rule does have a cost in our new, semi-tail call setting: the presence of the #1 in the example \nabove means we can t use callee-saves registers to pass values between the (f 5)call point and the .x.e \nreturn point.  8 Actual use The multiple-return mechanism is useful for many more programs than the \nsingle .lter function we described in section 2. Other ex\u00adamples would be: compiler tree traversals \nthat might or might not alter the code tree;  algorithms that insert and delete elements in an ordered-tree \nset;  search algorithms usually expressed with explicit success and failure continuations these can \nbe expressed more suc\u00adcinctly, and run on the stack, without needing to heap-allocate continuations. \n Scheme programmers frequently write functions that take multiple continuations as explicit functional \nparameters, accepting the awk\u00adward notational burden and run-time overhead of heap-allocated continuations \n(which are almost always used in a stack-like man\u00adner). This longstanding practice also gives some indication \nof the utility of multiple return points. We ve found that once we d added the mechanism to our mental \nalgorithm-design toolkit, opportunities to use it tend to pop up with surprising frequency. As an example, \nwe are currently in the midst of implementing a standard Scheme library for sorting [16]. This library \ncontains a function for deleting adjacent identical ele\u00adments in a linked list which exactly .ts the \npattern we exploited in the parsimonious .lter example. Since Scheme does not have multi-return function \ncalls, our implementation of this function is more complex and less ef.cient than it needs to be. Shao, \nReppy and Appel have shown [15] how to use multiple con\u00adtinuations to unroll recursions and loops in \na manner that allows functions to pack lists into larger allocation blocks4. The cost of explicit continuations \nrendered this impractical when conditional control information must be distributed past multiple continua\u00adtions; \nthe more restricted tool of the MRLC s multiple-return points would make this feasible. When casting \nabout for a larger example to try out in practice, however, one particular use took us by storm: LR parser \ngenera\u00adtors [3]. A parser generator essentially is a compiler that translates a context-free grammar \nto a program for a particular kind of ma\u00adchine, a push-down automaton (PDA), just as a regular-expression \nmatcher compiles regular expressions into a program for a .nite\u00adstate automaton. For our purposes, we \ncan describe a PDA as a machine that has three instructions: shift, goto, and reduce. Now, once we have \nour PDA program, we have two options for executing it. One path is to implement a PDA in the target language \n(say, for example, C), encode the PDA program as a data structure, and then run the PDA machine on the \nprogram. That is, we execute the PDA program with an interpreter. 4It s a curious but ultimately coincidental \nfact that their paper uses the same .lter-function example shown in section 2 for a completely different \npurpose. The other route, of course, is to compile: translate the PDA program down to the target language. \nThe attraction of compiling to the target language is the transitivity of compilation we usually have \na compiler on hand that will then map the target language all the way down to machine language, and so \nwe could run our parser at native-code speeds. Translating PDA programs to standard programming languages, \nhowever, has problems. Let s take each of the three PDA instruc\u00adtions in turn. The shift s instruction \nmeans save the current state on the stack, then transfer to state s. This one is easy to represent, encoding \nstate in the pc: if we represent each parser state with a different procedure, then shift is just function \ncall. The goto s instruction, similarly, is just a tail-recursive function call. How about reduce? The \nreduce n instruction means pop n states off the stack, and transfer control to the nth (last) state thus \npopped. Here is where we run into trouble. Standard programming languages don t provide mechanisms for \ncheaply returning several frames back in the call stack. Worse, the value of n used when re\u00adducing from \na given state can vary, depending upon the value of the next token in the stream. So a particular state \nmight wish to return three frames back if the next token is a right parenthesis, but .ve frames back \nif it is a semicolon. While this is hard to do in Java or SML or other typical program\u00adming languages, \nit can be done in assembler [13]. The problem with a parser generator that produces assembler is that \nit isn t portable, and, worse, has integration problems the semantic actions embed\u00added inside the grammar \nare usually written in a high-level language. For these reasons, standard parsers such as Yacc or Bison \n[8] usu\u00adally go the interpreter route: the grammar is converted to a C table which is interpreted by \na PDA written in C. Multi-return calls solve this problem nicely they give us exactly the extra expressiveness \nwe need to return to multiple places back on the stack. When our compiled PDA program does a shift by \ncalling a procedure, it passes the return points that any reduction from that state forward might need. \nTo gain experience with multi-return procedure calls, we started with a student compiler for Appel s \nTiger language [1], which one of us (Shivers) uses to teach the undergraduate compiler course at Georgia \nTech. Tiger is a fairly clean Pascal-class language. The stu\u00addent compilers are written in SML, produce \nMIPS assembly, and feature a coalescing, graph-coloring register allocator. One grad\u00aduate of the undergraduate \ncompiler course took his compiler and modi.ed it to add a multi-return capability to the language. This \nal\u00adlowed us to completely try out the notion of adding multiple-return points to a language, from issues \nof concrete syntax, through static analysis, translation and execution, giving us a tool for experiments. \nDesigning the syntactic extensions was a trivial exercise, requiring only the addition of the multi-ret \nform itself and modi.cation of the declaration form for procedures. We designed the syntax exten\u00adsions \nwith our pay-as-you-go criteria in mind code that doesn t use multiple return points looks just like \nstandard Tiger code. A second undergraduate modi.ed a LALR parser-generator tool written in Scheme by \nDominique Boucher, adding two Tiger back\u00adends, one compiling the recogniser to multi-return Tiger code, \nand the other producing a standard table&#38;PDA implementation. The only non-obvious part of this task \nis the analysis to determine which return points must be passed to a given state procedure. This is a \nsimple .xed-point computation over the PDA s state machine. (Speci.cally, a state procedure must be passed \nreturn points for any reduction it might perform, plus return points to satisfy the needs of any state \nto which it might, in turn, shift.) Input input size (symbols) non-MR parser MR parser MR parser with \ninlining loop matmul 8queens merge large 18 121 235 409 1,868 78,151 114,987 164,693 219,649 802,008 \n9,336 36,025 70,797 99,743 366,498 8,915 33,386 65,505 89,486 324,459 Table 1. Performance measurement \nfor standard/table-driven and multi-return-based LALR parsers generated from the Tiger grammar. Timings \nare instruction counts, measured on the SPIM Sparc simulator. Input samples are (1) a simple loop, (2) \nmatrix multiply, (3) eight-queens, (4) mergesort, (5) samples 2 4 replicated multiple times. We then \nbuilt two parsers to recognise the Tiger grammar (a rea\u00adsonably complex grammar which we happened to \nhave convenient to hand). The parser keeps pending state information, which drives execution control \ndecisions on the procedure call stack, and uses a separate, auxiliary stack to store the values produced \nand con\u00adsumed by the semantic actions. We were pleased to discover that the return-point requirements \nfor our sample grammars were very limited. Of the 137 states needed to parse the Tiger grammar, 106 needed \nonly one return point; none needed more than two. Reduc\u00adtions in real grammars, it seems, are sparse. \nThe compiled parser, of course, ran signi.cantly faster than the in\u00adterpreted one. The compiled PDA parsed \nour sample input 2.5 3.5 times faster than the interpreted PDA (see table 1). One source of speedup was \nthe fact that when a state is only shifted into from one other state, the Tiger compiler saw it as a \nprocedure only called from one site, and would inline the procedure. This happens quite frequently in \nreal grammars 78% of the Tiger-grammar states can be inlined. Representing the parser directly in a high-level \nlanguage allowed it to be handled by general-purpose optimisations. These simple experiments provide \nonly the most basic level of eval\u00aduation, in the sense that a real, end-to-end implementation has been \nsuccessfully constructed with no serious obstacles cropping up un\u00adforeseen, and that it performs roughly \nas expected. There is still much we could have done that we have not yet done. We did not, for example, \narrange for our parsers to execute semantic actions while parsing they are simply recognisers. This shows \noff the ef.ciency of the actual parsing machinery to best advantage. Our basic intent was simply to exercise \nthe multi-ret mechanism, which function our parsers performed admirably. 9 Variations We ve covered \na fair amount of ground in our rapid tour of the multi-return mechanism, providing views of the feature \nfrom mul\u00adtiple perspectives. But we ve left many possibilities unexplored. We ve pointed out some of \nthese along the way, such as normal\u00adorder semantics or static analyses. 9.1 Return-point syntax One variation \nwe have not discussed is the syntactic restriction of return points to . expressions. This is not a fundamental \nrequire\u00adment. The entire course of work we ve laid out goes through just as easily if we allow return \npoints to be any expression at all (i.e., r . RP ::= e | #i) and change the semantics schema for returning \nvalues in an equally trivial manner: <ver2 ...rm . ev. However, it doesn t seem to add much to the expressiveness \nof the language to allow return points to be true computations themselves (that is, function applications). \nOne can always .-expand a return point of the form e1 e2 to .x.(e1 e2) x. But allowing general expres\u00adsions \nfor return points does introduce issues of strictness and non\u00adtermination into the semantics of return \nthat were not there before, and this, in turn, restricts some of the possible transformations. A third \npossibility borrows from SML s value restriction: restrict return points to be either . expressions or \nvariable references [11]. Variable references are useful ret-pts for real programming, as they give the \nability to name and then use join points in multiple lo\u00adcations. This seems somewhat more succinct than \nthe awkward alternate of binding the join point to a name, and then referring to it with .-expanded return \npoints in the desired locations. Restricting return-point expressions to . expressions and variable references \neliminates code blowup in transformations, since large ret-pt expressions can be let-bound and replaced \nby a name before replication. It eliminates issues of control effect, since both forms of expression \ncan be guaranteed to evaluate in a small, .nite amount of time. For a real programming language, we prefer \nthis syntax best. 9.2 By-name binding In our design, the ith ret-pt of a form is established by making \nit the ith subform ri of the multi-ret expression <er1 ...rm . This is somewhat analogous to passing \narguments to procedures by posi\u00adtion, (instead of by name, as is allowed in Modula-3 or Common Lisp), \ne.g., when we call a print function, we must know that the .rst argument is the output channel, and the \nfollowing argument is the string to be printed. As a design exercise, one might consider a multi-return \nform based on some sort of by-name binding mechanism for return points, rather than the MRLC s positional \ndesign, with its associated nu\u00admeric #i references. This turns out to be trickier and more awk\u00adward than \none might initially suppose. By-name binding introduces the issue of requiring a new and distinct name \nspace for return points. More troubling is the issue of scope and name capture such a design would have \nto require that return-point bindings be dynamically, rather than lexically, scoped, to prevent lexical \ncap\u00adture of a return point by a procedure passed upward. This would be counter-intuitive to programmers \nused to lexically-scoped name binding. Nor would it buy much, we feel. Control is typically a sparer \nspace than data. It may be useful to bind a few return points at a call-point, but one does not typically \nneed simultaneously to bind thousands, or even dozens. There is no shame in positional binding: besides \nits simplicity, it has been serving the needs of programmers as a parameter-passing mechanism in the \nlion s share of the world s programming lan\u00adguages since the inception of the .eld. 10 Comparisons There \nare several linguistic mechanisms that are similar in nature to multi-return function call. Four are \nexceptions, explicit continu\u00adations, sum types and the weak continuations of C--. 10.1 Exceptions Exceptions \nare an alternate way to implement multiple returns. We can, for example, write the filter example using \nthem. This is clear, since exceptions are just a second continuation to the main continuation used to \nevaluate an expression. However, exceptions are, in fact, semantically different from mul\u00adtiple return \npoints. They are a more heavyweight, powerful mecha\u00adnism, which consequently increases their implementation \noverhead and makes them harder to analyze. This is because exceptions are used to implement non-local \ncontrol transfers, something that can\u00adnot be done with multi-ret function calls. For example, consider \nthe expression sin(1/f(x)) If f raises an exception, the program can abort the entire, pending reciprocal-and-then-sine \ncomputation by transferring control to a handler further back in the control chain. Multi-ret function \ncalls, in contrast, do not have this kind of global, dynamic scope. They do not permit non-local control \n.ow if a function is called, it returns. This makes them easier to analyze and permits the kind of transformations \nthat encourage us to use them to represent .ne-grained control transfers such as local conditional branches \nin short, they make for a better wide-spectrum, general\u00adpurpose control representation, as opposed to \na control mechanism tuned for exceptional transfers. The difference between exceptions and multi-ret \nfunction calls shows up in the formal semantics, in the transition rule for appli\u00adcations. In a form \n( fa), the evaluations of f , a, and the actual function call all share the same exception context. In \nthe MRLC, however, they each have different ret-pt contexts. This is the key distinction. (Note that \nwe can, by dint of a global program transformation, im\u00adplement exceptions using multi-ret constructs. \n. . just as we can im\u00adplement exceptions using only regular function calls, by turning the entire program \ninside-out with a global CPS transform. This fact of formal interconvertibility amounts to more of a \ncompilation step than a particularly illuminating observation about practical compar\u00adison at the source-code \nlevel.) 10.2 CPS and explicit continuations We can also implement examples such as our parsimonious filter \nfunction by using explicit continuations. This, however, is apply\u00ading far too powerful a mechanism to \nthe problem. Explicit con\u00adtinuations typically require heap allocation, which destroys the ef\u00ad.ciency \nof the technique. With multi-return function calls, there is never any issue with the compiler s ability \nto stack-allocate call frames. No analysis required; success is guaranteed. The multi-ret mechanism is \ncarefully designed to provide much of the bene.t of explicit continuations while still keeping continuations \nimplicit and out of sight. Once continuations become denotable, expressible el\u00adements of our language, \nthe genie is out of the bottle, and even powerful analyses will have a dif.cult time reining it back \nin. Note, also, that the MRLC still allows function calls to be syntac\u00adtically composable, i.e., we can \nnest function calls: f (g(x)). This is the essence of direct style; the essence of CPS is turning this \noff, since function calls never return. As a result, CPS is much, much harder for humans to read. While \nwe remain very enthusiastic about the use of CPS as a low-level internal representation for programs, \nit is a terrible notation for humans. In short, explicit continuations are ugly, heavyweight and powerful, \nwhile multi-return function call is clearer, simpler, lighter weight, and less powerful. 10.3 Sum types \nProviding multiple return points to a function call is essentially pro\u00adviding a vector of continuations \nto a function instead of just one. As Filinski has pointed out [5], a product type in continuation space \nis equivalent to a sum type in value space. For example, we can regard the %if function as being the \nconverter between these two forms for the boolean sum type. So any function we can write with multiple \ncontinuations we could also write by having the function return a value taken from a sum type. For example, \nour filter function s recursion could return a value from this SML datatype: datatype Identical | Sublist \nof a list But this misses the point without the tail-recursive property of the #i syntax, and the ability \nto distribute the post-call conditionally\u00addependent processing across a branch that happens inside the \nre\u00adcursion, we miss the optimisation that motivated us to write the function in the .rst place. Perhaps \nwe should write programs using sum-type values and hope for a static analysis to transform the code to \nuse an equivalent prod\u00aduct of continuations. Perhaps this might be made to work in local, simple cases \nmuch is possible if we invoke the mythical suf.\u00adciently optimising compiler. But even if we had such \na compiler, it would still be blocked by control transfers that occur across com\u00adpilation/analysis units \nof code. The important point is that the power of a notation lies in its abil\u00adity to allow decisions \nto be expressed.5 This is the point of the word intensional in the intensional typing movement that swept \nthe programming-language community in the 1990 s [12]. Having multi-return function calls allows us to \nchoose between value en\u00adcodings and pc encodings, as desired. It is a speci.c instantiation of a very \ngeneral and powerful programming trick: anytime we can .nd a means of encoding information in the pc, \nwe have new ways to improve the speed of our programs. Run-time code generation, .rst-class functions, \nand .rst-class continuations can all be simi\u00adlarly viewed as means of encoding information in the pc. \nFilinski s continuation/value duality underlies our mechanism; but the mechanism is nonetheless what \nprovides the distinction to the programmer a desireable and expressive distinction. 10.4 C--weak continuations \nPeyton Jones, Ramsey and others have developed a language, C--, intended to act as a portable, high-level \nback-end notation for com\u00adpilers [14]. C--has a control construct called weak continuations which has \nsimilarities to the multi-return mechanism we ve pre\u00adsented. Weak continuations allow the programmer \nto name multi\u00adple return points within a procedure body, and then pass these as parameters to a procedure \ncall. However, there are several distinc\u00adtions between C-- s weak continuations and the MRLC s multi-ret \nmechanism. Weak continuations are denotable, expressible values in the lan\u00adguage. They can be named, \nand produced as the value of expres\u00adsions. This makes them a dangerous construct it is quite possible \n5It is also true that the power of a notation lies in its ability to allow decisions to be glossed over \nor left locally undetermined. to write a C--program that invokes a control-transfer to a procedure whose \nactivation frame has already been popped from the stack. (C--also has a labelled stack-unwinding mechanism, \nbut this does not seem to permit the tail-recursive passing of unwind points, so it is not eligible as \na general-purpose MRLC mechanism.) There is also a difference of granularity. Languages and compil\u00aders \nbased on .-calculus representations tend to assume that . ex\u00adpressions and function-call are very lightweight, \n.ne-grain mech\u00adanisms. Some . expressions written by the programmer turn into heap-allocated closures, \nbut others turn into jumps, while still oth\u00aders simply become register-allocation decisions, and others \nvan\u00adish entirely. Programmers rely on the fact that . expressions are a general-purpose mechanism that \nis mapped down to machine code in a variety of ways, some of which express very .ne-grain, lightweight \ncontrol and environment structure. The MRLC is consistent with this design philosophy. While we have \ndiscussed at some length the implementation of multi-return function calls with multiple stack pointers, \nit should be clear from the extended anchor-pointing example of section 6 that multi\u00adreturn calls .ts \ninto this picture of function call as a general-purpose control construct. The translation of a multi-ret \nprocedure call into a machine call instruction, passing multiple stack pointers, lies at the large-granularity, \nheavyweight end of the implementation spec\u00adtrum, analogous to the implementation of a . expression as \na heap\u00adallocated closure. We are advocating more than the pragmatic goal of allowing pro\u00adcedure calls \nto return to higher frames on the stack. We are advo\u00adcating extending the general-purpose programming \nconstruct of . expressions to include multiway branching a semantic extension. This is an intermediate \npoint between regular .-calculus forms and full-blown CPS a design point that we feel strikes a nice \nbalance between the multiple goals of power, expressiveness, analysability and readability. This distinction \nbetween C-- s weak continuations and the MRLC s multi-ret construct is not accidental. Both languages \nwere carefully designed to a purpose. C--is not intended for human program\u00adming; it is intended for programs \nproduced by compilers. Thus C--provides a menu of control constructs that can be chosen once the compiler \nhas analysed its source program and committed to a particular choice for every control transfer in the \noriginal program. Thus, also, C--is able to export dangerous, unchecked constructs, by pushing the requirements \nfor safety back to the higher-level lan\u00adguage that was the original program. The attraction of the MRLC \ns general mechanism is the attraction of . a general-purpose con\u00adstruct that allows for a particular, \nlocal use to be implemented in a variety of ways, depending on surrounding context and other global considerations. \nC--would make a great target for the MRLC, but the compiler tar\u00adgeting C--would translate uses of the \nMRLC multi-return mech\u00adanism to a wide array of C--constructs: if/then/else statements, loops, gotos, \nsimple function calls. . . and weak continuations. 10.5 FORTRAN Computational archaeologists may .nd \nit of interest that the idea of passing multiple return points to a function goes back at least as far \nas FORTRAN 77 [7], which allows subroutines (but not functions the distinction being that functions return \nvalues, while subroutines are called only for effect) to be passed alter\u00adnate return points. Note, however, \nthat these subroutines are not reentrant, the return points cannot be passed to subsequent calls in a \ntail-recursive manner, and FORTRAN s procedure abstractions subroutine and function, both are not general, \n.rst-class, express\u00adible values.   11 Conclusion The multiple-return function call has several attractions: \n It has wide-spectrum applicability, from .ne-grain conditional control .ow, to large-scale interprocedural \ntransfers. This spectrum is supported by the simplicity of the model, which enables optimising transformations \nto manipulate the control and value .ow of the computation.  It is not restricted to a small niche of \nlanguages. It is as well suited to Pascal or Java as it is to SML or Scheme.  It is expressive, allowing \nthe programmer to clearly and ef.\u00adciently shift between control and value encodings of a com\u00adputation. \nIt enables the expression of algorithms that are dif\u00ad.cult to otherwise write with equal ef.ciency. As \nwe ve men\u00adtioned previously, the filter function is not the only such example functional tree traversals, \nbacktracking search, per\u00adsistent data structure algorithms, and LR parsers are all algo\u00adrithms that can \nbe expressed succinctly and ef.ciently with multiple return points. Multiple return points bring most \nuses of the general technique of explicit continuation passing into the realm of the ef.cient.  The \nexpressiveness comes with no real implementation cost. The compilation story for multi-ret function calls \nhas no ex\u00adotic elements or heavy costs; standard technology works well. Procedure call frames can still \nbe allocated on a stack; stan\u00addard register-allocation techniques work.  It is a pay-as-you-go feature \nin terms of implementation. If a language provides multi-ret function calls, the feature only consumes \nrun-time resources when it is used essentially, a pair of registers are required across procedure transfers \nfor each extra return point used in the linkage.  It is a pay-as-you-go feature in terms of syntax. \nProgrammers can still write nested function calls, and the notation only af\u00adfects the syntax at the points \nwhere the feature is used.  We feel it is a useful linguistic construct both for source-level, human-written \nprogramming languages, and compiler internal rep\u00adresentations. In short, it is an expressive new feature. \n. . but surpris\u00adingly affordable. 12 Acknowledgements The Tiger compiler and parser tool we described \nin section 8 was implemented, in part, by Eric Mickley and Shyamsundar Jayara\u00adman, using code written \nby David Zurow, Lex Spoon and Do\u00adminique Boucher. Matthias Felleisen provided useful discussions on the \nsemantics and type issues of the MRLC, as well as its impact on A-normal form. Peter Lee alerted us to \nthe impact of exceptions on callee-saves register allocation. Chris Okasaki and Ralf Hinze pointed out \nentire classes of algorithms where ef.cient multi-return function call could be exploited. Zhong Shao \nand Simon Peyton Jones provided helpful discussions of weak continuations. Several anonymous reviewers \nprovided thoughtful and detailed comments that improved the .nal version of this paper. 13 References \n[1] Andrew W. Appel. Modern Compiler Implementation in ML. Cambridge University Press, 1999. [2] Henk \nBarendregt. The Lambda Calculus. North Holland, re\u00advised edition, 1984. [3] F. DeRemer and T. Pennello. \nEf.cient Computation of LALR(1) Look-Ahead Set. TOPLAS, vol. 4,no. 4, October 1982. [4] C. Fisher and \nR. LeBlanc. Crafting a Compiler. Benjamin Cummings, 1988. [5] Andrzej Filinksi. Declarative Continuations \nand Categorical Duality. Master s thesis, Computer Science Department, Uni\u00adversity of Copenhagen (August \n1989). DIKU Report 89/11. [6] C. Flanagan, A. Sabry, B. Duba and M. Felleisen. The essence of compiling \nwith continuations. Proceedings of the SIG-PLAN 1993 Conference on Programming Language Design and Implementation, \n1993, 237 247. [7] American National Standard Programming Language FOR-TRAN. X3.9-1978, American National \nStandards Institute, Inc., April, 1978. Available at http://www.fortran.com/ F77_std/rjcnf.html [8] S. \nC. Johnson. Yacc yet another compiler compiler. Tech report CSTR-32, AT&#38;T Bell Laboratories, Murray \nHill, NJ. [9] David Kranz, et al. ORBIT: An optimizing compiler for Scheme. In Proceedings of the SIGPLAN \n86 Symposium on Compiler Construction, published as SIGPLAN Notices 21(7), pages 219 233. Association \nfor Computing Machinery, July 1986. [10] R. Milner. A theory of type polymorphism in programming. Journal \nof Computer and System Sciences, 17:348 375, Au\u00adgust 1978. [11] R. Milner, M. Tofte, R. Harper, D. MacQueen. \nThe De.nition of Standard ML (Revised) MIT Press, 1997. [12] G. Morrisett, D. Tarditi, P. Cheng, C. Stone, \nR. Harper, and P. Lee. TIL: A type-directed optimizing compiler for ML. 1996 SIGPLAN Conference on Programming \nLanguage De\u00adsign and Implementation, pages 181 192, Philadelphia, May 1996. [13] Thomas J. Pennello. \nVery fast LR parsing. In Proceedings of the SIGPLAN 86 Symposium on Compiler Construction, pages 145 \n151, 1986. [14] Norman Ramsey and Simon Peyton Jones. A single inter\u00admediate language that supports multiple \nimplementations of exceptions. Proceedings of the ACM SIGPLAN 2000 Confer\u00adence on Programming Language \nDesign and Implementation, in SIGPLAN Notices, 35(5):285 298, June 2000. [15] Zhong Shao, John H. Reppy, \nand Andrew W. Appel. Un\u00adrolling Lists. In Proceedings of the 1994 ACM Confer\u00adence on Lisp and Functional \nProgramming, Orlando, Florida, pages 185 195, June 1994. [16] Olin Shivers. SRFI-32: Sort libraries. \nScheme Request for Implementation 32, available at URL http://srfi. schemers.org/. Forthcoming. [17] \nGuy L. Steele Jr. RABBIT: A Compiler for SCHEME. Tech\u00adnical Report 474, MIT AI Lab, May 1978. [18] Mitchell \nWand. Complete type inference for simple objects, In Proceedings of the Second Symposium on Logic in \nCom\u00adputer Science, Ithaca, New York, pages 37 44, June 1987.  \n\t\t\t", "proc_id": "1016850", "abstract": "It is possible to extend the basic notion of \"function call\" to allow functions to have multiple return points. This turns out to be a surprisingly useful mechanism. This paper conducts a fairly wide-ranging tour of such a feature: a formal semantics for a minimal &#955; -calculus capturing the mechanism; a motivating example; a static type system; useful transformations; implementation concerns and experience with an implementation; and comparison to related mechanisms, such as exceptions, sum-types and explicit continuations. We conclude that multiple-return function call is not only a useful and expressive mechanism, both at the source-code and intermediate-representation level, but is also quite inexpensive to implement.", "authors": [{"name": "Olin Shivers", "author_profile_id": "81100129912", "affiliation": "Georgia Tech College of Computing", "person_id": "PP39028876", "email_address": "", "orcid_id": ""}, {"name": "David Fisher", "author_profile_id": "81100550981", "affiliation": "Georgia Tech College of Computing", "person_id": "PP15035977", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1016850.1016864", "year": "2004", "article_id": "1016864", "conference": "ICFP", "title": "Multi-return function call", "url": "http://dl.acm.org/citation.cfm?id=1016864"}