{"article_publication_date": "09-19-2004", "fulltext": "\n A Nanopass Infrastructure for Compiler Education Dipanwita Sarkar Oscar Waddell R. Kent Dybvig Indiana \nUniversity Abstrax, Inc. Indiana University dsarkar@cs.indiana.edu waddell@abstrax.com dyb@cs.indiana.edu \n ABSTRACT Compilers structured as a small number of monolithic passes are di.cult to understand and di.cult \nto maintain. Adding new optimizations often requires major restructuring of ex\u00adisting passes that cannot \nbe understood in isolation. The steep learning curve is daunting, and even experienced de\u00advelopers .nd \nit hard to modify existing passes without in\u00adtroducing subtle and tenacious bugs. These problems are \nespecially frustrating when the developer is a student in a compiler class. An attractive alternative \nis to structure a compiler as a collection of many small passes, each of which performs a single task. \nThis micropass structure aligns the ac\u00adtual implementation of a compiler with its logical organiza\u00adtion, \nsimplifying development, testing, and debugging. Un\u00adfortunately, writing many small passes duplicates \ncode for traversing and rewriting abstract syntax trees and can ob\u00adscure the meaningful transformations \nperformed by individ\u00adual passes. To address these problems, we have developed a method\u00adology and associated \ntools that simplify the task of build\u00ading compilers composed of many .ne-grained passes. We describe \nthese compilers as nanopass compilers to indi\u00adcate both the intended granularity of the passes and the \namount of source code required to implement each pass. This paper describes the methodology and tools \ncompris\u00ading the nanopass framework. Categories and Subject Descriptors D.3.4 [Programming Languages]: \nProcessors Compil\u00aders, Translator writing systems and compiler generators  General Terms Design, languages, \nreliability Keywords Compiler writing tools, nanopass compilers, domain-speci.c languages, syntactic \nabstraction Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. ICFP \n04, September 19 21, 2004, Snowbird, Utah, USA. Copyright 2004 ACM 1-58113-905-5/04/0009 ...$5.00.  \n 1. INTRODUCTION Production compilers often exhibit a monolithic structure in which each pass performs \nseveral analyses, transformations, and optimizations, both related and unrelated. An attrac\u00adtive alternative, \nparticularly in an educational setting, is to structure a compiler as a collection of many small passes, \neach of which performs a small part of the compilation pro\u00adcess. This separation of concerns aligns the \nactual imple\u00admentation of a compiler with its logical organization, yield\u00ading a more readable and maintainable \ncompiler. Bugs that arise are more easily isolated to a particular task. Writing individual passes is \neasier since new code need not be grafted onto existing passes nor wedged between two logical passes \nthat would be combined in a monolithic structure. A few years ago we switched to this micropass structure \nin our senior-and graduate-level compiler courses. Students are supported in the writing of their compilers \nby several tools: a pattern matcher with convenient notations for re\u00adcursion and mapping, a set of macros \nthat can be used to expand the output of each pass into executable code, a ref\u00aderence implementation \nof the compiler, a suite of (terminat\u00ading) test programs, and a driver. The driver runs the com\u00adpiler \non each of the programs in the test suite and evaluates the output of each pass to verify that it returns \nthe same re\u00adsult as the reference implementation. Intermediate-language programs are all represented \nas s-expressions, which simpli\u00ad.es both the compiler passes and the driver. The switch to the micropass \nmethodology and the tools that support it have enabled our students to write more ambitious compilers. \nEach student in our one-semester com\u00adpiler class builds a 50-pass compiler from the s-expression level \nto Sparc assembly code for the subset of Scheme below. expr -. constant | (quote datum) | var | (set! \nvar expr) | (if expr expr) | (if expr expr expr) | (begin expr expr *) | (lambda (var *) expr expr *) \n| (let ((var expr)*) expr expr *) | (letrec ((var expr)*) expr expr *) | (primitive expr*) | (expr expr \n*) The compiler includes several optimizations as well as a graph-coloring register allocator. Students \nin the gradu\u00adate course implement several additional optimizations. The Week 1: simpli.cation verify-scheme1 \nrename-var remove-implicit-begin1 remove-unquoted-constant remove-one-armed-if verify-a1-output1 Week \n2: assignment conversion remove-not mark-assigned optimize-letrec2 remove-impure-letrec convert-assigned \nverify-a2-output1 Week 3: closure conversion optimize-direct-call remove-anonymous-lambda sanitize-binding-forms \nuncover-free convert-closure optimize-known-call3 uncover-well-known2 optimize-free2 optimize-self-reference2 \nanalyze-closure-size1 lift-letrec verify-a3-output1 Week 4: canonicalization introduce-closure-primitives \nremove-complex-constant normalize-context verify-a4-output1 Week 5: pointer encoding/allocation specify-immediate-representation \nspecify-nonimmediate-representation Week 6: start of UIL compiler verify-uil Week 7: introducing labels \nand temps remove-complex-opera* lift-letrec-body introduce-return-point verify-a7-output1 Week 8: virtual \nregisterizing remove-nonunary-let uncover-local the-return-of-set! .atten-set! verify-a8-output1 Week \n9: brief digression generate-C-code4 Week 10: register allocation setup uncover-call-live2 optimize-save-placement2 \neliminate-redundant-saves2 rewrite-saves/restores2 impose-calling-convention reveal-allocation-pointer \nverify-a10-output1 Week 11: start of register allocation uncover-live-1 uncover-frame-con.ict strip-live-1 \nuncover-frame-move verify-a11-output1 Week 12: setting up call frames uncover-call-live-spills assign-frame-1 \nassign-new-frame optimize-fp-assignments2 verify-a12-output1 Week 13: introducing spill code .nalize-frame-locations \neliminate-frame-var introduce-unspillables verify-a13-output1 Week 14: register assignment uncover-live-2 \nuncover-register-con.ict verify-unspillables1 strip-live-2 uncover-register-move assign-registers assign-frame-2 \n.nalize-register-locations analyze-frame-tra.c1 verify-a14-output1 Week 15: generating assembly .atten-program \ngenerate-Sparc-code Table 1: Passes assigned during a recent semester, given in running order and grouped \nroughly by week and primary task. Notes: 1Passes supplied by the instructor. 2Challenge-assignment passes \nrequired only of graduate students, not necessarily during the week shown. 3Actually written during Week \n4. 4Pass not included in the .nal compiler. During Week 6, students also had an opportunity to turn in \nupdated versions of earlier passes. Week 9 was a short week leading up to spring-break week. Most of \nthe passes are run exactly once; the passes that comprise the main part of the register and frame allocator \nare repeated until all variables have been given register or frame homes. passes included in the compiler \nare listed in Table 1. Due to space limitations, we cannot go into the details of each pass, but the \npass names are suggestive of their roles in the compilation process. The micropass methodology and tools \nare not without problems, however. The repetitive code for traversing and rewriting abstract syntax trees \ncan obscure the meaningful transformations performed by individual passes. In essence, the sheer volume \nof code for each pass can cause the stu\u00addents to lose the forest for the trees. Also, although we have \nlearned the importance of writing out grammars describing the output of each pass, as documentation, \nthe grammars are not enforced, and it is easy for an unhandled speci.c case to fall through to a more \ngeneral case, resulting in ei\u00adther confusing errors or malformed output to trip up later passes. Finally, \nthe resulting compiler is slow, which leaves students with a mistaken impression about the speed of a \ncompiler and the importance thereof. To address these problems, we have developed a nanopass methodology \nand a domain-speci.c language for writing nanopass compilers. A nanopass compiler di.ers from a mi\u00adcropass \ncompiler in three ways: the intermediate-language grammars are formally speci.ed and enforced, each pass \nneeds to contain traversal code only for forms that undergo meaningful transformation, and the intermediate \ncode is represented more e.ciently as records, although all interac\u00adtion with the programmer is still \nvia the s-expression syntax. We use the word nanopass to indicate both the intended granularity of passes \nand the amount of source code required to implement each pass. The remainder of this paper describes \nthe nanopass methodology and supporting tools. Section 2 introduces our methodology for building nanopass \ncompilers. Section 3 describes tools for building nanopass compilers. Section 4 highlights the di.erent \nfeatures of the framework and their underlying implementation, with the help of some exam\u00adple language \nand pass de.nitions. Section 5 surveys related work. Section 6 concludes with a discussion of future \nwork. 2. NANOPASS METHODOLOGY In the nanopass framework, a compiler is composed of many .ne-grained \npasses, each operating on programs in a well\u00adspeci.ed input language and producing programs in a well\u00adspeci.ed \noutput language. A discipline we .nd helpful in maintaining this .ne granularity is to require that each \npass perform a single speci.c task to simplify, verify, convert, analyze, or improve the code. A simpli.cation \npass reduces the complexity of subsequent passes by translating its input into a simpler intermediate \nlanguage, e.g., removing the primitive not from the lan\u00adguage. A veri.cation pass checks compiler invariants \nthat are not easily expressed within the grammar, e.g., that all bound variables are unique. A conversion \npass makes ex\u00adplicit an abstraction that is not directly supported by the low-level target language, \ne.g., converting basic blocks to a linear instruction stream by inserting branches. An anal\u00adysis pass \ncollects information from the input program and records that information by annotating the output program, \ne.g., annotating each lambda expression with its set of free variables. An improvement pass attempts \nto optimize the run time or resource utilization of the program. We require a veri.cation or improvement \npass (or group of related analysis and improvement passes) to produce a program in the same intermediate \nlanguage as its input pro\u00adgram so that we may selectively enable or disable individ\u00adual checks or optimizations. \nEnabling veri.cation passes can help to identify bugs during the development of up\u00adstream passes. Veri.cation \npasses may be disabled to im\u00adprove compiler speed. The ability to disable or enable indi\u00advidual optimizations \nat will simpli.es development of tools that automate regression testing with various permutations of \ncompiler switches. Such testing may uncover cases where a routine optimization masks bugs in seldom-executed \npor\u00adtions of another pass. Bugs of this kind are otherwise es\u00adpecially di.cult to locate since they typically \nsurface only in programs su.ciently complex to defeat the compensating optimizations. Selectively disabling \noptimizations is also an easy way to support a range of compiler switches that trade compile-time speed \nfor code quality. We .nd it helpful to codify the transformation performed by each pass using formal \ngrammars to describe the input and output languages of the pass. Care is taken to make these grammars \nprecise. For example, a pass may transform the input program so that particular forms are eliminated \nor appear in more limited contexts. Although the output of the pass is still a valid program in the input \nlanguage, we de.ne a new output-language grammar that explicitly deletes or restricts the a.ected form. \nIntermediate-language grammars are speci.ed formally via language de.nitions (Section 3.1). Language \nde.nitions play an important role when de.ning passes, and precise grammars are also useful documentation. \nThe transforma\u00adtion performed by a pass can often be understood by com\u00adparing the grammars of the input \nand output languages of the pass. It may appear that much of the compiler complexity has been pushed \ninto the intermediate-language de.nitions, par\u00adticularly given our emphasis on precise grammars. Since \neach pass performs one, well-speci.ed transformation, how\u00adever, changes in the intermediate language \nfrom one pass to the next are usually slight. Often we can de.ne new lan\u00adguages via inheritance, described \nin Section 3.2, expressing the di.erences between the languages more concisely. More\u00adover, many passes, \nincluding improvement and veri.cation passes, operate on the same source and output languages. So, although \nwe write many passes and emphasize precise grammars, we write fewer language de.nitions than passes, \nand many of the ones we do write are concisely speci.ed in terms of an earlier language. In addition, \nthe compilation system compares the re\u00adsults obtained by evaluating the output of each pass against the \nresults produced by a reference implementation. This practice helps to isolate correctness-preservation \nfailures to a particular pass. Having isolated an o.ending pass, we can view intermediate-language programs \nin a readable s\u00adexpression form when tracing through the output of the pass searching for the cause of \nthe failure. 3. NANOPASS TOOLS This section describes tools for de.ning new intermediate languages and \ncompiler passes. These tools comprise a domain-speci.c language for writing nanopass compilers and are \nimplemented as extensions to the host language, Scheme, via the syntax-case macro system [6]. This language\u00adembedding \napproach provides access to the full host lan\u00adguage for de.ning auxiliary procedures and data structures, \nwhich are particularly useful when writing involved passes, such as a register allocation pass. The host \nlanguage also provides the evaluator that we use to evaluate the output of each compiler pass during \ndevelopment. 3.1 De.ning intermediate languages Intermediate language de.nitions take the following form. \n(de.ne-language name { over tspec+ }where production+) The optional tspec declarations specify the terminals \nof the language and introduce metavariables ranging over the var\u00adious terminals. Each tspec is of the \nform (metavariable+ in terminal) where the terminal categories are declared externally. A metavariable \ndeclaration for x implicitly speci.es metavari\u00adables of the form xn, where n is a numeric su.x. Each \nproduction corresponds to a production in the grammar of the intermediate language. A production pairs \na nonterminal with one or more alter\u00adnatives, with an optional set of metavariables ranging over the \nnonterminal. ({ metavariable+ in } nonterminal alternative+) Productions may also specify elements that \nare common to all alternatives using the following syntax. ({ metavariable+ in } (nonterminal common \n+) alternative+) #[lambda.program.10 (x) #[if.expr.10 #[x.expr .10 x] #[anon.18.expr #[x.expr.10 foo] \n(#[x.expr.10 x])] #[anon.18.expr (lambda (x) (if x (foo x) (bar x))) #[x.expr.10 bar] (#[x.expr.10 x])]] \nIllustration 1: All compiler-writer interactions are via the s-expression syntax. Common elements may \nbe used to store annotations, e.g., source information or analysis byproducts, that are common to all \nsubforms of the intermediate language. Each alternative is a metavariable or parenthesized form declaring \nan intermediate language construct, followed by an optional set of production properties property+ . \nParen\u00adthesized forms usually begin with a keyword and contain substructure that usually includes metavariables \nspecifying the language category into which each subform falls. At most one alternative of a production \nmay be a parenthe\u00adsized form that does not begin with a keyword, allowing the intermediate language to \ninclude applications using the natural s-expression syntax. Each property is a key, value pair. Properties \nare used to specify semantic, type, and .ow information for the associated alternative. Figure 1 shows \na simple language de.nition and the gram\u00admar it implicitly de.nes. It de.nes metavariables x, b, and \nn ranging over variables, booleans, and integers, and de.nes three productions. The .rst production de.nes \nProgram as an Expr. The second de.nes metavariables e and body rang\u00ading over Expr and declares that Expr \nis a boolean, integer, variable reference, if expression, seq expression, lambda ex\u00adpression, or application. \nThe third de.nes metavariable c ranging over Command and declares that Command is a set! command or seq \ncommand. The semantics of each intermediate language form may be speci.ed implicitly via its natural \ntranslation into the host language, if one exists. In Figure 1, this implicit translation su.ces for \nbooleans, numbers, variable references, lambda, set!, if, and applications. For seq expressions, the \ntransla\u00adtion is speci.ed explicitly using the => (translates-to) prop\u00aderty. Implicit and explicit translation \nrules establish the meaning of an intermediate language program in terms of the host language, which \nis an aid to understanding interme\u00addiate language programs and provides a mechanism whereby the output \nof each pass can be veri.ed to produce the same results as the original input program while a compiler \nis be\u00ading debugged. Explicit translations can become complex, in which case we often express the translation \nin terms of a syntactic abstraction (macro) de.ned in the host language. From a compiler writer s point \nof view, a language de.\u00adnition speci.es the structure of an intermediate language in terms of the familiar \ns-expression syntax (Illustration 1). All of the compiler writer s interactions with the intermediate \nlanguage occur via this syntax. Internally, however, inter\u00admediate language programs are represented \nmore securely and e.ciently as record structures. Intermediate language programs are also evaluable \nin the host language, using the translation properties attached to production alternatives. To support \nthese di.ering views of intermediate language programs, a language de.ned via de.ne-language implicitly \nde.nes the following items: 1. a set of record types representing the abstract syntax trees (ASTs) of \nintermediate-language programs, 2. a mapping from s-expressions to record structure, 3. a mapping from \nrecord structure to s-expressions, and 4. a mapping from s-expression patterns to record struc\u00adture \n  These products are packaged within a module and may be imported where they are needed. The remainder \nof this section describes these products in more detail. 3.1.1 Record-type de.nitions The language de.nition \nautomatically generates a set of record de.nitions as shown in Figure 2. A base record type is constructed \nfor the language along with a subtype for each nonterminal. The subtype for each nonterminal de\u00adclares \nthe common elements for that nonterminal. A new record type is also created for each alternative as a \nsubtype of the corresponding nonterminal. The de.ne-language form given in Figure 1 de.nes a record type \nfor L0, subtypes of this type for Program, Expr, and Command, and subtypes of these for each of their \nalternatives. For example, the record type for (if e1 e2 e3) is a subtype of the record type for Expr, \nwhich is in turn a subtype of the record type for L0. The record type for if contains three Expr .elds. \nWhere ellipses are used in the de.ne-language syntax, the .eld contains a list of elements. For example, \nan alternative (de.ne-language L0 over (x in variable) (b in boolean) (n in integer) where (Program (L0) \n-. (Program) Expr) (Program) -. (Expr) (e body in Expr b (Expr) -. (boolean) n | (integer) x | (var) \n(if e1 e2 e3) | (if (Expr) (Expr) (Expr)) (seq c1 e2) => (begin c1 e2) | (seq (Command) (Expr)) (lambda \n(x ...) body) | (lambda ((var)*) (Expr)) (e0 e1 ...)) | ((Expr) (Expr)*) (c in Command (set! x e) (Command) \n-. (set! (var) (Expr)) (seq c1 c2) => (begin c1 c2))) | (seq (Command) (Command)) Figure 1: A simple \nlanguage de.nition and the corresponding grammar (define-record L0 ()) (define-record program L0 ()) \n(define-record expr L0 ()) (define-record command L0 ()) (define-record b.expr expr (b)) (define-record \nn.expr expr (n)) (define-record x.expr expr (x)) (define-record if.expr expr (e1 e2 e3)) (define-record \nseq.expr expr (c1 c2)) (define-record lambda.expr expr (xs body)) (define-record app.expr expr (e0 es)) \n(define-record set!.command command (x e)) (define-record seq.command command (c1 c2)) Figure 2: Record \nde.nitions generated for L0 (let ((x e) ...) body) would declare a record type with two .elds, the .rst \nof which contains a list of (x e) records associating program variables with Expr records. Strong typing \nof record constructors ensures that ASTs are well\u00adformed by construction.  3.1.2 Parser Each language \nde.nition produces a parser capable of trans\u00adforming s-expressions to the corresponding record structure \nrepresenting the same abstract syntax tree. The parser for the .rst input language serves as the .rst \npass of the com\u00adpiler (after lexical analysis and parsing) and provides the record-structured input required \nby subsequent passes of the compiler. Providing parsers for each intermediate language aids debugging \nby allowing the compiler-writer to obtain inputs suitable for arbitrary passes of the compiler. Figure \n3 shows part of the code for the parser that is gen\u00aderated by the language de.nition for L0 in Figure \n1. The code for the parser mirrors the language de.nition. The language de.nition produces a set of mutually \nrecursive pro\u00ad (define (parser-lang.L0 s-exp) (define (parse-program s-exp) ...) (define (parse-expr \ns-exp) (if (pair? s-exp) (cond ... [(and (eq? seq (car s-exp)) (= 3 (length s-exp))) (make-seq.expr.L0.6 \n(parse-command (cadr s-exp)) (parse-expr (caddr s-exp)))] ... [else (make-anon.7 (parse-expr (car s-exp)) \n(map parse-expr (cdr s-exp)))]) (cond [((boolean? s-exp) (make-b.expr.L0.1 s-exp)] ... [else (error ---)])))) \n(define (parse-command s-exp) ...)) Figure 3: Parser for a language L0 cedures, parse-program, parse-expr \nand parse-command, each handling all the alternatives of the corresponding non\u00adterminal. The nonterminal \nparsers operate by recursive de\u00adscent on list structured input following the grammar. For example, the \nparse-expr case for seq .rst veri.es whether the list has three elements and begins with the keyword \nseq, then calls directly to parse-command for .rst subform and to parse-expr for the second subform. \nAn error is signaled if there is no match. Since parenthesized forms are disambiguated by the be\u00adginning \nkeyword, at most one parenthesized form per non\u00adterminal can begin with a metavariable, i.e., a nonkeyword. \n 3.1.3 Unparser The unparser converts the AST records to their correspond\u00ading host-language executable \nforms. Like the parser this also serves as a good debugging aid by allowing the compiler\u00adwriter to view \nthe output of any pass in host-language form. It enables the compiler-writer to trace and manually trans\u00adlate \nprograms, e.g., during the exploratory phase of the de\u00advelopment of a new optimization. Each record-type \nde.\u00adnition stores the parenthesized form and the host-language form for the alternative. The host-language \nform, if di.erent from the parenthesized form, is expressed as the translates\u00adto production property \nin the language de.nition, as in the case of (seq c1 e2) => (begin c1 e2) in Figure 1. Each record type \nstores the information required to unparse in\u00adstances of itself. As a result, all languages share one \nunparse procedure. Since the record type stores both the parenthesized form and the host-language form \nof the alternative, the unparser can also translate the record structures into their parenthe\u00adsized forms, \nthus allowing the compiler writer to pretty-print the output. 3.1.4 Partial Parser The partial parser \nis used to support input pattern match\u00ading and output construction, which are described in Sec\u00adtion 3.3.1 \nand Section 3.3.2. The partial parser translates s-expression syntax repre\u00adsenting an input pattern or \noutput template into its cor\u00adresponding record structure. The variable parts of the s\u00adexpression syntax \nare converted to a special list representa\u00adtion that is later used to generate code for the pass. For \nexample, the partial parser parses the s-expression syntax (let ((,x ,e) ...) (foo ,x ...)) into the \nfol\u00adlowing. #[let-record #[anon.1-record (maplist (list (x variable)(e expr)))] #[app-record #[ref-record \n(foo variable)] #[ref-record (x variable)]]] The actual list representations generated are slightly more \nelaborate than the ones shown above and will be discussed later in Section 4.2. The structures produced \nby the partial parser are not visible to the compiler-writer, but are used to generate the code for matching \nthe given pattern and constructing the desired output.  3.2 Language inheritance Consecutive intermediate \nlanguages are often closely related due to the .ne granularity of the intervening passes. To permit more \nconcise speci.cation of these languages, the de.ne-language construct supports a simple form of in\u00adheritance \nvia the extends keyword, which must be followed by the name of a base language, already de.ned. (de.ne-language \nname extends base { over { mod tspec }+ } { where { mod production }+ }) The terminals and productions \nof the base language are copied into the new language subject to modi.cations in the over and where sections \nof the de.nition, either of which may be omitted if no modi.cations to that section are nec\u00adessary. Each \nmod is either +, which adds a new terminal or production, or -, which removes the corresponding ter\u00adminal \nor production(s). The example below de.nes a new language L1 derived from L0 (Figure 1) by removing the \nboolean terminal and Expr alternative and replacing the Expr if alternative with an Expr case alternative. \n (de.ne-language L1 extends L0 over -(b in boolean) where -(Expr b (if e1 e2 e3)) + (default in Expr \n(case x (n1 e1) ... default)))  Language L1 could serve as the output of a conversion pass that makes \nlanguage-speci.c details explicit en route to a language-independent back end. For example, C treats \nzero as false, while Scheme provides a distinct boolean constant #f representing false. Conditional expressions \nof either lan\u00adguage could be translated into case expressions in L1 with language-speci.c encodings of \nfalse made explicit. Language inheritance is mainly a notational convenience. A new language de.nition \nis generated from the de.nition of the parent language and the implementation from that point is the \nsame as that for de.ne-language. The parent and child languages do not share any record de.nitions. \n3.3 De.ning passes Passes are speci.ed using a define-pass construct that names the input and output \nlanguages and speci.es transfor\u00admation functions that map input-language forms to output\u00adlanguage forms. \n(define-pass name input-language -> output-language transform ...) Some passes are run purely for e.ect, \ne.g., to collect and record information about variable usage. For such passes, the special output language \nvoid is used. Similarly, the spe\u00adcial output language datum is used when a pass traverses an AST to compute \nsome more general result, e.g., an estimate of object code size. Each transform speci.es the transformer \ns name, a signa\u00adture describing the transformer s input and output types, and a set of clauses implementing \nthe transformation. (name : nonterminal arg ... -> val val ... [input-pattern { guard } output-expression] \n...) The input portion of the signature lists a nonterminal of the input language followed optionally \nby the types of any addi\u00adtional arguments expected by the transformer. The output portion lists one or \nmore result types. Unless void or datum is speci.ed in place of the output language, the .rst result \ntype is expected to be an output-language nonterminal. Each clause pairs an input-pattern with a host-language \noutput-expression that together describe the transformation of a particular input-language form. Input \npatterns are speci.ed using an s-expression syntax that extends the syn\u00adtax of alternatives in the production \nfor the corresponding input-language nonterminal as described in Section 3.3.1. Output expressions may \ncontain templates for constructing output-language forms using syntax which extends that of alternatives \nin the production for the corresponding output\u00adlanguage nonterminal. The extended syntax of input pat\u00adterns \nand output templates is described in sections 3.3.1 and 3.3.2. The optional guard, if present, is a host-language \nexpression that imposes additional constraints on matching. Often, a pass performs nontrivial transformation \nfor just a few forms of the input language. In such cases, the two intermediate languages are closely \nrelated and the new lan\u00adguage can be expressed using language inheritance. When two intermediate languages \ncan be related by inheritance, a pass de.nition can specify transformers for only those forms that have \nundergone change, leaving the implemen\u00adtation of other transformers to a pass expander. The pass expander \ncompletes the implementation of a pass by con\u00adsulting the de.nitions of the input and output languages. \nStrong typing of passes, transformers, and intermediate lan\u00adguages helps the pass expander to automate \nthese simple transformations. The pass expander is an important tool for keeping pass speci.cations concise. \n 3.3.1 Matching input When invoked, a transformer matches its .rst argument, which must be an AST, against \nthe input pattern of each clause until a match is found. Clauses are examined in order, with user-speci.ed \nclauses preceding any clauses in\u00adserted by the pass expander. This process continues until the input \npattern of some clause is found to match the AST and the additional constraints imposed by guard expressions \nand pattern variables, described below, are satis.ed. When a match is found, the corresponding output \nexpression is evaluated to produce a value of the expected result type. An error is signaled if no clause \nmatches the input. Input patterns are speci.ed using an s-expression syntax that extends the syntax of \nalternatives for the correspond\u00ading nonterminal with support for pattern variables. Subpat\u00adterns are \nintroduced by commas, which indicate, by analogy to quasiquote and unquote [5], portions of the input \nform that are not .xed. For example, (seq (set! ,x ,n) ,e2) introduces three subpatterns binding pattern \nvariables x, n, and e2. Metavariables appearing within patterns impose further constraints on the matching \nprocess. Thus the pre\u00adceding pattern matches only those inputs consisting of a seq form whose .rst subform \nis a set! form that assigns a num\u00adber to a variable, and whose second subform is an Expr. Pattern variables \nare used within input patterns to con\u00adstrain the matching of subforms of the input AST. Within subpatterns, \npattern variables are used to bind matching subforms of the input to program variables that may be ref\u00aderenced \nwithin output expressions and to match the results of structural recursion on subforms of the input. \nThe var\u00adious forms that subpatterns may take are summarized be\u00adlow, where the metavariable a ranges over \nalternate forms of an input-language nonterminal A, and the metavariable b ranges over alternate forms \nof an output-language nonter\u00adminal B. 1. The subpattern ,a matches if the corresponding input subform \nis a form of A, and binds the pattern variable a to the matching subform. 2. The subpattern ,[f : a \n-> b] matches if the corre\u00adsponding input subform is a form of A, and the result  obtained by invoking \nthe transformer f on that sub\u00adform is a form of B. If the match succeeds, the pattern variable a is bound \nto the matching input subform, and b is bound to the result that is obtained by invok\u00ading f . 3. The \nsubpattern ,[a -> b] is equivalent to the subpat\u00adtern ,[f : a -> b] if f is the sole transformer map\u00adping \nA . B. 4. The subpattern ,[b] is equivalent to the subpattern ,[a -> b] if the corresponding input subform \nis a form of A.  Transformers may accept multiple arguments and return multiple values. To support \nthese transformers, the syn\u00adtax ,[f : ax ... -> by ...] may be used to supply additional arguments x \n... to f and bind program variables y ... to the additional values returned by f . The .rst argu\u00adment \nmust be an AST as must the .rst return value, unless void or datum is speci.ed as the output type. Subpatterns \n3 and 4 are extended in the same way to support the gen\u00aderal forms ,[ax ... -> by ...] and ,[by ...]. \nPat\u00adtern variables bound to input subforms may be referenced among the extra arguments x . . . within \nstructural-recursion patterns 2 and 3 above. Metavariables are used within pat\u00adterns to guide the selection \nof appropriate transformers for structural recursion. When present, the optional guard expression imposes \nad\u00additional constraints on the matching of the input subform prior to any structural recursion speci.ed \nby the subpat\u00adtern. Pattern variables bound to input forms are visible within guard expressions. The \nmechanism just described is consistent with the behavior of transformers, although, in fact, transformers \nlocate candidate clauses using e.cient type dispatch supported by the AST record structures. To avoid \nduplicate evaluation, the pass expander commonizes cases that scrutinize the results of structural recursion. \n 3.3.2 Constructing output When the input to a transformer matches the input pat\u00adtern of one of the \nclauses, the corresponding output expres\u00adsion is evaluated in an environment that binds the subforms \nmatched by pattern variables to like-named program vari\u00adables. For example, if an input language record \nrepresent\u00ading (set! y (f 4)) matches the pattern (set! ,x ,e), the corresponding output expression is \nevaluated in an environ\u00adment that binds the program variables x and e to the records representing y and \n(f 4). When a pattern variable is fol\u00adlowed by an ellipsis (...) in the input pattern, the corre\u00adsponding \nprogram variable is bound to a matching list of records. New abstract syntax trees are constructed via \noutput templates speci.ed using an overloaded quasiquote syntax that constructs record instances rather \nthan list structure. Where commas (i.e., unquote forms) do not appear within an output template, the \nresulting AST has a .xed struc\u00adture. An expression pre.xed by a comma within an output template is a \nhost-language expression that must be eval\u00aduated to obtain an AST to be inserted as the correspond\u00ading \nsubform of the new AST being produced. For example, (if (not ,e1) ,e2 ,e3) constructs a record representing \nan if expression with an application of the primitive not as its test and the values of program variables \ne1, e2, and e3 inserted where indicated. The constructor applications im\u00adplied by this output template \nare essentially the following. (make-if (make-primapp not e1) e2 e3) The record constructors available \nwithin output templates are determined by the output language speci.ed in the pass de.nition. Instantiating \nthe output template must produce an AST representing a form of the output nonterminal for the transformer \ncontaining the clause. A subtle point arises when writing a pass that maps pro\u00adgrams in one language \nto programs in another closely re\u00adlated language. When the input and output languages are di.erent, a \nclause [,x x] would produce a record of the wrong type, since it matches and returns an input-language \nrecord. The clause must instead be written [,x ,x] so that the output-language constructors are invoked \nto produce an AST of the expected type. Fortunately, the pass expander automates such trivial transformations. \nBy nature, more substantial transformations virtually always employ output templates and so are seldom \na.ected by this distinction. Where ellipses follow an unquote form in an output tem\u00adplate, the host-language \nexpression must evaluate to a list of objects. For example, (begin ,e ... ,c) requires that e be bound \nto a list of Expr forms. Lists of records are of\u00adten convenient when interfacing with common subroutines, \ne.g., to partition a set of expressions according to speci.c properties. Because quasiquote is overloaded \nto construct record structures of the output language, an expression (,e ...) may return a record representing \na procedure application, rather than a list of e records. Thus it is con\u00advenient that an input pattern \n(let ((,x ,e) ...) ,body) binds the program variables x and e to lists of variable and Expr records. \nWhen more complex list structures are desired, list-processing tools of the host language are conve\u00adnient. \nFor example, within an output expression associated with the preceding let pattern, (map list x e) could \nbe used to build a list structure that could not otherwise be constructed via the overloaded quasiquote. \n  4. EXAMPLES This section shows a few sample languages and compiler passes de.ned via de.ne-language \nand define-pass. It also shows portions of the code generated by the pass ex\u00adpander for representative \npasses and brie.y discusses how they are generated. For clarity, the following examples use fully quali.ed \nnames of the form name .language. 4.1 Removing a primitive Language L2 de.ned in Figure 4 is a simple \nlanguage of expressions. The objective of the pass remove-not in Fig\u00adure 5 is to eliminate the primitive \nnot from programs in this language. When not is used as the test part of an if ex\u00adpression, we express \nthe inversion in .ow of control simply by swapping the consequent and alternative expressions. In all \nother contexts, we replace calls to not with an equivalent if expression that does the conversion explicitly. \nThis pass operates as a source-to-source transformation. The pass def\u00adinition deals with only those language \nforms that undergo transformation and relies upon the pass expander to sup\u00adply code for the remaining \ncases. In particular, the pass expander supplies code for the case where the test part of (de.ne-language \nL2 over (x in variable) (b in boolean) (n in integer) (pr in primitive) where (Program Expr) (e body \nin Expr b n x (if e1 e2 e3) (lambda (x ...) body) (primapp e0 e1 ...) (e0 e1 ...))) Figure 4: A simple \nlanguage of expressions (de.ne-pass remove-not L2 -> L2 (process-expr : Expr () -> Expr () [(if (not \n,[e1]) ,[e2] ,[e3]) (if ,e1 ,e3 ,e2)] [(primapp ,pr ,[e]) (eq? not pr) (if ,e #f #t)])) Figure 5: A \npass that performs source-to-source transformation an if expression is not a primapp or is an application \nof a primitive other than not. As mentioned in Section 3.1.4, we generate the pattern\u00admatching code by \n.rst parsing the input pattern with the partial parser of the input language in order to identify the \ninput and output constraints that govern the pattern\u00admatching process. For example, when given the input \npat\u00adtern (if (not ,[e1]) ,[e2] ,[e3]) the partial parser generates the following record. #[if.L2 #[primapp.L2 \n#[var.L2 not] (e1 #f expr.L2 process-expr)] (e2 #f expr.L2 process-expr) (e3 #f expr.L2 process-expr)] \n By comparing this record with the if alternative in the de.nition of L2, we see that the .rst subpattern \nimposes a constraint on the input, since primapp is a subtype of expr. The list structures contained \nwithin this record im\u00adpose additional constraints on the matching of the results from implicit recursion, \nand they specify the targets of these recursive calls, as well as the names of the pattern vari\u00adables \nto which the results are to be bound. For example, the pattern variable e2 is to be bound to the result \nof call\u00ading process-expr on (primapp.L2-e (if.L2-e1 ir)), pro\u00advided that the result satis.es the expr.L2? \npredicate. Here ir is the input to the pass and record-.eldname is the syntax for .eld accessors. Normally, \nthe input to a pass is generated by either a parser or another pass and is thus well-formed by construction. \nTherefore, the individual .elds of the in\u00adput record need not be checked unless they indicate a type \nor pattern more speci.c than what is expected according to the input-language de.nition. A guard expression, \nif present, is evaluated only if the input record satis.es the input constraints of the pattern. If the \nguard condition is satis.ed, pattern matching continues with the input and output pattern variables bound \nto the corresponding pieces of the input or results from implicit recursion. Input pattern variables \ncan be referenced within guard expressions, as shown below. [(primapp ,pr ,[e] ...) (eq? pr not) (if \n,e #f #t)] The guard expression, (eq? pr not) is evaluated in an en\u00advironment where pr is bound to (primapp.L2-pr \nir). To generate the code that is responsible for constructing the output record, we parse the output \ntemplate with the partial parser of the output language and examine the re\u00adsulting record structure to \nidentify the record-constructors that need to be called. For the pattern in the output tem\u00adplate (if \n,e1 ,e3 ,e2), the partial parser generates the following record. #[if.L2 e1 e3 e2] Since output templates \nimpose no constraints, the partial parser suppresses that information for the pattern variables e1, e2, \nand e3. The following excerpt shows the expansion of the if clause speci.ed by the programmer in Figure \n5. (and (if.L2? ir) (primapp.L2? (if.L2-e1 ir)) (let ([e1 (process-expr (primapp.L2-e (if.L2-e1 ir)))] \n[e2 (process-expr (if.L2-e2 ir))] [e3 (process-expr (if.L2-e3 ir))]) (or (and (expr.L2? e1) (expr.L2? \ne2) (expr.L2? e3) (make-if .L2 e1 e3 e2)) (error ---)))) The following shows the expansion of the primapp \nclause in Figure 5. (and (primapp.L2? ir) (let ([pr (primapp.L2-pr ir)]) (and (primitive? pr) (eq? not \npr) (let ([e (process-expr (primapp.L2-e ir))]) (or (and (expr.L2? e) (make-if.L2 e #f #t)) (error ---)))))) \n 4.2 Beta reduction The objective of the optimize-direct-call pass in Figure 7 is to replace what would \notherwise be a call to an anony\u00ad (de.ne-language L3 extends L2 where + (Expr (let ((x e) ...) body))) \n Figure 6: A language derived from another language (de.ne-pass optimize-direct-call L2 -> L3 (process-expr \n: Expr () -> Expr () [((lambda (,x ...) ,[body]) ,[e] ...) (let ((,x ,e) ...) ,body)])) Figure 7: A pass \nthat performs beta reduction. mous lambda with a simple let expression. In practical terms, this transformation \navoids an unnecessary heap allo\u00adcation, an indirect jump, and, when there are free variables in the body, \nsome additional indirect memory references. If the expression is evaluated frequently, the savings can \nbe signi.cant. This pass translates expressions in language L2 to expressions in language L3, which di.ers \nfrom L2 only by the addition of a let form. L3 is concisely de.ned in Figure 6 by extending the previously \nde.ned language L2. The input and output constraints and the output con\u00adstructors in this example are \ngenerated from partially parsed records in the same way as described in Section 4.1. How\u00adever, the patterns \nin this case are more involved. Consider the following clause from Figure 7. [((lambda (,x ...) ,[body]) \n,[e] ...) (let ((,x ,e) ...) ,body)] For the input pattern, the partial parser produces the fol\u00adlowing \nrecord. #[app.L2 #[lambda.L2 (maplist-of (x variable.L2 #f #f)) (body expr.L2 expr.L3 process-expr)] \n(maplist-of (e expr.L2 expr.L3 process-expr))] (maplist-of (x variable #f #f)) indicates that the pat\u00adtern \nis to be matched against an input list containing an arbitrary number of items, where each item is a \nvariable record that needs no implicit recursion and imposes no out\u00adput type constraints. From this record, \nwe generate code like the following to bind the pattern variables to the appro\u00adpriate portions of the \ninput and to the results of processing the input. (let ([x (lambda.L2-x (app.L2-e0 ir))]) ... (let ([e \n(map process-expr (app.L2-e1 ir))] [body (process-expr (lambda.L2-body ir))]) ...)) For the output template, \nthe partial parser produces the following record. #[let.L3 (maplist-of (list-of x e)) body] The list \n(maplist-of (list-of x e)) representing the pat\u00adtern ((,x ,e) ...) is also processed to generate code \nto reconstruct the list from the instantiated pattern variables x and e as shown below. (let ([opfield.1 \n(map (lambda (g0 g1) (cons g0 (cons g1 ()))) x e)] [opfield.2 body]) (make-let .L3 opfield1 opfield2)) \n 4.3 Assignment conversion The process of assignment conversion involves two passes. The .rst pass, \nmark-assigned, locates assigned variables i.e., those appearing on the left-hand side of an assign\u00adment, \nand marks them by setting a .ag in one of the .elds of the variable record structure. A second pass, \nconvert-assigned, rewrites references and assignments to assigned variables as explicit structure accesses \nor muta\u00adtions. The mark-assigned pass runs for e.ect only on an input in language L4, which is may be \nderived from L0 by adding the forms let, primapp, and the terminal primitive, as shown below. (de.ne-language \nL4 extends L0 over + (pr in primitive) where  + (Expr (let ((x e) ...) body) (primapp pr e ...))  + \n(Command (primapp pr e ...)))  Only one language form, set!, need be handled explicitly by this pass. \nIf the input is a set! expression, the pass simply sets the assigned .ag in the record structure representing \nthe assigned variable. (define-pass mark-assigned L4 -> void (process-command : Command -> void [(set! \n,x ,[e]) (set-variable-assigned! x #t)])) Since convert-assigned removes the set! form, its output language, \nL5, is derived from its input language L4. (de.ne-language L5 extends L4 where -(Command (set! x e))) \nAs shown in Figure 8, the convert-assigned pass intro\u00adduces a let expression binding each assigned variable \nto a pair whose car is the original value of the variable, re\u00adplaces each reference to an assigned variable \nwith a call to car, and replaces each assignment with a call to set-car!. split-vars is an auxiliary \nprocedure that introduces tem\u00adporaries for assigned variables. This pass converts the L4 program (lambda \n(a b) (seq (set! a b) a)) into the following L5 program (lambda (t b) (let ((a (cons t #f))) (seq (set-car! \na b) (car a))))  5. RELATED WORK The Zephyr Abstract Syntax Description Language (ASDL) describes tree-like \nintermediate languages in a format that can be used to generate language-speci.c data structure dec\u00adlarations \nas well as language-speci.c functions that read and write these structures [16]. These functions provide \nfunc\u00adtionality similar to the parsers and unparsers generated by our language de.nitions. ASDL focuses \nsolely on intermedi\u00adate representation and therefore does not integrate support for de.ning passes. We \nhave introduced our own language for describing intermediate representations so that the pass expander \ncan refer to these descriptions when .lling in the details of passes. The TIL compiler performs all optimizations \non typed intermediate languages [11]. Tarditi, et. al, found that type-checking the output of each optimization \npass helped to identify and eliminate bugs in the compiler. In our framework, type information could \nbe encoded via proper\u00adties in the language de.nition, and a veri.cation pass could be inserted after \neach pass to typecheck its output. Our language de.nitions produce expanders that translate in\u00adtermediate \nrepresentations to semantically equivalent host\u00adlanguage programs. During development we often use this \nmechanism to evaluate the output of each pass and com\u00adpare the results with those produced by a reference \nimple\u00admentation. We could instead compare the results of static analysis, including type information, \non the input and out\u00adput programs after each pass. We already use veri.cation passes during development \nto check other static properties. Polyglot simpli.es construction of compilers for source\u00adlevel language \nextensions of Java [9]. A key design goal of Polyglot is to ensure that the work required to add new \npasses or new AST node types is proportional to the num\u00adber of node types or passes a.ected. This goal \nis achieved through the use of some fairly involved OOP syntax and mechanisms. Our pass expander and \nour support for lan\u00adguage inheritance approach the same goal with less syntactic and conceptual overhead. \nTm is a macro processor in the spirit of m4 that takes a source code template and a set of data structure \nde.ni\u00adtions and generates source code [12]. Tree-walker and ana\u00adlyzer templates that resemble define-pass \nhave been gener\u00adated using Tm [13]. These templates are low-level relatives of define-pass, which provides \nconvenient input pattern syntax for matching nested record structures and output template syntax constructing \nnested record structures. Tm data-structure de.nitions do not support extensible proper\u00adties described \nin Section 3.1. Some similarities also exist between the nanopass ap\u00adproach and approaches taken in the \nPFC and SUIF com\u00adpilers. Like the Nanopass compiler, the PFC compiler [3] uses macro expansion to .ll \nin boilerplate transformations. The SUIF system [1, 2] provides object-oriented tools for specifying \nintermediate language programs, including tools that allow compiler passes to focus on di.erent aspects \nof an intermediate language program. The SUIF compiler has (define-pass convert-assigned L4 -> L5 (process-expr \n: Expr -> Expr [,x (variable-assigned x) (primapp car ,x)] [(lambda (,x ...) ,[body]) (let-values ([(xi \nxa xr) (split-vars x)]) (lambda (,xi ...) (let ((,xa (primapp cons ,xr #f)) ...) ,body)))]) (process-command \n: Command -> Command [(set! ,x ,[e]) (primapp set-car! ,x ,e)])) (define split-vars (lambda (vars) \n(if (null? vars) (values () () ()) (let-values ([(ys xas xrs) (split-vars (cdr vars))]) (if (variable-assigned \n(car vars)) (let ([tmp (make-variable tmp)]) (values (cons tmp ys) (cons (car vars) xas) (cons tmp xrs)) \n(values (cons (car vars) ys) xas xrs))))))) Figure 8: Assignment conversion one intermediate language \nformat as opposed to the many intermediate languages encouraged by the Nanopass com\u00adpiler. The nanopass \napproach achieves e.ects similar to these systems but extends them by formalizing the language de.nitions, \nincluding su.cient information in the language de.nitions to allow automated conversion to and from the \nhost language, and separating traversal algorithms from in\u00adtermediate language and pass de.nitions. \n 6. CONCLUSIONS The nanopass methodology supports the decomposition of a compiler into many small pieces. \nThis decomposition sim\u00adpli.es the task of understanding each piece and, therefore, the compiler as a \nwhole. Adding new optimizations is eas\u00adier; there is no need to shoe-horn an analysis or transfor\u00admation \ninto an existing monolithic pass. The methodology also simpli.es the testing and debugging of a compiler, \nsince each task can be tested independently, and bugs are easily isolated to an individual task. The \nnanopass tools enable a compiler student to focus on concepts rather than implementation details while \nhaving the experience of writing a complete and substantial com\u00adpiler. While it is useful to have students \nwrite out all traver\u00adsal and rewriting code for the .rst few passes to understand the process, the ability \nto focus only on meaningful transfor\u00admations in later passes reduces the amount of tedium and repetitive \ncode. The code savings is signi.cant for many passes, including the passes shown in Section 4. With our \nold tools, remove-not was the smallest pass at 25 lines; it is now 7 lines. Similarly, convert-assigned \nwas 55 lines and is now 20 lines. On the other hand, the sizes of a few passes cannot be reduced. The \ncode generator, for example, must explicitly handle every grammar element. At present, the pass expander \ncan .ll in missing details only for passes implementing algorithms that are insensitive to the order \nin which the pass recurs on subforms. This is suitable for most passes, but not for passes that perform \na .ow-sensitive analysis or transformation, such as a live analysis. Such passes must presently be written \nout in full. A focus of our future research will be to extend language de.nitions to allow .ow information \nto be incorporated and to extend pass de.nitions to allow a forward or backward .ow-sensitive traversal \nalgorithm to be speci.ed. Our experience indicates that .ne-grained passes work ex\u00adtremely well in an \neducational setting. We are also inter\u00adested in using the nanopass technology to construct pro\u00adduction \ncompilers, where the overhead of so many repeated traversals of the code may be unacceptable. Another \nfocus of our future research will be to develop a pass combiner that can, when directed, fuse together \na set of passes into a single pass, using deforestation techniques [15] to eliminate rewriting overhead. \nThe .ne-grained nature of the passes may also tend to ex\u00adacerbate phase ordering problems [7, 18]. A \nphase ordering problem occurs when no ordering of a set of improvement passes takes advantage of all \noptimization opportunities, be\u00adcause each optimization may lead to opportunities for some of the others. \nThis problem can often be solved by iterating individual improvement passes until a .xed point is reached, \nbut this solution may result in signi.cant compile-time over\u00adhead. A more e.cient solution is to combine \nmany optimiza\u00adtions, or the analyses that enable them, into a single super optimizer that produces the \nsame or better residual code in fewer iterations due to the synergy among optimizations at the subexpression \nlevel [4, 8, 10, 14, 17]. Unfortunately, su\u00adper optimizers exhibit the undesirable monolithic structure \nthat the nanopass framework is designed to avoid, and, in our experience, adding new optimizations to \nan existing su\u00adper optimizer requires considerably more e.ort than writing them individually. The pass \ncombiner hypothesized above may be able to exploit this synergy and allow improvement passes to be developed \nindependently yet run as a single super optimizer. Acknowledgments Dan Friedman suggested the use of \nsmall, single-purpose passes and contributed to an earlier compiler based on this principle. Erik Hilsdale \ndesigned and implemented the matcher we used to implement micropass compilers. Our input-pattern subpatterns \nwere inspired by Erik s matcher. Jordan Johnson implemented an early prototype of the nanopass framework. \nDipanwita Sarkar was funded by a gift from Microsoft Research University Relations.  7. REFERENCES [1] \nG. Aigner, A. Diwan, D. Heine, M. Lam, D. Moore, B. Murphy, and C. Sapuntzakis. An overview of the SUIF2 \ncompiler infrastructure. Technical report, Stanford University, 2000.  [2] G. Aigner, A. Diwan, D. Heine, \nM. Lam, D. Moore, B. Murphy, and C. Sapuntzakis. The SUIF2 compiler infrastructure. Technical report, \nStanford University, 2000.  [3] J.R. Allen and K. Kennedy. Pfc: A program to convert fortran to parallel \nform. Technical Report MASC-TR82-6, Rice University, Houston, TX, 1982. [4] Cli. Click and Keith D. Cooper. \nCombining analyses, combining optimizations. ACM Transactions on Programming Languages and Systems, 17(2), \n1995. [5] R. Kent Dybvig. The Scheme Programming Language. MIT Press, third edition, 2002. [6] R. Kent \nDybvig, Robert Hieb, and Carl Bruggeman. Syntactic abstraction in Scheme. Lisp and Symbolic Computation, \n5(4):295 326, 1993. [7] Wilf R. LaLonde and Jim des Rivieres. A .exible compiler structure that allows \ndynamic phase ordering. In Proceedings of the ACM SIGPLAN Symposium on Compiler Construction, pages 134 \n139, 1982. [8] Sorin Lerner, David Grove, and Craig Chambers. Composing data.ow analyses and transformations. \nIn Proceedings of the 29th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, pages \n270 282, 2002. [9] N. Nystrom, M. Clarkson, and A. Myers. Polyglot: An extensible compiler framework \nfor Java. In Proceedings of the 12th International Conference on Compiler Construction, volume 2622 of \nLecture Notes in Computer Science, pages 138 152. Springer-Verlag, 2003. [10] Anthony Pioli and Michael \nHind. Combining interprocedural pointer analysis and conditional constant propagation. Research Report \n21532, IBM T. J. Watson Center, 1999. [11] D. Tarditi, G. Morrisett, P. Cheng, C. Stone, R. Harper, \nand P. Lee. TIL: a type-directed optimizing compiler for ML. In Proceedings of the ACM SIGPLAN 1996 conference \non Programming language design and implementation, pages 181 192, 1996. [12] C. van Reeuwijk. Tm: a \ncode generator for recursive data structures. Software Practice and Experience, 22(10):899 908, 1992. \n[13] C. van Reeuwijk. Rapid and robust compiler construction using template-based metacompilation. In \nProceedings of the 12th International Conference on Compiler Construction, volume 2622 of Lecture Notes \nin Computer Science, pages 247 261. Springer-Verlag, 2003. [14] Oscar Waddell and R. Kent Dybvig. Fast \nand e.ective procedure inlining. In Pascal Van Hentenryck, editor, Fourth International Symposium on \nStatic Analysis, volume 1302 of Lecture Notes in Computer Science, pages 35 52. Springer-Verlag, 1997. \n[15] P. Wadler. Deforestation: Transforming programs to eliminate trees. In ESOP 88: European Symposium \non Programming, volume 300 of Lecture Notes in Computer Science, pages 344 358. Springer-Verlag, 1988. \n[16] D.C. Wang, A.W. Appel, J.L. Korn, and C.S. Serra. The zephyr abstract syntax description language. \nIn Proceedings of the USENIX Conference on Domain-Speci.c Languages, pages 213 228, 1997. [17] M. N. \nWegman and F. K. Zadeck. Constant propagation with conditional branches. ACM Transactions on Programming \nLanguages and Systems, 3(2):181 210, 1991. [18] D. Whit.eld and M. L. So.a. An approach to ordering optimizing \ntransformations. In Proceedings of the Second ACM SIGPLAN Symposium on Principles &#38; Practice of Parallel \nProgramming, pages 137 146, 1990. \n\t\t\t", "proc_id": "1016850", "abstract": "Compilers structured as a small number of monolithic passes are difficult to understand and difficult to maintain. Adding new optimizations often requires major restructuring of existing passes that cannot be understood in isolation. The steep learning curve is daunting, and even experienced developers find it hard to modify existing passes without introducing subtle and tenacious bugs. These problems are especially frustrating when the developer is a student in a compiler class.An attractive alternative is to structure a compiler as a collection of many small passes, each of which performs a single task. This \"micropass\" structure aligns the actual implementation of a compiler with its logical organization, simplifying development, testing, and debugging. Unfortunately, writing many small passes duplicates code for traversing and rewriting abstract syntax trees and can obscure the meaningful transformations performed by individual passes.To address these problems, we have developed a methodology and associated tools that simplify the task of building compilers composed of many fine-grained passes. We describe these compilers as \"nanopass\" compilers to indicate both the intended granularity of the passes and the amount of source code required to implement each pass. This paper describes the methodology and tools comprising the nanopass framework.", "authors": [{"name": "Dipanwita Sarkar", "author_profile_id": "81100597039", "affiliation": "Indiana University", "person_id": "PP14206045", "email_address": "", "orcid_id": ""}, {"name": "Oscar Waddell", "author_profile_id": "81100242452", "affiliation": "Abstrax, Inc.", "person_id": "P212431", "email_address": "", "orcid_id": ""}, {"name": "R. Kent Dybvig", "author_profile_id": "81100181541", "affiliation": "Indiana University", "person_id": "PP14073331", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1016850.1016878", "year": "2004", "article_id": "1016878", "conference": "ICFP", "title": "A nanopass infrastructure for compiler education", "url": "http://dl.acm.org/citation.cfm?id=1016878"}