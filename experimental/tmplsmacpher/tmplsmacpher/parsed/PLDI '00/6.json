{"article_publication_date": "05-01-2000", "fulltext": "\n Safety Checking of Machine Code Zhichen Xu Barton P. Miller Thomas Reps Computer Sciences Department \nUniversity of Wisconsin Madison, WI 53706-1685 {zhichen,bart,reps}@cs.wisc.edu Abstract We show how \nto determine statically whether it is safe for untrusted machine code to be loaded into a trusted host \nsystem. Our safety-checking technique operates directly on the untrusted machine-code program, requiring \nonly that the initial inputs to the untrusted program be annotated with typestate information and linear \nconstraints. This approach opens up the possibility of being able to certify code produced by any com\u00adpiler \nfrom any source language, which gives the code producers more freedom in choosing the language in which \nthey write their programs. It eliminates the dependence of safety on the correct\u00adness of the compiler \nbecause the final product of the compiler is checked. It leads to the decoupling of the safety policy \nfrom the language in which the untrusted code is written, and conse\u00adquently, makes it possible for safety \nchecking to be performed with respect to an extensible set of safety properties that are specified on \nthe host side. We have implemented a prototype safety checker for SPARC machine-language programs, and \napplied the safety checker to several examples. The safety checker was able to either prove that an example \nmet the necessary safety conditions, or identify the places where the safety conditions were violated. \nThe check\u00ading times ranged from less than a second to 14 seconds on an UltraSPARC machine. 1 Introduction \nTwo prevailing trends in software development call for techniques to protect one software component from \nanother. The first trend is dynamic extensibility, where a trusted host is extended by import\u00ading and \nexecuting untrusted foreign code. For example, web browsers download plug-ins [33,48]; databases load \ntype-specific This work is supported in part by Department of Energy Grant DE-FG02-93ER25176, NSF grants \nCDA-9623632, EIA\u00ad9870684, CCR-9625667, CCR-9619219, U.S.-Israel Binational Science Foundation grant 96-0037, \na Vilas Associate Award form the University of Wisconsin, and DARPA contract N66001-97-C\u00ad8532. The U.S. \nGovernment is authorized to reproduce and dis\u00adtribute reprints for Governmental purposes notwithstanding \nany copyright notation thereon. Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that copies are not made or distributed \nfor profit or commercial advantage and that copies bear this notice and the full citation on the first \npage. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior \nspecific permission and/or a fee. PLDI 2000, Vancouver, British Columbia, Canada. Copyright 2000 ACM \n1-58113-199-2/00/0006...$5.00. extensions [13,47]; operating systems load customized policies [2,11,23,34,38,41,50] \nand performance-measurement code [50]. There are even proposals for loading application-specific policies \ninto Internet routers [52]. Certification of the safety of untrusted code is crucial in these domains. \nThe second trend is component\u00adbased software development, where software components pro\u00adduced by multiple \nvendors are used to construct a complete appli\u00adcation [6] (e.g., COM [21]). The component-based software\u00addevelopment \nmodel improves both software reusability and pro\u00adductivity. Because the software components can come \nfrom differ\u00adent sources, proper protection among software components is essential. In this paper, we \nshow how to determine statically whether it is safe for untrusted machine code to be loaded into a trusted \nhost system. In contrast to work that enforces safety by restricting the things that can be expressed \nin a source language (e.g., safe lan\u00adguages, certifying compilers [31], and typed-assembly languages \n[25,26,28]), we believe that safe code can be written in any source language and produced by any compiler, \nas long as nothing unsafe is expressed in the machine code. This philosophical dif\u00adference has several \nimplications. First, it gives the code producer more freedom in choosing an implementation language. \nInstead of building a certifying compiler for each language, we can certify code produced by an off-the-shelf \ncompiler. Second, it leads to the decoupling of a safety policy from the language in which the untrusted \ncode is written. This makes it possible for safety check\u00ading to be performed with respect to an extensible \nset of safety properties that are specified on the host side. In short, the most important, high-level \ncharacteristics of our safety-checking technique are (i) It operates directly on binary code; (ii) It \nprovides the ability to extend the host at a very fine\u00adgrained level, in that we allow the foreign code \nto manipulate the internal data structures of the host directly; (iii) It enforces a default collection \nof safety conditions to prevent array out-of\u00adbounds violations, address-alignment violations, uses of \nuninitial\u00adized variables, null-pointer dereferences, and stack-manipulation violations, in addition to \nproviding the ability for the safety crite\u00adrion to be extended according to an access policy specified \nby the host. The essence of our approach is to recover source-level type information (more precisely, \ntypestate information) based on a small amount of annotation about the initial inputs to the untrusted \ncode, and then to apply some techniques that were originally developed for program verification to determine \nwhether the untrusted code is safe. The initial annotation is in the form of typestates and linear constraints \n(i.e., linear equalities and inequal\u00adities that are combined with ., ., \u00ac, and the quantifiers . and \n.). Our analysis uses typestates (as opposed to types) because the con\u00addition under which it is safe \nto perform an operation is a function of not just the types of the operation s operands, but also their \nstates. For example, it is safe to write to a location that stores an uninitialized value, but it is \nunsafe to read from it. Typestates differ from types by providing information at a finer granularity. \nMore\u00adover, typestate checking [44,45] differs from traditional type checking in that traditional type \nchecking is a flow-insensitive analysis, whereas typestate checking is a flow-sensitive analysis. Typestates \ncan be related to security automata [3]. In a security automaton, all states are accepting states; the \nautomaton detects a security-policy violation whenever read a symbol for which the automaton s current \nstate has no transition defined. It is possible to design a typestate system that captures the possible \nstates of a security automaton (together with a security-violation state). Typestate checking provides \na method, therefore, for statically assessing whether a security violation might be possible. To perform \nsafety checking of machine-language programs, the issues we face include: (i) the design of a language \nfor specify\u00ading policies; (ii) the inference of a typestate at each program point; (iii) overload resolution \nof certain machine-language instructions;1 and (iv) synthesis of loop invariants. We use source-level \ntypestates to describe the state of the host when untrusted code is to be invoked. A safety policy specifies \nthe data that can be accessed and the functions (methods) that can be called by a piece of untrusted \ncode. We extend typestate checking to infer a typestate for each program point, and to perform overload \nresolution. We use the induction-iteration method [49] to synthesize loop invariants. The main contributions \nof this paper are as follows: Our technique opens up the possibility of being able to certify object \ncode produced by off-the-shelf compilers (independent of both the source language and the compiler). \nWe require only that the inputs to the untrusted code be annotated with typestate information and linear \nconstraints.  The technique is extensible: (i) in addition to a default collec\u00adtion of safety conditions \nthat are always checked, additional safety conditions to be checked can be specified by the host; (ii) \ntypestates can be related to security automata; this makes extending our technique to perform security \nchecking natural.  We extend the notion of typestate in several ways: (i) we use typestates to describe \nthe state information of abstract loca\u00adtions in an abstract storage model; (ii) we extend typestates \nto include access permissions (which are used to specify the extent to which untrusted code is allowed \nto access host resources); (iii) in addition to using typestates to distinguish initialized values from \nuninitialized ones, we also use typestates to track pointers.  Finally, by focusing only on enforcing \nfine-grained memory protection, we are able to use a decidable logic for expressing safety conditions \nand simple heuristics for synthesizing loop invariants. We wish to stress that, although we use techniques \noriginally developed for verification of correctness, we are not trying to prove either total or partial \ncorrectness [9,12]. Safety checking is less ambitious than verification of correct\u00adness.  We have implemented \na prototype safety checker for SPARC machine-language programs. We applied the safety checker to sev\u00aderal \nexamples (ranging from code that contains just a few branches, to code that contains nested loops, and \nto code with function and method calls). The safety checker was able to either prove an example met the \nnecessary safety conditions, or identify the places where the safety conditions were violated, in times \nrang\u00ad 1. Instructions such as add and ld are overloaded; for exam\u00adple, add can be adding two integers \nor adding the base address of an array and an array index. ing from less than a second to 14 seconds \non an UltraSPARC machine (see Section 6). Contrary to our initial intuition, we observed that certain \ncompiler optimizations, such as loop-invari\u00adant code motion and improved register-allocation algorithms, \nactu\u00adally make the task of safety checking easier. The remainder of the paper is organized as follows: \nSection 2 describes the default safety properties that we enforce and the notion of a host-specified \nsafety policy. Section 3 gives an over\u00adview of the safety-checking analysis. Section 4 describes the \nbasis of the safety-checking analysis. Section 5 describes certain phases of the safety-checking analysis \nin greater detail. Section 6 presents our initial experience. Section 7 discusses related work. Section \n8 discusses the limitations of our technique.  2 Safety Properties and Policies When untrusted code \nis to be imported into a host system, we need to specify acceptable behaviors for the untrusted code. \nThese behavior specifications take the form of safety conditions that include a collection of default \nsafety conditions and host-specified access policies. The default safety conditions check for array out\u00adof-bounds \nviolations, address-alignment violations, uses of unini\u00adtialized values, null-pointer dereferences, and \nstack-manipulation violations. The inputs to our safety-checking analysis include the untrusted code, \na host-typestate specification, an invocation speci\u00adfication, and a safety policy. All inputs except \nfor the untrusted code are provided by the host. A host-typestate specification includes (i) a data \naspect that describes the type and the state of host data before the invoca\u00adtion of the untrusted code, \nand (ii) a control aspect that pro\u00advides safety pre- and post-conditions for calling host functions and \nmethods (in terms of the types and states of the parameters and return values, and linear constraints \non them).  An invocation specification provides information about the initial values passed to the untrusted \ncode when it is invoked by the host.  The host-specified access policy specifies the host data that \ncan be accessed by a piece of untrusted code, and the host functions (methods) that can be called.  \nIn our model, we view any addresses passed to a piece of untrusted code as doors into the host data region. \nA safety policy controls the memory locations (resources) that are accessible by specifying the pointer \ntypes that can be followed. For the memory locations reachable, the safety policy specifies the ways \nthey can be accessed in terms of the types of the memory locations and their contents. A policy is specified \nby (i) a classification of the memory locations into regions, and (ii) a list of triples of the form \n[Region : Category : Access Permitted]. A Region can be as large as an entire address space or as small \nas a single variable. The Category field is a set of types or aggregate fields. The Access field can \nbe any subset of r, w, f, x, and o, meaning readable, writable, follow\u00adable, executable, and operable, \nrespectively. In our model, r and w are properties of a location, whereas f, x, and o are properties \nof the value stored in a location. The access permission f is introduced for pointer-typed values to \nindicate whether the pointer can be derefer\u00adenced. The access permission x applies to values of type \npointer to function (i.e., values that hold the address of a function) to indi\u00adcate whether the function \npointed to can be called by the untrusted code. The access permission o includes the rights to examine \n, to copy , and to perform other operations not covered by x and f. Untrusted Code 1: mov %o0,%o2 ! \nmove %o0 into %o2 2: clr %o0 ! set %o0 to zero 3: cmp %o0,%o1 ! compare %o0 and %o1 4: bge 12 ! branch \nto 12 if %o0= %o1 5: clr %g3 ! set %g3 to zero 6: sll %g3, 2,%g2 ! %g2 = 4 x %g3 7: ld [%o2+%g2],%g2 \n! load from address %o2+%g2 8: inc %g3 ! %g3 = %g3 + 1 9: cmp %g3,%o1 ! compare %g3 and %o1 10:bl 6 ! \nbranch to 6 if %g3< %o1 11:add %o0,%g2,%o0 ! %o0= %o0+ %g2 12:retl 13:nop  Host Typestate Safety Policy \nInvocation e: <int, initialized, ro> arr: <int [n], {e}, rfo> {n=1} arr is an integer array of size n, \nwhere n=1. e is an abstract location that summarizes all ele\u00adments of arr. V = {e, arr} [V : int : ro] \n[V : int [n] : rfo] arrand eare in the V region. All integers in the V region are readable and operable. \nAll base addresses to an integer array of size nin the V region are readable, oper\u00adable, and followable. \n%o0. arr %o1 . n arrand the size of arrwill be passed through the registers %o0, and %o1, respectively. \n Figure 1: A Simple Example: Summing the Elements of an Integer Array To get a feel for what a safety \npolicy looks like, suppose that a user is asked to write an extension (as a piece of untrusted code) \nthat finds out the lightweight process on which a thread is running, and suppose that information about \nthreads is stored in the host address space in a linked list defined by the structure thread struct thread \n{ int tid; int lwpid; ... struct thread * next; }; The following policy allows the extension to \nread and examine the tid and lwpid fields, and to follow only the next field (H stands for Host Region \n, which is the region in which the list of threads is stored): [H : thread.tid, thread.lwpid: ro] [H \n: thread.next: rfo] The above model can be used to specify a variety of different safety policies. For \nexample, we can specify something roughly equivalent to sandboxing [55]. The original sandboxing model \npar\u00adtitions the address space into protection domains, and modifies a piece of untrusted code so that \nit accesses only its own domain. In our model, sandboxing boils down to allowing untrusted code to access \nmemory only via valid addresses in the untrusted data region, but otherwise to examine, and operate on \ndata items of any type. Because an address of a location in the host region cannot be dereferenced, side-effects \nare confined to the untrusted region. Our approach differs from sandboxing in that it is purely static, \nand it does not make any changes to the untrusted code. While sandboxing works well in situations where \nit is appro\u00adpriate to limit memory accesses to only the untrusted data region, forbidding access to all \ndata in the host region is often too draco\u00adnian a measure. For instance, access to the host data region \nis nec\u00adessary for applications as simple as performance instrumentation (e.g., to read statistics maintained \nby the host environment). In our model, more aggressive policies are defined by allowing simple reads \nand writes to locations in the host data region, but forbidding pointers to be followed or modified. \nWe can go even further by specifying policies that permit untrusted code to follow certain types of valid \npointers in the host data region in order to traverse linked data structures. We can even specify more \naggressive poli\u00adcies that permit untrusted code to change the shape of a host data structure, by allowing \nthe untrusted code to modify pointers. What we have presented here is a simplified view of a safety policy. \nIn reality, a safety policy can also include a safety postcon\u00addition (typestates and linear constraints) \nfor ensuring that certain invariants defined on the host data are restored by the time control is returned \nto the host.  3 Overview of Safety-Checking Analysis Our goal is to verify that untrusted code (i) obeys \nthe collection of default safety conditions, and (ii) accesses data and calls host func\u00adtions only in \na manner that is consistent with a given safety policy. To do this, we introduce an abstract storage \nmodel, use the default safety conditions and host-specified access policy to attach a safety predicate \nto each instruction, use static analysis to determine an approximation to the contents of memory locations \nat each point in the program, and check whether each instruction obeys the corre\u00adsponding safety predicate. \nThe abstract storage model includes the notion of abstract locations and typestates. A typestate describes \nthe type, state, and access permissions of the value stored in an abstract location. We divide the safety-checking \nanalysis into five phases: prep\u00adaration, typestate propagation, annotation, local verification, and global \nverification. We now illustrate these phases informally by means of a simple example. Figure 1 shows \na piece of untrusted code (in SPARC assembly language) that sums the elements of an integer array. It \nalso shows the host-typestate specification, the safety policy, and the invocation specification. The \nhost-typestate specification states that arr is an integer array of size n, where n = 1. We have used \na single abstract loca\u00adtion e to summarize all elements of the array arr. The safety pol\u00adicy states that \narr and e are in the V region, that all integers in the V region are readable and operable, and that \nall base addresses to an integer array of size n in the V region are readable, operable, and followable. \nThe invocation specification states that arr and the size of arr will be passed through the registers \n%o0and %o1, respectively. The code uses three additional registers, %o2, %g2, and %g3. Phase 1: Preparation \ntakes the host-typestate specification, the safety policy, and the invocation specification, and translates \nthem into initial annotations that consist of linear constraints and the typestates of the inputs. For \nthe example given in Figure 1, the initial annotations are shown in Figure 2. The fact that the address \nof arr is passed via register %o0 is described in the second line in column 1. The fact that the size \nof arr is passed via the register %o1 is captured by the linear constraint n=%o1 . Note that %o0 and \n%o1 both have the r and w access permissions. These refer to the registers them\u00ad Initial Annotations \nInitial Typestate Initial Constraints e:<int, initialized, ro> %o0:<int[n], {e}, rwfo> %o1:<int, initialized, \nrwo> n= 1 . n=%o1 Figure 2: Initial Annotations selves (i.e., the untrusted code is permitted to read \nand change the value of both registers). However, array arr cannot be overwritten because the access \npermission for e, which acts as a surrogate for all elements of arr, does not have the w permission. \nPhase 2: Typestate propagation takes the untrusted code and the initial annotations as inputs. It annotates \neach instruction with an abstract representation of the memory contents that characterize the state before \nthe execution of that instruction. For our example, this phase discovers that the instruction at line \n7 is an array access, where %o2 holds the base address of the array and %g2 represents the index. Phase \n3: Annotation takes as input the typestate information discovered in Phase 2, and traverses the untrusted \ncode to annotate each instruction with local and global safety preconditions and with assertions. The \nlocal safety preconditions are conditions that can be checked using typestate information alone. The \nassertions are facts that can be derived from the results of typestate propaga\u00adtion. For our example, \nthe assertions, local safety preconditions, and global safety preconditions for the instruction at line \n7 are summarized in Figure 3. Phase 4: Local verification checks the local safety precondi\u00adtions. (In \nour example, the local safety preconditions are all true at line 7.) Local Safety Global Safety Assertions \nPreconditions Preconditions %o2 is the address of an integer array %o2 mod 4 = 0 %o2. NULL e is readable; \nArray bounds checks %g2 is writable; %g2= 0 . %g2< 4n %o2 is followable, .%g2 mod 4 = 0 and operable \nAlignment and null-pointer checks %o2. NULL. (%o2+ %g2) mod 4 = 0 Figure 3: Assertions and Safety Preconditions \nfor Line 7 Phase 5: Global verification attempts to verify the global safety preconditions using program-verification \ntechniques. In the pres\u00adence of loops, we use the induction-iteration method to synthesize loop invariants. \nIn proving that at line 7 index %g2 is less than the upper bound n, our safety checker automatically \nsynthesizes the loop invariant n> %g3. n= %o1 .  4 The Basis of Safety-Checking Analysis This section \ndescribes (i) the abstract storage model used in our safety-checking analysis, (ii) an abstract operational \nsemantics for SPARC machine-language instructions, and (iii) how safety predi\u00adcates are attached to instructions. \n 4.1 An Abstract Storage Model The abstract storage model we use includes the notion of abstract locations \nand typestates. An abstract location summarizes a set of physical locations so that the safety-checking \nanalysis has a finite\u00adsize domain to work over. (In general, the number of concrete acti\u00advation records \nis unbounded in the presence of recursion, as are the number of concrete objects allocated in a loop \nand the size of con\u00adcrete linked data-structures.) An abstract location has a name, a size, an alignment, \nand optional attributes r and w to indicate whether the location is readable and writable, respectively. \nA typestate records properties of the values stored in abstract locations. A typestate is defined by \na triple <type, state, access>. We define a meet operation .. on typestates so that typestates form a \nmeet semi-lattice. The meet of two typestates is defined as the meet of their respective components. \n t :: ground Ground types | abstract Abstract types | t [n] Pointer to the base of an array of type t \nof size n | t (n] Pointer into the middle of an array of type t of size n | t ptr Pointer to t | s {m1, \n..., mk} struct | u {|m1, ..., mk|} union | (t1, ..., tk) . t Function m:: (t, l, i) Member labeled \nl of type t at offset i ground:: int8| uint8| int16| uint16|int32| ... Figure 4: A Simple Type System \nt stands for type, and m stands for a struct or union member. Our type system is based on that of Siff \net al [42], with the addition of abstract types, pointers into the middle of arrays, and alignment and \nsize constraints on types. (See Figure 4; alignment and size constraints on types are not shown.) The \ntype t(n] denotes a pointer that points somewhere into the middle of an array of type t of size n. The \nmeet operation on types are defined as follows. The meet of two different non-pointer types is .t.2 The \nmeet of two dif\u00ad ferent pointer types, or the meet of a pointer type and a non-pointer type is .t. The \nmeet of type t[n] and type t(n] is t(n], whereas the meet of t[n] and t[m] or the meet of t(n] and t[m] \nis .t if m . n. The state component of a typestate captures the notion of an object of a given type being \nin an appropriate state for some opera\u00adtions but not for others. The state lattice contains a bottom \nelement .s that denotes an undefined value of any type. Figure 5 illustrates selected elements of the \nstate lattice. Since we also use the state descriptors to track abstract locations that represent pieces \nof stack- and heap-allocated storage, they resemble the storage-shape graphs of Chase et al [5]. An access \npermission is either a subset of {f, x, o}, or a tuple of access permissions. If an abstract location \nstores an aggregate, its access permission will be a tuple of access permissions, with the elements of \nthe tuple denoting the access permissions of the respective aggregate fields. The meet of two access-permission \nsets is their intersection. The meet of two tuples of access permis\u00adsions is given by the meet of their \nrespective elements. 2. Our implementation also incorporates a notion of subtyping for ground types. \nThis makes the analysis more precise when dealing with operations such as load byte and load half word \n. Scalars Pointers {m} {null} [it1] [it2] {m, null } [up] [ut1][ut2] .s  Figure 5: A Portion of the \nState Lattice For a scalar type t, its state can be [ut] or [it], which denote uninitialized and initialized \nvalues, respectively. For a pointer type p, its state can be [up], which is the state of an uninitialized \npointer, or P, a non-empty set of abstract locations referenced, where one of the elements of P can be \nnull. For sets P1 and P2, we define P1 = P2 iff P2 . P1. For an aggregate type G, its state is given \nby the states of its fields. Aggregates <[it1], [it2]> <[it1], [ut2]> <[ut1], [it2]> <[it1], . s> \n<[ut], [ut]> < .s, [it2]>  <[ut1], .s> < .s, [ut2]> The reader may be puzzled why a safety policy is \ndefined in terms of five kinds of access permissions (r, w, f, x, and o), whereas typestates have only \nthree kinds (f, x, and o). The reason is that f, x, and o are properties of a value, whereas r and w \nare properties of a location. Typestates capture properties of values. Policies specify the r and w permissions \nof abstract locations, as well as f, x, and o permissions of their values. The typestate lattice also \nincludes a top element T. In the remainder of the paper, we use absLoc to denote the set of all abstract \nlocations, and the symbols l and m to denote individ\u00adual abstract locations. We use Size(l), Align(l), \nto denote the size, and alignment, respectively, of abstract location l. We call an abstraction location \nthat summarizes more than one physical loca\u00adtion a summary location. A register is always readable and \nwrit\u00adable, and has an alignment of zero. A constant always has access permission o.  4.2 An Abstract \nOperational Semantics for SPARC Machine Instructions An abstract store is given by a total map M: absLoc \n. typestate. We define the abstract operational semantics of a SPARC machine instruction as a transition \nfunction R: M.M. We use T(l), S(l), and A(l) to denote the type, state, and access component of the typestate \nof abstract location l, respectively. 4.2.1 Overload Resolution We determine an appropriate typestate \nfor each abstract location at each program point by finding the greatest fixed point of a set of typestate-propagation \nequations (see Section 4.2.2). Overload res\u00adolution of instructions such as add and ld falls out as a \nby-prod\u00aduct of this process: The type components of the typestates obtained for the arguments of overloaded \ninstructions let us identify whether a register holds a scalar, a pointer, or the base address of an \narray (and hence whether an instruction such as add %o0,%g2,%o0 represents the addition of two scalars, \na pointer indirection, or an array-index calculation). To achieve this, we define the abstract operational \nsemantics of SPARC machine instructions to be strict in T. Consequently, during typestate checking, propagation \nof information through the instructions of a loop is delayed until a non-T value arrives at the loop \nentrance. One artifact of this method is that each occurrence of an over\u00adloaded instruction is resolved \nto just a single usage kind (e.g., sca\u00adlar addition, pointer indirection, or array-index calculation). \nWe call this the single-usage restriction. We believe that this restriction does not represent a significant \nlimitation in practice because we are performing typestate checking (which is flow sensitive). For example, \ntypestate checking allows an instruction such as add %o0,%g2,%o0 to be resolved as a pointer indirection \nat one occurrence of the instruction, but as an array-index calculation at a different occurrence. Operation \nAssumption Type-Propagation Rule State-Propagation Rule Access-Propagation Rule 1 add rs, Opnd, rd 2 \nScalar add 1. T (rd) = T(rs) .. T(Opnd). 2. for l .rd, T (l) = T(l). 1. S (rd) = S(rs) .. S(Opnd). 2. \nfor l .rd, S (l) = S(l). 1. A (rd) = A(rs) n A(Opnd). 2. for l .rd, A (l) = A(l). Array-index calculation \nT(rs) = t [n] 1. T (rd) = t (n]. 2. for l .rd, T (l) = T(l). 1. S (rd) = S(rs). 2. for l .rd, S (l) = \nS(l). 1. A (rd) = A(rs). 2. for l .rd, A (l) = A(l). 3 st rs, [ra+n] Store to an aggregate field Let \nF = {s.\u00df | s . S(ra), \u00df . lookUp(T(s), n, 4)} 1. if F={l}, if l is not a summary location, T (l)=T(rs); \notherwise T (l)=T(rs) .. T(l). 2. if | F | > 1, for l . F, T (l)=T(rs) .. T(l). 3. for l . F, T (l) = \nT(l). 1. if F={l}, if l is not a summary location, S (l)=S(rs); otherwise S (l)=S(rs) .. S(l). 2. if \n| F |>1, for l . F, S (l)=S(rs) .. S(l). 3. for l . F, S (l) = S(l). 1. if F = {l}, if l is not a summary \nlocation, A (l)=A(rs); otherwise A (l) =A(l) n A(rs). 2. if | F |>1, for l . F, A (l) =A(l) n A(rs). \n3. for l . F, A (l) = A(l). Table 1: Propagation of Type, State, and Access information 4.2.2 Propagation \nof Type, State, and Access Informa\u00ad Operation Assumptions Local Safety Predicates Global Safety Predicates \n1 add rs, Opnd, rd Scalar add operable(rs) . operable(Opnd) 2 add rs, Opnd, rd Array-index calculation \nT(rs) = t [n] operable(rs) . operable(Opnd) null . S(rs) . inbounds(sizeof(t), 0, n, Opnd) 4 st rs, [ra+n] \nStore to an aggregate field Let F = {s.\u00df | s . S(ra), \u00df . lookUp(T(s), n, 4)} followable(ra) . operable(ra) \n. F . \u00d8 . forall l . F, assignable(rs, l) null . S(ra) . forall a . S(ra), align(Align(a)+n, 4) . sizeof(T(rs))=4 \n Table 2: Attachment of Safety Properties tion For the sake of brevity, Table 1 shows the rules for propagating \ntype, state, and access information only for two different kinds of uses of the add instruction (scalar \nadd and array-index calcula\u00adtion) and for storing to an aggregate field. rs, ra, and rd are regis\u00ad ters, \nand Opnd is either an integer constant n or a register. We use l . rd to denote l .(absLoc - {rd}), and \nuse T(l) and T (l) to denote the types of abstract location l before and after the execution of an instruction, \nrespectively. We define S(l), S (l), A(l), and A (l) simi\u00adlarly. We use \u00df to refer to a (possibly empty) \nsequence of field names. The function lookUp takes a type and two integers n and m as input; it returns \nthe set of fields that are at offset n and of size m, or \u00d8 if no such fields exist. 1. The typestate-propagation \nrules for scalar-add state that after the execution of the add instruction, the typestate of rd is the \n meet of those of rs and Opnd before the execution, and the typestate of all other abstract locations \nin absLoc remain unchanged. 2. For an array-index calculation, the type of the destination reg\u00adister \nbecomes t(n] , where t is the type of an array ele\u00adment. The type t(n] indicates that rd could point \nto any element in the array. As to the state-propagation rule, at present we use a single abstract location \nto summarize the entire array; thus the state of the destination register is the same as that of the \nsource register. 3. The typestate-propagation rules for storing to an aggregate field are divided into \ntwo cases, depending on whether strong or weak update is appropriate. The abstract-location set F gives \nthe set of locations into which the st instruction may store. The pointer ra+n must point to l (. F), \nif |F|=1 and l  is not a summary location. In this case, l receives the typestate of the source register. \nThe pointer ra+n may point to the locations in F, if |F|>1, or F={l} and l is a summary location. In \nthis case, each possible destination receives the meet of the typestate before the operation and the \ntypestate of the source register.   4.3 Attachment of Safety Predicates Phase 3 of the safety-checking \nanalysis uses the default safety con\u00additions, the host-specified access policy, and the results of overload \nresolution to attach a collection of safety predicates to each instruction. These safety predicates are \ndivided into local safety predicates and global safety predicates, depending on whether or not the predicates \ncan be validated using typestate information alone. Table 2 summarizes the safety predicates for the \ntwo cases of add and the one case of st that were described earlier. 1. For scalar add, the safety predicate \nspecifies that uninitial\u00adized values must not be used. The predicate readable(l) is true iff l is readable, \nand the predicate operable is true iff o . A(l) and S(l) . {[uT(l)], .s}. 2. The safety predicates for \nan array-index calculation state that and Opnd must both be readable and operable, and the rs index \nmust be within the bounds of the array. The predicate inbounds(size, low, high, i) is true iff low \u00d7 \nsize = i < high \u00d7 size, and i mod size = 0.  3. The safety predicates for st state that (i) ra must \nbe follow\u00ad  able and n must be a valid index of a field of the right size; (ii) must be non-null, and \nthe address ra+n must be properly ra aligned. The predicate followable(l) is true iff f..(l), and T(l) \nis a pointer type; the predicate assignable(m, l) is true iff read\u00adable(m), writable(l), and (T(l)=T(m), \nAlign(l) mod Align(T(m))=0 and sizeof(T(m))=Size(l)) all hold; the predi\u00adcate align(A, n) is true iff \n. a st. A = na.  5 Elaboration of Phases 2 and 5 of Safety-Checking Analysis In this section, we expand \nupon Phases 2 and 5 of the safety-check\u00ading analysis of the array-summation example introduced in Section \n3. 5.1 Phase 2 Typestate Propagation The typestate-propagation phase works on interprocedural control\u00adflow \ngraphs, where the nodes in the graph represent instructions and the edges represent control flow in the \nusual way. To create a safe approximation of the program state before and after each node, each node \nhas two total maps (each of type absLoc . typestate) representing abstract stores. The algorithm for \ntypestate propagation is a standard worklist-based algorithm. It starts with the map .l.T at all program \npoints except for the entry node. The map for the entry node incorporates the initial annotations gener\u00adated \nduring Phase 1 (see Figure 2); the abstract locations for which there are no initial annotations are \ninitialized with the typestate <.t, .s, \u00d8>. Initially, the first instruction of the untrusted code is \nplaced on the worklist. An instruction is chosen from the worklist and exam\u00adined. The typestates of the \nabstract locations at the entry to the examined instruction become the meet of the corresponding typestates \nat the exits of the instruction s predecessors. The instruction is interpreted abstractly using the new \ntypestates. This may cause the abstract store associated with the exit of the instruc\u00adtion to change. \nIn that case, each instruction that is a successor of the examined instruction is added to the worklist. \nThis process is repeated until the worklist is empty.  Figure 6: Results of Typestate Propagation Figure \n6 shows the results of typestate propagation applied to our running example. The right column shows the \ninstructions. The left column shows the abstract store before the execution of the corresponding instruction. \nLines 6 to 11 correspond to the loop. Initially, %o0 holds the base address of the integer array arr \n(whose elements are summarized by e), and %o1 holds the size of the array. Typestate checking is initiated \nby placing the mov instruction at line 1 on the worklist. Abstract interpretation of the mov instruction \nat line 1 sets the contents of %o2 to point to e. Because the typestate of %o2 has changed, the instruction \nat line 2 is placed on the worklist. The interpretation of the clr instruction at line 2 sets the contents \nof %o0 to 0. This process continues until the worklist becomes empty. For line 7, the results show that \n%o2 holds the base address of an integer array and that %g2 is an inte\u00adger (and hence must be an index). \n 5.2 Phase 5 Verification of Global Safety Pre\u00adconditions The fifth phase verifies the global safety \npreconditions using pro\u00adgram-verification techniques. This phase involves (i) generating verification \nconditions (VCs), and (ii) verifying the VCs using a theorem prover. Unlike standard techniques for program \nverifica\u00adtion, in which one monolithic VC is created containing all proper\u00adties to prove, we check the \nvalidity of the global safety preconditions in a demand-driven fashion, and verify the condi\u00adtions one \nat a time. Since array-bounds, null-pointer, and address-alignment requirements can usually be represented \nas linear equalities and inequalities, our theorem prover is based on the Omega Library [17]. The Omega \nlibrary represents relations and sets as Presburger formulas, which are formulas constructed by combining \naffine constraints on integer variables with the logical operations \u00ac, ., and ., and the quantifiers \n. and .. The affine constraints can be either equality or inequality constraints [17]. Presburger formulas \nare decidable. For more details on the Omega Library, see Kelly et al [17] and Pugh et al [35,36,37]. \nWhen the untrusted code involves loops, an additional step is needed to synthesize loop invariants. In \nour system, the synthesis of loop invariants is attempted by means of the induction-iteration method \n[49]. We have extended the induction-iteration method to synthesize loop invariants for natural loops \n(and also to work on machine-language programs). 5.2.1 Induction Iteration Method The induction-iteration \nmethod uses the weakest liberal precondi\u00adtion (wlp) as a heuristic for generating loop invariants. The \nweak\u00adest liberal precondition of statement S with respect to postcondition Q, denoted by wlp(S, Q), is \na condition R such that if statement S is executed in a state satisfying R, (i) Q is always true after \nthe termination of S (if S terminates), and (ii) no condition weaker than R satisfies (i). A weakest \nliberal precondition differs from a weakest precondition in that a weakest liberal precondition does \nnot guarantee termination. Since our technique works on machine language programs, we have extended the \ninduction-itera\u00adtion method to work on reducible control-flow graphs [29], and partition the control-flow \ngraph into code regions that are either cyclic (natural loops) or acyclic. The method for generating \nwlps for non-conditional instruc\u00adtions is the same as those for generating weakest preconditions [9]. \n We compute the wlp of load and store instructions based on Mor\u00adris s general axiom of assignment [24], \nwhich provides a general framework for computing weakest precondition for assignments to pointer-typed \nvariables. To compute the wlp for an acyclic code region, the standard technique for verification generation \nis used. To compute the wlp for a natural loop, we define W(0) as the wlp generated by back-substituting \nthe postcondition Q to be proved until the entry of a loop is reached, i.e., W(0) = wlp(loop\u00adbody, Q), \nand define W(i+1) as wlp(loop-body, W(i)). The wlp of the loop is the formula .i=0W(i). We use L(j) to \ndenote .j= i = 0W(i). The induction-iteration method attempts to find an L(j) that is both (i) true on \nentry to the loop and (ii) a loop invariant (i.e., L(j) implies wlp(loop-body, L(j))). Suzuki and Ishihata \nshow that this can be established by showing: L(j) is true on entry to the loop, and (Inv.0(j)) L(j) \n. W(j+1) (Inv.1(j)). Their argument runs as follows [49]: 1. From the assumption that L(j) implies W(j+1), \nwe know that .j= i= 0W(i) implies .j = i = 0W(i+1). 2. Next, we observe that .j=i=0W(i+1) is equivalent \nto wlp(loop-body, L(j))  .j= i = 0 W(i+1) = . j= i = 0wlp(loop-body, W(i)) = wlp(loop-body, . j= i = \n0W(i)) = wlp(loop-body, L(j)) 1: Induction_Iteration() : SUCCESS | FAILURE { 2: i=0; Create formula \nW(0); //Try L(-1) 3: while (i < MAX_NUMBER_OF_ITERATIONS) { 4: switch (Theorem_prover((.i-1 = k = 0W \n(k)) . W (i))) { //inv.1(i-1) 5: TRUE: return SUCCESS; 6: OTHERWISE:{ //Try L(i) 7: switch (Theorem_prover(wlp(<on-entry-to\u00adloop>,W(i)))) \n{//inv.0(i) 8: TRUE: W(i+1)=wlp(loop-body, W(i)); 9: i=i+1; 10: OTHERWISE:return FAILURE; 11: } 12: } \n13: } 14:} 15:} Figure 7: The Basic Induction-Iteration Algorithm The induction iteration method, in \nessence, iterates the fol\u00adlowing steps: create the expression L(j) as the current candidate for the loop \ninvariant; generate VCs for (Inv.0(j)) and (Inv.1(j)); attempt to verify the VCs using a theorem prover. \nFigure 7 shows the basic induction-iteration algorithm taken from Suzuki and Ishi\u00adhata (rewritten in \npseudo code) [49]. The reader may be puzzled why the algorithm first tests for inv.1(i-1), and then tests \nfor inv.0(i). This is because the test inv.0(i-1) that matches the test for inv.1(i-1) is performed in \nthe previous iteration. In the case of L(-1), inv.0(-1) holds vacuously because L(-1)=.-1 = i = 0W(i) \n= true. We have made several enhancements to the basic induction\u00aditeration algorithm. The first enhancement \nis the ability to deal with multiple loops. To ensure that the induction-iteration algo\u00adrithm will terminate \nin the presence of nested loops, we extended the basic induction-iteration algorithm to treat an inner \nloop differ\u00adently when trying to verify Inv.0. In the case of computing a wlp for an inner loop due to \nthe synthesis of a loop invariant for an outer loop, instead of naively testing that W(i) of the inner \nloop is true on entry to the loop, we record the current trial invariant L(j) of the outer loop, and \nfirst try to verify that L(j) implies W(i). Second, procedure calls complicate the induction-iteration \nmethod in three ways: (i) handling a procedure call when perform\u00ading a back-substitution, (ii) reaching \nthe procedure entry before we can prove or disprove a condition, and (iii) recursion. To handle procedure \ncalls during back-substitution, we simply walk through the body of the callee as through it is inlined \nin the caller; this will generate a wlp for the callee function with respect to the postcondi\u00adtion that \nis propagated to the callsite. When we reach the entry of a procedure, we check that the conditions are \ntrue at each callsite by using these conditions as postconditions to be proven at each of the caller. \nTo simplify matters, our present system detects and rejects recursive programs. In principle, we could \nuse the induction-itera\u00adtion method to synthesize invariant entry conditions for recursive functions \nas we do for loops. Third, certain conditionals in a loop can sometimes weaken L(j) to such an extent \nthat it is prevented from becoming a loop\u00adinvariant. To address this problem, we strengthen L(j) by comput\u00ading \nthe disjunctive normal form of wlp(loop-body, W(i-1)), and try each of its disjuncts as W(i) in turn. \nWe rank the candidates according to a simple heuristic and test the potential candidates for W(i) using \na breadth-first strategy. Fourth, the breath-first strategy of the extended induction-iter\u00adation algorithm \nalso incorporates a technique called generaliza\u00adtion, also introduced by Suzuki and Ishihata [49]. The \ngeneralization of a formula f is defined as \u00ac(elimination(\u00acf)). Elimination uses the Fourier-Motzkin \nvariable-elimination method to eliminate variables from the set of variables in \u00acf to generate a simplified \nset of constraints that has the same integer solution as f. If there are several resulting generalizations, \nthen each of them in turn is chosen to be the generalized formula. Fifth, the conditionals in the program \ncan cause the formula under consideration to blow-up in size exponentially during VC generation. To reduce \nthis effect, back-substitution over a region is performed in backwards topological order (with respect \nto the pro\u00adgram s control-flow graph), and the formula at each junction point is simplified. This strategy \neffectively controls the size of the for\u00admulas considered, and ultimately the time that is spent in the \ntheo\u00adrem prover. Finally, to reduce the number of times the induction-iteration algorithm is performed, \nwe back-substitute all formulas to be proven until they reach a loop entry. We partition the formulas \ninto groups that are made of comparable constituents, and invoke the induction-iteration algorithm only \nfor the strongest formulas in each group.  5.2.2 Example Here we illustrate how the induction-iteration \nmethod is applied in our running example. The control-flow graph of the program is shown in Figure 8. \nThe instructions at lines 5 and 11 are replicated to model the semantics of delayed branches. We use \na single bool\u00adean variable icc to model the SPARC condition code and label each control-flow graph edge \nwith the condition for that edge to be taken. Below, we use the line number of an instruction to denote \nthe instruction, and a sequence of line numbers within square brackets to represent a path.  To verify \nthat %g2 < 4n holds at line 7, we perform back\u00adsubstitution, starting with %g2 < 4n . Back-substituting \nthis con\u00addition across line 6 produces %g3 < n . Since the instruction at line 6 is the entry of a natural \nloop, we attempt to synthesize a loop invariant that implies %g3< n . We set W(0) to %g3< n . Since W(0) \nis not a tautology, we need to verify that W(0) is true on entry to the loop and to create the formula \nfor W(1). The fact that W(0) is true on entry to the loop can be shown by back-substituting W(0) along \nthe path [5 ,4,3,2,1]. To create W(1), we perform back-substitution through the loop body, starting with \nthe formula %g3< n , until we reach the loop entry again: 1. wlp([11 ], %g3< n ) = %g3< n 2. wlp([11 \n,10], %g3< n ) = icc< 0 . %g3< n 3. wlp([10,9], icc< 0 . %g3< n ) = %g3< %o1. %g3< n 4. wlp([8], %g3< \n%o1. %g3< n ) = %g3+1 < %o1. %g3+1 < n 5. wlp([7,6], %g3+1 < %o1. %g3+1 < n ) = %g3+1 < %o1. %g3+1 < \nn Thus, W(1) is the formula %g3+1 < %o1. %g3+1 < n .  Unfortunately, W(0) . W(1) is not a tautology. \nInstead of continuing by creating W(2) etc., we strengthen W(1) using the generalization technique mentioned \nin Section 5.2.1. The steps to generalize W(1) are as follows: (i) negating %g3+1 < %o1 . %g3+1 < n produces \n%g3+1 < %o1. %g3+1 = n ; (ii) eliminat\u00ading %g3 produces %o1 > n ; (iii) negating %o1 > n produces %o1 \n= n . Consequently, we set W(1) to be the generalized for\u00admula %o1= n . It is still the case that W(0) \n. W(1) is not a tautology, but now the formula that we create for W(2) (by another round of back-sub\u00adstitution) \nis %o1= n . (The variables %o1 and n are not modified in the loop body.) We now have that W(0) . W(1) \nimplies W(2). By this means, the loop invariant synthesized for line 6 is %g3< n. %o1= n . This invariant \nimplies that %g3< n holds at line 6, which in turn implies that %g2< 4n holds at line 7.  5.2.3 Discussion \nIn this section, we address the scalability of the verification phase, and discuss other potential improvements \nto the induction-iteration method. One major cost of the verification phase is from performing the induction-iteration \nmethod, whose cost is determined by the number of iterations that have to be performed before an invariant \nis identified. The cost of iteration step of the induction-iteration method is determined by the cost \nto perform VC generation and invoking the theorem prover. These costs are ultimately determined by the \ncharacteristics of the untrusted code. From our experience, it seems to be sufficient to set the maximum \nallowable number of iterations to three. The intuition behind this number is as follows: the first iteration \nwill incorporate the conditionals in the loop into L(1), the second iteration will test if L(1) is already \na loop invari\u00adant, and no new information will be discovered beyond the second iteration. The situation \nfor the inner loops is better when synthesiz\u00ading loop invariants for an outer loop, since usually the \ntests in the inner loops will not contribute to the proof of a condition of an outer loop. Besides the \nenhancements that were described in the previous section, there are a few enhancements that can, in principle, \nbe made to our existing prototype: The first is to cache in the theorem prover: we can represent formulas \nin a canonical form and use previous results when\u00adever possible.  The second is to do tabulation at \nfunction calls or at nodes that have multiple incoming edges, and to reuse previous results of VCgen. \n The third is to use more efficient algorithms for simple formu\u00adlas. Several people have described methods \nthat trade the gen\u00aderality of the constraint system for better efficiency. Bodik at al [4] describe a \nmethod to eliminate array-bounds checks for Java programs. Their method uses a restricted form of linear \nconstraints called difference constraints that can be solved using an efficient graph-traversal algorithm \non demand. Wag\u00adner et al [53] have formulated the buffer overrun detection problem as an integer constraint \nproblem that can be solved in linear time in practice.  Finally, it might be profitable to use other \ninvariant-synthesis methods in conjunction with induction iteration.  We have used the induction-iteration \nmethod to synthesize loop invariants because it works well for linear constraints and is totally mechanical. \nIt is conceivable that we could use other tech\u00adniques, such as the heuristic methods introduced by Katz \nand Manna [16] and Wegbreit [54], the difference equations method introduced by Elspas et al [10], and \nabstract interpretation using convex hulls [7]. The method described in [7] works forward on a program \ns control-flow graph. It has the potential to speed up the induction\u00aditeration method by pushing the \nfacts down in the program s con\u00adtrol-flow graph. Simple experiments that we carried out demon\u00adstrated \nsubstantial speedups in the induction-iteration method by selectively pushing conditions involving array \nbounds down in the program s control-flow graph. The abstract-interpretation method also addresses some \nof our current limitations such as inferring bounds of local arrays or arrays in structures (see Section \n6). Sum PagingPolicy StartTimer Hash BubbleSort StopTimer Btree Btree2 HeapSort 2 HeapSort jPVM Stack\u00adsmashing \nMD5 Number of Each Feature Instructions 13 20 22 25 25 36 41 51 71 95 157 309 883 Branches 2 5 1 4 5 \n3 11 11 9 16 12 89 11 Loops (Inner loops) 1 2 (1) 0 1 2 (1) 0 2 (1) 2 (1) 4 (2) 4 (2) 3 7(1) 5(2) Procedure \nCalls 0 0 1 1 0 2 0 4 3 0 21(21) 2 6 Global Safety Conditions 4 9 13 14 19 17 41 42 56 84 57 162 135 \nTime(Seconds) Typestate Propagation 0.01 0.06 0.02 0.04 0.03 0.04 0.08 0.11 0.12 0.08 1.04 1.42 6.82 \nAnnotation + Local Verification 0.001 0.003 0.004 0.004 0.002 0.005 0.007 0.009 0.010 0.010 0.032 0.031 \n0.087 Global Verification 0.05 0.41 0.06 0.35 0.45 0.08 0.50 0.41 2.05 3.58 4.18 10.15 7.04 TOTAL 0.06 \n0.47 0.08 0.39 0.48 0.13 0.59 0.53 2.18 3.67 5.25 11.60 13.95 Figure 9: Characteristics of the Examples \nand Performance Results  6 Initial Experience We have implemented a prototype safety checker for SPARC \nmachine-language programs, and applied our safety checker to several examples. The time to check these \nexamples varies from 0.06 seconds to 14 seconds. The examples include array sum, start-timer and stop-timer \ncode taken from Paradyn s performance-instrumentation suite [22], two versions of Btree traversal (one \nversion compares keys via a function call), hash-table lookup, a kernel extension that implements a page-replacement \npolicy [46], bubble sort, two ver\u00adsions of heap sort (one manually inlined version and one interpro\u00adcedural \nversion), stack-smashing (example 9.b described in Smith s paper [43]), MD5Update of the MD5 Message-Digest \nAlgorithm [39], and Java_jPVM_addhosts of jPVM [15]. jPVM is a Java native interface to PVM for the Java \nplatform. Java Native Interface (JNI) is a native-programming interface that allows Java code that runs \ninside a Java Virtual Machine to interop\u00aderate with applications and libraries written in other programming \nlanguages, such as C, C++, and assembly [14]. In the jPVM exam\u00adple, we verify that calls into JNI methods \nand PVM library func\u00adtions are safe, i.e., they obey the safety preconditions (see Section 2). All examples \nare written in C and then compiled with gcc -O (version 2.7.2.3). In our experiments, we were able to \nfind a safety violation in the example that implements a page-replacement policy it attempts to dereference \na pointer that could be null and we iden\u00adtified all array out-of-bounds violations in the stack-smashing \nexample. Figure 9 summarizes the time needed to verify each of the examples on a 440MHz Sun Ultra 10 \nmachine. The times are divided into the times to perform typestate propagation, create annotations and \nperform local verification, and perform global ver\u00adification. It also characterizes the examples in terms \nof the number of machine instructions, number of branches, number of loops (total versus number of inner \nloops), number of calls (total versus number of calls to trusted functions),3 and number of global safety \nconditions. The time to check these examples ranges from about 0.06 seconds to 14 seconds. Our current \napproach has three major limitations. First, if the untrusted code uses local arrays, we may not be able \nto infer their bounds. For example, for the stack-smashing and MD5Update pro\u00adgrams, we have to annotate \nthe stackframes for the functions that use local arrays. Similarly, for structures that have multiple \narray\u00adtyped fields, our analysis may not be able to find out which array a pointer points to. To address \nthis limitation, a forward pass such as that described in Cousot and Halbwachs [7] could be used to prop\u00adagate \nthe preconditions forward to find out information about the bounds of local arrays and to disambiguate \npointers that point to arrays in structures. Second, our analysis may lose precision due to array refer\u00adences. \nRecall that we use a single abstract location to summarize all elements of an array, and model a pointer \nto the array base (or to an arbitrary array element) as a pointer that may point to the sum\u00admary location. \nOur analysis loses precision when we cannot deter\u00admine whether an assignment kills all elements of an \narray. For example, our analysis reported that some actual parameters to the host methods and functions \nare undefined in the jPVM example, when they were in fact defined. We believe that dependence-analy\u00adsis \ntechniques such as those used in parallelizing compilers can be used to address this limitation. Third, \nthe type system described in this paper is too restrictive in that it does not incorporate a notion of \nsubtyping for structures and pointers. This can cause the analysis to lose precision when certifying \nprograms written in an object-oriented style. Our experience to date allows us to make the following \nobser\u00advations: Certain compiler optimizations, such as loop-invariant code motion and improved register-allocation \nalgorithms, actually make the task of safety checking easier. The memory-usage analysis that is part \nof typestate checking can lose precision at instructions that access memory (rather than registers). \nWhen a better job of register allocation has been done, more precise typestate information will be recovered. \nLoop-invariant code motion makes induction iteration more efficient by making the loops smaller and simpler. \n3. A trusted function is either a host function or a function that we trust. We check that calls to a \ntrusted function obey the corresponding safety preconditions of the function.  Certain compiler optimizations, \nsuch as strength reduction and optimizations to address-calculations, complicate the task of global verification \nbecause they hide relationships that can be used by the induction-iteration method. Standard tech\u00adniques \nshould allow us to overcome this limitation [7,29].  There are several strategies that makes the induction-iteration \nmethod more effective: First, because certain conditions in a loop can pollute L(j), instead of using \nwlp(loop-body, W(i-1)) as W(i), we compute the disjunctive normal form of wlp(loop-body, W(i-1)), and \ntry each of its disjuncts as W(i) in turn. Second, we rank the potential candidates according to a simple \nheuristic, and test each candidate for W(i) using a breadth-first strategy, rather than a depth-first \none. Finally, forward propagation of information about array bounds can substantially reduce the time \nspent in the induction-iteration method (it reduces the time needed to verify that a W(i) is true on \nentry, and it eliminates the need to use generalization to synthesize a loop invariant).  Verifying \nan interprocedural version of an untrusted program can take less time than verifying a manually inlined \nversion because the manually inlined version replicates the callee functions and the global conditions \nin the callee functions. This is a place where our analysis benefits from the procedure abstraction. \n  7 Related Work There are several projects investigating topics related to our safety\u00adchecking technique. \nThe approaches taken in these projects range from statically identifying common programming errors, to \nstati\u00adcally ensuring type safety, to the run-time checking of certain secu\u00adrity properties. The projects \nthat are closest to ours are Proof-Carrying Code (PCC) [32], the Touchstone Certifying Compiler [31] \nand Typed-Assembly Language (TAL) [25,26,28]. Proof-Carrying Code (PCC) has a code producer generate \nnot only the code but also a proof that the code is safe. Consequently, verification of the safety of \nuntrusted code can be carried out by a small proof checker. Since manual generation of proofs is tedious \nand error-prone, a certifying compiler automates the generation of PCC by having the compiler generate \ncode that carries proofs. Touchstone is a proto\u00adtype certifying compiler that compiles a safe subset \nof C into machine code that carries proofs of type safety. Morrisett et al [25,26,28] introduced the \nnotion of typed assembly language (TAL). In their approach, type information from a high-level program \nis incorporated into the representation of the program in a platform-independent typed intermediate form, \nand carried through a series of transformations down to the level of the target code. The compiler can \nuse the type information to per\u00adform sophisticated optimizations. Certain internal errors of a com\u00adpiler \ncan be detected by invoking a type-checker after each code transformation. A compiler that uses typed \nassembly language cer\u00adtifies type safety by ensuring that a well-typed source program always maps to \na well-typed assembly program. The most prominent difference between our approach and the certifying \ncompiler (or the TAL) approach is a philosophical one. The certifying compiler approach enforces safety \nby preventing bad things from being expressible in a source language. For example, both the safe subset \nof C of the Touchstone compiler and the Popcorn language for TALx86 [28] do not allow pointer arith\u00admetic, \npointer casting, or explicit deallocation of memory. In con\u00adtrast, we believe that safe code can be written \nin any language and produced by any compiler, as long as nothing bad is said in the code. This philosophical \ndifference has several implications. It gives the code producer the freedom to choose any language (including \neven unsafe languages such as C or assembly), and the freedom to produce the code with an off-the-shelf \ncompiler or manually. It eliminates the dependence of safety on the correctness of a compiler. As with \nPCC, our technique checks the safety of the final product of the compiler. It leads to the decoupling \nof the safety policy from the source language, which in turn, makes it possible for safety checking to \nbe performed with respect to an extensible set of safety properties that are specified on the host side. \nThe second important difference between our approach and the certifying compiler (or TAL) approach is \nthat the safety proper\u00adties we enforce are based on the notation of typestate, which pro\u00advides information \nat a finer granularity than types. Finally, neither Touchstone nor the Popcorn compiler track aliasing \ninformation. We have introduced an abstract storage model and extended typestate checking to also track \npointers. As a result, the analysis we provide is more precise than that used in Popcorn and Touchstone. \nIn addition to these high-level differences, there are a few technical differences. It should be noted \nthat our safety checker can be viewed as a certifier that generates proofs by first recovering type information \nthat (may have) existed in the source-language program (an embodiment of a suggestion made by Necula \nand Lee [31, p. 342].) The approach used in our safety checker differs from that used in the Touchstone \ncompiler in the following respects: First, Touchstone replaces the standard method for generating VCs, \nin which formulas are pushed backwards through the pro\u00adgram, with a forward pass over the program that \ncombines VC generation with symbolic execution. In contrast, our system uses a forward phase of typestate \nchecking (which is a kind of symbolic execution) followed by a fairly standard backward phase of VC generation. \nThe VC-generation phase is a backwards pass over the program for the usual reason; the advantage of propagating \ninfor\u00admation backwards is that it avoids the existential quantifiers that arise when formulas are pushed \nin the forward direction to gener\u00adate strongest postconditions; in a forward VC-generation phase, quantifiers \naccumulate forcing one to work with larger and larger formulas. Second, our safety-checking analysis \nmechanically syn\u00adthesizes loop invariants for bounds checking and alignment check\u00ading, whereas Touchstone \ngenerates code that contains explicit bounds checks and then removes those checks that it can prove to \nbe redundant. Comparing with TAL, the type system of TAL is richer in the sense that they model several \nlanguage features that we have not considered so far, including exceptions, type variables, and exis\u00adtential \ntypes. However, their type system does not support general pointers into the stack; nor can stack and \nheap pointers be unified so that a function taking a tuple argument can be passed either a heap-allocated \nor stack-allocated tuple [27]. Furthermore, TALx86 introduce special macros for array subscripting and \nupdating to prevent an optimizer from rescheduling them. The macros expand into code sequences that perform \nbound checks. We impose no such restrictions. TAL is more restrictive than PCC. PCC suggests that the \nrelevant operational content of simple type systems may be encoded using extensions to first-order predicate \nlogic. Our type system is closer in spirit to PCC, in that we provide a meta lan\u00adguage to describe types \nincluding size and alignment constraints. Moreover, TAL achieve flow-sensitivity in a different way than \nwe do; they label different blocks of code as different functions, and assign types to the registers \nat the level of basic blocks. We achieve flow-sensitivity through more traditional dataflow-analysis \ntech\u00adniques. In this way we can use results from the pointer-analysis community in a more straightforward \nway. Despite the differences, it is interesting to note that if our safety checker were to be given programs \nwritten in typed assembly language rather than in an untyped machine language, less work would be required \nto recover type information and to perform overload resolution (although we would still have to propagate \nstate and access information). This also applies to Java bytecode [19], where type information is con\u00adtained \nin the bytecode instructions themselves. A static debugger uses static analysis to find unsafe opera\u00adtions \nrather than to guarantee safety. Detlefs et al [8] describe a static checker for common programming errors, \nsuch as array index out-of-bounds, null-pointer dereferencing, and synchroniza\u00adtion errors (in multi-threaded \nprograms). In common with our approach, their analysis makes use of linear constraints, automati\u00adcally \nsynthesizes loop-invariants to perform bounds checking, and is parameterized by a policy specification. \nHowever, their safety\u00adchecking analysis works on source-language programs and also makes use of analyses \nthat are neither sound nor complete. In their policy specifications, user-supplied MODIFIES lists (specifying \nwhich variables of a procedure can be modified) offer a certain degree of access control; our access \npolicies are given in terms of regions, categories, and access permissions, which is a more gen\u00aderal \nmechanism. Leroy and Rouaix [18] have proposed a theoretical model for systematically placing type-based \nrun-time checks into interface routines of the host code. Their technique differs from ours in sev\u00aderal \nrespects: it is dynamic, it checks the host rather than the untrusted code, and it requires that the \nsource of the host API be available. Furthermore, safety requirements are specified by enu\u00admerating a \nset of predetermined sensitive locations and invariants on these locations, whereas our model of a safety \npolicy is more general. Finally, they perform type checking, whereas we perform typestate checking. \n 8 Limitations The main limitation of our technique is that it only can enforce safety properties that \nare expressible using typestates and linear constraints. This excludes all liveness properties and some \nsafety properties. Our analysis uses flow-sensitive interprocedural analysis to propagate typestate information. \nThe verification phase is fairly costly due to the need to synthesize loop invariants to prove the safety \npredicates. The scalability of our analysis remains to be evaluated with bigger applications. Like all \nstatic techniques, our technique is incomplete. First, the analysis loses precision when handling array \nreferences, because we use a single abstract location to summarize all ele\u00adments of the array. Second, \nthe existing prototype cannot infer the bounds of local arrays. We have to annotate the stackframes of \nfunctions that use local arrays. Third, the induction-iteration method itself is incomplete even for \nlinear constraints. The induc\u00adtion-iteration method cannot prove the correctness of array accesses in \na loop if correctness depends on some data whose val\u00adues are set before the execution of the loop. One \nsuch example is the use of a sentinel at the end of the array to speed up a sequential search [49]. The \ngeneralization capabilities of the system may fall short for many problems, even though we only care \nabout memory safety: The induction-iteration method could fail in cases that a loop invariant must be \nstrengthened to the point where we end up verifying a large part of the partial correctness of the algorithm. \nFourth, our type system does not incorporate a notion of subtyping for structures and pointers. This \ncan hurt us when certifying pro\u00adgrams written in an object-oriented style. In principle, we could define \nthe meet operation for structures in terms of physical sub\u00adtypes [42] (i.e., if t1 = t2 then struct t2 \nis an initial prefix of struct t1) and define the meet of two pointer types t1 ptr and t2 ptr to be (t1 \n.. t2 ) ptr; however, the typestate-checking algorithm would have to be extended to track which pointers \nare mutable. (See [1] for a discussion of subtyping in the presence of mutable pointers.) Finally, our \nanalysis is not able to deal with certain unconventional usages of operations, such as swapping two non-integer \nvalues by means of exclusive or operations. Despite these limitations, the method shows some promise. \nIts limitations represent potential research opportunities, and we believe that future research will \nmake the analysis more precise and efficient, and continued engineering can make the technique practical \nfor larger programs. Acknowledgments We thank Rastislav Bodik, Jianjun Chen, Brian Wylie, Ari Tam\u00adches, \nand Philip Roth for their feedback on the paper. We thank the anonymous referees, whose comments have \nsubstantially improved both the technical quality and readability of the paper.  References [1] M. \nAbadi, and L. Cardelli. A Theory of Objects. Monographs in Computer Science, D. Gries, and F. B. Schneider \n(Ed.). Springer-Verlag New York. (1996). [2] B. Bershad, S. Savage, P. Pardyak, E. G. Sirer, M. Fiucynski, \nD. Becker, S. Eggers, and C. Chambers. Extensibility, Safety, and Performance in the SPIN Operating System, \n15th Symposium on Operating System Principles. Copper Mountain, CO. (December 1995). [3] B. Alpern, and \nF. B. Schneider. Recognizing Safety and Liveness. Distributed Computing 2 3. (1987). [4] R. Bodik, R. \nGupta, and V. Sarkar. ABCD: Eliminating Array Bounds Checks on Demand. SIGPLAN Conference on Programming \nLanguage Design and Implementation. Vancouver B.C., Canada. (June 2000). [5] D.R. Chase, M. Wegman, and \nF. Zadeck. Analysis of Pointers and Structures. SIGPLAN Conference on Programming Language Design and \nImplementation. New York, NY. (1990). [6] T. Chiueh, G. Venkitachalam, P. Pradhan. Integrating Segmentation \nand Paging Protection for Safe, Efficient and Transparent Software Extensions. 17th ACM Symposium on \nOperating Systems Principles. Charleston, SC. (December 1999). [7] P. Cousot, and N. Halbwachs. Automatic \nDiscovery of Linear Restraints Among Variables of a Program. Fifth Annual ACM Symposium on Principles \nof Programming Languages. Tucson, AZ. (January 1978). [8] D. K. Detlefs, R. M. Leino, G. Nelson, and \nJ. B. Saxe. Extended Static Checking. Research Report 159, Compaq Systems Research Center. Palo Alto, \nCA. (December 1998). [9] E. W. Dijkstra. A Discipline of Programming. Prentice-Hall. Englewood Cliffs, \nNJ. (1976). [10] B. Elspas, M. W. Green, K. N. Levitt, and R. J. Waldinger. Research in Interactive Program-Proving \nTechniques. SRI, Menlo Park, California. (May 1972). [11] D. Engler, M. F. Kaashoek, and J. O Toole. \nExokernel: An Operating System Architecture for Application-Level Resource Management. 15th Symposium \non Operating System Principles. Copper Mountain, CO. (December 1995). [12] C.A.R. Hoare. An Axiomatic \nBasis for Computer Programming. Communications of the ACM 12 10. (October 1969). [13] Illustra Information \nTechnologies. Illustra DataBlade Developer s Kit Architecture Manual, Release 1.1. (1994). [14] JavaSoft. \nJava Native Interface Specification. Release 1.1 (May 1997). [15] jPVM: A Native Methods Interface to \nPVM for the Java Platform. http://www.chmsr.gatech.edu/jPVM. (2000). [16] S. Katz, and Z. Manna. A Heuristic \nApproach to Program Verification. 3rd International Conference on Artificial Intelligence. (August 1973). \n[17] W. Kelly, V. Maslov, W. Pugh, E. Rosser, T. Shpeisman, and D. Wonnocott. The Omega Library, Version \n1.1.0 Interface Guide. omega@cs.umd.edu. http://www.cs.umd.edu/projects/omega. (November 1996). [18] \nX. Leroy, and F. Rouaix. Security Properties of Typed Applets. 25th ACM SIGPLAN-SIGACT Symposium on Principles \nof Programming Languages. San Diego, CA. (January 1998). [19] Lindholm T., and F. Yellin. The Java (TM) \nVirtual Machine Specification. Second Edition. http://java.sun.com/docs/books/vmspec/2ndedition/html/VMSpec \nToC.doc.html. (1999). [20] S. McCanne, and V. Jacobson. The BSD Packet Filter: A New Architecture for \nUser-Level Packet Capture. The Winter 1993 USENIX Conference. USENIX Association. San Diego, CA. (January \n1993). [21] Misrosoft. Microsoft COM Technologies-Information and Resources for the Component Object \nModel-Based Technologies. http://www.microsoft.com/com. (March 2000) [22] B. P. Miller, M. D. Callaghan, \nJ. M. Cargille, J. K. Hollingsworth, R. B. Irvin, K. L. Karavanic, K. Kunchithapadam, and T. Newhall.The \nParadyn Parallel Performance Measurement Tools. IEEE Computer 28 11. (November 1995). [23] J. C. Mogul, \nR. F. Rashid, and M. J. Accetta. The Packet Filter: An Efficient Mechanism for User-Level Network Code. \nACM Symposium on Operating Systems Principles. Austin, TX. (November 1987). [24] J. Morris. A General \nAxiom of Assignment. Theoretical Foundations of Programming Methodology, Lecture Notes of an International \nSummer School, directed by F. L. Bauer, E. W. Dijkstra and C.A.R. Hoare. Manfred Broy and Gunther Schmidt \n(Ed.). D. Reidel Publishing Company. (1982). [25] G. Morrisett, D. Tarditi, P. Cheng, C. Stone, R. Harper, \nand P. Lee. The TIL/ML Compiler: Performance and Safety Through Types. In 1996 Workshop on Compiler Support \nfor Systems Software. Tucson, AZ. (February 1996). [26] G. Morrisett, D. Walker, K. Crary, and N. Glew. \nFrom System F to Typed Assembly Language. 25th Annual ACM Symposium on Principles of Programming Languages. \nSan Diego, CA. (January 1998). [27] G. Morrisett, K. Crary, N. Glew, and D. Walker. Stack-Based Typed \nAssembly Language. 1998 Workshop on Types in Compilation. Published in Xavier Leroy and Atsushi Ohori, \neditors, Lecture Notes in Computer Science, 1473. Springer-Verlag 1998. [28] G. Morrisett, K. Crary, \nN. Glew, D. Grossman, R. Samuels, F. Smith, D. Walker, S. Weirich, S. Zdancewic. TALx86: A Realistic \nTyped Assembly Language. ACM Workshop on Compiler Support for System Software. Atlanda, GA (May 1999). \n[29] S. S. Muchnick. Advanced Compiler Design and Implementation. Morgan Kaufmann Publishers, Inc. (1997). \n[30] G. Necula. Compiling with Proofs. Ph.D. Dissertation, Carnegie Mellon University. (September 1998). \n[31] G. Necula, and P. Lee. The Design and Implementation of a Certifying Compiler. ACM SIGPLAN Conference \non Programming Language Design and Implementation. Montreal, Canada. (June 1998). [32] G. Necula. Proof-Carrying \nCode. 24th Annual ACM Symposium on Principles of Programming Languages. Paris, France. (January 1997). \n[33] Netscape. Browser Plug-ins, 1999. http://home.netscape.com/plugins/index.html. [34] C. Pu, T. Audrey, \nA. Black, C. Consel, C. Cowan, J. Inouye, L. Kethana, J. Walpole, and K. Zhang. Optimistic Incremental \nSpecialization: Streamlining a Commercial Operating System. 15th ACM Symposium on Operating Systems Principles. \nCopper Mountain, CO. (December 1995). [35] W. Pugh. The Omega Test: A Fast and Practical Integer Programming \nAlgorithm for Dependence Analysis. Supercomputing. Albuquerque, NM. (November 1991). [36] W. Pugh, and \nD. Wonnacott. Eliminating False Data Dependences Using the Omega Test. ACM SIGPLAN Conference on Programming \nLanguage Design and Implementation. San Francisco, CA. (June 1992). [37] W. Pugh, and D. Wonnacott. Experience \nwith Constraint-Based Array Dependence Analysis. Technical Report CS-TR-3371. University of Maryland. \n(1994). [38] D. D. Redell, Y. K. Dalal, T. R. Horsley, H. C. Lauer, W. C. Lynch, P. R. McJones, H. G. \nMurray, and S. C. Purcell. Pilot: An Operating System for a Personal Computer. Communications of the \nACM 23 2. (February 1980). [39] R. Rivest. The MD5 Message-Digest Algorithm. Request for Comments: 1321. \nMIT Laboratory for Computer Science and RSA Data Security, Inc. (April 1992). [40] F. B. Schneider. Towards \nFault-Tolerant and Secure Agentry. 11th International Workshop on Distributed Algorithms. Saarbr\u00fccken, \nGermany. (September 1997). [41] M. I. Seltzer, Y. Endo, C. Small, and K. A. Smith. Dealing With Disaster: \nSurviving Misbehaved Kernel Extensions. 2nd USENIX Symposium on Operating Systems Design and Implementation. \nSeattle, WA. (October 1996). [42] M. Siff, S. Chandra, T. Ball, K. Kunchithapadam, and T. Reps. Coping \nwith type casts in C. Seventh European Software Engineering Conference and Seventh ACM SIGSOFT Symposium \non the Foundations of Software Engineering. Toulouse, France. (September 1999). [43] N. P. Smith. Stack \nSmashing Vulnerabilities in the UNIX Operating System. http://www.destroy.net/machines/security. (2000). \n[44] R. Strom, and D. M. Yellin. Extending Typestate Checking Using Conditional Liveness Analysis. IEEE \nTransactions on Software Engineering 19 5. (May 1993). [45] R. Strom, and S. Yemini. Typestate: A Programming \nLanguage Concept for Enhancing Software Reliability. IEEE Transactions on Software Engineering 12 1. \n(January 1986). [46] C. Small, and M. A. Seltzer. Comparison of OS Extension Technologies. USENIX 1996 \nAnnual Technical Conference. San Diego, CA. (January 1996). [47] M. Stonebraker. Inclusion of New Types \nin Relational Data Base Systems. Readings in Database Systems. Second Edition. Michael Stonebraker (Ed.). \n(1994). [48] Sun Microsystems, Inc. Java (TM) Plug-in Overview. http://java.sun.com/products/1.1.1/index-1.1.1.html. \n(October 1999). [49] N. Susuki, and K. Ishihata. Implementation of an Array Bound Checker. 4th ACM Symposium \non Principles of Programming Languages. Los Angeles, CA. (January 1977). [50] A. Tamches, and B. P. Miller. \nFine-Grained Dynamic Instrumentation of Commodity Operating System Kernels. Third Symposium on Operating \nSystem Design and Implementation. New Orleans, LA. (February 1999). [51] M. Tamir. ADI: Automatic Derivation \nof Invariants. IEEE Transactions on Software Engineering SE-6 1. (January 1980). [52] D. Tennenhouse, \nand D. Wetherall. Towards an Active Network Architecture. Computer Communication Review 26 2. (April \n1996). [53] D. Wegner, J. Foster, E. Brewer, and A. Aiken. A First Step Towards Automated Detection of \nBuffer Overrun Vulnerabilities. The 2000 Network and Distributed Systems Security Conference. San Diego, \nCA. (February 2000). [54] B. Wegbreit. The Synthesis of Loop Predicates. Communications of the ACM 17 \n2. (February 1974). [55] R. Wahbe, S. Lucco, T. E. Anderson, and S. L. Graham. Efficient Software-Based \nFault Isolation. 14th Symposium on Operating System Principles. Asheville, NC. (December 1993).  \n\t\t\t", "proc_id": "349299", "abstract": "<p>We show how to determine statically whether it is safe for untrusted machine code to be loaded into a trusted host system.</p><p>Our safety-checking technique operates directly on the untrusted machine-code program, requiring only that the initial inputs to the untrusted program be annotated with typestate information and linear constraints. This approach opens up the possibility of being able to certify code produced by any compiler from any source language, which gives the code producers more freedom in choosing the language in which they write their programs. It eliminates the dependence of safety on the correctness of the compiler because the final product of the compiler is checked. It leads to the decoupling of the safety policy from the language in which the untrusted   code is written, and consequently, makes it possible for safety checking to be performed with respect to an extensible set of safety properties that are specified on the host side.</p><p>We have implemented a prototype safety checker for SPARC machine-language programs, and applied the safety checker to several examples. The safety checker was able to either prove that an example met the necessary safety conditions, or identify the places where the safety conditions were violated. The checking times ranged from less than a second to 14 seconds on an UltraSPARC machine.</p>", "authors": [{"name": "Zhichen Xu", "author_profile_id": "81451592612", "affiliation": "Computer Sciences Department, University of Wisconsin Madison, WI", "person_id": "PP95030401", "email_address": "", "orcid_id": ""}, {"name": "Barton P. Miller", "author_profile_id": "81452606461", "affiliation": "Computer Sciences Department, University of Wisconsin Madison, WI", "person_id": "PP14066473", "email_address": "", "orcid_id": ""}, {"name": "Thomas Reps", "author_profile_id": "81100117392", "affiliation": "Computer Sciences Department, University of Wisconsin Madison, WI", "person_id": "PP40023877", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/349299.349313", "year": "2000", "article_id": "349313", "conference": "PLDI", "title": "Safety checking of machine code", "url": "http://dl.acm.org/citation.cfm?id=349313"}