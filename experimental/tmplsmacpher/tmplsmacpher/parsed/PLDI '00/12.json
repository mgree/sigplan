{"article_publication_date": "05-01-2000", "fulltext": "\n ExploitingSuperwordLevelParallelism withMultimediaInstructionSets SamuelLarsenandSamanAmarasinghe MITLaboratoryforComputerScience \nCambridge,MA02139 fslarsen,samang@lcs.mit.edu Abstract Increasingfocusonmultimediaapplicationshasprompted \ntheadditionofmultimediaextensionstomostexistinggen\u00aderalpurposemicroprocessors.Thisaddedfunctionality \ncomesprimarilywiththeadditionofshortSIMDinstruc\u00adtions.Unfortunately,accesstotheseinstructionsislimited \ntoin-lineassemblyandlibrarycalls.Generally,ithasbeen assumedthatvectorcompilersprovidethemostpromis\u00adingmeansofexploitingmultimediainstructions.Although \nvectorizationtechnologyiswellunderstood,itisinherently complexandfragile.Inaddition,itisincapableoflocating \nSIMD-styleparallelismwithinabasicblock. InthispaperweintroducetheconceptofSuperword LevelParallelism(SLP),anovelwayofviewingparallelism \ninmultimediaandscientifcapplications.WebelieveSLP isfundamentallydiferentfromthelooplevelparallelism \nexploitedbytraditionalvectorprocessing,andthereforede\u00admandsanewmethodofextractingit.Wehavedeveloped asimpleandrobustcompilerfordetectingSLPthattar\u00adgetsbasicblocksratherthanloopnests.Aswithtechniques \ndesignedtoextractILP,oursisabletoexploitparallelism bothacrossloopiterationsandwithinbasicblocks.There\u00adsultisanalgorithmthatprovidesexcellentperformancein \nseveralapplicationdomains.Inourexperiments,dynamic instructioncountswerereducedby46%.Speedupsranged from1.24to6.70. \nIntroduction Therecentshifttowardcomputation-intensivemultimedia workloadshasresultedinavarietyofnewmultimediaex\u00adtensionstocurrentmicroprocessors[6,10,16,18,20].Many \nnewdesignsaretargetedspecifcallyatthemultimediado\u00admain[3,7,11].Thistrendislikelytocontinueasithasbeen \nprojectedthatmultimediaprocessingwillsoonbecomethe mainfocusofmicroprocessordesign[8]. Whilediferentprocessorsvaryinthetypeandnumber \nofmultimediainstructionsofered,atthecoreofeachisaset ofshortSIMDorsuperwordoperations.Theseinstructions \noperateconcurrentlyondatathatarepackedinasinglereg- Permissiontomakedigitalorhardcopiesofallorpartofthisworkfor \npersonalorclassroomuseisgrantedwithoutfeeprovidedthatcopies arenotmadeordistributedforproftorcommercialadvantageand \nthatcopiesbearthisnoticeandthefullcitationofthefrstpage.To copyotherwise,orrepublish,topostonserversortoredistributeto \nlists,requirespriorspecifcpermissionand/orafee. PLDI2000,Vancouver,BritishColumbia,Canada. Copyright2000ACM1-58113-199-2/00/0006...$5.00. \nisterormemorylocation.Inthepast,suchsystemscould accommodateonlysmalldatatypesof8or16bits,mak\u00adingthemsuitableforalimitedsetofapplications.With \ntheemergenceof128-bitsuperwords,newarchitecturesare capableofperformingfour32-bitoperationswithasingle \ninstruction.Byaddingfoatingpointsupportaswell,these extensionscannowbeusedtoperformmoregeneralpurpose \ncomputation. ItisnotsurprisingthatSIMDexecutionunitshaveap\u00adpearedindesktopmicroprocessors.Theirsimplecontrol, \nreplicatedfunctionalunits,andabsenceofheavily-ported registerflesmaketheminherentlysimpleandextremely \namenabletoscaling.Asthenumberofavailabletransis\u00adtorsincreaseswithadvancesinsemiconductortechnology, datapathsarelikelytogrowevenlarger. \nToday,useofmultimediaextensionsisdifcultsinceap\u00adplicationwritersarelargelyrestrictedtousingin-lineas\u00adsemblyroutinesorspecializedlibrarycalls.Theproblemis \nexacerbatedbyinconsistenciesamongdiferentinstruction sets.Onesolutiontothisinconvenienceistoemployvec\u00adtorizationtechniquesthathavebeenusedtoparallelizesci\u00adentifccodeforvectormachines[5,14,15].Sinceanumber \nofmultimediaapplicationsarevectorizable,thisapproach promisesgoodresults.However,manyimportantmultime\u00addiaapplicationsaredifculttovectorize.Complicatedloop \ntransformationtechniquessuchasloopfssionandscalarex\u00adpansionarerequiredtoparallelizeloopsthatareonlypar\u00adtiallyvectorizable[2,4,17].Consequently,nocommercial \ncompilercurrentlyimplementsthisfunctionality.Thispaper presentsamethodforextractingSIMDparallelismbeyond \nvectorizableloops. WebelievethatshortSIMDoperationsarewellsuited toexploitafundamentallydiferenttypeofparallelismthan \nthevectorparallelismassociatedwithtraditionalvectorand SIMDsupercomputers.WedenotethisparallelismSuper\u00adwordLevelParallelism(SLP)sinceitcomesintheformof \nsuperwordscontainingpackeddata.Vectorsupercomput\u00adersrequirelargeamountsofparallelisminordertoachieve \nspeedups,whereasSLPcanbeproftablewhenparallelismis scarce.Fromthisperspective,wehavedevelopedageneral \nalgorithmfordetectingSLPthattargetsbasicblocksrather thanloopnests. Insomerespects,superwordlevelparallelismisare\u00adstrictedformofILP.ILPtechniqueshavebeenverysuccess\u00adfulinthegeneralpurposecomputingarena,partlybecause \noftheirabilitytofndparallelismwithinbasicblocks.In thesamewaythatloopunrollingtranslateslooplevelpar\u00adallelismintoILP,vectorparallelismcanbetransformedinto \nSLP.Thisrealizationallowsfortheparallelizationofvector\u00ad a = b + c * z[i+0] d = e + f * z[i+1] r = s \n+ t * z[i+2] w = x + y * z[i+3]  a b c z[i+0] d e f z[i+1] = +SIMD *SIMD r s t z[i+2] w \n x y z[i+3] Figure1:Isomorphicstatementsthatcanbepackedand executedinparallel. izableloopsusingthesamebasicblockanalysis.Asaresult, \nouralgorithmdoesnotrequireanyofthecomplicatedloop transformationstypicallyassociatedwithvectorization.In \nfact,vectorparallelismalonecanbeuncoveredusingasim\u00adplifedversionoftheSLPcompileralgorithm. Theremainderofthispaperisorganizedasfollows:Sec\u00adtion2defnessuperwordlevelparallelismandcomparesit \ntootherformsofparallelism.Section3describesthecom\u00adpileralgorithmusedtoextractsuperwordlevelparallelism. \nAvariationofthisalgorithmtargetingvectorparallelismis discussedinSection4.Section5presentsresultsonmul\u00adtimediaandscientifcbenchmarks.Section6discussesar\u00adchitecturalfeaturesthatcomplementSLPcompilation.Sec\u00adtion7outlinesreasonswhywebelieveSLPalgorithmswill \nbesuccessful,andSection8concludes. 2SuperwordLevelParallelism ThissectionbeginsbyelaboratingonthenotionofSLP \nandthemeansbywhichitisdetected.Terminologyisin\u00adtroducedthatfacilitatesthediscussionofouralgorithmsin \nSections3and4.WethencontrastSLPtootherformsof parallelismanddiscusstheirinteractions.Thishelpsmoti\u00advatetheneedforanewcompilationtechnique. \n2.1DescriptionofSuperwordLevelParallelism SuperwordlevelparallelismisdefnedasshortSIMDparal\u00adlelisminwhichthesourceandresultoperandsofaSIMD \noperationarepackedinastoragelocation.Detectionis donethroughashort,simpleanalysisinwhichindependent isomorphicstatementsareidentifedwithinabasicblock. \nIsomorphicstatementsarethosethatcontainthesameop\u00aderationsinthesameorder.Suchstatementscanbeexecuted inparallelbyatechniquewecallstatementpacking,anex\u00adampleofwhichisshowninFigure1.Here,sourceoperands \nincorrespondingpositionshavebeenpackedintoregisters andtheadditionandmultiplicationoperatorshavebeenre\u00adplacedbytheirSIMDcounterparts.Sincetheresultofthe \ncomputationisalsopacked,unpackingmayberequiredde\u00adpendingonhowthedataareusedinlatercomputations. Theperformancebeneftofstatementpackingisdetermined \nbythespeedupgainedfromparallelizationminusthecost ofpackingandunpacking. Dependingonwhatoperationsanarchitectureprovides \ntofacilitategeneralpackingandunpacking,thistechnique canactuallyresultinaperformancedegradationifpacking \nandunpackingcostsarehighrelativetoALUoperations. OneofthemainobjectivesofourSLPdetectiontechnique istominimizepackingandunpackingbylocatingcasesin \nwhichpackeddataproducedasaresultofonecomputation canbeuseddirectlyasasourceinanothercomputation. for(i=0;i<16;i++){ \nlocaldiff=ref[i]-curr[i]; diff+=abs(localdiff); } (a)Originalloop. for(i=0;i<16;i++){ T[i]=ref[i]-curr[i]; \n} for(i=0;i<16;i++){ diff+=abs(T[i]); } (b)Afterscalarexpansionandloopfssion. for(i=0;i<16;i+=4){ localdiff=ref[i+0]-curr[i+0]; \ndiff+=abs(localdiff); localdiff=ref[i+1]-curr[i+1]; diff+=abs(localdiff); localdiff=ref[i+2]-curr[i+2]; \ndiff+=abs(localdiff); localdiff=ref[i+3]-curr[i+3]; diff+=abs(localdiff); } (c)Superwordlevelparallelismexposedafterunrolling. \nfor(i=0;i<16;i+=4){ localdiff0=ref[i+0]-curr[i+0]; localdiff1=ref[i+1]-curr[i+1]; localdiff2=ref[i+2]-curr[i+2]; \nlocaldiff3=ref[i+3]-curr[i+3]; diff+=abs(localdiff0); diff+=abs(localdiff1); diff+=abs(localdiff2); diff+=abs(localdiff3); \n} (d)Packablestatementsgroupedtogetherafterrenaming. Figure2:AcomparisonbetweenSLPandvectorparalleliza\u00adtiontechniques. \nPackedstatementsthatcontainadjacentmemoryrefer\u00adencesamongcorrespondingoperandsareparticularlywell suitedforSLPexecution.Thisisbecauseoperandsareef\u00adfectivelypre-packedinmemoryandrequirenoreshufing \nwithinaregister.Inaddition,anaddresscalculationfol\u00adlowedbyaloadorstoreneedonlybeexecutedoncein\u00adsteadofindividuallyforeachelement.Thecombinedef\u00adfectcanleadtoasignifcantperformanceincrease.Thisis \nnotsurprisingsincevectormachineshavebeensuccessfulat exploitingthesamephenomenon.Inourexperiments,in\u00adstructionseliminatedfromoperatingonadjacentmemory \nlocationshadthegreatestimpactonspeedup.Forthisrea\u00adson,locatingadjacentmemoryreferencesformsthebasisof \nouralgorithm,discussedinSection3. 2.2VectorParallelism Vectorparallelismisasubsetofsuperwordlevelparallelism. \nOurresultsinSection5showthat20%ofdynamicinstruc\u00adtionsavingsontheSPEC95fpbenchmarksuitearefrom non-vectorizablecodesequences. \nTobetterexplainthediferencesbetweensuperwordlevel parallelismandvectorparallelism,wepresenttwoshortex\u00adamples,showninFigures2and3.Althoughthefrstex\u00ad \n do{ dst[0]=(src1[0]+src2[0])\u00bb1; \u00bb dst[1]=(src1[1]+src2[1])\u00bb1; \u00bb dst[2]=(src1[2]+src2[2])\u00bb1; \u00bb dst[3]=(src1[3]+src2[3])\u00bb1; \n\u00bb dst+=4; src1+=4; src2+=4; } while(dst!=end); Figure3:Anexampleofahand-optimizedmatrixoperation thatprovesunvectorizable. \namplecanbemoldedintoavectorizableform,weknowof novectorcompilersthatcanbeusedtovectorizethesec\u00adond.Furthermore,thetransformationsrequiredinthefrst \nexampleareunnecessarilycomplexandmaynotworkin morecomplicatedcircumstances.Ingeneral,avectorcom\u00adpilermustemployarepertoireoftoolsinordertoparallelize \nloopsonacasebycasebasis.Bycomparison,ourmethodis simpleandrobust,yetstillcapableofdetectingtheavailable \nparallelism. Figure2(a)presentstheinnerloopofthemotionesti\u00admationalgorithmusedforMPEGencoding.Vectorization \nisinhibitedbythepresenceofaloopcarrieddependence andafunctioncallwithintheloopbody.Toovercomethis, avectorcompilercanperformaseriesoftransformationsto \nmoldtheloopintoavectorizableform.Thefrstisscalar expansion,whichallocatesanewelementinatemporary arrayforeachiterationoftheloop[4].Loopfssionisthen \nusedtodividethestatementsintoseparateloops[12].The resultofthesetransformationsisshowninFigure2(b).The \nfrstloopisvectorizable,butthesecondmustbeexecuted sequentially. Figure2(c)showstheloopfromtheperspectiveofSLP. \nAfterunrolling,thefourstatementscorrespondingtothe frststatementintheoriginalloopcanbepackedtogether. \nThepackingprocessefectivelymovespackablestatements tocontiguouspositions,asshowninpart(d).Thecode motionislegalbecauseitdoesnotviolateanydependences \n(oncescalarrenamingisperformed).Thefrstfourstate\u00admentsintheresultingloopbodycanbepackedandexecuted inparallel.Theirresultsarethenunpackedsotheycanbe \nusedinthesequentialcomputationofthefnalstatements. Intheend,thismethodhasthesameefectasthetrans\u00adformationsusedforvectorcompilation,whileonlyrequiring \nloopunrollingandscalarrenaming. Figure3showsacodesegmentthataveragestheele\u00admentsoftwo16x16matrices.Asisthecasewithmany \nmultimediakernels,ourexamplehasbeenhand-optimized forasequentialmachine.Inordertovectorizethisloop, avectorcompilerwouldneedtoreversetheprogrammer\u00adappliedoptimizations.Weresuchmethodsavailable,they \nwouldinvolveconstructingaforloop,restoringtheinduc\u00adtionvariable,andre-rollingtheloop.Incontrast,locating \nSLPwithintheloopbodyissimple.Sincetheoptimized codeisamenabletoSLPanalysis,hand-optimizationhas hadnodetrimentalefectsonourabilitytodetecttheavail\u00adableparallelism. \n2.3LoopLevelParallelism Vectorparallelism,exploitedbyvectorcomputers,isasub\u00adsetoflooplevelparallelism.Generallooplevelparallelismis \ntypicallyexploitedbyamultiprocessororMIMDmachine. Inmanycases,parallelloopsmaynotyieldperformance gainsbecauseoffne-grainsynchronizationorloop-carried \ncommunication.Itisthereforenecessarytofndcoarse-grain parallelloopswhencompilingforMIMDmachines.Tradi\u00adtionally,aMIMDmachineiscomposedofmultiplemicropro\u00adcessors.Itisconceivablethatlooplevelparallelismcouldbe \nexploitedorthogonallytosuperwordlevelparallelismwithin eachprocessor.Sincecoarse-grainparallelismisrequiredto \ngetgoodMIMDperformance,extractingSLPshouldnot detractfromexistingMIMDparallelperformance. 2.4SIMDParallelism \nSIMDparallelismcameintoprominencewiththeadvent ofmassivelyparallelsupercomputerssuchastheIlliacIV [9].Theassociationoftheterm\\SIMD\"withthistypeof \ncomputeriswhatledustoutilizethetermSuperwordLevel ParallelismwhendiscussingshortSIMDoperations. SIMDsupercomputerswereimplementedusingthou\u00adsandsofsmallprocessorsthatworkedsynchronouslyona \nsingleinstructionstream.WhilethecostofmassiveSIMD parallelexecutionandnear-neighborcommunicationwas low,distributionofdatatotheseprocessorswasexpensive. \nForthisreason,automaticSIMDparallelizationcenteredon solvingthedatadistributionproblem[1].Intheend,the \nclassofapplicationsforwhichSIMDcompilersweresuccess\u00adfulwasevenmorerestrictivethanthatofvectorandMIMD \nmachines. 2.5InstructionLevelParallelism SuperwordlevelparallelismiscloselyrelatedtoILP.Infact, SLPcanbeviewedasasubsetofinstructionlevelparal\u00adlelism.MostprocessorsthatsupportSLPalsosupportILP \nintheformofsuperscalarexecution.Becauseoftheirsimi\u00adlarities,methodsforlocatingSLPandILPmayextractthe \nsameinformation.Undercircumstanceswherethesetypes ofparallelismcompletelyoverlap,SLPexecutionispreferred \nbecauseitprovidesalessexpensiveandmoreenergyefcient solution. Inpractice,themajorityofILPisfoundinthepresence \nofloops.Therefore,unrollingtheloopmultipletimesmay provideenoughparallelismtosatisfybothILPandSLPpro-cessorutilization.Inthissituation,ILPperformancewould \nnotnoticeablydegradeafterSLPisextractedfromapro\u00adgram. 3SLPCompilerAlgorithm OurSLPcompileralgorithmcanbedividedintoseveraldis\u00adtinctphases.First,loopunrollingisusedtotransformvec\u00adtorparallelismintoSLP.Alignmentanalysisthenattempts \ntodeterminetheaddressalignmentofeachloadandstore instruction.Thisisneededforcompilingtoarchitectures \nthatdonotsupportunalignedmemoryaccesses.Next,the intermediaterepresentationistransformedintoalowlevel \nformandaseriesofstandardcompileroptimizationsisap\u00adplied. Thecoreofouralgorithmbeginsbylocatingstatements \nwithadjacentmemoryreferencesandpackingtheminto groupsofsizetwo.Fromthisinitialseed,moregroupsare discoveredbasedontheactivesetofpackeddata.Allgroups \narethenmergedintolargerclustersofasizeconsistentwith thesuperworddatapathwidth.Finally,anewscheduleis \n producedforeachbasicblock,wheregroupsofpackedstate\u00admentsarereplacedwithSIMDinstructions. Thefollowingsubsectionsdescribeeachofthesephases \nindetail.Figure4presentsasimpleexampletohighlight thecoreroutinesandFigure5liststhepseudocode.Both willbereferencedthroughoutthissection. \n3.1LoopUnrolling Loopunrollingisperformedearlysinceitismosteasilydone atahighlevel.Asdiscussed,itisusedtotransformvec\u00adtorparallelismintobasicblockswithsuperwordlevelparal\u00adlelism.Inordertoensurefullutilizationofthesuperword \ndatapathinthepresenceofavectorizableloop,theunroll factormustbecustomizedtothedatasizesusedwithinthe \nloop.Forexample,avectorizableloopcontaining16-bitval\u00aduesshouldbeunrolled8timesfora128-bitdatapath.Our \nsystemcurrentlyunrollsloopsbasedonthesmallestdata typepresent. 3.2AlignmentAnalysis Alignmentanalysisdeterminesthealignmentofmemoryac\u00adcesseswithrespecttoacertainsuperworddatapathwidth. \nForarchitecturesthatdonotsupportunalignedmemory accesses,alignmentanalysiscangreatlyimprovetheper\u00adformanceofoursystem.Withoutit,memoryaccessesare \nassumedtobeunalignedandthepropermergingcodemust beemittedforeverywideloadandstore. Onesituationinwhichmergingoverheadcanbeamor\u00adtizediswhenacontiguousblockofmemoryisaccessed \nwithinaloop.Inthissituation,overheadcanbereduced tooneadditionalmergeoperationperloadorstorebyusing datafrompreviousiterations. \nAlignmentanalysis,however,cancompletelyremovethis overhead.ForFORTRANsources,asimpleinterprocedu\u00adralanalysiscandeterminealignmentinformationinasingle \npass.Thisanalysisisfow-insensitive,context-insensitive, andvisitsthecallgraphinbreadth-frstorder.ForC \nsources,weuseanenhancedpointeranalysispackagede\u00advelopedbyRuginaandRinard[21].Sincethispassalso provideslocationsetinformation,wecanconsiderdepen\u00addencesmorecarefullywhencombiningpackingcandidates. \nAfulldiscussionofalignmentanalysisisbeyondthescope ofthispaper.Acompletedescriptionwillbegivenin[13]. \nOurcompilationsystemiscapableofoperatingbothwith andwithoutalignmentconstraints.Forsimplicity,wede\u00adscribesubsequentphasesofthealgorithmassumingnoar\u00adchitecturalsupportforunalignedaccesses.Assuch,later \nphasesassumealignmentinformationhasbeenannotated toeachloadandstoreinstructionwherepossible. 3.3Pre-optimization \nSLPanalysisismostusefulwhenperformedonathreead\u00addressrepresentation.Thisway,thealgorithmhasfullfex\u00adibilityinchoosingwhichoperationstopack.Ifisomorphic \nstatementsareinsteadmatchedbythetreestructureinher\u00aditedfromthesourcecode,longexpressionsmustbeidenti\u00adcalinordertoparallelize.Ontheotherhand,identifying \nadjacentmemoryreferencesismucheasierifaddresscalcu\u00adlationsmaintaintheiroriginalform.Wethereforeannotate \neachloadandstoreinstructionwiththisinformationbefore fattening. Afterfattening,severalstandardoptimizationsareap\u00adpliedtoaninputprogram.Thisensuresthatparallelism \nisnotextractedfromcomputationthatwouldotherwise beeliminated.Optimizationsincludeconstantpropaga\u00adtion,copypropagation,deadcodeelimination,commonsub\u00adexpressionelimination,loop-invariantcodemotion,andre\u00addundantload/storeelimination.Asafnalstep,scalarre\u00adnamingisperformedtoremoveoutputandanti-dependences \nsincetheycaninhibitparallelization. 3.4IdentifyingAdjacentMemoryReferences Becauseoftheirobviousimpact,statementscontainingadja\u00adcentmemoryreferencesarethefrstcandidatesforpacking. \nWethereforebeginthecoreofouranalysisbyscanningeach basicblocktofndindependentpairsofsuchstatements. Adjacencyisdeterminedusingbothalignmentinformation \nandarrayanalysis. Ingeneral,duplicatememoryoperationscanintro\u00adduceseveraldiferentpackingpossibilities.Dependences \nwilleliminatemanyofthesepossibilitiesandredundant load/storeeliminationwillusuallyremovetherest.Inprac\u00adtice,nearlyeverymemoryreferenceisdirectlyadjacentto \natmosttwootherreferences.Thesecorrespondtotheref\u00aderencesthataccessmemoryoneithersideofthereference inquestion.Whenlocated,thefrstoccurrenceofeachpair \nisaddedtothePackSet. Defnition3.1APackisann-tuple,hs 1,:::,sni,where s 1,:::,snareindependentisomorphicstatementsinabasic \nblock. Defnition3.2APackSetisasetofPacks. Inthisphaseofthealgorithm,onlygroupsoftwostate\u00admentsareconstructed.Werefertotheseaspairswithaleft \nandrightelement. Defnition3.3APairisaPackofsizetwo,wherethe frststatementisconsideredtheleftelement,andthesec\u00adondstatementisconsideredtherightelement. \nAsanintermediatestep,statementsareallowedtobe\u00adlongtotwogroupsaslongastheyoccupyaleftposition inoneofthegroupsandarightpositionintheother.En\u00adforcingthisdisciplinehereallowstheCombinationphaseto \neasilymergegroupsintolargerclusters.Thesedetailsare discussedinSection3.6. Figure4(a)presentsanexamplesequenceofstatements. \nFigure4(b)showstheresultsofadjacentmemoryidenti\u00adfcationinwhichtwopairshavebeenaddedtothePack\u00adSet.ThepseudocodeforthisphaseisshowninFigure5as \nfnd adj refs. 3.5ExtendingthePackSet OncethePackSethasbeenseededwithaninitialsetof packedstatements,moregroupscanbeadded.Thisisdone \nbyfndingnewcandidatesthatcaneither: Produceneededsourceoperandsinpackedform,or  Useexistingpackeddataassourceoperands. \n Thisisaccomplishedbyfollowingdef-useanduse-def chainsofexistingPackSetentries.Ifthesechainsleadto freshpackablestatements,anewgroupiscreatedandadded \ntothePackSet.Fortwostatementstobepackable,they mustmeetthefollowingcriteria: Thestatementsareisomorphic. \n U U P (7) (8) (9) (4) (5) (6) (1) (2) (3) h = a[i+2] j = 7 k = h + j e = a[i+1] f = 6 g = e + f b = \na[i+0] c = 5 d = b + c (9) (8) (6) (5) (3) (2) k = h + j j = 7 g = e + f f = 6 d = b + c c = 5 (7) (4) \n(4) (1) (b) h = a[i+2] e = a[i+1] e = a[i+1] b = a[i+0] (a) P (2) (5) (8) c = 5 f = 6 j = 7 U (4) (7) \n(1) (4) e = a[i+1] h = a[i+2] b = a[i+0] e = a[i+1] P (4) (1) (4) (7) (6) (3) e = a[i+1] b = a[i+0] e \n= a[i+1] h = a[i+2] g = e + f d = b + c (3) (6) d = b + c g = e + f (6) (9) g = e + f k = h + j (6) (9) \ng = e + f k = h + j (5) (2) f = 6 c = 5 (c) (5) (8) f = 6 j = 7 (d) (1) b = a[i+0]  (4) e = a[i+1] \n (7) h = a[i+2]  (3) d = b + c  (6) g = e + f  (9) k = h + j  (2) c = 5  (5) f = 6  (8) j = 7 \n  (e)  Figure4:VariousphasesofSLPanalysis.UandPrepresentthecurrentsetofunpackedandpackedstatements,respectively. \n(a)Initialsequenceofinstructions.(b)StatementswithadjacentmemoryreferencesarepairedandaddedtothePackSet. \n(c)ThePackSetisextendedbyfollowingdef-usechainsofexistingentries.(d)ThePackSetisfurtherextendedbyfollowing \nuse-defchains.(e)Combinationmergesgroupscontainingthesameexpression.(f)EachgroupisscheduledandSIMD operationsareemittedintheirplace. \n SLP extract:BasicBlockB!BasicBlock stmts can pack:BasicBlockBXPackSetPX PackSetP+ StmtsXStmts 0 XIntalign!Boolean \nP+fnd adj refs(B,P) ifisomorphic(s,s 0)then P+extend packlist(B,P) ifindependent(s,s 0)then P+combine \npacks(P) 8ht,t02:6s ifiPt=then returnschedule(B,[],P) if8ht,t0i2:t 0 6s 0 then P=Intaligns+get alignment(s) \n0 Intalign0 +get alignment(s) s ifaligns=_aligns=alignthen fnd adj refs:BasicBlockBXPackSetP!PackSet \nifalign=_align0 =align+data size(s)then 0 0 ss foreachStmts2Bdo returntrue foreachStmts 0 2Bwheres=6s \n0 do returnfalse 0 ifhas mem ref(s)^has mem ref(s)then 0 ifadjacent(s,s)then Intalign+get alignment(s) \nifstmts can pack(B,P,s,s 0,align)then P+P[fhs,s 0ig returnP follow  use defs:BasicBlockBXPackSetPXPackp!PackSet \n00 00 wherep=hs,s 0i,s=[x0:=f(x1,:::,xm)],s=[x0:=f(x1,:::,x m)] Intalign+get alignment(s) forj+1tomdo \nextend packlist:BasicBlockBXPackSetP!PackSet if9t2B:t=[xj:=:::]^9t 0 2B:t 0 =[x 0 :=:::]then j repeat \n0 ifstmts can pack(B,P,t,t,align) PackSetPprev+P ifest savings(ht,t0i,P)>0then foreachPackp2Pdo P+P[fht,t0ig \nP+follow use defs(B,P,p) 0 set alignment(s,s,align) P+followdefuses(B,P,p) returnP untilP=Pprev \nreturnP combine packs:PackSetP!PackSet follow  def uses:BasicBlockBXPackSetPXPackp!PackSet repeat 0i00 \nwherep=hs,s,s=[x0:=f(x1,:::,xm)],s 0 =[x:=f(x 0,:::,x)]PackSetPprev+P 01m Intalign+get alignment(s) foreachPackp=hs1,:::,sni2Pdo \n Intsavings+-1 000 foreachPackp=hs1,:::,si2Pdo m foreachStmtt2Bwheret=[::::=g(:::,x0,:::)]do ifsn=s \n0 then 0 2 1foreachStmttBwheret6t=[::::=h(:::,x0,:::)] = 00 do 0 00 P+P-fp,pg[fhs1,:::,sn,s2,:::,sig \n 0 mifstmtscanpack(B,P,t,t,align)then untilP=Pprev 0i ifestsavings(ht,t,P)>savingsthen returnP 0i savings+estsavings(ht,t,P) \nStmtu+t Stmtu 0 +t 0 ifsavings>0then schedule:BasicBlockBXBasicBlockB0 XPackSetP P+P[fhu,u 0ig !BasicBlock \n0 set alignment(u,u) fori+0tojBjdo returnP if9p=h:::,si,:::i2Pthen 0 if8s2p:deps scheduled(s,B)then \nforeachStmts2pdo B+B-s BB0 .s 0 +0 returnschedule(B,B,P) 0 elseifdeps scheduled(si,B)then 0 returnschedule(B-si,B.si,P) \nifjBj=60then P+P-fpgwherep=frst(B,P) 0 returnschedule(B,B,P) 0 returnB Figure5:PseudocodefortheSLPextractionalgorithm.Onlykeyproceduresarelisted.Helperfunctionsinclude:1) \nhasmemref,whichreturnstrueifastatementaccessesmemory,2)adjacent,whichchecksadjacencybetweentwomemory \nreferences,3)getalignment,whichretrievesalignmentinformation,4)setalignment,whichsetsalignmentinformationwhenit \nisnotalreadyset,5)depsscheduled,whichreturnstruewhen,foragivenstatement,allstatementsuponwhichitisdependent \nhavebeenscheduled,6)frst,whichreturnsthePackSetmembercontainingtheearliestunscheduledstatement,7)estsavings, \nwhichestimatesthesavingsofapotentialgroup,8)isomorphic,whichchecksforstatementisomorphism,and9)independent, \nwhichreturnstruewhentwostatementsareindependent. . Thestatementsareindependent. .Theleftstatementisnotalreadypackedinaleftpo\u00adsition. \n. Therightstatementisnotalreadypackedinaright position. . Alignmentinformationisconsistent. . Executiontimeofthenewparalleloperationisesti\u00admatedtobelessthanthesequentialversion. \nTheanalysiscomputesanestimatedspeedupofeachpo\u00adtentialSIMDinstructionbasedonacostmodelforeachin-structionaddedandremoved.Thisincludesanypackingor \nunpackingthatmustbeperformedinconjunctionwiththe newinstruction.Iftheproperpackedoperanddataalready existinthePackSet,thenpackingcostissettozero. \nAsnewgroupsareaddedtothePackSet,alignmentin\u00adformationispropagatedfromexistinggroupsviause-defor def-usechains.Onceset,astatement'salignmentdeter\u00admineswhichpositionitwilloccupyinthedatapathdur\u00adingitscomputation.Forthisreason,astatementcanhave \nonlyonealignment.Newgroupsarecreatedonlyiftheir alignmentrequirementsareconsistentwiththosealreadyin \nplace. Whenasingledefnitionhasmultipleuses,thereisthe potentialformanydiferentpackingpossibilities.Ifthis \noccurs,thecostmodelisusedtoestimatethemostprof\u00aditablepossibilitiesbasedonwhatiscurrentlypacked.These \ngroupsareaddedtothePackSetinorderoftheirestimated proftabilityaslongastherearenoconfictswithexisting \nPackSetentries. Intheexample,part(c)showsnewgroupsthatareadded afterfollowingdef-usechainsofthetwoexistingPackSeten\u00adtries.Part(d)introducesnewgroupsdiscoveredbyfollow\u00adinguse-defchains.Thepseudocodeforthisphaseislisted \nasextend packsetinFigure5. 3.6Combination Onceallproftablepairshavebeenchosen,theycanbecom\u00adbinedintolargergroups.Twogroupscanbecombinedwhen \ntheleftstatementofoneisthesameastherightstatementof theother.Infact,groupsmustbecombinedinthisfashion \ninordertopreventastatementfromappearinginmorethan onegroupinthefnalPackSet.Thisprocess,providedby thecombine \npacksroutine,checksallgroupsagainstonean\u00adotherandrepeatsuntilallpossiblecombinationshavebeen made.Figure4(e)showstheresultofourexampleafter \ncombination. Sincetheadjacentmemoryidentifcationphaseuses alignmentinformation,itwillnevercreatepairsofmem\u00adoryaccessesthatcrossanalignmentboundary.Allpacked \nstatementsarealignedbasedonthisinitialseed.Asare\u00adsult,thecombinationphasewillneverproduceagroupthat spansanalignmentboundary.Combinedgroupsarethere\u00adforeguaranteedtobelessthanorequaltothesuperword \ndatapathsize. 3.7Scheduling Dependenceanalysisbeforepackingensuresthatstatements withinagroupcanbeexecutedsafelyinparallel.However, \nitmaybethecasethatexecutingtwogroupsproducesa dependenceviolation.AnexampleofthisisshowninFig\u00adure6.Here,dependenceedgesaredrawnbetweengroups \nx = a[i+0] + k1 y = a[i+1] + k2 q = b[i+0] + y r = b[i+1] + k3 s = b[i+2] + k4  q = b[i+0] + y z \n= a[i+2] + s r = b[i+1] + k3 s = b[i+2] + k4 Figure6:Exampleofadependencebetweengroupsof packedstatements. \nifastatementinonegroupisdependentonastatementin theother.Aslongastherearenocyclesinthisdependence graph,allgroupscanbescheduledsuchthatnoviolations \noccur.However,acycleindicatesthatthesetofchosen groupsisinvalidandatleastonegroupwillneedtobeelim\u00adinated.Althoughexperimentaldatahasshownthiscaseto \nbeextremelyrare,caremustbetakentoensurecorrectness. Theschedulingphasebeginsbyschedulingstatements basedontheirorderintheoriginalbasicblock.Eachstate\u00admentisscheduledassoonasallstatementsonwhichitis \ndependenthavebeenscheduled.Forgroupsofpackedstate\u00adments,thispropertymustbesatisfedforeachstatementin \nthegroup.Ifschedulingiseverinhibitedbythepresenceof acycle,thegroupcontainingtheearliestunscheduledstate\u00admentissplitapart.Schedulingcontinuesuntilallstatements \nhavebeenscheduled. Wheneveragroupofpackedstatementsisscheduled,a newSIMDoperationisemittedinstead.Ifthisnewopera\u00adtionrequiresoperandpackingorreshufing,thenecessary \noperationsarescheduledfrst.Similarly,ifanystatements requireunpackingoftheirsourcedata,therequiredsteps \naretaken.Sinceouranalysisoperatesatthelevelofbasic blocks,eachbasicblockassumesalldataareinanunpacked \nconfgurationuponentrytotheblock.Forthisreason,all variablesthatareliveonexitareunpackedattheendofthe \nblock. SchedulingisprovidedbythescheduleroutineinFig\u00adure5.IntheexampleofFigure4,theresultofscheduling \nisshowninpart(f).Atthecompletionofthisphase,anew basicblockhasbeenconstructedwhereverparallelization \nwassuccessful.TheseblockscontainSIMDinstructionsin placeofpackedisomorphicstatements.Aswewillshowin Section5,thealgorithmcanbeusedtoachievespeedupson \namicroprocessorwithmultimediaextensions. 4ASimpleVectorizingCompiler TheSLPconceptspresentedintheprevioussectionleadto \nanelegantimplementationofavectorizingcompiler.Vec\u00adtorparallelismischaracterizedbytheexecutionofmultiple \niterationsofaninstructionusingasinglevectoroperation. Thissamecomputationcanbeuncoveredwithunrollingby \nlimitingpackingtounrolledversionsofthesamestatement. Withthistechnique,eachstatementhasonlyonepossible \ngrouping,whichmeansthatnosearchingisrequired.In\u00adstead,everystatementcanbepackedautomaticallywithits siblingsiftheyarefoundtobeindependent.Theproftabil\u00adityofeachgroupcanthenbeevaluatedinthecontextofthe \nentiresetofpackeddata.Anygroupsthataredeemedun\u00adproftablecanbedroppedinfavoroftheirsequentialcoun\u00adterparts.Thepseudocodeforthisalgorithmisshownin \nFigure7. Whilenotasgeneralasthealgorithmdescribedinthe vector parallelize:BasicBlockB!BasicBlock PackSetP+ \nP+fnd all packs(B,P) P+eliminate  unproftable packs(P) returnschedule(B,[],P) fnd all packs:BasicBlockBxPackSetP!PackSet \nforeachStmts2Bdo if8p2P:s2rpthen Packp+[s] foreachStmts 0 2Bwheres 0 #6sdo ifstmts are packable(s,s0)then \np+p's 0 ifjpj>1then P+P[fpg returnP stmts are packable:StmtsxStmts 0 !Boolean ifsame orig stmt(s,s0)then \nifindependent(s,s0)then returntrue returnfalse eliminate unproftable packs:PackSetP!PackSet repeat \nPackSetP0 +P foreachPackp2Pdo ifest savings(p,P)<0then P+Pfpg untilP= P0 returnP Figure7:Pseudocodeforthevectorextractionalgorithm. \nProceduresthatareidenticaltothoseinFigure5areomit\u00adted.same orig stmtreturnstrueiftwostatementsareun\u00adrolledversionsofthesameoriginalstatement. \nprevioussection,thistechniquesharesmanyofthesame desirableproperties.First,theanalysisitselfisextremely \nsimpleandrobust.Second,partiallyvectorizableloopscan beparallelizedwithoutcomplicatedlooptransformations. \nMostimportantly,thisanalysisisabletoachievegoodre\u00adsultsonscientifcandmultimediabenchmarks. Thedrawbacktothismethodisthatitmaynotbeap\u00adplicabletolongvectorarchitectures.Sincetheunrollfactor \nmustbeconsistentwiththevectorsize,unrollingmaypro\u00adducebasicblocksthatoverwhelmtheanalysisandthecode generator.Assuch,thismethodismainlyapplicabletoar\u00adchitectureswithshortvectors. \nInSection5,wewillprovidedatathatcomparethisap\u00adproachtothealgorithmdescribedinSection3. 5Results ThissectionpresentspotentialperformancegainsforSLP \ncompilertechniquesandsubstantiatesthemusingaMo\u00adtorolaMPC7400microprocessorwiththeAltiVecinstruc\u00adtionset.Allresultsweregatheredusingthecompileral\u00adgorithmsdescribedinSections3and4.Bothwereimple\u00admentedwithintheSUIFcompilerinfrastructure[23]. \n5.1Benchmarks WemeasurethesuccessofourSLPalgorithmonbothsci\u00adentifcandmultimediaapplications.Forscientifccodes,we \nName Description Datatype FIR Finiteimpulseresponseflter 32-bitfoat IRI Infniteimpulseresponseflter 32-bitfoat \nVMM Vector-matrixmultiply 32-bitfoat MMM Matrix-matrixmultiply 32-bitfoat YUV RGBtoYUVconversion 16-bitinteger \n Table1:Multimediakernelsusedtoevaluatetheefective\u00adnessofSLPanalysis. usetheSPEC95fpbenchmarksuite.Ourmultimediabench\u00admarksareprovidedbythekernelslistedinTable1. \n5.2SLPAvailability Toevaluatetheavailabilityofsuperwordlevelparallelismin ourbenchmarks,wecalculatedthepercentageofdynamic \ninstructionseliminatedfromasequentialprogramafterpar\u00adallelization.Allinstructionswerecountedequally,including \nSIMDoperations.Whenpackingwasrequired,weassumed thatn-1instructionswererequiredtopacknvaluesintoa singleSIMDregister.Thesevalueswerealsousedforun\u00adpackingcosts. \nMeasurementswereobtainedbyinstrumentingsource codewithcountersinordertodeterminethenumberof timeseachbasicblockwasexecuted.Thesenumberswere \nthenmultipliedbythenumberofstaticSUIFinstructions ineachbasicblock.Resultsforbothsetsofbenchmarks arelistedinTable2andillustratedinFigure8.Theper\u00adformanceofeachbenchmarkisshownforavarietyofhy-potheticaldatapathwidths.Itisassumedthateachdatap\u00adathcanaccommodateSIMDversionsofanystandarddata \ntype.Forexample,adatapathof512bitscanperformeight 64-bitfoatingpointoperationsinparallel.Touncoverthe \nmaximumamountofsuperwordlevelparallelismavailable, wecompiledeachbenchmarkwithoutalignmentconstraints. \nThisallowedforamaximumdegreeoffreedomwhenmaking packingdecisions. Forthemultimediabenchmarks,YUVgreatlyoutper\u00adformstheotherkernels.Thisisbecauseitoperateson16\u00adbitvaluesandisentirelyvectorizable.Theremainingkernels \narepartiallyvectorizableandstillexhibitlargeperformance gains. FortheSPEC95fpbenchmarksuite,someoftheappli- \nBenchmark128bits256bits512bits1024bits swim 61.59% 64.45% 73.44% 77.17% tomcatv 40.91% 61.28% 69.50% \n73.85% mgrid 43.49% 55.13% 60.51% 61.52% su2cor 33.99% 48.73% 56.06% 59.63% wave5 26.69% 37.25% 41.97% \n43.87% apsi 24.19% 29.93% 31.32% 29.85% hydro2d 18.53% 26.17% 28.88% 30.80% turb3d 21.16% 24.76% 21.55% \n15.13% applu 15.54% 22.56% 10.29% 0.01% fpppp 4.22% 8.14% 8.27% 8.27% FIR 38.72% 45.37% 48.56% 49.84% \nIRI 51.83% 60.59% 64.77% 66.45% VMM 36.92% 43.37% 46.63% 51.90% MMM 61.75% 73.63% 79.76% 82.86% YUV 87.21% \n93.59% 96.79% 98.36% Table2:Percentageofdynamicinstructionseliminatedby SLPanalysisforavarietyofhypotheticaldatapathwidths. \n 128 bits 256 bits 512 bits 1024 bits Vector Component Non-vector Component  Figure8:Percentageofdynamicinstructionseliminatedby \nSLPanalysisforavarietyofhypotheticaldatapathwidths. cationsexhibitaperformancedegradationasthedatapath \nwidthisincreased.Thisisduetothelargeunrollfactor requiredtofllawidedatapath.Ifthedynamiciteration countsfortheseloopsaresmallerthantheunrollfactor,the \nunrolledloopisneverexecuted.Forturb3dandapplu,the optimalunrollfactorisfour.A256-bitdatapathistherefore \nsufcientsinceitcanaccommodatefour64-bitoperations. Infpppp,themosttime-intensiveloopisalreadyunrolled \nbyafactorofthree.A192-bitdatapathcansupportthe availableparallelisminthissituation. InFigure9andTable3wecomparetheSLPalgorithmto \nthevectorizationtechniquedescribedinSection4.Forthe multimediabenchmarks,bothmethodsperformidentically. \nHowever,therearemanycasesinthescientifcapplications forwhichtheSLPalgorithmisabletofndadditionalpack\u00adingopportunities.InFigure10,weshowtheavailablevector \nparallelismasasubsetoftheavailablesuperwordlevelpar\u00adallelism. 5.3SLPPerformance TotesttheperformanceofourSLPalgorithminarealen\u00advironment,wetargetedourcompilationsystemtotheAl\u00adtiVec[19]instructionset.Ofthepopularmultimediaexten- \nVector Parallelism Superword Level Parallelism Figure10:Contributionofvectorizableandnon-vectorizable \ncodesequencesintotalSLPsavingsfortheSPEC95fp benchmarksuite. sionsavailableincommercialmicroprocessors,webelieve \nAltiVecbestmatchesthecompilationtechniquedescribed inthispaper.AltiVecdefnes128-bitfoatingpointandin\u00adtegerSIMDoperationsandprovidesacomplementarysetof \n32generalpurposeregisters.Italsodefnesloadandstore instructionscapableofmovingafull128bitsofdata. OurcompilerautomaticallygeneratesCcodewithAl\u00adtiVecmacrosinsertedwhereparallelizationissuccessful.We \nthenuseanextendedgcccompilertogeneratemachinecode. ThiscompilerwasprovidedbyMotorolaandsupportsthe AltiVecABI(applicationbinaryinterface).Duetotheex\u00adperimentalnatureoftheAltiVeccompilerextensions,itwas \nnecessarytocompileallbenchmarkswithoutoptimization. Basemeasurementsweremadebycompilingtheunparal\u00adlelizedversionforexecutionontheMPC7400superscalar \nunit.Inbothcases,thesamesetofSUIFoptimizationsand thesamegccbackendwereused.SinceAltiVecdoesnot supportunalignedmemoryaccesses,allbenchmarkswere \ncompiledwithalignmentconstraintsinplace[13]. Table4andFigure11presentperformancecompar\u00adisonsona450MHzG4PowerMacworkstation.Mostof \ntheSPEC95fpbenchmarksrequiredoubleprecisionfoat\u00adingpointsupporttooperatecorrectly.Sincethisisnot  swim \n64.45% 62.29% tomcatv 61.28% 56.87% mgrid 55.13% 34.29% su2cor 48.73% 44.20% wave5 37.25% 28.73% apsi \n29.93% 15.89% hydro2d 26.17% 22.91% turb3d 24.76% 20.35% applu 22.56% 14.67% fpppp 8.14% 0.00% FIR 45.37% \n73.63% Benchmark SLP Vector   IIR 60.59% 43.63% VMM 43.37% 60.59% 73.63% 45.37% 93.59% 93.59% 100% \n90% 80% 70% 60% 50% 40% 30% 20% 10% 0% % of dynamic instructions eliminated Table3:Percentageofdynamicinstructionseliminatedwith \n Figure9:PercentageofdynamicinstructionseliminatedSLPparallelizationandwithvectorparallelizationona256\u00adwithSLPparallelizationandwithvectorparallelizationonbitdatapath. \na256-bitdatapath. BenchmarkSpeedup 570% swim 1.24 tomcatv 1.57 FIR 1.26 IRI 1.41 VMM 1.70 MMM 1.79 YUV \n6.70 75% 100% % improvement of the execution time Table4:SpeeduponanMPC7400processorusingSLPcom\u00adpilation. \n50% 25% 0% supportedbyAltiVec,wewereunabletocompilevectorized versionsforallbuttwoofthebenchmarks.swimutilizessin\u00ad \ngleprecisionfoatingpointoperations,andtheSPEC92fp versionoftomcatvprovidesaresultsimilartothe64-bit version. \nOurcompilercurrentlyassumesthatallpackedopera\u00adtionsareexecutedontheAltiVecunitandallsequential operationsareperformedonthesuperscalarunit.Opera\u00adtionstopackandunpackdataarethereforerequiredtogo \nthroughmemorysinceAltiVecprovidesnoinstructionsto movedatabetweenregisterfles.Despitethishighcost,our \ncompilerisstillabletoexploitsuperwordlevelparallelism andprovidespeedups. 6ArchitecturalSupportforSLP \nThecompileralgorithmpresentedinSection3wasinspired bythemultimediaextensionsinmodernprocessors.How\u00adever,severallimitationsmakeitdifculttofullyrealizethe \npotentialprovidedbySLPanalysis.Welistsomeofthese limitationsbelow: .Manymultimediainstructionsaredesignedforaspe-cifchigh-leveloperation.Forexample,HP'sMAX-2 \nextensionsofermatrixtransforminstructions[16]and SUN'sVISextensionsincludeinstructionstocompute pixeldistances[18].ThecomplexCISC-likesemantics \noftheseinstructionsmakeautomaticcodegeneration difcult. .SLPhardwareistypicallyviewedasamultimediaen\u00adginealoneandisnotdesignedforgeneralpurpose \ncomputation.Floatingpointcapabilities,forexample, haveonlyrecentlybeenaddedtosomearchitectures. Furthermore,eventhemostadvancedmultimediaex\u00adtensionslackcertainfundamentaloperationssuchas \n32-bitintegermultiplicationanddivision[19]. .Incurrentarchitectures,datasetsareusuallycon\u00adsideredtobelongexclusivelytoeithermultimediaor \nsuperscalarhardware.Thisdesignphilosophyispor\u00adtrayedinthelackofinterregisterflemoveoperations intheAltiVecinstructionset.IfSLPcompilationtech\u00adniquescanshowaneedforabettercouplingbetween \nthesetwounits,futurearchitecturesmayprovidethe necessarysupport. .Mostcurrentmultimediainstructionsetsaredesigned \nwiththeassumptionthatdataarealwaysstoredin theproperpackedconfguration.Asaresult,data packingandunpackinginstructionsaregenerallynot \nwellsupported.Thisimportantoperationisusefulto oursystem.Withbettersupport,SLPperformance canbefurtherincreased. \nFigure11:Percentageimprovementofexecutiontimeonan MPC7400processorusingSLPcompilation. .Althoughoursystemiscapableofcompilingforma\u00adchinesthatdonotsupportunalignedmemoryaccesses, \nthealgorithmispotentiallymoreefectivewithoutthis constraint.Architecturessupplyingefcientunaligned loadandstoreinstructionsmightimprovetheperfor\u00admanceofSLPanalysis. \nThefrstthreepointsdiscusssimpleprocessormodifca\u00adtionsthatwehopewillbeincorporatedintofuturemulti\u00admediainstructionsetsastheymature.Thelasttwopoints \naddressdifcultissues.Solvingthemineitherhardwareor softwareisnottrivial.Moreresearchisrequiredtodeter\u00adminethebestapproach. \n7KeystoGeneralAcceptanceofSLP Manyofthetechniquesdevelopedbytheacademiccompiler communityarenotacceptedinmainstreamcomputing.A \ngoodexampleistheworkonlooplevelparallelizationthat hascontinuedforoverthreedecades.However,inavery shortperiodoftime,ILPcompilershavebecomeuniversal. \nWebelievethefollowingcharacteristicsarecriticaltothe generalacceptanceofacompileroptimization: .Robustness:Ifsimplesourcecodemodifcations \ndrasticallyalterprogramperformance,successbe\u00adcomesdependentupontheuser'sunderstandingof compilerintricacies.Forexample,techniquestoun\u00adcoverlooplevelparallelismarepronetowidefuctu\u00adationsinperformance.Achangeinonestatementof \ntheloopbodymayresultinavectorcompiler'sse\u00adquentializationoftheentireloop.InthecaseofILP andSLP,failuretoparallelizeafewstatementswill \nnotsignifcantlyimpactaggregateperformance.This makesmethodsfortheirextractionmuchmorerobust. .Scalability:Compilertechniquesmustbeableto \nhandlelargeprogramsiftheyaretogainacceptance forrealapplications.Someanalysesrequiredbyloop optimizationsdonotscalewelltolargecodesizesbe\u00adcauseofdependenceonglobalprogramanalysis.Al\u00adthoughglobalanalysiscanimprovetheefectivenessof \nILPandSLP,itisnotrequired.Therefore,complex\u00aditygrowslinearlywithprogramsize.Thisresultsin smoothscalingtolargerapplications. \n .Simplicity:Complexcompilertransformationsare morepronetobugsthansimpleanalyses.Problems arelikelytoappearonlyunderveryspecifcconditions, \nmakingthemdifculttodetect.Manytime-critical projectsarecompiledwithoutoptimizationsinorder toavoidpossiblecompilererrors.Coarse-grainparal\u00adlelizationandvectorizationrequireinvolvedanalyses \nthataremorelikelytoexhibitthisbehavior.However, mostILPtechniques,aswellastheSLPtechniques presentedinSection3,areextremelysimpletounder\u00adstand,implementandvalidate.Inaddition,itisoften \nthecasethatsimplicityleadstofastercompilation. .Portability:Optimizationsthataredependenton particularfeaturesofasourcelanguageorprogram\u00admingstylewillnotbecomeuniversal.Techniquesfor \nextractinglooplevelparallelismarelimitedbecause theyonlyapplytoprogramswrittenwithloopsand arrays.Alternatively,ILPandSLPtechniquesareap\u00adpliedatthelevelofbasicblocks,makingthemless \ndependentonsourcecodecharacteristics. .Efectiveness:Nocompilertechniquewillbeused ifitdoesnotsubstantiallyimproveprogramperfor\u00admance.InSection5,weshowedthatouralgorithm \nfordetectingSLPcanprovideremarkableperformance gains. WebelieveSLPcompilertechniqueshavethepotential \ntobecomeuniversallyacceptedasviableandefectivemeth\u00adodsofextractingSIMDparallelism.Asaresult,weexpect \nfuturearchitecturestoplaceincreasingimportanceonSLP operations. 8Conclusion Inthispaperweintroducedsuperwordlevelparallelism,the \nnotionofviewingparallelismfromtheperspectiveofparti\u00adtionedoperationsonpackedsuperwords.Weshowedthat SLPcanbeexploitedwithasimpleandrobustcompilerim\u00adplementationthatexhibitsspeedupsrangingfrom1.24to \n6.70onasetofscientifcandmultimediabenchmarks. WealsoshowedthatSLPconceptsleadtoanelegantim\u00adplementationofavectorizingcompiler.Bycomparingthe \nperformanceofthiscompilertothemoregeneralSLPalgo\u00adrithm,wedemonstratedthatvectorparallelismisasubset ofsuperwordlevelparallelism. \nOurcurrentcompilerimplementationisstillinitsin\u00adfancy.Whilesuccessful,webelieveitsefectivenesscanbe improved.ByextendingtheSLPanalysisbeyondbasic \nblocks,morepackingopportunitiescouldbefound.Fur\u00adthermore,SLPcouldoferaformofpredication,inwhich unflledslotsofawideoperationcouldbeflledwithspec\u00adulativecomputation.Ifdataareinvalidatedduetocontrol \nfow,theycouldsimplybediscarded. Recentresearchhasshownthatcompileranalysiscan signifcantlyreducethesizeofdatatypesneededtostore \nprogramvariables[22].Incorporatingthisanalysisintoour ownhasthepotentialofdrasticallyimprovingperformance \nbyincreasingthenumberofoperandsthatcanbepacked andexecutedinparallel. Today,mostdesktopprocessorsareequippedwithmul\u00adtimediaextensions.Nonuniformitiesinthediferentinstruc\u00adtionsets,exacerbatedbyalackofcompilersupport,hasleft \ntheseextensionsunderutilized.WehaveshownthatSLP compilationisnotonlypossible,butalsoapplicabletoa widerclassofapplicationdomains.Assuch,webelieveSLP \ncompilationtechniqueshavethepotentialtobecomeanin\u00adtegralpartofgeneralpurposecomputinginthenearfuture. \n9Acknowledgments WethankKrsteAsanoviccforhisinputonvectorprocess\u00adingandtheCAGgroupmemberswhoprovidedfeedback \nduringthewritingofthispaper,particularlyMichaelTay\u00adlor,DerekBruening,MikeZhang,DarkoMarinov,Matt FrankandMarkStephenson.RaduRuginacontributedhis \npointeranalysispackageaswellashisowntimetoimple-mentnewextensions.ManasMandal,KalpeshGala,Brian GraysonandJamesYangatMotorolaprovidedaccessto \nmuchneededdevelopmenttoolsandtechnicalexpertise.We alsothanktheanonymousreviewersfortheirconstructive \ncomments.Finally,wethankMattDeedsforhishelpinthe completionofthispaper. ThisresearchwasfundedinpartbyNSFgrant \nEIA9810173andDARPAgrantDBT63-96-C-0036. References [1]E.Albert,K.Knobe,J.Lukas,andG.Steele,Jr. CompilingFortran8xarrayfeaturesfortheConnec\u00adtionMachinecomputersystem.InProceedingsofthe \nACMSIGPLANSymposiumonParallelProgramming: ExperiencewithApplications,Languages,andSystems (PPEALS),NewHaven,CT,July1988. \n[2]J.R.AllenandK.Kennedy.PFC:AProgramtoCon\u00advertFortrantoParallelForm.InK.Hwang,editor, Supercomputers:DesignandApplications,pages186{ \n203.IEEEComputerSocietyPress,SilverSpring,MD, 1984. [3]KrsteAsanovicc,JamesBeck,BertrandIrissou, BrianE.D.Kingsbury,NelsonMorgan,andJohn \nWawrzynek.TheT0VectorMicroprocessor.InPro\u00adceedingsofHotChipsVII,August1995. [4]D.CallahanandP.Havlak.ScalarexpansioninPFC: \nModifcationsforParallelization.SupercomputerSoft\u00adwareNewsletter5,Dept.ofComputerScience,Rice University,October1986. \n[5]DerekJ.DeVries.AVectorizingSUIFCompiler:Imple\u00admentationandPerformance.Master'sthesis,University ofToronto,June1997. \n[6]KeithDiefendorf.PentiumIII=PentiumII+SSE. MicroprocessorReport,13(3):1,6{11,March1999. [7]KeithDiefendorf.Sony'sEmotionallyChargedChip. \nMicroprocessorReport,13(5):1,6{11,April1999. [8]KeithDiefendorfandPradeepK.Dubey.HowMulti-mediaWorkloadsWillChangeProcessorDesign.IEEE \nComputer,30(9):43{45,September1997. [9]G.H.Barnes,R.Brown,M.Kato,D.J.Kuck,D.L. Slotnick,andR.A.Stokes.TheIlliacIVComputer. \nIEEETransactionsonComputers,C(17):746{757,Au\u00adgust1968. [10]LinleyGwennap.AltiVecVectorizesPowerPC.Micro\u00adprocessorReport,12(6):1,6{9,May1998. \n[11]CraigHansen.MicroUnity'sMediaProcessorArchitec\u00adture.IEEEMicro,16(4):34{41,Aug1996. [12]D.J.Kuck,R.H.Kuhn,D.Padua,B.Leasure,and \nM.Wolfe.DependenceGraphsandCompilerOptimiza\u00adtions.InProceedingsofthe8thACMSymposiumon PriciplesofProgrammingLanguages,pages207{218, \nWilliamsburg,VA,Jan1981. [13]SamuelLarsen,RaduRugina,andSamanAmaras\u00adinghe.AlignmentAnalysis.TechnicalReportLCS\u00adTM-605,MassachusettsInstituteofTechnology,June \n2000. [14]CorinaG.LeeandDerekJ.DeVries.InitialResultson thePerformanceandCostofVectorMicroprocessors. \nInProceedingsofthe30thAnnualInternationalSym\u00adposiumonMicroArchitecutre,pages171{182,Research TrianglePark,USA,December1997. \n[15]CorinaG.LeeandMarkG.Stoodley.SimpleVector MicroprocessorsforMultimediaApplications.InPro\u00adceedingsofthe31stAnnualInternationalSymposium \nonMicroArchitecutre,pages25{36,Dallas,TX,Decem\u00adber1998. [16]RubyLee.SubwordParallelismwithMAX-2.IEEE \nMicro,16(4):51{59,Aug1996. [17]GlennLueckeandWaqarHaque.EvaluationofFor\u00adtranVectorCompilersandPreprocessors.Software| \nPracticeandExperience,21(9),September1991. [18]MarcTremblayandMichaelO'ConnorandVenkatesh NarayananandLiangHe.VISSpeedsNewMediaPro-cessing.IEEEMicro,16(4):10{20,Aug1996. \n[19]Motorola.AltiVecTechnologyProgrammingEnviron\u00admentsManual,November1998. [20]AlexPelegandUriWeiser.MMXTechnologyExten\u00adsiontoIntelArchitecture.IEEEMicro,16(4):42{50, \nAug1996. [21]RaduRuginaandMartinRinard.PointerAnalysisfor MultithreadedPrograms.InProceedingsoftheSIG\u00adPLAN'99ConferenceonProgrammingLanguageDe\u00adsignandImplementation,Atlanta,GA,May1999. \n[22]MarkStephenson,JonathonBabb,andSamanAma\u00adrasinghe.BitwidthAnalysiswithApplicationtoSili\u00adconCompilation.InProceedingsoftheSIGPLAN'00 \nConferenceonProgrammingLanguageDesignandIm\u00adplementation,Vancouver,BC,June2000. [23]R.P.Wilson,R.S.French,C.S.Wilson,S.P.Amaras\u00adinghe,J.M.Anderson,S.W.K.Tjiang,S.-W.Liao,C.\u00adW.Tseng,M.W.Hall,M.S.Lam,andJ.L.Hennessy. \nSUIF:AnInfrastructureforResearchonParallelizing andOptimizingCompilers.ACMSIGPLANNotices, 29(12):31{37,December1994. \n \n\t\t\t", "proc_id": "349299", "abstract": "<p>Increasing focus on multimedia applications has prompted the additionof multimedia extensions to most existing general purpose microprocessors.  This added functionality comes primarily with the addition of short SIMD instructions.  Unfortunately, access to these instructions is limited to in-line assembly and library calls. Generally, it has been assumed that vector compilers provide the most promising means of exploiting multimedia instructions. Although vectorization technology is well understood, it is inherently complex and fragile. In addition, it is incapable of locating SIMD-style parallelism within a basic block.</p><p>In this paper we introduce the concept of <italic>Superword Level Parallelism (SLP)</italic> ,a novel way of viewing parallelism in multimedia and scientific applications. We believe SLPP is  fundamentally different from the loop level parallelism exploited by traditional vector processing, and therefore demands a new method of extracting it.  We have developed a simple and robust compiler for detecting SLPP that targets basic blocks rather than loop nests.  As with techniques designed to extract ILP, ours is able to exploit parallelism both across loop iterations and within basic blocks. The result is an algorithm that provides excellent performance in several application domains. In our experiments, dynamic instruction counts were reduced by 46%. Speedups ranged from 1.24 to 6.70.</p>", "authors": [{"name": "Samuel Larsen", "author_profile_id": "81100221693", "affiliation": "MIT Laboratory for Computer Science, Cambridge, MA", "person_id": "PP14087074", "email_address": "", "orcid_id": ""}, {"name": "Saman Amarasinghe", "author_profile_id": "81100533031", "affiliation": "MIT Laboratory for Computer Science, Cambridge, MA", "person_id": "PP14184970", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/349299.349320", "year": "2000", "article_id": "349320", "conference": "PLDI", "title": "Exploiting superword level parallelism with multimedia instruction sets", "url": "http://dl.acm.org/citation.cfm?id=349320"}