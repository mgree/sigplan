{"article_publication_date": "05-01-2000", "fulltext": "\n O.-line Variable Substitution for Scaling Points-to Analysis Atanas Rountev Satish Chandra Department \nof Computer Science Bell Laboratories Rutgers University Lucent Technologies rountev@cs.rutgers.edu chandra@research.bell-labs.com \nAbstract Most compiler optimizations and software productivity tools rely on information about the e.ects \nof pointer dereferences in a program. The purpose of points-to analysis is to com\u00adpute this information \nsafely, and as accurately as is practical. Unfortunately, accurate points-to information is di.cult to \nobtain for large programs, because the time and space re\u00adquirements of the analysis become prohibitive. \nWe consider the problem of scaling .ow-and context-insensi\u00adtive points-to analysis to large programs, \nperhaps contain\u00ading hundreds of thousands of lines of code. Our approach is based on a variable substitution \ntransformation, which is performed o.-line, i.e., before a standard points-to analy\u00adsis is performed. \nThe general idea of variable substitution is that a set of variables in a program can be replaced by \na single representative variable, thereby reducing the input size of the problem. Our main contribution \nis a linear-time algorithm which .nds a particular variable substitution that maintains the precision \nof the standard analysis, and is also very e.ective in reducing the size of the problem. We report our \nexperience in performing points-to analysis on large C programs, including some industrial-sized ones. \nExperiments show that our algorithm can reduce the cost of Andersen s points-to analysis [2] substantially: \non average, it reduced the running time by 53% and the memory cost by 59%, relative to an e.cient baseline \nimplementation of the analysis. Introduction The goal of points-to analysis is to compute, for a given \npointer p in a program, the set of variables whose address p may contain during the execution of the \nprogram. Points\u00adto computation provides a conservative estimate of the set of variables read or written \nindirectly through the pointer dereference *p. This information is important in imple\u00admenting a number \nof compiler optimizations (e.g., common subexpression elimination) as well as software productiv- Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI 2000, Vancouver, \nBritish Columbia, Canada. Copyright 2000 ACM 1-58113-199-2/00/0006...$5.00. ity tools (e.g., program \nslicing). Without such information, overly pessimistic assumptions must be made about pointer dereferences. \nPrecision and e.ciency of points-to analysis are important issues. Without good precision small points-to \nsets the analysis may be of limited use for its intended purpose.1 Without good e.ciency, it is infeasible \nto scale the analysis to large programs. Unfortunately, precision and e.ciency usually come at each other \ns expense. In this paper, we consider the problem of performing points\u00adto analysis on rather large programs \nperhaps, up to a mil\u00adlion lines of code while maintaining a reasonable level of precision. The .rst points-to \nanalysis algorithm that could handle programs of this size is due to Steensgaard [15]. Steensgaard s \nanalysis runs in time almost linear in the size of the program. Unfortunately, the results obtained by \nthis analysis are relatively imprecise. An alternative algorithm due to Andersen [2] can produce much \nmore precise re\u00adsults. However, Andersen s analysis has cubic time com\u00adplexity, which limits its scalability. \n(These two algorithms are described further in Section 2.1.) Straightforward imple\u00admentations of this \nanalysis are ill-suited for handling large programs, because the time and space requirements become very \nhigh and often prohibitive. The key to scaling Ander\u00adsen s analysis to larger programs, therefore, is \nin reducing the space and time requirements of the implementation. An example of such an approach is \nthe implementation of An\u00addersen s analysis in [5], which uses cycle elimination to re\u00adduce signi.cantly \nthe resource requirements of the analysis. (Cycle elimination is discussed further in Section 4.3.) We \nhave designed and implemented a new algorithm that improves the scalability of Andersen s analysis by \nreducing the input size of the problem. Using this algorithm, we have been able to perform Andersen s \nanalysis on a program con\u00adtaining nearly 500,000 lines of code in about 7 minutes. Our experiments show \nthat in practice, our algorithm can reduce time requirements by as much as 65% (53% on average), and \nspace requirements by 68% (59% on average), relative to the time and space requirements of an e.cient \nimplementation based on the techniques in [5] and [16], which include cycle elimination and other optimizations. \nWe .nd it useful to describe our algorithm in the framework of variable substitution for points-to analysis. \nThe general 1Admittedly, the question whether the results are precise enough for an intended application \nis a di.cult one to answer without ac\u00adtually putting the results to use. We will talk of precision only \nin a relative sense. 47 3QQs idea behind variable substitution is to reduce the cost of points-to analysis \nby replacing a set of program variables with a single representative variable. Depending on how the substitution \nis performed, this may result in less precise points-to results. For example, one can consider Steens\u00adgaard \ns analysis as performing a speci.c kind of variable sub\u00adstitution that results in loss of precision relative \nto Ander\u00adsen s analysis. The approach that we present in this paper is essentially a new kind of variable \nsubstitution, with two key di.erences: one, it maintains the precision of Andersen s analysis, and two, \nit is performed o.-line, i.e., before the ac\u00adtual analysis. An o.-line technique decouples the resource \noptimization from the core analysis, making it independent of any particular implementation of the core \nanalysis. The o.-line variable substitution used by our algorithm ex\u00adploits a commonly found program \nbehavior, that many point\u00ader variables in a program have the same points-to set. The reason for this \nbehavior is that once an address is taken, it is often passed around the program; for example, the ad\u00address \nvalue may be used as an actual argument in a chain of procedure calls. If we could precompute a set of \npointer variables that must have the same points-to answer , we only need to perform points-to analysis \nfor one representa\u00adtive variable from that set. This lets us reduce the size of the problem that we hand \nover to the analysis. While we have concentrated on reducing the cost of Andersen s anal\u00adysis, we conjecture \nthat such precomputation can be helpful in other points-to analyses as well. The contributions of our \nwork are the following: We propose o.-line variable substitution as a technique for reducing the cost \nof points-to analysis, possibly at the expense of some loss of precision.  We present a linear-time \nalgorithm for computing a particular substitution which substantially reduces the cost of Andersen s \nanalysis without any loss of preci\u00adsion.  We present an experimental study on a set of large C programs, \nup to nearly 750,000 lines of code, to show the advantages of our substitution, and describe why these \nadvantages are achieved.  The rest of the paper is organized as follows. Section 2 pro\u00advides background \nmaterial on points-to analysis algorithms and describes the notion of variable substitution. Section \n3 de.nes the formal model for o.-line variable substitution, and presents a particular substitution based \non an equiva\u00adlence relation on program variables; Section 4 presents an algorithm for computing this \nequivalence relation. Section 5 describes the design and implementation of points-to analy\u00adsis based \non our algorithm. Section 6 presents experimental results on a set of large programs. Section 7 discusses \nre\u00adlated work, and Section 8 presents conclusions and future work. Points-to Analysis and Variable Substitution \nPoints-to analysis starts by de.ning a .nite set V of vari\u00adables; each variable represents one or more \nmemory loca\u00adtions. Points-to relationships are represented using a points\u00adto graph in which nodes correspond \nto variables. A directed edge from node v1 to node v2 shows that one of the memory Andersen s graph a \n=&#38;ba b =&#38;c a =&#38;d d =&#38;e Steensgaard s graph a - b,d - c,e  Figure 1: Andersen s vs Steensgaard \ns analysis. locations represented by v1 may contain the address of one of the memory locations represented \nby v2. Flow-sensitive points-to analysis takes into account the .ow of control be\u00adtween di.erent program \npoints; in contrast, .ow-insensitive points-to analysis ignores control .ow.2 Context-sensitive points-to \nanalysis considers (sometimes approximately) only program paths along which calls and returns are properly \nmatched, while context-insensitive points-to analysis does not make this distinction. 2.1 Flow-and Context-Insensitive \nPoints-to Analysis Flow-and context-insensitive points-to analysis computes a single points-to graph \nfor the whole program. Andersen s analysis [2] is a relatively precise analysis from this category; it \nhas O(n 3) worst-case complexity, where n is the size of the program. The analysis can be modeled as \nan iterative computation in which each iteration starts with a partial points-to graph and then adds \nnew edges according to the semantics of program statements. Thus, for the assignment p = q , new edges \nare added from p to all current successors of q; for the statement p = *q , new edges are added from \np to the successors of all successors of q;and so on for other statements. This model can be formalized \nby de.ning a lattice of points-to graphs L =2V \u00d7V and a transfer function f : L . L, which encodes the \ncumulative points-to e.ect of all program statements. The .nal solution is the limit of the .nite sequence \n\u00d8,f (\u00d8),f 2(\u00d8), etc. A relatively inexpensive .ow-and context-insensitive points\u00adto analysis is Steensgaard \ns analysis [15]. It has O(na(n, n)) complexity (where a is the inverse of Ackermann s function), but \nis much less precise than Andersen s analysis. In this approach, the nodes in the points-to graph represent \nsets of variables, and each node has at most one outgoing edge. After new edges are added in the manner \ndescribed above, a node may have more than one outgoing edge and all of its successors need to be merged \ninto a single node. An exam\u00adple of Andersen s and Steensgaard s analyses, due to [14], is shown in Figure \n1; points-to pairs (b, e)and (d, c) in Steens\u00adgaard s graph are spurious. The nodes of Steensgaard s \ngraph de.ne a partition of V into disjoint equivalence classes; the variables at each node form a separate \nequivalence class. The partition is stored in a fast union/.nd data structure [17], which allows e.cient \nnode merges. One of the variables in each class is used as an equivalence class representative (ECR) \n[15]. During the analysis, every reference to a variable v is translated into a reference to the ECR \nfor the equivalence class to which v 2Some analyses (e.g., [7, 9]) only ignore intraprocedural .ow of \ncontrol. currently belongs. Shapiro and Horwitz [14] propose a family of points-to anal\u00adyses whose cost \nand precision range from those of Steens\u00adgaard s analysis to those of Andersen s analysis. Similarly \nto Steensgaard s analysis, the nodes in the points-to graph represent sets of variables. Each node has \nat most k out\u00adgoing edges, where k is one of the parameters of the analy\u00adsis. This is achieved by assigning \neach variable to one of k categories and merging nodes only if they are in the same category. The boundary \ncases k =1 and k = |V|correspond to Steensgaard s and Andersen s analysis, respectively.  2.2 Variable \nSubstitution A common feature of Steensgaard s and Shapiro-Horwitz s analyses is the use of an ECR as \na placeholder for all vari\u00adables in the equivalence class. The analyses can be thought of as performing \non-the-.y translation from variables to representatives. Alternatively, this approach can be consid\u00adered \nas modifying the program by replacing (substituting) each occurrence of a variable with the current representative \nfor that variable. For example, the assignment p = q can be thought of as e(p)= e(q) , where e(p)and \ne(q)are the current ECRs for p and q. As seen from Steensgaard s and Shapiro-Horwitz s analyses, replacing \na set of variables with a single placeholder variable can be used to reduce the cost of the analysis; \nof course, some loss of precision may occur. We call this general cost-saving technique variable substitution. \nIn our speci.c variable sub\u00adstitution, rather than performing the substitution during the analysis (as \nin Steensgaard s and Shapiro-Horwitz s analy\u00adses), we perform it o.-line,or before the analysis. In this \napproach, the program is .rst modi.ed by replacing every occurrence of each variable with the representative \nfor that variable. Points-to analysis is then applied to the modi.ed program and the resulting solution \nis used to recover a so\u00adlution for the original program. The next section describes this approach in \nthe context of Andersen s analysis. 3 O.-line Variable Substitution In this section, we .rst de.ne a \nmodel for o.-line variable substitution and show that it is safe (i.e., the substitution does not cause \nany points-to relation to be missed from the .nal answer). We then describe a substitution based on a \nspeci.c partitioning of program variables into equivalence sets. This substitution preserves the precision \nof Ander\u00adsen s analysis, and in practice, also reduces substantially the cost of the remaining points-to \ncomputation. We also dis\u00adcuss simpli.cations made possible by discovering the precise points-to solutions \nof some variables during our computa\u00adtion of equivalence sets; the computation itself is described in \nSection 4. 3.1 De.nition and Safety To simplify the presentation, we assume that the program is represented \nby a set of basic statements, as described by the grammar in Figure 2; some preprocessing may be needed \nto transform the program to this standard form. O.-line variable substitution is based on a substitution \nfunc-Var .identi.er Assign .Var =&#38;Var |Var = Var |Var = *Var |*Var = Var FunDef .Var (Var,...,Var) \n.Var Call .Var = Var (Var,...,Var) FunPtrCall .Var =(*Var)(Var,...,Var) Stmt .Assign |FunDef |Call |FunPtrCall \nFigure 2: Grammar for basic statements. FunDef contains the function name, the formals, and a unique \nvariable rep\u00adresenting the return value of the function. A function call contains a variable to which \nthe return value is assigned. tion s : V .V' such that each variable from V is either mapped to itself, \nor to a fresh variable that is not in V. Thus, some variables from V are preserved by s, while others \nare replaced by representative variables. In general, many variables could be mapped to the same representative. \nBased on s, the original program is modi.ed by replacing each occurrence of a variable v with s(v). Let \nL and L' denote the lattices of points-to graphs for the original and the modi.ed program, respectively. \nThe modi.ed program corresponds to a new transfer function f' : L' .L'. Applying Andersen s analysis \nto this modi.ed program yields a solution G'S which is the limit of the se\u00adquence \u00d8,f'(\u00d8),f'(f'(\u00d8)), \netc. A solution GS for the original program can be obtained as GS = {(vi,vj)|(s(vi),s(vj)) . G'S}. This \napproach is illustrated by the example in Figure 3. Note that trivial assignments such as x= x can be \nelimi\u00adnated after the substitution. In general, G'S could be signif\u00adicantly smaller than Andersen s points-to \ngraph GA for the original program. Thus, the running time and memory cost of the analysis could be signi.cantly \nreduced. It is also clear that some loss of precision may occur for example, edges (a,q)and (p,q)in GS \nare spurious. The safety of o.-line variable substitution is guaranteed by the following claim: Theorem \n1 GA .GS ,where GA is Andersen s points-to graph for the original program. For each G .L,let sL(G)= {(s(u),s(v))|(u,v) \n.G}.We can prove by induction on i that sL(Gi) .Gi',where Gi is the partial points-to graph for the original \nprogram after it\u00aderation i,and G'i is the corresponding graph for the modi.ed program. Thus, sL(GA) .GS', \nwhich implies GA .GS . O.-line variable substitution can be used for constructing approximate points-to \nanalyses di.erent substitution func\u00adtions result in di.erent tradeo.s between cost and precision. Even \nthough we present and use o.-line variable substitu\u00adtion in the context of Andersen s analysis, the technique \nis applicable to other kinds of points-to analysis. For exam\u00adple, it could be useful in investigating \ntradeo.s between cost and precision for pointer analyses with some degree of .ow or context sensitivity \n(e.g., [8, 7, 4, 18, 9]). a - b - c  GS ' GS a=&#38;bs(a)= x b=&#38;cx =&#38;y s(p)= x - b=&#38;d \ny =&#38;cx - y - c  @R s(b)= y @ q = b y =&#38;d s(q)= yp= a - @@  @ @R R d GA = GS -{(a,q),(p,q)} \n Figure 3: O.-line variable substitution example. Graph G ' S is the points-to graph after the substitution. \nGraph GS is the corresponding points-to graph in terms of variables from the original program. Edges \n(a,q)and (p,q) are spurious with respect to Andersen s points-to graph GA; this substitution does not \npreserve precision. p - q - d   3.2 Substitution of Equivalence Sets In this subsection we propose \na particular o.-line variable substitution that reduces the cost of Andersen s analysis without any loss \nof precision. The substitution is based on a collection of equivalence sets. An equivalence set is a \nset of variables that have the same points-to solution in GA. Any two variables that belong to the same \nequivalence set are equivalent variables. In Section 4 we show how to com\u00adpute some equivalence sets \nin linear time. We would like to keep the cost of this computation low, because it must be included in \nthe total cost of the analysis. The substitution is based on a precomputed collection of disjoint equivalence \nsets S0,S1,...,Sk. (This collection need not cover the entire set of variables.) We use a single fresh \nrepresentative variable ri for each equivalence set Si.For each v .V, s(v) is de.ned as follows: If \nv is a function name, let s(v)= v  Otherwise, if v s address is taken somewhere in the program, let \ns(v)= v  Otherwise, if v .Si,let s(v)= ri  Otherwise, let s(v)= v  The precision of this substitution \nis guaranteed by the fol\u00adlowing theorem: Theorem 2 For the above de.nition of s, GA = GS. We can prove \nby induction on i that G ' i . sL(GA), where Gi ' and sL are the same as in the proof of Theorem 1; thus, \nGS . GA and therefore GS = GA. The proof is based on three observations. First, equivalent variables \nbehave simi\u00adlarly with respect to Andersen s analysis and can be replaced by a single representative \nwithout any loss of precision. Sec\u00adond, it is necessary to preserve all variables that are function names; \nthis is needed to ensure that actual/formal pairs in the modi.ed program are properly matched. Third, \nit is necessary to preserve all targets of points-to edges; this is achieved by preserving all variables \nwhose address is taken somewhere in the program. Consider again the example in Figure 3. Variables b \nand q are equivalent. However, since the address of b is taken, mapping both b and q to the same representative \nvariable results in the spurious edges (a,q) and (p,q). If there is a large number of equivalent variables, \nGS ' is sig\u00adni.cantly smaller than GA, resulting in reduction in the run\u00adning time and the memory cost \nof Andersen s analysis.  3.3 Non-pointer Elimination After the substitution has been performed, the \ncost of the analysis can be further reduced by non-pointer elimination. A non-pointer is a variable with \nan empty points-to set in GA. Our computation of equivalence sets is capable of identifying some of the \nnon-pointers in the program.3 Sup\u00adpose that all variables in equivalence sets Si,Sj ,...,Sm are non-pointers. \nThe following statements involving the repre\u00adsentatives ri,rj ,...,rm are irrelevant to the propagation \nof pointer values and can be safely eliminated: Assignments p= ri and ri = q  Assignments *p= ri and \nri = *q  Direct calls ri = f(rj,...,rm)  Indirect calls ri =(*fp)(rj ,...,rm )  Our computation of \nequivalence sets can sometimes also identify variables that point to exactly one variable in GA; we refer \nto such variables as known pointers.If all vari\u00adables in some equivalence set Si point only to the variable \nx, each occurrence of *ri in the modi.ed program can be replaced by x. This, in turn, may reduce the \ncost of the subsequent points-to analysis. We refer to this transforma\u00adtion as known-pointer instantiation. \n4 Equivalence Sets Computation In this section we present our linear-time algorithm for com\u00adputing equivalence \nsets. During the computation, the algo\u00adrithm also identi.es some of the non-pointers and known pointers \nin the program. In order to keep the cost of the computation low, the algorithm only computes equivalence \nsets that are easy to detect; nevertheless, our experiments show that this approach discovers a signi.cant \nnumber of equivalent variables. The algorithm starts by constructing a subset graph G.. Intuitively, \nthe edges in G. represent subset relationships between the points-to solutions for the nodes. For each \npro\u00adgram variable v, the subset graph contains node n(v)and node n(*v). If the address of v is takenanywhere \ninthe program, the subset graph also contains node n(&#38;v). The edge set of G. represents all subset \nrelationships that can be directly inferred from the program; it is de.ned as follows: 3 Due to the weak \ntype system of C, declared types cannot be used to identify non-pointers.              \n   . . 6 r =&#38;ps = *r x -*p  j *jp=&#38;xt = *r @@ R q =&#38;yy @ = *p I 0 @Ri 9 @*i  p= qi \n= j @ - k 0 *k 11 *sy *qq = *ri = k s =&#38;x 3  47 0 10 4 8 Figure 4: Subset graph example. The numbers \ndenote the labels assigned by the algorithm from Figure 6. The direct SCCs are {n(r)},{n(s)},{n(t)},{n(j)},{n(k)}, \nand {n(i)}. The non-trivial equivalence sets are {p,q,t} and {j,k,i}; the latter is a non-pointer set. \n For each assignment p=&#38;x , G. contains the edges (n(&#38;x),n(p)) and (n(x),n(*p)).  For each assignment \np = q , G. contains the edges (n(q),n(p)) and (n(*q),n(*p)).  For each assignment p = *q , G. contains \nthe edge (n(*q),n(p))  For each assignment *p = q , G. contains the edge (n(q),n(*p))  For each pair \nof an actual aand a corresponding formal f in a direct call, G. contains the edges (n(a),n(f)) and (n(*a),n(*f)) \n For each direct call where r is the return variable of the called function and p is assigned the return \nvalue at the call, G. contains the edges (n(r),n(p)) and (n(*r),n(*p))  Figure 4 shows a set of basic \nstatements and the correspond\u00ading subset graph; isolated nodes in the graph are not shown. Andersen s \npoints-to graph for this program is given in Fig\u00adure 5. Let Pt(v) be the points-to set for variable v \nin the .nal Andersen s points-to solution. We can associate a points-to set with each node in G. as follows: \nthe points-to set of n(v) is Pt(v); the points-to set of n(*v) is the union of Pt(x )for all variables \nx. Pt(v); the points-to set of n(&#38;v)is {v}.It is straightforward to prove the following claim: Theorem \n3 For every edge (n1,n2) in the subset graph, Pt(n1) . Pt(n2). As a corollary, all nodes in a strongly \nconnected component (SCC) of the subset graph have the same points-to solution. 4.1 Direct Nodes Intuitively, \na direct node n(v) corresponds to a variable v for which we can track directly all values assigned to \nv.The points-to set of a direct node depends only on the points-to sets of its predecessor nodes in the \nsubset graph. We do not consider dereference nodes n(*v) and address-of nodes n(&#38;v) to be direct. \nA variable node n(v)is directonly if all of the following conditions hold: Variable v never has its address \ntaken, because in this case we cannot track indirect assignments to v through statements of the form \n*p= q . Variable v is not a formal parameter for a function whose address is taken, because we may not \nknow all the corresponding actuals from which the formal gets its values.  Variable v is not assigned \nthe return value of a call through a function pointer, because, again, we do not know all the corresponding \nreturn values.  The last two conditions are needed because we do not know function pointer targets apriori; \nrecall that we ignore indi\u00adrect calls when constructing the subset graph. In the exam\u00adple from Figure \n4, all variable nodes except n(p), n(x)and n(y) are direct. It is easy to prove the following theorem: \nTheorem 4 If n is a direct node,Pt(n) is equal to the union of Pt(m),where m is a predecessor of n in \nthe subset graph. As a corollary, if all predecessors of n have thesamepoints\u00adto set, n also has that \nsame points-to set.  4.2 Computation of Equivalence Sets After constructing the subset graph, the analysis \ncomputes its strongly connected components and builds its SCC con\u00addensation. The resulting graph G ' \nis a directed acyclic . graph in which each node corresponds to a SCC in G..A direct node n ' in G ' \nrepresents a SCC that contains only . direct nodes from G.. Using Theorem 4, it is easy to prove that \nthe points-to set of such n ' is the union of the points-to sets of its predecessors. The equivalence \nsets can be computed by traversing G.' in topological sort order, as shown in Figure 6. The algorithm \nassigns an integer label to each node. If two nodes are as\u00adsigned the same label, their points-to sets \nare the same. Spe\u00adcial consideration is given to nodes that can be shown to have empty points-to sets \nall such nodes are assigned label 0. Labels that correspond to known points-to sets are stored in the \npartial map Solved. The algorithm outputs two maps. VarLabel is a complete map from program variables \nto integer labels. It encodes the equivalence sets if two variables have the same label, they belong \nto the same equivalence set. Furthermore, any variable with label 0 is a non-pointer. Solved is a partial \nmap from integer labels to program variables; if (l,x) . Solved, the points-to set of any variable with \nlabel l is exactly {x}. '' The algorithm runs in time O(max (|N.|,|E.|)), which, in turn, is linear in \nthe number of normalized statements in the input program. 51 s y q x r p r =&#38;ps = *r i j k \n' input G.' =(N.,E ' .) p =&#38;xt = *r output VarLabel : array[V ] of integer q =&#38;yy = *p Solved \n: map integer . V p = q i = j q = *ri = k ' declare SccLabel : array[N.] of integer t counter: integer \ns =&#38;x   PPPP Figure 5: Andersen s points-to graph for Figure 4. Variables + QQs  PPq Q ) + QQs \nQ counter := 1; '' foreach n . N. in topological sort order do if n ' is not a direct node then p, q, \ns,and t have the same points-to set. Variables i, j,and SccLabel[n ' ]:= counter; k are non-pointers. \nThe example in Figure 4 shows one possible assignment of labels to the SCCs of the subset graph. Suppose \n{n(&#38;p)} is the .rst SCC in the topological sort order. Since it is not direct, it is assigned label \n1 and (1,p) is added to Solved. The SCC {n(r)} is direct and gets label 1; furthermore, the points-to \nsolution for r canbe looked upin Solved as {p}. The SCC {n(q),n(p),n(*r)} is not direct and is assigned \nthe fresh label 4. Since {n(t)} is direct, its gets the same label; as a result, the equivalence set \n{p, q,t} is detected. The SCC {n(j)} is direct and has no predecessors; thus, in the original C program \nj gets all of its values from non-pointer assignments such as j = 1 . Since j and k are non-pointers, \n{n(j)} and {n(k)} are assigned label 0. The SCC {n(i)} is direct and gets label 0 as well; as a result, \nthe non-pointer equivalence set {j, k, i} is discovered. Note that the analysis does not detect the non-pointers \nx and y. Also, even though s has the same points-to solution as p, q,and t, the algorithm cannot add \ns to that equivalence set because the points-to sets of the two predecessors of n(s) are di.erent. We \ncould potentially maintain additional data structures to be able to track inclusion properties among \nlabels, which would allow us to infer that n(s) should also get the label 4. One might wonder why we \nkeep n(*v)nodes in G. at all, since these are not direct nodes, and any SCC that contains a n(*v) node \nis not direct as well. The reasons for keeping such nodes are, one, that they may lead to forming larger \nSCC components, and two, a successor of a non-direct node in G ' . could be a direct node which inherits \nthe label from its predecessor, thus adding members to the equivalence set. Both reasons are illustrated \nby node n(*r)in Figure4. In the example, the only non-singleton equivalence sets are S4 = {p, q,t} and \nS0 = {i, j, k}. They result in the following substitutions: s(q)= r4, s(t)= r4, s(i)= r0, s(j)= r0, and \ns(k)= r0.Variable p is not substituted by r4 because its address is taken in the program. The rest of \nthe variables belong to singleton equivalence sets and are not substituted. Variable r is a known pointer, \nas the computation reveals that r only points to p. Consequently, we instantiate each occurrence of *r \nby p. We present the following input to Andersen s analysis: r =&#38;p, p =&#38;x, r4 =&#38;y, p = r4,r4 \n= p, s =&#38;x, s = p, r4 = p, y = *p. By Theorem 2, the points\u00adto solution obtained by Andersen s analysis \non this input program, after reverse mapping, is the same as the points\u00adto solution obtained by applying \nAndersen s analysis on the original program. 4.3 Comparison with Cycle Elimination counter := counter \n+ 1; if n ' is an address-of node n(&#38;v) then add (SccLabel [n ' ],v)to Solved; else if n ' has no \npredecessors then SccLabel[n ' ]:= 0; else if all predecessors of n ' have the same label l then SccLabel[n \n' ]:= l; else SccLabel[n ' ]:= counter; counter :=counter +1; foreach v . V do VarLabel[v]:= SccLabel[ContainingScc(n(v))]; \nFigure 6: Computation of equivalence sets. set-inclusion constraint graphs [5]. In both graphs, nodes \nrepresent points-to sets and edges represent subset relation\u00adships.4 A cycle in the constraint graph \ncorresponds to vari\u00adables v0,...,vk such that Pt(v0 ) . Pt(v1 ) . ... . Pt(vk ) . Pt(v0 ) where Pt(vi \n)is the points-to set of vi. All variables on a cy\u00adcle have equal points-to sets and can be replaced \nby a single representative variable. In [5], cycle detection and elimi\u00adnation is used to improve the \nperformance of Andersen s analysis. The strongly connected components in the subset graph also .nd equivalence \nrelationships due to cycles, but only in the initial set of inclusion relationships apparent from the \npro\u00adgram. Most of the bene.t of cycle elimination in [5], how\u00adever, comes from .nding and eliminating \ncycles online as fresh ones appear during constraint solving, rather than only in the initial graph. \nOur algorithm does not discover such cycles, because unlike [5], no new edges are added during the analysis. \nFor example, consider the assignments p =&#38;x and *p = y . In [5], the analysis will use the points-to \npair (p, x) to determine that the value of y is assigned to x; this results in the new constraint Pt(y) \n. Pt(x ), which may result in new cycles being found. In our subset graph, there will be no edge from \nn(y)to n(x). Instead, our algorithm creates equivalence sets that cross cycle boundaries. By propagating \na label from a node to some of its successors, our approach is capable of discovering equivalent variables \nthat do not belong to any cycle. For example, in Figure 4, variables p, q,and t are found to be in the \nsame equivalence set, even though they do not form a cycle in the sense of constraint graphs. 4We refer \nthe reader to [5] to see how set constraints can be used Our algorithm, in particular the subset graph \nconstruction, to model Andersen s analysis. bears some connection with the idea of cycle elimination \nin 5 Design and Implementation We investigated the impact of o.-line variable substitution on the cost \nof Andersen s analysis by comparing two versions of the analysis. The standard version implements the \ntra\u00additional Andersen s analysis; the substitution version uses the algorithm from Section 4 to compute \nequivalence sets and then performs o.-line variable substitution, non-pointer elimination and known-pointer \ninstantiation. The standard version is implemented in ML and uses a front end for C also implemented \nin ML. The front end builds a simpli.ed program representation in which the statements are normalized \nto consist of only a few simple forms; for ex\u00adample, each statement contains at most one pointer deref\u00aderence. \nThe analysis traverses the simpli.ed representation and extracts a set of basic statements, as described \nin Fig\u00adure 2. Similarly to [15, 14, 5], structures and arrays are treated as monolithic objects and their \nindividual elements are not distinguished. Library functions are handled by pro\u00adviding stubs that simulate \ntheir points-to e.ects. Dynamic memory allocation is handled by considering each call to malloc and other \nheap-allocating functions as equivalent to taking the address of a new variable unique to the site of \nthe malloc call. As a practical matter, we phase our implementation by writ\u00ading intermediate results \nto disk. Thus the front end extracts the basic statements and stores them on disk. The analy\u00adsis works \non a list of basic statements available in a .le. This approach allows clean separation between the front \nend, equivalence set computation (if any), and the main analysis. The substitution version of Andersen \ns analysis is organized as follows. The equivalence set computation reads the basic statements from disk, \ncomputes variable labels and known pointers, and terminates by writing this information back to disk. \nThe next phase, invoked separately, reads the informa\u00adtion from disk, performs o.-line variable substitution, \nnon\u00adpointer elimination and known-pointer instantiation, and computes Andersen s solution similarly to \nthe standard ver\u00adsion. The standard version and the last phase of the substitu\u00adtion version are both \nbased on the implementation of An\u00addersen s analysis in Bane (Berkeley ANalysis Engine) [1]. Bane is a \ntoolkit for constructing constraint-based program analyses. The public distribution of Bane5 contains \nan ef\u00ad.cient constraint-solving engine, and an implementation of Andersen s analysis that uses this engine \n[5, 16]; to the best of our knowledge, this is the fastest publicly available im\u00adplementation of Andersen \ns analysis. Since our front end and program representation are di.erent, we did not use Bane s implementation \nof the analysis directly. Instead, we created a version that traverses our intermediate rep\u00adresentation, \ngenerates exactly the same kind of constraints as Bane s implementation would have generated, and then \nuses the constraint-solving engine provided by Bane to com\u00adpute the points-to solution. 6 Experiments \nand Results For our experiments, we used a set of C programs ranging in size from 30K to around 750K \nLOC. Some characteristics of the programs are given in Table 1. The third column in 5 http://bane.cs.berkeley.edu \nProgram LOC [K] NumStmt [K] NumVar [K] nethack 30.7 50.2 52.9 burlap 49.6 67.5 66.6 vortex 67.2 55.9 \n71.8 emacs 99.4 108.7 124.7 povray 133.9 116.8 119.1 gcc 217.7 255.3 254.5 switch1 ~ 500 366.5 419.9 \nswitch2 ~ 750 765.7 794.8 Table 1: Data programs. Program TA [sec] SA [MB] TIO [sec] nethack 114.4 76.5 \n3.6 burlap 143.6 94.4 4.8 vortex 154.8 91.9 4.9 emacs 235.2 145.2 8.8 povray 382.4 193.0 8.7 gcc 942.3 \n347.4 18.9 switch1 1054.7 485.3 28.4 switch2  57.3 Table 2: Cost of Andersen s analysis. TA is the \nrunning time and SA is the memory used by the analysis. TIO is the time for disk write/read of basic \nstatements. Table 1 gives the number of basic statements (of the form shown in Figure 2) and the last \ncolumn shows the number of variables in the basic statements. Most of the programs are publicly available, \nexcept switch1 and switch2,which are proprietary programs used in Lucent s products. All experiments \nwere performed using a single 195 MHz pro\u00adcessor on a multi-processor SGI Origin machine with 1.5 GB \nphysical memory. The reported time measurements are the best values out of three runs. The space was \nmeasured by in\u00adstrumenting the ML garbage collector to report the amount of live data after each garbage \ncollection (therefore, the num\u00adbers are somewhat approximate). Table 2 shows the cost of the standard \nversion of Andersen s analysis. Column TA gives the time to perform Andersen s analysis after the basic \nstatements are read from disk. Col\u00adumn SA shows the space needed by the analysis. Column TIO gives the \ntotal time for disk write/read of basic state\u00adments; the average increase in the running time due to \ndisk IO is 3%. The results show that the running time of the standard ver\u00adsion itself is quite reasonable, \ngiven the cubic worst-case complexity of Andersen s analysis. The results also show that for large programs \nthe memory cost of the analysis can be signi.cant. In fact, our largest program could not be analyzed \nbecause the analysis ran out of memory. Clearly, memory could be a bottleneck when analyzing large pro\u00adgrams. \nTable 3 shows the overall performance of the substitution version. Columns TE and SE show the time and \nspace needed to compute the equivalence sets. This computa\u00adtion currently has a simple implementation \nwith no perfor\u00admance tuning; we expect to reduce both the running time and the consumed space by using \na more mature imple\u00admentation. Columns T ' and S ' show the cost of Ander- AA sen s analysis after substitution, \nnon-pointer elimination, and known-pointer instantiation. The reduction in analy\u00adTable 3: Overall performance \nof the substitution version. TE and SE are the time and space needed to compute the equivalence sets. \nTA ' and S ' Program TE [sec] SE [MB] T ' A [sec] S ' A [MB] .T [%] .S [%] nethack 16.9 20.3 46.0 37.1 \n45.0% 51.5% burlap 21.5 25.2 46.5 41.4 52.6% 56.1% vortex 22.3 27.6 57.9 41.7 48.2% 54.6% emacs 35.0 \n46.3 63.3 45.2 58.2% 68.1% povray 45.1 44.6 165.8 90.1 44.8% 53.3% gcc 113.3 95.7 214.7 121.4 65.2% 65.1% \nswitch1 223.4 157.4 211.3 137.6 58.8% 67.6% switch2 465.2 297.0 A are the time and space for Andersen \ns analysis after substitution, non-pointer elimination, and known-pointer instantiation. The last two \ncolumns show the reduction in analysis cost. Program Overall [%] SCC [%] Nptr [%] nethack 66.0% 3.4% \n35.0% burlap 74.5% 4.7% 50.2% vortex 71.4% 1.5% 46.7% emacs 79.1% 5.0% 54.4% povray 76.1% 4.7% 38.3% \ngcc 76.8% 4.4% 31.1% switch1 81.9% 2.0% 56.6% switch2 78.4% 3.4% 38.8% Table 4: Reduction in the number \nof variables. The columns show the overall reduction, the reduction due to SCCs in the subset graph, \nand the reduction due to non-pointers. sis time .T is computed as 1 - (TE + TA' )/TA,where TA is the \ntime from Table 2. Since the equivalence sets are computed separately from Andersen s analysis, the over\u00adall \nspace cost is max(SE , SA' ) and the reduction in space is .S =1 - max(SE , SA' )/SA. It is clear that \nthe reduction in the number of variables results in signi.cant reduction of the analysis cost on average, \n53% for running time and 59% for space. Even with signi.cant reduction in the number of variables, the \nanalysis of switch2 ran out of memory. However, it would be misleading to conclude that either the original \nswitch2, or the program after variable substitution, def\u00adinitely needed more than 1.5 GB of space. The \ngarbage collector in SML/NJ has peak virtual-memory requirements that are considerably higher than the \nsize of the live data.6 In fact, at the time the substitution version ran out of mem\u00adory on a 1.5 GB \nmachine, the live data was only about 500 MB. But based on the experience with other programs, we extrapolate \nthat switch2 could run in much less mem\u00adory using the substitution version than using the standard version. \nTable 4 shows the e.ectiveness of substitution based on equivalence sets. The second column gives the \nfraction of variables eliminated due to substitution. On average, the number of variables is reduced \nby 76%. We performed fur\u00adther experiments to estimate the impact of di.erent sources of this reduction. \nFirst, we estimated the reduction that can be obtained by only computing the SCCs in the subset graph. \nIn this scenario, two variables are equivalent only if they belong to the same SCC. The reduction in \nthe num\u00adber of variables is showninthe thirdcolumnof Table 4. Clearly, little can be gained from SCC \ncomputation alone; The garbage collector [11] is optimized for speed and aimed at medium-sized data sets; \nour data sizes are unusually large for it. Program Size 100-101 Size 101-102 Size 102-103 Size > 103 \nnethack 4116 274 12 0 burlap 3596 230 12 0 vortex 4492 271 5 1 emacs 4563 334 18 2 povray 6834 1021 32 \n0 gcc 16009 1624 78 4 switch1 25667 1417 97 4 switch2 52541 3334 426 13 Table 5: Distribution of equivalence \nset sizes. this is consistent with the observations in [5]. Next, we measured the reduction obtained \nby performing substitution only on non-pointer variables. The reduction in the number of variables is \nshown in the last column of Table 4. Clearly, the approach from Section 4 detects a sig\u00adni.cant number \nof non-pointers, resulting in average reduc\u00adtion of 44%. We also computed the number of all variables \nthat have empty points-to sets in the .nal Andersen s so\u00adlution (the numbers are not shown), and discovered \nthat, on average, 82% of them are detected by the approach from Section 4. The large number of non-pointers \nis due to the fact that in C programs many variables are not intended to be used as pointers and have \nno e.ect on points-to analysis. Because of the weak type system of C (due to type casting), these variables \ncannot be eliminated solely on the basis of their declared types. The remainder of the reduction is obtained \nfrom equivalence sets that span multiple SCC nodes. To obtain some insight into these equivalence sets, \nwe computed the distribution of their sizes; the results are presented in Table 5. Singleton equivalence \nsets are trivial for the purposes of substitution and are not counted. The single non-pointer equivalence \nset is not counted either. In each case, we found that most of the equivalence sets are small, and in \nfact, the number of larger sets tends to decrease polynomially with increasing set size. We also examined \nsome of the programs manually to see why variables form large equivalence sets. We found this was mostly \ndue to passing around pointers to global data structures. Since such pointers are copied many times, \nincluding copies made from actual to formal parameters, a large number of pointers end up having the \nsame points-to set. To our knowledge, this empirical behavior has not been noticed or exploited in program \nanalysis work before. As described in Section 4, the computation of equivalence sets detects some of \nthe known pointers in the program. Af\u00ad ter the substitution, dereferences of such variables (or deref\u00aderences \nof their representatives) are instantiated by their known targets, resulting in a somewhat simpler program. \nOur algorithm .nds between 7% and 20% (12% on average) of all variables as having a known single target. \nFurther ex\u00adperiments show that known-pointer instantiation improves space and time reductions only marginally \n(about 2%). 7 Related Work There is a large body of work on pointer analysis. Some of the analyses concentrate \non pointers that point to heap\u00adallocated memory (e.g., [6, 13]). Others analyze pointers that point to \nstack-based memory; among them, some are .ow-sensitive (e.g., [8, 7, 4, 18]), while others are .ow-insensi\u00adtive \n(e.g., [2, 7, 15, 19, 14, 9]). Several analyses use placeholder variables to represent sets of related \nvariables. The use of equivalence class represen\u00adtatives in Steensgaard s and Shapiro-Horwitz s analyses \nwas already discussed in Section 2. Other examples are the non-visible names from [8], the invisible \nnames from [4], the extended parameters from [18], the cycle witness vari\u00adables from [5], the placeholder \nvariables from [12], and the equivalence class representatives from [10]. In general, the placeholders \nneed not be used throughout the program; for example, in [8, 7, 4], a placeholder is used when analyzing \na called procedure, and the original variable is used in the calling procedure. The .ow-sensitive pointer \nanalysis from [7] uses a subset of a procedure s control-.ow graph which only contains the program points \nat which the solution could change. This reduces the size of the problem because a set of program points \nthat share the same solution can be represented by a single program point. This technique resembles our \nuse of a representative variable for a set of variables that have the same points-to solution. The most \nclosely related work that focuses on speeding up Andersen s analysis is cycle elimination in the Bane \nanalysis system [1], which was already discussed in Section 4.3. The Bane engine was recently enhanced \nwith another important optimization, called projection merging [16]. The version of Bane that we used \nincorporates projection merging as well. The problem of scaling class analysis of object-oriented pro\u00adgrams \nalso lends itself to solutions somewhat analogous to those used for point-to analysis. While a relatively \nprecise O(n 3) algorithm exists, researchers have proposed di.erent levels of approximation, usually \nderived by merging nodes in a graph representation of the problem [3]. It would be interesting to see \nif our techniques for points-to analysis are also useful for class analysis. 8 Conclusions and Future \nWork We have shown that a particular form of o.-line variable substitution based on equivalence sets \ncan be used to re\u00adduce the cost of Andersen s analysis without any loss of pre\u00adcision. The computation \nof equivalence sets has linear-time complexity and signi.cantly reduces the number of variables (by 76% \non average). This reduction translates into signif\u00adicant reduction in the analysis running time (53% \non aver\u00adage) and memory cost (59% on average). We believe that this technique can be widely adapted, \nas it is simple, and independent of any particular implementation of Andersen s analysis. O.-line variable \nsubstitution can be used to develop approx\u00adimate versions of Andersen s analysis. It would be inter\u00adesting \nto investigate substitutions that further reduce the cost of the analysis, possibly at the expense of \nsome pre\u00adcision. O.-line variable substitution could also be useful for developing approximate versions \nof other pointer analy\u00adses, including analyses with some degree of .ow or context sensitivity. Finally, \nit would be interesting to consider uses of the substitution technique for analyses similar to pointer \nanalysis for example, class analysis and escape analysis for object-oriented languages. 9 Acknowledgments \nWe would like to thank the Bane team at Berkeley for distributing their code and answering questions \nabout it. Darren Atkinson provided the preprocessed source code for burlap, emacs and gcc. John Reppy \nprovided assistance with instrumenting the garbage collector. Nevin Heintze and Dino Oliva made several \nimprovements to the front end used in this work. Barbara Ryder, Matthew Arnold, and Pe\u00adter Mataga provided \nhelpful comments on earlier versions of this paper. The .rst author was supported by Bell Labora\u00adtories \nsummer internship and by NSF grant CCR-9900988. References [1] A. Aiken, M. F\u00a8ahndrich, J. Foster, and \nZ. Su. A toolkit for constructing type-and constraint-based program analyses. In Proc. Workshop on Types \nin Compilation, LNCS 1473, pages 78 96, 1998. [2] L. Andersen. Program Analysis and Specialization for \nthe C Programming Language.PhD thesis,DIKU, Uni\u00adversity of Copenhagen, May 1994. [3] G. DeFouw, D. Grove, \nand C. Chambers. Fast interpro\u00adcedural class analysis. In Proc. Symposium on Princi\u00adples of Programming \nLanguages, pages 222 236, 1998. [4] M. Emami, R. Ghiya, and L. Hendren. Context\u00adsensitive interprocedural \npoints-to analysis in the pres\u00adence of function pointers. In Proc. Conference on Pro\u00adgramming Language \nDesign and Implementation, pages 242 257, 1994. [5] M. F\u00a8ahndrich, J. Foster, Z. Su, and A. Aiken. Partial \nonline cycle elimination in inclusion constraint graphs. In Proc. Conference on Programming Language \nDesign and Implementation, pages 85 96, 1998. [6] R. Ghiya and L. Hendren. Is it a tree, a DAG or a cyclic \ngraph? In Proc. Symposium on Principles of Programming Languages, pages 1 15, 1996. [7] M. Hind, M. Burke, \nP. Carini, and J. D. Choi. Interpro\u00adcedural pointer alias analysis. ACM Transactions on Programming Languages \nand Systems, 21(4):848 894, July 1999. [8] W. Landi and B. G. Ryder. A safe approximation al\u00adgorithm \nfor interprocedural pointer aliasing. In Proc. Conference on Programming Language Design and Im\u00adplementation, \npages 235 248, 1992. [9] D. Liang and M. J. Harrold. E.cient points-to analysis for whole-program analysis. \nIn Proc. Symposium on the Foundations of Software Engineering, LNCS 1687, pages 199 215, 1999. [10] D. \nLiang and M. J. Harrold. Equivalence analysis: A general technique to improve the e.ciency of data-.ow \nanalyses in the presence of pointers. In Proc. Workshop on Program Analysis for Software Tools and Engineer\u00ading, \npages 39 46, 1999. [11] J. Reppy. A high-performance garbage collector for Standard ML. Technical memorandum, \nAT&#38;T Bell Laboratories, Dec. 1993. [12] A. Rountev, B. G. Ryder, and W. Landi. Data-.ow analysis \nof program fragments. In Proc. Symposium on the Foundations of Software Engineering, LNCS 1687, pages \n235 252, 1999. [13] M. Sagiv, T. Reps, and R. Wilhelm. Solving shape\u00adanalysis problems in languages with \ndestructive updat\u00ading. ACM Transactions on Programming Languages and Systems, 20(1):1 50, Jan. 1998. \n[14] M. Shapiro and S. Horwitz. Fast and accurate .ow\u00adinsensitive points-to analysis. In Proc. Symposium \non Principles of Programming Languages, pages 1 14, 1997. [15] B. Steensgaard. Points-to analysis in \nalmost linear time. In Proc. Symposium on Principles of Program\u00adming Languages, pages 32 41, 1996. [16] \nZ. Su, M. F\u00a8ahndrich, and A. Aiken. Projection merg\u00ading: Reducing redundancies in inclusion constraint \ngraphs. In Proc. Symposium on Principles of Program\u00adming Languages, pages 81 95, 2000. [17] R. E. Tarjan. \nData Structures and Network Algorithms. Society for Industrial and Applied Mathematics, 1983. [18] R. \nWilson and M. Lam. E.cient context-sensitive pointer analysis for C programs. In Proc. Conference on \nProgramming Language Design and Implementation, pages 1 12, 1995. [19] S. Zhang, B. G. Ryder, and W. \nLandi. Program decom\u00adposition for pointer aliasing: A step towards practical analyses. In Proc. Symposium \non the Foundations of Software Engineering, pages 81 92, 1996.  \n\t\t\t", "proc_id": "349299", "abstract": "<p>Most compiler optimizations and software productivity tools rely oninformation about the effects of pointer dereferences in a program.The purpose of points-to analysis is to compute this informationsafely, and as accurately as is practical. Unfortunately, accuratepoints-to information is difficult to obtain for large programs,because the time and space requirements of the analysis becomeprohibitive.</p><p>We consider the problem of scaling flow- and context-insensitivepoints-to analysis to large programs, perhaps containing hundreds ofthousands of lines of code. Our approach is based on a <italic>variable substitution</italic> transformation, which is performed off-line, i.e.,before a standard points-to analysis is performed. The general idea ofvariable substitution is that a   set  of variables in a program can be replaced by a single representative variable, thereby reducing the input size of the problem. Our main contribution is a linear-time algorithm which finds a particular variable substitution that maintains the precision of the standard analysis, and is also very effective in reducing the size of the problem.</p><p>We report our experience in performing points-to analysis on largeC programs, including some industrial-sized ones. Experiments show thatour algorithm can reduce the cost of Andersen's points-to analysis substantially: on average, it reduced the running time by 53% and the memory cost by 59%, relative to an efficient baseline implementation of the analysis.</p>", "authors": [{"name": "Atanas Rountev", "author_profile_id": "81100162864", "affiliation": "Department of Computer Science, Rutgers University", "person_id": "PP14067180", "email_address": "", "orcid_id": ""}, {"name": "Satish Chandra", "author_profile_id": "81100394237", "affiliation": "Bell Laboratories, Lucent Technologies", "person_id": "PP39040833", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/349299.349310", "year": "2000", "article_id": "349310", "conference": "PLDI", "title": "Off-line variable substitution for scaling points-to analysis", "url": "http://dl.acm.org/citation.cfm?id=349310"}