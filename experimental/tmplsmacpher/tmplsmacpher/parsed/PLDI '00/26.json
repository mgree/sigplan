{"article_publication_date": "05-01-2000", "fulltext": "\n Caching Function Calls Using Precise Dependencies Roy Levin Yuan Yu Compaq Computer Corporation Systems \nResearch Center 130 Lytton Avenue Palo Alto, CA 94301, USA  Abstract This paper describes the implementation \nof a purely func\u00adtional programming language for building software systems. In this language, external \ntools like compilers and linkers are invoked by function calls. Because some function calls are extremely \nexpensive, it is obviously important to reuse the results of previous function calls whenever possible. \nCaching a function call requires the language interpreter to record all values on which the function \ncall depends. For opti\u00admal caching, it is important to record precise dependencies that are both dynamic \nand fine-grained. The paper sketches how we compute such dependencies, describes the imple\u00admentation \nof an efficient function cache, and evaluates our implementation s performance. Introduction We consider \nthe problem of implementing a pure functional language in which some function calls are expected to be \nextremely costly. This problem arises in the context of the software configuration management system, \na system for managing and building potentially large-scale software As an integrated version control \nand build system,  provides several advantages over traditional version control systems like RCS and \nCVS, and over the build pro\u00adgram Make. These advantages include strong support for parallel development \nby multiple developers, support for easily specifying build customizations, and guarantees that all builds \nproduce correct results and are reproducible at any time in the future. tool invocations occur only at \nthe leaves of an evaluation s function call graph. In a large build, testing it is safe to reuse a previous \nresult at each of these might require thousands or tens of thousands of checks, adversely affecting \nincremental build performance. To produce a system whose incremental build perfor\u00admance scales to large \nsoftware, we claim that it is therefore important to reuse larger units of work than individual tool \ninvocations. The use of a functional language meshes well with this goal because function calls make \nconvenient, units of caching for later reuse. By using a cached function result whenever it is safe to \ndo so, unnecessary and other work can be avoided. But when is it safe to reuse a cached result? Only \nwhen the evaluation context at a candi\u00addate call site agrees with those parts of the context on which \nsome previous call to the same function depended. When detecting dependencies, it is of course essential \nnot to omit any, or the cache would be unsound, sometimes returning incorrect results. However, it is \nalso important not to err by introducing overly broad dependencies, or the cache will be ineffective, \nsometimes failing to return a result when it should. For the cache to be most effective, each function \ncall s dependencies must be recorded as precisely as possible. For example, consider the following simple \nfunction: return (if  0 then  else   Because of the conditional expression, the arguments on which \nthis function depends vary from call to call. For example, in the call the result depends In the instructions \nfor building software artifacts take the form of programs written in a functional system modeling Conceptually, \nthese programs describe how to build a system from scratch. When evaluated, such programs may call functions \nthat invoke external tools such as compilers and linkers, the net results of which are typi\u00adcally returned \nas part of the evaluation result. Invocations of external tools account for the vast majority of the \ntime spent building a software artifact, so it is obviously crucial to reuse results from previous builds \nwhenever possible. However, Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are not made or distributed for \nprofit or commercial advantage and that copies bear this notice and the full citation on the first page. \nTo otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission \nand/or a fee.  Vancouver, British Columbia, Canada. Copyright 2000 ACM  only on the values of and \n the value of  is irrelevant. Hence, the subsequent invocation in which the val\u00adues of  and y are \nidentical should produce a cache hit on the first call.  In this particular example, the observant reader \nwill no\u00adtice that the exact of  is also unimportant. What matters is simply whether or not  is positive. \nHence, to get the most accurate caching, dependencies should take the form of predicates on values, not \nthe exact values them\u00adselves. For now, we will assume that all dependencies are recorded on values, but \nwe describe how to represent more general dependencies in Section 4 below.  The caching problem is further \ncomplicated when the language includes composite value types. For example, let [a = 1, b denote a record \nwith two fields named a and b, whose values are 1 and 2, respectively. For such a record r, let the expression \n denote the selection of the a field. Now consider a slight modification to our previous example: f(x, \ny, z){ return (if x > 0 then y.a else z); } In this case, the result of the call f(1,[a =2,b =5],3) depends \nonly on x and y.a. The subsequent call f(1,[a = 2,b=9],7) should produce a cache hit. Recording a depen\u00addency \non the entire record y would cause the second call to get a false cache miss. This example demonstrates \nthat the dependencies calculated with respect to composite values should be as .ne-grained as possible. \nFrom these two examples, we conclude that the Vesta interpreter must compute .ne-grain dependencies dynami\u00adcally \n(that is, during an evaluation). The challenges in ac\u00adcurately implementing a caching scheme based on \ndynamic, .ne-grain dependencies are two-fold. First, algorithms must be developed for representing, computing, \nand propagating dynamic, .ne-grained dependencies. We sketch the tech\u00adnique used by the Vesta interpreter \nin Section 4. Second, due to the dynamic nature of the dependencies, it is im\u00adpossible to compute a single \ncache key at a function call site before the function has been evaluated. To handle this problem, we \ndivide the dependencies into two groups: the primary dependencies, on which the function is known to \ndepend before the call is performed, and the secondary de\u00adpendencies, on which the function dynamically \ndepends. In Section 3, we describe how we organize the function cache so as to perform lookups e.ciently. \nAs mentioned above, the Vesta interpreter caches both higher level user-de.ned functions and those that \ndirectly invoke external tools. By caching user-de.ned functions, the interpreter can get cache hits \non larger units of work than individual tool invocations; for example, on the construc\u00adtion of an entire \nlibrary archive. These higher-level cache hits make the cost of an incremental build proportional to \ntheamountofwork tobeperformed, not to thesizeof the system being built. As a result, Vesta s incremental \nbuild performance scales well to large software systems, a trait that is not shared by other software \nconstruction tools such as Make [4]. We demonstrate the e.ectiveness of Vesta s higher-level caching \nin Section 5. System Modeling Language Before describing our caching and dependency calculation algorithms, \nwe .rst sketch the main features of Vesta s sys\u00adtem modeling language. Vesta s build language is a full-.edged \nprogramming lan\u00adguage that is functional (that is, side-e.ect free), modular, dynamically typed, and \nlexically scoped. Its value space contains booleans, integers, text strings, lists, closures, and bindings. \nThe .rst four data types are the familiar ones from Algol-like languages and LISP. Closures are .rst-class, \nhigher-order functions that can be nested. Bindings are described below. The language contains about \n60 built-in functions for arithmetic and boolean operations, for basic manipulations of texts, lists, \nand bindings, and for invoking external tools. Overall, these core language facilities were designed \nto be as basic and methodology neutral as possi\u00adble so that support for particular styles of system construc\u00adtion \nor organization could be programmed in the language, rather than being built-in to the language or interpreter. \nThe complete language syntax and semantics are described elsewhere [5]. For the purposes of this paper, \ntwo features of the mod\u00adeling language are noteworthy: the binding type and the runtool primitive. Bindings \nare like records, except that the list of .elds in a binding can be dynamically extended at runtime. \nCon\u00adceptually, bindings are functions mapping texts to values. Bindings are constructed using a square \nbracket notation and may be nested, as in [f =1,g =[a =2,b = 3]]. The .eld f of a binding bis selected \nby writing b/f;it is a checked runtime error to select a nonexistent .eld. The boolean ex\u00adpression b!f \nis true if and only if the .eld f is de.ned in b. Bindings are used extensively in Vesta system models. \nThey are convenient for representing both build customiza\u00adtions (for example, [debug = -g ,opt = -O2 \n]) and nested .le directories, in which the leaves of the binding associate .le names with .le contents. \n(In fact, it was this latter usage that inspired the choice of / for the binding selection oper\u00adator.) \nIn our system models, both build customizations and .le directories are stored together in one composite \nbinding called the environment. The use of an environment bind\u00ading allows build customizations a.ecting \nthe entire build or selected parts of the build to be speci.ed once at a high level. For the environment \nto have this e.ect, the environ\u00adment binding is passed as a parameter to every function call. Almost \nall functions depend on selected parts of the environ\u00adment, so recording .ne-grain dependencies on this \nbinding is crucial. The language includes several primitives for combining bindings. One of these is \nthe binary overlay (+) operator, de.ned by: (b1 + b2)/n =if b2!n then b2/n else b1/n This operator merges \ntwo bindings, giving precedence to the second binding wherever both de.ne the same name. It can be used \nto override a default set of build options with user\u00adsupplied customizations. For example, the following \nstate\u00adment augments the build options to specify that compilation should produce debugging symbols: options \n= options + [ debug = \"-g\" ]; There are two classes of function calls in the Vesta mod\u00adeling language: \ncalls of the built-in runtool primitive, which is used to invoke external tools, and calls of user-de.ned \nfunctions. Calls of the runtool primitive are treated specially. The techniques used to implement it \nare beyond the scope of this paper, but we sketch them brie.y here. The runtool implementation is a cooperative \nprocess between the Vesta interpreter and the Vesta repository, which stores all of the versioned sources, \ntools, and libraries in the system. For maximum extensibility, the Vesta interpreter has no built-in \nknowledge of any of the tools it invokes. Given a command line and an evaluation environment as arguments, \nthe run\u00adtool primitive simply invokes the tool given in the command line on the given arguments. The \nenvironment contains a binding that de.nes the .le system in which the tool is to be run. Vesta arranges \nthat the tool (including any child processes it forks) has access to only the .les in this .le system, \nand that all its .le references are detected by the repository and reported back to the interpreter. \nThe inter\u00adpreter then records these references as dependencies on the environment. Although runtool caching \nis certainly important for in\u00adcremental build performance, this paper focuses on e.ec\u00adtively caching \nuser-de.ned functions, which often represent larger units of work in the function call graph. 3 E.cient \nCaching The interpreter and function cache use .ngerprints to rep\u00adresent the cache keys. A .ngerprint \nis a .xed-size hash of an arbitrary byte sequence [3, 10]. Fingerprints come with a mathematical guarantee \nbounding the probability of a col\u00adlision; by choosing long enough .ngerprints, the probability of a collision \ncan be made vanishingly small1.As a result, .ngerprints can be used as a basis for equality tests, since \nwe can safely assume that FP(a)= FP(b) .. a = b. Two operations supported on .ngerprints are extending \na .ngerprint by more bytes and extending a .ngerprint by another .ngerprint. In the latter case, we write \nfp1 . fp2 to denote the result of extending fp1 by fp2.The . operation is non-commutative. As described \npreviously, the dependencies for each func\u00adtion call are divided into two groups, primary and secondary. \nThe primary dependencies are determined at the call site, before the function is evaluated. The primary \ndependen\u00adcies normally include the body of the function being in\u00advoked and the values of the function \ns arguments that are of scalar types.2 From the primary dependencies, the inter\u00adpreter computes a primary \ncache key by .ngerprinting the body of the function being invoked, then extending it by the .ngerprints \nof the relevant argument values. The secondary cache key also uses .ngerprints, but it takes a di.erent \nform, namely, a set of name-value pairs in which no name occurs more than once. The secondary key represents \nthe names on which the function dynamically depends, as well as the values of those names in the evalua\u00adtion \ncontext. Since the only operation required on the val\u00adues when performing a cache lookup is testing for \nequality, the values can be represented in the cache by .ngerprints, thereby saving both space and time. \nThe .ngerprints of .le values are supplied by the Vesta repository3; the .ngerprints of all other values \nare computed by the interpreter. To make the concepts of primary and secondary keys more concrete, consider \nthe call of a user-de.ned compile function for compiling a single C source .le: compile(\"test.c\", env); \nHere, env is a binding that contains a representation of a .lesystem in which the compilation is performed. \nThe primary key for this call is the .ngerprint of the compile function s body combined with the .ngerprint \nof the literal test.c. The secondary key will contain the names of all parts of the env binding referenced \nduring the function eval\u00aduation, together with the .ngerprints of the corresponding values. For example, \nthe names in the secondary key might include: env/usr/lib/cmplrs/cc env/test.c env/usr/include/stdio.h \nenv/options/debug 1For safety, Vesta uses 128-bit .ngerprints, making the probability of a collision \nmuch less than 1 in 280 . 2Including argument values among the primary dependencies is a heuristic;strictly \nspeaking, until the function is evaluated, we do not know whether it will depend on these arguments. \nThe interpreter accepts pragmas to modify this heuristic;see Section 4.6.2. 3Whenever a .le changes, \nthe repository computes a new .nger\u00adprint for it. The .ngerprint of a small .le is computed from the \n.le s contents;the .ngerprint of a large .le is computed by .ngerprint\u00ading the .le s unique identi.er. \nThe threshold for distinguishing small .les from large .les is a con.guration parameter that defaults \nto 1 megabyte. Once the primary key has been computed, the question that immediately arises is how to \nperform a cache lookup. Due to the dynamic nature of the secondary dependencies, there is no way to know \napriori what the dependencies will be without evaluating the function. But obviously, if we had to evaluate \na function before we could look up its value in the cache, the cache would be useless. The way around \nthis chicken-and-egg problem is for the function cache to group entries by primary key. The in\u00adterpreter \nand cache then cooperate to search through the appropriate group for an entry whose secondary dependen\u00adcies \nmatch the current context. Using this idea, the lookup operation becomes a four-step process: 1. The \ninterpreter computes a primary key pk and com\u00admunicates it to the function cache. 2. The function cache \nexamines all of the cache entries with primary key pk, and returns to the interpreter the union of all \nnames in their associated secondary keys. 3. The interpreter computes (the .ngerprints of) the val\u00adues \nassociated with those names in the current evalu\u00adation context, and sends them to the function cache. \n 4. The cache again examines the entries with primary key pk, and checks to see if any have secondary \nkeys matching the values in the current context. If so, a cache hit occurs, the cached function result \nis returned, and the interpreter skips the function evaluation.  If the evaluation of a function is \ndeterministic, and if the dependencies recorded for all evaluations of that function are complete, then \nthe cached result returned in the event of a cache hit will be the same as the result that would be produced \nby actually evaluating the function. Of course, it is quite possible to invoke external tools that produce \nnondeterministic results (such as the Unix date program). In practice, however, all of the tools we have \nused (mainly compilers and linkers) are su.ciently deterministic to pro\u00adduce reliable caching behavior. \nFor example, the standard C compiler writes a timestamp into each object .le it gener\u00adates, but this \nnondeterminism does not prevent a previously\u00adwritten object .le from being safely reused at a later time. \nWe now consider the question of how best to organize en\u00adtries in the cache so as to e.ciently implement \nsteps 2 and 4 of the lookup algorithm. As mentioned, cache entries shar\u00ading the same primary key are \ngrouped together. In practice, however, there might be hundreds of such entries. For exam\u00adple, if the \nprimary key corresponds to compiling a particular source .le, there will be a new cache entry created \neach time a di.erent version of the .le is compiled. Moreover, it is not atypical for cache entries themselves \nto have hundreds of name-value pairs in their secondary keys. Considering the compilation example again, \neach .le that is accessed during the compilation is a secondary dependency. Hence, even if the entries \nwith a given primary key could be easily enumer\u00adated, a naive implementation of the lookup algorithm \nmight require tens of thousands of .ngerprint comparisons. To avoid this problem, cache entries are organized \nin a two-level hierarchy. First, all entries with the same primary key are grouped together. Then the \nentries in each group are partitioned in such a way that only a subset of the entries in each group need \nbe examined on any lookup. To explain how this partitioning is done, we introduce some notation. For \nany cache entry e,let e.pk denote e s primary key, let e.names denote the set of names in e s sec\u00adondary \nkey, and for any name n . e.names,let e.val(n)de\u00adnote the .ngerprint value associated with n in e s secondary \nkey. Now de.ne the following: Entries(pk)= {e | e.pk = pk}  AllNames(pk)= e.names e.Entries(pk) CommonNames(pk)= \ne.names e.Entries(pk) CFP(e)= e.val(n) n.CommonNames(e.pk) The set CommonNames(pk) thus consists of those \nnames that occur in every cache entry with the given primary key. The names in AllNames(pk) \\ CommonNames(pk) \noccur in some cache entries with the given primary key but not oth\u00aders; we call them uncommon.The value \nCFP(e), e s com\u00admon .ngerprint, is the result of combining the .ngerprints of all secondary values of \ne corresponding to e.pk s common names. Due to the non-commutative nature of the . oper\u00adation, it is \nimportant to enumerate the names n in a well\u00adde.ned order. To this end, the function cache maintains \na canonical ordering for each pk of the names in AllNames(pk); it uses that ordering when computing CFP(e). \nGiven these de.nitions, we can now describe how the cache implements steps 2 and 4 of the lookup algorithm. \nFor each primary key pk, the cache maintains Entries(pk), AllNames(pk), and CommonNames(pk) (the last \nof which is represented by a bit vector with respect to AllNames(pk)). In step 2 of the lookup algorithm, \nthe cache simply returns AllNames(pk) for the supplied primary key. To e.ciently perform step 4 of the \nlookup algorithm, the cache com\u00adputes CFP(e) for every entry e. It then groups the entries Entries(pk) \ninto equivalence classes according to their com\u00admon .ngerprints. As an example, consider Figure 1, which \nshows the sec\u00adondary names and values of four cache entries sharing a primary key. Each column is a di.erent \ncache entry, and the circled letters correspond to di.erent .ngerprint values. The absence of a .ngerprint \nin row i and column j means that name i is not in the secondary dependency set of the entry for column \nj. The entries shown could correspond to invocations of a C compiler on a source .le named test.c. In \nthis example, only the names env/usr/lib/cmplrs/cc and env/test.c are referenced by all four cache entries, \nso those are the only two names in CommonNames(pk). Fig\u00adure 2 shows the CFP(e) .ngerprints that might \nbe computed for these entries; notice that CFP(e3)= CFP(e4)because the values associated with the common \nnames agree on those two entries. The entries are then arranged in a hierarchy as shown in Figure 3; \nthe top two levels of the hierarchy are implemented using hash tables. All of the entries sharing the \nsame primary key and common .ngerprint are said to belong to the same cfp-group. In step 4 of the lookup \noperation, the cache is given a pk and the .ngerprints f1,f2,...,fk of the values corre\u00adsponding to the \nnames AllNames(pk)atthe call site. Call these .ngerprints fi the call site .ngerprints.To perform the \nlookup, the cache .rst combines those call site .ngerprints associated with CommonNames(pk), thereby \nproducing a common .ngerprint cfp. Next, the cache does a hash table Entries Secondary Names 1 2 3 4 \nenv/usr/lib/cmplrs/cc env/test.c env/usr/include/stdio.h env/defs.h  Figure 1: The secondary names \nand values of four cache entries sharing the same primary key. Entries Secondary Names 1 2 3 4 env/usr/lib/cmplrs/cc \nenv/test.c  CFP(e) env/usr/include/stdio.h env/defs.h  Figure 2: The common .ngerprints computed for \nthe cache entries of Figure 1. Here, X = A . B, Y = A . D,and Z = A . E. lookup to see if pk has cfp \nas one of its associated common .ngerprints. If not, the cache reports a miss. Otherwise, the cache examines \nthe entries in the identi.ed cfp-group. In testing for a hit, only the uncommon names need be ex\u00adamined, \nsince by virtue of being in the correct cfp-group, all entries beingconsidered are knowntohave matchingvalues \nfor the common names. This lookup algorithm results in two major cost savings compared to the brute-force \nalgorithm. First, only those en\u00adtries in the identi.ed cfp-group need be examined. In prac\u00adtice, the \ncfp-groups tend to be quite small (often containing only one entry), so this produces a major saving. \nSecond, when examining the entries in a cfp-group, only the values associated with the uncommon names \nneed be examined. In our experience, the fraction of names that are uncommon tends to be quite low, averaging \nonly a few percent. These two e.ects thus drastically reduce the number of .ngerprint comparisons required \nto perform a cache lookup. When a new entry is added to the set Entries(pk), the sets AllNames(pk)and \nCommonNames(pk) can change. Since changes to the latter would require common .nger\u00adprints to be recomputed \nand cache entries to be reorganized, newly added entries are kept in two side bu.ers (one for en\u00adtries \nthat have all of the common names, and one for those PKs CFPs  Entries 1 2 3 4 Figure 3: The hierarchical \narrangement of the cache entries .rst by primary key, and then by common .ngerprint for the entries of \nFigure 2. e ::=  | a literal (a . Literal) | x variable (x . Id) | .x.e lambda | if e1 then e2 else \ne3 conditional | [n1 = e1,n2 = e2, ..., nk = ek] binding constructor | e/n binding selection | e!n binding \ndomain test | e1 + e2 binding overlay | let x = e1 in e2 let construct | e1(e2) function application \nTable 1: The syntax for a subset of our system modeling language. that do not). The lookup algorithm \nmust consult these side bu.ers in addition to checking for a hit in Entries(pk). Once a large enough \nnumber of new entries are collected, they are merged into Entries(pk) together, thereby amortizing the \ncost of recomputing all of the common .ngerprints. The details of this update process are beyond the \nscope of this paper. In Vesta, the function cache is implemented by a per\u00adsistent, fault-tolerant server \nprocess. The Vesta interpreter communicates with the cache over a local network via RPC. This design \nhas two obvious advantages. First, the cache process can be run on a powerful server machine for bet\u00adter \nperformance. Second, since developers at the same site share the same function cache, they can bene.t \nfrom each other s builds. 4 Computing Dependencies In this section we consider how to calculate dependencies \nfor the Vesta language. The interpreter computes dependencies dynamically during an evaluation. This \nsection gives the mathematical rules we have developed for computing de\u00adpendencies. It also states (but \ndoes not prove) a correctness theorem. To describe the key ideas used in our dependency calcu\u00adlation, \nwe de.ne a subset of the Vesta modeling language [5]. Table 1 gives the subset language s syntax. Here, \nLiteral is the set of literals, and Id is the set of identi.ers. This subset has been chosen to include \nthe core parts of the Vesta lan\u00adguage that present the most signi.cant challenges to e.ec\u00adtive dependency \nanalysis. It omits the language s primitive functions (of which there are approximately 60), its itera\u00adtion \nconstruct, its provisions for importing one system model from another, and its support for binding program \nvariables to versioned directories and .les in the Vesta repository. Every expression is evaluated in \nsome evaluation con\u00adtext, which is a mapping from variable names to values. Let Eval(e, c) denote the \nresult of evaluating the expression e in the context c,and let Dpnd(e, c) denote the dependency information \nresulting from evaluating e in c.Since we use standard call-by-value evaluation, the rules for Eval(e, \nc)are straightforward, so we will not describe them here. The re\u00admainder of this section describes the \nrules for computing Dpnd(e, c).  4.1 Relation to Caching As outlined in the previous section, when faced \nwith a func\u00adtion invocation f(e1,e2,...,en), the interpreter .rst com\u00adputes a primary key from the function \ns body and zero or more of the argument values. In response to this pri\u00admary key, the function cache \nreturns a set of secondary names. The cache treats the secondary names as mean\u00adingless strings, but to \nthe interpreter they represent depen\u00addencies. The interpreter evaluates each of the secondary names in \nthe current context and sends the resulting list of value .ngerprints to the cache. The cache then tests \nfor a hit as previously discussed. In the event of a cache hit, the interpreter uses the cached result \nvalue. In the event of a cache miss, the interpreter proceeds to evaluate the function on the given \narguments. As it does so, it represents each runtime value by a pair containing the true value and the \ndependencies detected while computing that value. Value-dependency pairs are also stored for any sub-values \nnested inside composite values such as lists or bindings. Once it has .nished evaluating the function, \nthe interpreter collects up the dependencies in the function s re\u00adsult value. Any dependencies that are \nnot already part of the primary key are considered part of the secondary key. The interpreter then calls \nthe function cache to create a new cache entry with the computed primary key, secondary key, and result \nvalue. It is important to note that the cached result value, like all of the interpreter s runtime values, \nit\u00adself is a pair that includes dependencies. Whenever a later evaluation gets a hit on this entry, both \nthe value and its dependencies will be needed so that the dependency analysis for subsequent uses of \nthe value can proceed correctly. For caching to be correct, the cache entries created by the interpreter \nmust satisfy a theorem, a formal statement of which is given in Section 4.5 below. To understand the \nin\u00adtuition behind the theorem, imagine that the cache contains an entry with primary key pk that resulted \nfrom evaluating the expression e in a context c1. The secondary names asso\u00adciated with this cache entry \nwill be the set of dependencies Dpnd(e, c1). When attempting to evaluate e in another context c2, the \ncache will produce a hit on the cached entry only if the values computed in c2 for the dependencies Dpnd(e, \nc1) match the values stored in the cache. Note that the values associated with the dependencies in the \ncache are precisely the values computed in c1 for Dpnd(e, c1). We therefore de.ne equivalence between \ntwo contexts with respect to a set of dependencies d as follows: Equiv(c1,c2,d)=(.p . d : Value(p, c1)= \nValue(p, c2)). In this de.nition, Value(p, c) denotes the result of evaluat\u00ading the dependency p in \nthe context c. It corresponds to the .ngerprint associated with each secondary name in the cache. (We \nde.ne Value(p, c) for the particular dependencies created by the interpreter in the next section.) Type \nDependency on the component s ... Computed Value V X D T L E ...complete value ...existence ...domain \n...type ...length (number of subcomponents) ...expression (closures only) Value(V : path,c)= Eval(path,c) \nValue(X : path/id,c)= Eval(path!id,c) Value(D : path,c)= {n| Eval(path!n,c)}Value(T : path,c)= Eval(typeof(path),c) \nValue(L : path,c)= Eval(length(path),c) Value(E : path,c)= Eval(path,c).body Table 2: The meanings of \nthe six dependency types and the rules used by the interpreter to evaluate each type of path in a context \nc. Clearly, the cache hit will be correct if and only if per\u00adforming the evaluation of e in c2 would \nproduce the same result as the one stored in the cache, that is, Eval(e,c2)= Eval(e,c1). Therefore, to \nprove our caching correct, we must show that Equiv(c1,c2,d)=. Eval(e,c2)= Eval(e,c1). The next section \ndescribes how the interpreter computes the dependencies so as to satisfy this requirement.  4.2 Representing \nDependencies As mentioned in Section 1, dependencies in general are pred\u00adicates on the evaluation context. \nIn a practical implemen\u00adtation, however, allowing for arbitrary predicates would be costly in both space \nand time. We therefore use a small, .xed collection of predicates, encoded as dependency paths. It is \nthese dependency paths that are passed to the func\u00adtion cache as the names in a cache entry s secondary \nkey. The function Dpnd(e,c) evaluates to a set of such paths; Section 4.3 below describes the rules for \ncomputing it. The syntax of a dependency path is given by the follow\u00ading grammar: dpath ::= t: path t \n::= V | X | D| T | L| E path ::= E| id/path A dependency path takes the form t: path,where t denotes \nthe dependency type, and path speci.es the component of the evaluation context on which the evaluation \ndepends. We use E to denote an empty path; a path of the form id/E is equivalent to the path id. The \nmeanings of each of the interpreter s six dependency types are given in Table 2, along with the rules \nit uses to compute Value(t : path,c), the value of the dependency path t: path in the context c.The V \n(value) dependency type is the strongest, and hence subsumes the other dependency types. In this table, \nthe language s primitive typeof and length functions compute the dynamic type of a value and the length \nof a list or binding, respectively. The notation cl.body denotes the closure cl s body component.  4.3 \nDependency Calculation Rules We now give the mathematical rules for calculating de\u00adpendencies. We .rst \nde.ne D(e,c,p)where p is a depen\u00addency path; again, D(e,c,p) evaluates to a set of dependency paths. \nWe then de.ne Dpnd(e,c)= D(e,c,V : E). Intuitively, D(e,c,p) is the dependency for just the portion of \ne s value that is selected by p. The de.nition of D(e,c,p)now pro\u00adceeds by cases on the program structure: \nIf e= a(a. Literal), then D(e,c,t: p)= \u00d8. Evaluating a constant has no dependency on the context. If \ne = x (x . Id), then D(e,c,t: p)= {t: x/p}.Eval\u00aduating a variable x depends only on the dependency path \nextended on the left by x.  If e= .x.e1, only the following two cases arise:  D(e,c,V : E)= FVs(e) \nD(e,c,E: E)= {} where FVs(e) denotes the set of e s free variables. In both cases, the path must be empty. \nIf the type of the path is V, the lambda expression depends on the whole closure value, that is, the \nset of e s free variables. If the type of the path is E, it depends on only the lambda expression of \nthe closure value, namely, the expression e.Since eis incorporated into the primary key of every function \ncall whose body contains e, it is correct not to record any dependencies in this case. If e =if e1 then \ne2 else e3, then we have the following rule: d1 = D(e1,c,V : E) v1 = Eval(e1,c) d2 =if v1 then D(e2,c,t: \np)else D(e3,c,t: p) D(e,c,t: p)= d1 . d2 This rule states that the dependency of a conditional is the \nunion of the dependencies of the guard e1 and the dependency of either e2 or e3, depending on the value \nof e1. We use an empty path in computing the guard s dependencies because the guard evaluates to a boolean \nvalue that has no components. If e =[n1 = e1,n2 = e2,...,nk = ek], there are two cases to consider. If \np is empty, it means that we depend on the entire binding. If p= t: ni/p1,it means that we depend only \non the value of .eld ni.The following rules cover the two cases: D(e,c,t: E)= .ki=1D(ei,c,V : E) D(e,c,t: \nni/p1)= D(ei,c,t: p1)  If e = e1/n,then D(e,c,t: p)= D(e1,c,t: n/p). For binding selection, we recursively \ncall D with the path extended on the left by n.  If e = e1!n,then D(e,c,t: p)= D(e1,c,X: n/p). For the \nbinding domain test, we recursively call D with the path extended on the left by n. Note that the new \ndependency path has type X, regardless of the type t.  If e = e1 + e2, then there are two cases to consider: \nIf p is empty, it means that we depend on the entire binding. If p = t : n/p1, it means that we depend \n  only on the binding that supplies the n .eld. In the case that the n .eld comes from e1, we must add \nthe dependency that n is not de.ned in e2. The following rules cover the two cases: D(e, c, t: E)= D(e1,c, \nV : E) .D(e2,c, V : E) D(e, c, t: n/p1)= if Eval(e2!n, c) then D(e2,c,t: n/p1) else D(e2,c, X : n) .D(e1,c, \nt: n/p1) If e =let x = e1 in e2,then c1 = c .{x .Eval(e1,c)} d2 = D(e2,c1,t: p) d2a = {p' |p' .d2 .head(p')= \nx} d2b = {t' : p' |t' : x/p' .d2}  D(e, c, t: p)= d2a .p!.d2b D(e1,c, p') where .denotes the operation \nfor extending a context, and head(p) denotes the .rst element of p s path. We .rst augment the evaluation \ncontext with x mapped to Eval(e1,c) and compute d2 as the dependency of e2 in the augmented context. \nWe then divide the de\u00adpendency paths in d2 into two sets d2a and d2b.The set d2a contains paths unrelated \nto x.So, d2a must be included in the result. The set d2b contains paths starting with x. So, we need \nto recursively compute '' D(e1,c, p)for each path pin d2b. If e = e1(e2), then Eval(e1,c)=<.x.e3,c3 > \nd1 = D(e1,c,E : E) c1 = c3 .{x .Eval(e2,c)} d3 = D(e3,c1,t: p) d3a = {p' |p' .d3 .head(p')= x} d3b = \n{t' : p' |t' : x/p' .d3}  D(e, c, t: p)= d1 (.p!.d3a D(e1,c, p')) ' (.p!.d3b D(e2,c,p)) This rule is \nsimilar to the rule for the let construct, where e1 in the let expression is like the argument e2 here, \nand e2 in the let expression is like the closure body e3 here.  4.4 Example We now present a simple \nexample to demonstrate the above dependency rules. We compute the dependencies for the expression: e \n=let x =[r =[s= y],t= z]in x/r/s in the context c. Obviously, Eval(e, c)= c(y). Here is the start of \nthe dependency calculation: Dpnd(e, c) ={de.nition of Dpnd } D(e, c, V : E) Since the expression e is \na let construct, the let rule applies. The main step in calculating the dependencies for the let construct \ninvolves calculating the dependency set named d2 in that rule, where e1 =[r =[s = y],t = z]and e2 = x/r/s. \nHere we derive the value for d2,using c1 to denote the aug\u00admented context c .{x .Eval(e1,c)}: D(x/r/s, \nc1,V : E) ={binding selection rule } D(x/r, c1,V : s) ={binding selection rule } D(x, c1,V : r/s) ={variable \nrule } {V : x/r/s } From the dependency set d2, we compute the partitioned sets d2a and d2b: d2a = \n\u00d8 d2b = {V : r/s } We can now continue computing Dpnd(e, c): D(e, c, V : E) ={let rule } \u00d8.D([r =[s= \ny],t= z],c, V : r/s) ={binding constructor rule } D([s= y],c,V : s) ={binding constructor rule } D(y, \nc, V : E) ={variable rule } {V : y } Hence, the evaluation of e in c depends only on the value of y,aswewould \nexpect. 4.5Correctness The following theorem states the correctness of the depen\u00addency calculation rules. \nTheorem 1 (Caching Correctness) If the expression e evaluates to a value v in the context c1, then we \ncan compute Dpnd(e, c1), and, if every path in Dpnd(e, c1) evaluates to the same value in contexts c1 \nand c2,then e also evaluates to v in c2.Formally, (.v : Eval(e, c1)= v)=. (.d : Dpnd(e, c1)= d) .(Equiv(c1,c2, \nDpnd(e, c1)) =. Eval(e, c2)= Eval(e, c1))). Before implementing the dependency algorithm, we for\u00admalized \nthe above evaluation and dependency rules for this subset of the Vesta language in the Nqthm theorem \nprover [2], and mechanically checked the correctness theorem. The proof is beyond the scope of this paper. \nIt took several iterations of running the prover and correcting our rules before the mechanical proof \nsucceeded. This proof e.ort revealed a couple of subtle errors in earlier versions of the rules. We then \nimplemented the rules in the Vesta inter\u00adpreter. Although we did not mechanically check the rules for \nthe complete Vesta language, the subset we did verify covers the most complex aspects of the language, \nand so the mechanical veri.cation was quite useful. 4.6 Practical Considerations A number of practical \nissues arise in the implementation of the dependency calculation. Below, we brie.y describe three of \nthe more interesting ones. 4.6.1 Runtool Caching As a practical system modeling language, the Vesta lan\u00adguage \nprovides a number of primitive functions. Except for the runtool primitive, all of the other primitives \ncan be han\u00addled using mathematical rules similar the ones developed in Section 4.3. The runtool primitive \nmust be treated specially because any dynamic dependencies that result from .le sys\u00adtem references must \nbe recorded during tool invocations. Currently, we support two di.erent kinds of .le refer\u00adences: lookup \n(looking up a name in a binding representing a .lesystem directory) and list (listing the entries in \nsuch a .lesystem binding). When a lookup succeeds, we record a value (V) dependency on the .le or directory \nit returns. When a lookup fails, we record an existence (X) dependency asserting the nonexistence of \nsuch an object. Finally, for a list reference on a directory, we record a domain (D) depen\u00addency on the \nnames in that directory. 4.6.2 Primary Vs. Secondary Key As mentioned earlier, we use the .ngerprint \nof a closure s body as the basis for its primary key. Including none of the arguments in the primary \nkey would produce many cache entries with the same primary key. That would cause cache lookups to take \nlonger, since there would be more entries to search through for any given primary key one for each time \nthe same function was invoked. Conversely, folding all of the arguments into the primary key, as we have \nexplained, would produce cache entries that are too coarse-grained. We have chosen a course between these \ntwo extremes. We use a heuristic that folds the values of the simple argu\u00adments (booleans, integers, \nand texts) into the primary key, but does .ne-grained dependency analysis on the composite arguments \n(bindings, lists, and closures). The Vesta language includes pragmas for overriding this heuristic. None \nof the Vesta models written by end users have required these pragmas. Their use has been limited to the \nspecialized bridges written by wizard users. These bridges encapsulate the invocation of compilers and \nlink\u00aders, providing a less primitive interface than the one o.ered by runtool. The typical bridge model \ncontains less than 10 such pragmas. Although only a small number of pragmas are required, their e.ect \non performance can be noticeable, especially as the number of cache entries increases. Impor\u00adtantly, \npragmas impact performance only; their use cannot produce incorrect caching behavior such as false cache \nhits.  4.6.3 Coarse-Grained Entries When one function calls another, many dependencies of the callee \ntypically become dependencies of the caller. For ex\u00adample, the function responsible for compiling a library \nar\u00adchive will depend on the union of all of the header .les on which the compilations of the constituent \nsource .les de\u00adpend. Hence, if no special measures were taken, the root function of an evaluation would \nhave an extremely large number of dependencies. This situation would make cache lookup time proportional \nto the size of the system being built, and therefore would prevent evaluation from scaling well. To prevent \ntoo many dependencies from being propa\u00adgated up to the root of an evaluation, we automatically iden\u00adtify \nsome functions as special. In Vesta, these are typically the functions responsible for building large \ncomponents of a system, such as entire library archives. In addition to the normal .ne-grained cache \nentries created for evaluations of Source Source Runtool Test Lines Files Calls Hello 10 1 2 Interpreter \n53,304 103 117 Release 119,602 255 333 Table 3: Properties of three source collections. the special \nfunctions, the interpreter also creates cache en\u00adtries for these functions whose dependencies are more \ncoarse\u00adgrained. These special functions thus serve as cuto. points in the function call graph, above \nwhich overly .ne-grained and bulky dependencies do not propagate. 5Performance In this section, we demonstrate \nthat Vesta s dependency and caching algorithms perform well, and that it is important to cache user-de.ned \nfunction calls in addition to runtool calls. Table 3 summarizes three collections of source code used \nfor our measurements. The Hello test contains a single 10\u00adline hello world program. The Interpreter collection \ncon\u00adsists of the code for the Vesta interpreter. The Release col\u00adlection contains the sources for a complete \nVesta release. We performed incremental builds of these three collec\u00adtions using Vesta and Make. In both \ncases, the builder was run on a Digital AlphaStation 500 5/333, with a 333 MHz Alpha 21064 CPU and 192MB \nof memory. The server pro\u00adcesses (the function cache and repository for the Vesta build, and the NFS \nserver for the Make build) were run on a Dig\u00adital AlphaStation 400 4/233, with a 233 MHz Alpha 21064 \nCPU and 192MB of memory. These machines are now two generations old, so we expect the absolute performance \nin both the Vesta and Make cases would be substantially better on modern hardware. Figure 4 is a graph \ncomparing the performance of Vesta and Make on incremental builds of the three source collec\u00adtions. In \neach case, we modi.ed a single source .le, and then measured the elapsed time to rebuild the system. \nSince only a single source .le was modi.ed in each test, both Vesta and Make performed exactly the same \ntool executions; the dif\u00adferences shown in their performance are due entirely to the time required by \neach system to decide what sources to re\u00adcompile. Vesta is much faster than Make on the Interpreter and \nRelease tests because Vesta gets a high-level cache hit on the call for building the (unmodi.ed) libraries, \nwhereas Make has to stat hundreds of source, header, and object .les to ascertain that the libraries \nare up-to-date. Time (secs) Hello Interpreter Release  Figure 4: Incremental build performance of Vesta \nand Make on the three source collections of Table 3. 2.2 1.3 1.1 0.8 0.8 0.5 0.6 Hello Interpreter Release \nFigure 5: Elapsed time spent by various Vesta components performing the incremental builds of Figure \n4. Time (secs) 126 Time (secs) 10.9 10.7 Interpreter Cache Runtool All Calls Runtool Calls Only Hello \nInterpreter Release Figure 6: Elapsed time spent performing Vesta incremental builds when all calls are \ncached vs. when only runtool calls are cached. Figure 5 shows the overhead of performing the depen\u00addency \ncalculations and cache lookups in each of the incre\u00admental Vesta builds of Figure 4. The time spent in \nthe interpreter computing dependencies and in the cache per\u00adforming lookups is minimal. On average, the \ntwo phases of the cache lookup operation together take approximately 70 ms, and adding a new cache entry \ntakes approximately 22 ms. The vast majority of the time is spent performing runtool calls, that is, \ninvoking the compiler and linker. Since the tool invocations under Make take the same amount of elapsed \ntime as under Vesta, we conclude from Figures 4 and 5 that in the Release test, Make spends fully two-thirds \nof its time deciding which .le to recompile. By contrast, Vesta spends less than 20% of its time outside \nthe tool executions. We expect that as the software being built gets larger, the di.erences will become \neven more extreme. Finally, Figure 6 shows why it is so important to cache calls of user-de.ned functions. \nThe .gure compares the Vesta incremental build times of Figure 4 in which all func\u00adtion calls are cached \nto incremental builds in which only runtool calls are cached. The latter numbers were produced by running \nthe interpreter in a mode in which it did .ne\u00adgrain dependency analysis on runtool calls, but absolutely \nno other dependency analysis. When only runtool calls are cached, the incremental build performance su.ers \ngreatly, especially on the larger Release test. The main reason the runtool-only case is so much slower \nis that many more cache lookups are required in that case. When all calls are cached, Vesta s incremental \nbuild performance is proportional to the scope of the change rather than the size of the system being \nbuilt. Although we have not implemented a version of the inter\u00adpreter that computes either coarse-grain \nor static dependen\u00adcies, our experience using the system suggests that so many false cache misses on \nuser-de.ned functions would occur that the system s performance would sink to the levels shown for the \nruntool-only caching case in Figure 6. (In fact, the per\u00adformance would be even worse if coarse-grained \ndependen\u00adcies were recorded for runtool calls because then even some tool invocations would be unnecessarily \nrepeated.) This ef\u00adfect is due to the wide-spread use of the environment binding in our models. Recording \na coarse-grained dependency on the environment would cause false cache misses throughout the entire call \ngraph whenever any part of the environment was changed. This reasoning led us to conclude that record\u00ading \ndynamic, .ne-grain dependencies was critically impor\u00adtant if we wanted Vesta to exhibit good incremental \nbuild performance. 6 Related Work Memoization is a well-known technique for caching the re\u00adsults of \nfunction calls [7, 8]. Memoization is most e.ec\u00adtive when applied to recursive programs, particularly \nthose that use dynamic programming. For example, adding mem\u00adoization to the straightforward recursive \ncomputation of the nth Fibonacci number changes the program s time com\u00adplexity from exponential to linear. \nHowever, in standard memoization, all of a function s arguments are incorporated into the cache key, \nand there are no dynamic dependencies. This technique would produce cache entries that are far too coarse-grained \nfor our needs, resulting in costly and unnec\u00adessary cache misses. Pugh and Teitelbaum describe how to \nbetter structure recursive computations so as to improve the e.ectiveness of memoization when the arguments \nvary from call to call in certain ways [9]. Program slicing is a technique for determining the parts \nof a program that may contribute to some values of inter\u00adest [12, 14]. Computing the slice of a program \na.ecting the value of a variable v at a particular statement S involves determining all statements and \nvariables on which v s value at S depends. The algorithms in the literature do not admit composite values, \nso the dependencies computed by tradi\u00adtional program slicing techniques are coarse-grained. The work \nmost closely related to ours is a paper by Abadi, Lampson, and L\u00b4evy, which uses a labelled .-calculus \nto compute both dynamic and .ne-grained dependencies [1]. Their approach is quite di.erent from ours. \nIt associates la\u00adbels with all expressions in a function body, and then devel\u00adops rules for keeping the \nlabels of only those expressions that are evaluated during a call. As a result, it records only one kind \nof dependency, analogous to our value dependencies. Also, their calculus supports only the selection \noperation on records. Computing dependencies for the binding operators ! and + complicates the problem \nsigni.cantly. 7 Conclusions When interpreting a functional language in which some func\u00adtion calls may \nbe extremely expensive, it is imperative for good performance to perform only those function calls that \nare absolutely necessary. One obvious approach is to cache the results of each function call, and to \nperform a cache lookup before executing any candidate call. In the event of a cache hit, the cached result \ncan be used directly. As we have shown, accurately caching function calls re\u00adquires the interpreter to \nrecord precise dependencies that are both dynamic and .ne-grained. Otherwise, cache en\u00adtries are created \nthat can produce false cache misses, caus\u00ading unnecessary work to be performed. We have described a technique \nfor recording and propagating such precise de\u00adpendencies, and we have described a cache organization \nthat supports e.cient lookup in the presence of such dependen\u00adcies. The techniques described in this \npaper are an integral part of the Vesta software con.guration management sys\u00adtem. Vesta is now in daily \nuse by a group of over 100 Com\u00adpaq engineers designing the next-generation Alpha processor [11]. They \nuse Vesta to build simulators of the chip from its RTL models. Despite the signi.cant di.erences between \nsuch hardware design tasks and standard software develop\u00adment, Vesta has worked quite well for them. \nIn fact, they estimate that their use of Vesta instead of CVS (for ver\u00adsion control) and Make (for building) \nhave put them 3 to 6 months ahead of schedule on the .rst phase of their project. Analysis of the function \ncall graphs produced from their in\u00adcremental builds shows that our precise dependency analysis and caching \nis as e.ective for them as in our own use and performance studies. Acknowledgments We wish to thank Mart\u00b4in \nAbadi, Jim Horning, Butler Lamp\u00adson, and Tim Mann for helpful discussions related to this work, Tim Mann \nand Marc Najork for their comments on earlier drafts of the paper, and the anonymous PLDI referees for \ntheir many helpful suggestions. References [1] Mart\u00b4in Abadi, Butler Lampson, and Jean-Jacques L\u00b4evy. \nAnalysis and caching of dependencies. In Proceed\u00adings of the 1996 ACM SIGPLAN International Confer\u00adence \non Functional Programming (ICFP 96), pages 83\u00ad 91. Association for Computing Machinery, May 1996. [2] \nRobert S. Boyer and J Strother Moore. A Computa\u00adtional Logic Handbook. Academic Press. 1988. [3] Andrei \nBroder. Some applications of Rabin s .nger\u00adprinting method. In R. Capocelli, A. De Santis, and U. Vaccaro, \neditors, Sequences II: Methods in Commu\u00adnications, Security, and Computer Science, pages 143 152. Springer-Verlag, \n1993. [4] S. I. Feldman. Make A program for maintaining com\u00adputer programs. Software Practice and \nExperience, 9(4):255 265, April 1979. [5] Allan Heydon, Jim Horning, Roy Levin, Timothy Mann, Yuan Yu. \nThe Vesta-2 Software Description Language. Compaq Systems Research Center Techni\u00adcal Note 1997-005c, \nJune, 1998. [6] Allan Heydon, Roy Levin, Timothy Mann, Yuan Yu. The Vesta Approach to Software Con.guration \nMan\u00adagement. Compaq Systems Research Center Technical Note 1999-001, June 22, 1999. [7] John Hughes. \nLazy Memo-Functions. Lecture Notes in Computer Science, 201:129 146. Springer-Verlag, Berlin, Sept. 1985. \n[8] D. Michie. Memo Functions and Machine Learning. Nature, 218:19 22, April, 1968. [9] William Pugh \nand Tim Teitelbaum. Incremental Com\u00adputation via Function Caching. In Proceedings of the Sixteenth Annual \nACM Symposium on Principles of Programming Languages (POPL 89), pages 315 328, January, 1989. [10] M. \nO. Rabin. Fingerprinting by random polynomials. Report TR 15 81, Department of Computer Science, Harvard \nUniversity, 1981. [11] Matt Reilly. Designing an Alpha Microprocessor. IEEE Computer, 32(1):27 34, Jan. \n1999. [12] Frank Tip. A Survey of Program Slicing Techniques. Journal of Programming Languages, 3(3):121 \n189, Sept. 1995. [13] Vesta Home Page. http://www.research.compaq.com/\u00adSRC/vesta/. [14] Mark Weiser. \nProgram Slicing. IEEE Transactions on Software Engineering, SE-10(4):352 357, July 1984.  \n\t\t\t", "proc_id": "349299", "abstract": "<p>This paper describes the implementation of a purely functionalprogramming language for building software systems. In this language,external tools like compilers and linkers are invoked by function calls. Because some function calls are extremely expensive, it is obviously important to reuse the results of previous function calls whenever possible. Caching a function call requires the language interpreter to record all values on which the function call depends. For optimal caching, it is important to record precise dependencies that are both <italic>dynamic</italic> and <italic>fine-grained</italic>. The paper sketches how we compute such dependencies, describes the implementation of an efficient function cache, and evaluates our implementation's performance.</p>", "authors": [{"name": "Allan Heydon", "author_profile_id": "81100568808", "affiliation": "Compaq Computer Corporation, Systems Research Center, 130 Lytton Avenue, Palo Alto, CA", "person_id": "P15272", "email_address": "", "orcid_id": ""}, {"name": "Roy Levin", "author_profile_id": "81100404647", "affiliation": "Compaq Computer Corporation, Systems Research Center, 130 Lytton Avenue, Palo Alto, CA", "person_id": "PP31084481", "email_address": "", "orcid_id": ""}, {"name": "Yuan Yu", "author_profile_id": "81100472712", "affiliation": "Compaq Computer Corporation, Systems Research Center, 130 Lytton Avenue, Palo Alto, CA", "person_id": "PP43120276", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/349299.349341", "year": "2000", "article_id": "349341", "conference": "PLDI", "title": "Caching function calls using precise dependencies", "url": "http://dl.acm.org/citation.cfm?id=349341"}