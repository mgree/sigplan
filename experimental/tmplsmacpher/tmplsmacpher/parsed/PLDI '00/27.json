{"article_publication_date": "05-01-2000", "fulltext": "\n ABCD: Eliminating Array Bounds Checks on Demand Rastislav Bodik Rajiv Gupta Vivek Sarkar University \nof Wisconsin University of Arizona IBM T.J. Watson Research Center bodik@cs.wisc.edu gupta@cs.arizona.edu \nvsarkar@us.ibm.com Abstract To guarantee execution, Java and other strongly typed lan\u00adguages require \nbounds checking of array accesses. Because bounds checks may raise exceptions, they block code motion \nof instructions with side effects, thus preventing many useful code optimizations, such as partial redundancy \nelimination or instruc\u00adtion scheduling of memory operations. Furthermore, because it is not expressible \nat  level, the elimination of bounds checks can only be performed at run time, after the program is \nloaded. Using existing powerful bounds-check optimizers at run time is not feasible, however, because \nthey are too heavyweight for the dynamic compilation setting. ABCD is a light-weight algorithm for elimination \nof &#38;ray Checks on Demand. Its design emphasizes simplicity and efficiency. In essence, ABCD works \nby adding a few edges to the SSA value graph and performing a simple traversal of the graph. Despite \nits simplicity, ABCD is surprisingly powerful. On our benchmarks, ABCD removes on average 45% of dynamic \nbound check instructions, sometimes achieving near-ideal optimization. The efficiency of ABCD stems from \ntwo factors. First, ABCD works on a representation. As a result, it requires on av\u00aderage fewer than 10 \nsimple analysis steps per bounds check. Sec\u00adond, ABCD is demand-driven. It can be applied to a set of \nfre\u00adquently executed (hot) bounds checks, which makes it suitable for the dynamic-compilation setting, \nin which compile-time cost is constrained but hot statements are known. Introduction The advent of safe \nmobile computing in general, and Java in par\u00adticular, has brought about two significant changes for optimizing \ncompilers. First, type safety requires expensive run-time seman\u00adtic checks (e.g., bounds checks, null \nchecks, type checks, etc.). Second, because the mobile must be verifiably when it is loaded, the optimizer \ncan remove semantic checks from  only at run time. This paper addresses the problem of eliminating \nredundant bounds checks using lightweight techniques suitable for the time-constrained dynamic-compilation \nsetting. Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for profit or commercial \nadvantage and that copies bear this notice and the full citation on the first page. To copy otherwise, \nor republish, to post on servers or to redistribute to lists, requires prior specific permission and/or \na fee. 2000, Vancouver, British Columbia, Canada. Copyright 2000 ACM l-581  Bounds checks cause programs \nto execute slower for two rea\u00adsons. One is the cost of executing the bounds checks themselves since they \ncan occur quite frequently and involve a memory load of the array length and two compare operations. \nEven more impor\u00adtantly, the presence of bounds checks greatly limits the application of code optimizations. \nPrecise exception semantics (as in Java) re\u00adquires that all code transformations preserve the program \nstate at an exception point, and also the order in which exceptions occur. As a consequence, the application \nof traditional optimizations must be restricted to prevent side-effect-causing instructions from moving \nacross any exception points. Since the exception points introduced by array bounds checks can be frequent, \nthe scope over which timizations are applicable can be severely restricted.  When designing optimizations \nfor Java, we face two conflicting goals: the optimization algorithm must yet general enough to globally \n bounds checks that are fully redundant, or even partially redundant. The following observations provide \ninsight into meeting the challenge for keeping the cost of dynamic opti\u00admization low: We should employ \ndemand-driven-analysis techniques since they are highly suitable for dynamic optimization. driven analysis \ncan be used to focus attention on hot bounds checks; i.e., the bounds checks with the highest execution \nfre\u00adquencies. Furthermore, demand-driven analysis is well suited to dealing with incremental updates \nof data flow information after program transformations because it sidesteps the need for expensive initialization \nphases needed in exhaustive anal\u00adyses. We should develop algorithms that reuse representations that are \ncommonly used in compilers. Our approach in this work is to start with SSA form (i.e., we assume it \nto be already available) and develop an algorithm that works by simple traversals of a cheaply computable \nextension of the SSA graph. Many of the existing algorithms for array-bounds-check elim\u00adination are \nheavyweight (e.g., those based on theorem provers and are therefore not suitable for de\u00adployment in \na dynamic-optimization environment. Some simpler algorithms (e.g., those based upon value-range analysis \nPat95,  cannot eliminate partially redundant checks. gorithms that can eliminate partial redundancy \n  operate upon dense program representations (e.g., the control flow graph) and rely upon exhaustive \niterative data flow analyzers. Thus, they, too, do not meet our time requirements. In this paper, we \nintroduce a new algorithm, called ABCD, for 321 elimination of array bounds checks on demand. The ABCD \nalgo\u00adrithm makes the following contributions: 1. Sparse Representation: ABCD uses a novel sparse repre\u00adsentation \ncalled the inequality graph, which is built from an extended SSA representation. This representation \ncan be cheaply constructed from a given SSA representation of the program. We will see that the inequality \ngraph representation is powerful enough to enable all array-bounds checks to be eliminated from the BubbleSort \nprogram in Figure 1. (To the best of our knowledge, no other existing Java compiler can fully eliminate \nall the bounds checks in this example.) 2. Demand-Driven Analysis: ABCD employs a demand-driven approach \nin which the algorithm proceeds by propagating inequality assertions that need to be veri.ed to eliminate \na bounds check. Therefore, the optimization can be performed incrementally by starting with the hot bounds \nchecks. 3. Generality: The effectiveness of ABCD stems from its ability to remove both fully and partially \nredundant checks. Through simple traversals of the sparse inequality graph represen\u00adtation, our algorithm \nis able to locate insertion points for checks required for eliminating partial redundancy. To re\u00admain \nwithin the tight compile-time constraints of a dynamic compiler, ABCD exploits recent results in redundancy \nelimi\u00adnation, which show how simple pro.le-based algorithms can achieve nearly complete removal of redundancies \n[BGS98]. For the sake of ef.ciency, the assertion checking (theo\u00adrem proving) is restricted to the .ve \nclasses of constraint\u00adgenerating statements described in Section 2. 4. Empirical evaluation: Our experience \nshows that, despite its simplicity, ABCD is ef.cient, effective, and simple to imple\u00adment. We have implemented \nABCD in the Jalape no optimiz\u00ading compiler [BCF+99]. Our results show that ABCD re\u00admoves on average 45% \nof all dynamic bounds checks, while performing fewer than 10 analysis steps per bound check.  The rest \nof the paper is organized as follows. Section 2 gives an overview of ABCD. Section 3 presents our sparse \nprogram rep\u00adresentation. Section 4 describes the constraint system used for de\u00adtecting redundant checks \nand Section 5 shows how to solve the con\u00adstraint system to remove fully redundant bounds checks. Section \n6 extends the removal to partially redundant bounds checks. Sec\u00adtion 7 outlines possible extensions to \nthe ABCD algorithm. Sec\u00adtion 8 presents experimental evaluation of the ABCD algorithm. Finally, Section \n9 compares ABCD with existing work.  Overview of ABCD A bounds check check A[x] is redundant if 0:xA.length \nwhenever the check is executed. ABCD optimizes the lower-bound check (0:x) and the upper-bound check \n(xA.length)astwo independent problems.1 In this paper, we restrict our attention to the optimization \nof upper-bound checks. The (dual) algorithm for lower-bound checks can be derived trivially. A straightforward \napproach to detecting redundant checks is to i) construct a constraint system at each program point, \nperhaps by propagating the constraints using data.ow analysis, and then ii) apply a theorem prover at \nthe point of the bounds check. Both of these are expensive steps and ABCD streamlines them in the following \nways: 1We forgo the (rare) optimization opportunities created by the interplay of the two problems for \nthe sake of simplicity. limit = a.length; st= -1; while (st < limit) { st++; limit--; for (j = st; j \n< limit; j++) { if (a[j] = a[j+1]) ... } for (j = limit; --j >= st; ) { if (a[j] = a[j+1]) ... } } \n Figure 1: The running example: Bidirectional Bubble Sort. The .gure shows a relevant fragment of a program \nfrom the Symantec benchmark suite. A bounds check is performed before each of the four array accesses. \nTo simplify the presentation, the rest of the pa\u00adper omits the second for loop. ABCD can eliminate all \nfour bound checks in this example. 1. Instead of constraint propagation, ABCD builds a single, program-point-independent \nconstraint system. This con\u00adstraint system is sparse; in fact, it is nothing more than SSA form with \na few extra edges. 2. Instead of relying on a theorem prover, ABCD performs a simple, demand-driven \ntraversal of the sparse representation. Despite its limited power, our traversal prover was able to eliminate \n45% of dynamic upper-bound checks in our exper\u00adiments.  The remainder of this section outlines the \nmain components of ABCD: the types of constraints, the sparse constraint system and its graph representation, \nthe constraint solver, and .nally the handling of partially redundant bounds checks. The Constraints. \nABCD is ef.cient because it operates on sim\u00adple difference constraints [Pra77, CLR92] of the form x-y:, \nwhere xis a program variable, yis a program variable or a sym\u00adbolic literal (e.g., the length of an array), \nand is an integer con\u00adstant. Restricting the constraint form is what enables ABCD s ef.\u00adciency: constraints \non pairs of variables can be represented in the sparse, global constraint system; and the difference \nrelationships can be processed with the simple traversal solver. Other work (e.g., [Sho81]) has also \nfound it bene.cial to restrict attention to this class of inequalities. Furthermore, ABCD gathers only \nconstraints that can be ob\u00adtained with a local examination of the program code, without any prior global \nanalysis. The following .ve types of program state\u00adments have been found to generate constraints useful \nfor bounds\u00adcheck elimination: Clarray-length literal x:=A.length C2constant assignment x:= C3constant \nincrement or decrement x:=y+ C4conditional branches if/while x:y C5array-bounds check check A[x] Constraints \nCl-C3are generated by restricted forms of assign\u00adments. Constraint C4exploits invariants generated, on \neach exit of a branch, by the branch s conditional expression. Finally, con\u00adstraint C5exploits a successful \narray-bounds check check A[x], which guarantees that x<A.length. Note that all .ve of these 1. Build \ne-SSA form: constraint types can be expressed as difference constraints. If a insert 7-nodes (see Section \n3) variable is de.ned in an assignment that generates more complex compute SSA form (e.g., using [CFR+91]) \nconstraints (e.g., those outside CI-C3), then ABCD considers the 2. Build the Inequality Graph GI: variable \nunconstrained (unless it also appears in statements C4- C5). In general, disregarding existence of some \nconstraints is safe, (see Table 1) as it will, at worst, hide from us some redundant checks. 3. Remove \nredundant checks: for each check nof the form check A[x] do  Sparse Global Representation of Constraints. \nConstraints gen\u00ad see Figure 5 erated by program statements CI C5are program-point speci.c. if demandProve(GI,(x-A.length \n:-I))then Each of them is guaranteed to hold only in its live range, which is, remove nfrom the program \n informally, the range of CFG nodes between the generating state\u00adment and its dominance frontier or a \nkilling de.nition, whichever end for is closer. To encode the constraint live ranges compactly (i.e., \nglob\u00adally rather than per node), ABCD splits live ranges of program vari- Figure 2: The ABCD algorithm. \nables via SSA-style renaming. In effect, the renaming converts a .ow-sensitive constraint system into \nan equivalent .ow-insensitive system, as was done for pointer analysis in [HH98]. equality graph is encoded \nvia two kinds of nodes. The max nodes ABCD splits live ranges of variables such that i) each resulting \nare the \u00a2-nodes, which correspond to control .ow merges. The live range refers to a unique variable name, \nand ii) for any vari\u00ad remaining nodes act as the min nodes. able Vi, the live range of Viis no larger \nthan the live range of any The inequality graph also enables elimination of partially re\u00adconstraint involving \nVi. When expressed using the unique variable dundant bound checks. Partially redundant checks are redundant \nnames, the constraint system becomes program-point-independent along some, but not necessarily all, control \n.ow paths [MR79]. A (global). Moreover, inheriting the SSA properties, the constraint typical example \nis a loop-invariant check: its outcome is the same system is sparse, in that it directly connects statements \nthat generate in each loop iteration and therefore it can be optimized by being related assertions, thus \nspeeding up the constraint-solving process. executed once, before the loop is entered. While fully redundant \nAs in SSA, we split live ranges by inserting dummy assign\u00ad bounds checks can be deleted, partially redundant \nchecks require ments. To split live ranges of constraints CI C3, the standard insertion of compensation \nchecks that ensure that the check be-SSA \u00a2-assignments are suf.cient. However, constraints C4and comes \nredundant along all control .ow paths. C5require additional splitting, which we perform by inserting \n7- We identify the insertion points using the \u00a2-nodes in the in\u00adassignments at conditionals and bound \nchecks. We call the result\u00ad equality graph. As was shown in [CCK+97] in the context of ex\u00ading representation \nthe extended SSA (e-SSA) form. The e-SSA pression elimination, insertion points correspond to a certain \nsub\u00adform for the running example is shown in Figure 3. set of in-edges of the \u00a2-nodes. We extend this \napproach to the inequality graph. Because the insertion performed by ABCD may Solving the Constraint \nSystem. To simplify the presentation, be speculative, we rely on pro.ling data to determine pro.tability \nwe view the sparse constraint system not as a system of inequal-of the optimization. Our approach relies \nupon recent results that ities, but instead as a graph, called the inequality graph. On the demonstrate \nthat speculative insertion is nearly as effective as non\u00adgraph, substitutions of constraints can be viewed \nas a graph traver-speculative techniques that perform complete redundancy removal sal, which we exploit \nfor formulating a simple traversal prover using code duplication [BGS98]. for our constraint system. \nThe inequality graph is an extension of the SSA value The ABCD Algorithm. The entire ABCD algorithm \nfor fully re\u00adgraph [AWZ88]. There is one node for each e-SSA-variable, con\u00ad dundant checks is outlined \nin Figure 2. The .rst step transforms the stant, or array-length literal (e.g., the value A.length); \nedge weights program into e-SSA form; the second steps connects e-SSA vari\u00adconstrain the differences \nbetween pairs of nodes. Given an inequal\u00ad ables with constraints; the third step solves the constraint \nsystem ity graph, an (upper) bounds check check A[x] is redundant if the and removes checks that have \nbeen proven to be redundant. shortest path from A.length to xhas negative length, which corre\u00adsponds \nto the bounds check condition, x-A.length <0, being al\u00ad 3 The Extended SSA Form ways true. The inequality \ngraph for the running example is shown in Figure 4. ABCD encodes the scope of constraints by transforming \nthe pro-The traversal solver for our constraint system is thus equiv\u00adgram into the extended SSA (e-SSA) \nform, described in this sec\u00adalent to a shortest-path computation; a demand-driven version tion. The salient \nproperty of e-SSA is that constraints generated in of the solver amounts to computing the shortest path \nbetween an e-SSA program are valid wherever their variables are live. This a pair of vertices (A.length \nand x). The situation is compli\u00adimplicit encoding obviates the need to explicitly qualify constraints \ncated, however, by the fact that the inequality graph is a hyper\u00adwith their CFG scope, which results \nin a compact constraint system. graph [Ber73, GLPN93, RR96], which operates on a generalized Another \nimportant property is that constraints expressed on the e\u00adnotion of the shortest path (see Section 4). \n SSA program de.ne a sparse constraint system that can be solved ef.ciently. Handling Control Flow. The \nreason why ABCD s constraint To understand the relationship between live ranges of variables system induces \na non-standard notion of the shortest path is that and scopes of constraints, consider a constraint \nx-y:generatedconstraints must be treated in two different ways: Along a given in a CFG node n(for example \nby statement x:=y+). A safe control .ow path, each variable is bounded by the strongest con\u00ad rule for \ndelineating the scope of this constraint is to restrict it to straint generated on that path. In contrast, \nacross a set of control nodes at which xand yhave the same values as they had at node n,.ow paths, a \nvariable is bounded by the weakest constraint pro\u00ad which is typically approximated by requiring that \nthe values of x duced by any of the paths. This max-min semantics of the in\u00adand yoriginate from the same \nde.nition sites as they did at node n. Given this rule, the scope can be represented by enumerating nodes \nwhere this same-de.nitions condition holds. ABCD uses a more ef.cient approach: it splits and renames \nlive ranges of variables so that variables that appear in a constraint are used (i.e., are live) only \nwithin the constraint s scope.2 In effect, the renaming converts a .ow-sensitive constraint system into \nan equivalent .ow-insensitive system, without a loss of precision. ABCD splits variable live ranges \nby means of assignments, ei\u00adther existing or new ones. As in the SSA form, each assignment writes into \na unique variable name, which uniquely names live ranges. Assignments are needed at two points: Case \n1) When a constraint is created: Live ranges of constraints C1 C3already start at assignments, whose \ntarget variables can be directly renamed. Constraints C4start at exits of con\u00additionals, and constraints \nC5start at bounds checks. To cap\u00adture these two constraint types, ABCD inserts 7-assignments, as will \nbe discussed below. Case 2) When a constraint is killed: The scope of a constraint u-v:generated at node \nnis terminated (i) at a node where uor vis rede.ned, (ii) at a control .ow join that can be reached by \na path from nalong which uor vare rede.ned, or (iii) at a control .ow join that is not dominated by n. \nThe standard SSA form suf.ciently splits variable live ranges to en\u00adcode constraints C1 C3. The SSA renaming \nis performed as fol\u00adlows [AWZ88]: New de.nitions, called q-assignments, are placed (recursively) at control \n.ow join points that are reachable by differ\u00adent de.nitions of a variable v. Renaming the targets of \noriginal as\u00adsignments to venforces cases 1 and 2-i above; renaming the targets of q-assignments enforces \ncases 2-ii and 2-iii. After the renaming, each use of a variable is dominated by its single reaching \nde.nition, which guarantees the desired property that vhas the same value in its entire live range. To \nexpress constraints C4and C5, case 1 requires additional re\u00adnaming. The renaming is performed by introducing \n7-assignments on the exits of conditionals and bounds checks. Consider the con\u00additional statement shown \nbelow on the left: if (id:10) then if (id:10) then i2:=7(id) // i2:10 here else = else i3:=7(id) // i3>10 \nhere end if end if i4:=q(i2Vi3) The 7-assignments are inserted, for each variable appearing in the conditional \nexpression, into the CFG out-edges of the conditional branch.3 Thanks to the 7-assignemnts, each outcome \nof the con\u00additional is associated with a distinct variable name, which serves as a hook for attaching \nthe constraints generated by the branch. Although 7-assignments are inserted into CFG edges, in the tex\u00adtual \nrepresentation of the program used throughout this paper, 7\u00adassignments are placed at the beginning of \nthe basic blocks targeted by the branch. 2To reduce the amount of program transformation, we actually \nallow the variables appearing in the constraint to be live outside the scope, as long as they are simultane\u00adously \nlive only in the scope. 3Note that 1-assignments are analogous to the switch operators in the dependence \n.ow graph [JP93]. Similarly, to generate a new name for constraints C5,a 7\u00adassignment is inserted after \neach bounds check. A bounds check can be viewed as a special if statement that will transfer program \ns control into the exception handler if the check fails; if the check does not fail, a useful constraint \nis generated: check arid check arid = i2:=7(id)i // i2<a.length here  The constraint C5must be expressed \non the new name i2, rather on id, otherwise it could erroneously lead to elimination of some bound checks, \nincluding the generating check itself. Example 1 (e-SSA Form) Figure 3 depicts the running example before \nand after the conversion to e-SSA form. For the sake of brevity, the second for loop from Figure 1 is \nomitted. To reduce the number of q-assignments induced by 7-assignments, no q\u00adassignment for limit is \ninserted into the for loop. This is safe because there are no uses of limit4in the loop. . 4 The Constraint \nSystem and the Inequality Graph Once the program is converted into e-SSA form, the constraints generated \nthroughout the program can be connected into a sin\u00adgle, .ow-insensitive constraint system. Thanks to \nthe simplicity of constraints considered by ABCD (difference constraints of two variables), this constraint \nsystem can be represented as a weighted directed graph and can be solved with relatively ef.cient graph \ntraversal algorithms. This section presents the graph representation of the constraint system and its \nproperties. The following section presents an ef.cient solver for the constraint system. Representing \nsimple constraint systems with graphs is a stan\u00addard technique; a common example is the constraint graph \n[Pra77, CLR92]. Our Inequality Graph (GI) generalizes the constraint graph in that it allows representing \nprogram s control .ow, as fol\u00adlows. First, GIallows us to detect whether a given bounds check is redundant \nalong all control .ow paths. Second, GImaintains enough program structure information to perform code \nmotion of partially redundant bounds checks (see Section 6). The solution of the constraint system represented \nby GIis computed similarly to how it is computed on the constraint graph using a notion of the shortest \npath (see Section 5). Informally, vertices of the inequality graph represent program variables (in e-SSA \nform), constants, and literals. An edge con\u00adnects two vertices if the program generates a constraint \non their difference (variables for which the program does not generate a constraint will create disconnected \nvertices in GI). De.nition 1 (The inequality graph GI) Given a program Pin e-SSA form, the Inequality \nGraph of Pis a weighted, directed graph GI=(VVEVd)with a distinguished set of vertices V\u00a2.V: V={vid{Aj.lengthd{kd, \nwhere viis a program variable, Aj.length is a program literal denoting the length of array Ai, and kis \nan integer constant appearing in the program P.  Econtains a directed edge u-viff Pgenerates a constraint \nv-u:according to any rule in Table 1. The weight d(u-v)of the edge is EN.  V\u00a2is a distinguished subset \nof vertices, V\u00a2.V, such that viEV\u00a2iff variable viis de.ned in program Pusing an e-SSA q-assignment. . \n   Original program: limit := A.length st := -1 while: while (st < limit) if (st < limit) { st:= st+ \n1 limit := limit -1 j:= st for: for (j := st; j < limit; j++) if (j < limit) { check A[j] A[j] t:=j+1 \ncheck A[t] A[j+1] j:=j+1 j++ goto for } end for goto while } end while  e-SSA form: limito:= A.length \nsto:= -1 while: limitl:= q(limito,limit3) stl:= q(sto,st3) if (stl< limitl) { st2:=7(stl) limit2:=7(limitl) \nst3:= st2+1 limit3:= limit2-1 jo:= st3 for: jl:= q(jo,j4) if (jl< limit3) { j2:=7(jl) limit4:=7(limit3) \ncheck A[j2] j3:=7(j2) to:= j3+1 check A[to] tl:=7(to) j4:= j3+1 goto for } goto while }  Figure 3: \nThe running example before and after the conversion into e-SSA form. Recall that Table 1 de.nes the edges \nof GIfor the elimination of upper-bound checks; the elimination of lower-bound checks is based on analogous, \nbut separate, inequality graph. Example 2 (The Inequality Graph GI) Figure 4 shows the in\u00adequality graph \nGIfor the running example. The vertices are the program entities from Figure 3: (e-SSA) variables, the \nliteral A.length, and the constant -1. The set V\u00a2contains the three q\u00adassignments. Note that, thanks \nto e-SSA form, a vertex represents the de.nition as well as all uses of a variable. e-SSA form also guarantees \nthat GIis not a multigraph. Finally, note that the names in Vdenote, interchangeably, both the program \nentities and the vertices of the inequality graph. . It is worth mentioning why we decided to represent \nconstraints generated by program assignments with inequalities, rather than with equalities, which appear \nto be the more intuitive choice, and which would also allow using the same inequality graph for both \nupper-and lower-bounds check elimination. The .rst motivation is to represent all constraints uniformly, \nexclusively with inequal\u00adities. The more important motivation is to formulate a consistent constraint \nsystem in the presence of the constraints C4. Consider the C4rules in Table 1. If the assignments Vj:=7(Vi)and \nVk:=7(Vi)were translated into constraints Vj=Viand Vk=ViV rather than into Vj<Viand Vk<Vi, as it is done \nby ABCD, the constraint system would imply Vj=Vk. Similarly, the system would imply Ws=Wtfor the other \n7\u00adassignments in C4. As a result, the constraints generated on the exit of the conditional in rule C4, \nnamely Vj<Wsand Wt<Vk-1V would be inconsistent, as no value of the program variables Vand Wcould satisfy \nthe constraint on Vk: Vk=Vj<Ws=Wt<Vk-1. To avoid the inconsistency, ABCD translates the assignments \nVj:= 7(Vi)and Vk:=7(Vi)into constraints Vj<Viand Vk<ViV which makes variables Vj, Vkmutually unconstrained, \nre.ecting the fact that Vjand Vkare never live simultaneously at any node in the program and thus their \nvalues should not be compared. We are now ready to de.ne the system of difference constraints. De.nition \n2 (The constraint system) The constraint system of an inequality graph GI=(VVVd)with a distinguished \nset of ver\u00adtices V\u00a2is a set of inequalities Jax{u+d(u-V)}if VEV\u00a2, V<u-v(1) J .{u+d(u-V)}if VEV-V\u00a2. u-v \n constraint type generating statements generated constraint edge / edge weight C1 Vi:=AjVlength Vi<AjVlength \nAjVlength-Vi 0 C2 Vi:= Vi< -Vi 0 C3 Vi:=Vj+ Vi<Vj+ Vj-Vi C4 if Vi<Wrthen Vj:=7(Vi)Ws:=7(Wr) Vj<Vi Ws<Wr \nVj<Ws Vi-Vj 0 Wr-Ws 0 Ws-Vj 0 else Vk:=7(Vi)Wt:=7(Wr) Vk<Vi Wt<Wr Wt<Vk-1 Vi-Vk 0 Wr-Wt 0 Vk-Wt -1 C5 \ncheck Ak[Vi] Vj:=7(Vi) Vj<AkVlength-1 AkVlength-Vj-1 control .ow Vi:=q(Vj,Vk) Vi<max{Vj,Vk}(ViEV\u00a2) Vj-Vi \n0 Vk-Vi 0  Table 1: The edges of the inequality graph for elimination of upper-bounds checks. for each \nVEV. A feasible solution for the constraint system is an assignment of integer values to variables in \nVthat satis.es all constraints. . Handling Control Flow. ABCD extends the standard difference\u00adconstraint \nsystem [Pra77] in order to express the control .ow of the program. The standard system consists of a \nset of equations V-u<d(u-V)\\V,uEV which is equivalent to the following set of equations V<min{u+d(u-V)}\\VEVV(2) \nu-v To see how ABCD extends the standard difference-constraint sys\u00adtem, compare equations 1 and 2: a \nset V\u00a2of distinguished vertices is constrained not by the strongest constraint that holds on its in\u00adedges, \nbut instead by the weakest one. To re.ect the semantics of vertices in their mnemonics, we refer to vertices \nin V\u00a2as max ver\u00adtices and the vertices in V-V\u00a2as min vertices. Because V\u00a2corresponds to q-assignments, \nEq. 1 ensures that a bounds check is redundant along all incoming control .ow paths. As an example, consider \nvariable stlEV\u00a2de.ned in Figure 4 by stl:=q(sto,st3). The following constraints hold for arguments of \nthe q-assignment (the .rst line follows from the fact that array length is non-negative, which could \nbe represented as an edge in GI): sto<-1<AVlength -1 st3<st2+1<limit2<limitl<limito<AVlength i.e., the \ncontrol .ow predecessor corresponding to st3constrains the value of stlless than the predecessor corresponding \nto sto. The constraint on stlthat holds along all incoming control .ow paths is stl<AVlength, the weaker \nof the two constraints, as is (correctly) computed by Eq. 1: stl<max{sto,st3}. Consistency. Because GImay \ncontain negative cycles, its con\u00adstraint system may seem inconsistent, due to implying V<V+ ,<0. Informally, \nthe consistency of out constraint system is guaranteed by the presence of at least one max vertex in \neach cy\u00adcle, which breaks the cycle if it is negative. Consider the negative cycle limitl,limit2,limit3in \nFigure 4. Because each negative cy\u00adcle strengthens constraints, the weakest constraint at the max vertex \nlimitlmust come from outside the cycle. By propagating the con\u00adstraint on limitofrom outside the cycle, \nthe max node effectively breaks the cycle. More formally, let Cbe a negative cycle in GI. Because each \ncycle in GIis created as a result of cyclic control .ow, Cmust con\u00adtain at least one q-assignment whose \none or more arguments are de\u00ad.ned outside the cycle. Let Vl, de.ned by Vl:=q(V2,V3), be such a vertex. \nFrom the constraint system we have Vl<max{V2,V3}. Assume that V2is the argument de.ned outside the negative \ncycle C. Hence, V3 closes the negative cycle: V3<Vl+,<0. Therefore, Vl<max{V2,Vl+}=V2, which yields an \nequivalent constraint system without the negative cycle C. Redundancy of a Bounds Check. We now relate \nthe solution of the constraint system to the redundancy of a bounds check. De.nition 3 (Fully redundant \ncheck) An upper-bound check check Ai[Vj] is fully redundant if Vj<AiVlength -1whenever the check is executed. \n. In terms of the constraint system, a bounds check is redundant if the check is true under each feasible \nsolution of the constraint system. In other words, a bounds check is redundant if it is implied by the \nconstraint system. Theorem 1 Let bbe an upper-bound check check Ai[Vj] in pro\u00adgram P, and let GIbe the \ninequality graph of P. The check b is redundant if for any feasible solution Dto GI, the D(Vj)< D(AiVlength)-1. \n. Proving redundancy of the bounds check check Ai[Vj] thus entails computing the greatest value of t(AiVlength,Vj)= \nD(Vj)-D(AiVlength), for all feasible solutions Dof GI. The bounds check check Ai[Vj] is redundant if \nt(AiVlength,Vj)< -1. We call the value t(u,V)the distance between vertices uand Vin GI. Alternatively, \nthe distance t(u,V)can be de.ned as the smallest value dsuch that adding constraint V<u+dinto the constraint \nsystem will not change the set of feasible solutions of the system. The problem of computing the distance \nin GIis a generaliza\u00adtion of the shortest-path problem in a weighted directed graph. Be\u00adfore we outline \nthe generalization, we note that, for upper-bound check elimination, the distance in GIcorresponds to \nthe longest path, i.e., to the shortest path in the reverse problem, in which +0<-0and hence the max \noperator selects the shorter path. / st / .1 /  st=\u00a2(st/,st) / y3/j // / st2 st 3yj =\u00a2(j/,j4) y / \n / j 4 2j /  y/ .y limit/ A.length j 3 /  y . y  .y limity=\u00a2(limit/,limit3) t/ / / limit2 limit3 \nlimit4 .y/  Figure 4: The inequality graph GIof the running example. The meaning of an edge u.vis v:u+. \nEach (upper-bound) check is represented in GIwith two vertices: the array-length vertex and the array-index \nvertex. For example, the bounds check check A[j2], is represeneted with vertices A.length and j2. A bounds \ncheck is redundant if the distance between the two vertices is less than 0. To compute the distance from \na node u, we propagate from uthe sum of edge weights in the following way: at each \u00a2-node, the distance \nequals the greatest incoming distance; at each remaining node, the distance equals the smallest incoming \ndistance. The distance between A.length and j2is -2. The corresponding shortest path for this distance \nis (A.length,limit/,limity,limit2,limit3,limit4,j2). For the rest of the paper, the terms shorter/longer \nrefer to the re\u00adverse problem (in which 3 is shorter than 2). An intuitive way to describe how distance \ngeneralizes the short\u00adest path is to give an instance of GIin which the distance and the shortest path \nare de.ned identically. This is the case when only vertices from V\u00a2have multiple predecessors. Under \nsuch a restric\u00adtion, each path in GIcorresponds to a control .ow path, and the distance in GIcorresponds \nto the shortest path in GI. The shortest path thus .nds a constraint that is valid along all control \n.ow paths. In contrast, when vertices in V-V\u00a2are allowed to have multiple predecessors, GImay contain \npaths that corresponds to different constraints that hold along the same control .ow path. Therefore, \nwhen computing the distance along a given control .ow path, we pick the longest of these different paths \nin GI. An elegant formalism for dealing with two kinds of paths is the hypergraph [Ber73, GLPN93, RR96]. \nA directed hypergraph con\u00adsists of a set of nodes and a set of hyperarcs, where each hyperarc connects \na set of nodes to a single target node. The concept of hy\u00adperpaths is de.ned recursively. There exists \nan empty hyperpath from from a set Sof nodes to a node tif tES. A non-empty hyperpath from a set Sof \nnodes to a node tconsists of an hyperarc from a set S.to tand a hyperpath from Sto sfor every node s \nin S. . A hyperpath from Sto tis thus a set of component paths, which are traditional paths (i.e., sequences \nof vertices). To turn the inequality graph into a hypergraph, we group all in-edges of a vertex vEV-V\u00a2into \na single hyperarc; every in\u00adedge of a vertex in V\u00a2represent a separate hyperarc. The distance between \nuand vis de.ned as follows: The length of a hyperpath P from uto vequals the length of the longest of \nthe component paths of P. The distance from uto vequals the shortest of all hyperpaths from uto v. Example \n3 To determine whether the bounds check check A[j2] is redundant using the inequality graph in Figure \n4, we compute the distance between the vertex A.length and the vertex j2. If the distance is less than \n0, then the array index j2is at most -1greater than the array length, and hence is within its bounds. \nThe distance between A.length and j2is -2. Hence the check check A[j2] is redundant. The distance is \nequal to the longest component path of the shortest hyperpath between the two vertices, which is (A.length,limit/,limity,limit2,limit3,limit4,j2). \n. 5 The Constraint Solver This section presents the details of the solver that ABCD uses to identify \nfully redundant checks. The extensions for partially re\u00addundant checks are described in the next section. \nThe constraint system represented by the inequality graph can be solved in various ways. First, as it \nhas already been mentioned, the inequality graph GIcan be viewed as a hyper\u00adgraph [Ber73, GLPN93, RR96]; \nthe redundancy of a check is then reduced to computing the shortest hyperpath between two ver\u00adtices (the \narray-length vertex and the array-index vertex). Sec\u00adond, when the hypergraph is viewed as an abstract \ngrammar prob\u00adlem [Ram96], the shortest hyperpath can be found using a .xed\u00adpoint computation. Third, \nthe shortest hyperpath can be computed ef.ciently using the data.ow analysis solver in [GW75], in O(E2) \ntime (even if GIis irreducible). ABCD uses an algorithm with worse asymptotic time complex\u00adity but with \nvery good practical running time, which is mainly due to the typically small size of the sparse inequality \ngraph. In con\u00adtrast to the above three exhaustive analysis approaches, our solver works on demand. An \nexhaustive algorithm analyzes all bounds checks in the program, which in the context of shortest paths \nmeans computing the single-source shortest-path problem for each array\u00adlength vertex. A demand-driven \napproach analyzes a single bounds check, which amounts to computing the shortest path between the array-length \nvertex and the array-index vertex. Our solver is demand-driven in yet another sense: Because we expect \nthat a dynamic optimizer will optimize only a small fraction of all bounds checks, our design favors \nperformance of a single\u00adcheck analysis over a batch analysis of all bounds checks. There\u00adfore, our solver \ndoes not actually return the length of the shortest path, but only a boolean information whether the \nshortest path is below a limit suf.cient to prove that a given bounds check is re\u00addundant. This less \nstrict question sometimes allows the solver to examine fewer paths than would be necessary to compute \nthe pre\u00adcise value of the shortest path. To explain how the solver works, let us consider .rst GIthat \nis acyclic (but contains both min and max nodes). An ef.cient way of computing the shortest path between \na source s(an array-length vertex) and a target t(an array-index vertex) is to perform a topo\u00adlogical \ntraversal from sto t, performing the min/max operations in the process. However, to determine the topological \norder, one must .rst traverse the entire graph, which is what demand analysis seeks to avoid in the .rst \nplace. Therefore, instead of a topological traver\u00adsal, the solver performs a brute-force depth-.rst exploration \nof the graph, in which a node may be visited multiple times. Each successive visit of a node corresponds \nto a stronger question about the distance of that node from the array-length vertex. The algorithm, shown \nin Figure 5, performs a depth-.rst traver\u00adsal of GIstarting at the array-index vertex b. Intuitively, \nthe recur\u00adsive exploration of the graph proceeds forward (against the direc\u00adtion of GIedges) until either \nthe source vertex ais reached, or until a cycle is detected. The goal is to determine, for each traversed \npath from bto a, whether it is longer than . To this end, the forward pass propagates the value and adjusts \nit as it crosses each edge, so that when the source vertex is reached, the traversed path is longer than \niff the propagated value is greater than zero. Depending on the outcome of this comparison, the recursion \nwill return the value of either True (the path is shorter) or False (the path is longer or of the same \nlength). Let us now consider cyclic inequality graphs. As was mentioned in the previous section, although \narbitrary inequality graphs may contain negative-weight cycles, these cycles have non\u00addecreasing effect \non the path distance, because each cycle is broken by a max node. Cycles with positive weight, however, \nmay impact path distance, if they are not broken by a min vertex. Positive cycles in GIcorrespond to \nprogram loops in which the program variable is incremented in the loop body. We call these GIcycles ampli\u00adfying \ncycles. In Figure 4, the cycles involving stland the cycle involving jlare amplifying. In contrast, the \ncycle of limitlis not amplifying. Consider the cycle on variable stin the example in Figure 4. If the \nedge limit2-st2was removed from Figure 4, the cycle on stwould cause the distance from stoto joto be \nof positive in.nite weight. Conceptually, our algorithm works by identifying and reducing amplifying \ncycles. After such a cycle is broken, two situations may occur. If the cycle was an articulation point \nbetween the source vertex and the bounds check vertex, the distance will be (correctly) computed to have \nin.nite weight, which means that the check can\u00adnot be proven to be redundant. If, after breaking the \ncycle, another path leads to the source (via a min vertex), the value of distance may still be small \nenough to prove the check redundant. Consider j2in Figure 4. When its amplifying cycle is broken (i.e., \nby remov\u00ading edge jl-j2), an alternative path to A.length remains, via limit4, with weight of -2, which \nis suf.cient to prove the bounds check redundant. To detect positive-weight cycles, the forward exploration \nkeeps in active[v]the value of the propagated value for each vertex vthat is on the active depth-.rst \ntraversal path. When a back\u00adedge is traversed, the positive weight cycle is detected by com\u00adparing the \ncurrent value of and its value one cycle ago. When a positive-weight cycle is detected, the value False \nis returned. When a harmless cycle of weight zero or less is detected, we consider the path reduced, \nreturning the value Reduced. The cycle is reduced in the sense that it does not in.uence the distance \nfrom ato b. In summary, when the forward exploration stops at a node, we know for each path originating \nat bwhether its distance is smaller than or equal to (result True), greater than (result False), or is \nreduced (result Reduced). When the recursion is returning back\u00adwards, the algorithm merges these results \naccording to the min-max semantics of GIvertices, using the following lattice L: True >Reduced >False, \n where True is the top element and False is the bottom element. A max node vEV\u00a2merges these values using \na meet operator n, xny=x-x:y.A min node vEV-V\u00a2merges these values using a join operator U, xUy=x-x?y. \n 6 Removing Partially Redundant Checks The previous section identi.ed checks that never fail. For some \nchecks, however, such a strong guarantee cannot be proven. A typ\u00adical example is a loop-invariant bounds \ncheck all we can prove is that it either fails in the .rst iteration of the loop or it never fails. In \ngeneral, such checks are called partially redundant: they are guar\u00adanteed not to fail along some (but \nnot all) control .ow paths leading to them. Some partially redundant checks can be eliminated with an \nop\u00adtimization called Partial Redundancy Elimination (PRE), which generalizes common subexpression removal \nand loop invariant code motion [MR79]. PRE works by inserting compensating checks in such program points \nthat make the partially redundant check fully redundant (i.e., after the insertions, the check will have \nbeen performed along all control .ow paths) and hence it can be re\u00admoved. We present here PRE optimization \nof array-bounds checks, as a natural extension of the ABCD algorithm for full redundancies. Before we \nproceed to describe the extensions to ABCD, let us introduce into the running example a partially redundant \ncheck. This task is easily accomplished by removing the assignment limito:=A.length from Figure 3. The \neffect on the inequal\u00adity graph (Figure 4) is that the vertex limitobecomes disconnected from vertex \nA.length, which breaks the shortest path that was used to prove that check A[j2] is fully redundant. \nThe check is now only partially redundant; namely, it is loop-invariant. 6.1 The Analysis In order to \nturn partially redundant bounds checks into fully redun\u00addant checks, the analysis must determine: Where \nto insert the compensating checks. Our goal is to .nd, for each partially redundant check b, a set of \nCFG edges into which checks must be inserted to make bfully redundant. For check A[j2] in Figure 3, it \nis suf.cient to insert a check function demandProve(GIVt)return boolean  GI=(VVEVd)is the inequality \ngraph with max vertices V\u00a2.V.  t=(b-a:)is the check to be proven, where bEVis the check s index variable, \nand aEVis the array-length literal. For example, when analyzing check A[x], t=(x-A.length :-1).Vertices \naand bare the source and the target of the shortest-path computation, respectively.  Cmemoizes the result \nof proving v-a:, where athe array-length literal. C:VxN-L, i.e., Cmaps (v-a:) into {TrueVFalseVReduced}. \n active detects cycles: if active[v]= null, then active[v]is the distance of vfrom b, where bis the \ncheck s index variable. active maintains the distance for each vertex vthat is on the path on the current \nDFS stack.  begin  1C active 2 if prove(aVbV)E{TrueVReduced}then return true ; else return false function \nprove(vertex aVvertex vVint )return {True, Reduced, False} // same or stronger difference was already \nproven 3 if C[v-a:e]=True for some e:then return True // same or weaker difference was already disproved \n4 if C[v-a:e]=False for some e.then return False // vis on a cycle that was reduced for same or stronger \ndifference 5 if C[v-a:e]=Reduced for some e:then return Reduced // traversal reached the source vertex, \nsuccess if a-a: 6 if v=aand .0then return True // if no constraint exist on the value of v,wefail 7 if \nvhas no predecessor in GIthen return False // a cycle was encountered 8 if active[v]= null then 9 if \nactive[v]then return False // an amplifying cycle 10 else return Reduced // a harmless cycle 11 end if \n12 active[v] 13 if vEV\u00a2then 14 for each edge u-vEEdo C[v-a:] C[v-a:]nprove(aVuV-d(u-v)) 15 else 16 for \neach edge u-vEEdo C[v-a:] C[v-a:]Uprove(aVuV-d(u-v)) 17 end if 18 active[v] null 19 return C[v-a:] end \nprove end demandProve Figure 5: The algorithm for proving the redundancy of a bounds check. The procedure \ndemandProve(GIVt)returns true if the check t=(b-a:)is proven to be redundant on the inequality graph \nGI, which is equivalent to showing that the distance between vertex aand vertex bin GIis no greater than \n. into the edge that corresponds to the .rst argument of limith s q-node, which is the entry of the while \nloop. The ABCD algorithm extended for PRE computes the set of insertion edges during the backtracking \nfrom the recursive ex\u00adploration of the inequality graph. A check is inserted into a q-node s in-edge \nexactly when some of q-node s arguments were proven (i.e., procedure prove() returned True) and some \nwere not able to be proven (i.e., prove() returned False). The False arguments are then collected during \nthe backtracking into the insertion set. What compensating checks should be inserted. The com\u00adpensating \ncheck may be different than the check being opti\u00admized. Speci.cally, the checks may differ in their index \nex\u00adpression: the compensation of a check check A[Vi] may require insertion of a check check A[Vj+j]. \nIn our ex\u00adample, the check checkA[j2] is compensated with a check check A[limito+]. Identifying the \nindex expression in ABCD is trivial. Due to the reliance on simple difference constraints, the index \nex\u00adpression is always of the form Vi+d(we assume that the in\u00addex expression of the optimized bounds check \nis of the form Vj+O): The variable Vicorresponds to the q-node argument into which the insertion is being \nperformed. The constant d equals the distance from the insertion point to the array-index vertex; in \nour example, the distance between limitoand j2 equals 2; therefore, the compensating check in our example \nis check A[limito+]. This distance can easily be com\u00adputed from the value propagated by the recursive \nprocedure prove().  Pro.tability of removing the partial redundancies. To ensure that inserted checks \ndo not increase the dynamic number of checks in the program, traditional PRE techniques insert a check \nonly when its cost can be amortized by removing a check on each control .ow path emanating from the insertion \npoint. The amortization condition is computed as backward data.ow problem of anticipability [MR79]. \n ABCD estimates pro.tability using run-time pro.ling. We do not require that the check be removed on \neach emanat\u00ading path. Instead, we allow control-speculative insertion, in which we speculate that the \nprogram will follow a path on which the insertion will lead to a removal of a check. To determine whether \nspeculative insertion is pro.table, ABCD compares the cumulative execution frequency of the insertion \npoints with the frequency of the partially redundant check. If the impairment due to insertions is lower \nthan the bene.t due to removal of the check, we carry out the transforma\u00adtion. Such a pro.le-based PRE \nhas been shown to be nearly as powerful as a complete removal of redundancies based on program restructuring \n[BGS99]. The algorithm in Figure 5 can be easily extended to remove partial redundancies. The recursive \nfunction prove is extended to return not only the three lattice values but, when the value is False, \nalso the list of insertion edges that will make the optimized check fully redundant. The meet and join \noperators now also manipulate the insertion sets. At a min vertex, ABCD selects the set that has the \nlower execution frequency. At a max vertex, the propagated sets are merged.  6.2 The Transformation \nAlthough ABCD allows an elegant analysis of where compensat\u00ading checks should be inserted, it does not \naddress the problem of transforming the program. The broader problem is how to main\u00adtain exception semantics \nin the presence of code motion: When a hoisted (i.e., inserted) check fails, the exception cannot be \nraised immediately; it must be delayed, and raised at the location of the original (i.e., partially redundant) \ncheck. This broader problem is beyond the scope of the paper. In this subsection we describe the solution \nthat we currently use in our implementation of ABCD. We also sketch the solution that we are investigating. \n In the current solution, each bounds check is split into two in\u00adstructions: the compare instruction, \nwhich sets a register if the in\u00addex is out of bounds, and the trap instruction that raises the excep\u00adtion \nif the .ag is set. As we discuss below, traps require a different transformation than compares do. Therefore, \nour current approach optimizes only the compare instructions. While leaving traps in the original locations \nprevents us from moving code freely around ar\u00adray accesses, which was the primary motivation for bounds-check \noptimization, note that only partially redundant traps are left in the code; all fully redundant traps \na majority of all traps are removed. Removing traps is similar to removing conditional branches. Traditionally, \nconditional branches are optimized with program re\u00adstructuring (i.e., code duplication) [MW95, BGS97]. \nWe see re\u00adstructuring as too expensive for a dynamic compiler, and hence our current work is exploring \ntransformation techniques unique to the dynamic-optimization setting. Our approach for run-time removal \nof traps is as follows. The optimized version of the loop executes without a trap. When a compare instruction \nfails, the code for the unoptimized version of the loop (with the trap) is generated and the execution \nis transferred to it. Now, it is possible that the hoisted check failed spuriously, i.e., it was executed \nspeculatively (see Section 6.1). If this is the case, the unoptimized loop .nishes without encountering \nthe trap and we proceed with the execution of the optimized version of the code.  7 Extensions 7.1 Global \nValue Numbering The inequality graph G[can represent not only local constraints, i.e., those generated \nby individual program statements shown in Table 1, but also constraints deduced by a global program anal\u00adysis, \nfor example, by global value numbering [AWZ88]. When two SSA variables Viand Vjare found to be value \ncongruent, their equivalence can be re.ected in G[by a constraint edge Vi-Vj with weight O. Following \nthe restrictions of the value-numbering algorithm, this edge can be inserted only if the de.nition of \nVidom\u00adinates the de.nition of Vj, which serves to guarantee that any con\u00adstraint that holds on Viwill \nhold whenever Vjis executed. Our current implementation of ABCD exploits global value numbering in a \nmore restricted, but also more economical, fash\u00adion than described above. We do not encode the results \nof value\u00adnumbering analysis on G[. Instead, we consult the congruence information on demand, in the following \ncommon scenario: When attempting to eliminate a check check A[x], we were able to es\u00adtablish that xB \nlength but not that xA length. In that case, we consulted the value-numbering analysis and if Aand Bwere \ncongruent, we obtained the desired proof that xA length. 7.2 Elimination of both lower-and upper-bounds \nchecks The ABCD algorithm presented in the paper eliminates upper\u00adbound checks. To detect redundant lower-bound \nchecks, two changes to the algorithm are needed. First, we reverse the rela\u00adtional operator of the constraints \nCl C3from :to ?(see Ta\u00adble 1). Second, the source vertex for the shortest-path computation is not Ai.length, \nbut the lower bound, which in Java is the constant vertex O. It is interesting to note in this context \nthat ABCD performs (an implicit) subsumption of bound checks. For example, the upper\u00adbound check A[i.1]is \nredundant with respect to the upper-bound check for A[i]. A equivalent subsumption will be performed \nfor lower bound checks: a lower-bound check for A[i]is redundant with respect to a lower-bound check \nA[i.1]. Although ABCD treats the analysis of upper-and lower-bound checks as two independent problems, \nthese two problems are treated together in the transformation stage of the optimization. Next, we describe \na trick that can merge an upper-and a lower\u00adbound check into a single check instruction. This trick applies \nto arrays that have zero as their lower bound (as in Java). The merged check is performed as an unsigned \ncomparison, thanks to which a negative value of the array index is thus transformed into a large positive \nvalue that is guaranteed to exceed the size of the largest ar\u00adray allowed in a Java virtual machine. \nTherefore, the upper-bound check on the unsigned value is equivalent to performing a (lower\u00adbound) check \nfor a negative value as well as the upper-bound check on the signed value.  7.3 Pointer Aliasing and \nArray SSA Form To convince the reader that ABCD correctly handles pointer alias\u00ading, it is useful to \nhighlight how aliasing in a strongly typed lan\u00adguage like Java differs from that in a language like C. \nWe then explain how ABCD deals with the two kinds of aliases. In Java, local variables cannot be subject \nto pointer aliasing be\u00adcause their address cannot be taken. Furthermore, no statement can change the \nsize of an array referenced by a local Java variable. Consider the program below. x = new int[10]; y=x; \ny = new int[1]; x[2]; passes bounds check The two middle statements do not affect the value of x, and \nhence the array access x[2]remains a valid (in-bounds) reference to the array created in the x = new \nint[10] statement. SSA form correctly accounts for the fact that x is not modi.ed, by placing a def-use \nedge between the de.nition of xand the reference to x[2]. x.f = new int[10]; y= x; y.f = new int[1]; \nx.f[2]; fails bounds check! Now, consider the second example. Because of the y=x copy statement, array \naccess x.f[2] refers to the array created by the y.f = new int[1] statement, and hence the array access \nis out of bounds. Again, SSA form will correctly capture this effect, because the value of x.fwill be \nthe result of a memory load which is assumed to return an unknown array. Therefore, there will be no \nedge between the de.nition of x.fand the use of x.fin the array access, and hence the ABCD algorithm \nwill conclude that x.f[2] refers to an unknown array. In future work, we plan to use Array SSA form [SK98, \nKS98] to perform a more precise def-use analysis in the presence of point\u00aders that will enable the ABCD \nalgorithm to conclude that x.f[2] refers to the array created in the y.f = new int[2]statement.  8 Preliminary \nExperiments We present an initial experimental evaluation of the ABCD al\u00adgorithm. Our experimental results \nwere obtained by using the Jalape no optimizing compiler infrastructure [BCF+99] on a 166MHz PowerPC \n604e processor running AIX v4.3. For these ex\u00adperiments, the Jalape no optimizing compiler performed \na basic set of standard optimizations including copy propagation, type prop\u00adagation, null check elimination, \nconstant folding, devirtualization, local common subexpression elimination, load/store elimination, dead \ncode elimination, and linear-scan register allocation. Previous work [BCF+99] has demonstrated that Jalape \nno performance with these optimizations is roughly equivalent to that of the industry\u00adleading IBM product \nJVM and JIT compiler for the AIX/PowerPC platform. For our experiments, we used .ve Java benchmarks (db, \nmpeg, jack, compress, jess) from the SPECjvm98 suite, seven mi\u00adcrobenchmarks (bubbleSort, biDirBubbleSort, \nQsort, Sieve, Hanoi, Dhrystone, Array) from the Symantec suite [Sym], and three other Java programs (toba, \nbytemark and jolt). For the SPEC codes, we use the medium-size (-s10) inputs. The focus of our measurements \nwas on dynamic counts of bounds check operations. When we re\u00adport timing information, we report the best \nwall-clock time from three runs. Our preliminary implementation has several limitations. We do not use \nany interprocedural summary information, as the Jalape no optimizing compiler assumes on open-world due \nto dynamic class loading. We do not perform any code duplication, such as generation of multiple versions \nof a loop or partitioning a loop it\u00aderation space into safe and unsafe regions [MMS98]. Most impor\u00adtantly, \nthe Jalape no optimizing compiler still lacks many optimiza\u00adtions (e.g., global code motion) that can \nbene.t from removal of array bounds checks. For these reasons, these experimental results should be considered \na lower bound on the potential gains due to ar\u00adray bounds check elimination, and we expect the results \nto improve as Jalape no matures. Figure 6 shows the dynamic number of bounds checks removed by ABCD. \nThe baseline represents all the bounds checks that were analyzed by ABCD; these are upper-bound checks, \nmeasured in dynamic terms. For the .ve SPEC benchmarks, the number is broken down into local checks (those \nfrom the same basic block, and checks whose redundancy required global analysis). This di\u00advision is not \nmade for the remaining programs. Manual exami\u00adnation of the Symantec benchmarks showed that ABCD achieved \nnear-optimal performance, in the sense that checks that were left unoptimized were not optimizable with \nintraprocedural analysis (Hanoi), or requiring a complex pointer analysis (Dhrystone). In static terms, \nthe average number of checks that were found fully redundant was about 31%. Only bytemark had a signi.cant \nnum\u00adber of static checks that were partially redundant (26%). The average number of analysis steps (i.e., \ninvocations of the recursive procedure prove) was less than 10 per analyzed check. This low number con.rms \nthe bene.t of the sparse approach. The time to analyze one bounds check ranged from 0 to 35 milliseconds, \nand averaged around 4 milliseconds. This time does not include the time to construct the e-SSA form. \nWe measured run-time speedup on the Symantec benchmarks. We observed about 10% improvement. The number \nwas lower than we expected, mainly due to the limitations of the infrastructure out\u00adlined above. 0% \n20% 40% 60% 80% 100% bounds checks removed % of upper-bound checks, dynamic count Figure 6: The amount \nof bounds checks removed. The amount shown for each benchmark represents the fraction of upper-bound \nchecks that were removed, measured in terms of dynamic instruc\u00adtion counts. For the benchmarks from SPECjvm98 \n(the top .ve bars), this fraction is divided between local and global checks.   Related Work Elimination \nof Array Bounds Checks. A number of approaches have been taken for performing elimination of array bounds \nchecks. Theorem-proving-style algorithms have been used by Suzuki and Ishihata [SI77], Necula and Lee \n[Nec98, NL98], as well as Xu et al. [XMR00]. Although more powerful than ABCD, theorem proving is expensive \nand therefore unsuitable for a dynamic opti\u00admization setting. Value-range analysis has been used to compute \nbounds on the values of index expressions for the purpose of elim\u00adinating full redundancy [Har77, Pat95, \nRR99]. One of our goals was to handle non-scienti.c programs, which often have complex control .ow. Therefore, \nin addition to full redundancy, it was im\u00adportant for us to handle elimination of partial redundancy. \nSeveral conventional iterative data-.ow-style bounds check elimination al\u00adgorithms have been developed \nthat eliminate partial redundancy [MCM82, Gup90, Gup94, Asu92, KW95]. However, none of these algorithms \nexhibit the ef.ciency of our algorithm, which is based upon demand-driven analysis over a sparse representation. \nWhile all of the above research focuses on imperative programs, work by Xi and Pfenning considers functional \nprograms [XP98]. Midkiff et al. [MMS98] have recently focused on elimination of bounds checks from scienti.c \ncode written in Java. Their work is complementary to ours since our focus is on non-scienti.c code with \ncomplex control .ow. Techniques for eliminating partially redundant conditional branches [BGS97, MW95] \ndetermine through compile-time anal\u00adysis the outcomes of branch conditionals, which are similar in na\u00adture \nto bounds check conditions. However, these algorithms are limited in their power as, unlike our algorithm, \nthey do not in\u00adterpret increments of variables, which is critical for bounds check elimination because \nindex expressions in loops nearly always refer to loop-induction variables. While the algorithm by Bodik \net al. [BGS97] uses demand-driven analysis, it is still quite expensive for use in a dynamic-optimization \nsetting. It does not employ a sparse representation and uses expensive restructuring transformations \nfor elimination of partial redundancy, while we perform hoisting. Chow et al. [CCK+97] have developed \na PRE algorithm that also operates on the SSA graph. However, their algorithm does not employ demand-driven \nanalysis. Our algorithm is also simpler in that it does not require an explicit distinction between speculative \nand nonspeculative redundancy removal. Finally and most impor\u00adtantly, the focus of their work, like many \nother works on redun\u00addancy elimination [KRS92, BGS98], is on PRE of expressions and not array-bounds \nchecks. As we have demonstrated in this paper, elimination of bounds checks has a signi.cantly different \ncharacter since assertions generated from different sources (loop exit con\u00additions, increments of induction \nvariables, etc.) must be analyzed in concert to prove that a bounds check is redundant. Also array\u00adbounds \ncheck elimination exhibits min-max hypergraph behavior. Demand-driven Data-Flow Analysis. Duesterwald \net al. [DGS97] and Horwitz et al. [HRS95, SRH96] have de\u00adveloped demand-driven data .ow analysis frameworks \nfor iterative approaches that only allow lattices of .nite size or height. For the purpose of bounds \ncheck elimination we need to handle in.nite\u00adheight lattices and require elimination-style analysis to \nhandle loops. Bodik et al. present an elimination-style demand-driven analyzer that can handle lattices \nof in.nite height in [BGS98]. However, it operates on a dense program representation (control .ow graph) \nand thus does not satisfy the ef.ciency goals of dynamic optimization. In contrast, the algorithm developed \nin this paper achieves further ef.ciency by performing demand-driven analysis over a sparse program representation. \n Acknowledgements We are indebted to Susan Horwitz, G. Ramalingam, Tom Reps, Mark Wegman, and Zhichen \nXu for discussions and their sugges\u00adtions on the presentation of the paper. We also thank the anony\u00admous \nreferees for their suggestions. Thanks to Tom Reps for the discussions on the relationship of between \nthe inequality graph, hy\u00adpergraphs and grammar problems. References [Asu92] J.M. Asuru. Optimization \nof array subscript range checks. ACM Letters on Programming Languages and Systems, 1(2):109 118, June \n1992. [AWZ88] Bowen Alpern, Mark N. Wegman, and F. Kenneth Zadeck. De\u00adtecting equalities of variables \nin programs. In 15th Annual ACM Symposium on Principles of Programming Languages, pages 1 11, San Diego, \nCalifornia, January 1988. [BCF+99] M.G. Burke, J.-D. Choi, S. Fink, D. Grove, M. Hind, V. Sarkar, M.J. \nSerrano, V.C. Sreedhar, H. Srinivasan, and J. Whaley. The Jalape no Dynamic Optimizing Compiler for Java. \nIn ACM Java Grande Conference, June 1999. [Ber73] Claude Berge. Graphs and Hypergraphs (transl: E. Minieka). \nNorth-Holland, Amsterdam, 1973. [BGS97] Rastislav Bodik, Rajiv Gupta, and Mary Lou Soffa. Inter\u00adprocedural \nconditional branch elimination. In Proceedings of the ACM SIGPLAN 97 Conf. on Prog. Language Design and \nImpl., pages 146 158, June 1997. [BGS98] Rastislav Bodik, Rajiv Gupta, and Mary Lou Soffa. Complete removal \nof redundant expressions. In Proceedings of the ACM SIGPLAN 98 Conference on Programming Language Design \nand Implementation, pages 1 14, June 1998. [BGS99] Rastislav Bodik, Rajiv Gupta, and Mary Lou Soffa. \nLoad-reuse [MCM82] V. Markstein, J. Cocke, and P. Markstein. Optimization of analysis: Design and evaluation. \nIn Proceedings of the ACM range checking. In Proceedings of a Symposium on Compiler SIGPLAN 99 Conference \non Programming Language Design Optimization, pages 114 119, June 1982. and Implementation, May 1999. \n[MMS98] S. P. Midkiff, J. E. Moreira, and M. Snir. Optimizing bounds [CCK+97] F. Chow, S. Chan, R. Kennedy, \nS.-M. Liu, R. Lo, and P. Tu. checking in Java programs. IBM Systems Journal, 37(3):409 A new algorithm \nfor partial redundancy elimination based on SSA form. In Proceedings of the ACM SIGPLAN 97 Conf. on Prog. \nLanguage Design and Impl., pages 273 286, June 1997. [CFR+91] Ron Cytron, Jeanne Ferrante, Barry K. Rosen, \nMark N. Weg\u00adman, and F. Kenneth Zadeck. Ef.ciently computing static single assignment form and the control \ndependence graph. ACM Transactions on Programming Languages and Systems, 13(4):451 490, October 1991. \n[CLR92] T. H. Cormen, C. E. Leiserson, and R. L. Rivest. Introduction to algorithms (Chapter 25.5, pages \n539-543). MIT Press and McGraw-Hill Book Company, 1992. [DGS97] Evelyn Duesterwald, Rajiv Gupta, and \nMary Lou Soffa. A practical framework for demand-driven interprocedural data .ow analysis. ACM Transactions \non Programming Languages and Systems, 19(6):992 1030, November 1997. [GLPN93] G Gallo, G Longo, S Pallottino, \nand Sang Nguyen. Directed hypergraphs and applications. Discrete Applied Mathematics, 42:177 201, 1993. \n[Gup90] Rajiv Gupta. A fresh look at optimizing array bound checking. In Mark Scott Johnson, editor, \nProceedings of the ACM SIG-PLAN 90 Conference on Programming Language Design and Implementation (SIGPLAN \n90), pages 272 282, White Plains, NY, USA, June 1990. ACM Press. [Gup94] R. Gupta. Optimizing array bound \nchecks using .ow analysis. ACM Letters on Programming Languages and Systems, 1( \u00ad4):135 150, March-December \n1994. [GW75] Susan L. Graham and Mark Wegman. A fast and usually lin\u00adear algorithm for global .ow analysis. \nCommunications of the ACM, 18(12):716 716, December 1975. [Har77] W.H. Harrison. Compiler analysis for \nthe value ranges of variables. IEEE Transactions on Software Engineering, SE\u00ad3(3):243 250, May 1977. \n[HH98] Rebecca Hasti and Susan Horwitz. Using static single assign\u00adment form to improve .ow-insensitive \npointer analysis. In Pro\u00adceedings of the ACM SIGPLAN 98 Conference on Program\u00adming Language Design and \nImplementation (PLDI), pages 97 105, Montreal, Canada, 17 19 June 1998. [HRS95] Susan Horwitz, Thomas \nReps, and Mooly Sagiv. Demand In\u00adterprocedural Data.ow Analysis. In Proceedings of the Third ACM SIGSOFT \nSymposium on the Foundations of Software Engineering, pages 104 115, October 1995. [JP93] Richard Johnson \nand Keshav Pingali. Dependence-based pro\u00adgram analysis. Proceedings of the ACM SIGPLAN 93 Con\u00adference \non Programming Language Design and Implementa\u00adtion, pages 78 89, June 1993. [KRS92] Jens Knoop, Oliver \nR\u00a8uthing, and Bernhard Steffen. Lazy code motion. SIGPLAN Notices, 27(7):224 234, July 1992. Pro\u00adceedings \nof the ACM SIGPLAN 92 Conference on Program\u00adming Language Design and Implementation. [KS98] Kathleen \nKnobe and Vivek Sarkar. Array SSA form and its use in parallelization. In Conference Record of POPL 98: \nThe 25th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, pages 107 120, San Diego, \nCali\u00adfornia, 19 21 January 1998. [KW95] Priyadarshan Kolte and Michael Wolfe. Elimination of redun\u00addant \narray subscript range checks. ACM SIGPLAN Notices, 30(6):270 278, June 1995. Proceedings of the 1995 \nACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI).  453, August 1998. \n[MR79] E. Morel and C. Renviose. Global optimization by supression of partial redundancies. CACM, 22(2):96 \n103, 1979. [MW95] Frank Mueller and David B. Whalley. Avoiding conditional branches by code replication. \nIn ACM SIGPLAN Conference on Programming Language Design and Implementation, vol\u00ad ume 30 of ACM SIGPLAN \nNotices, pages 56 66. ACM SIG- PLAN, ACM Press, June 1995. [Nec98] George C. Necula. Compiling with Proofs. \nPhD thesis, Carnegie Mellon University, October 1998. Available as Tech\u00ad nical Report CMU-CS-98-154. \n[NL98] G. C. Necula and P. Lee. The design and implementation of a certifying compiler. In Proceedings \nof the 1998 ACM SIG- PLAN Conference on Prgramming Language Design and Im\u00ad plementation (PLDI), pages \n333 344, 1998. [Pat95] Jason R.C. Patterson. Accurate static branch prediction by value range propagation. \npages 67 78, June 1995. Proceed\u00ad ings of the 1995 ACM SIGPLAN Conference on Programming Language Design \nand Implementation (PLDI). [Pra77] V.R. Pratt. Two easy theories whose combinantion is hard. Technical \nreport, Massachusetts Institute of Technology, 1977. [Ram96] G. Ramalingam. Bounded incremental computation, \nvolume 1089 of Lecture Notes in Computer Science. Springer-Verlag Inc., New York, NY, USA, 1996. [RR96] \nG. Ramalingam and Thomas Reps. An incremental algorithm for a generalization of the shortest-path problem. \nJournal of Algorithms, 21(2):267 305, September 1996. [RR99] Radu Rugina and Martin Rinard. Automatic \nparallelization of divide and conquer algorithms. In Proceedings of the ACM SIGPLAN 1999 Symposium on \nPrinciples and Practice of Par\u00ad allel Programming, Atlanta, GA, May 1999. ACM SIGPLAN. [Sho81] Robert \nShostak. Deciding Linear Inequalities by Computing Loop Residues. Journal of the Association for Computing \nMa\u00ad chinery, 28(4), 1981. [SI77] Norihisa Suzuki and Kiyoshi Ishihata. Implementation of an array bound \nchecker. In Conference Record of the Fourth ACM Symposium on Principles of Programming Languages, pages \n132 143, Los Angeles, California, January 17 19, 1977. ACM SIGACT-SIGPLAN. [SK98] V. Sarkar and K. Knobe. \nEnabling sparse constant propaga\u00ad tion of array elements via array SSA form. Lecture Notes in Computer \nScience, 1503:33 40, 1998. [SRH96] Mooly Sagiv, Thomas Reps, and Susan Horwitz. Precise in\u00ad terprocedural \ndata.ow analysis with applications to constant propagation. Theoretical Computer Science, 167(1 2):131 \n170, 1996. [Sym] Just-In-Time Compilation. http://www.symantec.com/ cafe/analysis1.html#jitcomp. [XMR00] \nZhichen Xu, Barton Miller, and Thomas Reps. Safety checking of machine code. In Proceedings of the ACM \nSIGPLAN 00 Conf. on Progr. Language Design and Implementation, page to appear, jun 2000. [XP98] Hongwei \nXi and Frank Pfenning. Eliminating array bound checking through dependent types. ACM SIGPLAN Notices, \n33(5):249 257, May 1998. Proceedings of the 1998 ACM SIGPLAN Conference on Programming Language Design \nand Implementation (PLDI).  \n\t\t\t", "proc_id": "349299", "abstract": "<p>To guarantee typesafe execution, Java and other strongly typed languages require bounds checking of array accesses. Because array-bounds checks may raise exceptions, they block code motion of instructions with side effects, thus preventing many useful code optimizations, such as partial redundancy elimination or instruction scheduling of memory operations. Furthermore, because it is not expressible at bytecode level, the elimination of bounds checks can only be performed at <italic>run time</italic>, after the bytecode program is loaded. Using existing powerful bounds-check optimizers at run time is not feasible, however, because they are too heavyweight for the dynamic compilation setting.</p><p>ABCD is a light-weight algorithm for elimination of Array Bounds Checks on Demand.  Its design emphasizes simplicity and efficiency. In essence, ABCD works by adding a few edges to the SSA value graph and performing a simple traversal of the graph. Despite its simplicity, ABCD is surprisingly powerful. On our benchmarks, ABCD removes on average 45% of dynamic bound check instructions, sometimes achieving near-ideal optimization. </p><p>The efficiency of ABCD stems from two factors. First, ABCD works ona <italic>sparse</italic> representation. As a result, it requires on average fewer than 10 simple analysis steps per bounds check.  Second, ABCD is <italic>demand-driven</italic>. It can be applied to a set of frequently executed (hot) bounds checks, which makes it suitable for the dynamic-compilation setting, in which compile-time cost is constrained but hot  statements are known.</p>", "authors": [{"name": "Rastislav Bod&#237;k", "author_profile_id": "81100033082", "affiliation": "University of Wisconsin", "person_id": "P239460", "email_address": "", "orcid_id": ""}, {"name": "Rajiv Gupta", "author_profile_id": "81100027751", "affiliation": "University of Arizona", "person_id": "PP39072777", "email_address": "", "orcid_id": ""}, {"name": "Vivek Sarkar", "author_profile_id": "81100597290", "affiliation": "IBM T.J. Watson Research Center", "person_id": "PP14206121", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/349299.349342", "year": "2000", "article_id": "349342", "conference": "PLDI", "title": "ABCD: eliminating array bounds checks on demand", "url": "http://dl.acm.org/citation.cfm?id=349342"}