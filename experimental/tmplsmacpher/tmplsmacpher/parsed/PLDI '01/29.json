{"article_publication_date": "05-01-2001", "fulltext": "\n Facile: A Language and Compiler For High-Performance Processor Simulators 1 Eric C. Schnarr Mark D. \nHill James R. Larus QUIQ Incorporated University of Wisconsin--Madison Microsoft Research 25 Kessel Court, \nSuite 201 1210 West Dayton Street One Microsoft Way Madison, WI 53711 Madison, Wl 53706 Redmond, WA 98052 \nschnarr@quiq.com markhill @ cs.wisc.edu larus @ microsoft.com ABSTRACT Architectural simulators are \nessential tools for computer architec- ture and systems research and development. Simulators, however, \nare becoming frustratingly slow, because they must now model increasingly complex micro-architectures \nrunning realistic work- loads. Previously, we developed a technique called fast-forward- ing, which applied \npartial evaluation and memoization to improve the performance of detailed architectural simulations by \nas much as an order of magnitude [14]. While writing a detailed processor simulator is difficult, imple- \nmenting fast-forwarding is even more complex. This paper describes Facile, a domain-specific language \nfor writing detailed, accurate micro-architecture simulators. Architectural descriptions written in Facile \ncan be compiled, using partial evaluation tech- niques, into fast-forwarding simulators that achieve \nsignificant performance improvements with far less programmer effort. Facile and its compiler make this \nperformance-enhancing technique accessible to computer architects. Categories and Subject Descriptors \nD.3.2 [Programming Languages]: Language Classifications-- specialized application languages; D,3.4 [Programming \nLan- guages]: Processors--optimization, compilers; D.3.3 [Program- ming Languages]: Language Constructs \nand Features--constraints, frameworks; 1.6.2 [Simulation and Modeling] Simu-lation Languages. General \nTerms Algorithms, Languages, Performance. Keywords Micro-architecture simulation, out-of-order processor \nsimulation, memoization, and partial evaluation. Permission to make digital or hard copies of all or \npart of this work for personal or classroom use is granted without fee provided that copies are not made \nor distributed for profit or commercial advan- tage end that copies bear this notice and the full citation \non the first page, To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior specific permission and/or a fee. PLDI 2001 6/01 Snowbird, Utah, USA &#38;#169; 2001 ACM ISBN 1-58113-414-2/01/06...$5.00 \n 1. INTRODUCTION Detailed simulation is an essential tool for computer architecture and systems research \nand development. As computers, in particu- lar processor micro-architectures, have become increasingly \ncom- plex, so have their simulators. While this complexity has greatly improved processor performance, \nit has had the opposite effect on simulator speed. Complex simulators run slowly, which impairs their \nusefulness in evaluating processor implementations or devel- oping software. Moreover, architecture simulators \nare complex pieces of software that are difficult to write, debug, and validate. Facile---our domain-specific \nlanguage for writing detailed proces- sor architecture simulators--attacks both problems, with a goal \nof making efficient simulators more accessible to computer archi- tects. We previously showed that programming \nlanguage imple- mentation techniques--partial evaluation and memoization--could increase simulator performance \nby up to an order of magni- tude [14]. This approach, called fast-forwarding, was very difficult to implement \nby hand, which led to this research on the Facile lan- guage for describing instruction eneodings, instruction \nsemantics, and micro-architectural details and on the Facile compiler for translating a processor description \ninto an efficient simulator. The compiler uses partial evaluation analyses to translate a simulator written \nin Facile into a fast-forwarding simulator, which uses run- time partial evaluation to greatly improve \nperformance [13]. Instruction-level simulation focuses on modeling instrnctions' effect on user-visible \nstate; for example, that an add instruction puts the sum of two registers into a third. These simulators \ntypi- cally are interpreters, written in a procedural language such as C, that read instructions from \na target executable and execute their functional behavior. Many approaches have been tried to acceler- \nate this procesS. Some simulators pre-decode target instructions, to make interpretation easier, e.g., \nSPIM [6] and SimlCS [10]. Others compile target instructions into native host instructions and then directly \nexecute them, e.g., FX!32 [2] and Shade [3]. Architectural or micro-architectural simulation, by contrast, \nsimu- lates the behavior of a processor implementation by modeling instructions' effects on processor \nstructures. For example, the reg- isters used by the add instruction might be dynamically renamed and \nthe add operation dynamically scheduled on one of several This work was largely performed as part of \nEric Schnarr's Wisconsin Ph.D. [13]. It was supported in part by the National Science Foundation with \ngrants M1P-9625558 and EIA-9971256. 321 ALUs. Modem, out-of-order processors are built on complex micro-architectures, \nwhose simulation is very expensive. SimpleS- catar, a fast, popular out-of-order simulator, incurs approximately \na 4,000 times slowdown [1 ]. RSiM, an out-of-order multiprocessor simulator, executes fewer than 15,000 \ninstructions per second on a SUN Ultra 1/140 workstation, which is approximately a 10,000 times slowdown \nper processor [I 1 ]. MXS, the SimOS out-of-order simulator, also incurs a several thousand times slowdown \nper pro- cessor [7]. Techniques that enhance functional simulators offer little benefit for architectural \nsimulators, most of whose time is spent simulat- ing an out-of-order pipeline, not interpreting instructions. \nFortu- nately, simulators, like compilers and computer architectures, can exploit program locality to \nimprove perfomaance. Even detailed out-of-order simulators spend most of their time modeling a small \nnumber of instruction sequences. Run-time partial evaluation and memoization permit the rapid re-simulation \nof these sequences. This approach is the basis of our fast-forwarding technique. Writing fast-forwarding \nsimulators is difficult. A programmer must not only understand and correctly model a proeessor's micro- \narchitecture, but he or she must also distinguish run-time static and dynamic data; implement two, tightly-coupled \nsimulation engines; and ensure that both appropriately model simulator state. Facile is a new domain-specific \nprogramming language for writing fast-forwarding simulators. It is designed to facilitate the specifica- \ntion and implementation of detailed architecture simulators and to simplify the compiler analyses necessary \nto automatically produce a fast-forwarding simulator. For example, a simulator for the SPARC V9 instruction \nset running on a MIPS R10000-1ike out-of- order processor requires less than 2,000 lines of Facile and \nanother 1,000 lines of C code. The Facile compiler uses a new approach to the classic problem of partial \nevaluating an interpreter for a given input program [8]. Unlike much partial evaluation work, fast-forwarding \noccurs at run time. In many respects, Faerie is similar to run-time specialization and code generation \nsystems, though Faeile's domain-specific lan- guage and limited application domain permits a very fine-grain \nand aggressive translation that results in large performance gains. The rest of this paper is organized \nas follows. Section 2 describes how memoizailon works to speed miero-arehitecture simulation. Section \n3 introduces Facile, our new programming language, and discusses how it supports fast-forwarding simulators. \nSection 4 describes how Facile's compiler analyzes and optimizes simula- tors. Section 5 discusses related \nwork. Section 6 presents the per- formance of a hand-coded out-of-order simulator and an out-of- order \nsimulator optimized by the Facile compiler. 2. FAST-FORWARDING Fast-forwarding is an application of partial \nevaluation and memoization to micro-architecture simulation. This basic execu- tion model was described \npreviously [14]. Fast-forwarding improved the performance of a detailed out-of-order processor simulator \n5-12 times over the same simulator without memoiza- tion. Below is a brief description of this technique, \nwhich provides the run-time framework for this work. / / miss ] Figure 1, Architecture of a fast-forwarding \nsimulator. 2.1. Overview The basic idea is to capture a simulator's computation of an instruction's \neffects on processor micro-architecture in a form that can be rapidly re-executed. For example, an out-of-order \nsimulator might record timing and functional effects of one processor cycle, indexed by the collection \nof instructions in execution at the start of the cycle. This example is explored in Section 2.2. Recorded \neffects can be quickly re-executed without interpretive overhead or the need to re-compute, as opposed \nto simply re-applying, instruc- tions' effects on processor state. Most simulators can be modeled as \na simulator step function. Each call to this function advances the simulation by a single step. A simulator's \nauthor determines the amount of calculation performed in a step, which can range from simulating one \nprocessor cycle to simulating several cycles and instructions. The simulator step function's implementation \ndetermines the granularity of memoiza- tion, since calls on this function are memoized. If one instruction \nis simulated per step, the behavior of individual instructions can be looked up and replayed. If several \ninstructions are simulated per step, their combined behavior ean be replayed when the same group of instructions \nis encountered. Consider a simulator step function f This function takes as input the state of the simulated \nmicro-architecture and returns a new state that reflects the execution of one simulation step. To fast-for- \nward f, part of its input (i.e., a subset of the micro-architecture state) is designated run-time static \n(constant) and used to index a cache of specialized actions: f: (Sin \u00d7 Din) ~ Dou t As shown above,f's \ninput is divided into run-time static input (Sin) and dynamic input (Din) . Run-time static input typically \nincludes the binary insmactions being simulated and a subset of the proces- sor pipeline state. Dynamic \ninputs usually include values in simu- lated registers, addresses resident in a simulated data cache, \netc. All code infthat depends only on Sin is also run-time smile, so that in any two calls tofthat are \npassed the same values for Sin,f's run- time static code produces the same result. Only code that depends \non dynamic clam (Din) can produce a different result. Action Key (run-time static inputs) cache\" \\ 't \nent f  M,,,io0 ~ ~ control-, paths Dynamic actions Figure 2. The specialized action cache. When a \nfast-forwarding simulator encounters a particular run-time static input (Sin) for the first time, it \nboth executes fto effect the simulation and uses partial evaluation to specializefwith respect to Sin. \nThe specialized function, fsin : Din ~ Dout, is stored into a specialized action cache, indexed by Sin. \nSubsequent calls to f, with input Sin , execute the less expensive functionfsin instead off. Figure 1 \ndepicts the architecture of a fast-forwarding simulator. At the top is the slow/complete simulator, which \nconsists of the un- specialized simulator stop function. As the slow simulator exe- cutes, it writes \na description of its dynamic behavior--the partially evaluated function fSin--into the specialized action \neaehe. When the step function encounters a previously seen run-time static input, the dynamic behavior \nis found and replayed by a fast/resid- ual simulator. A replayed simulation can only re-execute a previously \nrecorded control flow path. While replaying, the fast simulator verifies that the control flow path is \nknown and re-executes actions associated along the path. If the path is not cached, an action cache miss \nreturns control to the eornpleto simulator. This process is a dynamic version of the classic partial \nevaluation of an interpreter for an input program [8]. An important difference is that fast-forwarding \nis a run-time process. Another difference is that traditional partial evaluation systems generate residual \ncode for an entire function, which is not feasible because simulator stop functions are large and must \nbe specialized for many different inputs. Fast-forwarding addresses these concerns by specializing only \ncontrol flow paths executed by the slow simulator and by spe- cializing these paths on demand. Because \nof the high degree of locality in program execution, the specialized action cache remains a manageable \nsize and actions are heavily reused [14]. Figure 2 illustrates the structure of the specialized action \ncache. It contains cache entries, which consist of index keys--the step func- tion's run-time static \ninput--and the simulator's residual dynamic actions. Actions are linked together in the order in which \nthey exe- cute. Actions that select control-flow paths, by examining dynamic values, have multiple successors. \nThe simulator records its actions when it executes a control flow path, so successors of an action may \nbe missing. For example, a branch that was previously always taken will have no recorded behavior for \nthe fall-through case. When the fast simulator encoun- ters a missing action, it incurs an action cache \nmiss and returns to slow simulation. To recover from the miss, this simulator rolls back the failed dynamic \nstep function, reads its static input from the cache entry's index key, and restarts the slow simulator \nstep function. This simulator executes the new control flow path, recording its actions, so that the \nnew control flow path can be replayed in the future. Recovery from a miss is complicated. Fast simulation \nmay already have moved beyond the beginning of the simulator's step function (i.e., the location of a \nkey in the specialized action cache), and modified dynamic simulator state, before encountering the miss. \nIntuitively, the dynamic state should be rolled back to the begin- ning of the step function call, but \nthis is not always possible. Instead of rolling the fast simulation back, the slow simulator pro- ceeds \ncautiously and does not execute dynamic actions until it reaches the cache miss. Until then, the slow \nsimulator only per- forms run-time static operations and stores no new data into the specialized action \ncache. ARer reaching the cache miss, the slow simulator returns to normal execution, executing both run-time \nstatic and dynamic code and adding new actions to the cache. 2.2. Example: Out-Of-Order Simulation Fast-forwarding \nis very effective at accelerating the simulation of a modem, out-of-order micro-architecture. One implementation \nuses a simulator step function that takes the current state of the out-of- order pipeline as its run-time \nstatic input and simulates the miere- architecture until the end of a processor cycle that performs some \ndynamic behavior. This approach allows the fast simulator to skip over most cycle-by-cycle pipeline simulation, \nreplaying only func- tional instruction behavior and other non-pipeline simulation such as branch prediction \nand cache simulation. Our simulator encodes the run-time static state of an out-of-order pipeline in \na data structure called the instruction queue. The instruction queue does not correspond to any micro-architecture \ncomponent. It is simply a convenient data structure for represent- ing run-time static data. The instruction \nqueue is a list of instruc- tions, in program order, currently in execution. A small amount of additional \ninformation specifies the pipeline stage and latency of each instruction, e.g., fetched, waiting in out-of-order \nqueue, exe- cuting, etc. The left side of Figure 3 shows an example of data in the instruc- tion queue. \nThe first 11 instructions are in the instruction queue. Over the next 6 simulated cycles, only run-time \nstatic behavior occurs: the first instruction (\u00a2:l.r) is retired, 9 more instructions are fetched, several \ninstructions use the simulated ALUs 2, and the load Cl.d) instruction at address 0x10078 counts down \nto the next time it needs to make a dynamic call to the cache simulator. The right side of Figure 3 shows \ndata in the specialized action cache for this pipeline state. An entry's key consists of a com- pressed \nrepresentation of the instruction queue. The instruction queue in this example can be compressed into \nfewer than 40 bytes, since its contents can be reconstructed from the underlined values. The first action \nincrements the simulated cycles by 6, for the 6 In this example, the functional simulation of instructions \nis dynamic, but separate from micro-architectural timing simulation: Instruetiuns are first interpreted \nfor their functional behavior, then their pipeline timing is simulated. 323 Addn ~0074 0x10078 0xl007c \n0xlO080 0xt0084 0x10088 OxlO08c 0x10098 0x1009e OxlOOa0 Ox100a4 0x3f378 0x3f380 0x3f384 0x3f388 0x56ce8 \n0x56cee 0x56cf0 Ox56ef4 Ox56cf8 Out-Of-Order Pipeline State Instruction elf %\u00a3p Xd [ %Sp + 0x40 ], %10 \nadd %sp, 0x44, %11 su3~ %sp, 0x20, %sp tst %gl be 0xi0098 SOy %gl, %00 sethi %hi (0xSb000) , %o0 or \n%o0, 0x148, %00 aa11 0x3 f378 nop save %sp, -96, %sp sethl %hi(0x75c00), %00 call 0x56ce8 or %o0, 0x2e0, \n%00 save %sp, -96, %sp seth\u00a3 %hi(0x77000) , %gl ld [ %gl + 0xl7c ], %gl call %gl restore  Stage done \n~kEhg 92~9. Latency I Previous Action ) Action Cache Entry Queue ggg/i~ queue fetch fetch fetch / Memory \n Figure 3. An out-of-order pipeline state with SPARC instructions and resulting specialized action cache \nentries. cycles of an-time static execution. The next action calls the cache simulator for the load at \naddress Ox 10078. The cache simulator has multiple possible results corresponding to a cache hit or miss. \nIn this example, the load previously missed in the cache and waited 18 cycles before checking if the \nvalue was returned from the next level of the cache hierarchy. Subsequent fast simulation can replay \nthese actions, so long as the load again misses in the cache with an 18-cycle delay. If during replay, \nthe load hits in the cache (or misses with a different delay), then it is an action cache miss and control \nreturns to the slow simulator.  3. FACILE LANGUAGE As the previous discussion probably made clear, \nwriting fast-for- warding simulators is difficult A programmer must not only understand and correctly \nmodel a processor's micro-architecture, but he or she must also distinguish between run-time static and \ndynamic data; implement two, tightly-coupled simulation engines; ensure that simulator state is appropriately \nmodeled by each; and transfer control and state between them. Facile is a new domain-specific programming \nlanguage for writing fast-forwarding micro-architecture simulators. It is designed to facilitate the \nspecification and implementation of detailed architec- ture simulators and to simplify the compiler analyses \nnecessary to automatically generate a fast-forwarding simulator. Section3.1 diseusses Faeile's syntax \nfor describing machine instructions and illustrates its use in a simple example. Section 3.2 discusses \nFaeile's support for partial evaluation and memoization. This support includes language restrictions \nto simplify compiler analyses and a framework for writing simulators as step functions. 3.1. Architecture \nDescription Facile provides a concise syntax for describing a computer's archi- tecture and instruction \nset. This syntax is partially derived from the New Jersey Machine Code Toolkit's description language \n[12]. This language was the starting point for Facile's instruction description syntax, because its descriptions \nare concise, designed to reduce programmer error, and flexible enough to describe instruction sets ranging \nfrom RISC to Intel x86. Facile extends this work to include instruction semantics, as well as syntax. \nFaerie describes binary encoding of instructions using patterns on streams of fixed width tokens. Figure \n4 illustrates a Facile token, field, and pattern declarations. These declarations describe the encoding \nof two instructions, add and bz (branch if zero), of a fie- titious RISC ISA. A token declaration defines \none fixed width token instruction[32] fields op 24:31, rl 19:23, r2 14:18, r3 0:4, i 13:13, imm 0:12, \noffset 0:18, fill 5:12; pat add = op==0x00 &#38;a (i==l I[ fill==0); pat bz = op==0x01;  Figure 4. \nInstruction encoding in Facile. val PC : stream; val nPC : stream; val R = array(32){0}; sem add { \nif(i) R[rl] = R[r2] + imm?sext(32); else R[rl] = R[r2] + R[r3]; }; sem bz { if(R[rl]==0) nPC = PC + \noffset?sext(32); };  FigureS. lnstructionsemanticdescription. token and names several bit fields \ncontained within that token. The op field in Figure 4 is contained in bits 24 to 31 of a 32-bit wide \ntoken named instruction. For fixed width instruction sets, such as SPARC and MIPS, one token suffices \nto encode each instruction. For variable width instructions, such as Intel's x86, several tokens may \nbe necessary. Pattern declarations associate mnemonic names with conditions on token fields and use these \nconstraints to describe instructions. In Figure 4, the add and bz pattern names are associated with values \n0x00 and 0x01 respectively in an instruction's op field. The add instruction has additional conditions: \nEither the immediate flag field (i) must be I or bits 5 through 12 (the f i 11 field) must be 0. A token \nstream is a sequence of tokens starting at an address in the text segment of a simulated executable. \nThe instruction at the front of a token stream is decoded by matching the binary data at that location \nagainst a pattem. Unlike the NJ Machine Code Toolkit, Facile describes instruction semantics as well \nas instruction encoding. Figure 5 shows the semantics of the add and bz instructions from Figure 4. First, \nglo- bal variables PC, nPC, and R are declared. PC and npC are token stream variables that represent \nthe current program counter and the program counter of the next instruction, respectively. R is an array \nof integers that holds the contents of the integer register file. Next, semantic declarations associate \nfunctional simulation code with the pattern names add and bz. Instruction add tests the value of the \ni field: if i is 1, then it adds an integer register to the instruction's sign extended immediale field \n(imm). Otherwise, it adds two integer registers. The added result is written back to the integer register \nfile. Instruction bz tests if an integer register is 0, and alters the next instruction address if true. \nThe semantic code associated with each instruction can perform micro-architecture simulation and other \ntasks in addition to behav- ioral simulation. The Facile language provides functions, loops, arrays, \nand other general programming constructs. These program- ming features were included in Facile because \nno single, special- purpose micro-architectt.re declarative syntax seemed likely to be able to model \nall possible micro-arehitecture structures. Given encoding and semantic functions, the Facile compiler \nauto- matically generates a function that decodes a binary instruction, dispatches to the appropriate \nsimulation code, and simulates func- val init = system?start pc; fun main(pc) { PC = pc; // set global \nPC variable nPC = PC + 4; // default value for npc PC?exec(); // simulate 1 instruction init = nPC; // \nprepare next iteration )  Figu~ 6. Simplifiedsimulatorstep function. tional and architectural behavior. \nThis function is called from a user-defined simulator step function.   3.2. Language Support For Fast-Forwarding \nFaeile's support for fast-forwarding starts with its organization of simulator code into a step function. \nFacile programmers define a function named main to perform one step in the simulation. This step can \nsimulate one or more processor cycles, one or more instructions, or any other appropriate simulation \nquantum. Faeile's run-time system repeatedly calls main to advance the simulation. Calls to main are \nmemoized. The main function receives two kinds of input: run-time static input is passed as arguments \nto main. Dynamic input comes from other sources, such as global variables or from external code not written \nin Facile. The arguments to main are the run-time static keys into the specialized action cache. The \nprogrammer defines the number and type of main's arguments and controls exactly what data values are \npassed to them. Figure 6 shows a trivial simulator step function that simulates one target instruction \nper call. A global variable (init) stores argu- ment values for the next call to main. At the start of \nsimulation, init is set to the target program's entry point (start_pc). This main function has one ran-time \nstatic input (pc), whose initial value comes from init. The body of main copies pc into global variable \nPC for use by the simulation code, predicts the value of nPC (branch instructions may reset it), and \nsimulates one target instruction. The last statement in main updates init with the pc for the next call \nto main. Facile has several other features that support writing fast-forward- ing simulators. These include \nbuilt-in data structures and functions useful for micro-architecture simulation, and language restrictions \nthat simplify compiler analysis. The built-in data types include token streams, condition code values \n(with functions for comput- ing them), and double-ended queues useful for modeling micro- architectures. \nBy including these functions and data types into the language, their semantics are known, so a compiler \ncan analyze and transform code that uses them. One built-in support function is the ?exec ( ) attribute \nused in Figure 6. The ?exec ( ) attribute is applied to a token stream (PC) and uses the declared tokens, \npat- terns, and instruction semantics to decode and simulate one instruction. A key language restriction \nis the absence of pointers, which increases the precision of the analysis that determines which parts \nof a simulator step function can be skipped by fast-forwarding. Another restriction is the absence of \nrecursion. Non-recursive code simplifies inter-procedural analyses. It also simplifies the mecha- nism \nfor recovering from action cache misses, by allowing 325 fun main(pc) { val npc = pc + 4; switch(pc) \n{ pat add: if(i) N[rl] = R[r2i__~ imm?sext(32); else R[rl] = R[r2] + R[r3]; pat beq: if(R[rl] == \n0) npc : pc + offset?sext(32); } init = npc; ) Figure7. Run-~mestatleand dynamiccode(underlined). \ndynamic data to be passed from the fast simulator to the slow sim- ulator in global variables, not a \nstack. Facile is sufficiently powerful to model a real instruction set and complex processor implementation. \nIt has been used to describe the SPARC-V9 instruction set and a detailed, out-of-order pipeline with \nnon-blocking data caches, branch prediction, and speculative execution. Code that is inconvenient in \nFacile, or that need not be memoized, can be implemented in other languages, such as C. Fac- ile provides \nan interface for calling external code, and memoizes Facile code correctly despite calls on external \nfunctions. 4. COMPILATION AND OPTIMIZATION The Facile compiler generates an optimized fast-forwarding \nsimu- lator. The compiler actually generates C code for two simulators-- slow and fast--and generates \ncode to coordinate their communica- tion through the specialized action cache. This process relies on \nbinding-time analyses to identify run-time static code and dynamic basic blocks. The analyses and code \ngeneration are described below. 4.1. Binding-Time Analysis Binding-time analysis is a technique widely \nused in partial evalua- tion to partition a function's code into static and dynamic parts, starting from \nan initial division of a function's input In Facile, binding-time analysis divides a simulator step function \ninto run- time static and dynamic parts. The run-time static code is a func- tion of the run-time static \ninput, so its computation can be memoized and skipped by fast-forwarding. The remaining dynamic code, \non the other hand, must be executed every time. Figure 7 shows the division of a Facile main function \ninto run- time static and dynamic parts. Dynamic code is underlined. All other code is run-time static. \nNote that this function performs the same actions as the main function in Figure 6, except that instruc- \ntion semantic code is written as a pattern switch statement for greater clarity. Binding-time analysis \nperforms an abstract interpretation of the code, computing two binding time labels--rt-static and dynamic--instead \nof actual data values. At the beginning of this interpreta- tion, the arguments to main (pc) and literal \nvalues (e.g., 4) are labeled rt-static and all global variables are labeled dynamic. The basic step is \nto label rt-static the result of expressions whose oper- ands are all rt-static. For example, the switch \nstatement is rt-static because it depends only on the rt-static values of pc and the target executable's \ntext segment 3. Variables assigned a run-time static value are labeled rt-static until they are re-assigned. \nHence, npc is rt-static because it depends only on the rt-static values pc and 4. Code is labeled dynamic \nwhen it depends on dynamic data. In the example, the statement adding a register to an immediate, the \nstate- ment adding a register to another register, and the condition testing a register against the value \n0 are dynamic. Sub-expressions (not underlined) in these statements that are rt-static (e.g., register \nindi- ces and the added imrnediate value) are skipped by fast-forward- ing, even though the statements \ncontaining them are not. Other complications are not exposed by this example, such as assignment to global \nvariables. In Figure 7, assignment to 2nit is labeled dynamic because its value is needed after main \nreturns. In other cases, a global variable is assigned a rt-static value and used within the body of \nmain. In these cases, the analysis labels the global variable as rt-static from the point at which it \nis assigned the value until it is assigned a dynamic value or ma in returns. Another complication is \nensuring the termination of binding-time analysis. Abstract interpretation may re-evaluate a basic block \nof the subject program each time the binding times of variables are changed by one or more of the block's \npredecessors. In the pres- ence of loops, basic blocks can be re-evaluated several times. Facile's binding \ntime analysis is guaranteed to reach a fixed-point and terminate eventually. This is because the binding \ntimes of vari- ables computed by a basic block's predecessors are merged on entry to the block, a block \nis re-evaluated only if its merged bind- ing time data changes, and merged binding times can only change \na finite number of times: 1) There are a finite number of different binding time labels (n-static and \ndynamic). 2) There are a finite number of variables in any given program. 3) When binding time data is \nmerged, the binding time of each variable never decreases--e.g., if a variable is n-static from one predecessor \nand dynamic from another, then it is dynamic after the merge. Inter-procedural binding-time analysis \nis another complication, although it is simplified by the absence of recursion. Facile uses a polyvariant \n(context-sensitive) binding time analysis. This means that each function can have several divisions corresponding \nto dif- ferent labellings of its input--i.e., both its arguments and the glo- bal variables. Polyvafiant \ndivision improves the accuracy of the analysis, hence the amount of code that can be skipped. However, \npolyvariant division also increases code size, since different ver- sions of a function are generated \nfor each of its divisions. Facile chooses greater accuracy and the ability to skip more simulation work \nover reduced simulator code size. 4.2. Extracting Actions The next step in producing a fast-forwarding \nsimulator is to orga- nize the dynamic code identified by binding-time analysis into basic blocks. These \nbasic blocks constitute the actions used in the specialized action cache. By replaying dynamic basic \nblocks in the same order as the slow simulator executed them, the fast simulator replays the memoized \ndynamic behavior. 3. Facile assumes target instructions do not change after they are loaded at the start \nof simulation. Target instructions are considered run-time static. Figure 8. Dynamic control flow graph. \n To find dynamic basic blocks, the compiler first constructs a dynamic control flow graph (dCFG). This \nrepresentation starts with a control flow graph of the entire program, and then removes all rt-static \nstatements. A dynamic basic block is a basic block in the resulting graph. Figure 8 shows the dCFG for \nthe simulator in Figure 7. In this example, each dynamic statement has its own block. In a richer simulator, \na basic block would contain multiple statements. These basic blocks contain only dynamic code; the run-time \nsmile sub- expressions are replaced by the plaeeholder s. After identifying these blocks, each one is \nassigned an action num- ber. In Figure 8, blocks are numbered 1 through 4. The specialized action cache \nstores these action numbers, not code. The fast simu- lator replays cached behavior by reading an action \nnumber and jumping to the corresponding code. The specialized aeilon cache also holds data for run-time \nstatic placeholders (the s's). When the fast simulator executes a dynamic basic block corresponding to \nan action, it also reads placeholder data from the specialized action cache and passes it to the dynamic \ncode. Plaeeholder data for each action number is stored immediately after its associated action number. \nNotice that block b 3 differs from the others. This block contains a condition expression in an if statement. \nThe result of this dynamic expression determines the simulator's control flow path, so this action can \nhave more than one successor action sequence. To han- dle this situation, the slow simulator records, \nin the cache, the predicate value along with the resulting action sequence. The fast simulator verifies \nthat the dynamic value matches a previously recorded value. If not, the action cache misses. This process \nis a dynamic result test. 4.3. Slow and Fast Simulators After binding-time analysis and assigning action \nnumbers, the compiler has enough information to generate fast and slow ver- sions of the simulator. The \nslow simulator contains the complete simulator code and additional compiler-generated code to write action \nnumbers and placeholder data into the specialized action cache. The fast simulator contains only dynamic \nsimulator code and code to read action nnmbers and data from the cache. fun fast_main() ( while(true) \n{ switch(get_next_action number()) { case INDEXACTION: verify_static_input(); case i: read static \ndata(rl, r2, tl); R[rl] = R[r2] + tl; case 2: read static data(rl, r2, r3); R[rl] = R[r2] + R[r3]; \n case 3: read static data(rl); val t2 = (R[rl] == 0); verify_dynamic_result(t2); case 4: read static_data(npc); \n init = npc; ) }  } Figure 9. Fas~sidual simulator (pseudo codA. Figure 9 shows pseudocode for the \nfast simulator corresponding to the simulator in Figure 7. This pseudocode uses Facile-like syntax for \nsimplicity, although the Facile compiler actually generates C code. The fast simulator consists of a \nloop surrounding a switch statement. It reads action numbers from the specialized action cache and jumps \nto the corresponding dynamie basic block code. Each switch case, except the INDEX_ACTION ease, corresponds \nto a dynamic basic block. Code in each ease reads run-time smile data from the eache and executes the \ndynamic basic block code. The INDEX_ACTION case handles the end of an action sequence from one simulator \nstep and the beginning of the next step by veri- fying that the current value of init matches the next \nkey in the specialized action cache. Alternatively, the fast simulator could do a full eacbe lookup to \nfind the next cache entry, but it is faster to follow the link to the next entry. Cases I, 2, and 4 read \nrun-time static data from the specialized action cache and execute their dynamic code. Case 3 contains \na dynamic result test, verify dynamic result(t2), that searches the current action's possible successors \nto find an action sequence appropriate for the value in t2. If a successor is found, the fast simulator \ncontinues replaying memoized actions. If none is found, an action cache miss occurs and control returns \nto the slow simulator. Figure 10 shows pseudocode for the slow simulator. It contains all source simulator \ncode, plus compiler-added code to write action numbers and run-time static plaeeholder data into the \nspecialized action cache. It also contains code to recover from an aeilon cache miss. This extra code \nis emboldened. Before executing the first statement of a dynamic basic block, the slow simulator writes \nthat block's action number into the special- ized aeilon cache. As run-time static placeholder values \nare com- puted, they too are written into the cache. The cache orders these 327 fun slow_main(pc) { val \nnpc : pc + 4; switch(pc) { pat add: if{i) { memoize_action_number(1); val tl = imm?sext(32); memoize \nstatic data(rl, r2, tl); if(Irecover) R[rl] = R[r2] + tl; } else { memoize_action_number(2); memoize_static_data(rl, \nr2, r3); if(Irecover) R[rl] = R[r2] + R[r3]; } pat beq: memoize_action_number(3); memoize_static_data(rl); \nval t2; if(recover) recover_dynaIaicresult(t2); else { t2 = (R[rl] == 0); memoize_dynamic_result \n(t2) ; ) if(t2) npc = pc + offset?sext(32); } memoizeaction_mamber(4); memoize static data(n~c); if(Irecover) \ninit = npc; }   FigurelO. Siow/eompletesimulaton actions and data in the same order in which they \nexecute, which is the order the fast simulator will replay them. For action 3--the bz instruction's predicate \nexpression--the slow simulator calls memoize_dynamic_result (t2) to record the dynamic result. The result \nis either 1 (true) or 0 (false). The subsequent memoized actions are valid only if the fast simulator \ncomputes the same result. If the fast simulator computes a different result, the slow simulator is restarted \nto produce actions for the other control-flow path. The slow simulator also contains code to recover \nfrom action cache misses. Its dynamic statements are guarded by predicates that allow dynamic code to \nexecute only when the recover flag is false. The recover variable is set true when the slow simulator \nrestarts after an action cache miss. It is changed to false when the slow simulator catches up to the \npoint at which the action cache miss occurred. When fast simulation has an action cache miss (e.g., action \n3 com- putes a dynamic result value not in the cache) the simulator searches back through the most recently \nreplayed actions until it finds the last cache entry. The action numbers and dynamic result values that \nwere replayed since this entry are saved on a stack called the recovery stack. The arguments to main \ncan be extracted from the entry's key, and the slow simulator is restarted in recovery mode (i.e., recover \nset to true). In this mode, the recovering slow simulator only executes run-time static statements. The \ndynamic statements' guards prevent them from executing, since they have previously run and updated dynamic \nsimulator state. In this mode, calling an action only veri- fies that the action number nmtches the expected \naction number popped off the recovery stack. Testing action numbers in this way is not necessary, but \nis useful to ensure that the fast and slow simu- lators communicate correctly. Dynamic result tests do \nnot modify the specialized action cache either. Instead, they retrieve the dynamic result previously \ncalcu- lated by the fast simulator and pass it m the slow simulator. This is necessary, because the slow \nsimulator cannot execute any dynamic computation until miss recovery has finished. When the slow simulator \ncaches up to the point at which the action cache miss occurred, it returns to normal slow simulation. \nAt this point, all run-time static and dynamic variables have consistent values, and the specialized \naction cache is ready to receive new actions. 5. RELATED WORK Fast-forwarding, like run-time code generation, \nspecializes a pro- gram, while it executes, for a particular input. Facile, however, does not currently \ngenerate or execute new instructions, instead it produces an optimized representation of its internal \nactions that can be interpreted efficiently. Some contemporary run-time code generation systems are Fabius, \nTempo, and DyC. Fabius performs run-time code generation for programs written in a functional sub- set \nof ML [9]. Tempo supports both compile-time partial evalua- tion and run-time specialization of C programs \n[4]. Like Facile, it uses compile-time binding analysis to identify run-time static code and specializes \nthe residual dynamic code at run time. DyC spe- cializes C code with run-time code generation and optimization \n[5]. Like Facile, DyC uses simple annotations to identify run-time static values in a subject program, \nuses polyvariant binding-time analysis, and caches specialized code indexed by its run-time static input. \nThese system are general-propose compilers. It is an inter- esting open question whether they could analyze \nthe complex code and state in a detailed simulator well enough to optimize these pro- grams. Traditional \npartial evaluation and run-time code generations sys- tems, such as those above, generate residual code \nfor functions or regions, neither of which is practical for this application. The residual code for a \nmicro-architectural simulator is large and must be generated for many different static inputs (potentially, \nthe cross product of instructions and micro-architectural states). Fast-for- warding specializes, on \ndemand, only the control flow paths that execute. This saves space in the specialized action cache, at \nthe cost of a recovery mechanism to handle control-flow path misses. Facile's domain-specific language, \nbesides facilitating the static program analysis, greatly simplifies the code for a simulator. Besides \nproviding a concise syntax for expressing the encoding and semantics of instructions, the language also \nprovides a frame- work for writing the architectural simulation and for understanding the specialization \nprocess. Other simulators, such as DyC, rely on potentially unsafe programmer annotations to ameliorate \na lack of pointer analysis and domain knowledge. 800 '1~ 700 O gO0 0 500 400 ~4 300 ~ 2o0 ~ lOO .................. \ni ............ = . ~with rn em oization ............... i mw ithout mem oization El Sire pie Scalar .......................... \ni Figure 11. FastSim (pre-Facile) performance, with and without memoization, vs. SimpleScalar performance. \n6. PERFORMANCE Fast-forwarding can be very effective at accelerating the simula- tion of a detailed out-of-order \nmicro-architecture. Section 6.1 dem- onstrates this acceleration in a hand-coded memoizing out-of-order \nsimulator written in C. Section 6.2 shows that a similarly large per- formance improvement is achieved \nwith an automatically opti- mized simulator written in Facile, although current inefficiencies in the \ncompiled Facile code prevent it from performing as well as the simulator coded by hand. Seetion 6.3 discusses \nthe shortcom- ings of the current Facile compiler implementation, and what can be done to improve generated \nsimulator performance. 6.1. The Potential of Fast-Forwarding FastSim [14] is a detailed out-of-order \nmicro-architecture simula- tor that models the SPARC-V8 ISA running on a MIPS RI0000- like out-of-order \npipeline. FastSim predates Facile. It was written in C to demonstrate the fast-forwarding technique. \nWe use FastSim's performance as a baseline to evaluate Facile simulators. Measurements used the SPEC95 \nbenchmarks. All runs were per- formed on a Sun Mierosystems Ultra En~rprise E5000, with 167 MHz UltraSPARC \nprocessors and 2 GByte of physical memory. All programs except compress used their \"test\" input set to \nreduce simulation time. Compress, whieh requires less time to execute, used its \"train\" input set. Figure \n11 shows the performance of FastSim with and without fast-forwarding, and compares this performance against \nthe Sim- pleScalar out-of-order simulator [1]. SimpleScalar is a widely used, comparable, conventional \nsimulator, that simulates a simi- larly detailed micro-arehit~ture. Moreover, it is among the fastest \ninstruction-level out-of-order simulators available. FastSim without fast-forwarding performed comparable \nto Sim- pleScalar: FastSim was 1.1-2.1 (mgfid-gce) times faster than Sim- pleScalar. With fast-forwarding, \nFastSim's performance improved by an order of magnitude, making it 8.5-14.7 (fpppp-ijpeg) times faster \nthen SimpleScalar. FastSim with fast-forwarding ran 4.9- 11.9 (li-mgrid) times faster than FastSim without \nthis technique, while computing exactly the same simulated cycle counts. The reason for FastSim's high \nperformance is that nearly all target instructions are simulated by the fast simulator, which skips most \nof the out-of-order pipeline simulation work. Table 1 shows the percentage of instructions simulated \nby the fast simulator. In the worst case--gee--99.689% of instructions were replayed by fast simulation. \nThe overhead of out-of-order pipeline simulation, which accounts for the majority of un-memoized simulation \ntime, was nearly eliminated from the simulator execution. Table 1: Percentage of instructions fast-forwarded. \n Integer % Floating-point % benchmarks Fast-Fwd. benchmarks Fast-Fwd. 099.go 99.901% 101.tomcatv 99.997% \n124.m88ksim 99.987% 102.swim 99.977% 126.gec 99.689% 103,su2cor 99.974% 129.compress 99.923%, 104.hydro2d \n99.972% 130.1i 99.997% 107.mgfid 99.999% 132.ijpeg 99.797% 110,applu 99.999% 134.perl 99.978% 125.turb3d \n99.999% 147.vortex 99.992% 141.apsi 99.998% 145,fpppp 99.987% 146.wave5 99.995% Fast-forwarding trades \nmemory consumption for speed. A memoizing simulator may consume significantly more memory than a conventional \nsimulator. Table 2 shows the amount of data memoized during simulation of the SPEC95 benchmarks. Most \nbenchmarks generated relatively little data, but a few benchmarks produced over 100 MB of data. The worst \nease was go, which required nearly 900 MB to simulate its test input set. Table 2: Quantity of memoized \ndata.  Integer MBytes Floating-point MBytes Benchmarks Cached Benchmarks Cached 099.go 889.4 101.tomcatv \n5.6 124.m88ksim 4.6 102.swim 16.8 126.gee 296.0 103.su2cor 32.8 129.compress 2.8 104.hydro2d 35.5 130.1i \n3.2 107.mgfid 9.5 132.ijpeg 199.5 110.applu 19.5 134.perl 142.9 125.turb3d 10.4 147.vo~ex 108.6 141 .apsi \n20.3 145 .fpppp 25.4 146.wave5 38.3 329 180 160 o t40 o 120 W with m ern otzation ~00 80 Mwithout m \nem oization 60 E3 SimpteScalar 4O 20 0 g. ooO, Figure 12. Pertbrmanee of out-of-order Facile simulator \nwith and without fast-forwarding. Memory utilization can be limited by fixing a maximum cache size and \nclearing the cache when it fills [14]. Just as when the program starts, new actions and data are memoized \nby the slow sinmlator. Using this simple policy, cache size can be reduced by a factor of ten, with little \nimpact on memoized simulator performance. 6.2. Facile Simulator Performance The previous section quantified \nthe effectiveness of fast-forward- ing, when applied by hand to out-of-order simulation. An out-of- order \nsimulator written in Facile and optimized by Faeile's com- piler achieves comparable performance gains. \nWe implemented an out-of-order micro-architecture simulator in Facile. Like FastSim, our new simulator \nmodels branch prediction, speculative execu- tion, non-blocking caches, and register renaming. A 32-instruction \nout-of-order pipeline window is used, which is similar in complex- ity to the R10000 pipeline modeled \nby FastSim. As in FastSim, the branch predictor and cache simulator are not memoized, while the pipeline \nsimulator (including register renaming) is memoized. Unlike FastSim, direct execution is not used and \nthe target instruc- tions' semantics are executed out-of-order. 4 This simulator con- sists of 1,959 \nnon-comment, non-blank lines of Facile code and 992 lines of C code. By contrast, a functional simulator \nrequired 703 lines of Facile code and an in-order pipeline with reservation tables required 965 lines \nof Facile and 11 lines of C code. We measured the SPEC95 benchmarks on the same 167 MHz UltraSPARC host. \nCompress ran with its \"train\" input set, and all other benchmarks ran with their \"test\" input sets. We \nsimulated the benchmarks with and without memoization. With memoization, the specialized action cache \nwas limited to 256 Mbytes and cleared when full. When memoization was not used, only the slow simula- \ntor was generated, with no extra code for fast-forwarding or manipulating the cache. Figure 12 shows \nthe performance of the out-of-order simulator written in Facile, with and without fast-forwarding, compared \nto SimpleScalar. Fast-forwarding improved simulator performance 2.8-23.8 (gcc-fpppp) times over the same \nsimulator without this technique. The harmonic mean of the performance improvement 4. FastSim uses direct \nexecution to simulate target instruction semantics in-order. An out-of-order simulator that calculates \nsimulated execution time runs intermittently as a co-routine of the direct execution. was 8.3. This is \ncomparable to the order-of-magnitude acceleration achieved by hand in FastSim. Nevertheless, the current \nFacile compiler produces relatively inef- ficient code. The Facile simulator ran at a sixth of the speed \nof the hand-coded FastSim simulator. In spite of its inefficiency, the fast- forwarding Facile simulator \nran the SPEC benchmarks 1.5 times faster than SimpleScalar (harmonic mean). Facile performs worse for \nonly one benchmark (gcc), which required significantly more than the 256 MB of memory allocated to the \nspecialized action cache in these experiments. A larger cache would have improved Facile performance \nfor this benchmark.  6.3. Future Compiler Optimizations Although we have no measurements to demonstrate \nthat the ineffi- ciencies in Facile's compiler can be corrected, examination shows several ways its generated \ncode can be improved, given more time and manpower. Here of some of the most obvious potential improvements: \n1. In the fast simulator, the switch on action numbers is inefficient and could be rewritten as indirect \nfunction calls or, in gce, an indirect goto. Gce compiles an indirect goto into a single indi- rect jump \ninstruction, which should run significantly faster than the current index calculation, load, bounds check, \nand indirect jump generated for a switch statement. 2. The slow simulator can be divided into two separate \nfunctions: one for normal slow simulation and another for recovering from a miss, making both tasks much \nfaster. This change would elim- inate the if-statement guards around dynamic statements in the slow simulator. \nDynamic statements could simply be left out of the recovery version of the simulator. The normal slow \nsimula- tor would have dynamic statements without guards. This opti- mization would help since the slow \nsimulator is so much slower that it still accounts for a significant fraction of simulator exe- cution \ntime, although most cycles/instructions are simulated by the fast simulator. 3. With liveness analysis, \nmany statements that support memoiza- tion, but act on variables that are not live, could be identified \nand removed. Of particular interest are non-live global variables that are run-time static at the end \nof a step function. Since glo- bal variables are considered dynamic at the start of a step func-  330 \ntion call, the Facile compiler generates extra statements at the end of the function to make their run-time \nstatic values dynamic for the next iteration. This in turn causes extra data to be written into the specialized \naction cache, which happens whenever a ran-time static value becomes dynamic. Skipping this unneces- \nsary work for variables that are not live would both reduce the number of statements in the fast and \nslow simulators, and reduce the amount of data in the action cache. 4. Many variables, stored globally \nto allow communication of dynamic values between the fast and slow simulators during miss recovery, are \nunnecessarily duplicated by our implementa- tions of function inlining and polyvariant division. Reducing \nthis duplication would reduce a simulator's memory footprint, improving its cache performance. 5. Although \nour binding-time analysis currently detects static, run- time static and dynamic code and data, it does \nnot perform par- tial evaluation at compile time. In our experience, micro-archi- tecture simulators \ndo not contain much compile-time static code, but constant folding and similar optimizations may bene- \nfit both the slow and fast simulators. The analysis (i.e., binding- time analysis) is already in place, \nmaking these optimizations a worthwhile addition to the compiler.  A more radical optimization would \nbe to apply ran-time code gen- eration and optimization to actions, similar to DyC [5]. As cur- rently \nimplemented, fast-forwarding stores action numbers in the specialized action cache and interprets them. \nWith run-time code generation, a simulator could write native host instructions directly into the action \ncache. This would eliminate the overhead of reading action numbers and dispatching to dynamic basic block \ncode, and reduce the overhead of reading placeholder data from the cache, both of which are bottlenecks \nto faster simulator performance. 7. CONCLUSION Fast-forwarding is a very effective technique for accelerating \ncom- plex detailed processor simulation, but is difficult to implement by hand. The Facile language simplifies \nboth writing instruction-level micro-architecture simulators and the compiler analyses needed to generate \na memoizing simulator. Simulators written in Facile can be compiled to use fast-forwarding and have demonstrated \nthe same order of magnitude performance improvement seen in a hand-coded memoizing simulator. Facile \nmakes the fast-forward- ing optimization accessible to simulator writers. Although the current incarnation \nof Facile's compiler produces rel- atively inefficient code, its fast-forwarding still makes out-of-order \nsimulation faster than similarly detailed out-of-order simulators without memoization. There is a lot \nof room for improvement in the Facile compiler. Additional time and effort should bring the performance \nof automatically optimized simulators closer to the performance of hand coded simulators, like FastSim. \nFor more information about Facile and fast-forwarding, see the FastSim web pages at http://www.cs.wisc.edu/-wwt/fastsim. \nThey contain descriptions and examples of Facile, FastSim, and fast-for- warding, and contain links to \nrelated publications. ACKNOWLEDGMENTS Many thanks to Ras Bodik, Charles Consel, Manuvir Das, Jakob Rehof, \nand Anne Rogers for their helpful comments. REFERENCES [1] Burger, D. and Austin, T.M., \"The SimpleScalar \nTool Set, Version 2.0,\" Tech Report #1342, University of Wisconsin- Madison, Department of Computer Sciences, \nJune 1997. [2] Chemoff, A., et.al, \"FX!32 a profile-directed binary transla- tor,\" in IEEE Micro98, March-April \n1998, 18 (2) 56-64. [3] Cmelik, B. and Keppel, D., \"Shade: A Fast Instruction-Set Simulator for Execution \nProfiling,\" in Proceedings of SIGMETRICS94, (Nashville TN, May 1994), 128-137. [4] Consel, C. and Noel, \nF., \"A General Approach for Run-Time Specialization and its Application to C,\" in Proceedings of POPL96 \n(St. Petersburgh Beach FL, January 1996), 145-156. [5] Grant, B., et.al, \"An Evaluation of Staged Run-time \nOptimi- zations in DyC,\" in Proceedings of PLDI99 (Atlanta GA, May 1999), 293-304. [6] Hennessy, J. and \nPatterson, D., Computer Organization and Design: The Hardware-Software Interface (Appendix A, by James \nR. Lares), Morgan Kaufman, 1993. [7] Herrod, S., et.al, \"The SimOS Simulation Environment,\" Computer \nSystems Laborato~, Stanford University, 1996. [8] Jones, N.D., Gomard, C., and Sestoft, P., Partial Evaluation \nand Automatic Program Generation, Prentice Hall, 1993. [9] Lee, P. and Leone, M., \"Optimizing ML with \nRun-Time Code Generation,\" in Proceedings of PLD[96 (Philadelphia PA, May 1996), 137-148. [10] Magnusson, \nRS., et.al, \"SimICS/sun4m: A Virtual Worksta- tion,\" in Proceedings of USENIX98 Technical Conference \n(New Orleans LA, June 1998). [11] Pai, V.S., Ranganathan, P., and Adve, S.V., \"RSIM: An Exe- cution-Driven \nSimulator for ILP-Based Shared-Memory Mul- tiproeessors and Uniprocessors,\" in the Workshop on Computer \nArchitecture Education held in conjunction with HPCA97, (San Antonio TX, February 1997). [12] Ramsey, \nN. and Fernandez, M., \"The New Jersey Machine- Code Toolkit,\" in Proceedings of USENIX95 Technical Con- \nference (New Orleans LA, January 1995), 289-302. [13] Schnarr, E., Applying Programming Language Implementa- \ntion Techniques To Processor Simulation, Ph.D. Dissertation, University &#38;Wisconsin-Madison, Fall \n2000. [14] Schnarr, E. and Larus, J.R., \"Fast Out-Of-Order Processor Simulation Using Memoization,\" in \nProceedings of ASPLOS98 (San Jose CA, October 1998), 283-294. 331   \n\t\t\t", "proc_id": "378795", "abstract": "<p>Architectural simulators are essential tools for computer architecture and systems research and development. Simulators, however, are becoming frustratingly slow, because they must now model increasingly complex micro-architectures running realistic workloads. Previously, we developed a technique called fast-forwarding, which applied partial evaluation and mermoization to improve the performance of detailed architectural simulations by as much as an order of magnitude [14].</p><p>While writing a detailed processor simulator is difficult, implementing fast-forwarding is even more complex. This paper describes Facile, a domain-specific language for writing detailed, accurate micro-architecture simulators. Architectural descriptions written in Facile can be compiled, using partial evaluation techniques, into fast-forwarding simulators that achieve significant performance improvements with far less programmer effort. Facile and its compiler make this performance-enhancing technique accessible to computer architects.</p>", "authors": [{"name": "Eric C. Schnarr", "author_profile_id": "81100559026", "affiliation": "QUIQ Incorporated, 25 Kessel Court, Suite 201, Madison, WI", "person_id": "P334735", "email_address": "", "orcid_id": ""}, {"name": "Mark D. Hill", "author_profile_id": "81100455115", "affiliation": "University of Wisconsin-Madison, 1210 West Dayton Street, Madison, WI", "person_id": "PP40027462", "email_address": "", "orcid_id": ""}, {"name": "James R. Larus", "author_profile_id": "81100277326", "affiliation": "Microsoft Research, One Microsoft Way, Redmond, WA", "person_id": "P132790", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/378795.378864", "year": "2001", "article_id": "378864", "conference": "PLDI", "title": "Facile: a language and compiler for high-performance processor simulators", "url": "http://dl.acm.org/citation.cfm?id=378864"}