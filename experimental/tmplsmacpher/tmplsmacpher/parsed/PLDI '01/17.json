{"article_publication_date": "05-01-2001", "fulltext": "\n Efficient Representations and Abstractions for Quantifying and Exploiting Data Reference Locality Trishul \nM. Chilimbi Microsoft Research One Microsoft Way Redmond, WA 98052  trishulc@ microsoft.com ABSTRACT \nWith the growing processor-memory performance gap, understand- ing and optimizing a program's reference \nlocality, and conse-quently, its cache performance, is becoming increasingly important. Unfortunately, \ncurrent reference locality optimizations rely on heu- ristics and are fairly ad-hoc. In addition, while \noptimization tech- nology for improving instruction cache performance is fairly mature (though heuristic-based), \ndata cache optimizations are still at an early stage. We believe the primary reason for this imbalance \nis the lack of a suitable representation of a program's dynamic data reference behavior and a quantitative \nbasis for understanding this behavior. We address these issues by proposing a quantitative basis for \nunderstanding and optimizing reference locality, and by describing efficient data reference representations \nand an exploitable locality abstraction that support this framework. Our data reference representations \n(Whole Program Streams and Stream Flow Graphs) are compact--two to four orders of magnitude smaller than \nthe program's data reference trace--and permit efficient analysis--on the order of seconds to a few minutes---even \nfor complex applications. These representations can be used to efficiently compute our exploitable locality \nabstraction (hot data streams). We demonstrate that these representations and our hot data stream abstraction \nare useful for quantifying and exploiting data reference locality. We applied our framework to several \nSPECint 2000 benchmarks, a graphics program, and a commercial Microsoft database application. The results \nsuggest significant opportunity for hot data stream-based locality optimizations. 1. INTRODUCTION In \nthe never-ending quest for greater performance, machine archi- tectures continue to grow more complex. \nAchieving near-peak per- formance on these modem machines places an immense burden on software. Machine \ncomplexity makes statically anticipating and improving a program's performance increasingly difficult. \nA more pragmatic approach is to observe a program's dynamic execution behavior and use this data to optimize \nthe program. Program paths--consecutively executed basic block sequences-- can offer insight into a program's \ndynamic control flow behavior and have been successfully used in compilers for program optimi- Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for profit or commercial advan- tage and that copies \nbear this notice and the full citation on the first page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior specific permission and/or a fee. PLD/2001 6/01 Snowbird, \nUtah, USA &#38;#169; 2001 ACM ISBN 1-58113-414-2/01/06.,.$5.00 zations [1, 2, 10, 12]. However, though \npaths provide a basis for understanding a program's dynamic control flow, they supply an incomplete picture \nof a program's dynamic behavior as they do not capture a program's data accesses. With the growing processor- \nmemory performance gap, understanding and optimizing a pro- gram's data accesses is becoming increasingly \nimportant. Recent research has attempted to address this problem by optimizing a pro- gram's data layout \nfor caches [3, 4, 5, 6, 9, 15, 24]. Since a cache miss can be up to two orders of magnitude slower than \na cache hit, these techniques promise significant performance improvements. Unfortunately, while aggregate \ndata access frequency information may be sufficient for page-level data layout [22], cache-level layout \nrequires fine-grain temporal information about sequences of data references [3, 4]. This is because cache \nblocks are much smaller than pages resulting in zero-tolerance for incorrect data co-location. This temporal \ninformation can be obtained by instrumenting every program load and store, but analyzing the resulting \ngigabytes of trace data is impractical. In addition, the need for data reference sequence information \nmakes it hard to use statistical sampling of loads and stores to reduce the amount of data generated. \nWe believe that the lack of suitable data reference representations and abstractions are the primary \nobstacle to analysis and optimiza- tion of a program's dynamic data reference behavior. Unlike the control \nflow graph and program paths, which are a convenient and compact abstraction of a program's control flow, \nno corresponding analogue exists for a program's data accesses: Compounding this problem is the fact \nthat current reference locality optimizations, both for code and data, are fairly ad-hoc and rely primarily \non heu- ristics. This paper attempts to address these issues by proposing a quantitative basis for understanding \nand improving reference locality, and by describing efficient data reference representations and an exploitable \nlocality abstraction that support this framework. We show that a hierarchical compression algorithm called \nSEQUITUR [20] can be used along with simple data abstractions to construct a compact representation of \na program's dynamic data reference behavior. The resulting structures, called Whole Program Streams (WPS), \nare small and permit analyses without decompression. The paper shows that these WPSs can be used to efficiently \ncompute our exploitable locality abstraction--hot data streams--which are frequently repeated sequences \nof consecutive data accesses, and are analogous to hot program paths. While WPSs capture a program's \ncomplete dynamic data reference behavior, this level of detail is often unnecessary for many optimizations. \nTo address this, we show that hot data streams eanbe combined with the SEQUITUR compression technique \nto produce a series of representations with increasing compactness, but lower precision. These representations, \nwhich are efficient to analyze, can be up to four orders of magnitude smaller than the original data \nreference trace, yet still retain enough information about data reterence 191 [--0--tw olf + perlbmk \n~ eon + mcf -4~-, sqlserver + tw otf+ perlbmk ~ eon+ rrcf ........+~,o sqlserver 1 100 100 8O 8 80 0 \nO \u00a21    {00 . 60 ~ 40 40 O 3 20 20 0 1' ~\" 0 2 4 6 8 0 1 2 3 % of load-store PCs % of data addresses \nFigure 1. Program data reference skew in terms of load-store PCs and data addresses. behavior for optimization \npurposes, The paper evaluates these representations and the hot data stream abstraction by applying them \nto quantify and exploit data reference locality in several SPECint 2000 benchmarks, boxsim, a graphics \napplication, and Microsoft SQL Server, a commercial database. It shows that the hot data stream abstraction \ncan be used to compute inherent exploitable locality metrics that are indicative of a program's algorithmic \nspatial and temporal reference locality, and a realized locality metric that evaluates the effectiveness \nof the program's mapping of data objects to memory addresses at exploiting the inherent reference locality. \nSince these locality metrics can be efficiently computed at the granularity of individual hot data streams, \nthey allow locality optimizations to focus solely on streams that show the greatest potential benefit. \nIn addition to guiding locality optimizations, hot data streams, which contain data objects that are \nfrequently accessed together, can help drive locality optimizations, such as clustering and prefetching. \nThe paper demonstrates that such hot data stream-based locality optimizations have the potential to significantly \nimprove cache performance. The rest of the paper is organized as follows. Section 2 presents a quantitative \nbasis for studying reference locality and defines an exploitable locality abstraction--hot data streams. \nIt discusses several exploitable locality metrics that can be computed in terms of this hot data stream \nabstraction. Section 3 describes the construction of Whole Program Streams and other more compact, though \nless precise, representations of a program's dynamic data reference behavior. It also discusses efficiently \ncomputing the hot data stream abstraction from these representations. Section 4 outlines the role of \nour data reference representations, hot data streams abstraction, and locality metrics in exploiting \nreference locality. Section 5 presents experimental results for several SPECint 2000 benchmarks, boxsim, \na graphics application, and Microsoft's SQL database. Finally, section 6 briefly discusses related work. \n 2. QUANTIFYING LOCALITY Programs appear to exhibit a property termed locality of reference [16]. Informally, \nthis locality of reference principle says that the most recently used data is likely to be accessed again \nin the near future. This phenomenon is widely observed and a rule of thumb, often called the 90/t0 rule, \nstates that a program spends 90% of its execution time in only 10% of the code. While cache miss rate \npro- vides an indirect measure of a program's reference locality, rarely do we see a program's reference \nlocality directly quantified inde- pendent of the mapping of data objects to memory addresses, and much \nless this quantification used as a basis for optimization. This section discusses how data reference \nlocality can be computed, shows that data references exhibit locality, describes a new concept that we \ncall exploitable locality, and defines metrics for quantify- ing exploitable locality. 2.1 Data Reference \nLocality A data reference sequence exhibits locality if the reference distri- bution is non-uniform in \nterms of either the data addresses/objects being accessed or the load/store instructions responsible \nfor the accesses. A consequence of data reference locality is that some addresses/objects or load/store \ninstructions are accessed much more frequently than others making these highly referenced enti- ties \nattractive optimization targets. One possible quantifiable defi- nition for data reference locality in \nthe spirit of the 90/10 rule is the smallest percentage of accessed data addresses/program instruc- tions \nthat are responsible for 90% of the data references. Thus, good reference locality implies a large skew \nin the reference distri- bution. The 90/10 rule predicts a reference locality of 10% and a uniform reference \ndistribution possesses a reference locality of 50%. Figure 1 indicates the fraction of program data references \n(90%) that are attributable to the most frequently referenced data addresses (1-2% of accessed data addresses), \nand to the hottest loads and stores (4.-8% of load/store accesses), for several SPE- Cint 2000 benchmarks, \nand Microsoft SQL Server, a commercial database, l The graphs indicate that programs possess significant \ndata reference locality, even more than the 90/10 rule predicts. Interestingly, they also indicate that \ndata addresses often exhibit greater access skew than a program's load/store PCs, making them 1 To avoid \nbiasing the data in favor of addresses, stack refer- ences were excluded. In addition, we modified the \nprograms to prevent reuse of heap addresses by removing all system calls that free memory. Reference \nLocality Sequence 1: abcacbdbaecfbbbcgaafadcc Reference Locality + Regularity Sequence 2: abcabcdefabcgabcfabcdabc \nRegularity Sequence 3: abchdefabchiklfimdefmklf Figure 2. Data reference sequence characteristics. \ngood optimization targets. Thus, in the rest of this paper we focus on reference locality expressed in \nterms of data addresses/objects. 2.2 Data Reference Regularity From an optimization standpoint, locality \nis only interesting if it can be exploited to improve program performance. Processor caches, which are \nthe ubiquitous hardware solution to exploiting reference locality, form the first line of attack. We \nbelieve that the key property that makes data reference locality exploitable by software optimization \n(over and above caches) is regularity. For a data reference subsequence to exhibit regularity, it must \ncontain a minimum of two references and must appear at least twice (non-overlapping) within a data reference \nsequence. In the first sequence in Figure 2, the only regular subsequence is bc, and in the second sequence, \nit is ab, bc, and abc. We quantify regularity in terms of four associated concepts called regularity \nmagnitude, regularity frequency, spatial regularity, and temporal regularity. To make the definitions \nconcrete, we apply them to quantify the regularity of subsequence abc in the second sequence shown in \nFigure 2. Regularity magnitude is defined as the product of the number of references in the regular data \nsubsequence (not necessarily to unique data objects) and the non-overlapping repetition frequency of \nthat subsequence in the data reference sequence. The regularity magnitude of subsequence abc is 18. Regularity \nfrequency is defined as the number of non-overlapping occurrences of the regular reference subsequence \nwithin the data reference sequence. The regularity frequency of abc is 6. Spatial regularity is defined \nas the number of references (not necessarily to unique data objects) in the regular subsequence. The \nspatial regularity of abc is 3. Temporal regularity is defined as the average number of references between \nsuccessive non-overlapping occurrences of the data reference subsequence that exhibits regularity. The \ntemporal regularity ofabc is 1.2. 2.3 Exploitable Data Reference Locality While hardware caches are \ncapable of exploiting reference locality independent of regularity, software locality optirnizations \nare heavily dependent on regularity. Figure 2 illustrates this with three data reference sequences. The \nfirst two data reference sequences have exactly the same reference locality or reference skew. However, \nthe first data reference sequence is less predictable than the second sequence. The second data reference \nsequence's regularity makes it a much more attractive optimization target. For example, a prefetching \noptimization could exploit the regularity of subsequence abc to initiate prefetches for data objects \nb, and c, once object a has been observed. A clustering optimization could attempt to allocate data objects \na, b, and c, together, in the same cache block or page. If a data reference sequence exhibits regularity \nthen do we need it to exhibit reference locality or reference skew as well? Or is regularity all we need? \nThe third data reference sequence in Figure 2 attempts to address these questions. While it is certainly \npossible for software optimizations to take advantage of the sequence's regularity, this reference sequence \ndoes not appear as promising as the second sequence since any optimization must track and analyze a larger \nnumber of data objects than the second sequence to get the same performance benefit. In any case, since \na program's data reference sequence exhibits locality (see Figure 1), such sequences are extremely unlikely \nto occur in practice. Thus, from an optimization perspective, data reference sequences that possess both \nreference locality and regularity are ideal optimization targets. Hence, we refer to the combination \nof reference locality and regularity as exploitable locality, and it is the focus of the rest of this \npaper. 2.3.1 Exploitable Data Reference Locality Abstraction Since exploitable locality forms the basis \nof our optimization framework, we define an exploitable locality abstraction called hot data streams. \nA data stream is a data reference subsequence that exhibits regularity. If the regularity magnitude of \na data stream exceeds a predetermined threshold, then it is termed a hot data stream. In the spirit of \nthe 90/10 rule, this heat threshold above which a data stream is labelled as \"hot\" is set such that the \ncollec- tion of hot data streams together cover 90% of all program data references (see Section 3 for \ndetails). Note that with this heat threshold, the 90/10 rule ensures that only a small fraction of accessed \ndata addresses/objects participate in hot data streams. We confirm this in Section 5 and additionally \ndemonstrate that this heat threshold generates a small number of hot data streams, mak- ing them attractive \noptimization targets. Hot data streams are by definition regular, and since they are extracted from program \ndata reference traces, which are known to exhibit reference locality (see Figure 1), they qualify as \nexploitable locality units. These hot data streams, which are analogous to hot program paths, can be \neffi- ciently extracted from large program data reference traces, as we demonstrate in Section 3. In \naddition, the collection of hot data streams taken together are representative of a program's data refer- \nence behavior since they cover 90% of all program data references.  2.4 Quantifying Exploitable Locality \nThis section presents two sets of exploitable locality metrics that are useful from an optimization perspective. \nThese metrics are defined in terms of hot data streams, our exploitable locality abstrraction. The first \nset of metrics quantify the inherent exploit-able data reference locality of an algorithm. This information \nmay be useful to a programmer and could suggest algorithmic modifica- tions. The second kind of exploitable \nlocality metric quantifies the impact of a particular mapping of data objects to memory addresses, and \nthe interaction of this mapping with hardware caches. This realized locality metric quantifies how effectively \nhardware caches exploit a program's inherent reference locality and provides a measure of the remaining \nopportunity for software- based locality optimizations. Together, these locality metrics define limits \non the performance improvements due to data refer- ence locality optimizations. In addition, these locality \nmetrics, which can be efficiently computed from hot data streams, can help guide data locality optimizations \nas described in Section 4. 2.4.1 Inherent Exploitable Locality Metrics Spatial and temporal locality \nare well-known concepts used to express locality [13]. However, these are unsuitable as inherent exploitable \nlocality metrics for three reasons. First, the standard definitions are qualitative and do not indicate \nhow these metrics 193 S -> BABB ~--~-t---J abcbcabcabc Reference sequence ---~\"- [ SEQUITUR B->aa Sequence \ngrammar ~ ' ~ ~ 4, 4, DAG representation ~ ofgrammar N / a \\ ~ \"~ b c Figure 3. Sequence compression \nusing SEQUITUR~ may be computed. Next, they do not incorporate regularity, which is important for quantifying \nexploitable locality. Finally, spatial locality in particular is defined in terms of cache blocks, making \nit ill-suited as a measure of inherent locality. To address this, we define exploitable algorithmic spatial \nand temporal data reference locality metrics in terms of our hot data stream abstraction. The inherent \nexploitable spatial locality of a hot data stream is its spatial regularity. A data reference sequence's \ninherent exploitable spatial locality is the weighted average spatial regularity across all of the sequence's \nhot data streams, where a hot data stream's weight is its regularity magnitude. Long hot data streams \nindicate good inherent exploitable spatial reference locality. Similarly, the inherent exploitable temporal \nlocality of a hot data stream is its temporal regularity. A data reference sequence's inherent exploitable \ntemporal locality is the weighted average temporal regularity across all of its hot data streams. A hot \ndata stream that repeats in close succession has good inherent temporal reference locality. As noted \nearlier, these metrics are characteristic of the algorithm and are independent of the mapping of data \nobjects to memory addresses.  2.4.2 Realized Exploitable Locality Metric Caches exploit a program's \nspatial locality by transferring data in cache blocks that encompass multiple words (typically 32 to \n128 bytes). Caches exploit temporal locality by storing the most recently accessed data. Our realized \nexploitable locality metric-- cache block packing efficiency--attempts to quantify the success of caches \nat exploiting a program's inherent locality. This cache block packing efficiency metric is determined \nby the mapping of data objects to memory addresses. Ignoring cache capacity and associativity constraints, \nan ideal mapping would enable hardware to fully exploit a program's inherent locality. More typically, \na sub- optimal mapping suggests software optimization opportunities. Finite cache capacity and limited \nassociativity provide additional optimization opportunities. A hot data stream's cache block packing \nefficiency is the ratio of the minimum number of cache blocks required by its data members if they could \nbe re-mapped to different memory addresses, to the actual number of cache blocks required, given the \ncurrent map- ping. A ratio close to one indicates that the existing data layout is good at exploiting \nthe hot data stream's inherent spatial locality. In addition, if the weighted average cache block packing \nefficiency over all program hot data streams is close to one, it indicates that the data layout assists \nthe cache in exploiting the temporal locality of hot data streams since it keeps to a minimum the number \nof cache blocks referenced between successive accesses to the same hot data stream (ignoring cache associativity \nconstraints).  3. DATA REFERENCE REPRESENTA- TIONS FOR COMPUTING LOCALITY Compression algorithms discover \nand exploit sequence regularity to produce a comprcssed sequence. Thus, they suggest themselves as obvious \ncandidates for efficiently discovering our exploitable locality abstraction--hot data streams--tYom a \ndata reference trace. They also offer the additional benefit of producing a reduced data reference trace \nrepresentation that is more efficient to analyze by virtue of its compactness. This section explores \nthese issues. Nevill-Manning and Witten's SEQUITUR compression algorithm infers hierarchical structure \nfrom a sequence of symbols [20]. This linear time, on-line algorithm constructs a context-free grammar \nfor its input. It does this by recursively replacing repetitions in the sequence with a grammatical role \nthat generates the repeated string. Figure 3 illustrates this for the string abcbcabcabc. The resulting \ngrammar requires fewer symbols than the input string and constitutes a hierarchical representation of \nthe string that explicitly captures the repeated sequences, bc and abc. In addition, the gram- mar can \nbe compactly represented as a DAG, as shown. Larus used this DAG representation of the SEQUITUR grammar, \nwhich he called Whole Program Paths (WPP), to represent a program's complete dynamic control flow behavior \n[17]. He showed that this WPP representation can be efficiently analyzed to extract hot pro- gram paths, \nwhich are acyclic path sequences that repeat fre-quently. For example, if the sequence in Figure 3 represents \na program's acyclic path sequence, then Lares showed how to effi- ciently compute the hot program path, \nabc, from the WPP repre- sentation of this sequence. This section extends the Whole Program Paths idea \nto provide a series of representations of a program's dynamic data reference behavior. Each representation \nin the series is more compact than its predecessor, but comprises a less precise representation of the \ndata reference trace. We describe simple data abstractions that enable the construction of these representations, \nwhich we call Whole Program Streams i (WPSi), from raw data address traces. The resulting representations \nare small and permit analyses without decompression (see Section 5.2). These representations can be used \nto efficiently discover hot data streams, our exploitable local- ity abstraction. While WPSs capture \na program's complete dynamic data reference behavior, this level of detail is often unnec- essary for \nmany optimizations. To address this, we discuss how WPSs can be combined with hot data streams to construct \nStream Flow Graphs (SFGs), which are yet more compact summarized representations that are similar to \ncontrol flow graphs in that refer- ence sequence information is no longer retained. 3.1 Whole Program \nStreams (WPS) While acyclic paths are a convenient and compact abstraction of a program's dynamic control \nflow, no corresponding analogue exists for a program's data accesses. Thus, direct application of Larus' \nWPP technique to a program's data reference trace faces several challenges. First, using SEQUITUR to \nidentify repetitions in a sequence of raw data addresses may obfuscate several interesting patterns that \nmay only be apparent at the granularity of data Address ] ~ab  abccO00 Abstraction / cdleO01 b SEQUITUR \na I Data m~mncetrace 0 Abstracted data reference trace o Whole Program Streams0 (WPS 0) a b c d t ot \ndata stream [ Hotdata / Graph I Abstraction \"~ ~stream analyse~ summafizatio~ a'l al b Icl Data reference \ntrace I Hot Data Streams 0 ~ l Stream Flow Graph 0 (SFG 0) Figure 4. Constructing data reference representations. \nobjects. Second, the size of the resulting WPS 0 grammar may be priately such that the hot data streams \nreferences taken together account for 90% of all data references. too large to be efficiently analyzed. \nFinally, the processing time required to construct a WPS o from a raw data address trace may be The algorithm \nused for detecting hot data streams in WPSs is the too large for the technique to be practical. Abstracting \ndata same algorithm Larus used to compute hot subpaths in WPPs (see addresses may alleviate these problems \nby increasing the regularity [17] for further details). The algorithm performs a postorder tra- in the \ndata reference stream and consequently reducing the size of versal of the DAG, visiting each node once. \nAt each interior node, the WPS 0 grammar. it examines each data stream formed by concatenating the sub- \nstreams produced by two or more of the node's descendents. The An obvious candidate for abstracting data \naddresses is to use the algorithm examines only concatenated strings, as the hot data name or type of \nthe object that generates the address. While this streams produced solely by a descendent node are detected \nin a approach works for static objects, heap addresses present an addi- recursive call. The algorithm \nfinds minimal hot data streams. It tional challenge. Heap space is reused as heap data is dynamically \nruns in time O(EL), where E is the number of edges in the WPS, allocated and freed. Consequently, the \nsame heap address can cor- and L is the maximum length of a data stream. The space require- respond to \ndifferent program objects during execution. These may ments are O(PL), where P is the number of partially \nvisited nodes need to be distinguished for several reasons. First, heap addresses in the DAG (which is \ntypically far lower than the number of nodes are unlikely to be reused in the same manner across multiple \npro- in the DAG). Our experiments indicate that the algorithm is effi- gram runs with different inputs. \nIn addition, source-level optimiza- cient in practice, requiring at most a minute to analyze the WPS \n0 tions need to distinguish between data objects that share the same representation, even for complex \nprograms, such as MS SQLheap address. A possible candidate for abstracting heap addresses server. is \nto use heap object names based on allocation site calling context. Allocation site calling context (depth \n3) has been shown to be a 3.2 Trace Reduction useful abstraction for studying the behavior of heap objects \n[22]. Since SEQUITUR is an on-line algorithm with tight time and An alternative approach that permits \ngreater discrimination space bounds, the resulting grammars are not minimal. In particu- between heap \nobjects is to associate a global counter with alloca- lar, the grammars contain many redundant productions. \nWhile this tion sites and increment the counter after each allocation. The com- does not affect the hot \ndata streams computed from the WPS repre- bination of the allocation site with this global counter (birth \nsentation, it does impact the WPS size and the analysis time identifier) can then be used to abstract \nheap addresses. Unlike pro- required to the compute the hot data streams. To address this, Larus gram \npaths, all these data abstractions are lossy--it is not possible proposed and used SEQUITUR(1), which \nlooks ahead a singleto regenerate the address trace once these abstractions have been symbol before introducing \na new rule to eliminate a duplicateapplied. However, depending on the nature of subsequent analyses digram, \nin his implementation of WPPs, but the grammars pro- on the WPSo structure, such an abstracted program \ndata access his- duced are not significantly smaller [ 17]. tory may suffice. Figure 4 illustrates the \nprocess of constructing a WPS representation from a data reference trace. We take an orthogonal approach \nthat uses the hot data streams as an abstraction mechanism. As shown in Figure 4, hot data streams The \nnext step is to analyze the WPS representation to discover hot can be used as an abstraction mechanism \nto produce a reduced ref- data streams. As defined earlier, a hot data stream is a data refer- erence \ntrace, tracei+l, which is composed solely of hot data ence subsequence that exhibits regularity and whose \nregularity streams with all cold references (which can be considered noise) magnitude exceeds a predetermined \n\"heat\" threshold, H. More excluded, from a trace i. This is done by traversing the WPS i toinformally, \na hot data stream is a frequently repeated sequence of regenerate the reference trace with hot data streams \nencoded as sin- consecutive data references. A minimal hot data stream is the min- gle symbols and cold \nreferences eliminated. This reduced trace, imal prefix of a data stream with a heat of H or more. To \nensure tracei+ 1, can again be processed by SEQUITUR to produce athat the collection of hot data streams \nare representative of a pro- WPSi+ 1 representation that can be used to detect hot datagram's data reference \nbehavior, the heat threshold H is set appro- 195 stmamsi+ 1. The encoding of hot data streams as single \nsymbols ale computed at the granularity of individual hot data streams, and elimination of cold addresses \n(i.e., noise) enables SEQUITUR to discover larger repetition patterns and accounts for the much more \ncompact grammar. This process can be repeated as many times as required. Each iteration produces a more \ncompact WPS representation and fewer, hotter hot data streams, but includes less of a program's original \ndata reference sequence since hot data streams are selected to cover 90% of references. Thus WPS 0 includes \nall original data references, hot data streams 0 include 90% of these references, WPS 1 includes 90% \nof original data refer- ences, and hot data streams I includes 81% of original data refer- ences. The \nreduced representations are useful since the WPS 0 representation captures a program's complete dynamic \ndata refer- ence behavior, a level of detail that is often unnecessary for many optimizations. In addition, \nthe reduced representations still retain information about frequently observed data reference behavior. \n 3.3 Graph Representation While WPSs are compact, they retain data reference sequence information that \ncan complicate analysis that does not require this information. For example, many analyses operate efficiently \non a program's control flow graph, which does not retain precise basic block sequence information. To \naddress this, we can combine the WPS with hot data streams to construct a &#38;ream Flow Graph (SFG), \nwhich is similar to a control flow graph with hot data streams replacing basic blocks as graph nodes, \nas shown in Figure 4. In the SFG, each hot data stream is represented by a single node and weighted \ndirected edges, <src, dest>, indicate the number of times an access to hot data stream src is immediately \nfollowed by an access to hot data stream dest. The SFG representation has the advantage that many control \nflow graph analyses can be directly adapted to operate on it. For example, dominators in the SFG may \nsuggest program load/store points to initiate prefetching. In addi- tion, the SFG captures temporal relationships \nthat are potentially more precise than Gloy et al.'s Temporal Relationship Graph (TRG) since they are \nnot determined by an arbitrarily selected tem- poral reference window size [ 11 ]. 3.4 Discussion The \nprevious discussion suggests using the WPS, and SFG representations, and the hot data stream abstractions \nwithin a data locality optimization framework, much as control flow graphs and program paths are used \nfor code optimizations. However, a major difference between these representations is that the control \nflow graph is a static representation and hence represents all possible program executions, whereas WPSs, \nand SFGs are abstract representations of one observed execution. Offsetting this is the fact that most \ncode optimizations have to be conservative since an incorrect transformation will affect program correctness, \nwhereas many data locality optimizations can afford to be aggressive as incorrect application affects \nonly a program's performance and not its correctness. In addition, our experiments indicate that hot \ndata streams, when expressed in terms of the program loads and stores that generate the references, are \nrelatively stable across program executions with different inputs [7]. 4. EXPLOITING LOCALITY Our exploitable \nlocality abstraction, hot data streams, and our associated exploitable locality metrics can be used to \nimprove data reference locality in at least four ways, which are described filrther in this section. \nFirst, they can help identify programs likely to benefit from data locality optimizations by computing \nexploitable locality metrics for them. Since these exploitable locality metrics locality optimizations \ncan focus solely on the streams that show the largest potential benefit. Second, they can help select \nthe most suitable locality optimization for a particular hot data stream. Next, hot data streams and \nour data reference representations can help drive locality optimizations. Finally, they can be used to \ncompute the potential benefits of data locality optimizafions in the limit. 4.1 Tools for Improving Data \nLocality We built a tool called DRILL (Data Reference Locality Locator) to help programmers improve a \nprogram's data reference locality. DRILL enumerates all of a program's hot data streams. Clicking on \na hot data stream displays its regularity magnitude (heat), spatial regularity (our inherent exploitable \nspatial locality metric), temporal regularity (our inherent exploitable temporal locality metric), and \nits cache block packing e ff~'cienc3~ (our realized exploitable locality metric). In addition, DRILL \ndisplays the program source responsible for the reference to the stream's first data member in a code \nbrowser window. The hot data stream can be traversed in data member order to see the code and data structures \nresponsible for the stream references. We have used DRILL to improve the data reference locality of \na few programs, including boxsim [8], by hand. We focused on hot data streams with high heat and poor \ncache block packing efficiencies. Streams with poor cache block packing efficiencies indicate data objects \nthat should be in the same cache block but are currently in different blocks. We attempted to co-locate \nthese data objects in the same cache block by modifying structure definitions to reorder fields, splitting \nstructures, and merging split portions of different structures. Preliminary results appear promising \nas our transformations improved execution time by 8-15%. Measurements of the optimized programs confirmed \nthat these improvements were due to improved cache block packing efficiencies. 4.2 Implementing Data \nLocality Optimizations A program's data reference locality can be improved by changing the order in which \nit accesses data (i.e., its inherent locality), or by changing the mapping of data objects to memory \naddresses (i.e., its realized locality). While changing the order in which a program accesses data has \nbeen used to improve the locality of dense matrix codes that access arrays, it is not a practical optimization \ntechnique for pointer-based data structures [5]. However, prefetching, which changes the order in which \nthe memory system sees data requests, can be used to improve data locality by tolerating memory access \nlatency [14, 19]. Techniques for changing a program's mapping of data objects to memory addresses include \nclustering, coloring, and compression [5]. Of these, compression typically requires source code modification \nand is not considered further. Coloring reduces cache conflict misses caused by multiple blocks mapping \nto the same cache location. Higher associativity caches--4 way, 8 way and higher associativity caches \nare becoming increasingly com- mon-reduce the benefit of this optimization. Thus, we focus on clustering \nas the primary data layout optimization for improving locality. We briefly outline how our exploitable \nlocality metrics, hot data stream abstraction, and data reference representations can be used to identify \ndata locality optimization targets, select the most effective optimization combination for a given target, \nand drive the selected optimization. The discussion is preliminary as this is a topic of current research. \n4.2.1 Identifying Data Locality Optimizations Targets The locality metrics described in Section 2 can \nbe used to identify data locality optimization targets. The best optimization targets are long hot data \nstreams that are not repeated in close succession, and that have poor cache block packing efficiency. \nShort hot data streams are indicative of poor inherent exploitable spatial locality and limit the benefit \nof any data locality optimization. Hot data streams that are repeated in close succession are likely \nto be cache resident on subsequent accesses and consequently unlikely to ben- efit from data locality \noptimizations. Poor cache block packing efficiency signifies that a hot data stream occupies a larger \nnumber of cache blocks than are strictly necessary. 4.2.2 Selecting Data Locality Optimizations Clustering \nand prefetching have different strengths and weak- nesses. In addition, we distinguish between two types \nof prefetch- ing--intra-stream prefetching, which fetches the data members of the stream being currently \naccessed, and inter-stream prefetching which fetches the data members of a stream that is different from \nthe one being currently accessed. Clustering alone is less effective for hot data streams with poor exploitable \ntemporal locality when the improvement in cache block packing efficiency is insufficient to make the \nstream's data members cache resident. In addition, in the absence of continuous reorganization, clustering \ncannot address competing layout constraints caused by data objects belonging to multiple hot data streams. \nPrefetching, on the other hand, can address both these shortcomings, but requires intelligent scheduling \nto be effective. In addition, it can increase a program's memory bandwidth requirements. Given these \nconstraints, cluster- ing should be used for hot data streams with poor cache block packing efficiency \nto enforce the dominant data layout. Inter- stream prefetching should be used for hot data streams with \npoor exploitable temporal locality. Finally, intra-stream prefetching should be used for those streams \nwith good exploitable spatial locality that have poor cache block packing efficiency even after clustering \ndue to competing layout requirements. 4.2.3 Driving Data Locality Optimizations In addition to guiding \ndata locality optimizations, such as prefetching, and clustering, our data reference representations \nand hot data streams can be used to drive these optimizations. For prefetching optimizations, hot data \nstreams supply an ordered list of data addresses to be fetched. For inter-stream prefetching, the SFG \ncan be analyzed to determine candidate pairs. In addition, dominators in the SFG suggest program load/store \npoints to initiate prefetching. Clustering optimizations use an object affinity graph to determine objects \nthat need to be co-located [4]. The SFG can be used as a more precise replacement for the object affinity \ngraph~ We quantified the potential benefit of hot data stream-based locality optimizations for several \nprograms. The detailed results are reported in Section 5.4. They indicate that locality optimizations \nbased on hot data streams appear promising, and can produce cache miss rate reductions of up to 92%. \nIn addition, preliminary results for an initial implementation of a hot data stream-based prefetching \noptimization indicate cache miss rate improvements of 15-43% for three benchmarks when different data \nreference profiles were used as train and test profiles [7].  5. EXPERIMENTAL EVALUATION This section \npresents results from applying our techniques to sev- eral programs and demonstrates that they produce \ncompact repre- sentations of data reference behavior that support efficient analysis. We show that our \nhot data stream abstraction is useful for mark programs. Finally, we compute the potential benefit of \ndata locality optimizations guided by our representations and abstrac- tions. 5.1 Experimental Methodology \nThe programs used in this study include several of the SPECint2000 benchmarks, boxsim,a graphics application \nthat sim- ulates spheres bouncing in a box [8], and Microsoft SQL server 7.0, a commercial database. \nThe benchmarks (and the standard libraries) were instrumented with Microsoft's Vulcan tool to pro- duce \na data address trace along with information about heap alloca- tions. Vulcan is an executable instrumentation \nsystem similar to ATOM [23]. Stack references were not instrumented to avoid bias- ing the data reference \nlocality results. In addition, they typically exhibit good locality and are seldom data locality optimization \ntar- gets. For experimentation purposes, the traces were written to a file, rather than processed on-line. \nEach data reference occupied 9 bytes in the trace (one byte encodes the reference type and the pro- gram \ncounter and data address each occupy four bytes). The heap allocation information was processed to build \na map of heap objects. A heap object is a <Start address, global counter> pair, where the global counter \nis incremented after each allocation. We used this naming scheme to achieve maximum discrimination between \nheap objects. Heap addresses in the trace were replaced by their corresponding heap object name. The \nabstracted trace was fed to SEQUITUR, which produced a context-free grammar. The DAG representation of \nthis grammar (i.e., the WPS0) was analyzed to identify hot data streams 0. These hot data streams 0 were \nused in conjunction with the WPSo to construct the SFG 0 representation. The process was repeated as \ndescribed in Section 3 to construct the WPS! representation. Finally, the WPS 1 was analyzed to identify \nhot data streams I and these were used to construct the Stream Flow Graph 1 (SFGi). Measurements were \nperformed on a dual proces- sor 550 Mhz Pentium III Xeon PC with 512 MB of memory run- ning Windows 2000 \nServer. The SPEC benchmarks were mn with their smallest input data set (test) with the exception of eon \nwhich was run with the larger train input set. boxsimwas used to simulate 100 bouncing spheres. The SQL \nserver measurements were per- formed while running the TPC-C benchmark. This is an on-line transaction \nprocessing benchmark that consists of a mix of five concurrent transactions of different types and complexity. \nThe Table 1: Benchmark characteristics Addresses Refs. Heap Global Refs./ Benchmark millions millions \nmillions heap+ Address global 176.gcc 464.7 125.0 146.1 22,647 11,972 197.parser 1,255.7 516.4 530.5 \n9,977 104,929 252.eon 1,784.0 165.4 152.5 18,744 16,961 253.perlbmk 112.1 36.3 27.4 26,715 2,385 255.vortex \n3,384.4 778.4 637.3 200,810 7,050 300.twolf 91.6 39.3 27.6 17,770 3.764 boxsim 183.4 60.1 43.6 75,677 \n1,371 quantifying data reference locality and can guide data locality opti- SQL server 279.2 139.4 39.8 \n1,606,890 112 mizations by using it to compute locality metrics for our bench- 1,000,000,000 Ill 1,000,000 \n1,000 i~ ,~ [] Trace iM WPSO [] WPS1 [] SFGO El SFG1 Figure 5. Relative sizes of the different data \nreference representations. benchmark runs for a fixed length of time, in this case a short (non- standard \nrun) of 60 seconds. Unlike the SPEC benchmarks, which are single threaded, SQL executes many threads. \nThe current sys- tem distinguishes data references between threads and constructs a separate WPS for \neach one. Table 1 reports some overall character- istics of the benchmark's data references. The last \ncolumn indi- cates the average number of references to each global and heap data address.  5.2 Evaluating \nthe Representations and Abstractions Figure 5 reports the sizes of the different data reference representa- \ntions. The WPS 0 and WPS 1 size is the size of the ASCII grammar produced by SEQUITUR (the binary representation \ncan be two times smaller), and the SFG o, SFG 1 size is the size of the respec- tive Stream Flow Graphs. \nThe WPS0s are one to two orders of magnitude smaller than the data reference traces, on average. The \nWPS 1 and SFG o, SFG 1 offer an additional order of magnitude size reduction, reducing a gigabyte trace \nto a representation that occu- pies a few megabytes or even a few hundled kilobytes. These small representations \npermit in-memory trace processing. Most promis- ingly, in all cases the WPS0s were small enough to permit \nefficient processing (avoiding the need to use the WPS 1 or SFG representa- tions, which are lossy representations \nof the data reference trace), with the analysis time for hot data stream identification ranging from \na few seconds to a minute. Hence, all following results are computed on the WPS 0 representation unless \nnoted otherwise. In addition, the compression is a measure of the regularity in a pro- gram's data reference \nstream and the high compression ratios sug- gest a large amount of inherent exploitable reference locality. \n The hot data stream analysis requires three parameters--the mini- mum and maximum length of a hot data \nstream, and the threshold above which a data stream is marked as \"hot\". We set the mini- mum hot data \nstream length at 2 and the maximum length at 100 since it is unclear whether there is significant opportunity \nfor exploiting longer streams and our data (not shown) indicated that only a few streams are longer than \nthis value. Because the goal is to use hot data streams for locality optimizations, we would like a large \nmajority of the data references in the trace (at least 90% of references) to belong to hot data streams, \nfor the optimization to have a significant impact on overall program locality. However, if this requires \nsetting the threshold so low that it produces an extremely large number of hot data streams, the optimization \nopportunity becomes less attractive. To investigate this issue, we defined our threshold for identifying \nhot data streams as follows--it is the smallest value at which over 90% of data references participate \nin hot data streams. If a pro- gram's references were uniformly distributed over its address space, each \naddress would be accessed (total references)/ (total addresses) times. We use multiples of this 'unit \nuniform access' for each program to report the threshold. This normalization process permits comparison \nacross the programs, independent of the num- ber of data references in the trace. Greater the threshold, \nhigher the program's data reference regularity. Since the threshold value rep- resents a plausible empirical \nmeasure of exploitable reference locality, we refer to it as 'exploitable locality threshold' or just \n'locality threshold'. Table 2 reports the 'locality threshold' for the various programs as well as hot \ndata stream information at this 'locality threshold'. The locality thresholds range from 1 for 176.gcc, \nto 126 for 252.eon. The table indicates that SQL server has a significantly larger num- Table 2: Hot \ndata stream information. Locality Number # of % of total threshold distinct Benchmark ('unit uniform \nof hot addresses programdata data access' in hot data streams addresses multiple) streams 176.gcc 1 7,461 \n3,912 17.27 197.pars er 69 i 105 14 0.14 i 252.eon 126 60 80 0.43 253.perlbmk 58 228 181 0.68 255.vortex \n75 475 250 0.12 300.twolf 5 1,260 419 2.36 boxsim 4 3,896 1,701 2.25 SQLserver 8 77,319 32,616 2.03 \n100 90 80 176.gcc 70 197.parser 60 .... 252.eon \"0 50 253.perlbmk 40 255.vortex 0 30 20 .....:: 300.tw \noil boxs im 10 0 ,,,-~ SQLserver 0 20 40 60 80 100 hot data stream size Figure 6. Cumulative distribution \nof hot data stream sizes. ber of hot data streams than any of the other benchmarks, probably 5.3 Using \nMetrics to Quantify Locality reflecting its more complex data reference behavior. In addition, Two factors \ncontribute to the regularity magnitude or 'heat' of a SQL server has a higher number of distinct data \naddresses that par- data stream. One is its spatial regularity, which is the number of ticipate in hot \ndata streams. However, when viewed as a percent- references it contains, and the other is its regularity \nfrequency, age of the total number of data addresses accessed during program which is the number of non-overlapping \noccurrences of the stream. execution, the number is comparable to the other benchmarks, and Another hot \ndata stream characteristic is its temporal regularity, at 2.03% is quite small. The sole exception is \n176.gcc, where data which is the average reference distance between consecutiveaddresses that participate \nin hot data streams comprise 17.3% of all stream occurrences. As described in Section 2.4, these characteris- \ndata addresses. These results suggests that commercial applica- tics can be used to compute exploitable \nlocality metrics. We per- tions such as SQL server, despite their richer data reference behav- formed \nexperiments to compute these metrics and the data is ior, possess regularity in their data reference \nstream much like the presented in Figures 6, 7 and summarized in Table 3. Figure 6 SPEC benchmarks. In \naddition, the results indicate that setting the illustrates the cumulative distribution of hot data stream \nsizes (i.e., \"heat\" threshold so that 90% of program data references participate spatial regularity), \nwhich is our inherent exploitable spatial locality in hot data streams, does not produce an excessive \nnumber of hot metric. The programs divide into three classes, with 176.gcc and data streams, and the \nresulting hot data streams include only a SQLserver in one class, boxsim in a category by itself, and \nthe rest small fraction of all program data addresses. This is encouraging of the benchmarks in a third \nclass. 176.gcc and SQLserver have for data locality optimizations based on the hot data stream relatively \nthe worst inherent exploitable spatial locality as 90% of abstraction. Finally, examination of the program \nsource associated their hot data streams are less than 20 references long. On the other with these hot \ndata streams indicate that while some streams occur hand, boxsim has the best inherent exploitable spatial \nlocality with in loops, many span procedure and data structure boundaries, a fairly uniform distribution \nof hot data stream sizes from 2 to 100. exposing potentially new optimization opportunities. 100 \"g .c \n90 80 70 60 so 40 30 176.gcc I 197.parser + 252.eon 253.perlbmk ..... ~i::, ,, 255.vortex ----X---. 300.tw \noff 20 boxsim 10 .. SQLserver 0 0 20 40 60 80 100 Cache block packing efficiency (64 byte blocks) Figure \n7. Cumulative distribution of hot data stream cache block packing efficiencies. ............ 199 The \nother benchmarks tie in between these extremes with a slight ent exploitable spatial locality. bias towards \nshort hot data streams. Table 3 shows weighted aver- age hot data stream sizes, where the \"heat\" (i.e., \nregularity magni- tude) of a data stream is used as its weight, so hotter data streams have a greater \ninfluence on the reported average value. As expected, 176.gcc and SQLserver have the smallest average \nhot data stream size. While boxsim has the longest streams, its weighted average is not significantly \ngreater than the other bench- marks, indicating that its \"hottest\" streams are small. In addition, 255.vortex \nexhibits a similar behavior wherein its weighted aver- age hot data stream size is smaller than Figure \n4 indicates. From a cache optimization standpoint, the data is encouraging as stream sizes are sufficiently \nlong to guide cache-conscious layouts or direct prefetching. Due to space constraints, we omit the graph \nfor cumulative distri- bution of average hot data stream repetition intervals (i.e., tempo- ral regularity), \nwhich is our inherent exploitable temporal locality metric. Instead, we present the weighted average \nhot data stream repetition intervals, expressed in terms of references, in Table 3. The weighted average \nnumbers reported are similar to the omitted graph with the exception of boxsim, which appears to have \nits \"hottest\" data streams repeat in closer succession than expected. Once again, the programs divide \ninto three categories with SQL server, 176.gcc, and 300.twolf in one class, boxsim and 253.per- lbmk \nin a second class, and 197.parser, 252.eon, and 255.vortex in the third class. The first class of benchmarks, \nwhich includes SQLserver, have relatively poor inherent exploitable temporal locality. 253.perlbmk and \nboxsim have reasonable exploitable tem- poral locality. Finally, 197.parser, 252.eon, and 255.vortex \npossess hot data streams that exhibit good exploitable temporal locality. Interestingly, these results \ncorrelate well with number of hot data streams present in these benchmarks, with the exception of boxsim \n(see Table 2). Benchmarks with a large number of hot data streams tend to have the same stream repeat \nless often. The inherent exploitable locality metrics computed from hot data streams indicate that 176.gcc \nand SQLserver have relatively poor inherent exploitable spatial and temporal locality, boxsim has excellent \ninherent spatial locality but only fair temporal locality, 300.twolf has reasonably good inherent spatial \nlocality but poor temporal locality, and the rest of the benchmarks have relatively good inherent exploitable \ntemporal locality and reasonable inher- Table 3: Summary of inherent and realized locality metrics. \nWt. avg hot Wt. avg. Wt. avg cache Benchmark data stream repetition block packing size interval efficiency \n 176.gcc 10.3 4,575.4 51.7 197.parser 24.0 86.9 64.8 252.eon 18.4 47.9 66.4 253.perlbmk 23.1 334.8 31.0 \n255.vortex 11.5 92.8 36.1 300.twolf 23.9 847.7 39.8 boxsim 25.8 228.2 49.2 SQLserver 10.9 2,544.1 41.4 \nThe next set of experiments use hot data streams to compute our realized exploitable locality metric \nfor the benchmarks as outlined in Section 2.4.2. Figure 7 illustrates the cumulative distribution of \nthe hot data stream's cache block packing efficiency for 64 byte cache blocks. The shape of the curves \nis remarkably similar for all benchmarks and indicates that their hot data streams fall into two distinct \ncategories--a small fraction (5--35% of hot data streams) that have ideal cache block packing efficiency, \nand the remaining streams that have suboptimal cache block packing efficiency. This metric permits focusing \noptimization efforts on those hot data streams with the most potential for improvement. 176.gcc and 197.parser \nhave the best packing efficiencies and 253.perlbmk and boxsim, the worst. From the weighted average data \nin Table 3 it appears that the boxsim's and 252.eon's \"hottest\" data streams have better than expected \npacking efficiencies. However, on aver- age, the benchmark's hot data streams occupy 2 to 3 times the \nnumber of caches blocks that an ideal layout would require. This suggests that the cmTent mapping of \ndata objects to memory addresses does not do a very good job of exploiting the program's inherent reference \nlocality and promises significant benefits from data locality optimizations. Combining the inherent and \nrealized exploitable locality metric data for the benchmarks it appears that boxsim and 300.twolf, which \nhave good inherent spatial locality, fair/poor inherent tem- poral locality, and poor cache block packing \nefficiencies, would benefit most from data locality optimizations, while 197.parser and 252.eon, which \nhave good inherent temporal locality, would bene- fit the least.  5.4 Evaluating the Potential of Stream-Based \nData Locality Optimizations We now evaluate the hot data stream abstraction for data locality optimizations \nalong two dimensions. First, are the locality metrics computed using hot data streams useful \"for indicating \nthe potential of different optimizations? If so, they could be used to select the data locality optimization \nlikely to produce the greatest benefit. Second, how much improvement can we expect from locality opti- \nmizations based on the hot data stream abstraction? Before attempting to answer these questions, we address \na more basic issue. How much do we give up by focusing our optimization efforts on hot data streams exclusively? \nTo answer this, we mea- sured cache miss rates for a variety of cache configurations. Figure 8 shows \nthe proportion of caches misses to references that partici- pate in hot data streams as the cache miss \nrate increases. The graph indicates that if cache performance is a bottleneck (miss rate > 5%) then around \n80% of misses are attributable to hot data stream refer- ences, with the exception of 197.parser for \nwhich this number is only 30%. This is not surprising as it suggests that when hot data streams fit in \nthe cache, most misses are to cold addresses and the cache miss rate is low. For programs with a cache \nperformance problem, their higher cache miss rates indicate a large number of misses to hot data stream \nreferences, making these streams suitable optimization targets. Figure 9 attempts to answer the previous \ntwo questions about the ability of hot data streams-based metrics to select data locality optimizations \nand the benefits of optimizations that target hot data streams. We first compute the potential impact \non cache miss rate of an ideal prefetching scheme based on hot data streams that is able to schedule \nall prefetches sufficiently in advance of their use such that the data is cache-resident when referenced \n(we ignore misses that occur when the data is prefetched, since these do not E 100 9O 8O 7O ~ 6o ~ 50 \n\"~ 40 \u00b0~ E 20 o 10 0 5 10 15 Cache miss  176.gcc + 197.parser 252.eon 253.perlbmk .---X-- 255.vortex \n+ 300.tw off I ......... boxsim ._4-- SQLserver 20 rate 25 30 35 Figure 8. Fraction of cache misses \ncaused by hot data streams. affect access latency if the prefetch is scheduled sufficiently in advance). \nSecond, we compute the effect .of using hot data streams to cluster objects as mentioned in Section 4. \nFinally, we compute the effect of combining the two optimizations. The results are com- puted for a 8K \nfully-associative cache with 64 byte blocks (we scaled the cache size since we used the SPEC benchmark's \ntest inputs) and are normalized to the base cache miss rate. As expected from the locality metric data, \nboxsim and 300.twolf benefit the most from locality optimizations and 197.parser (also see Figure 8), \n252.con, and 253.vortex, benefit the least. In addition, cluster- ing appears to be less effective for \n176.gcc, and SQLserver, possi- ble due to their complex data reference behavior that suggests multiple \ncompeting layouts. However, with the exception of 197.parser (see Figure 8), locality optimizations based \non hot data streams appear promising, producing cache miss rate reductions of 64-92%. Admittedly, these \nare ideal miss rate improvements. In a practical implementation the prefetch scheduling would not be \nper- fect. In addition, program constraints may prevent a faithful imple- mentation of the clustering \nscheme suggested by hot data streams. Nevertheless, the large cache miss rate improvements indicate that \npractical implementations of these hot data stream-based locality optimizations merit further consideration. \n100 8O om E 6O o o 40 20 6. RELATED WORK Larus used a hierarchical compression algorithm (SEQUITUR) \nto construct Whole Program Paths (WPP), which are a compact, yet analyzable representation of a program's \ndynamic control flow [17]. This paper demonstrates that a similar scheme can serve as an effective representation \nof a program's dynamic data reference behavior. Together, they provide a complete picture of a program's \ndynamic execution behavior. Compressing program traces has been the subject of much research. For example, \nPlezkun described a two-pass trace com- pression scheme, which infers basic block's successors and tracks \nlinear address patterns.This is used to produce a variable-length encoding that compresses address traces \nto a fraction of a bit per reference [21]. Larus's Abstract Execution scheme guides the re- execution \nof the address generating slice of a program with a small amount of run-time data [18]. While these techniques \nproduce con- siderable compression, the encoded files are not as analyzable as WPSs, and they require \nsignificant post-processing to regenerate the address trace. In any case, the focus of this research \nis not on compression but on efficient representations and abstractions for analysis and optimization \nof dynamic data reference behavior. [] Base il Pref etching I IZ] Clustering B Pref. + Clustering 0 \n,+/ ++++ j++++o+ +o;+,+ j Figure 9. Potential of locality optimizations based on the hot data stream \nabstraction. 201 7. CONCLUSIONS With the growing processor-memory performance gap, understanding and \noptimizing a program's data reference locality, and consequently, its cache performance, is becoming \nincreasingly important. The paper address this by proposing a quantitative basis for understanding and \nimproving reference locality. It describes data reference representations--Whole Program Streams, Stream \nFlow Graph--and an exploitable locality abstraction--hot data streams--that support this framework. The \npaper demonstrates that these data reference representations are compact and can be used to efficiently \ncompute the hot data stream abstraction. In addition, it shows that hot data streams are useful for quantifying \nas well as exploiting data reference locality. The results reported in this paper suggest significant \nopportunity for hot data stream-based locality optimizations. 8. ACKNOWLEDGMENTS Jim Lares generously \nprovided his implementation of the SEQUITUR algorithm as well as code for detecting hot subpaths in Whole \nProgram Paths. Tom Ball, Ras Bodik, Jim Larus, Scott McFarling, Ben Zorn and the anonymous referees provided \nuseful comments on earlier drafts of this paper. 9. REFERENCES [1] G. Ammons and J. R. Larus. \"Improving \ndata-flow analyses with path profiles.\" In Proceedings of the ACM SIGPLAN'98 Conference on Programming \nLanguage Design and Implemen- tation, pages 72-84, 1998. [2] R. Bodik, R. Gupta, and M. L. Sofia. \"Redefining \ndata flow in- formation using infeasible paths.\" In Proceedings of the ACM SIGSOFT Fifth Symposium on \nthe Foundations of Software En- gineering, May 1997. [3] B. Calder, C. Krintz, S. John, and T. Austin. \n\"Cache-conscious data placement.\" In Proceedings of the Eighth International Conference on Architectural \nSupport for Programming Lan- guages and Operating Systems (ASPLOS VIII), pages 139-149, Oct. 1998. [4] \nT. M. Chilimbi, and J. R. Larus. \"Using generational garbage collection to implement cache-conscious \ndata placement.\" In Proceedings of the 1998 International Symposium on Memory Management, Oct. 1998. \n[5] T. M. Chilimbi, B. Davidson, andJ. R. Larus. \"Cache-conscious structure layout.\" In Proceedings of \nthe ACM SIGPLAN'99 Conference on Programming Language Design and Implemen- tation, May 1999. [6] T. M. \nChilimbi, B. Davidson, and J. R. Larus. \"Cache-conscious structure definition.\" In Proceedings of the \nA CM SIGPLAN'99 Conference on Programming Language Design and Implemen- tation, May 1999. [7] T. M. Chilimbi. \n\"On the stability of temporal data reference pro- files.\" In Microsoft Research, Technical Report MSR-TR-2001- \n43, Apr. 2001. [8] S. Chenney. \"Controllable and scalable simulation for anima- tion.\" Ph.D. Thesis, \nUniversity of California at Berkeley, 2000. [9] C. Ding and K Kennedy. \"Improving cache performance in \ndy- namic applications through data and computation reorganiza- tion at run time.\" In Proceedings of \nthe ACM S1GPLAN'99 Conference on Programming Language Design and Implemen- tation, pages 229-241, May \n1999. [10] J. A. Fisher. \"Trace Scheduling: A technique for global micro- code compaction.\" In IEEE Transactions \non Computers, voI. C- 30, pages 478-490, 1981. [11] N. Gloy, T. Blackwetl, M. D. Smith, and B. Calder. \n\"Procedure placement using temporal ordering information.\" In Proceed-ings of the 30th Annual A CM/IEEE \nInternational Symposium on Mic~varchitecture, 1997. [12] R. Gupta, D. A. Berson, and J. Z. Fang. \"Path \nprofile guided partial dead code elimination using predication.\" In Proceed-ings of the International \nConference on Parallel Architecture and Compilation Techniques (PACT), 1997. [13] J. L. Hennessy and \nD. A. Patterson. \"Computer Architecture: A quantitative approach, Second Edition.\" Morgan Kaufmann Publishers, \nSan Mateo, CA, 1995. [14] M. Karlsson, F. Dahlgren, and P. Stenstrom. \"A prefetching technique for irregular \naccesses to linked data structures.\" In Symposium on High-Performance Computer Architecture, Jan. 2000. \n[15] T. Kistler and M. Franz. \"Automated record layout for dynam- ic data structures.\" In Department \nof Information and Computer Science, University of California at Irvine, Technical Report 98-22, May \n1998. [16] D. E. Knuth. \"An empirical study of FORTRAN programs.\" In Software--Practice and Experience, \nvol 1, pages 105-133, 1971. [17] J. R. Larus. \"Whole program paths.\" In Proceedings of the ACM S1GPLAN'99 \nConference on Programming Language Design and Implementation, pages 259-269, May 1999. [18] J. R. Larus. \n\"Abstract Execution: A technique for efficiently tracing programs.\" In Software--Practice and Experience, \nvol 20, pages 1241-1258, 1990. [19] C-K. Luk and T. Mowry. \"Compiler-based prefetching for re- cursive \ndata structures.\" In Proceedings of the 7th International Conference on Architectural Support for Programming \nLan- guages and Operating Systems (ASPLOS VII), Oct. 1996. [20] C. G. Nevill-Manning and I. H. Witten. \n\"Linear-time, incre- mental hierarchy inference for compression.\" In Proceedings of the Data Compression \nConference (DCC'97), 1997. [21]A. R. Plezkun. \"Techniques for compressing program address traces.\" In \nProceedings of the 27th Annual A CM/IEEE Interna- tional Symposium on Microarehitecture, pages 32-40, \n1994. [22] M. L. Seidl, and B. G. Zorn \"Segregating heap objects by ref- erence behavior and lifetime.\" \nIn Proceedings of the Eight In- ternational Conference on Architectural Support for Programming Languages \nand Operating Systems (ASPLOS VI- II), pages 12-23, Oct. 1998. [23] A. Srivastava and A. Eustace. \"ATOM: \nA system for building customized program analysis tools.\" In Proceedings of the ACM SIGPLAN'94 Conference \non Programming Language Design and Implementation, pages 196-205, May 1994. [24] D. Truong, F. Bodin, \nand A. Seznec. \"Improving cache behavior of dynamically allocated data structures.\" In Proceedings of \nthe International Conference on Parallel Architecture and Compilation Techniques (PACT), 1998.    \n \n\t\t\t", "proc_id": "378795", "abstract": "<p>With the growing processor-memory performance gap, understanding and optimizing a program's reference locality, and consequently, its cache performance, is becoming increasingly important. Unfortunately, current reference locality optimizations rely on heuristics and are fairly ad-hoc. In addition, while optimization technology for improving instruction cache performance is fairly mature (though heuristic-based), data cache optimizations are still at an early stage. We believe the primary reason for this imbalance is the lack of a suitable representation of a program's dynamic data reference behavior and a quantitative basis for understanding this behavior.</p><p>We address these issues by proposing a quantitative basis for understanding and optimizing reference locality, and by describing efficient data reference representations and an exploitable locality abstraction that support this framework. Our data reference representations (Whole Program Streams and Stream Flow Graphs) are compact&#8212;two to four orders of magnitude smaller than the program's data reference trace&#8212;and permit efficient analysis&#8212;on the order of seconds to a few minutes&#8212;even for complex applications. These representations can be used to efficiently compute our exploitable locality abstraction (hot data streams). We demonstrate that these representations and our hot data stream abstraction are useful for quantifying and exploiting data reference locality. We applied our framework to several SPECint 2000 benchmarks, a graphics program, and a commercial Microsoft database application. The results suggest significant opportunity for hot data stream-based locality optimizations.</p>", "authors": [{"name": "Trishul M. Chilimbi", "author_profile_id": "81100578606", "affiliation": "Microsoft Research, One Microsoft Way, Redmond, WA", "person_id": "P285175", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/378795.378840", "year": "2001", "article_id": "378840", "conference": "PLDI", "title": "Efficient representations and abstractions for quantifying and exploiting data reference locality", "url": "http://dl.acm.org/citation.cfm?id=378840"}