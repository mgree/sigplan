{"article_publication_date": "05-01-2001", "fulltext": "\n Asynchronous Exceptions in Haskell Simon Marlow and Simon Peyton Jones Andrew Moran Microsoft Research, \nCambridge Oregon Graduate Institute John Reppy Bell Labs, Lucent Technologies Abstract Asynchronous \nexceptions, such as timeouts, are important for robust, modular programs, but are extremely difficult \nto program with --so much so that most programming lan- guages either heavily restrict them or ban them \naltogether. We extend our earlier work, in which we added synchronous exceptions to Haskell, to support \nasynchronous exceptions too. Our design introduces scoped combinators for blocking and unblocking asynchronous \ninterrupts, along with a some- what surprising semantics for operations that can suspend. Uniquely, we \nalso give a formal semantics for our system. Introduction An important goal of language design is to \nsupport modu- larity. For concurrent languages, this goal means language support for localizing synchronization \nissues and support for composing components without interference. One concur- rent language feature that \nappears to be the antithesis of modularity is the asynchronous signaling (or killing) of one thread by \nanother. Since, by definition, such signaling can occur at any point in the target thread's execution, \nlocks held by the target may not be properly released and in- variants may not be maintained. For these \nreasons, few concurrent languages or thread libraries support truly asyn- chronous signalling, and those \nthat do have discouraged its use. There are situations, however, where allowing a thread to asynchronously \nsignal another thread is extremely useful. For example, we might wish to provide a timeout operator that \nlimits the execution time of a computation or we might wish to run two different computations in parallel \ntaking the first result and terminating the other. In this paper, we present an extension to Concurrent \nHaskell [14] that sup- ports true asynchronous signMling in a robust, modular way. The principM contributions \nof the paper are as follows: We explain why a fully-asynchronous signalling is both useful (as opposed \nto semi-asynchronous signMling) and feasible (Section 2). In fact, while imperative languages often use \npolling to implement a semi-asynchronous signalling mechanism, we explain that Permission to make digital \nor hard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for profit or commercial advan-tage and that copies bear this \nnotice and the full citation on the first page. To copy otherwise, to republish, to post on servers or \nto redistribute to lists, requires prior specific permission and/or a fee, PLDI 2001 6/01 Snowbird, Utah, \nUSA &#38;#169; 2001 ACM ISBN 1-58113-414-2101106...$5.00 signalling must necessarily be truly asynchronous \nin purely-functional languages like Concurrent Haskell. \u00ae We propose an extension to Concurrent Haskell \nthat supports asynchronous delivery of exceptions between threads (Section 5). This mechanism allows \none thread to terminate another. o Motivated by some subtle race conditions, we intro-duce a control \nmechanism for postponing the delivery of asynchronous exceptions, based around two scoped combinators, \nblock and unblock (Section 5.1). It is also necessary to allow indefinitely blocking operations to be \ninterrupted, we show how these mechanisms en- able us to acquire and release locks safely in the pres- \nence of asynchronous exceptions (Section 5.3). o We give an operational semantics for Concurrent Haskell \n(Section 6) and extend it with asynchronous exceptions (Section 6.3). A precise specification of ex- \nactly what asynchronous exceptions do is crucial for both programmers and implementors: asynchronous \nexceptions axe subtle beasts. We believe that this is the first formalization of an asynchronous exception \nor interrupt mechanism.  In addition, we give the definitions of some useful combi- nators built on \ntop of the low-level exception primitives, including a composable timeout combinator (Sections 7.4 and \n7); and we outline an implementation of asynchronous exceptions and the associated primitive operations \n(Section 8). 2 Asynchronous exceptions Many high-level languages provide exceptions as a way to support \nrobust handling of error conditions. Errors are sig- naled by throwing an exception and are handled by \ncatch-ing the exception. When we say \"exception,\" we normally mean \"synchronous exception\" in the sense \nthat an excep- tion can only be raised as a direct consequence of execut- ing the program itself. Examples \ninclude: divide by zero, pattern-match failure, and explicitly raising a user excep- tion. Synchronous \nexceptions are relatively tractable: The denotation, or meaning, of an expression says whether evaluating \nthe expression will raise a syn-chronous exception and, if so, specifies the set of ex- ceptions that \nmay be raised. In other words, the syn- chronous exceptions that an expression may raise is properly \npart of the semantics of that expression. It follows that a compiler can reasonably infer (an ap- proximation \nto) the set of synchronous exceptions that any given expression could possibly raise [21]. Since exceptions \nalready provide a control-flow mech- anism for signalling and handling exceptional conditions, it is \nnatural to consider extending the exception handling mechanism to include asynchronous exceptions. 1 \nSuch asyn- chronous exceptions are raised as the result of an \"external event,\" such as a signal from \nanother thread, and can oc- cur at any point during execution. Since the evaluation of any expression \ncould yield an asynchronous exception, we cannot sensibly consider asynchronous exceptions as part of \nthe semantics of the expression. This property makes asyn- chronous exceptions much less tractable, both \nsemantically and from a programmer's standpoint, than synchronous ex- ceptions. Nevertheless, there are \nseveral compelling reasons to sup- port asynchronous exceptions: Speculative computation. A parent thread \nmight start a child thread to compute some value speculatively; later the parent thread might decide \nthat it does not need the value so it may want to kill the child thread. Timeouts. If some computation \ndoes not complete within a specified time budget, it should be aborted. User interrupt. Interactive \nsystems often need to cancel a computation that has already been started, for example when the user clicks \non the \"stop\" button in a web browser. Resource exhaustion. Most Haskell implementations use a stack \nand heap, both of which are essentially finite resources, so it seems reasonable to inform the program \nwhen memory is running out, in order that it can take remedial action. Since such exceptions can occur \nat almost any program point, it is natural to treat them as asynchronous. A nffive approach to these \nproblems is to provide a mech- anism for one thread to kill another thread. While such a mechanism gets \nthe job done, it creates serious problems. If a thread is killed while it holds a lock, how does the \nlock get released? If the thread is in the process of mutating a shared data structure, how do we reestablish \nthe data structure's invariants? For these reasons, a simple kill mechanism is unacceptable. In practice, \nfew concurrent languages provide any mecha- nism for one thread to asynchronously signal another thread. \nMore common are semi-asynchronous mechanisms based on polling, where the target occasionally checks for \nsignals; or safe points, where the target accepts signals at certain des- ignated points. For example,to \nterminate a thread we might set a global flag, and rely on the thread to periodically check the flag, \nas is done in POSIX threads, Modula-3, and Java (Section 10 elaborates). While the semi-asynchronous \napproach avoids breaking synchronization abstrax~tions, it is non-modular in that the target code must \nbe written to use the signalling mechanism. Worse still (for us), the semi.asynchronous approach is sim- \nply incompatible with a purely-functional language, such as 1There are reasons why one might keep these \nnotions distinct, but they are orthogonal to the main points of the paper. See Section 9 for more discussion. \nConcurrent Haskell. The problem is that polling a global flag is not a functional operation, yet in a \nConcurrent Haskell program, most of the time is spent in purely-functional code. On the other hand, since \nthere is absolutely no problem with abandoning a purely-functional computation at any point, asynchronous \nexceptions are safe in a functional setting. In short, in a functional setting, fully-asynchronous exceptions \nare both necessary and safe -- whereas in an imperative con- text fully-asynchronous exceptions are not \nthe only solution and are unsafe. For these reasons, the semi-asynchronous approach is almost universal \nin imperative languages. All of the above motivations concern the premature abor- tion of a computation. \nWe do not deal with resumption, in which the interrupted computation can be resumed. We also do not deal \nwith killing rogue threads, since such threads can exploit our mechanisms to ignore asynchronous exceptions. \n3 Input/output in Haskell Haskell is a purely functional language with lazy semantics. Input/output in \nHaskell is done using a monad; in Haskell a value of type I0 a is an \"action\" that, when performed, may \ndo some input/output before delivering a value of type a. For example, here are two basic I/O functions2: \ngetChar :: IO Char putChar :: Char -> I0 ()  getChar is an I/O action that, when performed, reads a \ncharacter from the standard input, and returns it to the program as the result of the action, putChar \nis a function that takes a character and returns an action that, when performed, prints the character \non the standard output, and returns the trivial value (). I/O actions can be combined using the >>= operator: \n(>>=) :: 10 a -> (a -> IO b) -> I0 b  So, for example, we can make a compound I/O action that reads \na character from standard input and writes it to stan- dard output (\\x -> e is Haskell's notation for \nAx.e): getChar >>= \\c -> putChar c The action as a whole has type I0 (). Haskell also provides syntactic \nsugar, the do-notation, for expressing monadic combinations. The above expression could also be written \ndo { c <- getChar; putChar c } Most of the code examples in this paper will use this do- notation. For \na thorough introduction to I/O in Haskell, see [12]. 4 Concurrent Haskell Concurrent Haskell [14] extends \nstandard Haskell with a small set of primitives for creating new threads and per- forming simple inter-thread \ncommunication: forkIO :: IO a -> I0 ThreadId myThreadId :: IO ThreadId sleep :: In%-> I8 0 data MVar \na --abstract n'ewEmptyMVar :: IO (MVar a)  2The notation \"f : : t\" means \"f has type t\" putMVar :: MVar \na -> a -> IO () takeMVar :: MVar a -> IO a throw :: Exception -> I0 () catch :: IO a -> (Exception \n-> IO a) -> If] a  A new thread can be \"forked\" using forkIO, with the in- formal understanding that \nthe IO computation passed to forkIO may be arbitrarily interleaved with the current com- putation. Both \ncooperative and preemptive implementa- tions of Concurrent Haskell exist. The forkIO function re- turns \nthe Threadld of the forked thread. A thread can also obtain its own ThreadId by calling myThreadId. \nThreadIds support equality. Threads can sleep for a specified period of time (in microseconds) by calling \nsleep. MVars are a generic synchronization mechanism, similar to the M-structures of Id [3]. A value \nof type MVar t can be thought of as a box that can be in two possible states: empty or containing a value \nof type t. The takeMVar oper- ation waits if it finds the box empty, or removes and returns its contents \notherwise. The putMVar operation puts a new value in the box, waking up any threads that were waiting \nfor the MVar to become full, or waits if the MVar is already full s. Using only MVars, many complex datatypes \nfor concur- rent communication can be built, including typed channels, semaphores and so on [14]. A synchronous \nexception can be raised by throw, and caught by catch. The computation catch M H runs M. If M succeeds, \nthen its result is the result of the catch. If instead it raises an exception, the handler H is run, \npass-ing the exception raised by M. The types of throw and catch are identical to the ioError and catch \noperations in the Haskell 98 standard [13], except that we have enlarged the IOError type to Exception, \nto take account of non-IO exceptions. Asynchronous Exceptions in Haskell We are now ready to work on \nasynchronous exceptions. We start by adding a new primitive to Concurrent Haskell to enable one thread \nto asynchronously raise an exception in another, throwTo: throwTo :: ThreadId -> Exception -> IO 0  \nInformally, the meaning of throwTo t e is that the excep- tion e is raised in thread t as soon as possible, \nand the call returns immediately. In practice, the exception may not be delivered to the target thread \nuntil some time later, perhaps because it is running on another processor, or even another machine. If \nthe thread t has already died or completed, then throwTo trivially succeeds. Note that implementing throwTo \nis not as simple as it might seem: the target thread may be blocked, perhaps on an MVar, and will therefore \nhave to be woken up before the exception can be raised. The implementation of throwTo is covered in more \ndetail in Section 8. Note that throwTo is the only source of asynchronous exceptions in the system, but \nasynchronous interrupts from the environment may also be converted into asynchronous exceptions by the \nprogrammer. 3This semantics for putMV~is slightly different from that given in [14], where putMYar on \na full MYar was an error. 5.1 Safe Locking We now consider the issues raised by writing code in the presence \nof asynchronous exceptions. An example which illustrates a number of these issues is the use of locking \nto provide concurrency control for shared mutable state. It is important that asynchronous exceptions \nbe handled without leaving the shared mutable state in an internally inconsistent state. In Concurrent \nHaskell, shared mutable state is normally represented by an MVar, which holds the value of the current \nstate. Any thread wishing to access the state must first take the value from the MVar, leaving it temporarily \nempty, and put the new state back in the MVar afterwards. Hence only a single thread has access to the \nstate at. any one time. The problem with this approach is that if an exception is raised while the thread \nholds the MVar, it will be left in an empty state, and deadlock may ensue. To make this process safe \nin the presence of synchronous exceptions is straightforward: we simply arrange that should an exception \nbe raised while we are building the new value of the state, the old value is replaced in the MVar and \nthe ex- ception propagated to the caller. Ifm is the MVar in question, and compute is an IO operation \nthat takes the old state and returns the new state, then the code would look like this: do { a <- takeMVar \nm; b <- catch (compute a) (\\e -> do { putMVar m a; throw e }); putMVar m b } This is fine for synchronous \nexceptions, but in the pres- ence of asynchronous exceptions there is a race condition: an exception \ncan occur just after the takeMVar but before catch, when there is no exception handler in place to restore \nthe state. We could try to fix the hole by moving the catch around the takeMVar, but this opens another \nrace window: the exception could occur before the takeMVar, causing the putMVar which replaces the old \nstate to block forever on the still-full MVar (this is in addition to the fact that expanding the scope \nof the catch around the takeMVar makes it diffi- cult to propagate the value of the old state to the \nexception handler). 5.2 Blocking exceptions Clearly, some way to postpone the delivery of asynchronous \nexception s during critical sections is needed. The standard method is to disable interrupts, using two \noperations placed around the critical section: block :: IO () unblock :: 10 () The idea is that executing \nblock puts the thread into a state in which asynchronous exceptions are blocked, and unblock does the \nreverse. These combinators are still somewhat clumsy for our purposes though, as we can see if we try \nto use them to fix up the locking example: do { block; a <- takeMVar m; b <- catch (do { unblock; compute \na; block }) (\\e -> do { putMVar m a; throw e }); putMVar m b; unblock; }  Notice that we try to \nunblock asynchronous exceptions for the duration of the call to compute only. However, if an exception \nis raised during compute, whether synchronous or asynchronous, then control is passed to the exception \nhan- dler with asynchronous exceptions unblocked, which opens up another race window as there is the \npossibility that we can receive another exception before replacing the contents of the MVar. Also somewhat \nunsatisfactory is the fragility of the programming model: it is all too easy to forget to re-enable exceptions \nafter a critical section, especially if the control flow is complicated. A much better approach is to \nuse scoped combinators: block :: IO a -> IO a unblock :: IO a -> IO a block (do { a <- takeMVar m; \n b <- catch (unblock (compute a)) (\\e -> do { putMVar m a; throw e }); putMVar m b })  where the \nmeaning of (block a) is \"execute a in a state where asynchronous exception delivery is blocked,\" while \nunblock does the reverse. The block and unblock combi- nators may be arbitrarily nested. There is no \ncounting of scopes, i.e., two nested blocks behave the same as a single block. This is an important property \nfrom a modularity perspective: it means that unblock always unblocks asyn- chronous exceptions, regardless \nof the context. Notice how the scoping of block closes the race condition in the exception handler: if \nan exception is received during compute a, then we exit the scope of unblock and enter the exception \nhandler which is inside the blocked scope. In practical terms, the implementation of exception han- dling \nmust respect scopes. That is, it must save the current state of the thread (blocked or unblocked) when \nentering the scope of a block or unblock combinator, and restore it again when leaving that scope, whether \nnormally or by a synchronous or asynchronous exception being raised. This appears to solve our original \nproblem: there are now no windows of vulnerability during which the thread could terminate with the lock \nstill in hand. But there is a new problem, namely that if the thread has to wait for the lock, it now \nwaits in a blocked state. This is in violation of one of the cardinal rules of concurrent programming: \n\"do not block while holding a lock,\" the reason being that it increases the potential for deadlock. More \npractically, it means we cannot time-out the thread waiting for the lock until it acquires the lock. \n 5.3 Interruptible Operations How do we take the MVar, and atomically install an exception handler as \nsoon as we have the SVar? Our solution is a subtle change to the semantics of blocking operations: Any \noperation which may need to wait indefi- nitely for a resource (e.g., takeMVar) may receive asynchronous \nexceptions even within an enclosing block, but only while the resource is unavailable. Such operations \nare termed interruptible opera- tions. Wait a minute! Have we not just shot ourselves in the foot? Previously \nblock was sure protection from asyn-chronous interrupts, now it is not. Nevertheless, there are several \ngood reasons for adopting this approach: * Having made this modification to the semantics, takeMVar behaves \natomically when enclosed in a block. The takeMVar may receive asynchronous exceptions right up until \nthe point when it acquires the MVar, but not after. Without this change, takeMVar is unsafe to use inside \nblock at all. o Although it seems strange that operations inside a block may raise asynchronous exceptions, \nthe excep- tions are synchronous in nature since we specify exactly which operations are interruptible. \nThe code to acquire the MTar as given above works fine with this addition to the semantics, with the \ndifference that now the takeMVar operation is interruptible. Also note the careful wording of the definition \nabove: by implication it states that an interruptible operation cannot be interrupted if the resource \nit is attempting to acquire is always available. Looking back at our locking example from the previous \nsection, even though we used putMVar, an interruptible operation, in the exception handler, in this case \nthe putMVar is non-interruptible because we can be sure the MVar is always empty. 6 Operational Semantics \nIn this section we give an operational semantics for Con- current Haskell with exceptions, a and then \nproceed to add in our new features for asynchronous exceptions. We have so far introduced the new constructs \nusing informal defini- tions; the semantics in this section precisely specifies the intended meanings. \nOur semantics is stratified in two lev- els: an inner denotational semantics describes the behaviour \nof pure terms, while an outer monadic transition semantics describes the behaviour of I0 computations. \nM and N range over terms in our language, and V ranges over values (Figure 1). A value is a term that \nis considered by the inner, purely-functional semantics to be evaluated. The values in Figure 1 include \nconstants and lambda ab- stractions, as usual, but are unusual in two ways: \u00ae We treat the primitive \nmonadic I0 operations as values. For example, putChar 'c' is a value. No further work can be done on \nthis term in the purely-functional world; it is time to hand it over to the outer, behavioural semantics. \nIn the same way, M>>=N, sleep 3, and return M are all values. * Many of these monadic I0 values have \narguments that are not arbitrary terms (M, N, etc.), but are themselves values (m, c, d, etc.). For example, \nputChar (chr 65) is not a value, but putChar 'A' is. It is as if putChar is a strict data constructor. \nThe reason for this choice is that evaluating putChar's argument is indeed some- thing that can be done \nin the purely-functional world; indeed, it must be done before the output operation can take place. 4There \nexists a published operational semantics for Concurrent Haskell [14], a denotational semantics for exceptions \nin Haskell [15], and an operational semantics for exceptions in Haskell [11], but so far no published \nsemantics links these concepts or describes exceptions in the I0 monad. ................... 277  x, \ny k E E Variable Constant c E Constructor ch E Char d E Integer e E Exception m E MVar t, U E ThreadId \nValues V ::----xl\\x->M[ktcM1 \"\"Mn ] chidlelmltl return M]M >>= N] putChar ch I getChar I putMVar m N \n] takeMVar m I newEmptyMVar I sleep d I throw e ] catch M H I Terms M, N, H ::= VIMNIiZMt~enN~elseN21 \n' Figure 1: The syntax of values and terms.: P, Q, R ::= ~M~t thread of computation named t finished \nthread named t I 0m empty MVar named m I (M)~ full l~/ar named m, holding M I ux.P restriction 1 PIQ \nparallel composition Figure 2: The syntax of program states. In the following sections, we confuse \nm :: MVar with the name of that MVar, and t : : ThreadId with the name of the thread. We also treat MVar \nand thread names as normal variables (i.e., they may be bound and a-converted).  6.1 Program Transitions \n We give the semantics by describing how one program state evolves into a new program state by making \na transition. A program state consists of a collection of threads and MVars in parallel, see Figure 2. \nThe transition from one program state to the next may or may not be labelled by an event, oL. We write \na transition like this: P-~Q The events a represent communication with the external environment; that \nis, input and output. We will use just three events: * P !c~ Q means \"program state P can move to Q, \nby writing the character ch to the standard output\". . p ?oh Q means \"program state P can move to Q, \nby reading the character eh from the standard input\". P IQ ~-QIP (Comm) P I(QIR) ~ (PIQ) i R (Assoc) \nvx.uy.P -: uy.ux.P (Swap) (ux.P) I Q =-ux.(P [ Q), x ~ fn(Q) (Extrude) ux.P ~ uy.P[Y/x ], y ~ fn (P) \n(Alpha) P -2+ Q (Par) P ~ Q (Nu) PIR ~ QIR ux.P ~ ux.Q pup' p' __~ Q' P~Q Q'~_Q (Equiv) Figure 3: Structural \ncongruence and structural transitions. , P ~ Q means \"program state P can move to Q, when an amount of \ntime t has elapsed\". In the standard way [14], we define a structural equiv- alence over processes, formalizing \nthe idea of a \"solution\" of processes a la the chemical abstract machine [9]. Let be the least congruence \n(i. e., equivalence relation preserved by all process contexts) that also satisfies the (standard) rules \nin Figure 3 and contains alpha equivalence. Rules (Par) and (Nu) allow transitions within parallel composi- \ntions and inside restrictions respectively. The equivalence rules, (Comm), (Assoc) etc., say that ] is \nassociative and commutative and that the scope of v can be restricted or expanded as long as it does \nnot interfere with any exist- ing scopes, while (Equiv) says that we are free to use these equivalence \nrules to bring parts of the program state to-gether.  6.2 Transition Rules Transition rules for the \nstandard I0 and concurrency oper- ations are given in Figure 4. Transitions take place within evaluation \ncontexts, where an evaluation context is defined as E ::= [.lie >>= M I catch E M That is, to find the \nevaluation site, repeatedly look inside the first argument of >>= and catch. The rules in Figure 4 are \nstandard in form, so we describe them only briefly: Sequencing of I0 operations is handled by >>=. When \nits left operand becomes a return, rule (Bind) passes the returned value on to >>=% right operand. Input/output. \nThe canonical I0 operations are putChar and getChar, described in (PutChar) and (GetGhar). Other basic \nI/O operations, like openFile, have anal- ogous semantics. Rule (Sleep) deliberately underspec- ifies \nsleep. Here, the $d label represents an external clock interrupt, indicating that d microseconds have \npassed since sleep d first became blocked. A cor-rect implementation must guarantee that at least d microseconds \nhave passed before a thread executing sleep d is woken; further delay is acceptable. 278 dE[return N \n>>:MID t IE[putCh~r oh]De IE[getChar] ~t -Z-c-h~IE[return eh]~t  IE[sleep d]Dt 0m I IE[putMVar m \nM]Dt --+ (M)m liE[return ()]Dr <>mliE[re*urn M]Dt E[n.~ptyMWr]D m it In (E) IE[forkXO M]Dt -~ .u.(IE[return \nu]Dt I IMD~), u \u00a2/n(E,M) ~E[~yThr~dld]D~ ~ IE[~t~n t]D~ GEl'throw e>>: M]~t ~ dE[throw e]~t dE[catch \n(return M)H]Dt --). ~E[return M]~t. ~return MI)t -* O~ Ithrow eDt ~ Ot Omain [ P ---+ Om~in  M:: I0 \na M~V M~V M::I0 a MJ'e ~E[M]D~ ~ ~[Y]Dt (E~al) ~E[M]D~ ~ ~E[t~o~ ~]~ (Raise) Figure 4: Transition Rules \nfor Concurrent Haskell (without asynchronous exceptions). (Bind) ( PutChar ) (GetChar) ( stc,~p ) (PutMVar) \n(TakeMVar) ( NewMVar) (Fork) (Threadld) (Propagate) (Catch) (Handle) (Return GC) ( Throw GC) (Proc GC) \n MVar operations are described by rules (PutMVar), (TakeMVar) and (NewMVar). (Recall from Figure 2 that \n()r~ represents an empty MVar, while (M),~ repre- sents a full MVar containing M.) Note that if takeMVar \nfinds an empty MVar, no transition can take place; this is how a stuck thread is modeled in the semantics. \nSim- ilarly for putMVar: when the MVar is full, the thread cannot make any further transitions. Forking \na new thread is described by rule (Fork), while (ThreadId) allows a thread access to its own ThreadId. \nSynchronous exceptions raised by throw are propagated by >>= (Propagate), and caught by catch (Catch). \nRule (Handle) explains how catch behaves when the computation it protects succeeds. Termination. Rules \n(Return GC) and (Throw GC) state that final return values and uncaught exceptions are lost, while (Proe \nGC) says that once the main thread is finished, all other threads will eventually die. These rules are \nenough if there is a value at the evaluation site. But sometimes there is not --for example, after a \nuse of rule (Bind) the evaluation site is an application, which will not match any of the rules described \nso far. Of course, we must evaluate the application M N, using the \"inner\" semantics, and that is what \nrules (Eval) and (Raise) are about. The inner operational semantics, which we do not present here, is \ndescribed in [11]. It defines two relations over terms: Convergence of terms is written M $ V, meaning \nthat closed term M evaluates to value V.  Exceptional convergence, written M $ e, means that closed \nterm M may raise exception e.  Apart from describing call-by-name evaluation of our lan-guage, the inner \nsemantics also allows one to raise (but not catch) an exception in purely-functional code, using the \nfunction raise :: Exception -> a A crucial characteristic of the inner semantics is that conver- gence \nand exceptional convergence are mutually exclusive: no term both evaluates to some value and raises an \nexcep-tion. Moreover, while convergence is deterministic, the ex- ceptional convergence is not. In other \nwords, a term may raise many different exceptions; which it does raise when evaluated is decided upon \nat run-time. This is the essence of imprecise exceptions [15]. Given this inner semantics, rule (Eval) \n\"lifts\" evaluation in the inner semantics to a transition in the outer system. (We stipulate that M ~ \nV to prevent infinite sequences of the form V --+ V ---+ V --4 ....) Similarly, if the evaluation yields \nan exception, rule (Raise) replaces the failing evaluation by a throw of the exception. 279 ~[putCha~ \nc~]~ ( Put Char ) ( Get Char) (sleep) (Fork) (Block Return) ~[[unblock (return M)]Dt ..-+ dE[return \n2v/-]pt ( Unblock Return) ~[[block (throw e)]~t ~ ~[[throw e]Dt (Block Throw) ~[u~block (th~o~ e)]~t \n~ O~[th~o~ ~]Dt ( Unblock Throw) ~[[throwTo te]~u ---+ dE[return ()]~u [ [t~e] (ThrowTo) ~[[unblock \n~-[M]]~t I #~ e] ~ ~E[unblock [[throw e]]~t, M ~ block N (Receive) (Interrupt) ~[[putChar ch]~ --+ ~[putChar \nch]~; (Stuck PutChar) (Stuck GetChar) (Stuck Sleep) (M)m I ~[[putMVarm Y]~ --+ (M>m I ~[[putMVar m N]~ \n(Stuck PutMVar) <)~ I ~[t~Mw~ m]D~ ~ (>~ i ~E[t~,MW~ -~]D~ (Stuck TakeMVar) Figure 5: Transition Rules \nfor Asynchronous Exceptions. 6.3 Operational Semantics for Asynchronous Ex-ceptions We now extend the \nsemantics to support the asynchronous exceptions we introduced in Section 5. Firstly, we need to add \nnew values for throwTo, block, and unblock: V :: .... I throwTo te I block M I unblock M. Secondly, \nwe need to add a new form of process that repre- sents an \"exception in flight\": P ::.... I[t~e~. Here, \n~t~ e~ represents an exception e which has been thrown to thread t, but not yet received. Thirdly, we \nneed to extend our notion of evaluation con- text to distinguish blocked and unblocked contexts: ::= \n[.]lFF>>=Mlcatch ~'g I[ ::= ~-I FF[block Eli ~[unblock E l The split-level evaluation context allows \nus to specify whether the innermost context in a thread is block or unblock. Thus an unblocked context \nis of the form E[unblock ~]. We will follow the convention that when parsing a term with a view to \nmatching evaluation context rules, contexts must be maximal. We also need to distinguish between threads \nthat are runnable, and those that are stuck (e.g., trying to do a putRVar to a full MVar, or trying to \ntake an empty MVar). We denote runnable threads by a superscript o, thus: IMD~, and stuck threads by \na superscript \u00ae: IMI~. We will also write ~MD~ to mean that a thread is either runnable or stuck, but \nwe do not know (or care) which. Since most of the rules concern runnable threads, we normally elide the \no in the interests of reducing clutter. Most of the rules from Section 6.1 are still valid with this \nnew definition of E-contexts, and apply only to runnable threads. The rules that change are discussed \nbelow. Our new transition rules are given in Figure 5. The first four rules are revised versions of rules \nfrom Figure 4. The next four rules are concerned with propagating return values and exceptions through \nblock and unblock, and axe unsur- prising. Rule (ThrowTo) describes how invoking throwTo causes the exception \nto be spawned as a separate entity, with the caller of throwTo continuing immediately. Rule (Receive) \nsays that any runnable thread may be in- terrupted by an exception targeted at its Throadld, provided \nthe thread is executing in an unblocked context. Any thread that is stuck may be interrupted, (Interrupt), \nexcept that the interruption is allowed in any context. As a side-effect, the interrupted thread becomes \nrunnable. To express the fact that they each wait for some impetus from the outside world, putChar, getChar, \nand sleep may all immediately become stuck. (We say may since we allow a signal from the environment \nwill take precedence.) putMVar will become stuck when putting to a full MVar, and takeMVar will become \nstuck when trying to take from an empty MVar. Building more powerful combinators either executes both \nof its arguments concurrently, and re- The features introduced in Section 5.1 are expressive but rather \nlow-level. We do not advocate programming with them directly; instead, we hope to build a library of \nrobust abstractions, layered on top of the primitives, that express common programming patterns. 7.1 \nBracketing abstractions A useful combinator is finally, which embodies the concept of \"do A, then whatever \nhappens do B\": finally :: I0 a -> I0 b -> I0 a A possible implementation offinally is: finally a b = \nblock (do { r <- catch (unblock a) (\\e -> do { b; throw e }); b; return r; }) Notice that the second \nargument to finally is executed inside a block. This is necessary in order to guarantee that the second \nargument is always executed, and using block in this case ensures that. The behaviour is similar to that \nof interrupts or Unix signals: in a signal handler, signals of the same type are normally disabled, so \nthat the application has a chance to deal with the signal it has already received. Reversing the arguments \nto finally yields later, which is sometimes useful: later b a = finally a b In fact, finallyis aninstance \nofamore generalcombinator, bracket: bracket :: I0 a -> (a -> IO b) -> (a -> IO c) -> I0 b bracket is \nuseful for the class of tasks of the form \"acquire a resource, operate on it, free the resource.\" We \nwant the resource to be freed if either the operation succeeds or raises an exception. For example, consider \nopening a file: bracket (openFile \"file.imp\") (\\h -> workOnFile h) (\\h -> hClose h) Using bracket here \nmakes sure that the file will always be closed, regardless of what exceptions are flying around. Fur- \nthermore, it makes sure that the openFile operation be- haves atomically: it either succeeds, in which \ncase we have acquired the resource, or raises an exception, in which case we have not. The implementation \nof bracket is a straight- forward generalization of finally, above. 7.2 Symmetric process abstractions \nThe forkI0 primitive is asymmetric: it forks a child while the parent continues in parallel. Here are \ntwo more sym- metrical forms of forking: either :: I0 a -> I0 b -> I0 (Either a b) both :: I0 a -> I0 \nb -> I0 (a,b) turns the result from the first one to finish; the other thread is sent a KillThread exception, \nboth also evaluates its ar- guments concurrently, but waits for them both to terminate before returning \nthe results in a pair. These informal descriptions seem simple, but in the pres- ence of asynchronous \nexceptions we have to be more precise about the behaviour. For instance, what happens when an asynchronous \nexception is sent to a thread executing (either a b) ? Does it get propagated to the child threads? What \nhappens if one of the child threads raises an excep- tion? Here is a more precise specification of the \ndesired be- haviour of (either a b): a and b run concurrently \u00ae Result is (Left r) if a finishes first \nand returns r, (Right r) ifb finishes first and returns r, or (throw e) if either a or b raises an exception \ne before one of them returns a result.  If the thread executing either receives an asyn; ....... chronous \nexception, it is propagated to both children.  The behaviour is undefined if either computation throws \nan exception to the main thread.  One possible implementation uses two child threads, and an MVar to \nhold the result: data EitherRet a b = A a [ B b I X Exception either a b = do { m <- newEmptyMVar; \nblock (do a_id <- forkIO (catch (do { r <- unblock a; putMVar m (A r) }) (\\e -> putMVar m (X e))); b_id \n<- forkl8 (catch (do { r <- unblock b; putMVar m (B r) }) (\\e -> putMVar m (X e))); let loop = catch \n(takeMVar m) (\\e -> do { throwTo a_id e; throwTo b_id e; loop }) ; r <- loop; throwTo a_id KillThread; \nthrowTo b_id KillThread; case r of A r -> return (Left r); B r -> return (Right r); X e -> throw }})} \n Note how we propagate all received exceptions to the children until one of them has returned a result \nor raised an exception. It is important here that the throwTo calls in the main thread are non-interruptible: \nwe have to be sure that all exceptions are properly propagated to the children, and also that both children \nare sent the KillThread exception before we return. If throwTo was interruptible, these properties would \nbe hard to guarantee (see Section 9 for a discussion of an alternative design in which throwTo is interruptible). \n 7.3 Time-outs Having either allows us to define a composable timeout combinator: 281 timeout :: Int \n-> 10 a -> 10 (Maybe a) timeout t a = do r <- either (sleep t) a case r of Left -> Nothing Kight-a \n-> Just a  timeouts may be arbitrarily nested, and the semantics of either ensure that they cannot \ninterfere with each other. 7.4 Safe points Sometimes we cannot use an immutable value to represent the \ndata structure we are interested in; perhaps it has been passed to us across a foreign language interface, \nor the struc- ture we are dealing with is large enough that creating a new one for each operation would \nbe too expensive (standard Hasketl does not have mutable structures, but many com-pilers support them \nas extensions). In Concurrent Haskell, MVars are commonly used to hold an immutable value, but they can \nequally well be used in a more conventional way, as a semaphore to protect a directly mutable structure. \nIf an MVar is being used to protect a shared mutable data structure, such as a mutable array, then the \nchances are that we do not want to be disturbed at all while we operate on it, because an exception received \nduring the operation may leave the mutable data structure in an inconsistent state. In this case, it \nmakes sense to omit the call to unblock in the locking example in the previous section. But what if compute \nis going to take a long time? Then we have to explicitly program checkpoints into the code such that \ncompute will receive any pending asynchronous exceptions at designated safe points during execution. \nThe easiest way to implement a safe point is to unblock for a short period of time: safePoint :: IO () \n safePoint = unblock (return O)  8 Implementation Implementing synchronous exceptions is done in the \nstan-dard way: catch pushes a catch frame on the stack which contains a pointer to the handler, before \nbeginning to execute its argument.  when an exception is raised, the stack is truncated up to (and including) \nthe nearest enclosing catch frame, and control is passed to the handler with the exception given as an \nargument.  There is one additional issue in Haskelh what to do with \"computations in progress,\" or \nthunks. The program may later attempt to demand the value of a thunk that was un- der evaluation when \nthe exception was triggered. Since the exception is synchronous, we know that re-evaluating this thunk \nwould yield the same exception, so it is safe to over- write the thunk with a closure which will immediately \nraise the same exception if demanded. More details are given in [161. The implementation of asynchronous \nexceptions differs only in the treatment of thunks; since we cannot be sure that re-evaluating the thunk \nwould raise the same asynchronous exception, we must either revert the thunk to its initial state, or \n\"freeze\" it at the point where the exception was received. The difference between the two techniques \nis operational only, the effect is not observable by the programmer. We use the technique for freezing \nthunks given in [17]. 8.1 Implementation of block and unblock To extend our implementation of exceptions \nwith the block and unblock operations, we do the following: , Extend the per-thread data block to include \nthe cur-rent state of asynchronous exceptions, which is either blocked or unblocked, and a queue of pending \nasyn-chronous exceptions waiting to be delivered to the thread. As soon as a thread exits the scope of \na block, and at regular intervals during execution inside unblock, its pending exceptions queue must \nbe checked. If there are pending exceptions, the first one is removed from the queue and delivered to \nthe thread. \u00ae Extend the catch frame to include the state (blocked or unblocked) of asynchronous exceptions \nat the time when the frame was placed on the stack. This is nec- essary to restore the correct state \nafter handling an exception. \u00ae Add two new types of stack frame: the block frame and the unblock frame. \nWhen execution returns to an unblock frame, asynchronous exceptions are unblocked (waking up any threads \non the blocking queue), and the frame is removed from the stack. Block frames are identical, except that \nexceptions are blocked when execution returns to the frame. The implementation of block is fairly straightforward: \n1. If exceptions are already blocked, go to step 4. 2. Set the asynchronous exception state in the current \nthread to \"blocked.\" 3. If there is a block frame on the top of the stack, remove it. Otherwise, push \nan unblock frame on the stack. 4. Continue by executing the argument of block.  The implementation \nof unblock is obtained by reversing \"block\" and \"unblock\" in the above sequence. Step 3 appears confusing, \nbut it is designed to avoid unnecessary stack growth. Consider the following example: f = do { ...; block \n(do { ...; unblock f }) } The first block will push an unblock frame on the stack, which will still be \non the top of the stack when we reach unblock. If we simply pushed a block frame before calling f, the \nstack would look like: f's caller unblock frame block frame  and the stack would continue to grow by \ntwo frames for each recursive call to f. The adjacent block/unblock frames are superfluous: on return, \nwe will simply block asynchronous exceptions and then immediately unblock them again for each pair of \nblock/unblock frames. So step 3 in the above implementation of block is designed to remove the extra \nframes so that functions like f can run in constant stack space.  8.2 Implementation of throwTo The \nthrowTo operation is quite straightforward: \u00ae Place the exception on the target thread's queue of pending \nexceptions. This may involve sending a \"mes- sage\" to the target thread in a distributed or multipro- \ncessor implementation. 9 Design Alternatives An alternative design, and one which we experimented with \nfor some time, is to have throwTo be a synchronous opera-tion in that it waits for the exception to be \ndelivered before returning. In this design, throwTo also becomes an inter- ruptible operation, because \nit can block indefinitely. The choice between these two designs is a hard one, there are arguments in \nfavour of both approaches: The synchronous version of throwTo is sometimes eas- ier to program with, \nbecause it provides a guarantee that the target thread has received the exception. On the other hand, \nthe synchronous throwTo being an in- terruptible operation can cause headaches.  The asynchronous version \nof throwTo can easily be im- plemented in terms of the synchronous one simply by forking a new thread \nto perform the throwTo. The re- verse is somewhat harder, but can usually be achieved using an MVar. \n An asynchronous throwTo is likely to be easier to implement and more efficient in a multi-processor \nor distributed environment, because it doesn't re-quire synchronization with the target thread. In a \nsingle-processor environment, both designs are equally straightforward to implement.  \u00ae The presentation \nof the semantics for the asynchronous version of throwTo is simpler than the synchronous ver- sion (the \nsynchronous version needs a special case for a thread throwing an exception to itself, and extra cases \nto deal with the interruptibility of throuTo). Our proposal uses a single datatype for both synchronous \nand asynchronous exceptions. We choose this design for this paper, because it simplifies the presentation \nand semantics, but there are arguments in favor of distinguishing between them in the type system. Since \nsynchronous exceptions are dependent to the local execution of a thread, it is possible to use analysis \nto check for uncaught exceptions [21] and for a compiler to optimize the control-flow of statically match- \ning throw/catch pairs. Adding asynchronous exceptions to the mix means that any expression can be the \nsource of an exception, which renders these techniques useless. Another problem is that sequential code \nthat was written without thought of asynchronous exceptions may break assumptions of our combinators. \nFor example, if we put the expression e 'catch' \\_-> e' in the context of the timeout combinator, it \ncan intercept the Time0ut exception, which breaks the combinator. While one might argue that universal \nhandlers like this one are bad programming practice, such code is quite reasonable in a sequential setting, \nwhere one understands exactly which exceptions the expression e might raise. A solution is to de- fine \ntwo datatypes, exceptions and alerts, with a distinct catch operator for each type. Using Haskell's typeclasses, \nwe can overload the catch operator to provide some syntac- tic unification. Java addresses a similar \nproblem by distin- guishing in the type system between checkable and uncheck-able exceptions, where methods \nmust declare the checkable exceptions they may raise. 10 Related Work To our knowledge, no other language \nsupports fully- asynchronous exceptions in such a way that they can be used safely and without resorting \nto gratuitous use of exception handlers to recover from untimely exceptions. Furthermore, we believe \nthat our semantics is the first formal accounting of truly asynchronous signalling. Erlang [1] has asynchronous \nexceptions of a kind: pro-cesses can be linked together, such that each process will receive an asynchronous \nexception if the other dies for some reason. The exception can be caught in the normal way. Er- lang \nalso has a way to control delivery of these asynchronous exceptions, providing the opportunity to have \nthem deliv- ered using asynchronous message passing instead of as ex- ceptions. However, the control \nmechanism is stateful rather than scoped as in our approach, and hence doesn't allow safe exception handlers \nto be defined (asynchronous exceptions will always be enabled on entry to the exception handler, so there \nis a race window before they can be disabled again). Standard ML originally had a weak form of asyn-chronous \ninterrupt, whereby an external Control-C would asynchronously raise the Interrupt exception. Because \nit was not possible to write robust handlers for the Interrupt exception [18], it was removed from the \n1997 revision of the language [10]. The SML of New Jersey system uses a more general mechanism of asynchronous \nsignal handlers as a replacement [18]. In this mechanism, an asynchronous exception causes the current \nthread of control to be rei- fled as a first-class continuation, which is then passed to a signal handler. \nThe signal handler runs with signals masked, so additional signals are deferred until the handler is \ndone. The signal handler may either resume the inter- rupted thread or transfer control to a different \nthread. This mechanism is used to implement preemption in Concurrent ML (CML) [19], but CML does not \nsupport asynchronous signalling between threads. It should be possible to add asynchronous signalling \n(including the block and unblock combinators) to CML using first-class continuations and the signal handler \nmechanism, but we do not know how to im- plement the block combinator using these mechanisms in a way \nthat preserves tail recursion (as described in Section 8.1). OCaml [7] also provides support for concurrency, \nbut does not support asynchronous signaling. Some concurrent languages provide support for semi- asynchronous \nexceptions. For example, Modula-3 defines a mechanism for one thread to alert another, which causes the \nAlert exception to be raised in the target. The raising of the exception is deferred until either the \ntarget calls either TestAlert or WaitAlert. The alert mechanism has been formalized as part of a Larch \nspecification of Modula-3's thread synchronization [4]. Java supports a similar mech- anism for unblocking \na waiting or sleeping thread with an InterruptedException [2]. When the thread is not wait- ing or sleeping, \nhowever, the interrupt O method merely sets the thread's interrupt flag, which can be polled with interrupted(). \nThe big difference between these mecha- nisms and our design is that ours is fully asynchronous. Java \noriginally offered a fully asynchronous exception method (the stop method of the Thread class), but depre- \ncated the feature in Version 1.2 [20]. The reason given is the one discussed in Section 2, namely that \nsince a method may receive an asynchronous exception while making changes to the object's mutable state, \nthe feature was too dangerous to program with. There are several parallel Lisp and Scheme dialects that \nsupport speculative computation using some form of parallel-or operator (like our either combinator). \nIn the language QLisp, a child thread can throw an exception that is caught by its parent [5]; i.e., \nthe scope of a CATCH in QLisp includes any threads spawned below it. Furthermore, other computations \nbelow the CATCH are also terminated (e.g., the siblings of the throwing thread). QLisp also provides \nthe UNWIND-PROTECT form to support cleanup when an excep-tion is thrown; the cleanup handler runs in \nan unkillable state, so that multiple throws are not a problem. The main difference between the asynchronous \nsignalling mechanisms of QLisp and our mechanism is that QLisp is motivated by controlling speculative \ncomputation, and so asynchronous signalling is a heavy-weight mechanism that affects a whole tree of \nthreads. It should be possible to build similar mech- anisms using our more primitive construct. Another \ndiffer- ence is that QLisp does not have a formal semantics. In some respects, the language PaiLisp may \nhave the closest mech- anism to ours [6]. In PaiLisp, a thread can invoke a first-class continuation \nin another thread, which has the effect of forcing control in the target thread to the call/co that bound \nthe continuation. PaiLisp uses this primitive mech- anism to define higher-level combinators, such as \nparallel- or. From the published description, it does not appear that PaiLisp has any signal masking/unmasking \nmechanism like our block/unbleck combinators. While existing languages have not provided support for \nasynchronous signaling, many operating systems have such mechanisms. The best known example of these \nis the POSIX signal mechanism (which is the model for signals in SML/NJ). While POSIX signals are sufficient \nto imple- ment asynchronous signaling, they are expensive (all op- erations involve user/kernel transitions) \nand most POSIX library code is not asynchronous-signal safe. Extending POSIX signals to multithreaded \nprograms written using the POSIX Threads API (PThreads) has proven problematic and the recommended practice \nis for multithreaded programs to designate a single thread to han- dle all asynchronous signals. The \nPThreads API does pro- vide an asynchronous method for killing threads, called thread cancellation. A \nthread can define the type of can- cellation it accepts (deferred or asynchronous) and can en- able or \ndisable cancellation. Deferred cancellation, what we have called semi-asynchronous, is the default behavior. \nIn this mode, cancellation messages are deferred until the tar- get thread executes a library function \nthat is defined to be a cancellation point (similar to our notion of interruptible op- erations). A mechanism \nfor maintaining a stack of cleanup routines is also provided, which allows threads to restore invariants. \nThe use of asynchronous cancellation is discour- aged, since it can only be safely used for code that \ndoes not hold resources or modify global state. While the basic function of PThread cancellation is similar \nto our design, our language-based approach offers many advantages to the programmer. Our block and unblock \ncombinators are easier to use correctly than cancellation-state changing operations of PThreads. Furthermore, \nour combinators support robust cleanup of asynchronous exceptions~ whereas the PThread cleanup routines \nare not robust in the asynchronous cancel- lation mode (because of the possibility of multiple cancel- \nlation requests). Our design also has the advantage of al- lowing the signalled thread to continue executing, \nwhereas a canceled thread must terminate after cleanup. 11 Conclusion We have shown how asynchronous \nexceptions can be incor- porated into Concurrent Haskell in such a way that they can be used safely and \nrobustly. The changes required to existing code to make it safe to use in the presence of asyn- chronous \nexceptions are kept to a minimum. In the case of pure non-I/O code, no changes at all are required to \nbe able to use it in an asynchronous exception-enabled system, and this is a property guaranteed by the \ntype of the expression; no further analysis is required. This means that in Haskell, not only are a large \nproportion of libraries automatically thread-safe, they are also automatically exception-sath too! Our \nasynchronous exception model has several advan- tages over existing methods: for example, the compositional \nnature of our timeout function relies on true asynchronous exceptions. Synchronous exceptions just will \nnot do since we do not want to have to modify the code that we are timing (which might even be unavailable) \nto include checkpoints. The scoped nature of our block and unblock combina- tors leads to a clean and \nelegant operational semantics for Concurrent Haskell with exceptions. We hope to be able to formulate \nproofs, using this semantics, that simple combina- tots built using these primitives have the properties \nthat we expect. We believe that there two useful theories that arise from the semantics: a simple equational \ntheory, and a more subtle theory based on a commitment ordering, where a pro- cess will approximate another \nif the latter is committed to performing at least the same operations as the former. The commitment theory \nis novel, and would allow us to prove, for example, that finally a b is committed to performing the same \noperations as block b. Work on these theories is at a very early stage. Experience with using our asynchronous \nexception model is still limited, although we have used it to construct a pro- totype fault-tolerant \nHTTP server which makes heavy use of time-outs, multithreading and exceptions [8]. 12 Acknowledgements \nMany thanks to Tony Hoare for his valuable comments on an earlier draft of this paper. We are also grateful \nto Claus P~einke for his very detailed comments on an earlier draft of this paper, which, apart from \nimproving the paper generally, led to the correction of a bug in the semantics.  References [1] J. Armstrong, \nR. Virding, C. WikstrSm, and M. Williams. Concurrent Programming in Erlang. Prentice Hall Europe, second \nedition, 1996. [2] K. Arnold and J. Gosling. The Java Programming Lan- guage. The Java Series. Addison-Wesley, \nsecond edi- tion, 1998. [3] P. Barth, R. S. Nikhil, and Arvind. M-Structures: Ex- tending a parallel, \nnon-strict functional language with state. In R. J. M. Hughes, editor, Proc. FPCA '91, volume 523 of \nLNCS, pages 538-568. Springer-Verlag, 1991. [4] A. D. Birrell, J. V. Guttag, J. J. Homing, and R. Levin. \nThread synchronization: A formal specification. In G. Nelson, editor, Systems Programming with Modula- \n3, chapter 5, pages 119-129. Prentice Hall, Englewood Cliffs, N J, 1991.  [5] R. P. Gabriel and J. \nMcCarthy. Queue-based multi- processing Lisp. In Proc. LFP'84, pages 25-44, Aug. 1984. [6] T. Ito and \nM. Matsui. A parallel lisp language PalLisp and its kernel specification. In Parallel Lisp: Languages \nand Systems, volume 441 of LNCS, pages 58-100, June 1989. [7] X. Leroy, D. R@my, J. Vouillon, and D. \nDoligez. The Objective Carol system documentation and user's man- ual (release 2.04). Technical report, \nINRIA, 1999. At  http://caml.inria.fr/ocaml/htmlman/. [8] S. Marlow. Writing high-performance server \napplica- tions in Haskell, case study: A Haskell web server. In Haskell Workshop, Montreal, Canada, September \n2000. [9] R. Milner. The polyadic 7r-calculus: A tutorial. In F. L. Hamer, W. Brauer, and H. Schwichtenberg, \neditors, Logic and Algebra of Specification. Springer-Verlag, 1993.  [10] R. Milner, M. Torte, R. Harper, \nand D. MacQueen. The Definition of Standard ML (Revised). MIT Press, Cambridge, Massachusetts, 1997. \n[11] A. K. Moran, S. B. Lassen, and S. L. Peyton Jones. Im- precise exceptions, co-inductively. In Proc. \nHOOTS'99, volume 26 of Electronic Notes in Theoretical Computer Science, Paris, Sept. 1999. [12] S. Peyton \nJones. Tackling the awkward squad: monadic input/output, concurrency, exceptions, and foreign- language \ncalls in Haskell. In R. Steinbrueggen, editor, Engineering theories of software construction, Markto- \nberdorf Summer School 2000, NATO ASI Series. IOS Press, 2001. [13] S. Peyton Jones, R. Hughes, L. Augustsson, \nD. Bar- ton, B. Boutel, W. Burton, J. Fasel, K. Hammond, R. Hinze, P. Hudak, T. Johnsson, M. Jones, \nJ. Launch- bury, E. Meijer, J. Peterson, A. Reid, C. Runciman, and P. Wadler. Report on the programming \nlanguage Haskell 98. Technical report, Feb. 1998.  [14] S. L. Peyton Jones, A. Gordon, and S. Finne. \nCon-current Haskell. In Proc. POPL'g6, pages 295-308, St. Petersburg, Florida, Jan. 1996. ACM Press. \n[15] S. L. Peyton Jones, A. Reid, T. Hoare, S. Marlow, and F. Henderson. A semantics for imprecise exceptions. \nIn Proe. PLDI'99, volume 34(5) of ACM SIGPLAN No- tices, pages 25-36. ACM Press, May 1999.  [16] A. \nReid. Handling exceptions in Haskell. Research Report YALEU/DCS/RR-1175, Yale University, Aug. 1998. \n[17] A. Reid. Putting the spine back in the Spineless Tagless G-Machine: An implementation of resumable \nblack- holes. In Proc. IFL'98 (selected papers), volume 1595 of LNCS, pages 186-199. Springer-Verlag, \n1999. [18] J. H. Reppy. Asynchronous signals in Standard ML. Technical Report TR90-1144, Cornell University, \nCom- puter Science Department, Aug. 1990. [19] J. H. Reppy. Concurrent Programming in ML. Cam-bridge \nUniversity Press, Oct. 1999. [20] Wily are Thread.stop, Thread.suspend, Thread.resume, and Runtime.runFinalizersOnExit \ndeprecated? In the Java 2 SDK Standard Edition Documentation. At http://java.sun.com/products/jdk/l.3/docs/guide/ \n misc/threadPrimitiveDeprecation.html. [21] K. Yi. Compile-time detection of uncaught exceptions in Standard \nML programs. In Proc. SAS'9~, volume 864 of LNCS, pages 238-254, Sept. 1994. \n\t\t\t", "proc_id": "378795", "abstract": "<p>Asynchronous exceptions, such as timeouts are important for robust, modular programs, but are extremely difficult to program with &#8212; so much so that most programming languages either heavily restrict them or ban them altogether. We extend our earlier work, in which we added synchronous exceptions to Haskell, to support asynchronous exceptions too. Our design introduces scoped combinators for blocking and unblocking asynchronous interrupts, along with a somewhat surprising semantics for operations that can suspend. Uniquely, we also give a formal semantics for our system.</p>", "authors": [{"name": "Simon Marlow", "author_profile_id": "81100515135", "affiliation": "Microsoft Research, Cambridge", "person_id": "P265492", "email_address": "", "orcid_id": ""}, {"name": "Simon Peyton Jones", "author_profile_id": "81100271851", "affiliation": "Microsoft Research, Cambridge", "person_id": "PP43121273", "email_address": "", "orcid_id": ""}, {"name": "Andrew Moran", "author_profile_id": "81100516264", "affiliation": "Oregon Graduate Institute", "person_id": "PP14179363", "email_address": "", "orcid_id": ""}, {"name": "John Reppy", "author_profile_id": "81100590527", "affiliation": "Bell Labs, Lucent Technologies", "person_id": "PP17010305", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/378795.378858", "year": "2001", "article_id": "378858", "conference": "PLDI", "title": "Asynchronous exceptions in Haskell", "url": "http://dl.acm.org/citation.cfm?id=378858"}