{"article_publication_date": "05-01-2001", "fulltext": "\n Ultra-fast Aliasin9 Analysis using CLA: A Million Lines of C Code in a Second I Nevin Heintze Olivier \nTardieu Research, Agere Systems Ecole des Mines, Paris (formerly Lucent's Microeleetronics Division) \nolivier, tardieu@mines, org nch&#38;#169;agere,com ABSTRACT We describe the design and implementation \nof a system for very fast points-to analysis. On code bases of about a million lines of unpreprocessed \nC code, our system performs field- based Andersen-style points-to analysis in less than a second and \nuses less than 10MB of memory. Our two main contri- butions are a database-centric analysis architecture \ncalled compile-link-analyze (CLA), and a new algorithm for imple- menting dynamic transitive closure. \nOur points-to analysis system is built into a forward data-dependence analysis tool that is deployed \nwithin Lucent to help with consistent type modifications to large legacy C code bases. 1. INTRODUCTION \nThe motivation for our work is the following software main- tenance/development problem: given a million+ \nlines of C code, and a proposed change of the form \"change the type of this object (e.g. a variable or \nstruct field) from typel to type2\", find all other objects whose type may need to be changed to ensure \nthe \"type consistency\" of the code base. In particular, we wish to avoid data loss through implicit narrowing \nconversions. To solve this problem, we need a global data-dependence analysis that in effect performs \na forward data-dependence analysis (Section 2 describes this analysis, and how it differs from other \nmore standard de-pendence analyses in the literature.). A critical part of this dependence analysis is \nan adequate treatment of pointers: for assignments such as *p = x we need to determine what objects p \ncould point to. This kind of aliasing analysis is commonly called points-to analysis in the literature \n[4]. The scalability of points-to analysis has been a subject of inten- sive study over the last few \nyears [5, 8, 21, 11, 23]. However the feasibility of building interactive tools that employ some form \nof \"sufficiently-accurate\" pointer analysis on million line code-bases is still an open question. The \npaper has two main contributions. The first is an archi- 1This is a substantially revised version of \n[16]. Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for profit or commercial \nadvan-tage and that copies bear this notice and the full citation on the first page. To copy otherwise, \nto republish, to post on servers or to redistribute to lists, requires prior specific permission and/or \na fee. PLDI 2001 6/01 Snowbird, Utah, USA &#38;#169; 2001 ACM ISBN 1-58113-414-2/01/06... $5.00 tecture \nfor analysis systems that utilizes ideas from indexed databases. We call this architecture compile-link-analyze \n(CLA), in analogy with the standard compilation process. This architecture provides a substrate on which \nwe can build a variety of analyses (we use it to implement a number of different algorithms for Andersen-style \npoints~to analysis, dependence analysis and a unification-style points-to anal- ysis, all using a common \ndatabase format for representing programs). It scales to large code bases and supports sepa- rate and/or \nparallel compilation of collections of source files. Also, its indexing structures support rapid dynamic \nloading of just those components of object files that are needed for a specific analysis, and moreover \nafter reading a component we have the choice of keeping it in memory or discarding it and re-reading \nit if we even need it again (this is used to reduce the memory footprint of an analysis). We describe \nCLA in detail in Section 4, and discuss how it differs from other approaches in the literature, such \nas methods where sepaxate files are locally analyzed in isolation and then the individual results are \ncombined to analyze an entire code- base. The second contribution is a new algorithm for implement- ing \ndynamic transitive closure (DTC). Previous algorithms in the literature for Andersen's analysis are based \non a tran- sitively closed constraint graph e.g. [4, 10, 11, 21, 23, 22]. In contrast, our algorithm \nis based on a pre-transitive graph i.e. we maintain the graph in a form that is not transitively closed. \nWhen information about a node is requested, we must perform a graph reachability computation (as opposed \nto just looking up the information at the node itself in the case of a transitively closed constraint \ngraph). A direct im- plementation of the pre-transitive graph idea is impractical. We show how two optimizations \n-caching of teachability computations, and cycle elimination -yield aa efficient al- gorithm. Cycle elimination \nhas previously been employed in the context of transitively closed graph and shown to re- sult in significant \nimprovement [11], however in that work the cost of finding cycles is non-trivial and so completeness \nof cycle detection is sacrificed in order to contain its cost. However, in the pre-transitive setting, \ncycle detection is es- sentially free during graph teachability. We describe our algorithm in detail \nin Section 5. Section 6 presents various measurements of the performance of our system. For the Lucent \ncode bases for which our sys- tem is targeted, runtimes are typically less than a second (800MHz Pentium) \nand space utilization is about 10MB. These code bases are in excess of a million lines of code (un- \ncommented non-blank lines of source, before pre-processing). On gimp (a publicly available code base \nof about 440K lines), our system performs field-based Andersen-style points- to analysis in about a second \n(800MHz Pentium) and uses about 12MB. We also present data to illustrate the space advantages of CLA. \n 2. MOTIVATING APPLICATION- DEPEN- DENCE ANALYSIS Our points-to analysis system is built into a forward \ndata- dependence analysis tool that is deployed within Lucent to help with consistent type modifications \nto large legacy C code bases. The basic problem is as follows: suppose that the range of values to be \nstored in a variable must be in- creased to support additional system functionality. This may require \nchanging the type of a variable, for example from short to int. To avoid data loss through implicit nar- \nrowing conversions, any objects that take values from the changed variable must also have their types \nappropriately altered. Consider the following program fragment. short x, y, Z, *pp V~ W; y = x; z = y+l; \np = ~v; *p = Z; w = 1; If the type of x is changed from short to int, then we may also have to change \nthe types of y, z, v and probably p, but we do not need to change the type of w. Given an object whose \ntype must be changed (the target), we wish to find all other objects that can be assigned val- ues from \nthe specified object. This is a forward dependence problem, as opposed to backwards dependence used for \nex- ample in program slicing [25]. Moreover it only involves data-dependencies, as opposed to both data-dependencies \nand control-dependencies which are needed in program slic- ing. Our analysis refines forward data-dependence \nanalysis to reflect the importance of a dependency for the purposes of consistent type changes. The most \nimportant dependen- cies are those involving assignments such as x = y and z = y+l. On the other hand, \nan assignment such as zl = !y can be ignored, since changing the type of y has no effect on the range \nof values of zl, and so the type of zl does not need to be changed. Assignments involving operations \nsuch as division and multiplication are less clear. We discuss this later in the section. An important \nissue for the dependence analysis is how to treat structs. Consider the program fragment involving structs \nin Figure 1. If the target is the variable target, then u, w and s. x are all dependent objects. If the \ntype of target is changed from short to int, then the types of u, w and s.x should also be changed. To \neffect this change to the type of s. x, we can either change the type of the x field of the struct S, \nor we can introduce a new struct type especially for s. The advantage of the former case is that we make \nminimal changes to the program. The disadvantage is that we also change the type of t. x, and this may \nnot be Table 1: Classification of operations. Operations Argument 1 Argument 2 +, -, l, gq ^ Strong Strong \n* Weak Weak ~ >>, << Weak None unary: +,- Strong n/a None None None n/a  strictly necessary. However, \nin practice it is likely that if we have to change the type of the x field of s, then we will have to \nchange the type of the x field of t. As a result, it is desirable to treat objects that refer to the \nsame field in a uniform way. By \"same field\", we mean not just that the fields have the same name, but \nthat they are the same field of the same struct type. Since our ultimate use of dependence analysis is \nto help iden- tify objects whose type must be changed, we are not just in- terested in the set of dependent \nobjects. Rather, we need to give a user information about why one object is dependent on another. To \nthis end, we computes the dependence chains, which identify paths of dependence between one object and \nanother. In general there are many dependence paths be- tween a pair of objects. Moreover, some paths \nare more important than others. Dependencies arising from direct assignments such as x=y are usually \nthe most important; de- pendencies involving arithmetic operations x=y+l, x=y>>3, x=42Xy, x=l<<y are \nincreasingly less important. Our metric of importance is biased towards operations that are likely to \npreserve the shape and size of input data. Table 1 out- lines a simple strong/weak/none classification \nthat we have employed. Our analysis computes the most important path, and if there are several paths \nof the same importance, we compute the shortest path. Large code bases often generate many dependent \nobjects -typically in the range 1K-100K. To help users sift through these dependent objects and determine \nif they are objects whose type must be changed, we prioritize them according to the importance of their \nunderlying dependence chain. We also provide a collection of graphic user interface tools for browsing \nthe tree of chains and inspecting the corresponding source code locations. In practice, there are often \ntoo many chains to inspect -a common scenario is that a central ob- ject that is not relevant to a code \nchange becomes dependent (often due the context- or flow-insensitivity of the underly- ing analysis), \nand then everything that is dependent on this central object also becomes dependent. We address this \nis- sue with some additional domain knowledge: we allow the user to specify \"non-targets\", which are \nobjects that the user knows are certainly not dependent on the target ob- ject. This has proven to be \na very effective mechanism for focusing on the important dependencies. 3. ANDERSEN'S POINTS-TO ANALYSIS \nWe review Andersen's points-to analysis and introduce some definitions used in the rest of the paper. \nIn the literature, there are two core approaches to points-to analysis, ignoring context-sensitivity \nand flow-sensitivity. The first approach is i. short target;  2. struct S { short x; short y; ); 3. \nshort u, *v, w;  4. struct S s, t;   5. v = ~w; 6. u = target; 7. *v = u; 8. S.X = ~;  w/short <eg1.c:3> \n--3- u/short <egl.c:7> --~ target/short <egl.c:6> where target/short <egl.c:l> target/short <egl.c:l> \nu/short <egl.c:3> -+ target/short <egl.c:6> where target/short <egl.c:1> S.x/short <egl.c:2> --w/short \n<egl.c:8> -+ u/short <egl.c:7> ~ target/short <egl.c:6> ... Figure 1: A program fragment involving structs \nand its dependence results (the target is target). unification-based [24]: an assignment such as x = \ny invokes a unification of the node for x and the node for y in the points-to graph. The algorithms for \nthe unification-based approach typically involve union/find and have essentially linear-time complexity. \nThe second approach is based on subset relationships: an assignment such as x = y gives rise to a subset \nconstraint x D y between the nodes x and y in the points-to graph [4]. The algorithms for the subset-based \napproach utilize some form of subtyping system, subset con- straints or a form of dynamic transitive \nclosure, and have cubic-time complexity. The unification-based approach is faster and less accurate [22]. \nThere has been considerable work on improving the performance of the subset-based approach [11, 23, 21], \nal- though the performance gap is still sizable (c.f. [23, 21] and [8]). As Das very recently observed \n\"In spite of these efforts, Andersen's algorithm does not yet scale to programs beyond 500KLOC.\" [8] \nThere has also been work on improving the accuracy of the unification-based approach by incorporat- ing \nsome of the directional features of the subset-based ap- proach to produce a hybrid unification-based \nalgorithm [8]: for a small increase in analysis time (and quadratic worst- case complexity), much of \nthe additional accuracy of the subset-based approach can be recovered. A Deductive Reachability Formulation \nWe use a context-insensitive, flow-insensitive version of the subset-based approach that is essentially \nthe analysis due to Andersen [4]. One reason for this choice is the better accuracy of the subset-based \napproach over the unification- based approach. Another reason is that users of our depen- dence analysis \nsystem must be able to inspect the depen- dence chains produced by our system (Section 2), and un-derstand \nwhy they were produced. Subset-based approaches generate easier to understand results; unification-based \nap- proaches often introduce hard to understand \"backwards\" flows of information due to the use of equalities. \nPrevious presentations of Andersen's algorithm have used some form of non-standard type system. Our presentation \nuses a simple deductive reachability system. This style of analyses presentation was developed by McAllester \n[20]. It has also been used to describe control-flow analysis [18]. To simplify our presentation, we \nconsider a tiny language con- sisting of just the operations , and &#38;. Expressions e have x > &#38;y \n(if ,x = e in P) (STAre 1) y >e x ---~ &#38;y (if e = ,x in P) (STAR-2) e------+y (if el = e2 in P) (ASSIGN) \nel ~ 62 el ~ e2 e2 ) e3  (TRANS) el ---'~ e3 Figure 2: Deduction rules for aliasing analysis. the form: \n e ::= x I *~ I &#38;x We shall assume that nested uses of, and &#38; are removed by a preprocessing \nphase. Programs are sequences of assign- ments of the form el = e2 where el cannot be &#38;x. Given some \nprogram P, we construct deduction rules as specified in Figure 2. In the first rule, the side condition \n\"if *x : e in P\" indicates that there is an instance of this rule for each occurrence of an assignment \nof the form *x = e in P. The side conditions in the other rules are similarly interpreted. Intuitively, \nan edge el ~ e2 indicates that any object pointer that we can derive from e2 is also derivable from ex. \nThe first rule deals with expressions of the form *x on the left-hand-sides of assignments: it states \nthat if there is a transition from x to &#38;y, then add a transition from y to e, where e is the left-hand-side \nof the assignment. The second rule deals with expressions of the form *x on the right-hand-sides of assignments: \nit states that if there is a transition from x to &#38;y, then add a transition from e to y where e is \nthe right-hand-side of the assignment. The third rule adds a transition from el to e2 for all assignments \nel = e2 in the program, and finally, the fourth rule is just transitive closure. The core of our points-to \nanalysis can now be stated as follows: x can point to y if we can derive x ~ &#38;y. Figure 3 contains \nan example program and shows how y ~ &#38;x can be derived. Analysis of Full C Extending this core analysis \nto full C presents a number of choices. Adding values such as integers is straightforward. It is also \neasy to deal with nested uses of * and &#38; through the addition of new temporary variables (we remark \nthat int x, *y; int **Z; Z --> ~y (ASSIGN) Z =. 86y; *Z ~ ~X (ASSIGN) *z = &#38;X; Y ~ ~x (from STAR-l) \n Figure 3: Example program and application of de- duction rules to show y ~ ax. considerable implementation \neffort is required to avoid in- troducing too many temporary variables). However, treat- ing structs \nand unions is more complex. One possibility is, in effect, to ignore them: each declaration of a variable \nof struct or union type is treated as an unstructured mem- ory location and any assignment to a field \nis viewed as an assignment to the entire chunk e.g.x.f is viewed as an as- signment to x and the field \ncomponent f is ignored. We call this the field-independent approach and examples in- clude [10, 11, 22]. \nAnother approach is to use a field-based treatment of structs such as that taken by Andersen [4]. In \nessence, the field-based approach collects information with each field of each struct, and so an assignment \nto x.f is viewed as an assignment to f and the base object x is ig- nored. (Note that two fields of different \nstructs that happen to have the same name are treated as separate entities.) The following code illustrates \nthe distinction between field-based and field-independent. struct s ~ int *x; int *y; ]- A, B; int z; \nmain () 'C int *p, *q, *r, *s; A.x = ~z; /* field-based: assigns to \"x\" * field-independent: assigns \n~o \"A\" */ p = A.x; /* p gets ~z in both approaches */ q = A.y; /* field-independent: q gets Ez */ r = \nB.x; /* field-based: r gets ~z */ s = B.y; /* in neither approach does s get ~z */ } In the field-independent \napproach, the analysis determines that only p and q can point to ~zz. In the field-based ap- proach, \nonly p and r can point to ~z. Hence, neither of these approaches strictly dominates the other in terms \nof ac- curacy. We note that while the works [10, 11, 22] are based on Andersen's algorithm [4], they \nin fact differ in their treat- ment of structs: they are field-independent whereas Ander- sen's algorithm \nis field-based ~. In Section 6, we show this choice has significant implications in practice, especially \nfor large code bases. Our aliasing analysis uses the field-based approach, in large part because our \ndependence analysis is also field-based.   4. COMPILE-LINK-ANALYZE A fundamental problem in program \nanalysis is modular- ity: how do we analyze large code bases consisting of many source files? The simple \napproach of concatenating all of the source files into one file does not scale beyond a few thou- sand \nlines of code. Moreover, if we are to build interactive 1Strictly speaking, while Andersen's core algorithm \nis field- based, he assumes that a pre-processiug phase has dupli- cated and renamed struct definitions \nso that structs whose values cannot flow together have distinct names (see Section 2.3.3 and 4.3.1 of \n[4]). tools based on an analysis, then it is important to avoid re- parsing/reprocessing the entire'code \nbase when changes are made to one or two files. The most basic approach to this problem is to parse compila- \ntion units down to an intermediate representation, and then defer analysis to a hybrid link-analyze phase. \nFor example, at the highest level of optimization, DEC's MIPS compiler treats the internal ucode files \nproduced by the frontend as \"object files\", and then invokes a hybrid linker (uld) on the ucode files \n[9]. The uld \"linker\" simply concatenates the ucode files together into a single big ucode file and then \nperforms analysis, optimization and code generation on this file. The advantage of this approach is it \nmodularizes the parsing problem - we don't have to parse the entire program as one unit. Also, we can \navoid re-parsing of the entire code base if one source file changes. However, it does not mod- ularize \nthe analysis problem -the analysis proceeds as if presented with the whole program in one file. One common \nway to modularize the analysis problem is to analyze program components (at the level of functions or \nsource files), and compute summary information that cap- tures the results of these local analyses. Such \nsummaries are then combined/linked together in a subsequent \"global- analysis\" phase to generate results \nfor the entire program. This idea is analogous to the construction of principle types in type inference \nsystems. For example, assigning 'ca ~ o?' to the identity function in a simply typed language is essen- \ntially a way of analyzing the identity function in a modular way. Uses of the identity function in other \ncode fragments can utilize oz -4 ~ as a summary of the behavior of the identity function, thus avoiding \ninspection of the original function. (Of course, full polymorphic typing goes well be- yond simply analyzing \ncode in a modular way, since it allows different type instantiations for different uses of a function \n-akin to context-sensitive analysis -which is beyond the scope of the present discussion.) This modular \napproach to analysis has a long history. Ac-cording to folklore, one version of the MIPS compiler em- \nployed local analysis of separate files and then combined the local analysis results during a \"linking\" \nphase. The idea is also implicit in Aiken et. al.'s set-constraint type systems [3], and is much more \nexplicit in Flanagan and Felleisen's componential analysis for set-based analysis [12]. Recently, the \nidea has also been applied to points-to analysis. Das [8] describes a hybrid unification-based points-to \nanalysis with the following steps. First, each source file is parsed, and the assignment statements therein \nare used to construct a points-to graph with flow edges, which is simplified using a propagation step. \nThe points-to graph so computed is then \"serialized\" and written to disk, along with a table that associates \nsymbols and functions with nodes in the graph. The second phase reads in all of these (object) files, \nunifies nodes corresponding to the same symbol or function from different object files, and reapplies \nthe propagation step to obtain global points-to information. In other words, the analysis algorithm is \nfirst applied to individual files and the internal state of the algorithm (which in this case is a points- \nto graph, and symbol information) is frozen and written to a file. Then, all of these files are thawed, \nlinked and the algorithm re-applied. This means that the object files used are specific not just to a \nparticular class of analysis (points-to analysis), but to a particular analysis algorithm (hybrid unification-based \nanal- ysis), and arguably even to a particular implementation of that algorithm. The object files are \ndesigned with specific knowledge of the internal data-structures of an implemen-tation in such a way \nthat the object file captures sufficient information about the internal state of the implementation that \nthis state can be reconstructed at a later stage. The CLA Model Our approach, which we call compile-link-analyze \n(CLA), also consists of a local computation, a linking and a global analysis phase. However, it represents \na different set of tradeoffs from previous approaches, and redraws the bound- aries of what kind of work \nis done in each phase. A key difference is that the first phase simply parses source files and extracts \nassignment statements -no actual analysis is performed -and the linking phase just links together the \nassignment statements. One advantage of the CLA architec- ture is that the first two phases remains unchanged \nfor many different implementations of points-to armlysis and even dif- ferent kinds of analysis (we return \nto this point later). A follow-on advantage is that we can justify investing resources into optimizing \nthe representation of the collections of as- signments, because we can reuse this work in a number of \ndifferent analysis implementations. In particular, we have developed a database-inspired representation \nof assignments and function definitions/calls/returns. This representation is compact and heavily indexed. \nThe indexing allows rel- evant assignments for a specific variable to be identified in just one lookup \nstep, and more generally, it supports a mode where the assignments needed to solve a particular analysis \nproblem can be dynamically loaded from the database on demand. More concretely, CLA consists of three \nphases. The compile phase parses source files, extracts assignments and function calls/returns/definitions \n(in what follows we just call these \"assignments\"), and writes an object file that is basically an indexed \ndatabase structure of these basic program compo- nents. No analysis is performed yet. Complex assignments \nare broken down into primitive ones by introducing tempo- rary variables. The elements of the database, \nwhich we call primitive assignments, involve variables and (typically) at most one operation. The link \nphase merges all of the database files into one database, using the linking information present in the \nob- ject files to link global symbols (the same global symbol may be referenced in many files). During \nthis process we must recompute indexing information. The \"executable\" file produced has the same format \nas the object files, although its linking information is typically obsolete (and could be stripped). \nThe analyze phase performs the actual analysis: the linked object file is dynamically loaded on demand \ninto the running analysis. Importantly, only those parts of the object file that are required are loaded. \nAn additional benefit of the indexing structure of the object file is that when we have read information \nfrom the object file we can simply discard it and re-load it later if necessary (memory-mapped I/O is \nused to support efficient reading and re-reading of the object file). We use this feature in our implementation_ \nof Andersen's analysis to greatly reduce the memory footprint of the analysis. It allows us to maintain \nonly a very small portion of the object file in memory. An example source file and a partial sketch of \nits object file representation is given in Figure 4. These object files consist of a header section which \nprovides an index to the remaining sections, followed by sections containing linking information, primitive \nassignments (including information about function calls/returns/definitions) and string information, \nas well as indexing information tbr identifying targets for the depen- dence analysis. The primitive \nassignments are contained in the dynamic section; it consists of a list of blocks, one for each object \nin the source program. Each block consists of information about the object (its name, type, source code \nlocation and other attributes), followed by a list of prim- itive assignments where this object is the \nsource. For ex- ample, the block for z contains two primitive assigmnents, corresponding to the second \nand third assignments in the program (a very rough intuition is that whenever z changes, the primitive \nassignments in the block for z tell us what we must recompute). As mentioned before, one of the goals \nof our work is to build infrastructure that can be used for a variety of different anal- ysis implementations \nas well as different kinds of analysis. We have used our CLA infrastructure for a number of differ- ent \nsubset-based points-to analysis implementations (includ- ing an implementation based on bit-vectors, \nas well as many variations of the graph-based points-to algorithm described later in this paper), and \nfield-independent and field-based points-to analysis. The key point is that our object files do not depend \non the internals of our implementation and so we can freely change the implementation details without \nchanging the object file format. We have also used CLA infrastructure for implementing unification-based \npoints-to analysis, and for the dependence analysis described in Sec- tion 2. Finally, we note that we \ncan write pre-analysis op- timizers as database to database transformers. In fact, we have experimented \nwith context-sensitive analysis by writ- ing a transformation that reads in databases and simulates context-sensitivity \nby controlled duplication of primitive as- signments in the database -this requires no changes to code \nin the compile, link or analyze components of our system. We now briefly sketch how the dependence and \npoints-to analyses use object files. Returning to Figure 4, consider performing points-to analysis. The \nstarting point for points- to analysis is primitive assignments such as q = ~y in the static section. \nSuch an assignment says that y should be added to the points-to set for q. This means that the points- \nto set for q is now non-empty, and so we must load all prim- itive assignments where q is the source. \nIn this case, we load p = q, which imposes the constraint p D q. This is all we need to load for points-to \nanalysis in this case. Now consider a dependence analysis. Suppose that the target of the dependence \nanalysis is the variable z. We first look up \"z\" in the hashtable in the target section to find all vari- \nables in the object file whose name is \"z\" (strictly speaking, we find the object file offsets of all \nsuch variables). In this case we find just one variable. We build a data-structure to header section: \nsegment offsets and sizes global section: linking information static section: address-of operations; \nalways loaded for points-to analysis q= ~y file a.c: int x, y, z, x=y; x=z; *p= z; p=q; q= ~y; x = \n*p; string section: common strings target section: hashtable for finding targets *p, *q; dynamic section: \nelements are loaded on demand, organized by object x @ a.c:l none y @ a.c:l x = y @ a.c:2 ~ a.c:l x : \nz @ a.c:3 *p =z ~ a.c:4 p @ a.c:l x = *p ~ a.c:7 q Q a.c:1 p = q ~ a.c:5 Figure 4: Example program and \nsketch of its object file say that this variable is a target of the dependence analysis. We then load \nthe block for z, which contains the primitive assignments x = z and *p = z. Using the first assignment, \nwe build a data-structure for x and then we load the block for x, which is empty. Using the second assignment, \nwe find from the points-to analysis that p can point to g~y, and so we build a data-structure for y and \nload the block for y, etc. In the end, we find that both x and y depend on z. The compilation phase we \nhave implemented includes more information in object files that we have sketched here. Our object files \nrecord information about the strength of depen- dencies (see Section 2), and also information about any \noper- ations involved in assignments. For example, corresponding to a program assignment x = y + z, we \nobtain two prim- itive assignments x = y and x = z in the database. Each would retain information about \nthe \"+\" operation. Such information is critical for printing out informative depen- dence chains; it \nis also useful for other kinds of analysis that need to know about the underlying operations. We include \nsections that record information about constants in the pro- gram. To support advanced searches and experiment \nwith context-sensitive analysis, we also include information for each local variable that identifies \nthe function in which it is defined. We conjecture that our object file format can be used (or easily \nadapted) for any flow-insensitive analysis that computes properties about the values of variables i.e. \nany analysis that focuses entirely on the assignments of the program, and ignores control constructs. \nExamples include points-to analysis, dependence analysis, constant propaga- tion, binding-time analysis \nand many variations of set-based analysis. One advantage of organizing object files using sec- tions \n(much like COFF/ELF), is that new sections can be transparently added to object files in such a way that \nexist- ing analysis systems do not need to be rewritten. We conclude with a discussion of functions and \nfunction pointers. Function are handled by introducing standard- ized names for function arguments and \nreturns. For exam- ple, corresponding to a function definition int f(x, y) { .. return(z)}, we generate \nprimitive assignments x = fl, y = f2, f~t : z, where ffl, f2, f~ are respectively the standardized variables \nfor the two arguments of f and f's return value. Similarly, corresponding to a call of the form w = f(el, \ne2), we generate primitive assignments fl = el, f2 = e2 and w : f~t. These standardized names are treated \nas global objects, and are linked together, like other global objects, by the linker. The treatment of \nindirect function calls uses the same naming convention, however some of the linking of formal and actual \nparameters happens at analysis time. Specifically, corresponding to a function definition for g, there \nis an object file entry (in the block for g) that records the argument and return variables for 9. Corresponding \nto an indirect call (*f) (x, y), we mark f as a function pointer as well as adding the primitive assignments \nfl -- x, f2 = y, etc. During analysis, if a function g is added to the points-to set for f (marked as \na function pointer), then we load the record of argument and return variables for both f and g. Using \nthis information, we add new assignments gl = fl, g2 : f2 and fret = 9rct. 5. A GRAPH-BASED ALGORITHM \nFOR AN- DERSEN'S ANALYSIS Scalability of Andersen's context-insensitive flow-insensitive points-to analysis \nhas been a subject of much research over the last five years. One problem with Andersen's analysis is \nthe \"join-point\" effect of context-insensitive flow-insensitive analysis: results from different execution \npaths can be joined together and distributed to the points-to sets of many vari- ables. As a result, \nthe points-to sets computed by the anal- ysis cam be of size O(n) where n is the size of the program; \nsuch growth is commonly encountered in large benchmarks. This can spell scalability disaster if all points-to \nsets are explicitly enumerated. Aiken et. al. have addressed a variety of scaling issues for Andersen's \nanalysis in a series of papers. Their work has included techniques for elimination of cycles in the inclusion \ngraph [11], and projection merging to reduce redundancies in the inclusion graph [23]. All of these are \nin the context of a transitive-closure based algorithm, and their results show very substantial improvements \nover their base algorithm -with all optimizations enabled, they report analysis times of 1500s for the \ngimp benchmark on a SPARC Enterprise 5000 with 2GB [23]. Alternatively, context- and flow-sensitivity \ncan be used to 259 reduce the effect of join-points. However the cost of these ad- ditional mechanisms \ncan be large, and without other break- throughs, they are unlikely to scale to millions of lines of code. \nAlso, recent results suggest that this approach may be of little benefit for Andersen's analysis [13]. \nIn principle, ideas from sub-transitive control-flow analysis [18] could also be applied to avoid propagation \nof the in- formation from join-points. The basic idea of sub-transitive control-flow analysis is that \nthe usual dynamic transitive closure formulation of control-fiow analysis is redesigned so that the dynamic \nedge-adding rules are de-coupled from the transitive closure rules. This approach can lead to linear- \ntime algorithms. However, it is currently only effective on bounded-type programs, an unreasonable restriction \nfor C. The main focus of our algorithm, much like that of the sub- transitive approach, is on finding \na way to avoid the cost of computing the full transitive closure of the inclusion graph. We begin by \nclassifying the assignments used in the deduc- tive reachability system given in Section 3 into three \nclasses: (a) simple assignments, which have the form x ---- y, (b) base assignments, which have the form \nx = &#38;y, and (c) complex assignments, which have the form x = =~y or *x = y. For simplity, we omit \ntreatment of *z = *y; it can be split into *x =-z and z -= ~yj where z is a new variable. In what follows \nwe refer to items of the form &#38;y as Ivals. The central data-structure of our algorithm is a graph \nQ, which initially contains all information about the simple as- signments and base assignments in the \nprogram. The nodes of G are constructed as follows: for each variable x in the program, we introduce \nnodes n~ and n,~ (strictly speaking, we only need a node n,~ if there is a complex assignment of the \nform y = *x). The initial edges of ~ are constructed as follows: corresponding to each simple assignment \nx = y, there is an edge n~ --+ n u from nz to nu. Corresponding to every node n in G, there is a set \nof base elements, defined as follows: baseElements(n~) = {y : x = &#38;y appears in P} The complex assignments, \nwhich are not represented in G, are collected into a set C. The algorithm proceeds by it- erating through \nthe complex assignments in C and adding edges to G based on the information currently in G. At any point \nin the algorithm, G represents what we explicitly know about the sets of lvals for each program variable. \nA major departure from previous work is that ~ is maintained in pre-transitive form i.e. we do not transitively \nclose the graph. As a result, whenever we need to determine the current lvals of a specific variable, \nwe must perform graph reachability: to find the set of lvals for variable x, we find the set of nodes \nreachable from n~ in zero or more steps, and compute the union of the baseElements sets for all of these \nnodes. We use the function getLvals(n~) to denote this graph reachability computation for node n~. The \nprocess of iterating through the complex assignments in C and adding edges to ~ based on the information \ncurrently in Q is detailed in Figure 5. Note that line 7 need only be executed once, rather than once \nfor each iteration of the loop. Before discussing the getLvals 0 fimction, we give some in- tuition on \nthe computational tradeoffs invotved in maintain- ing the constraint graph in pre-transitive form and \ncomput- ing lvals on demand. First, during its execution, the algo- rithm only requires the computation \nof lvals for some subset of the nodes in the graph. Now, of course, at the end of' the algorithm, we \nmay still have to compute all lvals for all graph nodes. However, in the presence of cycle-elimination \n(dis- cussed shortly), it is typicMly much cheaper to compute all lvals for all nodes when the algorithm \nterminates than it is to do so during execution of the algorithm. Second, the pre- transitive algorithm \ntrades off traversal of edges versus flow of lvals along edges. More concretely, consider a complex assignment \nsuch as *x = y, and suppose t~hat the set of lvals for x includes &#38;xl and &#38;x2. As a result of \nthis complex assignment, we add edges from xl to y and x2 to y. Now, in an algorithm based on transitive-closure, \nall lvals asso-ciated with y will flow back along the new edges inserted and from there back along any \npaths that end with xl or x2. In the pre-transitive graph, the edges are added and there is no flow of \nlvals. Instead, Ivals are collected (when needed) by a traversal of edges. In the transitive closure \ncase, there are O(n.E) transitive closure steps, where n is the average number of distinct lvals that \nflow along an edge, and E is the number of edges, versus O(E) steps per reach- ability computation This \ntradeoff favors the pre-transitive graph approach when E is small and n is large. (We re-mark that this \nis analysis is for intuition only; it is not a formal analysis, since neither the transitive closure \nstep nor the reachability step are O(1) operations.) We next describe getLvals0, which is the graph reachabil- \nity component of the algorithm. A key part of the graph reachability algorithm is the treatment of cycles. \nNot only is cycle detection important for termination, but it has fun- damental performance implications. \nThe first argument of getLvals 0 is a graph node and the second is a list of ele- ments that define the \npath we are currently exploring; top- level calls have the form getLvals(n, nil). Each node in has a \none-bit field onPath. The function uni:fyNodes 0 merges two nodes. We imple- ment node merging by introducing \nan optional skip field for each node. Two nodes nl and n2 are then unified by setting skip(n1) to n2, \nand merging edge and baseElement information from nl into n2. Subsequently, whenever node nl is accessed, \nwe follow its skip pointer. We use an incre- mental algorithm for updating graph edges to skip-nodes \nto their de-skipped counter-parts. Cycle elimination was first used for points-to analysis by F/ihndrich \net. al [11]. In their work, the cost of finding cy- cles was non-trivial and so completeness of cycle \ndetection was sacrificed in order to contain its cost. In contrast, cy- cle detection is essentially \nfree in our setting during graph teachability computations. Moreover, we find almost all cy- cles -more \nprecisely, we find all cycles in the parts of the graphs we traverse during graph reachability. In essence, \nwe find the costly cycles -those that are not detected are in parts of the graph that we ignore. In other \nwords, one of the benefits of our algorithm is that it finds more of the important cycles and it does \nso more cheaply. 1. I. 2. 3. 4. 5. 6. 7. 8. 9. I0. /* The Iteration Algorithm */ do { nochange = true; \nfor each complex assignment *x = y in for each Rz in getLvals(nz) add an edge nz ~ ny to ~; for each \ncomplex assignment x = *y in add an edge n~ --~ n,y; for each &#38;z in getLvals(ny) add an edge n~y \n-+ nz } until nochange C 2. 3. 4. 5. 6. 7. 8. 10. 11, 12, 13. 14. 15. getLvals(n, path) { \u00b1f(onPath(n)) \n< /* we have a cycle */ foreach n' in path, unifyNode(n', n); return(emptySet); } else { /* explore \nnew node n */ onPath(n) = 1; Ivals = emptySet; path = cons(n, path); Ivals = union(ivals, baseElements(n); \nforeach n' such that there is an edge from n to n' Ivals = union(ivals, getLvals(n, path)); onPath(n) \n= O; return(ivals); }  Figure 5: The Pre-Transitive Graph Algorithm for Points-to Analysis This completes \nthe basic description of our algorithm. We conclude with a number of enhancements to this basic al- gorithm. \nFirst, and most important, is a caching of reach- ability computations. Each call to getLvals 0 first \nchecks to see if the Ivals have been computed for the node during the current iteration of the iteration \nalgorithm; if so, then the previous lvals axe returned, and if not, then they are recom- puted (and stored \nin the node). Note that this means we might use \"stale\" information; however if the information is indeed \nstale, the nochange flag in the iteration algorithm will ensure we compute another iteration of the algorithm \nusing fresh information. Second, the graph edges are main- tained in both a hash table and a per-node \nlist so that it is fast to determine whether an edge has been previously added and also to iterate through \nall of the outgoing edges from a node. Third, since many lval sets are identical, a mechanism is implemented \nto share common lvals set. Such sets axe im- plemented as ordered lists, mad are linked into a hash table, \nbased on set size. When a new lval set is created, we check to see if it has been previously created. \nThis table is flushed at the beginning of each pass through the complex assign- ments. Fourth, lines \n4-5 and lines 8-9 of Figure 5 are changed so that instead of iterating over all lvals in getLvals (n~), \nwe iterate over all nodes in getLvalsNodes (n~). Conceptually, the function getLvalsNodes() returns all \nof the de-skipped nodes corresponding to the lvals computed by getLvals (); however it can be implemented \nmore efficiently. From the viewpoint of performance, the two most signif-icant elements of our algorithm \nare cycle elimination and the caching of teachability computations. We have observed a slow down by a \nfactor in excess of > 50K for gimp(45,000s c.f. 0.8s user time) when both of these components of the \nalgorithm are turned off. 6. RESULTS Our analysis system is implemented using a mix of ML and C. The \ncompile phase is implemented in ML using the ckit frontend[6]. The linker and the analyzer axe implemented \nin C. Our implementation deals with full C including structs, unions, arrays and function call/return \n(including indirect calls). Support for many of these features is based on simple syntactic transformations \nin the compile phase. The field- based treatment of structs is implemented as follows: we generate a \nnew variable for each field f of a struct defini-tion, and then map each access of that field to the \nvariable. Our treatment of axrays is index-independent (we essentially ignore the index component of \nsub expressions). The bench- marks we use are described in Table 2. The first six bench- marks were \nobtained from the authors of [21], and the lines of code reported for these are the number of lines of \nnon- blank, non-# lines in each case. We do not currently have accurate source line counts for these \nbenchmarks. The sev- enth benchmark was obtained from the authors of [23]. The last benchmark is the \nLucent code base that is the main tar- get of our system (for proprietary reasons, we have not in- cluded \nM1 informations on this benchmark). For each bench- mark, we also measure the size of the preprocessed \ncode in bytes, the size of the object files produced by the analy- sis (compiler + linker, also in bytes), \nand the number of primitive assignments in the object files - the five kinds of assignments allowed in \nour intermediate language. We remark that line counts are only a very rough guide to program size. Source \ncode is misleading for many rea- sons. For instance, macro expansion can considerably in- crease the \namount of work that must be performed by an analysis. Preprocessed code is misleading because many ex- \ntraneous extern declarations axe included as the result of generic system include files. Moreover, these \nsystem include files can vary considerably in size from system to system. AST node counts of preprocessed \ncode are a better mea- sure of complexity because they de-emphasize the effects of coding style; however \nthere is no agreed upon notion of AST nodes, and AST nodes might still be inflated by unnecessary dedarations \ngenerated by rampant include files. Counts of primitive assignments may be a more robust measure. Results \nfrom these benchmarks are included in Table 3. These results measure analysis where (a) each static oc- \ncurrence of a memory allocation primitive (malloc, calloc, etc.) is treated as a fresh location, and \n(b) we ignore con- stant strings. This is the default setup we use for points-to and dependence analysis. \nThe first column of Table 3 rep- resents the count of program objects (variables and fields) for which \nwe compute non-empty pointer sets; it does not include any temporary variables introduced by the analysis. \nThe second column represents the total sizes of the points-to sets for all program objects. The third \nand fourth columns give wall-dock time and user time in seconds respectively, as reported by/bin/time \nusing a single processor of a two processor Pentium 800MHz machine with 2GB of memory running Linux 2. \nThe fifth column represents space utiliza- 2Red Hat Linux release 6.2 (Piglet) VA Linux release 6.2.3 \n07/28/00 bl.1 P2 Kernel 2:2.14-VA.5.1smp on a 2-processor i686. LOC LOC preproc, object (source) (preproc.) \nsize size nethack 44.1K 1.4MB 0.TMB burlap 74.6K 2.4MB 1.4MB vortex 170.3K 7.7MB 2.6MB emacs 93.5K 40.2MB \n2.6MB povray 175.5K 68.1MB 3.1MB gee 199.8K 69.0MB 4.4MB gimp 440K 7486.7K 201.6MB 27.2MB lucent 1.3M \n20.1MB Table 2: tion in MB, obtained by summing static data and text sizes (reported by /bin/size), \nand dynamic allocation (as re-ported by malloc_statsO). We note that for the lucent benchmark -the target \ncode base of our system -we see total wall-clock times of about half a second, and space uti- lization \nof under 10MB. The last three columns explain why the space utilizations are so low: these columns respectively \nshow the number of primitive assignments maintained \"in-core\", the number loaded during the analysis, \nand the total number of primitive assignments in the object file. Note that only primitive as-signments \nrelevant to aliasing analysis are loaded (e.g. non-pointer arithmetic assignments are usually ignored). \nRecall that once we have loaded a primitive assignment from an ob- ject file and used it, we can discard \nit, or keep it in memory for future use. Our discard strategy is: discard assignments x = y and x = &#38;y, \nbut maintain all others. These num-bers demonstrate the effectiveness of the load-on-demand and load-and-throw-away \nstrategies supported by the CLA architecture. Table 4 studies the effect of changing the baseline system. \nThe first group of columns represents the baseline and is just a repeat of information from Table 3. \nThe second group shows the effect of changing the underlying treat-ment of structs from field-based to \nfield-independent. We caution that these results are very preliminary, and should not be interpreted \nas a conclusive comparison of field-based and field-independent. In particular, there are a number of \nopportunities of optimization that appear to be especially important in the field-independent case that \nhave not im- plemented in our current system. We expect that these op- timizations could significantly \nclose the time and space gap between the two approaches. However, it is clear that the choice between \nfield-based and field-independent has signifi- cant implications in practice. Most points-to systems \nin the literature use the field-independent approach. Our results suggest that the field-based might \nin fact represent a better tradeoff. The question of the relative accuracy of the two approaches is open \n-even the metric for measuring their relative accuracy is open to debate. We conclude by briefly discussing \nempirical results from re- lated systems in the literature. Since early implementations of Andersen's \nanalysis [22], much progress has been made [11, 23, 21]. Currently, the best results for Andersen's are \nanalysis times of about 430 seconds for about 500K lines of code (using a single 195 MHz processor on \na multi-processor program t assignm~lt s______r__r_ ___:__ ~ variables X = y x = &#38;y ] *x = y I *x \n= ~..~_1~ 3856 9118 1115 30 34 6859 14202 1049 1160 714 1897 11395 24218 7458 353 231 1866 12587 31345 \n3461 614 154 1029 12570 29565 4009 2431 1190 3085 18749 62556 3434 1673 585 1467 131552 303810 25578 \n5943 2397 6428 96509 270148 72355 1562 991 3989_ Benchmarks SGI Origin machine with 1.5GB) [21]. The \nmain limiting factor in these results is that space utilization (as measured by the amount of live data \nafter GC) is 150MB and up -in fact the largest benchmark in [21] ran out of memory. Re-sults from [23] \nreport analysis times of 1500s for gimp (on a SPARC Enterprise 5000 with 2GB). We note that both of these \nimplementations of Andersen's analysis employ a field-independent treatment of structs, and so these \nresults are not directly comparable to ours (see the caveats above about the preliminary nature of results \nin Table 4). Implementations of Steensgaard's algorithm are faster and use less memory. Das reports that \nWord97 (about 2.2 mil- lion lines of code) runs in about 60s on a 450MHz Intel Xeon running Windows NT \n[8]. Das also reports that modifica- tions to Steensgaard's algorithm to improve accuracy yield analysis \ntimes of about 130s, and memory usage of \"less than 200MB\" for the same benchmark. We again note that \nDas uses a field-independent treatment of structs. 7. CONCLUSION We have introduced CLA, a database-centric \nanalysis archi- tecture, and described how we have utilized it to implement a variety of high-performance \nanalysis systems for points- to analysis and dependence analysis. Central to the per- formance of these \nsystems are CLA's indexing schemes and support for demand-driven loading of database components. We have \nalso described a new algorithm for implementing dynamic transitive closure that is based on maintaining \na pre-transitive graph, and computing teachability on demand using caching and cycle elimination techniques. \nThe original motivation for this work was dependence anal- ysis to help identify potential narrowing \nbugs that may be introduced during type modifications to large legacy C code bases. The points-to analysis \nsystem described in this paper has been built into a forward data-dependence analysis tool that is deployed \nwithin Lucent. Our system has uncovered many serious new errors not found by code inspections and other \ntools. Future work includes exploration of context-sensitivity, and a more accurate treatment of structs \nthat goes beyond field- based and field-independent (e.g. modeling of the layout of C structs in memory[7], \nso that an expression x.f is treated as an offset \"f\" from some base object x) Acknowledgements: Thanks \nto Satish Chandra and Jeff Foster for access to their respective systems and bench- 262 ] pointer variables \npoi to ii tea loser relations time time processsize IJin assignmentscore loaded in file nethack 1018 \n7K I 0.03s 10.01s 5.2MB 114 5933 10402 burlap 3332 201K 0.08si0.03s 5.4MB 3201 12907 19022 vortex 4359 \n392K 0.15s 0.11s 5.7MB 1792 15411 34126 emacs 8246 11232K 0.54s 0.51s 6.0MB 1560 28445 36603 povray 6126 \n141K i0.11s 0.09s 5.7MB 5886 27566 40280 gcc 11289 123K 0.20s 0.17s 6.0MB 2732 53805 69715 gimp 45091 \n15298K 1.05s 1.00s 12.1MB 8377 144534 344156 lucent 22360 3865K 0.46s 0.38s 8.8MB 4281 101856 349045 \n Table 3: Results field-based field-independent i(Preliminary) pointers relations utime size I pointers \nrelations utime [ size nethack 1018 7K 0.01s 5.2MB 1714 97K 0.03s 5.2MB burlap 3332 201K 0.03s 5.4MB \n2903 323K 0.21s 5.9MB vortex 4359 392K 0.11s 5.7MB 4655 164K 0.09s 5.7MB emacs 8246 11232K 0.51s 6.0MB \n8314 14643K 1.05s 6.7MB povray 6126 141KI 0.09s 5.7MB 5759 1375K 0.39s 6.6MB gcc 11289 123K 0.17s 6.0MB \n10984 408K 0.65s 8.8MB gimp 45091 15298K 1.00s 12.1MB 39888 79603K 30.12s 18.1MB lucent 22360 3865K 0.46s \n8.8MB 26085 19665K 137.20s 59.0MB Table 4: Effect of a field-independent treatment of structs. marks. \n 8. REFERENCES [1] A. Aiken, M. F~hndrich, J. Poster, and Z. Su, \"A Toolkit for Constructing Type-and \nConstraint-Based Program Analyses\", TIC'98. [2] A. Aiken and E. Wimmers, \"Solving Systems of Set Constraints\", \nLICS, 1992. [3] A. Aiken and E. Wimmers, \"Type Inclusion Constraints and Type Inference\", ICFP, 1993. \n[4] L. Andersen, \"Program Analysis and Specialization for the C Programming Language\", PhD. thesis, DIKU \nreport 94/19, 1994, [5] D. Atkinson and W. Griswold, \"Effective Whole-Program Analysis in the Presence \nof Pointers\", 1998 Symp. on the Foundations of Soft. Eng.. [6] S. Chandra, N. Heintze, D. MacQueen, D. \nOliva and M. Sift, \"ckit: an extensible C frontend in ML\", to be released as an SML/NJ library. [7] S. \nChandra and T. Reps, \"Physical Type Checking for C\" PASTE, 1999. [8] M. Das, \"Unification-Based Pointer \nAnalysis with Directional Assignments\" PLDI, 2000. [9] \"Appendix D: Optimizing Techniques (MIPS-Based \nC Compiler)\", Programmer's Guide: Digital UNIX Version 4.0, Digital Equipment Corporation, March 1996. \n[10] J. Foster, M. FShndrich and A. Aiken, \"Flow-Insensitive Points-to Analysis with Term and Set Constraints\" \nU. of California, Berkeley, UCB//CSD97964, 1997. [11] M. Fg.hndrich, J. Foster, Z. Su and A. Aiken, \"Partial \nOnline Cycle Elimination in Inclusion Constraint Graphs\" PLDI, 1998. [12] C. Flanagan and M. Felleisen, \n\"Componential Set-Based Analysis\" PLDI, 1997. [13] J. Foster, M. F~hndrich and A. Aiken, \"Polymorphic \nversus Monomorphic Flow-insensitive Points-to Analysis for C\", SAS 2000. [14] N. Heintze, \"Set Based \nProgram Analysis\", PhD thesis, Carnegie Mellon University, 1992. [15] N. Heintze, ,Set-Based Analysis \nof ML Programs\", LFP, 1994. [16] N. Heintze, \"Analysis of Large Code Bases: The Compile-Link-AnalyzeModel\" \nunpublished report, November 1999. [17] N. Heintze and J. Jaffar, \"A decision procedure for a class \nof Herbrand set constraints\" LICS, 1990. [18] N. Heintze and D. McAllester, \"On the Cubic-Bottleneck \nof Subtyping and Flow Analysis\" LICS, 1997. [19] \"Programming Languages -C\", ISO/IEC 9899:1990, Internation \nStandard, !990. [20] D. McAllester, \"On the Complexity Analysis of Static Analysis\", SAS, 1999. [21] \nA. Rountev and S. Chandra, \"Off-line Variable Substitution for Scaling Points-to Analysis\", PLDI, 2000. \n[22] M. Shapiro and S. Horwitz, \"Fast and Accurate Flow-Insensitive Points-To Analysis\", POPL, 1997. \n[23] Z. Su, M. F\u00a3hndrich, and A. Aiken, \"Projection Merging: Reducing Redundancies in Inclusion Constraint \nGraphs\", POPL, 2000. [24] B. Steensgaard, \"Points-to Analysis in Almost Linear Time\", POPL, 1996. [25] \nF. Tip, \"Generation of Program Analysis Tools\", Institute for Logic Language and Computation dissertation \nseries, 1995-5, 1995. 263   \n\t\t\t", "proc_id": "378795", "abstract": "<p>We describe the design and implementation of a system for very fast points-to analysis. On code bases of about million lines of unpreprocessed C code, our system performs field-based Andersen-style points-to analysis in less than a second and uses less than 10MB of memory. Our two main contributions are a database-centric analysis architecture called compile-link-analyze (CLA), and a new algorithm for implementing dynamic transitive closure. Our points-to analysis system is built into a forward data-dependence analysis tool that is deployed within Lucent to help with consistent type modifications to large legacy C code bases.</p>", "authors": [{"name": "Nevin Heintze", "author_profile_id": "81100251839", "affiliation": "Research, Agere Systems, Lucent's Microelectronics Division", "person_id": "P208265", "email_address": "", "orcid_id": ""}, {"name": "Olivier Tardieu", "author_profile_id": "81100366768", "affiliation": "Ecole des Mines, Paris", "person_id": "PP28003137", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/378795.378855", "year": "2001", "article_id": "378855", "conference": "PLDI", "title": "Ultra-fast aliasing analysis using CLA: a million lines of C code in a second", "url": "http://dl.acm.org/citation.cfm?id=378855"}