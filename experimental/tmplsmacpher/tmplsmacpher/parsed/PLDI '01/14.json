{"article_publication_date": "05-01-2001", "fulltext": "\n Using Annotations to Reduce Dynamic Optimization Time Chandra Krintz Brad Calder University of California, \nSan Diego Department of Computer Science and Engineering La Jolla, CA 92093-0114 { ckrintz,calder } \n@cs.ucsd.edu Abstract Dynamic compilation and optimization are widely used in heterogenous computing \nenvironments, in which an inter-mediate form of the code is compiled to native code during execution. \nAn important tradeoff exists between the amount of time spent dynamically optimizing the program and \nthe running time of the program. The time to perform dynamic optimizations can cause significant delays \nduring execution and also prohibit performance gains that result from more complez optimization. fn this \nresearch, we present an annotation framework that substantially reduces compilation overhead of Java \nprograms. Annotations consist of analysis information collected off-line and are incorporated into Java \nprograms. The annotations are then used by dynamic compilers to guide optimization. The annotations we \npresent reduce compilation overhead in- curred at all stages of compilation and optimization as well \nas enable complex optimizations to be performed dynami- cally. On average, our annotation optimizations \nreduce op- timized compilation overhead by 78 ~ and enable speedups of 7~ on average for the programs \nexamined. 1. INTRODUCTION The execution model for mobile programs consists of code and data first being \ntransferred to a remote destination and then executed. Typically, an architecture-independent pro-gram \nrepresentation (e.g., bytecodes for the Java language) is shipped to the execution site and interpreted \nby a vir-tual machine. However, to overcome the performance lim- itations interpretation usually imposes, \nthese systems now employ just-in-time compilation [25, 2, 19, 11]. These new virtual machines dynamically \ncompile the bytecode stream (on a method-by-method basis) into machine code before executing it. The \nresulting execution time is lower than for interpreted bytecodes, but execution must pause each time \na method is initially invoked so that it may be compiled. Permission to make digital or hard copies of \nall or part of this work for personal or classroom use is granted without fee provided that copies are \nnot made or distributed for profit or commercial advan-tage and that copies bear this notice and the \nfull citation on the first page. To copy otherwise, to republish, to post on servers or to redfstribute \nto lists, requires prior specific permission and/or a fee. PLD/2001 6/01 Snowbird, Utah, USA &#38;#169; \n2001 ACM ISBN 1-58113-414-2101106..,$5.00 Dynamically compiling and optimizing a program can cause significant \ndelays during execution. Most systems attempt to reduce this delay in one of two ways. The first approach \nis to invoke an optimizing compiler and resort to interpre- tation if the compilation delay exceeds an \narbitrary thresh- old [11, 20]. The other approach uses two dynamic compil- ers, a fast, non-optimizing \ncompiler, and a second optimizing compiler [5, 7]. The program is compiled first with the fast compiler, \nand then frequently executed methods are com-piled later with the optimizing compiler based on dynamic \ninformation gathered during execution. In [17], we exam- ine the use of off-line profile information \nto decide which compiler to use initially, however, we provide no automatic mechanism for the communication \nof this information to compilation system. In this paper, we present such a mech- anism that introduces \nannotation into the bytecode stream to communicate compilation analysis as well as off-line pro- file \ninformation. The goal of our research is to minimize the overhead introduced by dynamic compilation while \nachiev- ing optimized execution speeds. Existing annotation-based techniques annotate Java byte- code \nwith analysis information that is time-consuming to collect to guide dynamic compilation [4, 13, 21, \n10]. The goal of this prior work was to make costly optimizations feasible in dynamic compilation settings. \nIn this paper, we extend annotation-based compilation and optimization (1) to provide a general annotation \nrepresentation to guide dy- namic compilation, (2) to examine the effects of using an- notations to reduce \nthe startup delay and intermittent in-terruption caused by dynamic optimization, (3) to examine new profile-based \nannotations to guide, optimization, and (4) to generate annotations that do not increase the application \ntransfer size. The latter is very import.ant if annotated- execution is to be used in a mobile environment. \nIf the size of the annotations are not very small, they can intro- duce significant transfer delay which \ncan negate any benefit achieved through the use of the annotations. Since we in- tend for our annotation \noptimizations to be used in a mobile environment, we ensure that they not only improve perfor- mance \nat runtime but do not introduce trdnsfer overhead. A primary contribution of this work is the implementation \nof annotations that increase the size of annotated applications by less than 0.05% on average. Another \ncontribution our work makes is the reduction in program startup time. We have found that most of the \ndynamic compilation for Java programs occurs at program 156 startup. In the programs studied, 77% of \nof the compila- tion overhead occurs in the first 4 seconds (initial 10%) of program execution on average. \nThe application of our tech- niques reduces startup delay by more than 2 seconds in many cases which \nenables significantly more progress to be made by the programs. Startup delay has been the focus of much \npast research since it substantially effects a user's produc- tivity and perception of program performance \n[8, 24]. Using annotations extends and compliments these and many other efforts [15, 16, 23] to substantially \nreduce the startup time of mobile programs. 'vVe use the Open Runtime Platform (ORP) [19] from Intel \nCorporation as our experimental implementation infrastruc- ture. The compilation system and performance \ncharacter- istics of ORP are similar to other existing platforms [2, 19, 11] for dynamic Java optimization. \nIn the next section, we detail the ORP compilation environment and the overhead inherent in its optimization \nsystem. In Section 3, we de- scribe our annotation framework and multiple annotation- based optimizations. \nWe then empirically evaluate the effect of our optimizations in Section 4. We conclude the paper with \nrelated work and our conclusions in Sections 5 and 6. 2. THE OPEN RUNTIME PLATFORM The performance of \nmobile Java programs is greatly im- proved through the use of dynamic compilation over inter- pretation. \nHowever, the compilation process is more com- plex and imposes longer, intermittent delays during execu- \ntion since execution must pause waiting for compilation to complete. For this research, we use an open-source, \ndual-compiler sys- tem called the Open l~untime Platform (ORP), which was recently released by the Intel \nCorporation [19]. The first compiler (O1) provides very fast translation of Java pro- grams [1] and incorporates \na few very basic bytecode op- timizations that improve execution performance. The sec-ond (03) compiler \nperforms a small number of commonly used optimizations on bytecode and an intermediate form to produce \nimproved code quality and execution time. 03 optimization algorithms were implemented with compilation \noverhead in mind, hence only very efficient algorithms are used [7]. The compilation overhead, total \ntime, and the number of methods compiled by the ORP compilers is shown in Table 1. The applications are \ncommonly used ,Java appli- cations including some of the SPECJvm benchmarks. The input used for collection \nof these statistics is denoted as Inputl in the results section. Total time consists of both compile \nand execution time. For comparison, 03 execution time is 5% faster than O1 execugion time on average \nand the compilation time of the 03 compiler is 89% slower than that for O1 on average for the programs \nstudied. To evaluate where ORP compilation time is spent, Figure 1 gives a breakdown of where time is \nspent during the differ- ent 03 compilation phases. We use these results along with the speedups resulting \nfrom the different optimizations and their combinations in order to determine which optimiza- tions might \nbenefit from using annotations. The y-axis is time in seconds; the average O3 compile time for the bench- \nmarks is approximately 2.7 seconds. The bar for each ap- plication is broken down into eight pieces. \nOther denotes Table 1: ORP Compilation and total time (execu- tion plus compilation). O1 is the ORP fast \ncompiler, 03 is the ORP optimizing compiler. Columns 2 and 3 are O1 and 03 compilation overhead, respectively. \nColumns 3 and 4 show total time. Columns 5 and 6 show the number of classes and methods compiled, respectively \n(over the total number of classes and methods). Cmpl Time Total Time(s) in sees in secs Classes Methods \nProgram O1 03 o~I 03 Cmpl'd Cmpl'd Jack 0.30 2,80 42.2 41.5 46/56 295/315 JavaCup 0.30 3.20 48.9 47.7 \n3V36 216/385 Jess 0.32 2,55 44.1 42.9 134/151 445/690 JSrc 0.26 3.06 49.6 48.2 48/194 436/2o66 Mpeg 0.28 \n2.37 37.8 31.2 42/5~ 223/322 Soot 0.33 1.74 7.0 6.9 379/721 1119/3607 Avg 0.30 2.62 .....38.3 30.4 113/202 \n456/1231 3,ff 3.C 2.fi .E 2.0 i-. 1.5 E 1.0 0,5 0.0 Jack JavaCup Jess JSrc Mpeg Soot Avg Figure 1: ORP \n03 (Optimizing) Compilation Time Breakdown. The y-axis is time in seconds. memory allocation of data \nstructures and any other code transformation costing less that 100 milliseconds. Const-prop is constant \nand copy propagation. Global-reg is global register Mlocation, i.e., the time to allocate physical registers \nto the local variables of a method. Build-it is the interme- diate form translation time; the bytecode \nof each method is converted to a lower-level form for further optimization. DCE is dead code elimination. \nLocal-reg is local register al- location, i.e~, the time to allocate physical registers to tem- porary \nvariables required by the translation. Fg-create is the time for flow-graph construction, and loop-opts \nis the time for loop optimization.  3. ANNOTATION-BASED OPTIMIZATIONS A compiler annotation is additional \ninformation attached to program code and data to help guide optimization. Anno-tations have been widely \nused on program source code in various languages to exploit parallelization and optimiza- tion opportunities \nin parallel and distributed codes. More recently, annotation-based techniques have focused on com- municating \ninformation that aids optimization, but is too time-consuming to collect on-line [4, 13, 21, 10]: The \ngoal of these efforts has been to make costly optimizations feasible in dynamic compilation settings. \n We extend annotation-based compilation and optimization to provide a general annotation representation \nthat mini-mizes the number of bytes used to represent the annotation. To this end, we incorporate compression \ninto our frame-work and ensure that all annotations implemented impose very little space overhead in \nthe program bytecode stream. In addition, we examine using new static and profile-based optimization \ntechniques to guide dynamic compilation. 3.1 Annotation Framework Our framework incorporates a bytecode \nrewriting tool called BIT [18] with which we insert annotations into a Java pro- gram. Annotations are \nincluded in class files and are trans- ferred as part of the bytecode stream when remotely exe-cuted. \nAnnotations are stored in a bytecode data structure called an attribute as defined in the Java language \nspecifi- cation [9]. An attribute data structure is defined for class files as well as for the methods \nand fields contained in the class. For example, a Code attribute is defined in the Java language specification \nas a method-level attribute and con-tains the actual bytecode. The name of each attribute at any level \n(class, method, field), is included in the constant pool of the class file and is used to distinguish, \nparse, and make use of the attribute. When a virtual machine encoun- ters an undefined attribute it is \nrequired to ignore it. This makes attributes ideal for the storage of annotations since it allows annotated \nclass files to remain compatible with all JVMs that are not annotation-aware. In this work, we add a \nsingle, user-defined, class attribute for annotations. We combine multiple annotations into the same \nattribute and use a single character of Unicode I9] to represent the name of the attribute in the constant \npool. These two design decisions minimize the size increase of a class file required for annotations. \nTo further reduce annota- tion size, annotations within each attribute are compressed using gzip compression. \nGzip is a standard compression util- ity, commonly used on UNIX operating system platforms. These decisions \nalso distinguish our framework from prior research in this area (see section 5). In our design, annotations \nof variable length appear sequen- tially in a given attribute. The encoding we chose for our annotation \nlanguage is very similar to an instruction set ar- chitecture (ISA) format for a variable length ISA. \nThe gen- eral format of an annotation attribute is a series of triples of the form: < opcode, size, data \n>. The opcode tells the compiler how to parse and make use of the annotation. In addition, since there \nare possibly many annotations in a sin- gle attribute, the compiler must be able to determine where one \nstarts and the next begins. This is done by including the size of the attribute after the opcode. The \nannotation data then appears right after the opcode and size. The elements of each annotation are summarized \nas: * opcode: (2 bytes) The identifier of this annotation that tells the compilation system how to parse \nand make use of the following annotation. The end of the attribute section, and thus all annotations, \nis indicated by a 0 opcode. . size: (2 bytes) The number of bytes for the annotation data that follows. \nThe maximum size of an annotation is 64KB. * data: (variable number of bytes) The annotated infor- mation. \n To incorporate the annotations, the compiler decompresses the annotations contained in the attribute \n(using the gzip compression library) and reads the opcode and size elements of the first triple (6 bytes \ntotal). It then looks up the opcode to determine the use of the annotation. The annotation is parsed \nand placed in the appropriate data structure in mem- ory or processed directly. For annotations that \nare specific to a method within the class, the first two bytes of data contain a method identifier. Annotations \ncan also be used across all methods in a class file; for these no method iden- tifier is needed. This \nparsing procedure is repeated for the next annotation until the end of the attribute is reached. We use \na 1-byte opcode of 0 to delimit the end of an annotation stream. 3.2 Annotation Implementation A goal \nof this research is to reduce compilation overhead while maintaining optimized execution time. The annota-tions \nwe describe next are meant to achieve this goal for the ORP compiler. As shown previously in Figure 1, \nthe dynamic optimization time in ORP is spread across mul-tiple operations. To reduce time spent overall, \nwe consider annotations that effect those phases that are the largest con- tributors to total compilation \noverhead. We examine using annotations of both static and profile- based analysis to enable efficient, \ndynamic optimization of methods. Static information is structural and syntactic in- formation explicitly \navailable in Java bytecode and class files. Profile-based information consists of runtime program characteristics \nand is collected by instrumenting and exe-cuting the programs off-line. We present four types of an- \nnotations and describe an implementation of each: those that provide static analysis information, those \nthat enable optimization reuse, those that enable selective optimization, and those that enable optimization \nfiltering. The name of each specific annotation we will provide results for is shown in parentheses at \nthe start of each paragraph describing the annotation. 3.2.1 Provision of Static Analysis Information \nAll compilers collect static information about the code they are compiling to perform translation, transformations, \nand optimization. For example, information about local vari- ables, control flow, exception handling, \netc., may be col- lected for an optimization by scanning the code. If the col- lection of data analysis \ncan be performed independent of its use, the analysis and acquisition of it can be performed off- line. \nWe first present annotations that communicate such analysis information to the compilation system in \nan effi-cient format. Since the analysis is performed off-line, dy- namic compilation overhead is reduced. \nGlobal Register Allocation Annotation (global-reg). The Java bytecode format is base upon a stack architecture \nand hence, it is difficult to achieve acceptable performance of Java pro- grams on register-based architectures \nwithout complex anal- ysis and algorithms for register allocation. Many commonly used allocation routines \nprioritize variables in order to ap- ply more advanced algorithms for the assignment of reg- isters, \ne.g., prioritized graph-coloring. In ORP, priorities are determined by static counts of local variable \nuses in the bytecode. Counting is performed by walking through each method. To avoid this bytecode scan, \nwe use annotations to indicate the priorities. In addition, more advanced pri- oritization (via profiling \nor static heuristics) can be used to improve register allocation in ORP, but this is left for future \nwork. For this global register allocation annotation, we make static counts of local variable usage just \nlike those made in ORP and communicate this information via the annotation. The data element of the annotation \ntriple consists of two bytes to indicate the method and one byte for each local vari- able. The bytes \nare arranged in the same order as are the local variables in the local variable array [9]. In the pro- \ngrams studied, the maximum number of local variables used by a single program is 1719 (JSrc); the maximum \nfor any single method is 31 (Mpeg). The average number of locals per method is 2.5. These numbers include \nmethods in the system class libraries also. For non-local class files, the max- imum number of local \nvariables used by a program is 1247 (JSrc) with an average method use of 2.6 variables. Flow Graph Generation \nAnnotation (fg-ereate). A flow graph is a data structure commonly used by compilers to identify changes \nin program control flow for effective and correct op- timization. Most Java compilers generate a flow \ngraph for every method to find basic block boundaries and other per- tinent control flow information \n[5, 7, 14]. This construction requires multiple passes of the Java bytecode. To reduce the time required \nfor such passes we implemented a flow graph generation annotation using our framework. We character- \nize the control flow structure of each method and use an annotation to present it to the compiler for \nsingle pass flow graph construction. We construct this annotation for each method to enable au- tomatic \ngeneration of the flow graph without the prepass operation. As with all other optimizations, fg-create \nis im- plemented with a single annotation per method (using a sin- gle opcode). The annotation is a list \nof the basic blocks in the method. The annotation begins with a two-byte method identifier (id) followed \nby the count of the total number of basic blocks that follow. Each basic block representation in- cludes \nthe block id, its (annotation) size, id numbers of the predecessor and successor blocks, start and ending \nbytecode indices, and other special information (loop header flag, ex- ception handling block, etc), \nif any. The annotation enables construction of an ORP flow graph for a method with a sin- gle scan of \nthe annotation. Across all benchmarks studied there are just under 14000 basic blocks and each method \nrequires 4.2 blocks on average. 3.2.2 Optimization Memory Reuse The goal behind the implementation of \nthis next annotation is to enable reuse of analysis information required for mul- tiple optimizations \nduring execution. Most dynamic com-pilers regenerate information about a method compilation instead of \nstoring it for possible reuse. Since a compiler is unable to predict which analysis information will \nbe reused by future phases of optimization, it must store all of the in- formation or repeatedly regenerate \nit. Regeneration is per- formed since storage of the analysis information can sub-stantially increase \nthe size of the memory footprint of the execution. Some compiler stages may also modify analy- sis information \nrequiring additional copies to be stored for reuse. Annotations can be used to indicate which data it \nis cost effective for the compiler to store for reuse. The anno-tation optimization we implement is for \nthe reuse of inlining information in ORP. Inlinin9 (inlining) Annotation. Inlining is a common opti-mization \nused by all compilers to reduce method or function call overhead. By inlining a method call, the call \nand return are removed, the inlined method code becomes part of the method which contained the call, \nand that code is optimized along with the rest of the code in the method during opti- mization. Commonly \nin dynamic compilation systems [5, 7], when a method is inlined into another, its (control) flow graph \nis generated, processed, and possibly optimized prior to insertion into the method it is inlined into. \nIf this method is later inlined into a different method, the process of flow graph creation and optimization \nis repeated. For methods that are inlined many times, much redundant work is per- formed by the compiler. \nIt is not desirable to keep all flow graphs in memory in ease of reuse, since it can dramatically increase \nthe size of the memory footprint unnecessarily. We therefore, analyzed off-line profiles to determine \nwhich exe- cuted methods might be inlined multiple times. For this optimization, we include one annotation \nper method, the annotation contains the method identifier and a single bit of information as the data \nelement. When this bit is set, it indicates to the compiler that the optimized flow graph should be stored \nin memory for reuse. When unset, the bit indicates that the flow graph should not be stored but generated \neach time. 3.2.3 Selective Optimization We next present selective optimization annotations that de- termines \nif a function should be optimized or not, or se-lects among the existing optimizing compilers available \non a system. In ORP, the O1 fast compiler or the 03 opti-mizing compiler can be used. For this type of \nannotation we identify the most important methods to optimize and indicate to the compilation system \nthat all other can be fast-compiled. This annotation can potentially reduce com- pilation overhead since \nmethods that are fast-compiled are more prevalent. It also enables methods that are most fre- quently \nexecuted (and hence, important for the overall exe- cution performance) to be optimized. Method Priority \nAnnotation (top25Vo). For this annota-tion, we use off-line profile data to predict the methods that \nshould be optimized. This is similar to the function of an adaptive compilation system in which methods \nare first com- piled with a very fast, non-optimizing compiler, then opti- mized when deemed hot. Hot-ness \nis identified using anal- ysis of on-line profiles enabled by method instrumentation. With this annotation, \nwe indicate whether a method is hot using off-line profiles obviating the need for omline instru- mentation \nand profiling. The annotation indicates whether a method should be compiled using the fast O1 compiler \nor 159 50 45 40 ~ 35 \"~ 25 E N 20 N 10 Jack JavaCup Jess JSrc Mpeg Soot Avg Figure 2: The histogram \nused to determine the per-cent of methods most important to optimize. Each bar shows the total time (compilation \nplus execu-tion) for the program when different percentages of the most frequently executed methods are \nop-timized. The remaining methods are compiled with the O1 compiler. The top 25% of most frequently executed \nmethods should be optimized on average for the best performance. optimized. To determine the percentage \nof methods that are important to optimize we gathered execution times for the histogram shown in Figure \n2. The graph shows the total time (execution plus compilation). For each bench: mark, each bar (within \na set of nine) indicates the total time given optimization of some percentage of the most fre- quently \nexecuted methods. For example the 0% bar (left- most in each set) shows the total time when 0% of the \nmeth- ods are O3-compiled (optimized) and 100% of the methods are Ol-compiled (fast). The 100% bag (right-most \nof each set) shows the total time when 100% of the methods are O3-compiled (optimized) and 0% of the \nmethods are O1-compiled (fast). The remaining bars represent the total time for various percentages between \n0 and 100. When 0% of the of the methods are O3-compiled (all of the methods are Ol-compiled), the total \ntime (compilation plus execution) is dominated by the execution time. O1-compilation time is very small \n(0.3 seconds for the entire application on average) but since no optimizations are per- formed, execution \ntime is slow (38 seconds on average). At the far right of the spectrum (right-hand bars of each set) \nin which all methods are O3-compiled, compilation time is very high (2.6 seconds on average) and optimization \nenables improved execution time (33.8 seconds on average). We gen- erated this histogram to discover \nthe bMance between these two extremes; the point at which both execution mad com- pilation time are at \ntheir minimum. The figure shows that the top 25% of frequently executed methods should be optimized to \nachieve the minimum com- pilation time as well as execution time. We used profile data to determine which \nmethods were contained in the top 25% in terms of invocation frequency. The annotation for selective \noptimization is similar to that for the reuse opti-mization: it contains a one-byte method id and sets \na single bit for each method in the top 25% most frequently executed methods. The bit indicates to the \ncompiler that the method should be optimized. 3.2.4 Annotations for Optim&#38;ation Filtering The final \ntype of annotation we describe is used for optimiza- tion filtering. Currently an optimizing compiler \nperforms all available optimizations on a method. To reduce com-pilation overhead, it may be beneficial \nto perform a subset of the optimizations when static or profile-based analysis of the method indicates \nthat it is profitable to do so. We construct an annotation for each method that consists of a two-byte \nmethod identifier and a 1-byte bit mask as part of the annotation data for each method that maps to a \nlist of expensive optimizations. For example, the first bit might map to inlining, the second to register \nallocation, the third to constant propagation, and so on. Each bit that is set in the mask indicates \nto the compiler that the associ- ated optimization should not be performed for that method. This annotation \nhas the potential for reducing compilation overhead by filtering time-consuming optimizations that do \nnot result in substantially improved execution performance on a per method level. We are able to determine \nwhich opti- mizations improve performance of methods using profiling. Constant Propa9ation Filtering \n(const-prop). In Figure 1, the ORP 03 compiler spends a substantial amount of time performing constant \npropagation. For some methods, the reduction in runtime performance after applying this opti- mization \nis not enough to warrant the use of the optimiza- tion. For those methods, we use the bit mask of this \nan-notation to indicate that the copy propagation optimization should be excluded. We gathered total \ntimes (execution plus compilation) for tile benchmarks for which constant propagation was used on var- \nious percentages of the most frequently executed methods. We created a histogram of these values like \nwe did for the se- lective optimization annotation and discovered that 70% of the methods benefit from \nusing constant propagation on av- erage. For all other methods, it is not cost effective in terms of \nexecution time to warrant using constant propagation. For each of these latter methods, we include an \noptimiza-tion filtering annotation that has the constant propagation bit set. This indicates to the compilation \nsystem that con-stant propagation should be bypassed during optimization of the method. 3.3 Annotation \nSecurity An important issue that must be addressed by annotation- based systems for mobile-computing \nenvironments is secu- rity. Annotations must be verified or guaranteed that their use will not corrupt \nthe JVM or machine on which they are used. Use of most existing bytecode annotation sys-tems poses serious \nsecurity risks since the annotations imple- mented using these systems affect program semantics and no \nverification mechanism is provided. If t~he bytecode stream is intercepted and modified by an untrusted \nparty, illegal and possibly destructive program behavior can result. The annotations we present here \nwith the exception of the flow-graph generation optimization, if modified with harm- ful intent, can \nonly effect program performance. As part of the empirical evaluation of our techmques, we measure the \neffect of the optimizations in a mobile environment for which only remote (non-library) classes ~u'e \nannotated. We do not include the flow-graph optimization in that part of the study (Section 4.2) to guarantee \nthat untrusted execu- tion using our annotations are safe. Benefits from our secure annotations are two-fold: \ntheir modification does not effect program semantics and they do not require the additional runtime overhead \nof verification. The latter is significant for our work, since our goal is to eliminate (not introduce) \nas much runtime overhead as possible. 4. RESULTS The results we present were gathered by executing the \nappli- cations 100 times on a dedicated 300Mhz x86 Intel Pentium II machine running Linux v2.2.15 and \ntaking the average. The applications we examine, described in Table 2, are part of the SPEC Java benchmark \nsuite and additional applica- tions commonly used in Java studies [16, 7, 22]. Two of the optimizations \nwe present (inlining and top25%) require pro-- file information for annotation construction as described \nin the previous section. Since compilation overhead (method use) is dependent upon program inputs, we \ngather execution profiles using two different inputs, called Inputl and Input2. Table 3 shows some differences \nin the execution character- istics between the two inputs. The first two columns show data for class \nfile counts, the last two columns for method counts. Above the slash in each column of data is the num- \nber of classes (and methods) used during execution of the application using the associated input; below \nthe slash we show the total static class (and method) counts. Inputl is used to generate all of the results \nin this section as well as the compilation time statistics shown in Table 1. We generate profiles and \nguide our optimizations using both inputs. We denote results that use the profile generated us- ing Inputl \nby \"Same\". Using the same data set for both pro- file and result generation provides ideal performance \nsince we have perfect information about the execution character- istics of the programs. Results denoted \nby \"Diff\" are those that use the profile generated using the Input2 to guide opti- mization. Diff, or \ncross-input, results indicate realistic per- formance since the characteristics used to perform the opti- \nmization can differ across inputs and the input that will be used is not commonly known ahead of time. \nAs mentioned above, results are executed using Input1 regardless of the input used for profile creation. \nThe overhead associated with annotations consists of class file size increase and the execution overhead \nneeded to pro- cess and make use of the annotations. We detail the former in Section 4.3. Any execution \noverhead imposed by anno- tations is included in the overall results. We first present results in terms \nof the reduction in compilation time result- ing from the use of our annotations. The number of seconds \nof compilation time reduced is shown in Figure 3. Each bar shows the reduction due to each of the individual \noptimiza- tions. The two, right-most bars of each group is the number of seconds reduced using all of \nthe annotations we examine in this paper. Const_prop, Inlining, and topS5 use profile Table 2: Description \nof Benchmarks Used. Jack Spec Benchmark: Java parser generator based ! on the Purdue Compiler Construction \nToolset JavaCup LALR Parser Generator: A 'parser ~s created to parse simple mathematics expressions Jess \nSpec Expert System Shell Benchmark Computes solutions to rule based puzzles JSrc Bytecode Processing \nTool: converts Java class files to HTML pages in a javadoc-like format Mpeg Spec Audio File Decompression \nBenchmark: Conforms to the ISO MPEG Layer-3 audio specification Soot Bytecode Processing Tool: c\u00b0nverts \nJava class files to an intermediate format: Jimple Table 3: General characteristics of execution using \ntwo different inputs. Executed/Total Executed/Total Classes Methods Program Inputl J Input2 .. Inputl \nInput2 J'&#38;ck 46/56 .......... 46/56 295/315 265/3i5 JavaCup 31/36 28/36 216/385 213/385 Jess 134/151 \n133/151 445/690 412/690 JSrc 48/194 48/194 436/2066 436/2066 Mpeg 42/55 42/55 223/322 201/322 Soot 379/371 \n379/371 1119/360T no6/360r Avg 113/202 113/202'\"\" 456/1231 439']1231 i 2.s c 2.0 \"7 ~ O.S Jack JavaCup \nJess JSrc Mpeg Soot Avg Figure 3: The graph shows the number of seconds of compilation overhead reduced \ndue to the anno-tation optimizations we present. They are broken down by optimization. The two, right-most \nbars of each group is the number of seconds reduced us-ing all of the annotations we examine in this \npaper. ConsLprop, Inlining, and ~op25 use profile informa- tion to guide the optimization. We therefore \nshow two bars of each of these optimizatlons: the first bar of each pair shows the cross-input (Diff) \nresults and the second shows the same-input results (Same). 161 3.s ,o, ][ t 3,0 --[ \u00a2 2.5 o ~ 2.0 = \nI N 1.5 \u00ae 1 o I  0.5 ~J 0.0 Jack JavaCup Jess JSrc Mpeg Soot Avg Figure 4: Total compilation overhead \nfor 03 and O1 compilation, and combined annotated compila-tion. All of the annotation-guided optimizations \npresented in this paper are included in the latter denoted by Armor. information to guide the optimization. \nWe therefore show two bars of each of these optimizations (as well as for the combined results). The \nfirst bar of each pair (Diff) shows the cross-input results (different inputs were used to gener- ate \nthe profile and the results). The second bar of each pair (Same) shows the effect of using the same input \nfor profile and result generation. On average, our annotation opti-mizations reduce compilation overhead \nby 1.9 seconds using imperfect information and over 2 seconds using perfect in- formation (this equates \nto a 78.1% reduction for cross-input and a 79% reduction for same-input results). Figure 4 shows the \ntotal compilation time required for op- timized compilation (O3), mnnotated compilation (Armor), and \nunoptimized compilation (O1) for same-input and cross- input configurations. Annot results show the combined \nel- feat from all of the annotation optimizations we describe in this paper. These results are the same \nas shown in Fig- ure 3, however~ they are in terms of resulting compile time instead of the number of \nseconds reduced and include O1- compilation for comparison. On average, annotated com-pilation time is \napproximately 250 milliseconds slower than O1 compile time and 2 seconds faster than O3 compile time. \nThe results in Figure 3 show that the majority of compile time reduction is due to two optimizations, \ninlining and top25%. This indicates that using these two optimizations alone is enough to achieve substantial \nperformance benefit. As such, we also have collected results for the combination of these two optimizations \nalone. This data is shown by the Annot-Inlining_top25 results in the following graphs. We next present \nthe effect of our annotation optimizations on total time: compilation and execution time combined. Figure \n5 shows the speedup achieved through the use of annotation over 03 total time for both of our annotation \nschemes, Annot and Annot-Inlining_top25. The former are the results for all of the annotations and the \nlatter are when only the inlining and selective optimization annotations are t ,~,=(o~ mA,,o,(s,mol 1 \n25% \" iZlAnnoHnllnlng_t op2S (Dtff} 13 Annot-lnlining_top25 (Same) 20% 15%,-i~ 10% .4-- Jack JavaCup \nJoss JSrc Mpeg Soot Avg Figure 5: Speedup over optimized (ORP 03) to-tal time due to annotated execution. \nAnnot-Inlining_top25 denotes results that use the inlining and top 25% annotations alone to guide dynamic \ncompilation. used. Results are shown for both input configurations (Same and Diff). The cross-input (Diff) \nresults are very similar to having perfect information (Same results). The overall effect on total time \nis more dramatic the shorter the execution time, as expected. Our annotation optimizations achieve 2% \nto 23% speedup over O3 total time for the programs studied. However, a more important result from annotated- \ncompilation is in its effect on startup time. 4.1 The Effect on Startup Time A significant contribution \nof this work is the reduction in startup time that is achieved. Startup time is arguably more important \nto an end user over a few percent speedup in ex- ecution time. Our studies revealed that almost all compi- \nlation in Java programs occurs at startup: In the programs studied, 77% of the compilation overhead occurs \nin the first 4 seconds (initial 10%) of program execution on average. By reducing compilation overhead, \nannotated execution should substantially reduce this startup cost. Figure 6 confirms this with graphs \nof the cumulative distribution of compile time over program lifetime (in seconds on the y-axis). We show \nthe cumulative time in seconds on the x-axis as opposed to the percentages commonly shown for a cumulative \ndistri- bution function. The compilation overhead in seconds that has occurred since the start of the \nprogram's execution is expressed in this figure. The overhead for the 03 compiler is shown by the top \n(dark) line on each graph. The bottom two lines in each graph indicate the effect of our two best-performing \nannotation optimizations (inlining and top25%). The results show that startup overhead is substan- tially \nreduced for every program. For example, for Mpeg, all compilation completes in 3 seconds. Using annotated- \nexecution this point is reached in approximately one second. On average, 77% of the compilation overhead \noccurs in the first 4 seconds (10%) of program total time, e.g., for Jack, almost all of the 2.8 seconds \nof compilation overhead occur in the first 6 seconds. For all programs, startup compilation completes \nmore than 2 seconds earlier using annotation. 162 3.2 3.2 ......................................................................................................................................................................... \n~ 2.~ 2. ................ f f .~ 2.4 m  2.4. --~ .... E y , j 2.0. .~ 2.0 .2 ~ 1.6\" ~. ~.s. E 0 o \n]Aml~iini nlng_toP2 6 (Same) -.. ....... ~ 1,2\" C) 1.2\" -1Anli~ihiihlng~t op2S (Dlfi) ~ 0.8- ~ 0.8- \n ~ 0.4 0.4- o 0 p~  0.0 , ,~.. ,, ,.,., , ,,, , , .... ~ , , 0 2 4 6 8 10 12 14 16 18 20 22 24 26 \n28 30 32 34 36 38 40 42 44 46 2 4 6 8 10 12 14 18 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 Program \nExecution Time in Seconds Program Execution Time in Seconds Jack JavaCup 3.2~ 321................................................................................................................ \n 2.8 ~2.8 \"~ 2.4 2.4-~ ............ 2.0 C 2.0 '~ I ~nn~'t~!n!j~ii~g~tOp~5 (S~ [i ~ 1.6 O 1.2 _ o6 / \n ~ 0.4 0.4 ' ' 0,0 -~-- 0 2 4 6 8 10 12 14 1G 18 20 22 24 26 28 30 32 34 36 28 40 42 44 0 2 4 6 8 10 \n12 14 16 18 20 22 24 26 28 30 32 34 as 38 40 42 44 46 46 50 Program Execution Time in Seconds Program \nExecution Time In Seconds Jess JSrc 3.2 .................. 3.2 .......................................................................................................................................................................................................................... \n ~2,8 2.8, o~ 2.4. E ~ 2.0 2.0- O = 1,6 ~1.6\" E E 0 o ~ 1.2- ~ 0.8- ~ 0.8-- ! E ~ 0.4 ~ ~ 0.4i 0 \n8 0,0 0.0 ! 0 2 4 6 0 10 12 14 16 10 20 22 24 26 28 30 32 1 2 3 4 5 6 7 Program Execution Time in Seconds \nProgram Execution Time In Seconds Mpeg Soot Figure 6: Cumulative compilation time in seconds over the \nlifetime of each program (y-axis in seconds). These graphs show, throughout program execution~ the number \nof seconds of compilation overhead that have occurred (x-axis) since the start of the program. The overhead \nfor the 03 compiler is shown by the top dark line on each graph; the bottom lines indicate the effect \nof annotation using our inlining and top25% optimizations. 163 ......... I~ Remote.Annot-lnlining top25 \n(Diff) ! i 3.5 ~l_top25 (Same) MAnnot.lrdining_top25 (Diff) ' IAOl 3.0 2.5 ...... 03 .N 2.11 i= t,s ...... \ng~ oEL( 1 ................ O 0.(1 Jack JavaCup Jess JSre Mpeg Soot Avg Figure 7: Total compilation overhead \nfor 03, O1, and annotated compilation (both all class files and remote class files only). Annot-Inlining_top25 \nde-notes results that use the inlining and top 25% an-notations alone to guide dynamic compilation. \"l:te-mote\" \nindicates that only non-local class files use annotation-guided optimization. For this configura- tion, \nall other class files are compiled using the O1 compiler. Another interesting detail shown by these graphs \nis the com- pilation overhead that occurs at the end of execution for JavaCup and JSrc. The methods compiled \nduring this pe- riod are those for I/O and clean up. We plan to use this characteristic to guide future \nannotation implementation. For example, optimization of such methods for an interac- tive program can \nbe avoided since they are only used at the end of the program. 4.2 Local v/s Remote Execution Mobile \nJava programs are commonly tra~nsferred over a net- work for remote execution either through the use \nof dynamic class file loading of individual class files or by archiving and compressing the application \nas a single file, e.g. Java archive (jar). In addition, these programs commonly use Java class libraries \nduring execution that are not part of the appli-cation itself, but are shared by all such programs. These \nlibrary classes are not transferred for remote execution but are located at the destination for use by \nremotely executed Java programs. To this point, we have assumed that we are able to annotate both the \napplication as well as the libraries it uses. However, this is not always the case and as such, we present \nresults on the effect of annotating only non-local, or application, class files. Most of the prior work \n[4, 13] on bytecode annotations does not address the effect of limit- ing optimization to remote class \nfiles yet it is vital to the empirical evaluation of annotation-based techniques. Figure 7 shows the \nreduction in compile time due to annotated- execution of only non-local class files. Class files that \nare non-local (or remote) include non-library files used during execution of each program. Local (library) \nfiles are compiled using the O1 compiler (no-optimization). Realistically, 5- brary files should be optimized \nbut the overhead associated with their compilation should not degrade startup or cause o O == == Jack \nJavaCup Jess JSrc Mpeg Soot Avg Figure 8: Speedup over optimized (ORP 03) total time due to annotated \nexecution on remote classes only. For these results, only the inllning and top25% annotations are used. \nintermittent interruption. Because of this we collected re- sults for Ol-compilation of these files. \nSome execution en- vironments store optimized versions of library files on disk and dynamically load \nthem [2]. As part of future work we will incorporate such functionality into ORP. The graph in this figure \nis the same as the one presented earlier in Figure 4 with two additional bars: one for cross- input remote-only \ncompile time (second bar) and one for same-input remote-only (third bar) results. These results are achieved \nby using only the two best-performing annota- tions inlining and selective optimization (top25%). The \ntwo input configurations included (Same and Diff) again indicate that imperfect information has little \neffect on the overall per- formance of these annotations. Over 80% of the compilation delay is reduced \nby using annotation-guided optimization for non-local class files and Ol-compiling all others. Figure \n8 shows the percent speedup over O3-compilation due to an- notations on non-local class files. Our inlining \nand top25% annotation optimizations on remote class files alone achieve 1% to 21% speedup over O3 total \ntime for the programs studied. The average speedup across benchmarks is 6.1% and 5.6% for the same-input \nand cross-input configurations, respectively. As part of future work we will consider the effect of provid- \ning general annotations for local (library) class files that can be used to improve performance regardless \nof the invoking program. For example, it has been shown for C and Fortran that execution behavior of \ncommonly used Unix libraries is similar across different programs and that this informa- tion can be \nused to guide optimization [6]. This implies that profile-based techniques for a subset of programs can \npotentially be used to optimize shared libraries. We are investigating such techniques as part of future \nwork. 164 Table 4: The added size in kilobytes due to each annotation optimizations. Columns 2 through \n5 contain the added size fi-om each the flow graph creation, register allocation~ constant propagation, \nreuse optimization for inlining~ and selective optimization using the top 25~o of frequently executed \nmethods, respectively. Remote class file annotation alone is shown first in each column. In parenthesis~ \nwe show the annotation size across all class files in an application. The results show the percent increase \nin application size due to the annotation (in that column) on average across all benchmarks. Size of \nAnnotation in KBytes for Non-Local Classes - Total for All Classes in Parens. Program FG-Create tl~egalloc \nConst-prop Inlining SelOpt Jack 12'.55 (16.41) 0.74 (1.25) 0.28 (0.50) 0;64 (0.06) 0.04 (0.06) JavaCup \n7.02 (13.61) 0.47 (1.37) 0.20 (0.54) 0.03 (0.07) 0.03 (0.07) Jess 9.74 (14.62) 1.02 (1.61) 0.39 (0.64) \n0.05 (0.08) 0.05 (0.08) 3Src 12.29 (15.39) 1.22 (1.68) 0.41 (0.62) 0.05 (0.08) 0.05 (0.08) Mpeg 5.15 \n(8.35) 0.71 (1.17) 0.22 (0.40) 0.03 (0.05) 0.03 (o.o5) Soot 6.44 (9.66) 0.60 (1.09) 0.33 (0.54) 0.04 \n(0.07) 0.04 (0.07) Avg 8.87 (13:61) 0.79 (1.36) 0.28 (0.54) 0.04 (o:07) 0.04 (0.07) Incr 3.6% (5.2%) \n0.3% (0.5%) o.1% (0.3%) 0.0% (0.0%) 0.0% (0.0%) 4.3 Annotation Size Since annotations are added to class \nfiles that transfer for re- mote execution, we must ensure that our framework and an- notation implementations \nincrease class file size minimally. Since it is this size that dictates the transfer time on a partic- \nular network, we present overhead as the number of kilobytes added for annotation. Table 4 shows this \ndata for the differ- ent annotations implemented across all non-local class files. In parentheses is \nshown the number of kilobytes added for annotations in both the non-local and local class files com- \nbined. On average, the annotations add less than 0.05% to 3.4% to applications alone. By only incorporating \nthe two best performing annotations (inlining and top25%) we in- crease application size by less than \n0.05%. This is compared to the 7% to 97% increase in size by existing annotation im- plementations for \na single annotation optimization [12, 13, 21] 5. RELATED WORK In this section, we discuss work related \nto our Java anno-tation research. We first describe related work on dual-compiler systems, then describe \nrecent annotation research, and conclude with relevant research for the reduction of startup delay. 5.1 \nFast v/s Optimizing Compiler Existing Java systems use two compilers in an adaptive com- pilation scheme \n[3, 7, 11] in which methods are first com-piled with a very fast compiler that converts bytecode into \ninstrumented executable code. Instrumentation enables on- line measurements to be made to discover hot \nmethods. Hot methods are those that are frequently executed [7, 11] or those in which a substantial amount \nof execution time is spent [3]. Hot methods are re-compiled by the optimiz-ing compiler. The optimized \ncode is used by future invoca- tions of the method to improve execution time. This process amortizes \nthe cost of optimized compilation since only fre- quently executed (or methods otherwise deemed important) \nare optimized. In our work, we perform off-line instrumentation and anal- ysis to determine important \nmethods to optimize and pass this information to the compiler using our selective opti- mization annotation. \nThis obviates the need to spend time for both the instrumentation, on-line measurement, and re- compilation. \nIn [17], we proposed using a technique similar to selective optimization annotation to optimize important \nmethods on a background processor. If a method is encountered be-fore it has been optimized then it is \ncompiled with a fast compiler and possibly replaced with an optimized version if deemed important. Off-line \nprofiles are generated to deter- mine which methods are important based on the time spent executing them. \nThis infrastructure, however differs from our selective optimization annotation since it provided no \nmechanism for communicating the information to the com- piler except as a file on disk. This prior work \nalso does not differentiate between local and non-local files and their profiles include both local and \nnon-local methods. 5.2 Annotation-based Dynamic Optimization Pominville et al [21] recently published \na framework for an- notation of Java bytecode that enables static and dynamic, off-line analysis to be \nincorporated for optimization. The design of their framework differs from ours in that it does not consider \nthe size of their annotations or the transfer delay incurred through their use. The single annotation \nop- timization they implement as a prototype of their framework is array bounds check elimination which \nincreases applica- tion size 7% to 16% for the benchmarks they include in their study. They do not provide \nan implementation or de- scription of how dynamic (profile) information is used to construct annotations \nin their framework. They only pro- vide details on the array bounds check elimination, an opti- mization \nalready performed in the ORP 03 compiler. Our empirical breakdown of the time spent in 03 compilation \ngroups the time for this optimization in to Other indicat- ing that it alone requires less than 100 milliseconds \nfor the entire application. 165 ?% Hummel et al [12], show how static information about reg- ister allocation \nin the Kaffe JIT can be conveyed using an- notations. They use a virtual register scheme in which they \nassume an infinite number of registers, make virtual register assignments off-line, and communicate this \ninformation to the JIT compiler for register allocation. They do not pro-vide an annotation implementation \nor mechanism for reduc- ing the transfer delay imposed by their 33% to 97\u00b07o increase in class file size. \nIn a project similar to this prior work, Jones et al in [13] describes an annotation for register allo- \ncation. The annotation assigns 256 virtual register numbers to the bytecode of each method to improve \nexecution per- formance using the Kaffe JIT compiler. They also do not provide a mechanism for annotation \nsize reduction and in- crease class file size 31% to 38% for the programs presented. Lastly, no general \nframework or format for their annotations is provided. These related projects do not consider the use \nof annota- tions to reduce optimization overhead as we have done in this paper. The goal of each of the \nprior work is to en-able an expensive optimization to be performed, which is a side-effect of our framework. \nWe plan to evaluate the ef- fect of expensive optimizations that are now feasible in a dynamic compilation \nsetting through annotation. In doing this, we will consider the transfer delay trade-off associated with \nsuch optimizations to ensure that total time (transfer, compilation, and execution) is not degraded. \nAs mentioned above, each of these related works increase transfer delay substantially which can negate \nthe impact of their execu-tion time improvements. One reason for this is the use of multiple annotations \nper class file. This increases the size of the constant pool substantially. In our work, we use a single \nattribute for all annotations and a single Unicode character name to indicate that the attribute is an \nannotation to keep constant pool expansion to a minimum. Another issue the must be addressed by any \nannotation im- plementation is security. For annotations to be trusted, they must be verified or implemented \nso as to guarantee safety of the JVM or machine on which annotated execution is performed. The annotation-guided \noptimizations presented in [21, 4] are unsafe since the annotations contain informa- tion that effect \nthe semantics of the program and no verifi- cation is performed at the destination. For example, in [21], \nthe authors present an annotation for array bounds check elimination. If this annotation in intercepted \nand modified by an untrusted party, a boundary check might be elimi- nated and cause illegal memory access. \nLikewise, in [4], the authors implement register allocation, and annotation ma-nipulation can cause program \nbehavior that can potentially harm the JVM in which it is executing as well as the un-derlying machine. \nFor such annotations to be trusted, some mechanism for verification must be implemented. The an-notations \nwe present for remote execution (Section 4.2) are safe without requiring verification since their modification \nonly affects program performance not semantic behavior. 5.3 Reducing Startup Delay In Section 4.1, we \nshowed that annotation-guided compila- tion substantially reduces startup time in mobile programs. Related \nwork which reduces staxtup delay focuses on re-ducing the effect of transfer delay on such programs. \nOur annotation optimizations are complementary to these tech- niques and when combined with them can \nfurther reduce the startup time of mobile programs. Existing transfer delay techniques that improve startup \nper- tbrmance commonly rest.ructure mobile programs so that only those methods that will be executed \nare transferred across the network. One such technique is non-strict execu- tion [16], in which the transfer \nand execution model of Java is modified to allow execution to begin much earlier. The changes to the \nJVM required include method-level transfer and execution; when the code and data necessary for exe- cution \nhas arrived at the destination execution can begin. Methods are reordered across class files by profile-guided \ntechniques to enable their arrival at the destination in the order they are predicted to execute. Another \nform of program restructuring to improve staxtup delay is class file splitting as described in [15], \nand similarly in [23]. In these works, Java class files are repartitioned to enable more effective utilization \nof the available bandwidth during transfer without requiring modification to the JVM. Profile information \nis used to identify methods that are un- used during instrumented execution. Unused methods are then \nsplit out into new class files. Using existing Java class file loading techniques, the class files containing \nthe methods that are used during execution axe transferred. If methods predicted as unused and split \nout are not used during ac-tual execution, the methods are never transferred and the transfer delay is \nreduced. If such methods are used, then the class files containing them are transferred. 6. CONCLUSION \nWe have presented an annotation framework for Java pro- grams that substantially reduces compilation \noverhead in the ORP dynamic optimizing compiler. The annotation lan- guage is highly extensible and represents \nan instruction set with opcode, size, and annotation data, and requires only one bytecode attribute for \nmultiple annotations. The frame- work enables incorporation of highly-compressed, static and profile-based \ninformation into the Java bytecode stream for use in dynamic optimization. These annotations enable re- \nduction in compilation overhead of 75% on average, while increasing class file size (and hence transfer \ndelay) by less than 0.05%. Compilation overhead in execution environments for mobile code is expensive \nsince optimization, resulting in improved execution time, uses time-consuming analysis and processing \neven for very simple optimizations. However, the potential for execution speedup is large since runtime \ninformation can be used for program optimization and specialization. The annotation optimizations we \npresent perform analysis off- line and communicate it to the optimizing compiler to ob- viate the need \nfor its collection at runtime. In addition, we pass dynamic information from off-line profiles via annota- \ntions to the compilation system so that methods, predicted to be most important, axe selectively optimized. \nA significant contribution of this work is the substantial re- duction in startup time that our optimizations \nenable, while at the same time reducing the overall execution time of the program. Startup delay negatively \neffects the user's percep- tion of program performance as well as the raw performance of mobile Java \nprograms. In the programs studied, 77% of of the compilation overhead occurs in the first 4 seconds (initial \n10%) of program execution on average. Using our annotaeion-guided optimizations startup delay is reduced \nby more than two seconds on average, enabling substantial im- provement in the initial progress made \nby program execu-tion. In summary, our annotation framework provides informa- tion fi'om off-line, static \nand dynamic (profile-based) analy- sis to a dynamic, optimizing compilation system, to substan- tially \nreduce compilation overhead and enable more complex optimizations to be performed. We are currently designing \nand developing new annotations using this compact anno-tation representation. Some examples include annotations \nthat improve gal%age collection and memory performance e.g., code and data placement, on-stack allocation \nof ob- jects, lifetime estimates, and annotations that aid in branch and loop optimizations. Acknowledgments \nWe would like to thank the anonymous reviewers for provid- ing useful comments on this paper. This work \nwas funded in part by NSF CAREER grant No. CCR-9733278, and a grant from Intel Corporation. 7. REFERENCES \n[1] A. Adl-Tabatabai, M. Cierniak, G. Lueh, V. Parikh, and J. Stichnoth. Fast,Effective Code Generation \nin a Just-In-Time Java Compiler. In Proceedings of the A CM SIGPLAN '98 Conference on Programming Language \nDesign and Implementation, October 2000.  [2] B. Alpern, C. R. Attanasio, J. J. Barton, M. G. Burke, \nP.Cheng, J.-D. Choi, A. Cocchi, S. J. Fink, D. Grove,  M. Hind, S. F. Hummel, D. Lieber, V. Litvinov, \nM. F. Mergen, 2i'. Ngo, J. R. Russell, V. Sarkar, M. J. Serrano, J. C. Shepherd, S. E. Smith, V. C. \nSreedhar, H. Srinivasan, and J. Whaley. The Jalapefio virtual machine. IBM Systems Journal, 39(1), 2000. \n [3] M. Arnoldl S.J. Fink, D. Grove, M. Hind, and P. Sweeney. Adaptive optimization in the jalaepefio \njvm. In ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages, and Applications (OOPSLA), \nOctober 2000. [4] A. Azevedo, A. Nicolau, and J. Hummel. Java Annotation-Aware Just-In-Time Compilation \nSystem. In ACM Java Grande Conference, June 1999. [5] M. Burke, J. Choi, S. Fink, D. Grove, M. Hind, \nV: Sarkar, M. Serrano, V. Shreedhar, H. Srinivasan, and J. Whaley. The Jalapefio dynamically optimizing \ncompiler for Java. In ACM Java Grande Conference, June 1999.  [6] B. Calder, D. Grunwald, and A. Srivastava. \nThe Predictability of Branches in Libraries. In 28th International Symposium on Microarchitecture, November \n1995. [7] M. Cierniak, G. Lueh, and J. Stichnoth. Practicing JUDO: Java Under Dynamic Optimizations. \nIn Proceedings of the ACM SIGPLAN '00 Conference on Programming Language Design and Implementation, October \n2000. [8] W. Doherty and R. Kelisky. Managing VM/CMS systems for user effectiveness. IBM Systems Journal, \npages 143-163, 1979. [9] J. Gosling, B. Joy, and G. Steele. The Java Language Specification. Addison-Wesley, \n1996. [10] B. Grant, M. Mock, M. Philipose, C. Chambers, and S. Eggers. Dyc: An expressive annotation-directed \ndynamic compiler for c. Technical Report Tech Report UW-CSE-97-03-03, University of Washington, 2000. \n[11] The Java Hotspot performance engine architecture. [12] J. Hummel, A. Azevedo, D. Kolson, and A. \nNicolau. Annotating the Java Bytecodes in Support of Optimization. In Journal Concurrency:Practice and \nExperience, Vol. 9(11), November 1997. [13] J. Jones and S. Kamin. Annotating Java Bytecodes in Support \nof Optimization. In To appear in the Journal of Concurrency: Practice and Experience, 2000. [14] Kaffe \n- An opensource Java virtual machine. [15] C. Krintz, B. Calder, and U. Hhlzle. Reducing transfer delay \nusing Java class file splitting and prefetching. In ACM SIGPLAN Conference on Object-Oriented Programming \nSystems, Languages, and Applications (OOPSLA), November 1999. [16] C. Krintz, B. Calder, H. Lee, and \nB. Zorn. Overlapping execution with transfer using non-strict execution for mobile programs. In Eigth \nInternational Conference on Architectural Support for Programming Languages and Operating Systems, October \n1998. [17] C. Krintz, D. Grove, V. Sarkar, and B. Calder. Reducing the Overhead of Dynamic Compilation. \nSoftware--Practice and Experience, 31(8):717-738, 2001. [18] H. Lee and B. Zorn. BIT: A tool for instrumenting \nJava bytecodes. In Proceedings of the 1997 USENIX Symposium on Internet Technologies and Systems (USITS97), \npages 73-82, Monterey, CA, December 1997. USENIX Association. [19] Open Runtime Platform (orp) from Intel \nCorporation. http ://intel. cos/reseat ch/mrl/orp. [20] M. Plezbert and R. Cytron. Does just in time \n= better late than never? In Proceedings of the SIGPLAN'97 Conference on Programming Language Design \nand Implementation, January 1997. [21] P. Pominville, F. Qian, R. Vallee-Rai, L. Hendren, and C. Verbrugge. \nA Framewdrk for Optimizing Java Using Attributes. In Sable Technical Report No. 2000-2, 2000. [22] T. \nProebsting, G. Townsend, P. Bridges, J. Hartman, T. Newsham, and S. Watterson. Toba: Java for applications \na way ahead of time (war) compiler. In Proceedings of the Third Conference on Object-Oriented Technologies \nand Systems, 1997. [23] E. Sirer, A. Gregory, and B. Bershad. A practical approach for improving startup \nlatency in Java applications. In Workshop on Compiler Support for Systems Software, May 1999. [24] A. \nSrivastava. From communication on reducing startup delay in Microsoft Applications. [25] T. Suganuma, \nT. Ogasawara, M. 'I~keuchi, T. Yasue, M. Kawahito, K. Ishizaki, H. Komatsu, and T. Nakatani. Overview \nof the IBM Java Just-in-Time Compiler. IBM Systems Journal, 39(1), 2000.   \n\t\t\t", "proc_id": "378795", "abstract": "<p><i>Dynamic compilation and optimization are widely used in heterogenous computing environments, in which an intermediate form of the code is compiled to native code during execution. An important trade off exists between the amount of time spent dynamically optimizing the program and the running time of the program. The time to perform dynamic optimizations can cause significant delays during execution and also prohibit performance gains that result from more complex optimization.</i></p><p><i>In this research, we present an annotation framework that substantially reduces compilation overhead of Java programs. Annotations consist of analysis information collected off-line and are incorporated into Java programs. The annotations are then used by dynamic compilers to guide optimization. The annotations we present reduce compilation overhead incurred at all stages of compilation and optimization as well as enable complex optimizations to be performed dynamically. On average, our annotation optimizations reduce optimized compilation overhead by 78% and enable speedups of 7% on average for the programs examined.</i></p>", "authors": [{"name": "Chandra Krintz", "author_profile_id": "81100025597", "affiliation": "University of California, San Diego, Department of Computer Science and Engineering, La Jolla, CA", "person_id": "PP14021786", "email_address": "", "orcid_id": ""}, {"name": "Brad Calder", "author_profile_id": "81100088945", "affiliation": "University of California, San Diego, Department of Computer Science and Engineering, La Jolla, CA", "person_id": "PP17000250", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/378795.378831", "year": "2001", "article_id": "378831", "conference": "PLDI", "title": "Using annotations to reduce dynamic optimization time", "url": "http://dl.acm.org/citation.cfm?id=378831"}