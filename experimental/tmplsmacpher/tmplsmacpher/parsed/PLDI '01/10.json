{"article_publication_date": "05-01-2001", "fulltext": "\n Composing High-Performance Memory AHocators Emery D. Berger Dept. of Computer Sciences The University \nof Texas at Austin Austin, TX '78712  emery @ cs. utexas.edu Benjamin G. Zorn Microsoft Research One \nMicrosoft Way Redmond, WA 98052  zorn @ microsoft.com Kathryn S. McKinley Dept. of Computer Science \nUniversity of Massachusetts Amherst, MA 01003  mckinley@ cs.umass.edu ABSTRACT Current general-purpose \nmemory allocators do not provide suffi- cient speed or flexibility for modem high-performance applications. \nHighly-tuned general purpose allocators have per-operation costs around one hundred cycles, while the \ncost of an operation in a cus- tom memory allocator can be just a handful of cycles. To achieve high \nperformance, programmers often write custom memory allo- cators from scratch - a difficult and error-prone \nprocess. In this paper, we present a flexible and efficient infrastructure for building memory allocators \nthat is based on C++ templates and inheritance. This novel approach allows programmers to build cus- \ntom and general-purpose allocators as \"heap layers\" that can be composed without incurring any additional \nruntime overhead or ad- ditional programming cost. We show that this infrastructure simpli- fies allocator \nconstruction and results in allocators that either match or improve the performance of heavily-tuned \nallocators written in C, including the Kingsley allocator and the GNU obstaek library. We further show \nthis infrastructure can be used to rapidly build a general-purpose allocator that has performance comparable \nto the Lea allocator, one of the best uniprocessor allocators available. We thus demonstrate a clean, \neasy-to-use allocator interface that seam- lessly combines the power and efficiency of any number of \ngeneral and custom allocators within a single application. 1. Introduction Many general-purpose memory \nallocators implemented for C and C++ provide good runtime and low fragmentation for a wide range of applications \n[15, 17]. However, using specialized memory al- locators that take advantage of application-specific \nbehavior can dramatically improve application performance [4, 13, 26]. Hand- coded custom allocators \nare widely used: three of the twelve inte- ger benchmarks included in SPEC2000 ((parser, gcc, and vpr \n[22]) and several server applications, including the Apache web This work is supported by NSF grant EIA-9726401, \nNSF Infrastructure grant CDA-9502639, NSF ITR grant CCR-0085792, and DARPA grant 5- 21425. Part of this \nwork was done while the first author was at Microsoft Research. Emery Berger was also supported by a \ngrant from Microsoft Corporation. Any opinions, findings and conclusions or recommendations expressed \nin this material are the authors and do not necessarily reflect those of the sponsors. Permission to \nmake digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for profit or commercial advan-tage and that copies \nbear this notice and the full citation on the first page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior specific permission and/or a fee. PLDI 2001 6/01 \nSnowbird, Utah, USA &#38;#169; 2001 ACM ISBN 1-58113.414-2/01/06...$5.00 server [11 and Microsoft's SQL \nServer [10], use one or more cus- tom allocators. Custom allocators can take advantage of certain allocation \npat- terns with extremely low-cost operations. For example, a program- mer can use a regionallocator \nto allocate a number of small objects with a known lifetime and then free them all at once [11, 19, 23]. \nThis customized allocator returns individual objects from a range of memory (i.e., a region), and then \ndeallocates the entire region. The per-operation cost for a region-based allocator is only a hand- ful \nof cycles (advancing a pointer and bounds-checking), whereas for highly-tuned, general-purpose allocators, \nthe per-operation cost is around one hundred cycles [11]. Other custom allocators can yield similar advantages \nover general-purpose allocators. Figure 1 shows the estimated impact of the custom allocator used in \nthe SPEC benchmark 2.97.parser, running on its test input. Re-placing its custom allocator by the system \nallocator increases its runtime by over 60%. 1 197.parser runtime 25 I 20 I [llmemoryoperations [B computation \n o- custom allocator system allocator (estimated) Allocator  Figure 1: The impact of custom allocation \non performance for 197 .parser.  To attain high performance, programmers often write their own ad hoe \ncustom allocators as macros or monolithic functions in or- der to avoid function call overhead. This \napproach to improving application performance is enshrined among the best practices of 1We estimated \nthis cost by measunng the time spent in allocation using 197 .parser's custom allocator and computing \na conser- vative estimate of allocation time with the system allocator (which cannot directly be substituted \nbecause of the semantics of the cus- tom allocator). This and the other programs in this paper were com- \npiled with Visual C++ 6.0 and run under Windows 2000 on a 366 MHz Pentium II system. skilled computer \nprogrammers [8, 18]. Unfortunately, this kind of code is brittle and hard to maintain or reuse, and as \nthe application evolves, it can be difficult to adapt the memory allocator as needs change (e.g., to \na multithreaded environment). In addition, writ- ing these allocators is both error-prone and difficult. \nGood memory allocators are complicated pieces of software that require a sub- stantial engineering effort. \nBecause of this complexity, combining a custom and general-purpose allocator to allow them to share free \nmemory, for example, is currently almost impossible. In this paper, we present a flexible and efficient \ninfrastructure for building custom and general-purpose allocators called heap layers. This infrastructure \nis based on a combination of C++ templates and inheritance called mixins [7]. Mixins are classes whose \nsuperclass may be changed. Using mixins allows the programmer to code al- locators as composable layers \nthat a compiler can implement with efficient code. Unlike previous approaches, we show that this tech- \nnique allows programmers to write highly modular and reusable code with no abstraction penalty. We describe \na number of high- performance custom allocators that we built by mixing and match- ing heap layers. We \nshow that these allocators match or improve performance when compared with their hand-tuned, monolithic \nC counterparts on a selection of C and C++ programs. We further demonstrate that this infrastructure \ncan be used effec- tively to build high-performance, general-purpose allocators. We evaluate two general-purpose \nallocators we developed using heap layers over a period of three weeks, and compare their performance \nwith the Kingsley allocator, one of the fastest general-purpose al- locators, and the Lea allocator, \nan allocator that is both fast and memory-efficient. While the current heap layers allocator does not \nachieve the fragmentation and performance of the Lea allocator, the Lea allocator is highly tuned and \nhas undergone many revisions over a period of more than seven years [17]. The remainder of this paper \nis organized as follows. We discuss related work in Section 2. In Section 3, we describe how we use mixins \nto build heap layers and demonstrate how we can mix and match a few simple heap layers to build and combine \nallocators. We briefly discuss our experimental methodology in Section 4. In Section 5, we show how we \nimplement some real-world custom allocators using heap layers and present performance results. Sec- tion \n6 then describes two general-purpose allocators built with heap layers and compares their runtime and \nmemory consumption to the Kingsley and Lea allocators. We describe some of the software engineering benefits \nof heap layers in Section 7, and in Section 8, we show how heap layers provide a convenient infrastructure \nfor memory allocation experiments. We conclude in Section 9 with a discussion of future directions. 2. \nRelated Work In this section, we describe related work in memory allocation and memory management infrastructures. \nWe discuss two representa- tive general-purpose memory allocators and describe related work in custom \nmemory allocation. We then compare heap layers to pre- vious infrastructures for building memory managers. \n 2.1 General-Purpose Allocation The literature on general-purpose memory allocators is extensive [26]. \nHere we describe two memory allocators, the Kingsley allo- cator used in BSD 4.2 [26] and the Lea allocator \n[17]. In Section 6, we describe the implementation of these two allocators in heap lay- ers. We chose \nto implement these allocators because they are in widespread use and are on opposite ends of the spectrum \nbetween maximizing speed and minimizing memory consumption. The Kingsley allocator is a power-of-two \nsegregated fits alloca- tor: all allocation requests are rounded up to the next power of two. This rounding \ncan lead to severe internal fragmentation (wasted space inside allocated objects), because in the worst \ncase, it allo- cates twice as much memory as requested. Once an object is allo- cated for a given size, \nit can never be reused for another size; the allocator performs no splitting (breaking large objects \ninto smaller ones) or coalescing (combining adjacent free objects). This algo- rithm is well known to \nbe among the fastest memory allocators although it is among the worst in terms of fragmentation [15]. \nThe Lea allocator is an approximate best-fit allocator that pro- vides both high speed and low memory \nconsumption. It forms the basis of the memory allocator included in the GNU C library [12]. The current \nversion (2.7.0) is a hybrid allocator with different be- havior based on object size. Small objects (less \nthan 64 bytes) are allocated using exact-size quicklists (one linked list of freed objects for each multiple \nof 8 bytes). Requests for a medium-sized object (less than 128K) and certain other events trigger the \nLea allocator to coalesce all of the objects in these quicklists in the hope that this reclaimed space \ncan be reused for the medium-sized object. For medium-sized objects, the Lea allocator performs immediate \ncoalescing and spliting and approximates best-fit. Large objects are allocated and freed using mrftap. \nThe Lea allocator is the best overall allocator (in terms of the combination of speed and memory usage) \nof which we are aware [15]. 2.2 Special-Purpose Allocation Most research on special-purpose (custom) \nallocation has focused on profile-based optimization of general-purpose allocation. Grun- wald and Zorn's \nCustoMalloc builds memory allocators from allo- cation traces, optimizing the allocator based on the \nrange of object sizes and their frequency of usage [13]. Other profile-based allo- cators use lifetime \ninformation to improve performance and refer- ence information to improve locality [4, 20]. Regions, \ndescribed in Section 1, have received recent attention as a method for improving locality [23]. Aiken \nand Gay have developed safe regions which de- lay region deletion when live objects are present [11]. \nTechniques for building other application-specific custom allocators have re- ceived extensive attention \nin the popular press [8, 18]. 2.3 Memory Management Infrastructures We know of only two previous infrastructures \nfor building memory managers: vmalloc, by Vo, and CMM, by Attardi, Flagella, and Iglio. We describe the \nkey differences between their systems and ours, focusing on the performance and flexibility advantages \nthat heap layers provide. The most successful customizable memory manager of which we are aware is the \nvmalloc allocator [25]. Vmalloc lets the pro- grammer define multiple regions (distinct heaps) with different \ndis- ciplines for each. The programmer performs customization by sup- plying user-defined functions and \nstructs that manage memory. By chaining these together, vmalloc does provide the possibility of composing \nheaps. Unlike heap layers, each abstraction layer pays the penalty of a function call. This approach \noften prevents many useful optimizations, in particular method inlining. The vmalloc infrastructure limits \nthe programmer to a small set of functions for memory allocation and deallocation; a programmer cannot \nadd new functionality or new methods as we describe in Section 6.1. Unlike heap layers, vmalloc does \nnot provide a way to delete heaps and reclaim all of their memory in one step. These limitations dramati- \ncally reduce vmaUoc's usefulness as an extensible infrastructure. Attardi, Flagella, and Iglio created \nan extensive C++-based sys- tem called the Customizable Memory Management (CMM) frame- work [2, 3]. Unlike \nour work, the primary focus of the CMM framework is garbage collection. The only non-garbage collected \nheaps provided by the framework are a single \"traditional man- 115 ...... ual allocation discipline\" \nheap (whose policy the authors do not specify) called UncollectedHeap and a zone allocator called Tem- \npHeap. A programmer can create separate regions by subclassing the abstract class CmmHeap, which uses \nvirtual methods to obtain and reclaim memory. For every memory allocation, deallocation, and crossing \nof an abstraction boundary, the programmer must thus pay the cost of one virtual method call, while in \nheap layers, there is no such penalty. As in vmalloc, this approach often prevents com- piler optimizations \nacross method boundaries. The virtual method approach also limits flexibility. In CMM, subclasses cannot \nimple- ment functions not already provided by virtual methods in the base heap. Also, since class hierarchies \nare fixed, it is not possible to have one class (such as FreelistHeap, described in Section 3.1) with \ntwo different parent heaps in different contexts. In contrast, the mixin-based approach taken by heap \nlayers allows both inheritance and reparenting, making heap layers more flexible and reusable. The goal \nof heap layers is to provide the performance of existing custom and general-purpose allocators in a flexible, \nreusable frame- work that provides a foundation for programmers to build new allo- cators. We implement \ncustomized and general-purpose allocators using heap layers, demonstrating their flexibility and competitive \nperformance. 3. Heap Layers While programmers often write memory allocators as monolithic pieces of \ncode, they tend to think of them as consisting of separate pieces. Most general-purpose allocators treat \nobjects of different sizes differently. The Lea allocator uses one algorithm for small objects, another \nfor medium-sized objects, and yet another for large objects. Conceptually at least, these heaps consist \nof a number of separate heaps that are combined in a hierarchy to form one big heap. The standard way \nto build components like these in C++ uses virtual method calls at each abstraction boundary. The overhead \ncaused by virtual method dispatch is significant when compared with the cost of memory allocation. This \nimplementation style also greatly limits the opportunities for optimization since the compiler often \ncannot optimize across method boundaries. Building a class hierarchy through inheritance also fixes the \nrelationships between classes in a single inheritance structure, making reuse difficult. To address these \nconcerns, we use mixins to build our heap lay- ers. Mixins are classes whose superclass may be changed \n(they may be reparented) [7]. The C++ implementation of mixins (first described by VanHilst and Notkin \n[24]) consists of a templated class that subclasses its template argument: template <class Super> class \nMixin : public Super {} ; Mixins overcome the limitation of a si.ngle class hierarchy, enabling the \nreuse of classes in different hierarchies. For instance, we can use A in two different hierarchies, A \n~ B and A -+ 6' (where the arrow means \"inherits from\"), by defining A as a mixin and composing the classes \nas follows: class Compositionl : public A<B> {} ; class Composition2 : public A<C> {1; A heap layer \nis a mixin that provides a malloc and free method and that follows certain coding guidelines. The malloc \nfunc-tion returns a memory block of the specified size, and the free function deallocates the block. \nAs long as the heap layer follows the guidelines we describe below, programmers can easily com- pose \nheap layers to build heaps. One layer can obtain memory from its parent by calling SuperHeap : :malloc \n( ) and can re- turn it with SuperHeap: : free (). Heap laygrs also implement thin wrappers around system-provided \nmemory 'allocation func- tions like malloc, sbrk, or mmap. We term these thin-wrapper layers top heaps, \nbecause they appear at the top of any hierarchy of heap layers. We require that heap layers adhere to \nthe following coding guide- lines in order to ensure composability. First, malloc must cor- rectly handle \nNULLs retumed by SuperHeap : :malloc ( ) to allow an out-of-memory condition to propagate through a series \nof layers or to be handled by an exception-handling layer. Second, the layer's destructor must free any \nmemory that is held by the layer. This action allows heaps composed of heap layers to be deleted in their \nentirety in one step. 3.1 Example: Composing a Per-Class Allocator One common way of improving memory \nallocation performance is to allocate all objects from a highly-used class from a per-class pool of memory. \nBecause all such objects are the same size, memory can be managed by a simple singly-linked freelist \n[16]. Programmers often implement these per-class allocators in C++ by overloading the new and delete \noperators for the class. Below we show how we can combine two simple heap layers to implement per-class \npools without changing the source code for the original class. We first define a utility class called \nPerClassHeap that allows a programmer to adapt a class to use any heap layer as its allocator: template \n<class Object, class SuperHeap> class PerClassHeap : public Object { public: inline void * operator \nnew (size_t sz) ( return getHeap().malloc (sz); } inline void operator delete (void * ptr) getHeap().free \n(ptr); ) private: static SuperHeap&#38; getHeap (void) { static SuperHeap theHeap; return theHeap; \n } }; We build on the above with a very simple heap layer called Free- listHeap. This layer implements \na linked list of free objects of the same size. malloc removes one object from the freelist if one is \navailable, and free places memory on the freelist for later reuse. This approach is a common idiom in \nallocators because it provides very fast allocates and frees. However, it is limited to handling only \none size of object. The code for FreelistHeap appears in Figure 2 without the error checking included \nin the actual code to guarantee that all objects are the same size. We can now combine PerClassHeap \nand FreelistHeap with malloc- Heap (a thin layer over the system-supplied real loc and free) to make \na subclass of Foo that uses per-class pools. class FasterFoo : public PerClassHeap<Foo, FreelistHeap<mallocHeap> \n>{};  3.2 A Library of Heap Layers We have built a comprehensive library of heap layers that allows \nprogrammers to build a range of memory allocators with minimal effort by composing these ready-made layers. \nFigure 1 lists a num- ber of these layers, which we group into the following categories: Top heaps. A \n\"top heap\" is a heap layer that provides memory di- rectly from the system and at least one appears at \nthe top 116 te/nplate <class SuperHeap> class FreelistHeap : public SuperHeap { public: FreelistHeap \n(void) : myFreeList (NULL) {} ~FreelistHeap (void) { // Delete everything on the freelist. void * ptr \n= myFreeList; while (ptr ]= NULL) { void * oldptr = ptr; ptr = (void *) ((freeObject *) ptr)->next; \nSuperHeap::free (oldptr); } } inline void * malloc (size t sz) { // Check the freelist first. void \n* ptr = myFreeList; if (ptr == NULL) ( ptr = SuperHeap::malloc (sz); } else { myFreeList = myFreeList->next; \n} return ptr; } inline void free (void * ptr) { // Add this object to the freelist. ((freeObject *) \nptr)->next = myFreeList; myFreeList = (freeObject *) ptr; }  private: class freeObject { public: freeObject \n* next; }; freeObject * myFreeList; }; Figure 2: The implementation of FreelistHeap. of any hierarchy \nof heap layers. These thin wrappers over system-based memory allocators include mallocHeap, mmap- Heap \n(virtual memory), and sbrkHeap (built using sbrk ( ) for UNIX systems and an sbrk ( ) emulation for Windows). \n Bnilding-bioek heaps. Programmers can use these simple heaps in combination with other heaps described \nbelow to imple- ment more complex heaps. We provide an adapter called AdaptHeap that lets us embed a \ndictionary data structure inside freed objects so we can implement variants of Free- listHeap, including \nDLList, a FIFO-ordered, doubly-linked freelist that allows constant-time removal of objects from anywhere \nin the freelist. This heap supports efficient co- alescing of adjacent objects belonging to different \nfreelists into one object. Combiningheaps. These heaps combine a number of heaps to form one new heap. \nThese include two segregated-fits layers, SegHeap and StfictSegHeap (described in Section 6.1), and HybridHeap, \na heap that uses one heap for objects smaller than a given size and another for larger objects. Utility \nlayers. Utility layers include ANSIWrapper, which provides ANSI-C compliant behavior for raalloe and \nfree to al- low a heap layer to replace the system-supplied allocator. A number of layers supply multithreaded \nsupport, includ- ing LockedHeap, which code-locks a heap for thread safety (acquires a lock, performs \na malloc or free, and then releases the lock), and ThreadHeap and PHOThreadHeap, A Library of Heap Layers \n  Top Heaps aaalloeHeap A thin layer over real 1 oc aamapHeap A thin layer over the virtual memory \nmanage~ ~brkHeap A thin layer over sbrk (contiguous memory)  Building-Block Heaps AdaptHeap Adapts \ndata structures for use as a heap BoundedFreelistHeap A freelist with a bound on length EhunkHeap Manages \nmemory in chunks of a given size C.oalesceHeap Performs coalescing and splitting FreelistHeap A freelist \n(caches freed objects) Combining Heaps HybridHeap Uses one heap for small objects and another for large \nobjects SegHeap A general segregated lits allocator StdctSegHeap A strict segregated fits allocator Utility \nLayers ANSIWrapper Provides ANSI-malloc compliance DebugHeap Checks for a variety of allocation errors \nLockedHeap Code-locks a heap for thread safety PerClassHeap Use a heap as a per-class allocator PHOThreadHeap \nA private heaps with ownership allocator [6] ProfileHeap Collects and outputs fragmentation statistics \nI'hreadHeap A pure private heaps allocator [6] I'hrowExceptionHeap Throws an exception when the parent \nheap is out of memory l~raceHeap Outputs a trace of allocations UniqueHeap A heap type that refers to \none heap object Object Representation CoalesceableHeap Provides support for coalescing SizeHeap Records \nobject sizes in a header Spedal-Purpose Heaps ObstackHeap A heap optimized for stack-like behavior and \nfast resizing ZoneHeap A zone (\"region\") allocator XaUocHeap A heap optimized for stack-like behavior \nGeneral-Purpose Heaps KingsleyHeap Fast but high fragmentation LeaHeap Not quite as fast but low fragmentation \n Table 1: A library of heap layers, divided by category. which implement finer-grained multithreaded \nsupport. Error handling is provided by ThrowExceptionHeap, which throws an exception when its superheap \nis out of memory. We also provide heap debugging support with DebugHeap, which tests for multiple frees \nand other common memory management errors. Object representation. SizeHeap maintains object size in \na header just preceding the object. CoalesceableHeap does the same but also records whether each object \nis free in the header of the next object in order to facilitate coalescing. Special-purpose heaps; We \nimplemented a number of heaps opti- mized for managing objects with known lifetimes, including two heaps \nfor stack-like behavior (ObstackHeap and Xal- locHeap, described in Sections 5.1 and 5.2) and a region- \nbased allocator (ZoneHeap). General-purposeheaps. We also implement two heap layers use- ful for general-purpose \nmemory allocation: KingsleyHeap and LeaHeap, described in Sections 6.1 and 6.2. 117 4. Experimental Methodology \nWe wrote these heap layers in C++ and implemented them as a series of include files. We then used these \nheap layers to replace a number of custom and general-purpose allocators. For C++ pro- grams, we used \nthese heap layers directly (e.g., kHeap, free (p)). When replacing custom allocators in C programs, we \nwrapped the heap layers with a C API. When replacing the general-purpose al- locators, we redefined raalloc \nand free and the C++ operators new and clelet:e to refer to the desired allocator. Programs were compiled \nwith Visual C++ 6.0 and run on a 366 MHz Pentium II system with 128MB of RAM and a 256K L2 cache, under \nWin- dows 2000 build 2195. All runtimes are the average of three runs; variation was minimal. 5. Building \nSpecial-Purpose Allocators In this section, we investigate the performance implications of build- ing \ncustom allocators using heap layers. Specifically, we evaluate the performance of two applications (197. \nparser and 176. gec from the SPEC2000 benchmark suite) that make extensive use of custom allocators. \nWe compare the performance of the original carefully-tuned allocators against versions of the allocators \nthat we wrote with heap layers) 5.1 197,parser The 197. parser benchmark is a natural-language parser \nfor En- glish written by Sleator and Temperley. It uses a custom alloca- tor the authors call xalloc \nwhich is optimized for stack-like behav- ior. This allocator uses a fixed-size region of memory (in this \ncase, 30MB) and always allocates after the last block that is still in use by bumping a pointer. Freeing \na block marks it as free, and if it is the last block, the allocator resets the pointer back to the new \nlast block in use. Xalloc can free the entire heap quickly by setting the pointer to the start of the \nmemory region. This allocator is a good example of appropriate use of a custom allocator. As in most \ncus- tom allocation strategies, it is not appropriate for general-purpose memory allocation. For instance, \nif an application never frees the last block in use, this algorithm would exhibit unbounded memory consumption. \nWe replaced xalloc with a new heap layer, XallocHeap. This layer, which we layer on top of MmapHeap, \nis the same as the orig- inal allocator, except that we replaced a number of macros by inline static \nfunctions. We did not replace the general-purpose allocator which uses the Windows 2000 heap. We ran \n197. parser against the SPEC test input to measure the overhead that heap layers added. Figure 3 presents \nthese results, We were quite surprised to find that using layers actually slightly reduced runtime (by \njust over 1%), although this reduction is barely visible in the graph. The source of this small improvement \nis due to the increased opportunity for code reorganization that layers provide. When using layers, the \ncompiler can schedule code with much greater flexibility. Since each layer is a direct procedure call, \nthe compiler can decide what pieces of the layered code are most appropriate to inline at each point \nin the pro- gram. The monolithic implementations of xalloe/xfree in the original can only be inlined \nin their entirety. Table 2 shows that the executable sizes for the original benchmark are the smallest \nwhen the allocation functions are not declared inline and the largest when they are inlined, while the \nversion with XallocHeap lies in between (the compiler inlined the allocation functions with XallocHeap \nre- gardless of our use of :i.nline). Inspecting the assembly output reveals that the compiler made more \nfine-grained decisions on what 2We did not include 17 5. vpr, the other benchmark in SPEC2000 that uses \ncustom allocators, because its custom memory allocation API exposes too much internal allocator state \nto allow us to cleanly replace the allocator. 197. parser variant Executable size original 211,286 original \n(inlined) 266,342 XallocHeap 249,958 XallocHeap (inlined) 249,958 Table 2: Executable sizes for variants \nof 197. parser. Figure 3: Runtime comparison of the original 197. parser custom allocator and xallocHeap. \ncode to inline and thus achieved a better trade-off between program size and optimization opportunities \nto yield improved performance. 5.2 176.gcc Gcc uses obstacks, a well-known custom memory allocation library \n[26]. Obstacks also are designed to take advantage of stack-like be- havior, but in a more radical way \nthan xalloe. Obstacks consist of a number of large memory \"chunks\" that are linked together. Al- location \nof a block bumps a pointer in the current chunk, and if there is not enough room in a given chunk, the \nobstack allocator obtains a new chunk from the system. Freeing an object deallo. cares all memory allocated \nafter that object. Obstacks also support a grow () operation. The programmer can increase the size of \nthe current block, and if this block becomes too large for the cur- rent chunk, the obstack allocator \ncopies the current object to a new, larger chunk. Gcc uses obstacks in a variety of phases during compilation. \nThe parsing phase in particular uses obstacks extensively. In this phase, gcc uses the obstack grow operation \nfor symbol allocation in order to avoid a fixed limit on symbol size. When entering each lexi- cal scope, \nthe parser allocates objects on obstacks. When leaving scope, it frees all of the objects allocated within \nthat scope by free- ing the first object it allocated. Obstacks have been heavily optimized over a number \nof years and make extensive use of macros. We implemented ObstackHeap in heap layers and provided C-based \nwrapper functions that im- plement the obstack API. This effort required about one week and consists \nof 280 lines of code (around 100 are to implement the API wrappers), By contrast, the GNU obstack library \nconsists of around 480 lines of code and was refined over a period of at least six years. We ran gcc \non one of the reference inputs (scilab.i) and compared two versions of the original gcc with two versions \nof gee with Ob- stackHeap: the original macro-based code, the original with func- tion calls instead \nof macros, the ObstackHeap version layered on top of mallocHeap, and an ObstackHeap version that uses \na Free- 118 gcc: Obstack vs. ObstackHeap gcc parse: Obstack vs. ObstackHeap '~\" 250 \"~200 !50 100 50 \n  /m g j j lO / #a / J (a) Complete execution of gcc. (b) gcc's parse phase only. Figure 4: Runtime \ncomparison of gcc with the original obstack and ObstackHeap. Memory-Intensive Benchmarks Benchmark Description \nInput cfrac factors numbers a 36-digit number espresso optimizerfor PLAs test2 lindsay hypercubesimulator \nscript.mine LRUsim a locality analyzer an 800MB trace Perl Pert interpreter perfect.in roboop Roboticssimulator \nincluded benchmark Table 3: Memory-intensive benchmarks used in this paper. listHeap to optimize allocation \nand freeing of the default chunk size and mallocHeap for larger chunks: class ObstackType : public ObstackHeap<4096, \n HybridHeap<4096 + 8, // Obstack overhead Freel i s tHeap<ma i 1 ocHeap>, mallocHeap> { } ;  As with \n197. parser, we did not replace the general-purpose al- locator. Figure 4(a) shows the total execution \ntime for each of these cases, while Figure 4(b) shows only the parse phase. Layering Ob- stackHeap on \ntop of FreelistHeap results in an 8% improvement over the original in the parse phase, although its improvement \nover the original for the full execution of gcc is minimal (just over 1%). This result provides further \nevidence that custom allocators com- posed quickly with heap layers can perform comparably to care- fully \ntuned hand-written allocators. 6. Building General-Purpose Allocators In this section, we consider the \nperformance implications of build- ing general-purpose allocators using heap layers. Specifically, we \ncompare the performance of the Kingsley and Lea allocators [17] to allocators with very similar architectures \ncreated by composing heap layers. Our goal is to understand whether the performance costs of heap layers \nprevent the approach from being viable for building general-purpose allocators. We map the designs of \nthese allocators to heap layers and then compare the runtime and memory consumption of the original allocators \nto our heap layer implemen- tations, KingsleyHeap and LeaHeap. To evaluate allocator runtime performance \nand fragmentation, we use a number of memory-intensive programs, most of which were described by Zorn \nand Wilson [14, 15] and shown in Ta- ble 3: cfrac factors arbitrary-length integers, espresso is an opti- \nmizer for programmable logic arrays, lindsay is a hypercube simu- lator, LRUsim analyzes locality in \nreference traces, perl is the Perl interpreter included in SPEC2000 (2 5 3. per lbmk), and roboop is \na robotics simulator. As Table 4 shows, these programs exer- cise memory allocator performance in both \nspeed and memory effi- ciency. This table also includes the number of objects allocated and their average \nsize. The programs' footprints range from just 16K (for roboop) to over 1.5MB (for LRUsim). For all of \nthe programs except lindsay and LRUsim, the ratio of total memory allocated to the maximum amount of \nmemory in use is large. The programs' rates of memory allocation and deallocation (memory operations \nper second) range from under one hundred to almost two million per second. Except for LRUsim, memory \noperations account for a significant portion of the mntime of these programs. 6.1 The Kingsley Allocator \nWe first show how we can build KingsleyHeap, a complete general- purpose allocator using the FreelistHeap \nlayer described in Sec- tion 3.1 composed with one new heap layer. We show that Kingsley- Heap, built \nusing heap layers, performs as well as the Kingsley al- locator, The Kingsley allocator needs to know \nthe sizes of allocated ob- jects so it can place them on the appropriate free list. An object's size \nis often kept in metadata just before the object itself, but it can be represented in other ways. We \ncan abstract away object repre- sentation by relying on a getSize ( ) method that must be imple- mented \nby a superheap. SizeHeap is a layer that records object size in a header immediately preceding the object. \ntemplate <class SuperHeap>  class SizeHeap : public SuperHeap { public : inline void * malloc (size_t \nsz) { // Add room for a size field. freeObject * ptr = (freeObject *) SuperHeap: :malloc (sz + sizeof \n(freeObject)) ; // Store the requested size. ptr->sz = s2; return (void *) (ptr + i); } inline void \nfree (void * ptr) { SuperHeap: :free ((freeObject *) ptr -i) ;  119 Benchmark Objects cfrac 10,890,166 \nespmsso 4,477,737 lindsay 108,862 LRUsim 39,139 ped 8,548,435 roboop 9,268,221 Memory-Intensive Benchmark \nStatistics Total memory Max in use Average size 222,745,704 176,960 20.45 1,130,107,232 389,152 252.38 \n7,418,120 1,510,840 68.14 1,592,992 1,581,552 40.70 162,451,960 293,928 19.00 332,058,248 16,376 35.83 \nMemory ops Memory ops/sec 21,780,289 1,207,862 8,955,367 218,276 217,678 72,300 78,181 94 17,091,308 \n257,809 18,536,397 1,701,786 Table 4: Statistics for the memory-intensive benchmarks used in this paper. \nWe divide by runtime with the Lea allocator to obtain memory operations per second. } inline static size_t \ngetSize (void * ptr) { return ((freeOb ject *) ptr -l)->sz; } private: union freeObject { size_t sz; \n double _dummy; // for alignment. ); };  StfictSegHeap provides a general interface for im~ementing \nstrict segrega~d fits allocation. Segregated fits alloca~R ~vide o~ects into a number of sue cMsses, \nwhich are ranges of obje~ sizes. Memory ~quests for a given size are satisfied ~rectly from the '%in\" \nco~espon~ng to the ~quested size class. The heap ~mms deMlocated memory to the appropriate Mn. SttictSegHeap's \nargu- meres include the number of Mns, a function ~at maps oMe~ size to size class and size class to \nmaximum size, ~e heap Vpe for each bin, and the parent heap (for bigger obje~s). The implementation of \nStrictSegHeap is 32 lines of C++ code. The class definition ap- pear in ~gu~ 5. We now build KingsleyHeap \nusing these layeR. First, we im#e- ment the helper functions that support powevo~two size classes: inline \nint pow2getSizeelass (size_t sz) ( int c = 0; SZ = sz -i; while (sz > 7) { sz >>= i; c++; ) return \nc; ) inline size_t pow2getClassMaxsize (int i) { return 1 << (i+3); )  By combining these heap layers, \nwe can now define KingsleyHeap. We implement KingsleyHeap as a StrictSegHeap with 29 bins and power-of-two \nsize classes (supporting an object size of up to 282-1 bytes). Each size class is implemented using a \nFreelistHeap that gets memory from SbrkHeap (a thin layer over sbrk ( ) ). class KingsleyHeap : public \nStrictSegHeap<29, pow2getSizeClass, pow2getClassMaxSize, SizeHeap<FreelistHeap<SbrkHeap> >, SizeHeap<FreelistHeap<SbrkHeap> \n> > {}; A C++ programmer now uses this heap by declaring it as an object and directly using the malloc \nand free calls. KingsleyHeap kHeap; char * ch = (char *) kHeap.malloc (20); kHeap, free (ch) ; template \n<int NumBins, int (*getSizeClass) (size t), size_t (*getClassMaxSize) (int), class LittleHeap, class \nBigHeap> class StrictSegHeap : public BigHeap ( public: inline void * malloc (size_t sz) { void * ptr; \nint sizeClass = getSizeClass (sz); if (sizeClass >= NumBins) { // This request was for a \"big\" object. \nptr = BigHeap::malloc (sz); } else { size_t ssz = getClassMaxSize(sizeClass); ptr = myLittleHeap[sizeClass].malloc \n(ssz); } return ptr; } inline void free (void * ptr) { size_t objectSize = getSize(ptr); int objectSizeClass \n= getSizeClass (objectSize); if (objectSizeClass >= NumBins) { BigHeap::free (ptr); } else { while (getClassMaxSize(objectSizeClass) \n> objectSize) { objectSizeClass--; ) myLittleHeap[objectSizeClass].free (ptr); ) ) private: LittleHeap \nmyLittleHeap[NumBins]; ); Figure 5: The implementation of StrictSegHeap.  6.2 The Lea Allocator Version \n2.7.0 of the Lea allocator is a hybrid allocator with different behavior for different object sizes. \nFor small objects (_< 64 bytes), the allocator uses quick lists; for large objects (> 128K bytes), it \nuses virtual memory (raraap), and for medium-sized objects, it performs approximate best-fit allocation \n[17]. The strategies it em- ploys are somewhat intricate but it is possible to decompose these into a \nhierarchy of layers. Figure 7 shows the heap layers representation of LeaHeap, which is closely modeled \nafter the Lea allocator. The shaded area repre- sents LeaHeap, while the Sbrk and Mmap heaps depicted \nat the top are parameters. At the bottom of the diagram, object requests are managed by a SelectMmapHeap, \nwhich routes large size requests to be eventually handled by the Mmap parameter. Smaller requests are \nrouted to ThresholdHeap, which both routes size requests to a small and medium heap and in certain instances \n(e.g., when a 120 .... Run time: General-Purpose Allocators Space: General-Purpose Allocators ~ Kingsley \nB KingsleyHeap [] KingsleyHeap + coal. EILea [] LeaHeapJ [B Kingsley [] KingsleyHeap I~ KingsleyHeap \n+ coal. D Lea Ii LeaHeap [ @ 1 E Zo (a) Runtime normalized to the Lea allocator. (b) Space (memory consumption) \nnormalized to the Lea allocator. Figure 6: Runtime and space comparison of the original Kingsley and \nLea allocators and their heap layers counterparts. l\" .................... \"1 |................. I  \nlayers constitute around 500 lines of code, not counting comments or white space, while the Lea allocator \nis over 2,000 lines of code. LeaHeap is also more flexible than the original Lea allocator. For instance, \na programmer can use multiple instances of LeaHeaps to manage distinct ranges of memory, something that \nis not possible with the original. Similarly, we can make these heaps thread-safe when needed by wrapping \nthem with a LockedHeap layer. Be-cause of this flexibility of heap layers, we can easily include both \na thread-safe and non-thread-safe version of the same allocator in the same application. 6.3 Experimental \nResults We ran the benchmarks in Table 3 with the Kingsley allocator, KingsleyHeap, KingsleyHeap plus \ncoalescing (which we discuss in Section 8), the Lea allocator, and LeaHeap. In Figure 6(a) we present \na comparison of the mntimes of our benchmark applica- tions normalized to the original Lea allocator \n(we present the data used for this graph in Table 5). The average increase in runtime for KingsleyHeap \nover the Kingsley allocator is just below 2%. For Figure 7: A diagram of LeaHeap's architecture. the \ntwo extremely allocation-intensive benchmarks, cfrac and ro- boop, the increase in runtime is just over \n3%, demonstrating that the overhead of heap layers has minimal impact. Despite being cleanly decomposed \ninto a number of layers, KingsleyHeap per- sufficiently large object is requested), frees all of the \nobjects held in the small heap. We implemented coalescing and splitting using forms nearly as well as \nthe original hand-coded Kingsley allocator. Runtime of LeaHeap is between 1/2% faster and 20% slower \nthan two layers. CoalesceHeap performs splitting and coalescing, while the Lea allocator (an average \nof 7% slower). CoalesceableHeap provides object headers and methods that sup- Figure 6(b) shows memory \nconsumption for the same bench- port coalescing. SegHeap is a more general version of StrictSeg- marks \nnormalized to the Lea allocator (we present the data used Heap described in Section 6.1 that searches \nthrough all of its heaps for this graph in Table 6). We define memory consumption as the for available \nmemory. Not shown in the picture are AdaptHeap high-water mark of memory requested from the operating \nsystem. and DLList. AdaptHeap lets us embed a dictionary data structure For the Kingsley and Lea allocators, \nwe used the amount reported within freed objects, and for LeaHeap, we use DLList, which im- by these \nprograms; for the heap layers allocators, we directly mea- plements a FIFO doubly-linked list. While \nLeaHeap is not a com- sured the amount requested by both SbrkHeap and MmapHeap. plete implementation \nof the Lea allocator (which includes other KingsleyHeap's memory consumption is between 54% less and \nheuristics to further reduce fragmentation), it is a faithful model 11% more (on average 5.5% less), \nwhile LeaHeap's memory con- that implements most of its important features, including the hier- sumption \nis between 44% less and 19% more (on average 2% less) archy described here. than the Lea allocator. The \noutlier is roboop, which has an ex- We built LeaHeap in a total of three weeks. We were able to tremely \nsmall footprint (just 16K) that exaggerates the memory ef- reuse a number of layers, including SbrkHeap, \nMmapHeap, and ficiency of the heap layers allocators. Excluding roboop, the aver- SegHeap. The layers \nthat implement coalescing (CoalesceHeap age increase in memory consumption for KingsleyHeap is 4% and \nand CoalesceableHeap) are especially useful and can be reused to for LeaHeap is 6.5%. build other coalescing \nallocators, as we show in Section 8. The new 121 ...... Benchmark cfrac espresso Iindsay LRUsim perl \nroboop Runtime for General-Purpose Allocators Kingsley KingsleyHeap KingsleyHeap + coal. 19.02 19.75 \n25.94 40.66 40.91 44.56 3.05 3.04 3.16 Lea LeaHeap 19.09 20.14 41.12 46.33 3.01 3.03  836.67 827.10 \n826.44 831.98 828.36 66.94 70.01 73.61 66.32 68.60 10.81 11.19 17.89 10.89 13.08 Table 5: Runtime (in \nseconds) for the general-purpose allocators described in this paper. Memory Consumption for General-Purpose \nAllocators Benchmark Kingsley KingsleyHeap KingsleyHeap + coal. Lea LeaHeap cfrac 270,336 280,640 271,944 \n208,896 241,272 espresso 974,848 992,032 541,696 462,848 448,808 lindsay 2,158,592 2,120,752 1,510,688 \n1,515,520 1,506,720 LRUsim 2,555,904 2,832,272 1,887,512 1,585,152 1,887,440 perl 425,984 454,024 342,344 \n331,776 337,408 roboop 45,056 20,760 11,440 20,480 11,616 Table 6: Memory consumption (in bytes) for \nthe general-purpose allocators described in this paper. This investigation provides several insights. \nFirst, we have demon- strated that the heap layers framework is sufficiently robust that even quite sophisticated \nallocator implementations can be devel- oped using it. Purthermore, we have shown that we can quickly \n(in a matter of weeks) assemble an allocator that is structurally similar to one of the best general-purpose \nallocators available. In addition, while we have spent little time tuning our current implementation, \nits performance and fragmentation are comparable to the original allocator. 7. Software Engineering Benefits \nOur experience with building and using heap layers has been quite positive. Some of the software engineering \nadvantages of using mixins to build software layers (e.g., heap layers) have been dis- cussed previously, \nespecially focusing on ease of refinement [5, 9, 21]. We found that using heap layers as a means of stepwise \nrefine- ment greatly simplified allocator construction. We also found the following additional benefits \nof using layers. Because we can generally use any single layer to replace an allo- cator, we are often \nable to test and debug layers in isolation, making building allocators a much more reliable process. \nBy adding and re- moving layers, we can find buggy layers by process of elimination. To further assist \nin layer debugging, we built a simple DebugHeap layer (shown in Figure 8) that checks for a variety of \nmemory al- location errors, including invalid and multiple frees. During de- velopment, we insert this \nlayer between pairs of layers as a sanity check. DebugHeap is also useful as a layer for finding errors \nin client applications. By using it with our heap layers allocators, we discovered a number of serious \nallocation errors (multiple frees) in p2c, a program we had previously planned to use as a bench- mark. \nThe additional error-checking that heap layers enable, combined with compiler elimination of layer overhead, \nencourage the divi- sion of allocators into many layers. When porting our first ver- sion of the LeaHeap \nto Solads, we found that one of our layers, CoalesceSegHeap, contained a bug. This heap layer provided \nthe functionality of SegHeap as well as coalescing, splitting and adding headers to allocated objects. \nBy breaking out coalescing and header management into different layers (CoalesceHeap and Coalesceable- \nHeap) and interposing DebugHeap, we were able to find the bug quickly, The new layers had the additional \nbenefit of allowing us to apply coalescing to other heaps, as we do in the next section. 8. Heap Layers \nas an Experimental Infrastructure Because heap layers simplify the creation of memory allocators, we \ncan use them to perform a wide range of memory allocation experiments that previously would have required \na substantial pro- gramming effort. In this section, we describe one such experiment that demonstrates \nthe use of heap layers as an experimental infra- structure. As Figures 6(a) and 6(b) demonstrate, the \nKingsley allocator is fast but suffers from excessive memory consumption. Wilson and Johnstone attribute \nthis effect to the Kingsley allocator's lack of coalescing or splitting that precludes reuse of objects \nfor different- sized requests [15]. A natural question is to what extent adding coalescing remedies this \nproblem and what impact it has on per- forrnance. Using heap layers, we just add coalescing and splitting \nwith the layers we developed for LeaHeap. We ran our benchmarks with this coalescing Kingsley heap and \nreport runtime and performance numbers in Figures 6(a) and 6(b) as \"KingsleyHeap + coal.\" Coalescing \nhas a dramatic effect on memory consumption, bringing KingsleyHeap fairly close to the Lea allocator. \nCoalescing decreases memory consumption by an average of 50% (as little as 3% and as much as 80%). For \nmost of the programs, the added cost of coalescing has little impact, but on the extremely allocation-intensive \nbenchmarks (cfrac and ro- boop), this cost is significant. This experiment demonstrates that coalescing \nachieves effective memory utilization, even for an allo- cator with high internal fragmentation. It also \nshows that the perfor- mance impact of immediate coalescing is significant for allocation- intensive \nprograms, in contrast to the Lea allocator which defers coalescing to certain circumstances, as described \nin Section 2.  9. Conclusion and Future Work Dynamic memory management continues to be a critical part \nof many important applications for which performance is crucial. Pro- grammers, in an effort to avoid \nthe overhead of general-purpose al- location algorithms, write their own custom allocation implemen- \ntations in an effort to increase performance further. Because both general-purpose and special-purpose \nallocators are monolithic in design, very little code reuse occurs between implementations of either \nkind of allocator. 122 ten,plate <class SuperHeap> class DebugHeap : public SuperHeap { private: // \nA freed object has a special (invalid) size. enum { FREED = -I ); // \"Error messages\", used in asserts. \nenu~ { MALLOC_RETURNEDALLOCATED_OBJECT = 0, FREE_CALLED_ON_INVALID_OBJECT = 0, FREE_CALLED_TWICEONSAME_OBJECT \n= 0 }; public: inline void * malloc (size_t sz) { void * ptr = SuperHeap::malloc (sz); if (ptr == NULL) \n return NULL; // Fill the space with a known value. memset (ptr, 'A', sz); mapType::iterator i = allocated.find \n(ptr); if (i == allocated.end()) { allocated.insert (pair<void *, int>(ptr, sz)); } else { if ((*i).second \n!= FREED) { assert (MALLOC_RETURNED_ALLOCATED_OBJECT); } else { (*i).second = sz; } } return ptr; } \ninline void free (void * ptr) { mapType::iterator i = allocated.find (ptr); if (i == allocated.end()) \n{ assert (FREE_CALLED_ON INVALID_OBJECT); return; } if ((*i).second == FREED) { assert (FREE_CALLED_TWICE_ONSAMEOBJECT); \nreturn; ) // Fill the space with a known value. memset (ptr, 'F', (*i).second); (*i).second = FREED; \nSuperHeap::free (ptr); } private: typedef map<void *, int> mapType; // A map of tuples (obj address, \nsize). mapTyl~e allocated; }; Figure 8: The implementation of DebugHeap. In this paper, we describe \na framework in which custom and gen- eral purpose allocators can be effectively constructed from compos- \nable, reusable parts. Our framework, heap layers, uses C++ tem- plates and inheritance to allow high-performance \nallocators, both general and special purpose, to be rapidly created. Even though heap layers introduce \nmany layers of abstraction into an imple- mentation, building allocators using heap layers can actually \nmatch or improve the performance of monolithic allocators. This non- intuitive result occurs, as we show, \nbecause heap layers expand the flexibility of compiler-directed inlining. Based on our design, we have \nimplemented a library of reusable heap layers: layers specifically designed to combine heaps, lay- ers \nthat provide heap utilities such as locking and debugging, and layers that support application-specific \nsemantics such as region al- location and stack-structured allocation. We also demonstrate how these \nlayers can be easily combined to create special and general purpose allocators. To evaluate the cost \nof building allocators using heap layers, we present a performance comparison of two custom allocators \nfound in SPEC2000 programs (197. parser and 17 6. gee) against an equivalent implementation based on \nheap layers. In both cases, we show that the use of heap layers improves performance slightly over the \noriginal implementation. This demonstrates the surprising result that the software engineering benefits \ndescribed above have no performance penalty for these programs. We also compare the performance of a \ngeneral-purpose allocator based on heap layers against the performance of the Lea allocator, widely considered \nto be among the best uniprocessor allocators available. While the allo- cator based on heap layers currently \nrequires more CPU time (7% on average), we anticipate that this difference will shrink as we spend more \ntime tuning our implementation. Furthermore, because our implementation is based on layers, we can immediately \nprovide an efficient scalable version of our allocator for multithreaded pro- grams comparable to Hoard \n[6], whereas the Lea allocator requires significant effort to rewrite for this case. Our results suggest \na number of additional research directions. First, because heap layers are so easy to combine and compose, \nthey provide an excellent infrastructure for doing comparative per- formance studies. Questions like \nthe cache effect of size tags, or the locality effects of internal or external fragmentation can be studied \neasily using heap layers. Second, we anticipate growing our li- brary of standard layers to increase \nthe flexibility with which high- performing allocators can be composed. At the same time, we be- lieve \nthat we can build better general-purpose allocators by using heap layers. Finally, we are interested \nin expanding the applica- tion of mixin technology beyond memory allocators. The potential that C++ templates \nallow abstraction and composition at no per- formance cost opens up a number of possibilities to redesign \nand refactor other performance-critical infrastructures.   10. Acknowledgements Thanks to Michael Parkes \nfor many enjoyable discussions about al- locator design and construction and to Per-Ake Larson for pointing \nout the importance of custom allocators in server applications, to Yannis Smaragdakis for first bringing \nthe mixin technique to our attention, and to Don Batory, Rich Cardone, Doug Lea, Michael VanHilst, and \nthe PLDI reviewers for their helpful comments. The heap layers infrastructure and accompanying benchmark \npro- grams may be downloaded from http : //www. cs .utexas. edu/users/emery/research.  11. References \n[1] Apache Foundation. Apache web server. http://www.apaehe.org. [2] G. Attardi and T. Flagella. A customizable \nmemory management framework. In Proceedings of the USENIX C++ Conference, Cambridge, Massachussetts, \n1994. [3] Giuseppe Attardi, Tito Flagella, and Pietro Iglio. A customizable memory management framework \nfor C++. In So, ware Practice &#38; Experience, number 28(11), pages 1143-1183. Wiley, 1998. [4] David \nA. Barrett and Benjamin G. Zorn. Using lifetime predictors to improve memory allocation performance. \nIn Proceedings of the 1993 ACM SIGPLAN Conference on Programming Language Design and Implementation \n(PLDI), pages 187-196, Albuquerque, New Mexico, June 1993. [5] Don Batory, Clay Johnson, Bob MacDonald, \nand Dale yon Heeder. Achieving extensibility through product-lines and domain-specific languages: A case \nstudy. In Proceedings of the International Conference on Software Reuse, Vienna, Austria, 2000.  123 \n[6] Emery D. Berger, Kathryn S. McKinley, Robert D. Blumofe, and Paul R. Wilson. Hoard: A scalable memory \nallocator for multithreaded applications. In International Conference on Architectural Support for Programming \nLanguages and Operating Systems (ASPLOS-IX), pages 117-128, Cambridge, MA, November 2000. [7] Gilad Bracha \nand William Cook. Mixin-based inheritance. In Norman Meyrowitz, editor, Proceedings of the Conference \non Object-Oriented Programming: Systems, Languages, and Applications ( OOPSLA ) / Proceedings of the \nEuropean Conference on Object-Oriented Programming (ECOOP), pages 303-311, Ottawa, Canada, 1990. ACM \nPress. [8] Dov Bulka and David Mayhew. Efficient C++. Addison-Wesley, 2001. [9] Richard Cardone and \nCalvin Lin. Comparing frameworks and layered refinement. In Proceedings of the 23rd International Conference \non Software Engineering (ICSE), May 2001. [10] Trishul Chilimbi. Private communication. May 2000. [11] \nDavid Gay and Alex Aiken. Memory management with explicit regions. In Proceedings of the 1998 ACM S1GPLAN \nConference on Programming Language Design and Implementation (PLDI), pages 313 - 323, Montreal, Canada, \nJune 1998. [12] Wolfram Gloger. Dynamic memory allocator implementations in linux system libraries. http://www.dent.med.uni-muenchen.de/\" \nwmglo/malloc-slides.html. [13] Dirk Grunwald and Benjamin Zorn. CustoMalloc: Efficient synthesized memory \nallocators. In Software Practice &#38; Experience, number 23(8), pages 851-869. Wiley, August 1993. [14] \nDirk Grunwald, Benjamin Zorn, and Robert Henderson. Improving the cache locality of memory allocation. \nIn Proceedings of the 1993 ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI), \npages 177-186, New York, NY, June 1993. [15] Mark S. Johnstone and Paul R. Wilson. The memory fragmentation \nproblem: Solved? In International Symposium on Memory Management, Vancouver, B.C., Canada, 1998. [16] \nMurali R. Krishnan. Heap: Pleasures and pains. Microsoft Developer Newsletter, February 1999. [17] Doug \nLea. A memory allocator. http:llg.oswego.edu/dl/html/malloc.html. [18] Scott Meyers. More Effective \nC++. Addison-Wesley, 1997. [19] D.T. Ross. The AED free storage package. Communications of the ACM, 10(8):481.--492, \n1967. [20] Matthew L. Seidl and Benjamin G. Zorn. Segregating heap objects by reference behavior and \nlifetime. In International Conference on Architectural Support for Programming Languages and Operating \nSystems (ASPLOS-VIII), pages 12.-23, October 1998. [21] Yannis Smaragdakis and Don Batory. Implementing \nlayered design with mixin layers. In Eric Jul, editor, Proceedings of the European Conference on Object-Oriented \nProgramming (ECOOP '98), pages 550--570, Brussels, Belgium, 1998. [22] Standard Performance Evaluation \nCorporation. SPEC2000. http'J/www.spee.org. [23] Mads Tofte and Jean-Pierre Talpin. Region-based memory \nmanagement. Information and Computation, 1997. [24] Michael VanHUst and David Notkin. Using role components \nto implement collaboration-based designs. In Proceedings of OOPSLA 1996, pages 359-369, October 1996. \n[25] Kiem-Phong Vo. Vmalloc: A general and efficient memory allocator. In Software Practice &#38; Experience, \nnumber 26, pages 1-18. Wiley, 1996. [26] P. R. Wilson, M. S. Johnstone, M. Neely, and D. Boles. Dynamic \nstorage allocation: A survey and critical review. Lecture Notes in Computer Science, 986, 1995. 124 \n \n\t\t\t", "proc_id": "378795", "abstract": "", "authors": [{"name": "Emery D. Berger", "author_profile_id": "81100228645", "affiliation": "Dept. of Computer Sciences, The University of Texas at Austin, Austin, TX", "person_id": "PP14089241", "email_address": "", "orcid_id": ""}, {"name": "Benjamin G. Zorn", "author_profile_id": "81100190820", "affiliation": "Microsoft Research, One Microsoft Way, Redmond, WA", "person_id": "P28972", "email_address": "", "orcid_id": ""}, {"name": "Kathryn S. McKinley", "author_profile_id": "81100402805", "affiliation": "Dept. of Computer Science, University of Massachusetts, Amherst, MA", "person_id": "P157900", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/378795.378821", "year": "2001", "article_id": "378821", "conference": "PLDI", "title": "Composing high-performance memory allocators", "url": "http://dl.acm.org/citation.cfm?id=378821"}