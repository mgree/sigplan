{"article_publication_date": "05-01-2001", "fulltext": "\n A Framework for Reducing the Cost of Instrumented Code* Matthew Arnold Barbaxa G. Ryder Rutgers University, \nPiscataway, N J, 08854 {marnold, ryder}Ocs, rutgers, edu tIBM T.J. Watson Research Center, Hawthorne, \nNY, 10532 Abstract Instrumenting code to collect profiling information can canse substantial execution \noverhead. This overhead makes instru- mentation difficult to perform at runt/me, often preventing many \nknown o]fiine feedback-directed optimizations from being used in online systems. This paper presents \na gen-eral framework for performing instrumentation sampling to reduce the overhead of previously expensive \ninstrumenta- tion. The framework is simple and effective, using code- duplication and counter-based sampling \nto allow switching between instrumented and non-instrumented code. Our framework does not rely on any \nhardware or oper- ating system support, yet provides a high frequency sample rate that is tunable, allowing \nthe tradeoff between overhead and accuracy to be adjusted easily at runt/me. Experimen- tal results are \npresented to validate that our technique can collect accurate profiles (93-98% overlap with a perfect \npro- file) with low overhead (averaging ,-,6% total overhead with a naive implementation). A Jalapefio-specific \noptimization is also presented that reduces overhead further, resulting in an average total overhead \nof ~3%. Introduction Early virtual machines with JIT compilation relied on simple static strategies \nfor choosing compilation targets, typically compiling each method with a fixed set of optimizations the \nfirst time it was invoked. Examples of such virtual machines include [1, 13, 18, 24, 33, 40]. More advanced \nadaptive sys- tems [5, 22,30,31,35] moved beyond this simple strategy by dynamically selecting a subset \nof all methods for optimiza- tion, attempting to focus optimization effort on program hot spots. This \nselective optimization approach avoids the over- head of optimizing all methods, yielding larger performance \nimprovements for shorter running programs [7]. Long running applications, such as server applications, \nwill easily amortize the cost of optimizing all methods, for *Funded,in part, by NSF grant CCR-9808607. \n tMatthew Arnold is an intern at Watson; Barbara Ryder was on sabbatical leave at Watson from August \n2000 -March 2001. Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for profit or \ncommercial advan-tage and that copies bear this notice and the full citation on the first page. 'To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission \nand/or a fee. PLDI2001 6/01 Snowbird, Utah, USA &#38;#169; 2001 ACM ISBN 1-58113-414-2/01/06...$5.00 \n any reasonable level of optimization. For these applications the most substantial performance improvements \nwill come from feedback-directed optimizations, where profiling infor- mation is used to decide not only \nwhat to optimize, but how to optimize. There exists a large body of work on collecting ojfiine profiles \n[3, 10, 11, 15, 26], as well as optimizations based on offiine profiles [6,16,17,19,20,27]. Although \nsome systems [5, 9, 21, 22, 32] apply limited forms of online feedback-directed optimizations, most of \nthe offiine work mentioned above has not yet been applied in fully automated online systems. The main \ndifficulty in applying these optimizations online is that they often rely on instrumenting the code to \ncollect detailed information about program execution, and instrumentation can cause substantial performance \ndegradation. Overheads in the range of 30%-1,000% above non-instrumented code is not uncommon [3,10,11,16,17, \n27], and overheads in the range of 10,000% (100 times slower) have been reported [16]. An online system \nneeds to execute instrumented code for some period of time, prior to performing optimization. The overhead \nintroduced by instrumentation makes this task dif- ficult to perform for several reasons. First, when \ninstrumen- tation is expensive, the instrumented code must be run for only a short amount of time to \nkeep overhead to a minimum; however, being forced to profile for a small time interval is not desirable \nbecause the profile collected may not be repre- sentative of overall program behavior. A second problem \nis that there must be a way to stop the instrumented code from executing, to prevent the program from \nrunning indefinitely with poor performance. This could be achieved by using dy- namic instrumentation \n[28, 36] to insert and remove instru- mentation without recompiling the method, or by perform- ing on-stack \nreplacement [29] to hot-swap execution back to the non-instrumented version while the method is running. \nHowever, dynamic instrumentation is architecture-specific, and may introduce complexities such as maintaining \ncache consistency. On-stack replacement can be difficult for opti- mized code because a method can be \nswapped only at pro- gram points where full program state is known. A common solution (used by [29,31]) \nfor implemeflting on-stack replace- meat is to restrict optimizations by introducing interrupt points \nwhere state is required to be consistent. We present a general framework for performing in-strumentation \nsampling, allowing previously expensive in- strumentation to be performed accurately with low over- head. \nThe framework using code-duplication combined with compiler-inserted counter-based sampling to allow \nfine-grained switching between instrumented and non-instrumented code. Previous work [8,14, 25, 36, 39] \nhas used 168 sampling to reduce the cost of instrumentation, but these techniques axe specific to one \nkind of instrumentation. To the best of our knowledge, this is the first technique that allows general \ninstrumentation to be performed with low overhead. Modified Instrumented Method Our framework offers \nthe following advantages: Instrumentation can be performed for a longer pe- riod of time while causing \nonly minimal performance degradation, allowing previously expensive instrumen- tation techniques to be \nused at runtime, even without the ability to perform dynamic instrumentation or on- stack replacement. \no The framework is tunable, allowing the tradeoff be- tween overhead and accuracy to be adjusted easily \nat runtime. Most instrumentation techniques can be incorporated into our framework without modification. \nOverhead is controlled entirely by the framework, allowing imple- mentors of instrumentation techniques \nto concentrate on developing new techniques quickly and correctly, rather than focusing on minimizing \noverhead. \u00ae Multiple types of instrumentation can be used si-multaneously, without the normal concern \nfor over- head. This allows an adaptive system to perform sev- eral forms of instrumentation while recompiling \nthe method only once. \u00ae The framework does not rely on any hardware or op- erating system support. The \nframework is deterministic, which simplifies de- bugging, l~unning a deterministic application twice \nwill result in identical profiles. The framework is implemented and evaluated using the Jalapefio JVM \n[2], providing experimental evidence of the overhead and accuracy when applied to two types of in- strumentation. \nThe results show that high accuracy can be achieved (93-98% overlap with a perfect profile) with low \noverhead, (averaging --,6% total overhead with a naive im- plementation). A Jalapefio-specific implementation \nis also presented as an example of how hardware- or compiler-specific optimizations can be used to further \nreduce over- head, resulting in an average total overhead of ,-~3%. Section 2 describes the instrumentation \nsampling frame- work in detail. Section 3 describes two variations designed to reduce the space required \nby the framework. Section 4 describes an experimental evaluation of our framework im- plementation. Sections \n5 and 6 discuss related work and conclusions, respectively. Instrumentation Sampling Framework This \nsection describes our sampling framework. Section 2.1 discusses existing sampling mechanisms, and Section \n2.2 de- scribes counter-based sampling, the mechanism used to trig- ger samples in our framework. Our \nframework essentially transforms an instrumented method that has high overhead, into a modified instru- \nmented method that has low overhead. This is accomplished by introducing a second version of the code, \ncalled the du-plicated code, within the instrumented method, as shown in Figure 1. The original version \nof the code is now referred to as the checking code because it is modified slightly to al- low execution \nto swap back and forth between the checking Checking~ Duplicated Code Code  High Overhead  Figure \n1: A high-level view of an instrumented method gen- erated by our framework. A second version of the \ncode is introduced, called the duplicated code, which contains all instrumentation. The original code \nbecomes the checking code, which is minimally instrumented to allow control to transfer in and out of \nthe duplicated code in a fine-grained and controlled maimer. code and duplicated code in a fine-gained, \ncontrolled man- ner. On regular sample intervals, execution moves into the duplicated code for a small, \nbounded amount of time, so expensive instrumentation can be inserted in the duplicated code. Overhead \nmay be kept to a minimum by ensuring that the majority of execution occurs in the checking code. The \nswitching between the checking and duplicated code is illustrated in Figure 2. The checking code has \nconditional branches inserted (which we refer to as checks) that monitor a sample condition. When the \nsample condition is true, a sample is triggered and control jumps to duplicated code, rather than continuing \nin the checking code. Checks are placed on all method entries and backward branches (which will be referred \nto as backedges) in the checking code. This placement of the checks ensures that (a) only a bounded amount \nof execution occurs between checks, and (b) all code has the opportunity to be sampled. The duplicated \ncode is also modified so that there are no backedges within the duplicated code (also shown in Fig- ure \n2). Instead, all backedges in the duplicated code trans- fer control back to the checking code, ensuring \nthat only a bounded amount of time is spent in the duplicated code dur- ing each sample. The ratio of \ntime spent in duplicated code vs. checking code can be controlled by changing the rate at which the sample \ncondition is true. If the instrumented method is no longer needed, but the method continues to execute \n(does not return), setting the sample condition per- manently to false will ensure that execution remains \nin the checking code. Execution will not switch back to a totally non-instrumented version of the code \nuntil the method ex- its (nor can a newly optimized version of the code execute); however, the overhead \nof the checking code is small enough to be of little concern, especially compared to the cost of instrumentation. \nThis version of the framework will be referred to as Full-Duplication, since all of the code in the method \nis duplicated. Full-Duplication has the desirable property that the checking code performs checks on \nmethod entries and backedges only. For ease of reference this will be referred to as Property 1, defined \nas follows: 169  Checking Duplicated Legend I Method Entry ] Original Basic Block Duplicated Basic \nBlock ( \\ \\ B~nchifsample condffionis~ue > Edges already existing between basic blocks w..~,. Edgesadded \nbetween original and duplicated code Figure 2: Illustration of the flow of control between the checking \ncode and duplicated code. All method entries and backedges in the checking code contain a conditional \nbranch that jumps to the duplicated code when a sample condition is true. All backedges in the duplicated \ncode are modified to return to the checking code. Property 1 The number of checks executed in the checking \ncode is less than or equal to the number of backedges and methods entries executed, independent of the \ninstrumenta- tion being performed.  Applicability to various types of instrumentation Many of the common \nprofiling techniques can be used without mod- ification in our framework. For example, any instrumenta- \ntion designed to perform event counting (such as intrapro- cedural edge or path profiling, field access \nprofiling, value profiling, etc.) will work effectively when inserted as-is into the duplicated code. \nInstrumentation that counts events as- sociated with backedges is not a problem because the instru- mentation \ncan be attached to the edge transferring control from the duplicated code to the checking code (this \nedge was previously a backedge in the duplicated code). There are some profiling techniques that require \nspecial treatment to be sampled correctly in our framework. For example, some profiling techniques rely \non observing events in succession, such as [3], which updates a context-sensitive data structure on all \nmethod entries and exits. Profiling techniques such as these will need to be modified to produce accurate \nresults in a samphng context; work such as [8, 39] are examples of how this can be achieved. Another \nexample would be monitoring the behavior of multiple (N) consecu- tive loop iterations. This could be \nachieved in our framework by adding a counted backedge within the duplicated code. After N iterations \nave profiled, control would return to the checking code. There are also some profiles that are impossible \nto col- lect via sampling. For example, exhaustive instrumentation can be used to establish that a particular \nevent never oc- cured during the profiled interval; this is not possible with a sampled profile. However, \nit is not clear that this func- tionality is particularly useful for the purpose of an adaptive JVM, \nbecause even with perfect knowledge of the past, no guarantees can be made about future behavior. The \ngoal of profile-directed optimization is to predict the likelihood of future behavior, which can be achieved \nwith both sampled and exhaustive profiles. 2.1 Trigger mechanisms Our instrumentation sampling framework \nrelies on a trig-ger to determine when execution should transfer into the instrumented code. To keep \noverhead low, samples must be taken infrequently enough to ensure that the majority of execution remains \nin the checking code. However, to en- sure accuracy, samples must be taken frequently enough to allow \na reasonable sample set to be collected. Even more importantly, samples must be triggered in a manner \nthat is statistically meaningful; that is, the basic blocks in the instrumented code must be executed \nproportionally to their execution frequency in the non-instrumented code. One approach for triggering \nsamples is to use some type of hardware or operating system timer interrupt. In our framework, timer \ninterrupts could be used to set a \"trigger bit\" that is monitored by the checks in the checking code. \nThe approach of checking a timer-set bit has been used pre- viously (Self-91 [18], Jalapefio [2]) to \ndetermine when system services should be performed One drawback of relying on a timer interrupt is that \nthe sample rate is limited by the frequency of the inter- rupt, which may be a problem when sampling \non the level of basic blocks or instructions. A more serious drawback is that when used in our framework, \nthis technique would not produce a proper distribution of execution in instrumented code. Our framework \ndoes not take a sample immediately upon receiving the timer interrupt, but instead jumps to in- strumented \ncode only after the next check in the checking code is reached. Any sequence of instructions that executes \nfor a long time (due to an I/O operation, etc.) has a high probability of having a timer interrupt issued \nduring its ex- ecution, which, in turn, causes the next sequence of instruc- tions to be sampled. Section \n4.6 confirms that this improper attribution of samples, as well as the low sample frequency, substantially \nreduces the accuracy of our framework. DCPI [4] describes a sampling system that uses inter- rupts generated \nby the performance counters on the AL- PHA processor, allowing a very high sample rate (5200 sam- ples/sec \non a 333-MHz processor). This technique could be incorporated into our framework by using the high fre- \nquency interrupt to set the trigger bit that is monitored by the checking code. However, similar to the \ntimer-interrupt, this technique would improperly attribute samples in our framework. In addition, this \ntechnique requires hardware performance counters that signal interrupts upon overflow, a feature not \navailable on all architectures. To obtain an accurate distribution of samples in our framework, the number \nof times each check (in the checking code) triggers a sample should be proportional to the num- ber of \ntimes that particular check is executed. Since we do not know of any hardware performance counter that \ncounts backedges and method entries, our framework performs the counting in software, as described in \nthe next section.  2.2 Compiler-inserted counter-based sampling Counting a particular event and sampling \nwhen the counter reaches a threshold (which we refer to as counter-based sam-pling) is an effective way \nof triggering samples proportion- ally to the frequency of that event. This is the fundamental 170 if \n(globalCounter <= O) { takeSampleO; globalCounter = resetValue; } globalCounter--; Figure 3: Code \ninserted for a counter-based check principle behind the accuracy of DCPI [4], where perfor- mmnce counters \ncount instruction cycles, thus sampling in- structions proportionally to their execution frequency. How- \never, it may be desirable to sample events for which there is no counter-based hardware interrupt available, \nas is the case with our framework, which needs to count backedges and method entries. DCPI approximated \nfrequencies of non-counted events (intraprocedural edges) using flow con- straints, and showed the accuracy \nobtained to be inferior to the accuracy of counted events. To obtain high accuracy when no hardware counting \nsup- port is available, we propose implementing a counter-based trigger in software by having the compiler \ninsert code to decrement and check a global counter as shown in Figure 3. We call this technique compiler-inserted \ncounter-based sam- pling. There are several options for implementing such an approach; the simplest is \nto execute the code exactly as shown in Figure 3 each time an event occurs. The counter variable (globalCounter) \nwill most likely be in a register, or in the cach% and the branch will be predicted (not taken), therefore \nthe performance overhead should be low. Such an approach was implemented in Jalapefio without using a \ndedicated register, placing the code in Figure 3 on all backedges and method entries, and the overhead \naveraged 4.9% (for executing the checks only, when no samples were taken). A detailed description of \nthe overhead is included in Section 4.2. For multi-threaded applications, the global counter may raise \nsome concerns. First, access to the global counter is not synchronized for performance reasons, so data \nraces may occur. Fortunately, it is not necessary to maintain 100% ac- curacy of the global counter, \nas it is simply a means of trig- gering samples on a semi-regular basis. Having the counter value off-by-one \noccasionally would have little affect on the resulting accuracy. A more serious problem is that access \nto a single global counter could become a performance bottle- neck as the number of threads and processors \nincreases. In this case, the global counter could be replaced by thread- or processor-specific counters, \nallowing unsynchronized access to the counter, with no resource contention. As long as the overhead of \nthe counting and checking is kept to a minimum, the advantages of compiler-controlled counter-based sampling \nare numerous. First, it is simple to implement, and allows high frequency sample rates that can be adjusted \nat runtime. Second, it does not rely on any hardware or operating system support. ~ Finally, counter- \nbased checks trigger samples deterministically, creating re- producible sampling results for deterministic \napplications. Certain architectures may have instructions that cart be used to reduce the cost of counter-based \nchecks. For exam- ple, the powerPC architecture has a decrement-and-check instruction that decrements \na count register, compares it against zero, and performs a conditional branch, allowing 1Although hardware \nand O.S. techniques may be used to lower the overhead of the checks, no support from either is required. \n Checking Duplicated Checking Duplicated ,,' (B) \"~ Figure 4: Removing non-instrumented nodes from \nthe du- plicated code can increase the number of checks executed. Dark nodes represent basic blocks containing \ninstrumenta- tion. (A) represents an instrumented method with all ba- sic blocks duplicated. (B) shows \nan instrumented method where the non-instrumented node is not duplicated. Note the extra check (diamond) \nin the checking code of (B). the code in Figure 3 to be executed in one instruction. VLIW and superscalar \narchitectures may also hide the cost of the checks in unused cycles. There may also be compiler-specific \ntechniques that can further reduce the overhead of the checks. For example, in Jalapefio, all backedges \nand method entries contain yield-points that check whether or not the executing code should yield to \nthe thread scheduler. These yieldpoints can be ex- ploited to eliminate much of the overhead of the checking \ncode (described further in Section 4.3). Although this ex- ample is specific to Jalapefio, it serves \nas art example of how hardware- or compiler-specific techniques can be used to reduce the overhead of \nthe counter-based checks. 3 Space-saving variations Full-Duplication, the algorithm described in Section \n2, uses two versions (checking and duplicated) of each instru- mented method, thus increasing both space \nand compile time. Although this is clearly not desirable, it may not be as bad as it first appears. Duplicating \nthe code should have a minimal effect on locality because the duplicated code is ex- ecuted infrequently \nand cmn be placed somewhere out of the common path. Although compile time is increased, it is not necessarily \ndoubled. As discussed further in Section 4.3, our implementation performs the doubling of all code after \nmost of the compilation has taken place, increasing compile time by 34%. Finally, an adaptive system \nwill likely instrument only the hot methods, and not necessarily at the same time. If space is limited, \nthe number of methods instrumented si- multaneously cart be limited. Nevertheless, it is undesirable \nto increase space when unnecessary. In scenarios where instrumentation is sparse, ideally only those \nnodes containing instrumentation would need to be duplicated. Unfortunately, it is not always possi- \nble to remove a non-instrumented node from the duplicated code without violating Property 1. As shown \nin Figure 4, when the non-instrumented node is removed from the du- plicated code, an additional check \nmust be added to allow the second instrumented node to be sampled. There are many space-saving variations \nof our frame- 171 work. Two such variations, Partial-Duplication and No-Duplication, axe presented below. \n3.1 Variation 1: Partial-Duplication The goal of the Partial-Duplication algorithm is to re-move as many \nnon-instrumented basic blocks from the du- plicated code as possible without violating Property 1. Two \ntypes of nodes in the duplicated code axe defined: top-nodes and bottom-nodes, both of which can be removed \nfrom the duplicated code without invalidating Property 1. Both types of nodes axe defined on the duplicated \ncode DAG, which is the duplicated code with all backedges removed. A bottom-node is defined to be any \nnon-instrumented node, n, in the duplicated code DAG such that no instru- mented nodes are reachable \nfrom n. All bottom-nodes can be removed from the duplicated code without violating Property 1 because \nonce a bottom- node is executed, no further instrumentation will be per- formed without returning to \nthe checking code first. Any edge in the duplicated code that previously connected an instrumented node \nto a bottom-node needs to be adjusted to branch to the corresponding node in the checking code. A top-node \nis defined to be any non-instrumented node, n, in the duplicated code DAG such that no path from entry \nto n contains an instrumented node. All top-nodes can be removed from the duplicated code without violating \nProperty 1; however, the following two adjustments must be made: 1. In the checking code, all checks \nthat branch to a top- node should be removed. 2. In the duplicated code, for every edge that previously \nconnected a top-node to an instrumented node, the corresponding edge in the checking code should have \na check added.  Figure 5 revisits the code from Figure 2, but with Paxtial-Duplication updates applied, \nassuming that only the two shaded nodes contain instrumentation. The check after method entry is removed \nbecause it would have branched to a top-node. A check is added to the edge ex- iting the basic block \nlabeled \"1\" because the corresponding edge in the duplicated code connected a top-node to an in- strumented \nnode. In this particular example, the two checks can be combined into one. The edges labeled \"2\" and \n\"3\" now lead back to checking code because they previously con- nected an instrumented node and a bottom-node. \nThis technique eliminates duplicated nodes without vi- olating Property 1. Although the static number \nof checks may increase or decrease, the dynamic number of checks ex- ecuted is less than or equal to \nthe number executed with Full-Duplication. Instrumentation is performed identi- cally to Full-Duplication. \n 3.2 Variation 2: No-Duplication If Property 1 can be weakened, allowing more than one check to be executed \nper loop iteration or method call, there are several other alternatives for reducing code duplication; \nany non-instrumented node can be removed from the duplicated code as long as the appropriate checks are \nadded in the checking code. In fact, by guarding all instrumentationop- erations with checks, there is \nno need to duplicate any code. Such an approach will be referred to as No-Duplication and is shown in \nFigure 6, which illustrates one basic block with two instrumented instructions. Although none of the \n Checking Duplicated I Method Entry I Top Node ~ Bo11om Nodes Figure 5: Example of Partial-Duplication \nafter top-nodes and bottom-nodes are removed. The check after method en- try is removed because it branched \nto a top-node. A check is added to the edge exiting the basic block labeled \"1\" be- cause the corresponding \nedge in duplicated code connected a top-node to an instrumented node. The edges labeled \"2\" and \"3\" now \nlead back to checking code because they previ- ously connected an instrumented node and a bottom-node. \n\\/ V-'--I Non-instrumented instruction Instrumented instruction r m Instrumentation 0 Branch if sample \ncondition is true  Figure 6: Example of No-Duplication, showing one basic block containing two instrumented \ninstructions. No code is duplicated, but checks axe placed on all instrumenta- tion operations. Property \n1 is violated since there could be more than one check per loop iteration. However, the number of checks \nexecuted could also be less than with Full-Duplication if instrumentation is sparse. instructions themselves \nare duplicated, all instructions with associated instrumentation must check the sample condition before \nexecuting the instrumentation. No-Duplication will not perform instrumentation iden- tical to Full-Duplication. \nWith Full-Duplication, a sample causes execution in that method to remain in du- plicated code until \nthe next backedge is reached, whereas in No-Duplication, a sample triggers only one instrumenta- tion \noperation to be performed. Although they perform the instrumentation in a slightly different manner, \nthey both ex- ecute the instrumented instructions proportionally to their execution frequency, resulting \nin accurate sampling results, as demonstrated empirically in Section 4.4. The only drawback of the No-Duplication \napproach is that it may execute more checks at runtime them the pre- vious variations. The overhead for \nexecuting the additional 172 checks introduced by this technique may be significant if these are a substantial \nnumber of instrumentation opera- tions per loop iteration. However, the number of checks ex- ecuted could \nalso be reduced if instrumentation operations occur less frequently than backedges and method calls, \nmak- ing No-Duplication a good option in these situations. Combining both variations (Partial-Duplication \nand No-Duplication) is also possible, allowing some code to be duplicated, while executing some additional \nchecks at run- time. Exactly which variation (or a combination thereof) should be used depends on the \ntype of instrumentation be- ing performed as well as the time and space constraints that must be satisfied. \nA reasonable approach for a JVM would be to selectively instrument only the hot meth- ods, but to apply \nmany types of instrumentation at once. Full-Duplication is probably the best choice for this sce- nario, \nbecause the checking overhead does not increase as more instrumentation is added. 4 Experimental Results \nThe following section presents art implementation and eval- uation of our framework, demonstrating its \nlow overhead and high accuracy. Both the Full-Duplication and No-Duplication algorithms were implemented \nand applied to two example instrumentations. 4.1 Methodology Our framework was implemented using the \nJalapefio JVM [2, 5] being developed at IBM T.J. Watson Research Center. Currently, Jalapefio contains \ntwo fully operational compilers, a non-optimizing baseline compiler and art opti- mizing compiler [13]. \nJalapefio is written in Java, and begins execution by reading from a boot image file, which contains \nthe core services of Jalapefio precompiled to machine code. Our benchmark suite consists of the SPECjvm98 \n[23] benchmarks with input size 10, the Jalapefio optimizing compiler [13] run on a small subset of itself, \nthe Volano benchmark [38], and the pBOB benchmark [12]. The running times of the benchmarks ranged from \n1.1 to 4.8 seconds. To show that our framework can collect accurate profiles in a short amount of time, \nshort running benchmarks were chosen and all profiles used for accuracy comparisons were collected using \na single run. All overhead timings were col- lected using the median of 20 runs to eliminate as much \nnoise as possible. The benchmarks range in cumulative class file sizes from 10,156 (209_db) to 1,516,932 \n(opt-crop) bytes. All results were gathered on a 333MHz IBM RS/6000 powerPC 604e with 2096MB RAM, running \nAIX 4.3. All overhead and accuracy data reported (both exhaus- tive and sampled) were collected by instrumenting \nall meth- ods in the benchmark, including library methods, however methods in the Jalapefio boot-image \nwere not instrumented. An adaptive JVM would most likely instrument just a few of the hottest methods, \nso instrumenting all methods repre- sents a worst case scenario regarding overhead. All code (both instrumented \nand non-instrumented) was compiled prior to execution at level 02, which is currently Jalapefio's highest \noptimization level [5]. 4.2 Instrumentation examples The framework is evaluated using the following \ntwo exam- ples of instrumentation: Benchmark [I Call-edge (%) Field-access (%) I 201_compress 72.4 204.8'~b2_jess \n133.2 60.9 209_db 8.3 7.7 213_javac 75.7 14.2 222-mpegaudio 129.6 99.8 227_mitt 122.2 46.0 228_j ack \n34.3 108.7 opt-compiler 189 34.9 pBOB 72.3 20.2 Volano 46.6 7.6 Average I[ 88.3 6024 Table i: Time overhead \nof exhaustive instrumentation with- out our framework (where all methods are instrumented), compared \nto the original, non-instrumented code. 1. Call-edge instrumentation All method entries are instrumented \nto examine the call stack. The caller method, the callee method, and the call-site within the caller \nmethod (specified by a bytecode offset) are recorded as a call edge. A counter is maintained for each \ncall edge, and the counter is incremented on each call. 2. Field-access instrumentation A counter is \nmain- tained for each field of all classes. All field acces~es (generated from a get_field or put_field \nbytecode instruction) are instrumented to increment the counter for the field they are accessing. This \ntype of profile is useful for data layout optimizations, such as [16,17,20].  Table 1 characterizes \nthese two types of instrumenta- tion by showing their overhead when applied exhaustively (not using our \nframework) to all methods. The first column lists the benchmarks, while the second and third columns \nshow exhaustive instrumentation overhead for call-edge and field-access instrumentation, respectively. \nThe call-edge in- strumentation averages 88.3% overhead, and the field-access averages 60.4% overhead. \nClearly, these instrumentations as implemented here are too expensive to execute unnoticed at runtime. \nThese are not meant to represent the most efficient im- plementations for collecting call-edge and field-access \npro- files. Favoring simplicity over efficiency was an intentional design choice. One of the goals of \nour sampling framework is to allow new instrumentation techniques to be developed quickly and correctly, \nwithout requiring the implementor to spend time manually reducing overhead. These implemen- tations are \nsimply two examples of the numerous instrumen- tation techniques that could be sampled by our framework. \n 4.3 Framework overhead This section describes the overhead of the framework itself, when no samples \nare taken. The global counter was set sufficiently high so that execution never left the checking code, \nand no instrumentation was inserted in the duplicated code. Full-Duplication algorithm Table 2 presents \nthe over-head of a simple, non-optimized implementation of Full-Duplication. The second column, labeled \n\"To-tal Framework Overhead\" shows the running time of 173 Benchmarks 201_compress 202_j ess 209_db 213_j \navac 222_mpegaudio 227_mtrt 228_jack opt-compiler   pBOB Volano Total Framework ..... Overhead (%) \n8.7 3.3 .... 2.1 2.7 9.9 3.4 8.4 6.2 3.81 1.4 Time Overhead checking overhead breakdown Backedges (%) \nMethod Entry (%) 8.3 0.9 2.9 0.1 i.8 0.2 0.2 1.4 9i0 0.8 2.0 2.4 6.6 1.2 2.1 4.4 2 :5 0.9 0.3 1.0 Space \nRelated Overhead Maximum space Compile Time increase (KB) Increase (%) 106 37 244 37 123 34 442 38 156 \n31 163 31 258 18 976 48 306 37 75 32 Average II 4.9 I 3.5 1.3 II 2851 34 Table 2: Framework overhead \nof Full-Duplication compared to the original, non-instrumented code. No samples are taken (execution \nnever leaves checking code) so all figures reflect the overhead of the framework itself, without performing \nany instrumentation. Full-Duplication (with no instrumentation being sam-pled) relative to the original, \nnon-instrumented code. This overhead includes the direct overhead of executing the counter-based checks \non method entries and backedges, and also any indirect overhead associated with doubling the size of \nall methods. For example, the increase in code size could increase the number of instruction cache misses, \neven if the duplicated code is never executed. The total overhead of the framework ranged from 1.4% to \n9.9% and averaged 4.9% for all benchmarks. The third and fourth columns, labeled \"Backedges\" and \"Method \nEntry\" respectively, break down the direct over- head of executing the counter-based checks. These figures \nwere obtained by inserting the backedge and method entry checks independently, but without actually duplicating \nany code, 2 therefore eliminating any of the indirect overhead as- sociated with code duplication. It \nis difficult to accurately measure such small overheads, so these figures should be treated as an approximate \nbreakdown only. Nevertheless, the sum of the backedge and method entry checking over- head is roughly \nequivalent to the total framework overhead, suggesting the indirect cost is minimal. Although averaging \n4.9% overhead is reasonable, it does not represent a lower bound, as it could be improved by us- ing \na number of techniques. First, backedge checks introduce significant overhead in _201_corapress and _222_mpegaudio \nbecause their execution is dominated by tight loops. Loop unrolling, which is not currently implemented \nin Jalapefio, would significantly reduce this overhead by reducing the number of backedges executed. \n~ Second, these experiments were run using the default, non-aggressive static inlining heuristics. The \nmethod-entry overhead would be reduced if more aggressive inlining were performed before instrumenta- \ntion occurs, which is likely to be the case when used online in an adaptive system. Third, this checking \nimplementa- tion is naive, and does not use any hardware, operating sys- tem, or JVM specific techniques. \nFor example, no effort was made to keep the global counter in a register; each check 2This configuration \ncannot be used to sample instrumentation. It is included solely to provide an approximate breakdown of \nthe direct checking overhead. 3Only loops with a small number of instructions would need to be unrolled, \nbecause as the number of instructions executed per itera- tion increases, the overhead of inserting a \nbackedge check decreases; therefore no substantial increase in code size should be necessary. performs \na memory load, compare, branch, decrement, and store. Section 4.5 presents the overhead of our Jalapefio- \nspecific implementation. The last two columns of Table 2 report other sources of overhead introduced \nby the code duplication. The first of these columns, labeled \"Maximum Space Increase\", shows a rough \napproximate of the space overhead when Full-Duplication is applied to all methods. Since all meth- ods \nare doubled in size, the maximum space overhead is computed by summing the sizes of the final optimized \ncode for all methods. However, a JVM would need less than this space because it would most likely instrument \nonly a few of the hottest methods, not necessarily all at the same time. If space is limited, the number \nof methods that are instru- mented simultaneously can be restricted. When used selec- tively, the space \nrequirements of Full-Duplication could be less of a concern than other space consuming transfor- mations, \nsuch as inlining and specialization, which can po- tentially cause exponential code growth. The last \ncolumn, labeled \"Compile Time Increase\", shows the total increase in compile time for each benchmark \nwhen using Full-Duplication. Our implementation per- forms the doubling of all code relatively late in \nthe compila- tion process, 4 and therefore the majority of the compilation process is unaffected. The \naverage increase in compile time was 34%. This compile time increase is attributed mostly to instruction \nselection, instruction scheduling, and register allocation, all of which occur after code duplication. \nNo-Duplication algorithm No-Duplication inserts checks on all instrumentation operations, therefore the \noverhead introduced by the checks depends heavily on the type of in- strumentation being performed. Table \n3 shows the checking overhead of No-Duplication when applied to call-edge and field-access instrumentation. \nNo samples are taken, so the only overhead is the the cost of executing the cheeks. Recall that No-Duplication \ndoes not guarantee to maintain Prop- erty 1, and thus the overhead may be higher or lower than that of \nFull-Duplication. For field-access instrumentation, the checking overhead averages 51.1%, which is only \nslightly less than the cost of the exhaustive instrumentation. For each field access, 4performed in the \nlast phase of the Jalapefio's low-level IR (LIR) 174 ~3enchmarks Call-edge (%) Field-access (%) I 201-comPress \n0.9 151.5 202_jess 0.1 36.6 209_db 0.2 6.9 213_javac 1.4 21.3 222_mpegaudio 0.8 100.7 22;f_mtrt 2.4 49.1 \n228_jack 1.2 72.1 opt-compiler 4.4 41.1 PBO B 2.3 21.3 Volano 1.0 10.4 Average 1.3 51.1 Table 3: Framework \noverhead of No-Duplication compared to the original, non-instrumented code. No samples were taken, so \noverhead is for executing the checks only. the instrumentation performs two loads, an increment, and \na store, which is similar to the cost of a counter-based check, making the insertion of checks completely \nineffective. No-Duplication is beneficial only when the cost of execut- ing a check is cheaper than executing \nthe instrumentation itself. For the call edge instrumentation, however, the average checking overhead \nis 1.3%. Checks are placed on all call edges only (i.e., method entries), explaining why column 2 of \nTable 3 is identical to column 4 of Table 2. Clearly No-Duplication is beneficial for our implementation \nof call- edge instrumentation, which spends time examining the stack to determine the call-edge information. \nThere are also other types of profile information available at method entry, such as parameter values \nthat can be used to guide special- ization. An effective approach would be to use a counter- based check \nto guard a call to a method pro:fileEntryO that would collect all of the desired profile information \nabout the method. 4.4 Sampled instrumentation overhead and accuracy In this section overhead and accuracy \nof the actual instru- mentation (as opposed to just the checking code) are eval- uated when sampled by \nour framework. The overhead in- troduced by sampled instrumentation depends on the in- strumentation \nbeing performed, so call-edge and field-access instrumentation are used as examples; both were applied \ntogether during the same run, and sampling was performed using either Full-Duplication or No-Duplication. \nTable 4 shows the overhead and accuracy of the sampled instrumen- tation, averaged over all benchmarks, \nfor several different sample rates. Because samples are triggered by counter- based sampling, sample \nrates are reported as sample inter- vals, as shown in the first column of the table. The sample interval \nrepresents the number of checks in the checking code that are executed before each sample is triggered. \nA sample interval of 1000 means that roughly 1/1000 th of the execu- tion will occur in the duplicated \ncode. The column labeled \"Num Samples\" shows the average number of samples that were taken during a single \nrun of each benchmark. 5 The overhead and accuracy results are described in detail below: ~R.ecall that \nprofiles used for accuracy comparison were collected using a single run of the benchmark, whereas timings \nfor overhead comparison were collected using the median of 20 runs. Overhead The overhead of sampling \nboth call-edge and field-access instrumentation (at the same time) is reported in Table 4, averaged over \nall benchmarks and normalized in two different ways. First, the columns labeled \"Sampled Instrum.\" report \nthe overhead without including the frame- work overhead, thus representing the additional overhead (above \nthe framework overhead) that is introduced by tak- ing samples. This includes both the overhead of executing \ninstrumentation, and in the case of Full-Duplication, the cost of jumping to duplicated code, which will \nmost likely incur one or more instruction cache misses. As would be ex- pected, for both Full-Duplication \nand No-Duplication, a sample interval of I000 or greater results in low overhead (1% or less), since \nover 99.9% the time is spent in the check- ing code. The columns labeled \"Total\" report the total overhead \nof performing sampled instrumentation, compared to the orig- inal, non-instrumented code, and includes \nall possible over- head introduced by the sampling framework. As expected, a small sample interval (such \nas 1 or 10) cause the overhead to be dominated by the overhead of taking samplesf whereas a large sample \ninterval (such 10,000 or 100,000) cause the overhead to be dominated by the overhead of the framework \nitself. Since the framework overhead of No-Duplication is fairly high (averaging ,-,50%, mostly from \nthe field-access profiling, as shown in Table 2), the total overhead is also high even when few samples \nare being taken. The average framework overhead of Full-Duplication is around 5% (see Table 2) so at \nsample rate 1000, the total sampling overhead was 6.3%. Accuracy To assess accuracy, profiles collected \nat different sample rates are compared against a perfect profile (col- lected by setting the sample interval \nto 1, causing all exe- cution to occur in duplicated code) and an accuracy metric is computed. An overlap \npercentage metric is used, similar to that used in [26]. Informally, the overlap of two profiles represents \nthe percent of profiled information weighted by execution frequency that exists in both profiles. The \nfol- lowing example defines more specifically how the metric is computed. Figure 7 helps describe the \noverlap metric by visually representing the most frequently sampled edges of the call- edge profile for \njavac. Each bar along the x-axis represems a call edge, and the y-axis represents the sample-percentage, \ndefined as the percentage of samples attributed to that edge (sample-percentage(edge) = hum.samples(edge) \n/ to-taLsamples 100). The height of each bar shows the sample- percentage of an edge according to the \nperfect profile, while each circle (either within or above each bar) shows the sample-percentage of that \nedge according to a sampled pro- file. The overlap for an edge is simply the minimum of the two sample-percentages. \nThe overlap percentage of the benchmark is the sum of the overlaps for all edges. A sam- pled profile \nthat is identical to the perfect profile yields an overlap percentage of 100%. Any variation in the sampled \nprofile from the perfect profile yields an overlap percentage of less than 100%. T The javac profile \nshown in Figure 7 yields an overlap of 93.8%, showing that an overlap of this value corresponds to very \naccurate profile. Overlap for the 6Totai overhead with sample interval 1 is higher than exhaustive instrumentation \n(Table 1) due to the overhead of jumping back and forth between as original and duplicated code. 7The \ny-axis is reported as a percentage of all samples, so if the sampled profile overestimates the sample-percentage \nof some edge, it must also underestimate the sample-percentage of another edge. 175 Full-Duplication \nl] No-Duplication Overhead (%) Accura~cy (%) Overhead (%) Accuracy (%) Sample Interval 1 10 100 1',000 \n10,000 100,000 .... Num Samples 1.1xlO 7 1.1xl06 1.1xl05 1.1x104 1,137 109 Sampled I Call-[ ~ield- Instrum. \n[. Total Edge I J ~ccess 167.2 ..... 182.2! i00 100 26.4 29.3 99 100 4.2 10.3 98 99 0.8 6.3 94 97 0.1 \n5.1 82 94 0.1 5.0 71 83 Num Samples 617x10 \"r 6.7x106 6.7x10 ~ 6,7X104 6736 662 Sampled Instrum, 118.2 \n22.8 3.6 1,0 0.2 0.2 Call- Field- Total Edge Access 269.1 fill :tOO 100 79.,5 ii 98 100 61.3 97 99 57.2 \n93 98 '55.7 ....... 81 96 55.2 70 87  Table 4: Time overhead and accuracy of the sampled instrumentation \naveraged over all benchmarks, for a variety of sample intervals. Both call-edge and field-access instrumentation \nwere applied together during the same run, and sampled using either Full-Duplication or No-Duplication. \nColumns labeled \"Sampled Instrum.\" do not include the framework overhead, and thus represent the additional \noverhead introduced by taking samples. The columns labeled \"Total\" include the framework overhead, and \nthus represent the total overhead above the original, non-instrumented code.   n Perfect profile Sampled \nprofile @ gl. &#38; gl. r~ 0 10 20 30 40 50 Call edges Figure 7: A graphical representation of the javac \ncall-edge profile, illustrating an accuracy of 93.8% using the overlap percentage metric. field-access \nprofile was computed in the same way, but using sample-percentages for field-accesses rather thart call-edges. \nTable 4 reports the accuracy of the sampled profiles for both Full-Duplication and No-Duplication. As \nwould be expected, increasing the sample interval reduces accuracy. At sample interval 1,000 the accuracy \nremained quite high, ranging from 93-98% for the both variations and both in- strumentations, while the \noverhead remained low (excluding framework overhead, columns labeled \"Sampled Instrum'). Even at sample \ninterval 10,000, the accuracy is reasonably high, even though only 1/10, 000 th of the execution is spent \nin the duplicated code. The accuracy finally degrades at sample interval 100,000 where there are simply \nnot enough samples collected, given the short running time of the bench- marks. However, the sample intervals \nin this table are in- creasing exponentially, so there is actually a large range of sample intervals \n(from 100 to 10,000) that offer high accu- racy with low overhead. These results confirm the low overhead \nand high accu-racy of our technique, suggesting its effectiveness for driving online optimizations. The \nhigh accuracy also suggests that our technique could be useful for collecting offline profiles as well. \nOne possible concern is that is possible for program behavior to correlate with our deterministic sampling \nmech- anism, resulting in an inaccurate profile. For example, if a program performs some uncommon behavior \nevery 1000th loop iteration, any sample interval that is a multiple of 1000 could result in the uncommon \nbehavior being observed on every sample. Our experimental results suggest that this did not occur for \nbenchmarks used in this study, however it could be a concern if accuracy is critical. Adding a small \nrandom factor to the sample interval (as done in [4]) could be used to reduce the probability of this \nworst case behav- ior, and could possibly even increase the accuracy in the expected case as well. 4.5 \nJalapefio-specific optimization This section presents one example of a JVM-specific op- timization that \ncan be applied to lower the overhead the Full-Duplication framework. This Jalapefio-Specific implementation \nreduces checking overhead by taking advan- tage of the fact that Jalapefio implements thread scheduling \nusing yieldpoints (and therefore also applies to other sys- tems that use yieldpoints, such as [18]). \nA yieldpoint is a sequence of instructions that checks whether it is time for the current thread to stop \nexecuting and give control back to the thread scheduler. Jalapefio currently places yieldpoints on all \nmethod entries and backedges to guarantee that there is a finite amount of time between yieldpoints. \nOnce the code is duplicated using Full-Duplication, the yieldpoints can be moved to the duplicated code \nand removed from the check- ing code. As long as the sample-interval is Mways finite, the distance between \nyieldpoints is guaranteed to remain finite. This optimization does not affect the sampling accuracy, \nso the accuracy figures reported for Full-Duplication still ap- ply. The overhead of Jalapefio-Specific \nis reported in the two tables shown in Figure 8. Table (A) reports the frame- work overhead (full code \nduplication) while no samples are being taken. Because the sequence of instructions to imple- Benchmark \nI] Framework Overhead (%) 201_compress 1.4  202_jess -0.5 Sample 209_db 1.6 Interval Total Sampling \nOverhead(%) 213_javac 2.2 1 179.9 222_mpegaudio -2.1 10 27.6 227_mtrt 1.9 100 8.1 228_jack 0.8 1,000 \n3.0 opt-compiler 4.8 10,000 1.5 pBOB 1.4 100,0000 1.5 Volano 0.5 Table (B) Saxapled Instrumentation \nAverage II 1.4 Table (A) Framework Only Figure 8: Overhead of the Jalapefio-Specific framework, with \nyieldpoint optimization performed. Table (A) presents the overhead of the framework only, with no sampling. \nTable (B) presents the total overhead of performing sampling (compared to non-instrumented code) averaged \nover all benchmarks for several sample rates. merit a counter-based check is similar, but slightly different \nthan that of a yieldpoint, removing yieldpoints and inserting counter-based checks introduces a/most \nno overhead, and in some cases (_202_jess and _222_mpegaudio) performance is actua/ly improved. The average \noverhead was 1.4%, as op- posed to Full-Duplication which averaged 4.9% overhead. We have not yet determined \nwhy the yieldpoint optimization does not help much for opt-compiler and _213_javac. Table (B) of Figure \n8 presents the total overhead of sam- pling both instrumentations using the Jalapefio-Specific framework, \nrelative to the non-instrumented code. This total overhead is similar to the toti overhead of Full-Duplication \n(column 4 of Table 4), but converges on ,.,1.5% overhead rather than --,5%, due to the lower frame- work \noverhead. This Ja/apefio-specific technique allows us to sample at intervals 1,000 and 10,000 with a \ntotal overhead (including sampled instrumentation) of 3.0% and 1.5%, re- spectively. This result is encouraging \nas it will allow the Jalapefio adaptive system [5] to perform online instrumen- tation while introducing \noverhead so small that in many cases it is hardly visible above the noise from one run to the next. \n4.6 Trigger Mechanisms As discussed in Section 2.2, it is possible to use triggers different than a counter-based \ntrigger. To show the advan- tages of the counter-based trigger, its accuracy is compared against a time-based \ntrigger. Ja/apefio has a threadswiteh bit that is set every 10ms by a hardware interrupt; this bit is \nread by the yieldpoint instructions to determine when the executing code should yield to the thread scheduler. \nThis threadswitch mechanism was modified to also set a sample-bit that is monitored by the checking code \nto allow samples to be triggered based on a timer-set bit. To make a fair comparison, a sample interval \nof 30,000 was used for the counter-based sampling because it resulted in approximately the same number \nof samples as the time-based trigger for these benchmarks. Table 5 compares the accuracy of both techniques \n(com- pared against a perfect profile using tm overlap percentage) using Full-Duplication with field-access \ninstrumentation. Clearly our framework is more accurate when driven by a counter-based trigger, which \naveraged 84% accuracy, as op- posed to the time-based trigger, which averaged 63%. This lounter-based \n(%)- Benchmark Time-based (%) ] Counter-b~ 201_compress 88 98 202_jess 91 95 209_db 66 95 213_javac \n59 73 222_mpegaudio 69 95 227_mtrt 51 67 228_jack 45 94 i opt-compiler 58 65 i pBOB Volano 75 27 87 71 \nAverage 63 84 3 Table 5: Comparing the accuracy (overlap percentage) of field-access instrumentation \nwhen samples are driven by a time-based and counter-based trigger. difference in accuracy is most likely \ndue to the inaccurate at-tribution of samples by the time-based trigger, as discussed in Section 2.1. \nThe inaccurate attribution of samples is spe- cific to our framework, so these results do not imply that \ntime-based sampling is ineffective in general -- only that it is less effective than counter-based sampling \nwhen used in our framework. However, there is another advantage of the counter-based trigger that is \nindependent of our framework; the counter- based trigger allows a flexible sample rate that is not re-stricted \nby any hardware or operating system limitations. As previously shown in Table 4, a smaller sample interval \nof 1000 or 10,000 achieves a much higher accuracy (94-99% for field-access profiling), while the overhead \nof sampling re- mains near zero; a faster time:based trigger is not available in our system. 5 Related \nWork The contribution of our work is a simple and effective tech- nique that uses instrumentation sampling \nto allow gen-eral instrumentation to be p~rrformed accurately with low overhead. The most novel aspect \nof our work is the Full-Duplication algorithm, which allows multiple instru- 177 mentation operations \nto be guarded by a single check, thus amortizing the cost of the check over multiple instrumen- tation \noperations. Previous work has touched on many of the techniques used by our framework, such as sampling \nand event counting; however, we do not know of any system that combines these techniques in a way that \nallows gen- eral instrumentation to be performed accurately with low overhead. Traub et al. [36] describes \nusing ephemeral instrumen- tation to collect intraprocedural edge profiles. Their tech- nique allows \nefficient edge profiling by dynamically rewriting conditional branches in the executing code, causing \nexecu- tion to jump to instrumentation code that increments the frequency of the executed edge. When \nused to guide su- perblock scheduling, the edge profiles collected by this tech- nique produced speedups \nsimilar to those from a perfect profile, although a few benchmarks showed substantial dif- ferences. \nUnlike our framework, it is not clear how this technique can be used to perform general instrumentation. \nOur framework is also simpler to implement because it does not require the ability to perform binary \nrewriting. Dynamo [9] employs a technique named NET (Next Ex- ecuting Tail) [25] to reduce the profiling \noverhead of idea- tifying hot paths. Using NET, when counters on start-@ trace points exceed a threshold, \nthe next executing trace is recorded and assumed to be hot. Although NET potentially introduces more \nnoise [25] than traditional path profiling techniques, it identifies hot paths quickly, making it an \nef- fective choice for Dynamo, where all code is interpreted until identified as part of a hot path, \nand then optimized. There are several fundamental differences between our framework and NET. First, our \nframework assumes the exe- cution environment of a JVM, where code can be optimized either with or without \nprofiling information. Because rea- sonable performance can be expected prior to performing instrumentation, \nthere is less need to base optimization de- cisions on a single sample. Instead, multiple samples can \nbe collected over a longer time period to increase accuracy. Second, NET is specific to identifying hot \npaths, while our framework allows a variety of instrumentation to be per- formed. Finally, NET uses multiple \ncounters to exhaus-tively record the execution frequency of traces, whereas our counter-based sampling \nuses one counter to distribute sam- ples across all sample points. Self-93 [30] uses method invocation \ncounters to deter- mine when the call stack should be sampled. Similar to NET, Self-93 uses multiple \ncounters (one per method) and made optimization decisions based on a single sample. Previous work [16, \n26, 37] has used the idea of wrapping each instrumentation operation inside a conditional branch (as \nis done in our No-Duplication). Calder et al. [16] and Feller [26] describe convergent value profiling, \nwhere profil- ing is turned off once the profiled values appear to have converged; their technique is \ncompared against a random sampling, where (exhaustive) sampling is turned off for pe- riods of random \nlength. Viswanthan and Liang [37] de- scribe a Java virtual machine profiler interface, a general purpose \nmechanism for obtaining information from a JVM, which also uses boolean flag to conditionalize instrumenta- \ntion operations. However, unlike our work, none of these approaches provide a mechanism to turn the flag \non and off quickly to perform fine-grained sampling. Instead, the flag is used as a switch to turn exhaustive \nprofiling off when it is not being used; while the flag is on, the full cost of instrumentation is incurred. \nThere has been work on reducing the cost of specific types of instrumentation [10,11,15,26]; however, \nthese tech- niques are specific to one kind of instrumentation, and it is unclear whether they reduce \noverhead enough to allow the instrumentation to run unnoticed in an adaptive system. Previous work [8, \n39] has used sampling to reduce the cost of building the calling context tree [3]. Burrows et al., [14] \nuses sampling to reduce the cost of profiling program values. These techniques are specialized examples \nof the general technique proposed by our framework, that is, using sampling to reduce the overhead of \na previously exhaustive instrumentation. Recent work [34] proposes new hardware support for reducing \nprofiling overhead. In their approach, profiling events are collected and compressed in hardware before \nbe- ing fed to a software profiler. Their simulation shows ac- curacy and overhead results similar to \nthat of our work for a variety of profiling types, although the hardware support for their technique \ndoes not exist in today's processors. An- other fundamental difference between our work is the level \nof abstraction at which profiling is performed; their technique processes machine level information, \nwhereas our approach can incorporate existing software profiling implementations.  6 Conclusions This \npaper presents a general framework for performing in- strumentation sampling, allowing previously expensive \nin- strumentation to be performed with low overhead. The framework performs code duplication and uses \ncompiler-inserted counter-based sampling to switch between instru- mented and non-instrumented code in \na controlled, fine-grained manner. The sampling technique does not rely on any hardware or operating \nsystem support, yet provides a flexible, high-frequency sample rate, ensuring accurate and deterministic \nsampling results. The reduction in overhead provided by our sampling framework allows instrumentation \nto be performed for a longer period of time, while causing only minimal perfor- mance degradation, allowing \na system to utilize previously expensive instrumentation techniques at runtime even with- out the ability \nto perform dynamic instrumentation or on- stack replacement. Our framework also makes it possible for \nmany types of instrumentation to be collected at once, with- out the normal concern for overhead. Because \noverhead is controlled completely by the framework, implementors of in- strumentation techniques are \nno longer required to focus on minimizing overhead, but instead can concentrate on other issues surrounding \nonline feedback-directed optimization. An implementation and evaluation of the framework is presented \nusing the Jalapefio JVM with two different exam- pies of instrumentation, demonstrating that high accuracy \ncan be achieved (93-98% overlap with a perfect profile) with low overhead (averaging ~6% total overhead \nwith a naive implementation). A Jalapeiio-Specific implementation is also presented as an example of \nhow hardware- or compiler- specific optimizations can be used to further reduce over- head, resulting \nin an average total overhead of ,-~3%. Acknowledgments We are grateful to Stephen Fink, David Grove, \nMichael Hind, Nasko Rountev, and Peter Sweeney for their helpful discussions and comments on an earlier \ndraft of this work, as well as the anonymous reviewers for their helpful feedback. We also thank Sharad \nSinghai for the use of his field-access profiling implementation, as well as all of the Jalapefio members \nwho helped facilitate this research. Finally, we thank Michael Hind for his support of this work. 178 \nReferences [if A.-R. Adl-Tabatabai, M. Cierniak, C.-Y. Lueh, V. M. Parikh, and J. M. Stichnoth. Fast, \neffective code generation in a Just-in- Time Java compiler. In Proceedings of the ACM SIGPLAN'98 Conference \non Programming Language Design and Implemen- tation (PLDI), pages 280-290, Montreal, Canada, 17-19 June \n1998. SIGPLAN Notices 33(5), May 1998. [2] B. Alpern, C. R. Attanasio, J. J. Barton, M. G. Burke, P. \nCheng, J.-D. Choi, A. Cocchi, S. J. Fink, D. Grove, M. Hind, S. F. Hummel, D. Lieber, V. Litvinov, M. \nF. Mergen, T. Ngo, J. R. Russell, V. Sarkar, M. J. Serrano, J. C. Shepherd, S. E. Smith, V. C. Sreedhar, \nH. Srinivasan, and J. Whaley. The Jalapefio virtual machine. IBM Systems Journal, 39(1), 2000.  [3] \nG. Ammons, T. Ball, and J. Larus. Exploiting hardware per- formance counters with flow and context sensitive \nprofiling. In SIGPLAN '97 Conf. on Programming Language Design and Implementation, 1997. [4] J. M. Andersen, \nL. M. Berc, J. Dean, S. Ghemawat, M. R. Hen- zinger, S.-T. A. Leung, R. L. Sites, M. T. Vandevoorde, \nC. A. Waldspurger, and W. E. Weihl. Continuous profiling: Where have all the cycles gone? Technical Note \n1997-016a, Digital Systems Research Center, www.research.digital.com/SRC, Sept. 1997. [5] M. Arnold, \nS. Fink, D. Grove, M. Hind, and P. Sweeney. Adap- tive optimization in the Jalapefio JVM. In ACM Conference \non Object-Oriented Programming Systems, Languages, and Appli- cations, Oct. 2000. [6] M. Arnold, S. Fink, \nV. Sarkar, and P. Sweeney. A comparative study of static and dynamic heuristics for inlining. In ACM \nSIGPLAN Workshop on Dynamic and Adaptive Compilation and Optimization, 2000. [7] M. Arnold, M. Hind, \nand B. G. Ryder. An empirical study of selective optimization. In 13th International Workshop on Lan- \nguages and Compilers for Parallel Computing, Aug. 2000. [8] M. Arnold and P. F. Sweeney. Approximating \nthe calling context tree via sampling. Technical Report RC 21789, IBM T.J. Watson Research Center, July \n2000. [9] V. Bala, E. Duesterwald, and S. BanerjLia. Dynamo: A transpar- ent dynamic optimization system. \nIn SIGPLAN PO00 Confer- ence on. Programming Language Design and Implementation, June 2000. [10] T. \nBall and J. R. Larus. Optimally profiling and tracing pro- grams. ACM Transactions on Programming Languages \nand Systems, 16(4):1319-1360, July 1994. [11] T. Ball and J. R. Larus. Efficient path profiling. In Proceedings \nof the 29th Annual International Symposium on Microarchi-teeture, pages 46-57. ACM Press, 1996. [12] \nS. Baylor, M. Devarakonda, S. Fink, E. Gluzberg, M. Kalantar, P. Muttineni, E. Barsness, R. Arora, It. \nDimpsey, and S. Munroe. Java server benchmarks. IBM Systems Journal, 39(1), 2000.  [13] M. G. Burke, \nJ.-D. Choi, S. Fink, D. Grove, M. Hind, V. Sarkar, M. J. Serrano, V. C. Sreedhar, H. Srinivasan, and \nJ. Whaley. The Jalapefio dynamic optimizing compiler for Java. In ACM 199g Java Orande Conference, pages \n129-141, June 1999.  [14] M. Burrows, U. Erlingson, S.-T. A. Leung, M. T. Vandevoorde, C. A. Waldspurger, \nK. Walker, and W. E. Weihl. Efficient and flexible value sampling. In Ninth International Conference \non Architectural Support for Programming Languages and Oper- ating Systems, Nov. 2000.  [15] B. Calder, \nP. Feller, and A. Eustace. Value profiling. In the 30th International Symposium on Microarchitecture, \npages 259-269, Dec. 1997. [16] B. Calder, P. Feller, and A. Eustace. Value profiling and opti- mization. \nJournal of Instruction Level Parallelism, Vol 1, Mar. 1999. [17] B. Calder, C. Krintz, S. John, and \nT. Austin. Cache-conscious data placement. In Proceedings of the Eighth International Conference on Architectural \nSupport for Programming Lan- guages and Operating Systems, pages 139-149, San Jose, Cal- ifornia, Oct. \n1998. ACM SIGARCH, SIGOPS, SIGPLAN, and the IEEE Computer Society. [18] C. Chambers and D. Ungar. Making \npure object-oriented lan- guages practical. In ACM Conference on Object-Oriented Pro- gramming Systems, \nLanguages, and Applications, pages 1-15, Nov. 1991. SIGPLAN Notices 26(11). [19] P. P. Chang, S. A. \nMahlke, W. Y. Chen, and W. mei W. Hwu. Profile-guided automatic inline expansion for C programs. Soft-ware \n-Practice and Experience, 22(5):349-369, May 1992. [20] T. M. Chilimbi, B. Davidson, and J. R. Larus. \nCache-conscious structure definition. In Proceedings of SIGPLAN'99 Conference on Programming Languages \nDesign and Implementation, ACM SIGPLAN Notices, pages 13-24, Atlanta, May 1999. ACM Press. [21] T. M. \nChilimbi and J. R. Larus. Using generational garbage col- lection to implement cache-conscious data placement. \nIn Pro-ceedings of the International Symposium on Memory Manage- ment (ISMM-98), volume 34, 3 of ACM \nSIGPLAN Notices, pages 37-48, New York, Oct. 17-19 1999. ACM Press. [22] M. Cierniak, G.-Y. Lueh, and \nJ. M. Stichnoth. Practicing JUDO: Java Under Dynamic Optimizations. In SIGPLAN 2000 Confer- ence on Pro9rammin9 \nLanguage Desi9n and Implementation, June 2000. [23] T. S. P. E. Corporation. SPEC JVM98 Benchmarks. \nhttp://www.spec.org/osg/jvm98, 1998. [24] L. P. Deutsch and A. M. Schiffman. Efficient implementation \nof the Smalltalk-80 system. In 11th Annual ACM Symposium on the Principles of Programming Languages, \npages 297-302, Jan. 1984. [25] E. Duesterwald and V. Bala. Software profiling for hot path pre- diction: \nLess is more. In Ninth International Conference on Architectural Support for Programming Languages and \nOper- ating Systems, Nov. 2000. [26] P. T. Feller. Value profiling for instructions and memory loca- \ntions. Masters Thesis CS98-581, University of California, San Diego, Apr. 1998. [27] D. Grove, J. Dean, \nC. Garrett, and C. Chambers. Profile-guided receiver class prediction. In ACM Conference on Object- Oriented \nProgramming Systems, Languages, and Applications, pages 108-123, Oct. 1995. [28] J. K. Hollingsworth, \nB. P. Miller, M. J. R. Goncalves, O. Naim, Z. Xu, and L. Zheng. MDL: A language and compiler for dy- \nnamic program instrumentation. In Proceedings of the 1997 International Conference on Parallel Architectures \nand Com- pilation Techniques (PACT '97), pages 201-212, San Francisco, California, Nov. 10-14, 1997. \nIEEE Computer Society Press.  [29] U. HSlzle, C. Chambers, and D. Ungar. Debugging optimized code with \ndynamic deoptimization. In Proceedings of the ACM SIGPLAN'92 Conference on Programming Language Design \nand Implementation (PLDI), pages 32-43, San Francisco, Cali- fornia, 17-19 June 1992. SIGPLAN Notices \n27(7), July 1992. [30] U. HSlzle and D. Ungar. Reconciling responsiveness with per- formance in pure \nobject-oriented languages. ACM Transactions on Programming Languages and Systems, 18(4):355-400, July \n1996. [31] The Java Hotspot performance engine architecture. White paper available at http://java.sun.com/products/hotspot/whitepaper-html, \nApr. 1999. [32] T. P. Kistler. Continuous Program Optimization. PhD thesis, University of California, \nIrvine, 1999. [33] A. Krall. Efficient JavaVM Just-in-Time compilation, in J.-L. Gaudiot, editor, International \nConference on Parallel Architec- tures and Compilation Techniques, pages 205-212, Oct. 1998. [34] S. \nSastry, R. Bodik, and J. E. Smith. Rapid profiling via strat- ified sampling. In To appear in 28th International \nSymposium on Computer Architecture, 2001. [35] T. Suganama, T. Ogasawara, M. Takeuchi, T. Yasue, M. \nKawahito, K. Ishizaki, H. Komatsu, and T. Nakatani. Overview of the IBM Java Just-in-Time compiler. IBM \nSystems Journal, 39(1), 2000.  [36] O. Traub, S. Schechter, and M. D. Smith. Ephemeral instru- mentation \nfor lightweight program profiling. Technical report, Harvard University, 1999. [37] D. Viswanathan and \nS. Liang. Java Virtual Machine Profiler Interface. IBM Systems Journal, 39(1):82-95, 2000. [38] VolanoMark \n2.1. http://www, volano, eom/benehmarks,html. [39] J. Whaley. A portable sampling-based profiler for \nJava virtual machines. In ACM PO00 Java Grande Conference, June 2000. [40] B.-S. Yang, S.-M. Moon, S. \nPark, J. Lee, S. Lee, J. Park, Y. C. Chung, S. Kim, K. Ebcioglu, and E. Airman. LaTTe: A Java VM Just-in-Time \ncompiler with fast and efficient register allo- cation. In International Conference on Parallel Architectures \nand Compilation Techniques, Oct. 1999. 179    \n\t\t\t", "proc_id": "378795", "abstract": "<p>Instrumenting code to collect profiling information can cause substantial execution overhead. This overhead makes instrumentation difficult to perform at runtime, often preventing many known <i>offline</i> feedback-directed optimizations from being used in online systems. This paper presents a general framework for performing <i>instrumentation sampling</i> to reduce the overhead of previously expensive instrumentation. The framework is simple and effective, using code-duplication and <i>counter-based sampling</i> to allow switching between instrumented and non-instrumented code.</p><p>Our framework does not rely on any hardware or operating system support, yet provides a high frequency sample rate that is tunable, allowing the tradeoff between overhead and accuracy to be adjusted easily at runtime. Experimental results are presented to validate that our technique can collect accurate profiles (93-98% overlap with a perfect profile) with low overhead (averaging 6% total overhead with a naive implementation). A Jalape~ no-specific optimization is also presented that reduces overhead further, resulting in an average total overhead of 3%.</p>", "authors": [{"name": "Matthew Arnold", "author_profile_id": "81100021720", "affiliation": "Rutgers University, Piscataway, NJ and IBM T.J. Watson Research Center, Hawthorne, NY", "person_id": "PP14020675", "email_address": "", "orcid_id": ""}, {"name": "Barbara G. Ryder", "author_profile_id": "81100632248", "affiliation": "Rutgers University, Piscataway, NJ and IBM T.J. Watson Research Center, Hawthorne, NY", "person_id": "PP14217204", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/378795.378832", "year": "2001", "article_id": "378832", "conference": "PLDI", "title": "A framework for reducing the cost of instrumented code", "url": "http://dl.acm.org/citation.cfm?id=378832"}