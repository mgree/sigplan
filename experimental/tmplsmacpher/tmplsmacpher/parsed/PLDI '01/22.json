{"article_publication_date": "05-01-2001", "fulltext": "\n Optimal Spilling for CISC Machines with Few Registers Andrew W. Appel Lal George Princeton University \nLucent Technologies Bell Laboratories appel@cs.princeton.edu george @ research, bell-labs.corn ABSTRACT \nMany graph-coloring register-allocation algorithms don't work well for machines with few registers. Heuristics \nfor live-range split- ting are complex or suboptimal; heuristics for register assignment rarely factor \nthe presence of fancy addressing modes; these prob- lems are more severe the fewer registers there are \nto work with. We show how to optimally split live ranges and optimally use address- ing modes, where \nthe optimality condition measures dynamically weighted loads and stores but not register-register moves. \nOur al- gorithm uses integer linear programming but is much more efficient than previous ILP-based approaches \nto register allocation. We then show a variant of Park and Moon's optimistic coalescing algorithm that \ndoes a very good (though not provably optimal) job of remov- ing the register-register moves. The result \nis Pentium code that is 9.5% faster than code generated by SSA-based splitting with iter- ated register \ncoalescing. 1. INTRODUCTION. Register allocation by graph coloring has been a big success for machines \nwith 30 or more registers. The instruction selector gener- ates code using an unlimited supply of temporaries; \nliveness analy- sis constructs an interference graph with an edge between any two temporaries that are \nlive at the same time (and thus cannot be al- located to the same register); a graph coloring algorithm \nfinds a K-coloring of the interference graph (where K is the number of registers on the machine). If \nthe graph is not K-colorable, then some nodes are spilled: the temporaries are implemented in mem- ory \ninstead of registers, with a cost for loading them and storing them when necessary. Graph coloring is \nNP-complete, but simple algorithms can often do well. An important improvement to this algorithm was \nthe idea that the live range of a temporary should be split into smaller pieces, with move instructions \nconnecting the pieces. This relaxes the inter- ference constraints a bit, making the graph more likely \nto be K- colorable. The graph-coloring register allocator should coalesce two temporaries that are related \nby a move instruction if this can be done without increasing the number of spills. Permission to make \ndigital or hard copies of all or part of this work for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for profit or commercial advan- tage and that copies \nbear this notice and the full citation on the first page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior specific permission and/or a fee. PLDI 2001 6/01 \nSnowbird, Utah, USA &#38;#169; 2001 ACM ISBN 1-58113-414.2/01/06..,$5.00 Unfortunately, this approach \nhas not worked well for machines like the Pentium, which have K = 6 allocable registers (there are 8 \nregis- ters but usually two are dedicated to specific purposes). What hap- pens is that there will typically \nbe many nodes with degree much greater than K, and there is an enormous amount of spilling. Of course, \nwith few registers there will inevitably be spilling, as the live variables cannot all be kept in registers; \nbut if a variable is spilled because it has a long live range, then it stays spilled even (for example) \nin some loop where it is frequently used. On our test suite of 600 basic-block clusters comprising 163,355 \ninstruc- tions, iterated register coalescing produces 84 spill instructions for a 32-register machine, \nbut 22,123 spill instructions for an 8-register machine. This is about 14% of all instructions, which \nis worth the trouble to improve. In the last few years some researchers have taken a completely dif- \nferent approach to register allocation: formulate the problem as an integer linear program (ILP) and \nsolve it exactly with a general- purpose ILP solver. ILP is NP-complete, but approaches that com- bine \nthe simplex algorithm with branch-and-bound can be success- ful on some problems. Unfortunately, the \nwork to date in optimal register allocation via ILP has not quite been practical: Goodwin's optimal register \nallocator can take hundreds of seconds to solve for a large procedure [11, 12]. Goodwin has formulated \n\"near-optimal register allocation (NORA)\" as an ILP; our solution can be viewed as a different approach \nto near-optimal register allocation. A two-phase approach. Our new approach decomposes the regis- ter \nallocation problem into two parts: spilling, then register assign- ment. Instead of asking, \"at program \npoint p, should variable v be in register r?\" we first ask, \"at program point p, should variable v be \nin a register or in memory?\" Clearly, this is a simpler ques- tion, and in fact we can formulate an integer \nlinear program (ILP) that solves it optimally and efficiently (tens of milliseconds). This phase of register \nallocation finds the optimal set of splits and spills. Not only does our algorithm compute where to insert \nloads and stores to implement spills, but it also optimally selects addressing modes for CISC instructions \nthat can get operands directly from memory. For example, the add instruction on the Pentium takes two \noperands s and d, and computes d +- d + s. The operands can be in registers or in memory, but they cannot \nboth be in memory. On a modern implementation of the instruction set, the instruction mix] ~-- mix] +s \nis no faster than the sequence of instructions r +- m[x]; r ~ r+x; mix] +- r. However, the latter sequence \nrequires an explicit temporary r, and if there are many other live values at this point, some other value \nwill have to be spilled; the former sequence wouldn't require the spilling of some other value. Therefore, \nit is 243 important to make use of the CISC instructions. The second phase is to allocate the unspilled \nvariables to registers in a way that leaves as few as possible register-register moves in the program. \nThis is difficult to do optimally, but we will show an efficient algorithm can get very good results. \nIn judging our decomposition into two phases, there are three im- portant questions to ask: 1. When we \ndecompose the problem into two subproblems (spilling and coloring) and solve each subproblem optimally, \ndoes that lead to an optimal solution to the original problem? We will present empirical evidence that \nthe solutions are excellent, but there is no theoretical reason that they will be optimal. 2. Can the \nspilling subproblem be solved optimally and effi- ciently? We will show that it can, using integer linear \npro- gramming. For the entire class of allocators that do not use rematerialization, and keeps no more \nthan one copy of each variable at a time, our algorithm provably generates the least number of (weighted) \nloads, stores, and memory-operand in- structions. Rematerialization can be easily incorporated into our \nmodel, but we have not yet done so; variables that live in several locations at once require further \nresearch -our initial attempts produce integer linear programs that are too costly to solve. 3. Can \nthe coloring subproblem be solved optimally and effi- ciently? We can do it optimally but far too slowly \nusing integer programming; we can do it quickly and adequately (though suboptimally) using optimistic \ncoalescing.  2. OPTIMAL SPILLING VIA ILP We model the register-spilling problem as a 0-1 linear program: \nan optimization problem with constraints that are linear inequalities, a linear cost function, and the \nadditional constraint that every vari- able must take the value 0 or 1. We use AMPL [8] to describe and \ngenerate the linear program, and CPLEX [7] to solve it. The AMPL compiler derives an instance of the \noptimization problem by instantiating a mathematical model with problem-specific data, and feeds the \nresulting linear program (in a suitable form) to a standard off-the-shelf simplex solver such as CPLEX. \nThe AMPL model consists of variable, set, and parameter declara- tions, and templates to generate the \nconstraints for the linear pro- gram. The sets, in their simplest form, are a symbolic enumeration and \ndeclared in the model using a declaration similar to: set T ; set R;  Sets may also be built from cartesian \nproducts of other sets. Vari- ables are usually indexed over sets, so a declaration such as: var x {T,R}; \n defines a set of variables Xi, ] where i ranges over T and j over R. Parameter declarations inject \nconcrete values into the model, so a declaration such as: param cost {T};  defines a parameter cos t \nthat is indexed over elements in the set T. The equations are generated from templates and are derived \nfrom model : set T; set R; var x {T,R}; param cos t{T}; Vt E 7'... data : I ~t T = {tl t2} set R \n: {rt r2} ( param cost = {(tl 3) (t2 4)} ] l Xtz,rl +Xtz,r2 k 4 Figure h AMPL modeling system logical \nconnections among the sets. For example: Vt E T . ~ Xt,r _> cost[t] rER  If T = {q t2} and R = {rl r2} \nthen, the template above will gen- erate two equations, one for each member of T: Xtt,rl + Xtt,r2 ~_ \ncost[q] Xt2,rl + Xt2,r2 >_ cost[t2] This AMPL example is illustrated in Figure 1 which shows the model, \ndata, and system of linear equations that is generated. Set Declarations: The description of our ILP \nformulation of op- timal spilling begins with the various set declarations required to characterize the \ninput flowgraph containing Intel IA-32 instruc- tions. At the lowest level, our model contains a set \nof symbolic variables V corresponding to temporaries in the program, and a set P of points within ttle \nflowgraph. There is a point between any two sequential instructions. A branch instruction terminates \nin a single point that is then connected to all points at the targets of the branch. In the AMPL model, \nthese sets are declared simply as: set V; set P; The remaining data declarations deal with liveness \nproperties and a characterization of the type of IA-32 instructions between two points. There are several \ndifferent classes of instructions in the IA- 32 instruction set, such as two-address binary instructions \n(d ~ d @ s), and unary instructions (d +-- f(s)), for example. If there is an add instruction v2 +-- \nv2 + Vl between program point Pl and a successor point P2, with source variable vl and destination variable \nv2, we model this by writing, (Pl,P2, vb v2) E Binary, and similarly for Unary. That is, set Binary is \na subset of P x P x V x V and is declared in the AMPL model using: l set Binary C (P x P x V x V) ; set \nUnary C (P\u00d7P\u00d7VxV) ; 1AMPL actually uses the word cross instead of the symbol x, and within instead of \nC. The actual AMPL code is shown in the appendix. For any variable vl that is live at a point Pl, we \nwrite (Pl ,Vl) 6 Exists. The Exists set is similar to the live set but not iden- tical: if an instruction \nbetween points Pl and P2 produces a result v that is immediately dead, then v is nowhere live but (p2,v) \nE Exists. If a variable vl is live and carded unchanged from point Pl to P2, then we say that (Pl,P2,Vl) \n6 Copy. If from point Pl to point P2 variable Vl is copied to variable v2 (e.g., by a move instruction), \nwe write (Pl, P2, vl, v2) E Copy2. set Exists C (PXV) ; set Copy C (PxP\u00d7V) ; set Copy2 C (P\u00d7P\u00d7VxV) ; \n The compiler will sometimes refer to specific hardware registers (%eax, %esp .... ), either because \na machine instruction requires an operand in a specific register or because of parameter-passing conventions. \nNow consider the instruction: movl %eax, %v that moves the contents of register %eax to the variable \nv. We model this as an instruction that takes no argument (because no temporary is a source operand) \nand produces a result into v. Binary instructions (such as raovl) can take their source or destination \noperands from registers or memory, but they cannot both be from memory. In this case, since the source \n%eax is known to be a reg- ister, the destination can be a register or memory. The class of in- structions \nthat take no argument and produce a register or memory result we call Nu 11 ary. In contrast, in the \ninstruction movl 4 (%esp) , %v that moves the contents of memory at address (%esp+4) to v, the operand \nv must be a register. The instruction class that take no argument and produce a register-only result \nwe call Nul laryReg. set Nullary C (PxPxV) ; set NullaryReg C (PxPxV) ; Some instructions accomplish \nv +-- f(v), where v can be in a register or memory (e.g. addl (S256, %v), that adds an immediate to the \nvariable v); others require that v must be in a register and noth- ing else (e.g. addl (4 (%esp) , %v)). \nWe call these Mutate and MutateReg respectively: set Mutate C (P x Px V) ; set MutateReg C (PxPxV) ; \nFor cases where no results are produced, the instruction may take two operands of which at most one can \nbe in memory (e.g., the com-pare instruction); or take one operand which can be either a regis- ter or \nmemory (e.g. addl (%v, %eax)); or take one operand that must be in a register. We call these three instruction-classes \nUseUp2, UseUp, and UseUpReg respectively: set UseUp2 C (P x P x V x V) ; set UseUp C (PxPxV) ; set UseUpReg \nC (P\u00d7P\u00d7V) ; If there is a branch instruction between Pl and P2, then it is nee- essary to know about \npoints such as P2, associated with a branch, as we cannot insert spill or reload instructions at P2. \nWe therefore declare a set of branching points: set Branch C P Consider a branch instruction between \npoints Pl and P2 that branches to/)4 if vl = O, but otherwise falls through to P3. Suppose v 3 is live \nthroughout, and Vl is live only along the successor containing P4-   if (Vl : o) \u00aeP2 \u00aeP3 \u00b0P4 v3 Elive \nVl v361ive It is necessary to propagate this liveness information along the edges of the branch, and \nwe represent this by generating: (Pl,P2,Vl) 6 UseUp; { (Pl,P2, V3), (p2,P3, V3 ), (p2,Pa, v3 ), (m,p2,vl),(p2,pa,vl),} \nC Copy; Note that vl is used and propagated between the points Pl and P2, and the other variables are \npropagated along the appropriate branch edges. Special cases of instructions Consider an add instruction \nwhose destination is known to be in memory: mix] ~ mix] +v. This could occur because x is the address \nof an array element, for example. Then v must be in a register, and x must be in a register. We can model \nthis as: (p! ,p2,x) E UseUpReg (Pl~P2~ v) 6 UseUpReg  Similarly, the instruction v +-- v + mix] is modeled \nas: (Pl, PZ, v) 6 MutateReg (Pit p2~X) 6 UseUpReg  Or consider the case where the source operand is \na constant, v v+c: (pl,P2,V) 6 Mutate There are many variations on this theme, but the point is that \neach special case of an instruction (where one of the operands is forced to be in memory, or in registers, \nor constant) reduces to a case that can also be described in the model. The compiler does this reduc- \ntion before generating the data set sent to AMPL. Parameter Declarations: The model declares several \nscalar and vector parameters (that are indexed symbolically using sets such as P). Each point in the \nprogram has an estimated frequency of execution that is used to weight the cost of spill or reload instruc- \ntions in our optimal spilling framework. We obtain the frequencies by static estimation from branch predictions, \npropagated using Kir- choff's laws as described by Wu and Larus [18]; better frequencies could be obtained \nby dynamic profiling. In our model we have: param weight {P) ; to associate the frequency of execution \nwith each point. At points where the compiler has explicitly used a machine reg- ister, e.g., movl (%eax, \n%v), register %eax is not available for coloring temporaries live at that point. We communicate this \nto the model via a parameter K: 245 fac: pushl %ebp ;; save frame pointer movl %esp, %ebp ;; new frame \npointer mov] 8(%ebp), tl ;; n movl #i t2 ; ; fac := 1 testl tl tl ;; cc := n A n je L1 ;; if n:0 got \n L1 L2: imull tl t2 ; ; fac :: n * fac decl tl ;; n :: n -1 jnz L2 ;; if n <> 0 got L2 Li: movl t2 %eax \n; ; return register leave ; ; done ret Figure 2: Intel IA-32 instructions for the factorial function \n param K {P}; where K [ p ] is the number of available registers at point p. Finally we have some scalar \ncost parameters: param Goad, Cstore, Groove, Cinstr C]oad, Cstore and (?move are the cost of executing \na load, store, and move instruction. Cinstr is the cost of fetching and decoding one instruction byte. \nPresumably, Cload > Cstore > Cmove > Cinstr. (In fact, Cinstr really measures the cost of a slight extra \npressure on the instruction cache.) Example. Figure 2 shows the Intel IA-32 instructions that may be \ngenerated for the factorial function, and Figure 3 shows the cor- responding flowgraph annotated with \npoints surrounding each in- struction. The AMPL sets generated are: set P := {Pl P2 P3 ..- PI4 P15} set \nV :: {h t2} set Branch := {/97 Pi[} set NullaryReg := {(P3 P4 tl)} set UseUp2 := {(P5 P6 tl t2)} set \nUseUp := {(P8 P9 tl) (PI2 PI3 t2)} set Mutate := {(p9 PI0 tl)} set MutateReg := {(P8 P9 12)} set Binary \n:= {(P8 P9 tl t2)} set Copy := {(p4 P5 tl) (P5 P6 tl) (P6 P7 tl) (P7 P8 tt) (P8 P9 tl) (PI0 Pn tl) (pll \nP8 tl) (P5 P6 t2) (P6 P7 t2) (P7 P8 t2) (P9 Pl0 t2) (Pl0 Pll t2) (Pll P8 t2)} set Exists := {(P4 tl) \n(P5 tl) (P6 tl) (P7 tl) (P8 tl) (P9 tl) (Pl0 tl) (Pll tl) (P5 t2) (P6 t2) (P7 t2) (P8 t2) (P9 t2) (PI0 \nt2) (PI! t2) (Pl2 t2) (PI3 t2)} The imull instruction is not classified as a Binary instruction as the \ndestination must be a register operand, and cannot be mem- ory, whi!e the source operand can be in either \nclass. Therefore, imull is classified as MutateReg for the destination operand and Us eUp for the source \noperand. Missing in the data are the concrete parameters such as the execu- tion frequency of each point, \nthe costs, and the value of K at each point. If we assume that %esp and %ebp are dedicated, then the \nvalue of K at all points in the flowgraph is 6, except at point P13 where %eax is defined and the value \nof K is 5. 3. VARIABLES AND CONSTRAINTS Spilling is the insertion of loads and stores between the instructions \nof the program. Each instruction of our program spans a pair of fac : Pl pushl %ebp \u00aep2 movl %esp, %ebp \n\u00aeP3 movl 8(%ebp),tl op4 movl #1,t 2 ~P5 testl tl,tl \u00aeP6 je L1 oR7 ~2 : \u00aeP8 imull tl,t 2 \u00aeP9 decl tl \nPl0 j nz L2 Pl I LI: ....... / aPl2 movl t2, %eax P13 leave \u00b0Pl4 ret ePl5 Figure 3: Flowgraph annotated \nwith points points, and \"between the instructions\" means \"at a point.\" Thus, we will insert loads/stores \nat points, not between them. Consider a variable v live at a program point p. The variable v could: \u00ae \narrive at p in a register and depart in a register - rp,v, \u00ae arrive in memory and depart in memory - \nrap,v, \u00ae arrive in a register and depart in memory - Sp,v (for stored), or arrive in memory and depart \nin a register- lp,v (for loaded). A solution to the spilling problem is just the description of where \nthe loads and stores are to be inserted. We model this as follows: vat r {Exists} binary; var m {Exists} \nbinary; vat l {Exists} binary; var s {Exists} binary; This says that for each (p,v) in Exists -that \nis, for each variable v live at a program point p -there are linear-program variables rp,v, rnp,v, lp,v, \nand Sp,v; the binary keyword says that the variable must take on the value 0 or 1. We wish to find the \nvalues of these variables subject to a set of linear constraints. Exists: The first constraint is that \nexactly one of these variables is set for any p and v: V(p,v) 6 Exists. lp,v + rp,v +Sp,v +me,v = 1 \nBranch: At a branch-point it's not possible to load or store, be- cause we can't insert an instruction \nafter a conditional-branch in- struction but before its targets. V(p,v) E Exists s.t. p 6 Branch, Ip,v+Sp,v \n: 0 Coloring: At any point p, all the stores can be performed before all the loads. However, the variables \nto be stored originate in registers, therefore the sum of variables that are already in registers and \nthose that are to be spilled must be no more than the number of registers available for coloring at p. \nVpCP. K[p] > E rp,v+Sp,v (p,v)6Exists  Similarly, after all the loads have been done at a point, the \nnumber of variables in registers should be no more than K. VpCP. K[p] >_ ~ rp,v+lp,v (p,v)6Exists  Copy \npropagation: If a variable v is copied from Pl to P2, then either it departs from Pt in a register and \narrives at P2 in a register, or it departs from Pl in memory and arrives at P2 in memory. If it departs \nfrom Pl in a register it must have already been in a register (i.e. rp~,v = I), or was loaded into a \nregister at Pl (lpt,v = 1). If it arrives at P2 in a register, it can either continue in a register at \nP2 (rp2,v = 1) or it can be stored at P2 (sp2,v = t): V(pl,P2,V) e Copy. lpl,v + rp,,v = Sp2,v + rp2,v \n The constraint s m,v + mm,v = lp2,v + rnp2,v is redundant and must not be specified (redundant constraints \nwill -with the inevitable rounding errors -overconstrain the problem so that the LP solver fails to find \na solution). If a variable Vl at Pt is copied to a variable v2 at P2, then if it departs Vl in a register \nit must arrive v2 in a register. The constraint is similar to the Copy case except that two variables \nare involved. ~/(pl,P2,Vl,V2) 6 Copy2. lpl ,v, + rpl ,Vl = Sp2,v2 + rp2,v2   3.1 Specifying the CISC \ninstructions On the IA-32 (x86, Pentium), if there is a Binary instruction (e.g., two-operand add) between \nPI and P2, operating on source variable vl and destination variable v2, then at least one of Vl and v2 \nmust depart Pl in registers: V(pl,P2,Vl ,v2) 6 Binary Ip,,v, + rpt,v, + lpl,v2 + rpl,v2 >_ 1  Furthermore, \nthe destination operand v2 must be in registers depart- ing Pl if and only if it is in registers arriving \nP2: V(pl ,p2,vl ,V2) 6 Binary lp, ,vz + rp, ,vz = sp2,v~ + rpz,vz  There are similar constraints for \nthe other classes of instructions, as shown in the appendix. They say that the result of a Nul laryRe9 \nmust arrive P2 in a register; at least one operand of a Useup2 must be in a register; the operand of \na gseUpReg must be in a register; the operand of a Mutate must depart Pl in the same storage class as \nit arrives P2; the operand of a MutateReg must depart Pt in a register and arrive P2 in a register; and \nthat at least one operand of a unary must be in a register. These constraints are all Pentium-specific, \nbut by illustrating how easily they are specified we hope to convince the reader that many kinds of CISC \ninstructions could be specified within this frame- work. 3.2 Objective function The objective function \nof our linear program calculates the esti- mated runtime cost of the spill-related loads, stores, and \nCISC operands. The first component of the cost comes from loads and stores: minimize COST:   (Z(p,v)eExi~ts \n weightp((Cload + 3Cinstr)lp,v+ (Cstore + 3Cinstr)Sp,v))  The cost of executing a load is Cload. The \ncost of a 3-byte load instruction (in i-cache occupancy) is 3Cinstr. For each point p and variable v \nsuch that there is a spilt-load of v at p we incur this cost; and similarly for stores. If the destination \noperand of a Binary instruction is in memory, we incur a cost Cload and Cstore, and one extra byte of \nCinstr cost to specify the operand. If the source operand is in memory, then we incur a load cost and \none instruction-byte cost: \"Jff (E(p I ,p2,Vl ,v2)EBinary weightpl ((Cload + Cinstr )(mpl ,, + Sp, ,v~ \n) + (C~oa~ + Cstore + Cins t r) (mp2,v2 + lp2,v2)) ) ~-... There are similar clauses to account for \nthe cost of memory operands of the other classes of instructions: Unary, Mutate, and so on. 3.3 Temporary \nloads When we execute a load instruction to bring a value from mem- ory to registers, the value becomes \naccessible from both places, and similarly when we store from registers to memory. The model we have \ndescribed does not account for this fact; it acts as if a value lives only in one place at a time. We \nconstructed a more ambitious model that accurately accounts for values that continue to live in both \nmemory and registers after a load or store, but we had little success with it: the equations seem to \nbe sufficiently un- derconstrained that the integer LP solvers do enormous amounts of branch-and-bound \nsearch. Therefore we use the model that as- sumes that each value lives in one place (memory or registers) \nat a time. Our spilling is optimal only with respect to this model. However, we were able to incorporate \none useful special case into our model. A variable can be loaded from (a spill location in) mem- ory \nto a register for use in the very next instruction, with the as- sumption that the register is then dead \nand the memory value lives on. We have not described this mathematically in the body of the paper, but \nour implemented AMPL model includes this feature. This completes the description of our linear-program \nmodel of spill costs. 4. SOLVING THE MODEL. Our compiler [2][10] feeds the data associated with a flowgraph \ntogether with the model to AMPL. AMPL generates a linear pro- gram with variables, constraints, and an \nobjective function. From the example in Figure 3 the variables: Fp4,tl ~ Ip4,tl ~ Sp4,t I ~ mp4#l ............ \n247 would be generated for t 1 corresponding to the point P4, since (p4,tl) ~ Exists. A constraint corresponding \nto the Exlsts for- mula (Section 3) would establish the equation: I-p4~t 1 -~ [p4~t I @ Sp4,t I @ mp4,t \n1 : l In a typical large cluster of basic blocks spanning several source- program functions, there will \nbe a few thousand points p and sev- eral hundred temporaries v, yielding tens of thousands of linear- \nprogram variables. AMPL first runs a \"presolve\" phase in which as many variables as possible are eliminated; \nfor example, any use of mp,v could be re- placed by 1 - (rp,v + lp,v + Sp,~). After the presolve, AMPL \nformats the linear program in a way acceptable to the back end, which is any one of several commercial \nor noncommercial LP solvers. Some of these solvers can solve integer linear programs using a combination \nof the simplex method with branch-and-bound; others can do only continuous LP's using simplex alone. \nWe have used CPLEX [7] and IBM's OSL [13]; CPLEX is an order of magnitude faster but sometimes dumps \ncore. After the ILP solver is finished, AMPL formats the results - a table of r, l, s, m for each (p, \nv). Our compiler computes all the spilling from this information inserting load and store instructions \nat points where lp,v and Sp,v is set, and introduces memory operands at in- structions for which mp,v \nis set. A prior phase assigns a logical spill location for every temporary, ensuring that nonoverlapping \nlive ranges share the same memory location. 5. REGISTER COALESCING The resulting flowgraph has no more \nthan K variables simultane- ously live at any point, but it may still be the case that there is no K-coloring \nof the variables - that K registers do not suffice. If Xl interferes with Yl at point Pl, Yl interferes \nwith zl at point P2, and zl interferes with Xl at point P3, then even though there are only two temporaries \nlive at any time, there is no 2-coloring of the inter- ference graph. Our solution is to copy every variable \nto a freshly named temporary at every program point. At point Pl we will copy x2 +- xl and Y2 +- Yb at \nP2 we copy Y3 +- Y2 and z3 +- zi, and so on. We assume the copies are done in parallel, so that Y2 interferes \nonly with x2 and not with Xl or z3. Then each parallel copy moves at most K variables, and each temporary \ninterferes with no more than K - 1 others, and the graph is colorable. Whenever there is an edge from \nprogram point Pl to P2 such that the optimal-spill model has a Copy or copy2 relation, we also in- troduce \na copy in the optimal-coalescing graph. That is, all the variables copied across an edge are formed into \na parallel copy that is meant to occur simultaneously with any other instruction executed at the edge. \nFor edges that don't contain any \"real\" in- struction, a new basic block must sometimes be introduced; \nthis is called edge splitting and is common in register-allocation prob- lems [1, figs. 19.2-3]. The \nresulting flowgraph for the example in Figure 2 is shown in Figure 4. After the graph is colored, each \nK-way parallel copy must be im- plemented by a sequence of K register-register move instructions. If \nthe parallel copy corresponds to a permutation with one or more cycles, then extra work (and extra storage) \nmay be required to move a value out of the way and then move it back. Fortunately, the xchg (exchange \ntwo registers) instruction on the IA-32 avoids the need fac : ~Pl pushl %ebp QP2 raovl %esp, %ebp \u00aeP3 \nraovl 8( %ebp),ti \u00b0 ,p4 q movl #t,t~ ) testl t~,t~    ,p, t? q IL q +-4 jeLl \u00aeP7 t3, _q *P8 imull \n4 4 t I ,t 2 \u00aeP9 t~ 4-- t14 ][ t25 ~ t24 decl t~ pl0 4 4 II j nz L2 4 / \u00aeP 11 / tl 4 +-tl6I[ 4 +- t26 \nLI: oPl2 movl t23~ %eax oPl3 i eave OPl4 ret \u00aeP15 Figure 4: Flowgraph with internal splits for extra \nstorage. Because there are no more than K live variables at any time, and because a variable-span live \nat one time is never live at any other time (only related to other live ranges), the graph is trivially \nK- colorable. Any conflicts that arise at an instruction can be removed by an appropriate set of parallel \ncopies before the instruction. That is, from the result of the spill phase, we can construct an interfer- \nence graph in which every node 2 has degree less than K. Such a graph can be easily colored by Kempe's \nalgorithm [14] (rediseov- 2The situation is more complicated for machines with instructions that both \noverwrite some of the input operands and generate new result operands. (Neither the IA-32 (Pentium), \nMIPS, Sparc, or Al- pha have such instructions.) The interference graph after optimal spilling may have \nsome nodes of degree k K, but these nodes won't have high-degree neighbors, so the graph will still be \ntrivially col- orable by Kempe's algorithm. ered 102 years later by Chaitin [5]). Having K \"artificial\" \nmove instructions before every \"natural\" in- struction would be expensive. Given a move instruction u \n+- v, if u and v can be colored the same - assigned to the same register - then the move can be deleted. \nThe register coalescing problem is to find a coloring so that as many moves as possible have source and \ndestination colored the same. When we formulate the coloring problem, we say that u and v are move-related. \nThe coloring/coalescing problem is significantly simpler than the problem handled by most graph-coloring \nregister allocators, be-cause the spills have already been identified and the graph is guar- anteed K-colorable. \nTherefore it's worth stating exactly what the algorithmic problem is. Optimal register coalescing. Given \nan undirected graph of max- imum degree K- 1 (these are the interference edges), and an ad- ditional \nset of weighted edges (these are the move edges), find a K-coloring of the graph such that 1. No two \nnodes connected by an interference edge have the same color;  2. There is the lowest possible cost, \nwhere cost is the sum of the weights of those move edges whose endpoints are colored differently.  This \nproblem is clearly NP-complete; it reduces the general graph- coloring problem (though we won't show \nthe reduction here). 6. ALGORITHMS FOR COALESCING We have tried three approaches to the coalescing problem: \niterated register coalescing [9], integer linear programming, and optimistic coalescing [17]. The first \ntwo don't work: iterated coalescing is fast but too conservative for the highly constrained problems \nthat result from our optimal spiller, and our integer programs produce optimal solutions but not in a \nreasonable amount of time. Optimistic coalescing. Our third approach is based on Park and Moon's optimistic \ncoalescing [17] and works as follows: We per- form aggressive coalescing (fi lh Chaitin), which may overconstrain \nthe graph so that it becomes uncolorable. We do this coalescing in priority order, so that the expensive \nmoves get coalesced first. Of course, we do not coalesce nodes that interfere- hence the need for priorities. \nWe then do a Briggs-style [4] optimistic coloring: that is, we re- move nodes of degree < K and push \nthem on a stack. When the graph contains only nodes of degree > K, we select a spill candi- date using \nChaitin's heuristics and remove it from the graph, push- ing it on the stack. Briggs called this optimistic \nbecause there is always the chance that in the stack-popping (coloring) phase, sev- eral neighbors of \nthe spill candidate will be colored the same, so that a color is available. If no color is available, \nBriggs would spill the node. Park and Moon point out that we can instead undo the coalescing that caused \nthis node to have high degree. We go even further: in our context, because we start with a graph where \nall nodes have low degree, we know that it will always be possible to undo the coalescing of a spill \ncandidate and color the nodes individually. However, we don't always need to undo this coalescing all \nthe way. We first split the spill candidate into its constituent primitive nodes. Then we reconsider \neach move instruction, and coalesce it if the resulting node is colorable in the current context. 7. \nBENCHMARKS We evaluate the method as follows: How costly is the optimal spilling algorithm? \u00ae How many \nspills remain, compared to other algorithms? o How costly is the optimistic coalescing algorithm? We \nwill not even perform measurements to answer this question; the algorithm is clearly linear-time (for \nany given K), and should be about as fast as Briggs's algorithm, which is known to be very efficient \nwhen implemented carefully. How many moves remain, compared to other algorithms? It would also be interesting \nto know how much suboptimality is caused by splitting the problem into two phases, spilling and co- alescing. \nAnswering this question would require an optimal al- gorithm for coalescing; although we have implemented \none using integer programming, it blows up on any but the tiniest examples. We measure the performance \nof our algorithms on the Standard ML of New Jersey benchmark suite. These are not microbenehmarks; the \nmean size of the 14 programs is 10,117 instructions of gener- ated Pentium code (exclusive of spill-related \ninstructions). 7.1 Optimal Spilling Figure 5 shows the spill statistics. The first column (\"Base\") in \neach pair shows the number of spills produced by iterated register coalescing on the Pentium (6 general-purpose \nregisters and 2 con- ventionally reserved); this is the measured performance of SML/NJ version 110.23. \nThe second column (\"Opt\") shows the perfor- mance of our new algorithm - optimal spilling with optimistic \nco- alescing-on the Pentium. Within each column, the Spills and Reloads sections show the number of spill \nand reload instructions insetted into the program. In other words, these subcolumns are a count of the \nnumber of memory loads and stores from spill instructions. Some spills and reloads can be combined with \naddressing modes, and the number of instructions affected is showri in the top subcolumn. No distinction \nis made between instructions that use memory as a source operand and a destination, and those that use \nmemory for a source only. The height of each column is 100. spills/(spills + nonspills). Figure 6 aggregates \nthe values across the columns. The base compiler uses static single-assignment (SSA) form, which divides \neach program variable into several temporaries based on the relation of definitions of the variable to \nthe dominator tree of the program. Then a Chaitin-style spiller implements each temporary either entirely \nin registers or entirely in memory. Briggs [3] con- jectured that SSA was the best way to split the variables \nprior to coloring with coalescing. Our current paper can be viewed as a test of his conjecture; we have \ndescribed an entirely different method for splitting the variables. A characteristic of SSA form is that \nthere will typically be one spill and multiple reloads for any temporary that is spilled. The number \n249 40 Memory instructions Reloads 30-Spills 20- o~ c~ \u00a20 10- % % Figure 5: Comparison of static spill \nstatistics for SML/NJ v110.23 (Base) using previous algorithm (SSA splitting and iterated register coalescing) \nand same compiler based on optimal spilling via integer linear programming (Opt) Memory Spills Reloads \nInstructions Base Opt Base Opt Base Opt Total 3040 4310 6771 3804 12312 5009 Figure 6: Aggregate values \nof spill statistics from Figure 5 of spill and reloads from the base compiler is 21% higher than the \nOpt version, however the number of spills in the Opt version is higher than the base compiler. This can \neasily be explained as the ILP model is splitting a live range into multiple parts, some subset of which \nare implemented in registers and the others in memory. In other words, there is only one transition from \nregister to memory in the base compiler, but multiple transitions in the ILP model. A different story \napplies to the Reloads column. The Opt column reloads less than half as many variables as the base compiler, \nas the ILP model effectively keeps active temporaries in registers. The Memory instructions column demonstrates \nthat the optimal spilling has done a significantly better job at keeping temporaries in registers.  \n 7.2 Optimal-spill performance Figure 7 shows the size of the AMPL model and the speed of gen- erating \nan optimal solution. Each dot in these figures represents a cluster, and each benchmark is made up of \nmultiple clusters. A cluster is a call graph in which every function in the graph has at 1~. o t0~0, \n100, o  / oo o~ \u00b0\u00b0 o o \u00b0o o / /,k; Oo\u00b02  O oo \u00b0 o .1- 01-I0 100 1000 10O00 100O00 Number of IA32 \nInstructions Figure 7: Solve time versus program points. The circles show the performance of our algorithm; \nthe polygon shows the approximate performance of Goodwin's algorithm (as reported by him [11]) on a different \ndata set. least one call-edge with another function in the graph. Since this is a continuation passing \nstyle (CPS) compiler, there are usually a large number of clusters for each benchmark. Superimposed on \n.... 9,~ ~\u00b8 ~ Splits Non-splits Instructions  Base Opt Opt Per Split barnes-hut 391 326 7430 23 boyer \n489 254 16495 65 count-graphs 223 215 3705 17 fft 145 212 3669 17 icfpO0 1413 1008 19332 19 knuth-bendix \n776 648 7912 12 lexgen 1767 1352 14543 11 life 230 203 2118 10 logic 201 163 3653 22 mandelbrot 26 16 \n262 16 mlyacc 3184 2559 39267 15 ray 403 311 4735 15 simple 1288 930 15133 16 tsp 292 291 3395 12 Geometric \nMean 17 Figure 8: Number of splits and instructions scatter plot is a crude bounding box obtained from \nFigures 3.4 in Goodwin's thesis [1 I] .3 The most important result from Figure 7 is that every block-cluster \nof fewer than 5000 instructions can be solved within 30 seconds. The complexity is close to linear (O(nl'3), \ntaking the least square fit), and is significantly better than the O(n 2\"5) reported by Good- win and \nWilken [12] for general-purpose processors. Goodwin and Wilken's performance is so much worse because \nthey compute op- timal coloring via ILP, whereas we compute only optimal register pressure (assuming \nthat the cost of coalescing will be insignificant). Kong and Wilken [15] get much better performance \n(though they do not report any empirical complexity result), and they also solve the whole register allocation \nproblem. Our number of constraints grows almost linearly with the program size (O(n1\"3)) which is sig- \nnificantly better than the models solved by Wilken et al. [12, 15].  7.3 Register Allocation Figure \n8 shows the number of splits (uncoalesced moves) remain- ing in the SSA-based compiler.with iterated \nregister coalescing (Base), and the number remaining using ILP and optimistic co- alescing (Opt). The \nthird column is the number of non-split in- structions in the Opt compiler; the corresponding column \nfor the Base compiler should be similar. ILP with optimistic coalescing produces programs in which 1 \nin 17 instructions are moves, and the static number of splits in all but one benchmark is better than \nour SSA-based splitting with iterated register coalescing. We don't know how many of these are required \nby the two-address nature of the instruction set or by other constraints - that is, we don't know how \nmany moves an optimal coalescer would leave. However, we have measured the overall performance of several \nstandard ML benchmarks using our old algorithm (SSA-based splitting with iter- ated register coalescing \n[9]) and our new one (optimal spilling with optimistic coalescing). The results (in Figure 9) show a \nspeedup of 9.5% improvement in execution speed (taking the geometric mean of ratios). Some of the benchmarks \nhave a significant improvement in static spills (Figure 5) but no speedup; perhaps this is because we \nweight the spill costs by static estimation, and perhaps dynamic 3We believe the machine on which Goodwin \ngot his results (HP9000/770) is about as fast as our machine (SGI Origin 2100 @ 250 MHz). Benchmark Base \nOpt Speedup barnes-hut 2.92 2.92 0.0% boyer 12.57 12.49 0.0 mlyacc 9.14 9.11 0.0 tsp 6.92 6.77 2.2 lexgen \n9.08 8.84 2.7 count-graphs 24.07 22.15 8.7 icfp00 109.29 99.72 9.6 fft 8.58 7.80 10.0 logic 5.10 4.61 \n10.6 knuth-bendix 8.08 7.22 tl.9 mandelbrot 27.92 23.21 20.3 life 19.03 15.24 24.9 simple 31.53 25.12 \n25.5 Figure 9: Execution speed Iterated Register Optimal Spilling, Coalescing Optimistic Coalescing \nLiveness and Liveness 1.3' Interference graph 8.19* I/O to AMPL 82.7* Iter. reg. coalesce 27.11\" AMPL \n419.2 t CPLEX 594.1 ~ I/O from AMPL 30.7* Insert load/stores 9.27* Insert load/stores 1.8\" Liveness 1.2' \nInterference graph 30.6* Optimistic coalesce 360.4* TOTAL (seconds) 44.57 TOTAL (seconds) 1522.0  Figure \n10: Time spent in the register allocators *433 MHz Pentium II ~250 MHz SGI Origin 2100 profiling would \nsignificantly improve the performance of the opti- mal spiller. Figure 10 compares the compile-time cost \nof the old (iterated reg- ister coalescing) algorithm to the new one, totalled over register- allocating \nall but one of the benchmark programs (153,836 ma-chine instructions). We have omitted the ratio-regions \nbenchmark -which produces the uppermost point of Figure 7 - but with it the total times would be 57 and \n11306 seconds, respectively. The new allocator is only a prototype, however, and is not highly engineered \nfor efficiency. 8. RELATED WORK Goodwin and Wilken [12] address several optimization problems such as \nlive range splitting, register assignment, spill placement, rematerialization, callee/caller-save register \nmanagement, and copy elimination, within the single framework of 0-1 integer linear pro- gramming. They \ndo not handle CISC instruction selection, though our new result implies that instruction selection could \nbe incorpo- rated into their framework. Our optimal spilling algorithm can handle problem sizes at least \nan order of magnitude larger than theirs, as figure 7 shows. We believe this is an important benefit \nof separating spilling from coloring. Goodwin and Wilken's algorithm has a different (and incompara- \nble) optimality guarantee than ours. They guarantee an optimal set of spills and register-register moves, \ngiven a predetermined set of potential split points. We guarantee an optimal set of spills (but not optimal \nmoves) over all possible split points. In principle, one could run their algorithm on an input that specifies \na split point at every possible place, but we believe the resulting problem size would swamp their algorithm \nin practice. Each optimization performed by Goodwin and Wilken can be done in one of the two phases \nthat we have described: Optimal spilling Optimistic coalescing o Spilling \u00ae Register assignment Live \nrange splitting o Copy elimination \u00ae Callee/caller-save management \u00ae Rematerialization We did not implement \nrematerialization, but it should fit naturally into our spilling model. Kong and Wilken [15] extend the \nwork of Goodwin and Wilken to handle irregular architectures and in particular the IA-32 instruction \nset, but their treatment of addressing modes appears to be much weaker than ours. Many of the extensions \ndeal with special aspects of register assignment on the Intel architecture, such as the penalty in code \nsize for using addressing modes involving registers %esp and %ebp, and the use of short (8 and 16 bit) \nregisters; we do not deal with these issues. They also consider the insertion of splits before commutative \noperations, i.e., a commutative operation such as $3 +-S1 +$2 could be translated by either moving Sl \nor S2 into S3 and performing the appropriate two address instruction - the choice is made by the linear \nprogram. They do not consider the possibility of inserting splits at any program point. Lueh, Gross, \nand AdI-Tabatabai [16]. Fusion-based register al- location breaks up register allocation into a per-region \nbasis, where the simplest region is a basic block. Spilling is performed inside the region so that the \nresulting interference graph is simplifiable. It may be necessary to spill transparent live ranges for \nthe graph to be simplifiable, but the actual spilling of transparent live ranges is delayed. A transparent \nlive range is one that is live on entry and exit to a region and is not used within the region. It is \nsimilar to members of our Copy set. As neighboring regions are fused to- gether, each region can be individually \ncolored by inserting splits for all the transparent live ranges at the boundaries of the region and coloring \neach region individually. Of course this naive strat- egy is undesirable, and great effort is made to \nstretch the lifetimes of transparent live ranges in memory or registers across the multi- ple regions \nbeing fused together. This is precisely what our linear programming phase does, but with a lot less bookkeeping, \nand our version is simpler to specify. Chow and Hennessy [6] use priority-based coloring before in- struction \nselection. Higher-priority temporaries are more impor- tant to keep in registers. They assign colors \nto the interference graph in order of priority; when a temporary is uncolorable, they use a greedy heuristic \nto split it into smaller live ranges. Some of these live ranges will be colorable (with copies between \none and the next, if they have different colors), and some will spill. This algorithm is not particularly \nsimple to implement, makes no guar- antee of optimality, and they describe results only for the relatively \nunconstrained problem of a 32-register RISC machine. 9. CONCLUSIONS We have formulated the register allocation \nproblem for CISC archi- tectures with few registers into one involving optimal placement of spill code, \nfollowed by optimal register coalescing. We have given some empirical evidence that dividing the problem \ninto these two phases does not significantly worsen the overall quality of the so- lution, but a full \ndemonstration of this fact would require optimal solutions to the overall problem that no one has been \nable to cal- culate. We have demonstrated an efficient algorithm using integer linear programming for \noptimal spill-code placement. The optimal coalescing problem has a significantly simpler struc- ture \nthan the general register-allocation problem, as the spilling has already been taken care of, and every \nnode in the graph has small degree. Because of this, our adaptation of Park and Moon's opti- mistic coalescing \nalgorithm is simpler and stronger than the origi- nal. Although optimistic coalescing performs well, \nit is not optimal. We have formulated the optimal coalescing problem (at the end of section 5) in such \na simple way - significantly simpler than tradi- tional register-allocation problems that require spilling \n-that other researchers can continue to investigate optimal coalescing. Programs compiled with optimal \nspilling followed by optimistic coalescing run about 9.7% faster than when compiled with SSA- based splitting \nfollowed by iterated register coalescing (though this number is based on an inadequate set of small programs). \nThis refutes a conjecture by Briggs [3] that the splits induced by SSA would be appropriate for register \nallocation and spilling.  10. REFERENCES [1] A. W. Appel. Modern Compiler Implementation in ML. Cambridge \nUniversity Press, Cambridge, England, 1998. [2] A. W. Appel and D. B. MacQueen. Standard ML of New Jersey. \nIn M. Wirsing, editor, 3rd International Syrup. on Prog. Lang. Implementation and Logic Programming, \npages 1-13, New York, Aug. 1991. Springer-Verlag. [3] P. Briggs. Register Allocation via Graph Coloring. \nPhD thesis, Rice University, April 1992. [4] R Briggs, K. D. Cooper, and L. Torczon. Improvements to \ngraph coloring register allocation. ACM Trans. on Programming Languages and Systems, 16(3):428-455, May \n1994. [5] G. J. Chaitin, M. A. Auslander, A. K. Chandra, J. Cocke, M. E. Hopkins, and P. W. Markstein. \nRegister allocation via coloring. Computer Languages, 6:47-57, January 1981. [6] E C. Chow and J. L. \nHennessy. The priority-based coloring approach to register allocation. ACM Trans. on Programming Languages \nand Systems, 12(4):501-536, October 1990. [7] CPLEX mixed integer solver, www.cplex.com, 2000. [8] R. \nFourer, D. M. Gay, and B. W. Kemighan. AMPL: A Modeling Language for Mathematical Programming. Scientific \nPress, South San Francisco, CA, 1993. www.ampl.com. [9] L. George and A. W. Appel. Iterated register \ncoalescing. In 23rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, pages \n208-218, New York, Jan 1996. ACM Press. [lo] L. George, F. Guillame, and J. Reppy. A portable and (inReg[p,v] \n+ store[p,v]) < K[p]; optimizing back end for the SML/NJ compiler, volume 786 of LNCS, pages 83-97. Springer-Vcrlag, \n1994. subject to REGISTERS_K2 {p in Pts}: sum { (p,v) in Exists } [il] D. W. Goodwin. Optimal and Near-Optimal \nGlobal Register (inReg[p,v] + load[p,v] + loadt[p,v]) < K[p]; Allocation. PhD thesis, University of California \nat Davis, subject to COPY_PROPAGATE {(pl,p2,v) in Copy union Mutate}: 1996. load[pl,v] + inReg[pl,v] \n= storelp2,vl + inReglp2,v]; [12] D. W. Goodwin and K. D. Wilken. Optimal and near-optimal global register \nallocation using 0-1 integer programming. Software--Practice and Experience, 26(8):929-965, 1996. [13] \nM. S. Hung. Optimization with IBM-OSL. Scientific Press, South San Francisco, CA, 1993. [14] A.B. Kempe. \nOn the geographical problem of the four colors. American Journal of Mathematics, 2:193-200, 1879. [15] \nT. Kong and K. D. Wilken. Precise register allocation for irregular architectures. In 31st International \nMicroarchitecture Conference. ACM, December 1998. [16] G. Lueh, T. Gross, and A. Adl-Tabatabai. Global \nregister allocation based on graph fusion. In Languages and Cornpilers for Parallel Computing, pages \n246-265. Springer Verlag, LNCS 1239, August 1997. [17] J. Park and S.-M. Moon. Optimistic register coalescing. \nIn Proceedings of the 1998 International Conference on Parallel Architecture and Compilation Techniques, \npages 196-204, 1998. [18] Y. Wu and J. R. Larus. Static branch frequency and program profile analysis. \nIn 27th IEEE/ACM International Symposium on Microarchitecture (MICRO-27), Nov. 1994.  Appendix: AMPL \nmodel for spilling model; set Vats; set Pts; set Copy within (Pts cross Pts cross Vats); set Copy2 within \n(Pts cross Pts cross Vars cross Vats); set Mutate within (Pts cross Pts cross Vars); set MutateReg within \n(Pts cross Pts cross Vars); set Unary within (Pts cross Pts cross Vars cross Vars); set Binary within \n(Pts cross Pts cross Vars cross Vars); set UseUp2 within (Pts cross Pts cross Vats cross Vars); set UseUp \nwithin (Pts cross Pts cross Vats); set UseUpReg within (Pts cross Pts cross Vats); set Nullaty within \n(Pts cross Pts cross Vars); set NullaryReg within (Pts cross Pts cross Vars); set Branch within Pts; \nset Connect within (Pts cross Pts); set Exists within (Pts cross Vars); param weight {Pts}; param K \n{Pts}; param loadCost; param storeCost; param moveCost; param instrCost; var inReg {Exists} binary; \nvat inMem {Exists} binary; var load {Exists} binary; var loadt {Exists} binary; vat store {Exists} binary; \n subject to BRANCH {(p,v) in Exists : p in Branch}: load[p,v] + store[p,v] = 0; subject to EXISTS {(p,v) \nin Exists}: loadt[p,v] + load[p,v] + inReg[p,v] + store[p,v] + inMem[p,v] = 1; subject to REGISTERS_K1 \n{p in Pts): sum { (p,v) in Exists } subject to COPY2_PROPAGATE {(pl,p2,src,dst) in Copy2}: load[p 1,src] \n+ inReg[p t,src] = store[p2,dst] + inReglp2,dstl; subject to NULLARY_REG {(pl,p2,v) in NullaryReg}: \nstore[p2,v] + inReg[p2,v] = 1; subject to USEUP2{(pl, p2, srcl, src2) in UseUp2}: loadt[pl,srcl] + load[pl,srcl] \n+ inReg[pl,srcl I + loadtlpl,src2] + load[pl,src2] + inReg[pl,src2] _> 1; subject to USEUP..IN_REG1 \n{(pl,p2,v) in UseUpReg}: loadt[pl,v] + load[pl,v] + inReg[pl,v] = 1; subject to BINARY_PROPDST {(pl,p2,src,dst) \nin Binary}: toad[pl,dst] + inReg[pl,dst] = store[p2,dst] + inReg[p2,dst]; ............... subject to \nBINARY_IN_REG {(p 1,p2,src,dst) in Binary}: loadt[pl,src] + load[pl,src] + inReg[pl,src] + Ioadt[pl,dst] \n+ load[pl,dst] + inReg[pl,dst] > 1; subject to MUTATE_PROPDST {(pl,p2,dst) in Mutate}: load[pl,dst] \n+ inReglpl,dst] = store[p2,dst] + inReg[p2,dst]; subject to MUTATE_REG 1 {(pl,p2,dst) in MutateReg}: \nload[pl,dst] + inReg[pl,dst] = 1; subject to MUTATE.REG2 {(p 1,p2,dst) in MutateReg}: store[p2.dst] \n+ inReg[p2,dst] = 1; subject to UNARY_B INARY_IN_REG { (p t ,p2,src,dst) in Unary }: loadt[pl,sre] + \nIoad[pl,src] + inReg[pl,src] + store[p2,dst] + inReg[p2,dst] > 1; minimize COST: (sum {v in Vars, p \nin Pts: (p,v) in Exists} weight[p] * ( loadt[p,v] * (loadCost + 3 * instrCost) + load[p,v] * (loadCost \n+ 3 * instrCost) + store[p,v] * (storeCost + 3 * instrCost)))  + (sum {(pl,p2,src,dst) in Binary} weight[pl] \n* ( (inMem[pl,src] + store[pl,src]) * (loadCost + instrCost) + (1 -(inReg[p2,dstl + store[p2,dst])) * \n0.8 * (loadCost + storeCost + instrCost)))  + (sum {(pl,p2,src,dst) in Copy2} weight[pl] * ((inMem[pl,src] \n+ store[pl,src]) * (loadCost + instrCost) + (1 -(inReg[p2,dst] + store[p2,dst])) * (storeCost + instrCost))) \n + (sum {(pl,p2,src,dst) in Unary} weight[p 1] * ( (inMem[pl,src] + store[pl,src]) * 0.8 * (loadCost \n+ instrCost) + (1 -(inReg[p2,dst]+storelp2,dst])) * 0.8 * (storeCost+instrCost))) + (sum {(pl,p2,dst) \nin Mutate} weight[pl] * ( (1- - (inReg[p2,dst] + storelp2,dst])) * 0.8 * (loadCost + storeCost + instrCost))) \n + (sum {(pl,p2,dst) in Nullary} weight[pl] * ( (1 - (inReg[p2,dst] + store[p2,dst])) * 0.8 * (storeCost \n+ instrCost)))  + (sum {(pl,p2,src) in UseUp} weight[pl] * ( (inMem[pl,src] + store[pl,src]) * 0.8 * \n(loadCost + instrCost))) + (sum {(pl,p2,srcl,src2) in UseUp2} weight[p 1] * ( (inMem[pl,srcl] + store[pl,srcl]) \n* 0.8 * (loadCost + instrCost)+ (inMem[pl,src2l + store[pl,src2]) * 0.8 * (loadCost + instrCost))); \n 253  \n\t\t\t", "proc_id": "378795", "abstract": "<p>Many graph-coloring register-allocation algorithms don't work well for machines with few registers. Heuristics for live-range splitting are complex or suboptimal; heuristics for register assignment rarely factor the presence of fancy addressing modes; these problems are more severe the fewer registers there are to work with. We show how to optimally split live ranges and optimally use addressing modes, where the optimality condition measures dynamically weighted loads and stores but not register-register moves. Our algorithm uses integer linear programming but is much more efficient than previous ILP-based approaches to register allocation. We then show a variant of Park and Moon's optimistic coalescing algorithm that does a very good (though not provably optimal) job of removing the register-register moves. The result is Pentium code that is 9.5% faster than code generated by SSA-based splitting with iterated register coalescing.</p>", "authors": [{"name": "Andrew W. Appel", "author_profile_id": "81100498630", "affiliation": "Princeton University", "person_id": "PP14174176", "email_address": "", "orcid_id": ""}, {"name": "Lal George", "author_profile_id": "81100355594", "affiliation": "Lucent Technologies, Bell Laboratories", "person_id": "PP31038673", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/378795.378854", "year": "2001", "article_id": "378854", "conference": "PLDI", "title": "Optimal spilling for CISC machines with few registers", "url": "http://dl.acm.org/citation.cfm?id=378854"}