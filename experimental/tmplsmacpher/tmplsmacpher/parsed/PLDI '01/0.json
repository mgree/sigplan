{"article_publication_date": "05-01-2001", "fulltext": "\n Design and Implementation of Genetics for the .NET Common Language Runtime Andrew Kennedy Don Syme Microsoft \nResearch, Cambridge, U.K. {akenn,dsyme}~microsof t. corn Abstract The Microsoft .NET Common Language \nRuntime provides a shared type system, intermediate language and dynamic execution environment for the \nimplementation and inter-operation of multiple source languages. In this paper we extend it with direct \nsupport for parametric polymorphism (also known as generics), describing the design through examples \nwritten in an extended version of the C# programming language, and explaining aspects of implementation \nby reference to a prototype extension to the runtime. Our design is very expressive, supporting parameterized \ntypes, polymorphic static, instance and virtual methods, \"F-bounded\" type parameters, instantiation \nat pointer and value types, polymor- phic recursion, and exact run-time types. The implementation takes \nadvantage of the dynamic nature of the runtime, performing just- in-time type specialization, representation-based \ncode sharing and novel techniques for efficient creation and use of run-time types. Early performance \nresults are encouraging and suggest that pro- grammers will not need to pay an overhead for using genetics, \nachieving performance almost matching hand-specialized code. Introduction Parametric polymorphism is \na well-established programming lan- guage feature whose advantages over dynamic approaches to generic \nprogramming are well-understood: safety (more bugs caught at compile time), expressivity (more invariants \nexpressed in type signatures), clarity (fewer explicit conversions between data types), and efficiency \n(no need for run-time type checks). Recently there has been a shift away from the traditional com- pile, \nlink and run model of programming towards a more dynamic approach in which the division between compile=time \nand run-time becomes blurred. The two most significant examples of this trend are the Java Virtual Machine \n[11] and, more recently, the Common Language Runtime (CLR for short) introduced by Microsoft in its .NET \ninitiative [1]. The CLR has the ambitious aim of providing a common type system and intermediate language \nfor executing programs written in a variety of languages, and for facilitating inter-operability be-tween \nthose languages. It relieves compiler writers of the burden of dealing with low-level machine,specific \ndetails, and relieves pro- grarnmers of the burden of describing the data marshalling (typi- Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for profit or commercial advan- tage and that copies \nbear this notice and the full citation on the first page. To Copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior specific permission and/or a tee, PLDI 2001 6101 \nSnowbird, Utah, USA &#38;#169; 2001 ACM ISBN 1-58113-414-2101106...$5.00 cally through an interface definition \nlanguage, or IDL) that is nec- essary tbr language interoperation. This paper describes the design and \nimplementation of support for parametric polymorphism in the CLR. In its initial release, the CLR has \nno support for polymorphism, an omission shared by the JVM. Of course, it is always possible to \"compile \naway\" polymor- phism by translation, as has been demonstrated in a number of ex- tensions to Java [14, \n4, 6, 13, 2, 16] that require no change to the JVM, and in compilers for polymorphic languages that target \nthe JVM or CLR (MLj [3], HaskeU, Eiffel, Mercury). However, such systems inevitably suffer drawbacks \nof some kind, whether through source language restrictions (disallowing primitive type instanti- ations \nto enable a simple erasure-based translation, as in GJ and NextGen), unusual semantics (as in the \"raw \ntype\" casting seman- tics of GJ), the absence of separate compilation (monomorphizing the whole program, \nas in MLj), complicated compilation strategies (as in NextGen), or performance penalties (for primitive \ntype in- stantiations in PolyJ and Pizza). The lesson in each case appears to be that if the virtual \nmachine does not support polymorphism, the end result will suffer. The system of polymorphism we have \nchosen to support is very expressive, and, in particular, supports instantiations at reference and value \ntypes, in conjunction with exact runtime types. These to- gether mean that the semantics of polymorphism \nin a language such as C# can be very much \"as expected\", and can be explained as a relatively modest \nand orthogonal extension to existing features. We have found the virtual machine layer an appropriate \nplace to sup- port this functionality, precisely because it is very difficult to im- plement this combination \nof features by compilation alone. To our knowledge, no previous attempt has been made to design and im- \nplement such a mechanism as part of the infrastructure provided by a virtual machine Furthermore, ours \nis the first design and imple- mentation of polymorphism to combine exact run-time types, dy- namic linking, \nshared code and code specialization for non-uniform instantiations, whether in a virtual machine or not. \n 1.1 What is the CLR? The .NET Common Language Runtime consists of a typed, stack- based intermediate \nlanguage (IL), an Execution Engine (EE) which executes IL and provides a variety of runtime services \n(storage management, debugging, profiling, security, etch), and a set of shared libraries (.NET Frameworks). \nThe CLR has been success- fully targeted by a variety of source languages, including C#~ Vi- sual Basic, \nC++; Eiffel, Cobol, Standard ML, Mercury, Scheme and Haskell. The primary focus of the CLR is object,oriented \nlanguages, and this is reflected in the type system, the core of which is the defini, tion of classes \nin a single-inheritance hierarchy together with Java- style interfaces. Also supported are a collection \nof primitive types, arrays of specified dimension, structs (structured data that is not boxed, i.e. stored \nin-line), and safe pointer types for implementing call-by-reference and other indirection-based tricks. \nMemory safety enforced by types is an important part of the se- curity model of the CLR, and a specified \nsubset of the type system and of IL programs can be guaranteed typesafe by verification rules that are \nimplemented in the runtime. However, in order to support unsafe languages like C++, the instruction set \nhas a well-defined interpretation independent of static checking, and certain types (C- style pointers) \nand operations (block copy) are never verifiable. IL is not intended to be interpreted; instead, a variety \nof na- tive code compilation strategies are supported. Frequently-used libraries such as the base class \nlibrary and GUI frameworks are precompiled to native code in order to reduce start-up times. User code \nis typically loaded and compiled on demand by the mntime. 1,.2 Summary of the Design A summary of the \nfeatures of our design is as follows: 1. Polyrnorphic declarations. Classes, interfaces, structs, and \nmethods can each be parameterized on types. 2. Runtime types. All objects carry \"exact\" runtime type \ninformation, so one can, for example, distinguish a List<string> from a List<Object> at runtime, by look- \ning at the runtime type associated with an object. 3. Unrestricted instantiations. Parameterized types \nand poly- morphic methods may be instantiated at types which have non-uniform representations, e.g. List<int>, \nList<long> and List<double>. Moreover, our implementation does not introduce expensive box and unbox \ncoercions. 4. Bounded polymorphism. Type parameters may be bounded by a class or interface with possible \nrecursive reference to type parameters (\"F-bounded\" polymorphism [5]). 5. Polymorphic inheritance. The \nsuperclass and implemented interfaces of a class or interface can all be instantiated types. 6. Polymorphic \nrecursion. Instance methods on parameterized types can be invoked recursively on instantiations different \nto that of the receiver; likewise, polymorphic methods can be invoked recursively at new instantiations. \n 7. Polymorphic virtual methods. We allow polymorphic meth- ods to be overridden in subclasses and specified \nin interfaces and abslract classes. The implementation of polymorphic vir- tual methods is not covered \nhere and will be described in de- tail in a later paper.  What are the ramifications of our design choices? \nCertainly, given these extensions to the CLR, and assuming an existing CLR compiler, it is a relatively \nsimple matter to extend a \"regular\" class- based language such as C#, Oberon, Java or VisualBasic.NET \nwith the ability to define polymorphic code. Given the complexity of compiling polymorphism efficiently, \nthis is already a great win. We wanted our design to support the polymorphic constructs of as wide a \nvariety of source languages as possible. Of course, attempting to support the diverse mechanisms of the \nML family, Haskell, Ada, Modula-3, C++, and Eiffel leads to (a) a lot of fea- tures, and (b) tensions \nbetween those features. In the end, it was necessary to make certain compromises. We do not currently \nsup- port the higher-order types and kinds that feature in Haskell and in encodings of the SML and Caml \nmodule systems, nor the type class mechanism found in Haskell and Mercury. Neither do we support Eiffel's \n(type-unsafe) covariant subtyping on type construc- tors, though we are considering a type-safe variance \ndesign for the future. Finally, we do not attempt to support C++ templates in full. Despite these limitations, \nthe mechanisms provided are suf- ficient for many languages. The role of the type system in the CLR is \nnot just to provide runtime support -it is also to facili- tate and encourage language integration, i.e. \nthe treatment of cer- tain constructs in compatible ways by different programming lan- guages. Interoperability \ngives a strong motivation for implement- ing objects, classes, interfaces and calling conventions in \ncompati- ble ways. The same argument applies to polymorphism and other language features: by sharing \nimplementation inffrrastructure, un- necessary incompatibilities between languages and compilers can \nbe eliminated, and future languages are encouraged to adopt de- signs that are compatible with others, \nat least to a certain degree. We have chosen a design that removes many of the restrictions pre- viously \nassociated with polymorphism, in particular with respect to how various language features interact. For \nexample, allowing arbitrary type instantiations removes a restriction found in many languages. Finally, \none by-product of adding parametefized types to the CLR is that many language features not currently \nsupported as primitives become easier to encode. For example, n-ary product types can be supported simply \nby defining a series of parameter- ized types Prod2, Prod3, etc. 1.3 Summary of the Implementation Almost \nall previous implementation techniques for parametric polymorphism have assumed the traditional compile, \nlink and run model of programming. Our implementation, on the other hand, takes advantage of the dynamic \nloading and code generation capa- bilities of the CLR. Its main features are as follows: 1. \"Just-in-time\" \ntype specialization. Instantiations of parame- terized classes are loaded dynamically and the code for \ntheir methods is generated on demand. 2. Code and representation sharing. Where possible, compiled code \nand data representations are shared between different in- stantiations. 3. No boxing. Due to type specialization \nthe implementation never needs to box values of primitive type. 4. Efficient support of run-time types. \nThe implementation makes use of a number of novel techniques to provide op- erations on run-time types \nthat are efficient in the presence of code sharing and with minimal overhead for programs that make no \nuse of them.  2 Polymorphism in Generic C# In this section we show how our support for parametric poly- \nmorphism in the CLR allows a generics mechanism to be added to the language C# with relative ease. C# \nwill be new to most readers, but as it is by design a deriva- tive of C++ it should be straightforward \nto grasp. The left side of Figure 1 presents an example C# program that is a typical use of the generic \n\"design pattern\" that programmers employ to code around the absence of parametric polymorphism in the \nlanguage. The type object is the top of the class hierarchy and hence serves as a poly- morphic representation. \nIn order to use such a class with primitive element types such as integer, however, it is necessary to \nbox the Object-based stack Generic stack class Stack { private object[] store; private int size; public \nStack() store=new object[lO]; size=O; } public void Push(object x) { if (size>=store.Size) { object[] \ntmp= new object[size*2]; Array.Copy(store,tnrp,size); store = imp; store[size++] = x; } public object \nPop() { return store[--size]; } public static void Main() { Stack x = now Stack(); x.Push(17); Console.WriteLine((int) \nx.Pop() == 17); class Stack<T> { private T[] store; private int size; public Stack() store=now T[iO]; \nsize=O; public void Push(T x) { if (size>=store.Size) { T[] imp = new TEsize*2]; Array.Copy(store,imp,size); \n store = tnrp; store[size++] = x; public T Pop() { return storeE--size]; public static void Main() \n{ Stack<int> x = new Stack<int>(); x.Push(17); Console.WriteLine(x.PopO == 17); Figure 1: C# and Generic \nC# Stack implementations values. C# inserts box coercions automatically (as in x. Push (17)) and requires \nthe programmer to write unbox coercions explicitly (as in (int) x. Pop ()). Of course, the latter can \nfail at run-time. 2.1 Using parameterized types For the average user, polymorphism provides nothing more \nthan an expanded set of type constructors, along with some polymor- phic static methods to help manipulate \nthem. For example, a class- browsing tool might show a parameterized version of our example Stack class \nas: class Stack<T> { Stack(); // constructor void Push(T); /1 instance methods T Pop(); } The user now \nhas access to a new family of types which include Stack<int>, Stack<Stack<int>> and Stack<string>. He \ncan write code using these types, such as the following ~agment of an arithmetic evaluator: enum op { \nAdd, Neg }; Stack<int> s = new Stack<int>(); switch (op) { case Add : s.Push(s.Pop() + s.Pop()); break; \ncase Neg : s.Push(- s.PopO); break; }  With the vanilla C# implementation of Figure 1 he would have \nwrit- ten: Stack s = new Stack(); ... switch (Op) { case Add : s.push((int) s.Pop() \u00f7 (int) s.PopO); \nbreak;  case Neg : s.Push(-(int) s.PopO); break; The programmer can therefore replace the casts at \nevery use of s with some extra annotations at the point where s is created. The implementation of the \narithmetic evaluator using the parameterized Stack class will also be much more efficwnt, as the integers \nwill not be boxed and unboxed.  2.2 Exact runtlme types The type system of the CLR is not entirely static \nas it supports run-time type tests, checked coercions and reflection capabilities. This entails maintaining \nexact type information in objects, a feature that we wished to preserve in our design for polymorphism. \nThus each object carries with it full type information, including the type paraameters of parameterized \ntypes. In the following example, this ensures that the third line raises an Inv alidCastEx cept i on: \n Object obj = new Stack<int>O; Stack<int> s2 = (Stack<int>) obj; // succeeds Stack<string> sS = (Stack<string>) \nobj ; // exception  Exact runtime types are primarily useful for reflection, type-safe serialization \nand for interacting with components that do not, for whatever reason, use fully exact types (e.g. use \ntype Object): // Read some serialized form of an object: Object s = ReadFromStreamO ; // Check \"s\" is \na Stack<int>: Stack<int> s2 = (Stack<int>) s; 2.3 Using polymorphic methods Polymorphic methods take \ntype parameters in addition to normal parameters. Typically such methods will be associated with some \ninterface IComparer<T> { int Compare(T x, T y); interface ISet<T> { bool Contains(T x) ; void Add(T \nz); void Remove(T x) ; } class ArraySet<T> : ISet<T> { private T[] items; private iat size; priva%e \nIComparer<T> c; public ArraySet(IComparer<T> _c) { items = new T[IO0]; size = O; c = _c; public boo1 \nContains(T x) { for (int i = O; i < size; i++) if (c.Compare(x,items[i]) == O) return true; return false; \n} public void Add(T x) { ... } public void Remove(T x) { ... } } Figure 2: A parameterized interface \nand implementation parameterized class or built-in type constructor such as arrays, as in the following \nexample: class Array -[ ... static void Revsrss<T>(T~); static T[] Slice<T>(T[], int ix, int n) ; > \n Here Reverse and Slice are polymorphic static methods defined within the class Array, which in the \nCLR is a super-type of all built-in array types and is thus a convenient place to locate opera- tions \ncommon toall arrays. The methods can be called as follows: int [3 art = uev int [tO0] ; for (int i = \nO; i<iO0; i++) arz[i]= iO0-i; int[] err2 = Array.Slics<int>(arr, I0, 80)7 Array.Reverse<Jut> (art2) \n;  The type parameters (in this case, <int>) can be inferred in most cases arising in practice [4], \nallowing us here to write the more concise Array .Reverse (arr).    2.4 Defining parameterized types \nThe previous examples have shown the use of parameterized types and methods, though not their declaration. \nWe have presented this first because in a multi-language framework not all languages need support polymorphic \ndeclarations -for example, Scheme or Visual Basic might simply allow the use of parameterized types and \npoly- morphic methods defined in other languages. However, Generic C# does allow their definition, as \nwe now illustrate. We begin with parameterized classes. We can now complete a Generic C# definition of \nStack, shown on the right of Figure 1 for easy comparison. The type parameters of a parameterized class \ncan appear in any instance declaration: here, in the type of the private field store and in the type \nsignatures and bodies of the instance methods Push and Pop and constructor StackS. C# supports the notion \nof an interface which gives a name to a set of methods that can be implemented by many different classes. \nFigure 2 presents two examples of parameterized interfaces and a parameterized class that implements \none of them. A class can clasa Array { ... static T[] Slice<T>(T[] art, int ix, int n) { T[] arr2 = new \nTin]; for (int i = 0; i < n; i++) arr2[i] = arr[ix+i]; return arr2; }} class ArraySst<T> { ... ArraySet<Pair<T,U>> \nTimes<U>(ArraySet<U> that) { ArraySet<Pair<T,U>> r = new ArraySst<Pair<T,U>>(); for (int i = O; i < \nthis.size; i++) for (int j = O; j < tkat.size; j++) r.Add(new Pair<T,U>(this.items[i] ,that.items[j])); \nreturn r; >} Figure 3: Polymorphic methods also implement an imerface at a single instantiation: for \nexample, CharSet : ISet<char> might use a specialized bit-vector repre- sentation for sets of characters. \nAlso supported are user-defined \"struct\" types, i.e. values repre- sented as inline sequences of bits \nrather than allocated as objects on the heap. A parameterized struct simply defines a family of struct \ntypes: etruct Pair<T,U> { public T fst; public U end; public Pair(T t, U u) { fst = t; end = u; Finally, \nC# supports a notion of first-class methods, called dele-gates, and these too can be parameterized on \ntypes in our extension. They introduce no new challenges to the underlying CLR execution mechanisms and \nwill not be discussed further here. 2.5 Defining polymorphic methods A polymorphic method declaration \ndefines a method that takes type parameters in addition to its normal value parameters. Figure 3 gives \nthe definition of the Slice method used earlier. It is also possible to define polymorphic instance methods \nthat make use of type parameters from the class as well as from the method, as with the cartesian product \nmethod \"rimes shown here. 3 Support for Polymorphism in IL The intermediate language of the CLR, called \nIL for short, is best introduced through an example. The left of Figure 4 shows the IL for the non-parametric \nStack implementation of Figure 1. It should be apparent that there is a direct correspondence between \nC# and IL: the code has been linearized, with the stack used to pass ar- guments to methods and for expression \nevaluation. Argument 0 is reserved for the this object, with the remainder numbered from 1 upwards. Field \nand method access instructions are annotated witl7 explicit, fully qualified field references and method \nreferences. A call to the constructor for the class Object has been inserted al the start of the constructor \nfor the Stack class, and types are de- scribed slightly more explicitly, e.g. class System,0bject in- \nstead of the C# object. Finally, box and unbox instructions have been inserted to convert back and forth \nbetween the primitive int type and Object. Object-based stack .class Stack { .field private class Systsm. \nObject[] store .field private Jut32 size .method public void .ctor() idarg.O call void System. Object::.ctor() \nidarg.O idc.i4 I0 newarr System.Object stfld class System.Object[] Stack::store ldarg.O ldc.i4 0 stfld \nintS2 Stack::size ret } .method public void Push(class System.Object x) { .maxstack 4 .locals (class \nSystem.Object[], int32) idarE.O Idfld class System.Object[3 Stack::store lderg.O dup Idfld Int32 Stack::size \ndup stloc.1 idc.i4 1 add stfld int32 Stack::size idloc.1 idarg. 1 stelem.ref rat } .method public class \nSystem.Object Pop()~ .maxstack 4 Idarg.O idfld class System.Object[] Stack::store Idarg.O dup Idfld \nint32 Stack::size idc.i4 1 sub dup stfld int32 Stack::size idelem.ref rat } .method public static \nvoid Main() .entrypoint .maxstack S .locals (class Stack) newobj void Stack::.ctorO stloc.O Idloc.O \nldc.i4 17 box Systsm. Int32 call instance void Stack::Push(class System.Object) Idloc.O call instance \nclass System.Object Stack::Pop() unbox System.Int32 idind, i4 idc.i4 17 ceq call void System.Console::WriteLine(bool) \nrat }}  Generic stack .class Stack<T> .field private J_O[] store .field private isiS2 size .method public \nvoid .ctorO ldar E .0 call void System.Object: :.ctorO ldar E .0 Idc. i4 i0 newarr !_0 stfld )0[] Stack<)O>: \n:store idarg. 0 idc. i4 0 stfld int32 Stack< !0> : : size ret } .method public void Push(!O x) .[ .maxstack \n4 locals (!0[], int32) idarg. 0 lctfld LO[] Stack< !0> : :store idar E . 0 dup Idfld int32 Stack<)O>: \n:size dup stloc. 1 idc. i4 1 add stfld int32 Stack< !0> : : size idloc. 1 idarg. 1 stelem, any !0 ret \n } .method public !0 Pop() .maxstack 4 idar E . 0 idfld !0[] Stack<!O>: :store ldarg. 0 dup Idfld isiS2 \nStack< !0> : : size idc. i4 1 sub dup stfld int32 Stack< !0> : : size ldelem, any !0 rat method public \nstatic void Main() .entrypoint .maxstack 3 .locals (class Stack<isiS2>) newobj void Stack<Int32>::.ctor() \nstloc.O ldloc.O 1de.14 17 call instance void Stack<int32>::Push(10) ldloc.O call instance !_0 Stack<int32>::PopO \n idc. i4 17 csq call void System.Console: :WriteLine(bool) rat  }} Figure 4: The IL for Stack and Generic \nStack The right of Figurre 4 shows the IL for the parametric Stack im- plementation on the right of Figare \n1. For comparison with the non- generic IL the differences are underlined. In brief, our changes to IL \ninvolved (a) adding some new types to the IL type system, (b) in- troducing polymorphic forms of the \n1L declarations for classes, in- terfaces, structs and methods, along with ways of referencing them, \nand (c) specifying some new instructions and generalizations of ex- isting instructions. We begin with \nthe instruction set. 3.1 Polymorphism in instructions Observe from the left side of Figure 4 that: \u00ae \nSome IL instructions are implicitly generic in the sense that they work over many types. For example, \nldarg. 1 (in Push) loads the first argument to a method onto the stack. The JIT compiler determines types \nautomatically and generates code appropriate to the type. Contrast this with the JVM, which has instruction \nvariants for different types (e.g. iload for 32-bit integers and aload for pointers). , Other IL instructions \nare generic (there's only one variant) but are followed by further information. This is required by the \nverifier, for overloading resolution, and sometimes for code generation. Examples include ldfld for field \naccess, and newarr for array creation. \u00ae A small number of IL instructions do come in different vaf- \nants for different types. Here we see the use of ldelem, re\u00a3 and stelem.ref for assignment to arrays \nof object types. Separate instructions must be used for primitive types, for ex- ample, ldelem, 24 and \nstelem, i4 for 32-bit signed integer arrays. Now compare the polymorphic IL on the right of Figure 4. \n The generic, type-less instructions remain the same.  The annotated generic instructions have types \nthat involve T  and Stack<T> instead of System. Object and Stack. No- tice how type parameters are referenced \nby number. Two new generic instructions have been used for array access and update: Idelem. any and \nstelem, any.  Two instructions deserve special attention: box and a new instruc- tion unbox.val. The \nbox instruction is followed by a value type ~-and, given a value of this type on the stack, boxes it \nto produce a heap-allocated value of type Object. We generalize this instruc- tion to accept reference \ntypes in which case the instruction acts as a no-op. We introduce a new instruction unbox, val which \nperforms the converse operation including a runtime type-check. These re- finements to boxing are particularly \nuseful when interfacing to code that uses the Object idiom for generic programming, as a value of type \nT can safely be converted to and from 0bjeet. Finally, we also generalize some existing instructions \nthat are currently limited to work over only non-reference types. For ex- ample, the instructions that \nmanipulate pointers to values (ldobj, atobj, cpobj and initobj) are generalized to accept pointers to \n references and pointers to values of variable type. 3.2 Polymorphic forms of declarations We extend \nIL class declarations to include named formal type pa- rameters. The names are optional and are only \nfor use by compilers and other tools. The extends and implements clauses of class definitions are extended \nso that they can specify instantiated types. Interface, structure and method definitions are extended \nin a similar way. At the level of IL, the signature of a polymorphic method declaration looks much the \nsame as in Generic C#. Here is a simple example: .method public static void Reverse<T>(!!O[]) { \u00b0.. } \n We distinguish between class and method type variables, the latter being written in IL assembly language \nas ! ! n. 3.3 New types We add three new ways of forming types to those supported by the CLR: 1. Instantiated \ntypes, formed by specifying a parameterized type name (class, interface or struet) and a sequence of \ntype spec- ifications for the type parameters. 2. Class type variables, numbered from left-to-right \nin the rele- vant parameterized class declaration. 3. Method type variables, numbered from left-to-right \nin the rel- evant polymorphic method declaration.  Class type variables can be used as types within \nany instance decla- ration of a class. This includes the type signatures of instance fields, and in the \nargument types, local variable types and instructions of instance methods within the parameterized class. \nThey may also be used in the specification of the superclass and implemented inter- faces of the parameterized \nclass. Method type parameters can ap- pear anywhere in the signature and body of a polymorphic method. \n 3.4 Field and method references Many IL instructions must refer to classes, interfaces, structs, fields \nand methods. When instructions such as ldfld and eallvirt re-fer to fields and methods in parameterized \nclasses, we insist that the type instantiation of the class is specified. The signature (field type or \nargument and result types for methods) must he exactly that of the definition and hence include formal \ntype parameters. The actual types can then be obtained by substituting through with the instantiation. \nThis use of formal signatures may appear surprising, but it allows the execution engine to resolve field \nand method refer- ences more quickly and to discriminate between certain signatures that would become \nequal after instantiation. References to polymorphic methods follow a similar pattern. An invocation \nof a polymorphic method is shown below: idloc art call void Array: :Reverse<int32> ( ! !0[])  Again, \nthe full type instantiation is given, this time after the name of the method, so both a class and method \ntype instantiation can be specified. The types of the arguments and result again mast match the definition \nand typically contain formal method type parameters. The actual types can then be obtained by substituting \nthrough by the method and class type instantiations. 3.5 Restrictions There are some restrictions: class \nFoo<T> extends ! 0 is not allowed, i.e. naked type variables may not be used to specify the supeerclass \nor imple- mented interfaces of a class. It is not possible to determine the methods of such a class at \nthe point of definition of such a class, a property that is both undesirable for programming (whether \na method was overridden or inherited could depend on the instantiation of the class) and difficult to \nimplement (a conventional vtable cannot be created when the class is loaded). Constraints on type parameters \n(\"where clauses\") could provide a more principled solution and this is under consideration for a future \nextension. An instruction such as xxewobj void !0: : .ctorO is out- 1awed, as is call void 10: :myMethod(). \nAgain, in the absence of any other information about the type parameter, it is not possible to check \nat the point of definition of the enclos- ing class that the class represented by ! 0 has the appropriate \nconstructor or static method. o Class type parameters may not be used in static declara- tions. For \nstatic methods, there is a workaround: simply re- parameterize the method on all the class type parameters. \nFor fields, we are considering \"per-instantiation\" static fields as a future extension. o A class is \nnot permitted to implement a parameterized inter- face at more than one instantiation. Aside from some \ntricky design choices over resolving ambiguity, currently it is diffi- cult to implement this feature \nwithout impacting the perfor- mance of all invocations of interface methods. Again, this feature is under \nconsideration as a possible extension.   Implementation The implementation of parametric polymorphism \nin programming languages has traditionally followed one of two routes: o Representation and code specialization. \nEach distinct instan- tiation of a polymorphic declaration gives rise to data repre- sentation and code \nspecific to that instantiation. For example, C++ templates are typically specialized at link-time. Alterna- \ntively, polymorphic declarations can be specialized with re- spect to representation rather than source \nlanguage type [3]. The advantage of specialization is performance, and the rel- ative ease of implementing \nof a richer feature set; the draw- backs are code explosion, lack of true separate compilation and the \nlack of dynamic linking. Representation and code sharing. A single representation is used for all instantiations \nof a parameterized type, and polymorphic code is compiled just once. Typically it is a pointer that is \nthe single representation. This is achieved ei- ther by restricting instantiations to the pointer types \nof the source language (GJ, NextGen, Eiffel, Modula-3), by box- ing all non-pointer values regardless \nof whether they are used polymorphically or not (Haskell) or by using a tagged rep- resentation scheme \nthat allows some unboxed values to be manipulated polymorphically (most implementations of ML). Clearly \nthere are benefits in code size (although extra box and unbox operations are required) but performance \nsuffers. Recent research has attempted to reduce the cost of using uniform representations through more \nsophisticated boxing strategies [10] and run-time analysis of types [9]. In our CLR implementation, we \nhave the great advantage over conventional native-code compilers that loading and compilation is performed \non demand. This means we can choose to mix-and- match specialization and sharing. In fact, we could throw \nin a bit of boxing too (to share more code) but have so far chosen not to do this on grounds of simplicity \nand performance. 4.1 Specialization and sharing Our scheme runs roughly as follows: o When the runtime \nrequires a particular instantiation of a pa- rameterized class, the loader checks to see if the instantia- \ntion is compatible with any that it has seen before; if not, then a field layout is determined and new \nvtable is created, to be shared between all compatible instantiations. The items in this vtable are entry \nstubs for the methods of the class. When these stubs are later invoked, they will generate (\"just- in-time\") \ncode to be shared for all compatible instantiations. e When compiling the invocation of a (non-virtual) \npolymor- phic method at a particular instantiation, we first check to see if we have compiled such a \ncall before for some compatible instantiation; if not, then an entry stub is generated, which will in \nturn generate code to be shared for all compatible in- stantiations. Two instantiations are compatible \nif for any parameterized class its compilation at these instantiations gives rise to identical code and \nother execution structures (e.g. field layout and GC tables), apart from the dictionaries described below \nin Section 4.4. In particu- lar, all reference types are compatible with each other, because the loader \nand JIT compiler make no distinction for the purposes of field layout or code generation. On the implementation \nfor the In- tel x86, at least, primitive types are mutually incompatible, even if they have the same \nsize (floats and ints have different parameter passing conventions). That leaves user-defined struct \ntypes, which are compatible if their layout is the same with respect to garbage collection i.e. they \nshare the same pattern of traced pointers. This dynamic approach to specialization has advantages over \na static approach: some polymorphism simply cannot be special- ized statically (polymorphic recursion, \nfirst-class polymorphism), and lazy specialization avoids wasting time and space in generating specialized \ncode that never gets executed. However, not seeing the whole program has one drawback: we do not know \nahead of time the full set of instantiations of a polymorphic definition. It turns out that if we know \nthat the code for a particular instantiation will not be shared with any other instantiation then we \ncan sometimes generate slightly better code (see \u00a74.4). At present, we use a global scheme, generating \nunshared code for primitive instantiations and possibly-shared code for the rest. The greatest challenge \nhas been to support exact run-time types and at the same time share representations and code as much \nas possible. There's a fundamental conflict between these features: on the one hand, sharing appears \nto imply no distinction between instantiations but on the other hand run-time types require it. 4.2 \nObject representation Objects in the CLR's garbage-collected heap are represented by a vtable pointer \nfollowed by the objeet's contents (e.g. fields or array elements). The vtable's main role is virtual \nmethod dispatch: it contains a code pointer for each method that is defined or inherited by the object's \nclass. But for simple class types, at least, where there is a one-to-one correspondence between vtables \nand classes, it can also be used to represent the object's type. When the vtable is used in this way \nwe call it the type's type handle. In an implementation of polymorphism based on full special- ization, \nthe notion of exact run-time type comes for free as dif- ferent instantiations of the same parameterized \ntype have different vtables. But now suppose that code is shared between different in- stantiations such \nas List<string> and List<object>. The vta- bles for the two instantiations will be identical, so we need \nsome Class Slotno. I Typeparameter oropentype we must also include the type parameters of superclasses \nin the in- D 0 T = Tree<U> stantiation information recorded in the duplicated vtable (solution D I \nSet<T[]> = set<Tree<U>[]> 2(b) above).  D 2 List<T> = List<Tree<U>> 4,4 Instanflating open type expressions \nat run-time Given we can access class type parameters, we must now consider when and where to compute \ntype handles for type expressions that involve these parameters. These handles are required by instruc- \ntions such as newobj and eastclass mentioned above. For exam- ple, the code for method D.m in Figure \n7 must construct a run-time representation for the type List<T> in order to create an object of that \ntype. For monomorphic types, or when code is fully-specialized, the answer is simple: the JIT compiler \njust computes the relevant type handle and inserts it into the code stream. A newobj operation then involves \nallocation and initialization, as normal. However, for open type expressions (those containing type \nvari- ables) within shared code, we have a problem. For example, if we use vtable pointers to represent \ntype handles (see \u00a74.2) then the implementation must perform a look-up into a global table to see if \nwe've already created the appropriate vtable; if the look-up fails, then the vtable and other structures \nmust be set up. Whatever representation of run-time types is used, the required operation is potentially \nvery expensive, involving at least a hash-table look-up. This is not acceptable for programs that allocate \nfrequently within polymorphic code. Luckily, however, it is possible to arrange things so that the computation \nof type handles is entirely avoided in \"normal\" execu- tion. As we will see later, we can achieve an \nallocation slowdown of only 10-20%, which we consider acceptable.  4.4.1 Pre-computing dictionaries \nof type handles We make the following observation: given the IL for a method within a parameterized class \nsuch as D, all the sites where type han- dles will be needed can be determined statically, and furthermore \nall the type expressions are fully known statically with respect to the type parameters in scope. In \nclass D there are two such open type expressions: Set<T [] > and List<T>; Given this information we can \npre-compute the type handles corresponding to a particular instantiation of the open types when we first \nmake a new instan- tiation of the class. This avoids the need to perform look-ups re- peatedly at run-time. \nFor example, when building the instantiation D<string>, we compute the type handles for Set<str~ng [] \n> and Lis't<string>. These type handles are stored in the vtable that acts as the unique mntime type \nfor D<string>. That is, for each open type expression, a slot is reserved in a type handle dictionary \nstored in the vtable associated with a particular instantiation. As with type parameters, these slots \nmust be inherited by sub- classes, as methods that access them might be inherited. For ex- ample, suppose \nthat m is invoked on an object of type E<int>. The method code will expect to find a slot corresponding \nto the open type List<T>, in this case storing a type handle for List<Tree<int>>. So for the example \nclass E, the information that is stored per instantiation in each the vtable has the layout shown in \nFigure 8.  4.4.2 Lazy dictionary creation In classes with many methods there might be many entries in \nthe dictionary, some never accessed because some methods are never invoked. As it is expensive to load \nnew types, in practice we fill the dictionary lazily instead of at instantiation-time. The code se- quence \nfor determining the type for List<T> in method D.m is then E 3 O E 4 O[] Figure 8: Dictionary layout \nfor example in Figure 7 vtptr = ghispgr->vt; // extract the viable ptr rtt = vtptr->dict[2]; // slog \nno. 2 in dictionary if (rtt != NULL) goto Done; // it's been filled in rtt = rtt_helper(...); // look \nit up the slow way... vtptr->dict[2] = rtt; // and update the dictionary Done: This is just a couple \nof indirections and a null-test, so hardly im- pacts the cost of allocation at all (see \u00a75). Moreover, \nlaziness is crucial in ensuring termination of the loader on examples such as class C<T> { void mO ~ \n... new C<C<T>>O ... } } which are a form of polymorphic recursion. In fact, it is even nec- essary to \nintroduce some laziness into determining a type handle for the superclass in cases such as class C<T> \n: D<C<C<T>>> { ... } which in turn means that inherited dictionary entries cannot simply be copied into \na new instantiation when it is loaded. 4.4.3 Determining dictionary layout There is some choice about \nwhen the location and specification of the open type expressions is determined: I. At source compile-time: \ncompilers targeting IL must emit declarations for all open types used within a pararneterized class definition. \n2. At load-time: when first loading a parametefized class the CLR analyses the IL of its methods. 3. \nAt JIT compile-time: the open types are recorded when com- piling IL to native code.  We regard (1) \nas an excessive burden on source-language compil- ers and an implementation detail that should not be \nexposed in the IL, (2) as too expensive in the case when there are many methods most of which never get \ncompiled, and therefore chose (3) for our implementation. The only drawback is that discovery of open \ntype expressions is now incremental, and so the dictionary layout must grow as new methods are compiled. \n4.5 Polymorphic methods So far we have considered the twin problems of (a) accessing class type parameters, \nand (b) constructing type handles from open type expressions efficiently at run-time We now consider \nthe same questions for method type parameters. As with parameterized types, full specialization is quite \nstraightforward: for each polymorphic method the JIT-eompiler maintains a table of instantiations it \nhas seen akeady, and uses this when compiling a method invocation. When the method is first called at \nan instantiation, code is generated specific to the instanti- ation. Once again for shared code things \nare more difficult. One might think that we often have enough exact type information in the pa- rameters \nto the methods themselves (for example, using err in the Slice method from Figure 3), but this is not \ntrue in all cases, and, anyway, reference values include null which lacks any type infor- mation at all. \nSo instead we solve both problems by passing dictionaries as parameters. The following example illustrates \nhow this works. class C static void m<T>(T x) ... new Set<T> ... new T[] ... .. p<List<T>>(...); ... \n ) static void p<S>(S x) { ... new Vector<T> ... I- static void Main() ~ ... m<int>(...) ... } } When \nm is invoked at type int, a dictionary of type handles con- taining \u00a3nt, Set<int> and int [] is passed \nas an extra parameter to m. The first handle is to be used for whenever T itself is required, and the \nlatter two handles are ready to be used in the new operations of the method body. Note, however, that \nmore is required: m in turn invokes a poly- morphic method p, which needs a dictionary of its own. We \nap- pear at first to have a major problem: the \"top-level\" invocation of m<int> must construct dictionaries \nfor the complete transitive closure of all polymorphic methods that could ever be called by m. However, \nlaziness again comes to the rescue: we can simply leave an empty slot in the dictionary for m<int>, filled \nin dynamically the first time that p<List<int>> is invoked. And, again, laziness is crucial to supporting \npolymorphic recursion in examples such: static void f<T>(T x) { ... f<List<T>>(...) ... } Finally we \nnote that polymorphic virtual methods are altogether more challenging. If the code for a polymorphic \nvirtual method was shared between all instantiations, as it is in non-type-exact systems such as GJ, \nthen virtual method invocations could be made in the usual way through a single slot in the vtable. However, \nbecause the caller does not know statically what method code will be called, it cannot generate an appropriate \ndictionary. Moreover, in a system like ours that generates different code for different type instanti- \nations, there is no single code pointer suitable for placement in a vtable. Instead, some kind of run-time \ntype application appears to be necessary, and that could be very expensive. We will discuss efficient \nsolutions to both of these problems in a future paper. 4.6 Verification As with the existing CLR, our \nextensions have a well-defined se- mantics independent of static type checking, but a subset can be type-checked \nautomatically by the verifier that forms part of the run-time. The parametric polymorphism of languages \nsuch as Core ML and Generic C# can be compiled into this verifiable subset; more expressive or unsafe \nforms of polymorphism, such as the co- variance of type constructors in Eiffel, might be compiled down \nto the unverifiable subset. The static typing rules for the extension capture the parametric flavour \nof polymorphism so that a polymorphic definition need only be checked once. This contrasts with C++ templates \nand extensions to Java based on specialization in the class loader [2] where every distinct instantiation \nmust be checked separately. Element type Time (seconds) -Object Poly Mono object 2.9 2.9 2.9 string 3.5 \n2.9 3.1 int 8.5 1.g 2.0 double 10.4 2.0 2.0 Point 10.5 4.3 4.3 Figure 9: Comparing Stacks Type Time (seconds) \nRuntime Lazy Specialized look-up Dictionary List<T> 4.2 288 4.9 Dict<T[],List<T>> 4.2 447 4.9 Figure \n10: Costing run-time type creation 5 Performance An important goal of our extension to the CLR was that \nthere should be no performance bar to using polymorphic code. In par- ticular, polymorphic code should \nshow a marked improvement over the 0b jeer-based generic design pattern, and if possible should be as \nfast as hand-specialized code, another design pattern that is com- mon in the Base Class Library of .NET. \nFurthermore, the presence of run-time types should not significantly impact code that makes no use of \nthem, such as that generated by compilers for ML and Haskell. Figure 9 compares three implementations \nof Stack: the C# and Generic C# programs of Figure 1 (Object and Poly) and hand- specialized variants \nfor each element type (Mono). The structure of the test code is the following: S := new stack c :--- \nconstant for m e 1... i0000 do S.push(c) m times S.poPO m times As can be seen, polymorphism provides \na significant speedup over the crude use of 0bject, especially when the elements are of value type and \nhence must be boxed on every Push and unboxed on every Pop. Moreover, the polymorphic code is as efficient \nas the hand- specialized versions. Next we attempt to measure the impact of code sharing on the performance \nof operations that instantiate open type expressions at run-time. Figure 10 presents the results of a \nmicro-benchmark: a loop containing a single newobj applied to a type containing type parameters from \nthe enclosing class. We compare the case when the code is fully specialized, when it is shared but performs \nrepeated runtime type lookups, and when it is shared and makes use of the lazy dictionary creation technique \ndescribed in \u00a74.4. The run-time lookups cause a huge slowdown whereas the dictionary technique has only \nminor impact, as we hoped. 6 Related work Parametric polymorphism and its implementation goes back a \nlong way, to the first implementations of ML and Clu. Implementations that involve both code sharing \nand exact runtime types are rare: one example is Connor's polymorphism for Napier88, where exact types \nare required to support typesafe persistence [7]. Connor re- jects the use of dynamic compilation as \ntoo slow - the widespread acceptance of JIT compilation now makes this possible. Much closer to our work \nis the extension to Java described by Viroli and Natali [16]. They live with the existing JVM but tackle \nthe combi- nation of code-sharing and exact run-time types by using reflection to manage their own type \ndescriptors and dictionaries of instanti- ated open types, which they call \"friend types\". Such friend \ntypes are constructed when a new instantiation is created at load-time; the problems of unbounded instantiation \ndiscussed in Section 4.4 are avoided by identifying a necessary and sufficient condition that is used \nto reject such unruly programs. Our use of laziness is superior, we believe, as it avoids this restriction \n(polymorphic recursion can be a useful programming technique) and at the same time reduces the number \nof classes that are loaded. In a companion technical report [17] the authors discuss the implementation \nof polymorphic methods using a technique similar to the dictionary passing of Sec- tion 4.5. The observation \nthat the pre-eomputation of dictionaries of types can be used to avoid run-time type construction has \nalso been made by Minamide in the context of tagless garbage collec- tion for polymorphic languages [12]. \nOther proposals for polymorphism in Java have certainly helped to inspire our work. However, the design \nand implementation of these systems differ substantially from our own, primarily because of the pragmatic \ndifficulty of changing the design of the JVM. Age- sen, Freund and Mitchell's [2] uses full specialization \nby a modi- fied JVM class loader, but implements no code sharing. The PolyJ team [ 13] made trial modifications \nto a JVM, though have not pub- lished details on this. GJ [4] is based on type-erasure to a uni- form \nrepresentation, and as a result the expressiveness of the system when revealed in the source language \nis somewhat limited: instan- tiations are not permitted at non-reference types, and the runtime types \nof objects are \"non-exact\" when accessed via reflection, cast- ing or viewed in a debugger. Furthermore, \nthe lack of runtime types means natural operations such as new T [] for a type parameter 'r are not allowed, \nas Java arrays must have full runtime type infor- mation attached. NextGen [6] passes runtime types via \na rather complex translation scheme, but does not admit instantiations at non-reference types. Pizza \n[14] supports primitive type instantia- tions but implements this by boxing values on the heap so incurring \nsignificant overhead. Conclusion This paper has described the design and implementation of support for \nparametric polymorphism in the CLR. The system we have cho- sen is very expressive, and this means that \nit should provide a suit- able basis for language inter-operability over polymorphic code. Previously, \nthe code generated for one polymorphic language could not be understood by another, even when there was \nno real reason for this to be the case. For example, it is now easy to add the \"con- sumption\" of polymorphic \ncode (i.e. using instantiated types and calling polymorphic methods) to a CLR implementation of a lan- \nguage such as Visual Basic. Potential avenues for future investigation include the following: * Many \nsystems of polymorphism permit type parameters to be \"constrained\" in some way. F-bounded polymorphism \n[5] is simple to implement given the primitives we have described in this paper, and, if dynamic type \nchecking is used at call- sites, does not even require further verification type-checking rules. Operationally, \nwe believe that many other constraint mechanisms can be implemented by utilizing the essence of the dictionary-passing \nscheme described in \u00a74, i.e. by lazily creating dictionaries that record the essential information that \nrecords how a type satisfies a constraint. o Our polymorphie IL does not support the full set of operations \npossible for analysing instantiated types at runtime: for exam- pie, given an object known to be of type \nList<T> for some unknown r, the type 7- cannot be determined except via the CLR Reflection library. Instructions \ncould be added to permit this, though this might require further code generation. \u00ae Some systems of polymorphism \ninclude variance in type parameters, some safely, and some unsafely (e.g. Eiffel). Adding type-unsafe \n(or runtime type-checked) variance is clearly not a step to be taken lightly, and no source languages \ncurrently require type-safe variance. * The absence of higher-kinded type parameters makes com- piling \nthe module system of SML and the higher-kinded type abstraction of Haskell difficult. We plan on experimenting \nwith their addition. * The presence of runtime types for all objects of constructed type is objectionable \nfor languages that do not require them, even with careful avoidance of overheads. A refined type sys- \ntem that permits their omission in some cases may be of great value.  It would also be desirable to \nformalize the type system of polymor- phic IL, for example by extending Baby IL [8]. With regard to implementation, \nthe technique of \u00a74.4 is abso- lutely crucial to ensure the efficiency of polymorphic code in the presence \nof runtime types. In effect, the computations of the han- dles for the polymorphic types and call sites \nthat occur within the scope of a type variable are lifted to the point where instantiation occurs. Making \nthis computation lazy is essential to ensure the ef- ficient operation on a virtual machine. In the future \nwe would like to investigate a wider range of implementation techniques, and in particular obtain performance \nmeasurements for realistic polymorphic programs. The tradeoffs in practice between specialization and \ncode sharing are only begin- ning to be properly understood (see [15, 2, 3] for some preliminary results). \nWe have deliberately chosen a design and implementation strategy that allows flexibility on this point. \nAcknowledgements We would like to thank the anonymous referees for their construc- tive remarks. Also \nwe are grateful to Jim Miller, Vance Moirison and other members of the CLR development team, Anders Hejls- \nberg, Peter HaUam and Peter Golde from the C# team, and our colleagues Nick Benton, Simon Peyton Jones \nand others from the Programming Principles and Tools group in Cambridge. References [1] The .NET Common \nLanguage Runtime. See website at http: / ]msd.ll. microsoft, eom/net 1. [2] O. Agesen, S. Freund, and \nJ. C. Mitchell. Adding parameterized types to Java. In Object-Oriented Programming: Systems, Languages, \nAp- plications (OOPSLA), pages 215-230. ACM, 1997. [3] P.N. Benton, A. J. Kennedy, and G. Russell. Compiling \nStandard ML m Java bytecodes. In 3rd ACM SIGPLAN International Conference on Functional Programming, \nSeptember 1998. [4] Gilad Bracha, Martin Odersky, David Stoutamire, and Philip Wadler. Making the future \nsafe for the past: Adding generieity to the Java programming language. In Object-Oriented Programming: \nSystems, Languages, Applications (OOPSLA ). ACM, October 1998. [5] Peter S. Canning, William R. Cook, \nWalter L. Hill, John C. Mitchell, and William Olthoff. F-bounded quantification for object-oriented programming. \nIn Conference on Functional Programming Languages and Computer Architecture, t989. [6] R. Cartwright \nand G. L. Steele. Compatible genericity with run-time types for the Java programming language. In Object-Oriented \nPro- gramming: Systems, Languages, Applications (OOPSLA), Vancouver, October 1998. ACM. [7] R.C.H. Connor. \nTypes and Polyvnorphism in Persistent Programming Systems. PhD thesis, University of St. Andrews, 1990. \n[8] A. Gordon and D. Syme. Typing a multi-language intermediate code. In 27th Annual ACM Symposium on \nPrinciples of Programming Lan- guages, January 2001. [9] R. Harper and G. Morrisett. Compiling polymorphism \nusing inten- sional type analysis. In 22nd Annual ACM Symposium on Principles of Programming Languages, \nJanuary 1995. [10] X. Leroy. Unboxed objects and polymorphic typing. In 19th Annual ACM Symposium on \nPrinciples of Programming Languages, pages 177-188, 1992. [11] T. Lindholm and F. Yellin. The Java ~rtual \nMachine Specification. Addison-Wesley, second edition, 1999. [12] Y. Minamide. Full lifting of type parameters. \nTechnical report, RIMS, Kyoto University, 1997. [13] A. Myers, J. Bank, and B. Liskov. Parameterized \ntypes for Java. In 24th Annual ACM Symposium on Principles of Programming Lan- guages, pages 132-145, \nJanuary 1997. [14] M. Odersky, P. Wadler, G. Braeha, and D. Stoutamire. Pizza into Java: Translating \ntheory into practice. In ACM Symposium on Principles of Programming Languages, pages 146-159. ACM, 1997. \n[15] Martin Odersky, Enno Runne, and Philip Wadler. Two Ways to Bake Your Pizza - Translating Parametedsed \nTypes into Java. Technical Report CIS-97-016, University of South Australia, 1997. [16] M. Viroli and \nA. Natali. Parametric polymorphism in Java: an ap- proach to translation based on reflective features. \nIn Conference on Object-Oriented Programming, Systems, Languages and Applications (OOPSLA). ACM, October \n2000. [17] M. Viroli and A. Natali. Parametric polymorphism in Java through the homogeneous translation \nLM: Gathering type descriptors at load- time. Technical Report DEIS-LIA-00-001, Universit~t degli Studi \ndi Bologna, April 2000.   \n\t\t\t", "proc_id": "378795", "abstract": "<p>The Microsoft.NET Common Language Runtime provides a shared type system, intermediate language and dynamic execution environment for the implementation and inter-operation of multiple source languages. In this paper we extend it with direct support for parametric polymorphism (also known as generics), describing the design through examples written in an extended version of the C# programming language, and explaining aspects of implementation by reference to a prototype extension to the runtime.</p><p>Our design is very expressive, supporting parameterized types, polymorphic static, instance and virtual methods, &#8220;F-bounded&#8221; type parameters, instantiation at pointer and value types, polymorphic recursion, and exact run-time types. The implementation takes advantage of the dynamic nature of the runtime, performing just-in-time type specialization, representation-based code sharing and novel techniques for efficient creation and use of run-time types.</p><p>Early performance results are encouraging and suggest that programmers will not need to pay an overhead for using generics, achieving performance almost matching hand-specialized code.</p>", "authors": [{"name": "Andrew Kennedy", "author_profile_id": "81100450709", "affiliation": "Microsoft Research, Cambridge, U.K.", "person_id": "PP14158369", "email_address": "", "orcid_id": ""}, {"name": "Don Syme", "author_profile_id": "81100612953", "affiliation": "Microsoft Research, Cambridge, U.K.", "person_id": "P68181", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/378795.378797", "year": "2001", "article_id": "378797", "conference": "PLDI", "title": "Design and implementation of generics for the .NET Common language runtime", "url": "http://dl.acm.org/citation.cfm?id=378797"}