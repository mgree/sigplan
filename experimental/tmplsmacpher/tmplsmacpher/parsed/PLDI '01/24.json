{"article_publication_date": "05-01-2001", "fulltext": "\n Dynamic Variables David R. Hanson and Todd A. Proebsting Microsoft Research 1 Microsoft Way Redmond, \nWA 98052 {drh,toddpro} @ microsoft.corn ABSTRACT Most programming languages use static scope rules for \nassociating uses of identifiers with their declarations. Static scope helps catch errors at compile time, \nand it can be implemented efficiently. Some popular languages--Perl, Tcl, TeX, and Postscript--offer \ndynamic scope, because dynamic scope works well for variables that \"customize\" the execution environment, \nfor example. Programmers must simulate dynamic scope to implement this kind of usage in statically scoped \nlanguages. This paper describes the design and implementation of imperative language constructs for introducing \nand referencing dynamically scoped variables--dynamic variables for short. The design is a minimalist \none, because dynamic variables are best used sparingly, much like exceptions. The facility does, however, \ncater to the typical uses for dynamic scope, and it provides a cleaner mechanism for so-called thread-local \nvariables. A particularly simple implementation suffices for languages without exception handling. For \nlanguages with exception handling, a more efficient implementation builds on existing compiler infrastructure. \nException handling can be viewed as a control construct with dynamic scope. Likewise, dynamic variables \nare a data construct with dynamic scope. Categories and Subject Descriptors D.3.3 [Programming Languages]: \nLanguage Constructs and Features---dynamic scope, control structures. General Terms Languages. 1. INTRODUCTION \nNearly all modern programming languages use static (or lexical) scope rules for determining variable \nbindings. Static scope can be implemented very efficiently and makes programs easier to understand. Dynamic \nscope is usually associated with \"older\" languages; notable examples include Lisp, SNOBOL4, and APL. \nDespite the prevalence of static scope, several widely used \"newer\" languages-- PostScript, Tcl, TeX, \nand Perl---either use dynamic scope Permission to make digital or hard COllies of all or part of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor profit or commercial advan= rage and that copies bear this notice and the full citation on the first \npage, To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspecific permission and/or a fee. PLD/2001 6/01 Snowbird, Utah, USA &#38;#169; 2001 ACM ISBN 1-58113-414-2/01/06.,.$5.00 \n or include features that amount to dynamic scope. For example, Tcl's upvar can be used to access local \nvariables in other active procedures, and Perl supports both static and dynamic scope on a per-variable \nbasis. Shell environment variables are another example. Admittedly, these languages are used mostly in \nspecific application domains, but their wide use suggests that dynamic scope is a useful facility. Dynamic \nscope is particularly useful in applications and libraries that operate in a customizable \"environment.\" \nFull- featured GUI packages are good examples; they support a plethora of style and presentation features, \nwhich must be programmed in some way. Even many \"ordinary\" applications have some of these characteristics; \nfor example, compilers often have options that permit clients to specify target machines, compiler passes, \nlinker options, etc., and compilers in integrated development environments typically have more options, \nbecause their \"clients\" are other programs. Without dynamic scope, programming these kinds of features \nmust be done explicitly in terms of the implementation language. Usually, objects are loaded up with \nfields or methods that implement these features, or methods and functions have zillions of parameters \n(perhaps with defaults). Indeed, an argument for optional, keyword- style formal parameters with defaults \nis that they cater to this kind of programming problem. In extreme cases, programmers move this boatload \nof fields or parameters into an object, which is passed as a parameter nearly everywhere. Methods are \ncalled to read, write, save, and restore these data. This popular approach usually simplifies the code \nconsiderably. It is also tantamount to implementing a set of variables with dynamic scope. These unstructured, \nad hoc approaches, which are often not type-safe and perhaps inefficient, can be replaced with equivalent \nlanguage constructs that are structured, type- safe, and efficient. For example, Lewis et al. [6] extended \na statically scoped functional language with \"implicit parameters,\" which are dynamically scoped variables, \nand syntax for binding them and referencing them. The remainder of this paper describes new imperative \nlanguage constructs for supporting variables with dynamic scope. Like exception handling, such \"dynamic \nvariables\" are best used sparingly, and the particularly simple design caters to this anticipated usage \npattern. The simple implementation described below is probably sufficient for most languages, but a more \nsophisticated--and hence more efficient--implementation builds on the existing implementations of exception \nhandling. Consequently, compilers for modern languages already have most of the infrastructure necessary \nfor implementing dynamic variables. 2. DESIGN Only two statements are necessary to add dynamic variables \nto most languages--one that creates and initializes a dynamic variable and one that binds a local variable \nto an instance of a dynamic variable. The see statement instantiates and initializes a dynamic variable: \nset/d : T=einS The dynamic variable id is instantiated and initialized to the value of the expression, \ne, and the statement S is executed. The /d is visible within S and may be read and written within S, \nThe type of e must be a subtype of T or be assignable to identifiers of type T. The lifetime and scope \nof /d is restricted to S; when S terminates, the dynamic variable is destroyed. The use statement accesses \na dynamic variable introduced by a see statement: use/d: TinS The local variable id is bound by reference \nto a dynamic variable and the statement S is executed. The scope of the local introduced by the use statement \nis restricted to S, and /d may be read and written within S. The local id is bound to the most recently \ncreated dynamic variable /dk with the lexographically identical name and for which Tk is a subtype of \nT. In Java terms, id is bound to the first idk such that \"/d\" = \"idk\" and idkinstaneeof T is true. For \nexample: set x:T3 = ... in set x:T2 .... in { \u00b0o. use x:Tl in { ... } } \u00b0\u00b0\u00b0 If T2 and T3 are both \nsubtypes of T1 in these fragments, the x in the use statement is bound to the x:T2 in the nested see \nstatement. If an appropriate dynamic variable is not found, the exception VariableNoeFound is raised. \nIn languages without exceptions, a runtime error occurs. Types are considered in binding the identifier \nin a use statement because doing so is perhaps the only way to insure that existing code continues to \nwork in a component- based system. If the lookup was based only on lexographical names, programmers would \nhave to know the names of all of the other dynamic variables in a program, which is impossible when software \nis constructed from components. Likewise, if the lookup was based on exact types, instead of subtypes, \nexisting use statements could not cope with new code in which the identifiers in sot statements have \nbeen extended by declaring them subtypes of the original types. A single set statement can contain multiple \ndeclarations, which are evaluated sequentially. That is, set idl : Ti =el, id2 : T2 = e2 ..... idn : \nTn = e, in S is equivalent to set idl : T1 = el in set id2 : T2 = e2 in set.., in setidn : Tn = en in \nS Likewise, use idl : Ti, id2 : T2 ..... idm : TrainS is equivalent to use idl : T1 in use id2 : T2 \nin use ...in use idm : Tm in S 3. APPLICATIONS Dynamic variables can replace the plethora of extra parameters \nand globals that often appear in, for example, GUI applications. Dynamic variables cut considerable verbiage \nin functions that use a few or none of them, because only those that are actually used appear in use \nstatements. Compiling loops and switch statements in recursive-descent compilers or in abstract-syntax \ntree traversals provide another simple example. Typical implementations associate a label or other handle \nwith each loop and switch statement, and these data are used in compiling loop and switch exits, such \nas break and continue statements. Parameters for these data are passed to every recursive function in \nwhich the exit constructs can occur. For example, in lcc [2], every statement parsing function has three \nextra parameters, and most of those functions simply pass these parameters along to the functions they \ncall. Using dynamic variables for these data eliminated these parameters and confined the use of these \ndata to the functions for break and continue statements, and for case labels. Dynamic variables were \nconceived during the design and implementation of Minicon, a new object-oriented, C++ implementation \nof the Icon programming language [4]. Minicon is based on Jcon, an interpretative implementation Lewis \net al. [6] provide a compelling survey on ho of Icon written in Java [9]. Icon includes numerous built-in \ndynamic scope helps make functional programs more clea keywords and functions that access implementation \nand concise. information. For example, Icon's string-scanning facilities uses the keywords &#38;subject \nand &#38;pos hold the current string being examined and the position in that string; built- in functions \nread and write these values. Likewise, &#38;input, &#38;output, and &#38;errout give the default files \nfor the built-in i/O functions. The initial implementation of Minicon used ad hoc techniques for accessing \nkeywords and other internal values. The most prevalent technique passed an \"environment\" pointer to those \nbuilt~in functions that accessed keywords. Environments held a table of keywords and a cache of the values \nof a few frequently accessed keywords. This approach is essentially an implementation of dynamic scope. \nThe result wasn't pretty: There were two ways to call built-in functions--with and without the environment \npointer--so two data types represented built- ins. C++ derived classes and virtual functions hid some \nof this verbiage, but the clutter made the implementation harder to understand and to modify and complicated \nthe interpreter. These techniques were replaced with dynamic variables, using macros based on those shown \nin Appendix B. This change eliminated excess parameters to many functions and many lines of code, and \nthus cleaned up much of the original mess. Dynamic variables simplified the implementation and representation \nof built-in functions by reducing the number of ways to call them and the number of data types. The result \nis, unsurprisingly, easier to understand and to modify, and the performance cost is miniscule. Perhaps \na more important use of dynamic variables is in multithreaded applications. Languages and systems that \nsupport threads often provide mechanisms for specifying per-thread global variables; that is, variables \nwith global scope and with lifetimes associated with the lifetimes of individual threads. These facilities \nusually require operating-system support. For example, the Microsoft C/C++ compiler supports \"thread-local\" \nglobal variables by making the appropriate Windows system calls. Unfortunately, this facility doesn't \nwork properly in libraries that are loaded at runtime--thread-local variables are not initialized properly[10]. \nFor those libraries, programmers must call the Windows system calls explicitly. Dynamic variables are \nallocated on the stack, so they are automatically \"thread-local,\" assuming a thread-safe implementation. \nSpecifying them in a set statement in the thread's initial function makes them available (via use statements) \nto all functions called in the same thread, as if they were global variables.  4. IMPLEMENTATION TECHNIQUES \nFor languages without exception handling, a simph reasonably efficient, implementation of dynamic variabk \ncan be used. This implementation is also an operation~ semantics for the set and use statements. For \nlanguage that must support exception handling, the implementation c dynamic variables can use much of \nthe existing exceptio handling implementation infrastructure. 4.1 SimpleImplementation Conceptually, \nthe set statement creates a set of dynamiq variables and pushes it onto a global stack of such sets Reaching \nthe end of the set statement pops the stack. The use statement searches the sets from the top of the \nstac[ down for each identifier listed in use statement and, if th~ identifier is found, stores the address \nof the dynamk variable in a local variable of the same name. Within the use statement, references to \ndynamic variables are compiled into the appropriate indirections. The set statement can be implemented \nwith no heap allocation overhead, because all of the allocations can be done at compile time as local \nvariables. The stack of sets is simply a list of structures defined by the following pseudo- C code, \none for each dynamic variable. struct dVariable { struct dVariable *link; const char *name; Type * \ntype;  void *address; } *current = O; dVariable instances are linked via the link field, the name field \npoints to the name of the variable, the type field points to a type descriptor sufficient for testing \nthe subtype relation, and the address field holds the address of the variable. A per-thread global variable, \ncurrent, points to the head of the list of dVariables. The set statement set idl : Tl=el, id2 : T2=e2 \n..... idn : Tn=eninS is translated into the following compiler-generated code. The set statement declares \na local variable for each listed id, and \"pushes\" a dVariable onto the current list for each id. After \nexecuting S, the previous value of current is restored from the link field of the first listed id. In \nthe code below, typeof (T) returns a pointer to the type descriptor for T, and compiler-generated names \nare used where necessary. ~z ~ \u00b8 ~ {  T1 idl = el; } S struct dVariable dVar_/dl; dVar_idl,name = \"id/'; \ndVar idl.type = typeof (idl) ; dVar_ idl.address = &#38;idl; dVar_ idl.link = current; current = &#38;dVar \nid 1; T2 id2 = e2; struct dVariable dVar id2; dVar_ ide. name = \"/de\" ; dVar_ id2.type = typeof (id2) \n; dVar_id2.address = &#38;id2; dVar_ id2.link = &#38;dVar idl; current = &#38;dVar id2; T,~ idn = e,; \nstruct dVariable dVar_id.; dVar_idn.name = \"id.\" ; dVar_idn.type = typeof (id.) ; dVar_id..address = \n&#38;idn; dVar idn.link = &#38;dVar_id. 1; current = &#38;dVar idn; S current = dVar id1.1ink;  } \nAll the variables declared in the generated code fragment above are locals, so their storage is allocated \nas part of the stack frame. On 32-bit machines, the stack space overhead is 16n bytes for a set statement \nwith n identifiers, not counting the string space for the identifier names. The only runtime overhead \nis the code for fetching current and its fields and the assignments to the fields and to current. A similar \ntechnique is used to build the \"shadow stack\" in the debugger cdb [5]. The use statement generates a \nlinear search starting at current. The use statement declares a local variable for each listed /d and \nsearches for the appropriate dynamic variable instantiated by a set statement. The generated code below \nassumes that strings for the identifier names have been \"internalized\" so that there is only one copy \nof each distinct string. Consequently, two strings are identical if their addresses are equal. Internalization \ncan be done at program startup or even at link time. The statement use id~ : T1, id2 : T2 ..... idm : \nTm in S is translated into the following compiler-generated code: TI *idl = dSearch(\"id1\", typeof (Ti) \n) ; T2 *id2 = dSearch(\"id2\", typeof (T2)); ... Tm *idm = dSearch(,idm\", typeof (Tm) ) ;  where dSearch \nis: void *dSearch(const char *name, Type *type) { struct dVariable *p = current; for ( ; p I= 0; p = \np->link) if (p->name == name &#38;&#38; type is a subtype of p- >type) return p- >address ; raise VariableNotFound; \n } By design, the search occurs only once for each identifier at the entry to the use statement; multiple \nreferences to the idk in S are compiled into simple indirections through the pointers shown in the generated \ncode above. This simple implementation is easily encoded into macros or function calls that provide most \nof the benefits of dynamic variables in existing languages, such as C and C++. As an example, Appendix \nB gives macro definitions for use in C++; these are used in the Minicon implementation, as detailed above. \nMore sophisticated search techniques could be used, but they would undoubtedly require more sophisticated \ndata structures and thus complicate code generation for both set and use statements. For example, hashing \nwould make use statements more efficient at the expense of set statements. Hashing identifiers can be \ndone at compile time, so the dVariables could be linked onto the heads of the appropriate chains with \ncode as simple as shown above, assuming the size of the hash table is also known at compile time. At \nthe end of the set statement, however, the dVariables must be unlinked from the chains in the reverse \norder in which they were linked. The compiler could determine which dVariables were the first ones linked \nonto each chain and unlink just those, but the savings achieved by this extra effort is likely to be \nsmall. Appendix C gives details. Another possible improvement that maintains the simplicity of the implementation \nabove is to replace the calls to dSearch with code that searches for all m identifiers at once. Of course, \nthe asymptotic complexity of these improvements is no better than the simpler implementation, but the \naverage case might be improved, because failing searches are unexpected. 4.2 Novel Implementation Most \nmodern programming languages include exception- handling facilities, and typical implementations of these \nfacilities are quite efficient. The runtime overhead when exceptions are not raised is small or zero; \nit's all in the generated code and runtime library code that is executed 267 when an exception occurs, \nunder the assumption that exceptions are rare [1]. Dynamic variables can enjoy similar efficiencies by \nextending the mechanisms that a compiler for a modem language already implements. Java uses a typical \nimplementation of exception handling[7]. The Java compiler emits exception tables, which allow the runtime \nsystem to identify exception-handling code when an exception occurs. These tables take only space. For \nexample, given the tMlowing Java code, taken from Reference [7], void CatchOne () { try { tryltOut () \n; } catch (TestExc e) { handleExc (e) ; } }  the compiler emits four items into an exception table \nentry: the starting and ending locations in the generated code for the try statement, the location of \nthe exception handler, which is the call to handloNxc in this example, and the type of the exception, \ni.e., TestExc above. Armed with the current location counter and the stack, the runtime system interprets \nthese tables to find the appropriate exception handler, unwinding the stack as it goes, and transfers \ncontrol to that handler. Dynamic variables need similar data: the boundaries of a set statement, and \nthe types, names, and addresses of the local identifiers. The table entries could thus be defined by: \nstruct dEntry { void *from; //start of PC range void *to; //end of PC range Type *type; //declared type \nconst char *name; int offset; //local'S frame pointer offset } The from and to fields give the boundaries \nof the relevant part of the set statement. The type and name fields are the same as the dVariable fields \nby the same names. The offset field is the offset from the frame pointer or other known location in a \nstack frame to the local variable idk and is used to compute the runtime address of idk. For the sake \nof explanation, the following structure models stack frames. struct frame { struct frame *caller; void \n*retaddress; struct dEntry ~dTable; } Given a frame for a function, the caller field points to the \ncaller's frame, retaddress holds the return address in the function's caller, and dTable points to an \narray of dEntry structures that identify the set statements in that function; this array is terminated \nwith a from value equal to zero. Addresses of local variables are computed by adding their offsets to \nthe address of the frame. In actual implementations, the dTable field would more likely be computed directly \nfrom the location counter instead of being stored in the frame in order to avoid the initialization costs. \nThe salient detail is that the appropriate table can be found given frame pointer and an address in the \ncorresponding function. Similar comments apply to finding exception tables. The set statement is compiled \ninto a sequence of n assignments and n table entries as shown in Figure 1. As for exception-handling \ntables, the entries in the dynamic variable tables must be ordered so that the location counter ranges \nfor nested set statements appear first. dLookup searches the dynamic variable tables in the call stack \nfor the first variable of a given name and type. The function _self returns a pointer to the caller's \nstack frame. start1: 71 T2 idl id2 = = el; e2; f rom startl s tart2 to end end type Ti T2 name \" idt\" \n\" id2 \" offset offseh offset2 .., .\u00b0. start../: startn: end: } T., S id. -- e.; s tart. 0 end T. \" id.\" \noffset. Figure 1. Generated code and table for the set statement. void *dLookup(const char *name, Type \n*type) { struct frame *fp = _self()->caller; void *pc = self()->retaddress; for ( ; fp l= 0; pc --fp->retaddress, \n fp = fp->caller) if (fp->dTable I= 0) { struct dEntry *p = fp->dTable; for ( ; p->from I= 0; p++) \nif ( pc >= p->from &#38;&#38; pc < p->to &#38;&#38; p->name == name &#38;&#38; type is a subtype of p->type) \nreturn (char *)fp + p->offset; }  raise VariableNotFound;  } The use statement is compiled into code \nthat is nearly identical to the code shown in previous subsection, except that dSearoh is replaced by \ndLookup: ( T~ *idl = dLookup(\"idl\", typeof (T~)); T2 *id2 = dLookup(\"id2\", typeof (T2)) ; ..o Tm *idm \n= dLookup(\"idm\", typeof (Tin)) ; S  } Implementing dynamic variables is actually simpler than implementing \nexception handling, because there are no control-flow or stack unwinding issues, which simplifies optimizations \nand debugging. 5. DISCUSSION Much prejudice against dynamic scoping can be traced back to early LISP \nsystems, which suffered from slow implementations and the \"downward funarg problem\" [ 11 ]. As we've \nshown, dynamic variables need not be slow, and the funarg problem is not at issue, because it affects \nonly languages with closures. Lewis et al. [6] give a compelling description of the benefits of dynamic \nscoping in a functional language. They do so by implementing dynamic variables as implicit parameters, \nwhich can be inferred from the underlying code. To infer implicit parameters, they rely on lexically \ndistinguishing implicit parameters from ordinary identifiers. Our work differs from theirs in several \nways. We propose alternative implementation strategies to provide implicit parameters. Our scheme does \nnot rely on any inference mechanism to determine where dynamic variables might be needed. While their \nscheme nicely reflects their functional- language infrastructure, our scheme's dynamic variable binding \nmechanism reflects the common exception-handling facilities found in object'oriented languages. After \nthe use of implicit parameters improved samples of code, they assert, \"The resulting code is of a conciseness \nthat is difficult to achieve when working in C or C++\" (page 115). We agree completely, which is why \nwe propose our dynamic scoping extensions for exactly that class of languages. Lewis et al. [6] avoid \nthe thorny issue of failing searches by doing whole-program analysis to detect missing dynamic variables \nat compile time. While this approach is strictly more robust than ours, and it is certainly possible \nto use in an imperative language, it is impractical for component- based software where the entire program \nis not available-- and cannot ever be known. The set and use statements are--intentionally--a minimal \nfacility. They are ideal for the relatively infrequent use of dynamic variables, which, like exceptions, \nare best used in small doses. Reasonable improvements and enhancements are easy to imagine, but it is \ndifficult to test their ultimate value. For example, separate variables seem superfluous when there's \nalready a suitable local. Thus set M in S could abbreviate the idiom /d: T; ...; set/d: T=idinS Likewise, \nname conflicts might plague use statements, particularly in component-based software. Specifying an alias \ncould solve this problem: use id : Tas/d\" inS Within S, the dynamic variable id would be accessed as \nid\" in order to avoid conflicts with other uses of id in the context in which the use statement appears. \nThis extension is similar to the way Modula-3 provides aliases for imported interfaces [8]. Finally, \nobject-oriented languages usually include predicates for testing types, e.g., Java's instanceof, which \nsuggests that a similar predicate for testing the existence of dynamic variables might be useful, e.g., \nisdynamic (id, T) would return true if id is a dynamic variable of type T or a subtype of T and false \notherwise. Dealing with failing searches in use statements is perhaps the weak point of our design. Such \nfailures are bound to occur, and robust software components must anticipate and cope with them. Catching \nthe VariableNotFound exception and isdynamic are the only mechanisms in the present design for dealing \nwith missing dynamic variables. For example, suppose a code generator is to emit its output to the Stream \ngiven by the value of the dynamic variable output. If output isn't defined, the output should go to the \ndefault stream, stdout. Writing this code with either isdynamic or a Cry-catch statement is awkward: \n269 if (isdynamic(output, Stream))  use output: Stream in S else {  output: Stream = stdout; S } \n or try {  use output: Stream in S } catch (VariableNotFound) { output: Stream = stdout; S } It is, \nof course, possible to avoid the duplication of S by using additional variables, but the result remains \nawkward at best. The more important issue is whether or not to instantiate output as a dynamic variable \nwhen the search fails. The code fragments above do not; the following fragment does: try { use output: \nStream in S } catch (VariableNotFound) { set output: Stream = stdout in S } Missing dynamic variables \nmay occur frequently enough to warrant syntactic support. One possibility is to extend the use statement \nwith a default expression: use id : T -- default in S If /d is found, default is ignored and S is executed; \notherwise, /d is initialized to default and S is executed. There two possible implementations of this \nform of use corresponding to the alternatives described above: try { use id : T in S } catch (VariableNotFound) \n{ id : T = default; S } or try { use /d : T in S } catch (VariableNotFound) { set id: T = default in \nS } The second alternative has the surprising effect of making a use statement behave as a set statement \nwhen the search fails. And both alternatives complicate considerably the implementations described in \nSection 4. Much more experience with dynamic variables would help solidify a final design. Judging by \nthe significant new languages introduced in the past decade, the programming language community has embraced \nexception handling as a mandatory language feature, because it helps build reliable and adaptable software. \nIt is curious that only a control construct based on dynamic \"scope\" has found wide acceptance. Dynamic \nvariables are a data construct with similar semantics, and, if incorporated into modern languages, the \nmight prove to be a similarly important language facility. 6. REFERENCES [1] Chase, D. R. Implementation \nof exception handling, Part I. The Journal of C Language Translation 5 (4), 229--40. June t994. [2] Fraser, \nC. W. and D. R. Hanson. A Retargetable C Compiler: Design and Implementation. Menlo Park, Calif.: Addison- \nWesley. 1995. [3] Gosling, J., B. Joy, G. Steele, and G. Braeha. The Java Language Specification (second \nedition). Boston: Addison- Wesley. 2000. [4] Griswold, R. E. and M. T. Griswold. The Icon Programming \nLanguage (third edition). San Jose, Calif.: Peer-to-Peer Communications. 1997. www.cs.arizona.edu/iconL \n[5] Hanson, D. R. and M. Raghavachafi. A machine-independent debugger. Software Practice and Experience \n26 (11), 1277-99. Nov. 1996. www.research.microsoft.com/~drh/pubs/cdb.pdf. [6] Lewis, J. R., M. B. Shields, \nE. Meijer, and J. Launchbury. Implicit parameters: dynamic scoping with static types. Conference Record \nof the 27 th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages, Boston, 108-18. Jan., \n2000. www.acm.org/pubs/articles/proceedings/plan/325694/p 108- lewis/p108-1ewis.pdf. [7] Lindholm, T. \nand F. Yellin. The Java Virtual Machine Specification (second edition). Palo Alto, Calif.: Addison- Wesley. \n1999. [8] Nelson, G. Systems Programming with Modula-3. Englewood Cliffs, N.J.: Prentice-Hall. 1991. \nwww.research.eompaq.com/S RC/m3defrdhtml/m3 .htrnl. [9] Proebsting, T. A. and G. M. Townsend. A new implementation \nof the Icon language. Software--Practice and Experience 30 (8), 925-72. July 2000. www.cs.arizona.edu/icon/jcon/impl.pdf. \n[10] Richter, J. Advanced Windows (third edition). Redmond, Wash.: Microsoft Press. 1997. [ l 1] Scott, \nM. L. Programming Language Pragmatics. San Francisco: Morgan Kaufmann. 2000 APPENDIX A: SYNTAX SPECIFICATION \n statement: set def { ,def } in statement use ref { , ref } in statement def: identifier : type = expression \nre~ identifier : type APPENDIX B: C++ MACROS Listed below are macros that use the simple implementation \ntechnique to provide dynamic variables of pointers to class types in C++. Typical usage is: BEGiN SET \nSET(MI, el) ; ...; SET(id,, en) ; S END_SET; BEGIN_USE USE (idl, ~) ; ...; USE (idm, Tm) ; S END_USE; \n Where the Tk are pointers to class types; dynamic casts are used to test subtypes. In USE (id, T), if \nid is not found, id is set to O. In ~e code below, Atom: :New internalizes swings at program startup; \ni.e., it returns a pointer to the unique copy of its argument s~ing. This code is available at: ~p:Hfip.rese~ch.microso~.com/Users/dr~dynamic.h \nand fip:Hfip.rese~ch.microso~.com/Users/drh/dynamic.cpp. class dVariable {  public: const char *name; \nvoid *value; clasp dVariable *link; dVariable(const char *name, void *value, class dVariable *link) : \n name(name), value(value), link(link) {} ; class dEnviroament { private: class dEnvironment *prev; \n public: static class dEnvironment *current; class dVariable *vars; dEnvironment() : vars(current->vars), \nprey(current) { current = this; } ~dEnvironment() { current = prey; } }; #define BEGIN_SET do { class \ndEnvironment dEnv;  #define SET(id,e) \\ static const char *_name_##id = Atom::New(#id); \\ class dVariable \ndvar_##id(_name_##id, e,dEnvironment::current->vars); \\ dEnvironment::current->vars = &#38; dvar ##id \n #define END_SET } while (0) #define BEGIN USE do {  #define USE(id,T) \\ Tid= 0; \\  do{ \\  const \nchar * name = Atom: :New(#id); \\ class dVariable * p = dEnvironment: :current->vars; \\ for ( ; __p; \n_9 = _9->link) \\ if ( p->name == _name \\ &#38;&#38; (id --dynamic_cast<T> (static_cast<T> (p->value) \n) )) \\ break; \\ } while (0)  #define END_USE } while (0) 271 This implementation is slightly different \nthan the one described in Section 4.1. As depicted in Figure 2 below, current points to a dEnvironment, \nwhich has prey and vars fields. The prey field holds the previous value of current, and the vats field \npoints to the list of dVariable instances, current is initialized to 0. The BEGIN SET macro pushes a \nnew dEnvironmont instance onto the list headed by current. The SET macro declares a local variable for \nid, and pushes a dVariable onto the current->vars list. Using dEnvironments makes it possible to remove \nall of the dVariabtes created in BEGIN SET ... END SET with the single assignment current = prey in the \ndEnvi ronmen t destructor. ~.link tt.vats  dEnvironment, prey current Figure 2. Shadow stack of el.Environment \nand dVariable structures. APPENDIX C: HASH-TABLE IMPLEMENTATION The hash-table implementation described \nat the end of Section 4.1 involves only a few changes to the simple implementation. A single, per-thread \nglobal holds the hash table: struct dVariable *dVars [HASHSIZE] ;  The set statement links new dVariable \ninstances onto the appropriate hash chain. H(\"id\") denotes the hash value for id, and the hashing and \nmodulus shown below are computed at compile time. The dVariables are unlinked after executing S. 71 idl \n= el; struct dVariable dVar_/dl;  dVar idl.name = \"idl\" ; dVar idl.type = typeof (idl) ; dVar idl address \n= &#38;idl; dVar idl.link = dVars [H(\"id/')rood HASHSIZE] ; dVars [H(\"id/')mod HASHSIZE] = &#38;dVar_ \nidl; T. id. = e.; struct dVariable dVar_id.; dVar id..name = \"id.\"; dVar idn.type = typeof (idn) ; \ndVar idn.address = &#38;id.; dVar id..link = dVars [U(\"id.\")mod HASHSIZE] ; dVars [H(\"idn\")rood HASHSIZE] \n= &#38;dVar idn; S dVars [H(\"id.\")mod HASHSIZE] = dVar_ idn.link; dVars [H(\"id1\")rood HASHSIZE] = dVar \nidl.link; Some of the assignments following S can be omitted by unlinking only those dVariables that \nwere the first ones linked onto each hash chain. That is, the assignment for idk is dead and can be omitted \nif it is known that the link field refers to idj where j < k. The use statement calls a revision of dSearch \nthat searches just the appropriate hash chain: { TI *idl = dSearch(\"idl\", typeof (Ti), H(\"id/')modHASHSIZE) \n; Tm *idm = dSearch(\"idm\", typeof (Tin) , H(\"idm\" ) modHASHSIZE) ; S }  void *dSearch(const char *name, \nType *type, int index) { struct dVariable *p; for (p = dVars[index]; p I= 0; p = p->link) if (p->name \n== name &#38;&#38; type is a subtype of p->type) return p- >address; raise VariableNotFound; 273  \n \n\t\t\t", "proc_id": "378795", "abstract": "<p>Most programming languages use static scope rules for associating uses of identifiers with their declarations. Static scope helps catch errors at compile time, and it can be implemented efficiently. Some popular languages&#8212;Perl, Tel, TeX, and Postscript&#8212;offer dynamic scope, because dynamic scope works well for variables that &#8220;customize&#8221; the execution environment, for example. Programmers must simulate dynamic scope to implement this kind of usage in statically scoped languages. This paper describes the design and implementation of imperative language constructs for introducing and referencing dynamically scoped variables&#8212;dynamic variables for short. The design is a minimalist one, because dynamic variables are best used sparingly, much like exceptions. The facility does, however, cater to the typical uses for dynamic scope, and it provides a cleaner mechanism for so-called thread-local variables. A particularly simple implementation suffices for languages without exception handling. For languages with exception handling, a more efficient implementation builds on existing compiler infrastructure. Exception handling can be viewed as a control construct with dynamic scope. Likewise, dynamic variables are a data construct with dynamic scope.</p>", "authors": [{"name": "David R. Hanson", "author_profile_id": "81100646433", "affiliation": "Microsoft Research, 1 Microsoft Way, Redmond, WA", "person_id": "PP39052292", "email_address": "", "orcid_id": ""}, {"name": "Todd A. Proebsting", "author_profile_id": "81100592757", "affiliation": "Microsoft Research, 1 Microsoft Way, Redmond, WA", "person_id": "P283229", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/378795.378857", "year": "2001", "article_id": "378857", "conference": "PLDI", "title": "Dynamic variables", "url": "http://dl.acm.org/citation.cfm?id=378857"}