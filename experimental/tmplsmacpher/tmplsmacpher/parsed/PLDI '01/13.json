{"article_publication_date": "05-01-2001", "fulltext": "\n Bytecode Compression via Profiled Grammar Rewriting William S. Evans Christopher W. Fraser Microsoft \nResearch Computer Science Dept. One Microsoft Way University of British Columbia Redmond, WA 98052 Vancouver, \nBC V6T 1Z4 cwfraser@microsoft com will@cs, ubc. ca ABSTRACT This paper describes the design and implementation \nof a method for producing compact, bytecoded instruction sets and interpreters for them. It accepts a \ngrammar for programs written using a simple bytecoded stack-based instruction set, as well as a training \nset of sample programs. The system transforms the grammar, creating an expanded grammar that represents \nthe same language as the original grammar, but permits a shorter derivation of the sample programs and \nothers like them. A program's derivation under the expanded grammar forms the compressed bytecode representation \nof the program. The interpreter for this bytecode is automatically generated from the original bytecode \ninterpreter and the expanded grammar. Programs expressed using compressed bytecode can be substantially \nsmaller than their original bytecode representation and even their machine code representation. For example, \ncompression cuts the bytecode for lee from 199KB to 58KB but increases the size of the interpreter by \njust over 11KB. Categories and Subject Descriptors D.3.3 [Programming Languages]: Processors---optimization, \nrun-time environments. General Terms Algorithms, Performance, Design, Economics, Experimentation, Languages, \nTheory.  Keywords Program compression, bytecode interpretation, variable-to-fixed length codes, context-free \ngrammars. 1. INTRODUCTION Code compression can pay dividends in a variety of scenarios, saving network \ntransmission time, disk space, or memory at run- time. In general, programs can be stored at a variety \nof points in a memory hierarchy: 1. Instruction cache (possibly more than one level) Permission to make \ndigital or hard copies of all or part of this work for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for profit or commercial advan- tage and that copies \nbear this notice and the full citation on the first page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior specific permission and/or a fee. PLDI 2001 6/01 \nSnowbird, Utah, USA &#38;#169; 2001 ACM ISBN 1-58113-414-2/01106,.,$5.00 2. Main memory 3. Local disk \n 4. Installation media and remote disks, perhaps accessed via a fast LAN or even analog modems.  Code \ncompression can save space within a level, transmission time between levels, or both. It is obviously \nin common use at levels 3 and 4, but there exist applications at levels 2 [6, 27] and even at level 1 \n[1, 20]. Each scenario has somewhat different requirements. At slow levels, the cost of reading the program \nis high enough to mask significant decompression costs. Faster levels typically require faster decompression \nor, in the limit, a compressed form that can be interpreted directly, without the time or memory costs \nof a separate decompression step. Though recently there has been an increase in research on program compression \ne.g. [1, 3, 7, 8, 11, 13, 14, 16, 17, 20, 21, 22, 24, 27], very little of it has focused on methods, \nsometimes called code compaction methods, that avoid any decompression before execution. These methods \nproduce representations that can be directly executed [7, 8] or interpreted [11, 16, 17, 21, 24] and, \nas a result, are more limited than schemes that have the flexibility to decompress before execution. \nCertain embedded systems supply one of the clearest examples of the need for zero-overhead decompression. \nThese systems typically store much of their code in ROM. Competition drives manufacturers to add features, \nsome of which track events so infrequent (e.g. cellular telephone key presses) as to render moot the \ntraditional objections to direct interpretation of bytecode, whereas saving ROM or packing more features \ninto a fixed-size ROM can give a competitive advantage. Moreover, it may be unwise or impossible to decompress \nthe ROM temporarily to RAM, because on such systems RAM can be scarce, decompression costs power, and \nthe processor can access ROM faster to boot. Direct interpretation of bytecode may not suit all commercially \nimportant scenarios, but it clearly suits this one.  2. OVERVIEW Our system constructs a compressor \nthat compresses a program's bytecode representation, and an interpreter that reads and executes the compressed \nbytecode. To construct the compressor, the system accepts two inputs: 1. An initial grammar for a simple \nbytecoded stack-based instruction set. 2. A training set of sample bytecoded programs. This corpus is \nassumed to represent statistically the populations of the programs to be coded in the new bytecode. \n Training original+grammar ~ ..... expanded \u00b0riginaiinterpreter ~\" I parse trees I sample J---arser \n= -Igrammar.[ interpreter [ interpreter expander \" program 7p ...... I I I \"l generat\u00b0r I  Compression \n~ (derivation) / compressed program ~ parser / bytecode Figure 1. High-level structure of the byteeode \ncompression system. The compressor construction works as follows: A parser accepts the initial grammar \nand a training set of sample programs, and produces a forest of parse trees. Frequent rule pairs are \nidentified and new rules are added to the original grammar to decrease the overall size of the forest \n(i.e. the length of the derivation specified by the forest). The result of this training phase is an \nambiguous expanded grammar. The compressed bytecode for a program is a specification of a shortest derivation \nunder the expanded grammar. The key to compression is creating an expanded grammar that permits concise \nderivations. To construct the interpreter, the system also accepts two inputs: 1. The expanded grammar \nproduced in the training phase. 2. An interpreter for the original bytecode.  The interpreter generator \ncombines the expanded grammar and an interpreter for the original bytecode to form an inteipreter for \nthe compressed byteeode. Each instruction of the new interpreter implements an entire rule in the expanded \ngrammar. Thus the new interpreter can read and execute the compressed bytecode, which is a program's \nderivation under the expanded grarnmar. Figure 1 illustrates the operation of the system at a high level. \nThe figure is divided horizontally to emphasize the training and compression phases of the system.  \n3. THE INITIAL BYTECODE Our system starts with a bytecode that is a simple postfix encoding of 1co trees \n[15]. Most operators consist of an un-typed or generic base (such as ADD) followed by a one-character \ntype suffix (I for integer, F for float, etc), which indicates the type of value produced. Appendix 1 \nlists all of the operators that appear m the bytecode. All operators that produce values push these values \nonto a global stack. Most operators that require operands (such as ADD) obtain them from the stack. The \nexceptions are LIT[2234], ADDR[FGL], LocalCALL, JUMP, and BrTrue. These operators follow a prefix rather \nthan postfix format and take the bytes that follow the operator in the bytecode as their operands. LIT, \nfor example, simply pushes the specified number of bytes that follow it onto the stack. The bytecode \nis organized into procedures. A descriptor records three elements for each procedure: The bytecode for \nthe procedure.  A table of branch and jump offsets.  The size of the procedure's frame.  Branch offsets \ncould be placed in the bytecode, but doing so would require that the compressor change the offsets as \nit changes the instruction set, which would be unwieldy. For simplicity, we place label table indices \nrather than offsets in the bytecode. The specified label table entry holds the offset into the current \nprocedure's byteeode of the branch target needed when the corresponding branch is taken. When the compressor \nrewrites the original bytecode, it also rewrites the label table to reflect the new position of each \nlabel, but the label table indices in the bytecode do not change. Global addresses are handled similarly. \nGlobal addresses are not, in general, known until after final linking or loading, so they aren't available \nto the compressorl The byteeode thus contains instead an index into a single, global table of global \naddresses, and it relies on the linker to fill in the table entry with the appropriate address. For inter-operation \nwith existing libraries, 1co creates for some bytecoded procedures a \"trampoline\", which is a C-callable \ninterface that simply passes to the interpreter the procedure's descriptor number and incoming arguments. \nAppendix 3 illustrates this packaging. Trampolines take space, so they are created only for procedures \nthat must have them. If a procedure's address is taken and not consumed immediately by a call operator, \nwe conservatively assume that it could be the target of an indirect call and thus needs a trampoline. \nThe target of an indirect call requires a trampoline because the indirect call may call conventional \ncode (a library routine) or bytecode and uses the same calling mechanism for both. The trampoline is \nomitted if a call operator always immediately consumes the procedure's address. Calls to these procedures \nuse a specialized call operator (\"LocalCALL\") that recursively calls the interpreter without going through \na trampoline. These rules assume, of course, that the bytecode represents an executable with a single \nentry point (namely the routine main, which needs a trampoline) and not, for example, a dynamically linked \nlibrary, for which it could be difficult or impossible to omit any trampolines. t49 4. DERIVING A BETTER \nBYTECODE Our goal is to produce a program representation that is small and can be interpreted quickly. \nWe cannot afford the space to decompress a representation before executing it. Instead, we should be \nable to read part of the representation and immediately execute a sequence of machine instructions. In \naddition, if we encounter a control transfer instruction, we should be able to jump to the corresponding \npart of the compressed representation and continue interpretation. One possible approach is to encode \nthe original instructions using a fixed-to-variable length code. Each original instruction is assigned \na codeword whose length varies according to the instruction's frequency: low frequency instructions receive \nlong codewords; high frequency ones receive shorter codewords. Huffrnan's algorithm provides a method \nto construct such a code given instruction frequencies [18]. Though optimal for the given frequencies, \nsince the codewords vary in length, we may be forced to examine the program representation one bit at \na time to extract a single instruction. We cannot afford the time to extract and decode each instruction \nin this manner. Huffman codes can be decoded in larger increments than bits (e.g. a byte at a time) but \nthis uses a significant amount of space for look-up tables [5]. Rather than encode single instructions \nusing a variable number of bits, we instead encode multiple instructions using a fixed number of bits. \nIn essence, we adopt a variable-to-fixed length coding policy rather than producing a fixed-to-variable \nlength code as Huffman does.  4.1 Grammar Rewriting Our scheme is based on a grammar that describes \nthe set of legal instruction sequences. The grammar provides a model or structure that helps us obtain \na concise representation for these sequences. We represent a sequence by specifying its derivation with \nrespect to the grammar. Since we are only concerned with the representation of legal sequences, we can \nignore sequences that do not have such a derivation. We describe a sequence by its left:most derivation \nwith respect to the grammar, The derivation is a list of the rules used to expand the leftmost non-terminal \nin each sentential form of the derivation where each rule is represented as an index: the ith rule for \na non- terminal represented as the index i. For example, the sequence ADDRFP 00 INDIRU LIT10 NEU BrTrue \n00 LIT1 0 ARGU ADDRGP 0 0 CALLU POPU LABELV RETV which represents the C-code void check(int flag) { if \n(flag == 0) exit( 0 ); } couldbeencodedas 11101210000102 00001020 111010002100 withrespecttothe~llowinggrammar: \n0. <start> = i. <start> = <start> <x> 0. <x> = RETV I. <x> = <v> <xl> 0. <v> = <v0> i. <v> = <v> \n<vl> 2. <v> = <v> <v> NEU 0. <vl> = CALLU I. <vl> = INDIRU 0. <v0> = ADDRFP <byte> <byte> i. <v0> \n= ADDRGP <byte> <byte> 2. <v0> = LIT1 <byte> 0. <xl> = BrTrue <byte> <byte> I. <xl> = ARGU 2. <xl> \n= POPU 0. <byte~ = 0 Notice that the sequence is actually parsed into two separate derivations, one \nfor the code prior to the LABELV and one for the code after. The LABELV indicates a branch target; it \nis not an operator itself. Keeping the derivations separate allows direct interpretation of this representation. \nWhen the interpreter encounters a control transfer, it knows that wherever it jumps to in the derivation \nsequence, it can assume that this is the beginning of a derivation of the start non-terminal of the grammar \nand that the first rule it encounters applies to this start non-terminal. Unless we encode each rule \nnumber as a byte, this is not, in general, a very practical code for interpretation. The problem is that \nthe interpreter, as in the fixed-to-variable (Huffman) encoding scenario, may be forced to examine the \nrepresentation a single bit at a time, which is too costly. However, using one byte per rule number can \nbe very wasteful, especially for non-terminals with very few rules. In the sample grammar, we would use \nan entire byte to represent, in the case of the non-terminal <v>, only three possible values. This results \nin a not very concise encoding of the program. In order to create a practical and concise encoding of \nthe program, we modify the grammar so that each non-terminal has close to the same number (256) of rules. \nThe modification process takes two rules, A ~ ~x B [3 and B ~ 7, and adds to the grammar a third rule, \nA ~ c~ 713, where A and B are non-terminals and o~, 13, and Y are strings of terminals and non-terminals. \nWe call this process inlining a B rule into an A rule. Inlining doesn't change the language accepted \nby the grammar. However, it shortens the sequence of rules (the derivation) needed to express some strings, \nand it increases the number of rules for some non-terminal. The question is which rules should we inline. \nThe goal of the inlining is to produce a grammar that provides short derivations for programs. Starting \nwith a derivation of a program using the original grammar, the best single inline that we could perform \nis the most frequently occurring pair of rules; one used to expand a non-terminal on the right-hand side \nof the other. If this pair were used m times in the derivation, inlining would decrease the derivation \nlength by m rules. We can view this process as operating on the forest of parse trees obtained from parsing \nthe program using the original grammar. The parse produces a forest since we restart the parser from \nthe start uon-terrninal at every potential branch target (i.e. LABELV). For our purposes, a parse tree \nis a rooted tree in which each internal node is labeled with a rule and each leaf with a terminal 150 \nb c d Figure 2. Edge contraction, r A : = A --9 aBde r, := B -> bc r'A: = A ---> abcde symbol. The root \nis labeled with a rule for the start non-terminal. In general, an internal node that is labeled with \na rule A ----> ax a2 ... ak (where ai is a terminal or non-terminal symbol) has k children. If ai is \na non-terminal then the ith child (from the left) is labeled with a rule for non-terminal af. If at is \na terminal then the ith child is a leaf labeled with at. The program appears as the sequence of labels \nat the leaves of the parse trees in the forest, reading from left to right. A leftmost derivation is \nsimply the sequence of rules encountered in a preorder traversal of each parse tree in the forest. The \ninlining of one rule rs into another rule rA creates a new rule r'a whose addition to the grammar permits \na different (and shorter) parse of the program. One such new parse can be obtained by contracting every \nedge from a node labeled rA to a node labeled rs in the original forest - meaning the children of r8 \nbecome the children of rA - and relabeling the node labeled ra with the new rule r'A. See figure 2. If \nthe number of edge contractions is m, the resulting forest has m fewer internal nodes and thus represents \na derivation that is shorter by m steps. To construct an expanded grammar, we parse a sample program \n(or a set of sample programs) using the original grammar and obtain a forest of parse trees. We then \ninline the pair of rules at the endpoints of the most frequent edge in the forest, contract all occurrences \nof this edge, add the new inlined rule to the grammar, and repeat. We stop creating rules for a non-terminal \nonce it has 256 rules. Occasionally, a rule for a non-terminal may be subsumed by a new rule. That is, \nafter the addition of the new rule, the first rule is no longer used in the derivation. If the unused \nrule is one that was added via inlining, we are free to remove it from the grammar. (We cannot, however, \nremove one of the original grammar rules or we risk changing the grammar's language.) In our current \nimplementation, we remove unused inlined rules in order to decrease the size of the expanded grammar. \nThis may cause some non-terminals to have fewer than 256 rules. This construction procedure is greedy; \nit always inlines the most frequent pair of rules. This is a heuristic solution to the problem of finding \na set of rules to add to the grammar that permits the shortest derivation of the sample program. We rely \non this heuristic since finding an exact solution is, unfortunately, NP- hard. The resulting expanded \ngrammar is ambiguous, even if the original grammar was not, since we leave the original rules in the \ngrammar. Given a program that we wish to compress, we are free to choose any derivation under the expanded \ngrammar to represent the program's original bytecode sequence. The size of the representation is the \nnumber of rules in the derivation. Since our goal is compression, we want a minimum length derivation. \nWe use Earley's parsing algorithm [9], slightly modified, to obtain a shortest derivation for a given \nsequence. The derivation is then the compressed bytecode representation of the program and is suitable \nfor interpretation. Figure 1 shows the structure of this system. 5. THE INTERPRETERS This system has \ntwo interpreters. The initial interpreter accepts the initial, uncompressed bytecode. The initial interpreter \nand the expanded grammar form the raw material from which the system builds the second interpreter, which \naccepts compressed bytecode. At the core of the initial interpreter is a routine with a single statement, \nnamely a C switch: void interpretl ( unsigned char op, istate *istate ) { switch (op) { ... } )  It \naccepts a single, uncompressed operator and pointer to a structure that records the state of an interpreter. \nThe latter would be maintained as variables local to a single interpretation routine but for the need \nto modify its contents from several different routines. The switch above has one ease for each instruction \nin the initial instruction set, and the cases manipulate a small execution stack. Stack elements use \na union of the basic machine types. For example, the case for ADDI pops two elements, adds them as integers, \nand pushes the result: case ADDI : a = istate->stack[istate->top--] .i; b = istate->stack[istate->top--] \n.i; istate->stack[++istate->top] .i = a + b; return; Cases for operators that need a literal operand \ninvoke a macro GET (n) to collect an n-byte literal starting at istate->pc. To interpret a sequence of \noperators, the initial interpreter enters an infinite loop that passes the next bytecode to the switch \nroutine: void interp(istate *istate) { while (I) interpretl ( istate->code [ istate->pc++ ] , istate \n ); }  The case arms for return operators use the standard C routine longjrap to return to the top-level \nroutine (interpret, not shown) that handles control transfer between procedures. Initially, interpret \nis called by a trampoline or by a LocalCALL operator. It creates an istate record, calls interp, and, \nwhen 151 returned to via a longjmp, returns the value atop the execution constrained to support direct \ninterpretation or random access. For example, gzip is flee to exploit redundant patterns that span basic \nstack. The second interpreter, which interprets compressed bytecodes, introduces another level of interpretation \nbetween interp and interpretl. void interp(istate *istate) ( while (i) interpNT(istate, NT_start) ; \n } InterpNT adds an argument that identifies a non-terminal and thus which of several specialized bytecoded \ninstruction sets to use. ZnterpNT fetches the next bytecode, which, with the given non-terminal, identifies \nthe rule for the next derivation step. A table encodes for each rule the sequence of terminals and non- \nterminals on the rule's right-hand side. InterpNT advances left- to-right across this right-hand side. \nWhen it encounters a terminal symbol, it calls interpret2, to execute that symbol. When it encounters \na non-terminal, it calls itself recursively, with the given non-terminal to define the new context and \nnew specialized bytecode. Literals represent a modest complication because a rule can inline a literal \npartially. For example, the rule <start> = JUMPV 0 <byte> effectively creates a specialized jump bytecode \nfor which the first of two literal bytes is constrained to be zero. The second interpreter's GET macro \nmust decode the representation of the rule to determine that the first half of the literal comes from \nthe rule and the second half from the bytecoded instruction stream.  6. PERFORMANCE The table below \nreports the size of several bytecode sequences as compressed by our method. Each input was compressed \ntwice, with grammars generated from two different training sets, namely lcc and gcc. Predictably, lcc \nand gcc each compress somewhat better with their own grammar, but the other inputs compress about as \nwell with either grammar. compressed input original trained on gcc trained on lcc bytes raHo bytes ratio \ngcc 1,423,370 471,111 33% 577,814 41% Icc 199,497 75,077 38% 57,722 29% gzip 47,066 19,466 41% 19,706 \n42% 8q 436 138 32% 152 35% The interpreters are small: 7,855 bytes for the initial, uncompressed bytecode \nand 18,962 for the bytecode generated from the lee training set. Thus llKB of extra space in the interpreter \nsaves over 900KB in the bytecode for gcc. The grammar occupies 10,525 bytes and thus accounts for most \nof the difference in interpreter size. For calibration and as a very rough bound on what might be achievable \nwith good, general-purpose data compression, gzip [2] compresses the inputs above to 31-44% of their \noriginal size, with the larger inputs naturally getting the better ratios. Any comparison, of course, \nunfairly favors gzip, which is not blocks, where our bytecode compressor must stop and discard all contextual \ninformation at every branch target. The table below compares bytecoded and conventional object code for \none of our programs, namely lcc. The first two rows describe executables based on the original and compressed \nbytecodes. The third row describes a conventional x86 executable obtained by compiling lcc using lcc. \nrepresentation bytes Uncompressed 292,039 bytecode Compressed 161,386 bytecode lcc-compiled 240,522 x86 \nexecutable Each row above includes the code and data for any interpreter associated with the row, the \ncorresponding bytecode, the label and global tables, the procedure descriptors, the trampolines, and \nthe initialized and uninitialized data for the original program. Indeed, it includes everything but library \ncode and data, because the linker rounds each segment up to a page boundary and thus sacrifices some \nprecision and because the library code was not available for compression. Two experimental notes are \nin order: \u00ae The interpreters were compiled with Microsoft Visual C V6.0 (\"MSVC\"), in order to benefit \nfrom its thorough space optimizer. Compiling the interpreters with 1co instead increases the total size \nof the uncompressed version by 7,769 bytes and of the compressed version by 8,734 bytes. \u00ae All rows use \nan lee option that compiles switches into decision trees, because the current implementation of the bytecode \ncannot handle indirect jumps. The use of this option accounts for about 5KB of the conventional executable. \nAnother way to save space is simply to use a more ambitious optimizer. For example, MSVC compiles lee \nto 236,181 bytes without optimization but to 161,716 bytes when full space optimization is requested. \nIt would be interesting to run our compressor on bytecodes that have been through such an optimizer, \nbut this experiment requires obtaining a suitable bytecode representation from MSVC, which is currently \nimpossible. Highly optimized code is usually less regular and thus less compressible than more modestly \noptimized code, but it appears likely that the combination of an ambitious optimizer with bytecode compression \nwould yield a smaller result than either tool in isolation. Several other opportunities for further compression \nremain: The label and global tables were taken out of line to simplify implementation. These tables \naccount for 9,628 and 3,940 bytes of the byteeoded executables for ice, and, although they could save \ncode space for some constants, it's likely that switching to inline global addresses and branch offsets \nwould save much of that overhead.  Trampolines account for 1,674 bytes of code in each of the first \ntwo rows of the table above. In the embedded systems that motivate this research, trampolines, which \nexist mainly for inter-operation with other code, might be unnecessary.  The current implementation \nstores grammars sub-optimally. Straightforward receding should save another 1,863 bytes for the grammar \ngenerated from 1 cc. o Different starting grammars yield different compressors. The current grammar \neffectively tracks stack height. A more complex grammar that tracked the datatype of each element on \nthe stack did not do significantly better, but grammars that track more state or different state than \nthe current grammar might improve compression. 7. RELATED WORK Proebsting's work on superoperators [24] \nis the most comparable to our grammar-based method. Superoperators assign bytecodes to repeated patterns \nin expression trees. Our method, on the other hand, searches for repeated patterns in parse trees obtained \nby parsing a linealization of these expression trees. In addition to the difference in program representation, \nour approach differs from superoperators in two fundamental ways. First, a single bytecode in our system \nmay represent the code from several expression trees while a superoperator can only represent a pattern \nthat occurs within an expression tree. Second, the superoperator interpreter has only a single interpretive \nstate whereas our interpreter may have a state or context for every non-terminal in the original grammar. \nAn additional minor difference is that the original implementation of superoperators did not allow patterns \nto contain literals. Subsequent work, however, eliminated this restriction and resulted in a method that \nwas able to reduce bytecode representations to approximately 50% of their original size [16]. One should \nbe careful of comparing this ~vith the present work, since the initial bytecode and the target machine \ncode in the two cases are somewhat different. It is, however, safe to say that allowing a single bytecode \nto span several expression trees and supporting more contexts in the interpretation of bytecodes leads \nto substantial improvements in compression. After superoperators, the recent work most comparable to \nours is Lueco's work on split-stream code compression [11, 22]. The original code is designed for a virtual \nmachine that resembles common RISC machines. The compressed code represents frequently occurring instruction \nsequences and specialized instructions with \"burned in\" operands. This approach is similar to the superoperator \nwork in that it recognizes repeated local patterns. However, its separate treatment of opcodes and operand \ntypes, and its packaging of the compressed form into byte-aligned pieces results in a more succinct yet \nstill interpretable form. Unlike these local methods, our grammar-based approach has the ability to see \nmore global patterns (i.e. relations between non- adjacent code fragments) and to produce an interpretable \nlanguage that captures these patterns. The compression techniques that we use were inspired by Tunstall's \nconstruction of optimal variable-to-fixed length codes [26]. A variable-to-fixed length code assigns \ncodewords of a fixed length, say k bits, to variable length sequences of the original instructions. The \nset of sequences that have codewords is called the dictionary. The general idea is to choose a dictionary \nof about 2 k sequences that are long and occur frequently. Since the same number of bits represents each \nsequence, maximizing the average length of a dictionary sequence minimizes the compressed representation. \nGiven a distribution on symbols from a memory- less source, Tunstall's algorithm produces a uniquely \nparsable dictionary of sequences. The term \"uniquely parsable\" refers to the property of the dictionary \nthat any sequence can be partitioned into subsequences from the dictionary in exactly one way) There \nare two problems with applying \"Funstall's algorithm in our situation. The first is the assumption that \nthe sequence is produced by a memory-less source. Programs contain too much stnacture for this to be \na reasonable model of instruction sequences. Recent work on extending Tunstall's technique to finite \nstate sources provides a means of capturing some source stnlcture [25], however it does not capture the \ngrammatical restrictions of most source languages. This work is partly an attempt to extend Tunstall's \nmethod to grammar based sequences. The second problem is preserving branch targets under the constraint \nof unique parsability. Unique parsability implies that no prefix of a dictionary sequence is in the dictionary. \nThis means that if a branch target occurs after seeing a prefix of a dictionary sequence, we must code \nthat prefix explicitly. Since branch targets may occur at nearly any point, insisting on unique parsability \nresults in poor compression. Our technique produces a plurally parsable (allowing more than one encoding \nof a sequence) fixed length code based on a context- free grammar for the language, rather than a memory-less \nor finite state source. We force the preservation of branch targets by restarting the encoding procedure \nwhenever the sequence contains such a target. However, by using a plurally parsable code, we are still \nable to efficiently encode the resulting pre-target subsequences. Several compression techniques for \nstructured text have been designed around the use of context-free grammars [4, 10, 12, 19, 23]. The typical \napproach is to represent the steps in a derivation of a text using a grammar; frequent steps are encoded \nwith fewer bits than infrequent ones. Very little work has been done on the modification of the grammar \nto assist in compression. Lake considers choosing a derivation from an ambiguous grammar based on its \nsuccess in compressing the text [19], and NeviU- Manning constructs a succinct grammar that derives only \nthe given text (without the aid of an existing grammar) [23]. In some sense, the latter approach can \nbe seen as an extreme example of the grammar based, variable-to-fixed length coding we propose in this \npaper. Constructing a grammar that derives only the input text is like building an interpreter that can \ninterpret only a single program. 8. SUMMARY This paper describes a system that automatically designs \nand implements compact bytecoded instruction sets by rewriting a grammar for a simple stack-based bytecode. \nSubstantial savings over recent research, over the initial bytecode, and over machine code have been \nshown, and opportunities for further improvements remain, via more sophisticated grammar transformations \nas well as more sophisticated implementation strategies. 9. REFERENCES [1] B. Abali, H. Franke, D. E. \nPoff, and T. B. Smith. Performance of hardware compressed main memory. i The last subsequence in the \npartition may be a prefix of a sequence in the dictionary. Research report RC21799, IBM T. J. Watson \nResearch Center (July 2000). [2] M. Adler and J.-1.. Gailly. The gzip home page. http://www.gzip.org. \n[3] C. Benveniste, P. Franaszek, and J. Robinson. Cache-memory interfaces in compressed memory systems. \nResearch report RC21662, IBM T. J. Watson Research Center (Feb. 2000). [4] R. D. Cameron. Source encoding \nusing syntactic information models. IEEE Transactions on Information Theory, 34(4) pp.843-850 (1988). \n[5] Y. Choueka, S. T. Klein, and Y. Perl. Efficient variants of Huffman codes in high level languages. \nProc. of the 8 th Annual International ACM SIGIR Conference on Research and Development in Information \nRetrieval, pp. 122-130 (June 1985). [6] Connectix Corp. RAM Doubler 9. http://www.connectix.com/products/rd9.htmI, \n(Nov. 2000). [7] K.D. Cooper and N. Mclntosh. Enhanced code compression for embedded RISC processors. \nPLDI, pp.139-149 (May 1999). [8] S. K. Debray, W. Evans, R. Muth, and B. de SuRer. Compiler techniques \nfor code compaction. TOPLAS, 22(2) pp.378-415 (March 2000). [9] J. Earley. An efficient context-free \nparsing algorithm. Communications of the A CM, 13 (2) pp. 94-102 (Feb. 1970). [10] P. Eck, X. Changsong, \nand R. Matzner. A new compression scheme for syntactically structured messages (programs) and its application \nto Java and the interact. Proc. Data Compression Conference (poster session), p.542 (1998). [ll]J. Ernst, \nW. Evans, C. W. Fraser, S. Lucco, and T. A. Proebsting. Code compression. PLDI, pp.358-365 (June 1997). \n[12]W. Evans. Compression via guided parsing. Proc. Data Compression Conference (poster session), p.544 \n(1998). http://www.es.arizona.edu/people/will/papers/ [13]M. Franz and T, Kistler. Slim Binaries. Communications \nof the ACM, 40(12) pp.87-94 (Dec. 1997). [14]C. W. Fraser. Automatic inference of models for statistical \ncode compression. PLD1, pp.242-246 (May 1999). [15]C. W. Fraser and D. R. Hanson. A Retargetable C Compiler: \nDesign and Implementation. Addison Wesley Longman, 1995. [16]C. W. Fraser and T. A. Proebsting. Custom \ninstruction sets for code compression. Unpublished (October 1995). http://www.research.microso~.com/~toddpro/ \n[17]J. Hoogerbrugge, L. Augusteijn, J. Trum, and R. van de Wiel. A code compression system based on pipelined \ninterpreters. Soflw. Pract. Exper., 29(11) pp.1005-1023 (1999). [18] D. Huffman. A method for the construction \nof minimum redundancy codes. Proc. of IRE, 40 pp.1098-1101 (1952). [19]J. M. Lake. Prediction by grammatical \nmatch. Proc. Data Compression Conference, pp. 153-162 (March, 2000). [20] C. Lefurgy, E. Piccininni, \nand T. Mudge. Reducing code size with run-time decompression. Proceedings of the 6th International Symposium \non High-Performance Computer Architecture (HPCA) (Jan 2000). [21 ] S. Y. Liao. Code generation and optimization \nfor embedded digital signal processors. Ph.D. thesis, MIT (1996). [22]S. Lucco. Split-stream dictionary \nprogram compression. PLDI, pp. 27-34 (June 2000). [23]C. G. Nevill-Manning. Inferring sequential structure. \nPh.D. thesis, University of Waikato, (1996). [24] T. A. Proebsting. Optimizing an ANSI C interpreter \nwith superoperators. POPL, pp.322-332 (Jan. 1995). [251I. Tabus, G. Korodi, and J. Rissanen. Text compression \nbased on variable-to-fixed codes for Markov sources. Proc. Data Compression Conference, pp. 133-141 (March, \n2000). [26]B. P. Tunstall. Synthesis of noiseless compression codes. Ph.D. thesis, Georgia Inst. Technology \n(1967). [27] P. R. Wilson, S. F. Kaplan, and Y. Smaragdakis. The case for compressed caching in virtual \nmemory systems. USENIX Technical Conference (June 1999). Appendix 1. Initial instruction set. The table \nbelow describes the un-typed or generic operators from the initial instruction set. The only changes \nfrom lee [15] are literals and LocalCALL operators, which Section 3 describes, and comparisons, lcc comparisons \naccept two comparands and a literal branch address, but the comparisons here accept two eomparands and \npush a flag for SrTrue, which accepts the literal branch address. The type suffixes are: V for void or \nno value, C and S for char and short, I and U for signed and unsigned integers, F and D for single- and \ndouble-precision floating-point numbers, P for pointers, and B for instructions that operate on blocks \nof memory. The grammar in Appendix 2 shows the valid combinations of the type suffixes with the generic \noperators below and also shows how many literal bytes, if any, follow each operator. Operator Comment \nADD DIV SUB MUL Arithmetic. '' I BAND BOR BXOR !Bit-wise Booleans. BCOM Bit-wise negation. NEG ! Arithmetic \nnegation. CVD ! Convert from double. CVF Convert from float. CVI i Convert from int. cvI1 cvI2 [ Sign-extend \nchar, short. cvel cVU2 Zero-extend char, short. EQ GE GT LE LT NE Compare and push 0 or 1. LSH MOD RSH \nShifts, remainder INDIR Popp, push*p. ASGN Pop p and v, copy v to *p. ADDRF Push address of formal. \nADDRG Push address of global. ADDRL Push address of local. JUMP Pop label number, jump. ARG Top is next \noutgoing argumentl LocalCALL CALL RET Calls, return. ! POP Discard top element. LIT[1234] Push 1, 2, \n3, or 4 literal bytes. BrTrue Pop flag. Jump if true. Appendix 2. Initial byteeode grammar. The grammar \nfor the initial byteeode groups operators based on their effect on the evaluation stack. Non-terminals \nthat end in 0, 1, and 2 denote leaf, unary and binary operators. Such non-terminals that begin with \"v\" \ncollect operators that yield a value; those that begin with \"x\" collect operators that push no result \nand thus are executed for a side-effect (e.g. JUMPV). For example, an operator that removes two values \nfrom the stack is grouped under the non- terminal <x2 >. An operator that removes one value and pushes \none value is grouped with <vl>. <start> = <start> = <start> <x> <V> = <V0>  <V> = <V> <Vl>  <V> = \n<V> <V> <V2>  <X> = <X0>  <X> = <V> <Xl>  <X> = <V> <V> <X2>  <v2> = ADDD [ DIVD I MULD SUBD <v2> \n= ADDF [ DIVF [ MULF SUBF <v2> = DIVI I MODI I MULI <v2> = ADDU I DZVU I MODU l MULU SUBU <v2> = BANDU \n[ BORU [ BXORU <v2> = EQD [ GED GTD I LED [ LTD NED <v2> = EQF [ GEF GTF [ LEF [ LTF NEF <v2> = GEl GTI \n] LEI [ LTI <v2> = EQU [ GEU GTU I LEO [ LTU NEU <v2> = LSHI I LSHU I RSHI I RSHU <vl> = BCOMU <vl> = \nCALLD I CALLF [ CALLU <vi> CVDF ] CVOI I CVFD I CVFI <vl> = CVID I CVIF <vl> = CVIiI4 I CVI214 ] CVUiU4 \nI CVU2U4 <vl> = INDIRC [ INDIRS [ INDIRU <vl> = INDIRD I INDIRF <vl> = NEGD [ NEGF I NEGI <v0> = ADDRFP \n<byte> <byte> <v0> = ADDRGP <byte> <byte> <v0> = ADDRLP <byte> <byte> <v0> = LocalCALLD <byte> <byte> \n<v0> = LocalCALLF <byte> <byte> <v0> = LocalCALLU <byte> <byte> <v0> = LIT1 <byte> <v0> = LIT2 <byte> \n<byte> <v0> = LIT3 <byte> <byte> <byte> <v0> = LIT4 <byte> <byte> <byte> <byte> <x2> = ASGNB I ASGNC \n] ASGNS I ASGNU <x2> = ASGND [ ASGNF  <xl> = ARGB I ARGO I ARGr I ARGU <xl> = BrTrue <byte> <byte> \n[ CALLV  <xl> = POPD I POPF [ POPU  <xl> = RETD I RETF I RETU  <x0> = JUMPV <byte> <byte> <x0> = \nLocalCALLV <byte> <byte> <x0> = RETV <byte> = 0 [ 1 [ ... [ 255 Appendix 3. Packaging the byteeodes \nFor each procedure f, the system creates two vectors, which hold the compressed bytecodes and the table \nof branch offsets: static unsigned char _f_code [ ] = { ... } static short _f_labels[] = { ... }; A \nglobal table of procedure descriptors packages pointers to these vectors with the procedure's framesize: \nproc _procs[] = { { 12, _f_code, f labels }, For procedures that need a trampoline, the system also \ncreates a procedure that passes to the interpreter the index of the procedure's descriptor and the address \nof the block of incoming arguments: int f (unsigned argl) { return interpret(0, &#38;argl) .i; } The \ngenerated procedure's signature differs from the original procedure's--namely, it takes only a single, \nunsigned argument-- but we use an x86 calling convention that passes all arguments in contiguous memory, \nand the address of the first argument is all that the interpreter needs to access all arguments. The \ninterpreter returns a C union of the primitive C datatypes. The \". \u00a3\" above picks out the type that this \nparticular procedure returns. Finally, the system creates a global table of the addresses of global variables: \nvoid *_globals [ ] = ( &#38;malloc, 155  \n\t\t\t", "proc_id": "378795", "abstract": "<p>This paper describes the design and implementation of a method for producing compact, bytecoded instruction sets and interpreters for them. It accepts a grammar for programs written using a simple bytecoded stack-based instruction set, as well as a training set of sample programs. The system transforms the grammar, creating an expanded grammar that represents the same language as the original grammar, but permits a shorter derivation of the sample programs and others like them. A program's derivation under the expanded grammar forms the compressed bytecode representation of the program. The interpreter for this bytecode is automatically generated from the original bytecode interpreter and the expanded grammar. Programs expressed using compressed bytecode can be substantially smaller than their original bytecode representation and even their machine code representation. For example, compression cuts the bytecode for lcc from 199KB to 58KB but increases the size of the interpreter by just over 11KB.</p>", "authors": [{"name": "William S. Evans", "author_profile_id": "81100485414", "affiliation": "Computer Science Dept., University of British Columbia, Vancouver, BC", "person_id": "PP14170065", "email_address": "", "orcid_id": ""}, {"name": "Christopher W. Fraser", "author_profile_id": "81100364566", "affiliation": "Microsoft Research, One Microsoft Way, Redmond, WA", "person_id": "P47620", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/378795.378827", "year": "2001", "article_id": "378827", "conference": "PLDI", "title": "Bytecode compression via profiled grammar rewriting", "url": "http://dl.acm.org/citation.cfm?id=378827"}