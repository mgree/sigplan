{"article_publication_date": "05-01-2001", "fulltext": "\n Dynamic Software Updating* Michael Hicks Computer and Information Science Department University of \nPennsylvania  mwh@dsl.cis.upenn.edu  Jonathan T. Moore Computer and Information Science Department \nUniversity of Pennsylvania  jonm@dsl.cis.upenn.edu Scott Nettles Electrical and Computer Engineering \nDepartment University of Texas at Austin  nettles@ece.utexas.edu  ABSTRACT Many important applications \nmust run continuously and without interruption, yet must be changed to fix bugs or up- grade functionality. \nNo prior general-purpose methodology for dynamic updating achieves a practical balance between flexibility, \nrobustness, low overhead, and ease of use. We present a new approach for C-like languages that pro- vides \ntype-safe dynamic updating of native code in an ex- tremely flexible manner (code, data, and types may \nbe up- dated, at programmer-determined times) and permits the use of automated tools to aid the programmer \nin the up- dating process. Our system is based on dynamic patches that both contain the updated code \nand the code needed to transition from the old version to the new. A novel as- pect of our patches is \nthat they consist of verifiable native code (e.g. Proof-Carrying Code [17] or Typed Assembly Language \n[16]), which is native code accompanied by an-notations that allow on-line verification of the code's \nsafety. We discuss how patches are generated mostly automatically, how they are applied using dynamic-linking \ntechnology, and how code is compiled to make it updateable. To concretely illustrate our system, we have \nimplemented a dynamically-updateable web server, FlashEd. We discuss our experience building and maintaining \nFlashEd. Perfor-mance experiments show that for FlashEd, the overhead due to updating is typically less \nthan 1%. 1. INTRODUCTION Many computer programs must be 'non-stop', that is, run continuously and without \ninterruption. This is especially true of mission critical applications, such as financial trans- action \nprocessors, telephone switches, airline reservations and air traffic control systems, and a host of others. \nThe increased importance of the Internet and its link with the global economy has made non-stop service \nimportant to a *This work was supported by the NSF under contracts ANI #00-82386, ANI #98-13875, and \nANI #0081360. Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for profit or commercial \nadvan- tage and that copies bear this notice and the full citation on the first page. To copy otherwise, \nto republish, to post on servers or to redistribute to lists, requires prior specific permission and/or \na fee. PLDI 2001 6/01 Snowbird, Utah, USA &#38;#169; 2001 ACM ISBN 1-58113-414-2/01/06... $5.00 larger \nrange of less sophisticated users who wish to run e- commerce servers. On the other hand, companies must \nbe able to upgrade their software to fix bugs, improve performance, and ex-pand functionality. In the \nsimplest case, upgrades and bug fixes require the system to be shut down, updated, and then brought back \non-line. This, of course, is not acceptable for non-stop applications; at best, it will result in loss \nof service and revenue, and, at worst, may compromise safety. Thus, in general, non-stop systems require \nthe ability to update software without service interruption. Solutions to this problem exist and are \nwidely deployed. A common ap- proach is to provide application-specific software support in conjunction \nwith redundant hardware (already present to support fault tolerance) to enable so-called hot standbys. \nFor example, Visa makes use of 21 mainframe computers to run its 50 million line transaction processing \nsystem; it is able to selectively take machines down and upgrade them by preserving relevant state in \nthe on-line computers. This system is updated as many as 20,000 times per year, but tolerates less than \n0.5~ downtime [20]. Of course, Visa's approach is expensive and, perhaps worse, adds to the com- plexity \nof building applications. Much of the complexity ~ comes from the need for the standby machine(s) to \nkeep or gain the state maintained by the running application. Less sophisticated users do not have Visa's \nresources, and seek simpler, more general, but no less effective solutions. In particular, while redundant \nhardware may often be present to support fault tolerance, we would prefer not to require it for updating, \nsince it adds cost and complexity. By using a simpler, general-purpose approach, we can support systems \nthat do not typically require extra hardware, like commu- nications components (e.g. routers, firewalls, \nNAT transla- tors, etc.), simple Internet servers, monitoring systems, and others. Furthermore, there \nare many non-redundant sys- tems that do not necessarily require non-stop service but would certainly \nbenefit from it. For example, rather than having to reboot a desktop computer each time its operating \nsystem is upgraded, we would prefer to realize the updates dynamically. We present a general-purpose \nframework for updating pro- grams as they run, called dynamic software updating, that is flexible, robust, \neasy to use, and efficient. Our approach is both cheaper and less complex than typical application- specific \napproaches, and as we shall argue, improves signifi- cantly over existing general-purpose systems. After \nstating the goals of our approach in \u00a72, we describe our updating fl-amework in \u00a73 and our implementation \nof it using Typed Assembly Language [16] in \u00a74. Our experience with a real-world application, a dynamicMty-updateable \nweb server, FlashEd, is described in \u00a75; its performance is pre- sented in \u00a76. We then move on to a more \nin-depth discussion of existing research and future directions before concluding. This work summarizes \nthe first author's thesis [10]; readers seeking more explanation and analysis should took there.  2. \nGOALS AND APPROACH What properties define an effective dynamic updating frame- work? To evaluate general-purpose \ndynamic updating sys- tems, we establish four evaluation criteria: o Flexibility. Any part of a running \nsystem should be updateable without requiring downtime. o Robustness. A system should minimize the risk \nof error and crash due to an update, using automated means to promote update correctness.  e Ease of \nuse. Generally speaking, the less complicated the updating process is, the tess error-prone it will tend \nto be. The updating system should therefore be easy to use. e Low overhead. Making a program updateable \nshould impact its performance as little as possible. In this section, we first argue that existing systems \ndo not satisfy all of these criteria (we defer an in-depth discussion of related work to \u00a77). Next, we \ndescribe the key aspects of our approach, and explain how they combine to successfully meet these requirements. \n2.1 Existing Approaches Unfortunately, no existing general-purpose updating system meets all of the desired \ncriteria. Many systems have lim- ited flexibility, constraining their evolutionary capabilities; for \nexample, dynamic linking is a well-known mechanism, but while systems based upon dynamic linking [2, \n21] may add new code to a running program, they cannot replace existing bindings with new ones. Those \nsystems that do al- low replacement typically either limit what can be updated (e.g., only abstract types \n[7], whole programs [9], or class in- stances [12]), when the updates can occur (e.g., only when updated \ncode is inactive [7, 14, 6, 9]), or how the updates may occur (e.g., functions and values must not change \ntheir types [12], or changes to module and class signatures are restricted [14, 7]). These limitations \nleave open the possi- bility that a software update may be needed yet cannot be accomplished without \ndowntime. In many cases, there are few safeguards to ensure up-date correctness. Some systems, for example, \nbreak type safety [22, 12, 6, 9, 5] or have only dynamic checking [3], or require potentially error-prone \nhand-generation of complex patch files [13, 7, 14, 6, 9, 3, 5]. Others rely on uncommon source languages \nor properties [13, 4, 3] and hence are not broadly applicable. Finally, some systems impose a high overhead, \neither due to implementation complexities [13, 5], or due to a reliance on interpreted code [14]. 2.2 \nOur Framework Our framework, dynamic software updating, avoids the ex-tra equipment and added complexity \nof typical application- specific approaches, and unlike previous general-purpose sys- tems, it meets \nall four of the evaluation criteria through a novel combination of new and existing technology. Flexibility. \nOur system permits changes to programs at the granularity of individual definitions, be they functions, \ntypes, or data. Furthermore, we allow these definitions to change in arbitrary ways; most notably, functions \nand data may change type, and named types may change definition. The system does not restrict when updates \nmay be per-formed, even allowing active code to be updated. Our ap- proach uses an imperative, C-like \nlanguage, and should thus be widely usable. Robustness. In our system, dynamic patches consist of ver-ifiable \nnative code, in particular Typed Assembly Language (TAL) [16]. As a result, a patch cannot crash the \nsystem or perform many incorrect actions since it can be proven to respect important safety properties, \nincluding type safety; ours is the first dynamic updating system to use verifiable native code. Our implementation \nbuilds on top of basic dy- namic linking, keeping the implementation simple and ro-bust. Ease of use. \nConstruction of patches is largely automated and clearly separated from the typical development process. \nWhen a new software version is completed, a tool com-pares the old and new versions of the source files \nto develop patches that reflect the differences. Although total automa- tion is undecidable, our tool \ncan nonetheless generate useful patch code for a majority of cases, leaving placeholders for the programmer \nin the other (infrequent) cases. No previ- ous system both cleanly separates patch development from software \ndevelopment and provides automated support for patch construction. Low Overhead. Our system imposes only \na modest run-time overhead, largely inherited from dynamic linking. Be- cause we use TAL, programs and \npatches consist of native code, giving obvious performance benefits as compared to interpreted systems \nlike Java. In short, our approach provides type-safe dynamic updating of native code in an extremely \nflexible manner and permits the use of automated tools to aid the programmer in the updating process. \nAs a result, ours is the first dynamic updating system to satisfy all of the evaluation criteria.  3. \nFRAMEWORK In this section, we discuss our general framework; details about our implementation of a specific \ninstance of this frame- work are presented in \u00a74. We assume an imperative source language, using C-like \ncode in the examples. 3.1 Dynamic Patches Central to our approach are dynamic patches; namely, ones that \nare applied to a running program. Dynamic patches differ from static patches, such as those created and \napplied using the Unix programs di:ff and patch, because they must deal with the state of the running \nprogram. We can ab-stractly define a dynamic patch of some file f as the pair (f', S), where jet is the \nnew code and S is an optional state 14 static int hum = 0; int f(int a, int b) { hum++ ; return a + \nb; } Figure 1: A file f new version f': state transformer S static int hum = 0; void S 0 int f(int \na, int b) { f'::num = f::num; hum++; return a * b; } Figure 2: Dynamic patch for f: (f',S) transformer \nfunction, used to convert the existing state to a form usable by the new code. For example, consider \nthe file f shown in Figure 1. The function f increments num to track the number of times it was called \nand returns the sum of its two arguments. Sup- pose we modify f to return the product of its arguments, \nproducing f'. The dynamic patch that converts f to f' is shown in Figure 2. The state transformer function \nS is triv- ial: it copies the existing value of hum in the old version f to the num variable in the new \nversion f'. In general, arbitrary transformations are possible. Because patches are applied to individuM \nfiles, rather than whole programs, there is a problem in applying a single patch if exported code or \ndata changes type: existing referers of changed items will access them at the old (now incorrect) type. \nIn general, this problem can be 'corrected' by simul- taneously applying patches to correct the callers. \nIn most situations, it would not make sense to do otherwise; in tran- sitioning from one version of a \nprogram to another, it only makes sense to patch all of the files that changed. However, for greater \nflexibility, we can extend the notion of a patch to optionally include stub functions to be interposed \nbetween old callers and new definitions to get the types right. There is no analogous construct for data, \nso if a patch changes the type of some global variable, then all the code that refer- ences that variable \nmust be simultaneously changed. 3.2 Enabling Dynamic Patches For reasons of flexibility and simplicity, \nwe build dynamic patch application on top of dynamic linking. In essence, a patch is dynamically linked \ninto the running program, and then the running program is transitioned from the old code to the new code. \nIn this section, we consider possible mechanisms for transitioning the program to use dynami- cally linked \npatches. We arrive at our basic methodology by considering possible choices in light of our evaluation \ncrite- ria, particularly flexibility, efficiency, and ease of use. We first consider updates to code \nand data, and then updates to type definitions. 3.2.1 Code and Data Updates Once a patch has been dynamically \nlinked into the pro- gram, existing function calls and data must be redirected to the stubs and new definitions \nin the patch. There are es- sentially two ways to do this: either by code relinking or by reference indirection. \nWhen using code relinking, the rest of the program is relinked after loading a patch; as a result, all \nreferences to the old definitions will be redirected to refer to the new ones. By contrast, reference \nindirection requires modules to be compiled so that references to other modules are indirected through \na global indirection table. An update then consists of loading a patch and altering appropriate en- tries \nin the table to point to the patch. With relinking, the process of updating is active: the dy- namic \nlinker must go through the entirety of the program and 'fix up' any existing code to point to the new \ncode. With reference indirection, updating is passive: the existing code is compiled to notice changes. \nAs a result, the linker does not need to keep track of the existing code and simply makes changes to \nthe table, but at the cost of an extra indirection to access definitions through the table. In both cases \nit is the responsibility of the state transformer function to find references to old definitions that \nare stored in the program's data. For example, if the program defines a table of function pointers, the \nstate transformer must redirect each pointer in the table to its new version. We have chosen to use code \nrelinking because it has two main benefits: it avoids extra indirection, reducing over-head, and it is \nsimple to implement, enhancing robustness. In particular, we implement code relinking by reusing the \ncode in the dynamic linker (described in \u00a74.1). The only ap- parent burden is the need to keep track \nof the existing code to be able to relink it; but we must do this already, since the dynamic linker resolves \nexternal references in loaded code against all the existing code. Ultimately, we could take a hybrid \napproach in which some elements are compiled to notice updates, and others must be relinked. One possibility \nthat we have explored is to compile pointerful data (notably function pointers) to have an extra indirection, \nbut require code references to be relinked. This would ease the requirement that the state transformer \ntranslate pointer data. We touch on this idea further in \u00a77. ? 3.2.2 Updating Type Definitions If we \nwish to preserve type-safety, we need a way to upgrade the type definitions as understood by the type-checker \nused by the dynamic linker. Again, there are basically two ap- proaches we could take: replacement or \nrenaming. With re- placement, applying the patch replaces the existing type def- inition in the typechecking \ncontext with a new one. Newly loaded code is checked against the new definition, implying that to preserve \nconsistency we must also convert any ex- isting instances of the old type definition (whether in the \nheap, stack, or static data area) to the new one. Further-more, any code that makes use of the old type \nelsewhere in the program must itself be replaced (unless the type is abstract; then only the code that \nimplements the type must be replaced). The alternative approach is type renaming. Instead of al- lowing \ntype definitions to be replaced, we maintain a fixed notion of a type definition, and rely on the compiler \nto de- fine a new type that logically replaces the old one by syn- tactically renaming occurrences of \nthe old name with the new one. Renaming is similar to the idea of c~-conversion in scoped programming \nlanguages, in which a type definition can override a definition of the same name in a surrounding scope; \nthe overriding type is renamed to avoid the clash. The consequence of this approach is that when the \npatch is 15 applied, existing instances of the old type are left as they are; the state transformer \nfunction and/or the stub func- tions in the patch can be used to convert old instances at update-time \nor later if needed. The typechecking context retains its definition of the old type and adds a new one \nfor the new type. There are advantages to both approaches. Type replace- ment, in general, is quite flexible \nand easy to use: it main- rains the identity of a type within the program but lets its definition change. \nThe system updates the values of the changed type (perhaps using user-provided code), so long as the \nprogrammer has updated all of the modules that use that type. However, because the program has no notion \nof the old and new versions of the type, the system must ensure that it can (logically) convert all of \nthe old types in- visibly. This restriction prevents an update to a type while code in the program is \nusing values of that type [7]. In con- trast, type renaming only allows the loading of new types to logically \nreplace existing ones, placing more burden on the programmer to convert values from the old to new type \nin either the state transformer or stub functions. However, renaming provides more freedom in timing \nupdates, since the program is 'aware' of both versions. Implementing type renaming is quite simple, requiring \nno additional runtime support. To be practical it does require a standard method for renaming type definitions \nat compile- time so that different developers do not choose clashing or inconsistent names, which would \nresult in program errors. This problem can be solved by taking a cryptographic hash (e.g. using MD5 [18]) \nof the type's definition to arrive at a consistent name. In contrast, type replacement requires a way \nto find all existing instances of a given type, and a way to change them from the old version to the \nnew. Further-more, to ensure that type updates do not occur when code that uses them is active requires \nheavyweight mechanisms to track when modules are in use [6, 13, 9]. We favor the simpler type renaming \napproach over the more complex, though easier to use, type replacement ap- proach. Type renaming is more \nlikely to be correctly imple- mented because it is simple, and is more portable, relying on facilities \navailable in type-safe dynamic linkers. Renaming also provides more flexibility as to when and how values \nof changed type will be converted. Other approaches [14] have cited runtime type dispatch operators (e.g. \ninstanceof in Java) as a reason for performing type replacement, but we believe more study is needed \nto bring to light the problems of type renaming in such a context. In our experience, renam- ing types \nat compile-time, and having multiple notions of a type in the program, has not been problematic; we present \nsome of our experience in this regard in \u00a75.1.1. 3.3 Building Updateable Systems Now that we understand \nthe mechanisms for building a system that can have dynamic patches applied to it, two key methodological \nquestions remain. The first is how the patches are generated. The second is how to structure our system \nso that patches can be correctly applied, particularly with respect to the timing of patch application. \n3.3.1 Patch Construction Methodology Constructing correct software is already difficult, so having to \nwrite dynamic patches only compounds the difficulty. A key goal of our approach has been to reduce the \nadded bur- den as much as possible. In particular, we wish to simplify the process of constructing patches, \nand we wish cleanly to separate software development fi'om patch development. Our approach to generating \npatches is straightforward. First, the programmer develops and tests a new version of the code, exactly \nas if he was going to statically compile and deploy it. Next, our system automatically generates as much \nof the patch file as possible by comparing the source of the old code to that of the new code. Finally, \nthe programmer fills in the parts of the state transformer and stub functions that could not be automatically \ngenerated. A key benefit of this approach is that software develop- ment is separated fi-om patch development. \nThis is possible because our notion of patch (and our implementation of it) allows essentially arbitrary \nchanges to the running program, and all of these changes are encapsulated in the patch file. In many \nother systems, patches are limited to certain forms, and so software development is similarly limited. \nFor exam- ple, in Dynamic C++ classes [12], changes are limited to instance methods and data; static \nmethods and data can-not evolve. As a result, the process of generating patches is tied to development, \nwith the newest version of the software having artifacts of the old version, such as useless fields in \nstructures or additional copies of static data. 3.3.2 Automatic Patch Generation A novel aspect of our \napproach is the (mostly) automatic generation of patch files. This feature was originally born out of \nconvenience: it is very tedious to write state trans- formation and stub functions by hand. It has also \nproven invaluable in minimizing human error: it is less likely that a necessary state transformation \nor stub function will be ac- cidentally left out. As it turns out, a very simple syntactic comparison \nof files, informed by type information, can do a good job of identifying most changes. The job of the \npatch generator is twofold: identify changes to functions and data, and when possible, generate appropri- \nate stub functions and state transformers. The identification algorithm is simple. First, both the old \nand new version of the file to patch are parsed and type-checked. Then, for each definition in the new \nfile, the corresponding definition is looked up by name in the old file. In the case of type definitions, \nthe bodies of the definition are compared and differences are noted. In the case of value declarations, \nthe bodies are also compared syntactically, taking into account the differences in type definitions; \nin particular, the syn- tax of a function may remain the same from the old to the new version, but the \nfunction has actually changed if a type definition mentioned in the body has changed. After the identification \nhas completed, the state trans-formation code is generated. For all global variables that remain unchanged, \nan assignment statement is created from the old to the new versions, like the one for num in Figure 2. \nFor those global variables that have changed type, appropri- ate code is generated automatically, when \npossible. For ex- ample, in FlashEd, our updateable webserver (described in \u00a75), we often change the \ntype definition httpd_conn, which contains information about a pending connection. All con- nections \nare stored in a global array of httpd_conn's. In this case, the generator automatically inserts a loop \nthat copies from the old to new array, calling a type conversion function for each element, which is \nalso generated automatically (to the extent possible), as explained below. The patch generator also generates \ndefault stubs for func- tions that have changed type. Two basic modes are pos-sible. In the simplest \nmode, the generator merely inserts a statement that raises an exception. This is useful when all patches \nfor the running program are to be applied si- multaneously. In this case no stub functions should ever \nbe invoked, so the exception signals an unexpected error. The second mode is to automatically generate \na call to the new version of the function, first translating the arguments ap- propriately. Because we \nhave, to this point, only applied all patches simultaneously, we have not yet implemented this mode, \nalthough it should be straightforward. During the identification phase, the patch generator keeps track \nof any type definitions that have changed, and gener- ates new names for these types by taking the MD5 \nhash of the pretty-printed type definition. This allows development of patches by multiple programmers \nwithout the worry of choosing incompatible type names. Finally, type conversion functions are constructed \nto the extent possible for data conversion from old to new versions of a type, and vice versa. These \nare used by the state trans- formation and stub code, as mentioned above. For struct types, each field \nwith an unchanged type is copied; each field that is added is given a default value; and each field that \nhas changed type is translated. Values of union type are decon- structed and then reconstructed at the \nnew type, translating any fields that have changed. In the case that a translation is not possible, a \nplaceholder is left for the programmer to fill in the appropriate value. Currently we support transla- \ntion between like types (i.e., int and float), and struct and union types (by calling the appropriate \ntype conversion function). 3.3.3 When to Apply Patches A critical component of assuring patch correctness \nis the timing of an update. In particular, it is possible for a well- formed update to be applied at \na bad time, resulting in incorrect state. For example, consider the file f and its patch, shown in Figures \n1 and 2, respectively. Here the patch state transformation function S copies the current value of hum \nto the nev~ version. The new code then uses this new version of ram. If this patch is applied while f \nis inactive (that is, f is not currently running, and not on the stack) then everything will be fine. \nHowever, if (the old version of) f begins execution just before the patch is applied, it will increment \nthe old version of mun a~er it has been copied by S. The result is the new version of nuan will not reflect \nthe call of f. Unfortunately, Gupta has shown that the problem of cor- rect timing is, in general, undecidable \n[9]. Thus, in exist- ing systems, programmers must identify correct timing con- ditions for a given patch, \na task which typically must be done by hand [13, 6] or with very limited automated sup- port [9]. Furthermore, \ndynamically enforcing these condi- tions requires special runtime support [13] or restrictions to updating \nonly inactive code [7, 14, 6], which still does not necessarily guarantee that race conditions of the \nabove sort will not occur. Instead, we observe that the problem of timing can be greatly simplified by \nrequiring the program to be coded from the outset so that updates are only permitted at well- understood \ntimes. This transfers the timing enforcement issue from run-time to compile-time: rather than assuming, \nas past approaches do, that a program will not be aware that it is updateable, and thus updates may conceptually \noccur at any tinm, we instead require the program to be coded to perform its own updating. Furthermore, \nnot only can we 'eyeball' the code to determine an appropriate spot, we can use the techniques of previous \nauthors mentioned above to determine one. The difference is that this spot is codified at software construction \ntime, as opposed to specified and enforced at runtime. As a result, we avoid the implementation complexity \nof update timing enforcement, without losing the benefits of correctness. The cost is that the system \nmust he constructed appropriately from the outset. However, given that the clientele of dynamic updating \nsystems already recognize the need for updates, this is perfectly reasonable. Our own experience, and \nthat of other updating systems, such as Erlang [3], indicate that this burden is not great, espe- cially \ncompared to the complexity of application-specific ap- proaches that use hot and cold standbys. 4. IMPLEMENTATION \nWe have implemented our framework to target Typed As- sembly Language (TAL) [16]. Both TAL and its cousin, \nproof-carrying code [17], belong to a framework we call ver-ifiable native code, in which native machine \ncode is coupled with annotations such that the code is provably safe. A well- formed TAL program is memory \nsafe (i.e. no pointer forg- ing), control-flow safe (i. e. no jumping to arbitrary memory locations), \nand stack-safe (i.e. no modifying of non-local stack frames) among other desirable properties. TAL has \nbeen implemented for the Intel IA32 instruction set; this implementation, called TALx86 [15], includes \na TAL veri- fier and a prototype compiler from a safe-C language, called Popcorn, to TAL. This section \npresents the details of our implementation, including how we implement dynamic updating by code re- linking, \nand how we define patch files and compile them to TAL. 4.1 Dynamic Updating In previous work, we added \na type-safe dynamic linker to TALx86 [11]. Our current work extends that work to pro- vide dynamic updating \nfor Popcorn programs. We briefly describe the existing dynamic linker, and follow with the changes we \nmade to support dynamic updating. At the core of the TAL dynamic linker is a simple prim- itive, load, \nthat loads and verifies TAL modules. The re-mainder of the linker's functionality, which includes linking \nand symbol management, is written in Popcorn, and can thus be proven type-safe, adding to the implementation's \nrobustness. Dynamically loadable files are compiled so that their ex- ternal references are indirected \nthrough a local table called the global offset table (GOT) in the style of ELF dynamic linking [22]. \nAt load-time, the entries in this table are re- solved with the exported definitions of the running program. \nThese definitions are tracked by the dynamic linker within a global dynamic symbol table. This 'table' \nconsists of a linked list of hashtables, one per module, that maps sym- bol names to their addresses. \nIn ELF, both the GOT and the dynamic symbol table are encoded as part of the object file header, but \nin our system, they are written in Popcorn. In particular, the GOT for each loadable file is constructed \nautomatically via a source-to-source translation, and the dy- namic symbol table is generated and maintained \nby symbol management part of the dynamic linker. As a result, the indirection facility and the the process \nof linking can be checked for type-safety. To support dynamic updating, we alter this scheme only slightly. \nSay we are loading a patch for some program mod- ule A. 1. All files, whether statically- or dynamically-linked, \nare compiled to have a GOT, and external references are indirected through that GOT. 2. When the patch \nfor A is loaded, a new hashtable is created to be stored in the dynamic symbol table. Once the patch \nhas been linked with the running pro- gram, the patch's state transformer copies the old state from A, \ntransforming it as necessary. If an error oc- curs during linking (e.g a symbol is looked up at the wrong \ntype) or state transformation (some exception is raised), then we roll back to the old version of A. \nThis can be done by simply throwing out the new hashtable, since the old code and the old hashtable has \nnot been modified. Once state transformation is complete, the existing code in the program is relinked; \nthis includes the present version of A in case that code is still active. The result is that the GOT's \nof each of the existing files will have their entries redirected to the new code's symbols. Finally, \nthe old A's hashtable is essentially removed from the dynamic symbol table (see below). When applying \nmultiple patches simultaneously, we re- quire more than one linking pass, since patches may contain mutually-recursive \nreferences, but the gist is the same,  To properly support these operations, we modified our dy- namic \nlinker to support the following features:  Exporting static variables. This allows state trans- formers \nhave access to all global state. To avoid name clashes between files, we prepend local variables with \nfilename : : Local : :.  Customized linking order. This allows us to look up ex- isting table entries \nbefore they are overwritten; this is important for state transformer functions, which may refer to both \nold and new versions of a given variable.  Rebinding. We can map symbols in the program to different \nnames in the dynamic symbol table: This allows us to replace function symbols with stubs that do not \nhave the exact same name. \u00ae Secondary tookups. After a patch is loaded, say for module A, the old version \nof A needs to be relinked in case it is still active. In this case, if a lookup during the relinking \nfinds a requested symbol at the wrong type, it secondarily looks for the old version of that symbol in \nan older hashtable. This circumstance will only occur when a symbol changes type and does not, or cannot \nin the case of data, define a stub function. Because the old A is going to shortly be outmoded by the \nnew A, we allow the code to use the old version of the symbol. In contrast, when relinking the rest of \nthe program (i.e. everything but the old version of A), we do not allow secondary lookups, effectively \nenforcing that current code always refers to the most current symbols. Weak pointeT~. Once relinking \nis complete, we would like to remove any old hashtables from the dynamic symbol table to make the old \ncode unreachable, and thus garbage-collectible. However, doing so is strictly correct because there is \nno guarantee that this old code will not be active at the next update, and thus need to be relinked. \nAn effective compromise is to keep tile old hashtables linked into the dynamic symbol table with weak \npointers. Weak pointers do not keep data from being garbage collected when not reachable by some non-weak \npointer elsewhere in the program, and thus code can be collected when it is no longer needed. Unfortunately, \nTAL does not currently support weak pointers, but adding them would be straightforward. To simulate the \nweak pointer implementation, tbr pur- poses of understanding the performance of updated programs, we \nremove the old tables following an up-date, but ensure via program construction that the removed code \nwill not be active at the next update.  4.2 Patches Our implementation of dynamic patches closely follows \nthe abstract description of \u00a73.1. The contents of a patch are de- scribed by a patch description file \ncontaining four parts: the implementation filename, the interface code filename, the shared type definitions, \nand the type definitions to rename. The first two fields describe the patch: its implementation in the \nfirst file, and the state transformer and stub functions in the second file. The final two fields are \nfor type names- pace bookkeeping. The shared type definitions are those types that the new file has in \ncommon with the old, while the changed definitions are in the renaming list, along with a new name to \nuse for each. The compiler uses this infor- mation to syntactically replace occurrences of the old name \nwith the new one. As introduced in the state transformation function of Fig- ure 2, we need a way to \nrefer to different versions of a vari- able within the interface code file. For a variable x, we may \nwish to differentiate between the old version of x, the new version of x, or the stub function for x. \nThis is achieved by prepending the variable references in the interface code file with New: :, 01d: :, \nand Stub: : respectively. With no prefix, the reference defaults to the version available before the \npatch was applied; this turns out to simplify how we compile patch files. The patch file is compiled \nby translating it into a normal Popcorn file, and then using the normal Popcorn compiler. The translation \nworks as follows. First, all definitions in the implementation file whose variables are in the sharing \nlist are made into externs, which will resolve to the old ver-sion's definitions at link time. Second, \nall of the defined variables (non-extern) in the implementation file are pre-fixed with New: :. Third, \nthe interface code file and the implementation file are concatenated together. Finally, all the mappings \nfrom the renaming list are applied to the file's type names. The resulting file is then compiled to be \nload- able and updateable, as described above. 18 To I changed A source I total interface LOC vet files \ntypes LOC] patches auto by hand 0.2 11 3 433 16 1324 48 0.3 9 2 813 14 1261 99 0.4 7 1 1557 ~ 12 1214 \n99 Table 1: Summary of changes to versions 0.2 through 0.4 of FlashEd 5. THE FLASHED WEBSERVER To demonstrate \nour system, as well as to further inform its design and implementation, we developed a dynamically-updateable \nwebserver, based on the Flash webserver [19]. Flash consists of roughly 12,000 lines of C code and has \nper- formance competitive with popular servers, like Apache [1]. We constructed our version, called FlashEd \n(for Editable Flash), by recoding Flash in Popcorn while preserving its es- sential structure and coding \ntechniques, In this section, we use FlashEd as a case study to explain three aspects of our system: how \nto construct an updateable application, how to construct and test patches in practice, and how dynamic \nup- dateability affects application performance; we look at the first two of these points in this section, \nand discuss perfor- mance in the next section. 5.1 Building an Updateable Application Flash's structure \nis quite amenable to ensuring patches are well-timed. It is constructed around an event loop (in a file \nseparate from that of main) that does three things. First, it calls select to check for activity on client \nconnections and the connection listen socket. Second, it processes any client activity. Finally, it accepts \nany new connections. This kind of event loop is common in server applications. Only two changes were \nneeded to Flash to support dy- namic updating. First, we added a maintenance command interface. A separate \napplication connects to the webserver and sends a textual command with the files to dynamically load. \nAfter the select completes, a pending maintenance command is processed and the specified dynamic patches \nare applied. Upon completion, the event loop exits and re-enters the loop (thus reflecting any change \nto the file containing the loop) and continues processing. Relevant state is preserved between loop invocations. \nThe second change was to how errors were handled. Flash contains many places where exit is called upon \nthe discov- ery of illegal conditions. Such aborts are not acceptable in a non-stop program, so we changed \nthese cases to throw an exception instead. When the event loop catches any unex- pected exceptions, it \nprints diagnostics, shuts down existing connections, and restarts. If an exception is thrown from a module \nthat maintains state, that state is also reset. Thus the program can continue service until it can be \nrepaired, albeit with the loss of some information and connections. 5.1.1 Patching To gain experience \nevolving a program using our system, we constructed FlashEd incrementally: Our initial imple- mentation \n(version 0.1) lacked some of the Flash's features (such as dynamic directory listings) and performance \nen- hancements (such as pathname translation caching and file caching). We added these features, one \nat a time, following the process outlined in \u00a73.3.1. Version 0.2 adds pathname translation caching; version \n0.3 adds file caching; and ver-sion 0.4 adds dynamic directory listings. Information about the changes \nbetween versions, includ- ing the patches that resulted, is summarized in Table 1. Columns two to four \nof the table show the changes to the source code made from the previous version, including the number \nof changed or added source files (not including header files), the number of changed type definitions, \nand the num- ber of changed or added lines of code. The last three columns describe the patches, including \nthe total number of patches generated (not including the type conversion file), the total lines of generated \ncode for the patch interface code files, and those lines that were added or changed by hand. There are \ntwo things to notice in the table. First, the number of patches gencrated exceeds the number of changed \nsource files; this is because certain type definitions changed, such that functions in those files that \nrefer to those types also effectively changed. Second, the number of lines of in- terface code automatically \ngenerated far exceeds the amount modified or added by hand. This is not to say that the process of modifying \nthe automatically-generated files was simple (it was not in some cases), only that a large portion of \nthe total work. much of it tedious, could be done auto- matically. For example, many of the generated \nlines include extern statements that refer to the old and new versions of changed definitions; ~hese \nwould have had Go be placed by hand otherwise. Most importantly, using the patch gen- erator guaranteed \nthat the patches were complete all of the changes were identified automatically, even though some changes \nneeded to be addressed by the programmer. The alterations to the generated files usually had one of three \nforms. First, we had to write code in the state trans- former function to translate pointerful data. \nFor example, sometimes various connect ion handler functions changed~ so we had to translate references \nto those handlers in the global handler array to point to the new version. Second, we had to occasionally \nfill in placeholders in the generated type con- version functions, particularly for function pointers \n(like the per-connection timeout function) and newly-defined types (like a struct added to manage cached \nfiles). Finally, we had to add code to the stake transformer to initialize new functionality; this code \nalready exists in (and was copied from) main, but because the new functionality is added dy- namically, \nthe code must run in the state transformer. 5.2 Experience To simulate a production environment, we have \nbeen run- ning a public server and attempting to never shut it down. making all changes on-line. A brief \nchronology for FlashEd is shown in Figure 3. We started version 0.1 at http: //flashed. cis. upenn, edu \non October 12, 2000, to host the FlashEd homepage. We applied patches for version 0.2 on October 20 and \nfor version 0.3 on November 4. All patches were tested offline on a separate server under various con- \nditions, and when we were convinced they were correct, we applied them to the on-line server. Even so, \nwe found a mis- take in the first patch--a flag had not been properly set-- and applied a fix on October \n27. In addition, we applied roughly five small patches for debugging purposes, such as to print out the \ncurrent symbol table. Running the server has revealed which aspects of the sys- tem work well and which \ndo not. For instance, we learned 12 Oct 2000 initial version 0.1 (only / index, html) J'fixed date parsing \nbug 20 Oct 2000 version 0.2 ~added pathname translation caching 27 Oct 2000 completed date parsing fix \nadded 32 MB file cache version 0.3 ,~ added new maintenance commands 4 Nov 2000 ] handling for previously-fatal \nexceptions k eliminated spurious hangup message 7 Feb 2001 version 0.4 ... dynamic directory listing \nFigure 3: Timeline of FlashEd updates soon after we deployed the server that our version of the TAL \nverifier is buggy--it only checks a subset of all of the basic blocks in loaded files. Since the verifier \nis part of the trusted computing base, it cannot be updated. As a result, we shut down the server on \nFebruary 71 and redeployed it compiled with the new verifier. To accommodate these kinds of changes dynamically, \nwe could allow certain trusted code to be loaded without benefit of verification. We also made a human \nerror when compiling the server: we forgot to enable the exporting of static variables when compiling \nthe library code. This problem became apparent when we attempted to dynamically update the dynamic up- \ndating library. The library was not properly removing old entries from the dynamic symbol table, and \nso we wanted to patch the library to fix the problem, as well as clean up the existing symbol table. \nHowever, since the symbol table is declared static, it was not available for use by the patch. As a result, \nany update to the library is effectively precluded since the state cannot be properly transferred. On \nthe whole, however, the system has been easy to use, since the only burden on the programmer is to fill \nout parts of the patch that the automated generator leaves out, and then to test the patches off-line. \nIt has been particularly effective to be able to load code to print out diagnostic in- formation. For \nexample, on a number of occasions we loaded code that would print out the dynamic symbol table (by call- \ning an existing function in the updating library) to make sure that symbol names referenced in our patches, \nparticu- larly the ones chosen for static variables, matched the ones present in the table. We also loaded \ncode to print out the state of the file and translation caches, to make sure that things were working. \nHaving the verifier to check patches as they are being loaded has been quite valuable. For example, we \ntried to apply some patch files that were incorrectly generated; the implementation file path mentioned \nin the patch description file was for an incorrect version. As a result, some of the type definitions \nwere incorrect, and this fact wds caught by the verifier. Once we applied a patch whose state transformation \nfunction failed to account for null instances; the updating library caught the NullPointer exception \nand rolled back the changes made to the symbol table. Using an unsafe language, such as C, would have \nresulted in our non-stop 1Actually, the power was accidentally shut off on the server, and so we took \nthat opportunity to make the change. system stopping with a core dump. 6. PERFORMANCE ANALYSIS Adding \ndynamic-updating imposes a number of costs on the system. At update-time, each patch must be verified \nand linked. At run-time, each external reference entails an extra indirection, essentially inherited \nfrom dynamic linking. In this section, we present the results of some experiments that measure these \ncosts. Our experimental cluster is made up of four dual-300 MHz Pentium-II's with split first level caches \nfor instruction and data, each of which is 16 KB, 4-way set associative, write- back, and with pseudo \nLRU replacement. The second level 4-way set associative cache is a unified 512 KB with 32-byte cache \nlines and operates at 150 MHz. These machines receive a rating of 11.7 on SPECint95 and have 256 MBs \nof EDO memory. Each machine is connected to a single Fast Ethernet (100 Mb/s), switched by a 3Corn SuperStack \n3000. We run fully patched RedHat Linux 6.2, which uses Linux kernel version 2.2.17. 6.1 FlashEd runtime \nperformance The only runtime overhead of our implementation is in- curred through the use of the GOT, \nwhich is inherited from dynamic linking. We could avoid this cost by implement- ing our linker to resolve \nexternal references in-place, as de-scribed in [11]. Using a GOT, each external reference re, quires \ntwo additional instructions, which adds about 2 cy- cles (or 6.7 ns) on our machines. By itself, this \noverhead is not very meaningful, since its overall effect is application- specific, depending on both \nthe number of external function calls made during execution, and the amount of computa- tion that occurs \nbetween those calls. To provide context, we examined the impact of this overhead on FlashEd's applica-tion \nperformance. To measure server performance, we used httperf (v0.8), which is a single, highly-parameterizable \nexecutable process that acts as an HTTP client. It can generate HTTP loads in a variety of ways, being \nable to simulate multiple clients by using non-blocking sockets. To ensure that the server is saturated, \nmultiple httperf clients can be executed concur- rently on different machines. Throughput is measured \nby sampling the server response over fixed intervals, and then summarizing the samples at the end of \nthe test. Our experimental setup is as follows. One machine runs the webserver, and the other three run \nhttperf clients. To simulate 'typical' client activity, we ran a log-based test, in which each client \nuses an identical filelist containing a list of files to request, with a corresponding weight for each \nfile. Each request is determined pseudo-randomly, as preferenced by its weight. Our fitelist was obtained \nfrom the WebStone benchmarking system [23], which claims it to be a fair rep- resentation of file-based \ntraffic. We used 90 second sample times, and we measured each server for roughly 32 minutes, totalling \n21 samples. Because we observed skewed distribu- tions in many cases, we report the median, rather than \nthe mean, and use the quartiles to illustrate variability. (We had to make some minor changes to httperf \nto run this test.) Figure 4 shows the results of our measurements. The X- axis varies with server version; \nthe first three columns show the throughput for FlashEd 0.1, 0.2, and 0.3, respectively, and the fourth \ncolumn shows the throughput for Flash (com- piled using gcc version egcs-2.91.66 with flag -02) as a \npoint 80 d~ ,-~ 75 \u00d7 static m updateable \u00a2 updated t 70 t ttt 0.1 0.2 0.3 C Server Version Figure 4: \nFlash and FlashEd throughput by version of reference. The Y-axis shows throughput in Mb/s (note that \nit does not start at 0). For each version of FlashEd, we measured the server's performance when it was \ncompiled with and without updating support (labeled static and up-dateable in the figure, respectively), \nas well as when it was patched on-line (labeled updated in the figure); for exam- pie, the updateable \nFlashEd 0.3 was compiled directly from the version 0.3 sources, while updated FlashEd 0.3 was com- piled \nfrom the version 0.1 and then patched twice dynam- ically. We suspected (correctly or incorrectly) that \nan up-dated FlashEd would have higher overhead than an update able one due to a larger heap and memory \nfootprint, since it retains the original version of the code in the text segment while new code is loaded \ninto the heap. For each server we show the median throughput, with the quartiles as bars. 6.1.1 Analysis \nThe overhead due to updating is the difference in perfor- mance, per server version, between the medians \nof the static and updated/updateable versions. In all cases, this overhead is between 0.3% and 0.9%, \nwhich is negligible when com-pared to the measured variability. This variability is not un- expected \nbecause the URL request pattern seen by the server differs from sample to sample, since the URL's requested \nin aggregate by the three clients will differ during each sample (we chose longer sample times to mitigate \nthis effect). To reduce experimental variability, we also ran tests in which the httper:f clients constantly \nrequest the same URL, using a variety of URL file sizes. For these tests, the variabil- ity dropped significantly \n(the semi-interquartile range was typically 0.25% of the median, as opposed to about 3% for the log-based \ntest), and the overheads were similar, ranging from 2.3% for a 500B file and 0% for a 500KB file; details \non these tests can be found in [10]. The measurements do not consistently favor either the updated or \nupdateable code. In particular, for FlashEd 0.2, the updated server is slightly faster than the updateable \none, while the reverse is true for version 0.3. The fact that the relative and absolute locations of \nthe code in an updated program is different than the updateable one may be one source of difference, \nsince the same modules will be affected differently by cache policy. In addition, because the heap sizes \nare the same but the updated program uses some of this heap to store update code, we have observed that \nthe 3-o other m consistency checking u disassembly 2- 1 Illll I ' ' I 50000 100000 150000 file size (B) \n Figure 5: Time to apply dynamic patches updated code garbage collects more often, favoring the up- \ndateable code in this regard. However, in general this differ- ence is well within the measured variability \nof the numbers and may be due to experimental variation. We are encouraged by the fact that FlashEd 0.3 \nhas per- formance essentially identical to that of Flash, though at first this is surprising, given our \nprototype compiler. How-ever, much of the cost of file processing is due to I/O, re-ducing the benefit \nof compiler optimizations; a more CPU intensive task would certainly favor the C implementation. In any \ncase, FlashEd's favorable performance suggests that TAL, and verifiable native code in general, is a \nviable plat- form for medium-performance, I/O-intensive applications. 6.2 Load-time overhead Our updating \nsystem also imposes a load-time cost to link and verify dynamic patches. We measured the component costs \nto apply patch files, and in Figure 5 we present mea- surements for the patches to update FlashEd version \n0.2 to version 0.3. All of these files are applied together, due to the mutually-recursive references \namong them, for a total time of 16.2s; 0.81s of this time was needed to relink the program. In the figure, \nthe X-axis is the total size of the compiled patch file, and each bar sums the total time to perform \ndy- namic linking for that patch. We break down dynamic ap- plication into three operations: disassembly \nand consistency checking, which are the core of TAL verification, and other, which is the combination \nof the remaining costs, including the time to load and link the file, and to verify that its interface \n(the types of its imported and exported symbols and its exported type definitions) is consistent with \nthat of the program. We can see that the total time is dominated by verification, averaging 72% for consistency \nchecking and 25% for disassembly. According to [8], verification is gen- erally linear in the size of \nthe files being verified, which we find to be true here. In many contexts, loading times of this magnitude \nare not a problem. For example, 16 seconds of pause time is less intrusive than an OS reboot. In the \ncase of the webserver, an infrequent pause is at worse inconvenient to the user, but not harmful to the \nsystem. However, in other contexts, there may be good reason to want shorter update times. We have identified \nthree means of reducing the load-time cost. First, the verifier could be further optimized, given that \nproof-carrying code [17] has demonstrated much smaller verification times, albeit with a different type \nsystem, and even TAL's implementors recognize that further gains could be made [8]. Second, verification \ncould be performed in par- allel with normal service, meaning that the system need only be stopped for \nlinking and relinking, which have negligible cost. Finally, in the case of a completely trusted system \n(as is FlashEd, for example), we can safely turn off on-line consistency checking, running it instead \nfor each loaded file on some other machine. Leaving on link-checking ensures that the loaded code meshes \nwith the running program at the module level, but trusts that the contents of the loaded module are well-formed. \nSince consistency checking is the most time-consuming operation, we greatly reduce our total update times \nas a result. 7. DISCUSSION ~Ib conclude, we discuss related work, place our current work into a broader \ncontext, and consider future work. We orga- nize the discussion around our four major criteria for evalu- \nating updating systems: flexibility, robustness, ease of use, and low overhead. A more complete discussion \nof related work may be found in [10]. 7.1 Flexibility At one extreme of the flexibility axis are systems \nthat use dynamic linking alone to support updating [2, 21]. These solutions are only adequate when the \nprogrammer can cor- rectly anticipates the form of future updates. Other systems are more flexible, but \ndo not allow arbitrary changes. For example, Dynamic ML [7] only permits changing the defini- tions of \ntypes that are abstract, and updated modules cannot remove or change the types of existing elements. \nThe Dy- namic Virtual Machine [14], a Java VM with updating abil- ity, and Dynamic C++ classes [12] similarly \nrequire class signature compatibility. At the other extreme of the flexibility axis are systems that, \nlike ours, allow nearly arbitrary changes to programs at runtime [13, 6, 9, 3, 4]. DYMOS [13] (DYnamic \nMOd- ification System) is the most flexible existing system; pro- grammers can not only update functions, \ntypes, and data, but can also update infinite loops. Like ours, some past sys- tems permit updates to \nactive code. A gradual transition from old to new code occurs at well-defined points, such as at procedure \ncalls [3, 13, 6, 4], or during object creation [12]. We believe our system sufficiently balances flexibility \nwith the other updating criteria: the generality of our dynamic patches allows us to achieve most of \nthe flexibility of the most general solutions, and programmer control of patch ap- plication gives good \nflexibility in timing updating. However, there are some important flexibility limitations we would like \nto address, as informed by our experience with FlashEd. PointerfulData. As mentioned in \u00a73.1, we rely \non the state transformer function to alter pointers to updated definitions that are stored in the program's \ndata; such references could be to functions (i.e. function pointers), or to global data. For instance, \nwhen some function f is updated, a function pointer to f must be modified during state transformation \nto point to the new f; the system does not do this automat- ically. While handling 'pointerful data' \nin this way seems reasonable for imperative languages like C and Popcorn, this approach is likely insufficient \nfor functional languages that make heavy use of closures (which are essentially function pointers), and \nmay prove problematic for object-oriented languages as well. An effective way to automatically update \npointerhfl data is to selectively use reference indirection. In particular, we have experimented with \nhaving the compiler modify the code so that rather than passing or storing a function pointer, we pass \nthe function's GOT entry. When the function pointer is actuMly used, the GOT entry is dereferenced, effectively \nretrieving the most recent version. This approach should ap- ply equally well to pointers to data as \nwell. We have largely implemented this idea, but still have a number of loose ends to tie up. Updating \nabstract types. In Popcorn, structures and un-ions can be declared abstract, meaning that only the code \nin the local file may see the type's implementation; this is enforced by the TAL verifier. As a result, \nno dynamically- linked file will be able to see the implementation of an ab-stract type. In general, \nthis behavior is desirable, but it also potentially prevents updating the abstract types, since it may \nbe impossible to translate objects of the old type to ones of the new implementation. We can solve this \nproblem by implementing a more flexible type management policy. In particular, whether a type's implementation \nis visible or not to loaded code can be made a matter of policy; code that updates an abstract type may \nbreak the abstraction, but all other code may not. This should be reasonably easy to implement with our \nlinker [11]. Unchecked updates. We currently support updates con-sisting of verifiable native code, but \nas motivated in \u00a75.1.1, we would also prefer to occasionally update the trusted ele-ments of the system, \nincluding the TAL verifier and runtime system. Implementing trusted updates should be straight- forward \nby using the underlying loader without the verifier. 7.2 Robustness Dynamic linking alone provides a \nsignificant advantage with respect to robustness over the more general updating system we have proposed, \nsimply because bindings are stable: once bound, a reference never changes. Previous work has lever- aged \nthis fact to try to build support for evolving systems that only use dynamic linking. For example, Appel \n[2] de- scribes an approach in which the old and new version of code can run concurrently in separate \nthreads, with the old version phasing out after it completes its work. Similarly, Peterson et at. [21] \ndescribe an application-specific means of stopping a program, updating its code, and then invoking the \nnew version with the old version's state. Both of these approaches suffer the problem that they are more \ndifficult to use and less flexible. However, as we explained in \u00a73.3.3, allowing code to change arbitrarily \ncan result in incorrect behavior if timing is not considered. While our approach allows a programmer \nto de- termine when updates will occur, much work remains for de- termining where such safe points lie. \nIn particular, things get more complicated with multithreading. Previous work [9, 13, 6] can serve as \na starting point for this investigation. Robustness is greatly strengthened by verifying impor- tant \nsafety properties of loaded code, including type safety. This is a key benefit to our approach, and to \nthe DVM [14], which makes use of Java bytecode verification. Other sys- tems benefit from the use of \ntype-safe source languages, like SML [2, 7], Haskell [21] and Modula [13], but must trust the compiler; \nwe need only trust the verifier. Erlang is dynami- cally typed, so runtime type errors are possible. \nMost other approaches are for C [6, 9] and C++ [12], which lacks the benefit of type-safety. 7.3 Ease \nof use Dynamic linking is generally easy to use and is well inte- grated into standard programming environments. \nAlso due to its widespread support in current languages and systems, it is also quite portable. In contrast, \nthe more flexible sys- tems are quite hard to use. In all of the existing systems, patches must be constructed \nby hand: the programmer must identify parts of the system that have changed and reflect these in the \nfile to load. In many cases, the limitations of patch files hamper the normal development process. Ease \nof use is one of the areas that our system makes the greatest contributions. Our basic methodology, in \nwhich programs are developed normally, and dynamic patches up- date the old version to the new, limits \ndisruption of normal work flow. In particular, the semi-automatic generation of patches greatly increases \nthe ease of use of our system, au- tomating the most tedious parts of patch generation, while letting \nthe programmer control the more subtle aspects that are not amenable to automation. 7.4 Low Overhead \nSome systems provide updating at no runtime cost, includ- ing Gupta's system [9], and Dynamic ML [7]. \nMost sys- tems employ reference indirection, either as we described in \u00a73.2.1 [3, 14, 12], or in slightly \nmore clever ways [13, 6]. Dynamic linking may (as in the case of ELF [22]) or may not impose an indirection, \naffecting systems like ours and those that use it exclusively [21, 2]. As we have demon- strated here, \nhowever, this extra indirection does not trans- late to high overhead in practice. Furthermore, because \nour approach is based on native code, it lacks the overhead of interpretation, e.g., as in the DVM [14]. \n8. CONCLUSIONS We have presented a system for dynamic software updat- ing built on type-safe dynamic \nlinking of native code. Our framework provides significant advances in balancing the tradeoffs of flexibility, \nrobustness, ease of use, and low over- heads, as borne out by our experience with our dynamically updateable \nwebserver, FlashEd. Acknowledgments We would like to thank the TALC group at Cornell Uni- versity, and \nmost especially Stephanie Weirich, Karl Crary (now at CMU), and Greg Morrisett, for the use and support \nof the TALx86 implementation, and for contributions to this work and work that led up to it. 9[$] REFERENCES \nThe apache software foundation, http://uww, apache, org. [2] A. Appel. Hot-sliding in ML, December 1994. \nUnpublished manuscript. [3] J. Armstrong, R. Virding, C. Wikstrom, and M. Williams. Concurrent Programming \nin Erlang. Prentice Hall, second edition, 1996. [4] T. Bloom. Dynamic Module Replacement in a Distributed \nProgramming System. PhD thesis, Laboratory for Computer Science, The Massachussets Institute of Technology, \nMarch 1983. [5] B. Buck and J. K. Hollingsworth. An API for runtime code patching, dournal of High Performance \nComputin9 Applications, 14(4):317-329, 2000. [6] O. Frieder and M. E. Segal. On dynamically updating \na computer program: From concept to prototype. Journal of Systems and Software, 14(2):111-128, September \n1991. [7] S. Gilmore, D. Kirli, and C. Walton. Dynamic ML without Dynamic Types. Technical Report ECS-LFCS-97-378, \nLaboratory for the Foundations of Computer Science, The University of Edinburgh, December 1997. [8] D. \nGrossman and G. Morrisett. Scalable certification for Typed Assembly Language. In R. Harper, editor, \nProceedings of the ACM SIGPLAN Workshop on Types in Compilation, volume 2071 of Lecture Notes in Computer \nScience. Springer-Verlag, October 2000. [9] D. Gupta, P. Jalote, and G. Barua. A formal framework for \non-line software version change. ~Yansactions on Software Engineering, 22(2):120-131, February 1996. \n[10] M. Hicks. Dynamic Software Updating. PhD thesis, Department of Computer and Information Science, \nUniversity of Pennsylvania, August 2001. [11] M. Hicks, S. Weirich, and K. Crary. Safe and flexible dynamic \nlinking of native code. In R. Harper, editor, Proceedings of the ACM SIGPLAN Workshop on Types in Compilation, \nvolume 2071 of Lecture Notes in Computer Science. Springer-Verlag, September 2000. [12] G. HjAlmt~sson \nand R. Gray. Dynamic C++ classes, a lightweight mechanism to update code in a running program. In Proceedings \nof the USEN1X Annual Technical Conference, June 1998. [13] I. Lee. DYMOS: A Dynamic Modification System. \nPhD thesis, Department of Computer Science, University of Wisconsin, Madison, April 1983. [14] S. Malabarba, \nR. Pandey, J. Gragg, E. Barr, and J. F. Barnes. Runtime support for type-safe dynamic Java classes. In \nProceedings of the Fourteenth European Conference on Obsect-Oriented Programming, June 2000. [15] G. \nMorrisett, K. Cra W, N. Clew, D. Grossman, R. Samuels, F. Smith, D. Walker, S. Weirich, and S. Zdancewic. \nTALx86: A realistic typed assembly language. In Second Workshop on Compiler Support for System Software, \nAtlanta, May 1999. [161 G. Morrisett, D. Walker, K. Crary, and N. Clew. From System F to typed assembly \nlanguage. ACM Transactions on Programming Languages and Systems, 21(3):527-568, May 1999. [17] G. Necula. \nProof-carrying code. In Twenty-Fourth ACM Symposium on Principles of Programming Languages, pages 106-119, \nParis, Jan. 1997. [18] M. Oehler and R. Glenn. HMAC-MD5 IP Authentication with Replay Prevention. Internet \nRFC 2085, February 1997. [19] V. S. Pat, P. Druschel, and W. Zwaenepoel. Flash: An efficient and portable \nwebserver. In Proceedings of the USENIX Annual Technical Conference, pages 106-119, Monterey, 1999. [20] \nD. Pescovitz. Monsters in a box. Wired, 8(12):341-347, 2000. [21] J. Peterson, P. Hudak, and G. S. Ling. \nPrincipled dynamic code improvement. Technical Report YALEU/DCS/RR-1135, Department of Computer Science, \nYale University, July 1997. [22] Tool Interface Standards Committee. Executable and Linking Format (ELF) \nspecification, May 1995. [23] Mindcraft--webstone benchmark information. http ://www. mindcraft, com/webstone. \n    \n\t\t\t", "proc_id": "378795", "abstract": "<p>Many important applications must run continuously and without interruption, yet must be changed to fix bugs or upgrade functionality. No prior general-purpose methodology for dynamic updating achieves a practical balance between flexibility, robustness, low overhead, and ease of use.</p><p>We present a new approach for C-like languages that provides type-safe dynamic updating of native code in an extremely flexible manner (code, data, and types may be updated, at programmer-determined times) and permits the use of automated tools to aid the programmer in the updating process. Our system is based on <i>dynamic patches</i> that both contain the updated code and the code needed to transition from the old version to the new. A novel aspect of our patches is that they consist of <i>verifiable native code</i> (<i>e.g.</i> Proof-Carrying Code [17] or Typed Assembly Language [16]), which is native code accompanied by annotations that allow on-line verification of the code's safety. We discuss how patches are generated mostly automatically, how they are applied using dynamic-linking technology, and how code is compiled to make it updateable.</p><p>To concretely illustrate our system, we have implemented a dynamically-updateable web server, FlashEd. We discuss our experience building and maintaining FlashEd. Performance experiments show that for FlashEd, the overhead due to updating is typically less than 1%.</p>", "authors": [{"name": "Michael Hicks", "author_profile_id": "81100060959", "affiliation": "Computer and Information Science Department, University of Pennsylvania", "person_id": "PP40023253", "email_address": "", "orcid_id": ""}, {"name": "Jonathan T. Moore", "author_profile_id": "81100045762", "affiliation": "Computer and Information Science Department, University of Pennsylvania", "person_id": "PP39024686", "email_address": "", "orcid_id": ""}, {"name": "Scott Nettles", "author_profile_id": "81100150673", "affiliation": "Electrical and Computer Engineering Department, University of Texas at Austin", "person_id": "PP14062810", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/378795.378798", "year": "2001", "article_id": "378798", "conference": "PLDI", "title": "Dynamic software updating", "url": "http://dl.acm.org/citation.cfm?id=378798"}