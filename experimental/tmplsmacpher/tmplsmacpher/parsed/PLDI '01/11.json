{"article_publication_date": "05-01-2001", "fulltext": "\n A Parallel, Real-Time Garbage Collector\" Perry Cheng Guy E. Blelloch Computer Science Department Carnegie \nMellon University Pittsburgh, PA 15213 {pscheng, guyb}0cs, cmu. edu ABSTRACT We describe a parallel, \nreal-time garbage collector and present experimental results that demonstrate good scalability and good \nreal-time bounds. The collector is designed for share&#38; memory multiprocessors and is based on an \nearlier collector algorithm [2], which provided fixed bounds on the time any thread must pause for collection. \nHowever, since our earlier algorithm was designed for simple analysis, it had some im- practical features. \nThis paper presents the extensions nec- essary for a practical implementation: reducing excessive interleaving, \nhandling stacks and global variables, reducing double allocation, and special treatment of large and \nsmall objects. An implementation based on the modified algo- rithm is evaluated on a set of 15 SML benchmarks \non a Sun Enterprise 10000, a 64-way UltraSparc-II multiproces- sor. To the best of our knowledge, this \nis the first imple- mentation of a parallel, real-time garbage collector. The average collector speedup \nis 7.5 at 8 processors and 17.7 at 32 processors. Maximum pause times range from 3 ms to 5 ms. In contrast, \na non-incremental collector (whether generational or not) has maximum pause times from 10 ms to 650 ms. \nCompared to a non-parallel, stop- copy collector, parallelism has a 39% overhead, while real- time behavior \nadds an additional 12% overhead. Since the collector takes about 15% of total execution time, these fea- \ntures have an overall time costs of 6% and 2%. 1. INTRODUCTION The first garbage collectors were non-incremental, \nnon- parallel collectors appropriate for balmh workloads on unipro- cessors [14, 4, 21]. However, parallel \nor multi-threaded ap- plications that run on multiprocessors need parallel garbage collectors since even \na dedicated processor cannot keep up with the memory requirements of more than a few proces- sors. Early \negorts on designing parallel collectors include *This work was supported in part by the National Science \nFoundation under grants CCR-9706572 and CCR-0085982, in part by DARPA CSTO, issued by ESC/ENS under Con- \ntract No. Ft9628-95-C0050. and in pars by an IBM Gradu- ate Student Fellowship in Computer Science. Permission \nto make digital or hard copies of ell or part of this work for personal or classroom use .s granted without \nfee provided that copies are not made or distributed for profit or commercial advan- tage and that copies \nbear this notice and the full citation on the first page, To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior specific permission and/or a fee. PLDI2001 6/01 Snowbird, \nUtah, USA &#38;#169; 2001 ACM ISBN 1.58113.414-2101106,..$5.00 Halstead's collector for Multilisp [18]. \nThe Multilisp col- lector, however, has problems scaling because it does not load-balance the collection \nwork. More recently, Endo used load balancing to achieve a scalable parallel mark-sweep col- lector [12]. \nThese ideas'were extended by Flood et. al. for a copying collector [15]. Other researchers using incremen- \ntal and concurrent collectors tackled the problem of elim- inating the lengthy pauses applications experience \nduring garbage collection. The elimination of such pauses is nec- essary to make languages with garbage \ncollection useful in applicatiotls with real-time constraints. Incremental collec- tors break up each \ngarbage collection into smaller pieces of work and interleave these increments of collection work within \nthe execution of the program [1]. Concurrent col- lectors run a single collector thread concurrently \nwith one or more application threads [23, 8]. These collectors, how- ever, do not consider running multiple \ncollector threads in parallel. In an earlier paper, we presented a collector algorithm that is both scalably \nparallel and real-time [2]. By making all aspects of the collector incremental (e.g., incrementally copying \narrays) and allowing an arbitrary number of appli- cation and collector threads to run in parallel, we \nwere able to achieve tight theoretical bounds on the pause time for any application thread as well as \nbound the total memory usage. Since the main purpose of that paper was to prove the time and space bounds, \nthe interface was somewhat simpli- fied and the algorithm had features that were asymptotically but not \npractically efficient. Furthermore the algorithm was a paper design with no implementation. In this paper \nwe present an implementation based on our previous collector algorithm along with an experimen- tal analysis \nof the implementation. The collector is designed for shared-memory multiprocessors and implemented within \nthe runtime system [6] for the TILT SML compiler. To make the algorithm efficient in practice and to \nmake it work within the con~ex~ of an existing language and compiler, we had to extend the algorithm \nin several ways. These exten- sions are described in this paper. The important additions related to performance \ninclude extending the algorithm to work with generations, increasing the granularity of the in- cremental \nsteps, separately handling global variables, delay- ing the copy on write, reducing the synchronization \ncosts of copying small objects, paratellizing the processing of large objects, and reducing double allocation \nduring collection. The most important extension of the interface was to allow program stacks the original \nalgorithm assumed activations records were allocated on the heap. To evaluate the algorithm and implementation \nwe imple- mented several variants, including stop-copy and concurrent (real-time) variants, and semi-space \nand generational vari- ants. The implementations were evaluated on a set of 15 benchmarks on a Sun Enterprise \n10000 shared-nlemory mul- tiprocessor. Out\" experiments show that compared to a base- line non-incremental, \nnon-parallel collector, scalable paral- lelism on average has a 39% overhead while concurrency and real-time \nbounds add another 12070. Since the collector takes about 15% of the total execution time, these translate \nto an overall time cost of 6% and 2%, respectively. The average speedup of the collector is 7.5 at 8 \nprocessors and 17.7 at 32 processors. As for real-time bounds, our maximum panse time is about 3 ms to \n5 ms, b~rthermore we analyze the real-time performance in terms of a metric we call the min-imum mutator \n'utilization (MMU). The MMU allows one to see not just the maximum pause time, but the fraction of time \nthat the processor is available to the mutator (appli-cation) for any time window of a specified size \n(worst case across the full execution). This is a much more useful mea- sure of real-time capabilities \nthan just the maximum pause time. Depending on the parameter setting, our real-time collector provides \nthe program with 10% to 15% processor access in any 10 ms window. The features of our collector can \nbe summarized as follows. Parallel and Concurrent. Any number of collector and mutator (application) \nthreads can run simultane- ously.  Copying. The collector is a replicating collector, copy- ing reachable \ndata from the from-space to the to-space while the program is executing.  Real-Time. All phases of the \ncollector are incremen- tal, allowing pause times to be strictly bounded.  Handles Large Objects. Large \nobjects, like arrays of arbitrary size, are copied both incrementally and in parallel.  Uses Stacklets. \nThe program stacks of each thread are partitioned into stacklets to permit parallel pro- cessing and \nreal-time bounds even for deep stacks.  Barriers. Our collector never requires a read barrier  (i.e. \nmutators read data directly with no overhead). A write barrier is imposed on modifications to heap objects \nbut not to values in the stacks or registers. Header Words. For all small objects the per-object information \nused by the collector is kept in the tag word, requiring no extra space.  2. BACKGROUND AND DEFINITIONS \nWe describe a traditional semispace copying garbage col- lector [4], define when a collector is incremental, \nparallel, or concurrent, and introduce a new measure for analyzing the real-time behavior of a collector. \n2.1 A Semispace Stop-Copy Collector tn a semispace collector, heap memory is divided into two equally-sized \nregions: the from-space and the to-space. During normal execution, the mutator allocates new ob-jects \nfrom from-space. Eventually, continued allocation ex- hausts from-space causing the program to be suspended \nwhile the collector reclaims memory. The program's data can be viewed as a directed graph consisting \nof internal nodes corresponding to objects in the heap, root nodes corre-sponding to non-heap pointer \nvalues (i.e. in registers, global variables, and on the stack), and edges corresponding to pointers between \nobjects, and from the root nodes into the heap. The objects that can be reached fl'om the root: nodes \nare called the reachable objects. Under this interpretation, the collector traverses the memory graph \nstarting fkrom the roots and copies the reachable objects from from-space into to-space. When all reachable \nobjects are copied, the collec- tor is flipped off\" by updating the root values and reversing the roles \nof h'om-space and to-space. A useful terminology for describing garbage collection is the tri-color abstraction \n[8] which assigns one of three colors (white, gray, and black) to objects/roots. When a collection starts, \nall objects are white and the root nodes are gray. When a white object is copied, it becomes gray, signifying \nthat it has been copied but that it may still refer to white objects. When all of a gray object's children \nare copied it becomes black. A collection is complete when all reachable objects are black, or equivalently \nwhen there are no gray objects or roots left. At any time during the collection, the set of gray objects \nor roots can be seen as the fl'ontier of the graph traversal. To properly update all the pointers during \ncollection so that they point to the new copy, the collector needs to know the new location of each object. \nThis association is typically maintained with a forwardin 9 pointer from the old copy to the new one. \n 2.2 Types of Garbage Collectors The first garbage collectors, including the one described in the previous \nsection, were non-incremental (stop-collect) and programs (also called mutators) were periodically sus- \npended while the collector reclaimed memory. Over the years, various techniques were introduced to improve \ncot- lector efficiency, to reduce or eliminate pause times, and to adapt a collector suitable for use \non multiprocessors. Many of these techniques or collector properties are often used in isolation to address \na particular problem, creating a mis-perception that the techniques are mutually exclusive. In fact, \nthese techniques sometimes complement each other. To avoid confusion, we define the following terms: \n Incremental[1J: A single collection is divided into mul- tiple increments whose executions are interleaved \nwith the application on a single processor. Typically, the rate of collection is related to (and greater \nthan) the allocation rate so that the collection is guaranteed to terminate.  Concurrent[23, 8]: At \nleast one program thread and one collector thread are executing concurrently.  Parallel[18, 12]: Multiple \ncollector threads are collect- ing concurrently.  The reader should be aware that these terms are not \nused consistently across the literature. For example, some papers refer to concurrent collectors as being \nparallel while others exclude the notion of incrementality when they use the term parallel. The diagram \nbelow shows one collection cycle for the 3 types of collectors for the 2-processor case: [ Prog J Eprog \n--~ The collector is designed for a shared memory multipro- one collection ~r ~~2~'%1~'=~IZ incremental \nConcm~rent Parallel Both our original and modified collectors are simultane- ously incremental, concurrent \nand parallel. Each processor runs incrementally, but since the processors are not synchro- nized, any \ncombination of mutator and collector threads can run concurrently. This can be illustrated as follows: \n2 collectors/2 mutators ! ..... | [Prog 1~ Ill f!!i::!if [N I H f ! itl incremental, concurrent, and \nparallel 2.3 What is a Real-time Collector? Empirical studies of real-time garbage collectors typically \nreport pause times, with particular emphasis on the max-imum pause time. What can a programmer deduce \nfrom knowing that the maximum pause time is 25ms? Pessimisti- cally, he can deduce this is unsuitable \nfor applications that require a response time of 10ms. However, he cannot safely assume the collector \nis suitable for mouse tracking, which requires a response time of about 50ms. The problem is that statistics \nabout the pause time do not characterize the occurrences of the pauses, A burst of frequent short pauses \nis not much different from a single long pause. Several short pauses One long pause To capture both the \nsize and placement of pauses, we pro- pose a new notion called utilization. In any time window, we define \nthe utilization of that window to be the fraction of time that the mutator executes. For any window size, \nthe minimum utilization over all windows of that size captures the access the mutator has to the processor \nat that granular- ity. For example, mouse tracking might require a response time of 50ms and the mouse-tracking \ncode might take lms to execute. In that case, it is sufficient, to have a minimum utilization of 2% at \na granularity of 50ms. The minimum mutator utilization (MMU) is a function of window size and generalizes \nboth maximum pause time and collector over- head. The maximum pause time is the window size at and below \nwhich the MMU is zero and the collector overhead is the complement of the MMU at the granularity of the \ntotal execution time. 3. THEORETICAL ALGORITHM In this section, we review our previous collector algo- \nrithm [2] by a series of modifications to Cheney's simple copying collector [4]. Our treatment adds, \nin turn, paral- lelism, incremental collection, and concurrency. We end by describing the space and time \nbounds guaranteed by the col- lector. The reader is referred to our earlier paper for more details. cessor \nwith sequentially consistent memory. We assume that all objects are stored in a shared global pool of \nmemory. In addition to the typical uniprocessor instructions, the collec- tor uses a Fetchhndhdd instruction, \nwhich atomically reads a memory location and stores the incremented value back to the location, and a \nComparahndSwap, which atomically reads a memory location and, if the read value equals some given value, \nupdates the same memory location with a new value. The collector interfaces with the application via \n3 memory-related operations: allocating space for a new ob- ject, initializing the fields of a new objects, \nand modifying the field of an existing object. 3.1 Scalable Parallelism While running, the collector \nneeds to keep track of all the gray objects so that it can perform its traversal. The man- ner in which \nthe set of gray objects is maintained and the order in which they are visited is important. Cheney pro- \nposed a clever technique for maintaining the set implicitly with no additional space cost [4] by keeping \nthem in contigu- ous locations in to-space. This technique, however, restricts the traversal order to \nbreadth-first, and is also difficult to implement in a parallel setting. For these reasons, our col- \nlector represents the set of gray objects using an explicitly- managed local (per-processor) stack with \npointers to the gray objects. For the collector to scale to multiple processors, it is im- portant that \nno processor is idle during the collection. This can happen if one processor runs out, of gray objects \nwhile collectively there are more gray objects to be processed. Our collector uses work sharing to avoid \nidleness. In addition to the local per-processor stacks, we add a shared stack of gray objects accessed \nby all processors. Each processor periodi- cally transfers gray objects between its local stack and the \nshared stack. Thus no processor idles until there are no more gray objects in the shared stack. Other \nparallel collectors have used work-stealing for sharing work [12, 15]. Our shared stack is designed to \nallow fast parallel access. It separates in time the pushes from the pops, but allows an arbitrary number \nof pushes (or pops) to run in parallel. The pushes can proceed in parallel by using a FetchAndAdd to \nreserve a target region in the shared stack prior to the data transfer. This scheme fails when pushes \nand pops are concurrent since the target regions are no longer non- overlapping and the subsequent data \ntransfer cannot pro- ceed independently. To preventing concurrent pushes and pops while permitting multiple \npushes or multiple pops, we use a form of synchronization called room synchronization. This synchronization \nis described later in section 4.3. The main correctness issue that arises with multiple col- lector threads \nis the possibility that a white object is er- roneously copied twice, resulting in an incorrect graph \nin to-space. To prevent this from occurring, a per-object syn- chronization is used for object copying. \nBefore an object is copied, ~ copier must gain exclusive access to the object by atomically swapping \ninto the forwarding pointer field a flag designating that copying is in progress. Other proces- sors, \nif any, that had desired to copy the object would have failed to perform the atomic swap and instead \nnoticed that copying is in progress. These processors bu.sy-wait until the forwarding pointer is installed. \nWe refer to this mechanism as the copy-copy synchronization. 3.2 Incremental and Replicating Collection \nBaker proposed the first incremental collector [1]. For every unit of space that is allocated while the \ncollector is on, the collector would copy k units of data. This allowed him to homed tbe pause time by \nshowing that. each copy takes bounded time, and bound the memory usage by showing that the collector \nwill make sufficient progress toward the collection. The complication in designing an incremental collector \nis that the mutator can execute while the collection is only partially complete. Baker's algorithm handles \nthis by keeping a to-space invariant--the nmtator can only see the copied version of objects, which are \nin to-space. However, preserving this invariant means that each read of a pointer by the mutator requires \nan additional check. The check, called a read-barrier, has been experimentally found to be quite impractical \nsince reads are very fl'equent. To avoid a read-barrier, our algorithm uses a variant of the replicating \ncollector suggested by Nettles and O'Toole [22]. In a replicating collector the mutator can only see \nthe ori 9- inal copy, which is in from-space. While the collector exe- cutes, the mutator continues to \nmodify the objects in fi'om- space, which collectively form the prirnaw memory graph, while the collector \ngenerates a replica memory graph in to- space. To maintain the invariants necessary for correctness, \nreplicating collectors require a tvrite barrier. That is, when- ever an object's field is modified, both \nthe old and new object must be copied and colored gray. Since writes are significantly less frequent \nthan reads in nearly all languages and programs, this is less of a problem than the read-barrier This \nis particularly true in mostly functional languages such as SML. As with Baker's algorithm, while it \nis on our collec- tor collects k units of data for every unit that is allocated. This is done on a per-processor \nbasis--a processor that al- locates a unit of space must copy (i.e., gray) k units. 3.3 Concurrency A \ncollector is concurrent if the program and collector can execute simultaneously. Since the program only \nmanipu- lates the primary memory graph, which is not. disturbed by the collector (except for the forwarding \npointer field), the collector does not interfere with the mutator. On the other hand, the collector generates \nthe replica graph by travers- ing the primary graph, which is being modified by the mu- tator (hence \nthe name). Because a primary object might be modified after it has been copied, a replica object will \nnot necessarily be a faithful replica unless the correspond- ing modification is made to the replica. \nA race condition arises if a primary object is being modified around the same time that it is independently \ncopied. Correctly handling this situation requires a copy-write synchronization which delays the mutator's \nupdate to the replica if the modified field of the object is being copied by the collector, which is \nindicated by a flag in the replica object. More details on the necessity and correctness of this synchronization \ncan be found in our earlier paper [2]. The application model in our original algorithm prohib- ited \ndifferent threads from concurrently modifying the same memory location. Such concurrent writes create \na poten-tial problem in the collector. Normally, the write barrier requires that when a thread modifies \na (primary) object, it must perform the corresponding update to the replica ob- ject. If two threads \nmodify the same primary object, they will then perform concurrent updates to the same replica object. \nA write-write synchronization is necessary to pre- vent a thread from updating the replica with an ou~.-oLdate \nprimary value. More details on this synchronization can be found in Cheng's thesis [5]. 3.4 Space and \nTime Bounds The key property of our previous collector were the time and space bounds, These can be summarized \nas follows. Each memory operation will take no more than ca: time for some constant c. Intuitively, c \nis the time it takes to collect one word and k is the number of words we collect per word allocated. \nIf any application uses maximum reachable space R, maximum object count N, and maximum memory graph depth \nD (the depth of a path is the sum of the sizes of objects along that path), out\" collector on a P-way \nmul- tiprocessor requires at most 2 ( R( 1 + 1.5 / k) + N + 5 P D) space for any k > 1. For most applications, \nthe PD term is relatively small and objects are tagged so the space requirement reduces to 2//(1 + 1.5/k). \nThe proofs of these properties are contained in our previ- ous paper [2]. It should be noted that since \nthe emphasis of the algorithm was to prove the time and space bounds we kept the algorithm as simple \nas possible. This ment that the algorithm has some features that were asymptotically but not practically \nefficient. Furthermore it did not consider global-variables or program stacks--we assumed activation \nrecords and globals were stored in the heap. 4. EXTENDED ALGORITHM In this section, we present the extensions \nto the original al- gorithm necessary for practical efficiency and applicability in an language with \nstacks and globals. We can classify the ex- tensions into three categories. The first group includes \nfea- tures that are visible to the application and compiler such as the collector's ability to handle \nglobals and programs stacks. The challenge here lies not in adding code to the collector to process globals \nand stacks but rather to do so in such a way that the collector's scalability and real-time bounds are \nnot compromised. The second group of changes sys-tematically eliminates the unnecessarily fine granularity \nof the collector which imposes a huge overhead. Finally, there are fundamental extensions to the algorithm \nwhich produce significant performance improvements. 4.1 Globals, Stacks, and Stacklets Globals. Most \ncompilers assign globally accessible vari- ables to static locations in the data segment. Because global \nlocations are fixed at compile time, a procedure can always access them. Like registers, globals are \ndirectly accessible and so must be considered a root value when the collection starts. Similarly, a global \nlocation must be replaced with its replica value when the collection ends. Unlike the register set though, \nthere may be arbitrarily many globals. Thus the process of turning the collector off (which involves \nupdating global values) may take an unbounded amount of time mak- ing any real-time bounds impossible. \nThis problem can be solved by replicating globals like other heap-allocated object. That is, a global \nis actually a pair of locations, one containing the primary global value which is accessed by the mutator. \nThe other location is available to the collector for storing the replica global value. Because this \nalternate location can be modified without ef- fecting the mutator, it can be updated long before the \nend of the collection. A flag is used to indicate to the mutator which of the two location holds the \nprimary value and this flag is toggled at the end of a collection when the roles of the semi-spaces are \nreversed (only a single flag is needed for all the global variables). This arrangement doubles space \ncon- sumption for global locations and possibly imposes a slight penalty when accessing globals. The \npenalty, if any, is de- pendent on how gtobals are normally compiled. In theory, since globals are already \naccessed indirectly through a base pointer, there should be no cost. In practice, however, tak- ing advantage \nof this indirection prevents using the globals facilities already present in vendor-supplied assemblers \nand linkers. Stacks and Staeklets. Another som'ce of trouble con- cerning a timely flip comes from stacks \nof activation records that are used by many compilers. (Implementations that use heap-allocated read-onlyactivation \nrecords do not have this problem.) As the stack can hold pointer values, these loca- tions must be replaced \nwith their replicas when the garbage collector is turned off. However, the stack can be very deep and \ndisrupt the desired time bounds. In fact, the depth of the stack is a dynanfic property and so this problem \nis arguably worse than that of globais. Since we cannot wait until the end of a collection to flip all \nthe stack slots, we must replicate the stack as the collection proceeds so that the stack work is distributed \nover the entire collection. Replicating the stack is, however, difficult since the mutator is continually \nmodifying existing stack locations and pushing and popping stack frames. Instead we break up the stack \ninto fixed-sized stacklets which are linked together with some glue code. In this system, a deep stack \nbecomes a string of fixed- sized stacklets, at most one of which is active at any time. While the mutator \nis executing in the active stacklet, the collector has a chance to replicate the other stacldets. By \nthe time the collector is ready to turn off, only the most recent stacklet needs to be processed. Since \nthis stacklet is bounded in size, we can also bound the processing time. During the collection, the collector \nprocesses older stacklets before younger ones since younger stacklets are more likely to die and require \nno replication. Finally, traditional systems require that the size of a stack be fixed on creation of \na thread. Since the precise sta&#38; re- quirements of a thread are generally unknown, programmers typically \nmust estimate a safe upper bound and hope that the stack does not overflow. Safe overestimates waste \nspace or may be impossible for applications that generate many threads or threads whose stack requirements \nvary greatly. Stacklets are useful in this context as well since they bound the wasted space per stack \nto the size of one stacklet. In ad- dition, the programmer is not required to compute or guess a safe \nstack size [16]. 4.2 Granularity The original algorithm was designed for ease of proving correctness, \ntime, and space properties. Unfortunately, asymp- totic bounds fail to capture overheads a direct implementa- \ntion anay incur. In fact, the original algorithm invokes the collector whenever the collector allocates \nspace for an ob-ject, initializes a field of a new object, or modifies a field of an existing object. \nFor a language which typically inlines these operations such as SML, the increase in code size alone \nis prohibitive. For example, initializing a field, which nor- really takes one instruction, would become \na function call. Even for languages that do not inline these operations, the cost of a function call \noverhead for every field initializatmn, space allocation, and field modification is too high. In fact, \nthe overhead incurred is more than just from a function (:all, including also glue code for switching \nfrom the mutator to the collector as well as increasing the instruction cache miss rate. We next describe \ntwo changes which combine to batch up collector work, thus reducing the overhead associated with overly \nfrequent switching. Bloek Allocation and Free Initialization. Even though FetchAndAdd is a scalable construct \nira theory [17], using it for every memory allocation is inadvisable because of its ex- pense relative \nto an ordinary load-add-store sequence. Fur- thermore, since objects are often of a size comparable to \nthat of a cache line, allocating memory in such a fine grain fashion would split cache lines across different \nprocessors, causing false sharing. Instead of allocating memory directly from the shared heap, each processor \nmaintains a local pool of memory in from-space and, when the collector is on, a local pool in to- space. \nObjects are allocated from the pools if the request can be satisfied. Otherwise, the local pool is discarded \nand a new local pool is allocated from the shared heap using a FetchAndAdd. Field initialization does \nnot require calling the collector. Such a two-level allocation scheme has sev-eral benefits. By greatly \nreducing the frequency of the more expensive FetchAndAdd, the cost of memory allocation is re- duced \nto the usual copying collector allocation code which is short enough that it can be inlined. Cheap allocation \nis important for functional languages like ML. In addition, cache behavior is improved since related \nobjects, which are those manipulated by one processor, have improved spatial locality. Since the collector \nis no longer invoked per allocation or field initialization, context switching costs are much more reasonable. \nInstead, when the collector is on, an increment of collection work is performed whenever a local pool \nis dis- carded. Since the size of the work increment effects the reM-time bounds, the size of the local \npool must be chosen to balance efficiency and real-time demands. Finally, local pools make it possible \nfor the space for copy- ing an object to be eagerly allocated before a processor is certain it will be \nthe copier. If it should fail to be a copier, the memory is easily returned to its local pool without \ncaus- ing fragmentation. Without local pools, however, memory already allocated from a shared heap cannot \nbe returned without destroying the contiguity of the unallocated space in the heap. The ability to eagerly \nallocate space allows the busy-walt in the copy-copy synchronization for small objects to be eliminated. \nWrite Barrier. The need for a write barrier stems from the collector's incrementality. If the i TM field \nof object x con- taining pointer value y is updated with pointer value z, the write barrier grays the \nobject; y and if x has been copied, performs the corresponding update to x's replica and, in so doing, \ngrays object z. The code associated with these ac- tions is substantial compared to the actual pointer \nupdate which is one instruction. Inlining this code is impracticM. Even a function call is likely unacceptable \nbecause of the disruption to the instruction cache and processor pipeline and code size blowup. Instead, \nwe elect to only record the relevant information, the triple < z, i, p >, deferring the pro- cessing \nfor later. Note that the value z is not necessary since it is obtainable from x and i. Further, if the \nupdated loca- tion contains a non-pointer value, y may be omitted. This form of recording is similar \nto the write barrier often used in a generational collector, except that a generational collector needs \nto record only x + i. The memory traffic generated by the write barrier is very regular and well-behaved. \nIn ad- dition, on machines with multi-issue, the write barrier may have a low to zero cycle cost. No \nwrite barrier is necessary for the registers or the stacks. During execution, each thread has a write \nlog for storing the triples < x,i,y > that correspond to the mutations. Whenever the write log is full, \nthe collector is then invoked to process the write log. In fact, the collector may process the log whenever \nit is invoked, such as when its local pool is exhausted. Like block allocation, the write log serves \nto batch up write barrier work, eliminating freqnent context switches. Small and Large Objects. In our \noriginal algorithm, all objects, regardless of size, were treated in the same way. Because of the presence \nof large objects and the need to meet real-time bounds, all objects were copied incrementally, one field \nat a time. This turned out to have a significant overhead since each field requires reinterpretation \nof the tag word as well as transferring the object from and to the local stack. There- fore, instead \nof copying and scanning the fields of a small object one at a time, the entire object is locked down \nand the object is copied all at once. In addition to reducing the synchronization operations, the small \nobject is in the local stack once and its tag is decoded once. In practice, any object less than 512 \nbytes can be considered small so most objects are small, making this optimization very important. Because \nthe original algorithm scans fields one at a time in order, the collection can be incremental even when \nlarge objects are present. However, this scheme prevents the col- lection of any object, even large ones, \nfrom occurring in parallel. The processing of large objects can be parallelized by breaking them up into \nmultiple fixed-sized segments and associating a tag with each segment. The tags are stored as extra header \nwords of the object. The size of the seg- ment can be chosen for an appropriate degree of parallelism \n(e.g., 512 bytes). A smaller segment size allows increased parallelism but creates an increased space \noverhead due to more segment tags. Also, by using a per-segment tag, the copy-write synchronization is \nless likely to create contention for large objects.  4.3 Algorithmic Modifications Reducing Double Allocation. \nWhile the collector is on, all objects that are allocated in from-space by the mutator must be copied \ninto to-space. This allocation barrier policy is necessary to preserve the reachability invariant. As \nwith the write barrier, the code for this double allocation, in com- parison to the normal allocation, \nis substantial. Deferring the double allocation is simple and does not even have an additional recording \ncost like the write barrier. All that is required is to replicate all objects in the current local pool \nwhenever a new local pool is about to be allocated. In preliminary experiments, however, we found the \ncost of the double allocation to be substantial. This can be seen in the following analysis. Consider \nan application which in steady-state has L live data and is running with a fixed-sized heap of H. The \nliveness ratio is then r = L/[[. At the start of the collection, there is L live data which needs to \nbe copied by the end of the collection. During tile collection, L/k additional data is allocated and \nhence copied. This increases the effective survival rate from r to (r + r/k)/(1 + r/k). For not atypical \nvalues of 7\" = 0.2 and k = 2, the survival rate is increased from 20% to 27% which increases the collection \ntime by 35%. To reduce double allocation, we divide collection into two phases, aggressive and conservative. \nIn the first phase (ag- gressive), we perform collection without double-allocating new objects, so that \nat the end of this phase only data that was live at the beginning of the collection will have been copied. \nThe second phase (conservative) begins by recom- puting the root set and consists of a second much shorter \ncollection during which all new objects are double-allocated. At the end of the second phase, the replica \nmemory graph is complete and the collection can be terminated. The 2-phase scheme greatly reduces doubl4 \nallocation by confining it to a short second phase. In the first phase, L data is copied while L/k data \nis allocated. Of this, Lr/k is live and so we copy Lr/k data plus the additional Lr/k 2 since we are \nper- forming double allocation. The final effective survival rate is reduced to (r + r2/k + r2/k~)/(1 \n+ r/k + r~/k2), which, using r = 0.2 and k = 2 as before, yields 20.7%. This 2-phase algorithm requires \nan extra global synchro- nization mxd root set computation. However, out\" experi- ments show that this \ncost is more than compensated for by the reduced copying. This optimization does not require a read barrier \nnor modifying the existing write barrier. Rooms and Better Rooms. As explained in section 3.1, the shared \nstack needs a mechanism to ensure that pushes and pops do not occur at the same time. This can be ac- \ncomplished by creating two rooms, a pop room and a push room. A processor may push onto the shared stack \nonly when in the push room and pop from the shared stack only when in the pop room. The rooms mechanism \nguarantees that each processor may be in one of two rooms (or neither) but at most one of the rooms is \nnon-empty. Thus, concur-rent pushes and pops are avoided. The reader is referred to our paper on rooms \nsynchronization [3]. In our original algorithm and formulation of the rooms, a round of collection work \ninvolves the following steps in order: entering the pop room, fetching work from the shared stack, performing \nthe work, transitioning to the push room, return- ing work to the shared stack, and finally exiting the \npush room. The number and size of the rounds depend on the desired real-time bounds. This scheme has \nsome shortcom- ings. The time for graying objects is considerable compared to fetching work and a processor \ntrying to transition to the push room has to wait for all other processors already in the pop room to \nfinish graying their objects. The idle time can be significant since there may be significant variation \nin the time for different processors to gray objects. These problems can be eliminated by changing to \na more relaxed room abstraction in which a processor can leave the pop room rather than having to transition \nto the push room. Since graying objects is not related to the shared stack, it can be performed outside \nthe rooms. This greatly reduces the t;ime in which the pop room is active and the poten- tial wait time \nof any processor. This change relies on a new definition and efficient implementation of room synchroniza- \ntions that does not require processors to go through the rooms in order [3]. A subtle termination problem, \nhowever, arises with the more asynchronous rooms. In the original algorithm, since processors can have \ngray objects only within the pop-work- push cycle, we can detect that there are globally no more objects \nby having the last processor to leave the push room check that the shared stack is empty. The new version \nof the rooms places no constraints on when a per-processor lo- cal stack is empty. To correctly detect \nglobal emptiness, the shared stack must maintain a borrowed counter, updated with FetchAndAdd, that indicates \nhow many local stacks have borrowed objects from the global stack. Termination is now signalled when \nthe last processor to leave the push room detects that the shared stack is empty while the borrowed counter \nis zero. Generational Collection. Generational collectors were first proposed to reduce pause times [25]. \nThe simplest generational collector divides memory into a nursery and a tenured space. Objects are allocated \nfrom the nursery. When the nursery is exhausted, a minor collection is trig- gered in which the live \nobjects in the nursery are copied (or tenured) to the tenured area. When the tenured area is ex- hausted \nafter repeated minor collections, a major collection is triggered in which the live objects of the tenured \narea are copied to an alternate tenured area. The advantage of such an arrangement is based on the generational \nhypoth- esis which asserts that most allocated objects die shortly after they are allocated. If this \nis true, then a minor collec- tion is more productive at reclaiming space than a collection in a semispace \ncollector since a smaller fraction of objects needs to be copied. Pauses from minor collections are in- \ndeed short but the inevitable major collection still makes generational collectors unsuitable for real-time \napplications. However, since collections are on average more productive, generational collectors are \nmore efficient than semispace col- lectors when the generational hypothesis holds. In addition, they \nprovide better data locality for the application. Unfortunately, adding generations is not a straightforward \nextension to our collector. In a non-incremental generational collector, a minor collection involves \ncopying all live objects in the nursery (i.e., those objects reachable from the roots). In the presence \nof mutable data structures, objects in the tenured area may refer to objects in the nursery and such \nref- erences must be considered roots as well. Since the collector is incremental, these tenured references \nmay not be mod- ified during the collection since the mutator is executing. Instead the referenced objects \nare copied during the collec- tion and only at the end of the collection are the references themselves \nmodified. In general, the number of references is unbounded and these references cannot all be atomically \nmodified without brealdng the real-time property. The solution is to use two fields for each mutable \npointer field. During any particular collection, one field is used by the mutator while the other field \nis updated by the collec- tor. The roles of the fields are reversed at the end of each collection. This \nis similar to the technique we described for global variables. Our initial experiments show that this \nscheme allows the same real-time bounds to be met while giving most of the overall performance improvement \nof us- ing generations. Further, the benchmarks show that even a modestly-sized array of 10,000 pointer \nvalues was enough to make these modifications necessary. The major draw- back to this scheme is that \nfield access by the mutator is slightly more expensive and more space is required for mu- table pointer \nfields. 5. EVALUATION To evaluate the algorithms, we implemented several col- lectors within the runtime \nsystem [6] for the TILT SML compiler [24]. SML is a statically typed, functional language with a module \nsystem. The prevalence of closures, algebraic datatypes, and tupling in most SML programs leads to a \nhigh allocation rate, providing a tough challenge for a collec- tor. Further, the TILT compiler uses \nboth global variables and stacks and thus requires the modifications described in section 4.1. For our \nexperiments, the TILT compiler been extended with a parallel binding construct pval and .... This construct \nis like SML's construct val and ... except that the bound expressions are executed in parallel. The collector \ncode consists of approximately 6000 lines of C code and 500 lines of assembly code (mostly glue code). \nAltogether, we implemented 6 collectors: semispace stop- copy, semispace parallel, semispace concurrent, \ngenerational stop-copy, generational parallel, and generational concur-rent. Critical functions, determined \nwith profiling, are in- lined. Currently the TILT SML compiler as well as the runtime system and garbage \ncollector runs on Alpha work- stations running Digital UNIX and on UltraSparc-II work- stations running \nSolaris. Experiments were performed on a 6-processor Enterprise 3000 server and a 64-processor Enterprise \n10000 server. Ex-periments were performed multiple times and on both ma-chines to assure that the variance \nin timing was low. In-terestingly, these two machines exhibited slightly but con-sistently different \nperformance characteristics, probably be- cause the machines have different memory subsystems. The data \npresented in this section comes from executions on the Enterprise 10000. To obtain reasonable real-time \nbehavior, the runtime sys- tem locks down all pages that are part of the heap at the be- ginning of execution. \nEven so, context switches or schedul- ing glitches can disrupt our timings. Because Solaris is not a \nreal-time operating system, we can only cope with this by running at a higher priority and performing \nthe experiments when the machine is not heavily loaded. 5.1 Benchmarks The experiments were conducted \non 15 benchmarks, some of which are standard for SML [24]. They include symbolic processing (life, knuth-bendix, \nboyer-moore, grobner poly- nomials, lexgen, frank -tree search with backtracking), nu- merical applications \n(fit, pia), large application (the TILT compiler itself), data intensive (merge sort, red-black tree), \nand parallel applications (convex-hull, barnes-hut, treap ma- nipulation). Some of the benchmarks have \nlarge arrays (convex- hull and fit), deep stacks (knuth-bendix), and high mutation rates (frank -tree \nsearch, convex-hull, and fit). Figure 1 gives for each benchmark the number of instructions (in mil- \nlions), the allocation rate (number of kilobytes per million instructions), and the mutation rate (number \nof kilobytes of modified heap objects per million instructions). Benchmark Instruction Alloc Rate Mutate \nRate (Mi) (Kb/Mi) (Kb/Mi) pia 1185 72.8 0.05 rbtree 1205 44017 '8.9 convex-hull 1754 45.7 28.6 leroy \n1289 124.6 0.00 tyan 3111 68.4 0.29 fit 5747 76.7 11.1 boyer 408 167.2 0.08 pmsort 560 55.4 10.3 eree \n6754 12.6 9.01 treap 17600 67.1 0.04 frank 4460 183.5 3.7 lexgen 1753 52.8 1.5 barnes-hut 4271 81.0 ' \n0.0 life 533 176.6 0.14 msort 165 225.4 0.20 tilt 16602 72.4 2.93 Figure 1: Benchmark characteristics. \n5.2 Cost of Parallelism and Incrementality rio understand the overhead of using a parallel and/or in- \ncremental collector, we analyze the costs of various necessary mechanisms by measuring the cost of running \non one proces- sor a collector with various features enabled. The basis for our comparison is a non-parallel, \nnon-incremental semispace collector which does not have an explicit data structure for maintaining the \nset of gray objects. Figure 2 illustrates the relative time cost of these mecha- nisms with a stacked \nbar graph. The bottom component cor- responds to the basic Stop-Copy collector. We successively take \nsteps toward a parallel, real-time collector by adding in various features. To support multiple collectors, \neach pro- cessor must maintain a local pool of memory with a space check from which it allocates space \nfor the replica objects. As infrastructure for load-balancing and traversal control, a local stack must \nbe added. Multi-threaded programs support raultiple allocator(s) with a two-level allocation scheme, \nwork tracking is needed to provide both real-time bounds and parallelism. The shared stack component \nin- cludes the costs of rooms and is needed for load-balancing. The copy-copy synchronization is necessary \nfor correctness when there are multiple collectors. Finally, increraentality is required for real-time \nbounds. The time costs of these components have been normal- ized for each benchmark so that the cost \nof the base collec- tor is 1. On average, the cost of (unbalanced) parallelism (multiple allocators, \nthe space check, support for multiple allocators, the local stack, and copy-copy synchronization) total \n25%. Finally, load-balancing (room synchronization and communication between the local stacks and the \nshared stack) requires an additional 14%. Overall, the cost of scal- able parallelism is 39%. If incrementality \nis required, an additional 12% overhead is incurred due to double alloca- tion, more frequent context \nswitches, and the write barrier. For each benchmark the total memory available was kept constant across \nthe different collector variants. This means that part of the overhead of the incrementality is due to \nslightly more frequent collections. Because of the use of a write barrier, one might wonder how efficient \na concurrent collector would be in the context of a language that is more heavily based on mutation, \nsuch as Java. We don't have conclusive evidence to answer this Component costs of a semispace parallel-concurrent \ncollector Figure 2: Time costs of parallelism and incremen-tality. The cost of the various mechanisms \nare dis-played relative to the cost of a non-parallel, non-incremental collector which is normalized \nto 1.00. The components are listed in top-down order in the legend but bottom-up in the graphs. question, \nbut we have noticed that in the applications with a reasonable amount of mutations (frank, convex-hull, \nand fit), the cost of the write barrier was not significant. As is typical with other parallel algorithms, \nthe parallel version performs worse than its sequential counterpart when run on one processor. However, \nthis overhead (39% in our case) for scalable parallelism occurs once and is more than compensated for \neven with the addition of a single processor. For example, at 4 processors, assuming a scalability of \n3.8 which we do achieve, despite the overhead there is still a net speedup of 2.7 (= 3.8 ) 1.39 \" 5.3 \nUtilization and Maximum Pause Time As discussed in Section 2.3, maximum pause time alone is too limited \nfor quantifying the degree to which a garbage collector is real-time or incremental. It fails to take \ninto account whether the mutator has reasonable access to the processor and is able to make sufficient \nprogress towards its task. Instead we defined the more general notion of mini- mum mutator utifization \n(MMU). Figure 3 shows the MMU of all benchmarks for the stop- copy collector and the parallel, concurrent \ncollector for two values of k (the rate of collection). The minimum mutator utilization is linearly plotted \nagainst time-granularity which is shown logarithmically. As expected, the utilization curves generally \nincrease as the granularity increases (though it is not strictly monotone). At the low end, the utilization \nfalls to zero when the granularity is below the slowest collection for the stop-copy collector or, for \nthe incremental collector, below the largest increment of work. Naturally, this point of minimum granularity \nis much lower for an incremental col- lector. At the high end of granularity, the stop-collector provides \ngreater utilization than an incremental collector since the incremental collector has an overhead compared \nto the stop-copy collector. There is a granularity point, below which, if utilization is of concern as \nin a real-time collector, 1 1 0 75 tilt 0.75 I[ tilt 0.75 convex /,..,.,~-.rv~ 0.75 o5 1 O7 o5 oo[ \n# (5 0 ... ~ 0.25 . : ~ 0.25 s . 0 0 0.25 . r . 77.\" 3 10 30 1003001000 3 10 30 1003001000 3 10 30 1003001000 \n0 ~\" ' ~ \" o ,:~, .... 1 3 10 30 1003001000 3 10 30 1003001000 0.75, leroy 0.75 rbtree I ~ 0,7505 0.75 \n,,. ............. ~, z// Ii 05 0.25 =,f 10.20 0 ~-----, 0 0 o,j .....i 0.25 ~ ,\"') 0 ~0 30 10o30o10o0 \n10 30 1003001000 3 10 30 1003001000 t 0.75 boyer \" \" \" 0.75 0.75[ fra 0,5 0.5 0.5 ' \"\" ' ~ ..... 025 \nf 025:4 J .... 0 \"' : ~' '\" 0.250 0 t /\" 3 lO 30 1003o0100o 10 30 1003001000 3 10 30 1003001000 1 o7~5 \n[ resort ~. -j .... . . i 0.75 0.5 0.5 0.5 f -#7 0.25[ 025 0.25J~,:\" 0 ~ : :'-/~ ' 0 0 ~ 3 10 30 1003001000 \n10 30 1003001000 3 10 30 1003001000 1 -1 Dame 0.75 treap/~ 0.5 0.5 0.5 t [~ Stop-Copy [ [ - Oono k=2.0 \nI 0,25 0,25 025 :~::/__4~.~.~2 j \", \"-/ . l o ' 3 10 30 1003001000 10 30 1003001000 3 10 30 1003001000 \nGranularity (ms) 0 0 Figure 3: MMU vs granularity (ms) of semispace collectors: non-incremental, incremental \n(k=1.2), and incremental (k=2.0) the incremental collector is preferable. This crossover point ranges \nfi'om 3-10 ms for the smaller benchmarks ~o almost 1 s for more memory-intensive applications. For each \nbenchmark the total memory available is held constant for all three collector variants. The parameter \nk controls the rate of collection and effects the utilization level Recall that k is the number of units \nof space collected for each unit allocated. When k is low. less collection work is performed relative \n~o allocation and so the collector runs less oft en, leading ~o higher utilization. However, a lower \nvalue of k requires more memory. Given that we keep the avail- able memory fixed, lowering k translates \n~o more frequent collection and potentially lower overall performance. In summary, at the l0 ms granularity, \nthe incremental, concurrent collector provides a minimum utilization of about I0% for k = 2.0 and 15% \nfor k = 1.2. On the other hand, the stop-copy collector provides 0% utilization at 10 ms granu-larity \nfor t2 of the 15 benchmarks. The concurrent collec-tor's maximum pause times range from 3 to 5 ms. 5.4 \nGenerations Next. we examine the effect of generations on utilization level. Figure 4 shows the MMU curve \nof 4 collectors which are differentiated by whether they are generational and/or incremental. Generally, \nthe presence of generations do not greatly affect the MMU curve. On average, m the non-incremental collectors, \ngenerations only slightly lower the maximum pause time. Clearly, to gain significant (worst- case) responsiveness, \nadding generations is insufficient and incrementality is necessary. We also note that although it appears \nfrom the diagrams Figure 4: MMU vs granularity (ms) of 4 col-lectors. Generational collectors are marked \nwith darker lines. Independently, the line for the incre-mental collectors are (lashed. 3 10 30 100 300 \n1000 3 10 30 1003001000 1 0.75 t Gen Stop-Copy Semi Stop-Copy 0.5 f 0.25 Gen Conc Semi Conc 0 L 3 10 \n30 100 300 1000 Granularity (ms) that the generational concurrent collector is often less effi- cient \nthan the semispace concurrent version, this is actually not the case. Because of the locality effects \nof the genera- tional collector, the mutator runs significantly faster, so the overall time is faster \nalthough the utilization is sometimes worse. For efficiency and real-time bounds, both generations and \nconcurrency are required. 5.5 Scalability We consider two types of benchmarks for testing scalabil- ity. \nThe first group includes coarsely parallel programs in which there are as many threads as processors, \neach thread running a sequence of non-parallel benchmarks in a differ-ent order. The second group consists \nof the benchmarks that are already (finely) parallel using a fork-join construct (convex-hull, barnes-hut, \nand treap). The semispace paral- lel non-concurrent collector is used for the experiments in this section. \nFigure 5 show how the various time components of the ap- plication change as the number of processors \nvaries. The top graph corresponds to the coarsely parallel benchmarks and the bottom graph to the finely \nparallel benchmarks. Each bar in the graphs represents the total normalized work (the normalized sum \nof the times across the processors) of that benchmark. The 4 components, from bottom to top, are garbage \ncollecting (GC), idling during collection (Ge-Idle), running the application (Mutator), and idling during \nappli- cation [Mutator-Idle). Linear scalability corresponds to a flat line starting at the single processor \ncase. For the coarsely parallel benchmarks (top graph of fig- ure 5), the executions compare the effect \nof load-balancing as the number of processors vary from 1 to 8. All three benchmarks show similar behavior. \nWith almost no inter-action among the threads~ it is unsurprising that the appli- cation scaled almost \nperfectly with almost no Mutator-Idle time. What little mutator idle time is presenl; indicates that \nsome threads finish slightly before others even though each is running the same program. On the other \nhand, the effect Scalability (Work vs. Processors) Scalability (Wok vs. Processors) ~.~ ~\"-'-~-L~*-~ \n15 #9\",\":~ GC-Idle Mutalor ~-k4 --Mutator-ldle 1.3 \"~ 1.2 ............... S ......... -~ ...................... \n-\"iii!ii?iii: ~ii~,ii!i!i!iii!ii? __:!:i;:iiiii:';!;)!i!i:i;!i ..... ~!ii!!ii!i:i~!i!i!i!)iiii!:i!:i!!~: \n.............. ~i!;i!i;iiiiii;iiiii;iliiiii! .................. ~ )!;~ mm ii !i)i; i;!ii:i) ~ ~08 12345678 \n12345678 12345678 12345678 12345678 12345678 Balance NoBalan~e Balance NoBalance Balanoe NoBalan~e leroy-lft-rbtree \nrbtree-msort-frank life-fft-rbtree Scalability (Work vs. Processors) E E g -E z 4 8 12 16 '2(3 24 28 \n32 4 8 12 16 20 24 R8 32 4 8 12 16 20 24 28 barnes-hut4 treap4 tree4 Figure 5: Total processor time \nspent in the mutator and collector for coarsely (top) and finely (bottom) parallel benchmarks. The components \nare listed in top-down order in the legend but bottom-up in the graphs. of load-balancing on the scMability \nof the garbage collec- tor is pronounced. With load-balancing, there is almost no GC-Idte time. However, \nwithout load-balancing, GO-Idle is comparable to GC so we are wasting half our resources with- out load-balancing. \nAs expected, this effect is more evident as the number of processors increases. For the finely parallel \nbenchmarks (bottom graph of Fig- ure 5), we examine the benchmarks only with load-balancing enabled. \nUp to 32 processors, the total collector times (~C 4- 6C-Idle) increases on average by 86% while the \ncorrespond- ing mutator increase is 246%. In other words, the collector is scaling far better than the \napplication. In terms of speedup, the collector and mutator have respective speedups of 17.2 and 9.2 \nat 32 processors. At 8 processors, the respective speedups are 7.8 and 7.2. The idle time of the mutator \ngreatly varies depending on the parallel application while the collector shows more consistent behavior. \nWe conclude that our collector scales almost perfectly at 8 processors and Balance NoBalance i,~-@3 treap3 \nFigure 6: Total processor time spent, in garbage collection for the treap benchmark with and with-out \nload balancing. The components are listed in top-down order in the legend but bottom-up in the graph. \n at slightly better than 50% linear speedup at 32 processors. In both cases, load-balancing is critical \nto achieving this. Next, we examine the effect of two factors on collection sealability in Figures 6 \nand 7 by considering only the time components within the collection. In these graphs, the bot- tom component \nGO-Work corresponds to when a processor is actively doing collection work such as graying objects, allocating \nspace, copying fields, and so on. The next com-ponent up GC-1loomEnter corresponds to time spent when \na processor is waiting for access to a room. GC-Comm repre- sents the time to transfer objects to and \nfrom the shared stack while in a room. GC-RoomExit is the time for exiting a room. Finally, GC-Idle measures \ntime while a processor idles, waiting for work to appear on the shared stack. Figures 6 shows the effect \nof load-balancing on the treap benchmark. Since the treap benchmark involves threads op- erating on the \nsame tree, one might speculate that. load- balancing is not needed or much less needed than in the coarsely \nparallel benchmarks where the threads are unre-lated. In fact, load-balancing is still critical. Without \nload- balancing, the amount of work or time spent, in collection is approximately 3 times higher. Other \nbenchmarks (not shown) in this category show similar behavior. Figures 7 illustrates that dataset size \nwhich is propor- tional to the amount of data copied has a significant impact on sealability. The four \ngroups of data in this graph corre-spond, from left to right, to dataset sizes that double from one to \nthe next. The times are normalized so that the single- processor case is 1.0 despite changes in total \nwork. At 32 processors, scalability improves from 5.6 to 17.7 as we move from the smallest to the largest \ndataset. 6. RELATED WORK Concurrent garbage collection was independently intro- duced by Steele [23] \nand Dijkstra [7], both of whom based their work on mark-and-sweep collectors. By extending Ch- eney's \ncopying collector [4] with a read barrier, Baker pro- 134 ScalaNity (Work vs. Processors) 6 ~r rrrrnm,,,,, \n......... rtrm..,,,,,,,,,.,,,u,,,,m,,, .m,,,,,,,,,,mm,,,,,,m.~m,. ..................... mmHH  5 75~ \n............................................................ \" 5 L ............ i ...................................... \n5 25 ] ~ ] I~l GC-Work GC-Roomenter GC-RoomExit ~. 5 F | .................. I mi GC-ldlo z g tu g z \ntreapl treap2 treap3 treap4 Figure 7: Total processor time spent in garbage col- lection for the treap \nbenchmark at different dataset sizes. The components are listed in top-down order in the legend but bottom-up \nin the graph. posed the first real-time copying collector with bounds on memory use and time [1]. He \nproved that if each alloca- tion copied k locations then his collector would require only 2(R(1 + i/k)) \nlocations. He also suggested how it could be extended to arbitrarily sized structures, although this \nrequired both a read and write barrier and also an extra link-word for every structure. As part of the \nMultilisp system, Halstead developed a multiple-mutator multiple-collector variant of Baker's ba- sic \nalgorithm for shared memory multiprocessors [18]. In their scheme, every processor maintains its own \nfrom- and to-space and traces its own root set to move objects from any from-space to its own to-space. \nThis algorithm has sev- eral properties that make it neither time nor space efficient, both in theory \nand practice. First, the separation of space among the processors can lead to one processor running out \nof space, while others have plenty left over. In addition to maldng it unlikely that any space bounds \nstronger than 2RP can be shown (i.e., a factor of P more memory than the sequential version), Halstead \nnoted that this problem ac- tually occurred in practice. A related problem is that since each processor \ndoes its own collection from its own root set without sharing the work, one processor could end up doing \nmuch more copying than the others. Since no processor can discard its from-space until all processors \nhave completed scanning, all processors will have to wait while the one pro- cessor completes its unfair \nshare of the collection. Herlihy and Moss [19] described a lock-free variant of the Multilisp algorithm. \nThe locks are avoided by keeping mul- tiple versions of every object in a linked list. Although the algorithm \nis lock-free it cannot make guarantees about space or time. For example, to read or write an object might \nre- quire tracing down an arbitrarily long linked list. Endo [12] solved the problem with work imbalance \nby using work stealing to share the work among processors. His collector was a mark-sweep conservative \ncollector for C++. It was a stop-collect algorithm and therefore made no attempt to be real-time. Endo \nwas able to show good scalability on up to 64 processors of a shared-memory ma- chine (the same one we \nused), and furthermore noted, as we also found, that load-balancing is critical to achieving scalability. \nEndo's work was extended by Flood et. al. for a copying collector [15]. We use work-sharing instead of \nwork-stealing because it allowed us to prove our time and space bounds [2]. We expect that we could replace \nore\" work- sharing with work-stealing with little practical effect. Doligez and Gonthier describe a multiple \nmutator single collector algorithm [9]. They give no bounds on either time or space. In fact the collector \ncan generate garbage faster than it collects it, requiring unbounded memory. More gen- erally, no single \ncollector algorithm can scale beyond a few processors since one processor cannot keep up with an arbi- \ntrary number number of mutators. On the other hand the Doligez-Gonthier algorithm has some properties--including \nminimal synchronizations and no overheads on reads--that make it likely to be practical on small multiprocessors. \nDo-mani et. al. describe a generational version of the collec- tor [11], but it has the same scalability \nissues as the original. Our replicating algorithm is loosely based on the replicat- ing scheme of Nettles \nand O'Toole [22]. Beyond the basic idea of replication, however, there are few similarities. This is \nlargely due to the fact that they do not consider multi- pie collector threads and do not incrementally \ncollect large structures. IncrementM or real-time collectors are based on a variety of techniques, leading \nto varying worst-case pause times: 2 - 5 ms [13], 15 ms [20], 50 ms [22], and 360 ms [10]. However, direct \ncomparison of worst-case pause times without consid- ering mutator access to the processor can be misleading. \n7. ONGOING WORK Approximately half tile loss in scalability arises from im- perfect load-balancing. We \nbelieve that strategies that are sensitive to both the amount of work that is locally andglob-ally available \ncan maintain or improve load-balancing while decreasing room usage (which can have high synchronization \ncosts if overused). Providing better real-time behavior requires reducing the granularity of the collection \nincrements and/or maintaining or increasing the MMU level. Reduction of granularity can be achieved through \nadjustments of parameters, further sub- division of stacklet-related work, and improving the context- \nswitching code of the collector. Recently, we have found that scheduling the collection work by combining \nhigh-resolution timers with our notion of work can minimize throughput variations so that MMU can be \nimproved. Preliminary fig- ures indicate that we can improve MMU from 15% to about 25% and that pause \ntimes can be reduced from 3 to 1.5 ms. A more systematic and thorough treatment of the work in this paper \nand our earlier paper [2] canbe found in Cheng's doctoral thesis [5]. 8. CONCLUSION We have presented \nan implementation of a scalably par- allel, concurrent, real-time garbage collector. The collector allows \nfor multiple collector and mutator threads to run con- currently with minimal thread synchronization. \nWe believe that our modified algorithm maintains the time and space bounds of our previous theoretical \nalgorithm, which gives some justification for calling the algorithm \"real-time\". Our experiments and \nthe analysis of the MMU across 15 bench- marks gives further justification. Acknowledgements. Thanks \nto Toshio Endo and the Yonezawa Laboratory for use of their Enterprise 10000. Thanks go to the general \nsupport of Robert Harper and the FOX project. We are grateful to Doug Baker for his comments on an earlier \ndraft. 9. REFERENCES [1] Henry G. Baker. List processing in real-time on a serial computer. Communications \nof the A CM, 21(4):280-94, 1978. Also AI Laboratory Working Paper 139, 1977. [2] Guy E. Blelloch and \nPerry Cheng. On bounding time and space for multiprocessor garbage collection. In Proc. A CM SIGPLAN \nConference on Programming Languages Design and ~mplernentation, ACM SIGPLAN Notices, pages 104-117, Atlanta, \nMay 99. ACM Press. [3] Guy E. Blelloch, Perry Cheng, and Phil Gibbons. Room synchronizations. In A CM \nSymposium on Parallel Algorithms and Architecture. ACM Press, July 2001. [4] C. J. Cheney. A non-recursive \nlist compacting algorithm. Communications of the A CM, 13(11):677-8, November 1970. [5] Perry Cheng. \nScalable Real-time Parallel Garbage Collection for Symmetric Multiprocessors. PhD thesis, Carnegie Mellon \nUniversity, 2001. [6] Perry Cheng, Robert Harper, and Peter Lee. Generational stack collection and profile-driven \npretenuring. In Proc. ACM SIGPLAN Conference on Programming Languages Design and Implementation, ACM \nSIGPLAN Notices, Montreal, June 1998. ACM Press. [7] Edsgar W. Dijkstra, Leslie Lamport, A. J. Martin, \n C. S. Scholten, and E. F. M. Steffens. On-the-fly garbage collection: An exercise in cooperation. In \n Lecture Notes in Computer Science, No. 46. Springer-Verlag, New York, 1976. [8] Edsgar W. Dijkstra, \nLeslie Lamport, A. J. Martin, C. S. Scholten, and E. F. M. Steffens. On-the-fly garbage collection: \nAn exercise in cooperation. Communications of the ACM, 21(11):965-975, November 1978.  [9] Damien Doligez \nand Georges Gonthier. Portable, unobtrusive garbage collection for multiprocessor systems. In Proc. ACM \nSymposium on Principles of Programming Languages, ACM SIGPLAN Notices. ACM Press, January 1994. [10] \nDamien Doligez and Xavier Leroy. A concurrent generational garbage collector for a multi-threaded implementation \nof ML. In Proc. A CM Symposium on Principles of Programming Languages, ACM SIGPLAN Notices, pages 113-123. \nACM Press, January 1993. [11] Tamar Domani, Elliot Kolodner, and Erez Petrank. A generational on-the-fly \ngarbage collector for Java. In Proe. A CM S[GPLA N Conference on Programming Languages Design and Implementation, \npages 274-284, May 2000. [12] Toshio Endo. A scalable mark-sweep garbage collector on large-scale shared-memory \nmachines. Master's thesis, University of Tokyo, February 1998. [13] Steven L. Engelstad and James E. \nVandendorpe. Automatic storage management for systems with real time constraints. In Paul R. Wilson \nand Barry Hayes, editors, OOPSLA/ECOOP '91 Workshop on Garbage Collection in Object- Oriented Systems, \nAddendum to OOPSLA '9.l Proceedings, October 1991. [14] Robert R. /a%nichel and Jerome C. Yochelson. \nA bisp garbage collector for virtual ntemory computer systems. Communications of the A CM, 12(11):611 \n612, November 1969. [15] Christine Flood, Dave Detlefs, Nir Shavit, and Catherine Zhang. Parallel garbage \ncollection for shared memory multiprocessors. In Proe. USENIX JVM Conference, April 2001. [16] Seth \nGoldstein, Lazy Threads: Compiler and Runtime Structures for Fine-Grained Parallel Programming. PhD thesis, \nUniversity of California at Berkeley, Fall 1997. [17] A. Gottlieb, B. D. Lubachevsky, and L. Rudolph. \nBasic techniques for the efficient coordination of very large numbers of cooperating sequqnetial processors. \nA CM Trans. on Programming Languages and Systems, 5(2):164-189, April 1983. [18] Robert H. Ha lstead. \nMultilisp: A language for concurrent symbolic computation. A CM Transactions on Programming Languages \nand Systems, 7(4):501-538, October 1985. [19] Maurice Herlihy and J. Eliot B Moss. Lock-free garbage \ncollection for multiprocessors. IEEE Transactions on Parallel and Distributed Systems, 3(3), May 1992. \n[20] Martin Larose and Marc Feeley. A compacting incremental collector and its performance in a production \nquality compiler. In Richard Jones, editor, Proc. International Syrup. on Memory Management, volume 34(3) \nof ACM SIGPLAN Notices, pages 1-9, Vancouver, October 1998. ACM Press. [21] John McCarthy. Recursive \nfunctions of symbolic expressions and their computation by machine. Communications of the ACM, 3:184-195, \n1960. [22] Scott M. Nettles and James W. O'Toole. Real-time replication-based garbage collection. In \nProc. A CM SIGPLAN Conference on Programming Languages Design and Implementation, volume 28(6) of ACM \nSIGPLAN Notices. ACM Press. June 1993. [23] Guy L. Steele. Multiprocessing compactifying garbage collection. \nCommunications of the A CM, 18(9):495-508, September 1975. [24] Tarditi, Morrisett, Cheng, Stone, Harper, \nand Lee. TIL: A type-directed optimizing compiler for ML. In Proc. A CM S[GPLAN Conference on Programming \nLanguages Design and Implementation, pages 181-192, 1996. [25] David M. Ungar and David A. Patterson. \nBerkeley Smalltalk: Who knows where the time goes? In Glenn Krasner, editor, Smalltalk-80: Bits of History, \nWords of Advice, pages 189-206. Addison-\u00a5Vesley, 1983.    \n\t\t\t", "proc_id": "378795", "abstract": "<p>We describe a parallel, real-time garbage collector and present experimental results that demonstrate good scalability and good real-time bounds. The collector is designed for shared-memory multiprocessors and is based on an earlier collector algorithm [2], which provided fixed bounds on the time any thread must pause for collection. However, since our earlier algorithm was designed for simple analysis, it had some impractical features. This paper presents the extensions necessary for a practical implementation: reducing excessive interleaving, handling stacks and global variables, reducing double allocation, and special treatment of large and small objects. An implementation based on the modified algorithm is evaluated on a set of 15 SML benchmarks on a Sun Enterprise 10000, a 64-way UltraSparc-II multiprocessor. To the best of our knowledge, this is the first implementation of a parallel, real-time garbage collector.</p><p>The average collector speedup is 7.5 at 8 processors and 17.7 at 32 processors. Maximum pause times range from 3 ms to 5 ms. In contrast, a non-incremental collector (whether generational or not) has maximum pause times from 10 ms to 650 ms. Compared to a non-parallel, stop-copy collector, parallelism has a 39% overhead, while real-time behavior adds an additional 12% overhead. Since the collector takes about 15% of total execution time, these features have an overall time costs of 6% and 2%.</p>", "authors": [{"name": "Perry Cheng", "author_profile_id": "81451593218", "affiliation": "Computer Science Department, Carnegie Mellon University, Pittsburgh, PA", "person_id": "PP43116113", "email_address": "", "orcid_id": ""}, {"name": "Guy E. Blelloch", "author_profile_id": "81100282539", "affiliation": "Computer Science Department, Carnegie Mellon University, Pittsburgh, PA", "person_id": "P100820", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/378795.378823", "year": "2001", "article_id": "378823", "conference": "PLDI", "title": "A parallel, real-time garbage collector", "url": "http://dl.acm.org/citation.cfm?id=378823"}